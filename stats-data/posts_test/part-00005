  <row AnswerCount="2" Body="&lt;p&gt;I've been hunting around looking for a way to do this problem. I need to create a program to calculate linear regression for 100 3-dimensional points. I also have the matching outcomes of the points, so it's like a training set rather than a testing set.&#10;I'm also instructed to consider the bias term, but I'm not sure what that means.&lt;/p&gt;&#10;&#10;&lt;p&gt;I can find good documentation for 2-dimensional points, but not for 3 dimensions. Since there's so many equations for doing XY data points, surely there's a similar equation set for XYZ points. I'm not really desiring to be given the equation outright, but more interested in an understanding of the function and how it works, as well as how it's determined. Thanks to anyone who read and who can help me with this problem.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-03-01T20:48:05.630" Id="88426" LastActivityDate="2014-03-02T03:43:50.390" OwnerDisplayName="user2828965" OwnerUserId="41186" PostTypeId="1" Score="1" Tags="&lt;regression&gt;" Title="linear regression with 3-dimensional points" ViewCount="254" />
  
  
  
  <row AcceptedAnswerId="89155" AnswerCount="1" Body="&lt;p&gt;In simple linear regression, we have $y = \beta_0 + \beta_1 x + u$, where $u \sim iid\;\mathcal N(0,\sigma^2)$. I derived the estimator: &#10;$$&#10;\hat{\beta_1} = \frac{\sum_i (x_i - \bar{x})(y_i - \bar{y})}{\sum_i (x_i - \bar{x})^2}\ ,&#10;$$&#10;where $\bar{x}$ and $\bar{y}$ are the sample means of $x$ and $y$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Now I want to find the variance of $\hat\beta_1$. I derived something like the following: &#10;$$&#10;\text{Var}(\hat{\beta_1}) = \frac{\sigma^2(1 - \frac{1}{n})}{\sum_i (x_i - \bar{x})^2}\ .&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The derivation is as follow:&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{align}&#10;&amp;amp;\text{Var}(\hat{\beta_1})\\&#10;&amp;amp; =&#10;\text{Var} \left(\frac{\sum_i (x_i - \bar{x})(y_i - \bar{y})}{\sum_i (x_i - \bar{x})^2} \right) \\&#10;&amp;amp; =&#10;\frac{1}{(\sum_i (x_i - \bar{x})^2)^2} \text{Var}\left( \sum_i (x_i - \bar{x})\left(\beta_0 + \beta_1x_i + u_i - \frac{1}{n}\sum_j(\beta_0 + \beta_1x_j + u_j) \right)\right)\\&#10;&amp;amp; =&#10;\frac{1}{(\sum_i (x_i - \bar{x})^2)^2} &#10;\text{Var}\left( \beta_1 \sum_i (x_i - \bar{x})^2 + &#10;\sum_i(x_i - \bar{x})&#10;\left(u_i - \sum_j \frac{u_j}{n}\right) \right)\\&#10;&amp;amp; =&#10;\frac{1}{(\sum_i (x_i - \bar{x})^2)^2}\text{Var}\left( \sum_i(x_i - \bar{x})\left(u_i - \sum_j \frac{u_j}{n}\right)\right)\\&#10;&amp;amp; =&#10;\frac{1}{(\sum_i (x_i - \bar{x})^2)^2}\;\times \\&#10;&amp;amp;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;E\left[\left( \sum_i(x_i - \bar{x})(u_i - \sum_j \frac{u_j}{n}) - \underbrace{E\left[\sum_i(x_i - \bar{x})(u_i - \sum_j \frac{u_j}{n})\right] }_{=0}\right)^2\right]\\&#10;&amp;amp; =&#10;\frac{1}{(\sum_i (x_i - \bar{x})^2)^2} &#10;E\left[\left( \sum_i(x_i - \bar{x})(u_i - \sum_j \frac{u_j}{n})\right)^2 \right] \\&#10;&amp;amp; =&#10;\frac{1}{(\sum_i (x_i - \bar{x})^2)^2} E\left[\sum_i(x_i - \bar{x})^2(u_i - \sum_j \frac{u_j}{n})^2 \right]\;\;\;\;\text{ , since } u_i \text{ 's are iid} \\&#10;&amp;amp; =&#10;\frac{1}{(\sum_i (x_i - \bar{x})^2)^2}\sum_i(x_i - \bar{x})^2E\left(u_i - \sum_j \frac{u_j}{n}\right)^2\\&#10;&amp;amp; =&#10;\frac{1}{(\sum_i (x_i - \bar{x})^2)^2}\sum_i(x_i - \bar{x})^2 \left(E(u_i^2) - 2 \times E \left(u_i \times (\sum_j \frac{u_j}{n})\right) + E\left(\sum_j \frac{u_j}{n}\right)^2\right)\\&#10;&amp;amp; =&#10;\frac{1}{(\sum_i (x_i - \bar{x})^2)^2}\sum_i(x_i - \bar{x})^2 &#10;\left(\sigma^2 - \frac{2}{n}\sigma^2 + \frac{\sigma^2}{n}\right)\\&#10;&amp;amp; =&#10;\frac{\sigma^2}{\sum_i (x_i - \bar{x})^2}\left(1 - \frac{1}{n}\right)&#10;\end{align}&lt;/p&gt;&#10;&#10;&lt;p&gt;Did I do something wrong here?&lt;/p&gt;&#10;&#10;&lt;p&gt;I know if I do everything in matrix notation, I would get ${\rm Var}(\hat{\beta_1}) = \frac{\sigma^2}{\sum_i (x_i - \bar{x})^2}$. But I am trying to derive the answer without using the matrix notation just to make sure I understand the concepts. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-03-02T15:56:38.557" FavoriteCount="6" Id="88461" LastActivityDate="2014-03-07T13:35:38.177" LastEditDate="2014-03-07T02:00:33.393" LastEditorUserId="25936" OwnerUserId="40761" PostTypeId="1" Score="4" Tags="&lt;regression&gt;&lt;mathematical-statistics&gt;&lt;variance&gt;&lt;linear-model&gt;&lt;regression-coefficients&gt;" Title="Derive Variance of regression coefficient in simple linear regression" ViewCount="1779" />
  <row Body="&lt;p&gt;This is easy to treat with least squares. You just want to weight each point by the inverse variance if you know that or can estimate it. Most regression packages allow you to input either the weights or variance. &lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-03-02T17:31:06.717" Id="88467" LastActivityDate="2014-03-02T17:31:06.717" OwnerUserId="40967" ParentId="88276" PostTypeId="2" Score="0" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I'm facing a challenge: writing my own general equilibrium model in MATLAB. I would like to ask for:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;the basic needed knowledge in mathematics&lt;/li&gt;&#10;&lt;li&gt;the basic needed knowledge in programming&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;with references if possible.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm a master's degree student in economics, but I'm willing to put in the necessary efforts to achieve my aim. Many thanks in advance.&lt;/p&gt;&#10;" ClosedDate="2014-03-02T23:28:01.287" CommentCount="1" CreationDate="2014-03-02T18:34:41.747" Id="88473" LastActivityDate="2014-03-02T19:49:36.773" LastEditDate="2014-03-02T18:42:06.760" LastEditorUserId="32036" OwnerUserId="1251" PostTypeId="1" Score="0" Tags="&lt;matlab&gt;&lt;econometrics&gt;&lt;programming&gt;" Title="Steps to write my own general equilibrium model in MATLAB" ViewCount="236" />
  <row Body="&lt;ul&gt;&#10;&lt;li&gt;Non-parametric refers to a test that makes no assumption about the parameters of the data, whereby the parameters of a probability distribution are usually meant.&lt;/li&gt;&#10;&lt;li&gt;The output of the Shapiro-Wilk test is whether your data are more specifically &lt;strong&gt;normally&lt;/strong&gt; distributed. &lt;/li&gt;&#10;&lt;li&gt;The S-W test is well behaved for small to moderate datasets. If you get result of a p-value of 0.07, you cannot reject normality. So, you can use parametric tests for all your data.&lt;/li&gt;&#10;&lt;li&gt;PS: Even if your data were somewhat non-normal you could still apply some of the parametric tests, because of their robustness to departures from normality, such as t-tests and ANOVA, where the Central Limit Theorem kicks in very effectively.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="1" CreationDate="2014-03-02T18:59:26.693" Id="88475" LastActivityDate="2014-03-02T19:04:50.100" LastEditDate="2014-03-02T19:04:50.100" LastEditorUserId="36545" OwnerUserId="36545" ParentId="88470" PostTypeId="2" Score="0" />
  
  
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Two cyclists are racing over 50 (for example) km. Cyclist A has an estimated speed of X km per hour, cyclist B has an estimated speed of Y km per hour. Both of these estimated speeds are the means of two normal curves of the true actual speed. &lt;/p&gt;&#10;&#10;&lt;p&gt;The two normal curves overlap. If I know the estimated speeds of both A and B, and the respective standard deviations of their normal curves, how do I calculate the probability of A winning and of B winning? &lt;/p&gt;&#10;&#10;&lt;p&gt;What about when there are n cyclists, not just two?&lt;/p&gt;&#10;&#10;&lt;p&gt;And, if I only know the probabilities of A winning and B winning, and I guess the standard deviations of their normal curves, how can I calculate their relative speeds? &lt;/p&gt;&#10;&#10;&lt;p&gt;And for n cyclists?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-03-02T21:43:26.357" Id="88497" LastActivityDate="2014-03-02T21:43:26.357" OwnerUserId="40491" PostTypeId="1" Score="0" Tags="&lt;probability&gt;&lt;normal-distribution&gt;" Title="Probability - overlapping normal curves" ViewCount="112" />
  
  
  <row Body="&lt;p&gt;This looks like a truncated power basis. The answer is b) although $h_5(X)$ will only be non-zero if $X$ is greater than $\xi_1$ and similarly for $h_6(X)$ and $\xi_2$&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-03-02T22:48:32.133" Id="88505" LastActivityDate="2014-03-02T22:48:32.133" OwnerUserId="30201" ParentId="88490" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;I don't quite understand this.&lt;/p&gt;&#10;&#10;&lt;p&gt;A question was, pretend we have 4 predictors and all of them are binary - for the Naive Bayes method, how many parameters are there to estimate in the training step? The answer was 2*4.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then, another question was, if we draw an edge between predictors 1 and 2 to create a graphical model, how many parameters do we need to estimate in the training step? The answer was 1+1(2)+(2*7)=11.&lt;/p&gt;&#10;&#10;&lt;p&gt;Finally how about if all of the predictors have edges in between? The answer was 1+2(15)=31.&lt;/p&gt;&#10;&#10;&lt;p&gt;I don't get how these equations are being constructed. Could someone explain please?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-03-03T00:18:04.823" Id="88507" LastActivityDate="2014-05-07T07:57:51.977" LastEditDate="2014-03-03T00:41:56.977" LastEditorUserId="41216" OwnerUserId="41216" PostTypeId="1" Score="0" Tags="&lt;data-mining&gt;&lt;graphical-model&gt;&lt;predictor&gt;" Title="How do you calculate the amount of parameters needed to be estimated?" ViewCount="31" />
  <row AcceptedAnswerId="88794" AnswerCount="1" Body="&lt;p&gt;In the free online book &lt;a href=&quot;https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers&quot; rel=&quot;nofollow&quot;&gt;Bayesian Methods for Hackers&lt;/a&gt;, the last figure shows the estimation of the expected value of $\lambda$ for any given day:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/jkoAp.png&quot; alt=&quot;ente for image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;It looks like the author is calculating the expected value of the posterior for $\lambda$ for a given day. To do so, the author re-uses the MCMC samples from the full estimation of $\lambda_1$ and $\lambda_2$. &lt;/p&gt;&#10;&#10;&lt;p&gt;But how does the &lt;strong&gt;posterior of $\lambda$ for a given day&lt;/strong&gt; relate to $\lambda_1$ and $\lambda_2$? And why can one &lt;strong&gt;re-use&lt;/strong&gt; the samples from the full MCMC calculation?&lt;/p&gt;&#10;&#10;&lt;p&gt;In case it matters, below is the Python code in question that calculates the red line in the graph above.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;figsize(12.5, 5)&#10;# tau_samples, lambda_1_samples, lambda_2_samples contain&#10;# N samples from the corresponding posterior distribution&#10;N = tau_samples.shape[0]&#10;expected_texts_per_day = np.zeros(n_count_data)&#10;for day in range(0, n_count_data):&#10;    # ix is a bool index of all tau samples corresponding to&#10;    # the switchpoint occurring prior to value of 'day'&#10;    ix = day &amp;lt; tau_samples&#10;    # Each posterior sample corresponds to a value for tau.&#10;    # for each day, that value of tau indicates whether we're &quot;before&quot;&#10;    # (in the lambda1 &quot;regime&quot;) or&#10;    #  &quot;after&quot; (in the lambda2 &quot;regime&quot;) the switchpoint.&#10;    # by taking the posterior sample of lambda1/2 accordingly, we can average&#10;    # over all samples to get an expected value for lambda on that day.&#10;    # As explained, the &quot;message count&quot; random variable is Poisson distributed,&#10;    # and therefore lambda (the poisson parameter) is the expected value of&#10;    # &quot;message count&quot;.&#10;    expected_texts_per_day[day] = (lambda_1_samples[ix].sum()&#10;                                   + lambda_2_samples[~ix].sum()) / N&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-03-03T02:24:38.360" Id="88515" LastActivityDate="2014-03-04T22:27:40.463" LastEditDate="2014-03-03T02:40:29.520" LastEditorUserId="2798" OwnerUserId="2798" PostTypeId="1" Score="0" Tags="&lt;bayesian&gt;&lt;mcmc&gt;&lt;pymc&gt;" Title="Analytical formulation of a Hierarhical Bayes problem" ViewCount="72" />
  
  <row Body="&lt;p&gt;In an attempt to... un-unanswer this question:&lt;/p&gt;&#10;&#10;&lt;p&gt;If one can write the derivative of the log-likelihood as the OP states, then this estimator &lt;em&gt;equals&lt;/em&gt; the true value always, irrespective of the realized sample (what a dream, hey?). This is because we choose the estimator so as to make this derivative zero:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\hat \theta : \frac{\partial l(\hat \theta \mid X_1, \dots , X_n)}{\partial \theta} =0$$&lt;/p&gt;&#10;&#10;&lt;p&gt;So, if $$\frac{\partial l(\hat \theta \mid X_1, \dots , X_n)}{\partial \theta} =a(n, \theta) \cdot (\hat{\theta} - \theta) =0 \Rightarrow \hat \theta = \theta$$&lt;/p&gt;&#10;&#10;&lt;p&gt;(the case $a(n, \theta) =0$ is trivial). In such a case the estimator does not really need any of the usual properties, obviously.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-03T03:24:59.830" Id="88521" LastActivityDate="2014-03-03T03:24:59.830" OwnerUserId="28746" ParentId="31260" PostTypeId="2" Score="1" />
  <row AnswerCount="2" Body="&lt;p&gt;I have a binary classification problem. I trained my dataset using a Support Vector Machine (SVM). Now I want to report the model I trained to a 3rd party so that they can use.&lt;/p&gt;&#10;&#10;&lt;p&gt;For the primal probem of SVM, the classifier equation is of the form,&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;y = w.x + b&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;where &lt;code&gt;w&lt;/code&gt; is the weight vector, &lt;code&gt;b&lt;/code&gt; is the constant and &lt;code&gt;x&lt;/code&gt; is the new instance. Depending on the value of &lt;code&gt;y&lt;/code&gt;, we can predict the class.&lt;/p&gt;&#10;&#10;&lt;p&gt;I could report the weight vector w and constant b which was obtained my model.&lt;/p&gt;&#10;&#10;&lt;p&gt;But if I want to test my model using cross-validation, then how do I report my model? Everytime I cross-validate, the equation of the classifier built will change.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-03T06:00:20.003" FavoriteCount="1" Id="88535" LastActivityDate="2014-03-03T20:09:48.150" OwnerUserId="41232" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;svm&gt;&lt;cross-validation&gt;&lt;performance&gt;" Title="How to report a SVM model to a 3rd party after cross-validation?" ViewCount="110" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am working on phylogeography on one model species and I am a beginner. You can imagine one species that came on locality more than 5 mya (million years ago). It was a good environment, so the species expanded very quickly. After climatic changes this model species split in two refugiums (isolated locations for a once widespread species) and evolved into two species. This happened several times and now we have from that one ancestral species six valid species.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have data from MrBayes. Can you recommend some phylogeographical program which can consider also expansion and contraction of model species? &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;My input data:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;coordinates (lat, lon) for each species&lt;/li&gt;&#10;&lt;li&gt;geographical data of a continent (altitude, precipitation etc.)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;My  goal:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Create a map where will be polygons (areas) of each species. These areas will expand and contract (step-by-step not like a do not want create video).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;My problem:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I am able to make polygons by MCP, or kernel density, but unable to incorporate barriers into model.  For example, polygons of species which lives in mountain region created by MCP include areas of lowland inside...&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-03-03T08:30:46.237" FavoriteCount="1" Id="88545" LastActivityDate="2014-03-03T08:55:14.410" LastEditDate="2014-03-03T08:55:14.410" LastEditorUserId="28218" OwnerUserId="41203" PostTypeId="1" Score="2" Tags="&lt;data-visualization&gt;" Title="How to visualize phylogeographical data?" ViewCount="60" />
  <row AcceptedAnswerId="88556" AnswerCount="1" Body="&lt;p&gt;I am working on some data, more specifically some predictions of some outcomes.&#10;The predictions vary on the continuous scale, between $-3$ and $3$. They can for example be:&lt;/p&gt;&#10;&#10;&lt;p&gt;$x_1=-2.4, x_2=-2.1, x_3=1.4, x_4=0.4, \cdots, x_n=-1.2$&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, for each prediction, I also have the corresponding results,&lt;/p&gt;&#10;&#10;&lt;p&gt;$y_1, y_2, \cdots , y_n$,&lt;/p&gt;&#10;&#10;&lt;p&gt;which also are continuous.&lt;/p&gt;&#10;&#10;&lt;p&gt;How can I check if my predictions are good or not? I do not have any knowledge whatsoever on how the predictions are made, thus cannot create any confidence interval around the predictions.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-03T10:55:01.223" Id="88554" LastActivityDate="2014-03-03T11:19:18.300" LastEditDate="2014-03-03T11:19:18.300" LastEditorUserId="22047" OwnerUserId="30589" PostTypeId="1" Score="1" Tags="&lt;prediction&gt;&lt;expected-value&gt;" Title="How to compare observed and expected outcomes for continuous data" ViewCount="99" />
  <row AcceptedAnswerId="88583" AnswerCount="1" Body="&lt;p&gt;I have a similarity matrix M - the value M(i,j) indicates the similarity between two elements i and j. &lt;/p&gt;&#10;&#10;&lt;p&gt;I want to approximate that matrix using a Gaussian Mixture model or I want to cluster that matrix into set of K clusters.&lt;/p&gt;&#10;&#10;&lt;p&gt;How can I do it? I am not sure about the input of the clustering algorithm.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-03T12:13:14.330" FavoriteCount="2" Id="88561" LastActivityDate="2014-03-03T15:37:03.873" OwnerUserId="11409" PostTypeId="1" Score="4" Tags="&lt;machine-learning&gt;&lt;clustering&gt;&lt;k-means&gt;&lt;gaussian-mixture&gt;" Title="gaussian mixture model - approximate a matrix" ViewCount="86" />
  <row Body="&lt;p&gt;Note that some algorithms will try to optimize the gap, others won't.&lt;/p&gt;&#10;&#10;&lt;p&gt;By comparing different algorithms with a measure that correlates with some of the objective functions, you will be more likely measuring how similar the algorithm is to the gap statistic, but not how good it actually works.&lt;/p&gt;&#10;&#10;&lt;p&gt;A similar problem occurs with pretty much every measure.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, the &quot;sum of squares&quot; (SSQ) measure is internally used by k-means, and it improves with the number of clusters (up to 0, when k=number of objects). K-means is (approximately, as the common algorithms only find local minimas) &lt;em&gt;optimal&lt;/em&gt; with respect to this measure. And the optimum k is the number of objects.&lt;/p&gt;&#10;&#10;&lt;p&gt;So obviously, any other algorithm will look bad compared to k-means, and yet the optimum result will be entirely useless.&lt;/p&gt;&#10;&#10;&lt;p&gt;Be careful when relying on such metrics. You measure a mathematical quantity that may not be capturing your &lt;em&gt;needs&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Things such as using the gap statistic or silhouette with k-means sometimes work well, because they are actually &quot;orthogonal&quot; to the objective used by k-means. Instead of blindly searching for the best k-means result (which would yield a much too high k), you use this secondary measure to compare k-means results. It works, because even with different k, k-means still optimizes SSQ, and not the gap statistic.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that it already fails when you try different normalization before running k-means. It's trivial to reduce gaps just by scaling the data set down; the preprocessing has a strong effect on these statistics.&lt;/p&gt;&#10;&#10;&lt;p&gt;When you compare different algorithms, usually each optimizes different quantities; so the comparison will usually not be fair. Actually, in most cases the result will not be fair; the only situation where it works reasonably well is varying the cluster number of k-means, and keeping everything else as is.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-03T12:33:04.317" Id="88563" LastActivityDate="2014-03-03T12:33:04.317" OwnerUserId="7828" ParentId="88550" PostTypeId="2" Score="4" />
  
  
  <row AnswerCount="3" Body="&lt;p&gt;I need help thinking about and identifying the kind of regression analysis that would be appropriate for this problem. Nothing I've discovered so far seems quite right. Referrals to articles or examples would be helpful. Thank you.&lt;/p&gt;&#10;&#10;&lt;p&gt;The data look like this:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;The data are observational.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The sampling unit is a geographic location (EDIT: let's assume units are independent); I'm just trying to understand the basic analytical problem here).&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;At each sampling unit, there are events of two types: the event type of interest (A) and all other types (B). EDIT: In other words, each event is a binary outcome (success, failure). The outcomes are aggregated to the to the location level (Location 1: Success 3, Failure 2. Location 2: Success 0, Failure 1. Location 3: Success 0, Failure 0. Location 4: Success 4, Failure 9 .... etc. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Often, A=0 and, somewhat less often but still frequently, A+B=0. &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I am interested in testing a hypothesis about A, somehow controlling for the total count of events (A+B), so either a proportion A/(A+B) or a count model that controls for the total count.&lt;/p&gt;&#10;&#10;&lt;p&gt;If I understand correctly, if the counts were large, proportions could be calculated for all units, and I could do beta regression. But that definitely can't happen when the total number of events for a unit is zero.&lt;/p&gt;&#10;&#10;&lt;p&gt;If all I cared about was the count, I could use a ZIP or other count model (and maybe still can). But the research question regards the frequency of A relative to the total number of events.&lt;/p&gt;&#10;&#10;&lt;p&gt;But how to control for the total number of events? Does it just go in the predictors of a ZIP or similar model? I suspect it's more complicated than that.&lt;/p&gt;&#10;&#10;&lt;p&gt;It seems obvious to me that the individual events could be modeled directly using multi-level logistic regression (EDIT: or another model for clustered data), but I'm wondering if there is a simpler way to examine what I'm interested in, and I just somehow haven't seen an example of this. &lt;/p&gt;&#10;" CommentCount="11" CreationDate="2014-03-03T16:39:14.357" FavoriteCount="1" Id="88588" LastActivityDate="2014-03-04T19:38:20.267" LastEditDate="2014-03-03T18:37:51.337" LastEditorUserId="25759" OwnerUserId="25759" PostTypeId="1" Score="5" Tags="&lt;regression&gt;&lt;count-data&gt;" Title="Regression model for proportion or count when counts of outcome and total events are often zero" ViewCount="296" />
  <row AnswerCount="1" Body="&lt;p&gt;Typically, in Bayesian bootstrap, you have samples {$x_1,...,x_n$} of a random variable $X$.&#10;Choose $\{p_1,...,p_n\}$ from a Dirichlet distribution, by sorting $\{0,1,u_1,...,u_{n-1}\}$ where $u_i$ are independent samples from $U \sim Uniform(0,1)$, and taking differences of sorted neighbors.&#10;Use $\{p_1,...,p_n\}$ as weights for $\{x_1,...,x_n\}$ to calculate a sample from the statistic of interest $\theta$; i.e.&#10;$$\hat\theta_j = \sum_{i=1}^n f(x_i) \cdot p_i$$&#10;for bootstrap iteration $j$.&lt;/p&gt;&#10;&#10;&lt;p&gt;I already have weights $\{w_1,...,w_n\}$ from importance sampling, from which I compute&#10;$$\hat\theta = \frac{\sum_{i=1}^n f(x_i) \cdot w_i}{\sum_{i=1}^n w_i} $$&#10;With a new set of weights computed as $w_i' = w_i \cdot p_i$, computing&#10;$$\hat\theta_j = \frac{\sum_{i=1}^n f(x_i) \cdot w_i'}{\sum_{i=1}^n w_i'} $$&#10;doesn't work: the distribution of bootstrapped statistics is too narrow.&#10;Resampling from the weighted samples to get unweighted $\{x_1',...,x_n'\}$ and applying Bayesian bootstrap to these also doesn't work: the distribution of bootstrapped statistics is too wide. (This is also true if I resample anew for each Bayesian bootstrap iteration.)&lt;/p&gt;&#10;&#10;&lt;p&gt;I think I need to reweight differently. How is this done?&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT: Actually, using weights $w_i' = w_i \cdot p_i$ was the right solution. I was comparing my bootstrap results against the results of using a bad, too-high Monte Carlo variance estimate. I'm not ready to call this foobar my fault, though, because it's &lt;strong&gt;stupidly hard&lt;/strong&gt; to find the &lt;strong&gt;correct&lt;/strong&gt; formula for Monte Carlo variance of expected values computed using &quot;ratio&quot; or &quot;self-normalized&quot; importance samples. For the record, I finally found it here, on page 63:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.rni.helsinki.fi/~pek/test/mcmbc10.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.rni.helsinki.fi/~pek/test/mcmbc10.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;So thanks very much to Petri Koistinen. This is the formula for an unbiased Monte Carlo variance estimate, in case the link goes stale:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$Var[\widehat\theta] \approx \frac{n}{n-1} \frac{\sum_{i=1}^n \left(f(x_i)-\widehat\theta\right)^2 w_i^2}{\left(\sum_{i=1}^n w_i\right)^2}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I think this is right because 1) Monte Carlo variance of expected values computed using unweighted samples is a special case; 2) changing the importance distribution induces the expected changes to it; 3) all the bootstrap methods I've implemented agree with the values I compute from it (finally); 4) it's invariant to weight scaling; and 5) it seems to have a decent proof behind it, in Koistinen's notes and another place I found it. (Though in the other place it was followed up by a bogus formula for computing a confidence interval, with a spurious multiplication by $\sqrt{n}$.) As a bonus, the leading term is the standard bias correction term for variance estimates.&lt;/p&gt;&#10;&#10;&lt;p&gt;You would think that, given that importance sampling is so often presented and used as a variance reduction technique, &lt;em&gt;estimating the variance would be common&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Sorry for the noise, but at least it might help someone else in the future.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-03T19:05:47.333" Id="88615" LastActivityDate="2014-03-10T16:47:19.653" LastEditDate="2014-03-10T16:47:19.653" LastEditorUserId="3831" OwnerUserId="3831" PostTypeId="1" Score="1" Tags="&lt;bootstrap&gt;&lt;monte-carlo&gt;&lt;importance-sampling&gt;" Title="Reweighting importance-weighted samples in Bayesian bootstrap" ViewCount="139" />
  <row AcceptedAnswerId="88645" AnswerCount="1" Body="&lt;p&gt;I have a linear regression model with 3 independent variables (let's say &lt;code&gt;A1&lt;/code&gt;, &lt;code&gt;A2&lt;/code&gt;, &lt;code&gt;A3&lt;/code&gt;) and 2 different dummy variables, one for the gender (&lt;code&gt;d1&lt;/code&gt;) and the other one for the location (&lt;code&gt;d2&lt;/code&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;When I estimate the model with all the variables included, some of independent variables are not significant, but when I add just one of the dummy variables, all of the independent variables are significant. Could you please let me know why?&lt;/p&gt;&#10;&#10;&lt;p&gt;And when I estimate the model in the form of f(&lt;code&gt;A1&lt;/code&gt;, &lt;code&gt;A2&lt;/code&gt;, &lt;code&gt;A3&lt;/code&gt;, &lt;code&gt;d1&lt;/code&gt;), I get different coefficients for the independent variables in comparison with the ones for f(&lt;code&gt;A1&lt;/code&gt;, &lt;code&gt;A2&lt;/code&gt;, &lt;code&gt;A3&lt;/code&gt;, &lt;code&gt;d2&lt;/code&gt;). Why?&lt;/p&gt;&#10;&#10;&lt;p&gt;In general, should I add all the possible dummy variables to the model at once, or it is possible to have different similar models with just one of the dummy variables included? Thanks.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-03-03T22:36:45.803" Id="88635" LastActivityDate="2014-03-04T00:03:59.313" LastEditDate="2014-03-04T00:03:59.313" LastEditorUserId="32036" OwnerUserId="24805" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;multiple-regression&gt;&lt;categorical-data&gt;&lt;multiple-comparisons&gt;" Title="Dummy variables in a multiple regression" ViewCount="139" />
  <row AnswerCount="0" Body="&lt;p&gt;Please help me proving this:&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose that $Y_1,\ldots,Y_n$ are i.i.d. nonnegative RV's with CDF $F$ and $E(Y_i)=\mu&amp;lt;\infty$. Let $y_1,\ldots,y_n$ be a realization from which an EDF $\hat{F}$ is defined:&#10;$$&#10;\hat{F}(y)=\frac{1}{n}\sum_{i=1}^nI(y_i\leq y).&#10;$$&#10;&lt;strong&gt;Want to show&lt;/strong&gt;:&#10;$$&#10;\text{plim}_{n\to\infty}\left(n^{1/2}\int_0^\infty y(\hat{F}(y)-F(y))d(\hat{F}-F)(y)\right)=0.&#10;$$&#10;Please state any further necessary assumptions.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;EDIT responding to GUNG'S comment:&lt;/p&gt;&#10;&#10;&lt;p&gt;This result is a small step in a bigger argument in Russell Davidson 2009's paper on the Gini index. The author must have considered it obvious because he mentions it without proof. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have some understanding of asymptotic theory such as some laws of large numbers, some variants of the CLT, various modes of convergence, Slutsky Theorem, measure theory in general and probability theory in particular, and some stochastic calculus.&lt;/p&gt;&#10;&#10;&lt;p&gt;My problem is I've been learning asymptotic theory on my own and I haven't done a lot of exercises in this. So while I want to understand the author's reasoning, I don't know where to start with this one.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-03-04T00:23:59.543" Id="88648" LastActivityDate="2014-03-04T03:11:26.837" LastEditDate="2014-03-04T03:11:26.837" LastEditorUserId="41277" OwnerUserId="41277" PostTypeId="1" Score="2" Tags="&lt;probability&gt;&lt;convergence&gt;&lt;cdf&gt;&lt;empirical&gt;" Title="Asymptotic convergence involving EDF" ViewCount="52" />
  
  
  
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I posted this question on the JAGS help discussion, I was seeing if I could get any help here:&lt;/p&gt;&#10;&#10;&lt;p&gt;I wish to fit a survival model using the data below (first two columns are time points, NA represents censoring, third column is number of samples). Is there a way to weight the likelihood by the sample size?&lt;/p&gt;&#10;&#10;&lt;p&gt;If not, I could repeat each time point for given sample size but this would result in probably too larger of an overall sample.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;a1=matrix(c(&#10;0.01, 1, 463,&#10;1, 2, 369,&#10;9, 10, 116,&#10;10, 11, 163,&#10;11, 12, 149,&#10;12, 13, 230,&#10;12.5, NA, 150054,&#10;11.5, NA, 146349,&#10;10.5, NA, 118098,&#10;9.5, NA, 41633,&#10;8.5, NA, 30308,&#10;7.5, NA, 25934,&#10;6.5, NA, 29427,&#10;2.5, NA, 37997,&#10;1.5, NA, 40599,&#10;0.5, NA, 43300),ncol=3,byrow=T)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-03-04T05:24:52.600" Id="88670" LastActivityDate="2014-03-04T05:24:52.600" OwnerUserId="2310" PostTypeId="1" Score="2" Tags="&lt;jags&gt;&lt;bugs&gt;" Title="BUGS with very large sample size" ViewCount="35" />
  <row Body="&lt;p&gt;There are goodness of fit tests (many, many), including the Kolmogorov-Smirnov test you mentioned; many goodness of fit test statistics can be seen as a measure of discrepancy between data and some distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;The most obvious ones to consider first are the &lt;a href=&quot;http://en.wikipedia.org/wiki/Empirical_distribution_function&quot; rel=&quot;nofollow&quot;&gt;empirical-CDF&lt;/a&gt;-based tests, of which the Kolmogorov-Smirnov test is the simplest.&lt;/p&gt;&#10;&#10;&lt;p&gt;The Kolmogorov-Smirnov test statistic is the largest distance between the hypothesized cdf and the ECDF of the data (or sometimes a standardized version of it)&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/vgb6a.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;There are many others based on the ECDF. One example is the Cramer-von Mises test which (at least for the uniform) corresponds to a sum of squares of vertical distances between the two cdfs.&lt;/p&gt;&#10;&#10;&lt;p&gt;Another related measure is the Anderson-Darling statistic, which weights by the inverse of the variance of the ECDF.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are many other kinds of tests. One common one is the the Shapiro-Wilk test, for example. The Shapiro-Wilk (and related tests) can also be treated as a measure of discrepancy between the distribution of the data and some hypothesized distribution (in the case of the Shapiro-Wilk test, that's the normal distribution). The Shapiro-Wilk, however, is invariant to changes in mean and variance.&lt;/p&gt;&#10;&#10;&lt;p&gt;A full coverage of the goodness of fit territory could take a book -- indeed has taken several.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-03-04T09:46:17.620" Id="88679" LastActivityDate="2014-03-04T11:04:59.573" LastEditDate="2014-03-04T11:04:59.573" LastEditorUserId="805" OwnerUserId="805" ParentId="88675" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="88694" AnswerCount="3" Body="&lt;p&gt;I am working on a graph clustering algorithm (mcl). It gives the opportunity to give weights to the edges. The weights must be similarities, but I have a distance. The values of this distance range from 0 to infinity. I am looking for ways to convert this distance to a similarity. So far, my main idea is to use s = 1/(1+d). &lt;/p&gt;&#10;&#10;&lt;p&gt;Are there &quot;better&quot; alternatives? (and if so, how can I tell that a conversion is better than another?)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-04T10:28:49.727" Id="88684" LastActivityDate="2014-03-05T23:45:24.417" OwnerUserId="28183" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;clustering&gt;&lt;data-mining&gt;" Title="Converting a distance to a similarity" ViewCount="76" />
  
  <row Body="&lt;p&gt;One way to increase or decrease contrast is to raise your s to a given power, e.g. &lt;code&gt;s**0.5&lt;/code&gt; to decrease contrast or &lt;code&gt;s**2&lt;/code&gt; to increase contrast. It is worthwhile noting that this can be done by mcl itself, using the &lt;code&gt;-pi&lt;/code&gt; (pre-inflation) option, e.g. &lt;code&gt;-pi 0.5&lt;/code&gt; or &lt;code&gt;-pi 2&lt;/code&gt;, so there is no need to create many different input files. In fact, mcl has a more general transformation option called &lt;code&gt;-tf&lt;/code&gt;. The same effect can be achieved by supplying e.g. &lt;code&gt;-tf 'pow(0.5)'&lt;/code&gt; or &lt;code&gt;-tf 'pow(2)'&lt;/code&gt; (quotes are required to prevent the shell from interpreting these parentheses).&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, to judge the merits of different conversions, you will need to wade in and have a look at the data and the clusterings you get and preferably use your knowledge of the domain to make informed choices. If you have gold-standard data (manual curation of some sort) it is possible to be a bit more systematic, for example in the case where the gold standard annotation can be viewed as a clustering.&lt;/p&gt;&#10;&#10;&lt;p&gt;Choose an edge-weight (similarity) cutoff such that the topology of the network becomes informative; i.e. too many edges or too few edges yield little discriminative information in the absence/presence structure of edges. Choose it such that no edges connect things you consider very dissimilar, and that edges connect things you consider somewhat similar to quite similar. In the case of mcl, the dynamic range in edge weight between 'a bit similar' and 'very similar' should be, as a rule of a thumb, one order of magnitude, i.e. two-fold or five-fold or ten-fold, as opposed to varying from 0.9 to 1.0.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-04T11:30:44.200" Id="88694" LastActivityDate="2014-03-04T11:30:44.200" OwnerUserId="4495" ParentId="88684" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;I have encountered one last problem with regarding to the Metropolis-Hastings algorithm. I know that ergodicity is needed in the algorithm to imply convergence to a unique stationary distribution. But how is ergodicity proved in the M-H algorithm? Thank you for your help&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-04T12:52:46.180" Id="88698" LastActivityDate="2014-03-04T13:43:22.310" OwnerUserId="41181" PostTypeId="1" Score="1" Tags="&lt;bayesian&gt;&lt;metropolis-hastings&gt;&lt;ergodic&gt;" Title="Metropolis Ergodicity" ViewCount="56" />
  <row Body="&lt;p&gt;If you want to sample from a certain pdf, you can use&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;rejection sampling&lt;/strong&gt; which requires nothing more than the &lt;em&gt;density function&lt;/em&gt; and the specification of a value as upper bound which is at least as large as the largest value of the density function. The disadvantage is that it can eventually be a very inefficient way to sample depending on the shape of the density function.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;inverse transform sampling&lt;/strong&gt; is the preferred way if the inverse of the &lt;em&gt;distribution function&lt;/em&gt; is known. This is the case for the likelihood in your example since it is a Gaussian distribution and the associated quantile function (=inverse distribution function) is available in R. In general, inverse transform sampling works by sampling from a uniform distribution in the interval [0,1] and use the obtained values as the argument of the quantile function. The resulting values from the quantile function then follow the specified probability distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;To elaborate on the example: Since the likelihood is for a Gaussian distribution, it is maximized by setting $\mu$ to the arithmetic mean of the $D$ values&#10;$$\mu = \frac{1}{n}\sum_i^n D_i$$&#10;from which the variance can be calculated by&#10;$$\text{Var}(\mu)=\frac{1}{n^2}\sum_i^n \text{Var}(D_i)=\frac{\sigma^2}{n}$$&#10;Thus the $\mu$ value follows a Gaussian distribution $N(\mu,\sigma/\sqrt{n})$. Instead of implementing inverse transform sampling yourself, you could also use the &lt;code&gt;rnorm&lt;/code&gt; function. &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-03-04T13:20:43.933" Id="88701" LastActivityDate="2014-03-04T17:37:45.640" LastEditDate="2014-03-04T17:37:45.640" LastEditorUserId="40608" OwnerUserId="40608" ParentId="88697" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="88710" AnswerCount="1" Body="&lt;p&gt;Consider the one parameter exponential family on a &lt;strong&gt;finite&lt;/strong&gt; sample space $S$: $$p(x;\theta)=\frac{e^{\theta x}}{\sum_{x\in S} e^{\theta x}}, \theta\in \mathbb{R}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;My objective is to find an MLE for $\theta$ having observed a sample $x_1,\dots,x_n$ from $p(\cdot,\theta)$. In the continuous case (when $\theta$ is restricted to strictly negative and $x&amp;gt;0$) one can easily solve the likelihood equation and see that the MLE is negative of reciprocal of the sample mean.&lt;/p&gt;&#10;&#10;&lt;p&gt;When I do the same here, I obtain that the $\theta$ for which $p(x_1,\dots,x_n;\theta)$ is maximum satisfies&#10;$$\frac{\sum_x xe^{\theta x}}{\sum_x e^{\theta x}}=\frac{1}{n}\sum_i x_i.$$&#10;Is it possible to find MLE from here in a closed form expression? LHS is mean with respect to $p(\cdot,\theta)$. Is the mean known in terms of $\theta$? Any help is appreciated.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-03-04T13:21:37.923" Id="88702" LastActivityDate="2014-03-04T13:58:46.157" OwnerUserId="21423" PostTypeId="1" Score="0" Tags="&lt;estimation&gt;&lt;maximum-likelihood&gt;" Title="MLE of an exponential distribution in discrete case" ViewCount="89" />
  
  
  <row Body="&lt;p&gt;EM is not guaranteed to converge to a local minimum.  It is only guaranteed to converge to a point with zero gradient with respect to the parameters.  So it can indeed get stuck at saddle points.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-03-04T16:17:04.960" Id="88726" LastActivityDate="2014-03-04T16:17:04.960" OwnerUserId="2074" ParentId="83387" PostTypeId="2" Score="10" />
  
  <row Body="&lt;p&gt;If I've understood your research goals correctly, you have two questions you'd like to answer:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Is there a difference between clinician A's ability to correctly diagnose a condition using a clinicial exam vs. a diagnostic exam?&lt;/li&gt;&#10;&lt;li&gt;Do experts correctly diagnose the condition more often than trainees when both use a diagnostic exam?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Both of these can be answered using separate chisquare contingency tables (assuming your sample sizes are likely to be on the small side). A google search will turn up many examples of how to do this (the calculations can often be performed by hand), and there are a number of discussions on CrossValidated such as this one:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/29367/contingency-tables-what-tests-to-do-and-when&quot;&gt;Contingency tables: what tests to do and when?&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-03-04T16:32:21.147" Id="88731" LastActivityDate="2014-03-04T16:32:21.147" OwnerUserId="41304" ParentId="88724" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Here are two examples.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;$x$ is normally distributed around mean $\theta$.  The likelihood for $\theta$ is a normal distribution with mean $x$.&lt;/li&gt;&#10;&lt;li&gt;$x$ is Gamma distributed with rate $\theta$.  The likelihood for $\theta$ is a Gamma distribution with rate $x$.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="2" CreationDate="2014-03-04T16:53:10.780" Id="88738" LastActivityDate="2014-03-04T16:53:10.780" OwnerUserId="2074" ParentId="88399" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Just complementing whuber comment, here's an example in R showing that changing the mean of y and z will change cov(y*y, z) even if var(y), var(z), and cov(y,z) remains the constant.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;require(MASS)&#10;covarianceMatrix&amp;lt;-matrix(c(3,2,2,4),2,2)&#10;yz &amp;lt;- mvrnorm(100000, c(1,2), covarianceMatrix)&#10;var(yz[, 1]) #variance y&#10;var(yz[, 2]) #variance z&#10;cov(yz[, 1], yz[, 2]) #covariance yz&#10;cov(yz[, 1]*yz[, 1], yz[, 2]) #covariance yz&#10;&#10;yz &amp;lt;- mvrnorm(100000, c(2,3), covarianceMatrix)&#10;var(yz[, 1]) #variance y is approximatelly equal&#10;var(yz[, 2]) #variance z is approximatelly equal&#10;cov(yz[, 1], yz[, 2]) #covariance yz is approximatelly equal&#10;cov(yz[, 1]*yz[, 1], yz[, 2]) #covariance yz is different&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Output:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt;     require(MASS)&#10;&amp;gt;     covarianceMatrix&amp;lt;-matrix(c(3,2,2,4),2,2)&#10;&amp;gt;     yz &amp;lt;- mvrnorm(100000, c(1,2), covarianceMatrix)&#10;&amp;gt;     var(yz[, 1]) #variance y&#10;[1] 2.999731&#10;&amp;gt;     var(yz[, 2]) #variance z&#10;[1] 3.987699&#10;&amp;gt;     cov(yz[, 1], yz[, 2]) #covariance yz&#10;[1] 1.994687&#10;&amp;gt;     cov(yz[, 1]*yz[, 1], yz[, 2]) #covariance yz&#10;[1] 4.018674&#10;&amp;gt;     &#10;&amp;gt;     yz &amp;lt;- mvrnorm(100000, c(2,3), covarianceMatrix)&#10;&amp;gt;     var(yz[, 1]) #variance y is approximatelly equal&#10;[1] 3.005912&#10;&amp;gt;     var(yz[, 2]) #variance z is approximatelly equal&#10;[1] 3.994272&#10;&amp;gt;     cov(yz[, 1], yz[, 2]) #covariance yz is approximatelly equal&#10;[1] 2.004748&#10;&amp;gt;     cov(yz[, 1]*yz[, 1], yz[, 2]) #covariance yz is different&#10;[1] 7.963221&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-03-04T16:53:54.423" Id="88739" LastActivityDate="2014-03-04T16:53:54.423" OwnerUserId="16612" ParentId="88718" PostTypeId="2" Score="1" />
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I was told the following will generate 52 observations in range (1,52), with a shape parameter of .5 and a scale parameter of 2.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Yet, there is a 67 so that is perhaps wrong?&lt;br&gt;&#10;What I need is N observations in range (1,52) with the observations conforming to a Weibull distribution with specific parameters.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; print(rweibull(52,.5,2))&#10;&#10; [1]  0.406647225  0.052710518  2.158733355  0.500174364  2.074537224    &#10; [6]  0.378695917  0.396309715  0.131268753  4.182770394  0.293154663    &#10;[11]  2.045214882  0.016280681  0.268003469 17.612960070  2.375416850    &#10;[16]  6.482297632  0.151320479  0.214247990  2.613026032  3.397453127    &#10;[21]  7.357103077  2.054468323  3.090147233  2.594981565 21.316501285    &#10;[26] 35.364512001  2.272848255  1.186967955  4.178822565  0.006659194    &#10;[31]  6.006123713 ***67.918717193***  0.051010700  0.706372359  2.419090546    &#10;[36]  1.858408184  9.545857635  2.182673324  0.892914986  0.244567333    &#10;[41]  0.065993717  0.225678795 27.655436513  2.965634299  1.294741473    &#10;[46]  0.069503896  1.266168448  3.949787345  3.391448817  1.663533337    &#10;[51]  0.604392557  1.232525921&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Any help is greatly appreciated!&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;: I apparently have solved my own question. The following will give me the expected # of failures in the T&lt;sup&gt;th&lt;/sup&gt; month, given k as the shape, and 100 initial # of parts:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ceiling((pweibull(t,shape=k,scale=12)-pweibull(t-1,shape=k,scale=12))*100)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" ClosedDate="2014-03-04T21:10:58.187" CommentCount="2" CreationDate="2014-03-04T19:54:18.600" Id="88773" LastActivityDate="2014-03-04T21:07:15.173" LastEditDate="2014-03-04T21:07:15.173" LastEditorUserId="22468" OwnerUserId="41324" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;weibull&gt;" Title="Getting rweibull to output n observations in (1,52) given specific shape, scale parameters" ViewCount="75" />
  
  
  
  <row AcceptedAnswerId="88808" AnswerCount="1" Body="&lt;p&gt;I'm working on a time series model which includes multiple seasonal components (daily and weekly). I believe the best way to approach this would be BATS/TBATS model, however I have a concern if I can incorporate external factors in a proper way. I'm thinking about a regression with BATS/TBATS error, in analogy to a regression with ARIMA error:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Fit a linear regression to forecast the outcome based on available&#10;external predictors &lt;/li&gt;&#10;&lt;li&gt;Fit BATS/TBATS model to forecast residuals;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Do you see any fundamental problems with this approach and is there a better way to build a model with multiple seasonal components and external predictors?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-04T22:39:58.673" Id="88795" LastActivityDate="2014-03-05T00:02:27.477" LastEditDate="2014-03-04T22:42:13.820" LastEditorUserId="7071" OwnerUserId="31158" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;time-series&gt;&lt;arima&gt;" Title="Regression with TBATS error?" ViewCount="47" />
  <row AnswerCount="1" Body="&lt;p&gt;I want to find the relationship between two variables and a third. For example, in the graph below I have the total number of questions a student has completed on the x axis, the number of questions they have completed in a particular subtopic on the y-axis and the average score displayed using color. I can't post company data without permission, so I have posted random data instead. But I have found that these kind of graphs don't really help that much. Are there any other kinds of graphs that will visually represent the dependence between two factors in terms of their effective on the third&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/SkyVL.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-03-04T23:42:06.123" Id="88806" LastActivityDate="2014-03-12T11:01:50.830" LastEditDate="2014-03-12T11:00:58.570" LastEditorUserId="31208" OwnerUserId="31208" PostTypeId="1" Score="2" Tags="&lt;correlation&gt;&lt;data-visualization&gt;" Title="Graphing correlations - dependent variable against two factors" ViewCount="192" />
  
  <row Body="&lt;p&gt;The linear regression will be inefficient at best, and possibly inconsistent if the errors are non-stationary. A better approach is to add in multiple seasonal components as Fourier terms in the regression with ARIMA errors. An example with a single seasonal period is given at &lt;a href=&quot;http://robjhyndman.com/hyndsight/forecasting-weekly-data/&quot; rel=&quot;nofollow&quot;&gt;http://robjhyndman.com/hyndsight/forecasting-weekly-data/&lt;/a&gt;. That can be extended to multiple seasonal terms by adding Fourier terms with different periods.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-05T00:02:27.477" Id="88808" LastActivityDate="2014-03-05T00:02:27.477" OwnerUserId="159" ParentId="88795" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;I think if you are &lt;em&gt;sure&lt;/em&gt; that the process is ARMA, then the MA part doesn't affect the stationarity. But if you are &lt;em&gt;not sure&lt;/em&gt; of that, unit root tests of the MA part may suggest that it's &quot;likely&quot; that the process as specified is not actually ARMA (and so you would want to integrate it).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-05T00:06:25.043" Id="88810" LastActivityDate="2014-03-05T09:39:34.740" LastEditDate="2014-03-05T09:39:34.740" LastEditorUserId="10117" OwnerUserId="10117" ParentId="65716" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="88952" AnswerCount="2" Body="&lt;p&gt;I've been asked to review the literature for all studies that estimate the mean and median Blood Lead Level (BLL) in a certain country, and then perform a meta-analysis to come up with one overall value for the mean and median level. &lt;/p&gt;&#10;&#10;&lt;p&gt;Is such a thing typically done? I'm not interested in any &quot;effect size,&quot; just a univariate measure. &lt;/p&gt;&#10;&#10;&lt;p&gt;I'd like to create a forest plot and do some of the standard tests of heterogeneity across studies. Is there a package and function in R that will allow me to do a meta-analysis of mean and median BLLs?&lt;/p&gt;&#10;&#10;&lt;p&gt;Just to clarify, this would be a separate meta-analysis for means, and a separate meta-analysis for medians.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-03-05T00:54:57.617" FavoriteCount="1" Id="88813" LastActivityDate="2014-03-05T20:45:17.993" LastEditDate="2014-03-05T01:58:21.207" LastEditorUserId="32096" OwnerUserId="32096" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;meta-analysis&gt;" Title="Meta-analysis of means and medians in R?" ViewCount="181" />
  <row Body="&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/DBSCAN&quot; rel=&quot;nofollow&quot;&gt;DBSCAN&lt;/a&gt; works without knowing the number of clusters ahead of time, and it can apply a wide range of distance metrics.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-03-05T05:30:01.730" Id="88845" LastActivityDate="2014-03-05T05:30:01.730" OwnerUserId="41348" ParentId="14955" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;If you have medians and range, then you can get the formula for converting to mean and SD from this sentinel paper by Hozo et al., 2005 (&lt;a href=&quot;http://www.biomedcentral.com/1471-2288/5/13&quot; rel=&quot;nofollow&quot;&gt;http://www.biomedcentral.com/1471-2288/5/13&lt;/a&gt;).&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-03-05T06:36:24.397" Id="88848" LastActivityDate="2014-03-05T06:36:24.397" OwnerUserId="24137" ParentId="88813" PostTypeId="2" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;I got part a and part b just fine, but I am confused on how to do part (c), (d), and (e). I don't really understand what the question is asking, nor do I understand how to do that sort of interval estimation. Any help is greatly needed and appreciated! Thanks! &lt;/p&gt;&#10;&#10;&lt;p&gt;Consider the following sample of observations on coating thickness for low-viscosity paint (Achieving a Target Value for a Manufacturing Process: A Case Study, J. of Quality Technology, 1992: 2226): &lt;/p&gt;&#10;&#10;&lt;p&gt;0.83 0.88 0.88 1.04 1.09 1.12 1.29 1.31 &#10;1.48 1.49 1.59 1.62 1.65 1.71 1.76 1.83 &lt;/p&gt;&#10;&#10;&lt;p&gt;Assume that the distribution of coating thickness is normal (a normal probability plot strongly supports this assumption). &lt;/p&gt;&#10;&#10;&lt;p&gt;a.) Calculate a point estimate of the mean value of coating thickness, and state which estimator you used. Answer = 1.3481234 using x-bar &lt;/p&gt;&#10;&#10;&lt;p&gt;b.) Calculate a point estimate of the median of the coating thickness distribution, and state which estimator you used. Answer = 1.395 using ~x &lt;/p&gt;&#10;&#10;&lt;p&gt;c.) Calculate a point estimate of the value that separates the largest 10% of all values in the thickness distribution from the remaining 90%, and state which estimator you used. [Hint: Express what you are trying to estimate in terms of  and .] &lt;/p&gt;&#10;&#10;&lt;p&gt;d.) Estimate P(X &amp;lt; 1.5), i.e., the proportion of all thickness values less than 1.5. [Hint: If you knew the values of  and , you could calculate this probability. These values &#10;are not available, but they can be estimated.] &lt;/p&gt;&#10;&#10;&lt;p&gt;e.) What is the estimated standard error of the estimator that &#10;you used in part (b)?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-03-05T06:51:02.527" Id="88850" LastActivityDate="2014-03-05T08:27:42.370" LastEditDate="2014-03-05T08:27:42.370" LastEditorUserId="41352" OwnerUserId="41352" PostTypeId="1" Score="0" Tags="&lt;self-study&gt;&lt;point-estimation&gt;" Title="How to use a point estimator on an interval?" ViewCount="214" />
  <row Body="&lt;p&gt;In practice simulation is almost always done without full certainty about the distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, one might consider a lognormal distribution for some random variable... but what if it was a little lighter tailed (gamma, say)? Or heavier tailed (inverse gamma, say)? Or much heavier tailed (inverse Gaussian, say)? Or log-t?&lt;/p&gt;&#10;&#10;&lt;p&gt;What if it was some mixture of lognormals? ... or perhaps some form of regime switching?&lt;/p&gt;&#10;&#10;&lt;p&gt;What if there was a slight dependence in the data? &lt;/p&gt;&#10;&#10;&lt;p&gt;One can consider a host of ways in which the model may  be inadequate (preferably springing from an understanding of the process being dealt with), and see via simulation how that affects the conclusions.&lt;/p&gt;&#10;&#10;&lt;p&gt;Such things are commonplace. It's one reason why simulation is so useful&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Think of the stock price return distribution-- it doesn't fit into a Gaussian Distribution, but what distribution we can use?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;If you read around a little you can find a variety of reasonably good models; none will be perfect.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;A relevant, but probably narrower question is, is there any research done along the line of sensitivity analysis? By &quot;sensitivity analysis&quot; I mean we assume known distributions for the variables, but we perturb them a little by slightly changing the distribution shape or slightly changing some of the variables, will the outcome of the simulation results remain roughly the same, or completely different?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Yes, in a variety of guises (and not just for simulation; it's a standard tool in robustness for example) -- but the perturbations needn't be small; you can investigate the effect of just about any form of variation from the base assumptions.&lt;/p&gt;&#10;&#10;&lt;p&gt;In robustness the effect of small perturbations in distribution is considered through tools like &lt;a href=&quot;http://en.wikipedia.org/wiki/Robust_statistics#Influence_function_and_sensitivity_curve&quot; rel=&quot;nofollow&quot;&gt;influence functions&lt;/a&gt;, &lt;a href=&quot;http://en.wikipedia.org/wiki/Influence_function_%28statistics%29#Empirical_influence_function&quot; rel=&quot;nofollow&quot;&gt;empirical influence functions&lt;/a&gt; and sensitivity curves (and also via simulation)&lt;/p&gt;&#10;&#10;&lt;p&gt;The first several books at the above wikipedia page are a good starting place for references on robustness; I don't recall exactly what's in which book (I read Hampel et al in the mid 80's and Huber a year or two later, and some of the others since - it's been a while), but the Hampel et al book does discuss influence functions, so I'd start there I guess.&lt;/p&gt;&#10;&#10;&lt;p&gt;Edit: TooTone reminded me that I wanted to mention resampling methods, which can be thought of as a kind of simulation. For example, with simple bootstrapping, you use the ECDF (the cdf of the sample) rather than the distribution of the data to sample from. In more complex resampling schemes some function of the data (such as model residuals) may be resampled. So these approaches needn't rely on some parametric assumption about the data. &lt;/p&gt;&#10;&#10;&lt;p&gt;(Then again, there's also the parametric bootstrap which might be as readily thought of as a particular simulation technique as bootstrapping.)&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-03-05T06:55:58.603" Id="88851" LastActivityDate="2014-03-15T07:34:19.423" LastEditDate="2014-03-15T07:34:19.423" LastEditorUserId="805" OwnerUserId="805" ParentId="88836" PostTypeId="2" Score="5" />
  <row AnswerCount="0" Body="&lt;p&gt;I am using repeated 10-fold CV to calculate the accuracy of my ordinal regression model. I have 6 predictors, 10 ordered response categories, and a total of 1166 data points. &lt;/p&gt;&#10;&#10;&lt;p&gt;For the ordinal model, I have defined accuracy as &lt;em&gt;1 - loss&lt;/em&gt;, with &lt;em&gt;loss&lt;/em&gt; being a simple linear function of the distance between observed and predicted classes, assuming classes are equidistant. &lt;/p&gt;&#10;&#10;&lt;p&gt;I chose the number of repetitions for the 10-fold CV by examining the stability of the results as advised &lt;a href=&quot;http://stats.stackexchange.com/questions/82546/how-many-times-should-we-repeat-a-k-fold-cv&quot;&gt;here&lt;/a&gt;, and decided to use 5 repetitions.   &lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, I am using this measure of accuracy to compare the quality of the predictions given different predictor values. More precisely, I have a range of possible storm wave conditions and associated parameters, 10 levels of increasing damage for ships caught in the storm, and I am  looking to find the wave conditions which have more likely triggered the damage. So I run the model for each one and compare the accuracy.  &lt;/p&gt;&#10;&#10;&lt;p&gt;My problem is, for all conditions tested my accuracy is suspiciously high i.e. &gt; 80%. I am not expecting this kind of accuracy given the quality of some of my predictors (for example, the wave velocity has been estimated numerically on a coarse grid with a number of simplifying assumptions). &lt;/p&gt;&#10;&#10;&lt;p&gt;Also, there is not much variation in accuracy between the model outputs for different wave conditions, maximum +/- 1%. Again I would have expected more than this. &lt;/p&gt;&#10;&#10;&lt;p&gt;I am wondering if there is something I am missing here, regarding my estimation of accuracy. What could be the cause of this issue?&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2014-03-05T09:43:58.840" Id="88862" LastActivityDate="2014-03-05T09:43:58.840" OwnerUserId="31160" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;cross-validation&gt;&lt;ordinal&gt;&lt;error&gt;" Title="Abnormally high accuracy with repeated 10-fold CV and ordinal regression" ViewCount="71" />
  <row AcceptedAnswerId="88881" AnswerCount="1" Body="&lt;p&gt;Assuming I observe, in a unit square, $n_1$ circles of area $A_1$ (non-overlapping amongst themselves) and $n_2$ circles of area $A_2$ (again, non-overlapping) and that each of the centres is uniformly distributed across the square (well, I guess that can't be precisely true given the non-overlapping constraint), is there an expression for the proportion of the first circles that overlap with at least one of the second type of circles.&lt;/p&gt;&#10;&#10;&lt;p&gt;The actual situation is observed cancer cells and pores on a microscope slide, to which the above seems a good approximation.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the 1D case, I'd like to think that for each line segment (1D equivalent of a circle) of the first group, there's an interval of size $A_1+ A_2$ before the end of each segment of the second type to place the start of the interval so that they would overlap, and so $n_2(A_1+A_2)$ of the unit interval would result in an overlap.  That seems to be true for the first segment of the first group, but it won't necessarily hold for further segments as the non-overlapping criterion breaks the independence, so I'm probably looking at this (and the 2D case) from the wrong angle.&lt;/p&gt;&#10;&#10;&lt;h3&gt;Updated thoughts&lt;/h3&gt;&#10;&#10;&lt;p&gt;Maybe this is the way to look at it?  There's a total area $n_1A_1$ covered by type 1, and $n_2A_2$ covered by type 2, out of a total area of 1.  So an expected area of $n_1A_1n_2A_2$ is covered by both, and divide this by $A_1$ to give the number of type 1's that would be needed to cover that area, and divide by $n_1$ to get the proportion of type 1's that are in the double-covered area.  So if 7% of a slide's area is covered by spores, then 7% of cells are hit by spores?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-03-05T10:37:14.480" Id="88868" LastActivityDate="2014-03-05T12:59:52.667" LastEditDate="2014-03-05T12:27:35.570" LastEditorUserId="20312" OwnerUserId="20312" PostTypeId="1" Score="0" Tags="&lt;probability&gt;&lt;spatial&gt;" Title="Probability of overlap of areas in 2D space" ViewCount="119" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am faced with the following problem:&lt;/p&gt;&#10;&#10;&lt;p&gt;In general terms: I have a number dependent measurements in two biological samples and want to test if the difference between the samples is significant. Classical tests like a Mann Whitney U test (or t) test require independent measurements. My question is whether permutation based equivalent conditional tests like the ones implemented in the coin package in R can be applied to dependent measurements.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://cran.r-project.org/web/packages/coin/index.html&quot; rel=&quot;nofollow&quot;&gt;http://cran.r-project.org/web/packages/coin/index.html&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;A little more detail: I have number of methylation measurements which are located closely together on the genome. Biologically, these are independent (different positions on the DNA), but it is known that positions that are close together are highly correlated with respect to their methylation values. I.e. the methylation measurements do not seem to be independent.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am grateful for any suggestions. Thanks.&lt;/p&gt;&#10;&#10;&lt;p&gt;Best,&#10;Martin&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-03-05T13:16:46.343" Id="88888" LastActivityDate="2014-03-05T14:43:15.170" LastEditDate="2014-03-05T14:43:15.170" LastEditorUserId="88" OwnerUserId="34855" PostTypeId="1" Score="0" Tags="&lt;conditional-probability&gt;&lt;independence&gt;&lt;permutation&gt;" Title="Dependent variables" ViewCount="30" />
  
  
  <row Body="&lt;p&gt;It is possible that you have made an error of using the CV error as a criterion to select the best gene set during the feature selection -- this way you basically overfitted the model, so no wonder that the test set error is way higher.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you want to get a proper error estimate of your procedure, you must use nested cross-validation, i.e. add another loop in which you do the feature selection, and only use error estimate from the outer one, completely ignoring the inner one.&lt;/p&gt;&#10;&#10;&lt;p&gt;Though, well, note that FS methods optimising classification error may not be a best solution.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-05T14:59:56.800" Id="88906" LastActivityDate="2014-03-05T14:59:56.800" OwnerUserId="88" ParentId="88886" PostTypeId="2" Score="1" />
  
  
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I knew that, ROC  curve are use to assess the performance of classifiers.&#10;But is it possible to generate ROC curve for the regression model? If yes, How?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-03-05T16:33:00.883" Id="88920" LastActivityDate="2014-03-05T18:23:38.517" OwnerUserId="32397" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;machine-learning&gt;&lt;mathematical-statistics&gt;&lt;multiple-comparisons&gt;&lt;roc&gt;" Title="How to create ROC curve to assess the performance of regression models?" ViewCount="473" />
  
  
  
  <row Body="&lt;p&gt;if the data is &lt;a href=&quot;http://missingdata.lshtm.ac.uk/index.php?option=com_content&amp;amp;view=article&amp;amp;id=76%3amissing-at-random-mar&amp;amp;catid=40%3amissingness-mechanisms&amp;amp;Itemid=96&quot; rel=&quot;nofollow&quot;&gt;missing at random&lt;/a&gt;, you can skip the missing days. &lt;/p&gt;&#10;&#10;&lt;p&gt;if you have reason to believe that the fact that the data is missing is somehow related to the probabilities of interest, then it gets complicated. for instance, the measurement is low that it records as missing. in this case the fact it is missing is related to the fact that the measurement went down.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-05T22:17:35.770" Id="88962" LastActivityDate="2014-03-05T22:17:35.770" OwnerUserId="36041" ParentId="88959" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="88977" AnswerCount="2" Body="&lt;p&gt;A rather elementary question: If we have:&lt;/p&gt;&#10;&#10;&lt;p&gt;$X = X_1 + X_2 + ... + X_n$ and $X_i \sim N(0,\sigma^2)$ and independent, &lt;/p&gt;&#10;&#10;&lt;p&gt;can we express $X$ as $\sqrt n X_i$ for any $i = 1,...,n$ ? &lt;/p&gt;&#10;&#10;&lt;p&gt;I think since all the components $X_i$ follow the normal distribution and we are summing $n$ of them up, the resulting rv is $X \sim N(0, n\sigma^2)$ and therefor we can use $X = \sqrt n X_i$.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Is this reasoning correct? If yes, under what condition it can be false?  &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-03-05T23:54:34.147" Id="88973" LastActivityDate="2014-03-06T10:03:49.280" LastEditDate="2014-03-06T00:29:01.140" LastEditorUserId="41268" OwnerUserId="41268" PostTypeId="1" Score="1" Tags="&lt;normal-distribution&gt;" Title="Sum of normally distributed random variables expressed with respect to one component" ViewCount="65" />
  <row AcceptedAnswerId="89010" AnswerCount="1" Body="&lt;p&gt;From what I understand, there are 3 main types of predictor selection method for linear models, namely, 1 Subset Selection, 2 Shrinkage and 3 Dimension Reduction. &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;The subset selection includes the Best Subset Selection and the&#10;Stepwise Selection which could be forward, backward or hybrid. AIC,&#10;BIC, Cp or Adjusted R-Square can be used to select the predictors.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The Shrinkage includes the Ridge Regression and Lasso. This approach&#10;attempts to shrink the coefficients to 0&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Dimension reduction transforms the predictors and fit the model&#10;using the transformed predictor.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;If forecasting accuracy is my main goal and model interpretability is not important, which method(s) should I use ? &#10;What should I do if the methods give inconsistent results ?&#10;What are the main advantages and disadvantages of each approach ? &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-03-06T03:22:40.877" Id="88986" LastActivityDate="2014-03-06T10:12:16.933" OwnerUserId="40660" PostTypeId="1" Score="1" Tags="&lt;model-selection&gt;&lt;dimensionality-reduction&gt;&lt;stepwise-regression&gt;&lt;shrinkage&gt;" Title="Linear model predictor selection. Which method to use ?" ViewCount="147" />
  <row Body="&lt;p&gt;What you should do is conduct a &lt;em&gt;partial F-test&lt;/em&gt;.  The null hypothesis for a 'full' F-test tests whether all regression coefficients are equal to zero.  In your case, this null is $H_{0}: a_{1} = a_{2} = a_{3} = 0$.  R's lm should print this for you.  &lt;/p&gt;&#10;&#10;&lt;p&gt;A partial F-test is when you wish to test whether a subset of the regression coefficients is equal to zero.  In your case, you wish to test $H_{0}: a_{1} = a_{3} = 0$.  Instructions to implement a partial F-test in R and a brief discussion of regression hypothesis testing can be found &lt;a href=&quot;http://math.arizona.edu/~hzhang/waeso/multlr.pdf&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Instead of trying to &quot;guess&quot; based on the results of the two students t tests for these coefficients you should test both of them jointly.  Doing so helps mitigate your probability of committing type 2 errors (&lt;a href=&quot;http://en.wikipedia.org/wiki/Type_I_and_type_II_errors&quot; rel=&quot;nofollow&quot;&gt;wikipedia&lt;/a&gt;).&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-03-06T04:20:19.163" Id="88990" LastActivityDate="2014-03-06T21:26:53.033" LastEditDate="2014-03-06T21:26:53.033" LastEditorUserId="13902" OwnerUserId="13902" ParentId="88988" PostTypeId="2" Score="3" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm working with text recognition and currently I'm using support vector machine method. I would like to try with neural network also. I read a few documents about how neural network works, but the theory is quite heavy and I don't know exactly how will it apply to my case. So it would be good if someone can help me to make it clear, especially with the neural network's architecture. &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;Currently, in SVM, I have 200 features (divided into 4 main&#10;categories), which is used to recognize the text. If I move to neural&#10;network, With 200 features, does it mean that I will have 200&#10;neutrons in the input layer?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;With 200 features, how will that result in the architecture of the&#10;neural network (in term of numbers layers (hidden layer) and&#10;neutrons)?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;In SVM, I have one class classification (basically, true and false)&#10;and multi-class classification (labels), how this difference will&#10;apply to the output layer of the neural networks?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;And I also have a few general questions :&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;What will help to decide the number of hidden layers and the number&#10;of neutrons inside each hidden layer?&lt;/li&gt;&#10;&lt;li&gt;Does the number of hidden layer relate to the accuracy ?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I'm new to neural network so it would be great if you can explain to me in an understable way. :) &#10;Thank you very much.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-06T13:23:54.980" Id="89024" LastActivityDate="2014-03-07T13:26:07.103" OwnerUserId="41359" PostTypeId="1" Score="0" Tags="&lt;svm&gt;&lt;neural-networks&gt;" Title="Moving from support vector machine to neural network (Back propagation)" ViewCount="66" />
  <row Body="&lt;p&gt;The short answer is that your $\delta$ is fine, but your $\gamma$ is wrong. In order to get the positive stable distribution given by your formula in R, you need to set&#10;$$&#10;\gamma = |1 - i \tan \left(\pi \alpha / 2\right)|^{-1/\alpha}.&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The earliest example I could find of the formula you gave was in (Feller, 1971), but I've only found that book in physical form. However (Hougaard, 1986) gives the same formula, along with the Laplace transform&#10;$$&#10;\mathrm{L}(s) = \mathrm{E}\left[\exp(-sX)\right] = \exp\left(-s^\alpha\right).&#10;$$&#10;From the &lt;code&gt;stabledist&lt;/code&gt; manual (&lt;code&gt;stabledist&lt;/code&gt; is used in &lt;code&gt;fBasics&lt;/code&gt;), the &lt;code&gt;pm=1&lt;/code&gt; parameterization is from (Samorodnitsky and Taqqu, 1994), another resource whose online reproduction has eluded me. However (Weron, 2001) gives the characteristic function in Samorodnitsky and Taqqu's parameterization for $\alpha \neq 1$ to be&#10;$$&#10;\varphi(t) = \mathrm{E}\left[\exp(i t X) \right] = \exp\left[i \delta t - \gamma^\alpha |t|^\alpha \left(1 - i \beta \mathrm{sign}(t) \tan{\frac{\pi \alpha}{2}} \right) \right].&#10;$$&#10;I've renamed some parameters from Weron's paper to coinside with the notation we're using. He uses $\mu$ for $\delta$ and $\sigma$ for $\gamma$. In any case, plugging in $\beta=1$ and $\delta=0$, we get&#10;$$&#10;\varphi(t) = \exp\left[-\gamma^\alpha |t|^\alpha \left(1 - i \mathrm{sign}(t) \tan \frac{\pi \alpha}{2} \right) \right].&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that $(1 - i \tan (\pi \alpha / 2)) / |1 - i \tan(\pi \alpha / 2)| = \exp(-i \pi \alpha / 2)$ for $\alpha \in (0, 1)$ and that $i^\alpha = \exp(i \pi \alpha / 2)$. Formally, $\mathrm{L}(s)=\varphi(is)$, so by setting $\gamma = |1 - i \tan \left(\pi \alpha / 2\right)|^{-1/\alpha}$ in $\varphi(t)$ we get&#10;$$&#10;\varphi(is) = \exp\left(-s^\alpha\right) = \mathrm{L}(s).&#10;$$&#10;One interesting point to note is that the $\gamma$ that corresponds to $\alpha=1/2$ is also $1/2$, so if you were to try $\gamma=\alpha$ or $\gamma=1-\alpha$, which is actually not a bad approximation, you end up exactly correct for $\alpha=1/2$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's an example in R to check correctness:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(stabledist)&#10;&#10;# Series representation of the density&#10;PSf &amp;lt;- function(x, alpha, K) {&#10;  k &amp;lt;- 1:K&#10;  return(&#10;    -1 / (pi * x) * sum(&#10;      gamma(k * alpha + 1) / factorial(k) * &#10;        (-x ^ (-alpha)) ^ k * sin(alpha * k * pi)&#10;    )&#10;  )&#10;}&#10;&#10;# Derived expression for gamma&#10;g &amp;lt;- function(a) {&#10;  iu &amp;lt;- complex(real=0, imaginary=1)&#10;  return(abs(1 - iu * tan(pi * a / 2)) ^ (-1 / a))&#10;}&#10;&#10;x=(1:100)/100&#10;plot(0, xlim=c(0, 1), ylim=c(0, 2), pch='', &#10;     xlab='x', ylab='f(x)', main=&quot;Density Comparison&quot;)&#10;legend('topright', legend=c('Series', 'gamma=g(alpha)'),&#10;       lty=c(1, 2), col=c('gray', 'black'),&#10;       lwd=c(5, 2))&#10;text(x=c(0.1, 0.25, 0.7), y=c(1.4, 1.1, 0.7), &#10;     labels=c(expression(paste(alpha, &quot; = 0.4&quot;)),&#10;              expression(paste(alpha, &quot; = 0.5&quot;)),&#10;              expression(paste(alpha, &quot; = 0.6&quot;))))&#10;&#10;for(a in seq(0.4, 0.6, by=0.1)) {&#10;  y &amp;lt;- vapply(x, PSf, FUN.VALUE=1, alpha=a, K=100)&#10;  lines(x, y, col=&quot;gray&quot;, lwd=5, lty=1)&#10;  lines(x, dstable(x, alpha=a, beta=1, gamma=g(a), delta=0, pm=1), &#10;        col=&quot;black&quot;, lwd=2, lty=2)&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;$\hskip1in$&#10;&lt;img src=&quot;http://i.imgur.com/GczqB8r.png&quot; alt=&quot;Plot output&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Feller, W. (1971). &lt;em&gt;An Introduction to Probability Theory and Its Applications&lt;/em&gt;, &lt;strong&gt;2&lt;/strong&gt;, 2nd ed. New York: Wiley.&lt;/li&gt;&#10;&lt;li&gt;Hougaard, P. (1986). &lt;em&gt;Survival Models for Heterogeneous Populations Derived from Stable Distributions&lt;/em&gt;, &lt;em&gt;Biometrika&lt;/em&gt; &lt;strong&gt;73&lt;/strong&gt;, 387-396.&lt;/li&gt;&#10;&lt;li&gt;Samorodnitsky, G., Taqqu, M.S. (1994). &lt;em&gt;Stable Non-Gaussian Random Processes&lt;/em&gt;, Chapman &amp;amp; Hall, New York, 1994.&lt;/li&gt;&#10;&lt;li&gt;Weron, R. (2001). &lt;em&gt;Levy-stable distributions revisited: tail index &gt; 2 does not exclude the Levy-stable regime&lt;/em&gt;, International Journal of Modern Physics C, 2001, 12(2), 209-223.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="2" CreationDate="2014-03-06T13:35:21.450" Id="89026" LastActivityDate="2014-03-06T13:35:21.450" OwnerUserId="41260" ParentId="8515" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;One such test is the &lt;a href=&quot;http://en.wikipedia.org/wiki/Brown%E2%80%93Forsythe_test&quot; rel=&quot;nofollow&quot;&gt;Brown-Forsythe test&lt;/a&gt;, which is derived from ANOVA.&lt;/p&gt;&#10;&#10;&lt;p&gt;The basic idea is that you pick a way to partition your data and then compare the mean absolute deviations from the median of each group. One common way is to put observations whose time is before the half-way point in one group, and those whose time is at/after the half-way point in another.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-06T13:55:43.963" Id="89027" LastActivityDate="2014-03-06T13:55:43.963" OwnerUserId="41260" ParentId="89017" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;the measurements can be independent or not. if you describe the measured value as $y_t=x_t+\varepsilon_t$, where $x_t$ - true value, and $\varepsilon_t$ - measurement error, then independence means that $cov(\varepsilon_t,\varepsilon_{t-i})=0$ for all times. this may or may not be true. if you have two measurements one immediately after another then it's most likely not true. if two measurements were time separated but conducted buy the same technician again this may not be true. etc.&lt;/p&gt;&#10;&#10;&lt;p&gt;on the other hand it must be possible to setup the measurement in a way that $\varepsilon_t$ would be independent of each other and the $x_t$. &lt;/p&gt;&#10;&#10;&lt;p&gt;$y_t$'s are most definitely not independent through $x_t$ correlations, but that's not what is meant by independence&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-06T15:58:43.973" Id="89041" LastActivityDate="2014-03-06T15:58:43.973" OwnerUserId="36041" ParentId="41420" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;With financial data, the question you raised was first addressed in the 1980s by Robert Engle who later got a Nobel Prize for his contributions to econometrics, as well as Tim Bollerslev. They formalized exactly what you think makes sense: the greater the change in the asset price, the more volatility will endure. Models of these type are called &lt;a href=&quot;http://en.wikipedia.org/wiki/Autoregressive_conditional_heteroskedasticity&quot; rel=&quot;nofollow&quot;&gt;autoregressive conditional heteroskedastity models&lt;/a&gt;, or ARCH for short, and they say that variance at time $t$ is a function of the magnitudes of shocks in prior periods:&#10;$$&#10;\sigma_t^2 = \alpha_0 + \alpha_1 \epsilon_{t-1}^2 + \alpha_2 \epsilon_{t-2}^2 + \ldots + \nu_t&#10;$$&#10;with understanding that coefficients in this model are positive. There's been an unimaginable number of extensions this model has received, including all soft of transforms, putting lags of $\sigma_t^2$ into the model, treating positive and negative shocks differently, specifying thresholds, etc. These are reduced form models, in the sense that they may not provide a clear message about the mechanism, but they fit financial data very well, and in particular generate the desirable heavy tail distributions even with $\epsilon_t$ are conditionally $N(0,\sigma_t^2)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The heteroskedasticity test in this context would of course be the test that $\alpha_1 = \alpha_2 = \ldots = 0$.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-06T17:20:18.430" Id="89053" LastActivityDate="2014-03-06T17:20:18.430" OwnerUserId="5739" ParentId="89017" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;There is a lot to comment on here. Not sure what is the most useful.&lt;/p&gt;&#10;&#10;&lt;p&gt;(0) &lt;code&gt;I've also tried univariate linear regression of each independent but am not getting something that looks usable&lt;/code&gt;  If there isn't a strong association, statistics won't solve that. Exploratory data analysis with graphs can be useful here.&lt;br&gt;&#10;(1) &lt;code&gt;dependent variable (time span)&lt;/code&gt; Time variables: these can be tricky. They are often not normally distributed. You may want to examine this assumption&lt;br&gt;&#10;(2) &lt;code&gt;quantitative qualitative independent variable&lt;/code&gt; This isn't useful terminology. Continuous or categorical. And if you think that there isn't a linear relationship for continuous variables (as discovered in exploratory analysis), how you plan to accommodate that would be a better start&lt;br&gt;&#10;(3) X2 and X3 appear to be correlated based on regression. Are they? Exploratory analysis with X2 vs X3 plot would be useful.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-03-06T18:53:17.143" Id="89057" LastActivityDate="2014-03-06T19:04:16.850" LastEditDate="2014-03-06T19:04:16.850" LastEditorUserId="34658" OwnerUserId="34658" ParentId="89028" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Your covariance matrix construction is incorrect. In particular, you have a tridiagonal matrix, but an AR(1) covariance matrix has no zeros. The offdiagonal elements should have correlations that are the AR(1) coefficient raised to the difference in times between the two observations, i.e. the difference between the row and column number.&lt;/p&gt;&#10;&#10;&lt;p&gt;Define sigma using this code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;D     = diag(length(t))&#10;rho   = coef(residual.model)&#10;corr  = rho^abs(row(D)-col(D))&#10;sigma = 10^2/(1-rho^2) * corr&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This AR(1) covariance matrix construction in R was taken from &lt;a href=&quot;https://stat.ethz.ch/pipermail/r-help/2007-May/131728.html&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-03-06T18:59:18.157" Id="89058" LastActivityDate="2014-12-10T13:45:15.120" LastEditDate="2014-12-10T13:45:15.120" LastEditorUserId="40440" OwnerUserId="40440" ParentId="89049" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;It is the standard deviation of the resamples:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; lmFit$resample&#10;          RMSE  Rsquared Resample&#10;    1 4.702857 0.7283872    Fold1&#10;    2 5.266187 0.6838433    Fold2&#10;&amp;gt; apply(lmFit$resample[, 1:2], 2, sd)&#10;      RMSE   Rsquared &#10;0.39833479 0.03149727 &#10;&amp;gt; lmFit&#10;Linear Regression &#10;&#10;506 samples&#10; 13 predictors&#10;&#10;No pre-processing&#10;Resampling: Cross-Validated (2 fold) &#10;&#10;Summary of sample sizes: 253, 253 &#10;&#10;Resampling results&#10;&#10;  RMSE  Rsquared  RMSE SD  Rsquared SD&#10;  4.98  0.706     0.398    0.0315   &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Max&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-06T20:34:40.300" Id="89072" LastActivityDate="2014-03-06T20:34:40.300" OwnerUserId="3468" ParentId="89020" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="89095" AnswerCount="1" Body="&lt;p&gt;I am conducting &lt;code&gt;Goodness-of-fit&lt;/code&gt; test on 2 cities using chi square.  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;                  | Males    Females&#10;                  | --------------&#10;Test 1 : Boston   | 50000    10000&#10;say chi square = 29.45 with df=1&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Similiarly, &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;                  | Males     Females&#10;                  | --------------&#10;Test 2 : NYC      | 500000    20000&#10;say chi square = 455.45 with df=1    &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Can I say since chi-square for NYC was much higher than that of Boston, the 'unknown' relationship between city and sex ratio is stronger in NYC than Boston?  &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-03-06T21:20:56.317" Id="89078" LastActivityDate="2014-03-07T10:29:43.840" OwnerUserId="14843" PostTypeId="1" Score="2" Tags="&lt;chi-squared&gt;&lt;goodness-of-fit&gt;&lt;frequency&gt;" Title="Compared results of 2 goodness-of-fit chi-square test" ViewCount="84" />
  
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am conducting a study which is looking at the effect of 'Condition' (Quiet, Intelligible, Unintelligible) on the pupil(eye) response over time. Upon visual inspection of my data plots, pupil response slopes appear to change in a curvilinear way and there appear to be differences in the timing and amplitude of peaks and slopes between conditions at different timepoints.&lt;/p&gt;&#10;&#10;&lt;p&gt;In order to describe and statistically analyse these differences I decided to use 'Growth Curve analysis' and analyse the effect of 'Condition' on the pupil response intercept, slope (linear), quadratic, cubic, quartic, and quintic terms to see if any of these terms in particular best describe any difference across conditions.&lt;/p&gt;&#10;&#10;&lt;p&gt;So here is the base model which I constructed ('ot' refers to orthogonal time terms);&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;m.base &amp;lt;- lmer(PupilBaseCorrect ~ (ot1+ot2+ot3+ot4+ot5) + (ot1+ot2+ot3+ot4+ot5 | ParticipantNumber), data=Study2rbind, REML=F)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;To analyse, firstly, the effect of adding 'Condition' as a fixed effect on the intercept and, subsequently, the effect of 'Condition' on the slope and other time terms (quadratic, cubic, etc.) I entered the following code;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;m.0 &amp;lt;- lmer(PupilBaseCorrect ~ (ot1+ot2+ot3+ot4+ot5) + Condition + (ot1+ot2+ot3+ot4+ot5 | ParticipantNumber), data=Study2rbind, REML=F)&#10;m.1 &amp;lt;- lmer(PupilBaseCorrect ~ (ot1+ot2+ot3+ot4+ot5) + Condition + ot1:Condition +   (ot1+ot2+ot3+ot4+ot5 | ParticipantNumber), data=Study2rbind, REML=F)&#10;m.2 &amp;lt;- lmer(PupilBaseCorrect ~ (ot1+ot2+ot3+ot4+ot5) + Condition + (ot1+ot2:Condition)   + (ot1+ot2+ot3+ot4+ot5 | ParticipantNumber), data=Study2rbind, REML=F)&#10;m.3 &amp;lt;- lmer(PupilBaseCorrect ~ (ot1+ot2+ot3+ot4+ot5) + Condition + (ot1+ot2+ot3:Condition) + (ot1+ot2+ot3+ot4+ot5 | ParticipantNumber), data=Study2rbind, REML=F)&#10;m.4 &amp;lt;- lmer(PupilBaseCorrect ~ (ot1+ot2+ot3+ot4+ot5) + Condition + (ot1+ot2+ot3+ot4:Condition) + (ot1+ot2+ot3+ot4+ot5 | ParticipantNumber), data=Study2rbind, REML=F)&#10;m.5 &amp;lt;- lmer(PupilBaseCorrect ~ (ot1+ot2+ot3+ot4+ot5) + Condition + (ot1+ot2+ot3+ot4+ot5:Condition) + (ot1+ot2+ot3+ot4+ot5 | ParticipantNumber), data=Study2rbind, REML=F)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So I guess I've got two questions relating to my analyses:&lt;/p&gt;&#10;&#10;&lt;p&gt;(1) I'm not 100% sure that I have coded the fixed and random effects correctly, so any advice would be much appreciated. I believe that there will be variability across participants and have therefore including participants ('ParticipantNumber') as a random effect. I can't think of any reason to add a 'ParticipantNumber|Condition' random effect also although I have noticed that this is done in lots of other cases, so maybe there is a good reason for doing this (which I'm unaware of)?&lt;/p&gt;&#10;&#10;&lt;p&gt;(2) This is my output when I run an ANOVA to assess model fit;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Df     AIC     BIC logLik deviance  Chisq Chi Df Pr(&amp;gt;Chisq)    &#10;m.base 28 -1630.8 -1476.9 843.38  -1686.8                             &#10;m.0    30 -1626.8 -1461.9 843.38  -1686.8  0.000      2          1    &#10;m.1    32 -1651.7 -1475.9 857.87  -1715.7 28.977      2  5.102e-07 ***&#10;m.2    32 -1639.8 -1463.9 851.90  -1703.8  0.000      0          1    &#10;m.3    32 -1641.1 -1465.2 852.53  -1705.1  1.264      0  &amp;lt; 2.2e-16 ***&#10;m.4    32 -1631.8 -1455.9 847.88  -1695.8  0.000      0          1    &#10;m.5    32 -1646.0 -1470.1 855.00  -1710.0 14.234      0  &amp;lt; 2.2e-16 ***&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;My next step is to run pairwise comparisons between the three levels of my 'Condition' variable to uncover where the difference lies for linear, cubic, and quintic terms. However, I haven't yet been able to find a straightforward way of doing this on R. Any suggestions from people who have run similar analyses would be much appreciated.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-07T12:07:31.273" Id="89145" LastActivityDate="2014-03-07T12:44:02.253" LastEditDate="2014-03-07T12:44:02.253" LastEditorUserId="22468" OwnerUserId="41495" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;mixed-model&gt;&lt;lmer&gt;&lt;polynomial&gt;&lt;growth-model&gt;" Title="Growth curve analysis on orthogonal polynomial terms" ViewCount="135" />
  
  <row Body="&lt;p&gt;Welcome to the site&lt;/p&gt;&#10;&#10;&lt;p&gt;This is a question about model selection (see the tag with that label) but not completely.&lt;/p&gt;&#10;&#10;&lt;p&gt;The answer to your question depends on what you want to find out; it also depends on some things about the data.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you add one variable at a time, you get to see how each variable affects the group differences, but you are then not controlling for other variables. If you run one model with all the variables, you have controlled for them all. However, when you  have multiple variables you may run into a) Over fitting (if you don't have a large sample) b) Collinearity (if independent variables are strongly related to each other). In your case, I'd guess there might be colinearity among duration, medication and symptom variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, if those issues are not problems, my inclination is to include all the variables in one model.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-07T12:51:43.120" Id="89148" LastActivityDate="2014-03-07T12:51:43.120" OwnerUserId="686" ParentId="89146" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I am looking to run a series of performance tests for a software system across different configurations and determine which combination works best.  The software system processes a number of files, does some work with them and writes them to a database.&lt;/p&gt;&#10;&#10;&lt;p&gt;In order to carry out these tests, I have a script that will run the software using &lt;strong&gt;varying file sizes&lt;/strong&gt; (10, 100, 500, 1000), &lt;strong&gt;varying buffer sizes&lt;/strong&gt; (0, 10, 100, 300, 500), and &lt;strong&gt;two different &quot;run-configurations&quot;&lt;/strong&gt;.  Each combination is run 30 times and the execution time is recorded for each combination for each run-configuration I am looking to test.  The same files are used for each run and the database is cleared to ensure independent trials.&lt;/p&gt;&#10;&#10;&lt;p&gt;My first question is whether or not this is a statistically sound way of conducting these experiments.  Would the assumption that observations are independent hold?&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to run factorial ANOVA across the file sizes, buffer sizes, and configurations to determine if there are interaction effects between them and the execution time.&lt;/p&gt;&#10;&#10;&lt;p&gt;My next question is what should I do if the execution time variability across groups is roughly equal but execution times within the various groups do not follow a normal distribution?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-03-07T12:52:27.550" Id="89149" LastActivityDate="2014-03-07T12:52:27.550" OwnerUserId="41496" PostTypeId="1" Score="0" Tags="&lt;distributions&gt;&lt;anova&gt;&lt;statistical-significance&gt;&lt;software&gt;&lt;methodology&gt;" Title="Experiments for performance testing software" ViewCount="29" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have a joint distribution which can be written down as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;p(\Theta | D) \propto p(D|\Theta) \; p(w|\lambda) \; p(\lambda) \; p(\phi)&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\Theta = \{w, \lambda, \phi\}$. The prior distribution on $\lambda$ and $\phi$ are modelled using a Gamma distribution with some initial scale and shape parameters. The prior on $w$ is given by the term $p(w|\lambda)$ which is a multivariate normal with $0$ mean and some precision given by $\lambda \Sigma^{-1}$. The likelihood term is given by $p(D|\Theta)$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Now, I would like to get the updates for the parameters for $\lambda$ and $\phi$, for a given $w$. So, for a given estimate of $w$, I would like to update the current posterior estimates of the Gamma distribution for $\lambda$ and $\phi$. Since, the Gamma prior is conjugate to the likelihood term which is modelled as normal, there should be some explicit updates to these parameters, I am guessing. However, I do not know much about this and do not know how to proceed. Also, the prior $p(w|\lambda)$ has this dependency on $\lambda$ and I am not sure if this is a problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;Appreciate any suggestions on how to begin to proceed on this.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-07T15:05:59.417" Id="89170" LastActivityDate="2014-03-07T18:01:51.447" OwnerUserId="36540" PostTypeId="1" Score="1" Tags="&lt;bayesian&gt;&lt;normal-distribution&gt;&lt;basic-concepts&gt;&lt;gamma-distribution&gt;&lt;conjugate-prior&gt;" Title="Updating the hyper parameters for conjugate distributions" ViewCount="74" />
  
  
  
  <row AcceptedAnswerId="107450" AnswerCount="1" Body="&lt;p&gt;I had thought that a principal components analysis gives us information about the number of degrees of freedom (i.e. independent variables) underlying a system which we may only be able to observe through derived (and possibly correlated) variables. But a simple experiment has made me doubt this:&lt;/p&gt;&#10;&#10;&lt;p&gt;I generated 1,000 sets of 10 drawings from independent standard normal distributions ($\mathcal{N}(0,1)$) and calculated the PCA for that data matrix. The correlation matrix was very close to diagonal, as I'd hope. The eigenvalues, and corresponding proportion of variances were:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; 1.14   12%&#10; 1.11   11%&#10; 1.07   11%&#10; 1.04   10%&#10; 1.02   10%&#10; 0.98   10%&#10; 0.96   10%&#10; 0.93    9%&#10; 0.85    9%&#10; 0.81    8%&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This chimed with my expectations: this system has ten independent variables, and the eigenvalues make it clear that all ten principal components carry a significant contribution to the total variance of the system.&lt;/p&gt;&#10;&#10;&lt;p&gt;Next, for each set of observations, I derived a new set of variables, $\{y_1,...,y_{10}\}$, by taking linear combinations in the form of cumulative averages of the original variables $\{x_1,...,x_{10}\}$:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ y_n = \frac{1}{n}\sum_{i=1}^{n}x_i$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I've lost no information by doing this: all the $\{x\}$ can be exactly recovered from $\{y\}$. However, while the $x$ were independent, the $y$ are clearly correlated (and the correlation matrix confirms this).&lt;/p&gt;&#10;&#10;&lt;p&gt;I calculated the PCA for $\{y\}$ and obtained this set of eigenvalues:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; 1.98   68%&#10; 0.57   19%&#10; 0.18    6%&#10; 0.09    3%&#10; 0.04    1%&#10; 0.03    1%&#10; 0.01    0%&#10; 0.01    0%&#10; 0.01    0%&#10; 0.00    0%&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Which suggests that only five or six principal components are necessary to capture virtually all the variance in the system.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, to some extent, I see that this makes sense: the $y$ are quite strongly correlated, so e.g. knowing $\{y_1,...,y_9\}$ places tight constraints on $y_{10}$. But the system is still driven by ten underlying independent variables, and all I've done is take linear combinations of them to form my new variables. Shouldn't the PCA be able to see through this?&lt;/p&gt;&#10;&#10;&lt;p&gt;Clearly my intuition is a little awry. Can anyone help me to see this more clearly?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-03-07T20:16:31.763" Id="89208" LastActivityDate="2014-07-10T10:16:55.280" OwnerUserId="41522" PostTypeId="1" Score="1" Tags="&lt;pca&gt;&lt;degrees-of-freedom&gt;" Title="PCA and degrees of freedom" ViewCount="141" />
  
  
  
  <row Body="&lt;p&gt;It's obviously problem specific. This is the kind of question you can address with simulation. Furthermore, you didn't have $p \gg n$ since you were able to fit models going both directions in stepwise selection, so the &quot;best fitting&quot; model is the one using all parameters... unless you implemented split sample cross validation for model evaluation (you didn't describe this, I assume you didn't do it). LASSO default uses cv to choose the tuning parameter that minimizes MSE of prediction, so it automatically has leverage over &quot;out-of-the-box&quot; stepwise model selection. Lastly, stepwise model selection can have different criteria. If you use parameter p-values or model R^2, you will not do well. AIC or BIC are much better criteria for model selection.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are a number of problems with each method, though I think the problems with stepwise regression are more adequately documented and understood. The main problem I see with your question is that you are using &lt;em&gt;feature selection&lt;/em&gt; tools to evaluate &lt;em&gt;prediction&lt;/em&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;LASSO's great strength is that it can estimate models in which $p \gg n$, as can be the case forward stepwise regression. In both cases, these models can be effective for prediction only when there is a handful of very powerful predictors. If an outcome is better predicted by many weak predictors, then ridge regression or bagging/boosting will outperform forward stepwise regression and LASSO by a long shot. LASSO is much faster than forward stepwise regression. &lt;/p&gt;&#10;&#10;&lt;p&gt;There is obviously a great deal of overlap between feature selection and prediction, but I never tell you about how well a wrench serves as a hammer. In general, for prediction with a sparse number of model coefficients and $p \gg n$, I would prefer LASSO over forward stepwise model selection.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-07T21:32:51.917" Id="89219" LastActivityDate="2014-03-07T21:32:51.917" OwnerUserId="8013" ParentId="89202" PostTypeId="2" Score="2" />
  <row AnswerCount="0" Body="&lt;p&gt;Dear CrossValidated community,&lt;/p&gt;&#10;&#10;&lt;p&gt;Can anyone help me to prove the bias in a given parameter of a regression when there is omitted variable?&lt;/p&gt;&#10;&#10;&lt;p&gt;I know to do it using matrices and matrix algebra. For instance, consider the true equation:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Y = BX + dZ + e&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Where X is the matrix of all exogenous variables, B is the matrix of all parameters to be estimated for the exogenous variables, Z is one exogenous variables that will be omitted, d is its parameter and e is the error.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then,&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;E[B] = (X^'X)^(-1)X^'Y&#10;E[B] = (X^'X)^(-1)X^'(XB+dZ+e)&#10;E[B] = (X^'X)^(-1)X^'XB+(X^'X)^(-1)X^Zd+(X^'X)^(-1)X^'e&#10;E[B] = B+(X^'X)^(-1)X^'Zd+(X^'X)^(-1)X^'e&#10;E[B] = B+(X^'X)^(-1)X^'Zd&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Therefore the bias is (X^'X)^(-1)X^'Zd.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, I would like to achieve the samething by using variance-covariance notation and algebra, in a multivariate regression context. I mean,&lt;/p&gt;&#10;&#10;&lt;p&gt;For a true equation:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Y = b_0 + b_1*X_1 + b_2*X_2 + b_3*X_3 + b_4*X_4 + e&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Suppose we estimate:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Y = b_0 + b_1*X_1 + b_2*X_2 + b_3*X_3 + e&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And we are interested in X_2 and its coefficient b_2.&lt;/p&gt;&#10;&#10;&lt;p&gt;How can I find the bias in b_2 using Var(b_2) and Cov(b_2,e) notation and calculation?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks a lot&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-07T21:35:19.403" Id="89223" LastActivityDate="2014-03-10T00:13:52.040" LastEditDate="2014-03-10T00:13:52.040" LastEditorUserId="41460" OwnerUserId="41460" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;bias&gt;&lt;variance-covariance&gt;&lt;proof&gt;&lt;endogeneity&gt;" Title="Finding parameter bias under omitted variable, with variance covariance notation" ViewCount="72" />
  
  
  <row Body="&lt;p&gt;Looking at &lt;a href=&quot;http://www.cs.uvm.edu/~xwu/kdd/Slides/Kmeans-ICDM06.pdf&quot; rel=&quot;nofollow&quot;&gt;these&lt;/a&gt; notes time complexity of &lt;strong&gt;Lloyds algorithm for k-means clustering&lt;/strong&gt; is given as:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;O(n * K * I * d)&#10;&#10;n : number of points&#10;K : number of clusters&#10;I : number of iterations&#10;d : number of attributes&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;My gut feeling is that in your case number of iterations (and number of attributes) is assumed to be constant.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-03-08T00:57:38.147" Id="89245" LastActivityDate="2014-03-08T19:17:49.800" LastEditDate="2014-03-08T19:17:49.800" LastEditorUserId="7828" OwnerUserId="11708" ParentId="89229" PostTypeId="2" Score="2" />
  <row AnswerCount="1" Body="&lt;p&gt;This is the logistic distribution of single random variable (taken from Wikipedia).&lt;/p&gt;&#10;&#10;&lt;p&gt;$x$ = random variable&#10;$\mu$ = mean of all random variables&#10;$s$ = variance.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/qUHkD.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, I want to do a Bivariate logistic distribution (having two random variables $x_1$ and $x_2$). My dataset is going to be image pixel values!&lt;/p&gt;&#10;&#10;&lt;p&gt;When I find the covariance of two random variables, it turns out to be a 2 x 2 square matrix but I need a single number! How to actually compute a bivariate model?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-08T08:00:04.673" Id="89259" LastActivityDate="2014-03-09T01:45:30.550" LastEditDate="2014-03-08T08:36:34.960" LastEditorUserId="24808" OwnerUserId="41542" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;distributions&gt;&lt;logistic&gt;&lt;bivariate&gt;" Title="How to compute for Bivariate Logistic Distribution" ViewCount="35" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I am studying the effect a certain chemical and age on an outcome.  As variables, &lt;code&gt;age&lt;/code&gt; is a factor of with levels 1, 2, and 3 and &lt;code&gt;covar1&lt;/code&gt; is continuous.  &lt;/p&gt;&#10;&#10;&lt;p&gt;After fitting the model&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;mod1 &amp;lt;- glm(out1 ~ covar1*age, family=poisson, data=df)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The estimates are&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Parametric coefficients:&#10;                Estimate Std. Error t value      Pr(&amp;gt;|t|)    &#10;(Intercept)    -0.6236832  0.1037201  -6.013 0.00000000185 ***&#10;covar1          0.0204307  0.0101802   2.007        0.0448 *  &#10;age2            1.2523312  0.0649721  19.275       &amp;lt; 2e-16 ***&#10;age3            0.8373618  0.0673236  12.438       &amp;lt; 2e-16 ***&#10;covar1:age2    -0.0145580  0.0098667  -1.475        0.1401    &#10;covar1:age3     0.0109891  0.0101829   1.079        0.2805    &#10;---&#10;Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;How do I interpret the effects of &lt;code&gt;covar1&lt;/code&gt; and &lt;code&gt;age&lt;/code&gt; on my outcome?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-03-08T14:58:07.863" Id="89277" LastActivityDate="2014-03-08T15:29:36.070" LastEditDate="2014-03-08T15:22:52.127" LastEditorUserId="1739" OwnerUserId="41550" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;regression&gt;&lt;generalized-linear-model&gt;&lt;interaction&gt;&lt;interpretation&gt;" Title="How to interpret regression estimates" ViewCount="96" />
  <row AnswerCount="1" Body="&lt;p&gt;I'm analyzing a survey data that contains the following 3 variables (questions):&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;If the election was today who would you vote for in the following list?&#10;(A, B, C, D, or E)?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Let's imagine the respondent choose A in the previous item. Then, she is asked:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;What if A is not participating in the contest, who would you vote for?&#10;(B, C, D, or E)?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Let's imagine the respondent choose C in the previous item. Then, she is asked:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;What if A and C are not participating in the contest, who would you vote for?&#10;(B, D, or E)?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I have the answer for the three questions and I want to create an algorithm to help me calculate the chances of victory of A against all other candidates, the chances of B against all the others, and so on.&lt;/p&gt;&#10;&#10;&lt;p&gt;My goal is to report who are the most dangerous opponents of each candidate.&lt;/p&gt;&#10;&#10;&lt;p&gt;Anyone has an idea/suggestion?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-03-08T15:19:22.290" FavoriteCount="1" Id="89280" LastActivityDate="2014-12-01T08:14:04.293" OwnerUserId="16127" PostTypeId="1" Score="0" Tags="&lt;conditional-probability&gt;&lt;survey&gt;&lt;computational-statistics&gt;" Title="How to calculate the probability of winning an election?" ViewCount="136" />
  <row Body="&lt;p&gt;It means the random variables are statistically independent.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-03-08T15:26:10.610" Id="89281" LastActivityDate="2014-03-08T15:26:10.610" OwnerUserId="2074" ParentId="88769" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="89458" AnswerCount="1" Body="&lt;p&gt;I came a cross two ways to evaluate the performance of binary classifiers: accuracy and precision. When to choose each? And what are the advantages and disadvantages of each one?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-08T19:24:16.980" Id="89299" LastActivityDate="2014-03-10T14:11:29.557" OwnerUserId="27779" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;classification&gt;" Title="When to use accuracy and precision to evaluate binary classifiers?" ViewCount="86" />
  <row Body="&lt;p&gt;Well, it doesn't look as you have a lot of data - most search queries are just one or two terms, aren't they?&lt;/p&gt;&#10;&#10;&lt;p&gt;You might want to look at &lt;strong&gt;association rule mining&lt;/strong&gt; / frequent itemset mining.&lt;/p&gt;&#10;&#10;&lt;p&gt;This should be able to solve the &quot;frequently bought (sought) together&quot; problem.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-08T19:27:13.070" Id="89301" LastActivityDate="2014-03-08T19:27:13.070" OwnerUserId="7828" ParentId="89180" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;&lt;em&gt;Expanding on a comment:&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;My preference for showing confidence intervals is to use coloured contour plots, going from a deep coloured region for high certainty to pale colours for the edge of the distribution.  That way, you not only show the different boundaries of the confidence regions, but you also give the visual impression of a &quot;fuzzy&quot; region, fading from the best-prediction line to the edge of possibility.&lt;/p&gt;&#10;&#10;&lt;p&gt;I haven't been doing much advanced R graphics lately, so I can't help you too much with implementation, but I put together a mock-up in Javascript/D3:  &lt;a href=&quot;http://fiddle.jshell.net/WDc3H/1/&quot; rel=&quot;nofollow&quot;&gt;http://fiddle.jshell.net/WDc3H/1/&lt;/a&gt; &#10;&lt;img src=&quot;http://i.stack.imgur.com/nzBzt.png&quot; alt=&quot;Example of coloured contours for confidence interval&quot; title=&quot;Clear CI progresion&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Because I've implemented the fading colours through transparency, you can even add in a partially-overlapping second prediction+CI set and compare the two.  How easy it is to compare will depend on how much overlap you have.  You can use the checkboxes on the above link to toggle a second series, but there is really too much overlap, and the contour lines from the different confidence intervals are more dominant than the difference in colour.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/mmqTn.png&quot; alt=&quot;Coloured contours for two lines, with heavy overlap&quot; title=&quot;Confusing overlap&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;It's more effective when there is greater divergence in the distributions, as in this modified example where I've extended the lines over a wider domain:&lt;br&gt;&#10; &lt;a href=&quot;http://fiddle.jshell.net/WDc3H/2/&quot; rel=&quot;nofollow&quot;&gt;http://fiddle.jshell.net/WDc3H/2/&lt;/a&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/AEfhK.png&quot; alt=&quot;Coloured contours for two diverging lines&quot; title=&quot;Useful comparison of different CI regions&quot;&gt;&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-03-09T00:13:26.900" Id="89320" LastActivityDate="2014-03-09T00:13:26.900" OwnerUserId="36985" ParentId="89158" PostTypeId="2" Score="3" />
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;it seems like since $\bar x$ and $\bar y$ are values, not random variables, $W$ is just a value?  If that's the case, how can you take the expected value and variance of $W$?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;If that was the case then you couldn't take the expected value and variance. However the means are defined as&#10;\begin{align}&#10;\bar x &amp;amp;= \frac{1}{n}\sum_{i=1}^n X_i\\&#10;\bar y &amp;amp;= \frac{1}{m}\sum_{i=1}^m Y_i&#10;\end{align}&#10;and because the means are (scaled) sums of random variables they are themselves random variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;Therefore for part &lt;strong&gt;(a)&lt;/strong&gt;, using linearity of expectation,&#10;$$E(W) = aE(\bar x) + (1-a)E(\bar y) = aE\left(\frac{1}{n}\sum_{i=1}^n X_i\right) +  (1-a)E\left(\frac{1}{m}\sum_{i=1}^m Y_i\right) $$&#10;which can be computed using linearity of expectation again.&#10;\begin{align}&#10;aE\left(\frac{1}{n}\sum_{i=1}^n X_i\right) +  (1-a)E\left(\frac{1}{m}\sum_{i=1}^m Y_i\right)&#10;&amp;amp;= \frac{a}{n}\sum_{i=1}^n E(X_i) + \frac{1-a}{m}\sum_{i=1}^m E(Y_i)\\&#10;&amp;amp;= \frac{a}{n} n\mu + \frac{1-a}{m} m\mu\\&#10;&amp;amp;\\&#10;&amp;amp;= \mu&#10;\end{align} &lt;/p&gt;&#10;&#10;&lt;p&gt;Similarly for part &lt;strong&gt;(b)&lt;/strong&gt;, the variance of $W$ can be computed using the fact that the variance of a sum of independent random variables is the sum of their variances together with $V(kX)=k^2V(X)$.&#10;\begin{align}&#10;V(W)&#10;&amp;amp;= a^2V(\bar x) + (1-a)^2V(\bar y)\\&#10;&amp;amp;= a^2V\left(\frac{1}{n}\sum_{i=1}^n X_i\right) +  (1-a)^2V\left(\frac{1}{m}\sum_{i=1}^m Y_i\right)\\&#10;&amp;amp;= a^2\sigma_1^2/n + (1-a)^2\sigma_2^2/m&#10;\end{align}&#10;The minimum can be found by differentiating w.r.t. $a$.&#10;\begin{align}&#10;\frac{d}{a}V(W)&#10;&amp;amp;= 2a\sigma_1^2/n - 2(1-a)\sigma_2^2/m\\&#10;&amp;amp;= 2(a(\sigma_1^2/n + \sigma_2^2/m) - \sigma_2^2/m))&#10;\end{align}&#10;Setting this to zero gives&#10;$$a = \frac{\sigma_2^2/m}{\sigma_1^2/n + \sigma_2^2/m} = \frac{n\sigma_2^2}{m\sigma_1^2 + n\sigma_2^2}$$&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-03-09T01:28:46.390" Id="89323" LastActivityDate="2014-03-16T18:10:57.490" LastEditDate="2014-03-16T18:10:57.490" LastEditorUserId="25936" OwnerUserId="25936" ParentId="89321" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm a newbie using LinearSVM to train the classifier.&lt;br&gt;&#10;I labelled the images of 'buildings' as 1 and the others as -1. &lt;/p&gt;&#10;&#10;&lt;p&gt;The training result is as follows : &lt;img src=&quot;http://i.stack.imgur.com/oXmNA.png&quot; alt=&quot;enter image description here&quot;&gt; and &lt;img src=&quot;http://i.stack.imgur.com/5vduY.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;As you can see in the image some of the buildings have positive scores and others are having negative scores. Also the images labelled -1 are getting a positive score. Why is this happening?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-09T12:22:14.343" Id="89348" LastActivityDate="2014-03-09T12:22:14.343" OwnerUserId="41081" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;svm&gt;&lt;matlab&gt;" Title="Low accuracy for training in image classification" ViewCount="26" />
  <row AnswerCount="0" Body="&lt;p&gt;Hi all and thank you in advance for helping!&lt;/p&gt;&#10;&#10;&lt;p&gt;I have three variables: $y$ the dependent count variable, $x$ the independent positive continuous variable, $U$ is the observation unit (and I expect a random effect with it). I would like to test the hypothesis that the slope for $x$ is positive. I would normally use the following model:&lt;/p&gt;&#10;&#10;&lt;p&gt;$y = \beta_0 + \beta_1 x + \phi + \epsilon$, where &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;$\phi$ is the observation unit-level random error and&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;$\epsilon$ is the observation-level random error.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;But I have a complication: at each observation unit, the original $X$ is transformed in some unknown order-preserving way, but not necessarily the same way at each observation unit. For example:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;At $U=$&quot;A&quot;, $x=X/2$,&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;At $U=$&quot;B&quot;, $x=X^2$,&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;At $U=$&quot;C&quot;, $x=3X+3$&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;How would I test my hypothesis that the slope for $x$ is positive? Is it feasible?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-03-09T13:05:49.607" Id="89351" LastActivityDate="2014-03-09T13:05:49.607" OwnerUserId="41243" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;hypothesis-testing&gt;&lt;mixed-model&gt;&lt;data-transformation&gt;" Title="Two stage model with a randomly transformed independent variable" ViewCount="14" />
  
  
  <row Body="&lt;h2&gt;How to Construct a Confidence Region&lt;/h2&gt;&#10;&#10;&lt;p&gt;Let us begin with a general method for constructing confidence regions. It can be applied to a single parameter, to yield a confidence interval or set of intervals; and it can be applied to two or more parameters, to yield higher dimensional confidence regions.&lt;/p&gt;&#10;&#10;&lt;p&gt;We assert that the observed statistics $D$ originate from a distribution with parameters $\theta$, namely the sampling distribution $s(d|\theta)$ over possible statistics $d$, and seek a confidence region for $\theta$ in the set of possible values $\Theta$. Define a Highest Density Region (HDR): the $h$-HDR of a PDF is the smallest subset of its domain that supports probability $h$. Denote the $h$-HDR of $s(d|\psi)$ as $H_\psi$, for any $\psi \in \Theta$. Then, the $h$ confidence region for $\theta$, given data $D$, is the set $C_D = \{ \phi : D \in H_\phi \}$. A typical value of $h$ would be 0.95.&lt;/p&gt;&#10;&#10;&lt;h2&gt;A Frequentist Interpretation&lt;/h2&gt;&#10;&#10;&lt;p&gt;From the preceding definition of a confidence region follows&#10;$$&#10;d \in H_\psi \longleftrightarrow \psi \in C_d&#10;$$&#10;with $C_d = \{ \phi : d \in H_\phi \}$. Now imagine a large set of (&lt;em&gt;imaginary&lt;/em&gt;) observations $\{D_i\}$, taken under similar circumstances to $D$. i.e. They are samples from $s(d|\theta)$. Since $H_\theta$ supports probability mass $h$ of the PDF $s(d|\theta)$, $P(D_i \in H_\theta) = h$ for all $i$. Therefore, the fraction of $\{D_i\}$ for which $D_i \in H_\theta$ is $h$. And so, using the equivalence above, the fraction of $\{D_i\}$ for which $\theta \in C_{D_i}$ is also $h$.&lt;/p&gt;&#10;&#10;&lt;p&gt;This, then, is what the frequentist claim for the $h$ confidence region for $\theta$ amounts to:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Take a large number of imaginary observations $\{D_i\}$ from the sampling distribution $s(d|\theta)$ that gave rise to the observed statistics $D$. Then, $\theta$ lies within a fraction $h$ of the analogous but imaginary confidence regions $\{C_{D_i}\}$.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;The confidence region $C_D$ therefore does not make any claim about the probability that $\theta$ lies somewhere! The reason is simply that there is nothing in the fomulation that allows us to speak of a probability distribution over $\theta$. The interpretation is just elaborate superstructure, which does not improve the base. The base is only $s(d | \theta)$ and $D$, where $\theta$ does not appear as a distributed quantity, and there is no information we can use to address that. There are basically two ways to get a distribution over $\theta$:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Assign a distribution directly from the information at hand: $p(\theta | I)$.&lt;/li&gt;&#10;&lt;li&gt;Relate $\theta$ to another distributed quantity: $p(\theta | I) = \int p(\theta x | I) dx = \int p(\theta | x I) p(x | I) dx$.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;In both cases, $\theta$ must appear on the left somewhere. Frequentists cannot use either method, because they both require a heretical prior.&lt;/p&gt;&#10;&#10;&lt;h2&gt;A Bayesian View&lt;/h2&gt;&#10;&#10;&lt;p&gt;The most a Bayesian can make of the $h$ confidence region $C_D$, given without qualification, is simply the direct interpretation: that it is the set of $\phi$ for which $D$ falls in the $h$-HDR $H_\phi$ of the sampling distribution $s(d|\phi)$. It does not necessarily tell us much about $\theta$, and here's why.&lt;/p&gt;&#10;&#10;&lt;p&gt;The probability that $\theta \in C_D$, given $D$ and the background information $I$, is:&#10;\begin{align*}&#10;P(\theta \in C_D | DI) &amp;amp;= \int_{C_D} p(\theta | DI) d\theta \\&#10;&amp;amp;= \int_{C_D} \frac{p(D | \theta I) p(\theta | I)}{p(D | I)} d\theta&#10;\end{align*}&#10;Notice that, unlike the frequentist interpretation, we have immediately demanded a distribution over $\theta$. The background information $I$ tells us, as before, that the sampling distribution is $s(d | \theta)$:&#10;\begin{align*}&#10;P(\theta \in C_D | DI) &amp;amp;= \int_{C_D} \frac{s(D | \theta) p(\theta | I)}{p(D | I)} d \theta \\&#10;&amp;amp;= \frac{\int_{C_D} s(D | \theta) p(\theta | I) d\theta}{p(D | I)} \\&#10;\text{i.e.} \quad\quad P(\theta \in C_D | DI) &amp;amp;= \frac{\int_{C_D} s(D | \theta) p(\theta | I) d\theta}{\int s(D | \theta) p(\theta | I) d\theta}&#10;\end{align*}&#10;Now this expression does not in general evaluate to $h$, which is to say, the $h$ confidence region $C_D$ does not always contain $\theta$ with probability $h$. In fact it can be starkly different from $h$. There are, however, many common situations in which it &lt;em&gt;does&lt;/em&gt; evaluate to $h$, which is why confidence regions are often consistent with our probabilistic intuitions.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, suppose that the prior joint PDF of $d$ and $\theta$ is symmetric in that $p_{d,\theta}(d,\theta | I) = p_{d,\theta}(\theta,d | I)$. (Clearly this involves an assumption that the PDF ranges over the same domain in $d$ and $\theta$.) Then, if the prior is $p(\theta | I) = f(\theta)$, we have $s(D | \theta) p(\theta | I) = s(D | \theta) f(\theta) = s(\theta | D) f(D)$. Hence&#10;\begin{align*}&#10;P(\theta \in C_D | DI) &amp;amp;= \frac{\int_{C_D} s(\theta | D) d\theta}{\int s(\theta | D) d\theta} \\&#10;\text{i.e.} \quad\quad P(\theta \in C_D | DI) &amp;amp;= \int_{C_D} s(\theta | D) d\theta&#10;\end{align*}&#10;From the definition of an HDR we know that for any $\psi \in \Theta$&#10;\begin{align*}&#10;\int_{H_\psi} s(d | \psi) dd &amp;amp;= h \\&#10;\text{and therefore that} \quad\quad \int_{H_D} s(d | D) dd &amp;amp;= h \\&#10;\text{or equivalently} \quad\quad \int_{H_D} s(\theta | D) d\theta &amp;amp;= h&#10;\end{align*}&#10;Therefore, given that $s(d | \theta) f(\theta) = s(\theta | d) f(d)$, $C_D = H_D$ implies $P(\theta \in C_D | DI) = h$. The antecedent satisfies&#10;$$&#10;C_D = H_D \longleftrightarrow \forall \psi \; [ \psi \in C_D \leftrightarrow \psi \in H_D ]&#10;$$&#10;Applying the equivalence near the top:&#10;$$&#10;C_D = H_D \longleftrightarrow \forall \psi \; [ D \in H_\psi \leftrightarrow \psi \in H_D ]&#10;$$&#10;Thus, the confidence region $C_D$ contains $\theta$ with probability $h$ if for all possible values $\psi$ of $\theta$, the $h$-HDR of $s(d | \psi)$ contains $D$ if and only if the $h$-HDR of $s(d | D)$ contains $\psi$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now the symmetric relation $D \in H_\psi \leftrightarrow \psi \in H_D$ is satisfied for all $\psi$ when $s(\psi + \delta | \psi) = s(D - \delta | D)$ for all $\delta$ that span the support of $s(d | D)$ and $s(d | \psi)$. We can therefore form the following argument:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;$s(d | \theta) f(\theta) = s(\theta | d) f(d)$ (premise)&lt;/li&gt;&#10;&lt;li&gt;$\forall \psi \; \forall \delta \; [ s(\psi + \delta | \psi) = s(D - \delta | D) ]$ (premise)&lt;/li&gt;&#10;&lt;li&gt;$\forall \psi \; \forall \delta \; [ s(\psi + \delta | \psi) = s(D - \delta | D) ] \longrightarrow \forall \psi \; [ D \in H_\psi \leftrightarrow \psi \in H_D ]$&lt;/li&gt;&#10;&lt;li&gt;$\therefore \quad \forall \psi \; [ D \in H_\psi \leftrightarrow \psi \in H_D ]$&lt;/li&gt;&#10;&lt;li&gt;$\forall \psi \; [ D \in H_\psi \leftrightarrow \psi \in H_D ] \longrightarrow C_D = H_D$&lt;/li&gt;&#10;&lt;li&gt;$\therefore \quad C_D = H_D$&lt;/li&gt;&#10;&lt;li&gt;$[s(d | \theta) f(\theta) = s(\theta | d) f(d) \wedge C_D = H_D] \longrightarrow P(\theta \in C_D | DI) = h$&lt;/li&gt;&#10;&lt;li&gt;$\therefore \quad P(\theta \in C_D | DI) = h$&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Let's apply the argument to a confidence interval on the mean of a 1-D normal distribution $(\mu, \sigma)$, given a sample mean $\bar{x}$ from $n$ measurements. We have $\theta = \mu$ and $d = \bar{x}$, so that the sampling distribution is&#10;$$&#10;s(d | \theta) = \frac{\sqrt{n}}{\sigma \sqrt{2 \pi}} e^{-\frac{n}{2 \sigma^2} { \left( d - \theta \right) }^2 }&#10;$$&#10;Suppose also that we know nothing about $\theta$ before taking the data (except that it's a location parameter) and therefore assign a uniform prior: $f(\theta) = k$. Clearly we now have $s(d | \theta) f(\theta) = s(\theta | d) f(d)$, so the first premise is satisfied. Let $s(d | \theta) = g\left( (d - \theta)^2 \right)$. (i.e. It can be written in that form.) Then&#10;\begin{gather*}&#10;s(\psi + \delta | \psi) = g \left( (\psi + \delta - \psi)^2   \right) = g(\delta^2) \\&#10;\text{and} \quad\quad s(D - \delta | D) = g \left( (D - \delta - D)^2   \right) = g(\delta^2) \\&#10;\text{so that} \quad\quad \forall \psi \; \forall \delta \; [s(\psi + \delta | \psi) = s(D - \delta | D)]&#10;\end{gather*}&#10;whereupon the second premise is satisfied. Both premises being true, the eight-point argument leads us to conclude that the probability that $\theta$ lies in the confidence interval $C_D$ is $h$!&lt;/p&gt;&#10;&#10;&lt;p&gt;We therefore have an amusing irony:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;The frequentist who assigns the $h$ confidence interval cannot say that $P(\theta \in C_D) = h$, no matter how innocently uniform $\theta$ looks before incorporating the data.&lt;/li&gt;&#10;&lt;li&gt;The Bayesian who would not assign an $h$ confidence interval in that way knows anyhow that $P(\theta \in C_D | DI) = h$.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;h2&gt;Final Remarks&lt;/h2&gt;&#10;&#10;&lt;p&gt;We have identified conditions (i.e. the two premises) under which the $h$ confidence region does indeed yield probability $h$ that $\theta \in C_D$. A frequentist will baulk at the first premise, because it involves a prior on $\theta$, and this sort of deal-breaker is inescapable on the route to a probability. But for a Bayesian, it is acceptable---nay, essential. These conditions are sufficient but not necessary, so there are many other circumstances under which the Bayesian $P(\theta \in C_D | DI)$ equals $h$. Equally though, there are many circumstances in which $P(\theta \in C_D | DI) \ne h$, especially when the prior information is significant.&lt;/p&gt;&#10;&#10;&lt;p&gt;We have applied a Bayesian analysis just as a consistent Bayesian would, &lt;em&gt;given the information at hand&lt;/em&gt;, including statistics $D$. But a Bayesian, if he possibly can, will apply his methods to the raw measurements instead---to the $\{x_i\}$, rather than $\bar{x}$. Oftentimes, collapsing the raw data into summary statistics $D$ destroys information in the data; and then the summary statistics are incapable of speaking as eloquently as the original data about the parameters $\theta$.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-09T17:28:27.597" Id="89363" LastActivityDate="2014-06-09T01:56:31.037" LastEditDate="2014-06-09T01:56:31.037" LastEditorUserId="40747" OwnerUserId="40747" ParentId="89099" PostTypeId="2" Score="8" />
  <row AnswerCount="1" Body="&lt;p&gt;There are several methods to make forecasts of equidistant time series (e.g. Holt-Winters, ARIMA, ...). However I am currently working on the following irregular spaced data set, which has a varying amount of data points per year and no regular time intervals between those points:&lt;/p&gt;&#10;&#10;&lt;p&gt;Plot:&#10;&lt;img src=&quot;http://i.imgur.com/TV671rN.png&quot; alt=&quot;plot&quot;&gt;&#10;Sample Data:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;structure(list(date = structure(c(664239600, 665449200, 666658800, &#10;670888800, 672184800, 673394400, 674517600, 675727200, 676936800, &#10;678146400, 679356000, 680565600, 682984800, 684194400, 685404000, &#10;686613600, 687823200, 689036400, 690246000, 691455600, 692665200, &#10;695084400, 696294000, 697503600, 698713200, 699922800, 701132400, &#10;703548000, 705967200, 707176800, 708472800, 709682400, 710805600, &#10;712015200, 713224800, 714434400, 715644000, 716853600, 718063200, &#10;719272800, 720486000, 721695600, 722905200, 724114800, 726534000, &#10;727743600, 728953200, 730162800, 732668400, 733788000, 734911200, &#10;737416800, 739144800, 741650400, 744069600, 746575200, 751413600, &#10;756169200, 761612400, 766533600, 771285600, 776124000, 780962400, &#10;785804400, 790642800, 795481200, 800316000, 805154400, 808869600, &#10;813708000, 818463600, 823302000, 828741600, 833580000, 838418400, &#10;843256800, 848098800, 853542000, 858380400, 863215200, 868053600, &#10;872892000, 875311200, 880153200, 884991600, 892291920, 897122048, &#10;901956780, 907055160, 912501900, 917083860, 919500720, 924354660, &#10;929104882, 934013100, 938851554, 948540840, 958809480, 963647580&#10;), class = c(&quot;POSIXct&quot;, &quot;POSIXt&quot;), tzone = &quot;&quot;), y = c(3.36153, &#10;-0.48246, 5.21884, 18.74093, 37.91793, 28.54938, 33.61709, 63.06235, &#10;68.65387, 77.23859, 87.11039, 84.03281, 93.62154, 99.91251, 100.50264, &#10;93.77179, 84.5999, 67.36365, 41.30507, 18.19424, 0.958, -15.81843, &#10;-14.5947, 5.63223, 6.98581, 4.49837, 12.14337, 26.38595, 38.18156, &#10;39.49169, 45.91298, 64.2627, 65.20289, 95.34555, 98.09912, 102.53325, &#10;101.76982, 95.17178, 93.00834, 81.43244, 59.84896, 44.55941, &#10;22.71526, 8.64943, 12.36012, -3.73631, -1.29231, -1.24887, 27.38948, &#10;33.22064, 28.50297, 39.53514, 52.27092, 64.83294, 79.8159, 107.36236, &#10;69.52707, 12.95026, 13.36662, 27.65264, 61.13918, 82.24249, 85.89012, &#10;13.9803, -11.97099, 8.03575, 55.61148, 93.62154, 107.10067, 88.11689, &#10;18.06141, -32.83151, 18.01798, 60.92196, 100.39437, 112.40503, &#10;54.1048, 2.59809, 31.10314, 56.46477, 58.4749, 124.68055, 100.5016, &#10;43.5316, -7.5386, 35.20915, 37.08925, 83.0716, 83.22325, 29.5081, &#10;-32.7452, -50.63345, 29.00605, 58.2997, 85.3864, 110.4178, -38.66195, &#10;16.16515, 71.64925)), .Names = c(&quot;date&quot;, &quot;y&quot;), row.names = c(NA, &#10;-99L), class = &quot;data.frame&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;My first thought was aggregating the data by calculating monthly averages. However this will lead to many months with missing values and secondly accuracy will be lost if multiple values within a month are replaced by a mean aggregat. To solve the first problem one could propose to calculate quarterly aggregates. But in this case the data sample would get relatively small.&lt;/p&gt;&#10;&#10;&lt;p&gt;So my question is how your approach would look like to make a forecast of the next data point for the given data set (if possible with R). Are there any best practices to handle the irregular spaced time series?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-09T22:10:11.730" FavoriteCount="1" Id="89386" LastActivityDate="2014-06-13T07:03:52.113" LastEditDate="2014-03-09T22:21:22.417" LastEditorUserId="35119" OwnerUserId="35119" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;time-series&gt;&lt;forecasting&gt;" Title="Forecasting irregular time series (with R)" ViewCount="354" />
  <row Body="&lt;p&gt;When fitting parameters, the distribution of the values of the test statistic tend to be much smaller than with a fully specified distribution (because fitted distributions match the data better than the corresponding population distributions, since you optimize for fit). &lt;/p&gt;&#10;&#10;&lt;p&gt;This leads to smaller significance levels which lowers the power curve. Your p-values correspondingly don't carry the meaning that they should (the conditional probability statements associated with the p-values don't apply).&lt;/p&gt;&#10;&#10;&lt;p&gt;There are modifications of the Kolmogorov-Smirnov test (search the site on &lt;em&gt;Lilliefors&lt;/em&gt;) which solve that problem, but the test is no longer distribution-free. You need a new table for testing each distribution. (Lilliefors did tables for normal and exponential cases with parameter estimation.)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-09T23:55:23.073" Id="89390" LastActivityDate="2014-03-10T00:16:02.573" LastEditDate="2014-03-10T00:16:02.573" LastEditorUserId="805" OwnerUserId="805" ParentId="89384" PostTypeId="2" Score="4" />
  <row AnswerCount="0" Body="&lt;p&gt;I have experimental 2D data $ (x_i,y_i)$ that is used to fit a polynomial.&#10;$$&#10;y = \sum_{k=0}^n a_k x^k&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I am using the GSL library, using the &lt;a href=&quot;http://www.gnu.org/software/gsl/manual/html_node/Least_002dSquares-Fitting.html&quot; rel=&quot;nofollow&quot;&gt;linear fitting API&lt;/a&gt;.&#10;It does usually well however in some situations, I get an inversion of the slope of the polynomial, and &lt;strong&gt;I know&lt;/strong&gt; that this cannot be.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am looking for a way to add some constraint to the fitting process: shortly, I want to state that the derivative of the polynomial is always positive.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am not a mathematician, so my questions are:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Is there some known mathematical tool to do this ?&lt;/li&gt;&#10;&lt;li&gt;practically, does GSL provides such algorithm ? (I mean, I can read the manual, but I don't really know where to look)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;: To clarify my needs, the data is normalized in a range so that $x \in [0:1]$. What happens with the polynomial outside of this range has no importance, as the data will always stay in this range.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-03-07T00:15:52.650" Id="89392" LastActivityDate="2014-03-10T21:25:42.493" LastEditDate="2014-03-10T21:25:42.493" LastEditorUserId="23990" OwnerDisplayName="kebs" OwnerUserId="23990" PostTypeId="1" Score="1" Tags="&lt;regression&gt;" Title="Imposing a constraint on derivative when fitting of a polynomial" ViewCount="96" />
  <row AnswerCount="1" Body="&lt;p&gt;Chapter 11 of &lt;a href=&quot;http://jsresearch.net/&quot; rel=&quot;nofollow&quot;&gt;Introduction to Data Science&lt;/a&gt; is about Poisson distributions.&lt;/p&gt;&#10;&#10;&lt;p&gt;One example sample has 58638 observations out of 100000 with a value less than or equal to 10.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; sum(rpois(100000, lambda=10)&amp;lt;=10)&#10;[1] 58638&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I think I can read that as &quot;a 0.58638 probability that values observed from this distribution are 10 or less&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm comfortable with the fact that this is close to the theoretical probability, but not exactly the same.&lt;/p&gt;&#10;&#10;&lt;p&gt;The book says R can show us how much variation there is around these probabilities using the R function &lt;code&gt;poisson.test&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; poisson.test(58638, 100000)&#10;&#10;    Exact Poisson test&#10;&#10;data:  58638 time base: 1e+05&#10;number of events = 58638, time base = 1e+05, p-value &amp;lt; 2.2e-16&#10;alternative hypothesis: true event rate is not equal to 1&#10;95 percent confidence interval:&#10; 0.5816434 0.5911456&#10;sample estimates:&#10;event rate &#10;   0.58638 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The book's explanation confuses me. I've highlighted the part I don't get.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;For 95% of the samples that we could generate using rpois(), using a sample size of 100,000, and a &lt;strong&gt;desired mean of 10&lt;/strong&gt;, we will get a result that lies between 0.5816434 and 0.5911456&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;It's confusing because I didn't tell the function that the desired mean is 10. How does it know? What if my desired mean is 3, like here?&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; sum(rpois(100000, lambda=3)&amp;lt;=10)&#10;[1] 99970&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;When I set lambda (the mean) to 3, clearly there are a lot more observation with a value less than 10. How is the test relevant here?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-10T00:50:58.193" Id="89394" LastActivityDate="2014-05-14T03:48:16.930" OwnerUserId="13989" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;poisson&gt;" Title="How to interpret output of poisson.test?" ViewCount="445" />
  <row AnswerCount="2" Body="&lt;p&gt;I am learning Predictive modeling and building a Forecasting model to predict Insurance sales in US as a part of my academic project. I want to do Time Series forecasting.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have Y(t) as my response variable and x(t),X(t-1),.....X(0) as my exploratory input varaibles that are correlated to the the response predicted variable. I have Y(t-1),Y(t-2),..... for close to 100 observations.&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to build a Forecasting model that uses both the Y(t),Y(t-1)..so on and their corresponding X(t),X(t-1) to build the forecasting model to predict the Y(t=1).....so on.&lt;/p&gt;&#10;&#10;&lt;p&gt;When I went through some of the documentation available online, I saw that usually we generate a Timeseries ID and give the resposne variable and the time series ID( which is a continuous variable starting from 1 to count of response variables) and this will not use any of the X(t) values. &lt;/p&gt;&#10;&#10;&lt;p&gt;Is there any way where I can use both X(t),Y(t) to predict Y(t+1).&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in Advance,&#10;Sai&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-10T04:15:07.507" Id="89406" LastActivityDate="2014-04-24T18:17:13.420" OwnerUserId="41620" PostTypeId="1" Score="0" Tags="&lt;time-series&gt;&lt;predictive-models&gt;" Title="Time Series Forecasting Method to use both Predicted and Predictor variables" ViewCount="68" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;In a representative sample of country population I have very few missing data, around 3%. But when I checked the missing data among communities, I found that one of them has almost 30% of data lost. That's consistent in all ages.&lt;/p&gt;&#10;&#10;&lt;p&gt;Should I try to impute the data, remove that community from the analysis, or keep all the communities but warn the reader about this possible bias?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-03-10T11:57:07.260" FavoriteCount="2" Id="89437" LastActivityDate="2014-03-12T09:16:28.423" LastEditDate="2014-03-12T09:16:28.423" LastEditorUserId="35205" OwnerUserId="35205" PostTypeId="1" Score="1" Tags="&lt;missing-data&gt;" Title="Missing data: what's the best approach?" ViewCount="53" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I would like to compare 3 different treatments (no one treatment is considered the placebo, and these are not randomized control trials) from multiple studies in a meta-analysis. I am using the software called &quot;Review Manager&quot; from the Cochrane society, and I am wondering if what I am doing is correct or not:&lt;/p&gt;&#10;&#10;&lt;p&gt;Say A, B, and C are the different treatments. I make the direct comparisons A vs. B, A vs. C, and B and C to obtain the odds ratios, confidence intervals, Z scores, p values, etc. Then I use the results from A vs. B and A vs. C to calculate the indirect comparison for B vs. C, obtaining the odds ratio and SE.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;A vs. B&lt;/strong&gt;&lt;img src=&quot;http://i.stack.imgur.com/SMd4O.png&quot; alt=&quot;A vs. B&quot;&gt;&#10;Ignore where it says risk ratio, the studies that I will be looking at are all retrospective.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;A vs. C&lt;/strong&gt;&lt;img src=&quot;http://i.stack.imgur.com/I3Ajr.png&quot; alt=&quot;A vs. B&quot;&gt; Ignore where it says risk ratio, the studies that I will be looking at are all retrospective.&lt;/p&gt;&#10;&#10;&lt;p&gt;I make another analysis with both direct and indirect B vs. C on Review Manager to obtain the odds ratio and statistics for this. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Direct and Indirect B vs. C&lt;/strong&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/jhz8W.png&quot; alt=&quot;Results of the analysis&quot;&gt; Ignore where it says risk ratio, the studies that I will be looking at are all retrospective.&lt;/p&gt;&#10;&#10;&lt;p&gt;Indirect B vs. C was determined using:&#10;&lt;img src=&quot;http://i.stack.imgur.com/2lFfF.png&quot; alt=&quot;Indirect calculations&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;L or l = log&lt;/p&gt;&#10;&#10;&lt;p&gt;Please tell me if I am doing this correctly, and whether if this is valid.&lt;/p&gt;&#10;&#10;&lt;p&gt;If so, how would I report the results from the analysis between the direct and indirect B vs. C? This would be telling me whether if the difference between B and C is significant? &lt;/p&gt;&#10;&#10;&lt;p&gt;How would I report the overall findings comparing the 3 different treatments?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for the help!&lt;/p&gt;&#10;" CommentCount="10" CreationDate="2014-03-10T14:12:32.820" FavoriteCount="1" Id="89460" LastActivityDate="2014-03-11T15:16:30.893" LastEditDate="2014-03-11T15:16:30.893" LastEditorUserId="26628" OwnerUserId="26628" PostTypeId="1" Score="0" Tags="&lt;meta-analysis&gt;" Title="How to do multiple treatment meta-analysis?" ViewCount="74" />
  <row AnswerCount="0" Body="&lt;p&gt;Let me begin by saying that I'm not entirely sure if this is the correct forum, or if Mathematics would be more suitable. The problem I'm about to describe is statistical in nature, so I suppose it fits here.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a system $m_I$ that is either in state 1, 0 or -1. In order to determine which state it is in, I measure a different (related) system $N$ times, and I count how many 'positive occurances' $k$ I get. The chance of a positive occurrence is $p_i$, so this is binomially distributed. Depending on the state $m_I$, the there is a different $p_i$, so in principle I should measure three binomial distributions. These three binomial distributions have two intersections, $k1$ and $k2$. In order to decide what state of $m_I$ I'm dealing with, I simply see what distribution it was most likely to belong to by seeing on what side of the values $k_1$ and $k_2$ my measurement is.&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to optimize the fidelity of being correct about $m_I$ which I call $F_{avg} = \frac{1}{3}(F_1+F_0+F_{-1})$, which is defined using the cumulative distribution function of the binomial distribution &lt;/p&gt;&#10;&#10;&lt;p&gt;$&#10;P(X \leq k) = \textrm{Bincdf}(k,n,p) = \sum_{i=1}^{k} \left (\begin{matrix}&#10;    n \\&#10;    i &#10;\end{matrix}\right )p^i(1-p)^{n-i}&#10;$&lt;/p&gt;&#10;&#10;&lt;p&gt;and &lt;/p&gt;&#10;&#10;&lt;p&gt;$&#10;F_{1} = 1 - P(X \leq k_2)&#10;$&lt;/p&gt;&#10;&#10;&lt;p&gt;$&#10;F_{0} = P(X \leq k_2) - P(X \leq k_1)&#10;$&lt;/p&gt;&#10;&#10;&lt;p&gt;$&#10;F_{-1} = P(X \leq k_1)&#10;$&lt;/p&gt;&#10;&#10;&lt;p&gt;The fidelities are thus simply the chance that a certain $k$ belongs to the interval of one of the three binomial distributions. Here the probabilities $p$ are fixed, but $k1,k2$ vary when I vary $n$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, $k_1$ and $k_2$ I can numerically calculate for a specific $N$, and I can thus also just evaluate $F_{avg}$ for various $N$. This shows, as is to be expected, that it converges to 1 very quickly as the distributions become more narrow for higher $N$. But here's where the problem comes in. What I have not included in this model is that every time I make a measurement (so I make $N$ in total), there is a chance $P$ that something goes wrong and that my approach is no longer valid. Basically, by measuring I have a chance of changing $m_I = 1$ into $m_I = 0$, and the other permutations. Now, for simplicity lets say that 1 can only go to 0, 0 to -1 and -1 to 1, all with the same chance P. &lt;/p&gt;&#10;&#10;&lt;p&gt;My problem is, how do I include this in my fidelities. What I first thought of was just simply multiplying each $F$ by a factor of $(1-P)^N$, which is the chance that the system has not changed in N measurements. But my feeling is that this is not correct, and that the approach is a little more intricate. Intuitively the notion of a convolution comes to mind, but I'm not sure how applicable that is?&lt;/p&gt;&#10;&#10;&lt;p&gt;I apologize if my story is vague. I've tried rewriting it about 4 times now, and this is as legible as I can currently make it, but that's also because I know the situation very well. So if there's any part that is particularly unclear, please let me know and I'll try to rephrase it!&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-03-10T15:01:47.610" Id="89471" LastActivityDate="2014-03-10T15:44:56.533" LastEditDate="2014-03-10T15:44:56.533" LastEditorUserId="41657" OwnerUserId="41657" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;mathematical-statistics&gt;" Title="Fidelity of measurement using conditional probabilities" ViewCount="14" />
  <row AnswerCount="0" Body="&lt;p&gt;I saw &lt;a href=&quot;http://moz.com/blog/why-the-inbound-marketing-funnel-is-essential-whiteboard-friday&quot; rel=&quot;nofollow&quot;&gt;this post&lt;/a&gt; on Moz which presented a segmented marketing funnel: &lt;img src=&quot;http://i.stack.imgur.com/t9Y4g.gif&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;This kind of thing would have quite a bit of value in my job. What I have no idea is how to visualize raw data to show a segmented funnel like this one. The idea is that sales leads come from different sources (which we use to segment the data by) and go through several stages by the time they convert to a deal. From each stage to another some drop off. The width of each slice is determined by the absolute number of leads in each. [&lt;strong&gt;EDIT&lt;/strong&gt;: Notice the image used here for reference is misleading when it comes to the numbers specified on the right of each slice. There appears to be no relationship between the width of the slice and the number. The image should only be taken as a reference to the design of the segmented funnel].&lt;/p&gt;&#10;&#10;&lt;p&gt;Anyway, any idea how to visualize it? If possible, I'd love to have a way to do so in Python.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's a &lt;a href=&quot;https://docs.google.com/spreadsheets/d/1hoFac7f1pSM-UG04UTwbrIU7lY7fXluuOmSsUXTdCx4/edit?usp=sharing&quot; rel=&quot;nofollow&quot;&gt;Google Doc&lt;/a&gt; with some dummy data if anybody needs some...&lt;/p&gt;&#10;&#10;&lt;p&gt;Looking forward to your insights.&#10;Thanks!&lt;/p&gt;&#10;" CommentCount="10" CreationDate="2014-03-10T15:15:34.830" Id="89475" LastActivityDate="2014-03-15T17:29:38.593" LastEditDate="2014-03-15T17:29:38.593" LastEditorUserId="41655" OwnerUserId="41655" PostTypeId="1" Score="2" Tags="&lt;data-visualization&gt;&lt;python&gt;&lt;funnel-plot&gt;" Title="How would you visualize a segmented funnel? (and could you do it with Python?)" ViewCount="188" />
  <row Body="&lt;p&gt;The conditionals of the posterior for the Dirichlet likelihood model are not of standard form so (as far as I know) you can not write a straightforward Gibbs sampler. You can either&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Use a different MCMC sampling procedure, such as Metropolis-Hastings or a slice sampler. &lt;/li&gt;&#10;&lt;li&gt;Use a different likelihood, such as a constrained Gaussian model where it is possible to write a Gibbs sampler.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2014-03-10T15:40:58.767" Id="89480" LastActivityDate="2014-03-10T15:40:58.767" OwnerUserId="41429" ParentId="88672" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;There might be a little confusion in your subscripts. The 'i' in y_i usually represent a line/individual. But here, the 'i' in e_i would rather code for the value on the horizontal axis. Your problem can be seen has a simple linear regression of one variable on the other (&lt;a href=&quot;http://en.wikipedia.org/wiki/Linear_regression&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Linear_regression&lt;/a&gt;). &lt;/p&gt;&#10;&#10;&lt;p&gt;So what you are looking is the e_i variables, each corresponding to all points with the same value on horizontal axis. Now, remember that the assumption in regression is often that errors will behave like gaussian distributions (It is the case for your simple linear regression). So you will never expect something like your second graph because constant variance just means that the error values will lie in approximately the same interval.  &lt;/p&gt;&#10;&#10;&lt;p&gt;The conclusion you want to make is that there is no heteroscedasticity (&lt;a href=&quot;http://en.wikipedia.org/wiki/Heteroscedasticity&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Heteroscedasticity&lt;/a&gt;). If there was, It would have been necessary to change your Normal hypothesis. &lt;/p&gt;&#10;&#10;&lt;p&gt;Hope that helps. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-03-10T16:31:15.543" Id="89487" LastActivityDate="2014-03-10T16:31:15.543" OwnerUserId="35572" ParentId="89478" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a dataset that features a binary outcome, a binary predictor, and an unordered factor with 7 levels, and 120 subjects. Each of the 120 subjects were asked a binary question on seven issues, hence the 7-level unordered factor. Therefore, I have a total of 840 observations. Each of the 840 observations has a binary predictor, and a binary outcome. I want to regress the binary outcome, on the binary predictor of course.&lt;/p&gt;&#10;&#10;&lt;p&gt;The outcome is slightly complicated. I will use mac vs. PCs as an example. People are asked, &quot;Do you think macs are better or do you think PCs are better?&quot; That is recorded as a 1, for preferring macs, and 0 for preferring PCs. Then they are asked &quot;Is your opinion based on objective truth, or subjective opinion?&quot; In other words, do you think it is objective (1) or subjective (0). Then the last question is &quot;In 20 years, will people think macs are better or will people think PCs are better?&quot; Again, 1 will be coded for macs. A new variable is then created. The variable is 1 if their current opinion matches what they think the future opinion will be. Previous pilot studies found that people tend to believe their stance on issues is what the future will be (...I know, a pretty intuitive finding...people tend to think they are right). The outcome is that variable, whether their current opinion agrees with what they think the future will be. So the point is to see if how people view things as subjective vs. objective, predicts whether they think their vision of the future is in line with their beliefs. The mac vs PC is only an example of an issue. In reality, there are seven issues, ranging from gun control, to abortion, etc., and each of those seven issue ranges in &quot;objectivity&quot; given by how the participants answered the objectivity question.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;code&gt;Participant ID&lt;/code&gt;: 120 Factors &lt;/li&gt;&#10;&lt;li&gt;&lt;code&gt;Objectivity&lt;/code&gt;: Binary predictor of 0s and 1s &lt;/li&gt;&#10;&lt;li&gt;&lt;code&gt;Outcome&lt;/code&gt;: Binary outcome of 0s and 1s &lt;/li&gt;&#10;&lt;li&gt;&lt;code&gt;Issue&lt;/code&gt;: 7-factor level variable &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;So I tried something like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;  # Dummy coding for all participants&#10;contrasts(data$Participant) &amp;lt;- contr.treatment(120)  #$&#10;  # Deviance coding to compare to the overall mean&#10;contrasts(data$Issue)       &amp;lt;- contr.sum(7) &#10;model &amp;lt;- glmer(Outcome ~ Objectivity*Issue + (Objectivity*Issue|Participant), &#10;               family=binomial, data=data)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This gives a bunch of warnings. I essentially want to see if my predictor predicts the outcome, based on the seven issue, while adding a random effect for the participant ID. &lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-03-10T18:37:11.860" Id="89510" LastActivityDate="2014-03-10T19:49:09.077" LastEditDate="2014-03-10T19:49:09.077" LastEditorUserId="41670" OwnerUserId="41670" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;logistic&gt;&lt;repeated-measures&gt;&lt;contrasts&gt;" Title="Repeated measures logistic regression" ViewCount="124" />
  
  <row Body="&lt;p&gt;Does your software give you a parameter covariance (or variance-covariance) matrix?  If so, the standard errors are the square root of the diagonal of that matrix.  You probably want to consult a textbook (or google for university lecture notes) for how to get the $V_\beta$ matrix for linear and generalized linear models.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-03-10T20:21:55.063" Id="89520" LastActivityDate="2014-03-10T20:21:55.063" OwnerUserId="17359" ParentId="89484" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;You should think of the algorithm as producing draws from a random variable, to show that the algorithm works, it suffices to show that the algorithm draws from the random variable you want it to.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $X$ and $Y$ be scalar random variables with pdfs $f_X$ and $f_Y$ respectively, where $Y$ is something we already know how to sample from. We can also know that we can bound $f_X$ by $Mf_Y$ where $M\ge1$.&lt;/p&gt;&#10;&#10;&lt;p&gt;We now form a new random variable $A$ where $A | y \sim \text{Bernoulli } \left (\frac{f_X(y)}{Mf_Y(y)}\right )$, this takes the value $1$ with probability $\frac{f_X(y)}{Mf_Y(y)} $ and $0$ otherwise. This represents the algorithm 'accepting' a draw from $Y$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now we run the algorithm and collect all the draws from $Y$ that are accepted, lets call this random variable $Z = Y|A=1$.&lt;/p&gt;&#10;&#10;&lt;p&gt;To show that $Z \equiv X$, for any event $E$, we must show that $P(Z \in E) =P(X \in E)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;So let's try that, first use Bayes' rule:&lt;/p&gt;&#10;&#10;&lt;p&gt;$P(Z \in E) = P(Y \in E | A =1) = \frac{P(Y \in E \&amp;amp; A=1)}{P(A=1)}$,&lt;/p&gt;&#10;&#10;&lt;p&gt;and the top part we write as&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{align*}P(Y \in E \&amp;amp; A=1) &amp;amp;= \int_E f_{Y, A}(y,1) \, dy \\ &amp;amp;= \int_E f_{A|Y}(1,y)f_Y(y) \, dy =\int_E f_Y(y) \frac{f_X(y)}{Mf_Y(y)} \, dy  =\frac{P(X \in E)}{M}.\end{align*}&lt;/p&gt;&#10;&#10;&lt;p&gt;And then the bottom part is simply&lt;/p&gt;&#10;&#10;&lt;p&gt;$P(A=1) = \int_{-\infty}^{\infty}f_{Y,A}(y,1) \, dy = \frac{1}{M}$,&lt;/p&gt;&#10;&#10;&lt;p&gt;by the same reasoning as above, setting $E=(-\infty, +\infty)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;And these combine to give $P(X \in E)$, which is what we wanted, $Z \equiv X$.&lt;/p&gt;&#10;&#10;&lt;p&gt;That is how the algorithm works, but at the end of your question you seem to be concerned about a more general idea, that is when does an empirical distribution converge to the distribution sampled from? This is a general phenomenon concerning any sampling whatsoever if I understand you correctly. &lt;/p&gt;&#10;&#10;&lt;p&gt;In this case, let $X_1, \dots, X_n$ be iid random variables all with distribution $\equiv X$. Then for any event $E$, $\frac{\sum_{i=1}^n1_{X_i \in E}}{n}$ has expectation $P(X \in E)$ by the linearity of expectation. &lt;/p&gt;&#10;&#10;&lt;p&gt;Furthermore, given suitable assumptions you could use the &lt;a href=&quot;http://en.wikipedia.org/wiki/Law_of_large_numbers#Strong_law&quot; rel=&quot;nofollow&quot;&gt;strong law of large numbers&lt;/a&gt; to show that the empirical probability converges almost surely to the true probability.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-03-10T22:46:37.990" Id="89532" LastActivityDate="2014-03-10T22:46:37.990" OwnerUserId="40752" ParentId="89509" PostTypeId="2" Score="3" />
  
  <row AnswerCount="0" Body="&lt;p&gt;EDIT:&lt;/p&gt;&#10;&#10;&lt;p&gt;As pointed, there is already a similar question &quot;buried&quot; inside a larger-scope one. I'll reproduce the relevant part here:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;If I have multiple recordings of the same routes are there any valid techniques to combine them to get closer to the real route?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;The relevant part of the chosen answer, in the context of my question, is this:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;With multiple recordings, you can create a 2D kernel smooth of each, sum the smooths, and subject that to a topographic analysis to look for a &quot;ridgeline.&quot; You won't get a single connected line, usually, but you can often patch the ridges together into a continuous path. People have used this method to average hurricane tracks, for example.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;I have a set of polylines in bidimensional space (X,Y) representing object trajectories. Every polyline in the set is a measured track along the same prescribed path, most of them (but not all) starting around the same point, and ending around the same other point.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to statistically calculate the idealized path (&quot;average polyline&quot;) from the set of polylines, in the same way I would manually manually trace it in a drawing program, except I would like this to happen without manual intervention.&lt;/p&gt;&#10;&#10;&lt;p&gt;A sample image of a part of the polyline set (cropped) is below. It seems to me that tere is an &quot;obvious&quot; place where I would draw the average line with the mouse, but I can't figure which algorithm I could use.&lt;/p&gt;&#10;&#10;&lt;p&gt;Important notes:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;I know I could use the nodes for calculations, but some segments might be arbitrarily long, thus not having nearby nodes that would help in the calculations. That is, I would like to consider the line segments themselves for the calculation, not just the nodes.&lt;/li&gt;&#10;&lt;li&gt;There might be parts of a polyline behaving like outliers (see image). I think that, given the majority of polylines is well behaved, this outlier part would easily be &quot;absorbed&quot; by the well-behaved ones nearby.&lt;/li&gt;&#10;&lt;li&gt;Although the original data is time-stamped, I believe no temporal relation should be necessary to infer the spatial result I am after, isn't it?&lt;/li&gt;&#10;&lt;li&gt;Yes, this is GPS data (although this is incidental to the problem formulation, which has been purposefully abstracted). &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/FoIjE.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" ClosedDate="2014-03-13T13:23:27.310" CommentCount="6" CreationDate="2014-03-11T00:26:08.007" Id="89536" LastActivityDate="2014-03-13T14:11:21.383" LastEditDate="2014-03-13T14:11:21.383" LastEditorUserId="16656" OwnerUserId="16656" PostTypeId="1" Score="1" Tags="&lt;feature-selection&gt;&lt;algorithms&gt;&lt;spatial&gt;&lt;computational-statistics&gt;" Title="How to extract &quot;average polyline&quot; from a set of polylines" ViewCount="19" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Phase space learning &lt;a href=&quot;http://cseweb.ucsd.edu/users/gary/pubs/nips94.pdf&quot; rel=&quot;nofollow&quot;&gt;Paper1&lt;/a&gt; and &lt;a href=&quot;http://gliamac.mit.edu/people/seung/papers/continuous.pdf&quot; rel=&quot;nofollow&quot;&gt;Paper2&lt;/a&gt; in neural network represents the input  in higher dimension in auto associative learning. So, the network functions as an auto-associative memory where dynamical attractors are fixed points, each corresponding to one of the patterns that we want to store in the network. If the network has 3 input nodes, the the number of outputs will also be 3. Thus, for a time instant, $t$, Input is a vector of 3 features or 3 nodes. &lt;/p&gt;&#10;&#10;&lt;p&gt;The Authors mention that a trajectory is traced out. In my case, for every time instant I have a feature vector of dimension 3 i.e there are three features. In my opinion, for every time step, t =1,2,...N, where N = number of examples, a learning algorithm is used to update the weights. Also. the Authors mention delay embedding for time series reconstruction which is an essential point in any phase space representation (Page 5 of Paper1). Based on these requirements, the following concepts are unclear and I will be thankful for an intuitive explanation. &lt;/p&gt;&#10;&#10;&lt;p&gt;Q1. How come a feature vector is delay embedded and a trajectory is formed since we have only one example at every time instant? &lt;/p&gt;&#10;&#10;&lt;p&gt;Q2. It is difficult to follow the training and learning as the details are not mentioned in the paper. I will appreciate an illustration so as to make the concept clear in how to apply phase space learning using any supervised algorithm where the objective function is the mean square error between the actual output of the network and the target. In my application, I only have the final target vector not a target vector for every time step. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-11T03:00:15.630" Id="89543" LastActivityDate="2014-04-10T11:01:49.137" LastEditDate="2014-03-11T03:05:21.413" LastEditorUserId="28545" OwnerUserId="28545" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;neural-networks&gt;" Title="Supervised learning based on phase space representation" ViewCount="42" />
  
  <row AnswerCount="0" Body="&lt;p&gt;So I have a bunch of scores 0-100 and a two sets of weights, the frequency which is the number of people in the sample with an average score among those people. Then I have another value that is the fraction of people in that group for the given score. An example may look like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&quot;Average Score of sample&quot;    &quot;Number of people in sample&quot;    &quot;Subset of people in sample&quot;&#10;65                           8                            .75&#10;87                           4                            1.0&#10;34                           1                            1.0&#10;50                           3                            .33333&#10;72                           9                            .66666&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Here is a link to a Google spreadsheet that provides three examples of the calculations in questions on a set N=100:&#10;&lt;a href=&quot;https://docs.google.com/spreadsheet/ccc?key=0Ai9OXRu1UgWCdC1mbmkxQnFSSDREY1hPRTZRWnVMaGc&amp;amp;usp=sharing&quot; rel=&quot;nofollow&quot; title=&quot;Google Spreadsheet Example&quot;&gt;Spreadsheet Example&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;So my question is, on a larger sample size, can I use the fraction as the weight(Column M) and still expect similar results to the Frequency * Fraction (Column N) weighting approach? How could I go about analyzing data to find a bias between the two approaches?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for your insight! If any additional information is needed please let me know!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-11T03:14:36.523" Id="89550" LastActivityDate="2014-03-11T03:14:36.523" OwnerUserId="41695" PostTypeId="1" Score="0" Tags="&lt;estimation&gt;&lt;measurement-error&gt;" Title="Weighted averages of a subset using fractions vs a frequency" ViewCount="14" />
  
  <row Body="&lt;p&gt;SVC's predict just uses its decision function, which is distance from the hyperplane.&lt;/p&gt;&#10;&#10;&lt;p&gt;According sklearn documentation, SVC's predict_proba does the following&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The probability model is created using cross validation, so the results can be slightly different than those obtained by predict. Also, it will produce meaningless results on very small datasets.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;according to their documentation &lt;a href=&quot;http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC.predict_proba&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Much more details &lt;a href=&quot;http://scikit-learn.org/stable/modules/svm.html#scores-probabilities&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;. You will have to read Wu et al (2004) paper, mentioned in that section to figure out how exactly they did it. I am not familiar with it.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-11T07:38:31.827" Id="89561" LastActivityDate="2014-03-11T07:38:31.827" OwnerUserId="20875" ParentId="85966" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to understand how to derive the confidence interval for unpaired samples when the population variance is unknown and we assume that they are equal:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$({\bar X} - {\bar Y}) \pm t_{\alpha,n_x+n_y-2}S_p \sqrt{\frac{1}{n_x}+\frac{1}{n_y}} \tag{*}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $S_p$ is the pooled standard deviation and the pooled variance, $S^2_p $ is:&#10;$$S^2_p=\frac{(n_x-1)S^2_x+(n_y-1)S^2_y}{n_x+n_y-2}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I managed to figure out the following:&#10;$$\frac{(n_x+n_y-2)S^2_p}{\sigma^2}=\frac{(n_x-1)S^2_x}{\sigma^2}+\frac{(n_y-1)S^2_y}{\sigma^2} \sim \chi^2_{n_x+n_y-2} \tag{1}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, I am supposed to use the following:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$P\left( \bar{X} - t_{\frac{a}{2},n-1} \frac{S}{\sqrt{n}} \le \mu \le  \bar{X} + t_{\frac{a}{2},n-1} \frac{S}{\sqrt{n}} \right) = 100(1-\alpha)\% \tag{2}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \frac{(\bar{X} - \bar{Y}) - (\mu_x - \mu_y)}{\sqrt{\frac{\sigma^2_x}{m}+\frac{\sigma^2_y}{n}}} \tag{3}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;To deduce $(*)$&lt;/p&gt;&#10;&#10;&lt;p&gt;Please show me the steps I need to do to deduce $(*)$ as my textbook omitted it.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-11T08:31:49.550" Id="89563" LastActivityDate="2014-03-11T08:31:49.550" OwnerUserId="31661" PostTypeId="1" Score="1" Tags="&lt;confidence-interval&gt;&lt;sampling&gt;" Title="Why use the t-distribution for confidence interval for difference of means for unpaired samples when the population variance is unknown?" ViewCount="21" />
  <row AnswerCount="1" Body="&lt;p&gt;Hi I am looking for a result (if it exists !!!) in the direction of Normal approximation for sum of correlated Bernoulli random variables (edit : with the same parameter $p$) where correlation between any pair of distinct Bernoulli variables entering into the summand is constant and equal to $0&amp;lt;\rho&amp;lt;1$.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have googled the question but with no success, if any has reference or result in this particular case I would be grateful.&lt;/p&gt;&#10;&#10;&lt;p&gt;Edit as suggested by Glen_b here is my results for mean and variance calculations :&lt;/p&gt;&#10;&#10;&lt;p&gt;$$E[\sum_{i=1}^{n}X_i]=n.p$$&#10;$$E[(\sum_{i=1}^{n}X_i)^2]-E[\sum_{i=1}^{n}X_i]^2=n.p.(1-p)+ n.(n-1)\rho.p.(1-p)$$&#10;$$=np.(1-p)(1+\rho(n-1))$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Best regards&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-11T09:08:04.497" FavoriteCount="1" Id="89565" LastActivityDate="2014-03-11T23:11:19.090" LastEditDate="2014-03-11T23:11:19.090" LastEditorUserId="26200" OwnerUserId="26200" PostTypeId="1" Score="4" Tags="&lt;correlation&gt;&lt;approximation&gt;&lt;asymptotics&gt;&lt;bernoulli-distribution&gt;" Title="Normal Approximation of the sum of correlated Bernoulli Random Variables" ViewCount="198" />
  
  
  
  <row Body="&lt;p&gt;I'll give you an extreme example where they are different.&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose I create my 95% confidence interval for a parameter $\theta $ as follows.  Start by sampling the data.  Then generate a random number between $0 $ and $1 $.  Call this number $ u $.  If $ u $ is less than $0.95 $ then return the interval $(-\infty,\infty) $.  Otherwise return the &quot;null&quot; interval.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now over continued repititions, 95% of the CIs will be &quot;all numbers&quot; and hence contain the true value.  The other 5% contain no values, hence have zero coverage.  Overall, this is a useless, but technically correct 95% CI.&lt;/p&gt;&#10;&#10;&lt;p&gt;The Bayesian credible interval will be either 100% or 0%.  Not 95%.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-03-11T14:22:52.090" Id="89609" LastActivityDate="2014-03-11T14:22:52.090" OwnerUserId="2392" ParentId="89099" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a set of survey data related to 20 survey questions. Each of these questions represent a variable (&lt;code&gt;Q1&lt;/code&gt;, &lt;code&gt;Q2&lt;/code&gt;,...&lt;code&gt;Q20&lt;/code&gt;). I created a new variable &lt;code&gt;QCom&lt;/code&gt; which measures the response of the survey, and is given by a composite score obtained as the sum of the scores of responses from &lt;code&gt;Q1&lt;/code&gt; to &lt;code&gt;Q20&lt;/code&gt;. I then perform a t-test for &lt;code&gt;QCom&lt;/code&gt; to check for evidence of a difference in mean due to another variable (&lt;code&gt;Sex&lt;/code&gt; -&gt; Male or Female). I obtained the test statistics from SPSS's independent sample t test.&lt;/p&gt;&#10;&#10;&lt;p&gt;Subsequently, I created another variable called &lt;code&gt;StCom&lt;/code&gt; (equal to standardized score of &lt;code&gt;QCom&lt;/code&gt;). Again, I repeated the t-test as the above using &lt;code&gt;StCom&lt;/code&gt; this time. The t test statistics that I obtained from SPSS was exactly the same as the first test using &lt;code&gt;QCom&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;In this case, I am not sure if this is normal or the z-scores transformation is incorrect. Can someone help to enlighten me why the composite scores and composite z-scores t test results are exactly the same? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-11T14:49:59.627" Id="89614" LastActivityDate="2014-03-11T15:46:22.073" LastEditDate="2014-03-11T15:46:22.073" LastEditorUserId="24808" OwnerUserId="41710" PostTypeId="1" Score="2" Tags="&lt;spss&gt;&lt;t-test&gt;&lt;standardization&gt;&lt;composite&gt;" Title="Composite Scores and Standardized Composite Scores t test" ViewCount="148" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I am trying to programatically select a prior distribution from the Gamma family of distributions. The primary criteria that I need to satisfy is that the median of the distribution should be a given value $X$ (i.e. such that it's equally probable to have a parameter value above $X$ or below $X$)&lt;/p&gt;&#10;&#10;&lt;p&gt;Additionally, because I'm using the distribution as a conjugate prior for a Poisson distribution's lambda, and I know how much relative weight I want to give to the prior relative to the subsequent observations, I &lt;em&gt;think&lt;/em&gt; I know what an appropriate $\beta$ parameter is (though I could be wrong on this point, feel free to make comments if I'm thinking about that incorrectly).&lt;/p&gt;&#10;&#10;&lt;p&gt;So essentially my problem becomes how to select an appropriate $\alpha$ such that the median is $X$. The way I've considered this seems to work well to get a good prior distribution when constraining the mean (or even mode), but technically, I should be constraining the median. Is this possible formulaicly, or would I need to resort to some kind of computational search to find the right value?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-03-11T15:18:56.620" FavoriteCount="1" Id="89617" LastActivityDate="2014-03-12T05:13:16.670" OwnerUserId="29597" PostTypeId="1" Score="2" Tags="&lt;prior&gt;&lt;gamma-distribution&gt;&lt;median&gt;" Title="Prior Gamma distribution: Select appropriate alpha given beta and median" ViewCount="184" />
  <row AcceptedAnswerId="90179" AnswerCount="1" Body="&lt;p&gt;Let $\theta &amp;gt; 0$ be some model parameter for which properties (bias, ...) of an estimate are studied via simulations. &lt;/p&gt;&#10;&#10;&lt;p&gt;For a given data set, an estimate $\hat{\theta}_i$ of $\theta$ can be obtained by maximising a likelihood function.&lt;/p&gt;&#10;&#10;&lt;p&gt;As $\theta$ is positive, however, I perform the maximisation over $\eta = \log(\theta) \in \mathbb{R}$ and I set $\hat{\theta} = \exp(\hat{\eta})$.&lt;/p&gt;&#10;&#10;&lt;p&gt;After running $N$ simulations, how should I compute the mean value,&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;$\bar{\hat{\theta}} = \frac{1}{N} \sum_{i=1}^N \hat{\theta}_i$, or&lt;/li&gt;&#10;&lt;li&gt;$\bar{\hat{\theta}} = \exp(\frac{1}{N} \sum_{i=1}^N \hat{\eta}_i)$?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;:  &lt;/p&gt;&#10;&#10;&lt;p&gt;-- Minus log likelihood for a sample from the Exp distribution with mean $\theta = \exp(\eta)$:  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;minusloglik &amp;lt;- function(eta, sample)&#10;{&#10;  theta &amp;lt;- exp(eta)&#10;  - sum(dexp(x=sample, rate=theta, log=TRUE))&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;-- True value of $\theta$:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;theta &amp;lt;- 5.73&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;-- Simulations:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;thetaHat &amp;lt;- etaHat &amp;lt;- rep(NA, 1000)&#10;for(i in 1:1000)&#10;{  &#10;  sample &amp;lt;- rexp(n=100, rate=theta)&#10;  etaHat[i] &amp;lt;- nlm(f=minusloglik, p=0, sample=sample)$estimate&#10;  thetaHat[i] &amp;lt;- exp(etaHat[i])&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt;: &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Should I summarise the results as &lt;code&gt;mean(thetaHat)&lt;/code&gt; or as &lt;code&gt;exp(mean(etaHat))&lt;/code&gt;?&lt;/li&gt;&#10;&lt;li&gt;Is the answer the same if $\theta$ denotes the variance of a normal distribution?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="4" CreationDate="2014-03-11T15:31:04.060" Id="89618" LastActivityDate="2014-03-16T10:27:27.893" LastEditDate="2014-03-13T06:26:50.783" LastEditorUserId="8451" OwnerUserId="7064" PostTypeId="1" Score="3" Tags="&lt;data-transformation&gt;&lt;maximum-likelihood&gt;&lt;mean&gt;&lt;simulation&gt;" Title="Summarising simulations on a transformed parameter" ViewCount="77" />
  
  
  
  <row Body="&lt;p&gt;As Peter pointed out, it is impossible to calculate coincidences after the fact.&lt;/p&gt;&#10;&#10;&lt;p&gt;Your question got me thinking, and I realized my girlfriend and I also have a strange birthday coincidence.  She was born exactly 432 days before me!  And we are also in a successful relationship!&lt;/p&gt;&#10;&#10;&lt;p&gt;I don't know what this probability is, but it is the exact same as yours!&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2014-03-11T20:41:30.547" Id="89657" LastActivityDate="2014-03-11T20:41:30.547" OwnerUserId="2310" ParentId="89635" PostTypeId="2" Score="36" />
  <row Body="&lt;p&gt;There are several issues here:  &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Your multiple logistic regression model includes other variables that may well improve the performance of the model even if they aren't 'significant', thus increasing the power of the test of $p_1$.  If you want to learn more about this phenomenon, it may help you to read this thread: &lt;a href=&quot;http://stats.stackexchange.com/q/28474/7290&quot;&gt;How can adding a 2nd IV make the 1st IV significant?&lt;/a&gt;  &lt;/li&gt;&#10;&lt;li&gt;The Mann-Whitney U-test isn't a single-variable analog of multiple logistic regression, simple logistic regression is.  &lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Reverse regression (i.e., regressing $X$ on $Y$) isn't the same thing as 'regular' regression (i.e., regressing $Y$ on $X$). &lt;/p&gt;&#10;&#10;&lt;p&gt;Based on these facts, if you wanted to see the relationship of $p_1$ in isolation, you should fit a simple logistic regression of $y$ on $p_1$.  &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;I suspect these data came from an observational study, not an experimental study.  As a result, causal conclusions are not valid.  That is, you cannot &quot;recommend addressing $p_1$ to increase the number of people who do $y$&quot;.  &lt;/p&gt;&#10;&#10;&lt;p&gt;In addition, that phrasing implies that you may think of $y$ as the actual predictor variable here.  If so, in combination with point 3 above, you probably should be fitting a multivariate model (such as a &lt;a href=&quot;http://en.wikipedia.org/wiki/Multivariate_analysis_of_variance&quot; rel=&quot;nofollow&quot;&gt;MANOVA&lt;/a&gt;) instead.  &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2014-03-11T21:31:39.713" Id="89664" LastActivityDate="2014-03-11T21:31:39.713" OwnerUserId="7290" ParentId="89599" PostTypeId="2" Score="3" />
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;My data has 3 major inputs: &lt;code&gt;BLDDAY&lt;/code&gt; (a factor), &lt;code&gt;BLDMNT&lt;/code&gt; (a factor), and &lt;code&gt;D_BLD_SER&lt;/code&gt; (days as an integer variable).  The output is whether input variable has any impact on failure.  My model is: &lt;code&gt;model = glm(FAILED~BLDDAY+BLDMNT+D_BLD_SER, family=&quot;binomial&quot;, data=data_list)&lt;/code&gt;.  (I used &lt;a href=&quot;http://www.ats.ucla.edu/stat/r/dae/logit.htm&quot; rel=&quot;nofollow&quot;&gt;UCLA's statistics help site's guide to logistic regression in R&lt;/a&gt; to build this model.)  &lt;/p&gt;&#10;&#10;&lt;p&gt;Output: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Deviance Residuals: &#10;    Min       1Q   Median       3Q      Max  &#10;-1.3282  -0.9123  -0.8128   1.4056   2.1053  &#10;&#10;Coefficients:&#10;                  Estimate Std. Error z value Pr(&amp;gt;|z|)    &#10;(Intercept)     -0.7672583  0.1317132  -5.825 5.70e-09 ***&#10;BLDDAYMonday    -0.1545646  0.0839380  -1.841  0.06556 .  &#10;BLDDAYSaturday  -0.1257976  0.2028259  -0.620  0.53511    &#10;BLDDAYSunday    -0.1183008  0.1868713  -0.633  0.52669    &#10;BLDDAYThursday  -0.2007452  0.0772653  -2.598  0.00937 ** &#10;BLDDAYTuesday    0.0480453  0.0758603   0.633  0.52651    &#10;BLDDAYWednesday -0.0358585  0.0760027  -0.472  0.63707    &#10;BLDMNTAug        0.3009445  0.1405545   2.141  0.03226 *  &#10;BLDMNTDec        0.5562170  0.1338467   4.156 3.24e-05 ***&#10;BLDMNTFeb        0.3334978  0.2133475   1.563  0.11801    &#10;BLDMNTJan        0.4076504  0.2277978   1.790  0.07353 .  &#10;BLDMNTJul        0.1306585  0.1415302   0.923  0.35591    &#10;BLDMNTJun       -0.0357361  0.1428105  -0.250  0.80241    &#10;BLDMNTMar        0.4570491  0.1949815   2.344  0.01907 *  &#10;BLDMNTMay       -0.2292620  0.1614577  -1.420  0.15562    &#10;BLDMNTNov        0.3060012  0.1334034   2.294  0.02180 *  &#10;BLDMNTOct        0.2390501  0.1341877   1.781  0.07484 .  &#10;BLDMNTSep        0.2481405  0.1384901   1.792  0.07317 .  &#10;D_BLD_SER       -0.0020960  0.0003367  -6.225 4.82e-10 ***&#10;&#10;(Dispersion parameter for binomial family taken to be 1)&#10;    Null deviance: 10288  on 8182  degrees of freedom&#10;Residual deviance: 10154  on 8164  degrees of freedom&#10;AIC: 10192&#10;Number of Fisher Scoring iterations: 4&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The ANOVA table is the following:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;anova(model, test=&quot;Chisq&quot;)&#10;Analysis of Deviance Table&#10;Model: binomial, link: logit&#10;Response: FAILED&#10;Terms added sequentially (first to last)&#10;&#10;          Df Deviance Resid. Df Resid. Dev  Pr(&amp;gt;Chi)    &#10;NULL                       8182      10288              &#10;BLDDAY     6   20.392      8176      10268  0.002357 ** &#10;BLDMNT    11   70.662      8165      10197 9.142e-11 ***&#10;D_BLD_SER  1   43.797      8164      10154 3.642e-11 ***&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;My questions are:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Although the p-values for all three components are less than 0.05, which can be considered as significant, the deviance reduced due to each component is less than 1% of the total deviance. &lt;strong&gt;Normally the interpretation of output like this is input parameter affects output and it's better to use this parameter then using noting.&lt;/strong&gt; But does it really make sense of taking this parameter as significant input?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The p-values for &lt;code&gt;BLDDAY&lt;/code&gt; and &lt;code&gt;BLDMNT&lt;/code&gt; given by &lt;code&gt;anova()&lt;/code&gt; is the overall p-value,  which is significant, but &lt;code&gt;summary()&lt;/code&gt; gives detailed impact of each factor level. If I consider the p-values for each factor overall &lt;code&gt;BLDDAY&lt;/code&gt; is significant but individually only &lt;code&gt;BLDDAYThursday&lt;/code&gt; is significant. I am bit confused not as whether to consider &lt;code&gt;BLDDAY&lt;/code&gt; as significant input, or Thursday only, or Thursday &amp;amp; Friday both.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2014-03-12T05:02:20.663" FavoriteCount="1" Id="89692" LastActivityDate="2014-03-13T18:01:27.167" LastEditDate="2014-03-13T18:01:27.167" LastEditorUserId="7290" OwnerUserId="41480" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;regression&gt;&lt;logistic&gt;&lt;statistical-significance&gt;&lt;categorical-data&gt;" Title="Interpretation of p-values &amp; reduction of model deviance for factors in anova() vs summary()" ViewCount="321" />
  <row AnswerCount="0" Body="&lt;p&gt;Im using &lt;code&gt;RandomForestClassifier&lt;/code&gt; for a probability prediction task. I have a featureset of around 50 features and two possible labels - &lt;code&gt;first team wins&lt;/code&gt; and &lt;code&gt;second team wins&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;The feature set contains features for both teams, and the way I built it, since I know which team won, was have 50% of the set labeled 1st team wins, and 50% labeled 2nd team wins - with the respective features placed in the correct place in the feature set - for each match in training data, which initially has the winning team as the first one, I swap the features per team and change the label to &lt;code&gt;second team wins&lt;/code&gt;, using a counter modulo 2.&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem i see is that if I change the counter to start from 1 or 0, it makes a huge change in the final predictions, meaning that the data-set is asymmetrical. To tackle this problem I tried to add every match twice in normal order where the label is &lt;code&gt;first team wins&lt;/code&gt; , and reversed with the label being &lt;code&gt;second team wins&lt;/code&gt;. The question is - how does this affect the behavior of the model? I see some negative effect after making this change, although not enough to be statistically significant. It does however increase the running time for building the feature set and fitting the model obviously.&lt;/p&gt;&#10;&#10;&lt;p&gt;Will randomizing the label and team order be a more solid approach? what are my options?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks,&lt;/p&gt;&#10;&#10;&lt;p&gt;WeaselFox&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-12T08:42:54.763" Id="89705" LastActivityDate="2014-03-12T08:42:54.763" OwnerUserId="31956" PostTypeId="1" Score="0" Tags="&lt;predictive-models&gt;&lt;python&gt;&lt;scikit-learn&gt;" Title="building a feature set for scikit learn" ViewCount="48" />
  <row AnswerCount="0" Body="&lt;p&gt;I am a programmer and trying to do some data analysis. Since I am very interested in statistics, and I have learned a &lt;strong&gt;lot&lt;/strong&gt; of programming languages, learning how to use professional packages such as MATLAB, R or numpy etc. will be a waste of time (because I might not be using these all the time).  I decide to write my own program in pascal so as to understand the algorithm better.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, the answers I seek are:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Is my understanding of KDE and Bagging (bootstrap aggregation) correct?&lt;/li&gt;&#10;&lt;li&gt;What are the correct way to programatically (i.e. not via MATLAB or R, or use any rule-of-thumb assuming normal distribution...) get the optimal bandwidth for KDE?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;My experiments can be described as:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Generate 100 random numbers obeying the &lt;a href=&quot;http://en.wikipedia.org/wiki/Irwin%E2%80%93Hall_distribution&quot; rel=&quot;nofollow&quot;&gt;Irwin Hall Distribution&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;Use &lt;code&gt;bootstrapping&lt;/code&gt; method (resample with replacement) to generate 8 bootstrap samples.&lt;/li&gt;&#10;&lt;li&gt;Do KDE for each of the bootstrap sample, and average them.&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;calculate &quot;MISE&quot; by generate 8 &lt;strong&gt;new&lt;/strong&gt; bootstrap samples. The way I do the calculation is:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;//res[0] is the average of KDE result.&#10;MISE := 0;&#10;for i := 1 to 8 do begin&#10;  b := bootstrap(data);&#10;  cv := kde(b, h);  //&amp;lt;-- h is the bandwidth&#10;  for j := 0 to Length(cv) - 1 do MISE += sqr(res[0][j] - cv[j]);&#10;  b := nil;&#10;end;&#10;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;The KDE result I got is pictured as (&lt;code&gt;h=1&lt;/code&gt;):&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/3uhPP.png&quot; alt=&quot;Kernel Density Estimation for an Irwin-Hall distribution&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Running &lt;code&gt;h&lt;/code&gt; from &lt;code&gt;0.1&lt;/code&gt; to &lt;code&gt;5&lt;/code&gt;, I got the following &quot;MISE Plot&quot;:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/00qAf.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;It seems that the larger the bandwidth, the smaller the MISE, which does not seem right.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is my understanding of bagging and &quot;MISE&quot; (or &quot;AMISE&quot; for that matter) correct? Does Bagging has any relation with &lt;strong&gt;Cross Validation&lt;/strong&gt; (which is like what I did)?&lt;/p&gt;&#10;&#10;&lt;p&gt;BTW, if in order to answer my question, you need to review my (simple) pascal code, please let me know, I will post it somewhere.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-12T09:31:12.963" Id="89708" LastActivityDate="2014-03-12T09:31:12.963" OwnerUserId="41765" PostTypeId="1" Score="0" Tags="&lt;estimation&gt;&lt;pdf&gt;&lt;kernel&gt;&lt;bagging&gt;" Title="How to find out optimal KDE bandwidth via Bootstrap Aggregation" ViewCount="57" />
  
  
  
  
  <row Body="&lt;p&gt;The Stata command will show error because the &lt;code&gt;glm&lt;/code&gt; statement used specifies a logistic regression with 1/0 as outcome. You should see an error message &quot;&lt;em&gt;note: PercentageFemale has noninteger values.&lt;/em&gt;&quot; However, your R command is correctly specified.&lt;/p&gt;&#10;&#10;&lt;p&gt;Since you didn't provide data, let's start with a replicable example in R:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(MASS)&#10;data(menarche)&#10;out &amp;lt;- glm(cbind(Menarche, Total-Menarche) ~ Age,&#10;           family=binomial(logit), data=menarche)&#10;summary(out)&#10;&#10;# Export into Stata data&#10;library(foreign)&#10;write.dta(menarche, &quot;c:\\temp\\menarche.dta&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And the outcome is:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;[PRINTOUT SNIPPED FOR SPACE]&#10;&#10;    Coefficients:&#10;             Estimate Std. Error z value Pr(&amp;gt;|z|)    &#10;(Intercept) -21.22639    0.77068  -27.54   &amp;lt;2e-16 ***&#10;Age           1.63197    0.05895   27.68   &amp;lt;2e-16 ***&#10;---&#10;&#10;[PRINTOUT SNIPPED FOR SPACE]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now, to replicate that in Stata. First let's get the error:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;gen pm = Menarche / Total&#10;glm pm Age, link(logit) family(binomial)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Results:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;. glm pm Age, link(logit) family(binomial)&#10;note: pm has noninteger values  &#10;&#10;[SNIPPED FOR SPACE]   &#10;                                                   AIC             =  .5990425&#10;Log likelihood   = -5.488031242                    BIC             = -73.81271&#10;&#10;------------------------------------------------------------------------------&#10;             |                 OIM&#10;          pm |      Coef.   Std. Err.      z    P&amp;gt;|z|     [95% Conf. Interval]&#10;-------------+----------------------------------------------------------------&#10;         Age |   1.608169   .6215021     2.59   0.010      .390047    2.826291&#10;       _cons |  -20.91168   8.111063    -2.58   0.010    -36.80907   -5.014291&#10;------------------------------------------------------------------------------&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The results do not agree. And also notice the error message on line 2.&lt;/p&gt;&#10;&#10;&lt;p&gt;To make Stata talk to the format of your data set, use &lt;code&gt;blogit&lt;/code&gt;. Notice that while in R, it's the number of yes and number of no; in Stata, it's the number of yes followed by the number of total:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;blogit Menarche Total Age&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Here are the results that go with R:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Logistic regression for grouped data              Number of obs   =       3918&#10;                                                  LR chi2(1)      =    3667.18&#10;                                                  Prob &amp;gt; chi2     =     0.0000&#10;Log likelihood = -819.65237                       Pseudo R2       =     0.6911&#10;&#10;------------------------------------------------------------------------------&#10;    _outcome |      Coef.   Std. Err.      z    P&amp;gt;|z|     [95% Conf. Interval]&#10;-------------+----------------------------------------------------------------&#10;         Age |   1.631968   .0589532    27.68   0.000     1.516422    1.747514&#10;       _cons |  -21.22639   .7706859   -27.54   0.000    -22.73691   -19.71588&#10;------------------------------------------------------------------------------&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2014-03-12T13:05:05.487" Id="89737" LastActivityDate="2014-03-12T13:10:37.160" LastEditDate="2014-03-12T13:10:37.160" LastEditorUserId="13047" OwnerUserId="13047" ParentId="89734" PostTypeId="2" Score="3" />
  <row AnswerCount="0" Body="&lt;p&gt;I was wondering if someone knows a R-package or function library for the topic of shrinkage for regression with ARMA errors.&lt;/p&gt;&#10;&#10;&lt;p&gt;Please let me know if you came across something related.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you!&lt;/p&gt;&#10;&#10;&lt;p&gt;Regards,&lt;br&gt;&#10;Patricia Tencaliec&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-12T14:28:17.977" Id="89749" LastActivityDate="2014-03-12T14:28:17.977" OwnerUserId="38452" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;regression&gt;&lt;time-series&gt;&lt;arma&gt;&lt;shrinkage&gt;" Title="Shrinkage estimation for regression with ARMA errors" ViewCount="31" />
  <row AcceptedAnswerId="90916" AnswerCount="1" Body="&lt;p&gt;I am using Python's &lt;a href=&quot;http://statsmodels.sourceforge.net/devel/generated/statsmodels.graphics.boxplots.beanplot.html&quot; rel=&quot;nofollow&quot;&gt;statsmodels&lt;/a&gt; module to plot a &lt;strong&gt;violin/bean plot&lt;/strong&gt; of some data. I get the error &lt;em&gt;&quot;LinAlgError: singular matrix&quot;&lt;/em&gt; in the &lt;strong&gt;KDE&lt;/strong&gt; calculation whenever a single violin plot is drawn from a list of &lt;strong&gt;repeated instances of the same number&lt;/strong&gt;.   Regular boxplots work just fine.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is this a bug or is there a reason behind this?&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;/home/me/myscript.py in plot_data_in_violinplot()&#10;   3435         plot_opts={'violin_fc':(0.8, 0.8, 0.8), 'cutoff':True, &#10;                         'bean_color':'#FF6F00', 'bean_mean_color':'#009D91',&#10;                         'bean_median_color':'b', 'bean_median_marker':'+'}&#10;-&amp;gt; 3436         sm.graphics.beanplot(data_to_boxplot, ax=ax, jitter=jitter, plot_opts=plot_opts)&#10;&#10;&#10;/usr/local/lib/python2.7/dist-packages/statsmodels/graphics/boxplots.py in beanplot(data, ax, labels, positions, side, jitter, plot_opts)&#10;    333     for pos_data, pos in zip(data, positions):&#10;    334         # Draw violins.&#10;--&amp;gt; 335         xvals, violin = _single_violin(ax, pos, pos_data, width, side, plot_opts)&#10;    336 &#10;    337         if jitter:&#10;&#10;/usr/local/lib/python2.7/dist-packages/statsmodels/graphics/boxplots.py in _single_violin(ax, pos, pos_data, width, side, plot_opts)&#10;    170     pos_data = np.asarray(pos_data)&#10;    171     # Kernel density estimate for data at this position.&#10;--&amp;gt; 172     kde = gaussian_kde(pos_data)&#10;    173 &#10;    174     # Create violin for pos, scaled to the available space.&#10;&#10;/usr/local/lib/python2.7/dist-packages/scipy/stats/kde.pyc in __init__(self, dataset, bw_method)&#10;    186 &#10;    187         self.d, self.n = self.dataset.shape&#10;--&amp;gt; 188         self.set_bandwidth(bw_method=bw_method)&#10;    189 &#10;    190     def evaluate(self, points):&#10;&#10;/usr/local/lib/python2.7/dist-packages/scipy/stats/kde.pyc in set_bandwidth(self, bw_method)&#10;    496             raise ValueError(msg)&#10;    497 &#10;--&amp;gt; 498         self._compute_covariance()&#10;    499 &#10;    500     def _compute_covariance(self):&#10;&#10;/usr/local/lib/python2.7/dist-packages/scipy/stats/kde.pyc in _compute_covariance(self)&#10;    507             self._data_covariance = atleast_2d(np.cov(self.dataset, rowvar=1,&#10;    508                                                bias=False))&#10;--&amp;gt; 509             self._data_inv_cov = linalg.inv(self._data_covariance)&#10;    510 &#10;    511         self.covariance = self._data_covariance * self.factor**2&#10;&#10;/usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.pyc in inv(a, overwrite_a, check_finite)&#10;    381         inv_a, info = getri(lu, piv, lwork=lwork, overwrite_lu=1)&#10;    382     if info &amp;gt; 0:&#10;--&amp;gt; 383         raise LinAlgError(&quot;singular matrix&quot;)&#10;    384     if info &amp;lt; 0:&#10;    385         raise ValueError('illegal value in %d-th argument of internal '&#10;LinAlgError: singular matrix&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-03-12T15:09:05.563" Id="89754" LastActivityDate="2014-03-21T18:19:05.513" OwnerUserId="12287" PostTypeId="1" Score="0" Tags="&lt;covariance&gt;&lt;boxplot&gt;&lt;kde&gt;" Title="statsmodels: error in kde on a list of repeated values" ViewCount="113" />
  
  
  
  
  <row Body="&lt;p&gt;You cannot guarantee to maintain perfectly balanced fold-sets, however, you can try and get the fold-sets as balanced as possible. Here is one solution:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# Function to assign a elements from a factor to n fold-sets&#10;# Returns a list (of length n) of the indices in the factor&#10;nfoldsets &amp;lt;- function(x, n) {&#10;  if (!is.numeric(n)) stop(&quot;n must be numeric&quot;)&#10;  x &amp;lt;- as.factor(x)&#10;&#10;  # list of indices for each level&#10;  types &amp;lt;- split(1:length(x), x)&#10;&#10;  # fn to create random assignments for a vector of length l into n groups&#10;  nblocks &amp;lt;- function(l, n) sample(rep(sample(1:n), (l+n)/n)[1:l])&#10;&#10;  # assign indices for each level to n groups&#10;  assignments &amp;lt;- lapply(types, function(x) split(x, nblocks(length(x), n)))&#10;&#10;  # merge assignments for same group from each type&#10;  out &amp;lt;- as.data.frame(do.call(rbind, assignments))&#10;  lapply(out, function(x) sample(unname(unlist(x))))&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Usage:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# create a vector of length 110, with ratio 1:10, a:b&#10;&amp;gt; x &amp;lt;- sample(c(rep(&quot;a&quot;, 10), rep(&quot;b&quot;, 100)))&#10;&#10;# assign elements to each of 5 fold-sets, keeping as balanced as possible&#10;&amp;gt; fs &amp;lt;- nfoldsets(x, n=5)&#10;&#10;# check assignments are balanced&#10;&amp;gt; sapply(fs, function(i) table(x[i]))&#10;   1  2  3  4  5&#10;a  2  2  2  2  2&#10;b 20 20 20 20 20&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-03-12T16:21:50.863" Id="89771" LastActivityDate="2014-03-12T16:26:53.397" LastEditDate="2014-03-12T16:26:53.397" LastEditorUserId="23770" OwnerUserId="23770" ParentId="89768" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm analyzing some data right now and I have several planned comparisons I want to check.  There is one continuous response variable (response) and 3 categorical factor variables (profile, drug, disease).  Each factor variable (profile, drug, disease) has 2 levels (1,2).&lt;/p&gt;&#10;&#10;&lt;p&gt;I performed an ANOVA in Stata with the following command:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;anova response i.profile##i.drug##i.disease&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This works fine.&lt;/p&gt;&#10;&#10;&lt;p&gt;When I want to test my specific conditions and try to use the test command, I enter:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;test i1.profile#i1.drug#i1.disease = i2.profile#i1.drug#i1.disease&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;It gives me this error:  &quot;Constraint 1 dropped&quot;, and doesn't give me any p-value as output.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, if instead I run:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;anova response i.profile#i.drug#i.disease&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and then do&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;test i1.profile#i1.drug#i1.disease = i2.profile#i1.drug#i1.disease&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I get the desired result.  Is this the correct way to do this?  Why is Stata dropping a constraint (and what constraint)?  &lt;/p&gt;&#10;&#10;&lt;p&gt;I don't care about making comparisons like 1.profile vs 2.profile, just among specific combinations of profile, drug, and disease.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance for any help.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-03-12T16:25:17.433" Id="89772" LastActivityDate="2014-03-12T16:48:39.150" LastEditDate="2014-03-12T16:48:39.150" LastEditorUserId="41426" OwnerUserId="41426" PostTypeId="1" Score="1" Tags="&lt;anova&gt;&lt;stata&gt;" Title="Three-way ANOVA with contrasts in Stata" ViewCount="94" />
  <row Body="&lt;p&gt;The chances for this to happen.... two people having their birthday on the same day as explained by the other posters is 1/365 * 1/30 to be conservative here with the age ranges. To be in a relationship, a successful one multiply by maybe 1/2 or 1/3?! &lt;/p&gt;&#10;&#10;&lt;p&gt;However, for you to be in a relationship, you first have to be here. For you to be here, your mom and dad needed to get together - how likely was that then? Then their parents, grandparents, great grandparents, predecessors, apes, fish, amoebas, rays of sun hitting the first predecessors to plants, back to the big bang going as it did and whatever was before it. If you consider all, then every atom in the universe had to be exactly the way it was for you to be there. &lt;/p&gt;&#10;&#10;&lt;p&gt;You could almost say it's a miracle you guys got together.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-12T17:34:36.157" Id="89781" LastActivityDate="2014-03-12T17:34:36.157" OwnerUserId="41805" ParentId="89635" PostTypeId="2" Score="2" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm not really familiar with Weka, I learned to use it by watching some tutorials so I am not 100% sure if my approach is the correct one.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have collected the Reuters21758 dataset and I use the documents prescribed in the ModApte split. To make it easy I load training and test instances in Weka (first all training instances followed by all test instances), perform preprocessing and during classification is specify a 75% split.&lt;/p&gt;&#10;&#10;&lt;p&gt;I was wondering if someone can have a look at my .arff file as well as the output of this classification to tell me whether or not I did something wrong. It can be found here: &lt;a href=&quot;https://www.dropbox.com/s/k78h5dse3hkm973/dataset.zip&quot; rel=&quot;nofollow&quot;&gt;https://www.dropbox.com/s/k78h5dse3hkm973/dataset.zip&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;For clarification, I represent each topic in the Reuters dataset in its binary form {0,1} and train a classifier separately for each topic. In the zip-file in the link is the .arff file and output file of the topic &lt;strong&gt;acq&lt;/strong&gt;.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-03-12T18:50:16.613" Id="89790" LastActivityDate="2014-03-12T18:50:16.613" OwnerUserId="41211" PostTypeId="1" Score="0" Tags="&lt;classification&gt;&lt;text-mining&gt;&lt;weka&gt;" Title="Reviewing classification approach in Weka on Reuters21578" ViewCount="141" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I compared the number of pregnant women receiving specific health service indicators (deliveries, antenatal attendances and malaria prophylaxis uptake) at 14 facilities during three specific time periods; pre, during and after a project intervention; using the pre-intervention data set as the reference in a Poisson regression model, I calculated the Incident rate ratios and their confidence intervals and p-values. I have now been asked to run a test statistic to show whether the data sets pre, during and after the intervention are different or not. Is showing the IRR not enough and is there another test statistic beyond what I have just done?&lt;/p&gt;&#10;&#10;&lt;p&gt;here is an example of the STATA read out that I obtained on running an analysis.&#10;My challenge is in interpreting the print out especially the LR chi2(2) and prob &gt; chi2 results. what do they mean?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ZEt18.png&quot; alt=&quot;STATA READ OUT&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-12T19:54:34.203" Id="89799" LastActivityDate="2014-03-14T09:32:57.387" LastEditDate="2014-03-14T09:32:57.387" LastEditorUserId="41816" OwnerUserId="41816" PostTypeId="1" Score="3" Tags="&lt;hypothesis-testing&gt;&lt;poisson-regression&gt;" Title="Is there a test statistic for poisson regression?" ViewCount="183" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;Is anyone aware of propensity score matching methods for when there are more than 2 treatment groups? I am working on a project with 4 treatment groups: &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;A&lt;/li&gt;&#10;&lt;li&gt;B&lt;/li&gt;&#10;&lt;li&gt;A and B&lt;/li&gt;&#10;&lt;li&gt;Neither A nor B&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Calculating propensity scores using multinomial logistic regression might work, but then I'd get multiple scores for each observation so I'm not sure how I'd match/analyze the matched data.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-12T21:24:08.737" Id="89807" LastActivityDate="2014-03-12T22:06:08.913" OwnerUserId="23945" PostTypeId="1" Score="1" Tags="&lt;logistic&gt;&lt;matching&gt;&lt;propensity-scores&gt;" Title="Propensity score matching with multiple treatments" ViewCount="151" />
  <row AcceptedAnswerId="92714" AnswerCount="1" Body="&lt;p&gt;Suppose one has an expression for the probability density of a random variable- how does one simulate for the particular random variable. I understand that this isn't an issue for probability density functions like Gaussian which have a MATLAB implementations(say wgn()).&#10;I have a probability density function which doesn't have a Matlab implementation for realization. What does one do ? Are there any particular numerical methods for say turning a rand() into a particular probability density function.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-03-12T22:48:37.033" Id="89822" LastActivityDate="2014-04-06T17:16:11.303" LastEditDate="2014-03-14T22:03:19.473" LastEditorUserId="41827" OwnerUserId="41827" PostTypeId="1" Score="4" Tags="&lt;matlab&gt;&lt;simulation&gt;&lt;random-variable&gt;" Title="Method for simulation for random variables with a particular probability density function" ViewCount="263" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm trying to construct an algorithm that randomly sorts an array according to a weighted score for each item.  In the most straightforward version of the algorithm, the output array can be constructed iteratively, where the next item added is chosen randomly according to the distribution of the weights of the remaining items.  To illustrate with an example, if we have some items and weights like this:  (code is PHP but that's not really important)&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;$items = array('a' =&amp;gt; 5, 'b' =&amp;gt; 4, 'c' =&amp;gt; 1);&#10;$output = weighted_random_sort($items);&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Then in \$output, 'a' should be first with probability 0.5, 'b' should be first with probability 0.4 and 'c' should be first with probability 0.1.  The overall distribution of $output should be:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;P('a', 'b', 'c') = (5/10)*(4/5)&lt;/li&gt;&#10;&lt;li&gt;P('a', 'c', 'b') = (5/10)*(1/5)&lt;/li&gt;&#10;&lt;li&gt;P('b', 'a', 'c') = (4/10)*(5/6)&lt;/li&gt;&#10;&lt;li&gt;P('b', 'c', 'a') = (4/10)*(1/6)&lt;/li&gt;&#10;&lt;li&gt;P('c', 'a', 'b') = (1/10)*(5/9)&lt;/li&gt;&#10;&lt;li&gt;P('c', 'b', 'a') = (1/10)*(4/9)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;This algorithm is easy to implement with the iterative algorithm I described above.  However I actually want to implement it in a different way, and here's where the statistics part of my question comes in.  I want to implement it by perturbing each weight with a random function, and then just doing a plain reverse-sort of the perturbed scores.  So basically, my question is, what kind of random function f(n) would allow me to do:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;$items = array('a' =&amp;gt; f(5), 'b' =&amp;gt; f(4), 'c' =&amp;gt; f(1));&#10;$output = arsort($items)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and allow \$output to have same probability distribution as the original algorithm?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-13T06:15:58.553" FavoriteCount="1" Id="89844" LastActivityDate="2014-03-13T06:15:58.553" OwnerUserId="41840" PostTypeId="1" Score="0" Tags="&lt;probability&gt;&lt;distributions&gt;&lt;algorithms&gt;" Title="Random sorting of an array of weighted scores" ViewCount="106" />
  <row Body="&lt;p&gt;&lt;em&gt;(This is possibly a comment rather than an answer but is long)&lt;/em&gt;      &lt;/p&gt;&#10;&#10;&lt;p&gt;To assume a fixed number of factors a-priori usually means to be on the side of testing/confirming hypotheses, for instance by a chi-square test, and so, by common use of language means to be not &quot;on the exploratory side&quot;.       &lt;/p&gt;&#10;&#10;&lt;p&gt;If we don't want to assume simply an error in the use of statistical terms - what could then be the &quot;exploratory&quot; part after the number of factors has been assumed (and we have a framework to which we can approximate by a least-square-error model)? The type of rotation? The orthogonality-assumption at all? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-13T07:38:51.320" Id="89848" LastActivityDate="2014-03-13T13:07:38.250" LastEditDate="2014-03-13T13:07:38.250" LastEditorUserId="21654" OwnerUserId="1818" ParentId="89293" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="89864" AnswerCount="3" Body="&lt;p&gt;I have a device which rotates on a stepper motor through a belt and pulley system. I would like to know both the positional accuracy and precision of this movement.&lt;/p&gt;&#10;&#10;&lt;p&gt;It is of my opinion that this should be determined by taking a large number of measurements to random angles to give a bell curve then using the offset of the mean from 0 error to define the positional accuracy and defining the precision of the movement as 3 sigma.&lt;/p&gt;&#10;&#10;&lt;p&gt;The rest of the office are of the opinion that accuracy is the range of errors when rotating to random points and precision is the range of errors when rotating to a determined point, away to a random point, then back to the determined point.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is this just me applying my past experience in manufacturing to a problem that doesnt require it or is there merit in my method and everyone else is just specifying range, which is not what they really want?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-03-13T10:13:16.150" Id="89862" LastActivityDate="2014-03-14T08:08:05.847" LastEditDate="2014-03-13T10:46:10.623" LastEditorUserId="41846" OwnerUserId="41846" PostTypeId="1" Score="2" Tags="&lt;measurement-error&gt;" Title="Defining precision in a mechanical movement" ViewCount="169" />
  <row Body="&lt;p&gt;People use the two terms more or less interchangeably in daily life, which is why this can be confusing. In fact, your office has it exactly backwards. Precision doesn't require an external reference, but accuracy does. &lt;/p&gt;&#10;&#10;&lt;p&gt;Let's start by thinking about a scale. Suppose we measure the mass of an object many times. If the balance reads 1.0001 grams, 1.0000 grams, 0.9999 grams, 1.0000 grams, 0.9999 grams, then we might say that is is very precise, because the values are tightly clustered together. This is true even if the object really weighs 10 grams! However, a scale that consistently reads 1.000 grams when measuring a 10 gram object is not very accurate.&lt;/p&gt;&#10;&#10;&lt;p&gt;Precision is entirely &quot;internal&quot;--it's a description of how reliable or repeatable something is. Accuracy, however, measures accuracy according to an external standard (say, a known weight).  There's an ISO proposal to call that definition of accuracy &quot;trueness&quot;, which might help with your confusion. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-03-13T10:31:03.790" Id="89864" LastActivityDate="2014-03-13T22:13:34.687" LastEditDate="2014-03-13T22:13:34.687" LastEditorUserId="7250" OwnerUserId="7250" ParentId="89862" PostTypeId="2" Score="4" />
  <row AnswerCount="2" Body="&lt;p&gt;I am new to this site. A greenhorn at designing surveys, got a data collection question.&lt;/p&gt;&#10;&#10;&lt;p&gt;I designed a question to gather what individual thinks with regards to doing X at different locations L1, L2, L3 ... etc. My options for each location L, laid out as a matrix, are as follows e.g.:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;I am morally obliged to do X&lt;/li&gt;&#10;&lt;li&gt;Everyone does X, I just follow.&lt;/li&gt;&#10;&lt;li&gt;I am required to do X.&lt;/li&gt;&#10;&lt;li&gt;I am expected to do X.&lt;/li&gt;&#10;&lt;li&gt;I feel I should do X.&lt;/li&gt;&#10;&lt;li&gt;I need not do X.&lt;/li&gt;&#10;&lt;li&gt;I should not do X.&lt;/li&gt;&#10;&lt;li&gt;I must not do X.&lt;/li&gt;&#10;&lt;li&gt;X should be done for me.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;One of the responder commented &quot;please learn how to use a Likert Scale properly&quot;. Since the survey is anonymous, I cannot seek him for more advice.&lt;/p&gt;&#10;&#10;&lt;p&gt;So I went on to read about Likert Scale, to see what defines a Likert Scale. Now, I can see clearly my matrix of radio buttons is not a Likert Scale. I then went on to find what other kinds of (named) scale are there in this subject, hoping to find one that may fit what I just did, but everywhere I go is another Likert Scale.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt;: Is Likert Scale the only way to collect data with a matrix of radio buttons? What other scales are there? If Likert is the only scale, how should I modify my question?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-13T14:53:22.517" Id="89891" LastActivityDate="2014-06-18T12:15:15.417" OwnerUserId="41854" PostTypeId="1" Score="0" Tags="&lt;likert&gt;" Title="What are some scales other than Likert Scale?" ViewCount="96" />
  
  
  <row AcceptedAnswerId="108482" AnswerCount="1" Body="&lt;p&gt;I'm working on a project looking at the relationship between exposure to several different drugs on the risk of preterm delivery in a cohort of pregnant women with a particular disease. I'm confident that I need to use a time-dependent variable in a Cox PH model for the drug exposures as women started the drugs at various points in pregnancy, and some were not exposed at all. So far, I've coded the drug variable in two ways: 1) as a categorical variable listing the drug types (they were never taken concurrently, but sometimes sequentially) with no drug exposure as the reference group, and 2) as a binomial variable for each drug with a 1 for exposed time intervals and a 0 otherwise.&lt;/p&gt;&#10;&#10;&lt;p&gt;My problem lies in interpreting the HRs. For the case where the variable is categorical, the HR are relative to the reference group. But for the second case, are they relative to all other exposure types (including no exposure), or are they still relative to no exposure as the presence of all of the other drug variables in the model deals with the effects of those separately? In other words, is the interpretation of the HR more or less the same between the two types of models, or is the second model different, and less interpretable?&lt;/p&gt;&#10;&#10;&lt;p&gt;Please forgive me if this is a basic question! I've scoured the intertubes and my reference library and can't seem to find quite this same problem addressed. If you know of a good source I should read, please do point it out. I'll say though that all of the time-dependent Cox PH resources I've found seem to deal with binary exposures...&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-13T16:30:01.557" Id="89910" LastActivityDate="2014-07-19T00:25:59.630" OwnerUserId="41869" PostTypeId="1" Score="2" Tags="&lt;categorical-data&gt;&lt;modeling&gt;&lt;cox-model&gt;&lt;time-varying-covariate&gt;" Title="how to code and interpret categorical time varying variable in Cox PH model" ViewCount="219" />
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;would support vector machine help? as far as i know, SVM only deals with continuous - variables as the predictors ...&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Binary variables are not a problem for SVM. Even specialized kernels exist for exactly such data (Hamming kernel, Tanimoto/Jaccard kernel), though I don't recommend using those if you're not intimately familiar with kernel methods. &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;would logistic regression apply here? as far as i know, the predictors in logistic regression also are continuous&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Logistic regression works with binary predictors. It is probably your best option.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;how to explain the relationships in these models (especially to clinicians).&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;If you use linear SVM it is fairly straightforward to explain what's going on. Logistic regression is a better option, though, since most clinicials actually know these models (and by &lt;em&gt;know&lt;/em&gt; I mean &lt;em&gt;have heard of&lt;/em&gt;).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-13T18:26:31.933" Id="89922" LastActivityDate="2014-03-13T18:26:31.933" OwnerUserId="25433" ParentId="89914" PostTypeId="2" Score="2" />
  <row AnswerCount="0" Body="&lt;p&gt;Say there is a hypothesis that A causes B (A -&gt; B), and some likelihood that the hypothesis is correct (AB1%).&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, an experiment is run that claims to find a correlation between A and B.&lt;/p&gt;&#10;&#10;&lt;p&gt;What I would like to know is whether my new probability (AB2%) is greater than, or less than, my previous probability (AB1%).&lt;/p&gt;&#10;&#10;&lt;p&gt;Common sense tells me that AB2% &lt;strong&gt;must&lt;/strong&gt; be greater than AB1% -- that there's no way that finding a correlation would make it less likely that A -&gt; B.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is my common sense correct?  I would like to account for the possibilities of sampling bias, huge experimental error, reporting bias, etc. -- everything except for falsified data.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, knowing that things like these can happen:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;extremely biased experiment in which any correlation is much more likely to be due to biased sampling than to A -&gt; B.&lt;/li&gt;&#10;&lt;li&gt;very noisy experimental design, where random correlations between noise is much more likely than observing any true correlation&lt;/li&gt;&#10;&lt;li&gt;totally random data, cherry-picked to find correlations&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;how would I incorporate my knowledge of those (an any other) factors into an estimate for AB2%?&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;tl,dr; &lt;/p&gt;&#10;&#10;&lt;p&gt;Is this statement true:  a correlation is found, therefore it's more likely that there's a real causal link between two variables than it was before the correlation was found?&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;I'm not a statistician -- please be gentle! :)&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-03-13T18:30:23.170" Id="89923" LastActivityDate="2014-03-13T18:30:23.170" OwnerUserId="41874" PostTypeId="1" Score="1" Tags="&lt;correlation&gt;&lt;experiment-design&gt;&lt;bias&gt;&lt;causal-inference&gt;" Title="Estimating the probability of causation based on finding a correlation, including experimental details" ViewCount="40" />
  <row AnswerCount="0" Body="&lt;p&gt;In many statistical problems, I see the following formulation for maximizing rewards: Assuming that my total reward $R$ is the sum of individual rewards $R$:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\underset{\textbf{x}}{\operatorname{argmax}}E[R_T] = \underset{\textbf{x}}{\operatorname{argmax}} \left(E[R_1] + E[R_2] + E[R_3]\right)$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $R_i = r_i \cdot x_i$ is the reward on every bucket $i$, assuming a reward rate of $\alpha_i$ and a number of actions per bucket $x_i$&lt;/p&gt;&#10;&#10;&lt;p&gt;I assume that maximizing expectations is not the same as maximizing the mode of the distribution of $R_t$. Is that correct? &lt;/p&gt;&#10;&#10;&lt;p&gt;For example, in graphical models, it is common to run &lt;code&gt;max-sum&lt;/code&gt; algorithms to find the values that correspond to the &lt;strong&gt;MAP&lt;/strong&gt; of the distribution (i.e. mode maximization).&lt;/p&gt;&#10;&#10;&lt;p&gt;If so, &lt;strong&gt;when does it make sense to maximize the mode (i.e the probability of $R_t$) vs the expected value (i.e. $E[R_t]$), and viceversa?&lt;/strong&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-13T18:34:53.170" Id="89925" LastActivityDate="2014-03-13T18:53:12.310" LastEditDate="2014-03-13T18:53:12.310" LastEditorUserId="2798" OwnerUserId="2798" PostTypeId="1" Score="0" Tags="&lt;bayesian&gt;&lt;optimization&gt;&lt;expected-value&gt;&lt;graphical-model&gt;" Title="Maximizing expectations vs Mode maximization" ViewCount="27" />
  
  
  
  <row Body="&lt;p&gt;The above solution is not very good if X is sparse. Because taking !X will make a dense matrix, taking huge amount of memory and computation.&lt;/p&gt;&#10;&#10;&lt;p&gt;A better solution is to use formula &lt;strong&gt;Jaccard[i,j] = #common / (#i + #j - #common)&lt;/strong&gt;. With sparse matrixes you can do it as follows (note the code also works for non-sparse matrices):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(Matrix)&#10;jaccard &amp;lt;- function(m) {&#10;    ## common values:&#10;    A = tcrossprod(m)&#10;    ## indexes for non-zero common values&#10;    im = which(A &amp;gt; 0, arr.ind=TRUE)&#10;    ## counts for each row&#10;    b = rowSums(m)&#10;&#10;    ## only non-zero values of common&#10;    Aim = A[im]&#10;&#10;    ## Jacard formula: #common / (#i + #j - #common)&#10;    J = sparseMatrix(&#10;          i = im[,1],&#10;          j = im[,2],&#10;          x = Aim / (b[im[,1]] + b[im[,2]] - Aim),&#10;          dims = dim(A)&#10;    )&#10;&#10;    return( J )&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-03-13T23:12:02.380" Id="89947" LastActivityDate="2014-09-08T15:57:43.897" LastEditDate="2014-09-08T15:57:43.897" LastEditorUserId="-1" OwnerUserId="41844" ParentId="49453" PostTypeId="2" Score="5" />
  <row AnswerCount="0" Body="&lt;p&gt;I am having trouble to understand the min {X,Y} where X and Y are independent random variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;From the online source, it says that  min{X,Y} &amp;lt; x if and only if X&gt;x, Y&gt;y.&lt;/p&gt;&#10;&#10;&lt;p&gt;First of all, what exactly do you mean by the  term &quot;minimum &quot; of the two random variables? Does it refer to the domains that X and Y can take, and you the minimum is the smallest value in that domain or what? &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-03-14T00:26:25.650" Id="89956" LastActivityDate="2014-03-14T00:38:21.140" LastEditDate="2014-03-14T00:38:21.140" LastEditorUserId="2970" OwnerUserId="41890" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;random-variable&gt;" Title="What is the minimum of two independent variables?" ViewCount="51" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I've been working with JMP, and I have found that often I get a significant p value but a relatively small r. Is this supposed to happen?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-03-14T01:40:36.517" Id="89962" LastActivityDate="2014-03-14T03:22:56.617" LastEditDate="2014-03-14T02:14:44.667" LastEditorUserId="32036" OwnerUserId="41892" PostTypeId="1" Score="1" Tags="&lt;r-squared&gt;&lt;f-test&gt;&lt;jmp&gt;" Title="Is it possible to have an F-test with a $p&lt;.05$ even if there is an $R^2&lt;.1$ for a least squares analysis in JMP?" ViewCount="29" />
  <row Body="&lt;p&gt;Thanks for making the data accessible and for an interesting dataset and graphical challenge. &lt;/p&gt;&#10;&#10;&lt;p&gt;My main suggestion is of a (Cleveland) dot chart. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Cjp07.png&quot; alt=&quot;enter image description here&quot;&gt;  &lt;/p&gt;&#10;&#10;&lt;p&gt;The most important details I would like to emphasise: &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Superimposition here allows and eases comparison. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The order of topics in your displays appears quite arbitrary. Absent a natural order (e.g. time, space, an ordered variable) I would always sort on one of the variables to provide a framework. Which to use could be a matter of whether one is particularly interesting or important, a researcher's decision. Another possibility is to order on some measure of the differences between papers, so that topics receiving similar coverage were at one end and those receiving different coverage at the other end. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Open markers or point symbols allow overlap or identity to be resolved better than closed or solid markers or symbols, which in the worst cases obscure or occlude each other. (An alternative that might work quite well here is letters such as A, D and I for the three newspapers.) &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;There is clearly much scope for improving my design. For example, is the lettering too large and/or too heavy? On the other hand, the headings must be easily readable, or else the graph is a failure. &lt;/p&gt;&#10;&#10;&lt;p&gt;Some smaller, pickier points: &lt;/p&gt;&#10;&#10;&lt;p&gt;a. Red and green on your graph is a colour combination to be avoided. When different markers are used, colour choices are a little less crucial. &lt;/p&gt;&#10;&#10;&lt;p&gt;b. The horizontal ticks on your graph are distracting. In contrast, grid lines on mine are needed, but I try to make them unobtrusive by using thin, light lines. &lt;/p&gt;&#10;&#10;&lt;p&gt;c. Your graph shows percents and the total is about 20 $\times$ 0.1% or 2%, so 98% of the papers is something else? I used the proportions directly in the .csv provided. &lt;/p&gt;&#10;&#10;&lt;p&gt;Cleveland dot charts owe most to &lt;/p&gt;&#10;&#10;&lt;p&gt;Cleveland, W.S. 1984. Graphical methods for data presentation: full scale breaks, dot charts, and multibased logging. &lt;em&gt;American Statistician&lt;/em&gt; 38: 270-80.&lt;/p&gt;&#10;&#10;&lt;p&gt;Cleveland, W.S. 1985. &lt;em&gt;Elements of graphing data.&lt;/em&gt;  Monterey, CA: Wadsworth.&lt;/p&gt;&#10;&#10;&lt;p&gt;Cleveland, W.S. 1994. &lt;em&gt;Elements of graphing data.&lt;/em&gt;  Summit, NJ: Hobart Press.&lt;/p&gt;&#10;&#10;&lt;p&gt;One precursor (more famous statistically for quite different work!!!) was &lt;/p&gt;&#10;&#10;&lt;p&gt;Pearson, E.S. 1956.  Some aspects of the geometry of statistics: the use of visual presentation in understanding the theory and application of mathematical statistics. &lt;em&gt;Journal of the Royal Statistical Society&lt;/em&gt; A 119: 125-146.&lt;/p&gt;&#10;&#10;&lt;p&gt;For those interested, the graph was prepared in Stata after reading in the .csv with code &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;graph dot (asis) prop , over(pub) over(label, sort(1)) asyvars &#10;marker(1, ms(Oh)) marker(2, ms(+)) marker(3, ms(Th)) linetype(line)   &#10;lines(lc(gs12) lw(vthin)) scheme(s1color) &#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="5" CreationDate="2014-03-14T02:19:21.750" Id="89968" LastActivityDate="2014-03-14T02:24:34.840" LastEditDate="2014-03-14T02:24:34.840" LastEditorUserId="22047" OwnerUserId="22047" ParentId="89941" PostTypeId="2" Score="16" />
  
  <row Body="&lt;p&gt;The dot plot from Nick Cox is probably best for the complete picture. If you really want to emphasize the first versus second relationship, here's a modification to your chart that offsets the difference bar with the length of the second bar.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/EHBaY.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;And for a different big picture view, you can try something like a slope chart or parallel coordinates plot. The lines may be a bit too crowded here, but it may work if you want to highlight on a subset of the topics.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/wCiZf.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, you might try &lt;a href=&quot;http://helpmeviz.com/&quot;&gt;helpmeviz.com&lt;/a&gt; which is geared towards very specific data viz questions like this.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-03-14T02:32:57.317" Id="89969" LastActivityDate="2014-03-14T02:32:57.317" OwnerUserId="1191" ParentId="89941" PostTypeId="2" Score="12" />
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;How do I prove that $e^Z$ has a mean of $e^{0.5}$? &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;You can do it via integration, it's quite straightforward - you combine the $\exp$ terms into one exponent, complete the square and identify the density (which integrates to 1) leaving just a constant out the front.&lt;/p&gt;&#10;&#10;&lt;p&gt;An alternative is to do it via the MGF, which makes it trivial.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Also what is the pdf of $\exp(Z)$?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;You just use straight up change of variable, keeping in mind the Jacobian. Or you can do it from first principles (Let $Y=\exp(Z)$ then $P(Y\leq y) = P(\exp(Z)\leq y) = ...$)&lt;/p&gt;&#10;&#10;&lt;p&gt;You get a particular case of the &lt;a href=&quot;http://en.wikipedia.org/wiki/Log-normal_distribution&quot;&gt;lognormal density&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-14T02:55:06.300" Id="89972" LastActivityDate="2014-03-14T02:55:06.300" OwnerUserId="805" ParentId="89970" PostTypeId="2" Score="5" />
  
  <row AcceptedAnswerId="90007" AnswerCount="1" Body="&lt;p&gt;I have a mean time to failure variable that is Weibull-distributed with shape parameter less than $1$. Whenever a failure occurs, corrective action of cost $c_c$ is performed. From time to time, preventive maintenance of cost $c_m$ is performed, which resets the time in the Weibull distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is, how do I write an annual cost equation using this?&lt;/p&gt;&#10;&#10;&lt;p&gt;This is what I have so far. My expected number of failures in a year is $\frac{365}{\mu}$, where $\mu$ is the mean of the Weibull. Therefore, the cost for corrective action is:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$c_c * \frac{365}{\mu}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;When I add in the preventive maintenance, it becomes something like:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$c_c * \frac{365}{\mu} + c_m * f_m$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Where $f_m$ is the frequency of doing preventive maintenance in a year.&lt;/p&gt;&#10;&#10;&lt;p&gt;But the problem is the frequency of preventive maintenance affects the number of times corrective action is done (in this case it increases it). So a more correct formula would be:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$c_c * fcn(f_m) + c_m * f_m$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Where $fcn(f_m) = \frac{365}{\mu} + something$, so for every increase in $f_m$, the function value increases. Obviously, $something$ should have $f_m$ in there somewhere, but I don't know where. The idea is that every increase in frequency of preventive maintenance increases the expected number of times corrective maintenance has to be done, since the function is being reset to time zero and there is a large infant mortality in this distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;I tried thinking about it in terms of just probability, but it's difficult. Say preventive maintenance is done some time $t_p$ after corrective maintenance. Then the probability it will have broken down in that time is $Pr(t&amp;lt;T=t_p)$ and the probability for the Weibull time to failure resets to time zero.&lt;/p&gt;&#10;&#10;&lt;p&gt;But then I'm stuck, because how can I model the corrective actions in probability terms? Plus, I don't know how to convert that probability into frequency. I dont think it's as simple as $f = \frac{365}{t_p}$ because preventive maintenance happens on fixed intervals with respect to the previous preventive maintenance, not with respect to the previous corrective action, so $t_p$ always varies. And since the occurrence of one preventive maintenance resets the time to failure function, then all corrective actions are shifted.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any help or advice would be appreciated!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-03-14T07:05:14.920" Id="89985" LastActivityDate="2014-03-25T17:11:00.723" LastEditDate="2014-03-14T07:15:56.007" LastEditorUserId="41903" OwnerUserId="41903" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;distributions&gt;&lt;weibull&gt;" Title="How to write this cost equation?" ViewCount="127" />
  
  <row AnswerCount="0" Body="&lt;p&gt;We know if Gaussian Mixture Model is a probablistic counterpart of k-means algorithm. Is there a probablistic counterpart for decision-trees?&lt;/p&gt;&#10;&#10;&lt;p&gt;UPDATE: I know that branching in DT can be probablistic. But what I meant, is the way we create separated regions in the input space by each branching (as apposed to soft clustering of the input space). &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-03-14T09:22:04.437" Id="89993" LastActivityDate="2014-03-14T10:36:37.837" LastEditDate="2014-03-14T10:36:37.837" LastEditorUserId="17812" OwnerUserId="17812" PostTypeId="1" Score="0" Tags="&lt;bayesian&gt;&lt;classification&gt;&lt;classification-tree&gt;" Title="Probablistic counterpart for decision trees" ViewCount="28" />
  <row AnswerCount="1" Body="&lt;p&gt;There is an example on how to run a GLM for proportion data in Stata here: &lt;a href=&quot;http://www.ats.ucla.edu/stat/stata/faq/proportion.htm&quot; rel=&quot;nofollow&quot;&gt;http://www.ats.ucla.edu/stat/stata/faq/proportion.htm&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The IV is the proportion of students receiving free or reduced priced meals at school. The stata model looks like this.: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;glm meals yr_rnd parented api99, link(logit) family(binomial) robust nolog&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I'm interested in learning how to replicate this results in R (ideally using the same robust approach). Lets imagine that I have data about the number of students receiving free meals (Successes) and the rest of the students (Failures). I'm guessing the model in R could look something like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;fitglm &amp;lt;- glm(cbind(Successes,Failures) ~ yr_rnd + parented + api99, family=binomial)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Also, it was pointed out to me elsewhere (Penguin_Knight) that the error message &lt;code&gt;&quot;meals has non-integer values&quot;&lt;/code&gt; could be bad. I'm clueless regarding this error...&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-03-14T10:37:20.723" Id="89999" LastActivityDate="2014-03-14T11:25:40.783" LastEditDate="2014-03-14T10:55:47.397" LastEditorUserId="21054" OwnerUserId="41784" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;generalized-linear-model&gt;&lt;stata&gt;&lt;proportion&gt;" Title="How to replicate Stata's robust glm for proportion data in R?" ViewCount="207" />
  <row Body="&lt;p&gt;Using the R package &lt;a href=&quot;http://cran.r-project.org/web/packages/sandwich/index.html&quot; rel=&quot;nofollow&quot;&gt;&lt;code&gt;sandwich&lt;/code&gt;&lt;/a&gt;, you can replicate the results like that (I assume that you've already downloaded the dataset):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#-----------------------------------------------------------------------------&#10;# Load the required packages&#10;#-----------------------------------------------------------------------------&#10;&#10;require(foreign)&#10;require(sandwich)&#10;&#10;#-----------------------------------------------------------------------------&#10;# Load the data&#10;#-----------------------------------------------------------------------------&#10;&#10;dat &amp;lt;- read.dta(&quot;MyPath/proportion.dta&quot;)&#10;&#10;#-----------------------------------------------------------------------------&#10;# Inspect dataset&#10;#-----------------------------------------------------------------------------&#10;&#10;str(dat)&#10;&#10;#-----------------------------------------------------------------------------&#10;# Fit the glm&#10;#-----------------------------------------------------------------------------&#10;&#10;fitglm &amp;lt;- glm(meals ~ yr_rnd + parented + api99, family = binomial(logit), data = dat)&#10;&#10;#-----------------------------------------------------------------------------&#10;# Output of the model&#10;#-----------------------------------------------------------------------------&#10;&#10;summary(fitglm)&#10;&#10;#-----------------------------------------------------------------------------&#10;# Calculate robust standard errors&#10;#-----------------------------------------------------------------------------&#10;&#10;cov.m1 &amp;lt;- vcovHC(fitglm, type = &quot;HC0&quot;)&#10;&#10;std.err &amp;lt;- sqrt(diag(cov.m1))&#10;&#10;q.val &amp;lt;- qnorm(0.975)&#10;&#10;r.est &amp;lt;- cbind(&#10;  Estimate = coef(fitglm)&#10;  , &quot;Robust SE&quot; = std.err&#10;  , z = (coef(fitglm)/std.err)&#10;  , &quot;Pr(&amp;gt;|z|) &quot;= 2 * pnorm(abs(coef(fitglm)/std.err), lower.tail = FALSE)&#10;  , LL = coef(fitglm) - q.val  * std.err&#10;  , UL = coef(fitglm) + q.val  * std.err&#10;)&#10;&#10;r.est&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The model output using robust standard errors is:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;                Estimate   Robust SE         z     Pr(&amp;gt;|z|)            LL           UL&#10;(Intercept)  6.801682703 0.072368970  93.98618  0.000000e+00  6.659842129  6.943523277&#10;yr_rndYes    0.048252657 0.032167588   1.50004  1.336041e-01 -0.014794657  0.111299970&#10;parented    -0.766259824 0.039066917 -19.61403  1.173462e-85 -0.842829574 -0.689690073&#10;api99       -0.007304603 0.000215534 -33.89072 9.127821e-252 -0.007727042 -0.006882164&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The estimates and standard errors are fairly similar to those calculated using Stata. I don't know why the intercept is different though. The &lt;a href=&quot;http://www.ats.ucla.edu/stat/stata/faq/proportion.htm&quot; rel=&quot;nofollow&quot;&gt;Stata-output is&lt;/a&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;------------------------------------------------------------------------------&#10;             |               Robust&#10;       meals |      Coef.   Std. Err.      z    P&amp;gt;|z|     [95% Conf. Interval]&#10;-------------+----------------------------------------------------------------&#10;      yr_rnd |   .0482527   .0321714     1.50   0.134    -.0148021    .1113074&#10;    parented |  -.7662598   .0390715   -19.61   0.000    -.8428386   -.6896811&#10;       api99 |  -.0073046   .0002156   -33.89   0.000    -.0077271   -.0068821&#10;       _cons |    6.75343   .0896767    75.31   0.000     6.577667    6.929193&#10;------------------------------------------------------------------------------&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;There are several methods available for the function &lt;code&gt;vcovHC&lt;/code&gt;. Consult the help file of &lt;code&gt;vcovHC&lt;/code&gt; for the details.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that if you use the option &lt;code&gt;family = quasibinomial(logit)&lt;/code&gt;, there will be no error message (see &lt;a href=&quot;http://stackoverflow.com/questions/12953045/warning-non-integer-successes-in-a-binomial-glm-survey-packages&quot;&gt;here&lt;/a&gt;).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-03-14T11:18:27.557" Id="90002" LastActivityDate="2014-03-14T11:25:40.783" LastEditDate="2014-03-14T11:25:40.783" LastEditorUserId="21054" OwnerUserId="21054" ParentId="89999" PostTypeId="2" Score="3" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am looking for an estimator of the mean of normally distributed data (with known variance) in the regime where the sampling grid is much coarser than the variance. I.e. in the worst case consider count data where only two bins contain data points.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example assume I sampled 40 data points from a normal distribution with unit variance and have two bins:&lt;/p&gt;&#10;&#10;&lt;p&gt;1) Bin one from -1.5 up to 0.5 contains 28 data points&lt;/p&gt;&#10;&#10;&lt;p&gt;2) Bin two from 0.5 up to 2.5 contains 12 data points&lt;/p&gt;&#10;" ClosedDate="2014-03-14T15:35:08.250" CommentCount="1" CreationDate="2014-03-14T14:54:48.307" Id="90022" LastActivityDate="2014-03-14T14:54:48.307" OwnerUserId="41927" PostTypeId="1" Score="0" Tags="&lt;normal-distribution&gt;&lt;mean&gt;&lt;histogram&gt;&lt;estimators&gt;" Title="Compute mean from discrete normally distributed data" ViewCount="27" />
  <row AnswerCount="0" Body="&lt;p&gt;I've conducted an ordinal regression with two categorical variable (attitude toward fraud or probability of being caught=punishment). attitude range is 1:no wrong to 4:very wrong. probability of being caught range is 1:unlikely to 4: very likely. the research question is whether attitude toward fraud or probability of being caught is a stronger predictor of occurrence of fraud?&#10;The results are as below:&#10;How can I obtain an effect size or something like beta in regression when IV's are categorical? do you suggest another test? what if I wanted enter the age and gender as control?&lt;/p&gt;&#10;&#10;&lt;p&gt;Model Fitting Information&lt;br&gt;&#10;Model            -2 Log Likelihood  Chi-Square  df  Sig.&#10;Intercept Only        1558,485&lt;br&gt;&#10;Final                  359,231          1199,254    6     ,000&#10;Link function: Logit.               &lt;/p&gt;&#10;&#10;&lt;p&gt;Parameter Estimates&lt;br&gt;&#10;        Estimate    Std. Error  Wald    df  Sig.    95% Confidence Interval &#10;                            Lower Bound Upper Bound&lt;/p&gt;&#10;&#10;&lt;p&gt;Threshold&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;[occurence = ,00]   5,364   ,119    2046,148    1   ,000    5,132   5,596&#10;[occurence = 1,00]  6,704   ,129    2713,403    1   ,000    6,452   6,957&#10;[occurence = 2,00]  7,942   ,157    2570,944    1   ,000    7,635   8,249&#10;[occurence = 3,00]  9,175   ,227    1627,175    1   ,000    8,729   9,621&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Location&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;[attitude=1,00] 2,563   ,118    474,966 1   ,000    2,333   2,794&#10;[attitude=2,00] 2,155   ,093    537,802 1   ,000    1,973   2,337&#10;[attitude=3,00] ,794    ,094    71,764  1   ,000    ,610    ,977&#10;[attitude=4,00] 0a  .   .   0   .   .   .&#10;[punish=1,00]   ,444    ,137    10,430  1   ,001    ,174    ,713&#10;[punish=2,00]   1,182   ,118    99,799  1   ,000    ,950    1,414&#10;[punish=3,00]   ,514    ,121    18,118  1   ,000    ,277    ,751&#10;[punish=4,00]   0a  .   .   0   .   .   .&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Link function: Logit.&lt;br&gt;&#10;a This parameter is set to zero because it is redundant.                                &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-14T15:48:49.800" Id="90024" LastActivityDate="2014-03-14T15:48:49.800" OwnerUserId="32211" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;ordinal&gt;" Title="ordinal regression and calcuating effect size when predictor is categorial" ViewCount="48" />
  <row Body="&lt;p&gt;Depending on how complex of a model you want to look at, this could be as simple as looking at and testing the autocorrelation.  A little more complicated is fitting an ARIMA model.  More general is fitting a dynamic linear model (which would allow you to bring in other predictors as well).&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-03-14T16:07:19.100" Id="90026" LastActivityDate="2014-03-14T16:07:19.100" OwnerUserId="4505" ParentId="90010" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="90034" AnswerCount="1" Body="&lt;p&gt;I have a 2x2, between-subjects experimental design (2 independent variables (IVs) with 2 levels each) and one dependent variable (DV).  My data are unbalanced and an interaction between the IVs seems likely, so I plan to use ANOVA with Type III Sums of Squares to test whether my DV is different across one of my IVs while controlling for any influence of the other IV (or the interaction, if present).  &lt;/p&gt;&#10;&#10;&lt;p&gt;My question is this: what is the minimum number of data points that I need in each of my 4 cells, to ensure that ANOVA likely won't give spurious results? I'm aware of &lt;a href=&quot;http://pss.sagepub.com/content/early/2011/10/17/0956797611417632&quot; rel=&quot;nofollow&quot;&gt;Simmons et al.'s recommendation (2011)&lt;/a&gt; of having at least 20 data point in each cell, but that recommendation only serves to control the test's rate of false negatives. What I'm more worried about is that with a small enough cell size, the statistics on which ANOVA is based probably won't be very reliable, and so the results of the test would be unreliable as well (in terms of either false negatives &lt;em&gt;or&lt;/em&gt; false positives). Are there any papers or texts out there that have studied and made recommendations regarding this concern?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-03-14T16:14:01.403" Id="90028" LastActivityDate="2014-03-14T18:32:09.537" LastEditDate="2014-03-14T18:32:09.537" LastEditorUserId="7290" OwnerUserId="41800" PostTypeId="1" Score="4" Tags="&lt;anova&gt;&lt;references&gt;&lt;assumptions&gt;&lt;power&gt;&lt;robust&gt;" Title="What is the minimum viable cell size for 2x2 ANOVA?" ViewCount="1020" />
  <row Body="&lt;p&gt;There isn't really any absolute minimum except in a trivial sense (if you won't try to test for the interaction, the minimum $n_{ij}$ will be $1$, if you do want to test for interactions, the minimum cell size might be $2$).  Instead, there are two issues here:  &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;The first is the question of the &lt;a href=&quot;http://en.wikipedia.org/wiki/Robust_statistics&quot;&gt;&lt;strong&gt;robustness&lt;/strong&gt;&lt;/a&gt; of the ANOVA to the violation of &lt;a href=&quot;http://en.wikipedia.org/wiki/Anova#Assumptions_of_ANOVA&quot;&gt;&lt;strong&gt;assumptions&lt;/strong&gt;&lt;/a&gt;.  Like all linear models (regression, $t$-tests, etc.), the ANOVA assumes that the data within each cell (i.e., the residuals) are independent, have equal (homogenous) variance, and are normally distributed.  In truth, some of these assumptions are less necessary than others.  For instance, with enough data you don't really need the within-cell distributions to be normal.  However, what constitutes 'enough data' depends on how far from normal those distributions are.  Thus, the further you are from normality, the more data you need.  But there is another twist here, namely that with fewer data it is harder (or even impossible) to assess whether the assumptions of the ANOVA are met.  So with fewer data per cell, you really are going by blind faith.  If the assumptions are not met, then you can have increased type I error rates.  &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The second issue is the question of &lt;a href=&quot;http://en.wikipedia.org/wiki/Statistical_power&quot;&gt;&lt;strong&gt;power&lt;/strong&gt;&lt;/a&gt;.  The probability of getting significance is a function of the size of the effect and the amount of data you have.  If the effects are large enough, you will have good power even if you have only 1 datum per cell.  I suspect effects that large are uncommon, though.  Thus, you need to determine how large of an effect you want to be able to detect with what power (etc.), and calculate your $N$ accordingly.  &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="7" CreationDate="2014-03-14T17:03:17.847" Id="90034" LastActivityDate="2014-03-14T17:57:55.533" LastEditDate="2014-03-14T17:57:55.533" LastEditorUserId="7290" OwnerUserId="7290" ParentId="90028" PostTypeId="2" Score="8" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am looking for a good textbook (or other resource) that covers the analysis of satisfaction data.  Most of my data uses likert-type scales.  Can anyone recommend something with examples?  &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-03-14T17:26:37.437" Id="90037" LastActivityDate="2014-03-14T17:26:37.437" OwnerUserId="21702" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;likert&gt;&lt;validation&gt;" Title="Looking for a Good Text on Statistical Analysis of Satisfaction Data" ViewCount="31" />
  <row AcceptedAnswerId="90065" AnswerCount="1" Body="&lt;p&gt;I am working on a project where I have to separate base and promotional volume from the sales data. I have sales data for the last 4 years at week level. How can I separate base and promotional volume from only this much data?&lt;/p&gt;&#10;&#10;&lt;p&gt;Is it possible to do by regression or any time series approach?&#10;I'd appreciate any feedback.&#10;Thanks.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-14T18:46:04.600" Id="90048" LastActivityDate="2014-03-16T19:29:38.053" LastEditDate="2014-03-16T19:29:38.053" LastEditorUserId="1352" OwnerUserId="41950" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;time-series&gt;&lt;arima&gt;" Title="Separating Base and Promotional Volume" ViewCount="99" />
  <row AnswerCount="0" Body="&lt;p&gt;in my thesis i had a significant interaction effect between two variables and their main effect is insignificant.&#10;A variable had a p-value of 0.636&#10;B variable had a p-value of 0.219&#10;A*B had a p-value of 0.022&#10;A is employee training practices, B is Autonomy, and the response is explicit knowledge creation. my thesis is about evaluation of the impact of quality management practices on knowledge creation in a public corporation in Jordan.&lt;/p&gt;&#10;&#10;&lt;p&gt;how can i explain this result ???&lt;/p&gt;&#10;" ClosedDate="2014-03-14T19:06:54.703" CommentCount="3" CreationDate="2014-03-14T18:46:18.200" Id="90049" LastActivityDate="2014-03-14T18:46:18.200" OwnerUserId="41952" PostTypeId="1" Score="0" Tags="&lt;regression&gt;" Title="How can I interpret interaction effect in my response surface results?" ViewCount="42" />
  <row Body="&lt;p&gt;You could try &lt;a href=&quot;http://reasoning.cs.ucla.edu/samiam/&quot; rel=&quot;nofollow&quot;&gt;SamIAm&lt;/a&gt;. It's a great free program I've been using constantly since I first downloaded it a year ago. I'm not sure if it has any node limits, but I've had Bayesian networks with 50+ nodes, and it's handled them fine. It's really a nice program; I've been able to do almost all of my calculations and work within the application. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-14T20:08:58.093" Id="90060" LastActivityDate="2014-03-14T20:50:40.700" LastEditDate="2014-03-14T20:50:40.700" LastEditorUserId="32036" OwnerUserId="41956" ParentId="62149" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;I'll let someone else address your first question, but regarding the second bullet point, @Glen_b is right: the $t$ distribution is the posterior, not the prior.  The problem here is that there is an ambiguity in the text concerning what phrases are being joined by the &quot;and&quot;.  Consider these two possibilities:  &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;ol&gt;&#10;  &lt;li&gt;&lt;p&gt;The t distribution is $($the marginal posterior distribution for the normal mean with unknown variance$)$ and [it is also the] conjugate prior distribution...  &lt;/p&gt;&#10;  &#10;  &lt;p&gt;vs  &lt;/p&gt;&lt;/li&gt;&#10;  &lt;li&gt;&lt;p&gt;The t distribution is the marginal posterior distribution for the normal mean $($with unknown variance and [when the] conjugate prior distribution [had been used]$)$...  &lt;/p&gt;&lt;/li&gt;&#10;  &lt;/ol&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;The correct interpretation is #2.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-14T21:37:19.267" Id="90070" LastActivityDate="2014-03-14T21:37:19.267" OwnerUserId="7290" ParentId="89966" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;The question doesn't make sense. Significance is about hypotheses tests and hypotheses are about relationships in data, not about predicted values. &lt;/p&gt;&#10;&#10;&lt;p&gt;In the regression, you found that age of car was significant. That is, if, in the population (all cars on the road?) from which this sample (cars in your data set) was drawn there really was no effect of age on price, it would be unlikely to get a test statistic at least as extreme as the one you got in the sample. &lt;/p&gt;&#10;&#10;&lt;p&gt;Now you are saying you want to test a different hypothesis: e.g. that cars from 2005 are cheaper than 2010. To test that hypothesis you would need to do a different analysis, using cars only from those two years. A t-test might be right, or a different regression. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-03-14T22:22:50.857" Id="90073" LastActivityDate="2014-03-14T22:22:50.857" OwnerUserId="686" ParentId="90068" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;I think you are looking at a &lt;a href=&quot;http://en.wikipedia.org/wiki/Heat_map&quot;&gt;heat map&lt;/a&gt;. The one you link to is a fancy interactive version that presumably requires java code.  In &lt;code&gt;R&lt;/code&gt; the function would be &lt;a href=&quot;http://stat.ethz.ch/R-manual/R-patched/library/stats/html/heatmap.html&quot;&gt;?heatmap&lt;/a&gt;.  There is a tutorial for how to make them in &lt;code&gt;R&lt;/code&gt; &lt;a href=&quot;http://flowingdata.com/2010/01/21/how-to-make-a-heatmap-a-quick-and-easy-solution/&quot;&gt;here&lt;/a&gt;.  This is the initial output of their code:  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i1.wp.com/flowingdata.com/wp-content/uploads/2010/01/3heatmap.png?resize=545%2C565&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-03-14T22:28:28.623" Id="90075" LastActivityDate="2014-03-14T22:35:53.840" LastEditDate="2014-03-14T22:35:53.840" LastEditorUserId="7290" OwnerUserId="7290" ParentId="90074" PostTypeId="2" Score="7" />
  
  <row Body="&lt;p&gt;I suggest fetching the free PDF book &lt;a href=&quot;http://www.stanford.edu/~hastie/local.ftp/Springer/ISLR_print1.pdf&quot; rel=&quot;nofollow&quot;&gt;An Introduction to Statistical Learning with Applications in R&lt;/a&gt; and heading to chapters 3 through 9 which cover the introductory scope of the supervised learning problem and show you how to learn from your data.  You didn't say anything else about your data -- how many samples, how many responses, whether it's a classification problem -- but I would suggest 50 is not in fact a huge set of predictors. The methods described in the ISLR book show you how to find the most effective models from the predictors and responses given your training and testing data.  Insofar as R goes, you'll find it is quite fast and can do in two or three lines what requires dozens of lines of other languages.  R is your friend for this sort of exploration.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-15T01:59:39.190" Id="90083" LastActivityDate="2014-03-24T13:51:06.167" LastEditDate="2014-03-24T13:51:06.167" LastEditorUserId="17230" OwnerUserId="41867" ParentId="89965" PostTypeId="2" Score="3" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Given a linear regression model with all the assumptions checked and validated, I would like to obtain the probability that $Y&amp;gt;y|X=x$. For example for the iris dataset, I would do the following to obtain the probability of $Y&amp;gt;5|X=1,2,3...7$:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;plot(Sepal.Length~Petal.Length, data=iris)&#10;lm1&amp;lt;-lm(Sepal.Length~Petal.Length, data=iris)&#10;summary(lm1)&#10;abline(lm1)&#10;predict(lm1, newdata=data.frame(Petal.Length=1:7))&#10;(summary(lm1))$sigma&#10;    pnorm(5, mean = predict(lm1, newdata=data.frame(Petal.Length=1:7)),&#10;        sd = (summary(lm1))$sigma, lower.tail = F)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Is such an approach correct assuming constant variance?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-15T09:41:15.903" Id="90104" LastActivityDate="2014-03-15T10:31:26.693" LastEditDate="2014-03-15T09:48:45.517" LastEditorUserId="32036" OwnerUserId="6015" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;regression&gt;&lt;linear-model&gt;" Title="Linear regression: Evaluate probability of $Y&gt;y| X=x$" ViewCount="48" />
  
  <row Body="&lt;p&gt;I didn't really get if you want to know the probability of being born on the same day in 3 different months or precisely on the 12th on those months (in this case the probability is obviously lower).&lt;/p&gt;&#10;&#10;&lt;p&gt;Assume we are not in a leap year.&lt;/p&gt;&#10;&#10;&lt;p&gt;The probability of being born on the same day in 3 different months is equal to the one of being born from the first to the 28th + the one of being born in the 29th or 30th + the one of being born on the 31th:&lt;/p&gt;&#10;&#10;&lt;p&gt;$P(B) = \frac1{365^3} \cdot (336\cdot11\cdot10 + 22\cdot10\cdot9 + 7\cdot6\cdot5) = \frac{39150}{48627125} = 0.0008$&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's assume we have a proportion of $P(F)$ females and $1 - P(F)$ males and that the fact of being born on a specific day does not affect the gender, thus $P(F|B) = P(F)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;So $P(B|F) = \frac{P(F|B)\cdot P(B)}{P(F)} = \frac{P(F)\cdot P(B)}{P(F)} = P(B)$&lt;/p&gt;&#10;&#10;&lt;p&gt;If I got the point, there is no need to consider the gender.&#10;If there are mistakes in my reasoning, please show them to me. Thank you.&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2014-03-15T10:19:15.840" Id="90107" LastActivityDate="2014-03-15T13:47:24.390" LastEditDate="2014-03-15T13:47:24.390" LastEditorUserId="41788" OwnerUserId="41788" ParentId="90100" PostTypeId="2" Score="2" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have N binary classifier algoritmhs trained on same single train set and tested on single test set.&lt;/p&gt;&#10;&#10;&lt;p&gt;1) Which statistical test is appropriate for calculating their statistical difference?&lt;/p&gt;&#10;&#10;&lt;p&gt;2) Using McNemmar's test with pairwise manner is a valid option?&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;I know ANOVA, which works on K train/test sets whereas i have single train/test set.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2014-03-15T12:34:39.403" Id="90115" LastActivityDate="2014-03-15T12:43:39.400" LastEditDate="2014-03-15T12:43:39.400" LastEditorUserId="41982" OwnerUserId="41982" PostTypeId="1" Score="0" Tags="&lt;hypothesis-testing&gt;&lt;anova&gt;&lt;statistical-significance&gt;&lt;classification&gt;&lt;mcnemar-test&gt;" Title="Statistical Significance Test on Multiple Algorithms and Single Dataset" ViewCount="39" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have individual panel-level data and am using conditional binomial fe logit regressions to estimate the effect of minimum wage on Employment. &lt;/p&gt;&#10;&#10;&lt;p&gt;1) When I try to include year dummies most of them are excluded due to multicollinearity. I have tried evertying - dropping variables, changing the values of the year dummies - but nothing seems to work. Any advise? &lt;/p&gt;&#10;&#10;&lt;p&gt;2) When I try to include state dummies all of them are excluded due to no within-group variation. This makes sense, as the survey I use to get the data does not follow movers. Is there still a way to control for state effects?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-03-15T19:48:17.883" Id="90136" LastActivityDate="2014-03-15T20:27:56.457" LastEditDate="2014-03-15T20:27:56.457" LastEditorUserId="40908" OwnerUserId="40908" PostTypeId="1" Score="2" Tags="&lt;stata&gt;&lt;fixed-effects-model&gt;&lt;clogit&gt;" Title="Conditional FE Binomial Logit Models (Stata)" ViewCount="81" />
  
  <row Body="&lt;p&gt;You can't really compare $R^2$ before and after, because the underlying variability in $Y$ is different. So you literally can take &lt;em&gt;no comfort whatever&lt;/em&gt; from the change in $R^2$. That tells you nothing of value in comparing the two models.&lt;/p&gt;&#10;&#10;&lt;p&gt;The two models are different in several ways, so they mean different things -- they assume very different things about the shape of the relationship and the variability of the error term (when considered in terms of the relationship between $Y$ and $X$). So if you're interested in modelling $Y$ (if $Y$ itself is meaningful), produce a good model for that. If you're interested in modelling $\sqrt Y$ (/$\sqrt Y$ is meaningful), produce a good model for that. If $\sqrt{Y/X}$ carries meaning, then make a good model for that. But compare any competing models on comparable scales. $R^2$ on different responses simply aren't comparable.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you're just trying different relationships in the hope of finding a transformation with a high $R^2$ -- or any other measure of 'good fit' -- the properties of any inference you might like to conduct will be impacted by the existence of that search process.&lt;/p&gt;&#10;&#10;&lt;p&gt;Estimates will tend to be biased away from zero, standard errors will be too small, p-values will be too small, confidence intervals too narrow. Your models will on average appear to be 'too good' (in the sense that their out-of-sample behavior will be disappointing compared to in-sample behavior).&lt;/p&gt;&#10;&#10;&lt;p&gt;To avoid this kind of overfitting, you need, if possible, to do the model-identification and estimation on different subsets of the data (and model evaluation on a third). If you repeat this kind of procedure on many &quot;splits&quot; of the data taken at random, you get a better sense of how reproducible your results are.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are many posts here with relevant points on these issues: it might be worth trying some searches.&lt;/p&gt;&#10;&#10;&lt;p&gt;(If you have good &lt;em&gt;a priori&lt;/em&gt; reasons for choosing a particular transformation, that's a different issue. But searching the space of transformations to find something that fits carries all manner of 'data snooping' type problems with it.)&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-03-16T00:05:20.727" Id="90152" LastActivityDate="2014-03-17T03:48:28.297" LastEditDate="2014-03-17T03:48:28.297" LastEditorUserId="805" OwnerUserId="805" ParentId="90149" PostTypeId="2" Score="19" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am using a mixed linear model to analyze the effect of two types of treatment on symptoms.  The two treatments were administered in smaller groups (clusters), and delivered by several doctors, so I need to account for clustering (ICC is substantial).  I am using difference in symptom scores from baseline as my outcome measure, treatment type and baseline symptoms (to control for them) as fixed effects, and group cluster and doctor, specified in the 1st level as &quot;subjects&quot; as random (intercept) effects. My model is as follows:  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;MIXED outcome BY treatment WITH baseline_sx&#10;  /CRITERIA=CIN(95) MXITER(500) MXSTEP(10) SCORING(1) SINGULAR(0.000000000001) HCONVERGE(0, ABSOLUTE) LCONVERGE(0, ABSOLUTE) PCONVERGE(0.000001, ABSOLUTE) &#10;  /FIXED=treatment baseline_sx | SSTYPE(3) &#10;  /METHOD=REML &#10;  /PRINT=G  SOLUTION TESTCOV &#10;  /RANDOM=INTERCEPT | SUBJECT(doctor*cluster_gr) COVTYPE(VC).&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;For some reason, the model chokes (gives me a warning about the Hessian matrix not being positive definite) and does not produce any estimate for my random effects.  It does just fine for very data from a different outcome point -- the only difference is that some observations are available for one outcome point and not the other, but overall the number of observations is about the same.  I don't think it has anything to do with covariance structure -- tried different kinds, does not seem to solve the problem.  I pretty much narrowed it down to some kind of problem in one of the groups, possibly having to do with cluster variance...  Has anyone encountered anything similar or have any suggestions for how to overcome this issue?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-03-16T03:35:00.340" Id="90163" LastActivityDate="2014-03-16T03:46:18.527" LastEditDate="2014-03-16T03:46:18.527" LastEditorUserId="24808" OwnerUserId="42006" PostTypeId="1" Score="1" Tags="&lt;mixed-model&gt;&lt;spss&gt;" Title="random effect in mixed linear model SPSS" ViewCount="98" />
  <row Body="&lt;p&gt;The information about correlation of the shorter series with the longer ones is in the information you have. How do you plan to 'pad' the series in such a way that will add to the available information about that particular correlation?&lt;/p&gt;&#10;&#10;&lt;p&gt;You might be able to use some form of imputation, but if that helped it would tend to imply some structure between the variables that probably should be directly in your model for the dependence structure. &lt;/p&gt;&#10;&#10;&lt;p&gt;Of course, you may well want to be able to use longer series to estimate the correlation between series that have more data*. The problem there is keeping a matrix positive definite; you can take approaches like estimating SVDs or Choleski decompositions of correlations, or simply find the nearest (in some sense) psd matrix to the pairwise-estimated one.&lt;/p&gt;&#10;&#10;&lt;p&gt;*(though even then, expecting the correlation to be constant over long periods would rarely make sense, in which case the distant past should probably get less weight in any case).  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-16T03:51:05.843" Id="90166" LastActivityDate="2014-03-16T03:51:05.843" OwnerUserId="805" ParentId="90122" PostTypeId="2" Score="2" />
  
  <row AcceptedAnswerId="90185" AnswerCount="1" Body="&lt;p&gt;I would like to put an output from R to my appendix in my thesis; however when I pasted it to Word, the columns were moved.&lt;/p&gt;&#10;&#10;&lt;p&gt;Do you have any idea how to adjust it to be exactly the same as in R? so that it is easier to read &lt;/p&gt;&#10;" ClosedDate="2014-03-16T13:33:34.007" CommentCount="4" CreationDate="2014-03-16T10:41:01.897" Id="90184" LastActivityDate="2014-03-16T10:59:43.113" OwnerUserId="40023" PostTypeId="1" Score="0" Tags="&lt;r&gt;" Title="Can I copy output from R to Word or Excel?" ViewCount="526" />
  <row AnswerCount="2" Body="&lt;p&gt;I have a time series which has a very strong upward trend for the first half, then very strong downward for the second half and finishes pretty much back where it started.&#10;Should I split the data in two for analysis - or can I still account for a trend which is net neutral?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-03-16T12:00:45.230" Id="90187" LastActivityDate="2015-02-15T22:55:08.433" LastEditDate="2014-03-16T12:55:03.343" LastEditorUserId="22047" OwnerUserId="40124" PostTypeId="1" Score="2" Tags="&lt;time-series&gt;" Title="Time series trend" ViewCount="209" />
  <row AnswerCount="2" Body="&lt;p&gt;I'm developing a classifier system to detect objects of interest in images. I want to report a score, but I'm a bit lost as to what the most fair and informative number is.&lt;/p&gt;&#10;&#10;&lt;p&gt;Sensitivity and specificity seem to be commonly used, but while I have numbers for true positive, false positives and false negatives, I'm not sure how to properly calculate 'true negative'. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-16T17:50:45.987" Id="90204" LastActivityDate="2014-04-18T17:40:30.517" LastEditDate="2014-03-16T18:09:00.937" LastEditorUserId="26338" OwnerUserId="9334" PostTypeId="1" Score="0" Tags="&lt;statistical-significance&gt;&lt;classification&gt;" Title="How to score detector" ViewCount="23" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;Can anyone explain why the BP, Breusch-Pagan, test rejects homoscedasticity with such an apparently randomized plot of residuals? &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/VaaQi.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/hJTcw.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-16T20:09:13.687" Id="90213" LastActivityDate="2014-03-16T22:25:38.220" LastEditDate="2014-03-16T22:25:38.220" LastEditorUserId="17046" OwnerUserId="17046" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;regression&gt;&lt;residuals&gt;&lt;heteroscedasticity&gt;&lt;scatterplot&gt;" Title="Visually random model residuals, yet heteroskedastic? ( very small Breusch-Pagan Test P-Value)" ViewCount="73" />
  <row AnswerCount="1" Body="&lt;p&gt;Suppose we are using rejection sampling and we want to sample from a distribution, say $p$. In order to calculate the acceptance probability we use the ratio: &lt;/p&gt;&#10;&#10;&lt;p&gt;$$P(u &amp;lt; \frac{p(x)}{Mq(x)})$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Therefore we still use the distribution of $p$ for the randomly generated values $x$.&lt;/p&gt;&#10;&#10;&lt;p&gt;So why do we use rejection sampling exactly? Why don't we sample directly from $p$?&#10;In other words, what makes a distribution difficult to sample from?&#10;Thank you.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-03-16T20:20:01.933" Id="90215" LastActivityDate="2014-03-16T22:38:33.663" LastEditDate="2014-03-16T21:13:33.650" LastEditorUserId="32036" OwnerUserId="41181" PostTypeId="1" Score="1" Tags="&lt;distributions&gt;&lt;rejection-sampling&gt;" Title="Rejection Sampling" ViewCount="55" />
  <row AnswerCount="0" Body="&lt;p&gt;In Cox regression, it is sometimes inevitable that the strength of a predictor will vary across time (Singer &amp;amp; Willett). This violates the proportionality assumption, but can be incorporated in the model as an interaction term between the covariate and time (or the log of time). &lt;/p&gt;&#10;&#10;&lt;p&gt;How does one create this interaction term (by hand)? If the predictor has only one single value per subject (e.g., gender coded as 1 or 0), how can this be multiplied by time  which for each subject, takes on many values (e.g., if time is measured in days, and the study is 100 days long, then $t=\{1,...,100\}$)? Is the predictor value multiplied by the &lt;em&gt;event time&lt;/em&gt; (e.g., death)? &lt;/p&gt;&#10;&#10;&lt;p&gt;I've read from some sources that you multiply the covariate value by a &lt;em&gt;function&lt;/em&gt; of time, but I'm not sure if this is accurate, or how one would do this in a data set by hand.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-03-16T21:14:45.327" Id="90218" LastActivityDate="2014-03-16T21:54:59.563" LastEditDate="2014-03-16T21:54:59.563" LastEditorUserId="32036" OwnerUserId="24848" PostTypeId="1" Score="2" Tags="&lt;survival&gt;&lt;interaction&gt;&lt;cox-model&gt;" Title="How to create a covariate  time interaction term in Cox regression?" ViewCount="193" />
  
  <row Body="&lt;p&gt;There is a bunch wrong with both of these, and neither one involves two intercepts, in fact, neither has even one intercept.&lt;/p&gt;&#10;&#10;&lt;p&gt;The difference is that the first one forces a model with no intercept to have the same parameter for both independent variables while the latter ignores the main effects and includes only the interaction.&lt;/p&gt;&#10;&#10;&lt;p&gt;What's wrong?&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;You need a multi-level model because data will be dependent.&lt;/li&gt;&#10;&lt;li&gt;The second model includes an interaction but not the main effects&lt;/li&gt;&#10;&lt;li&gt;The first model forces both parameters to be the same&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2014-03-16T22:16:39.167" Id="90230" LastActivityDate="2014-03-16T22:16:39.167" OwnerUserId="686" ParentId="90228" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="90236" AnswerCount="1" Body="&lt;p&gt;Suppose I have a dataset $D$ and let it be split into 2 smaller datasets $D_1$ and $D_2$ such that $D = D_1 + D_2 $. Thus we can also say that $D_1=D - D_2$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Will we get a simple random sample of $D_1$ if we took a simple random sample of $D$ and removed from it those data points obtained by taking a random sample of $D_2$?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-03-16T23:29:19.517" Id="90232" LastActivityDate="2014-03-17T00:13:42.000" OwnerUserId="12005" PostTypeId="1" Score="1" Tags="&lt;sampling&gt;" Title="Effect of difference operator on sampling" ViewCount="40" />
  <row Body="&lt;p&gt;It seems to me that you need to start with understanding the difference between main effects and interaction effects (actually, to meditate on the latter, or find someone who will explain the idea of interaction effects to you very well).&lt;/p&gt;&#10;&#10;&lt;p&gt;Interaction effect is present when changes in the response for different levels of one factor are not linearly moving along the other factor, i.e. higher density for species A gives effect of some size but higher density for species B gives effect of significantly different size, or even reverses the direction of the effect (e.g. mean response decreases instead of increasing), the two factors are then &quot;interacting&quot; with each other, or are not acting &quot;independently&quot; in the linear model.&lt;/p&gt;&#10;&#10;&lt;p&gt;The result you quote is possible. Is Tukey test showing siginifcant differences in the cells of the desing on the &quot;extreme&quot; (opposite) ends ? What does the interaction plot look like (no picture necessary, just quote the values of the means of the response variable in the cells of your design) ?&lt;/p&gt;&#10;&#10;&lt;p&gt;What values have you got in the ANOVA table and what are the results of Tukey's HSD?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-03-16T23:31:57.673" Id="90233" LastActivityDate="2014-03-16T23:31:57.673" OwnerUserId="42002" ParentId="90231" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;I have Training data like following :&lt;/p&gt;&#10;&#10;&lt;p&gt;1 1:128.319 2:71.4336 3:130.255 4:292.948 5:96.3541 6:71.942 7:136.189 8:71.5032 9:148.21 10:304.011 11:90.3781 12:99.4496 13:164.268 14:138.892 15:220.871 16:139.198 17:151.709&lt;/p&gt;&#10;&#10;&lt;p&gt;1 1:118.431 2:97.5874 3:92.0313 4:242.383 5:32.5916 6:91.4476 7:122.88 8:86.6046 9:104.173 10:278.208 11:100.356 12:44.0638 13:159.678 14:163.307 15:181.011 16:83.5148 17:96.1238&lt;/p&gt;&#10;&#10;&lt;p&gt;1 1:137.925 2:103.664 3:129.569 4:280.765 5:48.4616 6:93.4706 7:104.143 8:87.889 9:125.967 10:284.253 11:80.2913 12:51.7028 13:125.539 14:162.068 15:203.335 16:175.856 17:177.638&lt;/p&gt;&#10;&#10;&lt;p&gt;1 1:134.952 2:141.661 3:247.987 4:288.032 5:41.1806 6:109.573 7:117.331 8:99.5418 9:159.184 10:232.882 11:78.1744 12:75.0752 13:168.181 14:132.876 15:204.355 16:107.003 17:118.515&lt;/p&gt;&#10;&#10;&lt;p&gt;0 1:115.603 2:61.5781 3:132.382 4:293.832 5:65.9111 6:106.214 7:103.853 8:116.03 9:142.142 10:293.42 11:49.4609 12:97.0318 13:157.035 14:146.374 15:179.402 16:135.535 17:125.605&lt;/p&gt;&#10;&#10;&lt;p&gt;0 1:128.367 2:68.9784 3:83.4542 4:233.359 5:71.6529 6:69.5444 7:134.49 8:64.3986 9:87.4766 10:282.327 11:87.1273 12:61.7704 13:136.858 14:131.094 15:164.514 16:68.9703 17:70.6821&lt;/p&gt;&#10;&#10;&lt;p&gt;0 1:109.747 2:85.5685 3:139.116 4:309.536 5:53.0768 6:93.8608 7:100.835 8:87.8225 9:131.276 10:277.85 11:93.1516 12:49.7809 13:163.615 14:148.949 15:236.783 16:148.671 17:186.764&lt;/p&gt;&#10;&#10;&lt;p&gt;0 1:112.509 2:79.2363 3:153.702 4:271.478 5:74.3454 6:86.4557 7:86.2066 8:84.2486 9:156.918 10:254.445 11:74.8842 12:75.8982 13:112.379 14:108.054 15:164.201 16:84.1002 17:102.252&lt;/p&gt;&#10;&#10;&lt;p&gt;0 1:141.522 2:78.2754 3:127.487 4:363.352 5:91.2171 6:81.9828 7:231.043 8:113.83 9:229.079 10:437.796 11:122.712 12:115.435 13:178.213 14:155.286 15:188.521 16:131.901 17:124.573&lt;/p&gt;&#10;&#10;&lt;p&gt;and when i run the following code :&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;[true_label, data_inst] = &#10;&#10;libsvmread('C:\Users\fypstudent\Documents\MATLAB\Training_Data\OVA_E0.txt');&#10;model = svmtrain(true_label, data_inst, '-c 1 -g 0.07 -t 0 -b 1');&#10;[true_label, data_inst] = libsvmread('C:\Users\fypstudent\Documents\MATLAB\Training_Data\OVA_E0.txt');&#10;[predict_label, accuracy, prob_estimates] = svmpredict(true_label, data_inst, model, '-b 1');&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;i get the following probabilities for the data i provided :&lt;/p&gt;&#10;&#10;&lt;p&gt;0.145614770432359   0.854385229567641&#10;0.146701861245572   0.853298138754428&#10;0.146126482529063   0.853873517470937&#10;0.148072155086074   0.851927844913926&#10;0.147143517011699   0.852856482988301&#10;0.145711409800836   0.854288590199164&#10;0.144862609974806   0.855137390025194&#10;0.145741700456742   0.854258299543258&#10;0.146935964238630   0.853064035761370&lt;/p&gt;&#10;&#10;&lt;p&gt;i need to know what these probabilities indicated, i also get probability &gt; 0.5 for false labels, can someone help me in understanding this.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-17T10:44:15.363" Id="90268" LastActivityDate="2014-04-22T21:11:24.487" OwnerUserId="42072" PostTypeId="1" Score="0" Tags="&lt;probability&gt;&lt;libsvm&gt;" Title="LIBSVM Probability estimates in binary classification" ViewCount="322" />
  
  
  
  
  
  <row Body="&lt;p&gt;The within-subjects test of equality of proportions is &lt;a href=&quot;http://en.wikipedia.org/wiki/McNemar%27s_test&quot; rel=&quot;nofollow&quot;&gt;McNemar's test&lt;/a&gt;.  I discuss it fairly thoroughly here: &lt;a href=&quot;http://stats.stackexchange.com/a/89415/7290&quot;&gt;What is the difference between McNemar's test and the chi-squared test, and how do you know when to use each?&lt;/a&gt;  The answer also provides an example demo in &lt;code&gt;R&lt;/code&gt;, should you use that, but it should be easy to do in any software.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-17T16:32:30.070" Id="90310" LastActivityDate="2014-03-17T16:32:30.070" OwnerUserId="7290" ParentId="90305" PostTypeId="2" Score="4" />
  
  <row AnswerCount="0" Body="&lt;p&gt;When an ARIMA model over estimates, i.e if the forecast from the model is always higher than the actual value, what's the cause? The model i used was SARIMA (1,1,0)x(1,0,0).&#10;Below is the time series data, the ACF &amp;amp; PACF and the time series plot. Please help me out with identifying the model.The 95% confidence limit is 0.183. I have the actual values for the year 2011 and 2012. &lt;a href=&quot;http://www.fileswap.com/dl/DOno6o9G8a/&quot; rel=&quot;nofollow&quot;&gt;The excel file of my data&lt;/a&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/RbrNO.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/JUbXo.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/1ebP1.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-03-17T18:20:49.410" Id="90323" LastActivityDate="2014-03-19T09:31:00.710" LastEditDate="2014-03-19T05:29:29.430" LastEditorUserId="39544" OwnerUserId="39544" PostTypeId="1" Score="0" Tags="&lt;forecasting&gt;&lt;arima&gt;&lt;model&gt;" Title="over estimating ARIMA model" ViewCount="55" />
  <row AcceptedAnswerId="90935" AnswerCount="2" Body="&lt;p&gt;Suppose you have $\theta=\{1,2\}$ and the sample of (0,1,2) with the task of finding MLE:&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{array} {|c|c|c|}&#10;\hline&#10;x &amp;amp; p(x|\theta=1) &amp;amp; p(x|\theta=2) \\&#10;\hline&#10;0 &amp;amp; 1/2 &amp;amp; 1/4 \\&#10;\hline&#10;1 &amp;amp; 0 &amp;amp; 1/4 \\&#10;\hline&#10;2 &amp;amp; 1/3 &amp;amp; 0 \\&#10;\hline&#10;\end{array}&lt;/p&gt;&#10;&#10;&lt;p&gt;I can't compare $\prod_i p(x_i|\theta)$, because of the zeroes. Even though I can compare probability for a single observation, I'm still having trouble quantifying the difference. $p(x=1|\theta=2)&amp;gt;p(x=1|\theta=1)$ for x=1, but can I do a similar comparison for the whole vector?&lt;/p&gt;&#10;&#10;&lt;h3&gt;Edit&lt;/h3&gt;&#10;&#10;&lt;p&gt;The original question is&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;One observation is taken on a discrete random variable X with pmf f(x|), where {1,2,3}. Find the MLE of .&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;This is followed by a 4x5 table which includes zeroes in every column. I think the idea behind this question is that we have a distribution that doesn't describe population well. Do we just say that the method of MLE is not applicable here?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-03-17T18:47:13.313" FavoriteCount="1" Id="90325" LastActivityDate="2014-03-22T04:57:22.390" LastEditDate="2014-03-21T18:23:46.933" LastEditorUserId="8013" OwnerUserId="39997" PostTypeId="1" Score="3" Tags="&lt;self-study&gt;&lt;maximum-likelihood&gt;&lt;discrete-data&gt;&lt;likelihood&gt;" Title="How to decide on the MLE when pmf is 0?" ViewCount="302" />
  
  <row Body="&lt;p&gt;Suppose you have some univariate time series $y_{t}$&#10;$$y_{t}=f\left(x_{t}\right)+\epsilon_{t}$$&#10;with&#10;$$\epsilon_{t}\equiv z_{t}\sigma_{t}$$&#10;where $f(x_{t})$ is some arbitrary function and $z_{t}$ is iid with zero mean. If $y_{t}$ is in levels, then it will likely have implications for how you model $f(x_{t})$. It is important that $\epsilon_{t}$ is stationary. This might involve fitting an ARMA model or setting $f(x)\equiv y_{t-1}$, effectively differencing. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you model $\sigma_{t}$ as &#10;$$\sigma_{t}^{2}=\alpha_{0}+\sum_{i=1}^{p}\alpha_{i}\epsilon_{t-i}^{2}+\sum_{j=1}^{q}\beta_{j}\sigma_{t-j}^{2}&#10;$$&#10;then you would say that the variance is Garch(p,q).&lt;/p&gt;&#10;&#10;&lt;p&gt;The above means that today's variance is a linear function of the previous period's squared error and variance. The expected value of $y_{t}$, however, does not necessarily depend on the variance. It is driven by whatever $f(x_{t})$ is (since $z_{t}$ has mean of zero). If $f(x_{t})$ contains a term for variances, such as the Garch-in-mean model, then the expected value of $y_{t}$ may depend on variances. Whether that is true is an empirical question, depending on what prices you're looking at.&lt;/p&gt;&#10;&#10;&lt;p&gt;Nevertheless, it is more common to perform econometric analysis on log prices (or returns) and then convert any forecasts back in terms of prices (if you need them that way). If $y_{t}$ is log prices and you try to convert $E(y_{t+k})$ to $E(\exp(y_{t+k}))$ (which would be the expected prices in $t+k$ instead of the expected log prices), then the expected price is influenced by the variance of the log price. &lt;/p&gt;&#10;&#10;&lt;p&gt;To that extent, prices often depend on past prices and past variances. For some types of prices, like stock returns for liquid stocks, you might be able to say that future returns depend more on past variances than past returns, at least in short forecast horizons. But do not mix up prices and returns.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-17T22:11:12.107" Id="90349" LastActivityDate="2014-05-17T18:30:30.577" LastEditDate="2014-05-17T18:30:30.577" LastEditorUserId="17230" OwnerUserId="11623" ParentId="90342" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;If you are using non-linear mixed effects models, you should probably not be using AIC to begin with. Parameters in multi-level models cannot vary independently, and simply counting them is not a great way to get an effective parameter size estimate. &lt;/p&gt;&#10;&#10;&lt;p&gt;Andrew Gelman and the people at Stan have a paper on WAIC. Check it out.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-18T00:38:34.583" Id="90368" LastActivityDate="2014-03-18T00:38:34.583" OwnerUserId="42113" ParentId="28823" PostTypeId="2" Score="3" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am having trouble figuring out the correction factor to choose for a Bonferroni correction. Let me explain.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have two datasets, control ($x$) and data from left ($y_1$, affected limb) and right limb ($y_2$, non-affected limb) of a pathology group. As $y_1$ and $y_2$ were from the same subject (paired data), I have performed a Wilcoxon signed-rank test to compare if these data were statistically different. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, when I compared the data between $x$ and $y_1$ or, between $x$ and $y_2$, I used a Wilcoxon Rank-Sum Test. &lt;/p&gt;&#10;&#10;&lt;p&gt;As there are multiple tests, should I use correction factor of 3 ($p&amp;lt;=\frac{0.05}{3}$)? &lt;/p&gt;&#10;&#10;&lt;p&gt;Or ($p&amp;lt;=\frac{0.05}{2}$) for group ($x$ and $y_1$) and ($x$ and $y_2$) only and no correction for group ($y_1$ and $y_2$)? &lt;/p&gt;&#10;&#10;&lt;p&gt;I would really appreciate your kind reply. Thank you.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-03-18T03:40:29.947" FavoriteCount="2" Id="90379" LastActivityDate="2014-03-18T03:43:33.653" LastEditDate="2014-03-18T03:43:33.653" LastEditorUserId="805" OwnerUserId="18482" PostTypeId="1" Score="4" Tags="&lt;statistical-significance&gt;&lt;wilcoxon&gt;&lt;bonferroni&gt;&lt;signed-rank-test&gt;" Title="Bonferroni correction for two different tests on the same dataset" ViewCount="172" />
  <row AnswerCount="1" Body="&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/nLqEA.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Why are there negative values in the range of the plot?  &lt;/li&gt;&#10;&lt;li&gt;How is the center of the plot (i.e., the $0.0$ point) found?  &lt;/li&gt;&#10;&lt;li&gt;How did the black-brown points end up in the third quadrant of the plot?  &lt;/li&gt;&#10;&lt;li&gt;Why are red, hazel, &amp;amp; brown positive on the y-axis and negative on the x-axis?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I just really need to learn how to interpret this kind of simple diagram before going further.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;sub&gt;This picture was taken from another question on Cross Validated.&lt;/sub&gt;&lt;/p&gt;&#10;" ClosedDate="2014-03-18T14:21:37.703" CommentCount="1" CreationDate="2014-03-18T03:40:52.173" Id="90380" LastActivityDate="2014-03-18T08:56:12.710" LastEditDate="2014-03-18T08:56:12.710" LastEditorUserId="3277" OwnerUserId="42120" PostTypeId="1" Score="1" Tags="&lt;interpretation&gt;&lt;correspondence-analysis&gt;&lt;biplot&gt;" Title="Interpreting negative coordinates and origin in 2D correspondence analysis plot" ViewCount="89" />
  <row AnswerCount="1" Body="&lt;p&gt;&lt;strong&gt;Intro:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I have recently made a simulation program which simulates patients with Type 1 diabetes. In that context I am creating artifical patients. Let's denote three of these as $\text{p}_1$, $\text{p}_2$, and $\text{p}_3$. Assume that $\text{p}_1$, $\text{p}_2$ and $\text{p}_3$ are treated with treatment $A$. Treatment $A$ treats $\text{p}_1$, $\text{p}_2$ and $\text{p}_3$'s diabetes to a certain degree. We evaluate the treatment with some measures conducted in a virtual trial; let's denote one of these measures $\text{M}1_A$. $\text{M}1_A$ has an average value and a standard deviation for Treatment A for the three patients. &lt;/p&gt;&#10;&#10;&lt;p&gt;Now let's introduce treatment B for the same patient group, $\text{p}_1$, $\text{p}_2$ and $\text{p}_3$. Again we put the treatment through a trial test and receive a list of measures, this time denoted $\text{M}1_B$. I am interested in testing whether the difference in $\text{M}1_A$ and $\text{M}1_B$ is significant.&lt;/p&gt;&#10;&#10;&lt;p&gt;I know:&#10;$N=3, \text{std(M}1_A), \text{std(M}1_B), \text{mean(M}1_A), \text{mean(M}1_B)$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to compare the two measures tested on the same group of 'people' and use a statistical tool to figure out if the difference in the two measures are significant.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am considering using &lt;a href=&quot;http://en.wikipedia.org/wiki/Welch%27s_t_test&quot; rel=&quot;nofollow&quot;&gt;Welch's t-test&lt;/a&gt;  would this be correct?&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2014-03-18T07:46:34.843" Id="90391" LastActivityDate="2014-03-18T08:29:46.780" LastEditDate="2014-03-18T08:29:46.780" LastEditorUserId="32036" OwnerUserId="42128" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;&lt;t-test&gt;&lt;group-differences&gt;" Title="Comparing two means: Same group, different variance (Welch's t-test?)" ViewCount="145" />
  
  <row Body="&lt;ul&gt;&#10;&lt;li&gt;&lt;strong&gt;Why are there negative values in the range of the plot?&lt;/strong&gt; Because the input contingency table of positive values (e.g. frequencies) was&#10;standardized by the analysis in a special way relative the central&#10;point.&lt;/li&gt;&#10;&lt;li&gt;&lt;strong&gt;How is the center of the plot (i.e., the 0.0  point) found?&lt;/strong&gt; It is the weighted mean row profile and the weighted mean column profile.&#10;It is thus the locus of &quot;perfectly typical&quot; or predominant hair and eye colour.&#10;According to your data, brown hair and hazel eyes are the closest&#10;candidates to be called &quot;typical&quot; of the population.&lt;/li&gt;&#10;&lt;li&gt;&lt;strong&gt;How did the black-brown points end up in the third quadrant of the plot?&lt;/strong&gt; What quadrant - it doesn't matter. Row and column points&#10;which are close together tend to co-occure in the population. Blach&#10;hair and brown eyes normally go together, as blond hair and blue&#10;eyes. Brown hair is also not uncommon with brown eyes; however, brown&#10;hair occurs very often in the population so it also is often seen with&#10;combination with some other eyes, mostly hazel.&lt;/li&gt;&#10;&lt;li&gt;&lt;strong&gt;Why are red, hazel, &amp;amp; brown positive on the y-axis and negative on the x-axis?&lt;/strong&gt; As said already, positive/negative sign doesn't matter&#10;itself. The general moral following from your map is that Dimension X&#10;corresponds to blond vs brown/black opposition, and weaker (less&#10;discriminative of the population) Dimension Y corresponds to &quot;colourful&quot;&#10;vs &quot;plain&quot; opposition. Note that in simple correspondence analysis you are allowed to rotate the axes as you wish if it facilitates interpretation.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="2" CreationDate="2014-03-18T08:24:51.273" Id="90394" LastActivityDate="2014-03-18T08:52:29.307" LastEditDate="2014-03-18T08:52:29.307" LastEditorUserId="3277" OwnerUserId="3277" ParentId="90380" PostTypeId="2" Score="3" />
  <row AnswerCount="0" Body="&lt;p&gt;I've read a lot of theory about graphic passwords and how people remember them. I wanted to test the theory, so I created an application that allowed people to create textual and graphic passwords and recall them after some distraction tasks. The graphic passwords were a combination of choosing an image from a grid and then positioning the chosen images on a canvas. In both cases, the only restriction was that the passwords weren't blank. I've already had people run through the application so I have some data to work with.&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to test two hypotheses:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Hypothesis 1&lt;/strong&gt;: People will choose passwords with equal effective strengths in both methods&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Hypothesis 2&lt;/strong&gt;: People will be able to recall their passwords equally well&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm looking for any high-level or algorithmic advice knowledgeable people can give on testing these hypotheses. My thoughts after talking to my academic supervisor and a mathematician friend are below:&lt;/p&gt;&#10;&#10;&lt;p&gt;Hypothesis 1:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;We should follow an established procedure for scoring textual passwords and attempt to generate a strength score that follows the same distribution for the graphical passwords. We can then compare the distributions based on knowledge of the theoretical password spaces.&lt;/li&gt;&#10;&lt;li&gt;I'm looking for a way to compare 2 sets of semi-scientific scores representing the effective password spaces that are broken down into groups of passwords generated per person, whilst having a more scientific understanding of the theoretical password spaces.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Hypothesis 2:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Although we have data about the Levenshtein distances of the passwords, the reality is that passwords are generally either right or wrong based on whether they are an exact match. With this in mind, we also have produced binary data for each password attempt. Unfortunately I don't know how to compare the binary data well other than taking a simple average although I've heard it's possible with binomial distribution.&lt;/li&gt;&#10;&lt;li&gt;Since each person recalled 3 passwords 2 times, I'm looking for a statistical method that takes into account the the person, the amount of distraction they've had (i.e. 5 minutes or 15 minutes) and whether the entry was an exact match or not to give some indication of how likely people are to get a password wrong after 5 or 15 minutes of distraction. I will also need to know if this difference is significant.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="4" CreationDate="2014-03-16T18:48:10.713" Id="90401" LastActivityDate="2015-02-17T07:19:05.257" LastEditDate="2015-02-17T07:19:05.257" LastEditorUserId="53618" OwnerDisplayName="James Hadley" OwnerUserId="42091" PostTypeId="1" Score="0" Tags="&lt;hypothesis-testing&gt;" Title="Comparing effective password spaces in textual and graphic passwords" ViewCount="28" />
  <row AnswerCount="0" Body="&lt;p&gt;Currently I'm using libsvm for my one class classification problem. I have 10 samples in my training set, 5 samples in my testing set, both of my training and testing set is scaled by svm_scale, then I use smv_train with the scaled training set for training, and svm_pridict with the scaled testing set for testing. But this problem occurs which makes me very confusing. In the result, why even my training sample fails....&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Accuracy = 26.666666667% (4/15) (classification)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" ClosedDate="2014-03-18T14:20:53.700" CommentCount="0" CreationDate="2014-03-18T10:29:38.340" Id="90410" LastActivityDate="2014-03-18T10:29:38.340" OwnerUserId="41359" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;svm&gt;&lt;libsvm&gt;" Title="Why even my training data failed during the prediction of libsvm" ViewCount="15" />
  <row AnswerCount="0" Body="&lt;p&gt;I am using the adonis function in the vegan package to determine effects of different environmental factors in forest plant community composition in different regions. I would like to first use adonis to remove the region effect, this is, to fit a model like&lt;/p&gt;&#10;&#10;&lt;p&gt;adonis_region&amp;lt;- adonis(community ~ region,permutations=999,method=&quot;bray&quot;)&lt;/p&gt;&#10;&#10;&lt;p&gt;Where community is a presence/absence data matrix of species in forest patches belonging to different regions.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then I would like to use the residuals of this model as the response variable for some other analyses. I know I could use strata to model region as a block variable, but this does not interest me, as afterwards I want to perform another kind of analysis where I would like the region effect to be removed.&lt;/p&gt;&#10;&#10;&lt;p&gt;My problem is that I cannot figure out how to get residual values from the adonis model.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any hint of some other kind of multivariate analysis that could solve this problem is very welcome.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you very much!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-18T11:03:28.773" Id="90414" LastActivityDate="2014-03-18T11:03:28.773" OwnerUserId="42139" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;multivariate-analysis&gt;&lt;residuals&gt;&lt;multivariate-regression&gt;&lt;residual-analysis&gt;" Title="extract residuals from adonis function in vegan" ViewCount="60" />
  <row AnswerCount="1" Body="&lt;p&gt;From Poisson's postulates, we know Poisson works for rare events. However, we also know binomial is an approximation of Poisson when the probability of an event is small. So can we use binomial and Poisson interchangeably for rare event? What is the benefit of using one rather than the other?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-18T11:49:59.057" Id="90422" LastActivityDate="2014-03-18T19:13:11.937" OwnerUserId="41916" PostTypeId="1" Score="4" Tags="&lt;binomial&gt;&lt;poisson&gt;&lt;rare-events&gt;" Title="Poisson vs Binomial for rare events" ViewCount="181" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm trying to predict the number of vehicles per model required to maximise utilisation and booking volume but I've hit a brick wall.&lt;/p&gt;&#10;&#10;&lt;p&gt;I've done some exploratory analysis and utilisation is affected by booking days. Also booking days is affected by booking length and bookings per car. &lt;/p&gt;&#10;&#10;&lt;p&gt;I've worked out that 5x 4-day bookings would reach the target but the number of vehicles required depends on how popular the model is but I'm not sure where to go next.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-18T13:03:49.433" Id="90431" LastActivityDate="2014-03-18T13:03:49.433" OwnerUserId="42145" PostTypeId="1" Score="0" Tags="&lt;prediction&gt;" Title="Predicting Required Number of Vehicles" ViewCount="7" />
  <row Body="&lt;p&gt;A good question that is faced very often in all fields!&#10;In either case you are technically removing them from the data set.&lt;/p&gt;&#10;&#10;&lt;p&gt;I know it is common practice when trying to find a trend graphically to use a form of truncation: use the whole data set for plotting purposes, but then exclude the extreme values for the interpretation.&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem with 'winsorizing' is that the parts you add are self-fullfilling, that is they originate from the data set itself and so just support it. There are simlar problems if you look at cross-validation/classification work in machine-learning, when deciding how to use training and test data sets.&lt;/p&gt;&#10;&#10;&lt;p&gt;I haven't come across a standardised approach in any case - it is always data specific. You can try finding out which percentile your data (the outliers) are causing a given percentage of the volatility/st. deviation, and find a balance between reducing that volatility but retaining as much of the data as possible.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-03-18T14:43:42.123" Id="90446" LastActivityDate="2014-03-19T11:34:54.067" LastEditDate="2014-03-19T11:34:54.067" LastEditorUserId="37863" OwnerUserId="37863" ParentId="90443" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;If you have the entire variance-covariance matrix and not just the standard errors (the square root of the diagonal elements), you can do that using the &lt;a href=&quot;http://en.wikipedia.org/wiki/Variance#Basic_properties&quot; rel=&quot;nofollow&quot;&gt;basic properties of a variance&lt;/a&gt;. So the standard error of $b_0 + b_2$ is:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\sqrt{\mathrm{var}(b_0) + \mathrm{var}(b_2) + 2\mathrm{cov}(b_0, b_2)}$&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-18T16:06:17.237" Id="90460" LastActivityDate="2014-03-18T16:06:17.237" OwnerUserId="23853" ParentId="90455" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Here are some results from a regression for 74 cars of &lt;code&gt;gpm&lt;/code&gt; (gallons per mile) as a function of &lt;code&gt;trunk&lt;/code&gt;, &lt;code&gt;weight&lt;/code&gt;, &lt;code&gt;length&lt;/code&gt; and &lt;code&gt;displacement&lt;/code&gt;, which are all measures of size of cars. Only one predictor achieves significance at conventional levels, although its P-value is pleasingly small. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;. regress gpm trunk weight length displacement&#10;&#10;      Source |       SS       df       MS              Number of obs =      74&#10;-------------+------------------------------           F(  4,    69) =   48.19&#10;       Model |  .008805719     4   .00220143           Prob &amp;gt; F      =  0.0000&#10;    Residual |  .003151908    69   .00004568           R-squared     =  0.7364&#10;-------------+------------------------------           Adj R-squared =  0.7211&#10;       Total |  .011957628    73  .000163803           Root MSE      =  .00676&#10;&#10;------------------------------------------------------------------------------&#10;         gpm |      Coef.   Std. Err.      t    P&amp;gt;|t|     [95% Conf. Interval]&#10;-------------+----------------------------------------------------------------&#10;       trunk |   .0003037   .0002702     1.12   0.265    -.0002354    .0008427&#10;      weight |   .0000121   3.90e-06     3.11   0.003     4.35e-06    .0000199&#10;      length |   .0000137   .0001189     0.12   0.909    -.0002235    .0002509&#10;displacement |   4.31e-06   .0000194     0.22   0.825    -.0000344     .000043&#10;       _cons |   .0059957    .012773     0.47   0.640    -.0194857    .0314771&#10;------------------------------------------------------------------------------&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Stata users will, or rather should, recognise regression output for the &lt;code&gt;auto&lt;/code&gt; dataset. Naturally none of the commentary here is intrinsic or specific to Stata. &lt;/p&gt;&#10;&#10;&lt;p&gt;If we look at correlations for the predictors with &lt;code&gt;gpm&lt;/code&gt;, here presented in terms of correlations and 95% confidence intervals, we see that all correlations between individual predictors and &lt;code&gt;gpm&lt;/code&gt; are significant at the 5% level; in fact stronger statements could be made. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;                            correlations and 95% limits&#10;trunk        gpm               0.632    0.472    0.752&#10;weight       gpm               0.854    0.778    0.906&#10;length       gpm               0.820    0.727    0.883&#10;displacement gpm               0.771    0.659    0.850&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;It is easy to reconcile these two findings. The correlations pay absolutely no attention to any other variables except the two named. (There are ways of taking other variables into account, notably partial correlation, but we haven't done that.) The regression on the other hand is a team effort and each coefficient depends not only on the associated predictor, but also on the other predictors. The way it shapes out here is that the predictors are strongly correlated with each other, but &lt;code&gt;weight&lt;/code&gt; looks like the best predictor, and given that &lt;code&gt;weight&lt;/code&gt; is in the equation, the other predictors cannot add much. &lt;/p&gt;&#10;&#10;&lt;p&gt;In a real problem, you should always look at the entire correlation matrix to check the relationships among the predictors; the corresponding scatter plot matrix; and various diagnostic plots. &lt;/p&gt;&#10;&#10;&lt;p&gt;Only when the predictors are uncorrelated with each other will the effects of all the predictors be the sum of the effects of individual predictors. If you have that situation, it is often bad news, not good, as it means your data are just noise. Absent some experimental design intended to secure independence, moderate if not strong relationships among the predictors are as much to be expected as moderate to strong relationships between the predictors and the response variable. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-18T18:30:54.553" Id="90475" LastActivityDate="2014-03-18T18:30:54.553" OwnerUserId="22047" ParentId="90468" PostTypeId="2" Score="3" />
  <row AnswerCount="0" Body="&lt;p&gt;I have k classifiers and n datasets, and I have only one accuracy measurement (which is actually the average of three independent repetitions of the 5-fold-CV, i.e. average over 15 accuracy values) for each algorithm+dataset combination. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have read Demsar 2006 and some more recent papers. I first use an Aligned Friedman test and, in case the result is significant, I apply a post-hoc test. The problem is, for some specific reasons, I do not just want to compare a control method vs the rest as usual (i.e., k-1 comparisons; a lot of post-hoc for 1 vs All exist with p-value correction for k-1 comparisons) but, instead, I want to perform all vs all comparisons. &lt;/p&gt;&#10;&#10;&lt;p&gt;I know I could use multiple paired Wilcoxon tests (&quot;Wilcoxon signed rank test&quot;) with some p-value correction (Holm, etc), just as the &lt;code&gt;pairwise.wilcox.test&lt;/code&gt; function in R, which automatically corrects the unadjusted Wilcoxon p-values to account for all comparisons. But I suspect there are more appropriate post-hoc tests after Aligned Friedman, could you give me advice please?&lt;/p&gt;&#10;&#10;&lt;p&gt;Further, I know this commonly used post hoc for 1 vs all after Aligned Friedman: &lt;/p&gt;&#10;&#10;&lt;p&gt;$z_{ij} = (R_i - R_j)/\sqrt{\frac{k(kn+1)}{6}}$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $R_i$ and $R_j$ are Aligned Friedman average ranks of algorithms i-th (control) and j-th. Note $z_{ij}$ = $-z_{ji}$. Since z follows a N(0,1), the unadjusted p-values of $z_{ij}$ and $z_{ji}$ are the same, &lt;code&gt;pval = 2*pnorm(-abs(z))&lt;/code&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;My second question is: Is it correct to use z to perform all vs all (for k=4, I would do 6 comparisons), providing that every pair of algorithms is compared just once, and that I perform p-value adjustment (via Holm for instance) of the 6 unadjusted pvalues calculated with the above formula? Or the z in this post-hoc was specifically conceived for 1 vs all after Aligned Friedman and cannot be used for anything else?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks a lot&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-18T19:17:23.773" Id="90482" LastActivityDate="2014-03-18T19:17:23.773" OwnerUserId="42167" PostTypeId="1" Score="1" Tags="&lt;nonparametric&gt;&lt;post-hoc&gt;" Title="All vs all post-hoc after Aligned Friedman (k classifiers over multiple datasets)" ViewCount="60" />
  <row Body="&lt;p&gt;Correlation can be two things. Correlation is a mathematical construct on one hand, which is &lt;em&gt;de facto&lt;/em&gt; Pearson correlation. Correlation is also a counterpart to causal on the other, meaning conditional dependence between an &quot;exposure&quot; and &quot;outcome&quot; that may be mediated by 100s of unmeasured factors. Calling work (i.e. the analyses/results of a study) &quot;correlational&quot; doesn't immediately suggest to me whether you mean they summarized several bivariate associations using partial correlations or whether the study was conducted from observational data. &lt;/p&gt;&#10;&#10;&lt;p&gt;I am strongly inclined to believe that you and the reviewer hold opposing ideas of what &quot;correlational&quot; means in this context. This is giving generous credit to the idea that &lt;em&gt;other&lt;/em&gt; aspects of this communication did not denigrate anyone's research/findings.&lt;/p&gt;&#10;&#10;&lt;p&gt;As far as regression analyses are concerned, you can use regression models to analyze &quot;quasiexperimental&quot; data (or observational data) in which adjustment for confounding variables is used to infer what a hypothetically controlled (blocked/randomized) experiment would yield as a result. This leads to the distinction between correlation and causation. Only randomized controlled trials are worthy of discussing results in a causal context. Other results are not &quot;correlational&quot; but you may refer to findings as &quot;associations&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;The word correlation is confusing. In literature presented to a statistical audience, I am careful to avoid correlation altogether except in the context of Pearson's correlation. I would favor &quot;empirical&quot; or &quot;epidemiological&quot; or something of that ilk to refer to findings from observational studies. &lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-03-18T19:24:42.163" Id="90483" LastActivityDate="2014-03-19T18:41:46.963" LastEditDate="2014-03-19T18:41:46.963" LastEditorUserId="8013" OwnerUserId="8013" ParentId="90477" PostTypeId="2" Score="8" />
  
  <row Body="&lt;p&gt;Since you're already using &lt;code&gt;randomForest&lt;/code&gt; after you cross-validate you might emit the chosen fit's computation of the predictor importance values.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; require(randomForest)&#10;&amp;gt; rf.fit = randomForest(Species~.,data=iris,importance=TRUE)&#10;&amp;gt; rf.fit$importance&#10;                  setosa   versicolor   virginica MeanDecreaseAccuracy MeanDecreaseGini&#10;Sepal.Length 0.036340893  0.021013369 0.032345037          0.030708732         9.444598&#10;Sepal.Width  0.005399468 -0.002131412 0.007499143          0.003577089         2.046650&#10;Petal.Length 0.319872296  0.297426025 0.290278930          0.299795555        42.494972&#10;Petal.Width  0.343995456  0.309455331 0.277644128          0.307843300        45.286720&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-03-19T01:55:14.677" Id="90521" LastActivityDate="2014-03-19T01:55:14.677" OwnerUserId="41867" ParentId="13869" PostTypeId="2" Score="0" />
  
  <row AcceptedAnswerId="90557" AnswerCount="3" Body="&lt;p&gt;A beta distribution with its parameters $\alpha = \beta = 1$ is the uniform $[0, 1]$ distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;What distribution is to the discrete uniform distribution (the sample space is left undecided), as the beta distribution is to the uniform distribution over $[0,1]$?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-03-13T20:16:27.973" Id="90547" LastActivityDate="2014-03-19T13:15:55.923" LastEditDate="2014-03-19T13:12:37.303" LastEditorUserId="22311" OwnerDisplayName="Tim" OwnerUserId="1005" PostTypeId="1" Score="3" Tags="&lt;probability&gt;&lt;distributions&gt;&lt;uniform&gt;&lt;beta-distribution&gt;" Title="What probability distribution is to the discrete uniform distribution as the beta distribution is to uniform distribution over $[0,1]$?" ViewCount="113" />
  <row Body="&lt;p&gt;You seem to just be talking about model contrasts, i.e. how you choose to compare different factor levels. &lt;/p&gt;&#10;&#10;&lt;p&gt;You might want to read a bit about contrasts to understand what they are doing, how they work etc, as they can be a bit confusing to get your head around. Some useful references are:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://stackoverflow.com/questions/2352617/how-and-why-do-you-use-contrasts-in-r&quot;&gt;http://stackoverflow.com/questions/2352617/how-and-why-do-you-use-contrasts-in-r&lt;/a&gt;&#10;&lt;a href=&quot;http://www.ats.ucla.edu/stat/r/library/contrast_coding.htm&quot; rel=&quot;nofollow&quot;&gt;http://www.ats.ucla.edu/stat/r/library/contrast_coding.htm&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;However, a very simple way to compare B with C is just to recode your factor levels so that level B is taken as the baseline in R, and then you can use summary() to present the results, which will show you the comparison between the estimated mean values of level B and C.&lt;/p&gt;&#10;&#10;&lt;p&gt;To recode your factor (here called &quot;your.factor&quot;) just use:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;your.factor &amp;lt;- relevel(factor(your.factor), &quot;b&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Alternatively, you can use the &lt;em&gt;glht&lt;/em&gt; function in the &lt;em&gt;multcomp&lt;/em&gt; package in R, although this will take a bit more investigation and learning about how to use it, and you might well be content with using the above solution, given you just need one extra comparison (glht is really useful when you need to specify many different comparisons not given by default, and to correct for multiple comparisons).&lt;/p&gt;&#10;&#10;&lt;p&gt;Hope that helps.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-03-19T13:01:21.207" Id="90555" LastActivityDate="2014-03-19T13:01:21.207" OwnerUserId="36661" ParentId="90548" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;You are correct that $X_1$ and $X_2$ are distributed the same if $P(X_1 \in E)=P(X_2 \in E) \forall E \in B(R)$.  However, in general the inverse maps are not equal, $X_1^{-1}(E) \neq X_2^{-1}(E)$!&lt;/p&gt;&#10;&#10;&lt;p&gt;Say we have two fair coins, and our random variables $X_1$ and $X_2$ map a simultaneous coin flip to heads or tails.  Since the coin is fair, $P(X_1 = heads) = P(X_2 = heads) = \frac{1}{2}$.  However, for many $\omega \in \Omega$, $X_1(\omega) = heads$ and $X_2(\omega) = tails$.      &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-19T13:20:42.087" Id="90560" LastActivityDate="2014-03-19T13:20:42.087" OwnerUserId="42173" ParentId="90544" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm currently planning an SEM study designed to investigate attitudes toward energy technologies. My past research has suggested that the persuasive effect of positive/negative information is minimal, and that differences in attitudes are better explained by stable personality traits.&lt;/p&gt;&#10;&#10;&lt;p&gt;So my model looks something like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/Heya4En.png&quot; alt=&quot;SEM model&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;(strong and weak relations are my predictions)&lt;/p&gt;&#10;&#10;&lt;p&gt;I've only dabbled with SEM in the past, so I was hoping to clear up this design. I intend to take measures of all the personality traits, and manipulate persuasion by giving participants one of two versions of a persuasive stimulus. So they'd either read something that was pro-fracking or anti-fracking (for instance), for various different technologies, as well as completing a bunch of personality scales, and then be tested on an output measure for their overall attitudes.&lt;/p&gt;&#10;&#10;&lt;p&gt;This means the personality and attitude measures are continuous, and the persuasion (blue) measures are binary/categorical.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question:&lt;/p&gt;&#10;&#10;&lt;h3&gt;Is it possible to meaningfully compare the strength of relationships in an SEM model between two continuous variables, and a continuous and a binary variable?&lt;/h3&gt;&#10;&#10;&lt;p&gt;If not, is there an appropriate method for doing so?&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm aware there is a limitation in this design, in that the difference between persuasion levels is to some degree arbitrary and thus a true quantitative comparison between causes is impossible, but comparing the size of effects (or something similar) would be good enough for my purposes.&lt;/p&gt;&#10;&#10;&lt;p&gt;Please let me know if this is a dumb question!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-19T13:30:24.760" Id="90561" LastActivityDate="2014-03-19T13:30:24.760" OwnerUserId="42202" PostTypeId="1" Score="0" Tags="&lt;sem&gt;" Title="Differentiating strength of relationships between categorical and continuous variables in SEM?" ViewCount="51" />
  <row Body="&lt;p&gt;&lt;em&gt;Note: I do not feel that I have sufficient talent to explain why an infinite mean implies that the sample mean is not a good estimator of the location parameter. This is just a demonstration to hopefully provide some intuition behind why that is the case&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Simply put, increased sample size will not make the mean &quot;tend toward&quot; the true location of the Cauchy distribution. For a demonstration, write a program to compute a large number $n$ of Cauchy deviates. The mean of the sample for the first $1...i$ s.t. $i\le n$ deviates will wildly oscillate between very small and very large values. You can see this easily in a plot of those running means versus number of deviates used to compute the running mean.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x   &amp;lt;- rcauchy(1000)&#10;y   &amp;lt;- NULL&#10;for(i in 1:length(x)){&#10;y[i]    &amp;lt;- mean(x[1:i])&#10;}&#10;plot(1:length(x),y, type=&quot;l&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now add another 1000 observations to &lt;code&gt;x&lt;/code&gt; and see what happens.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x   &amp;lt;- c(x, rcauchy(1000))&#10;y   &amp;lt;- NULL&#10;for(i in 1:length(x)){&#10;y[i]    &amp;lt;- mean(x[1:i])&#10;}&#10;plot(1:length(x),y, type=&quot;l&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The running mean &lt;strong&gt;still&lt;/strong&gt; doesn't appear to be returning to $0$ very quickly... it almost seems as if when it gets close, it a very, very large deviate will be drawn, so the mean will &quot;jump&quot; away from the location of the distribution.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-19T14:09:44.213" Id="90566" LastActivityDate="2014-03-19T14:09:44.213" OwnerUserId="22311" ParentId="90531" PostTypeId="2" Score="3" />
  <row AnswerCount="1" Body="&lt;p&gt;I have two dependent variables, &lt;code&gt;Abundance&lt;/code&gt; and &lt;code&gt;Richness&lt;/code&gt; of moths, and 12 independent climate variables. These are &lt;code&gt;Temperature&lt;/code&gt;, &lt;code&gt;Rainfall&lt;/code&gt; and &lt;code&gt;Sunlight&lt;/code&gt;, for each of the 4 seasons. &lt;/p&gt;&#10;&#10;&lt;p&gt;How do I go about analysing this? From doing individual simple linear regression I have found significance for summer rainfall and winter temperature as factors influencing my dependent variables, but I know that this isn't very statistically viable! &#10;Is principle component analysis a suitable way of analysing this data? Are there any other multivariate techniques I could use?&#10;Thanks.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-03-19T14:20:06.760" Id="90568" LastActivityDate="2014-04-12T23:04:47.377" LastEditDate="2014-04-12T23:04:47.377" LastEditorUserId="32036" OwnerUserId="40889" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;multivariate-analysis&gt;" Title="How to analyse data with multiple dependent and independent variables" ViewCount="252" />
  <row Body="&lt;p&gt;So, by the theorem on sums of normal distribution, if $Y_n = X_1 + X_2 + ... +X_n$ then we have that $Y_n \sim N(50n, 20.5n)$ (I can show you a proof for this if you'd like but hopefully you could do this on your own? Or at least your teacher/professor told you this)&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $Z \sim N(0,1)$ then we have that $\mathbb{P} (Y_n &amp;gt;2000) = \mathbb{P} (Z &amp;gt; \frac{2000 -50n} { \sqrt{20.5n}}) = 1 - \mathbb{P} (Z \leqslant \frac{2000 -50n} { \sqrt{20.5n}})$ And we want this to be equal to $0.95$.&lt;/p&gt;&#10;&#10;&lt;p&gt;So we get (by looking up in normal tables) $\frac{2000 -50n} { \sqrt{20.5n}} = \Phi ^{-1}(1- 0.95)  = \Phi^{-1} (0.05) \approx -1.645$&lt;/p&gt;&#10;&#10;&lt;p&gt;And now we have the equation $2000 - 50n = -1.645\sqrt{20.5n}$ (note n need not be an integer, we will just round up to the nearest integer for our final solution)&lt;/p&gt;&#10;&#10;&lt;p&gt;I just chucked this equation into Wolfram Alpha to solve it, but if you wanted to you would square both sides of the equation and solve the quadratic you get, and you check which root really is a solution to the equation (the only positive one I imagine though I don't actually know) and we get n = 40.9533, so we round up and get n = 41 as our solution.&lt;/p&gt;&#10;&#10;&lt;p&gt;As a quick sanity check we note tat $Y_{41} \sim N(2050, 840.5)$ then our standard deviation is $\sqrt{20.5*41} \approx 28.9$ and then $\mathbb{P} (Y_{41} (= X_1 + X_2 + ... + X_{41}) &amp;gt; 2000) = 1 -  \mathbb{P} (Y_{41} \leqslant 2000) \approx 1 - \mathbb{P} (Z \leqslant \frac{2000 - 2050}  {28.9}) = 1 - \mathbb{P} (Z \leqslant -1.725) &amp;gt;0.95$&lt;/p&gt;&#10;&#10;&lt;p&gt;So we are correct, and $n = 41$ is the answer.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-03-19T13:46:26.497" Id="90571" LastActivityDate="2014-03-19T13:46:26.497" OwnerDisplayName="CameronJWhitehead" ParentId="90569" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I want to run nine different ANOVAs on different variables that may change among groups. The ANOVAs are preceded by the Levene test of variance homogeneity. Of course, I want to correct for multiple comparisons in the final alpha threshold for the ANOVAs.&lt;/p&gt;&#10;&#10;&lt;p&gt;But as random p values can be under 0.05 in all tests, the same should happen to the Levene test, am I right?&lt;/p&gt;&#10;&#10;&lt;p&gt;So my question is, shall I correct the Levene test results also for multiple comparisons?&lt;/p&gt;&#10;&#10;&lt;p&gt;What argument can it be not to correct the homogeneity test the same was the results from those data are corrected for multiple comparisons?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-19T14:56:41.090" Id="90578" LastActivityDate="2014-03-19T15:19:12.040" LastEditDate="2014-03-19T15:19:12.040" LastEditorUserId="7290" OwnerUserId="42207" PostTypeId="1" Score="2" Tags="&lt;anova&gt;&lt;multiple-comparisons&gt;&lt;heteroscedasticity&gt;" Title="Multiple ANOVAs, shall I correct the homogeneity of variance test?" ViewCount="37" />
  <row AcceptedAnswerId="91061" AnswerCount="1" Body="&lt;p&gt;I've created an error plot with CI 95% to visualize the difference (post test - pre test) between achieved scores. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/C28LGA0.png&quot; alt=&quot;error bars&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;n = 64&lt;/p&gt;&#10;&#10;&lt;p&gt;Judging from the confidence intervals one could conclude that there's is no significant improvement between the pre and post test scores (the CI does include 0).&lt;/p&gt;&#10;&#10;&lt;p&gt;When I use a paired t-test for the pre and post scores in combination, none of them return significant as expected. &lt;/p&gt;&#10;&#10;&lt;p&gt;But since the differences of the scores are not normally distributed for Total Score Change (p &amp;lt; .001) and Sub Score Change I (p &amp;lt; .05) according to Shapiro-Wilk, i've use the paired Wilcoxon test, which turns out significant for Sub Score Change I (p &amp;lt; .05). &lt;/p&gt;&#10;&#10;&lt;p&gt;So which test should I believe and why?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-03-19T15:32:40.390" Id="90582" LastActivityDate="2014-03-23T14:15:43.880" LastEditDate="2014-03-23T14:04:07.987" LastEditorUserId="6358" OwnerUserId="6358" PostTypeId="1" Score="1" Tags="&lt;spss&gt;&lt;repeated-measures&gt;&lt;nonparametric&gt;&lt;mean&gt;&lt;parametric&gt;" Title="Confidence interval does not include 0, but Wilcoxon test is sgnificant" ViewCount="263" />
  <row AcceptedAnswerId="129390" AnswerCount="1" Body="&lt;p&gt;P-value is defined the probability of obtaining a test-statistic at least as extreme as what is observed, assuming null-hypothesis is true. In other words,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$P( X \ge t | H_0 )$$&#10;But what if the test-statistic is bimodal in distribution? does p-value mean anything in this context? For example, I am going to simulate some bimodal data in R:&lt;/p&gt;&#10;&#10;&#10;&#10;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;set.seed(0)&#10;# Generate bi-modal distribution&#10;bimodal &amp;lt;- c(rnorm(n=100,mean=25,sd=3),rnorm(n=100,mean=100,sd=5)) &#10;hist(bimodal, breaks=100)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/spCCE.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;And let's assume we observe a test statistic value of 60. And here we know from the picture &lt;em&gt;this value is very unlikely&lt;/em&gt;. So ideally, I would want a statistic procedure that I use(say, p-value) to reveal this. But if we compute for the p-value as defined, we get a pretty high-p value&lt;/p&gt;&#10;&#10;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;observed &amp;lt;- 60&#10;&#10;# Get P-value&#10;sum(bimodal[bimodal &amp;gt;= 60])/sum(bimodal)&#10;[1] 0.7991993&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;If I did not know the distribution, I would conclude that what I observed is simply by random chance. But we know this is not true. &lt;/p&gt;&#10;&#10;&lt;p&gt;I guess the question I have is: Why, when computing p-value, do we compute the probability for the values &quot;at least as extreme as&quot; the observed? &#10;And if I encounter a situation like the one I simulated above, what is the alternative solution?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-03-19T15:37:37.743" FavoriteCount="1" Id="90583" LastActivityDate="2014-12-17T00:16:54.803" LastEditDate="2014-12-17T00:16:54.803" LastEditorUserId="28666" OwnerUserId="11013" PostTypeId="1" Score="11" Tags="&lt;hypothesis-testing&gt;&lt;statistical-significance&gt;&lt;p-value&gt;&lt;summary-statistics&gt;" Title="If the distribution of test statistic is bimodal, does p-value mean anything?" ViewCount="369" />
  <row AnswerCount="0" Body="&lt;p&gt;I was trying to derive the equations from page 109 in &quot;elements of statistical learning&quot; (image below)&#10;&lt;img src=&quot;http://i.stack.imgur.com/qkaZi.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;To be honest, I am not sure how the covariance $\Sigma$ is estimated (the third bullet point in image). Can someone kindly show me and how $\hat{\Sigma}$ is derived (in particular I am not sure how $N - K$ appear in the denominator) ? Thanks&lt;/p&gt;&#10;&#10;&lt;p&gt;UPDATE: I still cannot figure why $N-K$ (where $K$ is the number of parameters) shows up in the denominator, but I suspect it has to do with making the covariance matrix estimator unbiased. This is similar to $\frac{1}{N} \sum_i (x_i - \bar{x})^2$ is biased, while $s^2 = \frac{1}{N-1} \sum_i (x_i - \bar{x})^2$ is not. Please CORRECT ME IF I AM WRONG. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-03-19T19:01:38.307" Id="90615" LastActivityDate="2014-03-19T19:53:36.163" LastEditDate="2014-03-19T19:53:36.163" LastEditorUserId="40761" OwnerUserId="40761" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;references&gt;&lt;discriminant-analysis&gt;&lt;mathematics&gt;&lt;statistical-learning&gt;" Title="Estimating the covariance matrix in LDA" ViewCount="99" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I want to determine the confidence interval for my set of data. I have obtained the data by sampling from several Normal Distributions and running a Monte Carlo Simulation. I was wondering how I could determine the confidence interval interval for the $\mu$ of my data? I don't know the $\mu$ or $\sigma$ of the population data. Also, is there a way by which I can determine the % confidence associated with my sampled data being a good representation of the overall data? &lt;em&gt;Is this possible&lt;/em&gt;?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;% Each of the parameters input to the Nielsenneworiginal() are samples from Normal  Distributions. &#10;% A single value from the output of Nielsenneworiginal() is then assigned to thresh_strain and a plot of this will provide a Normal Distribution itself.&#10;&#10;function [final_matrix] = MCsolutionsimilar()&#10;tic &#10;&#10;no_iterations = input('No. of iterations?:');&#10;&#10;thresh_strain = zeros(no_iterations, 1);&#10;% &#10;casechoice =input('Enter 1 for 1st Layup and 2 for 2nd layup:');&#10;&#10;diamchoice = input('Enter 1 for Christmas tree-shaped and 2 for Cylindrical:');&#10;&#10;[layup, overall_p0] = fibreanglerng(no_iterations, casechoice);&#10;&#10;% [E11 E22] = elasticmodulusrng(); % N/m^2&#10;Ef = 235e9; Em = 3.5e9 ; %GPa&#10;[vol_f, overall_p1] = vol_fraction(no_iterations);&#10;E11 = (Ef.*vol_f) + (Em.*(1-vol_f));&#10;E22 = 1./((vol_f./Ef) + ((1-vol_f)./Em));&#10;&#10;% Poisson Ratio&#10;nuf = 0.340; num = 0.33;&#10;v12 = (nuf*vol_f) + num*(1-vol_f);&#10;&#10;% % Shear Modulus&#10;Gf = 96.52; Gm = 1.8; %GPa&#10;G12 = (Gf*Gm)./(vol_f .* Gm + (Gf.*(1-vol_f)));&#10;[GIC, overall_p2] = shearmodulusstrainenergyrng(no_iterations);&#10;&#10;[diamfromimpact_energy, dentsize, overall_p3] = impactenergyrng(no_iterations);&#10;&#10;[diam, overall_p4] = diameterng(no_iterations,diamchoice, diamfromimpact_energy);&#10;&#10;[time] = detectiontime(no_iterations);&#10;&#10;[new_dentsize] = dent_size(dentsize,time);&#10;&#10;for i=1:no_iterations&#10;    for j=1:16&#10;        [J] = Nielsenneworiginal(layup,E11(1,i),E22(1,i),v12(1,i),G12(1,i),GIC(1,i));&#10;        if (isreal(J(1,j)))==0&#10;            J(1,j) = sqrt(imag(J(1,j))^2 + (real(J(1,j)))^2);&#10;        end    &#10;        [thresh_strain(i,1), I] = min(J,[],2); &#10;    end&#10;end&#10;&#10;%%%%%%% Plot 1 %%%%%%%%%%%%%&#10;figure(1); clf(1)&#10;&#10;[mu_j,sigma_j] = normfit(thresh_strain);&#10;x=linspace(mu_j-4*sigma_j,mu_j+4*sigma_j,200);&#10;pdf_x = 1/sqrt(2*pi)/sigma_j*exp(-(x-mu_j).^2/(2*sigma_j^2));&#10;plot(x,pdf_x/10000);&#10;title(sprintf('Mu = %g, sigma = %g',mu_j,sigma_j));&#10;xlabel('Threshold Strains','FontSize',12);&#10;ylabel('Probabilities of occurrence','FontSize',12);&#10;title('\it{Threshold Strains versus Probabilities of occurrence(Christmas tree diameter configuration)}','FontSize',16);&#10;&#10;figure(2); clf(2)&#10;X_j = min(thresh_strain) : (max(thresh_strain) - min(thresh_strain))/100 : max(thresh_strain);&#10;P_j = normcdf(abs(X_j),real(mu_j),real(sigma_j));&#10;title('Normal cdf')&#10;plot(X_j,P_j,'blue.-')&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="7" CreationDate="2014-03-19T19:42:59.527" FavoriteCount="1" Id="90619" LastActivityDate="2014-03-21T09:08:15.270" LastEditDate="2014-03-21T09:08:15.270" LastEditorUserId="42029" OwnerUserId="42029" PostTypeId="1" Score="1" Tags="&lt;confidence-interval&gt;&lt;estimation&gt;&lt;monte-carlo&gt;" Title="Determining the confidence interval of Monte Carlo data" ViewCount="81" />
  
  <row AcceptedAnswerId="90649" AnswerCount="4" Body="&lt;p&gt;I am trying to fit a model, and have the suspicion that what I am doing is not quite right. The data tracks what proportion of people made a decision, and what factors were active when they made their decision, i.e. something like this:&lt;br&gt;&#10;[1,0,1,0, 23%]&lt;br&gt;&#10;[1,1,0,1, 41%]&lt;br&gt;&#10;etc...&lt;br&gt;&#10;I also know how big each group is. The goal is to predict the percentage, based on the binary input. My initial thought was, the model cannot be a straight linear combinations, if only because the output is bounded. It is not exactly a logistic regression either, because the output is not a label, but the average for each group. My first take has been to transform the output in a fashion similar to logistic regression, into log (p/(1-p)), and fit a linear regression. This has given me some decent results, but I have a nagging feeling this is not quite right. Besides the output transformation, I am also concerned that treating the input as numbers, when they really represent binary values, is probably not the best way to go.&lt;br&gt;&#10;So my question is, if this is not the right way to go, are there models that address this specific type of situation? What should I be looking for?&lt;/p&gt;&#10;&#10;&lt;p&gt;[Edit for clarification]&#10;From the comments / responses it seems my description of the data was a bit lacking, so here is a bit more, as well as why I am uncertain about using logistic regression. I'll illustrate on similar data. Suppose products had a set of binary characteristics, and various products were presented to customers, and the result (buy/no buy) was recorded. Then the raw dataset would look like:&lt;br&gt;&#10;F1,F2,F3,... Fn, Buy/No Buy&lt;br&gt;&#10;1, 1, 0, ..  0,  1&lt;br&gt;&#10;1, 1, 1, ..  1,  0&lt;br&gt;&#10;where each row is a specific product, and what the customer did. Now I can aggregate these by identical products, which will have the same characteristics, and simply record the proportion of buys, as well as the number of customers that were presented with that choice. This is essentially what I have.&lt;br&gt;&#10;I could disaggregate back into the original dataset, and run a logistic regression on it, but the groups themselves are very large, and of very different sizes. On top of that, I have 2 problems. First, I could reconstruct synthetic groups that have the same proportion as the original (i.e. if 4% purchased, construct 4 Buy, 96 No Buy rows), but the the Buy/No Buy ratios are pretty small, which would mean reconstructing large groups to properly approximate. Second, the groups have very different size, and I believe the group composition in the complete sample should have similar composition to the original one, which would mean creating potentially very large groups. Which is why I was essentially wondering if there was a way to work off the much smaller dataset directly, without having to reconstruct an artificial gigantic dataset.&lt;br&gt;&#10;My current approach has been to use a gradient descent approach, weighting observations by the group sizes, but I was wondering if there was a smarter way to handle this!&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-03-19T20:00:09.103" FavoriteCount="1" Id="90622" LastActivityDate="2014-03-20T12:01:37.187" LastEditDate="2014-03-19T22:31:08.417" LastEditorUserId="945" OwnerUserId="945" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;logistic&gt;" Title="Regression model where output is a probability" ViewCount="241" />
  
  <row AcceptedAnswerId="90767" AnswerCount="1" Body="&lt;p&gt;Consider the dual with no offset and not slack. &lt;/p&gt;&#10;&#10;&lt;p&gt;In the dual we have that for data points that are support vectors:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\alpha_t &amp;gt; 0$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and that the constraint is satisfied with equality (since a support vector!):&lt;/p&gt;&#10;&#10;&lt;p&gt;$$y^{(t)}\sum^n_{t'=1} \alpha_{t'}y^{(t')}x^{(t')} \cdot x^{(t)} = 1$$&lt;/p&gt;&#10;&#10;&lt;p&gt;However, just because $\alpha_t &amp;gt; 0$ its not obvious to me if all the support vector have the same value of $\alpha_t$. Do all of the support vectors have the same value of $\alpha_t$? I was kind of looking for a mathematical justification or intuition for my question.&#10;My guess is that they would have the same $\alpha_t$ value in the absence of slack. &#10;My other guess was that, $\alpha_i$ intuitively, says the important of that point, right? (maybe thats wrong) If thats true then, not all support vector have the same value of $\alpha_i$ because not all support vectors are essential support vectors. What I mean is, that some support vectors will lie on the margin boundary but removing them will not necessarily change the margin (for example, consider the case where you have 3 SVs in a line, you can remove the middle one and keep the same boundary). So do essential support vectors get larger value of $\alpha_t$?&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, I was wondering, what would happen in the case of slack and offset to the value of $\alpha_t$. I was guessing that closer points to the decision boundary, get larger value of $\alpha$? However, are points inside the margin even consider support vectors?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-03-19T21:51:10.523" Id="90635" LastActivityDate="2014-03-20T20:04:38.900" LastEditDate="2014-03-20T19:46:27.973" LastEditorUserId="37632" OwnerUserId="37632" PostTypeId="1" Score="5" Tags="&lt;machine-learning&gt;&lt;svm&gt;" Title="Is the value of $\alpha$ the same for all support vectors (SV) in the dual and what is the reason for it if they do or don't?" ViewCount="73" />
  <row Body="&lt;p&gt;Well, after discussion in comments, I think it might be clear enough to hazard an answer to the question.&lt;/p&gt;&#10;&#10;&lt;p&gt;It seems that a symmetric-about-0 location-mixture of two normals is required, such that the probability of being in (0, 3) is about 0.15 and in (3, 5) is about 0.35 (and the same on the other side of 0).&lt;/p&gt;&#10;&#10;&lt;p&gt;This can be done; we'll do the positive component and the negative one will simply be the same but with $-\mu$ in place of $\mu$. The positive component should therefore have approximately 0.3 chance of being in (0, 3) about about 0.7 chance of being in (3, 5), since these probabilities will be halved when we select each half with probability 0.5.&lt;/p&gt;&#10;&#10;&lt;p&gt;Since almost all the probability for this positive part must lie in (0,5), and $\mu$ must exceed 3, $\sigma should probably be less than 1 (so that not too much probability is above 5).&lt;/p&gt;&#10;&#10;&lt;p&gt;Like so:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/QRA8W.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;For a given $\sigma$ we need $P(X&amp;lt;3) = 0.3$, so $P(\frac{(X-\mu)}{\sigma}&amp;lt;\frac{(3-\mu)}{\sigma})=0.3)$, or $\frac{(3-\mu)}{\sigma}=\Phi^{-1}(0.3)$ where $\Phi^{-1}$ is the inverse cdf of the standard normal. Hence $\mu = 3-\sigma\, \Phi^{-1}(0.3)$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Calculating in R:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; sig=c(0.6,0.8,1.0); data.frame( sigma = sig, mu = 3-sig*qnorm(0.3) )&#10;  sigma       mu&#10;1   0.6 3.314640&#10;2   0.8 3.419520&#10;3   1.0 3.524401&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Check how much probability is above 5 for the $\sigma=1$ case:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; pnorm(5,3.524,1,lower.tail=FALSE)&#10;[1] 0.06997195&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;That's perhaps a little high, we only get (0.7-0.07)/2 = 0.315 in (3,5). Checking $\sigma=0.8$:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; pnorm(5,3.4195,0.8,lower.tail=FALSE)&#10;[1] 0.02409863&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;That looks reasonably good, almost 0.34 in (3,5). (The value for $\sigma=0.6$ is 0.349.)&lt;/p&gt;&#10;&#10;&lt;p&gt;You can use $\mu = 3-\sigma\, \Phi^{-1}(0.3)$ with whatever value of $\sigma$ you prefer, or you can manipulate the equation so that $\mu$ is given and $\sigma$ is calculated.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-19T22:29:32.907" Id="90639" LastActivityDate="2014-03-19T22:46:06.983" LastEditDate="2014-03-19T22:46:06.983" LastEditorUserId="805" OwnerUserId="805" ParentId="90207" PostTypeId="2" Score="0" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Why do we have many forms of bivariate distributions? An example: Bivariate Exponential Distribution. I understand that they have been derived from transformations, conditions on marginals or correlations and so on. And what is the logic behind having so many forms?&#10;My point is:&lt;/p&gt;&#10;&#10;&lt;p&gt;If you are given a data from bivariate exponential, how will you find out whether its Marshall-Olkin or any other form? Is there any specific methodology?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-20T04:52:35.317" Id="90664" LastActivityDate="2014-05-14T05:04:38.477" LastEditDate="2014-05-14T05:04:38.477" LastEditorUserId="31505" OwnerUserId="31505" PostTypeId="1" Score="1" Tags="&lt;distributions&gt;&lt;bivariate&gt;" Title="Many forms of bivariate distributions" ViewCount="14" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I need to know how many samples need to be sampled to get an allele frequency in a population that is very close to a &quot;true&quot; allele frequency assuming that we know a population size and a true allele frequency.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, we know that there are 100 individuals in one population and genotypic frequencies (AA=30%, BB=10% and AB=60%) and a frequency of B allele (40%). 10, 20, 30, 40, or 50 samples are sampled from the population (N=100) (without replacement) to see differences between estimated and &quot;true&quot; genotypic or allele frequencies.. This is repeated 200 times or more... to see differences in variance among different sample sizes.&lt;/p&gt;&#10;&#10;&lt;p&gt;Next, I will increase a population size to maybe 500, 1000, 3000...&lt;/p&gt;&#10;&#10;&lt;p&gt;I know I can use R funtion &quot;boot&quot;. But I need to create &quot;statistics&quot; function for that and I can't figure out how. I would appreciate it if you could tell me how to do this in R.&lt;/p&gt;&#10;&#10;&lt;p&gt;Each sample has a genotypic value, AA, BB and AB. Maybe we can use numerical values for the three genotypes (i.e. AA=1, BB=0 and AB=0.5)?&lt;/p&gt;&#10;&#10;&lt;p&gt;Many thanks,&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-20T10:59:45.073" FavoriteCount="1" Id="90694" LastActivityDate="2014-03-20T11:05:20.423" LastEditDate="2014-03-20T11:05:20.423" LastEditorUserId="25672" OwnerUserId="25672" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;bootstrap&gt;" Title="Bootstrapping to find how many samples need to be sampled" ViewCount="32" />
  <row AnswerCount="0" Body="&lt;p&gt;Data sources: CDC National Survey of Family Growth, CDC vital stats.&lt;br&gt;&#10;Population: American women aged 15-44 in 1982, 95, 2002, 2006&lt;br&gt;&#10;Sample sizes: several thousand&lt;/p&gt;&#10;&#10;&lt;p&gt;Can I use Pearson r to show linear correlation between two percentage variables? That is, how % of American women who do X correlates with a percentage of events that are categorized a certain way? An example would be percent of Americans who report having driven drunk in the past month in various years compared to percent of motor vehicle collisions that were alcohol-related in same years. The sample is not the same over time but the population almost is, except that it's within an age range so for example the women who were 44 in 1982 will not be included in 1995 and so on.  &lt;/p&gt;&#10;&#10;&lt;p&gt;The reason I want to use percent is because the numbers from the 2 arms of the CDC don't exactly correspond. The National Survey of Family Growth asks women in the survey about births that they have experienced within the past 5 years and applies that to the whole American population, and that number is similar, but not identical, to the number of actual recorded births in the appropriate 5 year interval in the CDC Vital Statistics. I want to combine data from both sources into one aggregate category. Is that possible?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-20T12:57:53.563" Id="90710" LastActivityDate="2014-03-20T13:28:54.920" LastEditDate="2014-03-20T13:28:54.920" LastEditorUserId="7290" OwnerUserId="42255" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;epidemiology&gt;&lt;pearson&gt;" Title="Pearson r with percentages across time" ViewCount="17" />
  
  
  <row Body="&lt;p&gt;The linear kernel is what you would expect, a linear model. I believe that the polynomial kernel is similar, but the boundary is of some defined but arbitrary order (e.g. y = b1 + b2X + b3X^2 + b4X^3).&lt;/p&gt;&#10;&#10;&lt;p&gt;RBF uses normal curves around the data points, and sums these so that the decision boundary can be defined by a type of topology condition such as curves where the sum is above a value of 0.5. (see this &lt;a href=&quot;http://scikit-learn.org/0.11/auto_examples/svm/plot_oneclass.html&quot; rel=&quot;nofollow&quot;&gt;picture&lt;/a&gt; )&lt;/p&gt;&#10;&#10;&lt;p&gt;I am not certain what the sigmoid kernel is, unless it is similar to the logistic regression model where a logistic function is used to define curves according to where the logistic value is greater than some value (modeling probability), such as 0.5 like the normal case.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-03-20T17:02:49.253" Id="90742" LastActivityDate="2014-03-20T17:37:03.207" LastEditDate="2014-03-20T17:37:03.207" LastEditorUserId="31135" OwnerUserId="31135" ParentId="90736" PostTypeId="2" Score="2" />
  
  
  <row AcceptedAnswerId="90774" AnswerCount="1" Body="&lt;p&gt;suppose to have sample from 3 groups A,B,C.&#10;The hypothesis &lt;code&gt;H0: the mean of the 3 groups is the same&lt;/code&gt; can be tested using 3 independent t test. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;test1: mean(A)=mean(B) level 0.05&#10;test2: mean(B)=mean(C) level 0.05&#10;test3: mean(A)=mean(C) level 0.05&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;It's known that we should prefer an ANOVA test because with the previous method there is an increasing risk of type 1 error. &lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to have an example with simulated data where the first procedure lead us to an error while the   ANOVA return the good result. &lt;/p&gt;&#10;&#10;&lt;p&gt;The best would be an R code to simulate the experiment.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-03-20T19:20:27.730" Id="90760" LastActivityDate="2014-03-20T20:49:33.810" OwnerUserId="25392" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;hypothesis-testing&gt;&lt;anova&gt;&lt;t-test&gt;" Title="example where comparison of three mean anova and t-test have different results" ViewCount="245" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a model that has two significant main effects and where the interaction between the two variables (age and %gray) is also significant in a multiple regression. When the interaction (age * %gray) is present in the model, the beta of age changes sign with respect to the model where the main effects alone are included. Is it possible to plot the data (independent variable vs. dependent) in a way that reflects the new sign of the main effect, after the interaction term has been included?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-03-20T21:00:19.463" Id="90776" LastActivityDate="2014-03-20T21:00:19.463" OwnerUserId="42287" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;data-visualization&gt;&lt;interaction&gt;" Title="plotting a main effect in the context of a significant interaction" ViewCount="48" />
  
  <row AnswerCount="0" Body="&lt;p&gt;(We may just assume single parameter model and single variable data for this discussion).&lt;/p&gt;&#10;&#10;&lt;p&gt;I understand that the &quot;observed information&quot; evaluated at the MLE measures the curvature and hence gives a gauge of how much information our data is giving us. &quot;Observed information&quot; here refers to having already obtained a sample and then computing $-d^2l/d\theta^2$ ($l$ here is log-likelihood).&lt;/p&gt;&#10;&#10;&lt;p&gt;I also understand the definition of Fisher Information, and I understand that $l$ is random before the data is drawn. However, in terms of an intuitive explanation, Wikipedia and various other sources say that Fisher Information gives a measure of curvature or still tells us information in the same way as I described for observed information. This, I disagree. When I evaluate $I(\theta)$ by average the curvature over all possible $l$, we must keep in mind that $\theta$ need not be the peak of $l$ - for some $l$, it is, but for other $l$, we may be way, way away from the peak, and the curvature that these scenerios contribute to the Fisher Information may lead to funny effects. Why would we care about the curvature at $\theta$ for these weird $l$ where $\theta$ is clearly not a good estimation?&lt;/p&gt;&#10;&#10;&lt;p&gt;Basically, I am not very convinced on the intuitive explanation given for Fisher Information. Is the intuitive explanation commonly given a correct one?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-21T02:35:00.703" Id="90800" LastActivityDate="2014-03-21T03:23:53.367" LastEditDate="2014-03-21T03:23:53.367" LastEditorUserId="42299" OwnerUserId="42299" PostTypeId="1" Score="2" Tags="&lt;fisher-information&gt;" Title="Fisher Information as measurement of curvature and hence information" ViewCount="52" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Can somebody explain me how to perform cross validation while training Support vector machine via some example?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-03-21T06:35:22.347" FavoriteCount="1" Id="90814" LastActivityDate="2014-03-21T06:35:22.347" OwnerUserId="30570" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;" Title="Cross validation training support vector machine" ViewCount="22" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I am working on zero-inflated count data models using the &lt;code&gt;pscl&lt;/code&gt; package. I am just wondering why there is no development of models for one-inflated count data models! Also why there is no development of bimodal, say zero-and-2-inflated, count data models! Once I generated one-inflated Poisson data and found that neither the &lt;code&gt;glm&lt;/code&gt; with &lt;code&gt;family=poisson&lt;/code&gt; model nor the negative binomial (&lt;code&gt;glm.nb&lt;/code&gt;) model was good enough to fit the data well. If any one can shed some light on my thought, eccentric though it might be, it would be very helpful for me.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-03-21T06:55:50.677" Id="90817" LastActivityDate="2014-07-03T09:29:04.020" LastEditDate="2014-03-25T15:31:39.013" LastEditorUserId="17230" OwnerUserId="42238" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;generalized-linear-model&gt;&lt;zero-inflation&gt;&lt;poisson-regression&gt;" Title="Why are there no one-inflated count data models?" ViewCount="181" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Should my effect size be positive or negative when the control group gains more weight than the experimental group (when it is hypothesised that the experimental group would lose more weight i.e., the difference between groups is due to more weight gain in the control rather than weight loss int the experimental group)? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-21T11:06:43.980" Id="90842" LastActivityDate="2014-03-21T15:16:27.637" OwnerUserId="42324" PostTypeId="1" Score="2" Tags="&lt;effect-size&gt;" Title="direction of effect size" ViewCount="187" />
  <row AnswerCount="1" Body="&lt;p&gt;I am not a statistics expert and would like to check the validity of a test I hav used on a survey results.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Survey Results&lt;/strong&gt;&lt;br&gt;&#10;I asked people ($n$=264) to characterize a panoramic road they chose in 24 different categories. Each of them could choose as many categories he/she desired. Than I counted the number of times each category was mentioned for two subsets of my sample: experts ($n$=37) and public ($n$=227). I received a distribution of the magnitude (or frequencies) for each category in each group. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;The Question&lt;/strong&gt;&lt;br&gt;&#10;I would like to check whether the distribution of choices in two subsets is the same. If it is different, I would like to know which categories are the cause fr this variance.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;The test&lt;/strong&gt;&lt;br&gt;&#10;I chose to use Goodness of Fit (Chi-Square) Using R, while &lt;code&gt;x=&lt;/code&gt;(public counts) and &lt;code&gt;p=&lt;/code&gt;(experts counts) with &lt;code&gt;rescale.p = TRUE&lt;/code&gt;. I set my significance level to 5%.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then I wrote a &lt;code&gt;while&lt;/code&gt; loop, that works until the test's p-Value &gt; 5%.&lt;/p&gt;&#10;&#10;&lt;p&gt;The loop omits variables in each iteration. Omitting variables is based upon a delta vector (&lt;code&gt;delta&amp;lt;-abs(public_counts - expert_counts&lt;/code&gt;). For each iteration the variables that has the highest delta are omitted. That is until $H_0$ is accepted.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to know if this extension of the chi.square function using the while loop is a valid method to answer my question. In particular, is the delta vector that is based on absolute counts is a correct specification.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would be happy to provide raw data and code for my &lt;code&gt;multi.chi&lt;/code&gt; function if necessary.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-21T11:08:03.447" Id="90843" LastActivityDate="2014-12-31T03:44:41.853" LastEditDate="2014-12-31T03:44:41.853" LastEditorUserId="805" OwnerUserId="26572" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;chi-squared&gt;&lt;survey&gt;&lt;goodness-of-fit&gt;" Title="Validity of an iteration for Goodness of fit to a specific application" ViewCount="44" />
  <row AnswerCount="3" Body="&lt;p&gt;Can we use clustering output as predictor variable for classification?&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;I have a set of data and I do clustering analysis on it, it divides the data into different clusters.&lt;/li&gt;&#10;&lt;li&gt;Can I use this cluster information i.e. cluster1, cluster2, cluster3, as one of the input variable i.e. predictor variable for my decision tree algorithm&#10;is it statistically OK?&#10;I found great increase in the model accuracy when I do this&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="1" CreationDate="2014-03-21T11:12:38.080" Id="90844" LastActivityDate="2014-03-22T11:50:51.277" LastEditDate="2014-03-22T11:50:51.277" LastEditorUserId="88" OwnerUserId="42325" PostTypeId="1" Score="1" Tags="&lt;clustering&gt;&lt;classification-tree&gt;" Title="Can we use clustering output as predictor variable for classification?" ViewCount="286" />
  
  <row Body="&lt;p&gt;A one-inflated Poisson model for a count $Y_i$ is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\Pr(Y_i = 1) = \pi_i +(1-\pi_i)\cdot\mu_i\mathrm{e}^{-\mu_i} \\&#10;\Pr(Y_i = y_i) = (1-\pi_i)\cdot\frac{\mu_i^{y_i}\mathrm{e}^{-\mu_i}}{y_i!} \qquad \text{when } y_i\neq 1$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where the Poisson mean $\mu_i$ &amp;amp; Bernoulli probability $\pi_i$ are related to the predictors through appropriate link functions. You can define a similar model to inflate probabilities for any values you choose.&lt;/p&gt;&#10;&#10;&lt;p&gt;Still, &lt;a href=&quot;http://en.wikipedia.org/wiki/Zero&quot;&gt;zero&lt;/a&gt; has a special (&amp;amp; once controversial) place among the counting numbers&amp;mdash;in a sense representing the absence of anything to count. And it's the &quot;nothing&quot; vs &quot;something&quot; distinction, rather than the &quot;one&quot; vs &quot;any other count&quot; distinction that tends to be relevant across a wide range of phenomena we like to model: there's one process that gives a nought, one, two, ... count &amp;amp; another that gives no count at all.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-03-21T11:18:24.770" Id="90847" LastActivityDate="2014-03-21T11:34:14.787" LastEditDate="2014-03-21T11:34:14.787" LastEditorUserId="17230" OwnerUserId="17230" ParentId="90817" PostTypeId="2" Score="6" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;Does anyone know of any good and accessible papers on the random walk modelling of financial data from a statistics perspective? Most of the papers I've found have been written by economists or suchlike, and while still mathematical, they don't quite have the theoretical depth I would like.&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2014-03-21T13:57:12.713" CreationDate="2014-03-21T13:34:58.213" Id="90864" LastActivityDate="2014-03-21T18:54:15.590" LastEditDate="2014-03-21T18:54:15.590" LastEditorUserId="25936" OwnerUserId="40124" PostTypeId="1" Score="0" Tags="&lt;references&gt;&lt;finance&gt;&lt;random-walk&gt;" Title="Financial Random Walks" ViewCount="40" />
  <row Body="&lt;p&gt;Gaussian mixture model is rather a &lt;strong&gt;generalization&lt;/strong&gt; of $k$-means where variances are not identity matrices.  One way to construct a probabilistic model which will be a generalization of kNN is to assume there is a gaussian centered at each data point with unknown variances whose distribution is fixed. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-21T13:54:32.030" Id="90867" LastActivityDate="2014-03-21T13:54:32.030" OwnerUserId="30621" ParentId="90340" PostTypeId="2" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a simple poisson glm with one predictor that has three levels. Unfortunately, for one level my response, the variable has only counts of zero. I expected very low counts (perhaps a one or a two at most). Because of one level having all zeros, neither my poisson glm, nor &quot;zeroinfl&quot; (from pscl), work. The glm throws out a massive standard error for the all zero factor level and the zeroinfl gives me an error message.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a statistical way to say that the other two groups are different from the one with all zeros?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-21T15:06:43.433" FavoriteCount="1" Id="90882" LastActivityDate="2014-03-21T15:40:48.633" LastEditDate="2014-03-21T15:40:48.633" LastEditorUserId="8580" OwnerUserId="42336" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;zero-inflation&gt;&lt;poisson-regression&gt;" Title="Count data with one factor level containing only zeroes" ViewCount="35" />
  <row Body="&lt;p&gt;It all comes down to distance in the feature space into which your projecting your data. The $k$ in $k$NN tells the algorithm how many nearest neighbors, in terms of distance in your feature space, should be used to determine the class of an unknown data point, often by majority vote. Thinking of it in this way makes your observations more intuitive, I think: for your dataset, taking into account the class label of the two most similar, or nearest, neighbors to a data point seems to be a good way to go. Taking into account one is likely to small to overcome the variability of feature values in each class. Similarly, as you increase the number of data point contributing to the label of your new data point, you take into account points that are less and less similar to the point in question. This is why the experiment you ran is so important for $k$NN experiments! It is good practice to run cross-validation on a hold-out optimization data set over multiple $k$-values, to estimate the best choice for your data. Data sets in different domainsor even problem using data sets in the same domain, but different feature representationsare going to differ.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-21T15:11:29.037" Id="90883" LastActivityDate="2014-03-21T15:11:29.037" OwnerUserId="8580" ParentId="90880" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="90898" AnswerCount="1" Body="&lt;p&gt;I am unsure what I should do if I have two Normally Distributed variables with known parameters and I want to find the probability that one of these variables is greater than the other. Should I use the distribution of differences of the two normal variables, as is shown &lt;a href=&quot;http://mathworld.wolfram.com/NormalDifferenceDistribution.html&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;? &lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-03-21T16:23:27.737" Id="90896" LastActivityDate="2014-03-21T23:57:50.640" LastEditDate="2014-03-21T23:57:50.640" LastEditorUserId="805" OwnerUserId="42029" PostTypeId="1" Score="3" Tags="&lt;probability&gt;&lt;normal-distribution&gt;" Title="Difference in two Normal Distributions?" ViewCount="117" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a large survey data set, about 15 years of data.  One binary variable (yes/no) in which I'm interested was over-estimated due to a data collection error for about 1 1/2 years.  I'm not sure how to handle these data.  Could it treated as missing data?  Seems like MNAR in that case.  If so, and all the other missing data is MAR, how do you run an imputation model in SAS or IVEware?  Suppose one could assume the missing values were simply a linear between the two blocks of non-missing data.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-03-21T16:26:19.583" Id="90897" LastActivityDate="2014-03-21T16:26:19.583" OwnerUserId="28835" PostTypeId="1" Score="0" Tags="&lt;missing-data&gt;&lt;multiple-imputation&gt;" Title="Over-estimated response" ViewCount="8" />
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am looking for a little help on determining the correct way to run a &lt;strong&gt;power analysis&lt;/strong&gt; on a &lt;strong&gt;within-subjects repeated measures&lt;/strong&gt; design (i.e., no between subjects variable) with &lt;strong&gt;combined contrasts&lt;/strong&gt;. Originally, I was under the impression that I could run a power analysis on a repeated measures design with planned contrasts and not worry about the fact that we are combining the variables because everything is within subject. However, I have since realized that this is not the case because, for a three factor Repeated (Within) design, there will be 7 error terms owing to combination of main effects and interactions. Depending on how you want to form contrasts (and power or effect sizes) we pool different SS error and their associated df terms...&lt;/p&gt;&#10;&#10;&lt;p&gt;A SS&#10;Error(A)&lt;/p&gt;&#10;&#10;&lt;p&gt;B SS&#10;Error(B)&lt;/p&gt;&#10;&#10;&lt;p&gt;C SS&#10;Error(C)&lt;/p&gt;&#10;&#10;&lt;p&gt;AB&#10;Error(AB)&lt;/p&gt;&#10;&#10;&lt;p&gt;and so on...&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, we are looking at how different apps impact quality of life measures. One of the planned contrasts is to compare manufacturers (2). However, we would also like to collapse across manufacturers (i.e., combining the planned contrasts) to look at app use versus no app (baseline). &lt;/p&gt;&#10;&#10;&lt;p&gt;This website has been so helpful for me in my research, and I have been struggling for some weeks now to figure this out/get a straight answer out of some wonderful statisticians. Perhaps I am asking the wrong question, but hopefully someone here can help me. Any advice would be much appreciated. &#10;Thank you in advance!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-21T18:56:13.710" FavoriteCount="1" Id="90922" LastActivityDate="2014-03-21T18:56:13.710" OwnerUserId="40286" PostTypeId="1" Score="3" Tags="&lt;repeated-measures&gt;&lt;sample-size&gt;&lt;power&gt;&lt;contrasts&gt;" Title="How to run a power analysis for a repeated measures (all within-subjects) with combined contrasts?" ViewCount="79" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I want to add a 95% confidence interval to standardised residuals QQ plot obtained from rugarch plot function.&#10;I don't know how to add this confidence interval but check the QQ plot that I obtained:&#10;&lt;img src=&quot;http://i.stack.imgur.com/in63L.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I extracted the standardised residuals using the following code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;z=residuals(gfit,standardize=TRUE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Then I tried using the qqPlot function from the 'car' package using the following code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;qqPlot(z,distribution=&quot;norm&quot;,envelope=0.95)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Here is the resulting graph&#10;&lt;img src=&quot;http://i.stack.imgur.com/GM6IO.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Firstly this is the kind of confidence interval I'm looking for but I'm not sure if the graph is right.  This is supposed to be a replicate of the first graph with confidence intervals, but both seems to be different although it is supposed to be the same. &lt;/p&gt;&#10;&#10;&lt;p&gt;Can someone explain how to add confidence intervals to rugarch QQ plots(if it's actually possible) or how to get the correct QQ using another package? Basically I want to obtain the first QQ plot but with the second graph confidence lines.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-03-21T19:47:57.527" Id="90925" LastActivityDate="2014-11-20T14:08:19.507" LastEditDate="2014-11-20T14:08:19.507" LastEditorUserId="805" OwnerUserId="31104" PostTypeId="1" Score="0" Tags="&lt;garch&gt;&lt;diagnostic&gt;&lt;qq-plot&gt;" Title="rugarch - How to add confidence intervals to QQ plots?" ViewCount="119" />
  <row AnswerCount="1" Body="&lt;p&gt;I don't know high level math or statistics and wanted to ask about the math in a recent online article. The website 'TheMarySue.com' has an article saying:  &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;ON AVERAGE, THE TOP WOMEN-LED FILMS OF 2013 GROSSED HIGHER THAN MALE-LED FILMS&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;and then references this article that did the math:&lt;br&gt;&#10;&lt;a href=&quot;http://www.vanityfair.com/online/daily/2014/03/women-films-out-gross-male-films&quot;&gt;&quot;Women-Centric Films Out-Gross Male-Centric Films on Average: Twist!&quot;&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In that article the way the author determined that &quot;Actress-centered movies out-grossed actor-centered movies by almost exactly one-third!&quot; was by using the following:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Total gross of all the top 100 movies in 2013: \$10.039 billion.&lt;/li&gt;&#10;&lt;li&gt;Total gross of 15 actress-centered movies: \$1.908 billion.&lt;/li&gt;&#10;&lt;li&gt;Total gross of 79 actor-centered movies: \$7.525 billion.&lt;/li&gt;&#10;&lt;li&gt;Average gross of actress-centered movie: \$127 million.&lt;/li&gt;&#10;&lt;li&gt;Average gross of actor-centered movie: \$95 million.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;The author says 15 movies of 2013 had a female lead in his view, 5 were 'neuter' and 79 had a male lead. &lt;/p&gt;&#10;&#10;&lt;p&gt;I am not trained in statistics but it seemed to me that the author didn't apply math correctly. For example, I used his same logic to show that movies with numbers in the title grossed more than title without numbers:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Total gross of all the top 100 movies in 2013: \$10.048 billion.&lt;/li&gt;&#10;&lt;li&gt;Total gross of 18 number-included-titled movies: \$2.110 billion.&lt;/li&gt;&#10;&lt;li&gt;Total gross of 82 letters-only movies: \$7.938 billion.&lt;/li&gt;&#10;&lt;li&gt;Average gross of number-included-titled movies: \$117 million.&lt;/li&gt;&#10;&lt;li&gt;Average gross of letters-only movies: \$96 million.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I would think that does not show that movies with titles that had numbers were correlated to higher-returns, but a lot of people are telling me that I am wrong and that the argument made in the article based on the data is &quot;sound&quot; and &quot;solid.&quot; One person on the comments section of TheMarySue says:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Yeah, I was really happy about the article until I got to their statistics and realized the logic was flawed. :(&lt;/p&gt;&#10;  &#10;  &lt;p&gt;More films on the lower end of the spectrum means the average gets pulled lower.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;So I am confused because everyone else says the logic and the applied statistics is sound. :(&lt;/p&gt;&#10;&#10;&lt;p&gt;I don't have a preference of gender lead roles in movies. I thought the division of gross revenue to argue that those movies had some special quality was incorrect. I am trying to answer the question:&lt;br&gt;&#10;&quot;What affect does a male lead or female lead have in the gross revenue of a movie in top 100 films of 2013?&quot;&#10;by limiting myself to &lt;a href=&quot;http://boxofficemojo.com/yearly/chart/?yr=2013&quot;&gt;that writer's data&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;How I am wrong in the application of division to create a sound argument for their premise so I don't make that mistake again? Also, what kind of math or techniques should be used to answer that question?&#10;Thanks!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-03-21T20:17:56.077" Id="90927" LastActivityDate="2014-10-07T01:07:33.493" LastEditDate="2014-03-21T20:43:31.187" LastEditorUserId="32036" OwnerUserId="42359" PostTypeId="1" Score="5" Tags="&lt;regression&gt;&lt;statistical-significance&gt;&lt;mean&gt;" Title="Was the math/statistics application in this article correct?" ViewCount="153" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a merged database from two cross-sectional surveys for distinct years ($t_1$ and $t_2$). If all questions were exactly the same, I could just stack the data horizontally and run a normal panel data OLS.&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem is that some variables exist only for $t_1$, while other variables exist only for $t_2$. So my question is: should I just do the same as above and disregard these variables, should I run cross-sectional OLS on each separate databases using all variables available for that year (and then there is a way to &quot;aggregate&quot; the results?) or could I run a SUR model where for each year I use all available variables and constrain the coefficients to be the same?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-21T21:11:26.550" Id="90930" LastActivityDate="2014-03-21T21:11:26.550" OwnerUserId="41683" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;econometrics&gt;&lt;least-squares&gt;" Title="Using panel data with different variables: SUR or OLS separatedly?" ViewCount="41" />
  
  <row Body="&lt;p&gt;Short version: You don't use a t-test because the obvious statistic doesn't have a t-distribution. It does (approximately) have a z-distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;Longer version:&lt;/p&gt;&#10;&#10;&lt;p&gt;In the usual t-tests, the t-statistics are all of the form: $\frac{d}{s}$, where $s$ is an estimated standard error of $d$. The t-distribution arises from the following:&lt;/p&gt;&#10;&#10;&lt;p&gt;1) $d$ is normally distributed (with mean 0, since we're talking about distribution unde $H_0$)&lt;/p&gt;&#10;&#10;&lt;p&gt;2) $k.s^2$ is $\chi^2$, for some $k$ (I don't want to belabor the details of what $k$ will be, since I'm covering many different forms of t-test here)&lt;/p&gt;&#10;&#10;&lt;p&gt;3) $d$ and $s$ are independent&lt;/p&gt;&#10;&#10;&lt;p&gt;Those are a pretty strict set of circumstances. You only get all three to hold at the same time when you have normal data (indeed, the third one characterizes the normal).&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;If, instead, the estimate, $s$ is replaced by the actual value of the standard error of $d$ ($\sigma_d$), that form of statistic would have a $z-$distribution.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;When sample sizes are sufficiently large, a statistic like $d$ (which is often a shifted mean or a difference of means) is very often asymptotically normally distributed*, due to the central limit theorem.&lt;/p&gt;&#10;&#10;&lt;p&gt;* more precisely, a standardized version of $d$, $d/\sigma_d$ will be asymptotically standard normal&lt;/p&gt;&#10;&#10;&lt;p&gt;Many people think that this immediately justifies using a t-test, but as you see from the above list, we only satisfied the first of the three conditions under which the t-test was derived. &lt;/p&gt;&#10;&#10;&lt;p&gt;On the other hand, there's another theorem, called &lt;a href=&quot;http://en.wikipedia.org/wiki/Slutsky%27s_theorem&quot; rel=&quot;nofollow&quot;&gt;Slutsky's theorem&lt;/a&gt; that helps us out. As long as the denominator converges in probability to that unknown standard error, $\sigma_d$ (a fairly weak condition), then $d/s$ should converge to a standard normal distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;The usual one and two-sample proportions tests are of this form, and thus we have some justification for treating them as asymptotically normal, but we have no justification for treating them as $t$-distributed.&lt;/p&gt;&#10;&#10;&lt;p&gt;In practice, as long as $np$ and $n(1-p)$ are not too small**, the asymptotic normality of the one and two-sample proportions tests comes in very rapidly (that is, often surprisingly small $n$ is enough for both theorems to 'kick in' as it were and the asymptotic behavior to be a good approximation to small sample behavior).&lt;/p&gt;&#10;&#10;&lt;p&gt;** though there are other ways to characterize &#10;&quot;large enough&quot; than that, conditions of that form seem to be the most common.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;is there a simple way to conduct an omnibus test for significant differences between more than 2 proportions (in the form of percentages)&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Sure. You can put it into the form of a chi-square test.&lt;/p&gt;&#10;&#10;&lt;p&gt;(Indeed, akin to ANOVA you can even construct contrasts and multiple comparisons and such.)&lt;/p&gt;&#10;&#10;&lt;p&gt;It's not clear from your question, however, whether your generalization will have two samples with several categories, or multiple samples with two categories (or even both once, I guess). In either case, you can get a chi-square. If you are more specific I should be able to give more specific details.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-03-21T23:45:16.980" Id="90936" LastActivityDate="2014-03-22T22:51:14.263" LastEditDate="2014-03-22T22:51:14.263" LastEditorUserId="805" OwnerUserId="805" ParentId="90893" PostTypeId="2" Score="4" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;&lt;code&gt;FullModel&amp;lt;- (lm(Fubar~.-Foo-Bar,data=BarFoo))&#10;NullModel&amp;lt;-(lm(Fubar~1))&#10;step(NullModel,scope=formula(FullModel),direction=&quot;forward&quot;,k=log(nrow(BarFoo)))&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;When doing the above forward stepwise regression, the forward steps halt before certain variables are added in. does this mean that they do not improve the AIC score or does it mean that the inputs are in error?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-22T02:52:40.750" Id="90947" LastActivityDate="2014-03-22T10:14:28.353" OwnerUserId="17046" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;stepwise-regression&gt;" Title="R not testing certain variables in forward stepwise regression?" ViewCount="35" />
  
  
  
  
  <row Body="&lt;p&gt;You're already using git: why not use version control to organize your exploration? Create a new branch for each new &quot;branch&quot; of your exploration, and fork off branches for different versions of plots as well. This method will make it slightly more difficult to combine your end results, but you could always maintain an untracked directory where you could drop in the &quot;gems&quot; of your analysis. You'd probably want to somehow label your files in this directory to indicate which fork/commit they came from. This method has the added benefit of making it really easy to contrast different analyses via the &lt;code&gt;diff&lt;/code&gt; command.&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2014-03-22T13:39:47.563" CreationDate="2014-03-22T13:39:47.563" Id="90972" LastActivityDate="2014-03-22T13:39:47.563" OwnerUserId="8451" ParentId="89678" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;One typical application of the distance transform in computer vision is pattern matching, that is, being able to identify and locate an object in an image. An object is described by a set of primitives, typically edges. The distance transform is then employed to find the distance between corresponding edges. &lt;/p&gt;&#10;&#10;&lt;p&gt;When a new image is presented, pixels are marked as 0 or 1 depending on whether a feature is present or not. Then the distance transform is employed search of a position in the image where the distance of image features to the model features is small enough.&lt;/p&gt;&#10;&#10;&lt;p&gt;There many applications of this idea. See for example &lt;a href=&quot;http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;amp;arnumber=5206777&amp;amp;url=http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5206777&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-22T15:26:23.870" Id="90982" LastActivityDate="2014-03-22T15:26:23.870" OwnerUserId="17908" ParentId="90976" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="90994" AnswerCount="1" Body="&lt;p&gt;I grabbed an implementation of FuzzyKMeans (FuzzyCMeans) from the nightly build of the Apache Commons Math library, but I now realize I need to use &lt;strong&gt;Cosine Similarity instead of the Euclidean Distance&lt;/strong&gt;. Of course, just plugging in Cosine Similarity caused the algorithm to end up in an infinite loop (I tested it just in case, by some miracle, it would work).&lt;/p&gt;&#10;&#10;&lt;p&gt;I've looked into the &lt;a href=&quot;http://stats.stackexchange.com/questions/63558/difference-between-standard-and-spherical-k-means-algorithms&quot;&gt;differences between KMeans and SphericalKMeans&lt;/a&gt; to try and see how I should change FuzzyKMeans to become SphericalFuzzyKMeans, but I'm very inexperienced with clustering and am having trouble distilling any practical information from theoretical papers.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, what I'm looking for is a &lt;strong&gt;practical explanation of the differences between FuzzyKMeans and SphericalFuzzyKMeans, not just a theoretical one.&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Any pointers would be EXTREMELY helpful&lt;/strong&gt;, even if you can't say exactly what needs to be changed in the following code. Even what you suspect should be changed would go a long way in helping me.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Here's the implementation of FuzzyKMeans (FuzzyCMeans) that I have:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;/*&#10; * Licensed to the Apache Software Foundation (ASF) under one or more&#10; * contributor license agreements.  See the NOTICE file distributed with&#10; * this work for additional information regarding copyright ownership.&#10; * The ASF licenses this file to You under the Apache License, Version 2.0&#10; * (the &quot;License&quot;); you may not use this file except in compliance with&#10; * the License.  You may obtain a copy of the License at&#10; *&#10; *      http://www.apache.org/licenses/LICENSE-2.0&#10; *&#10; * Unless required by applicable law or agreed to in writing, software&#10; * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&#10; * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&#10; * See the License for the specific language governing permissions and&#10; * limitations under the License.&#10; */&#10;package org.apache.commons.math3.ml.clustering;&#10;&#10;import java.util.ArrayList;&#10;import java.util.Collection;&#10;import java.util.Collections;&#10;import java.util.List;&#10;&#10;import org.apache.commons.math3.exception.MathIllegalArgumentException;&#10;import org.apache.commons.math3.exception.MathIllegalStateException;&#10;import org.apache.commons.math3.exception.NumberIsTooSmallException;&#10;import org.apache.commons.math3.linear.MatrixUtils;&#10;import org.apache.commons.math3.linear.RealMatrix;&#10;import org.apache.commons.math3.ml.distance.DistanceMeasure;&#10;import org.apache.commons.math3.ml.distance.EuclideanDistance;&#10;import org.apache.commons.math3.random.JDKRandomGenerator;&#10;import org.apache.commons.math3.random.RandomGenerator;&#10;import org.apache.commons.math3.util.FastMath;&#10;import org.apache.commons.math3.util.MathArrays;&#10;import org.apache.commons.math3.util.MathUtils;&#10;&#10;/**&#10; * Fuzzy K-Means clustering algorithm.&#10; * &amp;lt;p&amp;gt;&#10; * The Fuzzy K-Means algorithm is a variation of the classical K-Means algorithm, with the&#10; * major difference that a single data point is not uniquely assigned to a single cluster.&#10; * Instead, each point i has a set of weights u&amp;lt;sub&amp;gt;ij&amp;lt;/sub&amp;gt; which indicate the degree of membership&#10; * to the cluster j.&#10; * &amp;lt;p&amp;gt;&#10; * The algorithm then tries to minimize the objective function:&#10; * &amp;lt;pre&amp;gt;&#10; * J = &amp;amp;#8721;&amp;lt;sub&amp;gt;i=1..C&amp;lt;/sub&amp;gt;&amp;amp;#8721;&amp;lt;sub&amp;gt;k=1..N&amp;lt;/sub&amp;gt; u&amp;lt;sub&amp;gt;ik&amp;lt;/sub&amp;gt;&amp;lt;sup&amp;gt;m&amp;lt;/sup&amp;gt;d&amp;lt;sub&amp;gt;ik&amp;lt;/sub&amp;gt;&amp;lt;sup&amp;gt;2&amp;lt;/sup&amp;gt;&#10; * &amp;lt;/pre&amp;gt;&#10; * with d&amp;lt;sub&amp;gt;ik&amp;lt;/sub&amp;gt; being the distance between data point i and the cluster center k.&#10; * &amp;lt;p&amp;gt;&#10; * The algorithm requires two parameters:&#10; * &amp;lt;ul&amp;gt;&#10; *   &amp;lt;li&amp;gt;k: the number of clusters&#10; *   &amp;lt;li&amp;gt;fuzziness: determines the level of cluster fuzziness, larger values lead to fuzzier clusters&#10; * &amp;lt;/ul&amp;gt;&#10; * Additional, optional parameters:&#10; * &amp;lt;ul&amp;gt;&#10; *   &amp;lt;li&amp;gt;maxIterations: the maximum number of iterations&#10; *   &amp;lt;li&amp;gt;epsilon: the convergence criteria, default is 1e-3&#10; * &amp;lt;/ul&amp;gt;&#10; * &amp;lt;p&amp;gt;&#10; * The fuzzy variant of the K-Means algorithm is more robust with regard to the selection&#10; * of the initial cluster centers.&#10; *&#10; * @param &amp;lt;T&amp;gt; type of the points to cluster&#10; * @version $Id: FuzzyKMeansClusterer.java 1512043 2013-08-08 21:27:57Z tn $&#10; * @since 3.3&#10; */&#10;public class FuzzyKMeansClusterer&amp;lt;T extends Clusterable&amp;gt; extends Clusterer&amp;lt;T&amp;gt; {&#10;&#10;    /** The default value for the convergence criteria. */&#10;    private static final double DEFAULT_EPSILON = 1e-3;&#10;&#10;    /** The number of clusters. */&#10;    private final int k;&#10;&#10;    /** The maximum number of iterations. */&#10;    private final int maxIterations;&#10;&#10;    /** The fuzziness factor. */&#10;    private final double fuzziness;&#10;&#10;    /** The convergence criteria. */&#10;    private final double epsilon;&#10;&#10;    /** Random generator for choosing initial centers. */&#10;    private final RandomGenerator random;&#10;&#10;    /** The membership matrix. */&#10;    private double[][] membershipMatrix;&#10;&#10;    /** The list of points used in the last call to {@link #cluster(Collection)}. */&#10;    private List&amp;lt;T&amp;gt; points;&#10;&#10;    /** The list of clusters resulting from the last call to {@link #cluster(Collection)}. */&#10;    private List&amp;lt;CentroidCluster&amp;lt;T&amp;gt;&amp;gt; clusters;&#10;&#10;    /**&#10;     * Creates a new instance of a FuzzyKMeansClusterer.&#10;     * &amp;lt;p&amp;gt;&#10;     * The euclidean distance will be used as default distance measure.&#10;     *&#10;     * @param k the number of clusters to split the data into&#10;     * @param fuzziness the fuzziness factor, must be &amp;amp;gt; 1.0&#10;     * @throws NumberIsTooSmallException if {@code fuzziness &amp;lt;= 1.0}&#10;     */&#10;    public FuzzyKMeansClusterer(final int k, final double fuzziness) throws NumberIsTooSmallException {&#10;        this(k, fuzziness, -1, new EuclideanDistance());&#10;    }&#10;&#10;    /**&#10;     * Creates a new instance of a FuzzyKMeansClusterer.&#10;     *&#10;     * @param k the number of clusters to split the data into&#10;     * @param fuzziness the fuzziness factor, must be &amp;amp;gt; 1.0&#10;     * @param maxIterations the maximum number of iterations to run the algorithm for.&#10;     *   If negative, no maximum will be used.&#10;     * @param measure the distance measure to use&#10;     * @throws NumberIsTooSmallException if {@code fuzziness &amp;lt;= 1.0}&#10;     */&#10;    public FuzzyKMeansClusterer(final int k, final double fuzziness,&#10;                                final int maxIterations, final DistanceMeasure measure)&#10;            throws NumberIsTooSmallException {&#10;        this(k, fuzziness, maxIterations, measure, DEFAULT_EPSILON, new JDKRandomGenerator());&#10;    }&#10;&#10;    /**&#10;     * Creates a new instance of a FuzzyKMeansClusterer.&#10;     *&#10;     * @param k the number of clusters to split the data into&#10;     * @param fuzziness the fuzziness factor, must be &amp;amp;gt; 1.0&#10;     * @param maxIterations the maximum number of iterations to run the algorithm for.&#10;     *   If negative, no maximum will be used.&#10;     * @param measure the distance measure to use&#10;     * @param epsilon the convergence criteria (default is 1e-3)&#10;     * @param random random generator to use for choosing initial centers&#10;     * @throws NumberIsTooSmallException if {@code fuzziness &amp;lt;= 1.0}&#10;     */&#10;    public FuzzyKMeansClusterer(final int k, final double fuzziness,&#10;                                final int maxIterations, final DistanceMeasure measure,&#10;                                final double epsilon, final RandomGenerator random)&#10;            throws NumberIsTooSmallException {&#10;&#10;        super(measure);&#10;&#10;        if (fuzziness &amp;lt;= 1.0d) {&#10;            throw new NumberIsTooSmallException(fuzziness, 1.0, false);&#10;        }&#10;        this.k = k;&#10;        this.fuzziness = fuzziness;&#10;        this.maxIterations = maxIterations;&#10;        this.epsilon = epsilon;&#10;        this.random = random;&#10;&#10;        this.membershipMatrix = null;&#10;        this.points = null;&#10;        this.clusters = null;&#10;    }&#10;&#10;    /**&#10;     * Return the number of clusters this instance will use.&#10;     * @return the number of clusters&#10;     */&#10;    public int getK() {&#10;        return k;&#10;    }&#10;&#10;    /**&#10;     * Returns the fuzziness factor used by this instance.&#10;     * @return the fuzziness factor&#10;     */&#10;    public double getFuzziness() {&#10;        return fuzziness;&#10;    }&#10;&#10;    /**&#10;     * Returns the maximum number of iterations this instance will use.&#10;     * @return the maximum number of iterations, or -1 if no maximum is set&#10;     */&#10;    public int getMaxIterations() {&#10;        return maxIterations;&#10;    }&#10;&#10;    /**&#10;     * Returns the convergence criteria used by this instance.&#10;     * @return the convergence criteria&#10;     */&#10;    public double getEpsilon() {&#10;        return epsilon;&#10;    }&#10;&#10;    /**&#10;     * Returns the random generator this instance will use.&#10;     * @return the random generator&#10;     */&#10;    public RandomGenerator getRandomGenerator() {&#10;        return random;&#10;    }&#10;&#10;    /**&#10;     * Returns the {@code nxk} membership matrix, where {@code n} is the number&#10;     * of data points and {@code k} the number of clusters.&#10;     * &amp;lt;p&amp;gt;&#10;     * The element U&amp;lt;sub&amp;gt;i,j&amp;lt;/sub&amp;gt; represents the membership value for data point {@code i}&#10;     * to cluster {@code j}.&#10;     *&#10;     * @return the membership matrix&#10;     * @throws MathIllegalStateException if {@link #cluster(Collection)} has not been called before&#10;     */&#10;    public RealMatrix getMembershipMatrix() {&#10;        if (membershipMatrix == null) {&#10;            throw new MathIllegalStateException();&#10;        }&#10;        return MatrixUtils.createRealMatrix(membershipMatrix);&#10;    }&#10;&#10;    /**&#10;     * Returns an unmodifiable list of the data points used in the last&#10;     * call to {@link #cluster(Collection)}.&#10;     * @return the list of data points, or {@code null} if {@link #cluster(Collection)} has&#10;     *   not been called before.&#10;     */&#10;    public List&amp;lt;T&amp;gt; getDataPoints() {&#10;        return points;&#10;    }&#10;&#10;    /**&#10;     * Returns the list of clusters resulting from the last call to {@link #cluster(Collection)}.&#10;     * @return the list of clusters, or {@code null} if {@link #cluster(Collection)} has&#10;     *   not been called before.&#10;     */&#10;    public List&amp;lt;CentroidCluster&amp;lt;T&amp;gt;&amp;gt; getClusters() {&#10;        return clusters;&#10;    }&#10;&#10;    /**&#10;     * Get the value of the objective function.&#10;     * @return the objective function evaluation as double value&#10;     * @throws MathIllegalStateException if {@link #cluster(Collection)} has not been called before&#10;     */&#10;    public double getObjectiveFunctionValue() {&#10;        if (points == null || clusters == null) {&#10;            throw new MathIllegalStateException();&#10;        }&#10;&#10;        int i = 0;&#10;        double objFunction = 0.0;&#10;        for (final T point : points) {&#10;            int j = 0;&#10;            for (final CentroidCluster&amp;lt;T&amp;gt; cluster : clusters) {&#10;                final double dist = distance(point, cluster.getCenter());&#10;                objFunction += (dist * dist) * FastMath.pow(membershipMatrix[i][j], fuzziness);&#10;                j++;&#10;            }&#10;            i++;&#10;        }&#10;        return objFunction;&#10;    }&#10;&#10;    /**&#10;     * Performs Fuzzy K-Means cluster analysis.&#10;     *&#10;     * @param dataPoints the points to cluster&#10;     * @return the list of clusters&#10;     * @throws MathIllegalArgumentException if the data points are null or the number&#10;     *     of clusters is larger than the number of data points&#10;     */&#10;    @Override&#10;    public List&amp;lt;CentroidCluster&amp;lt;T&amp;gt;&amp;gt; cluster(final Collection&amp;lt;T&amp;gt; dataPoints)&#10;            throws MathIllegalArgumentException {&#10;&#10;        // sanity checks&#10;        MathUtils.checkNotNull(dataPoints);&#10;&#10;        final int size = dataPoints.size();&#10;&#10;        // number of clusters has to be smaller or equal the number of data points&#10;        if (size &amp;lt; k) {&#10;            throw new NumberIsTooSmallException(size, k, false);&#10;        }&#10;&#10;        // copy the input collection to an unmodifiable list with indexed access&#10;        points = Collections.unmodifiableList(new ArrayList&amp;lt;T&amp;gt;(dataPoints));&#10;        clusters = new ArrayList&amp;lt;CentroidCluster&amp;lt;T&amp;gt;&amp;gt;();&#10;        membershipMatrix = new double[size][k];&#10;        final double[][] oldMatrix = new double[size][k];&#10;&#10;        // if no points are provided, return an empty list of clusters&#10;        if (size == 0) {&#10;            return clusters;&#10;        }&#10;&#10;        initializeMembershipMatrix();&#10;&#10;        // there is at least one point&#10;        final int pointDimension = points.get(0).getPoint().length;&#10;        for (int i = 0; i &amp;lt; k; i++) {&#10;            clusters.add(new CentroidCluster&amp;lt;T&amp;gt;(new DoublePoint(new double[pointDimension])));&#10;        }&#10;&#10;        int iteration = 0;&#10;        final int max = (maxIterations &amp;lt; 0) ? Integer.MAX_VALUE : maxIterations;&#10;        double difference = 0.0;&#10;&#10;        do {&#10;            saveMembershipMatrix(oldMatrix);&#10;            updateClusterCenters();&#10;            updateMembershipMatrix();&#10;            difference = calculateMaxMembershipChange(oldMatrix);&#10;        } while (difference &amp;gt; epsilon &amp;amp;&amp;amp; ++iteration &amp;lt; max);&#10;&#10;        return clusters;&#10;    }&#10;&#10;    /**&#10;     * Update the cluster centers.&#10;     */&#10;    private void updateClusterCenters() {&#10;        int j = 0;&#10;        final List&amp;lt;CentroidCluster&amp;lt;T&amp;gt;&amp;gt; newClusters = new ArrayList&amp;lt;CentroidCluster&amp;lt;T&amp;gt;&amp;gt;(k);&#10;        for (final CentroidCluster&amp;lt;T&amp;gt; cluster : clusters) {&#10;            final Clusterable center = cluster.getCenter();&#10;            int i = 0;&#10;            double[] arr = new double[center.getPoint().length];&#10;            double sum = 0.0;&#10;            for (final T point : points) {&#10;                final double u = FastMath.pow(membershipMatrix[i][j], fuzziness);&#10;                final double[] pointArr = point.getPoint();&#10;                for (int idx = 0; idx &amp;lt; arr.length; idx++) {&#10;                    arr[idx] += u * pointArr[idx];&#10;                }&#10;                sum += u;&#10;                i++;&#10;            }&#10;            MathArrays.scaleInPlace(1.0 / sum, arr);&#10;            newClusters.add(new CentroidCluster&amp;lt;T&amp;gt;(new DoublePoint(arr)));&#10;            j++;&#10;        }&#10;        clusters.clear();&#10;        clusters = newClusters;&#10;    }&#10;&#10;    /**&#10;     * Updates the membership matrix and assigns the points to the cluster with&#10;     * the highest membership.&#10;     */&#10;    private void updateMembershipMatrix() {&#10;        for (int i = 0; i &amp;lt; points.size(); i++) {&#10;            final T point = points.get(i);&#10;            double maxMembership = 0.0;&#10;            int newCluster = -1;&#10;            for (int j = 0; j &amp;lt; clusters.size(); j++) {&#10;                System.out.println(&quot;CYCLE&quot;);&#10;                double sum = 0.0;&#10;                final double distA = FastMath.abs(distance(point, clusters.get(j).getCenter()));&#10;&#10;                for (final CentroidCluster&amp;lt;T&amp;gt; c : clusters) {&#10;                    final double distB = FastMath.abs(distance(point, c.getCenter()));&#10;                    System.out.println(distA/distB + &quot; RAISED TO: &quot; + (2.0/(fuzziness - 1.0)));&#10;                    sum += FastMath.pow(distA / distB, 2.0 / (fuzziness - 1.0));&#10;                }&#10;&#10;                membershipMatrix[i][j] = 1.0 / sum;&#10;                System.out.println(&quot;MEM MAT: &quot; + membershipMatrix[i][j] + &quot; FROM: &quot; + sum);&#10;                if (membershipMatrix[i][j] &amp;gt; maxMembership) {&#10;                    maxMembership = membershipMatrix[i][j];&#10;                    newCluster = j;&#10;                }&#10;            }&#10;            System.out.println(&quot;CLUSTER SIZE: &quot; + clusters.size());&#10;            clusters.get(newCluster).addPoint(point);&#10;        }&#10;    }&#10;&#10;    /**&#10;     * Initialize the membership matrix with random values.&#10;     */&#10;    private void initializeMembershipMatrix() {&#10;        for (int i = 0; i &amp;lt; points.size(); i++) {&#10;            for (int j = 0; j &amp;lt; k; j++) {&#10;                membershipMatrix[i][j] = random.nextDouble();&#10;            }&#10;            membershipMatrix[i] = MathArrays.normalizeArray(membershipMatrix[i], 1.0);&#10;        }&#10;    }&#10;&#10;    /**&#10;     * Calculate the maximum element-by-element change of the membership matrix&#10;     * for the current iteration.&#10;     *&#10;     * @param matrix the membership matrix of the previous iteration&#10;     * @return the maximum membership matrix change&#10;     */&#10;    private double calculateMaxMembershipChange(final double[][] matrix) {&#10;        double maxMembership = 0.0;&#10;        for (int i = 0; i &amp;lt; points.size(); i++) {&#10;            for (int j = 0; j &amp;lt; clusters.size(); j++) {&#10;                double v = FastMath.abs(membershipMatrix[i][j] - matrix[i][j]);&#10;                maxMembership = FastMath.max(v, maxMembership);&#10;            }&#10;        }&#10;        return maxMembership;&#10;    }&#10;&#10;    /**&#10;     * Copy the membership matrix into the provided matrix.&#10;     *&#10;     * @param matrix the place to store the membership matrix&#10;     */&#10;    private void saveMembershipMatrix(final double[][] matrix) {&#10;        for (int i = 0; i &amp;lt; points.size(); i++) {&#10;            System.arraycopy(membershipMatrix[i], 0, matrix[i], 0, clusters.size());&#10;        }&#10;    }&#10;&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-03-22T17:21:17.940" Id="90990" LastActivityDate="2014-03-22T17:54:31.483" OwnerUserId="42365" PostTypeId="1" Score="-1" Tags="&lt;machine-learning&gt;&lt;clustering&gt;&lt;data-mining&gt;&lt;java&gt;&lt;code&gt;" Title="Converting FuzzykMeans to SphericalFuzzyKMeans?" ViewCount="27" />
  <row Body="&lt;p&gt;If your graph above represents a probability density, then you can find the density for $X_2 - X_1$ via the convolution integral. Then $P(X_2 &amp;lt;= X_1)$ is equivalent to the probability that $P(X_2 - X_1 &amp;lt;= 0)$, using the density function derived from convolution. If you have discrete distributions, then there's an equivalent process using a sum instead of an integral.&lt;/p&gt;&#10;&#10;&lt;p&gt;For the analytic approach, see if this helps: &lt;a href=&quot;http://courses.washington.edu/bioen316/Assignments/316_SCP.pdf&quot; rel=&quot;nofollow&quot;&gt;http://courses.washington.edu/bioen316/Assignments/316_SCP.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;If you can sample from both, then simulations of $X_2 - X_1$ will give a working approximation. I'm only familiar with how to do that with an inverse CDF, but there's a computational method mentioned here that may interest you: &lt;a href=&quot;http://blog.quantitations.com/tutorial/2012/11/20/sampling-from-an-arbitrary-density/&quot; rel=&quot;nofollow&quot;&gt;http://blog.quantitations.com/tutorial/2012/11/20/sampling-from-an-arbitrary-density/&lt;/a&gt; &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-22T17:34:55.340" Id="90992" LastActivityDate="2014-03-22T17:34:55.340" OwnerUserId="28462" ParentId="90968" PostTypeId="2" Score="1" />
  <row AnswerCount="2" Body="&lt;p&gt;I have some data where I have tested a binomial random variable under 4 conditions. The null hypothesis is that they all have equal means, alternative hypothesis is that one or more means differ from the others.&#10;What kind of test can I use to compare the means of each condition? Sample size for each condition is quite small (under 20), but if it is necessary, then I can get more.&#10;The data look like:&#10;$$\begin{array}{c|cccc}\rm{Condition}&amp;amp;A&amp;amp;B&amp;amp;C&amp;amp;D\\\hline\rm{Successes}&amp;amp;9&amp;amp;8&amp;amp;4&amp;amp;12\\\rm{Fails}&amp;amp;4&amp;amp;4&amp;amp;3&amp;amp;7\end{array}$$&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-03-22T19:57:14.510" Id="90998" LastActivityDate="2014-03-23T22:27:50.640" LastEditDate="2014-03-23T20:39:00.883" LastEditorUserId="32036" OwnerUserId="27271" PostTypeId="1" Score="2" Tags="&lt;hypothesis-testing&gt;&lt;binomial&gt;&lt;group-differences&gt;&lt;small-sample&gt;&lt;type-i-errors&gt;" Title="Comparing different conditions on a binomial distribution" ViewCount="100" />
  <row AnswerCount="1" Body="&lt;p&gt;I have a set of Likert type items asking respondents their attitude on different topics. For example, attitudes toward drugs/alcohol where 1=strongl disagree and 5=strongly agree to the statement &quot;I think it is okay for someone my age to smoke marijuana.&quot; The 6 items in this particular scale are then scored using the mean of non-missing items. Is there some advantage to first rescaling the items so that the range of responses goes from 0-4 rather than 1-5? The only difference is in interpretation of the scale where the average &quot;attitude&quot; is 3.5 on a scale of 0-4 and 4.5 on a scale of 1-5. From an interpretation standpoint, what is easier for a lay audience? Or is there some standard of practice in terms of reporting? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-22T20:33:28.203" Id="91001" LastActivityDate="2014-03-22T20:48:18.907" OwnerUserId="42393" PostTypeId="1" Score="1" Tags="&lt;likert&gt;" Title="Is there an advantage to rescaling a set of Likert items from 1-5 to 0-4?" ViewCount="69" />
  <row Body="&lt;p&gt;I would think that a scale of 1 to 5 would be a more advantageous scale since zero is a non-intuitive number.  Zero has its own connotations such as representing nothing or an absence of the value being measured.  From your example, a scale of 0 to 5 only &quot;sounds&quot; intuitive to me if 0 corresponds to a value which represents &quot;never&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;In addition, whenever you overhear someone say one a scale of ... to ... it is almost always a scale of 1 to 10 or 1 to 5.  I would stick with 1 to 5.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-03-22T20:48:18.907" Id="91003" LastActivityDate="2014-03-22T20:48:18.907" OwnerUserId="35753" ParentId="91001" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="91318" AnswerCount="1" Body="&lt;p&gt;consider the following sorted sample: &lt;/p&gt;&#10;&#10;&lt;p&gt;$x_1&amp;lt;x_2&amp;lt;\ldots&amp;lt;x_n$&lt;/p&gt;&#10;&#10;&lt;p&gt;and the kernel function:&lt;/p&gt;&#10;&#10;&lt;p&gt;$h(j)=\log(x_j-x_1), j&amp;gt;1$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Now, it turns out that when I draw data &#10;from many continuous distributions, sort&#10; them and compute $h(j)$, the plot of the&#10; $h(j)$ as a function of $j$ is concave in &#10; this sense: $h((j+k)/2)&amp;gt;(h(j)+h(k))/2$ &#10;for $1&amp;lt;j&amp;lt;j+1&amp;lt;k&amp;lt;n$ &lt;/p&gt;&#10;&#10;&lt;p&gt;My question is the following: will this &#10;observation (concavity of $h(j)$ as a &#10;function of $j$) be true for all continuous&#10; distributions? If not is there a class &#10;of distributions for which it will always be &#10;true?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance, &lt;/p&gt;&#10;&#10;&lt;p&gt;At this point, given the useful comments from @Glen_b and @Momo,&#10;I think I should give an example.&lt;/p&gt;&#10;&#10;&lt;p&gt;this is a vector of values of $x_j-x_1,j&amp;gt;1$ (where the $x$'s are drawn from &#10;a continuous distribution, but have been rounded here to the third digit &#10;so that they don't take too much place.)&lt;/p&gt;&#10;&#10;&lt;p&gt;My problem, is that when I plot $h(j)$ for this data-set, the result is clearly &#10;nor concave (in the sense I outlined above). &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x=c(1.403, 1.406, 1.408, 1.416, 1.417, 1.42, 1.427, 1.441, 1.448, &#10;1.456, 1.458, 1.465, 1.466, 1.472, 1.472, 1.477, 1.477, 1.479, &#10;1.482, 1.491, 1.5, 1.504, 1.518, 1.52, 1.52, 1.53, 1.544, 1.545, &#10;1.561, 1.573, 1.595, 1.595, 1.599, 1.603, 1.605, 1.612, 1.617, &#10;1.618, 1.618, 1.628, 1.64, 1.644, 1.646, 1.653, 1.679, 1.682, &#10;1.682, 1.693, 1.694, 1.71, 1.71, 1.711, 1.741, 1.75, 1.756, 1.773, &#10;1.794, 1.799, 1.804, 1.808, 1.86, 1.882, 1.895, 1.955, 1.992, &#10;1.995, 2.009, 2.009, 2.063, 2.123, 2.329, 2.356, 2.405, 2.535, &#10;2.632, 2.635, 2.725, 2.763, 2.783)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="10" CreationDate="2014-03-22T22:07:53.687" Id="91008" LastActivityDate="2014-03-30T20:59:19.887" LastEditDate="2014-03-25T18:10:55.047" LastEditorUserId="42397" OwnerUserId="42397" PostTypeId="1" Score="1" Tags="&lt;distributions&gt;&lt;self-study&gt;&lt;mathematical-statistics&gt;&lt;continuous-data&gt;" Title="For what class of distribution is this function concave?" ViewCount="85" />
  
  <row Body="&lt;p&gt;Broadly speaking, the more you understand a test (or indeed any other aspect  of the vrious calculations one does in statistics), the better, but it's not usually necessary to understand every specific detail of each constant in a formula to have a good understanding of what's going on.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, sometimes (particularly when trying to compute the distribution under the null hypothesis), people formulate a Spearman correlation as $ \rho = 1- {\frac {6 \sum d_i^2}{n(n^2 - 1)}}$, where $d_i$ is the difference between the $x_i$ rank and the $y_i$ rank. Is it necessary to understand where the &quot;6&quot; comes from? I don't think it's necessary if you're just trying to get intuition about the test (though in fact it's pretty straightforward) - if you comprehend the test as &quot;a correlation calculated on the ranks&quot;, you pretty much have most of the useful intuition there. There are some additional bits of intuition that can be gleaned from the fact that it can also be written as a linear function of squared rank-differences, but the constants themselves aren't especially enlightening.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can &lt;em&gt;check&lt;/em&gt; the formula easily enough, in several ways (e.g. by computing the value for both the 'correlation of the ranks' form and the '$ \sum d_i^2$' form for some small samples), and you can check the null distribution easily enough (by simple enumeration for very small samples, and by simulation for larger samples), without necessarily knowing how to do the algebra.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'd urge you to do as much as you can - there are few things I've learned along those lines that don't help in some ways - but not to fret over much when you can't. There's always somewhere you can ask about how the details come to be as they are (in the case of the Spearman correlation, some of those details have been explained here already).&lt;/p&gt;&#10;&#10;&lt;p&gt;Basic intuition about t-tests would include, for example, what the denominator is trying to measure (the formulas for different t-statistics look different, but they're always an estimate of the standard deviation of the distribution of the numerator), and also if possible why the different numerators are as they are, and why the denominators are then of the form they have.&lt;/p&gt;&#10;&#10;&lt;p&gt;With F-tests, it's certainly useful to know that the numerator and denominator are two different estimates of variance - which if the null is true should be independence estimates of the same variance. (It's also useful to have some sense of how the distribution changes as the degrees of freedom change.)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-22T23:51:38.330" Id="91012" LastActivityDate="2014-03-22T23:51:38.330" OwnerUserId="805" ParentId="91006" PostTypeId="2" Score="3" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a question about Prop. 9.1 on page 307 in &quot;Probabilistic Graphical Models&quot;&#10;(&lt;a href=&quot;http://books.google.de/books?id=7dzpHCHzNQ4C&amp;amp;printsec=frontcover&amp;amp;dq=koller%20probabilistic&amp;amp;hl=de&amp;amp;sa=X&amp;amp;ei=NiouU9T9N-en4ATN54CYCg&amp;amp;ved=0CDAQ6AEwAA#v=onepage&amp;amp;q=koller%20probabilistic&amp;amp;f=false&quot; rel=&quot;nofollow&quot; title=&quot;Google books link&quot;&gt;link to google books&lt;/a&gt;) (Koller / Friedman). I don't see why $\mathcal{H}_{\Phi}$ is a minimal I-map:&#10;If I have only one factor in $\Phi$ over two Variables $X$ and $Y$ and define it in a way $X$ and $Y$ are independent; then $\mathcal{H}_{\Phi}$ will be a 2-node clique (?). But that's not a minimal Map for the defined distribution (which would be 2 isolated nodes). Would be pleased if anyone could clarify at what point my thinking is wrong.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-23T00:57:08.463" FavoriteCount="0" Id="91016" LastActivityDate="2014-03-23T00:57:08.463" OwnerUserId="42399" PostTypeId="1" Score="0" Tags="&lt;random-variable&gt;&lt;graphical-model&gt;" Title="Minimal I-maps induced by sets of scopes. Clarification needed" ViewCount="14" />
  
  
  
  
  <row Body="&lt;p&gt;I agree with the principle that using more detail, as in looking at the entire distributions or sets of quantiles, would be much better if the data were available. Conversely, converting what you have to quartiles just discards yet more information and is not a good idea here. &lt;/p&gt;&#10;&#10;&lt;p&gt;You are right that side-by-side or back-to-back bar charts are both popular. In the case of age distribution by sex the latter is often called a population pyramid, but it's a very inefficient design for showing differences (or ratios for that matter) of distribution, as it obliges readers to make comparisons between bars pointing in different directions. Surprisingly few texts make this very simple point about the limitations of pyramids. The impression is that using this kind of graph is a custom or ritual passed on between generations. &lt;/p&gt;&#10;&#10;&lt;p&gt;For this kind of age-sex data, the context is that rather small differences or ratios are often of interest and importance, as if say the number of people in the oldest category is 2% or 3%, so you want to be able to see that easily. For any kind of data, indeed, that's a useful feature. &lt;/p&gt;&#10;&#10;&lt;p&gt;A competitive alternative is therefore just a (Cleveland) dot chart. For this example I just guessed roughly at your data from your own displays. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/SyzdP.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Small points of importance: &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Symbols such as o and + tolerate overlap well. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;A dot chart is compatible with e.g. logarithmic scale when that makes sense in a way that a bar chart isn't. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;A variant on this design connects the data points with explicit horizontal line segments or even arrows. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;We have here just two series, but the dot chart could show more. Naturally, the chart would get more crowded and be more difficult to interpret, but that is true of any alternative design as well. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;You accepted the Excel defaults of &quot;Series 1&quot; and &quot;Series 2&quot; and I copied you. It's not your question, but it's still immensely better practice to reach in and use informative text. &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;For another example see &lt;a href=&quot;http://stats.stackexchange.com/questions/89941/how-to-best-visualize-differences-in-many-proportions-across-three-groups/&quot;&gt;How to best visualize differences in many proportions across three groups?&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-23T16:03:46.413" Id="91068" LastActivityDate="2014-03-24T16:18:22.687" LastEditDate="2014-03-24T16:18:22.687" LastEditorUserId="22047" OwnerUserId="22047" ParentId="90948" PostTypeId="2" Score="6" />
  
  <row Body="&lt;p&gt;I think I found an answer. All you need to do in Kernel K means is to compute &#10;&lt;strong&gt;$$&#10;C^{(t+1)}(i) = argmin_k \{K(x_i,x_i) -\frac{2}{N_k}{\Sigma_{l\epsilon C^{t}_k}}K(x_i,x_l) +\frac{1}{N_k^2} {\Sigma_{{l,{l`}}\epsilon C^{t}_k} }K(x_l,x_{l`})\}   ...(1)&#10;$$&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;So this is the only operation that needs to be done. One need not to know each cluster center in high dimensional space. Just compute $(1)$ again and again till the algorithm converges.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Algorithm:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Step 1: Assign Random Cluster to points (Known as clsuter map $ C(i):= \{k: i\rightarrow k\}$ i.e point $i$ is assigned to cluster $k$&lt;/p&gt;&#10;&#10;&lt;p&gt;Step 2: For each point perform $(1)$ above and assign new $C(i)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Just to be more clear at this step: &lt;/p&gt;&#10;&#10;&lt;p&gt;$\rightarrow$After running this step for $(t-1)^{th} iteration $, you get a new $C^{(t)}(i)$ which will be used  in (1) again to calculate $C^{(t+1)}(i)$ &lt;/p&gt;&#10;&#10;&lt;p&gt;$\rightarrow$ So each iteration assigns new $C(i)$.Hence, $C^{(t)}(i)$ keeps on changing ( which is representative of cluster means).  &lt;/p&gt;&#10;&#10;&lt;p&gt;Step 3: Repeat 2 above till the point assignments do not change or any of your error metric is stable.&#10; (I am not sure about the error metric that should be used)&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;New Point:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Each new point will be classified according to $(1)$ above. &lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2014-03-23T17:34:41.757" CreationDate="2014-03-23T17:34:41.757" Id="91076" LastActivityDate="2014-03-23T18:13:14.480" LastEditDate="2014-03-23T18:13:14.480" LastEditorUserId="41146" OwnerUserId="41146" ParentId="91065" PostTypeId="2" Score="0" />
  
  
  <row Body="&lt;p&gt;The size of the population should make no difference at all. But be very careful about whether you're confusing sample and population. The way you use them terms in comments makes me concerned you don't have them straight.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;And hence I am accepting alternative hypothesis&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;You would more properly say 'reject the null' than 'accept the alternative'. You might conclude the alternative is the case, but the decision is simply to reject the null (or to fail to reject it).&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;(i.e two samples are different). &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;That the &lt;em&gt;samples&lt;/em&gt; differ is generally obvious from inspection. You don't need hypothesis tests for that. Hypothesis tests are about populations, and we make inferences about them from the samples. &lt;/p&gt;&#10;&#10;&lt;p&gt;(However, in the case of permutation tests - of which the MWW is an example, the conclusions can be based on the assumption of random allocation, and needn't assume random sampling from populations -- but in the absence of random sampling the extension of the conclusions back to the population of interest may be more difficult, requiring an additional stage of argument.)&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;I just want to ask is it natural to get value 0.0. &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;The term 'natural' doesn't fit here. It's possible - even common - to get a value which is zero to many decimal places, and will show as zero to finite precision (and you shouldn't overly fuss about exactly what the value is in those circumstances).&lt;/p&gt;&#10;&#10;&lt;p&gt;--&lt;/p&gt;&#10;&#10;&lt;p&gt;You should consider editing your question to clarify the circumstances.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-03-23T22:32:41.520" Id="91106" LastActivityDate="2014-03-25T21:44:35.940" LastEditDate="2014-03-25T21:44:35.940" LastEditorUserId="805" OwnerUserId="805" ParentId="91062" PostTypeId="2" Score="4" />
  <row AnswerCount="0" Body="&lt;p&gt;I am working with the survivorship bias free database of hedge funds and trying to estimate the persistence of performance in the future performance of such funds based on the past performance. In this research I follow the approach of Horst and Verbeek, see for example: &lt;/p&gt;&#10;&#10;&lt;p&gt;ter Horst, J.R. &amp;amp; Verbeek, M.J.C.M., 2004. &quot;Fund liquidation, self-selection and look-ahead bias in the hedge fund industry,&quot; ERIM Report Series Research in Management ERS-2004-104-F&amp;amp;A, Erasmus Research Institute of Management (ERIM), ERIM is the joint research institute of the Rotterdam School of Management, Erasmus University and the Erasmus School of Economics (ESE) at Erasmus Uni.&lt;/p&gt;&#10;&#10;&lt;p&gt;In their study the use the method of &quot;selection on observables&quot; as described in &lt;/p&gt;&#10;&#10;&lt;p&gt;John Fitzgerald &amp;amp; Peter Gottschalk &amp;amp; Robert Moffitt, 1998. &quot;An Analysis of the Impact of Sample Attrition on the Second Generation of Respondents in the Michigan Panel Study of Income Dynamics,&quot; Journal of Human Resources, University of Wisconsin Press, vol. 33(2), pages 300-344.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have created my own fund survival model and fitted the linear regression using the weights obtained from this survival model. I was able to replicate the analysis from the paper but now I am wondering how one can use the model for prediction. &lt;/p&gt;&#10;&#10;&lt;p&gt;MAIN QUESTION: Lets say I have two funds $A$ and $B$ and I would like to compare the expected future performance using the models above. If I use the regression model to $A$ and $B$ I get some  numbers $S_A$  and $S_B$ which are the &quot;potential performances&quot; unconditional on the fund survival. I also have the survival probabilities $p_A$ and $p_B$. Do I need to compare $S_A \cdot p_A$ with $S_B \cdot p_B$? What is the usual way of prediction using the censored data?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks, &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-23T23:07:48.003" Id="91109" LastActivityDate="2014-03-23T23:07:48.003" OwnerUserId="29514" PostTypeId="1" Score="0" Tags="&lt;survival&gt;&lt;econometrics&gt;&lt;finance&gt;&lt;censoring&gt;" Title="Censored data prediction" ViewCount="62" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am using the 'topicmodels' package in R. I tested the posteriori probability for &#10;newdata over jss_LDA result by this code : &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;post &amp;lt;- posterior(jss_LDA, newdata = dtmNew)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;For each words i have the probability over all topics. What is the signifiance of each value probability in terms of newdata?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-24T11:10:26.680" Id="91149" LastActivityDate="2014-03-24T11:10:26.680" OwnerUserId="38104" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;text-mining&gt;" Title="Posterior distribution for LDA and Newdata" ViewCount="28" />
  <row Body="&lt;p&gt;For a direct translation you need the obscure package &lt;a href=&quot;http://cran.r-project.org/web/packages/mvtnorm/index.html&quot; rel=&quot;nofollow&quot;&gt;mvtnorm&lt;/a&gt; which provides &lt;code&gt;rmvnorm&lt;/code&gt; (~ &lt;code&gt;mvtrnd&lt;/code&gt;). For the rest of the functions, the mappings are built in, but they are&#10; more terse than Matlab:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;tcdf(T, nu) ~ dt(T, nu)&#10;gaminv(p, Alpha, Beta) ~ qgamma(p, Alpha, shape=Beta) #the keyword argument is important here&#10;tinv(p, nu) ~ qt(p, nu)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And matrix concatenation is&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;cbind(left, right, ...)&#10;rbind(top, bottom, ...)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Why are you using a mutivariate-T in one line (&lt;code&gt;mvtrnd&lt;/code&gt;) and a univariate (&lt;code&gt;tcdf&lt;/code&gt;) in another?&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-03-24T12:36:33.093" Id="91156" LastActivityDate="2014-03-24T12:36:33.093" OwnerUserId="42461" ParentId="91153" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;The &lt;a href=&quot;http://en.wikipedia.org/wiki/Poisson_distribution#Maximum_likelihood&quot;&gt;maximum likelihood estimator&lt;/a&gt; for $\lambda$, given some observations, is just the mean of the observations. So if you do have observations, simply average them.&lt;/p&gt;&#10;&#10;&lt;p&gt;Your probabilities do not add up to one. You can take the weighted mean of (8, 7, 5, 2) with your weights (0,0.01,0.1,1), which yields 2.32. The corresponding probability mass function will look like this (R code underneath):&#10;&lt;img src=&quot;http://i.stack.imgur.com/iDTQX.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lambda &amp;lt;- weighted.mean(c(7,5,2),w=c(0.01,0.1,1))&#10;xx &amp;lt;- seq(0,8,by=1)&#10;foo &amp;lt;- barplot(dpois(xx,lambda))&#10;axis(1,foo[,1],xx)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This does not look much like your plot. You may want to consider overdispersion and try modeling your data using a negative binomial distribution.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-24T13:29:21.240" Id="91161" LastActivityDate="2014-03-24T13:29:21.240" OwnerUserId="1352" ParentId="91158" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;Ok, I finally figured out the intuitive reason or this. Thanks to @juampa for the tip. The thing in Bishop's book that brought home the point for me was figure 3.13. The thing is that  we need to think what happens to the model evidence with relation to model complexity.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, in my example when the regularisation term $\lambda$ is set low, it means that the predictive posterior distribution is going to be really spread out, so it will assign low probability to any particular observation (so the prior will have high variance and so the probabilities will be spread out). Similarly, when $\lambda$ is high, we will have low prior variance and the model won't fit the data well.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hence, the best fit will usually be in some intermediate value which is what $\lambda$ will tend to (unless there is some good reason for $\lambda$ to take extreme values).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-24T14:29:16.833" Id="91167" LastActivityDate="2014-03-24T14:36:49.470" LastEditDate="2014-03-24T14:36:49.470" LastEditorUserId="36540" OwnerUserId="36540" ParentId="90504" PostTypeId="2" Score="0" />
  <row AnswerCount="1" Body="&lt;p&gt;I am trying to use &lt;code&gt;logistic regression&lt;/code&gt; in a scenario where there are very few positives.  I'm aware that maximum likelihood suffers from small sample bias.  So MATLAB's &lt;code&gt;glmfit&lt;/code&gt; doesn't work for me.  I tried using &lt;code&gt;firth&lt;/code&gt; regression in R but it simply hangs up my powerful PC (150,000 observations, 9 dummy variables, 1500 positives).  MATLAB also has the function &lt;code&gt;lassoglm&lt;/code&gt; but I'm not sure if it can be used for logistic regression with few positives.  Can you please confirm/suggest alternative?  &lt;/p&gt;&#10;&#10;&lt;p&gt;Some useful links - &lt;a href=&quot;http://www.statisticalhorizons.com/logistic-regression-for-rare-events&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;http://gking.harvard.edu/files/gking/files/0s.pdf&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-03-24T15:42:37.833" FavoriteCount="3" Id="91174" LastActivityDate="2014-03-26T19:02:58.657" LastEditDate="2014-03-26T19:02:58.657" LastEditorUserId="7290" OwnerUserId="14843" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;logistic&gt;&lt;generalized-linear-model&gt;&lt;lasso&gt;&lt;logit&gt;" Title="Logistic regression using penalized likelihood (lasso?) in Matlab/R" ViewCount="907" />
  <row AcceptedAnswerId="91186" AnswerCount="1" Body="&lt;p&gt;I am going to configure a system for spam detection. What I have is a dataset of labeled (spam/not-spam) strings containing, mostly, sentences.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a background in machine learning techniques, but no background in machine learning applied to text.&lt;/p&gt;&#10;&#10;&lt;p&gt;One approach could be creating vectors of extremely high dimensions, in which the features are boolean and each feature represent one possible word (present/not present).&lt;/p&gt;&#10;&#10;&lt;p&gt;Of course such approach is unsatisfactory due the high dimensionality, but also for the extreme sparsity of one feature across the instances and extreme sparsity across the features.&lt;/p&gt;&#10;&#10;&lt;p&gt;What I am asking is just a few pointers (to tutorials, for example) to simple entry-level techniques that may address the aforementioned shortcomings of boolean per-word encoding.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any ideas on which tools may be more suitable for this task? Maybe RapidMiner?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-03-24T16:17:24.810" Id="91177" LastActivityDate="2014-03-26T17:32:37.983" LastEditDate="2014-03-26T17:32:37.983" LastEditorUserId="25936" OwnerUserId="40719" PostTypeId="1" Score="3" Tags="&lt;machine-learning&gt;&lt;text-mining&gt;&lt;dimensionality-reduction&gt;&lt;naive-bayes&gt;&lt;sparse&gt;" Title="Machine learning techniques for spam detection, and in general for text classification" ViewCount="220" />
  
  
  
  
  <row Body="&lt;p&gt;It seems like you are asking &quot;When does k-means converge to the true set of cluster centroids?&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;The limit on k-means' (or any clustering algorithm's) performance can be analyzed in terms of two factors: 1) separation between the true clusters and 2) the amount of data in the sample. &lt;/p&gt;&#10;&#10;&lt;p&gt;If the true clusters are not well separated or not enough sample data exists, then a clustering algorithm will not be able to converge to the correct result.&lt;/p&gt;&#10;&#10;&lt;p&gt;See &lt;a href=&quot;http://www.cs.nyu.edu/~roweis/papers/SrebroEtalICML06.pdf&quot; rel=&quot;nofollow&quot;&gt;Srebro et. al.&lt;/a&gt; and their references for a more-detailed explanation.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-02-15T17:06:48.463" Id="91195" LastActivityDate="2014-02-15T17:06:48.463" OwnerDisplayName="user1149913" OwnerUserId="9595" ParentId="91193" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Cohens Kappa is known to have limitations for skewed datasets. &lt;/p&gt;&#10;&#10;&lt;p&gt;Quoting an example from &lt;a href=&quot;http://www1.cs.columbia.edu/~julia/courses/CS6998/Interrater_agreement.Kappa_statistic.pdf&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Consider following matrix :&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;+---+----+&#10;| 1 | 6  |&#10;+---+----+&#10;| 9 | 84 |&#10;+---+----+&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The above example has an observed agreement of 0.85 but the Cohen' Kappa is 0.04.&lt;/p&gt;&#10;&#10;&lt;p&gt;The solution suggested in this &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/2189948&quot; rel=&quot;nofollow&quot;&gt;article&lt;/a&gt; is to report two separate agreement metrics for positive and negative classes.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-24T18:28:14.053" Id="91197" LastActivityDate="2014-03-24T18:28:14.053" OwnerUserId="35189" ParentId="69416" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I found the answer to be, to use a Log sigmoid in the output neuron of the model will guaranty a positive output.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-24T18:30:11.737" Id="91198" LastActivityDate="2014-03-24T18:30:11.737" OwnerUserId="35559" ParentId="89786" PostTypeId="2" Score="0" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have characterized cells into 6 categories and want to compare the percent distribution of the different categories at 3 time points. Once I figure that out, I actually have additional treatment groups as well. What is the appropriate statistical test for comparing the frequency distribution between the 3 timepoints? Also, I am not a programmer, but hoping to do this in GraphPad.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-03-24T21:32:36.553" Id="91220" LastActivityDate="2014-03-25T13:59:01.517" LastEditDate="2014-03-25T13:59:01.517" LastEditorUserId="7290" OwnerUserId="42485" PostTypeId="1" Score="1" Tags="&lt;repeated-measures&gt;&lt;multinomial&gt;" Title="How to compare frequency distributions between 3 groups" ViewCount="192" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a time series in which samples were collected approximately monthly over ten years.  I have wanted to identify whether a number of bacterial species were seasonally variable and whether they increased or decreased over time. I had been modelling species abundance with linear regression according to the equation&lt;/p&gt;&#10;&#10;&lt;p&gt;S = a + b*ED +c*DL + d*DDL.&lt;/p&gt;&#10;&#10;&lt;p&gt;Where S is the abundance of a species.&lt;/p&gt;&#10;&#10;&lt;p&gt;ED: Number of days elapsed since the beginning of the study.&lt;/p&gt;&#10;&#10;&lt;p&gt;DL: Day length on the day the samples were collected&lt;/p&gt;&#10;&#10;&lt;p&gt;DDL: The rate of change in day length, measured here as the day length minus &#10;the day length measured thirty days previous.&lt;/p&gt;&#10;&#10;&lt;p&gt;I then looked at the P values associated with each of these coefficients and the coefficients themselves. If ED was statistically significant, I said there was a long term triend, and if ED's coefficient was positive, that species abundance was increasing.&#10;If DL or DDL's coefficients were statistically significant, I said the species' abundance was seasonal. If DL had a statistically significant coefficient, I said the species was most abundant in the summer; if negative, most abundant in the winter.  Same pattern for DDL, positive, most abundant in spring, negative most in fall.&lt;/p&gt;&#10;&#10;&lt;p&gt;A reviewer tells me that &quot;The regression analysis may not be appropriate because day length and day length change are autocorrelated and also covarying.&quot; Is this criticism valid? If valid, is there anything I can salvage from this kind of analysis? Is there a better approach? If not, why isn't it valid? I feel like this approach is analogous to building trigonometric models to describe a data set eg y = a + b*t + c*sin(2pi*t/L) + d*cos(2pi*t/L), which people apply to model some time series. Can I just replace the sign and cosine terms with DL and DDL (which are sinusoidal and DDL is basically the derivative of DL) or am I breaking some rule there?&#10;Thanks!&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-03-24T23:14:02.050" Id="91230" LastActivityDate="2014-03-24T23:14:02.050" OwnerUserId="40294" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;time-series&gt;&lt;seasonality&gt;" Title="Using day length and rate of change of day length in linear regression" ViewCount="47" />
  <row AnswerCount="0" Body="&lt;p&gt;I was wondering what are the steps to perform factoring kriging. I know it is used to extract specific components that may have a physical meaning and this is achieved by kriging. However, I have some questions regarding the specifics of such procedure. By the way, the following description of the problem belongs to Wackernagel's excellent book &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/3540441425&quot; rel=&quot;nofollow&quot;&gt;Multivariate Geostatistics&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose we have a random variable $Z({\bf{x}})$ that can be decomposed into some stationary components $Z_{u}({\bf{x}_{\alpha}})$ and an intrinsic component $Z_{S}({\bf{x_{\alpha}}})$. We are interested in kriging this intrinsic component: $Z_{S}^{*}({\bf{x_{0}}}) = \sum_{\alpha=1}^{n}w_{\alpha}^{S}Z({\bf{x_{\alpha}}})$&lt;/p&gt;&#10;&#10;&lt;p&gt;Therefore, estimating the variance $\sigma_{E}^{2}$ we get:&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{array}{}&#10;\sigma_{E}^{2} &amp;amp;= var(Z_{S}^{*}({\bf{x_{0}}})-Z_{S}({\bf{x_{0}}}))\\&#10;&amp;amp; = \sum_{u=0}^{S-1}C^{u}({\bf{x_0}}-{\bf{x_0}}) - \gamma^{S}({\bf{x_0}}-{\bf{x_0}}) - \sum_{\alpha=1}^{n}\sum_{\beta=1}^{n}w_{\alpha}^{S}w_{\beta}^{S}\gamma({\bf{x_\alpha}}-{\bf{x_\beta}})\\&#10;&amp;amp;+2\sum_{\alpha=1}^{n}w_{\alpha}^{S}\gamma^{S}({\bf{x_\alpha}}-{\bf{x_0}})&#10;\end{array}&lt;/p&gt;&#10;&#10;&lt;p&gt;and minimizing (with respect to the weights) we obtain:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\sum_{\beta=1}^{n} w_{\beta}^{S}\gamma({\bf{x_\alpha}}-{\bf{x_\beta}}) + \mu_{S} = \gamma^{S}({\bf{x_\alpha}}-{\bf{x_0}}) \thinspace \thinspace \text{for $\alpha$=1,...n}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\sum_{\beta=1}^{n}w_{\beta}^{S}=1$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\mu_{S}$ is the Lagrange multiplier due to the constraint on the weights. &lt;/p&gt;&#10;&#10;&lt;p&gt;As I understand it, the idea usually is to get the weights $w_{\beta}$ that minimize the variance $\sigma_{E}^{2}$ in order to estimate new values ${\bf{x_{0}}}$. In this case, the values to be estimated are the intrinsic components $Z_{S}^{*}({\bf{x_{0}}})$. However, I'm not sure how to fit from data the variograms needed for these equations.&lt;/p&gt;&#10;&#10;&lt;p&gt;Furthermore, in factoring kriging the variogram or covariance function is decomposed into several variograms that correspond to the components of $Z({\bf{x}})$. Assuming I know the variograms, should I use something like $\gamma_{1}({\bf{x_\alpha}}-{\bf{x_\beta}}) + \gamma_{2}({\bf{x_\alpha}}-{\bf{x_\beta}}) ... $ in place of $\gamma({\bf{x_\alpha}}-{\bf{x_\beta}})$ in the kriging equations?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-03-24T23:43:20.693" Id="91234" LastActivityDate="2014-03-25T23:49:02.800" LastEditDate="2014-03-25T23:49:02.800" LastEditorUserId="2676" OwnerUserId="2676" PostTypeId="1" Score="1" Tags="&lt;variogram&gt;&lt;geostatistics&gt;&lt;kriging&gt;" Title="Details on kriging and variograms" ViewCount="49" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to estimate a multinomial logit model using an offset variable with the mlogit package of R&lt;/p&gt;&#10;&#10;&lt;p&gt;Using the syntax here [https://stat.ethz.ch/R-manual/R-devel/library/stats/html/formula.html] it seems that the correct way to do this is to use something like this&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;v &amp;lt;- mFormula(choice ~ gc + log_gc + offset(log_attr) + 0)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;which you would then pass to the model estimation as &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;model &amp;lt;- mlogit(v, mlogit_data)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Printing the coefficients seems to indicate that the parameter has not been estimated&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;model$coefficients&#10;          gc       log_gc &#10;-0.003023088 -0.477115780 &#10;attr(,&quot;fixed&quot;)&#10;    gc log_gc &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;However these parameters are exactly the same as the parameters I get if I don't include the variable log_attr at all... ie &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;v_no_log_attr &amp;lt;- mFormula(choice ~ gc + log_gc + 0)&#10;model_no_log_attr &amp;lt;- mlogit(v_no_log_attr, mlogit_data)&#10;model_no_log_attr$coefficients&#10;          gc       log_gc &#10;-0.003023088 -0.477115780 &#10;attr(,&quot;fixed&quot;)&#10;    gc log_gc &#10; FALSE  FALSE &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Given that the data has significant variation in this variable, I feel that this is unexpected and that the mlogit package is simply ignoring the offset variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/thCvR.png&quot; alt=&quot;variation_in_data&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Is this possible to do with this package? Do I have a misunderstanding of how to perform the estimation?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-03-25T08:19:10.977" Id="91258" LastActivityDate="2014-08-03T10:05:15.873" OwnerUserId="42502" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;multinomial&gt;&lt;logit&gt;&lt;offset&gt;" Title="How to estimate an mlogit (R package) model with a fixed (offset) variable" ViewCount="161" />
  <row AnswerCount="1" Body="&lt;p&gt;How do the principal components from PCA change on addition of new data, i.e. $$\frac{d(\mathrm{PC}_1(x))}{dx} = ?$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I am looking for any mathematical formulas and proofs (since it would be easy to understand).&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-03-25T10:10:32.813" FavoriteCount="1" Id="91266" LastActivityDate="2014-12-29T14:17:41.590" LastEditDate="2014-12-29T14:17:41.590" LastEditorUserId="28666" OwnerUserId="41635" PostTypeId="1" Score="3" Tags="&lt;pca&gt;" Title="How do PCA components change upon the addition of new data?" ViewCount="91" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I am looking for a &lt;code&gt;JavaScript&lt;/code&gt; or a &lt;code&gt;PHP&lt;/code&gt; library to calculate common business/statistical metrics (e.g. &lt;code&gt;Total&lt;/code&gt;, &lt;code&gt;Avg&lt;/code&gt;, &lt;code&gt;YTD&lt;/code&gt;, &lt;code&gt;YoY&lt;/code&gt;, &lt;code&gt;MoM&lt;/code&gt;, &lt;code&gt;CAGR&lt;/code&gt; growth, standard deviation, regression line coordinates) from simple arrays/objects with a structure such as:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/hhSyz.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The idea is that I would just provide the arrays with some basic parameters (e.g. time period I'm interested in) and would get all the numbers crunched automatically in return.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Can anybody point me to a library they've used and trust from experience?&lt;/strong&gt;&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2014-07-01T13:11:47.043" CreationDate="2014-03-25T14:28:44.863" Id="91285" LastActivityDate="2014-07-01T12:20:25.797" OwnerUserId="24414" PostTypeId="1" Score="0" Tags="&lt;code&gt;&lt;library&gt;&lt;javascript&gt;&lt;php&gt;" Title="JavaScript/PHP libraries for calculating common" ViewCount="47" />
  
  
  
  <row Body="&lt;p&gt;It's complicated, because the addition of one more data row can completely change all principal components and their eigenvalues.  However, we can do some analysis that sheds light on what is happening and allows for some general conclusions as well as providing a tool to understand any particular circumstance.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;I am relying on the text of the question for its interpretation because the mathematical expression literally makes no sense.  Here is what I make of it.  Suppose the data are arranged, as usual, in an $n$ by $p$ array $\mathbb{X}$ with rows representing cases and columns representing variables (optionally standardized and usually centered--but for simplicity of exposition I will ignore those effects and implicitly assume that the new data row will not materially change the means or variances of any of the variables).  PCA is essentially the analysis of the eigensystem of the symmetric non-negative definite $p\times p$ matrix $\mathbb{A}=\mathbb{X}^\prime\mathbb{X}.$  Adjoining a new row $v$ to $\mathbb{X}$ changes $\mathbb{A}$ into $\mathbb{A}+v^\prime v.$&lt;/p&gt;&#10;&#10;&lt;p&gt;To understand the change induced by the new row, begin with the PCA solution for the original data.  It consists of an orthogonal basis of unit-length eigenvectors $(e_i)$ and their associated eigenvalues $\lambda_i$ in descending order so that $\lambda_1\ge\lambda_2\ge\cdots\ge\lambda_p\ge 0$.  In this basis $\mathbb{A}$ is diagonal with the values $\lambda_i$ on its main diagonal (and zeros off the diagonal).  Let $u$ be the vector $v$ expressed in this basis.  (That is, $u = u_1e_1+u_2e_2+\cdots+u_pe_p$ where $u_i = v\cdot e_i.$)&lt;/p&gt;&#10;&#10;&lt;p&gt;By means of row-reduction (or otherwise, depending on your tastes) it is possible to show that when $\mathbb{Y}$ is any diagonal matrix with values $y_i$ on the diagonal, then&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\det(\mathbb{Y}+u^\prime u) = y_1y_2\cdots y_p\left(1 + \frac{u_1^2}{y_1} + \frac{u_2^2}{y_2} + \cdots + \frac{u_p^2}{y_p}\right).$$&lt;/p&gt;&#10;&#10;&lt;p&gt;(Because we have to contemplate the possibility of zero values for some of the $y_p$, this expression must be understood in the sense that $y_1y_2\cdots y_p / y_i = y_1\cdots y_{i-1}y_{i+1}\cdots y_p$.)&lt;/p&gt;&#10;&#10;&lt;p&gt;To find the new eigenvalues we must, by definition, solve the equation&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\det(\mathbb{A} + u^\prime u - \lambda) = 0$$&lt;/p&gt;&#10;&#10;&lt;p&gt;for the eigenvalues $\lambda$.  Setting $\mathbb{Y}=\mathbb{A}-\lambda$ (which is still a diagonal matrix) and plugging it into the preceding formula gives the &lt;em&gt;polynomial&lt;/em&gt; equation&lt;/p&gt;&#10;&#10;&lt;p&gt;$$p_{\mathbb{A},u}(\lambda) = (\lambda_1-\lambda)(\lambda_2-\lambda)\cdots (\lambda_p-\lambda)\left(1 + \sum_{i=1}^p\frac{u_i^2}{(\lambda_i-\lambda)}\right) = 0.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;We can begin to draw some general conclusions:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;If $u_j=0$--that is, $u$ is orthogonal to the eigenvector $e_j$--then $\lambda_j$ remains an eigenvalue.  (This is geometrically obvious; it is equally obvious that $e_j$ remains the associated eigenvector.)&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;When all the $u_j$ are sufficiently small, the eigenvalues will not change a lot.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;We should therefore be interested in how rapidly the eigenvalues will change with small perturbations $(u_j)$.  The rate at which $\lambda_k$ changes can be found by differentiating the characteristic polynomial $p_{\mathbb{A},u}(\lambda)$ with respect to $\lambda$ and evaluating it at $\lambda_k$.  I obtain&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\eqalign{&#10;\frac{\partial p_{\mathbb{A},u}(\lambda)}{\partial\lambda}\left(\lambda_k\right) &#10;&amp;amp;= (\lambda_1-\lambda_k)\cdots(\lambda_{k-1}-\lambda_k)(\lambda_{k+1}-\lambda_k)\cdots(\lambda_p-\lambda_k)\\&#10;&amp;amp;\sum_j \left(1 + \sum_{j\ne k}\frac{u_j^2}{\lambda_j-\lambda_k}\right).&#10;}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The derivative is infinite when $u_j\ne 0$ and $\lambda_j$ has multiplicity greater than $1$.  Otherwise, its size is determined by the magnitudes of the $u_j^2$ &lt;em&gt;relative to the gaps&lt;/em&gt; $\lambda_j-\lambda_k$ in the sequence of eigenvalues.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;There is much of a semi-quantitative nature we can glean from the last observation&lt;/strong&gt;. For instance, it is evident that when the new row $u$ has a substantial component along a small principal direction, it will effect dramatic alterations in the smallest eigenvalues and should be expected completely to change the smallest principal directions.  In order to perturb one of the &lt;em&gt;largest&lt;/em&gt; principal directions, though, $u$ must have a substantial component in a direction whose eigenvalue is of a comparable size.&lt;/p&gt;&#10;&#10;&lt;p&gt;One could go further to develop expressions for how the eigenvectors change as a function of the $(u_i)$, but they get sufficiently messy that it seems best to stop the exposition here and let interested readers carry out the calculations themselves, using the techniques illustrated here.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-03-25T17:14:41.657" Id="91310" LastActivityDate="2014-03-25T17:14:41.657" OwnerUserId="919" ParentId="91266" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;Here are the steps that I would take (and that I teach to my students):&lt;/p&gt;&#10;&#10;&lt;p&gt;1) Contact the authors of the original research. Be polite and request exact effect estimates to use in your meta-analysis. Worst thing that can happen is that they don't reply or refuse to give you the information. Best case scenario is you get the exact information you were looking for.&lt;/p&gt;&#10;&#10;&lt;p&gt;2) If you have exact p-values, you can often back calculate SD's with some degree of certainty.&lt;/p&gt;&#10;&#10;&lt;p&gt;3) You make some sort of imputation. This could be using 'borrowing' the effect estimate from similar sized trials, largest SD in the meta-analysis, SD from similar studies in the same meta-analysis, expert opinion, etc. There are many ways to impute the missing data, some more scientifically correct than others, but the most important thing is that you are crystal clear about what you did and to conduct a sensitivity analysis to determine the effect of the imputation(s) on the pooled effect estimate.&lt;/p&gt;&#10;&#10;&lt;p&gt;3) You put them the studies into the meta-analysis with the missing data. The program (e.g. RevMan) will not give these studies any weight in the analysis because it won't be able to calculate the effect estimate and variance for that study, but you will be able to visually show that there were additional studies with partial data that were not part of the pooled calculation.&lt;/p&gt;&#10;&#10;&lt;p&gt;4) You don't include data from these studies.&lt;/p&gt;&#10;&#10;&lt;p&gt;Pick you poison... &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-25T17:45:54.090" Id="91313" LastActivityDate="2014-03-25T17:45:54.090" OwnerUserId="24137" ParentId="91226" PostTypeId="2" Score="1" />
  
  <row AnswerCount="1" Body="&lt;p&gt;In a paper I came across the description of $R^2$ as &quot;it estimates the combined dispersion against the single dispersion of the observed and predicted series&quot;. I am not able to understand this statement. I understand that $R^2$ is the square of the correlation coefficient of predicted and observed values.&lt;/p&gt;&#10;&#10;&lt;p&gt;Please help! Thanks in advance. :)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-25T18:37:28.660" Id="91320" LastActivityDate="2014-03-25T23:23:48.240" LastEditDate="2014-03-25T19:00:44.710" LastEditorUserId="21599" OwnerUserId="42541" PostTypeId="1" Score="0" Tags="&lt;mathematical-statistics&gt;&lt;goodness-of-fit&gt;&lt;regression-coefficients&gt;" Title="What does &quot;$R^2$ estimates the combined dispersion against the single dispersion of the observed and predicted series&quot; mean?" ViewCount="27" />
  
  <row Body="&lt;p&gt;I think that the best solutions is to use R in combination with R Studio.  ( Python with the iPython notebook is another alternative, but not quite as easy or functional as R. )  &lt;/p&gt;&#10;&#10;&lt;p&gt;What works well for me is to use RStudio's.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are numerous ways to import excel (tabular) data @Nick Stauner provides perhaps the easiest solution using &lt;code&gt;read.csv&lt;/code&gt;; the limitation is that this requires the additional step of saving a worksheet as a CSV file.  This is not great if your data is spread across multiple sheets.  It can get tedious though there are VBA programs for saving all sheets as CSV files. Google for them. Another limitation is getting the types of the variables correct.  If you use &lt;code&gt;read.csv&lt;/code&gt;, you often have to fix your types after importing in R.  &lt;/p&gt;&#10;&#10;&lt;p&gt;There are a few packages that avoid these problems by allowing you to connect read/write  from the spreadsheet directly or by using ODBC.  Search on CRAN for &lt;em&gt;excel&lt;/em&gt; or &lt;em&gt;odbc&lt;/em&gt; to find the relevant one for your situation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Interms of getting plots into powerpoint, use Rstudio's export plot functions.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;export plot&lt;/code&gt; &gt; &lt;code&gt;copy plot to clipboard&lt;/code&gt; &gt; &lt;code&gt;copy as:  metafile&lt;/code&gt; captures the plot to the the paste buffer allowing you to paste directly into Power Point.&lt;/p&gt;&#10;&#10;&lt;p&gt;As far as plotting options, R has numerous.  The aforementioned &lt;em&gt;ggplot2&lt;/em&gt; package provides a very powerful interface for creating all sorts of plots.  There are additional packages for doing hundreds or thousands of other types of plots/animations/etc.  One limitation is that these are often buried in &lt;a href=&quot;http://cran.r-project.org&quot;&gt;CRAN&lt;/a&gt; packages&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-25T19:03:58.487" Id="91325" LastActivityDate="2014-03-25T19:03:58.487" OwnerUserId="42543" ParentId="91253" PostTypeId="2" Score="6" />
  <row AcceptedAnswerId="91342" AnswerCount="2" Body="&lt;p&gt;Say I have the following obvious Bayesian computation:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;p(\theta|D) = \frac{p(\theta) \cdot p(D|\theta)}{p(D)}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\theta$ is a model parameter that we try to infer and $D$ is &lt;strong&gt;observed data&lt;/strong&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have always understood $p(D)$ to relate to knowledge that we have about the data $D$, but this concept has always been somewhat abstract. Many texts for example, simply ignore  $p(D)$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Moreover, quite often, $p(D)$ is referred to as a &lt;strong&gt;normalizing constant&lt;/strong&gt;. But if this is the case, &lt;strong&gt;why write $p(D)$&lt;/strong&gt;? Is it &lt;strong&gt;always&lt;/strong&gt; the case that $p(D)$ is a constant? &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-03-25T21:26:20.890" FavoriteCount="1" Id="91338" LastActivityDate="2015-01-27T15:03:06.180" LastEditDate="2015-01-27T15:03:06.180" LastEditorUserId="62549" OwnerUserId="2798" PostTypeId="1" Score="5" Tags="&lt;bayesian&gt;&lt;modeling&gt;" Title="$p(D)$ in Bayesian Statistics" ViewCount="126" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;Description of the problem:&#10;The effect of a certain treatment is tested as follows: For n subjects, the endpoint is measured three times before and after the treatment is administered, respectively. The endpoint is modeled as an ordinal variable with outcomes A,B,C and order $A&amp;lt;B&amp;lt;C$. &lt;/p&gt;&#10;&#10;&lt;p&gt;There are two questions to be answered: 1) How precise is the method measuring the endpoint? 2) Is the treatment effective?&lt;/p&gt;&#10;&#10;&lt;p&gt;Statistical description of the problem:&#10;We consider a random variable $Y_{itk}$ with $i=1,\ldots,n$ the subject index, $t=1,2$ the time point index, and $k=1,2,3$ the index of the repeated measurements. The random variable $Y_{itk}$ has the three outcome possibilities A,B,C which can be ordered as mentioned above. Also, the random variables are independent in $i$ but, of course, neither in $t$ nor in $k$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Regarding the two questions:&lt;/p&gt;&#10;&#10;&lt;p&gt;1) What would be a good precision measure? I thought about the probability that a) all three results are the same, b) two are the same, c) all three measurements are different. For example the first probability could be estimated as follows: $\frac{1}{2n}\sum_{i,t}\mathbf{1}_{\{Y_{it1}=Y_{it2}=Y_{it3}\}}$. However, do I need to adjust of the dependence of the measurements at times $t=1$ and $t=2$?&#10;What other precision measure could be considered?&lt;/p&gt;&#10;&#10;&lt;p&gt;2) Concerning the comparison of the endpoint between the two time&#10;    points, I have several ideas.&lt;/p&gt;&#10;&#10;&lt;p&gt;2.1) Defining a ranking of the outcomes of $(Y_{it1},Y_{it2},Y_{it3})$ and comparing the the distribution of the ranks for both time points. The things that bothers me is that&#10;    the ranking system is up to a certain degree arbitrary and the&#10;    results might depend on the ranking system. &lt;/p&gt;&#10;&#10;&lt;p&gt;2.2) Taking the median of the outcomes $\{Y_{it1},Y_{it2},Y_{it3}\}$. In the end, one could compare the distributions of the medians at times $t=1$ and $t=2$.&lt;/p&gt;&#10;&#10;&lt;p&gt;2.3) Multinomial regression with a random predictor. Eventually, the sample size $(n=10)$ might be to small for a fancy model. &lt;/p&gt;&#10;&#10;&lt;p&gt;Any advice is highly appreciated. Thanks!&lt;/p&gt;&#10;&#10;&lt;p&gt;By the way, a similar question has already been asked &lt;a href=&quot;http://stats.stackexchange.com/questions/80580/before-after-measurement-and-ordinal-longitudinal-analysis&quot;&gt;here&lt;/a&gt;. An answer hasn't been given though.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-25T22:54:54.130" Id="91350" LastActivityDate="2014-08-07T12:58:11.573" OwnerUserId="31675" PostTypeId="1" Score="0" Tags="&lt;repeated-measures&gt;&lt;ordinal&gt;&lt;longitudinal&gt;" Title="Repeatedly measured, longitudinal and ordinal data" ViewCount="76" />
  <row Body="&lt;p&gt;the probability for one trial with n data points is 1/n.&#10;hence the equation looks like (1-1/n)**n&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-06T03:55:17.763" Id="91355" LastActivityDate="2014-03-06T03:55:17.763" OwnerDisplayName="helper" ParentId="91353" PostTypeId="2" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;How can I find out if the log-likelihood function has only one global maximum or if it has multiple local maxima?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-03-25T23:41:14.187" Id="91360" LastActivityDate="2014-03-27T01:46:03.620" LastEditDate="2014-03-27T01:46:03.620" LastEditorUserId="7290" OwnerUserId="11691" PostTypeId="1" Score="0" Tags="&lt;maximum-likelihood&gt;&lt;likelihood-function&gt;&lt;mode&gt;" Title="Unique or multiple maxima of log-likelihood function?" ViewCount="35" />
  <row AcceptedAnswerId="91362" AnswerCount="1" Body="&lt;p&gt;I am currently fitting models that are intended to be used for extrapolating from a limited sample to a large population. For a specific example, one model is predicting water temperature in rivers based on attributes of the river. The sample includes data from approximately 1,000 river segments (a river segment is a unique length of a river), which will be used to extrapolate to over 100,000 river segments. This is a neural network model, but I have the same question for a different model using hierarchical logistic regression models, so the question is not statistical method specific.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I have read and have found through experience that normalizing my modeling dataset can improve model fit. However, I am unsure in this instance because the goal is extrapolating to the population. Should the predictors be normalized [(y - mean)/stdev] based on their mean and sdev in the sample or in the population of river segments? &lt;/p&gt;&#10;&#10;&lt;p&gt;I have taken the opinion that I should normalize based on the population to help ensure that the population variability is represented adequately in the modeling dataset. &lt;/p&gt;&#10;&#10;&lt;p&gt;I apologize if there is a cross posting, as I have searched for this answer thoroughly and have not seen a similar question or discussion anywhere. So I am asking this here. Please point me to an answer if one exists elsewhere. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-03-19T16:14:28.580" Id="91361" LastActivityDate="2014-03-26T00:19:46.657" OwnerDisplayName="user2532704" PostTypeId="1" Score="2" Tags="&lt;normalization&gt;&lt;modeling&gt;" Title="normalizing dataset for extrapolation - sample or population mean and standard deviation?" ViewCount="214" />
  <row Body="&lt;p&gt;I would look into Business Intelligence tools... where similar issues arise.  In particular (data warehouses, dimensional analysis,) hierarchies and drill downs.&lt;/p&gt;&#10;&#10;&lt;p&gt;The basic idea is that you try to represent your underlying data as aggregatable quantities ( counts, earnings etc  rather than eg percentages). Then you design hierarchies to aggregate over the details ( eg months/ weeks/...)  . This allows you to have simple overviews of all your data and then zoom in on particular areas. see eg &lt;a href=&quot;http://cubes.databrewery.org/&quot; rel=&quot;nofollow&quot;&gt;http://cubes.databrewery.org/&lt;/a&gt; (python)  or excel power pivot&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2014-03-26T01:08:11.503" CreationDate="2014-03-26T01:08:11.503" Id="91367" LastActivityDate="2014-03-26T01:08:11.503" OwnerUserId="27556" ParentId="89678" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;for any random variable $\xi$, we can introduce a new variable $x=\xi+const$ so that&#10;$E[x]=E[\xi+const]=E[\xi]+const$&lt;/p&gt;&#10;&#10;&lt;p&gt;$Var[x]=Var[\xi+const]=Var[\xi]$&lt;/p&gt;&#10;&#10;&lt;p&gt;choosing $const=Var[\xi]-E[\xi]$ we can make $E[x]=Var[x]$. &lt;/p&gt;&#10;&#10;&lt;p&gt;So, you can make an example of your own using this method. There's no significance of this fact. It says that by moving the variable's mean you can make it equal to anything including its variance&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-03-26T04:30:28.150" Id="91390" LastActivityDate="2014-03-26T04:30:28.150" OwnerUserId="36041" ParentId="26815" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Not really an answer, but to know more about this I would take a look at the references given for the &lt;code&gt;pmvnorm&lt;/code&gt; function of the &lt;a href=&quot;http://essrc.hyogo-u.ac.jp/cran/web/packages/mvtnorm/mvtnorm.pdf&quot; rel=&quot;nofollow&quot;&gt;&lt;code&gt;mvtnorm&lt;/code&gt; R package&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-26T08:23:38.593" Id="91407" LastActivityDate="2014-03-26T08:23:38.593" OwnerUserId="8402" ParentId="91397" PostTypeId="2" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a model that predicts categorical response using the conditional trees, ctree() function in R package &quot;party&quot;. I am able to get predicted responses on the new data set after training the model on the training set.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is about communicating the prediction algorithm. How can I obtain the prediction algorithm from ctree() in an way so that someone else can use it to predict responses on a new data set. For example, in the case of the linear regression, I would report the coefficients for the predictors. I am not sure how to do it for conditional trees.&lt;/p&gt;&#10;&#10;&lt;p&gt;My model has 5 ordinal responses, and about 20 ordinal predictors.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance,&#10;Shamil.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-26T12:18:21.107" Id="91427" LastActivityDate="2014-03-26T12:18:21.107" OwnerUserId="41250" PostTypeId="1" Score="1" Tags="&lt;prediction&gt;" Title="Prediction algorithm in ctree() function in R party package" ViewCount="199" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I have two samples A and B.&#10;A contains 100 elements while B has 150 elements.&#10;I want to know whether Kolmogorov-Smirnov test can be performed to compare these two samples A and B of different size?&lt;/p&gt;&#10;&#10;&lt;p&gt;Kindly reply.&lt;/p&gt;&#10;&#10;&lt;p&gt;With Regards&#10;Puglu&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-26T13:54:35.250" Id="91436" LastActivityDate="2014-03-28T14:18:35.663" OwnerUserId="42588" PostTypeId="1" Score="0" Tags="&lt;hypothesis-testing&gt;" Title="kolmogorov smirnov test for two sample of different size" ViewCount="82" />
  <row AnswerCount="0" Body="&lt;p&gt;I am a beginner in this area so please bear with me. I have 2 data sets:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;information about the cars which were sold in&#10;particular area (cost, model, number of seats, manufacturer, etc.)&lt;/li&gt;&#10;&lt;li&gt;information about social status of people in particular&#10;area (occupation position, average salary, education, health &#10;insurance, etc.)&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Is there some math that can show me some information e.g. how is the model of car related to occupation position of person (I am assuming that richer people buy more expensive cars)?&#10;Or even more crazy idea of how is the social status of person related to number of family members (I am assuming that people who buy cars with more seats (van etc.) have larger families)?&lt;/p&gt;&#10;&#10;&lt;p&gt;Is this even possible with statistics/machine learning/data mining? In database world I would simply use selects and joins and get desired information, but what do do in this area? Can you point me to some resources with examples? &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-03-26T14:23:55.210" FavoriteCount="0" Id="91440" LastActivityDate="2014-03-26T14:41:26.490" LastEditDate="2014-03-26T14:41:26.490" LastEditorUserId="22047" OwnerUserId="42589" PostTypeId="1" Score="0" Tags="&lt;dataset&gt;&lt;combination&gt;" Title="Extract knowledge from the (possible unrelated) combination of multiple data sets" ViewCount="40" />
  
  <row AcceptedAnswerId="92870" AnswerCount="1" Body="&lt;p&gt;I like to keep analyses all in SAS or all in R when I can help it and lately have been using R more and more, but there's one analysis that I do somewhat routinely that has given me trouble in R.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have repeated measures data where I would like to fit the following model: $$Delta = Day + Group + Day\times Group$$ where $Delta$ is the change from baseline, $Day$ is the number of days from the beginning of the study, and $Group$ is the experimental group.  I fit a variance-covariance matrix to account for repeated measures (for this example I'm using compound symmetry, but the difference is the same using other variance-covariance matrices).  I have the data at the end of the post.&lt;/p&gt;&#10;&#10;&lt;p&gt;If I don't include the interaction, I can get the analysis to run as I want it to in both SAS and R.  In SAS:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;proc mixed data=df;&#10;  class group day id;&#10;  model delta = day group;&#10;  repeated day / subject=id type=cs;&#10;  lsmeans group / diff=all;&#10;run;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In R:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(nlme)&#10;library(lsmeans)&#10;fit.cs &amp;lt;- gls(Delta~Day+Group,&#10;              data=df,&#10;              corr=corCompSymm(,form=~1|ID))&#10;anova(fit.cs,type=&quot;marginal&quot;)&#10;lsmeans(fit.cs,pairwise~Group)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Obviously the results differ in terms of denominator DF, but I don't intend to start that discussion (unless that difference is causing the problem).  When I add interaction in SAS, everything is fine:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;proc mixed data=df;&#10;  class group day id;&#10;  model delta = day | group;&#10;  repeated day / subject=id type=cs;&#10;run;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;But when I do the same from R...&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;fit.cs &amp;lt;- gls(Delta~Day*Group,&#10;              data=df,&#10;              corr=corCompSymm(,form=~1|ID))&#10;# Error in glsEstimate(object, control = control) : &#10;#   computed &quot;gls&quot; fit is singular, rank 19&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Why does R complain about the fit being singular but SAS doesn't? &lt;/p&gt;&#10;&#10;&lt;p&gt;Here are some fake data that are representative of data that I work with (from R):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;df &amp;lt;- structure(list(Delta = c(-1.27, -0.34, 1.92, 0.45, 1.21, 0.43, -0.41, 0.16, -0.35,&#10;1.49, -0.85, -0.86, 1.04, 0.49, 2.32, 0.13, -0.32, 0.5, 0.48, 1.21, -0.82, 0.93,&#10;-0.58, 2.3, -0.9, 0.21, -0.72, 0.11, -0.28, -0.33, -0.7, -1.16, -0.23, -0.88, 0.97,&#10;0.25, 0.8, 0.16, 0.63, -0.49, -0.63, -0.9, 1.1, -1.45, 0.38, -0.93, 0.4, 0.45, 0.48,&#10;0.14, 1.02, -0.01, -1.98, 2.19, -1.53, -0.49, -1.57, -1.02, 1.09, 1.74, 0.54, -1.57,&#10;-1.5, -0.48, 0.26, 0.2, -0.36, -1.05, -1.73, -0.77, -0.65, -1.07, -0.45, -0.14,&#10;-0.56, 0.84, -2.66, -0.52, 1.44, 0.45, 0.24, -0.92), Day = structure(c(1L, 2L, 3L,&#10;1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 6L, 7L, 1L, 2L, 3L, 6L, 7L, 1L, 2L, 3L, 6L, 7L,&#10;1L, 2L, 3L, 6L, 7L, 1L, 2L, 3L, 6L, 1L, 2L, 3L, 6L, 7L, 1L, 2L, 3L, 6L, 7L, 1L, 2L,&#10;3L, 6L, 7L, 1L, 2L, 3L, 6L, 7L, 1L, 2L, 3L, 6L, 7L, 1L, 2L, 3L, 6L, 1L, 2L, 3L, 6L,&#10;7L, 1L, 2L, 3L, 6L, 7L, 1L, 2L, 3L, 6L, 7L, 1L, 2L, 3L, 6L, 7L), .Label = c(&quot;4&quot;,&#10;&quot;7&quot;, &quot;10&quot;, &quot;12&quot;, &quot;14&quot;, &quot;16&quot;, &quot;28&quot;), class = &quot;factor&quot;), Group = structure(c(1L, 1L, 1L,&#10;1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,&#10;2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,&#10;3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,&#10;4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L), .Label = c(&quot;1&quot;, &quot;2&quot;,&#10;&quot;3&quot;, &quot;4&quot;), class = &quot;factor&quot;), ID = structure(c(1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L,&#10;4L, 4L, 4L, 4L, 4L, 5L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L, 7L, 7L, 8L,&#10;8L, 8L, 8L, 9L, 9L, 9L, 9L, 9L, 10L, 10L, 10L, 10L, 10L, 11L, 11L, 11L, 11L, 11L,&#10;12L, 12L, 12L, 12L, 12L, 13L, 13L, 13L, 13L, 13L, 14L, 14L, 14L, 14L, 15L, 15L, 15L,&#10;15L, 15L, 16L, 16L, 16L, 16L, 16L, 17L, 17L, 17L, 17L, 17L, 18L, 18L, 18L, 18L, 18L),&#10;.Label = c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;, &quot;6&quot;, &quot;7&quot;, &quot;8&quot;, &quot;9&quot;, &quot;10&quot;, &quot;11&quot;, &quot;12&quot;, &quot;13&quot;, &quot;14&quot;,&#10;&quot;15&quot;, &quot;16&quot;, &quot;17&quot;, &quot;18&quot;), class = &quot;factor&quot;)), .Names = c(&quot;Delta&quot;, &quot;Day&quot;, &quot;Group&quot;, &quot;ID&quot;),&#10;class = &quot;data.frame&quot;, row.names = c(NA, -82L))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Here's the same data for SAS:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;DATA  df;&#10;INPUT Delta Day Group $ ID $ @@;&#10;CARDS;&#10;-1.27 4 1 1 -0.34 7 1 1 1.92 10 1 1 0.45 4 1 2 1.21 7 1 2 0.43 10 1 2&#10;-0.41 4 1 3 0.16 7 1 3 -0.35 10 1 3 1.49 4 2 4 -0.85 7 2 4 -0.86 10 2&#10;4 1.04 16 2 4 0.49 28 2 4 2.32 4 2 5 0.13 7 2 5 -0.32 10 2 5 0.5 16 2 5&#10;0.48 28 2 5 1.21 4 2 6 -0.82 7 2 6 0.93 10 2 6 -0.58 16 2 6 2.3 28 2 6&#10;-0.9 4 2 7 0.21 7 2 7 -0.72 10 2 7 0.11 16 2 7 -0.28 28 2 7 -0.33 4 2 8&#10;-0.7 7 2 8 -1.16 10 2 8 -0.23 16 2 8 -0.88 4 3 9 0.97 7 3 9 0.25 10 3 9&#10;0.8 16 3 9 0.16 28 3 9 0.63 4 3 10 -0.49 7 3 10 -0.63 10 3 10 -0.9 16 3 10&#10;1.1 28 3 10 -1.45 4 3 11 0.38 7 3 11 -0.93 10 3 11 0.4 16 3 11 0.45 28 3 11&#10;0.48 4 3 12 0.14 7 3 12 1.02 10 3 12 -0.01 16 3 12 -1.98 28 3 12 2.19 4 3 13&#10;-1.53 7 3 13 -0.49 10 3 13 -1.57 16 3 13 -1.02 28 3 13 1.09 4 4 14 1.74 7 4 14&#10;0.54 10 4 14 -1.57 16 4 14 -1.5 4 4 15 -0.48 7 4 15 0.26 10 4 15 0.2 16 4 15&#10;-0.36 28 4 15 -1.05 4 4 16 -1.73 7 4 16 -0.77 10 4 16 -0.65 16 4 16 -1.07 28 4 16&#10;-0.45 4 4 17 -0.14 7 4 17 -0.56 10 4 17 0.84 16 4 17 -2.66 28 4 17 -0.52 4 4 18&#10;1.44 7 4 18 0.45 10 4 18 0.24 16 4 18 -0.92 28 4 18&#10;;&#10;RUN;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="3" CreationDate="2014-03-26T15:04:10.117" Id="91445" LastActivityDate="2014-04-07T15:51:52.753" LastEditDate="2014-03-26T16:33:11.503" LastEditorUserId="27342" OwnerUserId="27342" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;repeated-measures&gt;&lt;interaction&gt;&lt;sas&gt;&lt;singular&gt;" Title="R gls() vs. SAS proc mixed with interaction: Why does R complain about a singular matrix when SAS does not?" ViewCount="436" />
  
  <row Body="&lt;p&gt;On a related note, which may be helpful, Tibshirani and colleagues have proposed a significance test for the lasso. The (not yet published) &lt;a href=&quot;http://statweb.stanford.edu/~tibs/ftp/covtest.pdf&quot;&gt;manuscript&lt;/a&gt; is available, and titled &quot;A significance test for the lasso&quot;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-26T14:49:32.523" Id="91464" LastActivityDate="2014-03-26T14:49:32.523" OwnerDisplayName="julieth" OwnerUserId="12548" ParentId="91462" PostTypeId="2" Score="14" />
  
  <row AcceptedAnswerId="91470" AnswerCount="1" Body="&lt;p&gt;If X and Y are independent random variables such that X = A + B and Y = C + D? Are the pairs (A, C), (B, C), (A, D), (B, D) also independent? By this I mean whether A and C are independent, B and C are independent and so on.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-03-26T17:16:10.933" Id="91466" LastActivityDate="2014-03-26T18:11:07.447" LastEditDate="2014-03-26T17:32:00.167" LastEditorDisplayName="user42598" OwnerDisplayName="user42598" PostTypeId="1" Score="0" Tags="&lt;random-variable&gt;&lt;independence&gt;" Title="decomposition of independent random variables" ViewCount="28" />
  <row Body="&lt;p&gt;How would you go about pairing them? A paired t-test asks whether the mean difference between paired observations is different from zero. For example, you might weigh people, put them on a diet, and then weigh them again some time later. An unpaired t-test would ask whether the average weight of the &quot;before&quot; group differs from the average weight of the &quot;after&quot; group. A paired t-test, however, compares each person's weight loss/gain. &lt;/p&gt;&#10;&#10;&lt;p&gt;It doesn't sound like you have any sensible way of pairing your questionnaires, so you cannot use a paired t-test. This isn't necessarily the end of the world--you can use an unpaired t-test instead: this would tell you whether there was a significant different in the two total scores.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-26T18:23:02.770" Id="91472" LastActivityDate="2014-03-26T18:23:02.770" OwnerUserId="7250" ParentId="91468" PostTypeId="2" Score="2" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm using R to compute robust multiple linear regression.&#10;I use the command &lt;code&gt;rlm&lt;/code&gt; from the package MASS.&lt;/p&gt;&#10;&#10;&lt;p&gt;As psi function I use &lt;code&gt;psi.huber&lt;/code&gt; or &lt;code&gt;psi.bisquare&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a way to get an estimator of the goodness of fit of the model? Maybe something comparable to the Adjusted R-squared, for the parametric multiple linear regression?&lt;/p&gt;&#10;&#10;&lt;p&gt;Moreover, am I right saying that this kind of robust regression doesn't solve the problem of non-normality of the dependent or independent variables?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-03-26T20:07:39.213" Id="91479" LastActivityDate="2014-03-26T20:07:39.213" OwnerUserId="36738" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;regression&gt;&lt;robust&gt;" Title="Robust regression goodness of fit" ViewCount="148" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Each week, John drives to his mother's house. The amount of time required for the trip varies and is normally distributed. On about 16% of trips, it takes him more than 54 minutes to reach his mother's house. On about 2.5% of trips, it takes him less than 33 minutes to reach his mother's house. Which is closest to the mean amount of time required to make the trip?&lt;/p&gt;&#10;&#10;&lt;p&gt;Full disclosure: I'm studying for a test and this is a question from the practice test and I don't know where to start. I have the answer; I want to know how to get there.&lt;/p&gt;&#10;" ClosedDate="2014-03-27T02:22:46.657" CommentCount="1" CreationDate="2014-03-26T23:49:30.577" Id="91495" LastActivityDate="2014-03-26T23:49:30.577" OwnerUserId="42615" PostTypeId="1" Score="0" Tags="&lt;self-study&gt;&lt;normal-distribution&gt;&lt;mean&gt;&lt;proportion&gt;" Title="normal distribution mean" ViewCount="18" />
  <row Body="&lt;p&gt;Different sample size calculators are used for different hypothesis tests.&lt;/p&gt;&#10;&#10;&lt;p&gt;It seems like you're using a calculator &lt;em&gt;specifically designed&lt;/em&gt; to compute sample size for tests of/CIs for proportions (it's difficult to tell exactly what calculations are being done because they give absolutely no details, but one can surmise from the request for the proportion)&lt;/p&gt;&#10;&#10;&lt;p&gt;Yet it sounds like you're not doing a proportions test. So it's little surprise when it asks for information that makes no sense. It's like going to a general store to buy some nuts for dinner and instead looking in a section for hardware, and wondering why the assistant keeps asking you apparently pointless questions unrelated to food in an effort to give you the right kind of nut.&lt;/p&gt;&#10;&#10;&lt;p&gt;[Except in this case it's worse, because the site doesn't even tell you what situations it's giving the sample sizes for... so it's more like a store with no signs and nothing on display. No wonder there's room confusion; you have no easy way to tell you're not in the right section for food. Attempting to comprehend apparently randomly selected internet sites that don't clearly tell you what they're doing may not be a good way to go about your problem]&lt;/p&gt;&#10;&#10;&lt;p&gt;There are a number of questions here relating to sample size determination; perhaps start there, and then read more than one document that relates to finding sample sizes.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you want clear advice here about what to do, ask a specific question that makes it clear what you want to do.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-27T00:39:02.287" Id="91501" LastActivityDate="2014-03-28T04:45:10.817" LastEditDate="2014-03-28T04:45:10.817" LastEditorUserId="805" OwnerUserId="805" ParentId="91500" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;If you consider the HLM to be a type of linear mixed model, you could consider the EM algorithm. Page 22-23 of the following course notes indicate how to implement the classic EM algorithm for mixed model:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.stat.ucla.edu/~yuille/courses/stat153/emtutorial.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.stat.ucla.edu/~yuille/courses/stat153/emtutorial.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;###########################################################&#10;#     Classical EM algorithm for Linear  Mixed Model      #&#10;###########################################################&#10;em.mixed &amp;lt;- function(y, x, z, beta, var0, var1,maxiter=2000,tolerance = 1e-0010)&#10;    {&#10;    time &amp;lt;-proc.time()&#10;    n &amp;lt;- nrow(y)&#10;    q1 &amp;lt;- nrow(z)&#10;    conv &amp;lt;- 1&#10;    L0 &amp;lt;- loglike(y, x, z, beta, var0, var1)&#10;    i&amp;lt;-0&#10;    cat(&quot;  Iter.       sigma0                 sigma1        Likelihood&quot;,fill=T)&#10;    repeat {&#10;            if(i&amp;gt;maxiter) {conv&amp;lt;-0&#10;                    break}&#10;    V &amp;lt;- c(var1) * z %*% t(z) + c(var0) * diag(n)&#10;    Vinv &amp;lt;- solve(V)&#10;    xb &amp;lt;- x %*% beta&#10;    resid &amp;lt;- (y-xb)&#10;    temp1 &amp;lt;- Vinv %*% resid&#10;    s0 &amp;lt;- c(var0)^2 * t(temp1)%*%temp1 + c(var0) * n - c(var0)^2 * tr(Vinv)&#10;    s1 &amp;lt;- c(var1)^2 * t(temp1)%*%z%*%t(z)%*%temp1+ c(var1)*q1 -&#10;                                                c(var1)^2 *tr(t(z)%*%Vinv%*%z)&#10;    w &amp;lt;- xb + c(var0) * temp1&#10;    var0 &amp;lt;- s0/n&#10;    var1 &amp;lt;- s1/q1&#10;    beta &amp;lt;- ginverse( t(x) %*% x) %*% t(x)%*% w&#10;    L1 &amp;lt;- loglike(y, x, z, beta, var0, var1)&#10;    if(L1 &amp;lt; L0) { print(&quot;log-likelihood must increase, llikel &amp;lt;llikeO, break.&quot;)&#10;                             conv &amp;lt;- 0&#10;break&#10;}&#10;    i &amp;lt;- i + 1&#10;    cat(&quot;  &quot;, i,&quot;  &quot;,var0,&quot;  &quot;,var1,&quot;  &quot;,L1,fill=T)&#10;    if(abs(L1 - L0) &amp;lt; tolerance) {break}  #check for convergence&#10;    L0 &amp;lt;- L1&#10;    }&#10;list(beta=beta, var0=var0,var1=var1,Loglikelihood=L0)&#10;}&#10;&#10;#########################################################&#10;#  loglike calculates the LogLikelihood for Mixed Model #&#10;#########################################################&#10;loglike&amp;lt;- function(y, x, z, beta, var0, var1)&#10;}&#10;{&#10;n&amp;lt;- nrow(y)&#10;V &amp;lt;- c(var1) * z %*% t(z) + c(var0) * diag(n)&#10;Vinv &amp;lt;- ginverse(V)&#10;xb &amp;lt;- x %*% beta&#10;resid &amp;lt;- (y-xb)&#10;temp1 &amp;lt;- Vinv %*% resid&#10;(-.5)*( log(det(V)) + t(resid) %*% temp1 )&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-03-27T01:05:24.127" Id="91503" LastActivityDate="2014-03-27T01:05:24.127" OwnerUserId="27855" ParentId="23551" PostTypeId="2" Score="0" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm currently dealing with large DNA sequences for machine learning purposes, I'm basically improving existing methods.&lt;/p&gt;&#10;&#10;&lt;p&gt;What I have is several millions of DNA sequences :&#10;ACGTAGGCAGGCTTTC ... &lt;/p&gt;&#10;&#10;&lt;p&gt;In the methods I'm currently reviewing they extracts the features like this : &#10;for every nucleobase they put 4 features, the first corresponding to A, the second to C, the third to G and the last to T. If for exemple the current base is G the corresponding 4 features will be 0 0 1 0. &lt;/p&gt;&#10;&#10;&lt;p&gt;The problem I have with that is that it multiply the number of &quot;effective&quot; feature by four and it will be very sparse. &lt;/p&gt;&#10;&#10;&lt;p&gt;I was wondering if there would be any disadvantages to put one feature per nucleobase which would be 0, 1, 2, 3 depending on the letter. &lt;/p&gt;&#10;&#10;&lt;p&gt;It is applied to DNA but my question extend to every kind of discrete features. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-27T07:55:43.350" Id="91519" LastActivityDate="2014-03-27T09:12:04.850" OwnerUserId="31612" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;categorical-data&gt;&lt;feature-construction&gt;" Title="feature representation for DNA bases classification" ViewCount="21" />
  
  <row AcceptedAnswerId="92272" AnswerCount="1" Body="&lt;p&gt;I have two lots of samples: one is the control lot and the other undergoes some treatment. I did three measurements for the samples: one at the initial time (&lt;code&gt;T1&lt;/code&gt;) and the other two later. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;           Descriptive Statistics               &#10;    Lot         Mean  Std. Deviation      N&#10;T1  1.00    124.3043         3.21127     23&#10;    2.00    124.1333         1.94286     30&#10;    Total   124.2075         2.54467     53&#10;T2  1.00    112.8261         5.81262     23&#10;    2.00    107.7000         7.42387     30&#10;    Total   109.9245         7.18398     53&#10;T3  1.00    90.3478          8.47783     23&#10;    2.00    114.7000         4.43458     30&#10;    Total   104.1321        13.77852     53&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The plotted data look like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/5NjiA.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;What I did:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Use IBM SPSS (ver 20) and run &lt;code&gt;General Linear Model/Repeated Measures&lt;/code&gt;. Established the factor to &lt;code&gt;MeasureTime&lt;/code&gt; with three levels and the measurement. Between subjects factor was set to the lot number (&lt;code&gt;1&lt;/code&gt; or &lt;code&gt;2&lt;/code&gt;), &lt;code&gt;Model full factorial&lt;/code&gt;, &lt;code&gt;Contrasts&lt;/code&gt; to &lt;code&gt;MeasureTime&lt;/code&gt;, &lt;code&gt;Simple&lt;/code&gt;, &lt;code&gt;Reference category&lt;/code&gt; set to first.&lt;/p&gt;&#10;&#10;&lt;p&gt;SPSS output a lot of statistics. Almost all are relevant (sig &amp;lt; 0.000) except Mauchly's Test of Sphericity.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;What I want to know:&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;How can I see if the mean differences in &lt;code&gt;T1&lt;/code&gt;/&lt;code&gt;T2&lt;/code&gt;/&lt;code&gt;T3&lt;/code&gt; are significant? (I expect that &lt;code&gt;T1&lt;/code&gt; is not and &lt;code&gt;T3&lt;/code&gt; is.)&lt;/li&gt;&#10;&lt;li&gt;What statistic (from SPSS output) tells me that the treatment works? &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;P.S. I read other related questions suggested by Stack Exchange, but no luck with my questions.&lt;/p&gt;&#10;&#10;&lt;p&gt;I think that ANOVA would work for a factor with multiple levels, but as I read on other questions, in case of repeated measurements, ANOVA assumptions fail. The data are not independent.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-27T11:04:24.017" Id="91533" LastActivityDate="2014-04-03T09:00:57.300" LastEditDate="2014-04-03T09:00:57.300" LastEditorUserId="32036" OwnerUserId="31077" PostTypeId="1" Score="1" Tags="&lt;mixed-model&gt;&lt;spss&gt;&lt;repeated-measures&gt;&lt;generalized-linear-model&gt;" Title="Statistically prove the effectiveness of a treatment using GLM repeated measurements" ViewCount="110" />
  <row AnswerCount="0" Body="&lt;p&gt;I know composites gives an overall assessment of multiple tests. But is there any statistical benefits to using composites?&lt;/p&gt;&#10;&#10;&lt;p&gt;I have noticed in my composite scores, the composite turned out to be significant, but the individual components are not. I used the z scores to calculate composites of 3 variables. &lt;/p&gt;&#10;&#10;&lt;p&gt;Why is this? Thank you so much&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-27T11:06:16.217" FavoriteCount="1" Id="91534" LastActivityDate="2014-03-27T11:06:16.217" OwnerUserId="42634" PostTypeId="1" Score="0" Tags="&lt;composite&gt;" Title="Any Statistical Benefit of Using Composite Scores?" ViewCount="23" />
  
  <row Body="&lt;p&gt;A naive approach:&lt;/p&gt;&#10;&#10;&lt;p&gt;In a Normal distribution, the 25% and 75% quantiles are located at $0.67\cdot\sigma$ distance from the center. That gives that the 50% centered density covers twice this distance ($1.35\cdot \sigma$). In a boxplot, the intequartile Range (IQR, the distance from the bottom of the box to the top) covers the 50% centered amount of sample. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you make the assumption that your population follows a Normal distribution (which sometimes is a BIG assumption to do, not so trivial), then the standard deviation of your population could be roughly estimated from the equation $IQR=1.35\cdot\sigma$, that is $\sigma=0.74\cdot IQR$.&lt;/p&gt;&#10;&#10;&lt;p&gt;And about comparing variances by boxplot: wider boxes mean bigger variances, but that gives you exploratory understanding, and you have to take into account also whiskers and outliers. For confirmation you should use hypothesis contrast.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-03-27T11:39:32.660" Id="91538" LastActivityDate="2014-03-27T12:23:12.327" LastEditDate="2014-03-27T12:23:12.327" LastEditorUserId="22047" OwnerUserId="39849" ParentId="91536" PostTypeId="2" Score="7" />
  <row Body="&lt;p&gt;SPSS sees an MA(1) process (that is the (0,0,1) term) and no seasonality in the data (that is the (0,0,0) term). An MA(1) term has very little impact on the point forecast. Here is an example in R with toy data, where I artificially inflate every 13th observation. Actuals in black, in-sample fits in red. Note how the forecast in blue is almost flat, although the series and the fits show clear spikes:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/iCY8F.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;R code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(forecast)&#10;set.seed(1)&#10;xx &amp;lt;- rnorm(200)&#10;xx[13*(1:15)] &amp;lt;- xx[26*(1:5)]+8&#10;model &amp;lt;- arima(x=xx,order=c(0,0,1),seasonal=list(order=c(0,0,0),period=52))&#10;model&#10;&#10;plot(forecast(model,h=53),ylim=range(xx+model$residuals))&#10;    lines(xx+model$residuals,col=&quot;red&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;ARIMA models can be extremely unintuitive. I suggest you play around with simulated ARIMA(p,d,q) models to get a feeling for them. I always recommend &lt;a href=&quot;http://otexts.com/fpp/&quot; rel=&quot;nofollow&quot;&gt;this textbook&lt;/a&gt; to learn more about forecasting.&lt;/p&gt;&#10;&#10;&lt;p&gt;To return to your data: you have giant spikes there, and an ARIMA model won't be able to &lt;em&gt;explain&lt;/em&gt; those and forecast them. I suggest that you go back to the data source and see whether you can find some explanatory variables (e.g., promotions) to build a &lt;em&gt;causal&lt;/em&gt; model. Such a model would allow you to forecast the spikes in the future again.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-27T11:55:00.520" Id="91541" LastActivityDate="2014-03-27T11:55:00.520" OwnerUserId="1352" ParentId="91529" PostTypeId="2" Score="1" />
  
  
  
  <row Body="&lt;p&gt;Another way to show this (part a ) is to write the Bayes rule as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;P(B|G) = P(G)P(B|G)  / P(~G)P(B|~G) + P(G)P(B|G)&#10;       = (0.5 * 0.5) / (0.5*0.25 + 0.5*0.5) = 2/3&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-03-27T16:49:56.303" Id="91577" LastActivityDate="2014-07-24T19:06:32.333" LastEditDate="2014-07-24T19:06:32.333" LastEditorUserId="42647" OwnerUserId="42647" ParentId="73593" PostTypeId="2" Score="3" />
  
  
  <row Body="&lt;p&gt;Based on @Aaron code I developed something called &quot;alluvial diagram&quot;. See &lt;a href=&quot;http://bc.bojanorama.pl/2014/03/alluvial-diagrams/&quot;&gt;http://bc.bojanorama.pl/2014/03/alluvial-diagrams/&lt;/a&gt; Example below:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ZE5TW.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-03-27T18:24:19.583" Id="91588" LastActivityDate="2014-03-27T18:24:19.583" OwnerUserId="31609" ParentId="12029" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;use bit code to do this.&#10;for example if a nominal variable have 1000 categories, then use variable &#10;u1,u2,u3... u10&#10;then represent each category as binary number i.e.&#10;level 10=1010 then use u4-u1 to represent them&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-03-27T20:06:22.470" Id="91600" LastActivityDate="2014-03-27T20:06:22.470" OwnerUserId="38037" ParentId="89815" PostTypeId="2" Score="-4" />
  <row Body="&lt;p&gt;You might be looking for the two sample &lt;a href=&quot;http://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test&quot; rel=&quot;nofollow&quot;&gt;K-S test&lt;/a&gt;.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Matlab Stats toolbox has an implementation, &lt;a href=&quot;http://www.mathworks.com/help/stats/kstest2.html&quot; rel=&quot;nofollow&quot;&gt;kstest2&lt;/a&gt;:  &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;kstest2(x1,x2) returns a test decision for the null hypothesis that&#10;  the data in vectors x1 and x2 are from the same continuous&#10;  distribution, using the two-sample Kolmogorov-Smirnov test. The&#10;  alternative hypothesis is that x1 and x2 are from different continuous&#10;  distributions.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="4" CreationDate="2014-03-27T20:57:51.233" Id="91607" LastActivityDate="2014-03-27T20:57:51.233" OwnerUserId="42614" ParentId="91419" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;The &lt;code&gt;pastecs&lt;/code&gt; package returns most of what you mentioned. Not sure what you meant by those $\alpha$ and $\beta$.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; library(pastecs)&#10;&amp;gt; &#10;&amp;gt; set.seed(121)&#10;&amp;gt; y1 &amp;lt;- rnorm(100)&#10;&amp;gt; y2 &amp;lt;- rnorm(100)&#10;&amp;gt; &#10;&amp;gt; stat.desc(cbind(y1,y2))&#10;                       y1           y2&#10;nbr.val      100.00000000 100.00000000&#10;nbr.null       0.00000000   0.00000000&#10;nbr.na         0.00000000   0.00000000&#10;min           -3.21551021  -2.26262801&#10;max            1.61533208   3.08124573&#10;range          4.83084228   5.34387374&#10;sum           -9.65469994  -3.02696571&#10;median        -0.01899477   0.03549059&#10;mean          -0.09654700  -0.03026966&#10;SE.mean        0.08899350   0.09853204&#10;CI.mean.0.95   0.17658241   0.19550894&#10;var            0.79198425   0.97085623&#10;std.dev        0.88993497   0.98532037&#10;coef.var      -9.21763467 -32.55142151&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;For graph, there isn't one as far as I know, but the command is extremely easy to make using &lt;code&gt;mfrow&lt;/code&gt; option in &lt;code&gt;par()&lt;/code&gt;.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-03-27T21:07:04.917" Id="91610" LastActivityDate="2014-03-27T21:07:04.917" OwnerUserId="13047" ParentId="91605" PostTypeId="2" Score="3" />
  <row AnswerCount="0" Body="&lt;p&gt;Let's say you have an overall group of financial institutions (fi) made of banks (ba) and federal credit unions (fcu). The total number of fi is 73. The number of ba is 46 and the number fcu is 27. Let's assume also the total numbers of employees working for ba is 2500 and the total numbers of employees working for fcu is 1080. How will you construct a sample or sample groups that are representative of each group; that is, ba and fcu?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-27T22:13:47.390" Id="91619" LastActivityDate="2014-03-27T22:42:04.640" OwnerUserId="42666" PostTypeId="1" Score="0" Tags="&lt;sample-size&gt;" Title="determining sample size from a pooled sample" ViewCount="15" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;My colleague and I are professors and we conducted an experiment in which we would please like some advice on deciding which tests to use in SPSS.&lt;/p&gt;&#10;&#10;&lt;p&gt;He taught 4 classes of students on 2 different occasions and I taught a different group of 4 classes of students on 2 different occasions. I was the experimental group and taught all of my sessions incorporating an educational game. He was the control group and did not use any games. We each gave the students a pre-test at the beginning of the first session and a post-test at the end of the second class.&lt;/p&gt;&#10;&#10;&lt;p&gt;Our hypothesis is that students in the experimental (games) classes performed better on the post-test than students in the control group. Unfortunately we didn't think to assign each student a number so that we could figure out which pre and post-test belonged to who. So basically we have a ton of pre and post-tests divided by class but not by student. Is there a a way we could conduct statistical analyses for the groups instead of individuals to see if our hypothesis was concerned?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-03-28T01:01:38.573" FavoriteCount="1" Id="91632" LastActivityDate="2014-03-28T21:21:25.217" LastEditDate="2014-03-28T15:56:24.133" LastEditorUserId="24808" OwnerUserId="42673" PostTypeId="1" Score="4" Tags="&lt;spss&gt;&lt;experiment-design&gt;" Title="How to compare pre- and post-tests when subject identifier is missing?" ViewCount="146" />
  
  <row AcceptedAnswerId="93414" AnswerCount="1" Body="&lt;p&gt;Are there methods available to conduct interval-regression style analysis where a predictor variable is the interval variable and the outcome variable is a dichotomous variable? &lt;/p&gt;&#10;&#10;&lt;p&gt;To give a bit of background, I would like to plot the relationship between two variables in a dataset where the x axis is a continuous measure has been collected in an ordinal format. The category widths are non-uniform (and increasing). To give a concrete example, the categories are about expenditure on a category of goods and were (in dollars): 0-50; 51-100; 101-250; 251-500; 501-1000; &gt; 1000. The y-axis is a yes/no style variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am particularly interested in determining whether the relationship is likely to be linear or exponential.&lt;/p&gt;&#10;&#10;&lt;p&gt;Are there any methods for doing this, other than arbitrarily picking a point in each x-axis category? Obviously, the final category (&gt; 1000) is the most problematic. Bonus points (metaphorically speaking) if you can point me in the direction of an R package.&lt;/p&gt;&#10;&#10;&lt;p&gt;If it helps, I am happy to look for an answer that requires a strong distributional assumption (e.g. that the data are zero-inflated gamma or log-normal distributed). &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-03-28T03:38:16.847" Id="91642" LastActivityDate="2014-04-11T08:21:42.000" LastEditDate="2014-03-30T09:52:32.663" LastEditorUserId="179" OwnerUserId="179" PostTypeId="1" Score="4" Tags="&lt;distributions&gt;&lt;ordinal&gt;&lt;curve-fitting&gt;&lt;interval-censoring&gt;" Title="How can I estimate the shape of a curve where the predictor variable is right censored interval variable?" ViewCount="105" />
  <row AnswerCount="0" Body="&lt;p&gt;I have $n$ observations which I want to cluster based on $p$ features. Those $p$ features are dependent to each other (or correlated). If I use Euclidean distance, the weights of all features would be the same, independent from the correlations between them. But what I want is to decrease the weight of a feature in clustering process if this feature is correlated with many other features. So, I want to arrange the weight of a feature so that it is somewhat inversely proportional to the amount of correlation of this feature with other features. I know that Mahalanobis distance can be used to take care of the correlations between the features, but I am not sure what exactly Mahalanobis does with those correlations. So, I wonder if it does what I need to do (arranging the weight of a feature so that it is somewhat inversely proportional to the amount of correlation of this feature with other features). Thanks!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-28T04:54:44.047" Id="91645" LastActivityDate="2014-03-28T04:54:44.047" OwnerUserId="36564" PostTypeId="1" Score="0" Tags="&lt;clustering&gt;&lt;euclidean&gt;" Title="Clustering with dependent features" ViewCount="18" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am interested in generating a covariance matrix of dimension say 100. I managed to get a correlation matrix with finite condition number.&lt;/p&gt;&#10;&#10;&lt;p&gt;To construct a covariance matrix I need to have standard deviations. I think for my case the most suitable one is to generate standard deviations from gamma distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, it gives me small standard deviations as well as large standard deviations. As a result of that, the resulting covariance matrix has a very high condition number.&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to know whether the condition number can be affected by the scale of the variables and if I want to incorporate different scales in the covariance matrix how can I get a covariance matrix with a reasonable condition number?&lt;/p&gt;&#10;&#10;&lt;p&gt;Any help or insight regarding this is highly appreciated.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-28T08:57:17.093" Id="91653" LastActivityDate="2014-03-28T09:21:35.217" LastEditDate="2014-03-28T09:21:35.217" LastEditorUserId="2116" OwnerUserId="35011" PostTypeId="1" Score="1" Tags="&lt;correlation&gt;&lt;matrix&gt;&lt;computational-statistics&gt;&lt;variance-covariance&gt;&lt;eigenvalues&gt;" Title="Condition number of covariance matrix" ViewCount="58" />
  
  <row Body="&lt;p&gt;Some very basic sample size calculations are discussed here:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.itl.nist.gov/div898/handbook/prc/section2/prc222.htm&quot; rel=&quot;nofollow&quot;&gt;http://www.itl.nist.gov/div898/handbook/prc/section2/prc222.htm&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.itl.nist.gov/div898/handbook/prc/section2/prc242.htm&quot; rel=&quot;nofollow&quot;&gt;http://www.itl.nist.gov/div898/handbook/prc/section2/prc242.htm&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;For a basic introduction, the book by Moore and McCabe - &lt;em&gt;Introduction to the Practice of Statistics&lt;/em&gt; covers some of the basics in chapter 6.&lt;/p&gt;&#10;&#10;&lt;p&gt;Some deeper discussion is in&lt;/p&gt;&#10;&#10;&lt;p&gt;Russell V.Lenth, 2001.&lt;br&gt;&#10;&quot;Some Practical Guidelines for Effective Sample Size Determination&quot;&lt;br&gt;&#10;&lt;em&gt;The American Statistician&lt;/em&gt;, August 2001, Vol. 55, No.3 &lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Edit:&lt;/p&gt;&#10;&#10;&lt;p&gt;I just noticed that I had left out Jacob Cohen's book. I meant to mention it -&#10; &lt;em&gt;Statistical Power Analysis for the Behavioural Sciences&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;I don't know this book, but I've seen some people recommend&lt;/p&gt;&#10;&#10;&lt;p&gt;Chow S, Shao J, Wang H. 2008.&lt;br&gt;&#10;&lt;em&gt;Sample Size Calculations in Clinical Research&lt;/em&gt;.&lt;br&gt;&#10;2nd Ed. Chapman &amp;amp; Hall/CRC Biostatistics Series. &lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;As I said in comments, I tend to use simulation.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-28T09:51:05.010" Id="91661" LastActivityDate="2014-03-29T15:31:39.240" LastEditDate="2014-03-29T15:31:39.240" LastEditorUserId="805" OwnerUserId="805" ParentId="91630" PostTypeId="2" Score="2" />
  <row AnswerCount="1" Body="&lt;p&gt;I have two linear regression models (with the same predictors) that try to estimate two different (although related) features of the same population. I am analyzing the hypothesis that these predictors are not as good for the second feature as they are for the first.&#10;Indeed, the RMSE of the second model is 7% worse than the RMSE of the first: it is not a huge difference, but for my purposes it is fine.&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem is: I have been asked the statistical significance of this result (the 7% worse). &lt;/p&gt;&#10;&#10;&lt;p&gt;I am a computer scientist, not a statistics expert, and I am at a loss here on which test I should use, given that I am trying to compare the squared errors of the two (paired) sets of predictions (I am using SPSS software).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-28T11:31:00.020" Id="91671" LastActivityDate="2014-04-03T07:20:05.163" LastEditDate="2014-03-28T15:58:03.140" LastEditorUserId="25936" OwnerUserId="42690" PostTypeId="1" Score="4" Tags="&lt;regression&gt;&lt;hypothesis-testing&gt;&lt;statistical-significance&gt;&lt;spss&gt;&lt;linear-model&gt;" Title="Statistical significance for comparison of linear regression models" ViewCount="279" />
  <row AcceptedAnswerId="91845" AnswerCount="1" Body="&lt;p&gt;Suppose I fitted a logistic model and get the estimates as well as their &lt;code&gt;vcov&lt;/code&gt; matrix. I would realize this: draw length($\beta_s$) independent $\mathcal N(0,1)$ values to create a random vector $z$, then $\beta^*$=$\hat{\beta}+Az$ where $A$ is the upper triangular matrix of the Cholesky decomposition matrix ($\hat{V}=A'A$). How can I draw the $\beta^*$ using the &lt;code&gt;vcov&lt;/code&gt; matrix? Here is an example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(123)&#10;df &amp;lt;- data.frame(y=rbinom(100,1,0.5),&#10;                 x1=rnorm(100,10,2),&#10;                 x2=rbinom(100,20,0.6))&#10;&#10;fit &amp;lt;- glm(y~x1+x2, data=df, family=&quot;binomial&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;$\hat{\beta}$:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;coef(summary(fit))&#10;            Estimate Std. Error  z value Pr(&amp;gt;|z|)&#10;(Intercept)   0.1482    1.57451  0.09413   0.9250&#10;x1           -0.1710    0.10962 -1.55984   0.1188&#10;x2            0.1181    0.09047  1.30567   0.1917&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;vcov matrix:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;vcov(fit)&#10;(Intercept)      2.4791 -0.1225802 -0.1020612&#10;x1              -0.1226  0.0120164  0.0003505&#10;x2              -0.1021  0.0003505  0.0081844&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Would somebody know how to draw new $\beta^*$ using the &lt;code&gt;coef&lt;/code&gt; and &lt;code&gt;vcov&lt;/code&gt;?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-03-28T13:23:49.353" Id="91682" LastActivityDate="2014-03-29T23:34:30.520" LastEditDate="2014-03-29T23:32:44.987" LastEditorUserId="7290" OwnerUserId="35736" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;regression&gt;&lt;random-generation&gt;&lt;variance-covariance&gt;" Title="How to draw estimates based on variance covariance matrix?" ViewCount="299" />
  <row Body="&lt;ul&gt;&#10;&lt;li&gt;When there is no model comparison, the difference between restricted&#10;(or residual) maximum likelihood (REML) and maximum likelihood (ML)&#10;is that, REML can give you unbiased estimates of the variance&#10;parameters. Recap that, ML estimates for variance has a term $1/n$, but the&#10;unbiased estimate should be $1/(n-p)$, where $n$ is the sample size, $p$&#10;is the number of mean parameters. So REML should be used when you are&#10;interested in variance estimates and $n$ is not big enough as&#10;compared to $p$.&lt;/li&gt;&#10;&lt;li&gt;When there is model comparison, notice that REML cannot be used to&#10;compare mean models, since REML transformes the&#10;data thus makes the likelihood incomparable.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="1" CreationDate="2014-03-28T14:30:29.700" Id="91692" LastActivityDate="2014-03-28T14:30:29.700" OwnerUserId="21599" ParentId="91652" PostTypeId="2" Score="2" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I am trying fit an ARIMA model to stock returns.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have reached a decent model using the AIC criterion. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, the ljung-box p value under a diagnostic plots are pretty weird. &#10;The null hypothesis get rejected at higher lags.&#10;I tried modifying the parameters, but L-B p value betters only marginally, with a loss in AIC.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any help how I can balance the two ?&#10;Also any reasons why the p-value is so low for higher lags.&lt;/p&gt;&#10;&#10;&lt;p&gt;Ihave attached the diagnostic's image:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/AL5p5.png&quot; alt=&quot;tsdiag&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-28T15:16:55.930" Id="91706" LastActivityDate="2014-07-15T14:02:03.257" OwnerUserId="42702" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;time-series&gt;&lt;arima&gt;&lt;finance&gt;" Title="High Ljung-Box p-values at large lags" ViewCount="110" />
  <row AnswerCount="1" Body="&lt;p&gt;I am trying to use &lt;code&gt;randomForest&lt;/code&gt; in R to find interaction terms to add to a model.  My plan was to fit trees with &lt;code&gt;maxnodes=4&lt;/code&gt; (two deep stubs), then compute how often &lt;code&gt;var A&lt;/code&gt; is a child of &lt;code&gt;var B&lt;/code&gt; and its improved accuracy to measure the importance of the &lt;code&gt;A*B&lt;/code&gt; interaction.  Essentially this is leveraging &lt;code&gt;randomForest&lt;/code&gt; for the sampling.&lt;/p&gt;&#10;&#10;&lt;p&gt;To make this work I also wanted &lt;code&gt;mtry=#&lt;/code&gt; variables, so all variables would have a chance at being the child of the root variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;This strategy failed in my data because all trees after the first few were identical. I did decrease &lt;code&gt;cp&lt;/code&gt;, but that did not help.  I removed &lt;code&gt;mtry=#vars&lt;/code&gt;, and still got all 95% of  trees the same (the last 95%, the first few are different).&lt;/p&gt;&#10;&#10;&lt;p&gt;When &lt;code&gt;maxnodes=32&lt;/code&gt;, I got a good set of trees, but that's not what I needed for determining interactions.  I need to do this because I can't deploy a &lt;code&gt;randomForest&lt;/code&gt; in production.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any ideas why this wouldn't work?  I am coding this myself now using &lt;code&gt;rpart&lt;/code&gt; on 2 variable models iterating over the potential good pairs.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-03-28T15:49:04.283" FavoriteCount="1" Id="91713" LastActivityDate="2014-03-28T17:48:27.130" LastEditDate="2014-03-28T15:50:47.290" LastEditorUserId="32036" OwnerUserId="34905" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;interaction&gt;&lt;feature-selection&gt;&lt;random-forest&gt;" Title="Finding interactions using randomForest" ViewCount="212" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I used coda package's &lt;code&gt;effectiveSize()&lt;/code&gt; to find the effective sample size of my MCMC simulation.&#10;My effective sample size is greater than the actual sample size, e.g. 9813.626 greater than 9501. I wonder if that makes sense? &lt;/p&gt;&#10;&#10;&lt;p&gt;My understanding is that the effective sample size cannot be greater than the actual sample size, and will be smaller when there is more autocorrelation. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-28T16:37:39.967" Id="91720" LastActivityDate="2014-04-17T13:22:07.483" LastEditDate="2014-03-28T16:41:27.307" LastEditorUserId="26338" OwnerUserId="1005" PostTypeId="1" Score="3" Tags="&lt;simulation&gt;&lt;mcmc&gt;" Title="Can effective sample size in MCMC simulation be greater than the actual sample size?" ViewCount="162" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I had a question in regards to PCA with times series data, and specifically how to possibly interpret it. Normally, PCA is used by other software that I use in relation to de-noising a data set by removing components of least eigenvalues/variance (not sure as to the precise criteria used here), and the de-noised data is passed along for other preprocessing steps.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, I decided to manually use PCA on my data and actually get a look at the results of doing so. Essentially, I have 5 variables (5 brain regions), each with 180 time points, with specific intensity values (unit is same across the 5 variables).&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, my experience with PCA is limited, but I was wondering if there are any potential interpretations that could be done with such a time series data set? &lt;/p&gt;&#10;&#10;&lt;p&gt;From what I understand, the 4 variables (NOT the Cerebellum) are highly related to the PCA axis 1, and these 4 (RightPM, RightM, LeftPM, leftM) can be regarded as positively correlated with each other in relation to the intensity values? Confusion sets with the time points however, such as saying whether this strong positive relationship occurs at the time points plotted near the bunching of the 4 variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm assuming there are issues with PCA and time series (related to autocorrelation perhaps), but I was curious if such an interpretation could be made. Any opinions on this would be great.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/baiN1.png&quot; alt=&quot;PCA Biplot of 5 variables with each having 180 timepoints&quot;&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-03-28T18:51:20.443" FavoriteCount="1" Id="91739" LastActivityDate="2014-03-28T18:51:20.443" OwnerUserId="42715" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;pca&gt;&lt;biplot&gt;" Title="Interpreting a PCA Biplot of a time series?" ViewCount="79" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a logistic regression with data that are kind of like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;y &amp;lt;- rep(c(&quot;A&quot;, &quot;A&quot;, &quot;B&quot;), each = 30)&#10;x &amp;lt;- c(     rep(1, 12), rep(2, 18), rep(3, 16), rep(4, 12), rep(5, 2),&#10;            rep(1, 3), rep(2, 5), rep(3, 8), rep(4, 10), rep(5, 4)  )&#10;&#10;da &amp;lt;- data.frame(y = y, x = x)&#10;table(da)&#10;   x&#10;y    1  2  3  4  5&#10;  A 12 18 16 12  2&#10;  B  3  5  8 10  4&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I'd like to show that there are more A's than B's in &lt;code&gt;y&lt;/code&gt; (instead of a Bernoulli with &lt;code&gt;p&lt;/code&gt; = 0.5), even after controlling for &lt;code&gt;x&lt;/code&gt;, so I fitted two logistic regression models and used an ANOVA to compare them.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;mlogis.x              &amp;lt;- glm(y ~ x,     family = binomial, da)&#10;mlogis.x.no_intercept &amp;lt;- glm(y ~ x + 0, family = binomial, da)&#10;&#10;summary(mlogis.x.no_intercept)&#10;summary(mlogis.x)&#10;&#10;anova(mlogis.x.no_intercept, mlogis.x, test = &quot;Chisq&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I have a few questions:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Does what I did make sense overall?&lt;/li&gt;&#10;&lt;li&gt;Is it okay to not have an intercept in the more basic model and then add it to the full(er) model?&lt;/li&gt;&#10;&lt;li&gt;The coefficient for &lt;code&gt;x&lt;/code&gt; changes sign between the two models, how should I interpret this?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="4" CreationDate="2014-03-28T19:47:43.303" Id="91747" LastActivityDate="2014-03-28T20:17:42.600" LastEditDate="2014-03-28T20:17:42.600" LastEditorUserId="7290" OwnerUserId="17186" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;logistic&gt;&lt;binomial&gt;&lt;intercept&gt;" Title="Interpretation of coefficient sign change and order of terms" ViewCount="59" />
  <row Body="&lt;p&gt;An ARMA process corresponds to a rational transfer function. The definition of linear process given is more general, as it allows transfer functions that are not rational.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-28T20:46:28.090" Id="91755" LastActivityDate="2014-03-28T20:46:28.090" OwnerUserId="42721" ParentId="65644" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;You have two parts to your answer: data and noise.&lt;/p&gt;&#10;&#10;&lt;p&gt;Someone could fit a line, or a spline to the mean of the data and if they were honest they would say something like &quot;given this model, the mean tendency is ...&quot;.  &lt;/p&gt;&#10;&#10;&lt;p&gt;There is variation from the mean.  It looks like a 3 million dollars claim got 0.  If you had a line fit to predict the amount you might get a number like 3 million.  When someone has a claim for anything above 7.5 million dollars the departure from the linear fit collapses.  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;First&lt;/strong&gt; things first - I don't like your coordinates.  They should be log-log.  This would more clearly show the regions with the majority of the data.  Your y-axis should be the claim de-rating.  $$ y = claim - actual $$ or the log base 10 thereof.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Second&lt;/strong&gt; - you want to account for variation.  You should be able to put a +/- 95% confidence interval around whatever someone asserts as the mean de-rating given a particular value of claim.  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Third&lt;/strong&gt; - there is a clear difference between the 7.5 million dollar claim and the 3.0 million dollar claim.  Access to a variable like &quot;how much did you pay your lawyer&quot; or &quot;which firm did you use&quot; might allow clustering of the claims into higher and lower yield buckets.  You would then say &quot;if you use x as lawyer and you agree to pay them y, then there is a 95% chance that if you claim z then you will actually get z-thisvalue.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;A plot in the form requested (log-log, and transformed y-coordinate) is more likely to result in a resonable distribution.  Could you make it one that looks like this? (&lt;a href=&quot;http://www.eviews.com/EViews8/overview/scatters.png&quot;&gt;link&lt;/a&gt;)&lt;/p&gt;&#10;&#10;&lt;p&gt;Best of luck.  &lt;/p&gt;&#10;&#10;&lt;p&gt;PS: I bet a power law governs here.  (&lt;a href=&quot;http://www.ted.com/talks/geoffrey_west_the_surprising_math_of_cities_and_corporations&quot;&gt;link&lt;/a&gt;)&lt;/p&gt;&#10;&#10;&lt;p&gt;PPS: And Peter is a genius.  If he suggests something and you can figure out how to do it - he is very likely to have given an insightful and informative answer.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-28T21:34:06.483" Id="91761" LastActivityDate="2014-03-28T21:39:39.390" LastEditDate="2014-03-28T21:39:39.390" LastEditorUserId="22452" OwnerUserId="22452" ParentId="91687" PostTypeId="2" Score="5" />
  <row AcceptedAnswerId="91786" AnswerCount="1" Body="&lt;p&gt;I have been trying to find a method to analyse variance on Weibull and/or Gamma distributions but a Google search for&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;anovar  Weibull &quot;gamma distribution&quot;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;yields nothing helpful.  The data I have cannot be fitted to a normal distribution but fits a Weibull or Gamma distribution quite well.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-03-28T21:35:19.220" FavoriteCount="1" Id="91762" LastActivityDate="2014-04-08T21:26:02.590" LastEditDate="2014-03-29T02:06:06.393" LastEditorUserId="88" OwnerUserId="40680" PostTypeId="1" Score="2" Tags="&lt;anova&gt;&lt;gamma-distribution&gt;&lt;weibull&gt;" Title="Analysis of variance with Weibull or Gamma distributions" ViewCount="235" />
  <row AnswerCount="1" Body="&lt;p&gt;While reading &lt;em&gt;Discovering Statistics Using R&lt;/em&gt; pp. 431-432, Dr. Field says that &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&quot;There are a variety of tests designed to deal with these situations&#10;  [multiple comparison procedures with unequal group sizes &amp;amp;/or&#10;  different population variances], none of which are implemented in R.&#10;  Hochberg's GT2 is one such test and is worth mentioning because it is&#10;  not implemented in R...&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I didn't find anything in a Google search that indicated otherwise neither have I found an alternative post-hoc test for this situation.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, does anybody know if R has post hoc tests robust to unequal sample sizes/population variances?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-28T22:07:46.990" Id="91765" LastActivityDate="2014-03-28T22:48:10.183" LastEditDate="2014-03-28T22:11:14.597" LastEditorUserId="26338" OwnerUserId="16393" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;multiple-comparisons&gt;&lt;inference&gt;" Title="Does R have post hoc tests robust to unequal sample sizes/population variances?" ViewCount="251" />
  <row Body="&lt;p&gt;There are many versions of Central Limit Theorem; iid versions tend to be taught because they're relatively easy to prove (e.g. if the MGF exists, that leads to a reasonably simple demonstration of the CLT). Because there are many CLTs, we should be careful of saying &quot;&lt;em&gt;the&lt;/em&gt; CLT&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are versions of the central limit theorem for cases where the variables are not iid.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are versions where you have independence but the distributions differ. Basically, individual variances can't be too large relative to the rest (in hand-wavy terms, individual variances have to be vanishing fractions of the total, the formal condition depends on which CLT you're looking at).&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, see the &lt;a href=&quot;http://en.wikipedia.org/wiki/Central_limit_theorem#Lyapunov_CLT&quot; rel=&quot;nofollow&quot;&gt;Lyapunov CLT&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;There are also cases of the CLT for situations where dependence exists (see later in the same article for some examples).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-28T23:56:18.090" Id="91777" LastActivityDate="2014-03-28T23:56:18.090" OwnerUserId="805" ParentId="91768" PostTypeId="2" Score="2" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;My research has 4 independent variables and 5 dependent variables. My hypotheses are constructed in such a way that 1 independent variable's effect size is checked on 5 dependent variables. MANOVA is an automatically good choice for this. My independent variables are not built around categories and groups e.g. gender and neither it is multi-level like income level. Hence post hoc tests are not relevant in this case. &lt;/p&gt;&#10;&#10;&lt;p&gt;My concerns are as follows,&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;whether MANOVA'S without post hoc tests is a good choice and as effective? Which values should Ii specifically look upto. Wiki's lamba, partial eta square and between subjects?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Secondly is there other alternative statistical test for the above mentioned situation.&lt;/p&gt;&#10;&#10;&lt;p&gt;3.Thirdly, a grave concern is, when I ran regression analysis and ANOVA of one independent variable on one dependent variable, some P values were insignificant, but in MANOVA those P values were significant. What does it signify, is there a concern here?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Ii would be thankful if above mentioned queries are answered&#10;Sincere regards&lt;/p&gt;&#10;&#10;&lt;p&gt;Salman &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-03-29T06:22:22.370" Id="91796" LastActivityDate="2014-03-29T06:22:22.370" OwnerUserId="42732" PostTypeId="1" Score="1" Tags="&lt;anova&gt;&lt;p-value&gt;&lt;manova&gt;&lt;predictor&gt;" Title="MANOVA, when independent variable is not multi-level and P value significant in MANOVA but not in ANOVA" ViewCount="109" />
  <row AcceptedAnswerId="91804" AnswerCount="1" Body="&lt;p&gt;I obtain Figure 1, which shows the probability that a certain event will take place based on the &lt;em&gt;min threshold strain&lt;/em&gt; (X-value). However, clearly the graph is hard to gather any info from. I then add up the probabilities in tiny intervals of the X-values. After that I divide each of the Y values by the sum of the Y values and it gives me Figure 2. I was wondering if what I have done is meaningful at all?&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there some way by which I could make Graph 1 slightly better and possibly try to gather some information from it?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;There is no clear purpose behind this method. I am just trying to gather some useful information from the data that makes up Figure 1.&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/kBNy1.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Figure including loess regression method. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/2a5Ns.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;: (Discarding values very close to 1 and 0)&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/CjYAf.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-29T09:45:57.053" Id="91802" LastActivityDate="2014-03-30T14:27:53.637" LastEditDate="2014-03-30T14:27:53.637" LastEditorUserId="42029" OwnerUserId="42029" PostTypeId="1" Score="0" Tags="&lt;probability&gt;" Title="Making better sense of probability graphs" ViewCount="90" />
  
  <row Body="&lt;p&gt;The first graph can be improved a lot with some basic changes: 1) Put the tick marks outside the box instead of inside  2) Make sure the box does not overlap data points. (Good software ought to do this for you - &lt;code&gt;R&lt;/code&gt; does, as does &lt;code&gt;SAS&lt;/code&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;Then you could add a smoothed line to the plot - loess or a spline for example - to see what pattern there is (again, this is easy with either &lt;code&gt;R&lt;/code&gt; or &lt;code&gt;SAS&lt;/code&gt; or probably other programs as well, but those are the two I know).&lt;/p&gt;&#10;&#10;&lt;p&gt;The second graph - I am not sure why you did what you did, but it looks like you tried to do something like loess on your own. &lt;/p&gt;&#10;&#10;&lt;p&gt;But, looking at the first graph, one reason it is hard to find information there is that the relationship is quite weak and there are many y = 0 and y = 1. &lt;/p&gt;&#10;" CommentCount="16" CreationDate="2014-03-29T10:34:47.193" Id="91804" LastActivityDate="2014-03-29T10:34:47.193" OwnerUserId="686" ParentId="91802" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I am comparing models that I created with the lmer function from the lme4 package using ANOVA. Paricipants and verbs are my random factors, and RT are the Reaction Times that I measured. From the little statistics that I know, I would expect F-statictic values in the output, but instead I get chi-square.&lt;/p&gt;&#10;&#10;&lt;p&gt;Could you please help me understand why I get that, or whether I should (or could) change that?&lt;br&gt;&#10;Here is my code and output:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;m0&amp;lt;-lmer((sqrt(RT))~(1|Participant)+(1|Verbs), data=data, REML=FALSE)&#10;m1&amp;lt;-lmer((sqrt(RT))~(1|Participant)+(1|Verbs)+verbT, data=data, REML=FALSE)&#10;&#10;anova(m0,m1)&#10;&#10;&#10;Data: data&#10;Models:&#10;m0: (sqrt(RT)) ~ (1 | Participant) + (1 | Verbs)&#10;m1: (sqrt(RT)) ~ (1 | Participant) + (1 | Verbs) + verbT&#10;   Df     AIC     BIC logLik deviance  Chisq Chi Df Pr(&amp;gt;Chisq)&#10;m0  4 -693.68 -672.44 350.84  -701.68                         &#10;m1  5 -693.71 -667.16 351.85  -703.71 2.0238      1     0.1548&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-03-29T11:06:20.340" Id="91805" LastActivityDate="2014-03-29T11:27:03.417" LastEditDate="2014-03-29T11:27:03.417" LastEditorUserId="26338" OwnerUserId="42741" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;anova&gt;&lt;chi-squared&gt;" Title="Chi-square in ANOVA" ViewCount="35" />
  
  <row AcceptedAnswerId="93349" AnswerCount="2" Body="&lt;p&gt;I am playing Warlight online (something like Risk), and my goal is to create a bot which plays against other bots.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;n=100 attacking troops will on average kill 60 (p=0.6) troops each turn, &#10;n=100 defending troops will on average kill 70 (p=0.7) troops each turn.&#10;They attack exactly at the same time.&#10;Binomial distribution such that variance=n*p*(1-p)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;My question is the following: imagine we have 40 troops and the enemy has 25 troops. I am interested in the case that I continuously attack. What would be the chance that I have won by turn X (e.g. X=5)?&lt;/p&gt;&#10;&#10;&lt;p&gt;For turn = 1, it is simply the cumulative sum from at least 25 kills: &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\sum_{i=25}^{40} {40 \choose i} * 0.6^i * (1-0.6)^{(40-i)} = 0.440$$ &lt;/p&gt;&#10;&#10;&lt;p&gt;I have played around with the binomial distribution, but I find it difficult to calculate because of this &quot;in X turns&quot;; it seems it really explodes the amount of calculations needed (by lack of a smart trick).&lt;/p&gt;&#10;&#10;&lt;p&gt;Can this be done analytically or am I best off just to simulate this (which I assume will be much slower)?&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT: Perhaps it can simply be done by using the 60% and 70%?&lt;/p&gt;&#10;" CommentCount="18" CreationDate="2014-03-29T14:14:45.840" Id="91814" LastActivityDate="2014-04-11T12:54:30.697" LastEditDate="2014-04-02T11:51:45.170" LastEditorUserId="16175" OwnerUserId="16175" PostTypeId="1" Score="5" Tags="&lt;probability&gt;&lt;binomial&gt;" Title="Probability enemy territory captured in X turns" ViewCount="210" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a series of data sets. Each data set represents a measurement in 3D space relative to a global origin. I want to model the extreme values of my data. If I were to calculate the extreme radius and use that would I still expect a gumbel distribution? Alternatively I could produce a separate model for each axis if necessary but this is not as elegant as a single model. Ideally I would like to be able to predict the probability that a future experiment will have an extreme value less then X.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there anyway I can determine the parameters of the extreme value distribution from my normally distributed data? I have done some testing and it appears that I could reasonably assume that all my data sets are sample from the same population so I could assume this. Or, is it necessary to perform an MLE fit. I appreciate this functionality is available in most analysis software I was just curious how much one can tell about the properties of the EVD from the mean and standard deviation from which the extreme values are drawn.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-03-29T14:17:02.890" Id="91815" LastActivityDate="2014-03-29T14:17:02.890" OwnerUserId="7734" PostTypeId="1" Score="1" Tags="&lt;multivariate-analysis&gt;&lt;extreme-value&gt;" Title="Extreme value distribution for multivariate normal" ViewCount="49" />
  
  
  <row Body="&lt;p&gt;You can (and should) take a Bayesian probabilistic approach with either in the sense of putting prior probabilities on node values. It is not a good name because they are not &lt;em&gt;more&lt;/em&gt; Bayesian than undirected models. I think the name just comes from the fact that DAGs were used in expert systems and people applied Bayesian techniques to them and called them Bayesian networks and the name stuck. They should really be called directed and undirected graphical models. Most machine learning techniques are based on probability theory. I don't know of any of those that could be called non-Bayesian. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-03-29T18:39:25.583" Id="91837" LastActivityDate="2014-03-29T18:39:25.583" OwnerUserId="40967" ParentId="91831" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;There are lots of machine learning toolkits nowadays, such as &lt;em&gt;weka&lt;/em&gt;, &lt;em&gt;sklearn&lt;/em&gt;, &lt;code&gt;R&lt;/code&gt; libs.  If we choose to use these toolkits, besides that it is convenient, sometimes we might lose control of what is really happening.&#10;For example, in some learning methods the value of features should be scaled to $[0,1]$, but there may be lots of different methods which can achieve this. When we use a scaling function provided by a specific toolkit, we just don't know what's really happening, and this will cause trouble, say, when we want to check if different scaling would affect the performance of learning algorithms.&lt;/p&gt;&#10;&#10;&lt;p&gt;If the learning system is entirely implemented by ourselves, there could not be that much confusion, since we know exactly what happens.&#10;So what is the right attitude toward open source machine learning toolkits? &#10;Just use them for the first phase, and then implement manually without the help of learning toolkits in order to have more control over algorithms, or can we just rely on such kinds of learning toolkits? &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-03-29T20:16:55.973" FavoriteCount="4" Id="91851" LastActivityDate="2014-03-30T07:04:36.950" LastEditDate="2014-03-29T23:13:34.563" LastEditorUserId="32036" OwnerUserId="30695" PostTypeId="1" Score="5" Tags="&lt;machine-learning&gt;&lt;methodology&gt;&lt;scikit-learn&gt;&lt;weka&gt;&lt;open-source&gt;" Title="What is the right attitude toward open source machine learning toolkits?" ViewCount="253" />
  <row AnswerCount="1" Body="&lt;p&gt;Does anyone know how to graph changes in a dependent variable &lt;em&gt;over time&lt;/em&gt;, based on changes in &lt;em&gt;several&lt;/em&gt; independent variables (i.e., graphs for panel data regressions)?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-03-29T21:01:36.177" Id="91854" LastActivityDate="2015-03-06T13:04:45.927" LastEditDate="2014-03-29T21:18:16.277" LastEditorUserId="42759" OwnerUserId="42759" PostTypeId="1" Score="2" Tags="&lt;data-visualization&gt;&lt;panel-data&gt;" Title="Panel data visualization" ViewCount="124" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I read a Q/A from &lt;a href=&quot;http://stats.stackexchange.com/questions/8309/how-to-calculate-regularization-parameter-in-ridge-regression-given-degrees-of-f&quot;&gt;here&lt;/a&gt; which is extremely nice.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the above Q/Answer the tuning parameter $$ was a scalar. But in my problem it is a vector $\lambda$ for a generalized ridge regression case. The degrees of freedom is a fixed integer (assumed known).&lt;br&gt;&#10;I am struggling to set up an iteration search for vector $\lambda$ like the one given herein.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-03-30T00:57:06.100" Id="91864" LastActivityDate="2014-12-29T13:19:03.730" LastEditDate="2014-12-29T13:19:03.730" LastEditorUserId="28666" OwnerUserId="42761" PostTypeId="1" Score="1" Tags="&lt;regularization&gt;&lt;ridge-regression&gt;&lt;shrinkage&gt;" Title="How to calculate regularization parameters in generalized ridge regression given the degrees of freedom and input matrix?" ViewCount="51" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a survey of 20 questions; answers are given on the 5-point Likert scale. &lt;/p&gt;&#10;&#10;&lt;p&gt;Goal: to understand whether the answer on the first question affects answers on other questions.&lt;/p&gt;&#10;&#10;&lt;p&gt;Normally speaking I would be compiling a contingency table for  &quot;the first question vs another question&quot; and running the chi-squared test. However, here I need to compare Q1 with Q2, Q1 with Q3, ..., Q1 with Q20. Do I understand correctly that I still can apply the chi-squared test for each pair and adjust the p-values obtained? Or should I use a different statistical test altogether? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-30T09:14:12.017" Id="91883" LastActivityDate="2014-03-30T09:19:52.467" LastEditDate="2014-03-30T09:19:52.467" LastEditorUserId="11058" OwnerUserId="11058" PostTypeId="1" Score="1" Tags="&lt;repeated-measures&gt;&lt;chi-squared&gt;&lt;survey&gt;" Title="Chi-square and repeated measure" ViewCount="40" />
  <row Body="&lt;p&gt;A simple Bayesian approach would be to model each question response as a categorical distribution, with a prior using the &lt;a href=&quot;http://en.wikipedia.org/wiki/Dirichlet_distribution#Conjugate_to_categorical.2Fmultinomial&quot; rel=&quot;nofollow&quot;&gt;Dirichlet distribution&lt;/a&gt;. The categorical and Dirichlet distributions are multivariate generalizations of, respectively, the Bernoulli and beta distributions. The categorical takes a vector parameter, say $\theta$, of the probability of each response to the question, and the Dirichlet describes the density of a vector of of $k$ values summing to one, given a parameter $\alpha$ of $k$ positive-valued reals. In this case, $\theta$ is the parameter of interest: We compare the distributions of $P(\theta_i|D)$ and $P(\theta_j|D)$ for some questions $i,j$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The Dirichlet is the conjugate prior for the categorical and multinomial, and the posterior distribution of $\theta$ will take the form of a Dirichlet distribution with parameter $\alpha + c$, where c is the vector of observed counts of responses to each question. As nicely described on &lt;a href=&quot;http://en.wikipedia.org/wiki/Dirichlet_distribution#Conjugate_to_categorical.2Fmultinomial&quot; rel=&quot;nofollow&quot;&gt;wikipedia&lt;/a&gt;:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Intuitively, we can view the hyperprior vector  as pseudocounts, i.e. as representing the number of observations in each category that we have already seen. Then we simply add in the counts for all the new observations (the vector c) in order to derive the posterior distribution.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;A typical convenience prior for the hyperparameter $\alpha$ is the Jeffreys prior, $\alpha_i = \frac{1}{2}$ for all $i = 1, ..., k$. (The Jeffreys priors are a class of non-informative priors or so-called &quot;objective&quot; priors, often used in cases of scant prior information.)&lt;/p&gt;&#10;&#10;&lt;p&gt;In sum, the above model would produce a posterior model $P(\theta|D)$ that takes the form of a $Dir(\alpha_{post})$ distribution, where $\alpha_{post}$ is a vector of the observed number of responses to each category plus the corresponding prior parameter, $\frac{1}{2}$. Then it's just a matter of interpreting the posterior.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-03-30T14:21:25.620" Id="91894" LastActivityDate="2014-03-30T14:21:25.620" OwnerUserId="28462" ParentId="91453" PostTypeId="2" Score="0" />
  
  <row AcceptedAnswerId="91947" AnswerCount="3" Body="&lt;p&gt;Is the probability calculated by a logistic regression model (the one that is logit transformed) the fit of cumulative distribution function of successes of original data (ordered by the X variable)?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; In other words - how to plot the probability distribution of the original data that you get when you fit a logistic regression model?&lt;/p&gt;&#10;&#10;&lt;p&gt;The motivation for the question was Jeff Leak's example of regression on the Raven's score in a game and whether they won or not (from Coursera's Data Analysis course). Admittedly, the problem is artificial (see @FrankHarrell's comment below). Here is his data with a mix of his and my code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;download.file(&quot;http://dl.dropbox.com/u/7710864/data/ravensData.rda&quot;, &#10;              destfile=&quot;ravensData.rda&quot;, method=&quot;internal&quot;)&#10;load(&quot;ravensData.rda&quot;)&#10;&#10;plot(ravenWinNum~ravenScore, data=ravensData)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Cr5ka.png&quot; alt=&quot;enter image description here&quot;&gt;  &lt;/p&gt;&#10;&#10;&lt;p&gt;It doesn't seem like good material for logistic regression, but let's try anyway:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;logRegRavens &amp;lt;- glm(ravenWinNum ~ ravenScore, data=ravensData, family=binomial)&#10;summary(logRegRavens)&#10;# the beta is not significant&#10;&#10;# sort table by ravenScore (X)&#10;rav2 = ravensData[order(ravensData$ravenScore), ]&#10;&#10;# plot CDF&#10;plot(sort(ravensData$ravenScore), cumsum(rav2$ravenWinNum)/sum(rav2$ravenWinNum), &#10;         pch=19, col=&quot;blue&quot;, xlab=&quot;Score&quot;, ylab=&quot;Prob Ravens Win&quot;, ylim=c(0,1), &#10;         xlim=c(-10,50))&#10;# overplot fitted values (Jeff's)&#10;points(ravensData$ravenScore, logRegRavens$fitted, pch=19, col=&quot;red&quot;)&#10;# overplot regression curve&#10;curve(1/(1+exp(-(logRegRavens$coef[1]+logRegRavens$coef[2]*x))), -10, 50, add=T)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;If I understand logistic regression correctly, R does a pretty bad job at finding the right coefficients in this case.  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Cb6o8.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;blue = original data to be fitted, I believe (CDF)  &lt;/li&gt;&#10;&lt;li&gt;red = prediction from the model (fitted data = projection of original data onto regression curve)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;SOLVED&lt;/strong&gt;&lt;br&gt;&#10; - lowess seems to be a good non-parametric estimator of the original data = what is being fitted (thanks @gung). Seeing it allows us to choose the right model, which in this case would be adding squared term to the previous model (@gung)&lt;br&gt;&#10; - Of course, the problem is pretty artificial and modelling it rather pointless in general (@FrankHarrell)&lt;br&gt;&#10; - in regular logistic regression it's not CDF, but point probabilities - first pointed out by @FrankHarrell; also my embarrassing inability to calculate CDF pointed out by @gung.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-30T15:49:18.277" Id="91903" LastActivityDate="2014-04-02T17:18:19.663" LastEditDate="2014-03-31T07:25:19.127" LastEditorUserId="42778" OwnerUserId="42778" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;logistic&gt;&lt;cdf&gt;" Title="CDF and logistic regression" ViewCount="414" />
  
  
  
  
  <row AcceptedAnswerId="91927" AnswerCount="1" Body="&lt;p&gt;If my data are non-normally distributed and I'm conducting a 2x2 ANOVA, what can I do to correct for this problem so I can report the main effect and interaction output appropriately? &lt;/p&gt;&#10;&#10;&lt;p&gt;Only one finding is significant (one of the main effects).  I've read that bootstrapping cannot be applied to a univariate ANOVA in SPSS...I also tried this out in SPSS myself and could not obtain any bootstrapped output.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;More information:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I used a 2 x 2 (Age [6 years, 7 years]  Educational Classification [Group A, Group B]) univariate ANOVA to explore the hypothesis that test performance (percentage of items correct) would increase with age for children in Group B but would not differ significantly across age for students in Group A.  I transformed the percentages into arcsine values to meet the assumption of a continuous dependent variable.  My sample is small, so the sample size for each cell created by the 2 x 2 are as follows:   Group A 6-year-olds = 10, Group A 7-year-olds = 13, Group B 6-year-olds = 20, and Group B 7-year-olds = 14.  I know that scores are non-normally distributed across Group A and across Group B due to most students obtaining high scores on the test (and I ran tests of normality and looked at the Q-Q plots).  Group A, which is also the smaller group of the 2, has  larger variance.  But should I run normality tests for all 4 groups created by the 2 x 2? I found a Main Effect for Educational Classification but not for Age, and there was no interaction.  I just want to know if my findings are valid given the non-normality, or if I should find a way to fix this problem in SPSS (e.g., bootstrapping).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-03-30T19:01:52.347" Id="91919" LastActivityDate="2014-04-19T00:36:28.580" LastEditDate="2014-04-18T13:30:34.473" LastEditorUserId="42784" OwnerUserId="42784" PostTypeId="1" Score="4" Tags="&lt;anova&gt;&lt;spss&gt;&lt;bootstrap&gt;&lt;normality&gt;" Title="Using Univariate ANOVA with non-normally distributed data" ViewCount="603" />
  
  <row Body="&lt;p&gt;&lt;a href=&quot;http://math.stackexchange.com/questions/242779/limit-of-lp-norm/242792#242792&quot;&gt;Here&lt;/a&gt; is a proof of the statement. The main ideas are the following:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;the inequality $\lVert f\rVert_p\leqslant \lVert f\rVert_\infty$ follows from the fact that $|f(x)|\leqslant \lVert f\rVert_\infty$ for $\mu$-almost every $x$;&lt;/li&gt;&#10;&lt;li&gt;for the converse inequality, we integrate over the set where $f$ is a greater than a number very close to the uniform norm. &lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="2" CreationDate="2014-03-30T20:27:35.583" Id="91925" LastActivityDate="2014-03-30T20:27:35.583" OwnerUserId="14675" ParentId="91914" PostTypeId="2" Score="6" />
  
  <row Body="&lt;p&gt;SPSS aside (I can't help you with that, sorry, I haven't used SPSS in decades), it's a relatively simple matter to use bootstrapping in an ANOVA, but before one even tries to do that it's important to consider what is being assumed and whether it makes sense with your variables. So you should be telling us some things about your response (DV).&lt;/p&gt;&#10;&#10;&lt;p&gt;The first point to make is that in ANOVA the marginal distribution of the response isn't assumed to be normal; it's the conditional distribution. &lt;em&gt;How&lt;/em&gt; are you coming to the conclusion that your data are non-normal (how are you identifying the distribution?), and how non-normal are we talking?&lt;/p&gt;&#10;&#10;&lt;p&gt;The second point is that the importance of normality changes with sample size, yet you don't mention total sample size.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;In using the bootstrap, you will need to regard some collection of quantities as exchangeable. In a two-way ANOVA this would normally be some form of residual but for residuals to be exchangeable, you need (for example) the variance of and the shape of the distribution not to change with the mean. These considerations would usually rule out applying it to count data, for example.&lt;/p&gt;&#10;&#10;&lt;p&gt;You do have alternatives; for some kinds of data you might consider applying a GLM to fit an ANOVA-like model -- I believe that is something you &lt;em&gt;can&lt;/em&gt; do in SPSS.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Edit in response to the additional information in your edit to your question:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;&quot;Percentage of items correct&quot; is a count (number of items correct) divided by a fixed total (number of items). Scaling aside, this is count data of a sort, for which ANOVA would not normally be appropriate, since you won't likely have linearity (because the response is bounded above and below, though this will only affect the size of the interaction in your case), and the equal variance assumption won't hold across different means (variance must vary as a function of the mean, because of the bounds). &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;an &lt;em&gt;arcsin square root&lt;/em&gt; transformation would help to stabilize the variance, but it will not &quot;make the data continuous&quot; -- it's still just as discrete as before. It may help a little with the skewness, but it might not make much different. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;There are models more suited to 2x2 count data (e.g. binomial GLMs, loglinear models, even chi-square tests) - but your data might not fit the usual models for counts because test questions are rarely of uniform difficulty and - even withing sub-groups - people are rarely of uniform ability. [You might try a binomial model and see if it's plausible. It's possible that a negative binomial model might be able to deal with the possible heterogeneity.]&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;If you're contemplating an &lt;em&gt;arcsin square root&lt;/em&gt; transformation, it's usually an indication you should have used a GLM.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="1" CreationDate="2014-03-30T21:20:11.963" Id="91927" LastActivityDate="2014-04-19T00:36:28.580" LastEditDate="2014-04-19T00:36:28.580" LastEditorUserId="805" OwnerUserId="805" ParentId="91919" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;Given that you trust your validation setup option 2 is the way to go. You have performed the CV to identify the most general parameter setup (or model selection or whatever you're trying to optimize). These findings should be applied to the entire trainingset and tested (once) on the test set. The picture below illustrates a setup I think works well when evaluating and testing the performance of machine learning algorithms.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ulwfk.png&quot; alt=&quot;Illustration of a rigorous cv setup&quot;&gt;&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-03-30T22:13:22.247" Id="91934" LastActivityDate="2014-03-30T22:20:31.497" LastEditDate="2014-03-30T22:20:31.497" LastEditorUserId="5764" OwnerUserId="5764" ParentId="91842" PostTypeId="2" Score="0" />
  <row AnswerCount="2" Body="&lt;p&gt;Can a unimodal multivariate distribution have a multimodal marginal distribution? &lt;/p&gt;&#10;&#10;&lt;p&gt;If all marginal distributions are unimodal, can the multivariate distribution be multimodal?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-30T22:31:32.780" Id="91937" LastActivityDate="2014-03-31T02:56:37.437" LastEditDate="2014-03-31T00:22:04.043" LastEditorUserId="919" OwnerUserId="1005" PostTypeId="1" Score="4" Tags="&lt;distributions&gt;&lt;marginal&gt;&lt;mode&gt;" Title="How are the numbers of modes of marginal and joint distributions related?" ViewCount="192" />
  <row Body="&lt;p&gt;I don't think you could have a precise estimation of the number of streets in an historical city (like Valladolid) with a regression (if you will not use a lot of variables). Personally, I would use a more ICT approach, writing a script to look into a website, such as &lt;a href=&quot;http://www.paginasamarillas.es/&quot; rel=&quot;nofollow&quot;&gt;http://www.paginasamarillas.es/&lt;/a&gt;, for the addresses associates to the stored public numbers. I will delete all the duplicates, and I will count the items inside the array to have an estimate.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-03-30T23:04:10.640" Id="91940" LastActivityDate="2014-03-30T23:04:10.640" OwnerUserId="40899" ParentId="91936" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Yes and yes, and examples are easy to devise.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $(X,Y)$ have discrete distribution that has&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;probability mass of $0.12$ at $(0,0)$ and $0.08$ at the $4$ points $(\pm 1,\pm 1)$.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;probability mass of $0.07$ at each of $(\pm 2, \pm 2)$ and $(\pm 3, \pm 3)$.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;This is a unimodal bivariate distribution with a mode of $0.12$ at $(0,0)$&#10;However, the marginal distributions &#10;are&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\begin{array}{cccccccc}&#10;-3&amp;amp; -2&amp;amp; -1&amp;amp; 0 &amp;amp; +1 &amp;amp; +2 &amp;amp; +3\\&#10;0.14 &amp;amp; 0.14&amp;amp; 0.16 &amp;amp; 0.12 &amp;amp; 0.16 &amp;amp; 0.14 &amp;amp; 0.14&#10;\end{array}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;with twin peaks of $0.16$ at $\pm 1$ and &lt;em&gt;least likely&lt;/em&gt; value $0$ where&#10;the mode of the bivariate distribution is!&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;As an example of unimodal marginal distributions but a &#10;multimodal joint distribution, consider random variables $X$ and $Y$&#10;that have identical unimodal marginal distributions &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\begin{array}{cccccccc}&#10;-2&amp;amp; -1&amp;amp; 0 &amp;amp; +1 &amp;amp; +2 \\&#10;0.05 &amp;amp; 0.2&amp;amp; 0.5 &amp;amp; 0.2 &amp;amp; 0.05&#10;\end{array}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and bivariate distribution that has masses of $0.2$ at $(0,1), (1,0), (0,-1), (-1,0)$ and masses of $0.05$ at $(0,2), (2,0), (0,-2), (-2,0)$ which has &#10;$4$ modes.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-03-30T23:19:10.897" Id="91941" LastActivityDate="2014-03-31T02:56:37.437" LastEditDate="2014-03-31T02:56:37.437" LastEditorUserId="6633" OwnerUserId="6633" ParentId="91937" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;There are several issues here.  The biggest is that the thinking behind the code for generating the blue points in the second plot is confused.  I can't make any sense of what the logic is supposed to be.  The $x$ values are the same as your data, but the $y$ values are the number of wins up to that point on the $x$ axis divided by the total number of wins.  This really makes no sense.  You aren't even dividing by the &lt;em&gt;total number of games&lt;/em&gt;, which means you don't have the cumulative probability at that point (although that still wouldn't make sense).  This also makes you wonder why the last blue point isn't at $1.0$.  Actually it is, but the last point is at $x=55$, which is excluded by your argument &lt;code&gt;xlim=c(-10,50)&lt;/code&gt;.  &lt;/p&gt;&#10;&#10;&lt;p&gt;At any rate, if you want to get a non-parametric estimate of what the underlying function might be, you could plot a lowess line:  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;plot(ravenWinNum~ravenScore, data=ravensData)&#10;lines(with(ravensData, lowess(ravenWinNum~ravenScore)))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/8IFBT.png&quot; alt=&quot;enter image description here&quot;&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;What is clear from this figure, is that there is a curvilinear relationship between the score and the probability of winning (at least in these data, we can debate whether that makes any theoretical sense).  If you thought about it, it was clear from the plot of the raw data in that there are increasing numbers of wins at both the low end and high end of $x$; the losses are in the center.  A very simple way to fit a curvilinear relationship is to add a squared term:  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lr.Ravens2 &amp;lt;- glm(ravenWinNum~ravenScore+I(ravenScore^2), data=ravensData, &#10;                  family=binomial)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;We can test these models as a whole by assessing their deviance against the null deviance and comparing the result to a chi-squared distribution with the number of degrees of freedom consumed.  We do this because we want to think of both the linear term and the squared term together as a unit.  This method of testing the model as a whole is analogous to the global $F$-test that comes with a multiple regression model.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# here is the test of the model with the squared term as a whole&#10;1-pchisq(24.435-16.875, df=2)&#10;[1] 0.02282269&#10;# here is the test of your model&#10;1-pchisq(24.435-20.895, df=1)&#10;[1] 0.05990546&#10;# this is the test of the improvement due to adding the squared term&#10;1-pchisq(20.895-16.875, df=1)&#10;[1] 0.04496371&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I wouldn't make too much of the arbitrary threshold of significance at $.05$, but we see that the initial model is not significant by conventional standards, or at least is slightly less significant.  (Given that you have so few data, I would think a higher $\alpha$ is fine.)  The model that includes the squared term as well is more significant, even though it uses up another of your precious few degrees of freedom.  The AIC is lower as well: the first model has $AIC=24.895$, whereas the second model has $AIC=22.875$.  The last of the tests listed shows that the addition of the squared term yields a significant improvement in fit.  &lt;/p&gt;&#10;&#10;&lt;p&gt;The implication of the fact that there is a curvilinear relationship between $x$ and $y$ is that the function that maps the conditional probability that $y_i=1$ given $x_i$ cannot be equated to a cumulative probability distribution that takes $x$ as its quantiles.  &lt;/p&gt;&#10;&#10;&lt;p&gt;For completeness, here is the same plot with the model's predicted probabilities overlaid:  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/bqwsd.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;We see that the predicted probabilities mirror the lowess line reasonably well.  Of course, they don't match perfectly, but there is no reason to believe that the lowess line is exactly correct; it just provides a non-parametric approximation of whatever the underlying function is.  &lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Having addressed the specific example in your question, it is worth addressing the assumption underlying the question more broadly.  &lt;em&gt;What is the relationship between logistic regression and a cumulative distribution function (CDF)?&lt;/em&gt;  &lt;/p&gt;&#10;&#10;&lt;p&gt;There can be a connection between binomial regression and a CDF.  For example, when the probit link is used, the conditional probability of 'success' ($\pi_i(Y_i=1|X=x_i)$) is transformed via the inverse of the normal CDF.  (For more about that, it may help to read my answer here: &lt;a href=&quot;http://stats.stackexchange.com/a/30909/7290&quot;&gt;difference between logit and probit models&lt;/a&gt;.)  But there are a couple of points to be made here.  First, as @FrankHarrell and @Glen_b note, binomial (e.g., logistic) regression is about modeling conditional probabilities at each point.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Second, although the fitted function can look sort of like a CDF when the conditions are just right, it will &lt;em&gt;never&lt;/em&gt; actually be a function (scaled, shifted, etc.) of the CDF of your $X$ data.  Your own data serve as an example of the function not even looking like a CDF because you have a polynomial relationship.  But it is true even when the function does look like a CDF.  The easiest way to understand this is that your last (highest $x$ valued) point will always be 100% of the way through your data, but the fitted function will never reach $\hat p_i=1.0$, it can only approach that value as $X$ (the variable, not your data) approaches infinity.  As a result, you should not think of a function of the CDF of your $x$ data as the values the logistic regression is supposed to approximate.  &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-03-31T00:46:53.293" Id="91947" LastActivityDate="2014-04-02T17:18:19.663" LastEditDate="2014-04-02T17:18:19.663" LastEditorUserId="7290" OwnerUserId="7290" ParentId="91903" PostTypeId="2" Score="0" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to do PCA on a series of variables (all are positive, real numbers) using correlation and varimax rotation. All the computation is done in R.&lt;/p&gt;&#10;&#10;&lt;p&gt;Although I got high loadings for all variables of interest, the frequency distribution of the obtained PC2 shows an exponential distribution, meaning that the extremes of one side of this axis contains a very small number of observations. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have tried several transformations: logarithmic, square root, centering and scaling, and rank, and none of them solves the problem. What do you think I should do? Leave it like that?&lt;/p&gt;&#10;&#10;&lt;p&gt;For further contextualization, I want to use the obtained PCs for their use in a multiple regression analysis.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2014-03-31T03:37:08.297" Id="91958" LastActivityDate="2014-03-31T10:08:22.137" LastEditDate="2014-03-31T10:08:22.137" LastEditorUserId="22047" OwnerUserId="14465" PostTypeId="1" Score="1" Tags="&lt;pca&gt;&lt;data-transformation&gt;" Title="How deal with exponential distribution of data during principal component analysis" ViewCount="61" />
  <row AnswerCount="3" Body="&lt;p&gt;An online module I am studying states that one should &lt;strong&gt;never&lt;/strong&gt; use Pearson correlation with proportion data. Why not?&lt;/p&gt;&#10;&#10;&lt;p&gt;Or, if it is sometimes OK or always OK, why?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-03-31T03:50:19.790" FavoriteCount="2" Id="91960" LastActivityDate="2015-01-24T23:15:36.760" LastEditDate="2014-03-31T23:48:45.227" LastEditorUserId="9162" OwnerUserId="9162" PostTypeId="1" Score="7" Tags="&lt;correlation&gt;&lt;proportion&gt;" Title="Why is it not OK to do a Pearson correlation on proportion data?" ViewCount="533" />
  <row Body="&lt;p&gt;I haven't worked the proof myself, but according to &lt;a href=&quot;http://www.lsv.uni-saarland.de/Seminar/ML_for_NLP_SS12/HinSal06.pdf&quot; rel=&quot;nofollow&quot;&gt;Hinton et al (2006)&lt;/a&gt; more RBMs should in fact always be better* as another RBM always increases the lower bound on the probability that the training data can be reproduced. &lt;/p&gt;&#10;&#10;&lt;p&gt;*The exception to this is when an RBM has as many or more visible units as the RBM before it has hidden layers, as then the chances of simply learning the identify function go up. For example, if you have an input of dimension 1000, then you should always get an increase in predictive performance if your next layers are 700, 500, 200, etc. up to the point at which the output of the last RBM matches the &quot;true&quot; dimensionality of variation in the data, which is a question of experimentation or expertise.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-31T04:24:29.460" Id="91962" LastActivityDate="2014-03-31T04:24:29.460" OwnerUserId="12678" ParentId="70258" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;Expanding on @Stephen 's comment, why do you expect the counts to be better with ZIP or ZINB than with P or NB? Predicting the total counts is not what regression tries to do: It tries to model the dependent variable on the covariates. So, if you want to see if (say) ZINB is better than NB you could (in addition to what you have already done), do something like this (untested code):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;plot(x = predprob(nb), y = jitter(data$TRUE))&#10;    plot(x = predprob(zinb), y = jitter(data$TRUE))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and compare them, or combine them in one graph:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;nberror &amp;lt;- predprob(nb) - data$TRUE&#10;    zinberror &amp;lt;- predprob(zinb) - data$TRUE&#10;plot(x = nberror, y = zinberror)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;to see where ZINB does better and where NB does better. Or you could plot both densities:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;plot(density(nberror)&#10;lines(density(zinberror, col = 2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and compare them.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-03-31T10:06:57.623" Id="91984" LastActivityDate="2014-03-31T10:06:57.623" OwnerUserId="686" ParentId="91978" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="92007" AnswerCount="1" Body="&lt;p&gt;There are 10 dependent variables and 2 groups. When I wanted to compare 10 dependent variables between two groups, I performed MANOVA.&lt;/p&gt;&#10;&#10;&lt;p&gt;I wonder if there is a multiple comparison problem in that case. Should I have performed Bonferroni correction because of 10 dependent variables? Or is Bonferroni useless because MANOVA just does analysis once?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-03-31T13:28:51.133" FavoriteCount="1" Id="92000" LastActivityDate="2014-04-01T10:12:42.860" LastEditDate="2014-04-01T10:12:42.860" LastEditorUserId="28666" OwnerUserId="42818" PostTypeId="1" Score="2" Tags="&lt;multiple-comparisons&gt;&lt;manova&gt;&lt;bonferroni&gt;" Title="Does one need to adjust for multiple comparisons when using MANOVA?" ViewCount="563" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am working on a 2-class classification problem and using LIBSVM. I am facing this following problem with cross validation (CV). Any help is highly appreciated. &lt;/p&gt;&#10;&#10;&lt;p&gt;When I perform a 10 fold CV with -v 10 option, I get an accuracy of 90%. But when I do a training and testing on the same data with 90-10 split, the accuracy drops to 50%. How can this be explained?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-03-31T16:12:48.953" Id="92022" LastActivityDate="2015-02-18T05:39:04.013" LastEditDate="2015-02-18T05:39:04.013" LastEditorUserId="53618" OwnerUserId="42830" PostTypeId="1" Score="0" Tags="&lt;cross-validation&gt;&lt;libsvm&gt;" Title="strange behavior for LIBSVM during cross-validation" ViewCount="38" />
  <row Body="&lt;p&gt;I'm guessing what you heard was related to the &lt;a href=&quot;http://en.wikipedia.org/wiki/Elo_rating_system&quot; rel=&quot;nofollow&quot;&gt;Elo ranking system&lt;/a&gt; or a variant thereof. Where a logistic function comes into play there is in estimating the likelihood that person X will beat person Y.&lt;/p&gt;&#10;&#10;&lt;p&gt;The factors you've mentioned would prevent logistic regression from being used to determine what rank in a tournament someone would be.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-03-31T17:18:59.370" Id="92030" LastActivityDate="2014-03-31T17:18:59.370" OwnerUserId="900" ParentId="92025" PostTypeId="2" Score="1" />
  <row AnswerCount="2" Body="&lt;p&gt;I am trying to implement and learn a Dirichlet Process to cluster my data (or as machine learning people speak, estimate the density). &lt;/p&gt;&#10;&#10;&lt;p&gt;I read a lot of paper in the topic and sort of got the idea. But I am still confused; here are a series of question,&lt;/p&gt;&#10;&#10;&lt;p&gt;1) What is the different between Chinese Restaurant Model and DP ?&#10;2) What is the different between Infinite Mixture Models and DP ?&lt;/p&gt;&#10;&#10;&lt;p&gt;To fully understand everything, I have implemented Chinese Restaurant Model, Polya Urn model and Stick-breaking; But it seems, implementing DP from scratch is a hard thing to do ! I can read and write python, R, Matlab. &lt;/p&gt;&#10;&#10;&lt;p&gt;1) Is there any code you recommend to read and improve to fully understand/work/develop DP ? &#10;2) Based on my research, codes, for Dirichlet Process were not easy to read ! really long, lengthy (probably since the efficiency were more important that clarity).&#10;3) However, there is more code on Infinite Mixture Model than Dirichlet Process. If these two methods are not far from each other can I use IMM ?! Basically, I want to build up my new model, and I don't want to re-invent a wheel.&lt;/p&gt;&#10;&#10;&lt;p&gt;I appreciate your comments&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-03-31T18:10:13.620" Id="92034" LastActivityDate="2014-12-11T10:43:09.093" OwnerUserId="4581" PostTypeId="1" Score="2" Tags="&lt;machine-learning&gt;&lt;clustering&gt;&lt;dirichlet-process&gt;" Title="Understanding and Implementing a Dirichlet Process model" ViewCount="226" />
  
  <row Body="&lt;p&gt;Here is a simple heuristic: if you assume elements in any vector sum to $1$ (or simply normalize each element with the sum to achieve this), then uniformity can be represented by L2 norm, which ranges from $1/\sqrt d$ to $1$, with $d$ being the dimension of vectors. The lower bound $1/\sqrt d$ corresponds to uniformity and upper bound to the $1$-hot vector.&lt;/p&gt;&#10;&#10;&lt;p&gt;To scale this to a score between $0$ and $1$, you can use $(n*\sqrt d - 1)/(\sqrt d - 1)$, where $n$ is the L2 norm.&lt;/p&gt;&#10;&#10;&lt;p&gt;An example modified from urs --- with elements summing to $1$ and all vectors with the same dimension for simplicity:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;0.10    0.11    0.10    0.09    0.09    0.11    0.10    0.10    0.12    0.08&#10;0.10    0.10    0.10    0.08    0.12    0.12    0.09    0.09    0.12    0.08&#10;0.03    0.02    0.61    0.02    0.03    0.07    0.06    0.05    0.06    0.05&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The following will yield $0.0028$, $0.0051$, and $0.4529$ for the rows:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;d=size(m,2); &#10;for i=1:size(m); &#10;    disp( (norm(m(i,:))*sqrt(d)-1) / (sqrt(d)-1) ); &#10;end&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2014-03-31T21:33:27.143" Id="92056" LastActivityDate="2014-03-31T21:58:21.737" LastEditDate="2014-03-31T21:58:21.737" LastEditorUserId="7290" OwnerUserId="42843" ParentId="25827" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="92119" AnswerCount="1" Body="&lt;p&gt;I'm wondering how an instrumental variable addresses selection bias in regression. &lt;/p&gt;&#10;&#10;&lt;p&gt;Here's the example I'm chewing on: In &lt;em&gt;Mostly Harmless Econometrics&lt;/em&gt;, the authors discuss an IV regression relating to military service and earnings later in life. The question is, &quot;Does serving in the military increase or decrease future earnings?&quot; They investigate this question in the context of the Vietnam war. I understand that military service cannot be randomly assigned, and that this is a problem for causal inference. &lt;/p&gt;&#10;&#10;&lt;p&gt;To address this issue, the researcher uses draft eligibility (as in &quot;your draft number is called&quot;) as an instrument for actual military service. That makes sense: the Vietnam draft &lt;em&gt;randomly&lt;/em&gt; assigned young American men to the military (in theory--whether the draftees actually served touches on my question). Our other IV condition seems solid: draft eligibility and actual military service are strongly, positively correlated. &lt;/p&gt;&#10;&#10;&lt;p&gt;Here's my question. It seems like you'd get self-selection bias: maybe richer kids can get out of serving in Vietnam, even if their draft numbers are called. (If that wasn't actually the case, let's pretend for the sake of my question). If this self-selection creates systemic bias within our sample, how does our instrumental variable address this bias? Must we narrow our scope of inference to &quot;the types of people who couldn't escape the draft?&quot; Or does the IV somehow salvage that part of our inference? If anybody could explain how this works, I'd be very grateful. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-01T01:06:49.607" FavoriteCount="1" Id="92072" LastActivityDate="2014-08-30T02:57:56.690" LastEditDate="2014-08-30T02:57:56.690" LastEditorUserId="42849" OwnerUserId="42849" PostTypeId="1" Score="8" Tags="&lt;regression&gt;&lt;econometrics&gt;&lt;bias&gt;&lt;causal-inference&gt;&lt;instrumental-variables&gt;" Title="How do instrumental variables address selection bias?" ViewCount="289" />
  <row Body="&lt;p&gt;When you fit a regression model such as $\hat y_i = \hat\beta_0 + \hat\beta_1x_i + \hat\beta_2x^2_i$, the model and the OLS estimator doesn't 'know' that $x^2_i$ is simply the square of $x_i$, it just 'thinks' it's another variable.  Of course there is some collinearity, and that gets incorporated into the fit (e.g., the standard errors are larger than they might otherwise be), but lots of pairs of variables can be somewhat collinear without one of them being a function of the other.  &lt;/p&gt;&#10;&#10;&lt;p&gt;We don't recognize that there are really two separate variables in the model, because &lt;em&gt;we&lt;/em&gt; know that $x^2_i$ is ultimately the same variable as $x_i$ that we transformed and included in order to capture a curvilinear relationship between $x_i$ and $y_i$.  That knowledge of the true nature of $x^2_i$, coupled with our belief that there is a curvilinear relationship between $x_i$ and $y_i$ is what makes it difficult for us to understand the way that it is still linear from the model's perspective.  In addition, we visualize $x_i$ and $x^2_i$ together by looking at the marginal projection of the 3D function onto the 2D $x, y$ plane.  &lt;/p&gt;&#10;&#10;&lt;p&gt;If you only have $x_i$ and $x^2_i$, you can try to visualize them in the full 3D space (although it is still rather hard to really see what is going on).  If you did look at the fitted function in the full 3D space, you would see that the fitted function is a 2D plane, and moreover that it is a flat plane.  As I say, it is hard to see well because the $x_i, x^2_i$ data exist only along a curved line going through that 3D space (that fact is the visual manifestation of their collinearity).  We can try to do that here.  Imagine this is the fitted model:  &lt;/p&gt;&#10;&#10;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;x     = seq(from=0, to=10, by=.5)&#10;x2    = x**2&#10;y     = 3 + x - .05*x2&#10;d.mat = data.frame(X1=x, X2=x2, Y=y)&#10;&#10;# 2D plot&#10;plot(x, y, pch=1, ylim=c(0,11), col=&quot;red&quot;, &#10;     main=&quot;Marginal projection onto the 2D X,Y plane&quot;)&#10;lines(x, y, col=&quot;lightblue&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/YDxju.png&quot; alt=&quot;enter image description here&quot;&gt;  &lt;/p&gt;&#10;&#10;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;# 3D plot&#10;library(scatterplot3d)&#10;s = scatterplot3d(x=d.mat$X1, y=d.mat$X2, z=d.mat$Y, color=&quot;gray&quot;, pch=1, &#10;                  xlab=&quot;X1&quot;, ylab=&quot;X2&quot;, zlab=&quot;Y&quot;, xlim=c(0, 11), ylim=c(0,101), &#10;                  zlim=c(0, 11), type=&quot;h&quot;, main=&quot;In pseudo-3D space&quot;)&#10;    s$points(x=d.mat$X1, y=d.mat$X2, z=d.mat$Y, col=&quot;red&quot;, pch=1)&#10;    s$plane3d(Intercept=3, x.coef=1, y.coef=-.05, col=&quot;lightblue&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Zx48N.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;It may be easier to see in these images, which are screenshots of a rotated 3D figure made with the same data using the &lt;code&gt;rgl&lt;/code&gt; package.  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/AvytM.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;When we say that a model that is &quot;linear in the parameters&quot; really is linear, this isn't just some mathematical sophistry.  With $p$ variables, you are fitting a $p$-dimensional hyperplane in a $p\!+\!1$-dimensional hyperspace (in our example a 2D plane in a 3D space).  That hyperplane really is 'flat' / 'linear'; it isn't just a metaphor.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-01T04:12:19.813" Id="92087" LastActivityDate="2014-09-12T20:11:41.137" LastEditDate="2014-09-12T20:11:41.137" LastEditorUserId="7290" OwnerUserId="7290" ParentId="92065" PostTypeId="2" Score="11" />
  
  
  <row AcceptedAnswerId="92128" AnswerCount="1" Body="&lt;p&gt;I've asked this question on &lt;a href=&quot;http://stackoverflow.com/questions/22767945/fitting-logistic-function-with-pymc&quot;&gt;stackoverflow&lt;/a&gt; too, but no answer yet. This seems a more appropariate place to ask this question:&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm messing around with pymc to understand it a bit better. Now I am trying to fit a logistic function to my generated data. Though the parameter it returns is not even close to the real parameter. Any idea why ?&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;import pymc as pm&#10;from matplotlib import pyplot as plt&#10;import numpy as np&#10;from pymc.Matplot import plot as mcplot&#10;&#10;&#10;#data generation&#10;def logistic(x, beta):&#10;    return 1.0 / (1.0 + np.exp(beta * x))&#10;&#10;#hidden&#10;beta = -5&#10;&#10;x = np.linspace(-4, 4, 600)&#10;data = logistic(x, -5)&#10;&#10;plt.plot(x,data , label=r&quot;$\beta = -5$&quot;)&#10;plt.legend();&#10;&#10;plt.show()&#10;&#10;&#10;&#10;&#10;#Priors&#10;beta = pm.Uniform(&quot;beta&quot;, -10, 10, value=1.)&#10;sig = pm.Uniform(&quot;sig&quot;, 0.0, 100.0, value=1.)&#10;&#10;&#10;@pm.deterministic&#10;def logistic(x=data, beta=beta):&#10;    return 1.0 / (1. + np.exp(beta * x))&#10;&#10;&#10;#likelihood&#10;y = pm.Normal(&quot;obs&quot;, mu=logistic, tau=1.0/sig**2, value=data, observed=True)&#10;&#10;&#10;model = pm.Model([y, data,beta,sig])&#10;mcmc = pm.MCMC(model) &#10;mcmc.sample(20000, 5000, 1)&#10;&#10;&#10;beta_samples = mcmc.trace('beta')[:]&#10;sig_samples = mcmc.trace('sig')[:]&#10;&#10;print beta_samples.mean()&#10;print sig_samples.mean()&#10;&#10;mcplot(mcmc.trace('beta'))&#10;mcplot(mcmc.trace('sig'))&#10;plt.show()&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2014-04-01T10:15:22.493" Id="92120" LastActivityDate="2014-04-01T13:02:10.357" OwnerUserId="17192" PostTypeId="1" Score="0" Tags="&lt;fitting&gt;&lt;curve-fitting&gt;&lt;pymc&gt;" Title="Fitting logistic function with pymc" ViewCount="134" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am doing GWAS SNP association studies on diseases by using a software called plink (&lt;a href=&quot;http://pngu.mgh.harvard.edu/~purcell/plink/download.shtml&quot;&gt;http://pngu.mgh.harvard.edu/~purcell/plink/download.shtml&lt;/a&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;With association results I get p-values for all the SNPs that was analyzed. Now, I use a QQ-plot of those p-values to show if a very low p-value differs from the expected distribution of p-values (a uniform distribution). If a p-value deviates from the expected distribution one &quot;may&quot; call that p-value for statistic significant.&lt;/p&gt;&#10;&#10;&lt;p&gt;As you can see in the QQ-plot, at the top tail end, the last 4 points are somewhat hard to interpret. Two of the last points in the grey suggests that those p-values are in the expected distribution of p-values, whilst the other two are not.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, how to interpret this, the last two points have &lt;em&gt;lower&lt;/em&gt; p-values but are not &quot;significant&quot; according to the QQ-plot, whilst the other two points with &lt;em&gt;higher&lt;/em&gt; p-values are &quot;significant&quot;? How can this be true?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/zwSwt.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-04-01T11:44:21.703" FavoriteCount="1" Id="92124" LastActivityDate="2014-04-01T11:44:21.703" OwnerUserId="21840" PostTypeId="1" Score="5" Tags="&lt;qq-plot&gt;" Title="How to interpret a QQ-plot of p-values" ViewCount="856" />
  <row AcceptedAnswerId="92166" AnswerCount="1" Body="&lt;p&gt;I have to conduct a meta-analysis. Following problem occured: a single study have measured Quality of Life and therefore, used two questionnaires. Now is my question whether I have two choose one of the instrument to include in meta-analysis or is there a possibility to combine the means of the two scales to provide one standard mean difference in the meta-analysis?&#10;THANK YOU!!!!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-01T13:51:24.813" Id="92134" LastActivityDate="2014-04-01T18:41:17.360" OwnerUserId="42880" PostTypeId="1" Score="2" Tags="&lt;meta-analysis&gt;&lt;combination&gt;" Title="Combining outcome measures for meta-analysis" ViewCount="136" />
  <row Body="&lt;p&gt;I don't see it as a curse.  I think in most cases statisticians are not the subject matter experts, hence they lack the deep knowledge to identify the important problems and the expertise needed to tackle them.  Whereas, the subject matter experts usually lack the deep statistical knowledge required to solve those problems.  Becoming aware of those strengths and weaknesses necessarily lead to collaboration. The best collaborative efforts are usually those where the investigator seeks the statistician's input early on. &lt;/p&gt;&#10;&#10;&lt;p&gt;Great collaboration usually lead the statistician to explore and dig deeper on aspects of the project beyond what the original question was. In fact, that's how many new methods are initially thought of. Cox's proportional hazard is probably the best example illustrating how collaboration leads to new ideas.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are also rare examples of professionals crossing bridges. Some epidemiologists are excellent statisticians (e.g. Sander Greenland) and I've also seen some statisticians becoming epidemiologists, as well as social scientists beoming excellent statisticians (Rand Wilcox, Paul Allison).&lt;/p&gt;&#10;&#10;&lt;p&gt;As someone famously said, statisticians get to play in everyone's sandbox.  A corollary is that in many cases, we do not own the sandbox.&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2014-04-01T15:38:23.287" CreationDate="2014-04-01T14:12:21.990" Id="92139" LastActivityDate="2014-04-01T14:12:21.990" OwnerUserId="21967" ParentId="92127" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;The best way to think of it may be as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\Pr(D)$:&#10;This represents the probability of having observed the training data. Consider the sample space to be the set of possible sets of observed data. Each will be observed with some probability and that probability for the training set is represented by $\Pr(D)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;$\Pr(h)$:&#10;I am not entirely sure I understand your last doubt, but the prior probability of the hypothesis is the probability ascribed to the hypothesis $h$ being true prior to drawing the sample. Perhaps you can consider the sample space as the set of possible hypotheses.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-04-01T14:53:51.110" Id="92145" LastActivityDate="2014-04-01T14:53:51.110" OwnerUserId="14771" ParentId="92138" PostTypeId="2" Score="2" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Consider $A_{m \times n}$ be an i.i.d. random matrix with finite first to fourth moments. There is a good number of asymptotic and non-asymptotic results regarding the spectral norm of $A$, $\|A\|_2$, e.g., [1, Proposition 2.4] yields a deviation result on the &lt;strong&gt;largest&lt;/strong&gt; spectral norm, and also reported in [1, Theorem 2.5] there is a very nice result by R. Latala on the expectation $\mathbf{E}[\|A\|_2]$ under suitable assumptions.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to ask whether anyone knows of a result on the &lt;strong&gt;smallest&lt;/strong&gt; spectral norm, so that I could assume with very high probability that $\|A\|_2 \geq \tau$ where e.g. $\tau = \mathbf{E}[\|A\|_2] - t$ for some $t &amp;gt; 0$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Looking at the proof in [1, Proposition 2.4] it seems to be non-trivial to find a value for which the previous holds with very high probability, at least for someone who is not familiar with this type of argument.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there any way to prove an inequality of the type:&#10;$$P[\|A\|_2 &amp;lt; \mathbf{E}[\|A\|_2] - t] &amp;gt; 1-e^{-c t^2}$$&#10;for some random matrix so that $\min_A \|A\|_2 \approx \mathbf{E}[\|A\|_2] - t$ for some $t &amp;gt; 0$, either for finite or infinite dimensions provided that $m/n \rightarrow \gamma$?&lt;/p&gt;&#10;&#10;&lt;p&gt;[1] Rudelson, Mark, and Roman Vershynin. &quot;Non-asymptotic theory of random matrices: extreme singular values.&quot; arXiv preprint arXiv:1003.2990 (2010).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-01T19:42:30.197" Id="92169" LastActivityDate="2014-04-01T19:42:30.197" OwnerUserId="22309" PostTypeId="1" Score="0" Tags="&lt;distributions&gt;&lt;extreme-value&gt;&lt;eigenvalues&gt;&lt;iid&gt;&lt;random-matrix&gt;" Title="Smallest Spectral Norm / Deviation Inequality" ViewCount="20" />
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;em&gt;null hypothesis in this case is that [...] they are different&lt;/em&gt;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;-- That's not how null hypotheses work. You need something you can calculate the distribution of a test statistic under; generally that's no effect/no difference (whence, &quot;null&quot;). &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;similarity ... whether or not the two groups were sampled from the same population&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Your definition of 'similarity' (&quot;from the same population&quot;) &lt;em&gt;is&lt;/em&gt; a suitable null, fortunately.&lt;/p&gt;&#10;&#10;&lt;p&gt;So if the null is the population distributions are identical and the alternative is that they differ in &lt;em&gt;some&lt;/em&gt; way, you're after a general test for distributional differences -- something that would pick up a difference in location, or spread, or shape.&lt;/p&gt;&#10;&#10;&lt;p&gt;This would be something like a &lt;a href=&quot;http://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test#Two-sample_Kolmogorov.E2.80.93Smirnov_test&quot; rel=&quot;nofollow&quot;&gt;two-sample Kolmogorov-Smirnov test&lt;/a&gt;. There are other possibilities, but that's the most commonly used one. If there are particular kinds of alternatives you especially want power against, there may be a more suitable choice.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-01T20:25:21.140" Id="92173" LastActivityDate="2014-04-01T20:25:21.140" OwnerUserId="805" ParentId="92085" PostTypeId="2" Score="2" />
  
  <row AnswerCount="1" Body="&lt;p&gt;It is trivial to create a boxplot in R with a full dataset. However, with limited access to the whole dataset, I just have 5 data point at min, 25%, 50% ,75%, and max. So is there any easy way to reproduce the boxplot with only these 5 values?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-04-01T23:27:07.517" Id="92182" LastActivityDate="2014-04-02T05:39:39.627" LastEditDate="2014-04-02T00:10:43.000" LastEditorUserId="32036" OwnerUserId="42916" PostTypeId="1" Score="1" Tags="&lt;boxplot&gt;" Title="Reproduce boxplot with only data points at given quantiles" ViewCount="371" />
  <row Body="&lt;p&gt;It's still pretty trivial. You can't reproduce the whiskers of a default boxplot effectively if the minimum and maximum values exceed Tukey's fences, but the box itself should remain unaltered. E.g., with &lt;code&gt;x=rnorm(9999)&lt;/code&gt;, compare &lt;code&gt;boxplot(x)&lt;/code&gt; vs. &lt;code&gt;boxplot(quantile(x))&lt;/code&gt;:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/gXhB9.png&quot; alt=&quot;&quot;&gt; $\leftarrow$ full dataset vs. your five values $\rightarrow$ &lt;img src=&quot;http://i.stack.imgur.com/05NlK.png&quot; alt=&quot;&quot;&gt; &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-04-02T00:04:54.000" Id="92187" LastActivityDate="2014-04-02T05:39:39.627" LastEditDate="2014-04-02T05:39:39.627" LastEditorUserId="32036" OwnerUserId="32036" ParentId="92182" PostTypeId="2" Score="6" />
  <row AnswerCount="0" Body="&lt;p&gt;The question asks to test if smoking status and level of alcohol consumption are independent using the usual five-step procure at alpha $=0.05$:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/95boi.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I am having trouble finding expected values. As the question states, the expected values can be generated from null hypothesis. But how?&#10;Another confusion I'm having is identifying the procedure that I need to follow, since it's not stated in the question. Since I need to have expected values, does it mean that I need to follow goodness of fit to a Poisson distribution in order to identify independence between two variables?&#10;Thank you!&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-04-01T23:00:41.713" Id="92189" LastActivityDate="2014-04-02T00:57:16.103" LastEditDate="2014-04-02T00:57:16.103" LastEditorUserId="32036" OwnerDisplayName="juknee" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;&lt;self-study&gt;&lt;poisson&gt;&lt;expected-value&gt;&lt;independence&gt;" Title="Testing if alcohol consumption and smoking are independent" ViewCount="45" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Let $\theta \in \Theta \subseteq \mathbb{R}^d$ be a parameter vector. &lt;/p&gt;&#10;&#10;&lt;p&gt;Let $Q: \Theta \rightarrow \mathbb{R}$ be a function mapping from the parameter space to the real numbers. &lt;/p&gt;&#10;&#10;&lt;p&gt;Let $Z_T$ be a a random vector representing a random sample, where $T$ is the sample size and $Z_T$ has support $\mathcal{Z}_T$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $Q_T(\theta): (\mathcal{Z}_T,\Theta)\rightarrow \mathbb{R}$ be an estimator of $Q(\theta)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Assume that &#10;$$&#10;\sup_{\theta \in \Theta}\sqrt{T}|Q(\theta)-Q_T(\theta)|=O_p(1)&#10;$$ &#10;i.e. $\forall \epsilon$, $\exists M_\epsilon&amp;gt;0$ s.t. $\mathbb{P}(\sup_{\theta \in \Theta}\sqrt{T}|Q(\theta)-Q_T(\theta)|&amp;gt;M_\epsilon)&amp;lt;\epsilon$ $\forall T$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Does this imply &#10;$$&#10;\sqrt{T}|Q_T(\theta)-Q(\theta)|=O_p(1)  \text{ } \forall \theta \in \Theta&#10;$$&#10;?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-02T10:46:11.610" Id="92232" LastActivityDate="2014-04-02T10:46:11.610" OwnerUserId="42198" PostTypeId="1" Score="0" Tags="&lt;distributions&gt;&lt;stochastic-processes&gt;&lt;bounds&gt;" Title="Implication of uniform stochastic boundedness?" ViewCount="16" />
  <row Body="&lt;p&gt;Here is a smaller example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Duration &amp;lt;- rnorm(20)&#10;Part &amp;lt;- c(1,1,1,2,2,&#10;          2,3,3,4,4,&#10;          5,5,6,6,6,&#10;          7,7,7,8,8)&#10;Cond &amp;lt;- c(rep(1,10), rep(2,10))&#10;&#10;plot(Part, Duration, type = 'n')&#10;points(jitter(Part), jitter(Duration), col = Cond)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="4" CreationDate="2014-04-02T11:19:11.900" Id="92235" LastActivityDate="2014-04-02T11:19:11.900" OwnerUserId="686" ParentId="92225" PostTypeId="2" Score="0" />
  
  <row AcceptedAnswerId="92294" AnswerCount="1" Body="&lt;p&gt;This is maybe annoyingly easy for some, but I am completely new to regression.&lt;/p&gt;&#10;&#10;&lt;p&gt;As an example, I shall use the data set in R, called &lt;code&gt;mtcars&lt;/code&gt;. I am interested in the columns &lt;code&gt;cyl&lt;/code&gt;,&lt;code&gt;drat&lt;/code&gt;,&lt;code&gt;gear&lt;/code&gt; and &lt;code&gt;carb&lt;/code&gt;, and will try to model &lt;code&gt;cyl&lt;/code&gt; using Poisson regression with interaction between &lt;code&gt;gear&lt;/code&gt; and &lt;code&gt;carb&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; mtcars2&amp;lt;-mtcars&#10;&amp;gt; mtcars2$gear&amp;lt;-as.factor(mtcars2$gear)&#10;&amp;gt; mtcars2$carb&amp;lt;-as.factor(mtcars2$carb)&#10;&amp;gt; mtcars.glm&amp;lt;-glm(cyl~drat + gear + carb + gear:carb, family=&quot;poisson&quot;, data=mtcars2)&#10;&amp;gt; summary(mtcars.glm)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Gives the following output:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Call:&#10;glm(formula = cyl ~ drat + gear + carb + gear:carb, family = &quot;poisson&quot;, &#10;    data = mtcars2)&#10;&#10;Deviance Residuals: &#10;     Min        1Q    Median        3Q       Max  &#10;-0.49482  -0.01587   0.00000   0.01705   0.26216  &#10;&#10;Coefficients: (7 not defined because of singularities)&#10;            Estimate Std. Error z value Pr(&amp;gt;|z|)  &#10;(Intercept)  1.97529    0.99358   1.988   0.0468 *&#10;drat        -0.09497    0.30377  -0.313   0.7546  &#10;gear4       -0.20374    0.44511  -0.458   0.6471  &#10;gear5        0.09532    0.49360   0.193   0.8469  &#10;carb2        0.39226    0.30885   1.270   0.2041  &#10;carb3        0.39570    0.32405   1.221   0.2220  &#10;carb4        0.40960    0.29615   1.383   0.1666  &#10;carb6        0.06493    0.63827   0.102   0.9190  &#10;carb8        0.34502    0.61194   0.564   0.5729  &#10;gear4:carb2 -0.38318    0.47243  -0.811   0.4173  &#10;gear5:carb2 -0.68770    0.55361  -1.242   0.2142  &#10;gear4:carb3       NA         NA      NA       NA  &#10;gear5:carb3       NA         NA      NA       NA  &#10;gear4:carb4 -0.01806    0.44170  -0.041   0.9674  &#10;gear5:carb4       NA         NA      NA       NA  &#10;gear4:carb6       NA         NA      NA       NA  &#10;gear5:carb6       NA         NA      NA       NA  &#10;gear4:carb8       NA         NA      NA       NA  &#10;gear5:carb8       NA         NA      NA       NA  &#10;---&#10;Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1&#10;&#10;(Dispersion parameter for poisson family taken to be 1)&#10;&#10;    Null deviance: 16.57428  on 31  degrees of freedom&#10;Residual deviance:  0.42691  on 20  degrees of freedom&#10;AIC: 141.09&#10;&#10;Number of Fisher Scoring iterations: 4&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now, shockingly, there seem to be little significance with regards to the coefficients in the output, but for the sake of my question, I do very much hope that we can all be mature about that and for now simply look away.&lt;/p&gt;&#10;&#10;&lt;p&gt;Lets say that we have another data set with the explanatory variables I used above (&lt;code&gt;drat&lt;/code&gt;, &lt;code&gt;gear&lt;/code&gt;, &lt;code&gt;carb&lt;/code&gt;) and I now wanted to predict &lt;code&gt;cyl&lt;/code&gt;. Let us say the data set looks like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;drat2&amp;lt;-rnorm(10,4,0.2)&#10;gear2&amp;lt;-c(4,4,4,4,4,5,5,5,5,5)&#10;carb2&amp;lt;-c(2,2,2,2,2,2,2,2,2,2)&#10;data.frame(drat2,gear2,carb2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now I would like to predict the outcome from these values, effectively using the coefficients from the summary of the regression above. Is there any effective way to do this? Also, how do you experts see from the following:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Null deviance: 16.57428  on 31  degrees of freedom&#10;    Residual deviance:  0.42691  on 20  degrees of freedom&#10;    AIC: 141.09&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;that my model is bad? Did one compare the residuals with the degrees of freedom, or something?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-02T14:33:42.727" FavoriteCount="1" Id="92253" LastActivityDate="2014-04-02T19:55:13.870" LastEditDate="2014-04-02T14:38:09.620" LastEditorUserId="32036" OwnerUserId="30589" PostTypeId="1" Score="0" Tags="&lt;generalized-linear-model&gt;&lt;prediction&gt;&lt;poisson-regression&gt;&lt;coefficient&gt;" Title="Effectively using coefficients from poisson regression" ViewCount="233" />
  <row AnswerCount="1" Body="&lt;p&gt;I want to learn Gibbs sampling for a Bayesian model. How can I sample the variable from the conditional distribution?&lt;br&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/ca6T4.gif&quot; alt=&quot;a Bayesian model&quot;&gt;&lt;br&gt;&#10; In this example, arrow means dependent; for example, &lt;code&gt;Grade&lt;/code&gt; depends on &lt;code&gt;Difficulty&lt;/code&gt; and &lt;code&gt;Intelligence&lt;/code&gt;.&#10;To use Gibbs sampling to calculate the joint distribution, first I set the &lt;code&gt;Difficulty&lt;/code&gt; and &lt;code&gt;Intelligence&lt;/code&gt; to (1,1).&lt;br&gt;&#10;The next step is to sample &lt;code&gt;Grade&lt;/code&gt; from the $\rm{P(Grade|Difficulty=1,Intelligence=1)}$, but how can I sample?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-04-02T14:40:06.810" FavoriteCount="2" Id="92255" LastActivityDate="2014-04-26T03:51:32.793" LastEditDate="2014-04-03T08:46:02.140" LastEditorUserId="32036" OwnerUserId="42959" PostTypeId="1" Score="2" Tags="&lt;gibbs&gt;&lt;bayes-network&gt;" Title="Gibbs sampling how to sample from the conditional probability? Bayesian model" ViewCount="433" />
  
  <row Body="&lt;p&gt;There's quite a lot to fitting an ARIMA model properly. As with most statistical models there are things you need to do to check that the modelling assumptions are correct and that you've chosen the most appropriate form. The auto.arima makes some sensible choices for you but to understand these choices you'll need to do some background reading. Here's a useful place to start &lt;a href=&quot;http://people.duke.edu/~rnau/411arim.htm&quot; rel=&quot;nofollow&quot;&gt;http://people.duke.edu/~rnau/411arim.htm&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;However a short answer to your question is that there apparently isn't much by way of trend or seasonality in your time series and therefore the best forecast is a straight line. (It's often difficult to convince people that straight line is the best forecast - they want to see something complicated looking in order to be convinced)&lt;/p&gt;&#10;&#10;&lt;p&gt;Simon&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-04-02T14:42:13.927" Id="92256" LastActivityDate="2014-04-02T14:42:13.927" OwnerUserId="42685" ParentId="92218" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;&lt;strong&gt;Segmentation vs. Clustering&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In control system engineering, the ideas of controllability and measurability are, through the Cayley-Hamilton theorem, two faces of the same phenomena.  One implies the other.&lt;/p&gt;&#10;&#10;&lt;p&gt;Segmentation and clustering are to faces of the same coin too.  The line of equal probability of cluster membership is the segmentation boundary.  This is a deep topic and discussions about convergence, nature of space, and appropriate basis functions are beyond the scope of this answer.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Retaining Temporal In formation&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;If I were doing this, then I would augment each temporal membership with cluster membership.  I would use both cluster index and cluster mahalanobis distance.  So if you took one measurement at each instant, then clustered the data, your augmented time-series would have three values at each instant - cluster index, mahalanobis distance (useful), and the measurement itself.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Algorithms&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I have not done much with Time-series as a formalism, so this is all hands-on.  When I have temporal data and I want to cluster, I just use the time-measurement as another measurement.  &lt;/p&gt;&#10;&#10;&lt;p&gt;This means that if you had one measurement per instant, then you have a &lt;a href=&quot;http://en.wikipedia.org/wiki/Random_walk&quot; rel=&quot;nofollow&quot;&gt;2d walk&lt;/a&gt;, of sorts, where time is strictly increasing.  You can throw away the time and cluster in measurement only.  (Here is a link to lots of appropriate approaches: &lt;a href=&quot;http://www.autonlab.org/tutorials/&quot; rel=&quot;nofollow&quot;&gt;AutonLab&lt;/a&gt;)  You can look at both.  You can transform to lagged coordinates, or time-difference coordinates and think in terms of velocities, accelerations or such.  The classic drunkards walk is 2d-random, and is a diffusion process.  Being able to contrive your data as such a walk opens up those analysis tools for use. (&lt;a href=&quot;http://www.stats.ox.ac.uk/~etheridg/popgen/notes.pdf&quot; rel=&quot;nofollow&quot;&gt;link&lt;/a&gt;,&lt;a href=&quot;http://arxiv.org/pdf/math/0702123.pdf&quot; rel=&quot;nofollow&quot;&gt;link&lt;/a&gt;, &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2474747/&quot; rel=&quot;nofollow&quot;&gt;link&lt;/a&gt;, &lt;a href=&quot;http://research.microsoft.com/en-us/um/people/xtong/editable_tog.pdf&quot; rel=&quot;nofollow&quot;&gt;link&lt;/a&gt;) Diffusion is studied in many disciplines including genetics, mathematics, materials science, epidemiology, and computer science.&lt;/p&gt;&#10;&#10;&lt;p&gt;There is no perfect &quot;&lt;a href=&quot;http://www.ted.com/talks/malcolm_gladwell_on_spaghetti_sauce&quot; rel=&quot;nofollow&quot;&gt;pepsi&lt;/a&gt;&quot; - no silver bullet that solves all problems with ease.  There are many good &quot;pepsis&quot;, tools in the toolbox for which some will outperform others in particular areas.  K-means, Gaussian Mixtures, Radial Basis Function Neural Networks, Support Vector Machines, even Q-learning lookups - these might have use for you.&lt;/p&gt;&#10;&#10;&lt;p&gt;Without a clearer description of the nature of the data, of what you are looking to cluster, it is harder to say which tool to use.  If I don't know whether it is a nail or a bolt - I can't say &quot;try to use a wrench&quot; or &quot;try to use a hammer&quot;.  I hope that you find a tool that works for you.&lt;/p&gt;&#10;&#10;&lt;p&gt;Best of Luck.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-02T15:11:37.033" Id="92259" LastActivityDate="2014-04-02T15:11:37.033" OwnerUserId="22452" ParentId="74351" PostTypeId="2" Score="2" />
  <row Body="A mathematical expression w/ &gt;1 term containing the same variable (eg, x &amp; x^2). Polynomials are commonly used to model curvilinear relationships." CommentCount="0" CreationDate="2014-04-02T15:44:27.833" Id="92270" LastActivityDate="2014-04-02T15:44:27.833" LastEditDate="2014-04-02T15:44:27.833" LastEditorUserId="7290" OwnerUserId="7290" PostTypeId="4" Score="0" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Let's say I have a &quot;mystery classifier&quot;. It is a &quot;black box&quot;, I don't know exactly what it is doing, I know neither the training set nor the learning algorithm that was used.  What I do have is a testing set, and the responses of the &quot;mystery classifier&quot; on this testing set.&lt;/p&gt;&#10;&#10;&lt;p&gt;From this information, what can I know about the type of learning algorithm that was used?&lt;/p&gt;&#10;&#10;&lt;p&gt;More specifically:&lt;/p&gt;&#10;&#10;&lt;p&gt;(1) Can I know something about the type of supervision that was used ? For example, if i have several possible sets of labels for my test data, can I know which set of label was most likely used?)&lt;/p&gt;&#10;&#10;&lt;p&gt;--&gt; For example, if I assume that the &quot;mystery classifier&quot; is a logistic regression, can I use the same &quot;cost function&quot; as the one from logistic regression to quantify the &quot;distance&quot; between the outputs of one &quot;candidate classifier&quot; and the &quot;mystery classifier&quot;?&lt;/p&gt;&#10;&#10;&lt;p&gt;(2) Can I know something about the type of dimension reduction (supervised/unsupervised) that was used before classification, if any?&lt;/p&gt;&#10;&#10;&lt;p&gt;--&gt; Is it true that a model which uses supervised dimension reduction + logistic regression (with the same label) would perform better than an unsupervised dimension reduction + logistic regression, because if dimension reduction is supervised then all the relevant information for the task will be there? Then, can I infer that my &quot;mystery classifier&quot; included some supervised dimensionality reduction related to the task if it performs way better than a simple PCA + logistic regression, or that it included some supervised dimension reduction unrelated to the task if it performs way worse? I assume there is the problem that algorithm can be better or worse for a lot of other reasons... can I know?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-02T16:09:43.853" Id="92275" LastActivityDate="2014-04-02T16:09:43.853" OwnerUserId="42174" PostTypeId="1" Score="0" Tags="&lt;logistic&gt;&lt;classification&gt;&lt;unsupervised-learning&gt;&lt;supervised-learning&gt;" Title="Select the most probable learning algorithm from the testing set and output of a classifier?" ViewCount="17" />
  <row AnswerCount="0" Body="&lt;p&gt;I have conducted several repeated-measures ANOVA's with post-hoc testing via Tukey's HSD whenever the omnibus F-test was significant. I'm having some confusion about controlling for type 1 errors when presenting these post hoc tests.&lt;/p&gt;&#10;&#10;&lt;p&gt;Q1: It's my understanding that Tukey's HSD will control the Type-1 error rate only for the set of post-hoc comparisons coming from one ANOVA. Is this correct? If so, do I need to take additional precautions to limit Type-1 errors with post-hoc comparisons from more than one ANOVA?&lt;/p&gt;&#10;&#10;&lt;p&gt;Q2: If I do need to take further action, what should I do? Suppose I have 2 ANOVA's with 6 post-hoc comparisons each. If I did a Bonferonni correction, would I use the number of post-hoc tests (so, 12) or the number of ANOVA's (2), and would I still use Tukey's HSD at all? Is there a completely different approach/control I should consider?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-02T16:18:06.920" Id="92278" LastActivityDate="2014-04-02T16:18:06.920" OwnerUserId="42969" PostTypeId="1" Score="1" Tags="&lt;anova&gt;&lt;multiple-comparisons&gt;&lt;bonferroni&gt;&lt;tukey-hsd&gt;&lt;type-i-errors&gt;" Title="Controlling for Type 1 Errors: Post-hoc testing on more than 1 ANOVA" ViewCount="181" />
  <row AcceptedAnswerId="92460" AnswerCount="1" Body="&lt;p&gt;I have asked a similar question here: &lt;a href=&quot;http://stackoverflow.com/questions/22648335/interpretation-of-interaction-term-in-r-lm-l-q&quot;&gt;stackoverflow&lt;/a&gt; I am puzzled by the interpretation for an interaction term. In my data my Y is an interval variable with the health outcome of an experiment. I have used an interaction term in which I have interacted the condition with the predisposition of the subject considering health status at base level. They are both categorical variables (factor variables in R).&lt;/p&gt;&#10;&#10;&lt;p&gt;Now it gets complicated because the Condition was two treatments: in treatment A subjects got the placebo first and then the real medicine whereas in treatment B they go the real medicine first and the placebo second. All it changes is the order. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Health outcome = a + Condition * Health.Base&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I have the worst state of health at the base level as my reference category I find that interaction with the Condition is statistically significant but I am not sure how to interpret this.&lt;/p&gt;&#10;&#10;&lt;p&gt;I use the &lt;code&gt;lm()&lt;/code&gt; function of R (although my design looks more like an ANOVA) and in the output I get the b coefficient in an output that looks like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ConditionB:Health.Base.So.and.So         (Beta and p-value)&#10;ConditionB:Health.Base.Excellent         (Beta and p-value)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;A statistically significant interaction term for those in Excellent health at baseline would mean that they are affected by the Condition B more than Condition A compared to the reference category people (Poor health at baseline). Is this right? What does the beta-coefficient represent? &lt;/p&gt;&#10;&#10;&lt;p&gt;If I would like to examine for each Health category at the base line separately without comparing to a reference category I would have to code each category as a dummy variable. However, in this case I would compare whether membership to a specific health status at the base line significantly changes between conditions compared to those who belong to the other health statuses at the base line. Is this right? Again, what does the beta-coefficient represent?&lt;/p&gt;&#10;&#10;&lt;p&gt;Would it be right to assume that the choice of the interaction between the Condition and the dummy variables is easier to interpret?&lt;/p&gt;&#10;&#10;&lt;p&gt;--- EDIT ---&#10;R output:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Call:&#10;lm(formula = HealthOutcome ~ Condition * HealthStatus, &#10;    data = datA)&#10;&#10;Residuals:&#10;    Min      1Q  Median      3Q     Max &#10;-1.5957 -0.5942 -0.2640  0.4423  2.4423 &#10;&#10;Coefficients:&#10;                                       Estimate Std. Error t value     Pr(&amp;gt;|t|)    &#10;(Intercept)                            2.595652   0.053044  48.934  &amp;lt; 2e-16 ***&#10;ConditionCondB                        -0.001449   0.077071  -0.019    0.985    &#10;HealthStatusSo.and.So                 -0.331693   0.078094  -4.247  2.35e-05 ***&#10;HealthStatusExcellent                 -0.836724   0.092692  -9.027  &amp;lt; 2e-16 ***&#10;ConditionCondB:HealthStatusSo.and.So   0.137490   0.110612   1.243    0.214    &#10;ConditionCondB:HealthStatusExcellent  -0.199787   0.133943  -1.492    0.136    &#10;---&#10;Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1&#10;&#10;Residual standard error: 0.8045 on 1059 degrees of freedom&#10;  (68 observations deleted due to missingness)&#10;Multiple R-squared:   0.16, Adjusted R-squared:  0.156 &#10;F-statistic: 40.33 on 5 and 1059 DF,  p-value: &amp;lt; 2.2e-16&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="4" CreationDate="2014-04-02T17:11:02.793" Id="92284" LastActivityDate="2014-04-04T02:17:41.867" LastEditDate="2014-04-03T20:56:03.090" LastEditorUserId="27708" OwnerUserId="27708" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;regression&gt;&lt;interaction&gt;&lt;interpretation&gt;&lt;crossover-study&gt;" Title="What is the meaning of the beta-coefficient for an interaction term in a crossover study?" ViewCount="310" />
  
  
  
  <row Body="&lt;p&gt;Minitab is the easiest transition from Excel. It will do Generalized Least Squares but not ARCH/GARCH from the drop-down menu. If you poke around, you can see Youtube tutorials about how to do ARCH/GARCH in Excel.&lt;/p&gt;&#10;&#10;&lt;p&gt;This is what you're getting with Minitab in terms of the user interface. Whether this is worth the $1700 license is up to you.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/dVKLP.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ChtJJ.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/nQBkr.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="2" CommunityOwnedDate="2014-04-03T02:24:44.170" CreationDate="2014-04-03T02:24:44.170" Id="92338" LastActivityDate="2014-04-03T02:24:44.170" OwnerUserId="36539" ParentId="92325" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm working on SVM and ANN classification tools. In order to improve the classification accuracy, I want to know the best or the recommended data-preprocessing, is it scaling/normalisation or standardization ? and why ?&lt;/p&gt;&#10;&#10;&lt;p&gt;Many thanks&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-03T08:27:29.643" Id="92364" LastActivityDate="2014-04-03T08:27:29.643" OwnerUserId="36316" PostTypeId="1" Score="0" Tags="&lt;data-mining&gt;&lt;svm&gt;&lt;neural-networks&gt;&lt;normalization&gt;&lt;standardization&gt;" Title="Scaling/Normalisation or Standardization" ViewCount="54" />
  
  <row Body="&lt;p&gt;The code below probably fits your need, but I use the &lt;code&gt;forecast&lt;/code&gt; package. I simulate a 30-year time series to illustrate.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;&#10;library(forecast)&#10;set.seed(1234)&#10;y &amp;lt;- ts(sort(rnorm(30)), start = 1978, frequency = 1) # annual data&#10;fcasts &amp;lt;- vector(mode = &quot;list&quot;, length = 10L)&#10;for (i in 1:10) { # start rolling forecast&#10;  # start from 1997, every time one more year included&#10;  win.y &amp;lt;- window(y, end = 1996 + i) &#10;  fit &amp;lt;- auto.arima(win.y)&#10;  fcasts[[i]] &amp;lt;- forecast(fit, h = 1)&#10;}&#10;&lt;/code&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-04-03T09:18:54.237" Id="92368" LastActivityDate="2014-04-03T09:18:54.237" OwnerUserId="22330" ParentId="91675" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I need to analyse questionnaire survey data with mixed data types (nominal, ordinal, continuous). I want to cluster the variables. So far I only have dead ends. &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;I know I can use daisy in the cluster package to cluster the cases, but I want to cluster the variables. &lt;/li&gt;&#10;&lt;li&gt;lots of cluster procedures like ICLUST can cluster variables but they don't allow nominal data types.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;The only thing I can think of is to split up the categorial variables into binary dummy variables, standardise everything, and then apply ICLUST.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-04-03T11:12:26.217" Id="92378" LastActivityDate="2014-04-03T11:12:26.217" OwnerUserId="43018" PostTypeId="1" Score="2" Tags="&lt;clustering&gt;&lt;nominal&gt;" Title="clustering variables of mixed types in R" ViewCount="131" />
  
  
  
  <row AcceptedAnswerId="92415" AnswerCount="1" Body="&lt;p&gt;I am studying ARMA models. &lt;/p&gt;&#10;&#10;&lt;p&gt;ARMA(25,25) or ARMA(1,1) which one better model? &lt;/p&gt;&#10;&#10;&lt;p&gt;Why? I think that the reason is the ommission of irrelevant variable &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-04-03T12:36:35.150" Id="92390" LastActivityDate="2014-04-04T09:01:28.413" OwnerUserId="35988" PostTypeId="1" Score="0" Tags="&lt;time-series&gt;&lt;self-study&gt;&lt;econometrics&gt;&lt;arma&gt;" Title="Which arma model is best one?" ViewCount="72" />
  <row Body="&lt;p&gt;The tower law (also known as the law of iterated expectation) is stated slightly wrong in your question, it should be $E(E[X|Y]) = E(X)$ (note that the left hand side is an expected value, which is a constant, and thus the right hand side must also be a constant instead of a random variable). &lt;/p&gt;&#10;&#10;&lt;p&gt;By applying the tower law to the term you are wondering about, we get&#10;\begin{equation}&#10;2E[(\gamma - \theta^B)(\theta^B - \theta)]= 2E[E[(\gamma - \theta^B)(\theta^B - \theta) \mid X]].&#10;\end{equation}&#10;The estimators $\gamma$ and $\theta^B$ are deterministic functions of $X$. Thus, also $(\gamma- \theta^B)$ is a deterministic function of $X$ and can be moved outside the inner expectation. Hence, the expression equals&#10;\begin{equation}&#10; 2E[(\gamma - \theta^B)E[(\theta^B - \theta) \mid X]].&#10;\end{equation}&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-03T12:45:58.320" Id="92392" LastActivityDate="2014-04-03T15:36:25.180" LastEditDate="2014-04-03T15:36:25.180" LastEditorUserId="24669" OwnerUserId="24669" ParentId="92376" PostTypeId="2" Score="1" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am applying ward hierarchical clustering on a data set for which I have pairwise similarities. Since hierarchical clustering need a dissimilarity matrix, I am trying to convert my similarity matrix into a dissimilarity one. Besides, ward algorithm needs a euclidian dissimilarity, so I tried several conversions like the one proposed &lt;a href=&quot;http://stats.stackexchange.com/questions/36152/converting-similarity-matrix-to-euclidean-distance-matrix/36158#36158&quot;&gt;here&lt;/a&gt;&#10;&lt;code&gt;Warning: ward's linkage specified with non-Euclidean dissimilarity matrix&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;And, when I compute the &lt;a href=&quot;http://www.mathworks.fr/fr/help/stats/cophenet.html&quot; rel=&quot;nofollow&quot;&gt;cophenet&lt;/a&gt;, I get a poor value (around 0.39, smaller than all the values I previously got).&lt;/p&gt;&#10;&#10;&lt;p&gt;How can I convert my spearman similarity matrix into a euclidian dissimilarity one?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-04-03T13:02:22.330" FavoriteCount="0" Id="92397" LastActivityDate="2014-04-03T13:02:22.330" OwnerUserId="28183" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;clustering&gt;&lt;data-mining&gt;" Title="Converting a spearman correlation to a euclidian dissimilarity" ViewCount="143" />
  <row Body="&lt;p&gt;You need to calculate &#10;$E[e^{sU}e^{tV}] = E[e^{sX_1 + tX_1^2 + sX_2 + tX_2^2}] &#10;= E[e^{sX_1 + tX_1^2}]E[e^{sX_2 + tX_2^2}]$.&#10;So, set up the two integrals using the &lt;a href=&quot;http://en.wikipedia.org/wiki/Law_of_the_unconscious_statistician&quot; rel=&quot;nofollow&quot;&gt;law of the unconscious statistician&lt;/a&gt; and then use the technique of &lt;em&gt;completing the square&lt;/em&gt; in the exponent to evaluate the integrals.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-03T13:47:58.423" Id="92408" LastActivityDate="2014-04-03T13:47:58.423" OwnerUserId="6633" ParentId="92375" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;Neural networks are &lt;i&gt;too&lt;/i&gt; often used as a black box without correct model specification. Indeed, you could take your 256 values per individual and model it as a Feed-forward NN with 256 inputs, maybe 5 to 10 perceptrons on the hidden layer, and hope to get a nice output.&lt;/p&gt;&#10;&#10;&lt;p&gt;However to choose a correct model it would be a good idea to do some preliminary analysis. Maybe you already did, but at least I have now the following questions:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;How are the values distributed? Normal, symmetric, skewed? (often pre-transformations are very useful!)&lt;/li&gt;&#10;&lt;li&gt;Do they correlate with each other? How many principal components are needed to explain most of the variation? (If only few, you need not a lot of perceptrons)&lt;/li&gt;&#10;&lt;li&gt;Do the 256 values measure the same thing? Do you have per subject a time series of 256 values ranging from second 0-2 until 510-512??&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;After getting a better idea of the data, the model specification is feasible.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-04-03T17:43:15.137" Id="92439" LastActivityDate="2014-04-03T17:43:15.137" OwnerUserId="42977" ParentId="90284" PostTypeId="2" Score="1" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm not sure how to give context to this question. We're to use Excel to analyze data and use log base 10 for each column of data that we analyze, which I'm not sure what they want here. Are they wanting us to use the log base 10 for the analyzed data or for the raw data? Then the assignment asks us to determine if the data is normally or lognormally distributed. I do not know how to do this and Google searches do not sound like English.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-04-03T18:01:32.753" Id="92445" LastActivityDate="2014-04-05T00:42:06.453" LastEditDate="2014-04-03T18:04:22.743" LastEditorUserId="22047" OwnerUserId="43058" PostTypeId="1" Score="1" Tags="&lt;self-study&gt;&lt;normal-distribution&gt;&lt;lognormal&gt;" Title="Is the data normally or lognormally distributed?" ViewCount="90" />
  <row Body="&lt;p&gt;A linear SVM can't do anything a linear model cannot do, so without a bias term, the hyperplane must pass through the origin.  This can be addressed by adding an extra input feature with a constant value (which is how a bias term is often implemented in linear regression).  &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-04-03T18:01:56.683" Id="92446" LastActivityDate="2014-04-03T18:01:56.683" OwnerUserId="887" ParentId="92438" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I hope this is even possible.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm doing some stats on a fantasy basketball league I am in. Let's assume I have the totaled stats for the week for each team in a pandas dataframe. It looks something like below:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/NwsZ2.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;My final goal is to go through this database and find what I can increase versus the trade-off of what I need to decrease to best improve my performance against the league.&lt;/strong&gt; In the example above, however, you can see PTS are much easier to replace than BK. So I can't say &quot;Ok increase my team by 10 pts and see how good I do...now increase team by 10 BK and see how good I do.&quot; I need some measure of how much each variable is &quot;worth&quot; in relation to other variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;So say I find these numbers, and it comes out that 10 PTS is worth 1 BLK. Now I can iterate through and see how my team does if I take out 10 units of one and add 10 units to the other(100 PTS and 10 BLKs). Do I improve a bunch? &lt;/p&gt;&#10;&#10;&lt;p&gt;Also I could see what types of players I want to go after if I know that a category is very important for my team and others don't value it correctly.&lt;/p&gt;&#10;&#10;&lt;p&gt;Sorry if this is easy or overly complicated, I just don't even know how to approach it. If there is a better approach than what I described, I'm all ears!&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks guys!&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-04-03T19:24:04.370" Id="92456" LastActivityDate="2014-04-03T19:24:04.370" OwnerUserId="29428" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;predictive-models&gt;" Title="Finding what change in each variables equates to the same impact" ViewCount="23" />
  
  <row Body="&lt;p&gt;i would try &lt;a href=&quot;http://en.wikipedia.org/wiki/Coherence_%28signal_processing%29&quot; rel=&quot;nofollow&quot;&gt;coherence&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;another idea is to apply &lt;a href=&quot;http://en.wikipedia.org/wiki/Haar_wavelet&quot; rel=&quot;nofollow&quot;&gt;haar wavelet&lt;/a&gt; analysis and compare the measurements in frequency space&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-04-03T21:17:32.427" Id="92464" LastActivityDate="2014-04-03T21:17:32.427" OwnerUserId="36041" ParentId="92432" PostTypeId="2" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;I have used a Box-Behnken experimental design. I have a full quadratic model. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, I had to transform the response, $Y$ for the model to fit; I did this using a Box-Cox transformation with $\lambda=0.5$. &lt;/p&gt;&#10;&#10;&lt;p&gt;For example, one of the regressions is like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$Y = 1.28 - 0.008X_1-0.025X_2-0.05X_3-0.13X_1^2-0.01X_2^2-0.006X_3^2\\&#10;+0.02X_1X_2-0.05X_1X_3+0.09X_2X_3$$&lt;/p&gt;&#10;&#10;&lt;p&gt;How do I interpret the terms?&lt;/p&gt;&#10;" ClosedDate="2014-04-04T16:04:11.580" CommentCount="3" CreationDate="2014-04-04T00:42:46.713" Id="92476" LastActivityDate="2014-04-04T04:15:59.753" LastEditDate="2014-04-04T04:15:59.753" LastEditorUserId="805" OwnerUserId="43075" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;data-transformation&gt;&lt;interpretation&gt;" Title="How to interpret Box-Cox Transformation" ViewCount="43" />
  
  <row Body="&lt;p&gt;I think maybe the best way to explain the notion of likelihood is to consider a concrete example.  Suppose I have a sample of IID observations drawn from a Bernoulli distribution with unknown probability of success $p$:  $X_i \sim {\rm Bernoulli}(p)$, $i = 1, \ldots, n$, so the joint probability mass function of the sample is $$\Pr[{\boldsymbol X} = \boldsymbol x \mid p] = \prod_{i=1}^n p^{x_i} (1-p)^{1-x_i}.$$  This expression also characterizes the likelihood of $p$, given an observed sample $\boldsymbol x = (x_1, \ldots, x_n)$:  $$L(p \mid \boldsymbol x) = \prod_{i=1}^n p^{x_i} (1-p)^{1-x_i}.$$  But if we think of $p$ as a random variable, this likelihood is not a density:  $$\int_{p=0}^1 L(p \mid \boldsymbol x) \, dp \ne 1.$$  It is, however, &lt;em&gt;proportional&lt;/em&gt; to a probability density, which is why we say it is a likelihood of $p$ being a particular value given the sample--it represents, in some sense, the relative plausibility of $p$ being some value for the observations we made.&lt;/p&gt;&#10;&#10;&lt;p&gt;For instance, suppose $n = 5$ and the sample was $\boldsymbol x = (1, 1, 0, 1, 1)$.  Intuitively we would conclude that $p$ is more likely to be closer to $1$ than to $0$, because we observed more ones.  Indeed, we have $$L(p \mid \boldsymbol x) = p^4 (1 - p).$$  If we plot this function on $p \in [0,1]$, we can see how the likelihood confirms our intuition.  Of course, we do not know the true value of $p$--it could have been $p = 0.25$ rather than $p = 0.8$, but the likelihood function tells us that the former is much less likely than the latter.  But if we want to determine a &lt;em&gt;probability&lt;/em&gt; that $p$ lies in a certain interval, we have to normalize the likelihood:  since $\int_{p=0}^1 p^4(1-p) \, dp = \frac{1}{30}$, it follows that in order to get a &lt;em&gt;posterior density&lt;/em&gt; for $p$, we must multiply by $30$:  $$f_p(p \mid \boldsymbol x) = 30p^4(1-p).$$  In fact, this posterior is a beta distribution with parameters $a = 5, b = 2$.  Now the areas under the density correspond to probabilities.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, what we have essentially done here is applied Bayes' rule:  $$f_{\boldsymbol \Theta}(\boldsymbol \theta \mid \boldsymbol x) = \frac{f_{\boldsymbol X}(\boldsymbol x \mid \boldsymbol \theta) p(\boldsymbol \theta)}{f_{\boldsymbol X}(\boldsymbol x)}.$$  Here, $p(\boldsymbol \theta)$ is a &lt;em&gt;prior&lt;/em&gt; distribution on the parameter(s) $\boldsymbol \theta$, the numerator is the likelihood $L(\boldsymbol \theta \mid \boldsymbol x) = f_{\boldsymbol X}(\boldsymbol  x \mid \boldsymbol \theta) p(\boldsymbol \theta) = f_{\boldsymbol X, \boldsymbol \Theta}(\boldsymbol x, \boldsymbol \theta)$ which is also the joint distribution of $\boldsymbol X, \boldsymbol \Theta$, and the denominator is the marginal (unconditional) density of $\boldsymbol X$, obtained by integrating the joint distribution with respect to $\boldsymbol \theta$ to find the normalizing constant that makes the likelihood a probability density with respect to the parameter(s).  In our numerical example, we implicitly took the prior for $p$ to be uniform on $[0,1]$.  It can be shown that, for a Bernoulli sample, if the prior is ${\rm Beta}(a,b)$, the posterior for $p$ is also Beta, but with parameters $a^* = a+\sum x_i$, $b^* = b + n - \sum x_i$.  We call such a prior &lt;em&gt;conjugate&lt;/em&gt; (and refer to this as a Bernoulli-Beta conjugate pair).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-04T07:52:50.767" Id="92504" LastActivityDate="2014-04-04T07:52:50.767" OwnerUserId="36771" ParentId="92496" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;NB the deviance (or Pearson) residuals are not expected to have a normal distribution except for a Gaussian model. For the logistic regression case, as @Stat says, deviance residuals for the $i$th observation $y_i$ are given by&lt;/p&gt;&#10;&#10;&lt;p&gt;$$r^{\mathrm{D}}_i=-\sqrt{2\left|\log{(1-\hat{\pi}_i)}\right|}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;if $y_i=0$ &amp;amp;&lt;/p&gt;&#10;&#10;&lt;p&gt;$$r^{\mathrm{D}}_i=\sqrt{2\left|\log{(\hat{\pi}_i)}\right|}$$ &lt;/p&gt;&#10;&#10;&lt;p&gt;if $y_i=1$, where $\hat{\pi_i}$ is the fitted Bernoulli probability. As each can take only one of two values, it's clear their distribution cannot be normal, even for a correctly specified model:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#generate Bernoulli probabilities from true model&#10;x &amp;lt;-rnorm(100)&#10;p&amp;lt;-exp(x)/(1+exp(x))&#10;&#10;#one replication per predictor value&#10;n &amp;lt;- rep(1,100)&#10;#simulate response&#10;y &amp;lt;- rbinom(100,n,p)&#10;#fit model&#10;glm(cbind(y,n-y)~x,family=&quot;binomial&quot;) -&amp;gt; mod&#10;#make quantile-quantile plot of residuals&#10;qqnorm(residuals(mod, type=&quot;deviance&quot;))&#10;abline(a=0,b=1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/XUUWq.png&quot; alt=&quot;Q-Q plot n=1&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;But if there are $n_i$ replicate observations for the $i$th predictor pattern, &amp;amp; the deviance residual is defined so as to gather these up&lt;/p&gt;&#10;&#10;&lt;p&gt;$$r^{\mathrm{D}}_i=\operatorname{sgn}({y_i-n_i\hat{\pi}_i})\sqrt{2\left[y_i\log{\frac{y_i}{n\hat{\pi}_i}} + (n_i-y_i)\log{\frac{n_i-y_i}{n_i(1-\hat{\pi}_i)}}\right]}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;(where $y_i$ is now the count of successes from 0 to $n_i$) then as $n_i$ gets larger the distribution of the residuals approximates more to normality:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#many replications per predictor value&#10;n &amp;lt;- rep(30,100)&#10;#simulate response&#10;y&amp;lt;-rbinom(100,n,p)&#10;#fit model&#10;glm(cbind(y,n-y)~x,family=&quot;binomial&quot;)-&amp;gt;mod&#10;#make quantile-quantile plot of residuals&#10;qqnorm(residuals(mod, type=&quot;deviance&quot;))&#10;abline(a=0,b=1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/gLFaO.png&quot; alt=&quot;Q-Q plot n=30&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Things are similar for Poisson or negative binomial GLMs: for low predicted counts the distribution of residuals is discrete &amp;amp; skewed, but tends to normality for larger counts under a correctly specified model.&lt;/p&gt;&#10;&#10;&lt;p&gt;It's not usual, at least not in my neck of the woods, to conduct a formal test of residual normality; if &lt;a href=&quot;http://stats.stackexchange.com/questions/2492/is-normality-testing-essentially-useless&quot;&gt;normality testing is essentially useless&lt;/a&gt; when your model assumes exact normality, then &lt;em&gt;a fortiori&lt;/em&gt; it's useless when it doesn't. Nevertheless, for unsaturated models, graphical residual diagnostics are useful for assessing the presence &amp;amp; the nature of lack of fit, taking normality with a pinch or a fistful of salt depending on the number of replicates per predictor pattern.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-04T09:16:39.037" Id="92518" LastActivityDate="2015-02-21T22:06:09.503" LastEditDate="2015-02-21T22:06:09.503" LastEditorUserId="17230" OwnerUserId="17230" ParentId="92394" PostTypeId="2" Score="8" />
  
  <row Body="&lt;p&gt;Yes. One simple case to consider is curvilinear regression, in which the response variable is modeled as a &lt;a href=&quot;/questions/tagged/polynomial&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;polynomial&amp;#39;&quot; rel=&quot;tag&quot;&gt;polynomial&lt;/a&gt; function of the predictor. I offer an example in &lt;a href=&quot;/questions/tagged/r&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;r&amp;#39;&quot; rel=&quot;tag&quot;&gt;r&lt;/a&gt; using $y_i=\beta_1x_i+\beta_2x_i^2+\varepsilon_i$.&lt;br&gt;&#10;First, some random data that fits the model: &lt;code&gt;x=rnorm(99);y=x^2-x+2*rnorm(99);XY=data.frame(x,y)&lt;/code&gt;&#10;Using &lt;code&gt;set.seed(1)&lt;/code&gt;, the correlation (&lt;code&gt;cor(x,y)&lt;/code&gt;) $r_{x,y}=-.35$. Next, a curvilinear regression plot:&#10;&lt;code&gt;require(ggplot2);ggplot(XY,aes(x,y))+geom_point()+geom_smooth(method=lm,formula=y~x+I(x^2))&lt;/code&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/Nv074.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&#10;Finally, just to demonstrate that the model should curve like this, &lt;code&gt;summary(lm(y~x+I(x^2),XY))&lt;/code&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Coefficients:&#10;            Estimate Std. Error t value Pr(&amp;gt;|t|)    &#10;(Intercept)  -0.2772     0.2368  -1.171    0.245    &#10;x            -1.1893     0.2163  -5.498 3.17e-07 ***&#10;I(x^2)        1.2602     0.1697   7.427 4.55e-11 ***&#10;---&#10;Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 &#10;&#10;Residual standard error: 1.916 on 96 degrees of freedom&#10;Multiple R-squared: 0.4446, Adjusted R-squared: 0.4331 &#10;F-statistic: 38.43 on 2 and 96 DF,  p-value: 5.496e-13&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;For more statistics on how much better the polynomial model fits versus the simple linear model, use &lt;code&gt;anova(lm(y~x,XY),lm(y~x+I(x^2),XY))&lt;/code&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Analysis of Variance Table&#10;&#10;Model 1: y ~ x&#10;Model 2: y ~ x + I(x^2)&#10;  Res.Df    RSS Df Sum of Sq      F    Pr(&amp;gt;F)    &#10;1     97 554.83                                  &#10;2     96 352.38  1    202.45 55.153 4.552e-11 ***&#10;---&#10;Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This example fits two conceivable interpretations of your question's premises:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Beyond the &lt;a href=&quot;http://www4b.wolframalpha.com/Calculate/MSP/MSP18311c11623a7f0i1c4000004ga4h74i2cc8db7d?MSPStoreType=image/gif&amp;amp;s=47&amp;amp;w=461.&amp;amp;h=37.&quot; rel=&quot;nofollow&quot;&gt;global minimum at $x=.472$&lt;/a&gt;, predictions of $\hat y$ increase.&lt;/li&gt;&#10;&lt;li&gt;The slope of the regression line changes positively as $x$ increases.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;And of course, $x$ correlates negatively with $y$, as noted above.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-04-04T09:22:55.297" Id="92519" LastActivityDate="2014-04-04T09:31:12.810" LastEditDate="2014-04-04T09:31:12.810" LastEditorUserId="32036" OwnerUserId="32036" ParentId="92505" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="93621" AnswerCount="1" Body="&lt;p&gt;I know about &lt;a href=&quot;http://en.wikipedia.org/wiki/Probability_integral_transform&quot; rel=&quot;nofollow&quot;&gt;PIT&lt;/a&gt;, but this works only when you know the distribution, or at least have a strong hint. What I am trying to achieve is to transform a given sample into an equivalent sample with continuous standard uniform distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a sample of size $n$. I choose arbitrary a value $m$, and estimate $m+1$ quantiles (for example, if $m=4$, I compute quantiles for $\{0, .25, .5, .75, 1\}$). The procedure is described in &lt;a href=&quot;http://en.wikipedia.org/wiki/Quantile#Estimating_the_quantiles_of_a_population&quot; rel=&quot;nofollow&quot;&gt;Wikipedia&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Using quantiles I transform each $x_i$. If $x_i$ happens to be exactly a computed quantile, than I know precisely its equivalent value, otherwise I interpolate the equivalent value linearly.&lt;/p&gt;&#10;&#10;&lt;p&gt;I've done a small simulation. I build a random sample from standard normal with $10^6$ values. I applied the described transformation and make KS test for some values of $m$. The results looks like:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;  m         D   p-value&#10;100  0.006090   0.000000000000 ***&#10;200  0.003151   0.000000004733 ***&#10;300  0.001991   0.000720875707 ***&#10;400  0.001484   0.024403417075 *&#10;500  0.001057   0.213437843144&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;It looks like I can do a PIT on sample with only 500 interpolation points.&lt;/p&gt;&#10;&#10;&lt;p&gt;The question is: can I use Kolmogorov-Smirnov one sample goodness-of-fit to find a proper value for $m$ (the number computed quantiles)?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-04-04T10:41:07.540" Id="92524" LastActivityDate="2014-04-13T16:04:12.453" LastEditDate="2014-04-04T12:32:35.457" LastEditorUserId="16709" OwnerUserId="16709" PostTypeId="1" Score="1" Tags="&lt;data-transformation&gt;&lt;goodness-of-fit&gt;&lt;kolmogorov-smirnov&gt;&lt;uniform&gt;" Title="PIT on a sample with m bins, and KS test used to estimate a good value for m" ViewCount="85" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I want to conduct multivariate regression (Regression on more than 1 dependent variables) in SPSS. But I couldn't find any option. I am using SPSS 22. All I could find is linear regression option but that doesn't serve my purpose. My question is how can I conduct multivariate regression in spss :(&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-04T15:03:26.140" Id="92550" LastActivityDate="2014-04-04T15:03:26.140" OwnerUserId="42732" PostTypeId="1" Score="0" Tags="&lt;multivariate-regression&gt;" Title="Multi variate regression in SPSS.... How?" ViewCount="19" />
  <row Body="&lt;p&gt;Since I run a lot of analyses in JAGS and prefer the output format from R2jags I decided to hack together a temporary solution to this problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;Essentially I added an argument to the jags() function in R2jags, n.adapt, to specify the number of 'adaptation' iterations and got rid of the code chunk that sets n.adapt = n.burnin. Then in between the R2jags calls to jags.model() [where the adaption occurs] and coda.samples() which generates samples from the posterior, I added a call to update() that updates the model for a number of iterations equal to the burnin period specified in the jags() call. Then, the output from coda.samples() will not contain those samples from the burnin period regardless of how JAGS decides to handle adaption.&lt;/p&gt;&#10;&#10;&lt;p&gt;My solution seems to work for the toy example provided in the question (i.e., the burn-in occurs and the traceplots look good)&lt;/p&gt;&#10;&#10;&lt;p&gt;You can download the modified R2jags package &lt;a href=&quot;https://bitbucket.org/kenkellner/r2jagsfix/downloads#download-340813&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;. I've tested it on Windows and Linux (only the jags() function, though)&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm no expert on building R packages, but maybe this will be helpful to others. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-04-04T15:04:42.063" Id="92551" LastActivityDate="2014-04-04T15:04:42.063" OwnerUserId="43125" ParentId="45193" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;Testlet theory shares a very tight relationship to a bifactor model, so yes this can be done in IRT. Testlets basically are formed when there is one construct of interest, however there are 'packets' of inter-item dependencies due to similar content, or whatever else. The usual testlet model, at the item level, gives the items with inter-dependencies equal weights to form each respective composite (whereas in the less restricted bifactor structure, each item slope may be unequal. However, constraining them to be equal in estimation gives the same effect).&lt;/p&gt;&#10;&#10;&lt;p&gt;If you are looking to explore where these interdependencies are you are basically looking at the problem as an item-level factor analysis. Several approaches are therefore possible, such as limited information factor analysis with polychoric matrix, or multidimensional IRT. Either way, you are looking to recover the dependency pattern empirically rather than modeling them theoretically from an a priori standpoint. If you are instead looking to 'confirm' your hypothesized testlet patter, then both frameworks also provide a confirmatory modelling approach as well. Hope that helps. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-04T16:29:37.197" Id="92571" LastActivityDate="2014-04-04T16:29:37.197" OwnerUserId="18152" ParentId="92558" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="92589" AnswerCount="1" Body="&lt;p&gt;I am about to write my bachelor thesis about non-parametric density estimation, especially kernel density estimators and their application in classification. As I am quite new to looking for academic literature, I am having a hard time finding the most important and modern papers, or other resources, and would be glad if someone could give me a hint. Right now I am mainly working with older works (especially by Silverman and Devroye).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-04T18:02:53.957" Id="92584" LastActivityDate="2014-04-04T19:05:51.077" LastEditDate="2014-04-04T18:42:14.537" LastEditorUserId="22311" OwnerUserId="43140" PostTypeId="1" Score="2" Tags="&lt;estimation&gt;&lt;nonparametric&gt;&lt;kernel-density-estimate&gt;" Title="Literature on nonparametric density estimation" ViewCount="42" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I was reading &lt;a href=&quot;http://www.ling.upenn.edu/~joseff/avml2012/&quot; rel=&quot;nofollow&quot;&gt;this tutorial on &lt;code&gt;ggplot2&lt;/code&gt;&lt;/a&gt; and was struck by a bit where it said that &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;durations are typically best displayed along a logarithmic scale&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Is this an established principle? The author states the claim as if it were, but I've never seen them plotted like this (at least not in phonetics), and to me, a linear scale makes more sense (time moving at a constant rate and all).&lt;/p&gt;&#10;&#10;&lt;p&gt;If it is an established principle, I would appreciate some references and maybe an explanation of why this is the case.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-04-04T18:18:15.343" Id="92586" LastActivityDate="2014-04-05T14:04:41.763" LastEditDate="2014-04-04T19:02:38.180" LastEditorUserId="43142" OwnerUserId="43142" PostTypeId="1" Score="3" Tags="&lt;time-series&gt;&lt;data-visualization&gt;" Title="Should plots for time series use a logarithmic scale for duration/time?" ViewCount="71" />
  
  <row Body="&lt;p&gt;Since your values are measurements, I assume the actual density close to 0 is not an issue and we can safely assume convergence for the Taylor expansion. &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;(I understand that I should be using the t-distribution to best model it, as we only have an average of 4 reads, but I am trying to work it out for the Normal case first.)&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;This appears to be a common misconception. Having small samples doesn't cause approximately normal data (or its mean) to have a t-distribution; if your reasons for assuming approximate normality are good, small samples doesn't change that.&lt;/p&gt;&#10;&#10;&lt;p&gt;Your discussion doesn't make it clear how $V$ relates to $\alpha$. &lt;/p&gt;&#10;&#10;&lt;p&gt;It's &lt;em&gt;also&lt;/em&gt; not clear how finding the expectation of $1/V$ tells you about $K_d$. I feel like there are important details missing. &lt;/p&gt;&#10;&#10;&lt;p&gt;In any case, I'll do the expansion:&#10;\begin{align}&#10;g(X) &amp;amp;= g(\mu_X + X - \mu_X)  \\&#10;     &amp;amp;= g(\mu_X) + (X - \mu_X)\cdot g'(\mu_X) + \frac{(X - \mu_X)^2}{2!}\cdot g''(\mu_X) + \frac{(X - \mu_X)^3}{3!}\cdot g'''(\mu_X) + ...&#10;\end{align}&#10;Taking expectations term by term:&#10;\begin{align}&#10;E(g(X)) &amp;amp;= g(\mu_X) + g'(\mu_X)\cdot E(X - \mu_X) + \frac{g''(\mu_X)}{2}\cdot E[(X - \mu_X)^2] +  ...  \\&#10;        &amp;amp;=g(\mu_X) + 0 + g''(\mu_X)/2\cdot \text{Var}(X) + ...   \\&#10;        &amp;amp;\approx g(\mu_X) + g''(\mu_X)/2 \cdot\text{Var}(X) &#10;\end{align}&#10;When $g(X) = 1/X$,&lt;/p&gt;&#10;&#10;&lt;p&gt;$\quad\quad\quad\quad\ \ \approx \frac{1}{\mu_X} - \frac{1}{2\mu_X^2} \sigma^2_X $&lt;/p&gt;&#10;&#10;&lt;p&gt;An approximation for the variance can also be obtained.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-04-05T00:55:45.163" Id="92607" LastActivityDate="2014-12-05T15:02:48.550" LastEditDate="2014-12-05T15:02:48.550" LastEditorUserId="7290" OwnerUserId="805" ParentId="92495" PostTypeId="2" Score="3" />
  
  
  
  <row Body="&lt;p&gt;Both answers posted so far are useful (+1) but let me present this in a slightly different way using the &lt;a href=&quot;https://en.wikipedia.org/wiki/Minimum_description_length&quot; rel=&quot;nofollow&quot;&gt;Minimum description length&lt;/a&gt; principal. The basic idea behind MDL is related to &lt;em&gt;Kolmogorov Complexity&lt;/em&gt; and the concept of the minimum sized program required to reproduce a sequence. The MDL principle states that one should prefer models that can communicate the data in the smallest number of bits &lt;a href=&quot;http://statweb.stanford.edu/~tibs/ElemStatLearn/&quot; rel=&quot;nofollow&quot;&gt;Hastie09&lt;/a&gt;. As Shannon's source coding theorem has shown the expected code message length for a given prefix code (ie. model) is : $L = -\Sigma_{a\epsilon A} P(a) \log_2 P(a)$ where $A$ is the set of all possible messages we would like to transmit; if we write this for an infinite set of messages (effectively something in $R$) $L = -\int P(a) \log_2 P(a) da$. One can therefore see that in terms of bits we need $-\log_2P(a)$ bits to transmit a random variable $a$ with probability density function $P(a)$. Now given that when transmitting  a dataset $y$ of model outputs one effectively has to transmit it by sending the best fit parameters of the model $m_i$, $\theta^*$, as well as the discrepancy between the original data and the fitted data, one can write the total length as :&#10;\begin{align}&#10;L = (-\log_2 Pr(\theta^*|m_i)) + (- \log_2 Pr(y|\theta^*,m_i))&#10;\end{align}&#10;So while you will decrease the second term by over-fitting, you will increase your first term by adding &quot;redundant&quot; information. In essence you will increase the variance of $\theta^*$  unnecessarily. &lt;/p&gt;&#10;&#10;&lt;p&gt;This is by no means a (formal) proof but I thought it might be fun to consider. :)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-05T14:03:55.157" Id="92639" LastActivityDate="2014-04-05T14:03:55.157" OwnerUserId="11852" ParentId="88476" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to estimate and forecast the garch models directly with wavelet transforming but  I am having an estimation error in the estimation step &quot;constraints: 'Boundary Constraints Active; Errors may be Inaccurate&quot;&#10;The of C in mean equation is in minus and the always the optimization stopped after the 10 iterations. I guess that I am doing something wrong. Although I am following the steps as they are exactly mentioned in the wavelet toolbox in Matlab.&#10;I am using the SWT with Db5 and level 4. Daily stock index data which transformed into continues return series. &#10;The length of sample is long enough for garch, 4336 observations and extended via the extension tool in Matlab( with special and carful extension for SWT in wavelet toolbox). Also I checked the data after doing the extension just to make sure that the series shape and characteristics have not significantly changed.   &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-05T14:51:18.023" Id="92642" LastActivityDate="2014-04-05T14:51:18.023" OwnerUserId="42603" PostTypeId="1" Score="0" Tags="&lt;matlab&gt;&lt;garch&gt;&lt;wavelet&gt;" Title="Can I use the Stationary wavlet transforming directly with GARCH models" ViewCount="25" />
  <row AcceptedAnswerId="92703" AnswerCount="1" Body="&lt;p&gt;I have some Poisson data {${y_1,...,y_n}$} and a Gamma prior, and I wish to construct a predictive posterior distribution. &lt;/p&gt;&#10;&#10;&lt;p&gt;As I understand, if my Gamma hyperparameters are $\alpha$ (the prior number of occurrences) and $\beta$ (the prior number of observations), the predictive posterior is: ${y\space|\space y_1...y_n,\alpha,\beta}\space\tilde\space NegBin({\sum\limits_{i=1}^n y_i+\alpha}, \frac{1}{1+\beta+n})$&lt;/p&gt;&#10;&#10;&lt;p&gt;However, because of the multiple parametrizations of both Gamma and negative binomial, my attempts to actually implement this in R were so far futile. &lt;/p&gt;&#10;&#10;&lt;p&gt;Am I right that in the above formula $\alpha$ is &lt;code&gt;shape&lt;/code&gt; and $\beta$ is &lt;code&gt;rate&lt;/code&gt; in &lt;code&gt;dgamma()&lt;/code&gt;? Am I right then that $\sum\limits_{i=1}^n y_i+\alpha$ is &lt;code&gt;size&lt;/code&gt; in &lt;code&gt;dnbinom()&lt;/code&gt;? And what is &lt;code&gt;prob&lt;/code&gt; then? I thought it was $\frac{1}{1+\beta+n}$, but the resulting distribution doesn't seem to make sense.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-05T14:53:42.117" Id="92643" LastActivityDate="2014-04-06T11:18:36.727" OwnerUserId="6649" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;bayesian&gt;&lt;gamma-distribution&gt;&lt;negative-binomial&gt;" Title="Parametrization of Gamma and Negative Binomial in R" ViewCount="143" />
  
  
  <row Body="&lt;p&gt;My read on this is that the day-to-day variance in your weight is likely to dominate any effect of the number of calories on the same or previous day. For example, how dehydrated you are. I think your data would be more meaningful by looking at weekly or monthly means. In that context, something like a regression of weight at end of week on this week's calories, previous week's calories and weight at start of week might be informative.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-04-05T19:17:44.940" Id="92663" LastActivityDate="2014-04-05T19:17:44.940" OwnerUserId="23972" ParentId="92657" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I am new to data mining and currently working on an online news article from TOI in RapidMiner. My aim is to get results which shows the most important things mentioned in the article or to find the valuable information hidden in it.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am done with document processing on the article &quot;TOI manifesto&quot; &lt;a href=&quot;http://timesofindia.indiatimes.com/home/specials/lok-sabha-elections-2014/news/TOI-manifesto-An-agenda-for-the-new-government/articleshow/31973967.cms&quot; rel=&quot;nofollow&quot;&gt;http://timesofindia.indiatimes.com/home/specials/lok-sabha-elections-2014/news/TOI-manifesto-An-agenda-for-the-new-government/articleshow/31973967.cms&lt;/a&gt;. I have applied tokenization, filtering and ngrams and got some result.&lt;/p&gt;&#10;&#10;&lt;p&gt;Im stuck here. I don't know how to proceed further. Which technique/algorithm should i use in RapidMiner to get required results? Please give details, really need help with it.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-05T20:00:42.787" Id="92667" LastActivityDate="2014-04-05T20:00:42.787" OwnerUserId="42983" PostTypeId="1" Score="0" Tags="&lt;data-mining&gt;" Title="Text Mining a single text document in RapidMiner" ViewCount="98" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have build a sentiment index and am now validating its statistical properties and significance. I stumbled upon two problems.&lt;/p&gt;&#10;&#10;&lt;p&gt;(1) &lt;code&gt;dfgls&lt;/code&gt;indicates that my sentiment is non-stationary as the t-statistics are all above (less negativ) than the critical value. Accordingly, I use the first difference. My sentiment now granger-causes the (also non-stationary) market index, which is predicted by theory. But after taking the first difference it is not able to explain movements in the index using univariate regression, which it did before.&lt;/p&gt;&#10;&#10;&lt;p&gt;Do I have to accept this as it is or do you distinguish between the applicability of first differences in the different tests? For example, would it be possible to use first differences for the granger-causality test and an estimator which takes into account the non-stationarity for the univariate analysis?&lt;/p&gt;&#10;&#10;&lt;p&gt;(2) I want to apply my sentiment in panel data. Here, before taking the first difference, it applies very well. Afterwards, it does not. I also checked my dependend variable for unit root using &lt;code&gt;xtunitroot&lt;/code&gt;and all p-values are 0.000. Therefore, I would judge that the data all panels are stationary. When I use &lt;code&gt;dfgls&lt;/code&gt; for seperate panel IDs though, the tests imply non-stationarity for certain lags.&lt;/p&gt;&#10;&#10;&lt;p&gt;Do I misunderstand the test?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for any advice.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-05T23:59:09.763" Id="92680" LastActivityDate="2014-04-06T10:59:12.803" LastEditDate="2014-04-06T10:59:12.803" LastEditorUserId="42795" OwnerUserId="42795" PostTypeId="1" Score="0" Tags="&lt;statistical-significance&gt;&lt;panel-data&gt;&lt;stationarity&gt;" Title="Non-Stationarity, first differences and Panel Data" ViewCount="105" />
  <row AnswerCount="0" Body="&lt;p&gt;I need to estimate an equation using some iterative method to optimally choose the coefficients. I won't go into the reasons that I can't use a simpler method like OLS, GLS or instrumental variables (it's a long, sad story). &lt;/p&gt;&#10;&#10;&lt;p&gt;My question is about whether I can expect equivalent results from &lt;code&gt;ml&lt;/code&gt; and &lt;code&gt;nl&lt;/code&gt;. Say I want to estimate&lt;/p&gt;&#10;&#10;&lt;p&gt;$y_t = \alpha + \beta y_{t-1} + \gamma x_t + u_t$&lt;/p&gt;&#10;&#10;&lt;p&gt;where I have autocorrelation in the error terms and they are correlated with $y_{t-1}$. Do I need to put in the effort to specify a likelihood function and ask Stata to maximize it, or will I get the same result if I ask Stata to minimize the sum of squares using &lt;code&gt;nl&lt;/code&gt;?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-04-06T00:08:26.927" Id="92681" LastActivityDate="2014-04-06T20:22:12.383" LastEditDate="2014-04-06T20:22:12.383" LastEditorUserId="22047" OwnerUserId="43177" PostTypeId="1" Score="0" Tags="&lt;maximum-likelihood&gt;&lt;stata&gt;&lt;nonlinear&gt;" Title="Stata: ml versus nl" ViewCount="67" />
  
  <row Body="&lt;p&gt;A strictly positive distribution $D_{sp}$ has values $D_{sp}(x)&amp;gt;0$ for all $x$. This is different from a non-negative distribution $D_{nn}$ where $D_{nn}(x) \geq 0$.&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2014-04-06T03:19:27.943" Id="92689" LastActivityDate="2014-04-06T03:19:27.943" OwnerUserId="11852" ParentId="92686" PostTypeId="2" Score="5" />
  
  
  <row Body="&lt;p&gt;Just get the data, decide how you want to define feature groups (colours per shirt type, or shirt type seperately, colors seperately, etc' ) and encode thema s features using sci-kit learn.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features&quot; rel=&quot;nofollow&quot;&gt;http://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-06T08:32:55.573" Id="92708" LastActivityDate="2014-04-06T08:32:55.573" OwnerUserId="38427" ParentId="60311" PostTypeId="2" Score="0" />
  
  <row AcceptedAnswerId="92717" AnswerCount="1" Body="&lt;p&gt;I have been implementing the (kernelized) Pegasos algorithm, but am running into problems in terms of scalability. I will use notations as in the &lt;a href=&quot;http://ttic.uchicago.edu/~nati/Publications/PegasosMPB.pdf&quot; rel=&quot;nofollow&quot;&gt;original manuscript&lt;/a&gt;. Here's a typical time measurement:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;percent         m   t1      t2&#10;0.05%           32  701     961&#10;0.1%            64  676     1890&#10;0.2%            128 648     2777&#10;0.41%           256 725     3851&#10;0.83%           512 861     5267&#10;1.67%           1024 1293    7635&#10;3.34%           2048 1928    11404&#10;6.68%           4096 3256    18210&#10;13.36%          8192 6312    31750&#10;26.72%          16384 11420   56923&#10;53.44%          32768 18838   96755&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Where $m$ is the number of data points in the training set, t1 the training time (includes cross validation over many folds) and t2 the testing time (just 1 prediction over a larger, but fixed part of the data). &lt;/p&gt;&#10;&#10;&lt;p&gt;As you can see, the train time scales reasonably well, but the prediction time scales very badly. I think this is due to the fact that the Pegasos algorithm requires one to compute the (kernel) product of every test-point with a large number of training inputs, that increases as the total training set ($m$) increases. Are there known ways to cope with this problem? I thought the L1-norm would take care of this problem due to sparsification of the model (#support vectors), but I don't think it does. I was thinking of simply dropping terms with low 'counts' ($\alpha$ value in Fig. 3 in the original manuscript), but that seems terribly unelegant ...&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-06T12:30:08.073" FavoriteCount="1" Id="92716" LastActivityDate="2014-04-06T15:35:11.723" OwnerUserId="12752" PostTypeId="1" Score="1" Tags="&lt;svm&gt;&lt;algorithms&gt;" Title="Pegasos prediction time" ViewCount="79" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I want to construct a $3^{5-2}$ design with $I=ABC$ and $I=CDE$ as generators. The complete defining relation for this design is: $I=ABC=CDE=ABC^{2}DE=ABD^{2}E^{2}$. This is a resolution III design with $x_4=2x_1+2x_2+x_3$ and $x_5=x_1+x_2+2x_4\:({\rm mod}\:3)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt;&#10;I wonder how to get $x_4=2x_1+2x_2+x_3$ and $x5=x_1+x_2+2x_4\:({\rm mod}\:3)$ in this specific problem. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edited&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/p9x94.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;This is a question from &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/1118146921&quot; rel=&quot;nofollow&quot;&gt;Design and Analysis of Experiments by Douglas C. Montgomery&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-06T14:42:19.463" Id="92721" LastActivityDate="2014-04-12T16:32:13.870" LastEditDate="2014-04-12T16:32:13.870" LastEditorUserId="24808" OwnerUserId="3903" PostTypeId="1" Score="1" Tags="&lt;self-study&gt;&lt;experiment-design&gt;&lt;computational-statistics&gt;&lt;fractional-factorial&gt;" Title="Constructing one-ninth fraction of the $3^{5}$ design" ViewCount="89" />
  
  <row AnswerCount="1" Body="&lt;p&gt;How do I calculate the correlation using ordinal data based on a 5-point Likert scale associating perioperative education to patient satisfaction scores? A numerical value (&lt;code&gt;1&lt;/code&gt;: strongly agree  &lt;code&gt;5&lt;/code&gt;: strongly disagree) will  represent the participant's perception of   satisfaction as a patient as related to perioperative educational material. The numerical value is the patient's response to 5 specific statements. &lt;/p&gt;&#10;&#10;&lt;p&gt;The Likert scale is being used during a telephone survey to gather patient responses to 5 statements regarding educational material provided during his or her surgical experience. An example of the statement may appear as, &quot;The day surgery nurse provided clear and easy to understand verbal instruction regarding personal care once at home.&quot; The goal is to correlate patient satisfaction with the surgical experience to patient education received by the surgical team of nurses.&lt;/p&gt;&#10;&#10;&lt;p&gt;Question: What is the best way to analyze this data? Would Spearman's rank correlation be appropriate? &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-04-06T15:01:40.417" FavoriteCount="1" Id="92724" LastActivityDate="2014-04-08T08:48:42.033" LastEditDate="2014-04-06T22:33:35.937" LastEditorUserId="32036" OwnerUserId="43198" PostTypeId="1" Score="2" Tags="&lt;correlation&gt;&lt;ordinal&gt;&lt;likert&gt;&lt;spearman-rho&gt;" Title="Correlational study or ordinal data using 5-point Likert scale" ViewCount="2706" />
  <row AnswerCount="0" Body="&lt;p&gt;Can I determine cutoffs for the values of R-squared and its slope that are not arbitrary? Can I do anything to improve my method? &lt;/p&gt;&#10;&#10;&lt;p&gt;Exposition: I work with a type-2 superconducting magnet for the measurement of very small magnetic moments. There is a problem which occurs when the the magnet passes through &quot;critical magnetic fields&quot; in which magnetic flux becomes trapped on the surface, this affects measurements. This trapped flux issue can only be alleviated by waiting.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am trying to construct an algorithm which takes into account the R correlation between the retrieved data points (each individual point takes 14 seconds to measure) and the slope of the R linear-regression line to an acceptable set of data points.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/yGl5q.png&quot; alt=&quot;Datapoints between 0 and 5&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Above, for x in minutes between 0 and 5, is the type of situation I am trying to avoid by using the linear regression slope to determine when the data is leveled off.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/wc3Ni.png&quot; alt=&quot;Data points between 10 and 15&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Above, for x in minutes between 10 and 15, is &quot;good&quot; data. (Neither of these is actual data, just for example)&lt;/p&gt;&#10;&#10;&lt;p&gt;My current algorithm:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Take N points (minimum N = 10)&lt;/li&gt;&#10;&lt;li&gt;Throw away points which are 1 Standard Deviation away from the mean&lt;/li&gt;&#10;&lt;li&gt;Obtain R^2 value&lt;/li&gt;&#10;&lt;li&gt;Obtain Slope of R&lt;/li&gt;&#10;&lt;li&gt;Check Rslope and R^2 with arbitrary limits &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;If this fails, the algorithm takes data points to fill those which were taken away in Step 2 and cycles. If it meets the criteria then the points are averaged out and their mean and Std. Dev. are exported as one point. Which is why I need Rslope ~&gt; 0.&lt;/p&gt;&#10;&#10;&lt;p&gt;I've found this algorithm is actually pretty good for the high-noise, small-sample data that I extract. &lt;/p&gt;&#10;&#10;&lt;p&gt;The limits/cutoffs that are placed on R^2 and Rslope are arbitrary and were obtained by &quot;eye-balling&quot; the data.&lt;/p&gt;&#10;&#10;&lt;p&gt;To restate the question: Can I determine cutoffs for the values of R-squared and its slope that are not arbitrary? Can I do anything to improve my method? Can anyone direct me toward other methods to consider? (Algorithm written in Delphi/Pascal and used for lots of varying samples)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-06T16:46:31.317" Id="92733" LastActivityDate="2014-04-06T16:46:31.317" OwnerUserId="43201" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;r-squared&gt;" Title="Use of R-squared and Slope for determining time-stabilized Data" ViewCount="79" />
  
  
  
  <row AcceptedAnswerId="92965" AnswerCount="1" Body="&lt;p&gt;I've got four dependent correlations (i.e., from the same sample) that involve predictor variables $(A,B,C,D)$ and an outcome variable $(E)$. $N=172$.&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\begin{array}{c|cccc}{\rm Statistic}&amp;amp;AE&amp;amp;BE&amp;amp;CE&amp;amp;DE\\\hline r&amp;amp;-.480&amp;amp;.276&amp;amp;.395&amp;amp;.327\\p&amp;amp;&amp;lt; .01&amp;amp;&amp;lt; .01&amp;amp;&amp;lt; .05&amp;amp;&amp;lt; .01\end{array}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I am trying to run three chi-square tests on this set of four dependent correlations, where $H_0: r_{AE} = r_{BE} = r_{CE} = r_{DE} = 0$; and $H_1$ is simply &quot;NOT $H_0$&quot;. Assuming $H_0$ is rejected, I will then be testing: $H_0: r_{AE} = r_{BE} = r_{CE} = r_{DE}$; with $H_1$ being &quot;NOT $H_0$&quot; again. Assuming that $H_0$ is rejected again, I will then be testing $H_0: r_{BE} = r_{CE} = r_{DE}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;SPSS cannot help with me comparing dependent correlations and Field's &quot;Discovering Statistics...&quot; (2013) only discusses comparing two dependent correlations, not an entire set. So, I have been directed to several places so far, with no luck:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&quot;Multicorr&quot; by Steiger (&lt;a href=&quot;http://www.statpower.net/Software.html&quot; rel=&quot;nofollow&quot;&gt;http://www.statpower.net/Software.html&lt;/a&gt;). I simply cannot figure out how to use it.&lt;/li&gt;&#10;&lt;li&gt;A paper by Steiger (&quot;Tests for Comparing Elements of a Correlation Matrix&quot;, &lt;em&gt;Psychological Bulletin&lt;/em&gt;, 1980, &lt;em&gt;87&lt;/em&gt;(2)). But I cannot figure out how/where my numbers fit with the formulas he has provided there.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Ideally, I'd just like the mathematical formula(s) I can use to get my chi-square observed value (I have the critical value tables already).&#10;Any guidance that could be offered would be most appreciated.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-06T19:21:18.567" Id="92747" LastActivityDate="2014-04-08T05:04:13.880" LastEditDate="2014-04-08T04:54:13.053" LastEditorUserId="43206" OwnerUserId="43206" PostTypeId="1" Score="2" Tags="&lt;correlation&gt;&lt;chi-squared&gt;&lt;non-independent&gt;" Title="Chi-square test to see if set of dependent correlations are equal" ViewCount="75" />
  
  <row AcceptedAnswerId="93694" AnswerCount="1" Body="&lt;p&gt;I'm trying to do A/B testing the Bayesian way, as in &lt;a href=&quot;http://nbviewer.ipython.org/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter2%5fMorePyMC/MorePyMC.ipynb&quot;&gt;Probabilistic Programming for Hackers&lt;/a&gt; and &lt;a href=&quot;http://engineering.richrelevance.com/bayesian-ab-tests/&quot;&gt;Bayesian A/B tests&lt;/a&gt;. Both articles assume that the decision maker decides which of the variants is better based solely on the probability of some criterion, e.g. $P(p_A &amp;gt; p_B) = 0.97 $, therefore, $A$ is better. This probability doesn't provide any information on whether there was sufficient amount of data to draw any conclusions from it. So, it is unclear to me, when to stop the test.&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose there are two binary RVs, $A$ and $B$, and I want to estimate how likely it is that $ p_A &amp;gt; p_B $, and $ \frac{p_A - p_B}{p_A} &amp;gt; 5\% $ based on the observations of $A$ and $B$. Additionally, suppose that $p_A$ and $p_B$ posteriors are beta-distributed.&lt;/p&gt;&#10;&#10;&lt;p&gt;Since I can find the $\alpha, \beta$ parameters for $p_A\,|\,\text{data} $ and $p_B\,|\,\text{data} $, I can sample the posteriors, and estimate $P(p_A &amp;gt; p_B\ |\ \text{data})$. Example in python:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;import numpy as np&#10;&#10;samples = {'A': np.random.beta(alpha1, beta1, 1000),&#10;           'B': np.random.beta(alpha2, beta2, 1000)}&#10;p = np.mean(samples['A'] &amp;gt; samples['B'])&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I could get, for example, $P(p_A &amp;gt; p_B) = 0.95$. Now I would want to have something like $P(p_A &amp;gt; p_B\ |\ \text{data}) = 0.95 \pm 0.03$. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have researched about credible intervals and Bayes factors, but can't understand how to calculate them for this case if they are applicable at all. How can I calculate these additional stats so that I'd have a good termination criterion?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-04-06T19:51:19.907" FavoriteCount="8" Id="92752" LastActivityDate="2014-04-14T13:43:57.490" LastEditDate="2014-04-14T13:43:57.490" LastEditorUserId="43165" OwnerUserId="43165" PostTypeId="1" Score="7" Tags="&lt;bayesian&gt;&lt;beta-binomial&gt;" Title="When to terminate the Bayesian A/B test?" ViewCount="504" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have about 30000 particles distributed (not randomly) in space; I have their vector positions and velocities.&#10;I'm trying to characterize if and how the (underlying) velocity (magnitude) distribution these particles are assumed to sample changes on different scales.&#10;The underlying velocity distribution is of unknown form.&#10;Note: My real interest is in the distribution on a scale that I have near zero particles; so I'd like to be able to get a sense of how the scale effects the distributions generally.&lt;/p&gt;&#10;&#10;&lt;p&gt;I can of course look at distributions made from subsets of nearby points; to my eye these look pretty similar near the edges and very discrepant in the centers. I'd like a way to quantify and visualize this change across scales.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm fairly new to statistics so any/all references are welcome. Feel free to suggest/add tags.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-06T19:59:16.697" Id="92756" LastActivityDate="2014-04-06T19:59:16.697" OwnerUserId="43211" PostTypeId="1" Score="2" Tags="&lt;data-visualization&gt;&lt;inference&gt;" Title="How do I check for variation of a distribution over spatial scales?" ViewCount="23" />
  <row AnswerCount="1" Body="&lt;p&gt;Suppose we have some data, $\{y_i, x_{1i}, \dots, x_{ki}\}_{i=1}^n$ and we want to build a linear model of the form $y_i = \beta_0 + \beta_{1'i}x_{1'i} + \dots + \beta_{k'i}x_{k'i} + \epsilon_i$, where the primed indexes are a subset of $\{1,\dots, k\}$. There are $2^k$ possible models to choose from. Now if we randomly (uniformly) draw a model from all $2^k$ , and fit it to the data, once, can we still interpret the standard errors and CIs as usual?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-06T20:24:04.407" FavoriteCount="2" Id="92758" LastActivityDate="2014-05-05T01:39:57.420" LastEditDate="2014-04-07T04:12:26.447" LastEditorUserId="32420" OwnerUserId="32420" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;self-study&gt;&lt;multiple-regression&gt;&lt;linear-model&gt;" Title="Random model selection and validity of significance tests" ViewCount="53" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/R0NYV.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm not sure how to find part a, what formula can be used to find this mgf?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-04-07T00:23:32.500" Id="92771" LastActivityDate="2014-04-07T00:23:32.500" OwnerUserId="43222" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;self-study&gt;" Title="Gamma Distribution MGF" ViewCount="26" />
  <row AcceptedAnswerId="92815" AnswerCount="1" Body="&lt;p&gt;I recently reread some statistics books and noted something weird: They all discuss the assumptions of linear regression and mention the need for a normal distributed dependent variable. In the next step, there is always a discussion of variables which are non-normal and potential &quot;cures&quot; like sqrt, log, inverse etc. &lt;/p&gt;&#10;&#10;&lt;p&gt;But the books are never going to discuss how to interpret the transformed variable - which strikes me as odd, since a sqrt function changes the interpretation from &quot;the GDP of a state increases by 1 when the social-freedom index increases by 1&quot; to &quot;the square root of GDP of a state increases by 1 when the social-freedom index increases by 1&quot;, which is quite a difference.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can somebody point me to some good explanation about this?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-04-07T06:45:05.243" Id="92796" LastActivityDate="2014-04-07T09:53:16.920" LastEditDate="2014-04-07T08:48:13.130" LastEditorUserId="22047" OwnerUserId="42062" PostTypeId="1" Score="0" Tags="&lt;normal-distribution&gt;&lt;data-transformation&gt;" Title="Transformation of dependent variable: how to interpret it?" ViewCount="71" />
  
  
  
  <row AcceptedAnswerId="92908" AnswerCount="1" Body="&lt;p&gt;I'm doing my master thesis on FDI effect on Chinese wage inequality. I am new to quantitative econometrics so I have no idea if my wage equation is correct.&lt;/p&gt;&#10;&#10;&lt;p&gt;$$W_{it} =  X_{it} + _t + _i + _{it}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Where the wage paid by firm $i$ in year $t$ is denoted as $W_{it}$. $X_{it}$ contains a set of control variables (including include total sales, total exportations, total labor compensations, firms fixed asset, firms R&amp;amp;D expenditures, employees turn over rate and a dummy variable of foreign ownership). A time effect, $_t$, controls for time varying elements that affect all establishments in a given year. An individual effect, $_i$, captures time invariant element that differ across establishments. An error term, $_{it}$. All variables are measured in logarithm units.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is that correct? If yes, what should I do next? I don't know if I should use pooled OLS or GMM....I have already reshaped and treated my data. I really appreciate your help. Thank you in advance.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-04-07T10:31:59.403" Id="92821" LastActivityDate="2014-04-07T18:42:17.210" LastEditDate="2014-04-07T10:58:09.383" LastEditorUserId="32036" OwnerUserId="43240" PostTypeId="1" Score="2" Tags="&lt;time-series&gt;&lt;econometrics&gt;&lt;fixed-effects-model&gt;" Title="My fixed effect model and methodology" ViewCount="97" />
  
  
  <row Body="&lt;p&gt;The variance of binomial data is determined by the mean. One number rules them all. Logistic regression is designed around this and therefore there is no assumption of equal variance. The assumptions are:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;linearity in log odds space &lt;/li&gt;&#10;&lt;li&gt;independent errors&lt;/li&gt;&#10;&lt;li&gt;multicollinearity among predictors isn't too high&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="8" CreationDate="2014-04-07T12:53:05.233" Id="92840" LastActivityDate="2014-04-07T14:11:35.103" LastEditDate="2014-04-07T14:11:35.103" LastEditorUserId="601" OwnerUserId="601" ParentId="92839" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;You applied incorrectly the change-of-variable formula.&lt;br&gt;&#10;The density function of $Z$ is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ f_Z(z) = n\lambda e^{-n\lambda z}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Define the variable $Y\equiv n\lambda Z$.&#10;Then  $Z = \frac 1{n\lambda}Y$ and so &lt;/p&gt;&#10;&#10;&lt;p&gt;$$f_Y(y) = \left|\frac {\partial Z}{\partial Y}\right|\cdot f_Z\left(\frac 1{n\lambda}y\right) = \frac 1{n\lambda}\cdot n\lambda e^{-n\lambda \frac 1{n\lambda}y} = e^{-y}$$&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-07T14:30:30.187" Id="92859" LastActivityDate="2014-04-07T14:30:30.187" OwnerUserId="28746" ParentId="92845" PostTypeId="2" Score="5" />
  
  <row AnswerCount="2" Body="&lt;p&gt;If I have a random variable $X$ which has mean $\mu$ and variance $\sigma^2$, what is the approximate expression of $\log(X)$ and $\sqrt{X}$? Do I assume normal approximation or use Taylor expansion?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-04-07T16:33:06.987" Id="92883" LastActivityDate="2015-03-07T07:18:08.007" LastEditDate="2015-03-07T07:18:08.007" LastEditorUserId="35989" OwnerUserId="43273" PostTypeId="1" Score="0" Tags="&lt;self-study&gt;&lt;mathematical-statistics&gt;&lt;random-variable&gt;" Title="Function of random variable" ViewCount="154" />
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to understand how Kernel Density Estimation (KDE) or (nonparametric) Quantile Regression can be used to forecast values given historical observations. For example, consider the following nonparametric regression model&lt;/p&gt;&#10;&#10;&lt;p&gt;$Y_t = g(X_t) + u_t$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $X_t = (Y_{t - 1}, \ldots, Y_{t - q})$. In the following equation (one-step-ahead forecasting)&lt;/p&gt;&#10;&#10;&lt;p&gt;$\hat{Y}_{n + t,1} \equiv \hat{\text{E}}(Y_{n + t}|X_{n + t}) = \dfrac{\sum_{j = q + t}^{n + t - 1}Y_jK_h(X_j - X_{n + t})}{\sum_{j = q + t}^{n + t - 1}K_h(X_j - X_{n + t})}$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $K_h(X_j - X_{n + t}) = \prod_{s = 1}^q h_s^{-1}k((Y_{j - s} - Y_{n + t - s})/h_s)$, it doesn't seem to me that the order of the observations is taken into account when evaluating the kernel function at previous observations. Shouldn't it matter? Otherwise, it seems to me that observations in previous time periods are just treated as i.i.d. data (no time structure). Similarly to the prediction of future quantiles if I were to use the function npqreg in the R np package for time series data.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you in advance for any comments.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-07T16:38:19.917" Id="92884" LastActivityDate="2014-04-07T16:38:19.917" OwnerUserId="25350" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;nonparametric&gt;&lt;quantile-regression&gt;&lt;kernel-density-estimate&gt;" Title="Nonparametric Time Series Forecasting" ViewCount="35" />
  <row AcceptedAnswerId="93004" AnswerCount="1" Body="&lt;p&gt;I read on the &lt;a href=&quot;http://en.wikipedia.org/wiki/Cluster_analysis&quot; rel=&quot;nofollow&quot;&gt;cluster analysis page of wikipedia&lt;/a&gt;:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;For example, k-means clustering can only find convex clusters, and many evaluation indexes assume convex clusters. On a data set with non-convex clusters neither the use of k-means, nor of an evaluation criterion that assumes convexity, is sound.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I can't see how an evaluation index assumes convex clusters. Can anyone illustrate this idea?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-07T16:40:54.673" Id="92885" LastActivityDate="2014-04-08T12:25:16.747" OwnerUserId="28183" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;clustering&gt;&lt;data-mining&gt;" Title="Evaluation indexes hypothesis for clustering" ViewCount="27" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Suppose you are given an odds ratio of an interaction term in logistic regression. Is the coefficient associated with this odds ratio simply $\log( \text{OR})$?&lt;/p&gt;&#10;" ClosedDate="2014-04-07T17:20:50.813" CommentCount="0" CreationDate="2014-04-07T17:19:24.503" Id="92895" LastActivityDate="2014-04-07T17:19:24.503" OwnerUserId="43282" PostTypeId="1" Score="0" Tags="&lt;logistic&gt;" Title="Odds Ratio Coefficient" ViewCount="10" />
  <row AnswerCount="0" Body="&lt;p&gt;HI there: I have a one likert item I would like to analyze.  I know that most scales require multiple likert items, but this is just an exploratory exercise of an existing survey. It's out of my control.&lt;/p&gt;&#10;&#10;&lt;p&gt;When I treat the likert categories (strongly support - support - oppose -strongly oppose) as a numeric variable (coding 0 to 1) and conduct a multiple regression, I get some predictors in my models as statistically s ignificant.  When I try to create some predicted values based on different configurations of my indendent variables, however, some combinations of values create predicted values larger than 1, which is impossible given the original scale.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Incidentally, when I recode the four categories to two (support - oppose) and rerun it with a binomial logistic regression, the same predictors do not appear as statistically significant.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I have three questions: in the first scenario, is it meaningful and / or appropriate to rescale the predicted values so that the maximum predicted value is 1 and the minimum is 0? Or is that not appropriate? &lt;/p&gt;&#10;&#10;&lt;p&gt;Second question, can someone explain why the predictors would not be showing up as statistically significant when turning to logistic regression? Is there important variance at the poles of the DV (between strongly support and support) that is being lost when squishing them into binomial?&lt;/p&gt;&#10;&#10;&lt;p&gt;Third question, should I just analyze this as ordinal? &lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you very much for your time. &lt;/p&gt;&#10;&#10;&lt;p&gt;I am conducting this on R 3.0.2.&#10;Simon&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-04-07T18:08:59.030" Id="92902" LastActivityDate="2014-04-07T18:08:59.030" OwnerUserId="43288" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;logistic&gt;&lt;ordinal&gt;&lt;likert&gt;" Title="Likert Item, Regression analysis" ViewCount="89" />
  <row Body="&lt;p&gt;I assume that $L(W)$ is the log likelihood function. The answer is convergence.&#10;When the difference between old W and updated W is small enough, you stop the iteration. Detailed implemention in R has been posted here: &lt;a href=&quot;http://mathiology.blogspot.nl/2014/04/logistic-regression-with-gradient_7.html&quot; rel=&quot;nofollow&quot;&gt;http://mathiology.blogspot.nl/2014/04/logistic-regression-with-gradient_7.html&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-07T19:06:56.083" Id="92909" LastActivityDate="2014-04-07T19:06:56.083" OwnerUserId="7787" ParentId="55992" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;The question is to find similarities between groups of rows. Data Clustering is required. That will be k-means or some other algorithm will do the task.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-04-07T19:09:11.193" Id="92911" LastActivityDate="2014-04-07T19:09:11.193" OwnerUserId="42406" ParentId="91037" PostTypeId="2" Score="-1" />
  
  
  
  
  
  
  <row Body="&lt;p&gt;I think I found a solution. I read two book chapters about multilevel event history models (Courgeau, 2007; Goldstein, 2011), which discuss similar cases and suggest using a three-level structure such as time (level-1) nested within households (level-2), which are in turn nested within municipalities (level-3). Goldstein (2011, p. 221) explicitly states for this structure that The exploratory variables can be defined at any level. They may also vary over time, allowing so-called time varying covariates.&lt;/p&gt;&#10;&#10;&lt;p&gt;So here is a quick explanation why I think that such a three-level model is able to correctly incorporate time-varying predictors at the municipality-level (level-3), such as the environmental variable Env1. Because Env1 varies across time, the model automatically treats it as a level-1 variable. It does not know that at each time step (e.g., year 1990), the values for Env1 are the same for all households located in a particular municipality. However, I dont think that this biases the standard errors for the Env1 variable because I have household random effects (level-2) included in the model, which estimate a separate intercept for each household. Moreover, I also include an additional variance component at level-2 that allows the slope of Env1 to vary randomly across households. In this way the effect of Env1 is uniquely computed for each household.&lt;/p&gt;&#10;&#10;&lt;p&gt;References:&lt;/p&gt;&#10;&#10;&lt;p&gt;Courgeau, D. (2007). Multilevel synthesis: From the group to the individual. Dordrecht, The Netherlands: Springer.&lt;/p&gt;&#10;&#10;&lt;p&gt;Goldstein, H. (2011). Multilevel statistical models (4th ed.). Chichester, U.K.: John Wiley &amp;amp; Sons.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-08T03:10:59.297" Id="92958" LastActivityDate="2014-04-08T03:10:59.297" OwnerUserId="42121" ParentId="90381" PostTypeId="2" Score="0" />
  
  
  
  
  
  <row Body="&lt;p&gt;The score you calculated will be correct. The rule doesn't get violated since all you're doing is scoring an entity.&lt;/p&gt;&#10;&#10;&lt;p&gt;The &quot;holding constant&quot; rule essentially helps you identify how much influence each variable has on the outcome. In the presence of multiple factors, the only way for us to estimate impact of one variable is to control for the others.&lt;/p&gt;&#10;&#10;&lt;p&gt;Interpreting 1.1 to be the contribution of apples would only hold if you compare two people who both eat the same number of oranges.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-08T09:42:28.937" Id="92983" LastActivityDate="2014-04-08T09:45:15.243" LastEditDate="2014-04-08T09:45:15.243" LastEditorUserId="17230" OwnerUserId="30417" ParentId="92977" PostTypeId="2" Score="5" />
  <row AcceptedAnswerId="93057" AnswerCount="2" Body="&lt;p&gt;We have a random number generator which is supposed to generate 64 bit floats, uniformly.&lt;/p&gt;&#10;&#10;&lt;p&gt;We want to test whether it is a good uniformly random.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am not asking the general way to test it, as it was asked here &lt;a href=&quot;http://stackoverflow.com/questions/22916519/test-the-randomness-of-a-black-box-that-outputs-random-64-bit-floats&quot;&gt;http://stackoverflow.com/questions/22916519/test-the-randomness-of-a-black-box-that-outputs-random-64-bit-floats&lt;/a&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;As I understand, the first step for the test is to somehow divide the whole range into equal sizes like bins, then we keep generating and see the number of &lt;em&gt;&quot;balls&quot;&lt;/em&gt; falling into each bin.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is about this first step.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;How should I split the whole range of 64 bits float?&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;The range is between &lt;code&gt;-1.79769313486231571e+308&lt;/code&gt; and &lt;code&gt;1.79769313486231571e+308&lt;/code&gt; and it is huge. &lt;/p&gt;&#10;&#10;&lt;p&gt;My idea is to utilise the &lt;strong&gt;64 bits&lt;/strong&gt;. Can I design the equal size bins like this:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Every float has 64 bits, so we have 64 bins.&lt;/li&gt;&#10;&lt;li&gt;For every float generated, we read all those bits, if a bit is 1, then we increased the according bin's number by 1&lt;/li&gt;&#10;&lt;li&gt;After N samplings, the expected number of each bin should be &lt;em&gt;N/2&lt;/em&gt;. &lt;/li&gt;&#10;&lt;li&gt;Then we carry on &lt;em&gt;pearson's chi-square test&lt;/em&gt;, etc.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2014-04-08T09:52:46.973" Id="92984" LastActivityDate="2014-04-08T18:43:11.720" LastEditDate="2014-04-08T11:09:05.503" LastEditorUserId="43342" OwnerUserId="43342" PostTypeId="1" Score="5" Tags="&lt;chi-squared&gt;&lt;random-generation&gt;&lt;uniform&gt;&lt;randomization&gt;" Title="Test the randomness (uniformly distributed) on a 64 bit float random generator" ViewCount="239" />
  
  <row Body="&lt;p&gt;Your interpretation is fine, but I wonder about your regression. I would imagine that apples and oranges have nonlinear effects on health. Eating an apple a day is good. Eating an apple a year - not so good. But, eating 10 apples a day, probably also not so good. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-04-08T10:30:54.603" Id="92988" LastActivityDate="2014-04-08T10:30:54.603" OwnerUserId="686" ParentId="92977" PostTypeId="2" Score="3" />
  
  <row AcceptedAnswerId="93124" AnswerCount="1" Body="&lt;p&gt;I've read &lt;a href=&quot;http://www.utdallas.edu/~herve/abdi-prc-pretty.pdf&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; that &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;When the independent variables are pairwise orthogonal, the effect of&#10;  each of them in the regression is assessed by computing the slope of&#10;  the regression between this independent variable and the dependent&#10;  variable. In this case, (i.e., orthogonality of the IVs), the partial&#10;  regression coefficients are equal to the regression coefficients. In all&#10;  other cases, the regression coefficient will differ from the partial&#10;  regression coefficients.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;However, the document did not previously explain what the difference between these two types of regression coefficients is.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-08T11:06:14.210" Id="92992" LastActivityDate="2014-04-19T16:15:34.460" LastEditDate="2014-04-19T16:15:34.460" LastEditorUserId="7290" OwnerUserId="43338" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;multiple-regression&gt;&lt;terminology&gt;" Title="What's the difference between regression coefficients and partial regression coefficients?" ViewCount="131" />
  
  
  <row Body="&lt;p&gt;it's not clear what t-test you're running, but it's clear that the assumptions are violated. the samples are heavily correlated when you overlap them.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-08T13:35:03.007" Id="93014" LastActivityDate="2014-04-08T13:35:03.007" OwnerUserId="36041" ParentId="93003" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;The document is incorrect. At least the exogenous variables can be nominal or ordinal, see for example: &lt;/p&gt;&#10;&#10;&lt;p&gt;Heise, David R. (1972). Employing nominal variables, induced variables, and block variables in path analysis. &lt;em&gt;Sociological Methods &amp;amp; Research&lt;/em&gt;, 1(2): 147--173. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-08T14:36:41.287" Id="93026" LastActivityDate="2014-04-08T14:36:41.287" OwnerUserId="23853" ParentId="93017" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;I realize this is a year old post but its likely to come up again.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are many factors that play into this, I'd argue the most important is your hypothesis. So there is no clear answer but I generally follow these rules-of-thumb:&lt;/p&gt;&#10;&#10;&lt;p&gt;1) Type II is only when you don't have an interaction term.&lt;/p&gt;&#10;&#10;&lt;p&gt;2) Type I vs Type III to test the interaction term...I go Type I all the time reason is this&lt;/p&gt;&#10;&#10;&lt;p&gt;Type I SS for dv ~ A + B + A*B depends on order so...its sequential&#10;SS(A)&#10;SS(B|A)&#10;SS(A*B|A B) &lt;/p&gt;&#10;&#10;&lt;p&gt;This is great to test your interaction...but not great to test the main effects since the effect of B is dependent on A.&lt;/p&gt;&#10;&#10;&lt;p&gt;Type II gets around this&#10;SS(A|B)&#10;SS(B|A)&lt;/p&gt;&#10;&#10;&lt;p&gt;Which looks great to test your main effects IF there is no interaction term.&lt;/p&gt;&#10;&#10;&lt;p&gt;Type III:&#10;SS(A| A*B B)&#10;SS(B| A*B A)&lt;/p&gt;&#10;&#10;&lt;p&gt;Which given the most common hypotheses...doesn't seem very useful since most people are interested in the interaction term, not the main effects when an interaction term is present.&lt;/p&gt;&#10;&#10;&lt;p&gt;So in this case I'd use Type-I to test the interaction term.  If not significant I'd refit without the interaction term and use Type-II to test the main effects.&lt;/p&gt;&#10;&#10;&lt;p&gt;warning: anova() in R is Type-I, to get Type-II (or III) use the Anova() in the car package.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-08T14:48:13.800" Id="93031" LastActivityDate="2014-04-08T14:48:13.800" OwnerUserId="29884" ParentId="60362" PostTypeId="2" Score="2" />
  <row AnswerCount="0" Body="&lt;p&gt;In particular, I am wondering why we have this concept Multiple R (which I can understand as the correlation between observed and predicted scores in multiple regression), and then a separate concept R-squared which is just the square or R. &lt;/p&gt;&#10;&#10;&lt;p&gt;I've been informed that R-squared is the percentage variation explained and R is not, but I don't understand the distinction that is being made between correlation and explained variation.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-04-08T15:33:41.543" Id="93037" LastActivityDate="2014-04-08T15:33:41.543" OwnerUserId="43338" PostTypeId="1" Score="1" Tags="&lt;multiple-regression&gt;" Title="What does it mean to explain variance?" ViewCount="46" />
  <row Body="&lt;p&gt;In general I don't think this is going to be possible (let alone useful). Recall that VC-dimension is an existence property, i.e., for some hypothesis class $H$,  $\text{VCdim}(H) = d$ if &lt;strong&gt;there exists&lt;/strong&gt; some set $X \subseteq \mathcal{X}$ such that $|X| = d$ and $H$ shatters $X$ and there does not exist a set of size $&amp;gt; d$ which is shattered by $H$. Importantly, this does not mean every set of size $d$ is shattered, just at least one. &lt;/p&gt;&#10;&#10;&lt;p&gt;To understand the difference let's take the canonical example where $H$ is the set of lines in 2d space. Then we know $\text{VCdim}(H) = 3$. To show this we first appeal to &lt;a href=&quot;http://en.wikipedia.org/wiki/Radon%27s_theorem&quot; rel=&quot;nofollow&quot;&gt;Radon's theorem&lt;/a&gt; to show $\text{VCdim}(h) &amp;lt; 4$. Next we observe that 3 points arranged in a triangle can be shatter by lines. Notice how this doesn't mean every set of three points in shattered. Indeed, no set of three &lt;a href=&quot;http://en.wikipedia.org/wiki/Collinearity&quot; rel=&quot;nofollow&quot;&gt;collinear&lt;/a&gt; points can be shattered by lines.&lt;/p&gt;&#10;&#10;&lt;p&gt;I think to actually know if some hypothesis class shatters a specific set of points you would just need to try (unless you can make some special assumptions about the points).&lt;/p&gt;&#10;&#10;&lt;p&gt;Lastly, and most importantly, the process you are considering is seriously flawed. If you observe a set of points and then pick a hypothesis class that ensures this set of points is shattered, you throw away any VC style bounds you might get. For those type of bounds to hold you need to pick your hypothesis class &lt;strong&gt;before observing&lt;/strong&gt; the data. What you propose actually results in a hypothesis class with infinite VC-dimension, since you just keep considering more and more complex hypothesis classes until you shatter the observed data. At this point you might as well be using a 1 nearest neighbor classifier.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-08T15:50:31.077" Id="93041" LastActivityDate="2014-04-08T15:50:31.077" OwnerUserId="6248" ParentId="92897" PostTypeId="2" Score="1" />
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have the following likelihood function:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/1d5Tw.gif&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm given this information about the $\Omega$ matrix ($\boldsymbol{1}$ is a $T \times 1$ vector of ones):&#10;&lt;img src=&quot;http://i.stack.imgur.com/YLjFa.gif&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to be able to show that the likelihood function can be rewritten to this:&#10;&lt;img src=&quot;http://i.stack.imgur.com/sKuPa.gif&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I think there is a misprint in the article, hence the screen images from the article I attached are wrong, meaning that $\Omega^{-1}$ is supposed to be&#10;$$\Omega^{-1}=\frac{1}{\sigma^2_{\varepsilon}+T\sigma^2_{c}}\boldsymbol{1}(\boldsymbol{1}'\boldsymbol{1})^{-1}\boldsymbol{1}'+\frac{1}{\sigma^2_{\varepsilon}}(I_t-\boldsymbol{1}(\boldsymbol{1}'\boldsymbol{1})^{-1}\boldsymbol{1}')$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and not as stated above. &lt;/p&gt;&#10;&#10;&lt;p&gt;So far I've inserted the expression for $\Omega^{-1}$ in the top log-likelihood function. However, especially the last expression of the rewritten log-likelihood function bothers me, i.e. this part:&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{equation*}&#10;\begin{split}&#10;...&amp;amp;&#10;&amp;amp;+\frac{1}{2}\frac{1}{\sigma^2_{\varepsilon}}\frac{1}{N}\sum_{i=1}^{N}&#10;\left(Y_i-\boldsymbol{1}\bar{Y}_i-\left(X_i-\boldsymbol{1}\bar{X}_i\right)\beta\right)'\left(Y_i-\boldsymbol{1}\bar{Y}_i-\left(X_i-\boldsymbol{1}\bar{X}_i\right)\beta\right).&#10;\end{split}&#10;\end{equation*}&lt;/p&gt;&#10;&#10;&lt;p&gt;Does anyone see the solution?&lt;/p&gt;&#10;&#10;&lt;p&gt;I also have this lemma, but not sure if it is to be used in the rewritting of the likelihood function:&#10;&lt;img src=&quot;http://i.stack.imgur.com/wlhbU.gif&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-04-08T18:28:38.930" FavoriteCount="1" Id="93061" LastActivityDate="2014-04-09T14:33:34.897" LastEditDate="2014-04-09T14:33:34.897" LastEditorUserId="919" OwnerUserId="43400" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;maximum-likelihood&gt;&lt;panel-data&gt;&lt;matrix&gt;" Title="Understanding the derivation of a ML-estimator" ViewCount="65" />
  <row AnswerCount="0" Body="&lt;p&gt;Maximum Likelihood is given by the formula&lt;/p&gt;&#10;&#10;&lt;p&gt;$h_{ML}=arg\space max_{h \in H } \space\space p(D/h)$&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to transform it in terms of mechanism involved in Expectation maximization.&lt;/p&gt;&#10;&#10;&lt;p&gt;$h_{ML}=arg\space max_{h \in H } \space\space p(D/h)$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\implies h_{ML}=arg\space max_{h \in H } \space\space \ln (p(D/h))$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\implies h_{ML}=arg\space max_{h \in H } \space\space \ln (p(D/h))$&lt;/p&gt;&#10;&#10;&lt;p&gt;Now i divided the above in to two parts .&lt;/p&gt;&#10;&#10;&lt;p&gt;$Q(h'/h) \leftarrow   E_{h,X}[\ln (p(D/h))]$&lt;/p&gt;&#10;&#10;&lt;p&gt;$h_{ML}=arg\space max\space Q(h'/h)$&lt;/p&gt;&#10;&#10;&lt;p&gt;$Q$ is just an intermediate function.&lt;/p&gt;&#10;&#10;&lt;p&gt;$E_{h,X}$ is an expectation function that expects unknown data w.r.t observed data $X$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is it correct way of interpretation  ?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-08T18:43:09.453" Id="93064" LastActivityDate="2014-04-08T18:43:09.453" OwnerUserId="42883" PostTypeId="1" Score="0" Tags="&lt;maximum-likelihood&gt;&lt;expectation-maximization&gt;" Title="Maximum likelihood hypothesis vs Expectation maximization" ViewCount="21" />
  <row AcceptedAnswerId="93259" AnswerCount="1" Body="&lt;p&gt;I am trying to decide on an approach to estimate design effect for a multi stage cluster survey.  The clusters were selected with probability proportional to size sampling WITH replacement.  The primary sampling units (districts) are large enough that some were sampled multiple times.  When defining the PSU variable should the unique districts within the sample be used, or a cluster variable representing the true number of clusters sampled?&lt;/p&gt;&#10;&#10;&lt;p&gt;For example:&#10;A district gets sampled twice, should the PSU variable described, be one or two individual clusters?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-08T19:03:18.290" Id="93068" LastActivityDate="2014-04-10T00:54:45.913" OwnerUserId="28131" PostTypeId="1" Score="3" Tags="&lt;sampling&gt;&lt;survey&gt;&lt;cluster-sample&gt;" Title="Defining PSU in &quot;Sampled with Replacement&quot; Cluster Samples" ViewCount="42" />
  <row Body="&lt;p&gt;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0471754986&quot; rel=&quot;nofollow&quot;&gt;Rencher's Linear Models in Statistics&lt;/a&gt; is pretty easy to find book and has fairly straight-forward proofs (and this is coming from a non-math major).&lt;/p&gt;&#10;&#10;&lt;p&gt;Since you are coming from a math background, you might appreciate the extensive use of matrices to explain everything instead of statistics notation, which you might get in some statistics texts.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, the Gauss-Markov Theorem starts on the bottom of page 146.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-04-08T21:07:26.757" Id="93084" LastActivityDate="2014-04-08T21:07:26.757" OwnerUserId="17455" ParentId="93072" PostTypeId="2" Score="0" />
  
  
  
  <row AcceptedAnswerId="93103" AnswerCount="2" Body="&lt;p&gt;I'm a bit troubled about how to report linear regression statistics after log transformation of the dependent variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;I suppose I should report the transformed coefficient, but would they be easily interpreted?&lt;/p&gt;&#10;&#10;&lt;p&gt;In the regression plot should I use the transformed values or the original ones? while the seconds would be clearer than it would make no sense to plot the regression line produced after the transformation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any direction?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-08T21:37:18.400" FavoriteCount="2" Id="93089" LastActivityDate="2014-04-09T01:21:44.120" LastEditDate="2014-04-08T22:01:53.363" LastEditorUserId="88" OwnerUserId="6479" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;data-transformation&gt;&lt;logarithm&gt;&lt;reporting&gt;" Title="Reporting regression statistics after logarithmic transformation" ViewCount="262" />
  
  
  
  <row Body="&lt;p&gt;As a general thing, this is a pretty broad question (and so may not speak to your immediate, unstated problem), but I'll do my best.&lt;/p&gt;&#10;&#10;&lt;p&gt;The common approaches to estimation of parameters are maximum likelihood (ML) and method-of-moments (MoM), but there are variations of these, and some that aren't really all that close to either. &lt;/p&gt;&#10;&#10;&lt;p&gt;Most estimation problems are, or can be cast as optimization or root-finding problems in some guise. &lt;/p&gt;&#10;&#10;&lt;p&gt;ML seen directly is an optimization problem, while MoM is a root-finding problem (find the parameter values that solve a system of - possibly nonlinear - equations). [Note, however, that if you can take derivatives, ML can also be cast as root-finding.]&lt;/p&gt;&#10;&#10;&lt;p&gt;Solution techniques for the following are worth knowing about:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;linear least squares problems (e.g. Cholesky, singular value, and QR decompositions)&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;weighted linear least squares problems (ditto)&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;nonlinear least squares (and weighted nls) (e.g. iterative reweighted least squares)&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;ML for generalized linear models (e.g. Fisher scoring)&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;ML more generally (e.g. E-M algorithm; there's a host of other optimization/root-finding techniques used here)&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Some useful optimization/root-finding/estimation techniques besides those already mentioned, very much incomplete:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;Newton-Raphson&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;quasi-Newton methods (BFGS and DFP, for example, also secant method for 1-d) &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;conjugate-gradient methods&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;steepest descent&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;downhill simplex method (aka Nelder-Mead, amoeba)&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;golden section&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;binary search&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;quadratic programming&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;$\quad\vdots$&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Oh, heck, I haven't even touched on things like Levenberg-Marquardt yet. I may have to come back and add a bunch more at some point.&lt;/p&gt;&#10;" CommentCount="7" CommunityOwnedDate="2014-04-09T14:34:16.480" CreationDate="2014-04-09T05:30:35.563" Id="93126" LastActivityDate="2014-04-09T09:09:03.917" LastEditDate="2014-04-09T09:09:03.917" LastEditorUserId="805" OwnerUserId="805" ParentId="93119" PostTypeId="2" Score="4" />
  
  <row AcceptedAnswerId="93143" AnswerCount="1" Body="&lt;p&gt;When using k-fold cross validation in a neural network, do you also need a separate validation set?  Or is the use of the k-fold on its own good enough to minimise the possibility of over-training?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-09T07:00:43.113" Id="93135" LastActivityDate="2014-04-09T07:41:22.993" OwnerUserId="43442" PostTypeId="1" Score="1" Tags="&lt;neural-networks&gt;" Title="K-fold cross validation for neural networks: separate validation set also needed?" ViewCount="93" />
  <row Body="&lt;p&gt;I would leave the &lt;strong&gt;data&lt;/strong&gt; as they are. 200,000 is not an enormous data size for anyone, unless the problem is that they need a decent computer. Not changing the dataset keeps all the detail and stops you discarding information that might be interesting or useful. &lt;/p&gt;&#10;&#10;&lt;p&gt;That still leaves the question of appropriate interval size for those &lt;strong&gt;specific occasions when a reduction is needed for display&lt;/strong&gt;, as say for histograms or tabulations. Irregular size intervals are just puzzling to many readers (especially those better informed technically) and raise the question of what is a property of the data and what a side-effect of arbitrary choices.  &lt;/p&gt;&#10;&#10;&lt;p&gt;A compromise could be to use intervals that are (at least approximately) equal-sized on a logarithmic scale, e.g. 1, 2, 3-4, 5-8, 9-16, etc. or possibly equal-sized on a square root scale. Logarithms and square roots are the most useful transformations for highly skewed data such as these. &lt;/p&gt;&#10;&#10;&lt;p&gt;(Detail: I remain unclear where project size &lt;strong&gt;is&lt;/strong&gt; number of users, or something else.) &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-04-09T09:11:40.743" Id="93149" LastActivityDate="2014-04-09T09:11:40.743" OwnerUserId="22047" ParentId="93146" PostTypeId="2" Score="2" />
  
  <row AnswerCount="0" Body="&lt;p&gt;In a multivariate regression, suppose we want to calculate the metric coefficients from the standardized ones. Is the method (standardized coeffcient times standard deviation of the dependent variable) valid in both OLS and IV or not?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-09T13:02:28.037" Id="93173" LastActivityDate="2014-04-09T13:02:28.037" OwnerUserId="43467" PostTypeId="1" Score="0" Tags="&lt;regression-coefficients&gt;&lt;standardization&gt;&lt;instrumental-variables&gt;" Title="Standardized coefficients and IV method" ViewCount="23" />
  <row AnswerCount="0" Body="&lt;p&gt;I want to determine compliance to a certain standard in ISMS and to determine that, I have the standards started and it has sub-questions (say 2,3,4 questions), with each having a 5 point likert type choices. I have a sample of about 32 questionnaires filled. My difficulty now is how to treat or analyse the sub-question and draw one chart to represent the 5 point scales (for each standard, not each question) and also determine the degree of compliance.  For example: &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;A. Information Security Policy&lt;br&gt;&#10;    A1. Is a published policy document, approved by management:&lt;br&gt;&#10;  (&quot;Not compliant&quot;, &quot;Partly Compliant&quot;, &quot;Currently Compliant&quot;,&quot;Largely Compliant&quot;, &quot;Compliant&quot;)&lt;/p&gt;&#10;  &#10;  &lt;p&gt;A2. Is the published policy reviewed regularly, (at planned intervals) or if significant changes occur to ensure its continuing suitability, adequacy and effectiveness?&lt;br&gt;&#10;  (&quot;Not compliant&quot;, &quot;Partly Compliant&quot;, &quot;Currently Compliant&quot;, &quot;Largely Compliant&quot;, &quot;Compliant&quot;)&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="0" CreationDate="2014-04-09T13:50:35.780" Id="93177" LastActivityDate="2014-04-09T14:13:35.550" LastEditDate="2014-04-09T14:13:35.550" LastEditorUserId="7290" OwnerUserId="43475" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;machine-learning&gt;&lt;correlation&gt;&lt;maximum-likelihood&gt;" Title="sub-questions with likert scales choices" ViewCount="30" />
  <row AnswerCount="1" Body="&lt;p&gt;I'm trying to create Random Forests in Matlab and there are more observations in some classes than there are in others. Do I need to specify this as a cost matrix or as a prior probability or will Matlab figure this out automatically and the fact the data is skewed won't matter.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-09T14:49:52.080" Id="93184" LastActivityDate="2014-04-10T21:11:40.123" OwnerUserId="41023" PostTypeId="1" Score="0" Tags="&lt;classification&gt;&lt;matlab&gt;&lt;random-forest&gt;&lt;unbalanced-classes&gt;" Title="Should I specify Prior or Cost matrix with Tree Bagger in Matlab" ViewCount="237" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;Hayy all,, Im going to do classification using SVM.&lt;br&gt;&#10;As I understand we have to project our data into higher dimensional by using kernel. And there are 4 common use kernel (linear, RBF, polynomial, and sigmoid).&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;What is the different between those kernel?&lt;/li&gt;&#10;&lt;li&gt;We need to find the linear separable line / hyperplane to classify our data. And to do that, we have to chose large margin to avoid any overfitting?&#10;What is overfitting?&lt;/li&gt;&#10;&lt;li&gt;As u can &lt;a href=&quot;http://postimg.org/image/63qcxmb6h/&quot; rel=&quot;nofollow&quot;&gt;see in this image&lt;/a&gt;, there is a w. And in many reference, they said w is a line? What is that actually? And what is that lamda??&lt;/li&gt;&#10;&lt;li&gt;What is SMO mean in SVM?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="1" CreationDate="2014-04-09T17:55:52.523" Id="93208" LastActivityDate="2014-04-09T18:02:20.260" OwnerUserId="43491" PostTypeId="1" Score="0" Tags="&lt;svm&gt;" Title="SVM kernel parameters?" ViewCount="85" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have created a linear model (which has multiple predictors) using the lm() function and I would like to interpret the &quot;coefficients&quot; that I get when I use the summary() function on the linear model. &lt;br&gt;&lt;br&gt;Now I want to consider how the coefficients reflect on the predictors' influence in the model - am I right in thinking that a large value for the coefficient means that the corresponding predictor has a greater effect? I'm not sure what else I need to consider or if I'm even thinking along the right lines.&lt;br&gt;&lt;br&gt;Also, am I correct in thinking these &quot;coefficients&quot; are in fact the Beta coefficients?&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-04-09T19:39:42.407" Id="93223" LastActivityDate="2014-04-10T07:58:56.577" LastEditDate="2014-04-09T20:05:37.177" LastEditorUserId="17140" OwnerUserId="17140" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;regression&gt;&lt;linear-model&gt;" Title="Interpreting the &quot;coefficient&quot; output of the lm function in R" ViewCount="295" />
  
  <row AcceptedAnswerId="93278" AnswerCount="1" Body="&lt;p&gt;I am using lmer in R to run LMM. My DV is continuous and my IVs are categorical. Many statistician said if the three-level parameter is significant, I cannot interpret the two-level parameter. Does it also include when the parameters are not contradict to each other too?&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, in the two-level, Group2:color2 is significant, but in three-level, Group2:color3:male is significant. Does it mean that I can interpret both levels as they are not contradict to each other, i.e. The first one is telling me that group2 and the other group is different in color2 than the baseline color, whereas the second one is telling me that the effect of group2 and color3 is higher in male compared to female. Or should in just ignore the two-level interaction regardless of whether than they contradict to each other or not?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-09T21:38:55.210" Id="93234" LastActivityDate="2014-04-10T08:14:15.400" OwnerUserId="40023" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;interaction&gt;&lt;linear-model&gt;" Title="Interaction: ignore two-level if three-level is significant in every case?" ViewCount="43" />
  
  <row Body="&lt;p&gt;Actually, the &lt;em&gt;interval&lt;/em&gt; carries over just fine. The transformation is monotonic; the probability statement that applies on the log-scale transforms directly to the original scale, so as long as the assumptions under which the original interval was computed do apply, then it works as an interval for the original population parameter after transformation.&lt;/p&gt;&#10;&#10;&lt;p&gt;It's the estimate that may be problematic (but may be okay, depending on what you want). Note that $E[\exp(X)]\neq \exp[E(X)]$ if $\sigma_X^2&amp;gt;0$. If the log-scale estimate is unbiased, the transformed estimate is biased. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you're happy to have an estimate that's &lt;a href=&quot;http://en.wikipedia.org/wiki/Bias_of_an_estimator#Median-unbiased_estimators&quot; rel=&quot;nofollow&quot;&gt;&lt;em&gt;median&lt;/em&gt;-unbiased&lt;/a&gt;, then the back-transformed estimate is also okay, for the same reason that the interval works.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you seek mean-unbiasedness there are some choices. For example, if you're prepared to assume a normal distribution on $\hat\beta$ you can unbias it by using the properties of the lognormal. Alternatively, you can use a &lt;a href=&quot;http://en.wikipedia.org/wiki/Taylor_expansions_for_the_moments_of_functions_of_random_variables&quot; rel=&quot;nofollow&quot;&gt;Taylor expansion&lt;/a&gt; to get an approximate adjustment (details are also in a number of posts on this site). If the standard error of the estimate is small, it won't matter much.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-09T22:12:19.050" Id="93240" LastActivityDate="2014-04-09T22:22:01.567" LastEditDate="2014-04-09T22:22:01.567" LastEditorUserId="805" OwnerUserId="805" ParentId="93230" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="93255" AnswerCount="1" Body="&lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Consider the following process:&#10;$$2y_t-3y_{t-1}+y_{t-2}=\epsilon_t-\theta\epsilon_{t-1}$$&#10;What is the model for the process $w_t=\Delta y_t = y_t-y_{t-1}$?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Attempt&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I have solved the question in two ways, which seem to be giving conflicting answers.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Substitute the form of $y_t$ into the expression of $w_t$: &#10;$$w_t=\frac{3}{2}y_{t-1}-\frac{1}{2}y_{t-2}+\frac{1}{2}\epsilon_t-\frac{\theta}{2}\epsilon_{t-1}-y_{t-1}&#10;    \\ \implies&#10;    w_t=\frac{1}{2}y_{t-1}-\frac{1}{2}y_{t-2}+\frac{1}{2}\epsilon_t-\frac{\theta}{2}\epsilon_{t-1}&#10;    \\ \implies w_t = \frac{1}{2}w_{t-1}&#10;    +\frac{1}{2}\epsilon_t-\frac{\theta}{2}\epsilon_{t-1} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;This is an ARMA(1,1) model.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;Use the expanded forms of $y_t$ and $y_{t-1}$, and then substitute into the expression for $w_t$:&#10;$$y_t=\frac{3}{2}y_{t-1}-\frac{1}{2}y_{t-2}+\frac{1}{2}\epsilon_t-\frac{\theta}{2}\epsilon_{t-1} \\ y_{t-1} =\frac{3}{2}y_{t-2}-\frac{1}{2}y_{t-3}+\frac{1}{2}\epsilon_{t-1}-\frac{\theta}{2}\epsilon_{t-2}  \\ \implies w_t = \frac{3}{2}w_{t-1} - \frac{1}{2}w_{t-2}+\frac{1}{2}\epsilon_t-\frac{\theta+1}{2}\epsilon_{t-1}+\frac{\theta}{2}\epsilon_{t-2}$$&#10;where the facts that $w_{t-1}=\Delta y_{t-1}= y_{t-1} - y_{t-2}$ and $w_{t-2}=\Delta y_{t-2}= y_{t-2} - y_{t-3}$ were used. This is an ARMA(2,2) model.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;As can be seen, both these approaches yield different models. The first is stationary, while the second is not (by checking the roots of the AR characteristic polynomials). Is this possible, or is there a mistake in one approach? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-09T22:56:22.410" Id="93247" LastActivityDate="2014-04-10T00:38:01.353" OwnerUserId="27581" PostTypeId="1" Score="2" Tags="&lt;time-series&gt;&lt;self-study&gt;&lt;arima&gt;&lt;arma&gt;" Title="Representation of ARMA processes" ViewCount="52" />
  <row AcceptedAnswerId="93256" AnswerCount="1" Body="&lt;p&gt;I want to test if the $\alpha$ for a scale is dependent upon a personality test, which is a continuous variable. I understand that there are many methods to test if two $\alpha$'s are significantly different for two different groups when they are given the same test. But what if the grouping variable is not dichotomous, but continuous? Should I just do a median or tertial split to artificially create separate groups and test if they have different $\alpha$'s? I've heard bad things about median split (like reducing power and whatnot), so I'm hesitant to use this method. Any suggestion?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-09T23:23:14.930" Id="93251" LastActivityDate="2014-04-10T00:46:19.637" LastEditDate="2014-04-10T00:46:19.637" LastEditorUserId="7290" OwnerUserId="43511" PostTypeId="1" Score="4" Tags="&lt;reliability&gt;" Title="How to compare Cronbach's alpha when the grouping variable is continuous?" ViewCount="92" />
  
  <row Body="&lt;p&gt;You need to take all levels into account when interpreting 3 level interactions, which is why they are so hard to interpret. &lt;/p&gt;&#10;&#10;&lt;p&gt;It often helps to look at predictions for all combinations of the three variables. Alternatively, and in some sense equivalently, you can use some tricks discussed &lt;a href=&quot;http://www.maartenbuis.nl/publications/ref_cat.html&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;. This refers to Stata, but I would not be surprised if similar tricks are easy to implement in R.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-04-10T08:14:15.400" Id="93278" LastActivityDate="2014-04-10T08:14:15.400" OwnerUserId="23853" ParentId="93234" PostTypeId="2" Score="1" />
  
  
  <row AcceptedAnswerId="93288" AnswerCount="2" Body="&lt;p&gt;I have a dataset composed by 5000 observations. Each observation contains the income per year of a person (from 50 to 50.000.000) and the fact of having a car (yes/no).&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to check if a correlation exists between these two features.&#10;Which test I should run?&lt;/p&gt;&#10;&#10;&lt;p&gt;thanks in advance &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-10T08:53:02.030" FavoriteCount="1" Id="93285" LastActivityDate="2014-04-10T10:42:10.153" LastEditDate="2014-04-10T10:16:46.380" LastEditorUserId="88" OwnerUserId="42594" PostTypeId="1" Score="1" Tags="&lt;correlation&gt;&lt;nominal&gt;&lt;qualitative&gt;" Title="Correlation between quantitative and qualitative variables" ViewCount="804" />
  <row Body="&lt;p&gt;If the factor analysis is putting that question with other questions then is &lt;em&gt;is&lt;/em&gt; related. &lt;/p&gt;&#10;&#10;&lt;p&gt;Unless you are misinterpreting the results of the factor analysis. &lt;/p&gt;&#10;&#10;&lt;p&gt;FA constructs linear combinations of variables and calls them factors. Every factor includes every question, just with different weights. If the question that you are worried about has a high weight (positive or negative) in a factor where other questions also have high weights, then it is related to those questions. &lt;/p&gt;&#10;&#10;&lt;p&gt;So, you've learned something about the questions or about your sample. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-04-10T10:06:22.610" Id="93292" LastActivityDate="2014-04-10T10:06:22.610" OwnerUserId="686" ParentId="93161" PostTypeId="2" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;Im designing a system (using Matlab) that I can optimize parameters of a support vector machine (SVM) with genetic algorithm, harmony search and another optimization algorithms to find the best structure of SVM for a specific data. My problem is binary classification with 0 and 1 output and I normalize data (mapmaxmin o mapstd) before insert it to system. Besides it in some cases I use dimension reduction (for example FDA) to reduce my features. For this normalized data I must set the boundary of searching space in optimization algorithm. This is my SVM function:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;svmstruct=svmtrain(TrainInputs,TrainTargets,...&#10;'boxconstraint',Penalty,...&#10;'kernel_function','rbf','method','QP',...&#10;'rbf_sigma',Sigma,...&#10;'autoscale','false');&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I optimize only 'boxconstraint' and rbf sigma. For boxconstraint, my algorithm is searching in [0.001 400] and for sigma the searching space is same [0.001 400]. IS this searching boundaries is suitable for my problem or I must change these boundaries? Otherwise, I set autoscale to false. Which one is better in my problem? false or true ? &lt;/p&gt;&#10;&#10;&lt;p&gt;I set kernel function to rbf. is this a good approach for this problem?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-10T10:22:45.250" Id="93296" LastActivityDate="2014-04-10T10:22:45.250" OwnerUserId="43534" PostTypeId="1" Score="0" Tags="&lt;classification&gt;&lt;svm&gt;&lt;matlab&gt;&lt;optimization&gt;" Title="Finding best parameters of SVM in matlab" ViewCount="327" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Are there some general rules, or Monte Carlo studies, that shows evidence of the approximate number of time periods needed to implement the NeweyWest estimator?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-10T11:40:21.147" Id="93310" LastActivityDate="2014-04-10T11:40:21.147" OwnerUserId="37765" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;standard-error&gt;" Title="NeweyWest estimator - Number of time observations needed" ViewCount="18" />
  
  
  <row Body="&lt;p&gt;To pick up from &lt;a href=&quot;http://stats.stackexchange.com/a/93284/5739&quot;&gt;Max Gordon's answer&lt;/a&gt;, the zero values of $\beta$ correspond to the unit values of the hazard ratios. Hence a ``strong'' result would be that the hazard ratios are sufficiently far from 1. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, while hazard ratios are more interpretable than coefficients themselves, inference is weird in that scale, and asymptotic normality of results requires larger (often much larger) samples. To overcome this weirdness, Stata reports an asymmetric confidence interval for the ratios, exponentiating the endpoints of the CI obtained on the scale of coefficients. Thus instead of looking at hazard ratios and their standard errors, I would advise looking at the confidence intervals.&lt;/p&gt;&#10;&#10;&lt;p&gt;(This has nothing to do with the debate about p-values, null hypothesis testing, Bayes factors, effect sizes, and any of that philosophy. This is just plain higher order asymptotics.)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-10T13:37:32.893" Id="93324" LastActivityDate="2014-04-10T13:37:32.893" OwnerUserId="5739" ParentId="91490" PostTypeId="2" Score="0" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Asked to compute estimator for the following function,&lt;/p&gt;&#10;&#10;&lt;p&gt;$\theta = \int_0^\infty e^{-x^2}$&lt;/p&gt;&#10;&#10;&lt;p&gt;which can be solved by transforming the limits to 0 to 1 and solving the following expectation using Monte Carlo,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ E[1/U \space e^{-1(-1+1/U)^2}]$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $U$ is uniformly distributed&lt;/p&gt;&#10;&#10;&lt;p&gt;The estimator is&#10;$$\hat\theta = \frac 1N \sum_0^n \frac 1U \space e^{-1(-1+1/U)^2}. $$&lt;/p&gt;&#10;&#10;&lt;p&gt;How do I go about computing the sample variance and then the error? Are there any formulas to choose the best $N$?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-04-10T14:31:38.007" Id="93330" LastActivityDate="2014-04-10T21:10:00.080" LastEditDate="2014-04-10T21:10:00.080" LastEditorUserId="14675" OwnerUserId="43553" PostTypeId="1" Score="2" Tags="&lt;variance&gt;&lt;monte-carlo&gt;" Title="Sample variance and error using Monte Carlo" ViewCount="28" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Is there a standard approach for testing goodness-of-fit to a probability distribution when the samples are independent but not identically distributed? In other words, given the data $\{\{x_1, y_1\}, \{x_2, y_2\},..., \{x_n, y_n\}\}$ and the candidate density function $f(x;y)$, what is an accepted approach for testing whether the $x_i$ are sampled from $X|y_i \sim f(x; y_i)$? What are the assumptions of said approach?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;In the comments, whuber suggests it is more appropriate to call $y_i$ a parameter for the distribution on $x_i$ and requests more information about how $f(x; y)$ depends on $y$. NB, I'm interested in &quot;standard methods&quot; more than a custom solution, so the answer should not depend on the details of $f$. That said, in my application, $f$ is a mixture of $m$ Dirichlet-Multinomial distributions (so $x$ is discrete, I asked about continuous variables above in order to reference the K-S test). The vector $\{y_{i,1},y_{i,2},...,y_{i,m}\}$ is the mixture weighting. The parameter for each Dirichlet-Multinomial distribution is independent of $y_i$.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2014-04-10T21:36:56.727" Id="93374" LastActivityDate="2014-04-11T15:27:53.643" LastEditDate="2014-04-11T15:27:53.643" LastEditorUserId="43122" OwnerUserId="43122" PostTypeId="1" Score="2" Tags="&lt;goodness-of-fit&gt;&lt;kolmogorov-smirnov&gt;" Title="Generalization of one-sample Kolmogorov-Smirnov test for non identically distributed data?" ViewCount="65" />
  <row AnswerCount="0" Body="&lt;p&gt;Given a generic classification model &#10;$y=f(x_1,x_2,..,x_p)$ where $y\in \left\lbrace 0,1 \right\rbrace$  is it possible to compute the odds ratio for each variable?   &lt;/p&gt;&#10;&#10;&lt;p&gt;A theoretical explanation and an example with R code would be the  best answer&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-04-10T21:55:34.103" Id="93376" LastActivityDate="2014-04-10T21:55:34.103" OwnerUserId="25392" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;classification&gt;&lt;modeling&gt;&lt;odds-ratio&gt;" Title="how to compute odds ratio" ViewCount="43" />
  
  
  
  
  
  
  <row Body="&lt;p&gt;Here is one possibility (for $b\neq 0$): let your $P$ be the probability mass function of a random variable $X$ and let $Y$ have the same distribution as $bX$. Then we see that $$P(X=k)=P(bX=bk)=P(Y=bk)=af(bk)$$ where $a=1$ and $f=P$. Now if you let $a$ be any real number, you will in general not get a probability mass function on $\{0,\ldots,bN\}$ but a multiple of it, a signed measure.&lt;/p&gt;&#10;&#10;&lt;p&gt;Edit:&lt;/p&gt;&#10;&#10;&lt;p&gt;Because $P$ is a probability mass function, $af(bk)$ must be nonnegative for all $k\in\{0,\ldots,N\}$ and fulfil $\sum_{k=0}^N af(bk)=1$. So clearly $a\neq 0$. Without loss of generality, assume $a&amp;gt;0$ (if not, replace $f$ by $-f$). Because $a&amp;gt;0$ and $P$ defines a probability measure, also $f$ must be nonnegative. We now find that $a = 1/(\sum_{k=0}^N f(bk))$. Now define the function $g$ on $\{0,b,\ldots,bN\}$ via $g(bk)=af(bk)$. Then $g$ is the probability mass function of $bY$. So you see that $b$ scales the state space of the random variable $X$ while $a$ may be any positive number that just scales the total mass of $f$, with the total mass of $af$ being one.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-04-11T12:29:14.323" Id="93439" LastActivityDate="2014-04-11T13:19:11.370" LastEditDate="2014-04-11T13:19:11.370" LastEditorUserId="42609" OwnerUserId="42609" ParentId="93428" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I know this is not exactly what you asked but in R (&lt;a href=&quot;http://www.r-project.org/&quot; rel=&quot;nofollow&quot;&gt;http://www.r-project.org/&lt;/a&gt;) I would have run this formula:&#10;&lt;strong&gt;&lt;em&gt;pain ~ drug + time + (1+time|meow) + (1+time|subject)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Some notes:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;This should take care of nesting issues.&lt;/li&gt;&#10;&lt;li&gt;I gave time both a fixed effect and a random effect (since I'm assuming time has some fixed influence that is not related to group/subject and that each group/subject can react differently to time)&lt;/li&gt;&#10;&lt;li&gt;Regarding leveling - in you case the 16 groups and 160 subjects can be used for random effects. What is your concern here?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I hope this answers some of your questions..&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, I'm currently researching new ways to automate statistical analysis. My focus is on enabling users to easily create mixed models - Will you be willing to share your data with me so I could try and see analysis results with your data...&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-11T12:51:07.440" Id="93445" LastActivityDate="2014-04-11T12:51:07.440" OwnerUserId="43614" ParentId="92391" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a dataset of 2.000.000 projects. Each project is defined by its size and the number of active developers.&lt;/p&gt;&#10;&#10;&lt;p&gt;After applying a logarithmic transformation on the project size, I've plotted the data.&#10;It seems to me that the number of active developers decreases when the project size goes up. When I try to apply Spearman's correlation rank, I find only little evidence of it (0.09520886)&lt;/p&gt;&#10;&#10;&lt;p&gt;I tried to use linear regression: &lt;code&gt;lm(size ~ active developers)&lt;/code&gt; (in R), but I got negative results:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Coefficients:&#10;        Estimate Std. Error t value Pr(&amp;gt;|t|)    &#10;(Intercept)   41.672      2.871  14.513   &amp;lt;2e-16 ***&#10;colls         18.381     11.034   1.666   0.0957&#10;Residual standard error: 4089 on 2126091 degrees of freedom&#10;Multiple R-squared:  1.305e-06, Adjusted R-squared:  8.351e-07 &#10;F-statistic: 2.775 on 1 and 2126091 DF,  p-value: 0.09572&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Then I tried to use a negative binomial regression &lt;code&gt;glm.nb(commits ~ colls)&lt;/code&gt;, but I got negative results again:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Coefficients:&#10;        Estimate Std. Error z value Pr(&amp;gt;|z|)    &#10;   (Intercept)  3.73200    0.04862   76.76   &amp;lt;2e-16 ***&#10;   colls        0.32318    0.18683    1.73   0.0837 . &#10;&#10;Null deviance: 2168.2  on 2126092  degrees of freedom&#10;Residual deviance: 2164.6  on 2126091  degrees of freedom&#10;AIC: 41924708&#10;&#10;Number of Fisher Scoring iterations: 1&#10;&#10;&#10;          Theta:  0.0002086134 &#10;      Std. Err.:  7.43e-08 &#10;&#10;2 x log-likelihood:  -41924701.8870000020 &#10;qchisq(0.95, 2126091) =  2129484&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Which method I should use to get more insights about the relation between these two variables?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/dXGuP.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I add the plot of mean log against file[,4]&#10;&lt;img src=&quot;http://i.stack.imgur.com/zcpb8.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I add the plot of mean reciprocal against number of active developers&#10;&lt;img src=&quot;http://i.stack.imgur.com/lKQJF.png&quot; alt=&quot;enter image description here&quot;&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/AwMMr.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-04-11T13:47:16.133" Id="93447" LastActivityDate="2014-04-11T20:45:32.217" LastEditDate="2014-04-11T20:45:32.217" LastEditorUserId="42594" OwnerUserId="42594" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;correlation&gt;&lt;data-transformation&gt;" Title="correlation with logarithmic transformation" ViewCount="91" />
  <row AcceptedAnswerId="93462" AnswerCount="3" Body="&lt;p&gt;I am trying to estimate (fit) the distribution of a variable. The first step in doing so is to draw a normal probability plot. This is what I have obtained (using R):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;qqnorm(x)&#10;qqline(x)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/YoSHg.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;This is the histogram of the data:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/MhqPm.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;As you can see, there is a quadratic pattern in the normal probability plot, but most point fall ABOVE the reference line. &#10;The end purpose for this analysis is to detect outliers. Estimating the distribution of this variable would allow me to determine the threshold to set to determine outliers.&lt;/p&gt;&#10;&#10;&lt;p&gt;What can we deduce from both the normality plot and the histogram, in terms of the distribution of x, and the outliers?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you!&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2014-04-11T14:01:49.980" Id="93448" LastActivityDate="2014-08-28T01:12:16.830" LastEditDate="2014-08-28T01:12:16.830" LastEditorUserId="805" OwnerUserId="29183" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;distributions&gt;&lt;normal-distribution&gt;&lt;qq-plot&gt;" Title="Estimating the distribution of a variable" ViewCount="171" />
  <row AcceptedAnswerId="93470" AnswerCount="2" Body="&lt;p&gt;I have a document classification problem in which the estimated class proportions in the population are severely unbalanced: the population is ~99% class 0 and ~1% class 1. &lt;/p&gt;&#10;&#10;&lt;p&gt;I am using a logistic regression classifier (LibLINEAR), and have little flexibility in this decision.&lt;/p&gt;&#10;&#10;&lt;p&gt;To maximize the classifier's F1 score, should I try to collect training data with the same class proportions as the population (while ensuring that there are enough instances of the minority class)? Or should I collect equal proportions of the classes, and use asymmetric misclassification penalties?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-11T14:50:40.907" Id="93456" LastActivityDate="2014-04-11T16:12:19.263" OwnerUserId="42848" PostTypeId="1" Score="2" Tags="&lt;classification&gt;&lt;unbalanced-classes&gt;" Title="Collecting training data for document classification with unbalanced classes" ViewCount="68" />
  <row AnswerCount="0" Body="&lt;p&gt;Running some analysis for 65 patients who have gone through a medical treatment post-diving.&#10;I have a dependent variable which is ordinal: 1-bad outcome, 2-okay outcome, 3-good outcome.&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to test some correlations with different independent variables:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Sex - nominal obviously. Should I use the Kruskal-Wallis test?&lt;/li&gt;&#10;&lt;li&gt;Dive depth - which is numeric in meters. Can I use univariate analysis?&lt;/li&gt;&#10;&lt;li&gt;Pre-treatment severity - ordinal variable as 0-no symptoms, 1-mild, 2-moderate, 3-severe. Kruskal-Wallis as well?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2014-04-11T15:40:54.687" Id="93466" LastActivityDate="2014-04-11T16:16:42.673" LastEditDate="2014-04-11T16:16:42.673" LastEditorUserId="22047" OwnerUserId="43628" PostTypeId="1" Score="2" Tags="&lt;ordinal&gt;" Title="How to test correlations between ordinal outcomes and various kinds of variables?" ViewCount="41" />
  
  <row AcceptedAnswerId="93656" AnswerCount="1" Body="&lt;p&gt;I am studying customer satisfaction in a large hierarchical organization. I plan to administer a voluntary survey to customers across the organization, and need to address non-response in my analysis.&lt;/p&gt;&#10;&#10;&lt;p&gt;I know:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;How many customers there are (population size)&lt;/li&gt;&#10;&lt;li&gt;Some demographic/market segment data for all customers&lt;/li&gt;&#10;&lt;li&gt;The products/services they used&lt;/li&gt;&#10;&lt;li&gt;The account manager and department that handles their account&lt;/li&gt;&#10;&lt;li&gt;50% response rate expected&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I want to draw conclusions about departments, account managers and products. It'd also be nice to be able to break things out by customer demographics, but that's secondary.&lt;/p&gt;&#10;&#10;&lt;p&gt;The naive approach would assume that non-response is random and treat the data as a simple random sample of the population. My concern is that non-response is probably not completely random.&lt;/p&gt;&#10;&#10;&lt;p&gt;So here are my questions:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Is the 50% response rate sufficiently high that the naive approach isn't terrible?&lt;/li&gt;&#10;&lt;li&gt;If not, is &lt;a href=&quot;http://en.wikipedia.org/wiki/Resampling_%28statistics%29&quot; rel=&quot;nofollow&quot;&gt;resampling&lt;/a&gt; or &lt;a href=&quot;https://en.wikipedia.org/wiki/Statistical_benchmarking&quot; rel=&quot;nofollow&quot;&gt;post-stratification&lt;/a&gt; a sufficient remedy?&lt;/li&gt;&#10;&lt;li&gt;If not, does it make sense to try to model the sampling probability mechanism using the known customer demographic information?&lt;/li&gt;&#10;&lt;li&gt;If none of these are sufficient, would I be better off designing a smaller stratified sample and aggressively pursuing non-responders?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Related:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/81847/does-surveymonkey-ignore-the-fact-that-you-get-a-non-random-sample&quot;&gt;Does SurveyMonkey ignore the fact that you get a non-random sample?&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/11018/recommend-references-on-survey-sample-weighting&quot;&gt;Recommend references on survey sample weighting&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/13456/ways-to-overcome-small-number-of-survey-responses&quot;&gt;Ways to overcome small number of survey responses&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/24882/are-benchmarking-and-post-stratification-equivalent&quot;&gt;Are benchmarking and post-stratification equivalent?&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;EDIT 4/22/2014:&#10;Reviewing @Steve Samuels response and additional independent research, I think what I'm dealing with is a census with non-response. The population (all current customers) is sufficiently small and well-known to serve as the sample frame. Thus by definition we have a census. It's actually less feasible to address a random sub-population than the whole population due to the platform we're using to deliver the questionnaire.&lt;/p&gt;&#10;&#10;&lt;p&gt;My plan moving forward is to execute the census, then study the difference between the responding &amp;amp; non-responding populations in a variety of ways. Moving forward, follow-up efforts will be adjusted to address specific problems found in the non-response analysis.&lt;/p&gt;&#10;&#10;&lt;p&gt;Some good resources for this problem include:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://books.google.com/books?id=mcupizbuXrkC&amp;amp;printsec=frontcover&amp;amp;dq=Modern+Analysis+of+Customer+Surveys%3a+with+applications+using+R&amp;amp;hl=en&amp;amp;sa=X&amp;amp;ei=zchWU_XWC6jKsASxjoHwCA&amp;amp;ved=0CD8Q6AEwAA&quot; rel=&quot;nofollow&quot;&gt;Analysis of Customer Surveys: with applications using R&lt;/a&gt; (Kenett and Salini, 2011) has a good chapter on sample design and continues with a solid overview of survey analysis techniques, all with a focus on customer satisfaction surveys.&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://www.stanford.edu/dept/communication/faculty/krosnick/docs/2007/2007%20TSMII%20chapter%20proof.pdf&quot; rel=&quot;nofollow&quot;&gt;The causes and consequences of response rates in surveys by the news media and government contractor survey research firms&lt;/a&gt; (Holbrook, Krosnick, and Pfent, 2007) does a meta-analysis of 114 phone surveys conducted by professional survey firms and &quot;challenges the assumptions that response rates are a key indicator of survey data quality and that efforts to increase response rates will necessarily be worth the expense&quot;&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://hum.sagepub.com/content/61/8/1139.short&quot; rel=&quot;nofollow&quot;&gt;Survey response rate levels and trends in organizational research&lt;/a&gt; (Bruch and Holtom, 2008) provides benchmarks for response rates in organizational surveys (50% is typical) and guidelines for responsibly reporting response statistics.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2014-04-11T17:08:02.610" FavoriteCount="1" Id="93480" LastActivityDate="2014-04-22T23:20:00.540" LastEditDate="2014-04-22T23:20:00.540" LastEditorUserId="22047" OwnerUserId="43630" PostTypeId="1" Score="1" Tags="&lt;sampling&gt;&lt;modeling&gt;&lt;bootstrap&gt;&lt;survey&gt;&lt;resampling&gt;" Title="Addressing Non-response in a Convenience Sample" ViewCount="102" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Let $\mu$ be the mean and $\sigma$ the standard deviation of a probability distribution defined on the bounded interval $[a,b]$ (that is, the probability that the random variable lies outside $[a,b]$ is zero).&lt;/p&gt;&#10;&#10;&lt;p&gt;Does the following inequality hold generally for any such probability distribution?&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\sigma^2 \le (\mu-a)(b-\mu)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Motivation:&lt;/strong&gt; This inequality holds for the Beta distribution (as is obvious from the formulae &lt;a href=&quot;http://stats.stackexchange.com/a/93479/5536&quot;&gt;here&lt;/a&gt;). &lt;/p&gt;&#10;" ClosedDate="2014-04-11T19:19:27.843" CommentCount="0" CreationDate="2014-04-11T17:25:19.937" FavoriteCount="0" Id="93486" LastActivityDate="2014-04-11T18:08:28.593" OwnerUserId="5536" PostTypeId="1" Score="2" Tags="&lt;variance&gt;&lt;standard-deviation&gt;&lt;probability-inequalities&gt;" Title="$\sigma^2 \le (\mu-a)(b-\mu)$ for all probability distributions bounded on $[a,b]$?" ViewCount="33" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;After I read chapter 7 of the new edition of Bayesian Data Analysis&lt;sup&gt;1&lt;/sup&gt;, I have come to understand that while Information Criteria like DIC and WAIC are: &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;A way measuring the adjust of the model and penalize by the effective number of parameters.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;&lt;br /&gt;They are also:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;A way to get the expected out-of-sample (pointwise) log predictive density by starting with the within-sample (pointwise) log predictive density and correct its bias (with the measure of effective number of parameters).&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;&lt;br /&gt;&#10;Although I does seem intuitive that the more parameters we have in the model, the more we tend to overestimate how well the model explains with-sample predictive accuracy, I still can't see why these two things are exactly the same thing, specially how the number of parameters gets us from within-sample to expected out-of-sample. (Or I am wrong, and if so, where/why)&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;sub&gt;[1] Gelman, A.; Carlin, J. B.; Stern, H. S.; Dunson, D. B.; Vehtari, A. &amp;amp; Rubin, D. B. (2014). Bayesian&#10;data analysis. CRC Press, third edition.&lt;/sub&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-11T18:15:10.547" Id="93494" LastActivityDate="2014-04-11T18:15:10.547" OwnerUserId="16612" PostTypeId="1" Score="0" Tags="&lt;model-comparison&gt;" Title="Information Criteria: penalization for number of parameters or for within sample?" ViewCount="27" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;Imagine I have many coins ($&amp;gt;100$), and for each coin I have the data: $(n_{\rm heads}, n_{\rm tails})$ where $\sum(n_{\rm heads}, n_{\rm tails})$ is decreasing for each next coin. Any one of the coins can be unfair ($P({\rm head})\ne 0.5$). Some coins might be an exact copy of any other coin, some might not be.  &lt;/p&gt;&#10;&#10;&lt;p&gt;How do I determine all coins are fair?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-04-11T21:10:05.963" Id="93503" LastActivityDate="2014-04-11T21:44:29.027" LastEditDate="2014-04-11T21:34:34.820" LastEditorUserId="7290" OwnerUserId="43644" PostTypeId="1" Score="3" Tags="&lt;hypothesis-testing&gt;&lt;binomial&gt;&lt;bernoulli-distribution&gt;" Title="Determine if many coins are fair" ViewCount="48" />
  
  <row Body="&lt;p&gt;Before using any test see if a log transform will make the variances more similar and if so then apply the test to the transformed values. Any conclusion that you might draw form log-transformed data will be equally applicable to the raw untransformed data. See my answeer to this question and the comments elicited for more thoughts: &lt;a href=&quot;http://stats.stackexchange.com/questions/25671/comparing-smoke-and-hormones-level-in-two-groups-of-people-which-test/25719#25719&quot;&gt;Comparing smoke and hormones level in two groups of people. Which test?&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-04-12T01:52:19.323" Id="93511" LastActivityDate="2014-04-12T01:52:19.323" OwnerUserId="1679" ParentId="93472" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;You could use the idea of &lt;code&gt;partial dependency plots&lt;/code&gt; which basically plot the change in the average predicted value (from a given model) as specific variable(s) vary over their marginal distribution. This means that you plot the average predicted value as the predictor(s) vary over their domain, averaging over the values of the other predictors with their values set as observed in the training data.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Here is a snippet from Elements of Statistical Learning&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/MlvFm.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;One could always use a brute force method to average the predicted value that results by &quot;re-scoring&quot; the same (training) data set repeatedly, each time only changing the value of the specific variable(s) of interest, leaving everything else as was observed. There are computational shortcuts in tree algorithms and &lt;code&gt;randomForest&lt;/code&gt; in R contains the function &lt;code&gt;partialPlot&lt;/code&gt; that makes this brute force method unnecessary.&lt;/p&gt;&#10;&#10;&lt;p&gt;In this example, the predicted probability for aspiration is estimated and the average value at the two levels of numOfDoors is obtained. In theory, you can do this for any predictor(s). Continuous variable effects are not linear in a random forest or decision tree generally, so you would have to create interesting discrete partitions of the variable to calculate the odds ratio. I notice how volatile the value is in this small data set, so you wont get the same result twice and wont match the value I have listed, but the process should be clear.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(randomForest)&#10;library(boot)&#10;library(car)&#10;&#10;data(imports85)&#10;table(na.omit(imports85)$aspiration)&#10;    table(na.omit(imports85)$numOfDoors)&#10;&#10;mod.rf&amp;lt;-randomForest(aspiration~numOfDoors+wheelBase+length+width+height, data=na.omit(imports85),ntree=1000,keep.forest=TRUE)&#10;&#10;#partial dependence plot for numOfDoors (two or four)&#10;#note: n.pt is useful for continuous variables&#10;pdp&amp;lt;-boot::inv.logit(partialPlot(mod.rf,na.omit(imports85),numOfDoors)$y)&#10;&#10;#odds ratio for aspiration = std&#10;&#10;#odds two cyclinder&#10;odds.2&amp;lt;- (pdp[2]/(1-pdp[2]))&#10;odds.4&amp;lt;- (pdp[1]/(1-pdp[1]))&#10;&#10;odds.2/odds.4  #1.74&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="8" CreationDate="2014-04-12T02:34:05.060" Id="93512" LastActivityDate="2014-04-12T02:34:05.060" OwnerUserId="2040" ParentId="93202" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;One reason is that the predictor doesn't have a very big impact on the binary response. What was the p-value of the predictor when output by the regression? If it was high, that would explain it.&lt;/p&gt;&#10;&#10;&lt;p&gt;Alternatively, if you didn't specify the link function properly whatever software you're using could be using an identity link function (which is not, strictly speaking, logistic regression in that case). &lt;/p&gt;&#10;&#10;&lt;p&gt;Otherwise we'll need more information to diagnose.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-12T04:21:23.600" Id="93520" LastActivityDate="2014-04-12T04:21:23.600" OwnerUserId="29087" ParentId="93516" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;What is the significant difference between the Fligner Wolfe test and 2-sample Wilcoxon test?&lt;br&gt;&#10;How can I compare the treatment groups with the control groups of different studies after pooling?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-12T05:08:33.323" Id="93522" LastActivityDate="2014-04-12T05:20:44.373" LastEditDate="2014-04-12T05:20:44.373" LastEditorUserId="32036" OwnerUserId="43654" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;&lt;statistical-significance&gt;&lt;mathematical-statistics&gt;" Title="Fligner Wolfe test and 2-sample Wilcoxon test" ViewCount="49" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Is it correct to say that the purpose of classifier (e.g. K-NN, Logistic Regression, LDA) is to approximate the Bayes Decision boundary?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-12T08:13:36.660" Id="93532" LastActivityDate="2014-04-12T11:36:01.323" OwnerUserId="40010" PostTypeId="1" Score="1" Tags="&lt;classification&gt;" Title="Bayes Decision Boundary and classifier" ViewCount="359" />
  <row AnswerCount="0" Body="&lt;p&gt;Given that I want to run Weka in a single PC, is it a good a idea to use boosting with a three layer feedforward neural network as a base learner? Why or why not?&lt;/p&gt;&#10;&#10;&lt;p&gt;I think boosting requires a lot of computational power as it trains several base learners and averages them (am I right?), but how can I know it will be efficient or not taking into account the base learner (again for a single PC)?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-12T14:53:50.693" Id="93546" LastActivityDate="2015-02-18T09:39:53.810" LastEditDate="2015-02-18T09:39:53.810" LastEditorUserId="53618" OwnerUserId="43637" PostTypeId="1" Score="0" Tags="&lt;neural-networks&gt;&lt;weka&gt;&lt;boosting&gt;" Title="Boosting in Weka with single PC" ViewCount="16" />
  <row AnswerCount="1" Body="&lt;p&gt;I have some problem to interpret the result of MLE estimation :&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/cDUpN.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Is it possible to get some advise about how to interpret it?&lt;/p&gt;&#10;&#10;&lt;p&gt;the log likelihood function : &#10;$\sum^{n}_{i=1}\log\left( \phi\left( \frac{w-\mu}{\sigma}\right)\right) -\log(\sigma P(1))&#10;+\log\left[ \frac{\left[ \Phi^{2}\left( \frac{w-\mu}{\sigma}\right)\right] }{2}-\delta \Phi\left( \frac{w-\mu}{\sigma}\right)+\left( \frac{a^{2}}{2} + b\right) \right]. $ &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-04-12T16:32:34.160" Id="93553" LastActivityDate="2014-07-09T00:39:35.767" LastEditDate="2014-07-09T00:39:35.767" LastEditorUserId="805" OwnerUserId="29386" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;regression&gt;&lt;probability&gt;&lt;statistical-significance&gt;&lt;maximum-likelihood&gt;" Title="Interpretation of Maximum likelihood estimation" ViewCount="131" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I have X possible predictors for response Y.  In my case X &gt;&gt; Y.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have noticed in my runs of cv.glmnet (leave-on-out and all other params default) that if I try to predict using lambda.min that it simply returns the mean value of Y.  If I run the prediction with choices of lambda &amp;lt; lambda.min, it gives actual predictions - which have a lower error than using the mean value of Y.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm not sure what's going on here.  It's as if the code is defaulting to a dummy predictor (the mean response) for some reason.  It appears that this behavior is a function of the size of X.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Here's a simple example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x=replicate(100,rnorm(10))&#10;&#10;y=replicate(1,rnorm(10))&#10;&#10;cvfit=cv.glmnet(x,y,nfolds=10)&#10;&#10;ypred1=predict(cvfit,newx=x,s=&quot;lambda.min&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;(in a case I just ran, this gives a &lt;code&gt;cvfit$lambda.min = 0.8453387&lt;/code&gt; and all entries in &#10;ypred1 are the mean value of y.  So, let's choose a different lambda)&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ypred2=predict(cvfit,newx=x,s=0.1)&#10;&#10;mse1=mean((ypred1-y)^2) = 1.20&#10;&#10;mse2=mean((ypred2-y)^2) = 0.03&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I understand that &quot;newx=x&quot; doesn't make sense for any real work, but I don't understand why it returns the predictions it does.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-04-12T17:59:03.113" Id="93561" LastActivityDate="2014-04-12T19:57:39.100" LastEditDate="2014-04-12T19:57:39.100" LastEditorUserId="8451" OwnerUserId="36475" PostTypeId="1" Score="0" Tags="&lt;glmnet&gt;&lt;overfitting&gt;" Title="Why is cv.glmnet giving a lambda.min that is clearly not the lambda for minimum error?" ViewCount="324" />
  <row AcceptedAnswerId="93587" AnswerCount="1" Body="&lt;p&gt;I'm analyzing reaction time data from a repeated measures ANOVA with the following design:&lt;br&gt;&#10;Factor 1 (between-group): &lt;code&gt;GROUP&lt;/code&gt; (controls, clinical)&lt;br&gt;&#10;Factor 2 (within-group): &lt;code&gt;TASK TYPE&lt;/code&gt; (social, non-social)&lt;br&gt;&#10;Factor 3 (within-group): &lt;code&gt;DECISION DIFFICULTY&lt;/code&gt; (easy, hard)&lt;/p&gt;&#10;&#10;&lt;p&gt;ANOVA results indicate a significant 2-way interaction (&lt;code&gt;GROUP&lt;/code&gt;&amp;times;&lt;code&gt;DIFFICULTY&lt;/code&gt;). No other main effects, 2-way interactions, nor the 3-way interaction are significant (all $p &amp;gt; .4)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Since there was no effect of task, I pooled data across the 2 task types. I'm not sure what is the right way to go from here in terms of reporting and interpreting the results. My specific questions are:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Should I always perform follow-up tests with a significant 2-way interaction, or should I just stop there and interpret based on the figures (and without further stats)?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Between- or within-group post-hoc: Should post-hoc tests be restricted to an independent &lt;em&gt;t&lt;/em&gt;-test between groups (i.e. comparing &quot;pooled easy&quot; data between groups, and &quot;pooled hard&quot; data between groups)? Can I also perform and report a within-group paired &lt;em&gt;t&lt;/em&gt;-test comparing &quot;pooled easy&quot; to &quot;pooled hard&quot; within each group?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Interpretation: If neither &lt;em&gt;independent t&lt;/em&gt;-test from (2) is significant, can I still interpret the 2-way interaction based on a significant &lt;em&gt;paired t&lt;/em&gt;-test from (2)? (The clinical group's reaction times appear to increase with task difficulty, whereas reaction times don't change with task difficulty in controls.)&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;In the absence of significant between-group &lt;em&gt;t&lt;/em&gt;-tests, can I still report these results based on the 2-way interaction and a within-group &lt;em&gt;t&lt;/em&gt;-test as &quot;task difficulty affects reaction times in the clinical group and not the control group&quot;? &#10;Thank you. I appreciate any help you can give!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-12T23:55:41.253" Id="93584" LastActivityDate="2014-04-13T01:10:05.203" LastEditDate="2014-04-13T00:59:31.957" LastEditorUserId="32036" OwnerUserId="41607" PostTypeId="1" Score="0" Tags="&lt;anova&gt;&lt;repeated-measures&gt;&lt;interaction&gt;&lt;interpretation&gt;&lt;post-hoc&gt;" Title="Interpreting a significant 2-way interaction with post-hoc tests" ViewCount="380" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I normally cross-validate my tree-models (rpart) only on cp-value, e.g. by using caret or xpred or the internal rpart cross validation function. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, there are other rpart parameters -- for example &lt;a href=&quot;http://stat.ethz.ch/R-manual/R-devel/library/rpart/html/rpart.control.html&quot; rel=&quot;nofollow&quot;&gt;minimum size and surrogate usage&lt;/a&gt;. Should one cross-validate over these parameters as well, or is this superfluous (and hence not included in the standard cross-validation packages?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-13T08:51:40.227" Id="93599" LastActivityDate="2014-04-13T12:48:34.070" LastEditDate="2014-04-13T12:48:34.070" LastEditorUserId="88" OwnerUserId="36680" PostTypeId="1" Score="0" Tags="&lt;cross-validation&gt;&lt;rpart&gt;&lt;classification-tree&gt;" Title="Rpart and tree models -- cross validation beyond cp value" ViewCount="81" />
  <row Body="&lt;p&gt;For people with a similar question, let me provide a simple outline of the answer.&lt;/p&gt;&#10;&#10;&lt;p&gt;The trick is to set up the two equations as a system of seemingly unrelated equations and to estimate them jointly. That is, we stack $y_1$ and $y_2$ on top of each other, and doing more or less the same with the design matrix. That is, the system to be estimated is: &lt;/p&gt;&#10;&#10;&lt;p&gt;$\left(\array{y_1 \\ y_2}\right) = \left(\array{X_1 \ \ 0 \\ 0 \ \ X_2}\right)\left(\array{\beta_1 \\ \beta_2 }\right) + \left(\array{e_1 \\ e_2 }\right) $ &lt;/p&gt;&#10;&#10;&lt;p&gt;This will lead to a variance-covariance matrix that allows to test for equality of the two coefficients. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-04-13T10:05:31.450" Id="93602" LastActivityDate="2014-04-13T10:05:31.450" OwnerUserId="31634" ParentId="93540" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;In R when you fit a regression or glm (though GLMs are themselves typically heteroskedastic), you can check the model's variance assumption by plotting the model fit. &lt;/p&gt;&#10;&#10;&lt;p&gt;That is, when you fit the model you normally put it into a variable from which you can then call &lt;code&gt;summary&lt;/code&gt; on it to get the usual regression table for the coefficients. If you &lt;code&gt;plot&lt;/code&gt; the same variable you get some diagnostic plots.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, consider:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;carmdl &amp;lt;- lm(dist~speed,cars)&#10;plot(carmdl)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The third of the default plots that it produces is the scale-location plot:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/sCT7O.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;[Other common choices for the y-axis in such a plot are the absolute residual and the log of the squared residual.]&lt;/p&gt;&#10;&#10;&lt;p&gt;That's a basic visual diagnostic of the spread of standardized (for model-variance) residuals against fitted values, which is suitable for seeing if there's variability related to the mean (not already accounted for by the model). If the assumption of homoskedasticity is true, we should see roughly constant spread. In this case the indication of increase with fitted values is fairly mild. &lt;/p&gt;&#10;&#10;&lt;p&gt;A common form of heteroskedasticity to look for would be where there's an increase in spread against fitted values. That would show as an increasing trend in the plot above. It can also be formally tested by the Breusch-Pagan test (though formal hypothesis tests of model assumptions aren't necessarily the best choice).&lt;/p&gt;&#10;&#10;&lt;p&gt;There are other forms of heteroskedasticity that are possible, but that's the most common one to check for. For example, if changing spread against a particular predictor was expected, that would suggest plotting the residual spread measure above against that predictor.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-13T15:34:46.113" Id="93618" LastActivityDate="2014-04-13T15:34:46.113" OwnerUserId="805" ParentId="93464" PostTypeId="2" Score="3" />
  
  
  
  
  
  <row Body="&lt;p&gt;I think 50% response is acceptable only if your supervisor really doesn't care what the numbers are. To the ordinary bounds of error you would need to add plausible bounds related to response bias. The extreme tabulation is one mandated for certain satisfaction surveys conducted by managed health care plans (related by a friend in that business): every non-respondent is assigned to a &quot;dissatisfied&quot; category.&lt;/p&gt;&#10;&#10;&lt;p&gt;So my first suggestion is to devote efforts to bring the response rate up. I would do a series of pilot experiments to see how much different combinations of approaches, incentives, and questions will improve the response rate. If this is a survey that will be repeated, the experiments could yield continual improvement over time. Continuing surveys throughout the year will also provide more timely data than a once-a-year survey. Because the sample size is smaller at any one time, quality will be higher.&lt;/p&gt;&#10;&#10;&lt;p&gt;You say you need to do a &quot;convenience&quot; sample, but you then allude to the possibility of a stratified sample; without random sampling, this is quota sampling. I urge you to experiment with a random sampling approach.. Don't try to aggressively follow-up &lt;em&gt;all&lt;/em&gt; non-responders; it would take too much effort (50%!). Select a random sub-sample; after a 1 in k sub-sample, you can multiply the sub-sample respondents by k.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;About the analytic remedies&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm not sure what you mean by re-sampling as a remedy for non-response. That topic is not mentioned on the page you link to.&lt;/p&gt;&#10;&#10;&lt;p&gt;I take it that by post-stratification, you mean any technique that re-weights the data so that sample estimates of multiple characteristics closely resemble known population quantities. Three standard techniques are survey raking via iterative proportional fitting (IPF), calibration, and generalized regression (GREG). See Little (2007) and Sarndal (2007).&#10;They will not touch that part of non-response &lt;strong&gt;related only to characteristics known only for the selected sample&lt;/strong&gt;. For those factors, if any, you can model the probability of response and then apply inverse probability weighting. &lt;/p&gt;&#10;&#10;&lt;p&gt;The major problem with post-stratification techniques is that good matching of sample totals to the population totals can result in more bias for subgroups whose definitions were not part of the post-stratification. So include managers, departments, and products, if possible, among the post-stratification variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Little, RJA. 2007. Should we use the survey weights to weight? JPSM Distinguished Lecture&#10;&lt;a href=&quot;http://www.jpsm.umd.edu/jpsm/archived/specialevents/little_lecture/weights407.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.jpsm.umd.edu/jpsm/archived/specialevents/little_lecture/weights407.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Sarndal, C.E. 2007. The calibration approach in survey theory and practice. Survey Methodology 33, no. 2: 99-119.&#10;&lt;a href=&quot;http://www.statcan.gc.ca/pub/12-001-x/2007002/article/10488-eng.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.statcan.gc.ca/pub/12-001-x/2007002/article/10488-eng.pdf&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-04-14T01:18:55.687" Id="93656" LastActivityDate="2014-04-22T22:22:56.603" LastEditDate="2014-04-22T22:22:56.603" LastEditorUserId="36068" OwnerUserId="36068" ParentId="93480" PostTypeId="2" Score="2" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I'm working with a Single-nucleotide polymorphism (SNP) dataset with over 2.2 million features and roughly 2000 samples. &lt;/p&gt;&#10;&#10;&lt;p&gt;I wish to do feature selection on this dataset to reduce it down to approximately ~20,000 features so that I can then use them to train different classifiers for a binary classification problem. The 2 classes are in the ratio of 3:1&lt;/p&gt;&#10;&#10;&lt;p&gt;What are the best feature selection methods in this scenario? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-14T03:25:36.190" Id="93661" LastActivityDate="2014-04-15T21:08:08.927" LastEditDate="2014-04-15T21:08:08.927" LastEditorUserId="43736" OwnerUserId="43736" PostTypeId="1" Score="0" Tags="&lt;feature-selection&gt;" Title="Feature selection with SNP datasets" ViewCount="71" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I actually asked the same question in &lt;a href=&quot;http://math.stackexchange.com/&quot;&gt;http://math.stackexchange.com/&lt;/a&gt; as well at &lt;a href=&quot;http://math.stackexchange.com/questions/753105/proving-that-markov-chain-monte-carlo-converges&quot;&gt;http://math.stackexchange.com/questions/753105/proving-that-markov-chain-monte-carlo-converges&lt;/a&gt; but since the question is very related to this site, too, I am asking it here once again:&lt;/p&gt;&#10;&#10;&lt;p&gt;I am trying to understand how the very basic Markov Chain Monte Carlo approach works: We try to approximately calculate the expected value $E_{\pi(x)}[X]$ by drawing sequential samples from a Markov Chain $(x_0,x_1,...)$ with the stationary distribution $\pi(x)$ and  transition matrix $T(x_i|x_{i-1})$. So, according to the MCMC approach, it should be $E_{\pi(x)}[X] \approx \frac{1}{N-N_0} \sum_{i=N_0}^{N} x_i$ where $N_0$ is the point where we assume that $p(x_{N_0})$ is close enough to the stationary distribution $\pi(x)$. We simulate the Markov Chain such that $x_0$ comes from an initial distribution $p(x_0)$ and each $x_i$ comes from $T(x_i|x_{i-1})$ .&lt;/p&gt;&#10;&#10;&lt;p&gt;I am trying to show that $\frac{1}{N-N_0} \sum_{i=N_0}^{N} x_i \rightarrow E_{\pi(x)}[X]$ as $N \rightarrow \infty$.&lt;/p&gt;&#10;&#10;&lt;p&gt;What I am doing currently is the following:&lt;/p&gt;&#10;&#10;&lt;p&gt;1-In order to simplify the problem I assume that $x_0$ comes already from the stationary distribution so we won't need a $N_0$ value. I have $S_N = \frac{1}{N} \sum_{i=0}^{N-1} x_i$ where $x_0$ comes from $\pi(x_0)$ and each $x_i$ comes from $T(x_i|x_{i-1})$ and $(x_0,x_1,...)$ is a Markov Chain.&lt;/p&gt;&#10;&#10;&lt;p&gt;2-The convergence of the regular Monte Carlo is shown with the Law of Large Numbers, so I try the same approach. By using the Weak Law of Large Numbers, I want to show that $P(|S_N - E_{\pi(x)}[X]| \geq \epsilon) \rightarrow 0$ as $ N \rightarrow \infty$ where $\epsilon &amp;gt; 0$.&lt;/p&gt;&#10;&#10;&lt;p&gt;3-Weak Law of Large Numbers can be proven by using Chebyshev's Inequality. For $S_N$, I write the inequality as $P(|S_N - E[S_N]| \geq \epsilon) \leq \frac{V[S_N]}{\epsilon^2}$ where $V[S_N]$ is the variance of $S_N$ and again $\epsilon &amp;gt; 0$. &lt;/p&gt;&#10;&#10;&lt;p&gt;4-I first want to show that the expected value of $S_N$, $E[S_N]$ is equal to the expected value we are after: $E_{\pi(x)}[X]$. I showed this by using the fact that the marginal of each $x_i$ in the Markov Chain is equal to the stationary distribution $\pi(x)$: $E[S_N]=E[\frac{1}{N} (x_0 + x_1 + ... + x_{N-1})] = \frac{1}{N} (E[x_0] + E[x_1] + ... + E[x_{N-1}]) = \frac{1}{N} (E_{\pi(x)}[x] + E_{\pi(x)}[x] + ... + E_{\pi(x)}[x]) = E_{\pi(x)}[x]$&lt;/p&gt;&#10;&#10;&lt;p&gt;5-Now I need to evaluate the variance of $S_N$, $V[S_N]$ in order to complete the Chebyshev's Inequality. But I failed to form a closed form expression for $V[S_N]$ as I did for $E[S_N]$ and I became stuck.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have actually two questions. First one: Is my way of proving the convergence of Markov Chain Monte Carlo is correct to begin with? Second one is, how can I carry on with the proof from the 5. step? It seems that the variance of the Monte Carlo sum doesn't have a closed form solution since $(x_0,x_1,...)$ are not i.i.d and come from a Markov Chain instead. Please note that I am a Computer Engineer, not exactly from a Mathematician background, so I could have done something very naive.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-04-14T10:00:58.177" Id="93683" LastActivityDate="2014-04-14T10:00:58.177" OwnerUserId="31611" PostTypeId="1" Score="1" Tags="&lt;self-study&gt;&lt;simulation&gt;&lt;mcmc&gt;&lt;monte-carlo&gt;&lt;markov-chain&gt;" Title="Proving that Markov Chain Monte Carlo converges" ViewCount="68" />
  
  <row AcceptedAnswerId="93716" AnswerCount="1" Body="&lt;p&gt;I am reading an internal paper that says: &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Let $\sigma^2 V_1$ equal the variance of $\sum_{m\in M}Z_m - Z_0$ and $\sigma^2V_2$ equal the covariance of $||M||^{-1}\sum_{m\in M}Z_m - Z_0$  and $Z_m - Z_0$. &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I am unfamiliar with the notation of the $V$s. Can anyone explain what this is referring to or point me in the right direction? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-14T13:18:25.670" Id="93699" LastActivityDate="2014-04-14T14:33:30.947" LastEditDate="2014-04-14T14:04:51.040" LastEditorUserId="43754" OwnerUserId="43754" PostTypeId="1" Score="1" Tags="&lt;variance&gt;&lt;covariance&gt;&lt;notation&gt;" Title="Variance and covariance notation: $\sigma^2 V_1$, $\sigma^2 V_2$" ViewCount="171" />
  <row Body="&lt;p&gt;I think that if you use several normality tests, then any rejection should be counted as a rejection. The tests which you mentioned all have null hypotheses of normal distribution.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-14T13:48:24.813" Id="93706" LastActivityDate="2014-04-14T13:48:24.813" OwnerUserId="36041" ParentId="93704" PostTypeId="2" Score="0" />
  
  <row AnswerCount="1" Body="&lt;p&gt;How many lags should one use a Ljung-Box test for the returns on 1320 daily price quotes? Is there a rule of thumb? What is exactly the impact in the end result of misuse of lag number?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-14T14:36:58.317" Id="93717" LastActivityDate="2014-04-14T14:44:57.007" OwnerUserId="43763" PostTypeId="1" Score="0" Tags="&lt;gretl&gt;" Title="How many lags in Q-statistic?" ViewCount="15" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I counted the number of geese on an intertidal mudflat on 100+ days over the winter. I made two counts on each of these days: one at low tide and one at high tide. I want to know if the number of geese present differs at high and low tide. As the data are very positively skewed, using a Wilcoxon test is appropriate. However, should I use rank sum test (unpaired test) or signed rank test (paired test)? &lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-04-14T16:05:59.680" Id="93734" LastActivityDate="2014-04-15T01:47:17.623" OwnerUserId="12492" PostTypeId="1" Score="1" Tags="&lt;wilcoxon&gt;&lt;paired-comparisons&gt;" Title="Paired or unpaired Wilcoxon test?" ViewCount="274" />
  <row Body="A discrete probability distribution corresponding to the sum of independent Bernoulli trials that are not necessarily identically distributed. " CommentCount="0" CreationDate="2014-04-14T16:14:29.343" Id="93739" LastActivityDate="2014-04-14T16:15:14.690" LastEditDate="2014-04-14T16:15:14.690" LastEditorUserId="27403" OwnerUserId="7290" PostTypeId="4" Score="0" />
  
  
  <row AcceptedAnswerId="93803" AnswerCount="1" Body="&lt;p&gt;I know that McElroy R^2 is a measure of goodness of fit for Seemingly Unrelated Regressions (SUR models), but how can one judge that the estimated equations are well fit by using the McElroy R^2?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-14T17:52:21.243" Id="93751" LastActivityDate="2014-04-15T02:15:45.257" OwnerUserId="6245" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;goodness-of-fit&gt;" Title="Goodness of fit of a SUR model" ViewCount="67" />
  
  <row AcceptedAnswerId="93786" AnswerCount="1" Body="&lt;p&gt;I'm trying to fit the data in this message (daily temperatures) using the HoltWinters technique in R, but can't get the seasonal example in &lt;a href=&quot;http://a-little-book-of-r-for-time-series.readthedocs.org/en/latest/src/timeseries.html&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; to work. Is this not possible with these data, or am I doing something wrong? The data are sampled every 10 minutes, so 144 points should correspond to one day.&lt;/p&gt;&#10;&#10;&lt;p&gt;My code is:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; time&amp;lt;-scan(&quot;timeseries.dat&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Read 2160 items:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; timets&amp;lt;-ts(time,frequency=144)&#10;&amp;gt; hwOut&amp;lt;-HoltWinters(timets)&#10;&#10;Warning message:&#10;In HoltWinters(timets) :&#10;  optimization difficulties: ERROR: ABNORMAL_TERMINATION_IN_LNSRCH&#10;enter code here&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The data are below. Thanks for any help you can offer.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;17.17&#10;16.49&#10;15.81&#10;15.47&#10;15.36&#10;15.29&#10;15.22&#10;15.15&#10;15.15&#10;15.15&#10;15.15&#10;15.65&#10;16.23&#10;16.59&#10;15.59&#10;15.05&#10;14.95&#10;14.95&#10;14.95&#10;14.95&#10;14.89&#10;14.83&#10;14.85&#10;15.62&#10;15.63&#10;14.95&#10;14.87&#10;14.84&#10;14.85&#10;15.39&#10;15.36&#10;14.74&#10;14.71&#10;14.82&#10;14.95&#10;14.95&#10;14.95&#10;14.95&#10;15.01&#10;15.08&#10;15.15&#10;15.3&#10;15.37&#10;15.36&#10;15.41&#10;15.58&#10;15.87&#10;15.87&#10;15.91&#10;16.08&#10;16.49&#10;16.85&#10;17.01&#10;16.62&#10;16.55&#10;16.7&#10;16.68&#10;16.76&#10;16.9&#10;16.97&#10;17.04&#10;17.11&#10;17.1&#10;17.2&#10;17.42&#10;17.49&#10;17.55&#10;17.62&#10;17.69&#10;17.76&#10;17.83&#10;17.9&#10;17.96&#10;18.03&#10;18.12&#10;18.14&#10;18.14&#10;18.14&#10;18.14&#10;18.14&#10;18.2&#10;18.23&#10;18.24&#10;18.45&#10;18.65&#10;18.86&#10;18.95&#10;18.96&#10;18.96&#10;19.1&#10;19.2&#10;19.27&#10;19.41&#10;19.48&#10;19.47&#10;19.62&#10;19.65&#10;19.58&#10;19.65&#10;19.72&#10;19.78&#10;19.78&#10;19.78&#10;19.78&#10;19.73&#10;19.62&#10;19.47&#10;19.49&#10;19.35&#10;19.06&#10;18.94&#10;18.7&#10;18.34&#10;18.22&#10;18.04&#10;17.83&#10;17.62&#10;17.42&#10;17.21&#10;17.07&#10;16.94&#10;16.8&#10;16.73&#10;16.63&#10;16.49&#10;16.65&#10;16.68&#10;16.59&#10;16.45&#10;16.38&#10;16.39&#10;16.33&#10;16.25&#10;16.18&#10;16.11&#10;16.05&#10;15.98&#10;15.84&#10;15.73&#10;15.67&#10;15.65&#10;15.76&#10;16.08&#10;16.61&#10;16.55&#10;15.46&#10;15.11&#10;15.57&#10;16.59&#10;16.7&#10;16.66&#10;16.59&#10;16.6&#10;16.27&#10;15.36&#10;15.45&#10;15.38&#10;15.15&#10;14.62&#10;14.39&#10;14.33&#10;14.19&#10;14.09&#10;14.02&#10;13.95&#10;13.89&#10;13.82&#10;14.67&#10;14.62&#10;13.72&#10;13.64&#10;13.57&#10;13.51&#10;13.51&#10;13.51&#10;13.51&#10;13.66&#10;13.73&#10;13.72&#10;13.78&#10;13.88&#10;14.02&#10;14.02&#10;14.06&#10;14.23&#10;14.6&#10;14.79&#10;14.85&#10;14.99&#10;15.02&#10;14.85&#10;14.99&#10;15.53&#10;16.39&#10;16.31&#10;16.39&#10;16.59&#10;16.49&#10;16.67&#10;17.11&#10;17.29&#10;17.39&#10;17.46&#10;17.53&#10;17.6&#10;17.67&#10;17.9&#10;18.04&#10;18.11&#10;18.25&#10;18.44&#10;18.66&#10;18.82&#10;18.92&#10;18.99&#10;19.13&#10;19.25&#10;19.31&#10;19.3&#10;19.35&#10;19.42&#10;19.49&#10;19.65&#10;19.97&#10;19.88&#10;19.93&#10;20.08&#10;20.15&#10;20.23&#10;20.3&#10;20.3&#10;20.35&#10;20.51&#10;20.35&#10;20.39&#10;20.62&#10;20.63&#10;20.59&#10;20.51&#10;20.67&#10;20.7&#10;20.62&#10;20.63&#10;20.54&#10;20.3&#10;20.39&#10;20.33&#10;20.19&#10;20.11&#10;20.11&#10;20.19&#10;19.96&#10;19.82&#10;19.75&#10;19.53&#10;19.33&#10;19.2&#10;19.33&#10;19.15&#10;18.77&#10;18.46&#10;18.23&#10;18.11&#10;18.04&#10;17.93&#10;17.78&#10;17.71&#10;17.64&#10;17.57&#10;17.73&#10;17.72&#10;17.57&#10;17.48&#10;17.48&#10;17.57&#10;17.49&#10;17.52&#10;17.67&#10;17.53&#10;17.42&#10;17.35&#10;17.36&#10;17.31&#10;17.24&#10;17.17&#10;17.07&#10;16.91&#10;17.75&#10;17.65&#10;16.69&#10;16.43&#10;16.33&#10;16.26&#10;16.19&#10;16.09&#10;15.93&#10;15.85&#10;15.8&#10;15.82&#10;16.65&#10;16.6&#10;15.71&#10;15.55&#10;15.45&#10;15.38&#10;15.39&#10;15.34&#10;15.27&#10;15.2&#10;15.12&#10;15.05&#10;16&#10;16.1&#10;15.71&#10;15.22&#10;15.01&#10;15.05&#10;14.98&#10;14.94&#10;15.05&#10;15.78&#10;15.91&#10;15.49&#10;15.56&#10;15.64&#10;15.71&#10;15.78&#10;15.9&#10;16.15&#10;16.19&#10;16.45&#10;16.91&#10;17.26&#10;17.39&#10;17.46&#10;17.53&#10;17.68&#10;18&#10;18.15&#10;18.29&#10;18.44&#10;18.8&#10;19.14&#10;19.42&#10;19.65&#10;19.82&#10;19.97&#10;20.19&#10;20.41&#10;20.62&#10;20.79&#10;20.82&#10;20.73&#10;20.88&#10;21.02&#10;21.17&#10;21.32&#10;21.43&#10;21.39&#10;21.2&#10;21.21&#10;21.28&#10;21.35&#10;21.41&#10;21.39&#10;21.23&#10;21.17&#10;21.17&#10;21.1&#10;21.02&#10;20.95&#10;20.95&#10;20.91&#10;20.84&#10;20.99&#10;21.1&#10;21.17&#10;21.24&#10;21.3&#10;21.28&#10;21.12&#10;21.05&#10;21.06&#10;20.97&#10;20.95&#10;20.95&#10;20.95&#10;20.93&#10;20.84&#10;20.77&#10;20.71&#10;20.73&#10;20.81&#10;20.85&#10;20.84&#10;20.92&#10;20.93&#10;20.84&#10;20.93&#10;20.85&#10;20.62&#10;20.62&#10;20.62&#10;20.62&#10;20.4&#10;20.36&#10;20.51&#10;20.22&#10;19.96&#10;19.75&#10;19.68&#10;19.54&#10;19.31&#10;19.41&#10;19.29&#10;18.99&#10;18.92&#10;18.84&#10;18.77&#10;18.76&#10;18.86&#10;19.09&#10;18.8&#10;18.58&#10;18.55&#10;18.7&#10;18.84&#10;18.99&#10;18.98&#10;19.05&#10;19.2&#10;19.31&#10;19.27&#10;19.2&#10;19.13&#10;18.77&#10;17.78&#10;17.52&#10;17.42&#10;17.35&#10;17.28&#10;17.21&#10;17.24&#10;17.8&#10;18.31&#10;18.44&#10;17.76&#10;17.31&#10;17.13&#10;17.13&#10;17.13&#10;17.13&#10;17.13&#10;17.11&#10;17.02&#10;18.01&#10;18.01&#10;17.13&#10;16.97&#10;16.86&#10;16.8&#10;16.8&#10;16.8&#10;16.8&#10;16.63&#10;16.72&#10;17.13&#10;17.21&#10;17.24&#10;17.24&#10;17.62&#10;17.82&#10;17.89&#10;18.04&#10;18.18&#10;18.33&#10;18.34&#10;18.39&#10;18.55&#10;18.37&#10;18.38&#10;18.55&#10;18.78&#10;18.95&#10;19.09&#10;19.24&#10;19.36&#10;19.42&#10;19.57&#10;19.68&#10;19.75&#10;19.98&#10;20.06&#10;19.97&#10;20.13&#10;20.23&#10;20.3&#10;20.37&#10;20.33&#10;20.08&#10;20.06&#10;20.12&#10;20.19&#10;20.19&#10;20.19&#10;20.19&#10;20.35&#10;20.4&#10;20.4&#10;20.4&#10;20.42&#10;20.51&#10;20.42&#10;20.49&#10;20.73&#10;20.74&#10;20.7&#10;20.62&#10;20.95&#10;21&#10;20.84&#10;20.83&#10;20.88&#10;20.95&#10;20.88&#10;20.8&#10;20.73&#10;20.9&#10;20.89&#10;20.73&#10;20.64&#10;20.63&#10;20.73&#10;20.98&#10;21.04&#10;20.95&#10;20.96&#10;20.92&#10;20.84&#10;20.85&#10;20.78&#10;20.62&#10;20.55&#10;20.42&#10;20.19&#10;20.21&#10;20.1&#10;19.86&#10;19.55&#10;19.32&#10;19.2&#10;19.14&#10;19.02&#10;18.88&#10;18.73&#10;18.6&#10;18.55&#10;18.63&#10;18.67&#10;18.66&#10;18.58&#10;18.55&#10;18.55&#10;18.48&#10;18.43&#10;18.44&#10;18.29&#10;18.19&#10;18.22&#10;18.37&#10;18.49&#10;18.55&#10;18.55&#10;18.55&#10;18.55&#10;18.4&#10;18.32&#10;18.33&#10;18.25&#10;18.22&#10;18.22&#10;18.14&#10;18.11&#10;18.11&#10;18.03&#10;18&#10;18&#10;17.93&#10;17.85&#10;17.78&#10;17.71&#10;17.68&#10;17.78&#10;17.79&#10;17.75&#10;17.67&#10;17.74&#10;17.79&#10;17.78&#10;17.62&#10;17.57&#10;17.57&#10;17.57&#10;17.59&#10;17.67&#10;17.6&#10;17.58&#10;17.67&#10;17.91&#10;18.04&#10;18.11&#10;18.18&#10;18.23&#10;18.22&#10;18.39&#10;18.44&#10;18.44&#10;18.44&#10;18.44&#10;18.44&#10;18.59&#10;18.63&#10;18.55&#10;18.78&#10;18.95&#10;19.09&#10;19.24&#10;19.35&#10;19.42&#10;19.64&#10;19.86&#10;20.08&#10;20.16&#10;20.25&#10;20.4&#10;20.63&#10;20.78&#10;20.84&#10;21.07&#10;21.23&#10;21.28&#10;21.2&#10;21.2&#10;21.28&#10;21.2&#10;21.2&#10;21.28&#10;21.2&#10;21.2&#10;21.28&#10;21.13&#10;21.04&#10;21.06&#10;21.21&#10;21.33&#10;21.39&#10;21.28&#10;21.37&#10;21.61&#10;21.94&#10;22.08&#10;22.15&#10;22.22&#10;22.32&#10;22.48&#10;22.53&#10;22.7&#10;22.92&#10;23.15&#10;23.31&#10;23.46&#10;23.61&#10;23.54&#10;22.81&#10;22.35&#10;22.26&#10;22.26&#10;22.26&#10;22.29&#10;22.37&#10;22.22&#10;22.11&#10;22.04&#10;21.89&#10;21.81&#10;21.82&#10;21.6&#10;21.5&#10;21.5&#10;21.28&#10;21.06&#10;20.84&#10;20.62&#10;20.48&#10;20.51&#10;20.28&#10;20.14&#10;20.08&#10;20.1&#10;19.96&#10;19.64&#10;19.48&#10;19.39&#10;19.31&#10;19.16&#10;19.02&#10;18.88&#10;18.94&#10;19&#10;18.99&#10;18.92&#10;18.84&#10;18.77&#10;18.7&#10;18.68&#10;18.77&#10;18.54&#10;18.39&#10;18.33&#10;18.34&#10;18.24&#10;18&#10;18.01&#10;17.93&#10;17.78&#10;17.87&#10;17.82&#10;17.67&#10;17.59&#10;17.57&#10;17.57&#10;17.49&#10;17.46&#10;17.46&#10;17.37&#10;17.35&#10;17.35&#10;17.35&#10;17.35&#10;17.35&#10;17.27&#10;17.24&#10;17.24&#10;17.24&#10;17.24&#10;17.24&#10;17.17&#10;17.13&#10;17.13&#10;17.06&#10;17.02&#10;17.02&#10;16.87&#10;16.76&#10;16.69&#10;16.62&#10;16.51&#10;16.36&#10;16.34&#10;16.4&#10;16.47&#10;16.47&#10;16.47&#10;16.47&#10;16.47&#10;16.47&#10;16.47&#10;16.54&#10;16.59&#10;16.58&#10;16.75&#10;16.8&#10;16.8&#10;16.8&#10;16.8&#10;16.8&#10;16.87&#10;16.9&#10;16.91&#10;17.06&#10;17.2&#10;17.35&#10;17.62&#10;17.93&#10;18.22&#10;18.3&#10;18.39&#10;18.55&#10;18.77&#10;18.99&#10;19.2&#10;19.42&#10;19.66&#10;19.97&#10;20.2&#10;20.34&#10;20.4&#10;20.7&#10;20.96&#10;21.17&#10;21.23&#10;21.38&#10;21.61&#10;21.92&#10;22.07&#10;22.04&#10;22.13&#10;22.15&#10;22.15&#10;22.15&#10;22.24&#10;22.48&#10;22.48&#10;22.48&#10;22.48&#10;22.48&#10;22.43&#10;22.26&#10;22.35&#10;22.3&#10;22.15&#10;22.15&#10;22.15&#10;22.15&#10;22.15&#10;22.15&#10;22.15&#10;22.1&#10;21.94&#10;21.72&#10;21.57&#10;21.43&#10;21.28&#10;21.22&#10;21.04&#10;20.73&#10;20.5&#10;20.31&#10;20.19&#10;20.21&#10;20.12&#10;19.97&#10;19.82&#10;19.68&#10;19.53&#10;19.46&#10;19.35&#10;19.2&#10;19.06&#10;18.89&#10;18.66&#10;18.67&#10;18.59&#10;18.44&#10;18.37&#10;18.29&#10;18.22&#10;18.15&#10;18.07&#10;18&#10;18&#10;17.94&#10;17.78&#10;17.9&#10;17.79&#10;17.57&#10;17.35&#10;17.18&#10;17.13&#10;16.98&#10;17.01&#10;17.35&#10;17.25&#10;16.93&#10;16.58&#10;16.51&#10;16.43&#10;16.36&#10;16.29&#10;16.24&#10;16.26&#10;16.74&#10;16.72&#10;16.15&#10;16.14&#10;16.17&#10;16.26&#10;16.61&#10;16.95&#10;17.13&#10;16.38&#10;15.92&#10;15.82&#10;15.92&#10;15.86&#10;15.71&#10;15.72&#10;15.67&#10;15.6&#10;15.6&#10;15.57&#10;15.49&#10;15.54&#10;15.71&#10;15.93&#10;15.93&#10;15.93&#10;15.93&#10;15.93&#10;15.97&#10;16.15&#10;16.31&#10;16.4&#10;16.47&#10;16.54&#10;16.57&#10;16.47&#10;16.53&#10;16.65&#10;16.8&#10;16.86&#10;16.98&#10;17.13&#10;17.28&#10;17.34&#10;17.24&#10;17.53&#10;17.76&#10;17.89&#10;17.89&#10;17.93&#10;18&#10;17.99&#10;17.97&#10;17.99&#10;18.17&#10;18.35&#10;18.53&#10;18.56&#10;18.74&#10;18.98&#10;19.25&#10;19.37&#10;19.34&#10;19.34&#10;19.39&#10;19.52&#10;19.46&#10;19.41&#10;19.43&#10;19.55&#10;19.67&#10;19.79&#10;19.97&#10;20.14&#10;20.24&#10;20.12&#10;20.03&#10;19.97&#10;19.83&#10;19.79&#10;19.79&#10;19.79&#10;19.71&#10;19.52&#10;19.58&#10;19.64&#10;19.7&#10;19.76&#10;19.84&#10;19.97&#10;20.09&#10;20.17&#10;20.06&#10;19.87&#10;19.75&#10;19.7&#10;19.5&#10;19.41&#10;19.43&#10;19.5&#10;19.45&#10;19.25&#10;19.18&#10;19.16&#10;19.16&#10;18.71&#10;18.48&#10;18.44&#10;18.45&#10;18.41&#10;18.35&#10;18.29&#10;18.23&#10;18.17&#10;18.23&#10;18.26&#10;18.26&#10;18.34&#10;18.32&#10;18.26&#10;18.2&#10;18.24&#10;18.53&#10;18.56&#10;18.47&#10;18.35&#10;18.22&#10;18.14&#10;18.08&#10;18.02&#10;17.97&#10;17.99&#10;18.47&#10;18.44&#10;17.9&#10;17.75&#10;17.72&#10;17.72&#10;17.73&#10;17.7&#10;17.63&#10;17.57&#10;17.51&#10;17.45&#10;17.93&#10;17.92&#10;17.45&#10;17.3&#10;17.33&#10;17.45&#10;17.38&#10;17.36&#10;17.36&#10;17.31&#10;17.28&#10;17.36&#10;17.7&#10;17.68&#10;17.28&#10;17.22&#10;17.16&#10;17.1&#10;17.04&#10;16.99&#10;17.01&#10;17.07&#10;17.07&#10;17.01&#10;17.07&#10;17.13&#10;17.19&#10;17.3&#10;17.39&#10;17.45&#10;17.58&#10;17.61&#10;17.54&#10;17.6&#10;17.66&#10;17.72&#10;17.78&#10;17.85&#10;17.99&#10;18.26&#10;18.34&#10;18.26&#10;18.25&#10;18.34&#10;18.53&#10;18.53&#10;18.53&#10;18.53&#10;18.65&#10;18.74&#10;18.8&#10;18.86&#10;18.95&#10;19.07&#10;19.07&#10;19.07&#10;19.07&#10;19.19&#10;19.28&#10;19.34&#10;19.4&#10;19.46&#10;19.52&#10;19.47&#10;19.42&#10;19.43&#10;19.55&#10;19.6&#10;19.52&#10;19.79&#10;19.83&#10;19.7&#10;19.68&#10;19.73&#10;19.79&#10;19.85&#10;19.86&#10;19.79&#10;19.85&#10;19.91&#10;19.97&#10;20.03&#10;20.08&#10;20.06&#10;19.9&#10;19.94&#10;20.06&#10;20.18&#10;20.2&#10;19.97&#10;19.95&#10;20.07&#10;20.24&#10;20.12&#10;20&#10;19.88&#10;19.88&#10;19.85&#10;19.79&#10;19.73&#10;19.63&#10;19.43&#10;19.31&#10;19.19&#10;19.07&#10;19.01&#10;18.95&#10;18.89&#10;18.83&#10;18.77&#10;18.71&#10;18.59&#10;18.48&#10;18.44&#10;18.5&#10;18.56&#10;18.62&#10;18.56&#10;18.55&#10;18.62&#10;18.53&#10;18.61&#10;18.8&#10;18.94&#10;18.98&#10;18.89&#10;18.7&#10;18.61&#10;18.62&#10;18.5&#10;18.38&#10;18.26&#10;18.88&#10;18.89&#10;18.44&#10;18.25&#10;18.1&#10;18.08&#10;18.72&#10;19.29&#10;19.61&#10;19.58&#10;19.01&#10;17.99&#10;18.42&#10;18.32&#10;17.9&#10;17.85&#10;17.75&#10;17.63&#10;17.63&#10;17.6&#10;17.54&#10;17.54&#10;17.54&#10;17.54&#10;17.87&#10;17.88&#10;17.54&#10;17.56&#10;17.48&#10;17.36&#10;17.29&#10;17.28&#10;17.28&#10;17.28&#10;17.28&#10;17.28&#10;17.39&#10;17.44&#10;17.36&#10;17.63&#10;17.7&#10;17.63&#10;17.69&#10;17.75&#10;17.81&#10;17.81&#10;17.83&#10;17.9&#10;18.18&#10;18.22&#10;18.08&#10;17.99&#10;18.02&#10;18.08&#10;18.14&#10;18.24&#10;18.44&#10;18.36&#10;18.41&#10;18.53&#10;18.65&#10;18.77&#10;18.89&#10;19.01&#10;19.1&#10;19.15&#10;19.28&#10;19.37&#10;19.43&#10;19.56&#10;19.65&#10;19.71&#10;19.83&#10;19.95&#10;20.08&#10;20.08&#10;20.1&#10;20.17&#10;20.29&#10;20.41&#10;20.54&#10;20.66&#10;20.83&#10;21.09&#10;21.11&#10;21.06&#10;21&#10;21.2&#10;21.2&#10;21&#10;21.06&#10;21.12&#10;21.19&#10;20.92&#10;20.84&#10;20.91&#10;20.92&#10;20.89&#10;20.82&#10;20.9&#10;20.85&#10;20.72&#10;20.66&#10;20.6&#10;20.54&#10;20.42&#10;20.29&#10;20.17&#10;20.04&#10;19.92&#10;19.8&#10;19.75&#10;19.65&#10;19.52&#10;19.4&#10;19.31&#10;19.25&#10;19&#10;18.79&#10;18.6&#10;18.41&#10;18.27&#10;18.23&#10;18.1&#10;18&#10;17.95&#10;17.95&#10;17.93&#10;17.86&#10;17.85&#10;17.94&#10;18.14&#10;17.89&#10;17.73&#10;17.67&#10;17.47&#10;17.39&#10;17.4&#10;17.34&#10;17.28&#10;17.21&#10;17.15&#10;17.13&#10;17.21&#10;17.23&#10;17.16&#10;17.03&#10;16.97&#10;16.91&#10;16.84&#10;16.78&#10;16.72&#10;16.66&#10;16.65&#10;16.69&#10;16.75&#10;16.69&#10;16.63&#10;16.57&#10;16.58&#10;16.54&#10;16.47&#10;16.41&#10;16.39&#10;16.47&#10;16.41&#10;16.37&#10;16.38&#10;16.46&#10;16.44&#10;16.38&#10;16.31&#10;16.28&#10;16.29&#10;16.29&#10;16.29&#10;16.29&#10;16.34&#10;16.41&#10;16.47&#10;16.47&#10;16.5&#10;16.57&#10;16.57&#10;16.59&#10;16.66&#10;16.86&#10;16.98&#10;17.03&#10;17.02&#10;17.08&#10;17.21&#10;17.2&#10;17.24&#10;17.31&#10;17.37&#10;17.4&#10;17.4&#10;17.52&#10;17.64&#10;17.77&#10;17.96&#10;18.03&#10;17.95&#10;18.01&#10;18.07&#10;18.14&#10;18.39&#10;18.58&#10;18.69&#10;18.77&#10;18.74&#10;18.6&#10;18.52&#10;18.53&#10;18.6&#10;18.6&#10;18.6&#10;18.6&#10;18.6&#10;18.68&#10;18.97&#10;18.95&#10;19.06&#10;19.25&#10;19.38&#10;19.43&#10;19.43&#10;19.79&#10;20.15&#10;20.35&#10;20.29&#10;20.23&#10;20.17&#10;20.15&#10;20.29&#10;20.63&#10;20.59&#10;20.46&#10;20.26&#10;20.85&#10;20.83&#10;20.45&#10;20.18&#10;20.01&#10;19.89&#10;19.77&#10;19.6&#10;19.34&#10;19.29&#10;19.17&#10;18.97&#10;18.65&#10;18.43&#10;18.32&#10;18.25&#10;18.25&#10;18.32&#10;17.96&#10;17.67&#10;17.67&#10;18.34&#10;18.61&#10;18.32&#10;17.64&#10;17.29&#10;17.12&#10;17&#10;16.89&#10;16.84&#10;16.99&#10;16.95&#10;16.75&#10;16.68&#10;16.66&#10;16.66&#10;16.61&#10;16.44&#10;16.1&#10;16.11&#10;16.01&#10;15.83&#10;15.85&#10;15.74&#10;15.55&#10;15.39&#10;15.36&#10;15.36&#10;15.36&#10;15.33&#10;15.27&#10;16.18&#10;16.93&#10;17.12&#10;16.74&#10;16.11&#10;15.46&#10;15.45&#10;15.4&#10;15.27&#10;15.42&#10;15.35&#10;15.09&#10;15.03&#10;14.97&#10;14.9&#10;14.84&#10;14.78&#10;14.72&#10;14.6&#10;14.5&#10;14.53&#10;15.23&#10;15.22&#10;14.53&#10;14.47&#10;14.41&#10;14.35&#10;14.35&#10;14.32&#10;14.26&#10;14.31&#10;14.41&#10;14.53&#10;14.4&#10;14.35&#10;14.35&#10;14.35&#10;14.35&#10;14.35&#10;14.7&#10;15.04&#10;15.27&#10;15.43&#10;15.46&#10;15.46&#10;15.46&#10;15.5&#10;15.64&#10;15.69&#10;15.82&#10;16.01&#10;16.07&#10;16.13&#10;16.2&#10;16.26&#10;16.36&#10;16.57&#10;16.67&#10;16.92&#10;17.31&#10;17.62&#10;17.92&#10;18.23&#10;18.11&#10;18.14&#10;18.41&#10;18.11&#10;18.18&#10;18.51&#10;18.56&#10;18.73&#10;18.97&#10;18.79&#10;18.61&#10;18.51&#10;18.94&#10;19.25&#10;19.43&#10;19.57&#10;19.63&#10;19.62&#10;19.72&#10;19.92&#10;20.17&#10;20.17&#10;20.23&#10;20.35&#10;20.28&#10;20.32&#10;20.45&#10;20.32&#10;20.21&#10;20.17&#10;20.23&#10;20.29&#10;20.35&#10;20.37&#10;20.25&#10;19.98&#10;19.98&#10;19.97&#10;19.89&#10;19.59&#10;19.28&#10;19.06&#10;19.11&#10;19.17&#10;19.15&#10;18.97&#10;18.78&#10;18.6&#10;18.53&#10;18.48&#10;18.41&#10;18.12&#10;17.82&#10;17.58&#10;17.4&#10;17.19&#10;16.94&#10;16.75&#10;16.58&#10;16.47&#10;16.48&#10;16.41&#10;16.29&#10;16.24&#10;16.14&#10;16.01&#10;15.89&#10;15.75&#10;15.55&#10;15.41&#10;15.35&#10;15.36&#10;15.24&#10;15.15&#10;15.09&#10;14.96&#10;14.84&#10;14.72&#10;14.51&#10;14.41&#10;14.35&#10;14.29&#10;14.2&#10;14.07&#10;14.01&#10;13.95&#10;13.89&#10;13.76&#10;13.69&#10;13.7&#10;13.64&#10;13.58&#10;13.52&#10;13.45&#10;13.41&#10;13.42&#10;13.3&#10;13.2&#10;13.15&#10;13.22&#10;13.19&#10;13.05&#10;13.13&#10;13.13&#10;13.05&#10;12.91&#10;12.87&#10;12.87&#10;12.87&#10;12.88&#10;12.96&#10;13.29&#10;13.46&#10;13.52&#10;13.64&#10;13.73&#10;13.79&#10;13.98&#10;14.11&#10;14.16&#10;14.41&#10;14.6&#10;14.72&#10;15.03&#10;15.29&#10;15.46&#10;15.44&#10;15.57&#10;15.83&#10;15.97&#10;16.04&#10;16.1&#10;16.16&#10;16.33&#10;16.75&#10;17.07&#10;17.31&#10;17.49&#10;17.75&#10;17.95&#10;18.14&#10;18.34&#10;18.55&#10;18.78&#10;18.91&#10;18.99&#10;19.04&#10;18.96&#10;19.02&#10;19.21&#10;19.32&#10;19.5&#10;19.8&#10;19.84&#10;20.04&#10;20.4&#10;20.6&#10;20.66&#10;20.65&#10;20.69&#10;20.82&#10;20.99&#10;21.11&#10;21.2&#10;21.25&#10;21.16&#10;21.23&#10;21.42&#10;21.7&#10;21.74&#10;21.67&#10;21.62&#10;21.54&#10;21.42&#10;21.49&#10;21.38&#10;21.08&#10;20.95&#10;20.88&#10;20.82&#10;20.77&#10;20.71&#10;20.65&#10;20.54&#10;20.45&#10;20.4&#10;20.29&#10;20.2&#10;20.14&#10;19.91&#10;19.74&#10;19.63&#10;19.47&#10;19.24&#10;18.95&#10;18.83&#10;18.75&#10;18.7&#10;18.33&#10;18.16&#10;18.1&#10;18.05&#10;17.93&#10;17.67&#10;17.99&#10;17.92&#10;17.5&#10;17.57&#10;17.59&#10;17.59&#10;17.73&#10;17.68&#10;17.42&#10;17.25&#10;17.31&#10;17.42&#10;17.53&#10;17.6&#10;17.5&#10;17.45&#10;17.41&#10;17.42&#10;17.37&#10;17.28&#10;17.16&#10;17.16&#10;17.11&#10;16.99&#10;17.07&#10;17.05&#10;16.99&#10;16.94&#10;16.9&#10;16.91&#10;16.98&#10;16.95&#10;16.82&#10;16.75&#10;16.74&#10;16.74&#10;16.74&#10;16.74&#10;16.74&#10;16.68&#10;16.64&#10;16.65&#10;16.71&#10;16.77&#10;16.82&#10;16.81&#10;16.85&#10;16.91&#10;16.96&#10;17.02&#10;17.08&#10;17.19&#10;17.28&#10;17.33&#10;17.44&#10;17.55&#10;17.59&#10;17.41&#10;17.32&#10;17.33&#10;17.39&#10;17.45&#10;17.5&#10;17.67&#10;17.83&#10;17.93&#10;17.74&#10;17.7&#10;17.76&#10;17.77&#10;17.73&#10;17.67&#10;17.74&#10;17.76&#10;17.76&#10;17.76&#10;17.82&#10;18.1&#10;18.33&#10;18.55&#10;18.78&#10;18.96&#10;19.13&#10;19.29&#10;19.4&#10;19.52&#10;19.63&#10;19.74&#10;19.84&#10;19.89&#10;19.88&#10;19.94&#10;20.06&#10;20.13&#10;20.14&#10;20.14&#10;20.14&#10;20.18&#10;20.31&#10;20.31&#10;20.34&#10;20.4&#10;20.4&#10;20.42&#10;20.48&#10;20.48&#10;20.5&#10;20.57&#10;20.38&#10;20.34&#10;20.4&#10;20.34&#10;20.28&#10;20.23&#10;20.27&#10;20.36&#10;20.48&#10;20.5&#10;20.41&#10;20.23&#10;20.23&#10;20.2&#10;20.14&#10;20.14&#10;20.1&#10;19.97&#10;20.05&#10;19.9&#10;19.46&#10;19.35&#10;19.21&#10;19.04&#10;18.98&#10;18.87&#10;18.7&#10;18.65&#10;18.53&#10;18.36&#10;18.24&#10;18.15&#10;18.1&#10;17.92&#10;17.84&#10;17.85&#10;17.91&#10;17.87&#10;17.67&#10;17.75&#10;17.68&#10;17.5&#10;17.46&#10;17.34&#10;17.16&#10;17.11&#10;17.02&#10;16.91&#10;16.85&#10;16.76&#10;16.65&#10;16.6&#10;16.54&#10;16.48&#10;17.25&#10;17.23&#10;16.48&#10;16.35&#10;16.28&#10;16.23&#10;16.23&#10;16.18&#10;16.06&#10;16.06&#10;16.06&#10;16.06&#10;16.19&#10;16.17&#10;15.97&#10;15.88&#10;16.25&#10;17.25&#10;17.32&#10;17.33&#10;17.33&#10;17.4&#10;17.1&#10;16.06&#10;15.73&#10;15.57&#10;15.46&#10;15.38&#10;15.4&#10;15.46&#10;15.52&#10;15.55&#10;15.55&#10;15.7&#10;15.91&#10;16.14&#10;16.14&#10;16.19&#10;16.31&#10;16.37&#10;16.46&#10;16.65&#10;16.94&#10;17.2&#10;17.42&#10;17.63&#10;17.65&#10;17.5&#10;17.32&#10;17.23&#10;17.25&#10;17.3&#10;17.38&#10;17.5&#10;17.56&#10;17.63&#10;17.76&#10;17.93&#10;18.1&#10;18.27&#10;18.5&#10;18.66&#10;18.7&#10;18.95&#10;19.01&#10;18.87&#10;18.99&#10;19.06&#10;19.12&#10;19.29&#10;19.46&#10;19.63&#10;19.8&#10;19.91&#10;19.89&#10;20.19&#10;20.37&#10;20.4&#10;20.34&#10;20.28&#10;20.23&#10;20.17&#10;20.18&#10;20.4&#10;20.52&#10;20.58&#10;20.57&#10;20.7&#10;20.73&#10;20.65&#10;20.51&#10;20.51&#10;20.57&#10;20.63&#10;20.62&#10;20.48&#10;20.42&#10;20.4&#10;20.4&#10;20.28&#10;20.25&#10;20.31&#10;20.26&#10;20.2&#10;20.14&#10;20.09&#10;20.03&#10;19.97&#10;19.87&#10;19.73&#10;19.55&#10;19.32&#10;19.1&#10;18.87&#10;18.81&#10;18.65&#10;18.36&#10;18.3&#10;18.22&#10;18.1&#10;17.86&#10;17.7&#10;17.59&#10;17.48&#10;17.34&#10;17.16&#10;17.17&#10;17.08&#10;16.91&#10;16.85&#10;16.79&#10;16.74&#10;16.57&#10;16.45&#10;16.4&#10;16.21&#10;16.1&#10;16.06&#10;16.06&#10;16.04&#10;15.97&#10;15.86&#10;15.74&#10;15.63&#10;15.57&#10;15.5&#10;15.38&#10;15.38&#10;15.33&#10;15.21&#10;15.34&#10;15.35&#10;15.29&#10;15.29&#10;15.27&#10;15.21&#10;15.21&#10;15.16&#10;15.04&#10;15.05&#10;15.01&#10;14.95&#10;14.89&#10;14.88&#10;14.95&#10;14.95&#10;14.95&#10;14.95&#10;14.95&#10;14.95&#10;14.95&#10;15.06&#10;15.16&#10;15.21&#10;15.21&#10;15.24&#10;15.38&#10;15.66&#10;15.91&#10;16.06&#10;16.11&#10;16.19&#10;16.31&#10;16.36&#10;16.49&#10;16.74&#10;17.06&#10;17.2&#10;17.25&#10;17.3&#10;17.29&#10;17.16&#10;17.33&#10;17.48&#10;17.59&#10;17.57&#10;17.71&#10;18.02&#10;18.2&#10;18.33&#10;18.44&#10;18.61&#10;18.78&#10;18.87&#10;18.87&#10;18.87&#10;18.87&#10;18.8&#10;18.84&#10;19.04&#10;19.08&#10;19.18&#10;19.29&#10;19.4&#10;19.48&#10;19.46&#10;19.58&#10;19.66&#10;19.72&#10;19.77&#10;19.81&#10;19.8&#10;19.86&#10;19.9&#10;19.89&#10;20.08&#10;20.12&#10;20.06&#10;20.05&#10;20.08&#10;20.14&#10;20.09&#10;20.05&#10;20.06&#10;20.17&#10;20.27&#10;20.31&#10;20.13&#10;20.08&#10;20.14&#10;20.03&#10;19.91&#10;19.8&#10;19.91&#10;19.97&#10;19.97&#10;19.97&#10;19.99&#10;20.06&#10;20.11&#10;20.16&#10;20.14&#10;20.04&#10;19.9&#10;19.72&#10;19.37&#10;19.14&#10;19.04&#10;18.67&#10;18.47&#10;18.44&#10;18.51&#10;18.53&#10;18.53&#10;18.53&#10;18.41&#10;18.02&#10;17.95&#10;17.92&#10;17.93&#10;18.05&#10;18.11&#10;18.1&#10;18.04&#10;18.13&#10;18.44&#10;18.19&#10;18.09&#10;18.1&#10;18.17&#10;18.12&#10;17.93&#10;17.99&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2014-04-14T19:32:06.320" Id="93765" LastActivityDate="2014-11-24T16:25:50.237" LastEditDate="2014-06-18T20:03:29.403" LastEditorUserId="32036" OwnerUserId="31833" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;time-series&gt;" Title="Trying to use Holt-Winters to fit this data" ViewCount="432" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;There are four different types of mobile phones (same brand) that are used as stimuli in an experiment.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;In the first example, if &quot;mobile phone&quot; is the independent variable&#10;in the study, which one  of the two is appropriate? (i) mobile phone&#10;is the independent variable with four levels or (ii) this study has 4&#10;independent variables (since there are 4 different mobile phones)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;The same webpage is modified into two versions: one with a blue background, and the other version with an orange background. &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;In the second example, if webpage is the independent variable, can we&#10;say that the study has one independent variable with 2 levels or does it&#10;have two independent variables?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Kindly clarify.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-04-15T01:06:20.123" Id="93797" LastActivityDate="2014-04-15T04:26:48.220" LastEditDate="2014-04-15T01:51:19.207" LastEditorUserId="39531" OwnerUserId="39531" PostTypeId="1" Score="0" Tags="&lt;anova&gt;" Title="Levels or Independent Variables in ANOVA" ViewCount="46" />
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to use lars (matlab implementation:&lt;a href=&quot;http://www.ece.ubc.ca/~xiaohuic/code/LARS/LARS.htm&quot; rel=&quot;nofollow&quot;&gt;http://www.ece.ubc.ca/~xiaohuic/code/LARS/LARS.htm&lt;/a&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to do a leave one out cross validation on my data using this code. I have the following doubts:&#10;This code has a parameter t, is this the parameter which has to be tuned during cross validation? If it is not given, the default is taken to be infinity. &#10;And the coefficients (weight matrix) is calculated to be of arbitrary dimension. &lt;/p&gt;&#10;&#10;&lt;p&gt;Can somebody help me understand whats going on in this code? I would like to perform a leave one out cross validation on my data using lars.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I am also trying to use lasso function in-built with MATLAB. It has an option to perform cross validation. It outputs B and Fit Statistics, where B is Fitted coefficients, a p-by-L matrix, where p is the number of predictors (columns) in X, and L is the number of Lambda values.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now given a new test sample, how can I calculate the output using this model?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-15T01:45:00.193" Id="93800" LastActivityDate="2014-04-15T09:56:42.237" LastEditDate="2014-04-15T09:56:42.237" LastEditorUserId="43608" OwnerUserId="43608" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;matlab&gt;&lt;lasso&gt;&lt;lars&gt;" Title="Parameter tuning in lars (lasso) matlab" ViewCount="170" />
  <row Body="&lt;p&gt;See Blie's &lt;a href=&quot;http://machinelearning.wustl.edu/mlpapers/paper_files/BleiNJ03.pdf&quot; rel=&quot;nofollow&quot;&gt;original LDA paper&lt;/a&gt; for how to set something like that up. He uses the topic assignments from LDA as the new features per data point. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-04-15T03:08:13.347" Id="93805" LastActivityDate="2014-04-15T03:08:13.347" OwnerUserId="34874" ParentId="93785" PostTypeId="2" Score="0" />
  
  
  
  
  <row Body="&lt;p&gt;This isn't really a software issue, as it hinges on a misapplication or misunderstanding of chi-square tests. &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Your manual calculation in the second case takes no account of the fact that &lt;code&gt;o2&lt;/code&gt; and &lt;code&gt;e2&lt;/code&gt; have different totals, &lt;code&gt;e2&lt;/code&gt; 50590, &lt;code&gt;o2&lt;/code&gt; 50891. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;R's result is what I get independently in different software if I scale the expected &lt;code&gt;e2&lt;/code&gt; to sum to the sum of the observed &lt;code&gt;o2&lt;/code&gt;. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;It is surprising in any case that the expected frequencies come as integers.  This is unusual in practice. My guess is that this is a two-way problem being presented wrongly as a one-way problem. If so, a chi-square test yields 1.6304 with 11 d.f. and $P =$ 0.999 and the result rings alarm bells as almost too good to be true! &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="8" CreationDate="2014-04-15T12:15:48.473" Id="93847" LastActivityDate="2014-04-15T14:47:11.330" LastEditDate="2014-04-15T14:47:11.330" LastEditorUserId="22047" OwnerUserId="22047" ParentId="93843" PostTypeId="2" Score="5" />
  <row AnswerCount="1" Body="&lt;p&gt;There is many variants of type of solver in &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot; rel=&quot;nofollow&quot;&gt;liblinear&lt;/a&gt; but I don't understand their differences.Which one I must choose?&lt;/p&gt;&#10;&#10;&lt;p&gt;Also why data must be scaled? duo to some numerical issues?&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;&lt;pre&gt;&lt;code&gt;-s type : set type of solver (default 1)&#10;  for multi-class classification&#10;   0 -- L2-regularized logistic regression (primal)&#10;   1 -- L2-regularized L2-loss support vector classification (dual)&#10;   2 -- L2-regularized L2-loss support vector classification (primal)&#10;   3 -- L2-regularized L1-loss support vector classification (dual)&#10;   4 -- support vector classification by Crammer and Singer&#10;   5 -- L1-regularized L2-loss support vector classification&#10;   6 -- L1-regularized logistic regression&#10;   7 -- L2-regularized logistic regression (dual)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="1" CreationDate="2014-04-15T12:39:12.363" Id="93851" LastActivityDate="2015-03-02T22:37:31.457" OwnerUserId="16843" PostTypeId="1" Score="0" Tags="&lt;svm&gt;&lt;linear-model&gt;&lt;regularization&gt;&lt;libsvm&gt;&lt;loss-functions&gt;" Title="Liblinear types of solver" ViewCount="125" />
  <row Body="&lt;p&gt;The birth of each child is an &lt;a href=&quot;https://en.wikipedia.org/wiki/Independence_%28probability_theory%29&quot;&gt;&lt;strong&gt;independent&lt;/strong&gt; event&lt;/a&gt; with P=0.5 for a boy and P=0.5 for a girl. The other details (such as the family decisions) only distract you from this fact. The answer, then, is that &lt;strong&gt;the ratio is 1:1&lt;/strong&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;To expound on this: imagine that instead of having children, you're flipping a fair coin (P(heads)=0.5) until you get a &quot;heads&quot;. Let's say Family A flips the coin and gets the sequence of [tails, tails, heads]. Then Family B flips the coin and gets a tails. Now, what's the probability that the next will be heads? &lt;strong&gt;Still 0.5&lt;/strong&gt;, because that's what &lt;em&gt;independent&lt;/em&gt; means. If you were to do this with 1000 families (which means 1000 heads came up), the expected total number of tails is 1000, because each flip (event) was completely independent.&lt;/p&gt;&#10;&#10;&lt;p&gt;Some things are &lt;strong&gt;not&lt;/strong&gt; independent, such as the sequence within a family: the probability of the sequence [heads, heads] is 0, not equal to [tails, tails] (0.25). But since the question isn't asking about this, it's irrelevant.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-04-15T13:56:08.183" Id="93863" LastActivityDate="2014-04-15T19:34:35.467" LastEditDate="2014-04-15T19:34:35.467" LastEditorUserId="41878" OwnerUserId="41878" ParentId="93830" PostTypeId="2" Score="11" />
  <row AnswerCount="0" Body="&lt;p&gt;Let $\mathbf{y_1} =\begin{bmatrix}g_1x_1 &amp;amp; g_2x_1 &amp;amp; \dots &amp;amp; g_Nx_1 \end{bmatrix}$ and $\mathbf{y_2} = \begin{bmatrix} f_1x_2 &amp;amp; f_2x_2 &amp;amp; \dots &amp;amp; f_Nx_2\end{bmatrix}$. All the elements of $\mathbf{g}=\begin{bmatrix} g_1&amp;amp;g_2 &amp;amp;\dots &amp;amp;g_N\end{bmatrix}$ and $\mathbf{f}= \begin{bmatrix} f_1 &amp;amp; f_2 &amp;amp; \dots f_N\end{bmatrix}$ are drawn from a rayleigh distribution. $x_1$ and $x_2$ are taken randomly from the set $\{-1,1\}$ with equal probability, i.e, $x_1$ can be $1$ or $-1$ with $0.5$ probability. There are four combinations for $(x_1, x_2)$: $(1,1),(1,-1),(-1,1),(-1,-1)$ which will lead to four combinations for $(\mathbf{y_1}, \mathbf{y_2})$ as well. Considering all combinations, how can I find the average $\it{minimum}$ euclidean distance between the vectors $\mathbf{y_1}$ and $\mathbf{y_2}$?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-15T14:00:15.960" Id="93866" LastActivityDate="2014-04-15T14:00:15.960" OwnerUserId="43826" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;distance-functions&gt;&lt;distance&gt;" Title="Average minimum distance between two random vectors" ViewCount="55" />
  <row Body="&lt;p&gt;&lt;em&gt;My comments are so long, I am putting them in as an answer.&lt;/em&gt;  &lt;/p&gt;&#10;&#10;&lt;p&gt;I think the question is more philosophical than mathematical at this point.  Namely,  what do you mean by a space, and in this case, a manifold?  The typical definition of a manifold does not involve an embedding into an affine space.  This is the 'modern' (150 year old?) approach.  For example, to Gauss, a manifold was a manifold with a &lt;em&gt;specific&lt;/em&gt; embedding into a specific affine space ($R^n$).  If one has a manifold with an embedding in a specific $R^n$, then the tangent space (at any point of the manifold) is isomorphic to a specific subspace of the tangent space to $R^n$ at that point.  Note that the tangent space to $R^n$ at any point is identified with the 'same' $R^n$.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I think the point is that in the Amari article, the space he refers to as $S^n$ comes with some 'natural' embedding in an affine space with coordinates the $\theta_{i}$ for which  the $p_{\theta}$ can be considered as coordinates on the tangent space of $S^n$.  I might add that it is only clear if the function $p$ is 'general' in some sense- for degenerate $p$, this will fail.  For example if the function didn't involve all the variables $\theta_{i}$ .  The main point is that this embedding of the manifold in a specific $R^n$, gives rise to a specific identification of the tangent space with the $p_{\theta}$.  His next point is that because of the properties of $p$,  he can map his manifold using the log function to another affine space in which the tangent space has a different identification in terms of the new coordinates (the logs and their derivatives).  He then says that because of properties of his situation, the two manifolds are isomorphic and the map induces an isomorphism on the tangent spaces.  That leads to an identification (i.e., isomorphism) of the two tangent spaces.  &lt;/p&gt;&#10;&#10;&lt;p&gt;The key idea is that the two tangent spaces are not the same sets, but are isomorphic (which is basically Greek for 'same') after the correct identification.  For example, is the group of all permutations of $\{1,2,3\}$ the 'same' group as the group of all permutations of $\{a,b,c\}$?  As a simple thought experiment, consider $R^{+}$, the positive reals mapping to $R$, all the reals under the map log.  Pick your favorite real number $&amp;gt;0$ and consider what the map is on tangent spaces.  Am I finally understanding your question?  A caveat is in order, namely that differential geometry is not my main area of expertise.  I think I've got it right, but feel free to criticize or still question this answer.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-04-15T15:11:33.987" Id="93876" LastActivityDate="2014-04-15T15:39:48.967" LastEditDate="2014-04-15T15:39:48.967" LastEditorUserId="7290" OwnerUserId="39770" ParentId="92831" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;You'll have to either &lt;strong&gt;(a) increase the number of permutations&lt;/strong&gt; so the p-values won't have a lower bound as high as 0.01 (even with a single measure I'd use at least 1000 permutations!) or &lt;strong&gt;(b) incorporate multiple testing correction within the permutation testing&lt;/strong&gt;. This can be achieved by the &lt;strong&gt;Max-T&lt;/strong&gt; method: in each permutation, record the maximal statistic across all the 300 measures. You'll end up with a single empirical sampling distribution that describes the maximal statistic across all measures, given the null hypothesis. Then, for each measure in the unpermuted set, determine its p-value according to this distribution. The resulting 300 p-values will be already corrected for family-wise error.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-04-15T15:40:25.250" Id="93880" LastActivityDate="2014-04-15T15:40:25.250" OwnerUserId="20587" ParentId="93827" PostTypeId="2" Score="1" />
  
  
  
  <row AnswerCount="2" Body="&lt;p&gt;The Cox model does not depend on the times itself, instead it only needs an ordering of the events. How come it doesn't need the time, as all of the models I've seen so far are dependent on the exact time point and/or interval?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-04-15T17:33:08.057" FavoriteCount="1" Id="93900" LastActivityDate="2014-04-17T04:39:38.403" LastEditDate="2014-04-16T01:04:48.507" LastEditorUserId="43840" OwnerUserId="43840" PostTypeId="1" Score="4" Tags="&lt;survival&gt;&lt;cox-model&gt;" Title="How exactly can the Cox-model ignore exact times?" ViewCount="78" />
  <row Body="&lt;p&gt;Let the random variable denoting the $i^{th}$ child in the country be $X_i$ taking on values 1 and 0 if the child is a boy or girl respectively. Assume that the marginal probability that each birth is a boy or girl is $0.5$. &lt;/p&gt;&#10;&#10;&lt;p&gt;The expected number of boys in the country = $E[\sum_i X_i] = \sum_i E[X_i] = 0.5 n$ (where $n$ is the number of children in the country.) &lt;/p&gt;&#10;&#10;&lt;p&gt;Similarly the expected number of girls = $E[\sum_i (1- X_i)] = \sum_i E[1-X_i] = 0.5 n$. &lt;/p&gt;&#10;&#10;&lt;p&gt;The independence of the births is irrelevant for the calculation of expected values.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Apropos @whuber's answer, if there is a variation of the marginal probability across families, the ratio becomes skewed towards boys, due to there being more children in families with higher probability of boys than families with a lower probability, thereby having an augmentative effect of the expected value sum for the boys.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-15T17:43:07.603" Id="93904" LastActivityDate="2014-04-17T15:42:05.153" LastEditDate="2014-04-17T15:42:05.153" LastEditorUserId="7690" OwnerUserId="7690" ParentId="93830" PostTypeId="2" Score="0" />
  
  <row AnswerCount="0" Body="&lt;p&gt;A question about equality&lt;/p&gt;&#10;&#10;&lt;p&gt;$$F_{r_{1},r_{2},\alpha}=\frac{1}{F_{r_{2},r_{1},1-\alpha}}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;We know if $X\sim\chi^{2}\left(  r_{1}\right)  ,~Y\sim\chi^{2}\left(&#10;r_{2}\right)  $ and $X,~Y$ independent then&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac{X/r_{1}}{Y/r_{2}}=F\implies\frac{Y/r_{2}}{X/r_{1}}=\frac{1}{F}$$&#10;distributed. But how can we get $1-\alpha$ instead of $\alpha$. Do i have to get&#10;this result using probability density function and transformations or is there&#10;another sensible explanation?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-04-15T19:20:39.750" Id="93918" LastActivityDate="2014-04-15T19:20:39.750" OwnerUserId="43827" PostTypeId="1" Score="0" Tags="&lt;distributions&gt;&lt;statistical-significance&gt;" Title="1/F distribution equality significance level" ViewCount="16" />
  <row AnswerCount="0" Body="&lt;p&gt;Is it true that one should generally scale each of the features before feeding them into common classification models such as Support Vector Machine, Logistic Regression, etc?&lt;/p&gt;&#10;&#10;&lt;p&gt;What about for regression models?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-15T19:27:56.473" Id="93921" LastActivityDate="2014-04-15T20:01:10.467" LastEditDate="2014-04-15T20:01:10.467" LastEditorUserId="88" OwnerUserId="43046" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;classification&gt;" Title="Feature scaling for classification and regression" ViewCount="43" />
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;I sense this is not a good idea&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I'd agree that it's not a good idea, if it can be avoided, but sometimes you only have one and that's all you can have.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;but am struggling to think of an reason why it is wrong&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;'not a good idea' is not the same as 'wrong'. You're left with low power and an uncheckable assumption, both of which should be avoided if you can reasonably do so. But if the usual assumptions hold, it's all still valid.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;other than that the assumption of homogeneity of variances would surely be violated. &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;You have no obvious basis to form an estimate of the population variance in the singleton group aside from the usual assumption of homogeneity of variance, but that doesn't mean it has been violated. On what basis do you assert that since n=1, homogeneity of variance is &lt;em&gt;surely&lt;/em&gt; violated (aside from some basis that would apply to say the n=2 or n=5 case)?&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Does it (&lt;em&gt;the appropriateness of doing a one-way ANOVA under the conditions described&lt;/em&gt;) depend on how many participants are in the other levels of the factor, or on how many other levels there are?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;No, it's still appropriate if the assumptions are satisfied, whether the other sample sizes are small or large, and whether there are only one other group or many. If you do it correctly, significance levels will still be as they should be. Power - of course - will be affected by changes in sample size.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-04-15T20:50:53.327" Id="93929" LastActivityDate="2014-04-16T05:37:47.830" LastEditDate="2014-04-16T05:37:47.830" LastEditorUserId="805" OwnerUserId="805" ParentId="93857" PostTypeId="2" Score="5" />
  
  <row Body="&lt;p&gt;Let $W_i$ be the event that Link $i$ is working.  Then, there is a path&#10;of working links from $A$ to $B$ if the event $(W_1W_2 \cup W_3W_4)W_5 = W_1W_2W_5 \cup W_3W_4W_5$ occurs.  This has probability&#10;\begin{align}&#10;P(W_1W_2W_5 \cup W_3W_4W_5) &amp;amp;= P(W_1W_2W_5) + P(W_3W_4W_5) - P(W_1W_2W_5 \cap W_3W_4W_5)\\&#10;&amp;amp;= P(W_1W_2W_3) + P(W_3W_4W_5) - P(W_1W_2W_3W_4W_5)\\&#10;&amp;amp;= (1-p)^3+(1-p)^3 + (1-p)^5.&#10;\end{align}&#10;The event that exactly one link has failed is the union of $5$ mutually&#10;exclusive events of probability $p(1-p)^4$ each. Exactly one of these&#10;$5$ events (when Link 5 has failed) results in there being no path of &#10;working links from&#10;$A$ to $B$.  So the desired &lt;em&gt;conditional&lt;/em&gt; probability is $\frac 45$.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-16T02:46:22.400" Id="93958" LastActivityDate="2014-04-16T02:46:22.400" OwnerUserId="6633" ParentId="86433" PostTypeId="2" Score="1" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I am exploring a tree structured as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;A/ parent node = parent population, size N, with a proportion of success for an experiment (eg. p = nb success / N)&lt;/p&gt;&#10;&#10;&lt;p&gt;B/ child nodes = sub populations for a give parent node. Ex.: level one child sizes are n1, n2, ... nn, where n1 + n2 + ... + nn = N. Success proportions are also computed (p1 = nb1 success / n1, ..., pn = nbn success / nn)&lt;/p&gt;&#10;&#10;&lt;p&gt;This could be summarized by the enclosed picture.&#10;&lt;img src=&quot;http://i.stack.imgur.com/6DJXT.png&quot; alt=&quot;The problem&quot;&gt;&#10;What I am trying to do is cut the nodes where the proportions of success are not statistically significant. In other words, how can I trust the proportion of success for a given node, according to its population size?&lt;/p&gt;&#10;&#10;&lt;p&gt;The context of this question is pretty specific: p, p1, ... are always very low (between 0 and 0.0002, and node size can vary a lot, from 10 to several thousands of observations.&lt;/p&gt;&#10;&#10;&lt;p&gt;So how can I compute the minimum node size, so that I can consider a node proportion of success as statistically significant? Moreover, do I have to do a computation for each of the tree node, do I have to take the dependencies between the nodes into account, or is a global approximation for all the nodes acceptable?&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm working with R. My first insight was use the prop.test function for all node and check the p-value, but I realize this completely wrong.&lt;/p&gt;&#10;&#10;&lt;p&gt;As anyone an idea to deal with this tricky problem?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-04-16T06:53:13.950" FavoriteCount="1" Id="93969" LastActivityDate="2015-01-13T14:02:17.777" LastEditDate="2014-04-16T12:54:16.460" LastEditorUserId="615" OwnerUserId="41072" PostTypeId="1" Score="2" Tags="&lt;statistical-significance&gt;&lt;proportion&gt;" Title="Tree structured proportion of success : statistical significance?" ViewCount="101" />
  
  
  <row Body="&lt;p&gt;Yes, using OLS for cardinal/count data is suboptimal. See &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.214.8814&amp;amp;rep=rep1&amp;amp;type=pdf&quot; rel=&quot;nofollow&quot;&gt;King &lt;sup&gt;(1988)&lt;/sup&gt;&lt;/a&gt; for a comparison of OLS regression to exponential Poisson regression in a political science application. Here's a few excerpts:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The two models used most frequently in political science for analyzing this type of data are either &lt;strong&gt;misspecified&lt;/strong&gt; (the OLS model...) or &lt;strong&gt;biased and inconsistent&lt;/strong&gt; (the logged OLS model...). Since statistical theory is known only for unrealistically large sample sizes, I use Monte Carlo experiments to demonstrate the empirical unbiasedness of the EPR model in finite samples and the &lt;strong&gt;bias and inefficiency&lt;/strong&gt; of the logged OLS model of event counts even in very large samples. &lt;sup&gt;[Emphasis added.]&lt;/sup&gt;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Misspecification becomes apparent if OLS results in negative predictions, and near zero:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;It makes the unrealistic assumption that the difference between zero and one event occurring in a particular time interval is the same as the difference between, say, 20 and 21 events. Thus, the true relationship is not linear, and a linear approximation would not in most cases even be a reasonable working assumption. OLS is an unbiased estimator of a linear conditional expectation function (CEF); the problem here is that the CEF is neither linear nor necessarily close to linear. &lt;/p&gt;&#10;  &#10;  &lt;p&gt;Second, the statistical inefficiency (the variance of the estimates across samples) of the OLS estimator is much higher than it could be. By taking into account neither the heteroskedasticity, the particular asymmetric form of the heteroskedasticity, the correct functional form, nor the underlying Poisson distribution of the disturbances, &lt;strong&gt;OLS does not use all available information in the estimation. Insufficiency and inefficiency result.&lt;/strong&gt; &lt;sup&gt;[Emphasis added.]&lt;/sup&gt;&lt;/p&gt;&#10;  &#10;  &lt;p&gt;These statistical problems are more than just technical points. They usually result in substantively biased conclusions. In applications, coefficients will have the wrong size and will often have the incorrect sign. Questions such as &quot;How many disruptive events will occur next month if unemployment decreases to 10 percent?&quot; will many times yield nonsensical answers like, &quot;The estimates indicate that there will be about negative four disruptive events.&quot; (This does not mean that there will be four less events; it means that the predicted number of events is -4.0.) Furthermore, estimates will often be very imprecise, making many empirical analyses inconclusive. In fact, since the standard errors and test statistics are themselves biased, there will usually be no indication of this imprecision. Unfortunately, these serious criticisms of the OLS model apply to most existing analyses of event count data in political science.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Poisson regression isn't ideal for every dataset of counts, because the Poisson distribution's mean and variance are defined as equal. When variance is greater than the mean, &lt;em&gt;overdispersion&lt;/em&gt; results, and &lt;em&gt;underdispersion&lt;/em&gt; is its opposite. These can be addressed with generalized alternatives presented by &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.214.8157&amp;amp;rep=rep1&amp;amp;type=pdf&quot; rel=&quot;nofollow&quot;&gt;King &lt;sup&gt;(1989)&lt;/sup&gt;&lt;/a&gt; using the negative binomial distribution. The &lt;a href=&quot;http://www.ats.ucla.edu/stat/stata/seminars/count_presentation/count.htm&quot; rel=&quot;nofollow&quot;&gt;UCLA Statistical Consulting Group &lt;sup&gt;(2007)&lt;/sup&gt; offers a good overview of other alternatives&lt;/a&gt; for modeling zero-inflation and random effects.&lt;/p&gt;&#10;&#10;&lt;p&gt;If your dataset is a &lt;a href=&quot;/questions/tagged/time-series&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;time-series&amp;#39;&quot; rel=&quot;tag&quot;&gt;time-series&lt;/a&gt;, you probably have many more analyses to consider than these alone.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;sup&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;br&gt;&#10;- King, G. (1988). Statistical models for political science event counts: Bias in conventional procedures and evidence for the exponential Poisson regression model. &lt;em&gt;American Journal of Political Science, 32&lt;/em&gt;(3), 838863. Retrieved from &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.214.8814&amp;amp;rep=rep1&amp;amp;type=pdf&quot; rel=&quot;nofollow&quot;&gt;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.214.8814&amp;amp;rep=rep1&amp;amp;type=pdf&lt;/a&gt;.&lt;br&gt;&#10;- King, G. (1989). Variance specification in event count models: From restrictive assumptions to a generalized estimator. &lt;em&gt;American Journal of Political Science, 33&lt;/em&gt;(3), 762784. Retrieved from &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.214.8157&amp;amp;rep=rep1&amp;amp;type=pdf&quot; rel=&quot;nofollow&quot;&gt;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.214.8157&amp;amp;rep=rep1&amp;amp;type=pdf&lt;/a&gt;.&lt;br&gt;&#10;- Statistical Consulting Group. (2007, April).  &lt;em&gt;Regression models with count data.&lt;/em&gt; UCLA. Retrieved from &lt;a href=&quot;http://www.ats.ucla.edu/stat/stata/seminars/count_presentation/count.htm&quot; rel=&quot;nofollow&quot;&gt;http://www.ats.ucla.edu/stat/stata/seminars/count_presentation/count.htm&lt;/a&gt;.&lt;/sup&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-16T07:51:13.127" Id="93977" LastActivityDate="2014-04-16T07:51:13.127" OwnerUserId="32036" ParentId="93968" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;What you are describing is recursive partitioning, implemented, for example, in the R &lt;code&gt;rpart&lt;/code&gt; package.  It is with difficulty that this is accomplished, because the data are not capable of revealing the correct connections/configurations except when the signals are very strong or the sample size is extremely large.  Whenever you wish to use a single dendrogram to represent the patterns (as opposed to random forests, bagging, and boosting trees) it is incumbent upon you to demonstrate that the patterns are replicable.  This can be done by bootstrapping the entire process and seeing if the same or similar trees emerge each time.  See &lt;a href=&quot;http://biostat.mc.vanderbilt.edu/ComplexDataJournalClub&quot; rel=&quot;nofollow&quot;&gt;http://biostat.mc.vanderbilt.edu/ComplexDataJournalClub&lt;/a&gt; for an example where this was done.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-04-16T11:49:24.947" Id="93986" LastActivityDate="2014-04-16T19:54:37.507" LastEditDate="2014-04-16T19:54:37.507" LastEditorUserId="22047" OwnerUserId="4253" ParentId="93969" PostTypeId="2" Score="1" />
  
  
  
  <row Body="&lt;p&gt;The estimated coefficients would be the same subject to the condition that you create your dummy variables (i.e. the numerical ones) consistent to R. For example:  lets' create a fake data and fit a Poisson glm using factor. Note that &lt;code&gt;gl&lt;/code&gt; function creates a factor variable.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; counts &amp;lt;- c(18,17,15,20,10,20,25,13,12)&#10;&amp;gt; outcome &amp;lt;- gl(3,1,9)&#10;&amp;gt; outcome&#10;[1] 1 2 3 1 2 3 1 2 3&#10;Levels: 1 2 3&#10;&amp;gt; class(outcome)&#10;[1] &quot;factor&quot;&#10;&amp;gt; glm.1&amp;lt;- glm(counts ~ outcome, family = poisson())&#10;&amp;gt; summary(glm.1)&#10;&#10;Call:&#10;glm(formula = counts ~ outcome, family = poisson())&#10;&#10;Deviance Residuals: &#10;    Min       1Q   Median       3Q      Max  &#10;-0.9666  -0.6713  -0.1696   0.8471   1.0494  &#10;&#10;Coefficients:&#10;            Estimate Std. Error z value Pr(&amp;gt;|z|)    &#10;(Intercept)   3.0445     0.1260  24.165   &amp;lt;2e-16 ***&#10;outcome2     -0.4543     0.2022  -2.247   0.0246 *  &#10;outcome3     -0.2930     0.1927  -1.520   0.1285    &#10;---&#10;Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1&#10;&#10;(Dispersion parameter for poisson family taken to be 1)&#10;&#10;    Null deviance: 10.5814  on 8  degrees of freedom&#10;Residual deviance:  5.1291  on 6  degrees of freedom&#10;AIC: 52.761&#10;&#10;Number of Fisher Scoring iterations: 4&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Since outcome has three levels, I create two dummy variables (dummy.1=0 if outcome=2&#10;and dummy.2=1 if outcome=3) and refit using these numerical values:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; dummy.1=rep(0,9)&#10;&amp;gt; dummy.2=rep(0,9)&#10;&amp;gt; dummy.1[outcome==2]=1&#10;&amp;gt; dummy.2[outcome==3]=1&#10;&amp;gt; glm.2&amp;lt;- glm(counts ~ dummy.1+dummy.2, family = poisson())&#10;&amp;gt; summary(glm.2)&#10;&#10;Call:&#10;glm(formula = counts ~ dummy.1 + dummy.2, family = poisson())&#10;&#10;Deviance Residuals: &#10;    Min       1Q   Median       3Q      Max  &#10;-0.9666  -0.6713  -0.1696   0.8471   1.0494  &#10;&#10;Coefficients:&#10;            Estimate Std. Error z value Pr(&amp;gt;|z|)    &#10;(Intercept)   3.0445     0.1260  24.165   &amp;lt;2e-16 ***&#10;dummy.1      -0.4543     0.2022  -2.247   0.0246 *  &#10;dummy.2      -0.2930     0.1927  -1.520   0.1285    &#10;---&#10;Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1&#10;&#10;(Dispersion parameter for poisson family taken to be 1)&#10;&#10;    Null deviance: 10.5814  on 8  degrees of freedom&#10;Residual deviance:  5.1291  on 6  degrees of freedom&#10;AIC: 52.761&#10;&#10;Number of Fisher Scoring iterations: 4&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;As you can see the estimated coefficients are the same. But you need to be careful when creating your dummy variables if you want to get the same result. For example if I create two dummy variables as  (dummy.1=0 if outcome=1 and dummy.2=1 if outcome=2) then the estimated results are different as follow:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; dummy.1=rep(0,9)&#10;&amp;gt; dummy.2=rep(0,9)&#10;&amp;gt; dummy.1[outcome==1]=1&#10;&amp;gt; dummy.2[outcome==2]=1&#10;&amp;gt; glm.3&amp;lt;- glm(counts ~ dummy.1+dummy.2, family = poisson())&#10;&amp;gt; summary(glm.3)&#10;&#10;Call:&#10;glm(formula = counts ~ dummy.1 + dummy.2, family = poisson())&#10;&#10;Deviance Residuals: &#10;    Min       1Q   Median       3Q      Max  &#10;-0.9666  -0.6713  -0.1696   0.8471   1.0494  &#10;&#10;Coefficients:&#10;            Estimate Std. Error z value Pr(&amp;gt;|z|)    &#10;(Intercept)   2.7515     0.1459   18.86   &amp;lt;2e-16 ***&#10;dummy.1       0.2930     0.1927    1.52    0.128    &#10;dummy.2      -0.1613     0.2151   -0.75    0.453    &#10;---&#10;Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1&#10;&#10;(Dispersion parameter for poisson family taken to be 1)&#10;&#10;    Null deviance: 10.5814  on 8  degrees of freedom&#10;Residual deviance:  5.1291  on 6  degrees of freedom&#10;AIC: 52.761&#10;&#10;Number of Fisher Scoring iterations: 4&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This is because when you add &lt;code&gt;outcome&lt;/code&gt; variable in glm.1, R by default creates two dummy variables namely &lt;code&gt;outcome2&lt;/code&gt; and &lt;code&gt;outcome3&lt;/code&gt; and defines them similarly to &lt;code&gt;dummy.1&lt;/code&gt; and &lt;code&gt;dummy.2&lt;/code&gt; in glm.2 i.e. the first level of outcome is when all other dummy variables (&lt;code&gt;outcome2&lt;/code&gt; and &lt;code&gt;outcome3&lt;/code&gt;) are set to be zero.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-04-16T16:09:48.623" Id="94018" LastActivityDate="2014-04-16T16:09:48.623" OwnerUserId="13138" ParentId="94010" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;According with the &lt;a href=&quot;http://stat.ethz.ch/R-manual/R-patched/library/stats/html/cor.html&quot; rel=&quot;nofollow&quot;&gt;R manual&lt;/a&gt; the cor function, with default parameter values computes Pearson correlation. A detailed description of Pearson correlation you can find on the &lt;a href=&quot;http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient&quot; rel=&quot;nofollow&quot;&gt;dedidated Wikipedia page&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;What is important to note here is that Pearson's correlation coefficient is a measure of &lt;em&gt;linear&lt;/em&gt; correlation. So a value close to $1$ or $-1$ shows only that there is a linear dependence between the actual values of $y$ and your prediction $\hat{y}$. &lt;/p&gt;&#10;&#10;&lt;p&gt;It is a known fact that RF has some bias on regression. That mostly happens due to averaging on leaves, but I think it is a longer discussion.&lt;/p&gt;&#10;&#10;&lt;p&gt;However the value of the Pearson correlation coefficient might give you some hints on how the prediction bias looks like (you need also a graphical inspection of your residuals, at least), but it is certainly &lt;strong&gt;not&lt;/strong&gt; a measure for accuracy. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-16T16:16:28.490" Id="94022" LastActivityDate="2014-04-16T16:16:28.490" OwnerUserId="16709" ParentId="94009" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;The Cox model itself depends on time. Indeed, the Cox model is written (using standard notations)&#10;$$&#10;h(t) = h_0(t) \exp(x^\prime \beta)&#10;$$&#10;Time-dependency is captured by the baseline hazard function $h_0(\cdot)$. Typical for the Cox model is that $h_0(\cdot)$ is left unspecified.&lt;/p&gt;&#10;&#10;&lt;p&gt;Based on a sample of survival data&#10;$$&#10;z=\big\{(y_j, \delta_j, x_j) \, \big|\, j = 1, \dotsc, N\big\}&#10;$$&#10;where $y_j$ is the minimum between the actual event time and the censoring time, $\delta_j$ is the event indicator ($\delta_j = 1$ if the observation corresponds to an event, and $\delta_j = 0$ if the observation is right-censored), and $x_j$ is the covariate vector, the survival likelihood is of form&#10;$$&#10;L(h_0(\cdot), \beta;\, z) = &#10;  \prod_{j=1}^N \big( h_0(y_j) \exp(x_j^\prime \beta) \big)^{\delta_j} &#10;  \exp \big\{ -H_0(y_j) \exp(x_j^\prime \beta) \big\}&#10;$$&#10;with $H_0(\cdot)$ the cumulative hazard function.&lt;/p&gt;&#10;&#10;&lt;p&gt;As we can see, the survival likelihood depends on the survival times (the $y_j$'s).&lt;/p&gt;&#10;&#10;&lt;p&gt;As the survival likelihood involves the (unspecified) baseline hazard, it cannot be used for inference in the Cox model. Instead, the Cox model is fitted based on a partial likelihood given by&#10;$$&#10;L_p(\beta;\, z) = \prod_{j=1}^N \left( \frac{\exp(x_j^\prime \beta)}{\sum_{\ell \in R(y_j)} \exp(x_\ell^\prime \beta)} \right)^{\delta_j}&#10;$$&#10;with $R(y_j)$ the risk set at time $y_j$ containing all subjects still under observation just prior to $y_j$, i.e.&#10;$$&#10;R(y_j) = \big\{ \ell \,\big|\, y_\ell \geq y_j \big\}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;As we can see, the partial likelihood depends on the survival times only through the risk set.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, if we change the survival times but keep the ranking identical, then this won't modify the risk sets. In other words, &quot;&lt;em&gt;the Cox model does not depend on the times itself, instead it only needs an ordering of the events.&lt;/em&gt;&quot;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-16T16:44:31.447" Id="94025" LastActivityDate="2014-04-17T04:39:38.403" LastEditDate="2014-04-17T04:39:38.403" LastEditorUserId="3019" OwnerUserId="3019" ParentId="93900" PostTypeId="2" Score="2" />
  
  <row Body="&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Store the mean and standard deviation of the training dataset features. When the test data is received, normalize each feature by subtracting its corresponding training mean and dividing by the corresponding training standard deviation.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Normalizition by min/max is usually a very bad idea since it involves scaling your entire data according to two particular observations. This leads your scaling to be dominated by noise. mean/std is a standard procedure and you can even experiment with more robust measures (e.g. median/MAD)&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Why scale/normalize? Because of the way the SVM optimization problem is defined, features with higher variance have greater effect on the margin. Usually this doesn't make sense - we'd like our classifier to be 'unit invariant' (e.g. a classifier that combines patients' weight and height shouldn't be affect by the choice of units - kgs or grams, centimeters or meters). &lt;/p&gt;&#10;&#10;&lt;p&gt;However, I guess that there might be cases in which all of the features are given in the same units and the differences in their variance indeed reflect differences in importance. In such case I'd try to skip scaling/normalization and see what it does to the performance.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-04-16T18:14:34.760" Id="94034" LastActivityDate="2014-04-16T18:14:34.760" OwnerUserId="20587" ParentId="94028" PostTypeId="2" Score="3" />
  
  
  
  <row Body="&lt;p&gt;You're asking about whats known as 'ensembling'. I'd suggest reading about it here &lt;a href=&quot;http://en.wikipedia.org/wiki/Ensemble_learning&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Ensemble_learning&lt;/a&gt;. Two popular methods are boosting and bagging.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-16T21:11:50.490" Id="94055" LastActivityDate="2014-04-16T21:11:50.490" OwnerUserId="30268" ParentId="94054" PostTypeId="2" Score="3" />
  <row AnswerCount="1" Body="&lt;p&gt;As I know the standard linear equation has the following form in $R^2$:&lt;/p&gt;&#10;&#10;&lt;p&gt;$w_1 x_1 + w_2 x_2 = b$ where $b$ is the intercept with $x_2$&lt;/p&gt;&#10;&#10;&lt;p&gt;Also I know that a decision boundary in $R^2$ for a perceptron is a line. However I read everywhere that the decision boundary equation is the following:&lt;/p&gt;&#10;&#10;&lt;p&gt;$w_1 x_1 + w_2 x_2+ b = 0$ &lt;/p&gt;&#10;&#10;&lt;p&gt;Why is that? Shouldn't it be $w_1 x_1 + w_2 x_2 - b = 0$!?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-16T22:08:35.360" Id="94059" LastActivityDate="2014-04-17T06:03:14.850" LastEditDate="2014-04-16T23:00:50.500" LastEditorUserId="27779" OwnerUserId="27779" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;classification&gt;&lt;neural-networks&gt;" Title="Decision boundary equation of the perceptron" ViewCount="98" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;If the eigen-decomposition of sample covariance matrix is $S=PDP'$ where $D$ is a diagonal matrix with eigen value of $S$ and $P$ are eigenvectors.&lt;/p&gt;&#10;&#10;&lt;p&gt;If we define the empirical distribution function of sample eigenvalues as $F = p^{-1}\sum_{i=1}^{p}I_{d_{i} \leq d}$, how can we show that, stieltjes transformation of $F$ is &lt;/p&gt;&#10;&#10;&lt;p&gt;$\frac{1}{p} \sum_{i=1}^{p} \frac{1}{d_{i}-z} = p^{-1}tr[S-zI]^{-1}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any help is appreciated.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-17T00:28:52.320" Id="94069" LastActivityDate="2014-04-17T00:28:52.320" OwnerUserId="35011" PostTypeId="1" Score="0" Tags="&lt;mathematical-statistics&gt;&lt;data-transformation&gt;&lt;eigenvalues&gt;&lt;mathematics&gt;&lt;numerical-integration&gt;" Title="Stieltjes transformation for empirical distribution function" ViewCount="20" />
  <row AnswerCount="0" Body="&lt;p&gt;Edit : this is the data I used for the first part of the problem :&#10;\begin{matrix}&#10;  Rocks   &amp;amp; 0&amp;amp; 1&amp;amp; 2&amp;amp; 3&amp;amp; 4&amp;amp; 5&amp;amp; 6 &amp;amp; 7\\&#10;  samples &amp;amp; 12&amp;amp; 27&amp;amp; 28&amp;amp; 19&amp;amp; 8&amp;amp; 3&amp;amp; 1 &amp;amp; 1\\&#10;\end{matrix}&lt;/p&gt;&#10;&#10;&lt;p&gt;I did hypothesis to test if it follows poisson or binomial and concluded it could follow both.&lt;/p&gt;&#10;&#10;&lt;h2&gt;Part 2&lt;/h2&gt;&#10;&#10;&lt;p&gt;Let p be the probability that a sample taken on a land contains strictly more than one rock.&lt;/p&gt;&#10;&#10;&lt;p&gt;A) What is the p value if we suppose the following : finding rock in samples follows a Poisson P(2) with $\lambda = 2$ ?&lt;/p&gt;&#10;&#10;&lt;p&gt;For this part I just do Poisson average is 2 and random is 1 : $0.594$&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;B) What is the &lt;em&gt;empirical proportion&lt;/em&gt; $\hat p$ of the samples that contains $ &amp;gt; 1$ rock ?&lt;/p&gt;&#10;&#10;&lt;p&gt;For this part, I have, using the continuity correction ( +0.5) :&lt;/p&gt;&#10;&#10;&lt;p&gt;$P(X &amp;gt; 1 - 0.5 - 2) / \sqrt{2} =  p(z &amp;gt;0.5 - 2) / 1.41 =  -1.0638$&lt;/p&gt;&#10;&#10;&lt;p&gt;$ = p(z &amp;lt; 1.0638)$&lt;/p&gt;&#10;&#10;&lt;p&gt;my answer : everything below 1.06 standard deviation left from the mean.&#10;Am I right with my interpretation of &lt;em&gt;empirical proportion ?&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;C) Give a 98 % confidence interval for $p$&lt;/p&gt;&#10;&#10;&lt;p&gt;I found an &lt;a href=&quot;http://statpages.org/confint.html&quot; rel=&quot;nofollow&quot;&gt;interval using this calculator&lt;/a&gt; but I'm not sure if it's right to use 1 as my observed events value.&lt;/p&gt;&#10;&#10;&lt;p&gt;$[0.01, 6.64]$&lt;/p&gt;&#10;&#10;&lt;p&gt;also found this is the formula to use $\lambda \pm 1.96\sqrt{\lambda / n}$&lt;/p&gt;&#10;&#10;&lt;p&gt;but in my case it's going to be $\lambda \pm 1.96 * \sqrt{1}$ ?&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;D) &lt;strong&gt;Test at $\alpha = 5%$ the hypothesis $p = 0.5$ versus $ p &amp;gt; 0.5$&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I don't know where to start this part.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-04-17T00:37:39.620" Id="94071" LastActivityDate="2014-04-17T14:51:08.323" LastEditDate="2014-04-17T14:51:08.323" LastEditorUserId="43932" OwnerUserId="43932" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;&lt;self-study&gt;&lt;estimation&gt;&lt;poisson&gt;" Title="Poisson distribution confidence interval" ViewCount="80" />
  <row AnswerCount="0" Body="&lt;p&gt;I have the following data (relating to consecutive months at current job):&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Mean: 45.4&lt;/li&gt;&#10;&lt;li&gt;Standard Deviation: 60.89&lt;/li&gt;&#10;&lt;li&gt;Sample Size: 48&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I am asked to do a one-tailed test at a 1% significance level and provide:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;the value of the test statistic (from above).&lt;/li&gt;&#10;&lt;li&gt;P-Value of the test statistic.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;My initial assumption about the data is that most of the values would fall under 60 months. With that said, my hypothesis is:&lt;/p&gt;&#10;&#10;&lt;p&gt;H&lt;sub&gt;0&lt;/sub&gt;: x&amp;#772; &amp;#8804; 60, H&lt;sub&gt;a&lt;/sub&gt;: x&amp;#772; &gt; 60&lt;/p&gt;&#10;&#10;&lt;p&gt;How do I proceed with this problem? I am not sure I understand the process.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-04-17T03:19:18.990" Id="94087" LastActivityDate="2014-04-17T03:55:50.703" LastEditDate="2014-04-17T03:55:50.703" LastEditorUserId="43937" OwnerUserId="43937" PostTypeId="1" Score="2" Tags="&lt;hypothesis-testing&gt;&lt;self-study&gt;&lt;p-value&gt;" Title="How would I find the P-Value of this data?" ViewCount="43" />
  
  
  
  
  <row AcceptedAnswerId="94754" AnswerCount="3" Body="&lt;p&gt;I have two 2 hours of GPS data with a sampling rate of 1 Hz (7200 measurements). The data are given in the form $(X, X_\sigma, Y, Y_\sigma, Z, Z_\sigma)$, where $N_\sigma$ is the measurement uncertainty.&lt;/p&gt;&#10;&#10;&lt;p&gt;When I take the mean of all measurements (e.g. the average Z value of those two hours), what is its standard deviation? I can of course calculate the standard deviation from the Z values, but then I neglect the fact that there are known measurement uncertainties...&lt;/p&gt;&#10;&#10;&lt;p&gt;Edit: The data is all from the same station, and all coordinates are remeasured every second. Due to satellite constellations etc., every measurement has a different uncertainity. The purpose of my analysis is to find the displacement due to the an external event, (i.e. an earthquake). I would like to take the mean for 7200 measurements (2h) before the earthquake and another mean for 2h after the earthquake, and then calculate the resulting difference (in height for example). In order to specifiy the standard deviation of this difference, I need to know the standard deviation of the two means.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-04-17T08:56:30.963" Id="94109" LastActivityDate="2014-04-24T16:30:27.797" LastEditDate="2014-04-24T16:30:27.797" LastEditorUserId="43954" OwnerUserId="43954" PostTypeId="1" Score="10" Tags="&lt;time-series&gt;&lt;mean&gt;&lt;standard-deviation&gt;" Title="Standard deviation of several measurements with uncertainties" ViewCount="423" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm making a project connected with identifying the dynamics of sales. My database concerns 26 weeks (so equally in 26 time-series observations) after launching the product, 126 time-series=126 different products.&lt;/p&gt;&#10;&#10;&lt;p&gt;I used clustering to group products with similar dynamics of sales. I applied two different approaches to pam() = k-medoids clustering. First one groupes time-series - 3 clusters, second one groupes S-curve parameters, counted before by curve fitting to each time-series - 6 clusters. &lt;/p&gt;&#10;&#10;&lt;p&gt;Now I would like to compare these two methods, but have no idea how to compare the quality of two clustering methods if I have different number of clusters.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would be grateful for any suggestions.&lt;/p&gt;&#10;" CommentCount="10" CreationDate="2014-04-17T09:13:40.857" Id="94110" LastActivityDate="2014-04-17T09:13:40.857" OwnerUserId="42910" PostTypeId="1" Score="0" Tags="&lt;time-series&gt;&lt;clustering&gt;&lt;k-medoids&gt;" Title="How to compare two different clustering approaches?" ViewCount="70" />
  
  
  <row AcceptedAnswerId="94122" AnswerCount="1" Body="&lt;p&gt;I have what appears to be a relatively simple question, but am struggling to understand how to go about answering it.&lt;/p&gt;&#10;&#10;&lt;p&gt;The general question is as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;What is the expected value of $S_{I}$, where:&lt;/p&gt;&#10;&#10;&lt;p&gt;$S_{I} = S$ if $S &amp;lt;3000 $&lt;/p&gt;&#10;&#10;&lt;p&gt;$S_{I} = 3000$ if $S &amp;gt;3000$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $S$ is a compound distribution (details not necessary for my problem here)&lt;/p&gt;&#10;&#10;&lt;p&gt;My initial attempt is as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;$E[S_{I}] = E[E[S_{I}|S]] = E[S * P(S\leq3000) + 3000*P(S&amp;gt;3000)] = E[S]*P(S\leq 3000)+3000*P(S&amp;gt;3000).$&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, from the definition of $S_{I}$, it is clear that we should have $E[S_{I}]&amp;lt;3000$. For this particular problem $E[S] = 4000$, and hence my proposed method for solving is wrong.&lt;/p&gt;&#10;&#10;&lt;p&gt;So I thought that my $E[S]$ in the line above should actually be $E[S|S\leq 3000]$&lt;/p&gt;&#10;&#10;&lt;p&gt;Is this assumption correct?&lt;/p&gt;&#10;&#10;&lt;p&gt;If so, is it true that $E[S|S\leq 3000] = E[S]*P(S\leq 3000)$ ??&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for any help.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-04-17T11:07:11.907" Id="94120" LastActivityDate="2014-04-17T11:35:13.737" OwnerUserId="24566" PostTypeId="1" Score="1" Tags="&lt;expected-value&gt;&lt;conditional-expectation&gt;&lt;truncation&gt;" Title="Expectation of Truncated &amp; Random Variable" ViewCount="74" />
  
  <row Body="&lt;p&gt;&quot;Multiple regression&quot; refers to situations in which you have more than one predictor / explanatory variable ($X$).  &lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;Multivariate regression&quot; refers to situations in which you have more than one response / outcome / dependent variable ($Y$).  &lt;/p&gt;&#10;&#10;&lt;p&gt;It is also possible to have both multiple predictors and multiple responses, in which case you could call it a &quot;multivariate multiple regression&quot;.  But since people rarely have only one predictor, I don't think people are worried about making the multiple predictor part distinct.  This raises the question of why we worry about &quot;multiple&quot; vs. &quot;simple&quot; (only one predictor) regression in the typical case when you have only one response.  I think that it is mostly for historical and pedagogical (teaching) reasons: simple regression was worked out first, and is taught first to help students get the main ideas before going further.  &lt;/p&gt;&#10;&#10;&lt;p&gt;In your case, I gather you have only one response variable (Pakistan's GDP growth), and several predictor variables (growth in mining, electricity, communication, manufacturing and electricity), so your regression model will be a regular old multiple regression.  &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-04-17T13:47:25.010" Id="94138" LastActivityDate="2014-04-17T14:55:27.877" LastEditDate="2014-04-17T14:55:27.877" LastEditorUserId="28746" OwnerUserId="7290" ParentId="94129" PostTypeId="2" Score="11" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have some bivariate data and I have calculated the error ellipse in the following way: I have first calculated the covariance matrix and then to obtain the radii of the ellipse I have taken the eigenvalues and then their squared root.&lt;/p&gt;&#10;&#10;&lt;p&gt;Does the length of each of these radii correspond to one standard deviation in the direction of the semi-principal axes?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-04-17T17:22:16.150" Id="94159" LastActivityDate="2014-07-28T11:07:29.457" LastEditDate="2014-04-18T07:59:45.463" LastEditorUserId="9768" OwnerUserId="9768" PostTypeId="1" Score="2" Tags="&lt;variance&gt;&lt;standard-deviation&gt;&lt;standard-error&gt;&lt;covariance&gt;&lt;bivariate&gt;" Title="Which error is displayed in an error ellipse?" ViewCount="131" />
  
  <row AcceptedAnswerId="94187" AnswerCount="1" Body="&lt;p&gt;Quite a simple question, looking to see if anybody has experience using both. I have no knowledge of PRIMER, but wondering if R (currently) has the same capacity for data analysis specifically for ecological data using things like perMANOVAs and NMDS plotting. &lt;/p&gt;&#10;&#10;&lt;p&gt;I know it can be done in R (what can't!?) but I'm more concerned if it's a much longer route to the same conclusion.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-17T19:20:09.663" FavoriteCount="1" Id="94173" LastActivityDate="2014-07-07T18:16:34.420" OwnerUserId="43994" PostTypeId="1" Score="2" Tags="&lt;r&gt;" Title="Can R substitute PRIMER efficiently and effectively?" ViewCount="161" />
  <row Body="&lt;p&gt;If $X$ is a continuous random variable with pdf $f(x)$ and $Y=kX+\theta,$ then the pdf of $Y$ is given by $$g(y) = {{1} \over {|k|}}f \left({{y-\theta} \over {k}} \right).$$&lt;/p&gt;&#10;&#10;&lt;p&gt;In your case you have $x=(b-a)x'+a,$ so the pdf of $X$ is given by $$f(x) = {{1} \over {|b-a|}}h \left( {{x-a} \over {b-a}} \right),$$ where $h$ is the pdf you found in your optimization. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-17T19:42:27.053" Id="94177" LastActivityDate="2014-04-17T19:42:27.053" OwnerUserId="24073" ParentId="93939" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;On your plot of beta, there's really no way to tell whether the curve should turn up/down or remain flat below 16. I tried making some plots using different smoothing splines, and a small tweak in a tuning parameter can completely change the shape of the curve outside the range of the data.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you have good reason to believe the curve should be flat below 16, I think you can just make it be so. It seems to me that, in this case, the subject matter knowledge you have will take you much further than any automatic/quantitative methods can.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-04-18T01:02:23.800" Id="94203" LastActivityDate="2014-04-18T01:02:23.800" OwnerUserId="24498" ParentId="94195" PostTypeId="2" Score="1" />
  
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I'll be working with data from an intelligent tutor system similar to one studied in the &lt;a href=&quot;http://www.sigkdd.org/kdd-cup-2010-student-performance-evaluation&quot; rel=&quot;nofollow&quot;&gt;KDD-Cup 2010 on student performance prediction&lt;/a&gt; and plan to use IRT models to infer item and ability parameters. Other covariates such as 'number of attempts' or 'time spent on the problem' are readily available. I am interested in &lt;strong&gt;incorporating these kind of covariates ('paradata')  in the estimation of the latent traits&lt;/strong&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Do you consider this a meaningful project, or do you expect that not much can be gained by including these additional variables? Are you aware of similar projects?&lt;/p&gt;&#10;&#10;&lt;p&gt;What would be a good place to &lt;em&gt;learn how to fit IRT models in the more general framework of GLMs/ SEMs?&lt;/em&gt; &lt;a href=&quot;https://stats.stackexchange.com/questions/21153/introduction-to-structural-equation-modeling?rq=1&quot;&gt;I have been pointed toward publications from Sophia Rabe-Hesketh and others&lt;/a&gt;, but &lt;strong&gt;ideally&lt;/strong&gt; I would be interested in  any &lt;strong&gt;(online) resources or demos that are specific for a learning context and IRT&lt;/strong&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;I currently have an alright understanding of IRT models, but don't have much experience in fitting them yet. So recommendations on R packages to work with in this context would also be appreciated. (I am aware of the psychometric task view on CRAN.)&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-04-18T11:36:54.213" FavoriteCount="2" Id="94241" LastActivityDate="2014-04-19T05:53:46.560" LastEditDate="2014-04-19T05:53:46.560" LastEditorUserId="32036" OwnerUserId="11678" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;generalized-linear-model&gt;&lt;references&gt;&lt;sem&gt;&lt;psychometrics&gt;" Title="How can IRT-Models be understood in GLM/ SEM Framework? (Predict Learning with added Paradata-Covariates)" ViewCount="75" />
  
  <row AnswerCount="0" Body="&lt;p&gt;i'm not sure what statistical test to use to compare a scenario like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a list of candidate regions A (560k regions) and I want to know if my candidate regions enrich for a particular set of attributes X (14k items). So I did an overlap with A and X and the overlap is 3300 of X overlapping A. &lt;/p&gt;&#10;&#10;&lt;p&gt;I also generated a randomly distributed, size match control regions B (560k regions) and did the overlap between B and X and found 1800 of X overlapping B. &lt;/p&gt;&#10;&#10;&lt;p&gt;So I want to test if the 3300 overlap is statistically significant. &lt;/p&gt;&#10;&#10;&lt;p&gt;Which test to use here? I kinda feel I can use Fisher's exact test but not sure if it's valid. &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-18T14:38:38.007" Id="94262" LastActivityDate="2014-04-18T14:38:38.007" OwnerUserId="30475" PostTypeId="1" Score="0" Tags="&lt;hypothesis-testing&gt;&lt;statistical-significance&gt;" Title="which test to use here to test the significance of overlap with respect to a control?" ViewCount="14" />
  <row Body="&lt;p&gt;It's the absolute value. So |-5| = 5, and |5| = 5.&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ P( |Z| \leq 2) = P( Z \leq 2\text{  and}-Z \leq 2) = P(Z \leq 2\text{ and }Z \geq -2) = P(-2 \leq Z \leq 2) = P(Z \geq -2) - P(Z \geq 2) = 1 - H(2) - H(2)$$ &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-04-18T14:47:57.100" Id="94264" LastActivityDate="2014-04-18T16:11:16.470" LastEditDate="2014-04-18T16:11:16.470" LastEditorUserId="22047" OwnerUserId="17661" ParentId="94263" PostTypeId="2" Score="6" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;We were asked this question for homework: &#10;B. Use MC integration to estimate the probability that X  exp(X ) &amp;lt; 2.5, assuming that X  Gamma(1.2, 3.7). &#10;Details. The PDF, CDF, and QUANTILE functions for the Gamma distribution in SAS have two parameters called a and  (lambda). The distribution in this problem uses a = 1.2 and  = 3.7. However, the RAND function assumes  = 1 and doesnt directly allow  = 3.7. One way around this difficulty is to generate X using the inverse transform method. You are welcome to use another approach if you like, but full credit will require detailed justification for why it works. &lt;/p&gt;&#10;&#10;&lt;p&gt;This is my code: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;data problem_mc;&#10;    call streaminit(45056);&#10;    count = 0;&#10;    do i=1 to 2000;&#10;        p = rand('uniform'); *random numbers in order to calculate probability;&#10;        x = quantile('Gamma', p, 1.2, 3.7); *GAMMA distribution with &#10;probability p, a=1.2 and lambda=3.7;&#10;        count = count + (x*exp(x) &amp;lt; 2.5); *count of # times requirements are met;&#10;end;&#10;&#10;t = count/2000; &#10;se = sqrt(t*(1-t)/2000);&#10;&#10;put '===============================================';&#10;put &quot;estimated proportion is&quot;;&#10;put &quot;    &quot; p= &quot; with standard error &quot; se;&#10;put '================================================';&#10;run;&#10;&#10;/*output from LOG window*/&#10;&#10;===============================================&#10;estimated proportion is&#10;    p=0.419209095  with standard error 0.0080603272&#10;================================================&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Is my output correct?&lt;/p&gt;&#10;" ClosedDate="2014-04-18T19:22:14.873" CommentCount="1" CreationDate="2014-04-18T16:56:15.340" Id="94272" LastActivityDate="2014-04-18T19:21:27.757" LastEditDate="2014-04-18T19:21:27.757" LastEditorUserId="919" OwnerUserId="44039" PostTypeId="1" Score="0" Tags="&lt;sas&gt;" Title="MC Integration Interval Probability" ViewCount="9" />
  <row Body="&lt;p&gt;Are you using some package to fit the data or is this a school problem where you are writing a non-linear least squares code? Just about every package allows you to input the errors on the data points and it will output the errors on the fitted parameters or full covariance matrix. In fact, you should be able to input the covariance matrix of errors (if you know it) but power spectra usually have fairly diagonal covariance matrices.  &lt;/p&gt;&#10;&#10;&lt;p&gt;The basic idea is to look at weighted least squares. You don't want to just ignore the errors when doing the fit and then use them to calculate the error. The fitter needs the errors in order to optimize the fit. &lt;/p&gt;&#10;&#10;&lt;p&gt;If the errors are small enough, the distribution of errors may be reasonably well described as multivariate normal. The terms in the output covariance matrix actually comes from linearizing the model around the best fit solution. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you really want to characterize the error distribution fully, you'll have to go to something like markov chain monte carlo.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-04-18T17:12:34.023" Id="94275" LastActivityDate="2014-04-18T17:12:34.023" OwnerUserId="40967" ParentId="94253" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Am I right that your design and expected results look something like this?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/q0kru.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;B stands for baseline.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-18T17:46:18.927" Id="94278" LastActivityDate="2014-04-18T17:54:11.103" LastEditDate="2014-04-18T17:54:11.103" LastEditorUserId="38102" OwnerUserId="38102" ParentId="94162" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;Some people are annoyingly obsessed with unbiasedness. Bias and dispersion are measures of uncertainty which roughly correspond to &lt;a href=&quot;http://www.kaushik.net/avinash/emetrics-dc-07-reflections-accuracy-precision-predictive-analytics/&quot; rel=&quot;nofollow&quot;&gt;accuracy and precision&lt;/a&gt;. You usually have a trade-off between accuracy and precision, some estimators may be more precise but less accurate and vice versa.&lt;/p&gt;&#10;&#10;&lt;p&gt;MSE is a sum of bias and the variance: $MSE=E[\beta-\hat{\beta}]^2=E[\beta-E[\hat{\beta}]+E[\hat{\beta}]-\hat{\beta}  ]^2=bias^2+Var[\hat{\beta}]$&lt;/p&gt;&#10;&#10;&lt;p&gt;Example. Let's say your parameter is from &lt;a href=&quot;http://rd11.web.cern.ch/RD11/rkb/AN16pp/node23.html&quot; rel=&quot;nofollow&quot;&gt;Breit Wiegner&lt;/a&gt; distribution.&#10;&lt;img src=&quot;http://i.stack.imgur.com/is0TN.gif&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;There is no unbiased estimator. The bias is defined as $bias=E[\hat{\beta}]-\beta$, where $\beta$ - true value, and $\hat{\beta}$ is its estimator. In this case $E[\hat{\beta}]$ is not defined mathematically, so you can't compute the bias. &lt;/p&gt;&#10;&#10;&lt;p&gt;This is an extreme example where unbiased estimator does not exist at all.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-04-18T20:12:29.270" Id="94296" LastActivityDate="2014-04-18T20:54:05.220" LastEditDate="2014-04-18T20:54:05.220" LastEditorUserId="36041" OwnerUserId="36041" ParentId="94294" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="94310" AnswerCount="1" Body="&lt;p&gt;Kevin Murphy's &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0262018020&quot; rel=&quot;nofollow&quot;&gt;book&lt;/a&gt; discusses a classical Hierarchical Bayesian problem (originally discussed in &lt;code&gt;Johnson and Albert, 1999, p24&lt;/code&gt;):&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Suppose that we are trying to estimate the cancer rate in $N$ cities. In&#10;  each city, we sample a number of individuals $N_i$ and measure the number of people with cancer $x_i \sim \text{Bin}(N_i, \theta_i)$, where $\theta_i$ is the true cancer rate in the city. &lt;/p&gt;&#10;  &#10;  &lt;p&gt;We would like to estimate the $\theta_i$'s while allowing the data-poor cities to borrow statistical strength from data-rich cities.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;To do so, he models $\theta_i \sim \text{Beta}(a,b)$ so that all cities share the same prior, so the final models looks as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;$p(\mathcal{D}, \theta, \eta|N)=p(\eta)\prod\limits^N_{i=1}\text{Bin}(x_i|N_i, \theta_i)\text{Beta}(\theta_i|\eta)$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\eta = (a,b)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The crucial part about this model is of course (I quote), &quot;that we infer $\eta=(a,b)$ from the data, since if we just clamp it to a constant, the $\theta_i$ will be conditionally independent, and there will be no information flow between them&quot;.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;I am trying to model this in &lt;a href=&quot;https://github.com/pymc-devs/pymc&quot; rel=&quot;nofollow&quot;&gt;PyMC&lt;/a&gt;, but as far as I understand, I need a prior for $a$ and $b$ (I believe this is $p(\eta)$ above). What would be one good prior for this model?&lt;/p&gt;&#10;&#10;&lt;p&gt;In case it helps, the code, as I have it now is:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;bins = dict()&#10;ps   = dict()&#10;for i in xrange(N_cities):&#10;    ps[i]   = pm.Beta(&quot;p_{}&quot;.format(i), alpha=a, beta=b)&#10;    bins[i] = pm.Binomial('bin_{}'.format(i), p=ps[i],n=N_trials[i],  value=N_yes[i], observed=True)&#10;&#10;mcmc = pm.MCMC([bins, ps])&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;where I believe I need a prior for &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;. How should I pick one? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-18T21:08:53.353" FavoriteCount="2" Id="94303" LastActivityDate="2014-04-19T22:01:34.810" LastEditDate="2014-04-18T23:34:26.623" LastEditorUserId="22311" OwnerUserId="2798" PostTypeId="1" Score="4" Tags="&lt;bayesian&gt;&lt;hierarchical-bayesian&gt;&lt;pymc&gt;&lt;probabilistic-programming&gt;" Title="Hierarchical Bayesian modeling of incidence rates" ViewCount="191" />
  
  
  
  <row AcceptedAnswerId="94343" AnswerCount="1" Body="&lt;p&gt;I am doing a self-study exercise which attempts to exemplify a case where the &lt;strong&gt;Normal Distribution&lt;/strong&gt; is used to approximate the &lt;strong&gt;Poisson Distribution&lt;/strong&gt;, since the population mean is more than 10.&lt;/p&gt;&#10;&#10;&lt;p&gt;I understand that when using &lt;strong&gt;Normal Distribution&lt;/strong&gt; to Approximate &lt;strong&gt;Poisson / Binomial Distributions&lt;/strong&gt;, there is a need for &lt;strong&gt;Continuity Correction Error&lt;/strong&gt; to be managed. From my understanding, this means subtracting 0.5 from the lower bound and adding 0.5 to the upper bound.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/KJJud.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;However, I was faced with the following example, where 0.5 was subtracted from the equation.&lt;/p&gt;&#10;&#10;&lt;p&gt;I will like to clarify:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;if this is because it is lower bound, as the value is less than W?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;and if the following concept is correct&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;From my understanding, this means subtracting 0.5 from the lower bound and adding 0.5 to the upper bound.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="0" CreationDate="2014-04-19T07:48:37.660" Id="94334" LastActivityDate="2014-12-16T21:25:24.493" LastEditDate="2014-12-16T21:25:24.493" LastEditorUserId="7290" OwnerUserId="37976" PostTypeId="1" Score="3" Tags="&lt;distributions&gt;&lt;self-study&gt;&lt;normal-distribution&gt;&lt;poisson&gt;" Title="Continuity correction error when using normal distribution to estimate Poisson distribution" ViewCount="360" />
  <row AnswerCount="1" Body="&lt;p&gt;I have a sample of 65 participants.&#10;20 chose A.&#10;18 chose B.&#10;14 chose C and &#10;13 chose D.   What is the suitable test to determine if there is significant difference in the proportions?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-19T08:29:24.783" Id="94336" LastActivityDate="2014-04-19T17:36:36.710" LastEditDate="2014-04-19T09:21:40.583" LastEditorUserId="22047" OwnerUserId="44072" PostTypeId="1" Score="0" Tags="&lt;proportion&gt;" Title="Comparing proportions" ViewCount="47" />
  <row Body="&lt;p&gt;There is no equivalent of the &lt;code&gt;seqdistmc&lt;/code&gt; function for computing multichannel distances between event sequences in TraMineR.&lt;/p&gt;&#10;&#10;&lt;p&gt;Actually, &lt;code&gt;seqdistmc&lt;/code&gt; implements the principle defined by Pollock (Pollock, G., 2007,  Holistic trajectories: A study of combined employment, housing and family careers by using multiple-sequence analysis, JRSS, series A, 170(1), 167-183). The method applies to optimal matching distances only and aims at reducing the number of required substitution costs. It consists in deriving the substitution costs between combined states of different domains from the substitution costs in each domain. For two domains with each say 10 states, one needs to specify twice $9 * 10/2 = 45$, i.e., 90 substitution costs for the total of $49 * 50/2 = 1225$ pairs that can be formed with the $10*10/2 = 50$ combined states.&lt;/p&gt;&#10;&#10;&lt;p&gt;We do not have this problem with event sequences where simultaneous events are allowed. The OME measure computed by the &lt;code&gt;seqedist&lt;/code&gt; function considers the following two basic operations (see p 240 in Ritschard, G., Brgin, R. &amp;amp; Studer, M. , 2013, &quot;Exploratory Mining of Life Event Histories&quot;, In McArdle, J.J. &amp;amp; Ritschard, G. (eds) Contemporary Issues in Exploratory Data Mining in the Behavioral Sciences. pp. 221-253. New York: Routledge.) &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;insertion/deletion of an event;&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;changing the time stamp by one unit.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;A single cost can be specified for each operation, but event dependent costs can also be specified. So the number of costs would be at most twice the event alphabet size. &lt;/p&gt;&#10;&#10;&lt;p&gt;For a multichannel analysis of event sequences, we can simply merge the multi-sequences associated with each individual, i.e., consider the event sequences that include the events of both dimensions. For two channels, the number of costs to be specified would then simply be the sum of the number of costs for each channel. With two channels having each an alphabet of size 10, the number of costs would be at most $2 * 2 * 10 = 40$. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-19T10:21:57.930" Id="94347" LastActivityDate="2014-04-19T11:20:20.667" LastEditDate="2014-04-19T11:20:20.667" LastEditorUserId="11805" OwnerUserId="11805" ParentId="94247" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Informally ... &quot;highly non linear&quot; means &quot;even a blind man can see its not a straight line!&quot; ;) Personally I take it as a danger sign, that it will somehow &quot;blow up in your face&quot; when used with real world examples. &lt;/p&gt;&#10;&#10;&lt;p&gt;The Tower of Hanoi could be called an example of highly non linear ... the legend being when the monks finish a 64 disk stack, the world will end. If you count total time spent in training, feeding, housing, and motivating everyone to support a thankless boring pointless multi-generational task, I would expect the total cost in man hours to really blow out!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-19T11:11:43.503" Id="94352" LastActivityDate="2014-04-19T11:11:43.503" OwnerUserId="44080" ParentId="94136" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;From &lt;a href=&quot;http://www.mathworks.com/help/nnet/ug/train-and-apply-multilayer-neural-networks.html&quot; rel=&quot;nofollow&quot;&gt;MATLAB docs&lt;/a&gt;:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The magnitude of the gradient and the number of validation checks are used to terminate the training. The gradient will become very small as the training reaches a minimum of the performance. If the magnitude of the gradient is less than 1e-5, the training will stop. This limit can be adjusted by setting the parameter &lt;code&gt;net.trainParam.min_grad&lt;/code&gt;. The number of validation checks represents the number of successive iterations that the validation performance fails to decrease. If this number reaches 6 (the default value), the training will stop. (...) You can change this criterion by setting the parameter &lt;code&gt;net.trainParam.max_fail&lt;/code&gt;. &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;So yeah what you are doing is perfectly legitimate. I would suspect that most of the difference (improvement) actually comes form changing the property &lt;code&gt;net.trainParam.min_grad&lt;/code&gt; as by reducing its value you keep optimizing while taking small steps you would otherwise not have taken rather than increasing &lt;code&gt;net.trainParam.min_grad&lt;/code&gt;. &lt;code&gt;net.trainParam.min_grad&lt;/code&gt; will probably allow your optimization not to terminate in the case is hits a small &quot;plateau&quot;. &lt;/p&gt;&#10;&#10;&lt;p&gt;One last thing: While you do not say what are you doing with your NN, if you using a sigmoid function and you use your NN for regression you might want to rescale your target data to be in a $[-0.8,0.8]$ interval so you avoid the saturation that comes to a sigmoid near its edges. (MATLAB, and most software I suspect you probably normalize this to $[-1,1]$ that while correct from a theoretical point of you it might now be optimal in some hard cases.)&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-04-19T12:44:43.000" Id="94358" LastActivityDate="2014-04-19T12:44:43.000" OwnerUserId="11852" ParentId="94346" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;This sounds like a job for confirmatory factor analysis (CFA). If you're willing to consider vulnerability score a latent variable that affects the other variables used to measure it, CFA can produce factor scores based on an empirically derived set of linear weights applied to your indicator variables. Factor loadings are correlations between individual items and the latent factor estimated from their covariance structure. Items' unique variance is often considered measurement error, while their common variance determines their loadings. Loadings reflect the contributions of measured variables to the common factor. This should tell you everything that it sounds like you want to learn from your proposed analysis, whereas that analysis would tell you nothing, as @PeterFlom has explained.&lt;/p&gt;&#10;&#10;&lt;p&gt;An additional perk of CFA is that overall model fit statistics can tell you how well your latent factor explains the covariance among your measured variables. Modification indices can then tell you how your model might be improved. For instance, if you have very many items, they're likely to be multidimensional. If some of those additional dimensions aren't useful to you, you can control the variance they explain and use the remaining variance to estimate the general factor of primary interest with bifactor analysis &lt;sup&gt;(&lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2981404/&quot; rel=&quot;nofollow&quot;&gt;Reise, Moore, &amp;amp; Haviland, 2010&lt;/a&gt;)&lt;/sup&gt;. I've said a little more about this method in my answer to &quot;&lt;a href=&quot;http://stats.stackexchange.com/q/2374/32036&quot;&gt;Factor analysis of questionnaires composed of Likert items&lt;/a&gt;&quot;, which may have more info of use to you if your data aren't continuous and normally distributed.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;sup&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;br&gt;&#10;Reise, S. P., Moore, T. M., &amp;amp; Haviland, M. G. (2010).  Bifactor models and rotations: Exploring the extent to which multidimensional data yield univocal scale scores.  &lt;em&gt;Journal of Personality Assessment, 92&lt;/em&gt;(6), 544559.  Retrieved from &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2981404/&quot; rel=&quot;nofollow&quot;&gt;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2981404/&lt;/a&gt;.&lt;/sup&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-19T19:11:54.247" Id="94383" LastActivityDate="2014-04-19T19:19:20.203" LastEditDate="2014-04-19T19:19:20.203" LastEditorUserId="32036" OwnerUserId="32036" ParentId="73251" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;What was proposed to you is sometimes referred to as a forbidden regression and in general you will not consistently estimate the relationship of interest. Forbidden regressions produce consistent estimates only under very restrictive assumptions which rarely hold in practice (see for instance Wooldridge (2010) &quot;Econometric Analysis of Cross Section an Panel Data&quot;, p. 265-268).&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem is that neither the conditional expectations operator nor the linear projection carry through nonlinear functions. For this reason only an OLS regression in the first stage is guaranteed to produce fitted values that are uncorrelated with the residuals. A proof for this can be found in Greene (2008) &quot;Econometric Analysis&quot; or, if you want a more detailed (but also more technical) proof, you can have a look at the &lt;a href=&quot;http://graduateinstitute.ch/files/live/sites/iheid/files/sites/international_economics/shared/international_economics/prof_websites/arcand/courses/notes_3.pdf&quot;&gt;notes&lt;/a&gt; by Jean-Louis Arcand on p. 47 to 52.&lt;/p&gt;&#10;&#10;&lt;p&gt;For the same reason as in the forbidden regression this seemingly obvious two-step procedure of mimicking 2SLS with probit will not produce consistent estimates. This is again because expectations and linear projections do not carry over through nonlinear functions. Wooldridge (2010) in section 15.7.3 on page 594 provides a detailed explanation for this. He also explains the proper procedure of estimating probit models with a binary endogenous variable. The correct approach is to use maximum likelihood but doing this by hand is not exactly trivial. Therefore it is preferable if you have access to some statistical software which has a ready-canned package for this. For example, the Stata command would be &lt;code&gt;ivprobit&lt;/code&gt; (see the Stata &lt;a href=&quot;http://www.stata.com/manuals13/rivprobit.pdf&quot;&gt;manual&lt;/a&gt; for this command which also explains the maximum likelihood approach).&lt;/p&gt;&#10;&#10;&lt;p&gt;If you require references for the theory behind probit with instrumental variables see for instance:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Newey, W. (1987) &quot;Efficient estimation of limited dependent variable models with endogenous explanatory variables&quot;, Journal of Econometrics, Vol. 36, pp. 231-250&lt;/li&gt;&#10;&lt;li&gt;Rivers, D. and Vuong, Q.H. (1988) &quot;Limited information estimators and exogeneity tests for simultaneous probit models&quot;, Journal of Econometrics, Vol. 39, pp. 347-366&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Finally, combining different estimation methods in the first and second stages is difficult unless there exists a theoretical foundation which justifies their use. This is not to say that it is not feasible though. For instance, &lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0927539808000388&quot;&gt;Adams et al. (2009)&lt;/a&gt; use a three-step procedure where they have a probit &quot;first stage&quot; and an OLS second stage without falling for the forbidden regression problem. Their general approach is:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;use probit to regress the endogenous variable on the instrument(s) and exogenous variables&lt;/li&gt;&#10;&lt;li&gt;use the predicted values from the previous step in an OLS first stage together with the exogenous (but without the instrumental) variables&lt;/li&gt;&#10;&lt;li&gt;do the second stage as usual&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;A similar procedure was employed by a user on the Statalist who wanted to use a Tobit first-stage and a Poisson second stage (see &lt;a href=&quot;http://www.stata.com/statalist/archive/2012-03/msg00017.html&quot;&gt;here&lt;/a&gt;). The same fix should be feasible for your estimation problem.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-19T20:40:35.013" Id="94392" LastActivityDate="2014-04-20T14:26:59.887" LastEditDate="2014-04-20T14:26:59.887" LastEditorUserId="26338" OwnerUserId="26338" ParentId="94063" PostTypeId="2" Score="7" />
  <row AnswerCount="0" Body="&lt;p&gt;I read an article that says the dependent variables in a regression model must be normally distributed. The way i understand it, is that the observations for the regression model must then be normally distributed. Or in other words if i choose sample data from a population then the sample must be normally distributed. But then i read answers to a similar question in another forum and apparently the the observations or dependent variables does not need to be normally distributed, just the error terms after the model is implemented must be normally distributed. &lt;/p&gt;&#10;&#10;&lt;p&gt;So now i am not sure, does the observations (dependent variables) has to be normally distributed as well as the error terms or is it just the error terms.&lt;/p&gt;&#10;" ClosedDate="2014-04-20T13:20:36.807" CommentCount="5" CreationDate="2014-04-19T21:10:32.610" Id="94395" LastActivityDate="2014-04-19T21:10:32.610" OwnerUserId="44023" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;normal-distribution&gt;&lt;econometrics&gt;" Title="Non-normal observations in regression modelling" ViewCount="48" />
  <row Body="&lt;p&gt;I agree with Mike's answer: You need to know how you want to use the data you're collecting. If you're trying to analyze daily market data you should try to get as much data at that frequency as you can because munging data together with different frequencies is a challenge. The same goes for weekly, monthly, etc.&lt;/p&gt;&#10;&#10;&lt;p&gt;Having said that, you could take your weekly data and calculate a 4 wk moving average so it will line up with your monthly data, but your quarterly GDP data is going to have even fewer observations so you need to know how you want to handle that circumstance. You could fill in the dataset with current GDP data so you have 3 months of repeating GDP values.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are other techniques I guess you could use (interpolation, etc.), but ultimately it goes back to making sure you have a clear understanding of how you want to make this analysis work for you.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hope this helps.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-04-19T22:16:57.037" Id="94403" LastActivityDate="2014-04-19T22:16:57.037" OwnerUserId="24001" ParentId="94215" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Unfortunately, I don't think it works that way.  If you want to hold the familywise error rate to $.05$, then you need to adjust for more than $4$ comparisons.  However, you can use a 'stepdown' (a.k.a., Holm's) procedure to gain some power back.  Here's how it works:  &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Conduct an ANOVA over all $8$ groups.  If this is significant, there is at least $1$ false null.  Thus, you don't need to control for $16$ contrasts.  The most extreme case is that $7$ of the groups have identical means, but $1$ group differs from the rest.  Since each group is involved in $4$ contrasts, this implies that a significant overall ANOVA means there can be at most $12$ true nulls among your $16$ contrasts.  &lt;/li&gt;&#10;&lt;li&gt;Run all of your contrasts and sort them into ascending order by $p$-value.  You can begin assessing the first $5$ (lowest) $p$-values to see if they are lower than your adjusted $\alpha$ ($\alpha_{\rm adj}$, which is your nominal $\alpha/12$).  You can call any of the $5$ most significant contrasts that have $p &amp;lt; \alpha_{\rm adj}$ significant.  &lt;/li&gt;&#10;&lt;li&gt;If $5$ contrasts have been declared significant by this procedure, then there can be at most $11$ true nulls among the remaining set.  Thus, your new $\alpha_{\rm adj}$ would be $\alpha/11$, which is a slightly easier threshold to cross, giving you slightly more power.  It the sixth lowest $p$-value is less than this threshold, you call it significant, and conclude that there can be at most $10$ true nulls among your set of remaining contrasts.  &lt;/li&gt;&#10;&lt;li&gt;So your next lowest $p$-value is compared to $\alpha_{\rm adj} = \alpha/10$.  Etc.  &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;At any point, if the next lowest $p$-value is $&amp;gt; \alpha_{\rm adj}$, you stop.  &lt;/p&gt;&#10;&#10;&lt;p&gt;This is true even if you need to stop within the first $4$ contrasts.  In which case logically, either the ANOVA was a &lt;a href=&quot;http://en.wikipedia.org/wiki/Type_I_and_type_II_errors&quot; rel=&quot;nofollow&quot;&gt;type I error&lt;/a&gt;, or there are some type II errors amongst your non-significant contrasts.  Nonetheless, the status of the remaining contrasts is ambiguous.  Remember that the fact a test is non-significant does not mean there is no difference, only that you cannot tell with the desired level of confidence.  (For more on this, it may help to read my answer here: &lt;a href=&quot;http://stats.stackexchange.com/a/85914/7290&quot;&gt;Why do statisticians say a non-significant result means &quot;you cannot reject the null&quot; as opposed to accepting the null hypothesis?&lt;/a&gt;)&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-04-20T00:48:35.600" Id="94409" LastActivityDate="2014-04-20T15:59:12.537" LastEditDate="2014-04-20T15:59:12.537" LastEditorUserId="7290" OwnerUserId="7290" ParentId="94377" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;Levene's test is one of several tests that is essentially an ANOVA but using a response that converts the spread of the values into locations. In the case of Levene, the observations put into the ANOVA are:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$Z_{ij} = |Y_{ij} - \bar{Y}_{i\cdot}|, \text{ where } \bar{Y}_{i\cdot} \mbox{ is the mean of the }i^\text{th} \text{ group } $$&lt;/p&gt;&#10;&#10;&lt;p&gt;which are the absolute deviations from their own sample means, for which larger spread on the $Y_{ij}$ would give typically larger $Z_{ij}$. Consequently, a difference in mean $Z$s implies a difference in spread on the $Y$s.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/87LE9.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In ANOVA, if you have $k$ groups and $N$ observations, the d.f. for the overall F-test for differences in mean has $(k-1,N-k)$ df.&lt;/p&gt;&#10;&#10;&lt;p&gt;Levene's test simply uses the df of the ANOVA it applies to the $Z$ values.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-20T02:49:14.790" Id="94413" LastActivityDate="2014-04-20T03:22:47.837" LastEditDate="2014-04-20T03:22:47.837" LastEditorUserId="805" OwnerUserId="805" ParentId="94412" PostTypeId="2" Score="4" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a question about Arellano-Bond model in Stata (xtabond/xtabond2). The slopes I get, are they for levels or differences of values? My model to be estimated has a form of (D is first difference):&lt;/p&gt;&#10;&#10;&lt;p&gt;DY=a+DX1+DX2+....&lt;/p&gt;&#10;&#10;&lt;p&gt;So should I use already differentiated variables in my analysis or levels variables to get this? I know A-B first-differiantes everything at first, so if I use already differentiated data, are the slopes for differences of differences? In most examples levels variables are used.&lt;/p&gt;&#10;&#10;&lt;p&gt;Risto from Finland&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-20T04:08:50.587" Id="94414" LastActivityDate="2014-04-20T04:08:50.587" OwnerUserId="44113" PostTypeId="1" Score="1" Tags="&lt;dynamic-regression&gt;" Title="Arellano-Bond estimator in Stata" ViewCount="110" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I am working on a project where I have to do multi-label text classification. I want to understand that whether my approach is correct or I am missing something. I am using R to do it.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;ol&gt;&#10;  &lt;li&gt;Clean the text&lt;/li&gt;&#10;  &lt;li&gt;Create a corpus. While creating  the corpus I am  removing the sparse term with   sparse value set to .86&lt;/li&gt;&#10;  &lt;li&gt;Create a DTM from this corpus and attach the label to it.&lt;/li&gt;&#10;  &lt;li&gt;Divide the DTM into training and test set&lt;/li&gt;&#10;  &lt;li&gt;Build the model using the train set&lt;/li&gt;&#10;  &lt;li&gt;Test the model using the test set.&lt;/li&gt;&#10;  &lt;li&gt;Do statistical analysis to prove the model&lt;/li&gt;&#10;  &lt;/ol&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;So my questions are&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;ol&gt;&#10;  &lt;li&gt;Do we have to remove sparse term while creating the corpus and if so does the sparse value is too high and is there any range of sparse&#10;  value to be set in text classification or it depends on the result we&#10;  get.&lt;/li&gt;&#10;  &lt;li&gt;Is it a thumb rule that the number of rows should be more than the square of the number of features. And also does number of features depend on the number of unique classes you have&lt;/li&gt;&#10;  &lt;li&gt;Can we do feature selection once we do sparse term removal.&lt;/li&gt;&#10;  &lt;li&gt;Which one gives good result dimension reduction or feature selection.&lt;/li&gt;&#10;  &lt;/ol&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="0" CreationDate="2014-04-20T12:45:10.840" Id="94434" LastActivityDate="2014-04-20T12:54:19.017" OwnerUserId="43328" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;machine-learning&gt;&lt;classification&gt;&lt;multilevel-analysis&gt;" Title="Some Basic things we need to do when we are doing text classification" ViewCount="53" />
  
  
  
  <row Body="&lt;p&gt;You can simulate a variety of time series (VAR, VARX, VARMA, state space...) with the function &lt;code&gt;simulate&lt;/code&gt; from the R-package &lt;code&gt;dse&lt;/code&gt;. &lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-04-20T14:40:46.143" Id="94449" LastActivityDate="2014-04-20T14:40:46.143" OwnerUserId="25944" ParentId="94443" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;This is an old question, but someone may be looking for a quick answer (this is in R, but quite quick). &#10;The package &lt;a href=&quot;http://www3.nd.edu/~kkelley/site/MBESS.html&quot; rel=&quot;nofollow&quot;&gt;MBESS&lt;/a&gt; provides a straightforward conversion tool. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;install.packages(&quot;MBESS&quot;)&#10;library(MBESS)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;e.g. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ci.smd(smd=.69,n.1=X, n.2=Y) &#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-04-20T15:14:32.370" Id="94451" LastActivityDate="2014-04-20T15:14:32.370" OwnerUserId="16282" ParentId="8487" PostTypeId="2" Score="3" />
  
  
  <row AcceptedAnswerId="94475" AnswerCount="1" Body="&lt;p&gt;I need to correlate employee engagement (gathered data using the 9 item UWES questionnaire) and organizational commitment (gathered data using the 18 item Organizational Commitment Scale).&lt;/p&gt;&#10;&#10;&lt;p&gt;The both of them can be divided into different components;&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;UWES - &lt;code&gt;Vigour&lt;/code&gt; (3-items in questionnaire), &lt;code&gt;Dedication&lt;/code&gt; (3 items in questionnaire) &amp;amp; &lt;code&gt;Commitment&lt;/code&gt; (3 items in questionnaire).&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;OCS - &lt;code&gt;Affective Commitment&lt;/code&gt; (6 items in questionnaire), &lt;code&gt;Continuance Commitment&lt;/code&gt; (6 items in questionnaire) &amp;amp; &lt;code&gt;Normative Commitment&lt;/code&gt; (6 items in questionnaire).&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I have an R-matrix set up correlating all the individual questions from both questionnaires, using Pearson's &lt;em&gt;r&lt;/em&gt;. However I would like to correlate the components, and not individual items from the questionnaires.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ZjT4U.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;What would you recommend I use?&#10;Thanks!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-20T18:43:28.497" Id="94470" LastActivityDate="2014-04-20T20:08:40.367" LastEditDate="2014-04-20T18:49:19.653" LastEditorUserId="32036" OwnerUserId="44146" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;correlation&gt;&lt;factor-analysis&gt;&lt;pearson&gt;" Title="Correlating two questionnaires with grouped items" ViewCount="91" />
  <row Body="&lt;p&gt;thank you for everyone comments and answers. i have run another simulation  using sample = n (from 3 to 50), repeating 1000 times and plot the result here (the population is also normal distribution). I think i can now understand why z distribution can replace t distribution when sample size get large. &lt;/p&gt;&#10;&#10;&lt;p&gt;I believe there will be some curious guy having similar question as mine in the future. I have attached my R code here so that &quot;he&quot; can play around with my code if interested.  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;https://www.flickr.com/photos/118085696@N08/13952940654/&quot; rel=&quot;nofollow&quot;&gt;https://www.flickr.com/photos/118085696@N08/13952940654/&lt;/a&gt; &#10;(i dont like flickr because its rule doesn't allow me to give out.gif link out directly. but at least it wont delete my graph within a short time)&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;is.between &amp;lt;- function(x, a, b) {&#10;  x &amp;gt; a &amp;amp; x &amp;lt; b&#10;}&#10;set.seed(09101165)&#10;population&amp;lt;-rnorm(n=10000, mean=100, sd =10)&#10;##population&amp;lt;-runif(n=10000, min=50,max=100)&#10;## you can use runif() instead of rnorm for generating a evenly distributed population&#10;mean_of_population&amp;lt;-mean(population)&#10;var(population)&#10;## var = 102.5493&#10;master_of_sample&amp;lt;-list()&#10;for (ss in 3:50){&#10;  group_of_sample&amp;lt;-list()&#10;  for (i in 1:1000){&#10;    group_of_sample[[i]]&amp;lt;-sample(population,size=ss)&#10;  }&#10;  master_of_sample[[ss]]&amp;lt;-group_of_sample&#10;}&#10;Mean&amp;lt;-vector()&#10;Variance&amp;lt;-vector()&#10;upper_interval&amp;lt;-vector()&#10;lower_interval&amp;lt;-vector()&#10;upper_interval2&amp;lt;-vector()&#10;lower_interval2&amp;lt;-vector()&#10;within_confidence_interval&amp;lt;-logical()&#10;within_confidence_interval2&amp;lt;-logical()&#10;finalt&amp;lt;-vector()&#10;finalz&amp;lt;-vector()&#10;&#10;for (ss in 3:50){&#10;  Mean&amp;lt;-vector()&#10;  Variance&amp;lt;-vector()&#10;  upper_interval&amp;lt;-vector()&#10;  lower_interval&amp;lt;-vector()&#10;  within_confidence_interval&amp;lt;-logical()&#10;  for (i in 1:1000){Mean[i]&amp;lt;-mean(master_of_sample[[ss]][[i]])&#10;                    Variance[i]&amp;lt;-var(master_of_sample[[ss]][[i]])&#10;                    upper_interval[i]&amp;lt;-Mean[i]+qt(0.975,df=(ss-1))*(sqrt(Variance[i]/ss))&#10;                    lower_interval[i]&amp;lt;-Mean[i]-qt(0.975,df=(ss-1))*(sqrt(Variance[i]/ss))&#10;                    upper_interval2[i]&amp;lt;-Mean[i]+qnorm(0.975)*(sqrt(Variance[i]/ss))&#10;                    lower_interval2[i]&amp;lt;-Mean[i]-qnorm(0.975)*(sqrt(Variance[i]/ss))&#10;                    within_confidence_interval[i]&amp;lt;-is.between(mean_of_population,lower_interval[i],upper_interval[i])&#10;                    within_confidence_interval2[i]&amp;lt;-is.between(mean_of_population,lower_interval2[i],upper_interval2[i])&#10; }&#10;  finalt[ss]&amp;lt;-sum(within_confidence_interval)/length(within_confidence_interval)&#10;  finalz[ss]&amp;lt;-sum(within_confidence_interval2)/length(within_confidence_interval2)&#10;}&#10;&#10;plot(y=finalt[3:50], x=3:50,ylim=c(0.8,1),type=&quot;n&quot;, ylab=&quot;population mean lying within sample's confidence interval (proportion)&quot;, xlab=&quot;sample size&quot;, &#10;     main=&quot;efficiency of calculating confidence interval using z and t distribution for unknown population variance&quot; )&#10;points(y=finalt[3:50], x=3:50,col=&quot;green&quot;,pch=3)&#10;fort&amp;lt;-smooth.spline(finalt[3:50]~3:50,df=3)&#10;lines(fort,col=&quot;green&quot;)&#10;points(y=finalz[3:50], x=3:50,col=&quot;blue&quot;,pch=2 )&#10;forz&amp;lt;-smooth.spline(finalz[3:50]~3:50,df=3)&#10;lines(forz,col=&quot;blue&quot;)&#10;legend(&quot;topleft&quot;,legend=c(&quot;using t distribution&quot;,&quot;using z distribution&quot;),pch=c(3,2),col=c(&quot;green&quot;,&quot;blue&quot;))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2014-04-20T18:51:53.080" Id="94471" LastActivityDate="2014-04-21T14:19:52.097" LastEditDate="2014-04-21T14:19:52.097" LastEditorUserId="44126" OwnerUserId="44126" ParentId="94425" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Depends on how much (many?) data you have. If your sample is rather small, you'll probably need to settle for the classical test theory (CTT) assumptions. I've reviewed these in a few other answers to:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://stats.stackexchange.com/q/92724/32036&quot;&gt;Correlational study or ordinal data using 5-point Likert scale&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://stats.stackexchange.com/q/83512/32036&quot;&gt;Regression testing after dimension reduction&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Basically, you'd treat your ordinal data as numeric, take the sum or average of individuals' responses to items intended to measure the same latent factor (which you refer to as a component), and use that as your score for the individual on that factor. Repeat for the other factors and proceed to calculate your correlations among the scores, if that's all you're really after. &lt;a href=&quot;http://www.beanmanaged.com/doc/pdf/arnoldbakker/articles/articles_arnold_bakker_87.pdf&quot; rel=&quot;nofollow&quot;&gt;Schaufeli and Bakker &lt;sup&gt;(2003)&lt;/sup&gt;&lt;/a&gt; seem to describe similar procedures for the 9-item UWES (but I've just skimmed it TBH), so this may be the best approach despite the assumptions it makes, because it would probably make your measurement model maximally comparable to that used in other studies with this measure.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you have plenty of data, you might consider other methods of estimating factor scores like I've discussed on yet another answer to &quot;&lt;a href=&quot;http://stats.stackexchange.com/q/2374/32036&quot;&gt;Factor analysis of questionnaires composed of Likert items&lt;/a&gt;&quot;. chl's answer there is well worth considering too, since all sorts of response bias can affect Likert ratings. There's a surprising amount of correction you can perform with enough data.&lt;/p&gt;&#10;&#10;&lt;p&gt;Bias corrections aside, you could implement the factor analytic approach by calculating a polychoric correlation matrix of your 27 items. Feed that into a confirmatory factor analysis, and estimate correlations among the latent factors. Getting accurate estimates will require a lot of data, but these estimates won't involve as many assumptions as the CTT approach. They'll accommodate the ordinal nature of Likert data, estimate factor scores using only common variance among items, estimate how much unique variance each item has that doesn't belong to the intended factors, and estimate how much item covariance is explained by the model. That's a lot of extra info, if you have the data for it.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;sup&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;br&gt;&#10;Schaufeli, W. B., &amp;amp; Bakker, A. B. (2003). &lt;em&gt;Utrecht work engagement scale: Preliminary manual&lt;/em&gt;. Occupational Health Psychology Unit, Utrecht University, Utrecht.&lt;/sup&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-20T20:08:40.367" Id="94475" LastActivityDate="2014-04-20T20:08:40.367" OwnerUserId="32036" ParentId="94470" PostTypeId="2" Score="0" />
  <row AnswerCount="2" Body="&lt;p&gt;I am working to analyze poverty rate using census data. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have a huge dataset. I want to extract the likelihood from this dataset in order to create patterns for energy consumption.&lt;/p&gt;&#10;&#10;&lt;p&gt;What is the best approach to tackle this problem?&lt;/p&gt;&#10;" ClosedDate="2014-04-21T11:50:32.413" CommentCount="6" CreationDate="2014-04-20T20:15:32.883" Id="94477" LastActivityDate="2014-04-21T12:18:28.110" LastEditDate="2014-04-21T12:18:28.110" LastEditorUserId="43653" OwnerUserId="43653" PostTypeId="1" Score="-1" Tags="&lt;classification&gt;&lt;clustering&gt;&lt;likelihood&gt;" Title="How do we analyse likelihood in a dataset?" ViewCount="64" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;This is kind of a follow-up question from this post: &lt;a href=&quot;http://stats.stackexchange.com/questions/25949/gradient-descent-vs-lm-function-in-r&quot;&gt;Gradient descent vs lm() function in R?&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there any literature available for the QR decomposition concept involved in the &lt;code&gt;lm()&lt;/code&gt; function in R?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-04-21T00:40:33.317" Id="94496" LastActivityDate="2014-04-21T02:38:53.483" LastEditDate="2014-04-21T00:59:02.887" LastEditorUserId="32036" OwnerUserId="44155" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;regression&gt;" Title="lm() function in R" ViewCount="50" />
  
  
  <row Body="&lt;p&gt;The covariance will be $\sigma^2$. I will elaborate: &lt;/p&gt;&#10;&#10;&lt;p&gt;$\hat{y}_i = \hat{\beta}_0 +\hat{\beta}_1x_1$ and $y_i = \beta_0+\beta_1 x_1 +\varepsilon_i$, suppose $\varepsilon_i \sim N(0,\sigma^2)$. &lt;/p&gt;&#10;&#10;&lt;p&gt;The residual: $$e_i = y_i-\hat{y}_i = \beta_0+\beta_1 x_1 +\varepsilon_i - (\hat{\beta}_0 +\hat{\beta}_1x_1) = (\beta_0 - \hat{\beta}_0) + (\beta_1-\hat{\beta}_1)x_1 + \varepsilon_i$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Then:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\text{Cov}[e_i, \varepsilon_i] = \text{Cov}[(\beta_0 - \hat{\beta}_0) + (\beta_1-\hat{\beta}_1)x_1 + \varepsilon_i, \varepsilon_i] = \text{Cov}[\varepsilon_i, \varepsilon_i]=\text{Var}[\varepsilon_i]=\sigma^2$$&lt;/p&gt;&#10;&#10;&lt;p&gt;See full list of covariance properties:&#10;&lt;a href=&quot;http://en.wikipedia.org/wiki/Covariance&quot; rel=&quot;nofollow&quot;&gt;covariance&lt;/a&gt; and &lt;a href=&quot;http://en.wikipedia.org/wiki/Variance&quot; rel=&quot;nofollow&quot;&gt;variance&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-04-21T03:19:09.603" Id="94506" LastActivityDate="2014-04-21T04:20:40.550" LastEditDate="2014-04-21T04:20:40.550" LastEditorUserId="17661" OwnerUserId="17661" ParentId="94504" PostTypeId="2" Score="4" />
  
  <row AcceptedAnswerId="94518" AnswerCount="1" Body="&lt;p&gt;What is the difference between Neural network, Bayesian network, Decision tree and Petri Nets eventhough they are all graphical models and visually depict cause-effect relationship. Thank you&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-21T04:16:02.420" FavoriteCount="1" Id="94511" LastActivityDate="2014-04-21T17:13:12.510" LastEditDate="2014-04-21T17:13:12.510" LastEditorUserId="21961" OwnerUserId="21961" PostTypeId="1" Score="2" Tags="&lt;machine-learning&gt;&lt;neural-networks&gt;&lt;bayes-network&gt;&lt;fuzzy&gt;" Title="Difference between Bayes network, neural network, Petri Nets and decision tree" ViewCount="1326" />
  <row AnswerCount="0" Body="&lt;p&gt;Prediction is the ability to statistically foretell the occurrence of future events by learning from historical data.&lt;/p&gt;&#10;&#10;&lt;p&gt;In all the cases, if we have a large enough sample of data on how people behaved in the past, we can predict their behaviors with a reasonable degree of confidence. But, how can we predict events that are random and rare.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am working on study to predict earth-quack. I am doing this study with a limited resources, so I don't have enough data about geology. I want to model this problem statistically based on what happened before. &lt;/p&gt;&#10;&#10;&lt;p&gt;As first approach, I was thinking to use poisson model, so I count earth-quack that happened in specific place, calculate the next events using this model, how can we add a trick to make sure that this event would happen. &lt;/p&gt;&#10;&#10;&lt;p&gt;My questions are: &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;Is it good approach to use counting model to tackle this problem ?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Can you, please, suggest any other effective ways to tackle this problem ?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2014-04-21T08:56:25.250" Id="94532" LastActivityDate="2014-04-21T10:07:39.230" LastEditDate="2014-04-21T10:07:39.230" LastEditorUserId="43653" OwnerUserId="43653" PostTypeId="1" Score="1" Tags="&lt;predictive-models&gt;&lt;prediction&gt;" Title="How can we predict random non-balanced events?" ViewCount="34" />
  <row AcceptedAnswerId="94549" AnswerCount="1" Body="&lt;p&gt;Hoping you can help me understand the probability density function for the exponential distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;Given that the distribution's PDF is described as follows when x &gt; 0:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\lambda e^{-\lambda x}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Why do we multiply by ? Sure I'm missing something basic here, but seems that if we're modeling time to event, using e and the exponents would be enough. Is it because we might expect the event to happen multiple times in the time interval?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you in advance.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-21T11:45:02.777" Id="94548" LastActivityDate="2014-04-21T11:47:57.657" OwnerUserId="44182" PostTypeId="1" Score="3" Tags="&lt;distributions&gt;&lt;exponential&gt;" Title="Exponential Distribution PDF: Why multiply by lambda?" ViewCount="143" />
  <row Body="&lt;p&gt;Eventually, you could also run a quantile regression in order to understand what happens at some specific quantiles. This should help to better understand the phenomenon you are studying.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-04-21T11:52:45.250" Id="94551" LastActivityDate="2014-04-21T11:52:45.250" OwnerUserId="44171" ParentId="94542" PostTypeId="2" Score="1" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I am confused and struggling to read this simple crosstabs. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/NNdbMsP.jpg&quot; alt=&quot;Crosstabs&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I am trying to create a profile of downloaders. One of the demographics I am looking at is age. Should I be looking at the percentage within Age or percentage within Type of downloader?&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, Mixed downloaders are most likely to be 18-21 (50%)? Or should it be they are most likely to be 18-21 (68.4%)??&lt;/p&gt;&#10;&#10;&lt;p&gt;Could someone help explain please&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-21T16:37:32.333" Id="94573" LastActivityDate="2014-04-21T18:00:31.647" OwnerUserId="41534" PostTypeId="1" Score="0" Tags="&lt;spss&gt;&lt;descriptive-statistics&gt;" Title="Reading a crosstabs" ViewCount="32" />
  <row Body="&lt;p&gt;Since we generally treat age as a stratifying variable, we would look at age standardized proportions. That's because, comparing like-to-like in usage habits, we expect great variation in usage between age groups. Indeed we see that. The &quot;% within age&quot; proportions are the values you're after.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-21T16:52:41.357" Id="94576" LastActivityDate="2014-04-21T18:00:31.647" LastEditDate="2014-04-21T18:00:31.647" LastEditorUserId="8013" OwnerUserId="8013" ParentId="94573" PostTypeId="2" Score="1" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;A friend of mine needs to estimate a non-linear GMM on Panel data. As I have checked, the softwares for Panel GMM only estimate linear forms (STATA gmm, xtabond, ...; R pgmm from plm package). How can I do nonlinear Dynamic Panel GMM estimation?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-04-21T17:47:26.290" Id="94580" LastActivityDate="2014-12-01T10:34:23.167" OwnerUserId="44198" PostTypeId="1" Score="2" Tags="&lt;panel-data&gt;&lt;nonlinear&gt;&lt;generalized-moments&gt;&lt;dynamic-regression&gt;" Title="Nonlinear GMM for Dynamic Panel Data" ViewCount="149" />
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Stata allows for fixed effects and random effects specification of the logistic regression through the &lt;code&gt;xtlogit fe&lt;/code&gt; and &lt;code&gt;xtlogit re&lt;/code&gt; commands accordingly. I was wondering what are the equivalent commands for these specifications in R.&lt;/p&gt;&#10;&#10;&lt;p&gt;The only similar specification I am aware of is the mixed effects logistic regression &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; mymixedlogit &amp;lt;- glmer(y ~ x1 + x2 +  x3 + (1 | x4), data = d, family = binomial)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;but I am not sure whether this maps to any of the aforementioned commands.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-04-22T00:23:29.177" Id="94619" LastActivityDate="2014-04-22T00:49:57.143" LastEditDate="2014-04-22T00:49:57.143" LastEditorUserId="7290" OwnerUserId="44223" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;logistic&gt;&lt;stata&gt;&lt;random-effects-model&gt;&lt;fixed-effects-model&gt;" Title="Stata's xtlogit (fe, re) equivalent in R?" ViewCount="160" />
  <row Body="&lt;p&gt;Yes, it is, as long as x is successfully being drawn as intended. &lt;/p&gt;&#10;&#10;&lt;p&gt;2000 is quite a low number of MC draws to use.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can test it by doing a numerical integration in Excel or checking the Gamma cdf by some other means.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-22T02:20:50.470" Id="94629" LastActivityDate="2014-04-22T02:20:50.470" OwnerUserId="44232" ParentId="94212" PostTypeId="2" Score="0" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I would like to understand if there exists any method to find confidence interval for the parameters of inverse gamma distribution. Thanks in advance!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-04-22T07:26:08.200" Id="94647" LastActivityDate="2014-04-22T07:26:08.200" OwnerUserId="44241" PostTypeId="1" Score="1" Tags="&lt;confidence-interval&gt;&lt;inverse-gamma&gt;" Title="Confidence Interval for Inverse Gamma Distribution" ViewCount="62" />
  
  <row AcceptedAnswerId="94732" AnswerCount="2" Body="&lt;p&gt;Suppose we done K-Means and got K centroids of clusters and we want to tag new points based on those K centroids. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;UPDATE:&lt;/strong&gt; These K centroids are given to me, so I can't go for another clustering algorithm. Also I have a large number of 2d points and a big k, so it's very important to get appropriate complexity.&lt;/p&gt;&#10;&#10;&lt;p&gt;The solution came into my mind is to do:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;For every p in newPoints do:&#10;    For every c in centroids do:&#10;        calculate distance between c and p&#10;        if distance &amp;lt; minDistance&#10;            minDistance = distance&#10;            p.tag = c.tag&#10;    end for&#10;end for&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;But the complexity of this solution is O(K*N) where N is the number of new points.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to know if there is a solution with less complexity.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-04-22T07:35:06.957" Id="94648" LastActivityDate="2014-04-22T17:43:08.353" LastEditDate="2014-04-22T11:26:13.077" LastEditorUserId="38012" OwnerUserId="38012" PostTypeId="1" Score="3" Tags="&lt;k-means&gt;&lt;discriminant&gt;" Title="Is there an efficient way to discriminate space based on K-Means results?" ViewCount="50" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am carrying out an exploratory regression looking at the determinants of corporate governance. The final model is: Board independence= Country Dummies+ Democracy level+ Sector dummy+ log GDP+ logMarketCapitalisation&#10;The dependant variable for board independence is a fraction, so going by this ( &lt;a href=&quot;http://www.ats.ucla.edu/stat/stata/faq/proportion.htm&quot; rel=&quot;nofollow&quot;&gt;http://www.ats.ucla.edu/stat/stata/faq/proportion.htm&lt;/a&gt; )I'm using a GLM regression; however it won't give a measure of goodness of fit, which is needed for an exploratory regression. What am I doing wrong?&#10;Thanks in advance&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-22T14:38:26.113" FavoriteCount="0" Id="94706" LastActivityDate="2014-04-22T14:38:26.113" OwnerUserId="44265" PostTypeId="1" Score="0" Tags="&lt;generalized-linear-model&gt;&lt;goodness-of-fit&gt;&lt;proportion&gt;" Title="I'm in over my head, need help on specification" ViewCount="22" />
  <row AnswerCount="1" Body="&lt;p&gt;What is a simple description of how the GEE algorithm works? How exactly does the GEE process come up with the final estimates of the parameters?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-22T14:43:51.020" Id="94707" LastActivityDate="2014-05-11T18:00:53.517" LastEditDate="2014-05-11T18:00:53.517" LastEditorUserId="930" OwnerUserId="43282" PostTypeId="1" Score="1" Tags="&lt;algorithms&gt;&lt;gee&gt;" Title="GEE iteration process" ViewCount="60" />
  <row AnswerCount="2" Body="&lt;p&gt;Suppose we have a hierarchical model summarised by the following:&lt;/p&gt;&#10;&#10;&lt;p&gt;$y_{i} \sim N(\mu_{i}, \sigma^{2})$, for $i = 1, \ldots, n$; (For these purposes, assume $\sigma^{2}$ is known)&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\mu_{i} = \alpha + \beta x_{i}$ and each $x_{i}$ is a given predictor variable for $i = 1, \ldots, n$&lt;/p&gt;&#10;&#10;&lt;p&gt;Next,&lt;/p&gt;&#10;&#10;&lt;p&gt;$\alpha \sim N(\mu_{\alpha}, \sigma_{\alpha}^{2})$ and $\beta \sim N(\mu_{\beta}, \sigma_{\beta}^{2})$&lt;/p&gt;&#10;&#10;&lt;p&gt;Finally,&lt;/p&gt;&#10;&#10;&lt;p&gt;$p(\mu_{\alpha}, \mu_{\beta}, \sigma_{\alpha}^{2}, \sigma_{\beta}^{2}) \propto \frac{1}{\sigma^{2}_{\alpha}\sigma^{2}_{\beta}}$&lt;/p&gt;&#10;&#10;&lt;p&gt;How can this model be fitted in WinBUGS?&lt;/p&gt;&#10;&#10;&lt;p&gt;I know that the last two levels are given by:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;for (i in 1:N){&#10; y[i] ~ dnorm(mu[i], sigma.squared)&#10; mu[i] &amp;lt;- alpha + beta*x[i]&#10;}&#10;alpha ~ dnorm(mu.alpha, sigma.squared.alpha)&#10;beta ~ dnorm(mu.beta, sigma.squared.beta)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;However, I'm not sure, how the joint prior for $\mu_{\alpha}, \mu_{\beta}, \sigma_{\alpha}^{2}, \sigma_{\beta}^{2}$ should be incorporated into the model.&lt;/p&gt;&#10;&#10;&lt;p&gt;In particular, how is the hyper-prior distribution represented in the WinBUGS code, how can I achieve values for $\mu_{\alpha}$ and $\mu_{\beta}$ from this distribution and how should $\sigma_{\alpha}^{2}$ and $\sigma_{\beta}^{2}$ be chosen?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-22T15:05:56.177" Id="94712" LastActivityDate="2014-12-23T14:32:48.953" OwnerUserId="44267" PostTypeId="1" Score="4" Tags="&lt;bayesian&gt;&lt;hierarchical-bayesian&gt;&lt;winbugs&gt;" Title="Joint prior distributions in WinBUGS" ViewCount="140" />
  <row Body="&lt;p&gt;MA(1): your salary depends on the random mood swings of your boss in this and previous period, he has a short memory. If you look at the salary this period, it is clearly correlated with a salary last period, because they share the same random mood swing in last period. However, the last period salary is also correlated with a salary two periods ago, since they share the random mood swing two periods ago. This in fact makes the current salary correlated with a salary two periods ago, and with salaries in distant past. That's why MA(1) is represented with AR($\infty$). Think of it as an infinite chain, where the link is the error term in period t which connects the observations in period t+1 and t.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-04-22T15:54:43.517" Id="94715" LastActivityDate="2014-04-22T15:54:43.517" OwnerUserId="36041" ParentId="94700" PostTypeId="2" Score="4" />
  <row AcceptedAnswerId="94726" AnswerCount="1" Body="&lt;p&gt;Let's assume I have two models M1 and M2:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;M1:  y ~ x1 + x2 + x3&#10;M2:  y ~ x1 + x2 + x3 + x4&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Since variable x4 has some missing values the sample size of M2 is lower than sample size of M1: n2 &amp;lt; n1.&lt;br&gt;&#10;Is it still admissible to use AIC to compare models based on different sample sizes?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-22T16:09:52.290" Id="94718" LastActivityDate="2014-04-22T16:45:38.700" LastEditDate="2014-04-22T16:42:32.650" LastEditorUserId="17147" OwnerUserId="17147" PostTypeId="1" Score="0" Tags="&lt;model-selection&gt;&lt;aic&gt;&lt;model-comparison&gt;" Title="Model comparison with AIC based on different sample size" ViewCount="201" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Eight rooks are placed in distinct squares of an 8 x 8 chessboard, with all possible replacements being equally likely. Find the probability that all the rooks are safe from one another.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-04-22T17:45:36.173" Id="94733" LastActivityDate="2014-04-22T18:02:23.523" OwnerUserId="43055" PostTypeId="1" Score="0" Tags="&lt;probability&gt;&lt;self-study&gt;" Title="Probability problem regarding rooks on a chessboard" ViewCount="36" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have a dataset containing two columns and a total of 90 rows. The data is from my experiment where in the first column I have an integer representing the quantity, in the second column I have a percentage. A small example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Quantity   Percentage&#10;1          53%&#10;1          51%&#10;1          67%&#10;2          73%&#10;2          69%&#10;3          73%&#10;...        ...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;As you can see in both columns the numbers can occur more than once. Now I wish to plot this in a graph (I was thinking a scatter plot) in R. I just am a real beginner in using R and statistics so I was hoping someone can help me out how to get a good graph. If someone has an other suggestion that would give a better representation, shoot!&lt;/p&gt;&#10;&#10;&lt;p&gt;I just need to have a visual representation that shows the correlation between the two values.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-04-22T19:51:37.787" Id="94756" LastActivityDate="2014-04-23T01:50:29.490" LastEditDate="2014-04-22T20:11:31.167" LastEditorUserId="41211" OwnerUserId="41211" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;data-visualization&gt;" Title="Plot graph with more than one value on x and y-axis in R" ViewCount="261" />
  
  
  <row Body="&lt;p&gt;The big issue, I think, is that you need a model of the data to work with.  These could be regression models or simply your data fit to a distribution.  Your data could theoretically be consistent with a number of models / distributions (such as the normal distribution, proportions of successes arising from a binomial distribution with the appropriate total numbers of trials, or a beta distribution).  Since you say that these are fake data just to illustrate the problem, and that you have continuous proportions, I will simply go with the idea that these proportions come from a &lt;a href=&quot;http://en.wikipedia.org/wiki/Beta_distribution&quot; rel=&quot;nofollow&quot;&gt;beta distribution&lt;/a&gt;.  &lt;/p&gt;&#10;&#10;&lt;p&gt;In &lt;code&gt;R&lt;/code&gt;, you can model beta distributed data using the &lt;a href=&quot;http://cran.r-project.org/web/packages/betareg/index.html&quot; rel=&quot;nofollow&quot;&gt;betareg package&lt;/a&gt;.  You should read the vignette (&lt;a href=&quot;http://cran.r-project.org/web/packages/betareg/vignettes/betareg.pdf&quot; rel=&quot;nofollow&quot;&gt;pdf&lt;/a&gt;), which is very helpful for understanding the ideas about beta regression and how to use the package to do it.  Note in particular that they parameterize the beta distribution differently than the typical parameterization.  Instead of using $\alpha$ and $\beta$ (which are, e.g., the &lt;code&gt;shape1&lt;/code&gt; and &lt;code&gt;shape2&lt;/code&gt; arguments in the standard &lt;a href=&quot;http://stat.ethz.ch/R-manual/R-patched/library/stats/html/Beta.html&quot; rel=&quot;nofollow&quot;&gt;?pbeta&lt;/a&gt;, etc., functions), they use the mean proportion, $\mu$, and the precision, $\phi$, which are defined as follows:  &lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{align}&#10;\mu    &amp;amp;= \frac{\alpha}{(\alpha + \beta)}  &amp;amp;\text{and,}&amp;amp;  &amp;amp;\phi = \alpha + \beta \\&#10;\      &amp;amp;\                                  &amp;amp;\           &amp;amp; &amp;amp;\                     \\&#10;       &amp;amp;                                   &amp;amp;\text{thus,}&amp;amp; &amp;amp;                      \\&#10;\      &amp;amp;\                                  &amp;amp;\           &amp;amp; &amp;amp;\                     \\&#10;\alpha &amp;amp;= \mu\times\phi                    &amp;amp;\text{and,}&amp;amp;  &amp;amp;\beta = \phi - \alpha&#10;\end{align}  &lt;/p&gt;&#10;&#10;&lt;p&gt;We can use these equations to move between the two parameterizations and therefore the two sets of functions.  Then we can link the modeling in &lt;code&gt;betareg&lt;/code&gt; to the distribution and quantile functions.  The operative transformation will be based on an analogy to how &lt;a href=&quot;http://en.wikipedia.org/wiki/Qq-plot&quot; rel=&quot;nofollow&quot;&gt;qq-plots&lt;/a&gt; can be used to compare two distributions.  &lt;/p&gt;&#10;&#10;&lt;p&gt;First, let's model these data with &lt;code&gt;betareg&lt;/code&gt; and determine if the precision is constant between the groups:  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(betareg)&#10;library(lmtest)&#10;  # test of constant precision based on beta distribution:&#10;cp = betareg(x~f, df)&#10;vp = betareg(x~f|f, df)&#10;lrtest(cp, vp)&#10;# Likelihood ratio test&#10;# &#10;# Model 1: x ~ f&#10;# Model 2: x ~ f | f&#10;#   #Df LogLik Df  Chisq Pr(&amp;gt;Chisq)&#10;# 1   3 9.2136                     &#10;# 2   4 9.4793  1 0.5314      0.466&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;For these (fake) data, there is little reason to believe that the precision differs, but for your real data, they might.  So I will demonstrate the more complicated version, which you could simplify if you want.  At any rate, the next step is to determine the estimated $\alpha$ (&lt;code&gt;shape1&lt;/code&gt;) and $\beta$ (&lt;code&gt;shape2&lt;/code&gt;) parameters for &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt; by converting the model's coefficients to the distribution parameters (don't forget the link functions!) and using the above formulas to convert between the two parameterizations:  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;summary(vp)&#10;# ... &#10;# Coefficients (mean model with logit link):&#10;#             Estimate Std. Error z value Pr(&amp;gt;|z|)    &#10;# (Intercept)  -1.3153     0.2165  -6.074 1.25e-09 ***&#10;# fB            1.4260     0.3178   4.487 7.23e-06 ***&#10;#   &#10;# Phi coefficients (precision model with log link):&#10;#             Estimate Std. Error z value Pr(&amp;gt;|z|)    &#10;# (Intercept)   3.0094     0.5695   5.284 1.26e-07 ***&#10;# fB           -0.5848     0.7943  -0.736    0.462    &#10;# ... &#10;lo.to.prop = function(x){&#10;  o = exp(x)&#10;  p = o / (o+1)&#10;  return(p)&#10;}&#10;alpha.A = lo.to.prop(coef(vp)[1]            ) * exp(coef(vp)[3]            )&#10;alpha.B = lo.to.prop(coef(vp)[2]+coef(vp)[1]) * exp(coef(vp)[4]+coef(vp)[3])&#10;beta.A  = exp(       coef(vp)[3]            ) - alpha.A&#10;beta.B  = exp(       coef(vp)[4]+coef(vp)[3]) - alpha.B&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Due to the complexity of this, we might do a common-sense check on the final values.  (This also suggests another--simpler--way to get these parameters, if you don't need to model them and/or don't need to test if the precisions are the same.)  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;cbind(c(alpha.A, beta.A), c(alpha.B, beta.B))&#10;#                        [,1]     [,2]&#10;# (Intercept)        4.290073 5.960932&#10;# (phi)_(Intercept) 15.984533 5.336616&#10;&#10;  # parameterization for beta distribution:&#10;library(fitdistrplus)&#10;fitdist(with(df, x[f==&quot;A&quot;]), &quot;beta&quot;)&#10;# Fitting of the distribution ' beta ' by maximum likelihood &#10;# Parameters:&#10;#         estimate Std. Error&#10;# shape1  4.290883   2.389871&#10;# shape2 15.987505   9.295580&#10;fitdist(with(df, x[f==&quot;B&quot;]), &quot;beta&quot;)&#10;# Fitting of the distribution ' beta ' by maximum likelihood &#10;# Parameters:&#10;#        estimate Std. Error&#10;# shape1 5.960404   3.379447&#10;# shape2 5.335963   3.010541&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now, we convert the raw proportions in &lt;code&gt;B&lt;/code&gt; to their corresponding beta probability (that is, how far through their own beta distribution they lie), and convert those probabilities to quantiles of &lt;code&gt;A&lt;/code&gt;'s beta distribution:  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;p.B      = pbeta(df$x[df$f==&quot;B&quot;], shape1=alpha.B, shape2=beta.B)&#10;q.B.as.A = qbeta(p.B,             shape1=alpha.A, shape2=beta.A)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;For a final step, let's take a look at the new values:  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;cbind(df$x[df$f==&quot;B&quot;], q.B.as.A)&#10;#             q.B.as.A&#10;# [1,] 0.50 0.18502881&#10;# [2,] 0.30 0.08784683&#10;# [3,] 0.45 0.15784496&#10;# [4,] 0.60 0.24648412&#10;# [5,] 0.54 0.20838604&#10;# [6,] 0.77 0.38246863&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/au7tD.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-22T20:24:04.363" Id="94766" LastActivityDate="2014-06-30T20:10:36.147" LastEditDate="2014-06-30T20:10:36.147" LastEditorUserId="7290" OwnerUserId="7290" ParentId="94539" PostTypeId="2" Score="5" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;In a highly imbalanced binary classification (rare class &amp;lt; 10% of whole data), when I perform random sample selection (less than 15% of whole data to be selected for training) in a trial of 1000 times, there are some iterations whereby the selected samples only belong to the majority class, meaning that the training set is a single-class training set.&lt;/p&gt;&#10;&#10;&lt;p&gt;I don't use the stratified sampling, as in my problem it is considered to be cheating!&#10;Because sample selection should be considered as unsupervised action, due to the active learning nature of the problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;The question is&lt;/strong&gt; using which statistical test I can show that the presence of single-class training set (in 1000 trials) is significant when using random sampling? &lt;/p&gt;&#10;&#10;&lt;p&gt;I mean that, in 1000 trials of random sample selection over different datasets, I face between 10-150 single-class training set (for each dataset), and I want to test the significance of facing this situation for each dataset.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm thinking of formulating the problem as some kind of contingency table:&lt;/p&gt;&#10;&#10;&lt;p&gt;Expected outcome for 1000 iteration should be 1000 (double-class training), and 0 (Single-Class Training)&lt;/p&gt;&#10;&#10;&lt;p&gt;Observed outcome for 1000 iteration is: 1000-X (double-class training), and X (Single-Class Training)&lt;/p&gt;&#10;&#10;&lt;p&gt;Is this right?&#10;Am I going to the right direction?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-04-23T07:47:26.560" Id="94822" LastActivityDate="2014-04-23T07:47:26.560" OwnerUserId="18192" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;statistical-significance&gt;&lt;mathematical-statistics&gt;&lt;sampling&gt;&lt;unsupervised-learning&gt;" Title="Facing Single-Class Training Set when Using Random Sampling" ViewCount="51" />
  <row Body="&lt;p&gt;The Op started correctly in the Re-attempt, but then lost it, so I provide the analytical steps.&#10;By the definition of the natural logarithm and its base, we have&lt;/p&gt;&#10;&#10;&lt;p&gt;$$X_i = \frac 1{e^{-\ln X_i}} \Rightarrow \prod_{i=1}^{30} X_i= \frac 1{\exp{\{\sum_{i=1}^{30}(-\ln X_i)}\}}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;So&lt;/p&gt;&#10;&#10;&lt;p&gt;$$P(X_1 \cdot X_2 \cdot \ldots \cdot X_{30} \leq 1.85 \cdot 10^{-5}) = P\left(\frac 1{\exp{\{\sum_{i=1}^{30}(-\ln X_i)}\}} \leq 1.85 \cdot 10^{-5}\right)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$=P\left(\frac {10^5}{1.85}\leq \exp{\{\sum_{i=1}^{30}(-\ln X_i)}\} \right)=P\left(\ln\left(\frac {10^5}{1.85}\right)\leq \sum_{i=1}^{30}(-\ln X_i) \right)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$=1-P\left(\sum_{i=1}^{30}(-\ln X_i)\leq \ln\left(\frac {10^5}{1.85}\right) \right)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;which is essentially where the OP arrived. But then the CLT is not to be used for &lt;em&gt;one&lt;/em&gt; variable but for this &lt;em&gt;sum&lt;/em&gt; of variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;As already mentioned in the comments, this sum is the sum of i.i.d. exponential random variables, each having mean $1/3$ and variance $1/9$. So the sum, call it $S_n$, has mean and variance $E(S_n) = 30\frac 13 = 10$ and $\operatorname{Var}(S_n)= 30\frac 19 \Rightarrow \sigma_S = \frac {\sqrt{30}}{3}$&lt;/p&gt;&#10;&#10;&lt;p&gt;Then the normal approximation through the CLT tells us that $[S_n-E(S_n)]/\sigma_S \approx Z\sim N(0,1)$, and then numerical calculations.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-23T08:37:05.997" Id="94827" LastActivityDate="2014-04-23T08:37:05.997" OwnerUserId="28746" ParentId="94436" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Not one that will be of much use, unless the circumstances are very specific (known bounds on the values might help a bit).&lt;/p&gt;&#10;&#10;&lt;p&gt;To see why no formula will be much use in general:&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's imagine there's some formula that could be used.&lt;/p&gt;&#10;&#10;&lt;p&gt;Consider some set of values ($x_1,x_2,\ldots,x_{10},$ with average $\bar x$). I tell you $\bar x$, you have to figure out the range for $x$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Now consider $y_i=2x_i-\bar x$, which has the same average, but is &lt;em&gt;twice as spread&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now consider $z_i=100x_i -99\bar x$. 100 times as spread, same average.&lt;/p&gt;&#10;&#10;&lt;p&gt;So if I give you $\bar x$, do you have the values in $x$, $y$, or $z$? The bounds for $z$ will be useless for $x$ and vice-versa. (How can any any formula tell whether $\bar x$ is the mean of the first, second or third set of values?)&lt;/p&gt;&#10;&#10;&lt;p&gt;Is it really the case that you &lt;em&gt;only&lt;/em&gt; have access to the average? Or is there anything else you either know or can get?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-23T09:13:55.793" Id="94829" LastActivityDate="2014-04-23T09:25:13.960" LastEditDate="2014-04-23T09:25:13.960" LastEditorUserId="805" OwnerUserId="805" ParentId="94828" PostTypeId="2" Score="3" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;For my research on Mental health problems and correlates, child-parent relationship have identified as a correlate. Child-parent relationship is planned to measure with 6 questions and each question have five point scale of answers&#10;Example-&lt;/p&gt;&#10;&#10;&lt;p&gt;question    Strongly agree      Agree   Neither agree/disagree   disagree    Strongly disagree&lt;br&gt;&#10;1&#10;2&#10;3&#10;4&#10;5&lt;/p&gt;&#10;&#10;&lt;p&gt;Is it justifiable if I score them 5,4,3,2,1 and take the sum for each item and add up scores of 5 items together.&#10;Can I decide on cutoff points to categorized study participnts in to three groups like good relatioship, neutral relationship and poor relationship. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-23T10:27:29.730" Id="94842" LastActivityDate="2014-04-23T11:17:10.240" OwnerUserId="44311" PostTypeId="1" Score="1" Tags="&lt;scales&gt;&lt;measurement&gt;" Title="composite scale to measure child-parent relationship" ViewCount="46" />
  <row AnswerCount="1" Body="&lt;p&gt;I am trying to do a study of deaths due to malaria in order to find the best way to predict how dangerous this disease is. &lt;/p&gt;&#10;&#10;&lt;p&gt;I don't have a strong background in statistics, I am an auto-learner building my knowledge using online courses. &lt;/p&gt;&#10;&#10;&lt;p&gt;First, I collected the data this way :&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Statistics(gender, age, ..)   |   Number_Death&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;As far as I know, my options are&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;GLM with a Binomial distribution: for predicting if this dangerous or not. In this case, I labelled the predictor to be 0 (for no death), 1 for one or more cases.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;GLM with a Poisson distribution: for predicting the number of events based on the predictor. &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Now, I am confused.  For what purposes would we use a GLM with Gaussian distribution, a GLM with a Gaussian and a log link function, or a GLM with a Gamma distribution? &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-04-23T12:35:04.943" Id="94852" LastActivityDate="2015-03-04T04:49:48.230" LastEditDate="2014-12-27T16:47:22.050" LastEditorUserId="1739" OwnerUserId="43653" PostTypeId="1" Score="1" Tags="&lt;generalized-linear-model&gt;" Title="GLM Gaussian vs GLM Binomial vs log-link GLM Gaussian" ViewCount="96" />
  
  
  <row Body="&lt;p&gt;I think it depends what you want to convey. &lt;/p&gt;&#10;&#10;&lt;p&gt;To report the standard error or your estimate you could just write $\hat{\theta} (\hat{\sigma})$. This notation is quite common in the statistical literature, but for the sake of clarity you might mention that the standard error is in parenthesis/brackets the first time you use this notation.&lt;/p&gt;&#10;&#10;&lt;p&gt;To provide a confidence interval with your estimate you might want to write $\hat{\theta} \pm z_{\alpha/2}\hat{\sigma}$, where $z_{\alpha/2}$ is the quantile of the standard normal distribution and $1-\alpha$ the confidence level of your choice. A common choice is $\alpha = 0.05$ leading to $\hat{\theta} \pm 1.96\hat{\sigma}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The second option is a much stronger statement. In addition of providing the standard error of the estimate, it gives a confidence interval relying on the normality of the MLE, which might be too coarse in finite samples: it might include invalid parameter values (e.g., probabilities larger than one), and/or wrongly imply that the confidence interval is symmetric.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-04-23T12:55:57.087" Id="94859" LastActivityDate="2014-04-23T12:55:57.087" OwnerUserId="27403" ParentId="94191" PostTypeId="2" Score="4" />
  
  
  
  
  
  
  <row Body="&lt;p&gt;The model you fit is the following:&#10;$$&#10;\log(T) = \mu + \alpha_1 \text{age} + \alpha_2 \text{gender} + \sigma \epsilon&#10;$$&#10;where $\epsilon$ has a Gumbel distribution with density $f(e) = \exp\{e - \exp(e)\}$. The scale parameter is $\sigma$. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;survfit&lt;/code&gt; outputs $\hat{\mu}$, $\hat{\alpha}_1$, $\hat{\alpha}_2$ (&lt;code&gt;fit$coef&lt;/code&gt;) and $\hat{\sigma}$ (&lt;code&gt;fit$scale&lt;/code&gt;), along with the covariance matrix of $(\hat{\mu}, \hat{\alpha}_1,\hat{\alpha}_2, \log(\hat{\sigma}))^\prime$ (&lt;code&gt;fit$var&lt;/code&gt;) from which standard errors are extracted.&lt;/p&gt;&#10;&#10;&lt;p&gt;The relationship with the parameters in the Weibull proportional hazards model&#10;$$&#10;h(t) = \lambda \rho t^{\rho - 1} \exp(\beta_1 \text{age} + \beta_2 \text{gender})&#10;$$&#10;is as follows&#10;$$&#10;\rho = \frac{1}{\sigma} \qquad \lambda = \exp \left(- \frac{\mu}{\sigma} \right) \qquad \beta_1 = -\frac{\alpha_1}{\sigma} \qquad \beta_2 = -\frac{\alpha_2}{\sigma}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ &#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;On a side note, the delta method is usually used to compute the standard errors of $\hat{\rho}$, $\hat{\lambda}$, $\hat{\beta}_1$, and $\hat{\beta}_2$ (see &lt;a href=&quot;http://link.springer.com/book/10.1007/b97377&quot; rel=&quot;nofollow&quot;&gt;Klein and Moeschberger (2003), Section 12.2&lt;/a&gt; for the explicit formulas). (In &lt;code&gt;R&lt;/code&gt;, there is a function &lt;code&gt;deltamethod()&lt;/code&gt; in the &lt;code&gt;msm&lt;/code&gt; library).&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-04-23T14:59:20.370" Id="94878" LastActivityDate="2014-04-23T15:30:32.580" LastEditDate="2014-04-23T15:30:32.580" LastEditorUserId="3019" OwnerUserId="3019" ParentId="94875" PostTypeId="2" Score="3" />
  <row AnswerCount="0" Body="&lt;p&gt;Im hoping to get some guidance in specifying a mixed model using the lme4 package in R.&lt;/p&gt;&#10;&#10;&lt;p&gt;The study is quite straightforward.  Its a repeated measures design with pre/post measurements on the variable of interest where half of the participants received the treatment (a physical therapy program) and the other half did not.  The research was carried out in groups at 8 different hospitals.  Because the research was carried out in groups, Im treating the data as multilevel with Time at level 1, Individual at level 2, and Group at level 3.  The structure of the data can be seen below:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;'data.frame':   1856 obs. of  6 variables:&#10; $ ID           : num  1 1 2 2 3 3 4 4 5 5 ...&#10; $ Group        : Factor w/ 8 levels &quot;Richmond&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...&#10; $ Age          : num  12 12 14 14 13 13 18 18 16 16 ...&#10; $ Treatment    : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 2 2 2 2 2 2 2 2 ...&#10; $ Time         : Factor w/ 2 levels &quot;Pre&quot;,&quot;Post&quot;: 1 2 1 2 1 2 1 2 1 2 ...&#10; $ DV           : num  4 7 4 3 4 8 8 9 5 5 ...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Im trying to answer the following questions:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Was there an improvement in the DV over time?&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Did participants in the treatment program improve more than those in the control group?&lt;/p&gt;&#10;&#10;&lt;p&gt;Im not expecting any, but Id also like to know:&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;If there are differences in the DV attributable to age.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;If there are differences in the DV attributable to the location (group).&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;To that end I have specified the following model treating Time, Treatment and Age as fixed effects and Group as a random effect.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lmer(DV ~ Time + Treatment + Time*Treatment + Age + (1 | ID) + (1 | Group), &#10;     data = TLHB.data, REML = FALSE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;However, Im not sure that this model accounts for the fact that participants were nested in groups so am looking for guidance that my approach so far seems reasonable given my research questions and how to properly specify that the participants within each group were not necessarily independent.  &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-04-23T15:15:02.657" Id="94882" LastActivityDate="2014-04-24T03:11:12.693" LastEditDate="2014-04-24T03:11:12.693" LastEditorUserId="31008" OwnerUserId="31008" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;mixed-model&gt;&lt;multilevel-analysis&gt;&lt;lmer&gt;" Title="Repeated measures mixed model using lmer in R" ViewCount="268" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I am trying to model the time until some event occurs for individuals observed over a 24 month period. &lt;/p&gt;&#10;&#10;&lt;p&gt;For about 75% of people, no event occurs. For 15% of people, we know exact time of the event. For the other 10%, we only know a time window in which it occurs. So some sample &quot;survival times&quot; might be: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt;24 &#10;2.5 &#10;&amp;gt;24 &#10;5.0 &#10;0 to 6&#10;6 to 12 &#10;&amp;gt;24 &#10;18 to 24 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and so on. Can this type of data be accommodated within &quot;standard&quot; survival models? If so, some hint at how to construct the &lt;code&gt;Surv()&lt;/code&gt; object in &lt;code&gt;R&lt;/code&gt; and how to pass it to &lt;code&gt;survreg&lt;/code&gt; would be appreciated. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-04-23T16:56:27.283" Id="94890" LastActivityDate="2014-04-23T19:46:15.177" LastEditDate="2014-04-23T19:44:23.690" LastEditorUserId="44336" OwnerUserId="44336" PostTypeId="1" Score="3" Tags="&lt;survival&gt;" Title="Incorporating interval-censored event times into standard survival models" ViewCount="73" />
  <row AnswerCount="0" Body="&lt;p&gt;I have two samples $ X_1,\cdots,X_n $ and $ Y_1, \cdots, Y_n $, and would like to re-order them so that they have a pre-specified value of rank correlation such as Kendall's tau.&lt;/p&gt;&#10;&#10;&lt;p&gt;I do know that there are many ways to generate two samples that have a specific Pearson's correlation coefficient, but don't know whether there is a way to do this about Kendall's tau.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-04-23T18:08:29.680" FavoriteCount="1" Id="94901" LastActivityDate="2014-04-23T18:08:29.680" OwnerUserId="15997" PostTypeId="1" Score="3" Tags="&lt;correlation&gt;&lt;rank-correlation&gt;&lt;kendall-tau&gt;" Title="Generate two sequences that have a pre-specified rank correlation" ViewCount="25" />
  
  <row AcceptedAnswerId="94926" AnswerCount="6" Body="&lt;p&gt;I am building a model and I think that geographic location is likely to be very good at predicting my target variable.  I have the zip code of each of my users. I am not entirely sure about the best way to include zip code as a predictor feature in my model though.  Although zip code is a number, it doesn't mean anything if the number goes up or down.   I could binarize all 30,000 zip codes and then include them as features or new columns (e.g., {user_1: {61822: 1, 62118: 0, 62444: 0, etc.}}.  However, this seems like it would add a ton of features to my model. &lt;/p&gt;&#10;&#10;&lt;p&gt;Any thoughts on the best way to handle this situation?  &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for any help.  &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-04-23T18:10:37.063" FavoriteCount="2" Id="94902" LastActivityDate="2014-11-23T18:00:25.053" LastEditDate="2014-04-23T18:36:40.590" LastEditorUserId="44340" OwnerUserId="44340" PostTypeId="1" Score="10" Tags="&lt;machine-learning&gt;&lt;feature-construction&gt;" Title="how to represent geography or zip code in machine learning model or recommender system?" ViewCount="540" />
  <row Body="&lt;p&gt;Assuming $X$ is your data and $C$ is the parameter, the likelihood is $p(X|C)$. $p(C)$ is the prior. I am not aware of a particular word for $p(C)/p(X)$. &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-04-23T18:17:32.417" Id="94904" LastActivityDate="2014-04-23T18:17:32.417" OwnerUserId="22311" ParentId="94896" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;You could transform your zip code into a nominal variable (string/factor). However, as far as I remember, zip code might contain other information like county, region, etc. What I would do is to understand how zip code encodes information and decode that into multiple features.&lt;/p&gt;&#10;&#10;&lt;p&gt;Anyway letting zip code as a numeric variable is not a good idea since some models might consider the numeric ordering or distances as something to learn.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-04-23T18:25:40.703" Id="94907" LastActivityDate="2014-04-23T18:25:40.703" OwnerUserId="16709" ParentId="94902" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;There's 2 good options that I've seen:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Convert each zipcode to a dummy variable.  If you have a lot of&#10;data, this can be a quick and easy solution, but you won't be able&#10;to make predictions for new zip codes.  If you're worried about the number of features, you can add some regularization to your model to drop some of the zipcodes out of the model.&lt;/li&gt;&#10;&lt;li&gt;Use the latitude and longitude of the center point of the zipcode as variables.  This works really well in tree-based models, as they can cut up the latitude/longitude grid into regions that are relevant to your target variable.  This will also allow you to make predictions for new zipcodes, and doesn't require as much data to get right.  However, this won't work well for linear models.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Personally, I really like tree-based models (such as random forest or GBMs), so I almost always choose option 2.  If you want to get really fancy, you can use the lat/lon of the center of population for the zipcode, rather than the zipcode centroid.  But that can be hard to get ahold of.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-04-23T18:42:57.330" Id="94910" LastActivityDate="2014-04-23T18:42:57.330" OwnerUserId="2817" ParentId="94902" PostTypeId="2" Score="11" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have modified the original question.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Does &lt;a href=&quot;http://en.wikipedia.org/wiki/Beta_distribution&quot; rel=&quot;nofollow&quot;&gt;beta distribution function&lt;/a&gt; $$f(x,\alpha) = \frac{[x^a(1-x)^b]^\alpha}{B(a\alpha+1,b\alpha+1)}$$&#10;where $B$ is the beta function, approach delta function $\delta(x-a)$ on $[0,1]$ as $\alpha\rightarrow\infty$, in distribution, for fixed positive $(a,b) \ni a+b=1$?&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2014-04-24T00:21:09.273" FavoriteCount="1" Id="94947" LastActivityDate="2014-04-24T20:04:32.023" LastEditDate="2014-04-24T05:08:14.507" LastEditorUserId="44368" OwnerUserId="44368" PostTypeId="1" Score="1" Tags="&lt;distributions&gt;&lt;pdf&gt;&lt;approximation&gt;&lt;beta-distribution&gt;" Title="Beta function approximation of delta function" ViewCount="87" />
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/bsFiQ.jpg&quot; alt=&quot;Refer to this for question 4![1]&quot;&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/32AuD.jpg&quot; alt=&quot;Refer to this for question 6&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I am trying to find which factors in Ques 6 are strongly related to company being rated high  as compared to others in  Ques 4 and similarly factors related to company being rated low.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-24T07:05:26.590" Id="94982" LastActivityDate="2014-04-24T07:05:26.590" OwnerUserId="36045" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;chi-squared&gt;&lt;nonparametric&gt;&lt;factor-analysis&gt;&lt;ordinal&gt;" Title="Which Non-parametric test to use with two Ordinal sets of data" ViewCount="25" />
  <row Body="&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;The type 1 / false rejection error rate $\alpha=.05$ isn't &lt;strong&gt;completely&lt;/strong&gt; arbitrary, but yes, it is close. It's somewhat preferable to $\alpha=.051$ because it's less cognitively complex (&lt;a href=&quot;http://cogsci.stackexchange.com/q/3514/4086&quot;&gt;people like round numbers and multiples of five&lt;/a&gt;). It's a decent compromise between skepticism and practicality, though maybe a little outdated  modern methods and research resources may make higher standards (i.e., lower $p$ values) preferable, if standards there must be &lt;sup&gt;(&lt;a href=&quot;http://www.pnas.org/content/110/48/19313.full.pdf&quot;&gt;Johnson, 2013&lt;/a&gt;)&lt;/sup&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;IMO, the greater problem than the choice of threshold is the often unexamined choice to use a threshold where it is not necessary or helpful. In situations where a practical choice has to be made, I can see the value, but much basic research does not necessitate the decision to dismiss one's evidence and give up on the prospect of rejecting the null just because a given sample's evidence against it falls short of almost any reasonable threshold. Yet much of this research's authors feel obligated to do so by convention, and resist it uncomfortably, inventing terms like &quot;marginal&quot; significance to beg for attention when they can feel it slipping away because their audiences often don't care about $p$s $\ge.05$. If you look around at other questions here on $p$ value interpretation, you'll see plenty of dissension about the interpretation of $p$ values by binary &lt;code&gt;fail to&lt;/code&gt;/&lt;code&gt;reject&lt;/code&gt; decisions regarding the null.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Completely different  no. Meaningfully different  maybe. One reason to show a ridiculously small $p$ value is to imply information about effect size. Of course, just reporting effect size would be much better for several technical reasons, but authors often fail to consider this alternative, and audiences may be less familiar with it as well, unfortunately. In a null-hypothetical world where no one knows how to report effect sizes, one may be right most often in guessing that a smaller $p$ means a larger effect. To whatever extent this null-hypothetical world is closer to reality than the opposite, maybe there's some value in reporting exact $p$s for this reason. Please understand that this point is pure devil's advocacy...&lt;/p&gt;&#10;&#10;&lt;p&gt;Another use for exact $p$s that I've learned by engaging in a very similar debate here is as indices of likelihood functions. See Michael Lew's comments on and article &lt;sup&gt;(&lt;a href=&quot;http://arxiv.org/abs/1311.0081&quot;&gt;Lew, 2013&lt;/a&gt;)&lt;/sup&gt; linked in my answer to &quot;&lt;a href=&quot;http://stats.stackexchange.com/q/7134/32036&quot;&gt;Accommodating entrenched views of p-values&lt;/a&gt;&quot;.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;I don't think the Bonferroni correction is the same kind of arbitrary really. It corrects the threshold that I think we agree is at least close-to-completely arbitrary, so it doesn't lose any of that fundamental arbitrariness, but I don't think it adds anything arbitrary to the equation. The correction is &lt;a href=&quot;http://en.wikipedia.org/wiki/Bonferroni_correction#Definition&quot;&gt;defined&lt;/a&gt; in a logical, pragmatic way, and minor variations toward larger or smaller corrections would seem to require rather sophisticated arguments to justify them as more than arbitrary, whereas I think it would be easier to argue for an adjustment of $\alpha$ without having to overcome any deeply appealing yet simple logic in it.&lt;/p&gt;&#10;&#10;&lt;p&gt;If anything, I think $p$ values &lt;strong&gt;should&lt;/strong&gt; be more open to interpretation! I.e., whether the null is really more useful than the alternative ought to depend on more than just the evidence against it, including the cost of obtaining more information and the added incremental value of more precise knowledge thusly gained. This is essentially the Fisherian no-threshold idea that, AFAIK, is how it all began. See &quot;&lt;a href=&quot;http://stats.stackexchange.com/q/55691/32036&quot;&gt;Regarding p-values, why 1% and 5%? Why not 6% or 10%?&lt;/a&gt;&quot; &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;If &lt;code&gt;fail to&lt;/code&gt;/&lt;code&gt;reject&lt;/code&gt; crises aren't forced upon the null hypothesis from the outset, then the more continuous understanding of statistical significance certainly does admit the possibility of continuously increasing significance. In the dichotomized approach to statistical significance (I think this is sometimes referred to as the Neyman-Pearson framework; &lt;sup&gt;cf. &lt;a href=&quot;http://psych.stanford.edu/~jlm/pdfs/DienesChapters/Chapter3_Neyman_Pearson.pdf&quot;&gt;Dienes, 2007&lt;/a&gt;),&lt;/sup&gt; no, any significant result is as significant as the next  no more, no less. This question may help explain that principle: &quot;&lt;a href=&quot;http://stats.stackexchange.com/q/10613/32036&quot;&gt;Why are p-values uniformly distributed under the null hypothesis?&lt;/a&gt;&quot; As for how many zeroes are meaningful and worth reporting, I recommend Glen_b's answer to this question: &quot;&lt;a href=&quot;http://stats.stackexchange.com/q/78839/32036&quot;&gt;Reporting p-values $\le$ 2.22e-16&lt;/a&gt;&quot;  it's much better than the answers to the version of that question you linked on Stack Overflow!&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;sup&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;br&gt;&#10;- Johnson, V. E. (2013). Revised standards for statistical evidence. &lt;em&gt;Proceedings of the National Academy of Sciences, 110&lt;/em&gt;(48), 1931319317. Retrieved from &lt;a href=&quot;http://www.pnas.org/content/110/48/19313.full.pdf&quot;&gt;http://www.pnas.org/content/110/48/19313.full.pdf&lt;/a&gt;.&lt;br&gt;&#10;- Lew, M. J. (2013). To P or not to P: On the evidential nature of P-values and their place in scientific inference. arXiv:1311.0081 [stat.ME]. Retrieved from &lt;a href=&quot;http://arxiv.org/abs/1311.0081&quot;&gt;http://arxiv.org/abs/1311.0081&lt;/a&gt;.&lt;/sup&gt;&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-04-24T07:06:41.310" Id="94983" LastActivityDate="2014-04-24T07:06:41.310" OwnerUserId="32036" ParentId="94974" PostTypeId="2" Score="16" />
  <row Body="&lt;p&gt;It is hard to say on this little information, except that transforming different groups differently makes no sense and that mild skewness is often acceptable. Data that are exactly normal are probably fabricated! &lt;/p&gt;&#10;&#10;&lt;p&gt;It is possible that no transformation will help overall, or that one transformation will help overall. It's not just the sign of the skewness that is important but also its magnitude and why it arises. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you could post the data, or graphs of them, you would be likely to get firmer advice. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-24T07:19:00.603" Id="94984" LastActivityDate="2014-04-24T07:41:23.990" LastEditDate="2014-04-24T07:41:23.990" LastEditorUserId="22047" OwnerUserId="22047" ParentId="94965" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="94991" AnswerCount="1" Body="&lt;p&gt;I am employing EFA to 56 items.&#10;However, there were cross-loadings occurred and, therefore decision to drop the items is made. The question:&#10;The rotated components matrix showed there were a few items with no-significant loadings in any of the components, so, should I remove/drop the non-loading items and re-run the EFA until all the items were loaded into the respective component? &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-04-24T07:27:15.127" Id="94985" LastActivityDate="2014-04-24T08:19:14.393" OwnerUserId="44391" PostTypeId="1" Score="0" Tags="&lt;factor-analysis&gt;" Title="EFA: Can I remove/drop variables with non significant loadings and re-run the EFA?" ViewCount="150" />
  
  <row Body="&lt;p&gt;The five-number summary was, I believe, introduced by John W. Tukey about 1970. The point was that once you have ordered the data (e.g. using a stem-and-leaf plot), then those summaries could be produced by at most counting and averaging pairs of values. The context was pencil and paper methods for tens or (say) a few hundred values. &lt;/p&gt;&#10;&#10;&lt;p&gt;Now it is, as we all know, immensely more likely that people have their data on a computer and may even be unused to mechanical arithmetic such as adding numbers and dividing by 2. But there is usually no difficulty in calculating a mean. Whether a mean is a useful summary is open to discussion, but we can always have a look. &lt;/p&gt;&#10;&#10;&lt;p&gt;The five-number summary idea lives on in the form of box plots. Arguably, box plots have even been oversold, as when box plots without means or SDs are presented as cognate to analysis of variance. More on that in &lt;a href=&quot;http://stats.stackexchange.com/questions/85372/when-to-use-boxplot-and-when-barplot-rules-of-thumb&quot;&gt;this thread&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-04-24T10:16:09.127" Id="95005" LastActivityDate="2014-04-24T10:16:09.127" OwnerUserId="22047" ParentId="95003" PostTypeId="2" Score="6" />
  
  
  
  
  <row AcceptedAnswerId="95041" AnswerCount="2" Body="&lt;p&gt;I have constant time series (yearly from 1950 to 2010) of the abundance of several species that were captured by different groups. These series are somewhat related because the quantity of all species captured is dependent on the same effort (which is not always the same each year, but evolves differently per group). Also, parts of the area were groups capture species is common ground. &lt;/p&gt;&#10;&#10;&lt;p&gt;My data looks like this: Year | group | species | abundance&lt;/p&gt;&#10;&#10;&lt;p&gt;How can I analyse the variability (variance) in abundance between species and groups? So I would want to know for which species the captures fluctuate more heavily, independent of the groups. And the inverse for the groups. &lt;/p&gt;&#10;&#10;&lt;p&gt;I could calculate the variance of each series (per species and per group) and then do an anova, but this wouldnt take into account the dependence of the data, Also, I already performed crosscorrelations, autocorrelations, spectrum analyses, for other purposes, but I cant find anything to answer my question.&lt;/p&gt;&#10;&#10;&lt;p&gt;Could I have a sort of ordered list output?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-24T14:06:47.257" Id="95035" LastActivityDate="2014-04-24T14:59:20.987" OwnerUserId="43543" PostTypeId="1" Score="0" Tags="&lt;time-series&gt;&lt;variance&gt;" Title="analysing difference in variability between multiple related time series" ViewCount="62" />
  
  <row Body="&lt;p&gt;You're running two completely different tests. The first is a (two variable) test of no association in a 2x2 contingency table. The second is of a (one variable) test that the observed proportion is equal to the provided frequency.&lt;/p&gt;&#10;&#10;&lt;p&gt;Which one you should use depends on what exactly you are measuring.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Are you measuring on one variable and trying to compare it to the proportions in a reference population?&lt;br&gt;&#10;Example: measuring the # of successes/failures, and comparing it to a reference set&lt;/li&gt;&#10;&lt;li&gt;Are you measuring on two variables and trying to determine if there is an association between them?&lt;br&gt;&#10;Example: measuring the # of successes/failures by blue and red, and seeing if there is an association of success/failure with blue/red&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I suspect it's the latter you're trying to do, and thus should use the first test.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;To answer your comment - see this &lt;a href=&quot;http://stats.stackexchange.com/a/85906/4485&quot;&gt;answer&lt;/a&gt; for how the (by-hand) calculation differs. &lt;/p&gt;&#10;&#10;&lt;p&gt;And as for why specifying the proportions matters (and is different from giving both replications) is that you are making an assumption that the proportion you give are the &quot;true&quot; value. Why is one replication the expected value and not the other?&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;data &amp;lt;-matrix(c(227,751,193,541), ncol=2)&#10;c1 &amp;lt;- data[,1]&#10;p1 &amp;lt;- data[,2] / sum(data[,2])&#10;c2 &amp;lt;- data[,2]&#10;p2 &amp;lt;- data[,1] / sum(data[,1])&#10;&#10;[1] chisq.test(c1, p=p1)&#10;[2] chisq.test(c2, p=p2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Why do you perform &lt;strong&gt;&lt;code&gt;[1]&lt;/code&gt;&lt;/strong&gt; when &lt;strong&gt;&lt;code&gt;[2]&lt;/code&gt;&lt;/strong&gt; is just as valid and gives you a different result? (Though in this case, the conclusion is the same if you are looking at a cutoff value)&lt;/p&gt;&#10;&#10;&lt;p&gt;If that assumption that one measurement is the &quot;real&quot; proportion that should be compared to - fine, the second test will do. But you generally can't justify that.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-04-24T14:24:09.223" Id="95040" LastActivityDate="2014-04-26T14:05:19.570" LastEditDate="2014-04-26T14:05:19.570" LastEditorUserId="4485" OwnerUserId="4485" ParentId="95027" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;Whether or not it makes sense to impute year of birth, and how to do it involves a number of considerations. &lt;/p&gt;&#10;&#10;&lt;p&gt;Firstly, imputation is probably only reasonable if the missingness pattern is Missing Completely at Random (MCAR) or Missing at Random (MAR). Some discussion of these missingness types are given in Section 25.1 of &lt;a href=&quot;http://www.stat.columbia.edu/~gelman/arm/missing.pdf&quot;&gt;this paper&lt;/a&gt;. Ask yourself which type of missingness you are likely finding yourself confronted with. If you believe that there is a mechanism to the missingness you observe you may want to reconsider imputation. &lt;/p&gt;&#10;&#10;&lt;p&gt;Another question is do you consider birth year a categorical or a continuous variable. &#10;If you believe it should be treated continuously, you can make use of a number of imputation methods. Multiple imputation may be one of the most appropriate. An bird's eye view is given &lt;a href=&quot;http://sites.stat.psu.edu/~jls/mifaq.html#em&quot;&gt;here&lt;/a&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you believe birth year should be treated categorically, you face the challenge of imputing a categorical variable. This is treated in &lt;a href=&quot;http://www2.sas.com/proceedings/sugi30/113-30.pdf&quot;&gt;this paper&lt;/a&gt;, which discusses the merits of a number of imputation procedures for categorical variables and provides some examples.&lt;/p&gt;&#10;&#10;&lt;p&gt;Multiple imputation for continuous and categorical variables can both be performed using the &lt;a href=&quot;http://cran.r-project.org/web/packages/mi/mi.pdf&quot;&gt;mi package&lt;/a&gt; in R. &lt;/p&gt;&#10;&#10;&lt;p&gt;So to summarize, you can impute birth year whether you want to treat it continuously or categorically. First though, think about whether there is a reason why those observations might be missing. Do you think they are MCAR or MAR, or can you imagine there is a systemic reason for their missingness? If so, is it &lt;em&gt;Missingness that depends on unobserved predictors&lt;/em&gt;? If so, can you model the missingness somehow and prevent this from biasing you imputation? Is it &lt;em&gt;Missingness that depends on the missing value itself&lt;/em&gt;? In either of the last two scenarios, you may want to think carefully about how to proceed with your analysis, and what conclusions you can reasonably draw from it.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-04-24T14:28:22.880" Id="95042" LastActivityDate="2014-04-24T14:28:22.880" OwnerUserId="17672" ParentId="95024" PostTypeId="2" Score="8" />
  <row AnswerCount="1" Body="&lt;p&gt;I would like to use neural networks to classification of time series ( I have some Patterns and I want to adjust input time series to an appropriate class) &lt;/p&gt;&#10;&#10;&lt;p&gt;-ist it possible to do this job with neural network? which type of neural network should I use?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-24T14:35:34.723" Id="95045" LastActivityDate="2014-04-24T16:56:28.007" LastEditDate="2014-04-24T14:46:24.283" LastEditorUserId="30701" OwnerUserId="30701" PostTypeId="1" Score="0" Tags="&lt;time-series&gt;&lt;classification&gt;&lt;neural-networks&gt;&lt;pattern-recognition&gt;" Title="Which type neural networks for time series classification" ViewCount="52" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have this dataset to model, but I'm not sure how to do it. &lt;/p&gt;&#10;&#10;&lt;p&gt;I want to model the &lt;strong&gt;surviving probability&lt;/strong&gt; of different populations of two &lt;strong&gt;species&lt;/strong&gt; depending on a &lt;strong&gt;treatment&lt;/strong&gt; applied. &lt;/p&gt;&#10;&#10;&lt;p&gt;Populations should be &lt;strong&gt;nested within species&lt;/strong&gt;, as they belong to either species 1 or species 2. And, as I have replicated measures for each population x treatment combination, I need to include this as a random effect as well.&lt;/p&gt;&#10;&#10;&lt;p&gt;What R function can I use to do that? Is there one?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-04-24T18:58:30.210" Id="95084" LastActivityDate="2014-04-24T18:58:30.210" OwnerUserId="44439" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;mixed-model&gt;&lt;survival&gt;&lt;random-effects-model&gt;" Title="Is there an R function to do a survival analysis with right censoring + nested + crossed factors" ViewCount="98" />
  <row AnswerCount="1" Body="&lt;p&gt;I'm trying to develop a predictive model for an angular dependent variable (on $[0,2\pi])$ using several independent measurements  also angular variables, on $[0,2\pi]$  as predictors.  Each predictor is significantly but not extremely strongly correlated with the dependent variable.  How can I combine the predictors to determine a predictive model for the dependent variable that is optimal in some sense?  And how can I rigorously identify the strongest predictor(s)?&lt;/p&gt;&#10;&#10;&lt;p&gt;For variables on Euclidean space(s), I'd employ multiple regression (or similar) and principal components analysis.  But the periodicity of all variables mucks with these approaches, e.g., 0.02 must be highly correlated with 6.26, but not with 3.14.  How are &quot;the usual&quot; procedures generalized to directional / circular statistics? Any insights or cites to useful references would be useful.  (I'm already aware of the texts by N. Fisher and Mardia &amp;amp; Jupp, but don't have handy access to these.) &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-24T19:00:44.823" Id="95085" LastActivityDate="2014-04-25T04:56:09.330" LastEditDate="2014-04-25T04:56:09.330" LastEditorUserId="44436" OwnerUserId="44436" PostTypeId="1" Score="7" Tags="&lt;regression&gt;&lt;multiple-regression&gt;&lt;directional-statistics&gt;" Title="Multiple regression in directional / circular statistics?" ViewCount="399" />
  <row Body="&lt;p&gt;PLS isn't necessarily a cure for insufficient sample size (though I recognize you're not claiming otherwise exactly). Check out &lt;a href=&quot;http://indigo.uic.edu/bitstream/handle/10027/7655/Lower%20bounds%20on%20sample%20size%20in%20structural%20equation%20modeling.pdf?sequence=3&quot; rel=&quot;nofollow&quot;&gt;Westland &lt;sup&gt;(2010)&lt;/sup&gt;&lt;/a&gt; for a discussion of factors that determine the necessary sample size (and often determine that it's much larger than published articles often use). He discusses PLS too. One notable argument is that: &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;In PLS, the sample size question is probably both less relevant and less critical, because hypothesis testing is better left to LISREL and systems of equation approaches.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Some collinearity is generally expected among independent variables in basically any general(ized) linear model or structural equation model. It's mostly a problem when collinearity is very strong. &lt;a href=&quot;http://en.wikipedia.org/wiki/Multicollinearity#Remedies_for_multicollinearity&quot; rel=&quot;nofollow&quot;&gt;Several methods exist for managing excessive collinearity&lt;/a&gt;, including ridge and principal components regression. Low collinearity is not a problem for any of these methods, including PLS. That they are somewhat more robust against high multicollinearity does not imply that they are biased by a lack thereof. Hence whether PLS is advisable in your circumstance probably depends on other factors.&lt;/p&gt;&#10;&#10;&lt;p&gt;You might try calculating the &lt;a href=&quot;http://en.wikipedia.org/wiki/Variance_inflation_factor&quot; rel=&quot;nofollow&quot;&gt;variance inflation factors&lt;/a&gt; for your set of regressors. These can give you a useful index of how much collinearity exists among them  it is often not as bad as one fears initially. This was the case in my own research: my concern about multicollinearity in my own regressions led me here to Cross Validated several months ago, incidentally.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;sup&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;br&gt;&#10;Westland, C. J. (2010). Lower bounds on sample size in structural equation modeling. &lt;em&gt;Electronic Commerce Research and Applications, 9&lt;/em&gt;(6), 476487.&lt;/sup&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-24T19:14:08.843" Id="95092" LastActivityDate="2014-04-24T19:14:08.843" OwnerUserId="32036" ParentId="95082" PostTypeId="2" Score="1" />
  
  
  <row AcceptedAnswerId="95147" AnswerCount="1" Body="&lt;p&gt;I am having 56 likert scaled items for IV and 28 likert scaled items for dv.&#10;As to fulfill the assumption for EFA, outliers and linearity need to be checked.&#10;Can anyone tells me what method/analysis is the best for identifying outliers and linearity?&#10;For outliers, can I use Z-scores? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-25T07:46:14.387" Id="95137" LastActivityDate="2014-04-25T11:17:43.943" OwnerUserId="44391" PostTypeId="1" Score="0" Tags="&lt;factor-analysis&gt;&lt;outliers&gt;" Title="Outliers and Linearity for EFA" ViewCount="43" />
  <row Body="&lt;p&gt;You should normalize your data before passing it to your SVM.&lt;/p&gt;&#10;&#10;&lt;p&gt;For SVM it is recommended to linearly scale each attribute to the range $[-1, +1]$ or $[0,1]$&lt;/p&gt;&#10;&#10;&lt;p&gt;However, &lt;strong&gt;it is very important that you use the same scaling factors for training and testing sets.&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose you scaled the training data from the range $[-10, +10]$ to $[-1, +1]$.&lt;/p&gt;&#10;&#10;&lt;p&gt;If a testing data attribute lies in the range $[-11, +8]$ you must scale it to $[-1.1, +0.8]$&lt;/p&gt;&#10;&#10;&lt;p&gt;So to answer your question:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;normalize your training set&lt;/li&gt;&#10;&lt;li&gt;perform training&lt;/li&gt;&#10;&lt;li&gt;normalize your test set &lt;em&gt;using the same scaling factor as the training set&lt;/em&gt;&lt;/li&gt;&#10;&lt;li&gt;perform predictions&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2014-04-25T08:01:39.367" Id="95138" LastActivityDate="2014-04-25T08:01:39.367" OwnerUserId="44458" ParentId="94389" PostTypeId="2" Score="0" />
  
  <row AcceptedAnswerId="95151" AnswerCount="1" Body="&lt;p&gt;I am using AdaBoost Classifier to predict values I have. How can evaluate the accuracy of prediction model (I'd like to see how the accuracy of predicted values).&lt;/p&gt;&#10;&#10;&lt;p&gt;You can check an example here: &lt;a href=&quot;http://scikit-learn.org/stable/modules/ensemble.html#usage&quot; rel=&quot;nofollow&quot;&gt;http://scikit-learn.org/stable/modules/ensemble.html#usage&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I found two options : using confusion matrix&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;from sklearn.metrics import confusion_matrix&#10;cm = confusion_matrix(expected, y_1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;or using cross val score&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;scores = cross_val_score(clf_1, X_train, y_train)&#10;print scores.mean()&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;There is also: AdaBoostClassifier.staged_score(X, y) AdaBoostClassifier.score(X, y)&lt;/p&gt;&#10;&#10;&lt;p&gt;So, I am little bit confused.&lt;/p&gt;&#10;&#10;&lt;p&gt;One last question: Should I use predict() or predict_proba().&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-25T09:00:44.100" FavoriteCount="1" Id="95146" LastActivityDate="2014-06-01T22:08:15.323" OwnerUserId="43653" PostTypeId="1" Score="3" Tags="&lt;predictive-models&gt;&lt;prediction&gt;&lt;scikit-learn&gt;" Title="How can we evaluate the predicted values using Scikit-Learn" ViewCount="1105" />
  <row AnswerCount="0" Body="&lt;p&gt;I managed to label and get set of pixels for every single object of interest in the image.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now I'm looking into clustering and am not sure what features are worth using. I currently have two in mind&lt;/p&gt;&#10;&#10;&lt;p&gt;$x_1 $= Areas of objects&lt;/p&gt;&#10;&#10;&lt;p&gt;$x_2 $= Mean Grayscale Pixel Intensity&lt;/p&gt;&#10;&#10;&lt;p&gt;Now i'm not sure if I should get more features in order for clustering to work better.&#10;I was thinking maybe feature 3 be a ratio of feature 1 and 2, or get the histogram of each object, and find a parameter to describe the histogram. &#10;I am able to take any of these features of &lt;a href=&quot;http://www.mathworks.com/help/images/ref/regionprops.html&quot; rel=&quot;nofollow&quot;&gt;reigionprops&lt;/a&gt; as well, but I'm not sure how much useful would half of them be.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-25T11:30:35.677" FavoriteCount="1" Id="95157" LastActivityDate="2014-04-25T11:48:22.030" LastEditDate="2014-04-25T11:48:22.030" LastEditorUserId="21958" OwnerUserId="21958" PostTypeId="1" Score="1" Tags="&lt;feature-selection&gt;" Title="What kinds of features are worth taking from objects on images?" ViewCount="13" />
  
  
  <row AnswerCount="3" Body="&lt;p&gt;I would also be interested to know why it is one or the other.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-25T14:44:37.413" Id="95187" LastActivityDate="2014-04-25T16:33:58.187" OwnerUserId="43338" PostTypeId="1" Score="5" Tags="&lt;p-value&gt;" Title="Is a p-value a sample statistic, or a population parameter, or neither?" ViewCount="380" />
  <row Body="&lt;p&gt;I don't think it matters, since it is mostly clear from context. The hiddens are nearly always binary. Bernoulli and binary mean the same thing. If another distribution is mentioned, e.g. as in &quot;Gaussian RBM&quot; this refers to an RBM with Gaussian visibles.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-04-25T15:11:29.433" Id="95191" LastActivityDate="2014-04-25T15:11:29.433" OwnerUserId="2860" ParentId="95181" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;A $p$-value is the probability of observing a test static value as or more extreme than the test statistic created from one's data &lt;em&gt;if the null hypothesis is true&lt;/em&gt;. You can therefore interpret the $p$-value as a measure of how extreme your test statistic is under H$_{0}$ and the probability distribution attached to H$_{0}$. The $p$-value is therefore a statistic that is a function of one's data, and one's choice of H$_{0}$.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-25T15:36:37.573" Id="95199" LastActivityDate="2014-04-25T15:36:37.573" OwnerUserId="44269" ParentId="95187" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;In data analysis you typically don't want to reinvent the wheel&lt;/p&gt;&#10;&#10;&lt;p&gt;There are packages to do this in R namely: &lt;a href=&quot;http://cran.r-project.org/web/packages/biglm/biglm.pdf&quot; rel=&quot;nofollow&quot;&gt;biglm&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;sorry that is for linear regression. &lt;/p&gt;&#10;&#10;&lt;p&gt;GLM with large data sets can be fit with &lt;a href=&quot;http://cran.r-project.org/web/packages/speedglm/speedglm.pdf&quot; rel=&quot;nofollow&quot;&gt;speedglm&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;install.packages('speedglm')&#10;library(speedglm)&#10;set.seed(123)&#10;trt &amp;lt;- c(rep(1,500000),rep(0,500000))&#10;x &amp;lt;- matrix( rnorm(1000000*29), ncol=29)&#10;beta &amp;lt;- c(10,rep(1,29))&#10;y &amp;lt;- exp(cbind(trt,x) %*% beta)/(1+exp(cbind(trt,x) %*% beta))&amp;gt;0.5&#10;&#10;data &amp;lt;- data.frame(y=y,trt=trt,x=x)&#10;&#10;m &amp;lt;- speedglm(y~trt+x, data,family=binomial())&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2014-04-25T16:38:47.870" Id="95207" LastActivityDate="2014-04-25T18:22:13.137" LastEditDate="2014-04-25T18:22:13.137" LastEditorUserId="17661" OwnerUserId="17661" ParentId="95203" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;I dealt with something similar when training a classifier that used native language as a feature (how do you measure similarity between English and Spanish?) There are lots of methods out there for determining &lt;a href=&quot;http://scholar.google.com/scholar?q=similarity+measure+for+categorical+data&amp;amp;hl=en&amp;amp;as_sdt=0&amp;amp;as_vis=1&amp;amp;oi=scholart&amp;amp;sa=X&amp;amp;ei=1ZpaU6L0N7HlsATNt4G4CQ&amp;amp;ved=0CCIQgQMwAA&quot; rel=&quot;nofollow&quot;&gt;similarity among non-categorical data&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;It depends on your data, but if you find that geographic distance from a zip code is not as important as whether a given input contains particular zip codes, then non-categorical methods might help. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-25T17:33:21.517" Id="95213" LastActivityDate="2014-04-25T17:33:21.517" OwnerUserId="44485" ParentId="94902" PostTypeId="2" Score="4" />
  
  
  <row Body="&lt;p&gt;On y-axis confusion matrix has the actual values, and on the x-axis the values given by the predictor. Therefore, the counts on the diagonal are the number of correct predictions. And elements of the diagonal are incorrect predictions. &lt;/p&gt;&#10;&#10;&lt;p&gt;In your case:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; confusion_matrix(y_true, y_pred)&#10;    array([[2, 0, 0],  # two zeros were predicted as zeros&#10;           [0, 0, 1],  # one 1 was predicted as 2&#10;           [1, 0, 2]]) # two 2s were predicted as 2, and one 2 was 0&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="3" CreationDate="2014-04-25T17:52:53.537" Id="95216" LastActivityDate="2014-04-25T18:06:59.517" LastEditDate="2014-04-25T18:06:59.517" LastEditorUserId="11708" OwnerUserId="11708" ParentId="95209" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;The question is one of interpretation and policy, not statistics.&lt;/p&gt;&#10;&#10;&lt;p&gt;It looks like this particular cutoff (16th percentile is below average, 84th is above) comes from taking $\pm \sigma$ in Normal distribution (&quot;bell-shaped curve&quot;). For example, SAT scores are normalized to have average 500 and $\sigma = 100$. Thus, a score between 400 and 600 will be considered &quot;average&quot;. &lt;/p&gt;&#10;&#10;&lt;p&gt;It's clear that the &quot;Average&quot; interval should include 50th percentile, but how wide should it be? 40 to 60%? 25 to 75%? This is purely subjective. &lt;/p&gt;&#10;&#10;&lt;p&gt;There is a certain danger in assessing performance by strictly comparing someone to their peers (that is, looking at percentile rank). If a child reads at grade level, but most of her peers read faster, then the child's performance will be considered poor. If, on the other hand, most of her peers read below grade level, then she will be considered above average.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-25T18:56:12.567" Id="95223" LastActivityDate="2014-04-25T18:56:12.567" OwnerUserId="43634" ParentId="95182" PostTypeId="2" Score="0" />
  
  <row AnswerCount="3" Body="&lt;p&gt;I have data on the first to fourth moments of a continuous random variable and I am trying to find what distribution best fits the data. Wikipedia has a list of about 20 distributions that could fit the data and I would like to try each of them to see which is the best. Some of these distributions have one parameter, some have two or three. Is there some test I can use to compare the fit of each distribution? Or at least a test to compare distributions with the same number of parameters?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-04-25T19:32:17.387" Id="95227" LastActivityDate="2014-04-26T02:17:40.193" OwnerUserId="27271" PostTypeId="1" Score="2" Tags="&lt;distributions&gt;&lt;fitting&gt;&lt;moments&gt;" Title="Fitting moments to a distribution" ViewCount="127" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I heard from an university course that a normal method for turning categoric variables into binary variables was to simply create a binary variable for each category (one-out-of-K coding). However, these variables should be standardized so the weight of the original variable remains unchanged.&lt;/p&gt;&#10;&#10;&lt;p&gt;How should these new variables be standardized? Independently with their own standard deviation or should they be scaled using a shared standard deviation?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-04-25T20:20:26.363" Id="95230" LastActivityDate="2014-04-25T20:20:26.363" OwnerUserId="29025" PostTypeId="1" Score="0" Tags="&lt;standardization&gt;" Title="One-out-of-K coding and standardization" ViewCount="77" />
  <row AnswerCount="0" Body="&lt;p&gt;I will soon have the results of a questionnaire that I have carried out, and I have made a plan on how I will analyse the data. I was hoping that someone could comment on my plan to advise me.&lt;/p&gt;&#10;&#10;&lt;p&gt;About the questionnaire: there are 3 groups of people (let's call them &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;B&lt;/code&gt;, and &lt;code&gt;C&lt;/code&gt;), and they have all answered a questionnaire with 18 questions, all on a 5-point Likert scale. The questions are grouped into 4 subsets. Group &lt;code&gt;A&lt;/code&gt; is the reference group, and my aim is to see if the the responses of group &lt;code&gt;B&lt;/code&gt; or &lt;code&gt;C&lt;/code&gt; are different to &lt;code&gt;A&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;The primary outcome measure is the percentage of the total possible score for each subset of questions.&#10;The secondary outcome measure is the percentage of the total possible score for each question.&lt;/p&gt;&#10;&#10;&lt;p&gt;The current outline in which I would present the data is as follows:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Descriptive analysis: I would detail the demographics of each group of patients and use ANOVA or KruskalWallis (most likely as the data will probably not be normally distributed) to determine if there are differences in the demographics of the groups&lt;/li&gt;&#10;&lt;li&gt;Primary outcome measure: Use ANOVA or KruskalWallis&lt;/li&gt;&#10;&lt;li&gt;Secondary outcome measure: Use ANOVA or KruskalWallis&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;As I understand with ANOVA and KruskalWallis, you can find out where a difference lies if the statistic comes out below the threshold. This is something I would plan on doing.&lt;/p&gt;&#10;&#10;&lt;p&gt;My problem is that I am unsure how I could go about testing to see if any of the demographic variables are associated with outcome. Regression?&#10;Furthermore if a demographic variable is associated with outcome, how can I adjust for that in my statistical analysis?&#10;I will be using SPSS.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-04-25T20:25:10.607" Id="95232" LastActivityDate="2014-04-25T20:32:39.127" LastEditDate="2014-04-25T20:32:39.127" LastEditorUserId="32036" OwnerUserId="44492" PostTypeId="1" Score="2" Tags="&lt;spss&gt;&lt;survey&gt;&lt;likert&gt;&lt;group-differences&gt;&lt;statistical-control&gt;" Title="How does my questionnaire analysis plan sound?" ViewCount="171" />
  <row Body="&lt;p&gt;I fully agree with Andy, and I was actually thinking of writing something similar, but then I started to wonder myself about this topic. I think we all agree that Granger causality itself really has not much to do with causality as understood in the potential outcomes framework, simply because Granger causality is more about time precedence than anything else. However, suppose there is a causal relationship between $X_t$ and $Y_t$ in the sense that the former causes the latter, and suppose that this happens along a temporal dimension with a lag of one period, say. That is, we can easily apply the potential outcomes framework to two time series and define causality in this way. The issue then becomes: while Granger causality has no &quot;meaning&quot; for causality as defined in the potential outcomes framework, does causality imply Granger causality in the time series context? &lt;/p&gt;&#10;&#10;&lt;p&gt;I have never seen a discussion on this, but I think if you or any researcher wants to make a case for this, you need to impose some additional structure. Clearly, the variables need to react sluggishly, i.e. the causal relationship here must not be simultaneous but defined with a lag. Then, I think, it might be reassuring to not reject Granger causality. While this is clearly no evidence in favour of a causal relationship, if you were to claim such, then I would take the GNC test as subjective evidence. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-25T22:00:33.373" Id="95241" LastActivityDate="2014-04-25T22:00:33.373" OwnerUserId="31634" ParentId="94200" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;I would follow Henry's tip and check Lyapunov with $\delta=1$. The fact that the distributions are mixed should not be a problem, as long as the $a_i$'s and $b_i$'s behave properly. Simulation of the particular case in which $a_i=0$, $b_i=1$, $k_i=2/3$ for each $i\geq 1$ shows that normality is ok.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;xbar &amp;lt;- replicate(10^4, mean(pmin(runif(10^4), 2/3)))&#10;hist((xbar - mean(xbar)) / sd(xbar), breaks = &quot;FD&quot;, freq = FALSE)&#10;curve(dnorm, col = &quot;blue&quot;, lwd = 2, add = TRUE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/zCTR1.png&quot; alt=&quot;CLT&quot;&gt;&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-04-26T02:24:24.437" Id="95260" LastActivityDate="2014-04-27T00:06:24.977" LastEditDate="2014-04-27T00:06:24.977" LastEditorUserId="9394" OwnerUserId="9394" ParentId="95214" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;Yes, you should always use a continuous variable as a continuous variable.  There is little reason to ever categorize a continuous variable into bins.  If you like, you may want to read my answer here: &lt;a href=&quot;http://stats.stackexchange.com/a/24080/7290&quot;&gt;How to choose between ANOVA and ANCOVA in a designed experiment&lt;/a&gt;, which discusses this in greater detail.  &lt;/p&gt;&#10;" CommentCount="9" CreationDate="2014-04-26T03:57:19.137" Id="95266" LastActivityDate="2014-04-26T03:57:19.137" OwnerUserId="7290" ParentId="95263" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;You should use Dunn's test$^{*}$. If one proceeds by moving from a rejection of Kruskal-Wallis to performing ordinary pair-wise rank sum tests (with or without multiple comparison adjustments), one runs into two problems: (1) the ranks that the pair-wise rank sum tests use &lt;em&gt;are not&lt;/em&gt; the ranks used by the Kruskal-Wallis test; and (2) Dunn's test preserves a pooled variance for the tests implied by the Kruskal-Wallis null hypothesis.&lt;/p&gt;&#10;&#10;&lt;p&gt;Of course, as with any omnibus test (e.g. ANOVA, Cochran's &lt;em&gt;Q&lt;/em&gt;, etc.), &lt;em&gt;post hoc&lt;/em&gt; tests following rejection of a Kruskal-Wallis test which have been adjusted for multiple comparisons may fail to reject all pairwise tests for a given family-wise error rate or given false discovery rate corresponding to a given $\alpha$ for the omnibus test.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;br&gt;&#10;$^{*}$ Caveat: there are a few less well-known &lt;em&gt;post hoc&lt;/em&gt; pair-wise tests to follow a rejected Kruskal-Wallis, including Conover-Iman (like Dunn, but based on the &lt;em&gt;t&lt;/em&gt; distribution, rather than the &lt;em&gt;z&lt;/em&gt; distribution), and the Dwass-Steel-Critchlow-Fligner tests.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-26T04:41:25.280" Id="95270" LastActivityDate="2014-09-15T05:39:13.443" LastEditDate="2014-09-15T05:39:13.443" LastEditorUserId="44269" OwnerUserId="44269" ParentId="25815" PostTypeId="2" Score="9" />
  <row AcceptedAnswerId="95728" AnswerCount="1" Body="&lt;p&gt;In many approaches to learning Bayesian Networks a solution to tackle continuous variables is to discretize them and apply one of the well established techniques for learning Bayesian Networks containing discrete variables only. Most discretization techiques are univariate in the sense that they act on each variable by itself. This naturally does not consider the interactions between them. Does anyone know if using K-means on all or groups of the variables included in the network and then using the extremes of the resulting clusters as thresholds for the discretization makes sense? If not, which (multivariate) discretization techinques are recommended and why? Thank you in advanced.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-26T04:45:10.900" FavoriteCount="1" Id="95271" LastActivityDate="2014-04-29T23:01:32.730" LastEditDate="2014-04-26T17:10:43.620" LastEditorUserId="11748" OwnerUserId="11748" PostTypeId="1" Score="3" Tags="&lt;bayesian&gt;&lt;clustering&gt;&lt;k-means&gt;&lt;discrete-data&gt;&lt;bayes-network&gt;" Title="Bayesian Networks and discretization of variables using K-means clustering" ViewCount="169" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I realise there are other reasons why p-values will decrease with increased sample size, but I am wondering if this reason is valid at all.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-04-26T09:09:08.393" Id="95281" LastActivityDate="2014-04-26T11:53:46.527" OwnerUserId="43338" PostTypeId="1" Score="0" Tags="&lt;correlation&gt;&lt;p-value&gt;" Title="Is it correct to say that p-values for Pearson's r decrease with increasing sample size because bias decreases with increased sample size?" ViewCount="170" />
  <row Body="&lt;p&gt;A logistic regression shouldn't have constant variance. &lt;/p&gt;&#10;&#10;&lt;p&gt;As far as I've seen (but please correct me if I am mistaken), &lt;a href=&quot;http://en.wikipedia.org/wiki/Breusch%E2%80%93Pagan_test&quot; rel=&quot;nofollow&quot;&gt;Breusch-Pagan&lt;/a&gt; is for linear regression, not logistic regression.&lt;/p&gt;&#10;&#10;&lt;p&gt;It looks to me like &lt;code&gt;?bptest&lt;/code&gt; has the same view.&lt;/p&gt;&#10;&#10;&lt;p&gt;A version of the test could probably be developed for logistic regression, but it would be testing a different hypothesis than homoskedasticity vs heteroskedasticity (you'd test whether there was deviation from the particular variance assumption in logistic regression).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-26T09:45:48.087" Id="95282" LastActivityDate="2014-04-26T12:29:16.570" LastEditDate="2014-04-26T12:29:16.570" LastEditorUserId="805" OwnerUserId="805" ParentId="95273" PostTypeId="2" Score="6" />
  
  <row Body="&lt;p&gt;The p-values need not decreases as sample size increases, but if there is a correlation that differs from your null value, then the p-values will tend to decrease. First, say that the null hypothesis, $H_0: r= 0$. &#10;Suppose you had two variables, $\pmb x_1$ and $\pmb x_2$, for which you would like to know if the correlation is significant.&lt;/p&gt;&#10;&#10;&lt;p&gt;As a first toy example, consider the case where $\pmb x_1$ and $\pmb x_2$ are independent, standard normal variables.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;library(Hmisc)&lt;/p&gt;&#10;  &#10;  &lt;p&gt;n = c(20,50,100, 500, 1000, 10000)&lt;/p&gt;&#10;  &#10;  &lt;p&gt;storeCorr &amp;lt;- rep(NA, 6)    &lt;/p&gt;&#10;  &#10;  &lt;p&gt;storePval &amp;lt;-  rep(NA,6)&lt;/p&gt;&#10;  &#10;  &lt;p&gt;set.seed(123)&lt;/p&gt;&#10;  &#10;  &lt;p&gt;X1 &amp;lt;- rnorm(n[6])&lt;/p&gt;&#10;  &#10;  &lt;p&gt;set.seed(125)&lt;/p&gt;&#10;  &#10;  &lt;p&gt;X2 &amp;lt;- rnorm(n[6])&lt;/p&gt;&#10;  &#10;  &lt;p&gt;for (i in 1:6){ &lt;/p&gt;&#10;  &#10;  &lt;p&gt;x1 &amp;lt;- X1[1:n[i]]&lt;/p&gt;&#10;  &#10;  &lt;p&gt;x2 &amp;lt;- X2[1:n[i]]&lt;/p&gt;&#10;  &#10;  &lt;p&gt;storeCorr[i] &amp;lt;- cor(x1,x2)&lt;/p&gt;&#10;  &#10;  &lt;p&gt;pval &amp;lt;- rcorr(x1,x2)&lt;/p&gt;&#10;  &#10;  &lt;p&gt;storePval[i] &amp;lt;- pval$P[2] }&lt;/p&gt;&#10;  &#10;  &lt;p&gt;storeCorr&#10;  [1] -2.049893e-01 -1.190268e-01 -6.825500e-02  2.582957e-02  4.984328e-02 -5.708832e-05&lt;/p&gt;&#10;  &#10;  &lt;p&gt;storePval&#10;  [1] 0.3859574 0.4103422 0.4998307 0.5644682 0.1152124 0.995445619 &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;As you can see, even as the sample size grows larger and larger the p-value does not necessarily get smaller since the the null hypothesis is true.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now run the same code again, but create some correlation between X1 and X2. This will produce results more in line with your expectation. To do this replace X2 with&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;set.seed(125)&lt;/p&gt;&#10;  &#10;  &lt;p&gt;X2 &amp;lt;- 0.1*X1+rnorm(n[6])&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Run the loop from the first experiment&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;storeCorr&#10;  [1] -0.11052361 -0.02957047  0.01781262  0.12003696  0.14685975  0.09896175&lt;/p&gt;&#10;  &#10;  &lt;p&gt;storePval&#10;  [1] 6.427365e-01 8.384702e-01 8.603720e-01 7.207274e-03 3.107059e-06 0.000000e+00&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;What we see now is that as the sample size grows the p-value eventually gets smaller and smaller. It starts out insignificant, because there is not enough information to convincingly show correlation, but as the sample size grows enough information is obtained.&lt;/p&gt;&#10;&#10;&lt;p&gt;To sum up, the larger your sample size, the more information you have about the generating process behind your data. If there is correlation between two variables, then you are able to more precisely identify it when you have more information (bigger sample). If there is no correlation between those variables, you will not find it (beyond Type 1 error).&lt;/p&gt;&#10;&#10;&lt;p&gt;Some more examples of the role of sample size on the significance of correlation coefficients is &lt;a href=&quot;http://irthoughts.wordpress.com/2010/10/18/on-correlation-coefficients-and-sample-size/&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;, though this is not an authoritative source.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-26T10:24:56.507" Id="95284" LastActivityDate="2014-04-26T10:24:56.507" OwnerUserId="17672" ParentId="95281" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;No, it seems you misinterpreted the sentence.  Squaring a factor is not meaningful.  The paper includes a quadratic term for age.  There is also a dummy variable for the driver.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-26T11:47:06.993" Id="95291" LastActivityDate="2014-04-26T11:47:06.993" OwnerUserId="44451" ParentId="95287" PostTypeId="2" Score="2" />
  
  
  
  
  
  <row Body="&lt;p&gt;Assuming that by &quot;parametric model&quot; the OP means &lt;em&gt;fully&lt;/em&gt; parametric, then this sounds like a question about the appropriate data structure for discrete time survival analysis (aka discrete time event history) models such as logit (1), probit (2), or complimentary log-log (3) hazard models, then the appropriate answer is that the data typically need to be structured in a &lt;em&gt;person-period&lt;/em&gt; format.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;$h_{t} = \frac{e^{\mathbf{BX}}}{1 + e^{\mathbf{BX}}}$&lt;/li&gt;&#10;&lt;li&gt;$h_{t} = \Phi(\mathbf{BX})$&lt;/li&gt;&#10;&lt;li&gt;$h_{t} = 1 - e^{-e^{\mathbf{BX}}}$&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;where $\mathbf{BX}$ are the parameters and predictors in the model. Often discrete time survival analysis models will include &lt;em&gt;dummy variables&lt;/em&gt; for each time period (see below) and also often include time period itself, or even functions of it, as a variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's what a person-period data set looks like:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ID period y x1 x2 x3 t1 t2 t3 . . . tT&#10;1  1      0 1  3  12 1  0  0  . . . 0&#10;1  2      0 1  0  12 0  1  0  . . . 0&#10;1  3      1 1  9  12 0  0  1  . . . 0&#10;2  1      0 0  4  6  1  0  0  . . . 0&#10;3  1      0 1  0  17 1  0  0  . . . 0&#10;3  2      0 1  3  17 0  1  0  . . . 0&#10;3  3      0 1  3  17 0  0  1  . . . 0&#10;etc.&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;First of all notice both &lt;code&gt;ID&lt;/code&gt; and &lt;code&gt;period&lt;/code&gt; which define the hierarchical period of observation nested in person structure of these data. Also notice that &lt;code&gt;x2&lt;/code&gt; is &lt;em&gt;time varying&lt;/em&gt; (i.e. within the same individual it can take different values in different periods), and that &lt;code&gt;x1&lt;/code&gt; and &lt;code&gt;x3&lt;/code&gt; are static; understand that the model is agnostic as to whether predictors are time-varying or static. Finally examine the relationship between period and the indicator variables for time/period (i.e. &lt;code&gt;t1&lt;/code&gt; through &lt;code&gt;tT&lt;/code&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;Often times you will receive data in a &lt;em&gt;person-time&lt;/em&gt; format such as this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ID TimeToEvent Censored x1 x2t1 x2t2 . . . x2tT x3,&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and will need to transform the data appropriately. Here &lt;code&gt;TimeToEvent&lt;/code&gt; measures how many periods each subject was observed while in the study, and &lt;code&gt;Censored&lt;/code&gt; indicates whether or not the subject left the study &lt;em&gt;without experiencing the event&lt;/em&gt; (i.e. whether that subject was &lt;em&gt;right censored&lt;/em&gt;). In your data &lt;code&gt;TimeToEvent&lt;/code&gt; probably equals &lt;code&gt;end&lt;/code&gt; - &lt;code&gt;start&lt;/code&gt;, and &lt;code&gt;Censored&lt;/code&gt; is certainly some function of &lt;code&gt;state&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are often tools available for transforming data such as these. For example, in Stata, see &lt;code&gt;net describe dthaz, from(http://www.doyenne.com/stata)&lt;/code&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-26T16:52:09.980" Id="95312" LastActivityDate="2014-04-26T16:52:09.980" OwnerUserId="44269" ParentId="21278" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;&lt;em&gt;Post hoc&lt;/em&gt; pair-wise tests following rejection of Kruskal-Wallis should be performed with Dunn's test, not a run of the mill rank sum test, because (1) the Kruskal-Wallis test decision was based on a separate set of rankings, and (2) Dunn's test uses a pooled estimate of the variance implied by the null hypothesis. See also: &lt;a href=&quot;http://stats.stackexchange.com/questions/25815/difference-between-post-hoc-test-after-kruskal-wallis-and-bonferroni-corrected-m/95270#95270&quot;&gt;Difference between post-hoc test after Kruskal-Wallis and Bonferroni corrected Mann-Whitney tests&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-26T17:29:17.323" Id="95314" LastActivityDate="2014-04-26T17:29:17.323" OwnerUserId="44269" ParentId="67440" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;As ttnphns commented, neither Kruskal-Wallis nor rank sum tests have any assumptions about distributional similarity between groups. There is a point of confusion that somtimes arises in these tests because, while in the most general sense they are tests for &lt;em&gt;stochastic dominance&lt;/em&gt; (e.g. H$_{0} \text{: P}(X_{A} &amp;gt; X_{B}) = \frac{1}{2})$, with two additional assumptions(1) that the distributions are the same shape, and (2) that any differences between the distributions of the groups are differences of central locationthe tests can be interpreted as tests for median difference (e.g. H$_{0} \text{: } \tilde{x}_{A} = \tilde{x}_{b}$).&lt;/p&gt;&#10;&#10;&lt;p&gt;Therefore, significance is not an issue, and there is nothing to &quot;mitigate.&quot; However, substantive interpretation (i.e. stochastic dominance versus median difference) will entail.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-04-26T17:38:39.397" Id="95315" LastActivityDate="2014-04-26T18:34:10.820" LastEditDate="2014-04-26T18:34:10.820" LastEditorUserId="44269" OwnerUserId="44269" ParentId="65735" PostTypeId="2" Score="1" />
  
  
  
  
  <row AcceptedAnswerId="95397" AnswerCount="2" Body="&lt;p&gt;I have developed a general linear model to predict my dependent variable Y. I am unsure on how to present my equation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Should it be: (not sure if the i should be there or not)&lt;/p&gt;&#10;&#10;&lt;p&gt;$Yi= .432 + .320 Age_i + .520 WE_i + .300 JP1_i + .210 JP2_i$&lt;/p&gt;&#10;&#10;&lt;p&gt;or&lt;/p&gt;&#10;&#10;&lt;p&gt;$Y= .432 + .320 Age + .520 WE + .300 JP1 + .210 JP2$&lt;/p&gt;&#10;&#10;&lt;p&gt;Where:&lt;/p&gt;&#10;&#10;&lt;p&gt;WE= work experience;&#10;JP= Job Position;&#10;JP1= floor level staff;&#10;JP2= Managers;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-04-27T09:45:11.577" FavoriteCount="0" Id="95367" LastActivityDate="2014-06-13T15:44:16.607" LastEditDate="2014-04-27T10:13:14.640" LastEditorUserId="307" OwnerUserId="40183" PostTypeId="1" Score="4" Tags="&lt;regression&gt;" Title="Should there be an &quot;i&quot; in a regression equation?" ViewCount="196" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm evaluating a few supervised feature selection algorithms, with a focus on redundancy.&lt;/p&gt;&#10;&#10;&lt;p&gt;As a base correlation statistic, I'm using Mutual Information - so discrete variables are also a focus for simplicity.&lt;/p&gt;&#10;&#10;&lt;p&gt;Does anyone know any good methods of redundant feature generation for discrete data? Either from fully simulated data or, preferably, inserting new redundant features into a 'real-world' dataset.&lt;/p&gt;&#10;&#10;&lt;p&gt;Some simple methods would be:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Taking an existing variable and changing values with some probability distribution.&lt;/li&gt;&#10;&lt;li&gt;Binarizing an existing variable by setting values to either 0 or 1&lt;/li&gt;&#10;&lt;li&gt;Fusing two or more existing variables &lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2014-04-27T12:12:38.557" Id="95379" LastActivityDate="2014-04-27T12:12:38.557" OwnerUserId="10961" PostTypeId="1" Score="0" Tags="&lt;feature-selection&gt;&lt;feature-construction&gt;" Title="Redundant feature generation" ViewCount="8" />
  <row AnswerCount="0" Body="&lt;p&gt;This is my first post ever so I beg you please forgive me any mistake (I'm also super newbie in stats). &#10;I have this dataset that comes from interviews filled by users (divers) about their perception on resources (six shark species) population trends across four decades. Every user was asked to state their perception of population trend for every shark based on five categories: major decrease, decrease, estable, increase, and major increase. They were asked to constrain their answers per decade: trend for species X within decade 80s, trend for the same species within 90s, 00s, etc for the rest species. Worth to note that the amount of answers per decade varied according to the level of experience of divers, which means that there were more divers with experience toward the last two decades, and just few since the 80s (a total of four divers with experience since 80s; 14 since 90s; and, 25 since 2000s). I have attached a figure on the answers of those 25 divers on one shark species.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/OD2XU.png&quot; rel=&quot;nofollow&quot;&gt;Obtained scores from interviewing divers about an X shark species population trend across the last four decades. Answers obtained from 25 divers. &lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/z1Jfu.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I've been looking for the simplest way to assess their level of agreement in any specific trend across decades. Find out that ANOVAs are no near a good way to assess any type of categorical data and literature redirected me into logic mixed models, although I still don't have clear the picture how those will let me assess level of agreement and overall species trends across time. Could anybody please give some light to my analysis and tell me which should be the best statistical test to assess level of agreement and trends across time of this dataset? Will logic mixed models work for this? &#10;Please, I use R so I would like to kindly request your help and point me in any specific R routine/package. Also, if there is anything I should add to make my question clearer, please do let me know.&#10;Thanks so much!!&#10;C.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-04-27T12:26:05.113" Id="95382" LastActivityDate="2014-04-27T12:26:05.113" OwnerUserId="44536" PostTypeId="1" Score="1" Tags="&lt;categorical-data&gt;&lt;model-selection&gt;&lt;multilevel-analysis&gt;&lt;test-for-trend&gt;" Title="what test for analysing level of agreement of users perception on species population trends from ordinal categorical data" ViewCount="16" />
  
  <row Body="&lt;p&gt;Without context it is difficult to answer the question.  If the sample size is 100,000, 0.05 of the sample will represent an adequate number of subjects on which to estimate the interaction effect.  Otherwise you may be right to be concerned.  Interaction tests (double differences) have much lower power and precision than main effect tests (single differences).  The precision of an interaction effect estimate for two binary $X$s is limited by the lowest sample size in the four cells.&lt;/p&gt;&#10;&#10;&lt;p&gt;How did you pre-specify the potential interactions?  Or did you search many interactions?  If the one interaction was not fully pre-specified you have other problems to deal with.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-04-27T12:52:51.267" Id="95385" LastActivityDate="2014-04-27T12:52:51.267" OwnerUserId="4253" ParentId="95372" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;What I have is a medical data set with several variables, all 0-1 variables. I want to make inference about them with logistic regression. I have a few problems:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;I have location variables for the disease. I was advised by my statistic advisor to put them in bins as follows: If it was solely in the right part of the organ then I would mark 1 in the column for right and similarily for left. However if it were in both places I marked in neither of the left and right column but marked one in column both. Using this approach I get error in R, numeric 0 1 error when I use glm in R and I think it is due to how these variables are constructed. Shouldn't I rather have just left and right variables and when we have the disease in both sides I should mark in left and right column and skip the both column and maybe introduce interaction term between left and right (that I would at least do in a linear model).&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Using glm (family binomial for logistic regression) in R I was thinking how to find the best model describing some variable. I started with one usual approach with finding univarietly which variables had p-value less than $0.1$ in Fischer exact test. Then I included those variables in my model and started to delete them after which had the highest p-value. In most medical reasearches I have read when applying multivariate regression I see the usage of p-value $0.05$ but I have a feeling that it might be because of lack of understanding of the subject. When I ranked the model according to AIC and explored the best model I usually got variable with p-value around $0.1$. Which approach is preferably, is it justifyable to just cut of at p-value $0.05$ or should use AIC as an estimator of the best multivariate model? AIC does punish for extra variables and so it shouldnt give one too many variables.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="2" CreationDate="2014-04-27T12:55:13.643" Id="95386" LastActivityDate="2014-04-27T13:12:27.410" OwnerUserId="16666" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;regression&gt;&lt;logistic&gt;&lt;aic&gt;" Title="About logistic regression in R" ViewCount="73" />
  
  
  <row Body="&lt;p&gt;If you simply want a measure and test of correlation between satisfaction and esteem, you (or your statistical software) could calculate a quantity called &lt;a href=&quot;https://en.wikipedia.org/wiki/Spearman%27s_rank&quot; rel=&quot;nofollow&quot;&gt;Spearman's rank correlation coefficient&lt;/a&gt;, $r_{\text{S}}$a &lt;a href=&quot;https://en.wikipedia.org/wiki/Monotonic&quot; rel=&quot;nofollow&quot;&gt;monotonic&lt;/a&gt; measure of correlation. A monotonic correlation assumes only that satisfaction always either changes in the same direction (i.e. either increases or decreases) or remains constant as esteem increases.&lt;/p&gt;&#10;&#10;&lt;p&gt;You could then, while bearing in mind Peter Flom's caveat about your low sample size, test whether $r_{\text{S}}$ is significantly different than 0 (i.e. there is &lt;em&gt;some&lt;/em&gt; correlation, rather than none) using one of several methods detailed in the Wikipedia article. If you have statistical software, it is likely that it will perform such a test for you. If you need help performing such a test (e.g. how to perform a &lt;em&gt;t&lt;/em&gt; test) by hand, that is probably a good candidate for another search through the CrossValidated archives.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-27T15:19:28.417" Id="95398" LastActivityDate="2014-04-27T15:19:28.417" OwnerUserId="44269" ParentId="95388" PostTypeId="2" Score="1" />
  
  
  <row AcceptedAnswerId="95427" AnswerCount="1" Body="&lt;p&gt;Consider a variant of the classical linear model:&lt;/p&gt;&#10;&#10;&lt;p&gt;$y_i=a + b\left(x_i-\bar{x} \right)+e_i $ &lt;/p&gt;&#10;&#10;&lt;p&gt;Because $e_i \sim N \left (0, \sigma^2 \right)$, $y_i \sim N \left(a+ b \left(x_i -\bar{x} \right), \sigma^2 \right)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The OLS estimates of the slope and the intercept coefficient are:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\hat{b}=\frac{\sum \left(x_i - \bar{x} \right) \left(y_i-\bar{y} \right)}{\sum \left(x_i - \bar{x} \right)^2} $ (as usual),   &lt;strong&gt;but&lt;/strong&gt;   $\hat{a}=\bar{y}$ &lt;/p&gt;&#10;&#10;&lt;p&gt;The two estimates, being a linear combination of normal variables are themselves normal with $\hat{b}\sim N \left( b ,\frac{ \sigma^2}{\sum \left(x_i - \bar{x} \right)^2}\right)$ and $\hat{a} \sim   N \left( a, \frac{\sigma^2}{n}\right)$&lt;/p&gt;&#10;&#10;&lt;p&gt;I then need to show that the covariance between $\hat{a}$ and $\hat{b}$ is $0$&lt;/p&gt;&#10;&#10;&lt;p&gt;but I am stuck evaluating $$E \left[ \hat{a} \hat{b} \right]=\frac{1}{\sum \left( x_i-\bar{x} \right)^2} E \left[\bar{y} \sum \left(x_i -\bar{x} \right) y_i \right] $$&lt;/p&gt;&#10;&#10;&lt;p&gt;Could you please help me here?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-27T16:51:06.457" Id="95414" LastActivityDate="2014-04-27T19:48:10.983" LastEditDate="2014-04-27T18:57:17.410" LastEditorUserId="31420" OwnerUserId="31420" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;self-study&gt;&lt;linear-model&gt;&lt;covariance&gt;" Title="Covariance between OLS estimates in a non-standard linear model" ViewCount="255" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I have a model with multiple categories for several categorical variables. I'm interested in prediction. So far I've been using step(). The issue with it is that it doesn't treat variables with multiple categories as 1 block, so to speak. So in the output, I might get 2 of the levels of x1 and say 1 of the levels of x2. My question is, is there a way to do this using step() s.t., an entire variable (as in all the categories) are selected/rejected? And if not, is there some other way of doing this in R, using some other command? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-27T19:15:38.457" Id="95430" LastActivityDate="2014-04-27T21:02:59.530" OwnerUserId="44560" PostTypeId="1" Score="0" Tags="&lt;computational-statistics&gt;" Title="Categorical Data with multiple categories (variable selection)" ViewCount="47" />
  <row AnswerCount="1" Body="&lt;p&gt;I have 3 replicas for a value in different individuals. Each of these values are ratios $ab$, and $a$ and $b$ are means from $n=20$ sample pool each. Thus there are 3 times each ratio $ab$ for each individual. When comparing for differences is student t-test correct?&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;                 1st sampling -   2nd         -   3rd         -   4th&#10;individual 1  -  0.7164165213 -  0.6057539083 -  0.5242174359 -  0.7670756899&#10;individual 2  -  0.6540839702 -  0.8140762612 -  0.6057645321 -  X&#10;individual 3  -  0.611493629  -  0.7270260938 -  0.5255522645 -  0.9964242368&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;as said each sampling value comes from A/B, whereas A and B are means for treated and non treated for a given variable with n ranging from 15-25. &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-04-27T19:17:50.377" Id="95431" LastActivityDate="2014-12-15T07:01:54.357" LastEditDate="2014-06-28T04:42:39.287" LastEditorUserId="805" OwnerUserId="44559" PostTypeId="1" Score="-1" Tags="&lt;hypothesis-testing&gt;" Title="proper statistical test to check differences" ViewCount="80" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm working on stats homework, and I've come across a problem I cannot figure out. It's a multi-part problem, and I've correctly answered a number of the questions, but there are a few that are not coming up right.&lt;/p&gt;&#10;&#10;&lt;p&gt;Problem:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Healthy bones are continually being renewed by two processes. Through bone formation, new bone is built; through bone resorption, old bone is removed. If one or both of these processes is disturbed, by disease, aging, or space travel, for example, bone loss can be the result. Tartrate resistant acid phosphatase (TRAP) is a biochemical marker for bone resorption that is also measured in blood. The data below gives values for this biomarker and a measure of bone resorption &lt;code&gt;VO--&lt;/code&gt;. Analyze these data using the questions in the previous exercise as a guide. (Give answers accurate to 2 decimal places.) &lt;a href=&quot;https://www.webassign.net/mintrostat/data/10_13/data1.dat&quot; rel=&quot;nofollow&quot;&gt;Data&lt;/a&gt;&lt;/p&gt;&#10;  &#10;  &lt;p&gt;Analyze these data using the logs of both TRAP and &lt;code&gt;VO--&lt;/code&gt;.&#10;  Run the regression using TRAP to predict &lt;code&gt;VO--&lt;/code&gt;. Summarize the results. (Round your answers to two decimal places.)&lt;br&gt;&#10;  $\log($VO--$) = \mathbf{5.62} + .42 \cdot \log($TRAP$)$&lt;br&gt;&#10;  $F = \mathbf{1590.97}$&lt;br&gt;&#10;  $P = \mathbf0$&lt;br&gt;&#10;  $R^2= .1$&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I wasn't quite sure how to figure out the answers, but I entered the data into list 3 and list 4 on my TI-83 calculator. Then I entered &lt;code&gt;ln(L3)&lt;/code&gt; into list 1 and &lt;code&gt;ln(L4)&lt;/code&gt; into list 2. From there, I did &lt;code&gt;STAT &amp;gt; TESTS &amp;gt; LinReg(a+bx)&lt;/code&gt; to get the regression equation: $y = 5.62 + .42x$ and $R^2= .1$. I input these numbers, and the $R^2$ and slope were correct, but the $y$-intercept was not. I then used &lt;code&gt;STAT &amp;gt; TESTS &amp;gt; ANAOVA(L1, L2)&lt;/code&gt; to get $F = 1590.97$ and $P = 0$, but neither of those answers were correct. I've listed my answers above with incorrect answers bold. Can anyone help let me know what I'm doing wrong? I'd be very appreciative!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-04-27T21:44:50.887" Id="95436" LastActivityDate="2014-04-27T22:01:20.230" LastEditDate="2014-04-27T22:01:20.230" LastEditorUserId="32036" OwnerUserId="44531" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;self-study&gt;&lt;p-value&gt;&lt;f-test&gt;" Title="Regression: finding p-value and F" ViewCount="54" />
  <row Body="&lt;p&gt;After taking the advice from above, I have formed up the following equations. Appreciate some advice if I am doing it right.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/IPYN7.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-04-28T06:25:00.627" Id="95465" LastActivityDate="2014-04-28T06:25:00.627" OwnerUserId="37976" ParentId="95412" PostTypeId="2" Score="0" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I am doing factor analysis with Principal component extraction method.&lt;/p&gt;&#10;&#10;&lt;p&gt;Below is the Structure matrix&lt;/p&gt;&#10;&#10;&lt;p&gt;Extraction method: PCA&lt;/p&gt;&#10;&#10;&lt;p&gt;Rotation method: Promax with kaiser normalization&#10;....................................................................COMPONENT.............................................................&#10;.............................................................  1 ............   2  ......... 3 .............4 ..........................................&lt;br&gt;&#10;Effeciency class ............................    .893..........            .236........               .139.........           .876&lt;/p&gt;&#10;&#10;&lt;p&gt;Pre sales support.........................              .839 .........          -.224 ........               .811..........&#10;          .326&lt;/p&gt;&#10;&#10;&lt;p&gt;I have two variable Eff_class and Pre_sales support and four component..Now variable seems to load alomost equally on two components. How to decide which component to choose.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-28T11:28:13.890" Id="95486" LastActivityDate="2014-11-27T08:42:51.063" LastEditDate="2014-04-28T12:17:33.183" LastEditorUserId="36045" OwnerUserId="36045" PostTypeId="1" Score="-1" Tags="&lt;pca&gt;&lt;factor-analysis&gt;" Title="Which component to choose when variable load heavily on more than one component" ViewCount="73" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I want to calculate the test log likelihood of an estimated Gaussian graphical model inverse covariance matrix in leave-one-out cross validation test in R. So, what I want to calculate is $log(det(\theta))-trace(S\theta)$ where $S$ is the sample covariance matrix of the CV test data and $\theta$ is the inverse covariance matrix estimated using CV training data. The problem is that, since test data contains just one sample (dimension of $1\times p$ where $p$ is the number of variables), the empirical covariance matrix in this formulation ($S$) is not $p \times p$ but it is $1 \times 1$ (calculated using $var()$ or $cov()$ functions in R). But $\theta$ is estimated using $n-1$ samples of training data, so it is $p\times p$, so I am confused about how to calculate $trace(S\theta)$ since the dimensions of $S$ and $\theta$ are not suitable to multiply. If the sample size of test data is bigger than 1 (other cross validations than leave-one-out), there is no such a problem. So, how to calculate GGM test log likelihood for leave-one-out CV? Thanks.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-28T14:03:37.793" Id="95507" LastActivityDate="2014-04-28T14:16:20.340" LastEditDate="2014-04-28T14:16:20.340" LastEditorUserId="36564" OwnerUserId="36564" PostTypeId="1" Score="0" Tags="&lt;cross-validation&gt;" Title="Leave-one-out cross-validation for Gaussian graphical models inverse covariance estimation" ViewCount="70" />
  
  <row Body="&lt;p&gt;I don't know if this should go in a comment instead, but I wanted to point the following out:&lt;/p&gt;&#10;&#10;&lt;p&gt;Under the &lt;a href=&quot;http://en.wikipedia.org/wiki/Best_linear_unbiased_estimator&quot;&gt;Gauss-Markov assumptions&lt;/a&gt;, the OLS estimator is the &lt;em&gt;best linear unbiased estimator&lt;/em&gt;. Here, &quot;best&quot; means minimizing variance. However, if we deviate from the assumption of homoskedasticity, i.e. a constant variance on the errors, the following happens&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;OLS is &lt;em&gt;still&lt;/em&gt; unbiased. &lt;/li&gt;&#10;&lt;li&gt;It is no longer the minimum-variance estimator, i.e. it is &lt;a href=&quot;http://en.wikipedia.org/wiki/Statistical_efficiency&quot;&gt;inefficient&lt;/a&gt;.&lt;/li&gt;&#10;&lt;li&gt;The estimate of the variance of the coefficients - $\hat{V(\hat{\beta})}$ becomes &lt;em&gt;biased&lt;/em&gt;, which means that our standard inferential techniques - the $t$ and $F$-tests - becomes invalid. &lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="6" CreationDate="2014-04-28T16:07:25.520" Id="95529" LastActivityDate="2014-04-28T16:07:25.520" OwnerUserId="11878" ParentId="95505" PostTypeId="2" Score="5" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Can anyone suggest a standard reference (benchmark) dataset using replicate weights comparable to NIST's Statistical Reference Datasets (&lt;a href=&quot;http://itl.nist.gov/div898/strd/&quot; rel=&quot;nofollow&quot;&gt;http://itl.nist.gov/div898/strd/&lt;/a&gt;) for software testing? Ideally the data would have balanced repeated replication (BRR) weights. Thank you.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-28T16:43:11.750" Id="95534" LastActivityDate="2014-04-28T16:43:11.750" OwnerUserId="44598" PostTypeId="1" Score="0" Tags="&lt;references&gt;" Title="standard reference (benchmark) dataset using replicate weights" ViewCount="7" />
  
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I am looking for the word/phrase that is used to say that a (potential) erroneous conclusion has been drawn from data without having a correlation to other supporting data points.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, if someone has the dataset of the number of red cars sold in the US per month as &lt;code&gt;Jan: 10, Feb: 10, Mar: 9, Apr: 8, May: 7, etc.&lt;/code&gt; and draws the conclusion of &lt;code&gt;&quot;Red cars are going out of style&quot;&lt;/code&gt; without correlating this data to total car sales for the same time period, or the number of other color cars sold, etc.&lt;/p&gt;&#10;&#10;&lt;p&gt;This conclusion might be correct if there was data showing that car sales as a whole have not declined, but this conclusion could be false if car sales as a whole have declined, etc, etc.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is this called &quot;Sampling Bias&quot; or something else?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-28T19:59:43.573" Id="95563" LastActivityDate="2015-01-21T15:43:59.727" OwnerUserId="44612" PostTypeId="1" Score="1" Tags="&lt;correlation&gt;&lt;bias&gt;" Title="Drawing a conclusion from data without a correlation to other supporting data" ViewCount="36" />
  <row Body="&lt;p&gt;Since you clearly state that your main goal is to test whether both groups come from the same population, I do believe the K-S test would do fine. My guess is that Wilcoxon would be redundant if K-S test is not significant.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-28T20:23:27.657" Id="95566" LastActivityDate="2014-04-28T20:23:27.657" OwnerUserId="44613" ParentId="95559" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="95596" AnswerCount="1" Body="&lt;p&gt;I am working on predictive model and when I evaluate it, I find good &lt;code&gt;accuracy_score&lt;/code&gt;, &lt;code&gt;precision_score&lt;/code&gt;, &lt;code&gt;recall_score&lt;/code&gt;, and &lt;code&gt;f1_score&lt;/code&gt;. But I don't get good results using confusion matrix. &lt;/p&gt;&#10;&#10;&lt;p&gt;What is the metric that should I Trust ?&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;accuracy_score:  0.987727165447&#10;confusion_matrix: [[338961   3857 ]&#10;                   [   252     27  ]]&#10;precision_score: 0.998449790932&#10;recall_score:    0.987727165447&#10;f1_score:        0.993031083087&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I am using Scikit-Learn metrics.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;precision sklearn.metrics.precision_score&#10;recall    sklearn.metrics.recall_score&#10;f1        sklearn.metrics.f1_score&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-04-28T21:32:23.030" Id="95577" LastActivityDate="2014-04-29T02:13:52.243" LastEditDate="2014-04-29T02:13:52.243" LastEditorUserId="7290" OwnerUserId="43653" PostTypeId="1" Score="2" Tags="&lt;predictive-models&gt;&lt;prediction&gt;&lt;scikit-learn&gt;&lt;unbalanced-classes&gt;&lt;mathematics&gt;" Title="Which metric should I trust to evaluate my predictive model" ViewCount="214" />
  <row AnswerCount="1" Body="&lt;p&gt;Say we have a time series $x_i, i=1, \dots$. If it is hard to model, then is it possibly acceptable to model the resampled one $y_i := x_{ik}, i = 1, \dots$ for some fixed natural number $k$? (Assume no special requirement or restriction, just a general case.)&lt;/p&gt;&#10;&#10;&lt;p&gt;If I have a model for $y_i$'s, can that be helpful in modeling the original $x_i$'s, and how? Thanks!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-28T22:22:46.987" Id="95580" LastActivityDate="2014-04-29T11:56:04.183" LastEditDate="2014-04-29T11:56:04.183" LastEditorUserId="30201" OwnerUserId="1005" PostTypeId="1" Score="0" Tags="&lt;time-series&gt;" Title="Can a model for a resampled time series be useful for modeling the original time series?" ViewCount="28" />
  <row Body="&lt;p&gt;Usually, scheduling based on seeding seems to be set up to improve the chance to retain the better players. &lt;/p&gt;&#10;&#10;&lt;p&gt;Here's one obvious possibility:&lt;/p&gt;&#10;&#10;&lt;p&gt;Going by seed, if we want to maximize the chances of keeping the top few seeds, we want the highest seed they face to be low and within that, the average seed they face to be fairly low. &lt;/p&gt;&#10;&#10;&lt;p&gt;One approach: Consider that the second-best player in each group is the 'main danger' to the top seeds. We can hold the average of the seedings of the 2nd and 3rd ranked player in each group constant, while keeping the lowest of the second-ranked seeds for the top seed, the second-lowest second-ranked player for the second seed and so on:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; 1 24 25&#10; 2 23 26&#10; 3 22 27&#10; 4 21 28&#10; 5 20 29&#10; 6 19 30&#10; 7 18 31&#10; 8 17 32&#10; 9 16 33&#10;10 15 34&#10;11 14 35&#10;12 13 36&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This has the advantage that the top second-ranked players also have a reasonable chance to go through (it's not &lt;em&gt;that&lt;/em&gt; much worse being 13th seed than 12th), so the difficulty of the draw for the top 2/3 of players seems to increase relatively smoothly. It might be particularly appropriate for the top 2/3 of players if the strengths in the bottom third are more nearly even than in the top groups (though the bottom-ranked players might then argue that the schedule for 36 is easier than for 25).&lt;/p&gt;&#10;&#10;&lt;p&gt;You could flip the third column top to bottom, but it's not clear to me that it's better. This would make the progression from easy to hard more rapid as you go down the first column of seed (weighting it more strongly to the top few seeds):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; 1 24 36&#10; 2 23 35&#10; 3 22 34&#10; 4 21 33&#10; 5 20 32&#10; 6 19 31&#10; 7 18 30&#10; 8 17 29&#10; 9 16 28&#10;10 15 27&#10;11 14 26&#10;12 13 25&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This could be more suitable, for example, if the strength in the bottom third goes down more quickly than in the top groups (if the top seeds were more nearly evenly matched than the lowest-ranked 12 players were). If you want to focus on the task for the bottom 12 players, this makes the progression in difficulty more clear for them.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you have some model for the chances of going through that might give you a better idea of which of those schedules will have the best chances of giving outcomes that are 'desirable'.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-04-29T01:19:15.200" Id="95593" LastActivityDate="2014-04-29T01:57:33.203" LastEditDate="2014-04-29T01:57:33.203" LastEditorUserId="805" OwnerUserId="805" ParentId="95588" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;It can be the median.  Not always, but sometimes.  I have no idea what it is called in other occasions.  Hope this helped.  (At least a little.)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-29T02:32:49.873" Id="95599" LastActivityDate="2014-04-29T02:32:49.873" OwnerUserId="44632" ParentId="16198" PostTypeId="2" Score="-2" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I am prepping for an interview and here is one of the questions I have come across:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;How would you conduct an A/B test on an opt-in feature?&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I think you cannot perform a &quot;fair&quot; test with an opt-in option.  People viewing the A version of the site will come from the whole distribution of users whereas people viewing B will only come from the set who have opted in.  This seems to violate the conditions for a controlled experiment.&lt;/p&gt;&#10;&#10;&lt;p&gt;What is your take on this?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-04-29T10:41:55.237" Id="95620" LastActivityDate="2014-04-30T08:10:38.203" LastEditDate="2014-04-30T07:44:01.707" LastEditorUserId="264" OwnerUserId="17754" PostTypeId="1" Score="1" Tags="&lt;self-study&gt;&lt;ab-test&gt;" Title="How to conduct am A/B test for a feature which cannot be accessed by every visitor?" ViewCount="105" />
  <row AnswerCount="1" Body="&lt;p&gt;I have developed a system to trace the outlines of (images of) objects. Now I want to test whether two independent traces represent a common feature.&lt;/p&gt;&#10;&#10;&lt;p&gt;Imagine two people (or machines) tracing the outline of a feature in an image, recording it as sequence of vertices. Inaccuracies in recognizing the feature boundaries and in specifying the vertices can be viewed as random errors in vertex positions. The problem is the two traces might use completely different (Cartesian) coordinate systems (set up on two digitizing tablets, for instance). The null hypothesis to test is that they represent a common feature.&lt;/p&gt;&#10;&#10;&lt;p&gt;This is illustrated below.  I drew a figure and recorded the $x$ and $y$ co-ordinates of its vertices in its coordinate system. Let's call this figure $M$.  It is represented as a sequence $(x_i,y_i), i=1, 2, \ldots, m$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then I drew the same figure in a bigger size in another coordinate system (with no known relationship to the first coordinates system) and recorded the $x$ and $y$ co-ordinates. Let's call this $N$, represented as a sequence $(x_i^\prime, y_i^\prime), i=1, 2, \ldots, n$.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;&#10;Having these data, how can I test whether the figures $M$ and $N$ represent the same image features even though they have different sizes and co-ordinates?&lt;/p&gt;&#10;&#10;&lt;p&gt;If the conclusion is that yes, they do represent a common object, then how can I estimate a similarity transformation between $M$ and $N$ so that I can work on formulas or equations to check the results with different figures?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/VNR37.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="12" CreationDate="2014-04-29T11:52:48.357" Id="95622" LastActivityDate="2014-05-01T13:00:18.990" LastEditDate="2014-04-29T17:18:51.097" LastEditorUserId="919" OwnerUserId="27494" PostTypeId="1" Score="4" Tags="&lt;hypothesis-testing&gt;&lt;similarities&gt;&lt;geometry&gt;" Title="Finding similarity between the objects ( Actually how to justify the similarity of objects in mathematics form )" ViewCount="139" />
  <row AnswerCount="2" Body="&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/E2C95.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I am using an Extended Technology Acceptance Model and have adopted an instrument from a research paper. &#10;After performing factor analysis in SPSS, most questions related to different constructs (attitude, perceived usefulness, etc.) are shown in the same factor.&lt;/p&gt;&#10;&#10;&lt;p&gt;The variables &lt;code&gt;Attitude&lt;/code&gt;, &lt;code&gt;Perceived ease of use&lt;/code&gt;,  &lt;code&gt;Perceived usefulness&lt;/code&gt; &amp;amp; &lt;code&gt;Innovativeness&lt;/code&gt; are different but if a respondent agrees to my statements for &lt;code&gt;Perceived usefulness&lt;/code&gt; with &lt;code&gt;STRONGLY AGREE&lt;/code&gt; then he is also likely to answer for statements related to attitude with &lt;code&gt;STRONGLY AGREE&lt;/code&gt; since there is a positive effect (theoretically and based upon lit. reviews) between all these variables in the model.&lt;/p&gt;&#10;&#10;&lt;p&gt;What should I do if respondents are answering everything as &lt;code&gt;STRONGLY AGREE&lt;/code&gt; or &lt;code&gt;AGREE&lt;/code&gt;? &lt;/p&gt;&#10;&#10;&lt;p&gt;Technically it is right, but how can I get my factor analysis results right? Excluding the dependent variable (&lt;code&gt;Behavioral Intention&lt;/code&gt;) it should make 4 factors.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;sup&gt;After then I've got to learn Amos (as per my supervisor instruction) for analysis. I need guidance in easy wordings. I'm no good in SPSS and not familiar with Amos.&lt;/sup&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-04-29T13:44:40.317" Id="95638" LastActivityDate="2014-05-09T10:06:04.463" LastEditDate="2014-04-30T18:43:32.627" LastEditorUserId="32036" OwnerUserId="44653" PostTypeId="1" Score="0" Tags="&lt;spss&gt;&lt;factor-analysis&gt;&lt;sem&gt;&lt;eda&gt;&lt;confirmatory-factor&gt;" Title="factor analysis problem in structural equation model" ViewCount="164" />
  <row Body="&lt;p&gt;RMSE is definitely the first thing that comes to mind. It punishes large deviations more than small ones, and its size is meaningful in terms of the underlying variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can also define your own error function, if that is not suitable for your needs. Say, if small errors of 1-2 are not important when the baseline is large, you could define some sort of relative error (say $|y_i - \hat y_i|/y_i$). &lt;/p&gt;&#10;&#10;&lt;p&gt;Overall I would suggest you use a measure that is both familiar, and ideally the same measure you use to train your model. RMSE should fit both those criteria, unless you have a strong reason to prefer something else.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-04-29T14:37:27.940" Id="95651" LastActivityDate="2014-04-29T14:50:44.337" LastEditDate="2014-04-29T14:50:44.337" LastEditorUserId="43959" OwnerUserId="43959" ParentId="95641" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="96096" AnswerCount="1" Body="&lt;p&gt;Hypothesis tests for equivalence differ from the more common hypothesis tests for difference.&lt;/p&gt;&#10;&#10;&lt;p&gt;In tests for difference, the null hypothesis is some form of &quot;separate quantities are the same&quot;, and extreme enough evidence prompts rejection in favor of a conclusion that &quot;separate quantities are different.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;In tests for equivalence, the null hypothesis is some form of &quot;separate quantities differ by at least $\Delta$&quot; and extreme enough evidence prompts rejection in favor of a conclusion that &quot;separate quantities are equivalent within an interval defined by $\Delta$.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Pro-tip:&lt;/strong&gt; Combining inference from tests for difference with tests for equivalence &lt;em&gt;rocks&lt;/em&gt;, because it places power and relevant effect size within the testing framework (following Reagle &amp;amp; Vinod (2003), I adopt the nomenclature H$^{+}_{0}$ refers to the &lt;em&gt;positivist&lt;/em&gt; null hypothesis associated with a test for difference, and H$^{-}_{0}$ refers to the &lt;em&gt;negativist&lt;/em&gt; null hypothesis associated with a test for equivalence):&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ADW3r.png&quot; alt=&quot;combined inference from difference and equivalence tests&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I am comfortable enough articulating and calculating the &lt;em&gt;two one-sided tests&lt;/em&gt; (TOST; see for example, Hauck and Anderson, 1984 or Schuirmann, 1987) approach to tests for equivalence (i.e. H$^{-}_{0}\text{: }\left|\theta\right| \ge \Delta$  translates the to one-sided tests H$^{-}_{01}\text{: }\theta \ge \Delta$ or H$^{-}_{02}\text{: }\theta \le -\Delta$, and rejecting both of these implies H$_{\text{A}}\text{: }-\Delta &amp;lt; \theta &amp;lt;\Delta$). However I am still undertaking the steep learning curve for &lt;em&gt;uniformly most powerful&lt;/em&gt; (UMP) tests for equivalence.&lt;/p&gt;&#10;&#10;&lt;p&gt;In intuitive terms:&lt;/p&gt;&#10;&#10;&lt;p&gt;What is the motivation for the UMP equivalence tests? I gather that the interval hypothesis H$^{-}_{0}$ alters rejection probabilities by way of noncentral distributions. But I don't understand how that works in a general sense.&lt;/p&gt;&#10;&#10;&lt;p&gt;Aside from regulatory preferenece for TOST, what considerations would lead to a preference from TOST versus UMP equivalence tests? One thing I like about TOST is that the equivalence term can be expressed &lt;em&gt;and communicated&lt;/em&gt; easily either in units of the measured variable or in units of the test statistic's distribution, and these quantities are readily translated back and forth. Less clear to me are the units of the equivalence term in UMP equivalence tests.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Reagle, D. P. and Vinod, H. D. (2003). Inference for negativist theory using numerically computed rejection regions. &lt;em&gt;Computational Statistics &amp;amp; Data Analysis&lt;/em&gt;, 42(3):491512.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hauck, W. W. and Anderson, S. (1984). A new statistical procedure for testing equivalence in two-group comparative bioavailability trials. &lt;em&gt;Journal of Pharmacokinetics and Pharmacodynamics&lt;/em&gt;, 12(1):8391.&lt;/p&gt;&#10;&#10;&lt;p&gt;Schuirmann, D. A. (1987). A comparison of the two one-sided tests procedure and the power approach for assessing the equivalence of average bioavailability. &lt;em&gt;Pharmacometrics&lt;/em&gt;, 15(6):657680.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-29T16:29:09.507" FavoriteCount="2" Id="95667" LastActivityDate="2014-05-02T18:05:16.700" LastEditDate="2014-05-02T18:05:16.700" LastEditorUserId="44269" OwnerUserId="44269" PostTypeId="1" Score="2" Tags="&lt;equivalence&gt;&lt;tost&gt;" Title="Intuitive explanation of differences between TOST and UMP tests for equivalence" ViewCount="258" />
  
  <row AcceptedAnswerId="95697" AnswerCount="1" Body="&lt;p&gt;I was studying Deep Belief Network (DBN) and have questions. &lt;/p&gt;&#10;&#10;&lt;p&gt;1) According to the definition of DBN, DBN is formed by stacking RBM on top of each other such that the hidden layer in a lower layer becomes the input layer in the above layer. However, when I read the papers by Geoff Hinton (for example, &quot;a fast learning algorithm for deep belief nets&quot;), his DBN doesn't seem to have multiple RBMs. His DBN has only one RBM (i.e., undirected Markov Random Field with two layers) which sits on top  while the other layers have only directional edges. I am confused by this difference. &lt;/p&gt;&#10;&#10;&lt;p&gt;Are these two architectures essentially same? When the term, &quot;DBN&quot; is used in literatures, does it refer to Geoff Hinton's DBN? Or does it refer to a general class of multi-layer architectures with one or multiple RBMs.&lt;/p&gt;&#10;&#10;&lt;p&gt;2) Following up the above question, does anybody know why Geoff Hinton's DBN has undirected RBM (called associative memory) on top? What role does it play? What would happen if, instead of undirected edges, directed edges are used on top?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks,&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-04-29T17:29:30.960" Id="95672" LastActivityDate="2014-04-29T20:00:35.887" OwnerUserId="27783" PostTypeId="1" Score="0" Tags="&lt;deep-learning&gt;&lt;deep-belief-networks&gt;" Title="definition of deep belief network" ViewCount="111" />
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have data from an experiment that evaluated 3 methods for recommendation of news articles based on their content.&#10;The experiment showed an article that the user read and 6 recommendations presented in random order below it (screenshot: &lt;a href=&quot;http://cl.ly/image/2R10073Z0i39&quot; rel=&quot;nofollow&quot;&gt;http://cl.ly/image/2R10073Z0i39&lt;/a&gt;). The task of the participants was to choose one recommendation that they found the most interesting.&#10;The 6 recommendations were generated by 3 methods, each creating 2 recommendations.&lt;/p&gt;&#10;&#10;&lt;p&gt;I evaluated precision, recall and the F1 measure. But I received feedback that since the task was to choose one recommendation from 6, the users didn't have a chance to respond to every recommendation and so precision can't be applied.&#10;I also calculated the R-score (Expected Utility) of the methods (the problem here is that the user saw only 2 recommendations from each method at once).&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is, what would be the best way to evaluate the 3 methods based on the data from this experiment?&#10;I have data about user clicks, read articles and the recommendations that the users saw (actually I have top 10 recommendations by each method at each point - including the ones the users didn't see).&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-29T20:06:38.250" Id="95700" LastActivityDate="2014-04-29T20:06:38.250" OwnerUserId="44682" PostTypeId="1" Score="0" Tags="&lt;recommender-system&gt;&lt;precision-recall&gt;" Title="Evaluating methods for news recommendation from an online experiment" ViewCount="42" />
  
  <row Body="&lt;p&gt;Absolute values of skewness and kurtosis could be both indexes of deviation from the normal distribution (be careful for the exact definition of kurtosis since most of statistical software subtract 3 by default). Then, having these numbers in two columns the most simple solution is to create a scatterplot and visually spot the outliers as the points that will lay far from the main body of the shape. &lt;/p&gt;&#10;&#10;&lt;p&gt;If this method is not successful then as a more quantitative method I would &lt;em&gt;not&lt;/em&gt; propose the formula |0 - Skewness| * |3 - Kurtosis| since there is no reason for a distribution to have &lt;em&gt;both&lt;/em&gt; these values large or small, thus multiplication is not the appropriate procedure to detect deviations from normality. Instead of this and especially in case you will program it your self, I would propose as a simple solution the computation of the Euclidean distance from the skewness and kurtosis mean values $mean_{skew}$ and $mean_{kurt}$ defined as &#10;$$&#10;d(cluster) = \sqrt{(mean_{skew} - \text{cluster skew})^2 + (mean_{kurt} - \text{cluster kurt})^2}&#10;$$&#10;and consider as outliers the greater 5% (or 10% it is up to you) of these distances.&lt;/p&gt;&#10;&#10;&lt;p&gt;Another option, that is used routinely in ANOVA methods is the computation of Mahalanobis distance for each cluster's pair of statistics (skewness, kurtosis) and consider as outliers all clusters with Mahalanobis distance greater than 13.82 which is the critical value of chi square distribution with 2 degrees of freedom at 0.001 significance level. If no cluster has Mahalanobis distance larger than 13.82 then you may simply consider as outliers the predefined percent of clusters with the larger absolute Mahalanobis distances.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hope this will help you.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-29T20:29:40.557" Id="95707" LastActivityDate="2014-04-29T21:09:29.323" LastEditDate="2014-04-29T21:09:29.323" LastEditorUserId="27608" OwnerUserId="27608" ParentId="95642" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Problem 1: Agreed, play the game.  The key here is that you know the actual probabilities of winning 5 vs 20 since the outcome is dependent upon the flip of a fair coin.&lt;/p&gt;&#10;&#10;&lt;p&gt;Problem 2: The problem is the same as problem 1 because you are told that there is an equal probability that either 5 or 20 is in the other envelope.&lt;/p&gt;&#10;&#10;&lt;p&gt;Problem 3: The difference in problem 3 is that telling me the other envelope has either $X/2$  or $2X$ in it does not mean that I should assume that the two possibilities are equally likely for all possible values of $X$.  Doing so implies an improper prior on the possible values of $X$.  See the Bayesian resolution to the paradox. &lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-04-29T20:31:48.657" Id="95710" LastActivityDate="2014-04-29T20:31:48.657" OwnerUserId="44451" ParentId="95694" PostTypeId="2" Score="3" />
  
  
  <row AcceptedAnswerId="95744" AnswerCount="1" Body="&lt;p&gt;I have noticed that the sign at the front of the log-likelihood ratio formula changes depending on what source I have been looking at.&lt;/p&gt;&#10;&#10;&lt;p&gt;Below are the two formula I have seen, exactly as they were written (in case the notation is offering something I am missing):&lt;/p&gt;&#10;&#10;&lt;p&gt;$LLR = -2 ( LL_{\rm Simple} - LL_{\rm General})$&#10;(&lt;a href=&quot;http://warnercnr.colostate.edu/~gwhite/fw663/LikelihoodRatioTests.PDF&quot; rel=&quot;nofollow&quot;&gt;Source&lt;/a&gt;)&lt;/p&gt;&#10;&#10;&lt;p&gt;or&#10;$LLR = 2(LL_{\rm new} - LL_{\rm baseline}$)&#10;(Source: &lt;em&gt;Discovering Statistics Using SPSS&lt;/em&gt;, By Andy Field)&lt;/p&gt;&#10;&#10;&lt;p&gt;I was wondering if someone could tell me the significance of the sign at the front of the model? My suspicion is that it is something to do with whether you are introducing or removing parameters.&#10;Thanks.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-29T23:38:58.073" Id="95739" LastActivityDate="2014-04-30T00:37:04.770" LastEditDate="2014-04-30T00:22:46.987" LastEditorUserId="32036" OwnerUserId="27881" PostTypeId="1" Score="1" Tags="&lt;chi-squared&gt;&lt;likelihood-ratio&gt;" Title="Log-Likelihood Ratio Test: Difference of Equations" ViewCount="229" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Say I have to mix &lt;strong&gt;n&lt;/strong&gt; double Truncated Normal distributions $\textit{TNormal(,$^{2}$,0,1)}$. Furthermore, each distribution has a weight &lt;strong&gt;w&lt;/strong&gt;. &lt;strong&gt;w&lt;/strong&gt; represents the influence of the distribution in the mixture: the greatest &lt;strong&gt;w&lt;/strong&gt;, the greatest is its influence in the mixture. The problem is to calculate the  and $^{2}$ for the resulting distribution (I am always using &lt;em&gt;0&lt;/em&gt; and &lt;em&gt;1&lt;/em&gt; as the bounds to truncate the Normal distribution).&lt;/p&gt;&#10;&#10;&lt;p&gt;This is the procedure I am using to mix the distributions:&lt;/p&gt;&#10;&#10;&lt;p&gt;1) For each &lt;em&gt;TNormal&lt;/em&gt;, I get a sample that represents the population &quot;well enough&quot;. For this purpose, I am getting a large sample (i.e., 100000 samples).&lt;/p&gt;&#10;&#10;&lt;p&gt;2) I use the following equation to calculate the resulting &lt;strong&gt;$_{y}$&lt;/strong&gt;:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/I0X7l.png&quot; alt=&quot;Equation to calculate the resulting **$_{y}$**&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Where &lt;strong&gt;X$_{i}$&lt;/strong&gt; is a sample of a distribution to be mixed and &lt;strong&gt;w$_{i}$&lt;/strong&gt; its weight.&lt;/p&gt;&#10;&#10;&lt;p&gt;The R code for this is:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;for (i in 1:100000) {Y[i] = ((wA*A[i])+(wB*B[i]))/(wA+wB}&lt;/code&gt; where &lt;strong&gt;Y&lt;/strong&gt; is the resulting &lt;em&gt;&lt;/em&gt;, &lt;strong&gt;A&lt;/strong&gt; and &lt;strong&gt;B&lt;/strong&gt; are the distributions to be mixed, &lt;strong&gt;wA&lt;/strong&gt; is the weight for distribution &lt;strong&gt;A&lt;/strong&gt; and &lt;strong&gt;wB&lt;/strong&gt; is the weight for distribution &lt;strong&gt;B&lt;/strong&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;In other words, &lt;strong&gt;$_{y}$&lt;/strong&gt; is a set of values calculated given samples of the mixed distributions.&lt;/p&gt;&#10;&#10;&lt;p&gt;3) Select a variance (&lt;strong&gt;$^{2}$&lt;/strong&gt;) for the resulting distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;4) Generate a doubly Truncated Normal distribution with the calculated mean and selected variance $\textit{TNormal(Y,$^{2}$,0,1)}$&lt;/p&gt;&#10;&#10;&lt;p&gt;I tested using the R package &lt;a href=&quot;http://cran.r-project.org/web/packages/truncnorm/index.html&quot; rel=&quot;nofollow&quot;&gt;&lt;strong&gt;truncnorm&lt;/strong&gt;&lt;/a&gt;. This procedure is based on &lt;a href=&quot;http://www.agenarisk.com/resources/technology_articles/FentonRankedNodes_TKDE-0095-0206_R2.pdf&quot; rel=&quot;nofollow&quot;&gt;this article&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-30T01:29:07.050" Id="95750" LastActivityDate="2014-04-30T01:29:07.050" OwnerUserId="44658" PostTypeId="1" Score="0" Tags="&lt;normal-distribution&gt;&lt;mixture&gt;&lt;truncation&gt;" Title="Is this method for mixing doubly Truncated Normal distribution acceptable?" ViewCount="42" />
  <row AnswerCount="1" Body="&lt;p&gt;If an ARMA process (or just a AR(p) process) has real unit roots (i.e. 1 or -1), then differencing it repeatedly will make the differenced process weakly stationary.&lt;/p&gt;&#10;&#10;&lt;p&gt;An ARMA process (or just a AR(p) process) may have complex unit roots. If that happens, can repeatedly differencing make the differenced process weakly stationary? Thanks!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-04-30T02:09:20.123" Id="95755" LastActivityDate="2014-10-17T21:06:04.350" OwnerUserId="1005" PostTypeId="1" Score="4" Tags="&lt;time-series&gt;&lt;arma&gt;" Title="can an ARMA process with complex unit roots be made stationary by differencing?" ViewCount="63" />
  
  
  
  
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;In Tsay's Financial Time Series, for a AR(p) process, &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;In the time series literature, &lt;strong&gt;inverses&lt;/strong&gt; of the two solutions (to the&#10;  characteristic equation of AR(2)) are referred to as the&#10;  &lt;strong&gt;characteristic roots of the AR(2) model&lt;/strong&gt;.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;From other areas of mathematics where the concept of characteristic equations also exists, e.g. difference equations, differential equation, eigenvalues of matrices, isn't the characteristic roots define as the solutions to the characteristic equations? &lt;/p&gt;&#10;&#10;&lt;p&gt;If yes, what the book says is very unusual, and confusing. I hope this is not widely used in time series.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-04-30T11:19:24.880" Id="95803" LastActivityDate="2014-04-30T11:33:34.623" LastEditDate="2014-04-30T11:33:34.623" LastEditorUserId="1005" OwnerUserId="1005" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;" Title="characteristic roots of AR(p)" ViewCount="38" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I was wondering what the difference between pooled OLS and random effects model is? I know that random effects eliminates the time-constant effect. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, when i run my regression with pooled ols and random effects model I get different results. The pooled ols regression looks find where all the variables are significant. When i run the random effects model (with the same dependent and independent variables) many of the variables get insignificant. &#10;The sample size is smaller for the random effects model because for some observations I only have one year which I can use in my pooled OLS regression but not in my random effects regression?&lt;/p&gt;&#10;&#10;&lt;p&gt;Could this be the reason why I get different results? &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-04-30T12:56:26.313" Id="95815" LastActivityDate="2014-04-30T12:56:26.313" OwnerUserId="43560" PostTypeId="1" Score="2" Tags="&lt;statistical-significance&gt;&lt;sample-size&gt;&lt;random-effects-model&gt;&lt;pooling&gt;" Title="Why do I get difference results when using pooled ols and random effects model?" ViewCount="324" />
  <row Body="&lt;p&gt;There are basically four methods:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;em&gt;Manual Search:&lt;/em&gt; Using knowledge you have about the problem guess parameters and observe the result. Based on that result tweak the parameters. Repeat this process until you find parameters that work well or you run out of time.&lt;/li&gt;&#10;&lt;li&gt;&lt;em&gt;Grid Search:&lt;/em&gt; Using knowledge you have about the problem identify ranges for the hyperparameters. Then select several points from those ranges, usually uniformly distributed. Train your network using every combination of parameters and select the combination that performs best. Alternatively you can repeat your search on a more narrow domain centered around the parameters that perform the best.&lt;/li&gt;&#10;&lt;li&gt;&lt;em&gt;Random Search:&lt;/em&gt; Like grid search you use knowledge of the problem to identify ranges for the hyperparameters. However instead of picking values from those ranges in a methodical manner you instead select them at random. Repeat this process until you find parameters that work well or use what you learn to narrow your search. In the paper &lt;a href=&quot;http://jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf&quot; rel=&quot;nofollow&quot;&gt;Random Search for Hyper-Parameter Optimization&lt;/a&gt; Dr. Bengio proposes this be the baseline method against which all other methods should be compared and shows that it tends to work better than the other methods.&lt;/li&gt;&#10;&lt;li&gt;&lt;em&gt;Bayesian Optimization:&lt;/em&gt; More recent work has been focus on improving upon these other approaches by using the information gained from any given experiment to decide how to adjust the hyper parameters for the next experiment. An example of this work would be &lt;a href=&quot;http://dash.harvard.edu/handle/1/11708816&quot; rel=&quot;nofollow&quot;&gt;Practical Bayesian Optimization of Machine Learning Algorithms&lt;/a&gt; by Adams et al.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2014-04-30T14:14:30.633" Id="95821" LastActivityDate="2014-10-22T20:05:25.550" LastEditDate="2014-10-22T20:05:25.550" LastEditorUserId="16922" OwnerUserId="16922" ParentId="95495" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;First, let me preface this by stating that missing data is its own specialty in statistics, so there's lots and lots of different answers to this question.&lt;/p&gt;&#10;&#10;&lt;p&gt;As you've discovered, by default, R uses case-wise deletion of missing values. This means that whenever a missing  value is encountered in your data (on either side of your regression formula), it simply ignores that row. This isn't great, since if you have 100 observations, but half of your rows has at least one variable value missing, you effectively have 50 observations. In some disciplines, the prevalence of missing data can rapidly diminish the size of your data. When I was an undergraduate, I analyzed a 3,000-person survey which shrank to just 316 people when using case-wise deletion! &lt;/p&gt;&#10;&#10;&lt;p&gt;But this gets even worse than shrinking your sample size: there may be hidden problems, such as people with higher income being more likely to not disclose their salary. This will make it difficult to conduct meaningful, statistically sound judgments related to income.&lt;/p&gt;&#10;&#10;&lt;p&gt;One common method for dealing with missing values is imputation. There are many packages for imputation in R available. In my specialty area, political science, a widely-used one is AMELIA II, by Gary King. This treats your variables as multivariate normal and iteratively improves its &quot;guesses&quot; of what the missing values must be based on some convergence criteria: when convergence is declared when the &quot;guess&quot; seems to fit well with the rest of the data. (I'm sorry that this is nonspecific. I haven't used AMELIA II in several years. The documentation is thorough and lucidly written, so I would start there.)&lt;/p&gt;&#10;&#10;&lt;p&gt;But this is just one option. I'm sure that more knowledgeable people will speak up with their contributions.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-04-30T15:39:06.857" Id="95836" LastActivityDate="2014-04-30T18:28:43.817" LastEditDate="2014-04-30T18:28:43.817" LastEditorUserId="22311" OwnerUserId="22311" ParentId="95832" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;You can't do this. You are dealing with ARMAX model, and X - exogenous variables - includes your time components. The exogenous variables must be stationary, and $ct$ is not. &lt;/p&gt;&#10;&#10;&lt;p&gt;There are other time series methods, such as state space models, which work with non-stationary series, but ARMAX can't. &lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-04-30T15:46:15.000" Id="95837" LastActivityDate="2014-04-30T15:46:15.000" OwnerUserId="36041" ParentId="95829" PostTypeId="2" Score="1" />
  
  
  
  <row AnswerCount="2" Body="&lt;p&gt;Let $X$ have a Poisson distribution with parameter $\lambda$. Find the maximum likelihood estimator of $\alpha=\mathbb P(X=0)$. In a sample of size 100 from a Poisson distribution, it is found that the mean is 0.75. Calculate an approximate 95% confidence interval for ${\alpha}$. &lt;/p&gt;&#10;&#10;&lt;p&gt;For the MLE, we want to maximise $e^{-\lambda}$, but isn't that a constant?&lt;/p&gt;&#10;&#10;&lt;p&gt;For the confidence interval, I think it is an interval of radius $\frac{0.075}{0.075/100}$, but I can't quite remember. A few years since A levels...&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2014-04-30T17:55:31.280" Id="95847" LastActivityDate="2014-05-09T02:57:16.733" LastEditDate="2014-05-09T02:57:16.733" LastEditorUserId="32036" OwnerUserId="41856" PostTypeId="1" Score="0" Tags="&lt;self-study&gt;&lt;confidence-interval&gt;&lt;estimation&gt;&lt;maximum-likelihood&gt;&lt;poisson&gt;" Title="Compute maximum likelihood estimator and confidence interval" ViewCount="112" />
  
  
  <row Body="&lt;p&gt;$$E[X_t e_t \mid  F_{t-1}] = E[X_t(\epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2})\mid \{X_{t-1}, e_{t-1}, X_{t-2}, e_{t-2}, ...\}]$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$=E[X_t[\epsilon_t + X_t\theta_1 \epsilon_{t-1} + X_t\theta_2 \epsilon_{t-2})\mid \{X_{t-1}, e_{t-1}, X_{t-2}, e_{t-2}, ...\}]$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$=E[X_t]\cdot E[\epsilon_t] + \theta_1E[X_t]\cdot E[\epsilon_{t-1}\mid \{e_{t-1}\}] +\theta_2 E[X_t]\cdot E[\epsilon_{t-2}\mid \{e_{t-1}, e_{t-2}\}]$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The first term is zero but the other two are not: $\epsilon_{t-1}$ is not independent of $e_{t-1}$ and $\epsilon_{t-2}$ is not independent of $(e_{t-1}, e_{t-2})$. But non-independence implies that &lt;/p&gt;&#10;&#10;&lt;p&gt;$$E[\epsilon_{t-1}\mid \{e_{t-1}\}] \neq E[\epsilon_{t-1}]=0,\;\; E[\epsilon_{t-2}\mid \{e_{t-1}, e_{t-2}\}] \neq E[\epsilon_{t-2}]=0$$&lt;/p&gt;&#10;&#10;&lt;p&gt;So $$E[X_t e_t \mid  F_{t-1}] \neq 0$$&#10;and therefore it is not a martingale-difference sequence. There is no need to actually calculate the expected values.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-04-30T23:45:20.250" Id="95881" LastActivityDate="2014-05-01T02:06:24.403" LastEditDate="2014-05-01T02:06:24.403" LastEditorUserId="20148" OwnerUserId="28746" ParentId="95862" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;You can specify &lt;code&gt;method=&quot;none&quot;&lt;/code&gt; in &lt;code&gt;trainControl&lt;/code&gt;. For example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;train(Species ~ ., data=iris, method=&quot;rf&quot;, tuneGrid=data.frame(mtry=3),&#10;  trControl=trainControl(method=&quot;none&quot;))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I'm not sure when this was implemented.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-01T00:00:07.300" Id="95886" LastActivityDate="2014-05-01T00:00:07.300" OwnerUserId="30389" ParentId="23763" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;Despite your feelings, you might want to try multiple regression nevertheless. Put your two populations into one regression and add an indicator for group membership (i.e. to which population does the observation belong). This allows you to test if there are any significant mean differences across the populations after controlling for your explanatory variables. If you think that the two populations' dependency on the explanatory variables are different as well (i.e. different $\beta$s), you can also easily interact the variables with your group membership dummy, which allows you to formally test if the parameters are equal across the two populations. &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-05-01T00:38:37.050" Id="95890" LastActivityDate="2014-05-01T00:38:37.050" OwnerUserId="31634" ParentId="95888" PostTypeId="2" Score="1" />
  <row AnswerCount="2" Body="&lt;p&gt;I understand the coefficients of a logistic equation can be interpreted as odd ratio.  If a regularization term is added to control for over-fitting, how does this change the interpretation of the coefficients? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-01T00:42:22.677" Id="95892" LastActivityDate="2014-12-09T03:05:17.853" LastEditDate="2014-05-01T04:02:53.173" LastEditorUserId="7290" OwnerUserId="44761" PostTypeId="1" Score="5" Tags="&lt;logistic&gt;&lt;interpretation&gt;&lt;regression-coefficients&gt;&lt;regularization&gt;&lt;odds-ratio&gt;" Title="Interpreting logistic regression coefficients with a regularization term" ViewCount="154" />
  
  
  <row Body="&lt;p&gt;Probably the revenue per cookie has a normal distribution (you can check this with bootstrapping). The revenue per si do not have a normal distribution, just the revenue per cookie. That said, you can applied an hypothesis test as usually an check is the difference in revenues per user in two groups are significant.&lt;/p&gt;&#10;&#10;&lt;p&gt;Other approach is considers the revenue distribution as a combination of two others distribution. The first one is the distribution for the conversion (buy or not) and the other distribution if for the average revenue per order.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-01T03:41:57.947" Id="95903" LastActivityDate="2014-05-01T03:41:57.947" OwnerUserId="8447" ParentId="89723" PostTypeId="2" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm comparing samples of remote sensing data points collected over two different areas, so basically I come up with a pair of data set and I run the same analysis on both of them.&#10;Comparing the linear models (t-test) I found that the slopes and intercepts are significantly different from each other.&#10;Nonetheless, since my independent variable ranges from 0 to 1 and since the two trend lines approaches each other as x increases, I would like to assess whether the values assumed by the trend lines for x=1 are significantly different.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;Do I have to use again the t-test, as I did to compare the two models? In this case are the degrees of freedom the same?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Is it sufficient to consider the uncertainties derived by summing slopes and intercepts (and their relative standard errors) and check if the ranges overlap?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Do I have to plot confidence intervals for the two distributions?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Thanks in advance.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-01T10:28:25.790" Id="95930" LastActivityDate="2014-05-01T10:28:25.790" OwnerUserId="31565" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;linear-model&gt;" Title="compare the values assumed by two trend lines in correspondence a certain x value" ViewCount="18" />
  
  <row AcceptedAnswerId="96024" AnswerCount="1" Body="&lt;p&gt;I see sometimes written (see below for examples) that given a random sample from less than 10% of the population then it's reasonable to assume that the data is independent. I wonder why 10% and not another percentage.&lt;/p&gt;&#10;&#10;&lt;p&gt;Examples:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://www.openintro.org/stat/textbook.php&quot; rel=&quot;nofollow&quot;&gt;http://www.openintro.org/stat/textbook.php&lt;/a&gt;, see equation 4.4 on page 164&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://apcentral.collegeboard.com/apc/members/repository/ap03_stats_assumption_31840.pdf&quot; rel=&quot;nofollow&quot;&gt;http://apcentral.collegeboard.com/apc/members/repository/ap03_stats_assumption_31840.pdf&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="4" CreationDate="2014-05-01T13:20:06.780" Id="95948" LastActivityDate="2014-05-02T00:36:57.883" LastEditDate="2014-05-01T20:58:57.350" LastEditorUserId="39728" OwnerUserId="39728" PostTypeId="1" Score="2" Tags="&lt;sampling&gt;&lt;independence&gt;" Title="How to check for independence within a sample" ViewCount="39" />
  <row Body="&lt;p&gt;My detailed answer is below, but the general (i.e. real) answer to this kind of question is: 1) experiment, screw around, look at the data, you can't break the computer no matter what you do, so . . . experiment; or 2) &lt;a href=&quot;http://www.urbandictionary.com/define.php?term=RTFM&quot;&gt;RTFM&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is some &lt;code&gt;R&lt;/code&gt; code which replicates the problem identified in this question, more or less:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# This program written in response to a Cross Validated question&#10;# http://stats.stackexchange.com/questions/95939/&#10;# &#10;# It is an exploration of why the result from lm(y_x+I(x^2))&#10;# looks so different from the result from lm(y~poly(x,2))&#10;&#10;library(ggplot2)&#10;&#10;&#10;epsilon &amp;lt;- 0.25*rnorm(100)&#10;x       &amp;lt;- seq(from=1, to=5, length.out=100)&#10;y       &amp;lt;- 4 - 0.6*x + 0.1*x^2 + epsilon&#10;&#10;# Minimum is at x=3, the expected y value there is&#10;4 - 0.6*3 + 0.1*3^2&#10;&#10;ggplot(data=NULL,aes(x, y)) + geom_point() + &#10;       geom_smooth(method = &quot;lm&quot;, formula = y ~ poly(x, 2))&#10;&#10;summary(lm(y~x+I(x^2)))       # Looks right&#10;summary(lm(y ~ poly(x, 2)))   # Looks like garbage&#10;&#10;# What happened?&#10;# What do x and x^2 look like:&#10;head(cbind(x,x^2))&#10;&#10;#What does poly(x,2) look like:&#10;head(poly(x,2))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The first &lt;code&gt;lm&lt;/code&gt; returns the expected answer:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Call:&#10;lm(formula = y ~ x + I(x^2))&#10;&#10;Residuals:&#10;     Min       1Q   Median       3Q      Max &#10;-0.53815 -0.13465 -0.01262  0.15369  0.61645 &#10;&#10;Coefficients:&#10;            Estimate Std. Error t value Pr(&amp;gt;|t|)    &#10;(Intercept)  3.92734    0.15376  25.542  &amp;lt; 2e-16 ***&#10;x           -0.53929    0.11221  -4.806 5.62e-06 ***&#10;I(x^2)       0.09029    0.01843   4.900 3.84e-06 ***&#10;---&#10;Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1&#10;&#10;Residual standard error: 0.2241 on 97 degrees of freedom&#10;Multiple R-squared:  0.1985,    Adjusted R-squared:  0.182 &#10;F-statistic: 12.01 on 2 and 97 DF,  p-value: 2.181e-05&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The second &lt;code&gt;lm&lt;/code&gt; returns something odd:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Call:&#10;lm(formula = y ~ poly(x, 2))&#10;&#10;Residuals:&#10;     Min       1Q   Median       3Q      Max &#10;-0.53815 -0.13465 -0.01262  0.15369  0.61645 &#10;&#10;Coefficients:&#10;            Estimate Std. Error t value Pr(&amp;gt;|t|)    &#10;(Intercept)  3.24489    0.02241 144.765  &amp;lt; 2e-16 ***&#10;poly(x, 2)1  0.02853    0.22415   0.127    0.899    &#10;poly(x, 2)2  1.09835    0.22415   4.900 3.84e-06 ***&#10;---&#10;Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1&#10;&#10;Residual standard error: 0.2241 on 97 degrees of freedom&#10;Multiple R-squared:  0.1985,    Adjusted R-squared:  0.182 &#10;F-statistic: 12.01 on 2 and 97 DF,  p-value: 2.181e-05&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Since &lt;code&gt;lm&lt;/code&gt; is the same in the two calls, it has to be the arguments of &lt;code&gt;lm&lt;/code&gt; which are different.  So, let's look at the arguments.  Obviously, &lt;code&gt;y&lt;/code&gt; is the same.  It's the other parts.  Let's look at the first few observations on the right-hand-side variables in the first call of &lt;code&gt;lm&lt;/code&gt;.  The return of &lt;code&gt;head(cbind(x,x^2))&lt;/code&gt; looks like:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;            x         &#10;[1,] 1.000000 1.000000&#10;[2,] 1.040404 1.082441&#10;[3,] 1.080808 1.168146&#10;[4,] 1.121212 1.257117&#10;[5,] 1.161616 1.349352&#10;[6,] 1.202020 1.444853&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This is as expected.  First column is &lt;code&gt;x&lt;/code&gt; and second column is &lt;code&gt;x^2&lt;/code&gt;.  How about the second call of &lt;code&gt;lm&lt;/code&gt;, the one with poly?  The return of &lt;code&gt;head(poly(x,2))&lt;/code&gt; looks like:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;              1         2&#10;[1,] -0.1714816 0.2169976&#10;[2,] -0.1680173 0.2038462&#10;[3,] -0.1645531 0.1909632&#10;[4,] -0.1610888 0.1783486&#10;[5,] -0.1576245 0.1660025&#10;[6,] -0.1541602 0.1539247&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;OK, that's really different.  First column is not &lt;code&gt;x&lt;/code&gt;, and second column is not &lt;code&gt;x^2&lt;/code&gt;.  So, whatever &lt;code&gt;poly(x,2)&lt;/code&gt; does, it does not return &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;x^2&lt;/code&gt;.  If we want to know what &lt;code&gt;poly&lt;/code&gt; does, we might start by reading its help file.  So we say &lt;code&gt;help(poly)&lt;/code&gt;.  The description says:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Returns or evaluates orthogonal polynomials of degree 1 to degree over the specified set of points x. These are all orthogonal to the constant polynomial of degree 0. Alternatively, evaluate raw polynomials.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Now, either you know what &quot;orthogonal polynomials&quot; are or you don't.  If you don't, then use &lt;a href=&quot;http://en.wikipedia.org/wiki/Orthogonal_polynomials&quot;&gt;Wikipedia&lt;/a&gt; or Bing (not Google, of course, because Google is evil---not as bad as Apple, naturally, but still bad).  Or, you might decide you don't care what orthogonal polynomials are.  You might notice the phrase &quot;raw polynomials&quot; and you might notice a little further down in the help file that &lt;code&gt;poly&lt;/code&gt; has an option &lt;code&gt;raw&lt;/code&gt; which is, by default, equal to &lt;code&gt;FALSE&lt;/code&gt;.  Those two considerations might inspire you to try out &lt;code&gt;head(poly(x, 2, raw=TRUE))&lt;/code&gt; which returns:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;            1        2&#10;[1,] 1.000000 1.000000&#10;[2,] 1.040404 1.082441&#10;[3,] 1.080808 1.168146&#10;[4,] 1.121212 1.257117&#10;[5,] 1.161616 1.349352&#10;[6,] 1.202020 1.444853&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Excited by this discovery (it looks right, now, yes?), you might go on to try &lt;code&gt;summary(lm(y ~ poly(x, 2, raw=TRUE)))&lt;/code&gt;  This returns:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Call:&#10;lm(formula = y ~ poly(x, 2, raw = TRUE))&#10;&#10;Residuals:&#10;     Min       1Q   Median       3Q      Max &#10;-0.53815 -0.13465 -0.01262  0.15369  0.61645 &#10;&#10;Coefficients:&#10;                        Estimate Std. Error t value Pr(&amp;gt;|t|)    &#10;(Intercept)              3.92734    0.15376  25.542  &amp;lt; 2e-16 ***&#10;poly(x, 2, raw = TRUE)1 -0.53929    0.11221  -4.806 5.62e-06 ***&#10;poly(x, 2, raw = TRUE)2  0.09029    0.01843   4.900 3.84e-06 ***&#10;---&#10;Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1&#10;&#10;Residual standard error: 0.2241 on 97 degrees of freedom&#10;Multiple R-squared:  0.1985,    Adjusted R-squared:  0.182 &#10;F-statistic: 12.01 on 2 and 97 DF,  p-value: 2.181e-05&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;There are at least two levels to the above answer.  First, I answered your question.  Second, and much more importantly, I illustrated how you are supposed to go about answering questions like this yourself.  Every single person who &quot;knows how to program&quot; has gone through a sequence like the one above sixty million times.  Even people as depressingly bad at programming as I am go through this sequence all the time.  It's normal for code not to work.  It's normal to misunderstand what functions do.  The way to deal with it is to screw around, experiment, look at the data, and RTFM.  Get yourself out of &quot;mindlessly following a recipe&quot; mode and into &quot;detective&quot; mode.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-05-01T14:20:11.780" Id="95952" LastActivityDate="2014-05-01T20:34:27.820" LastEditDate="2014-05-01T20:34:27.820" LastEditorUserId="25212" OwnerUserId="25212" ParentId="95939" PostTypeId="2" Score="18" />
  <row AcceptedAnswerId="95966" AnswerCount="1" Body="&lt;p&gt;If I want to normalize some data using a median normalization or trimmed mean normalization, do I multiply or divide my data by the normalization factors? Does it matter?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-01T16:03:51.023" Id="95964" LastActivityDate="2014-05-01T16:13:23.003" OwnerUserId="31289" PostTypeId="1" Score="0" Tags="&lt;normalization&gt;" Title="Normalization Factor divide or multiply" ViewCount="74" />
  
  
  
  <row Body="&lt;p&gt;For sample size of $n$ normally distributed values $X_1,...,X_n$,&lt;/p&gt;&#10;&#10;&lt;p&gt;You calculate the sample mean $\bar X$ and sample variance $S_x^2$.&lt;/p&gt;&#10;&#10;&lt;p&gt;You have $\frac{\bar X-E(X)}{S_x/\sqrt n}\sim t(df=n-1)$ a t-distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can construct a $95\%$ CI of $E(X)$ as $\bar X\pm S_x/\sqrt n \times t(0.975,n-1)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Based on the $95\%$ CI of $E(X)$ you can calculate the total number of $N$ marbles among $M$ sacks.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-01T19:09:10.043" Id="95992" LastActivityDate="2014-05-01T19:09:10.043" OwnerUserId="44635" ParentId="95989" PostTypeId="2" Score="0" />
  <row AnswerCount="3" Body="&lt;p&gt;Given the following model: &lt;/p&gt;&#10;&#10;&lt;p&gt;${\rm Wage}_i=\beta_0+\beta_1 {\rm Married}_i+\beta_2{\rm Female}_i+\beta_3 {\rm Married}_i \times {\rm Female}_i + \varepsilon_i$, &lt;/p&gt;&#10;&#10;&lt;p&gt;I am interested in finding $\beta_1+\beta_3$ for different quantiles of the dependent variable (i.e. on average, how much more/less does married females make compared to single females for a given quantile). I already have the results of the quantile regression model, and I calculated $\beta_1+\beta_3$ for all quantile levels.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, I am trying to find a confidence interval for $\beta_1+\beta_3$ to determine whether this quantity is statically significant for a given quantile level. One thing that came to mind is the Bonferroni joint confidence interval. Using this method I was able to find a lower bound for the joint confidence interval for each of $\beta_1$ and $\beta_3$, but I am not sure how to find a confidence interval for the sum of the two covariates. &lt;/p&gt;&#10;&#10;&lt;p&gt;Would it be correct to sum the two confidence intervals that we got (I didn't find evidence that support this approach)? Is there a way to determine such a confidence interval? Appreciate your help. Thanks.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-05-01T22:55:25.353" FavoriteCount="1" Id="96018" LastActivityDate="2014-11-14T02:56:24.287" LastEditDate="2014-05-01T23:18:55.227" LastEditorUserId="32036" OwnerUserId="44805" PostTypeId="1" Score="3" Tags="&lt;confidence&gt;&lt;interval&gt;&lt;bonferroni&gt;" Title="Joint Confidence Interval" ViewCount="135" />
  <row AnswerCount="1" Body="&lt;p&gt;In a problem we are asked to &quot;refine the fitted model by using an indicator for the outlier&quot;. What does it mean to use an indicator for an outlier?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-02T02:10:03.297" Id="96029" LastActivityDate="2014-06-29T23:54:23.507" LastEditDate="2014-06-29T23:54:23.507" LastEditorUserId="805" OwnerUserId="17046" PostTypeId="1" Score="2" Tags="&lt;self-study&gt;&lt;model-selection&gt;&lt;outliers&gt;" Title="What is an indicator in statistics and why is it used to refine models?" ViewCount="37" />
  <row Body="&lt;p&gt;It means to introduce a predictor variable (explanatory variable, independent variable) which consists of a column of 0's, except for a 1 in the row where the outlier is.&lt;/p&gt;&#10;&#10;&lt;p&gt;The aim would be to estimate and thereby completely account for the size of the outlier by the coefficient of the dummy, removing its effect on the other coefficients.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am not sure this has any value above simple deletion, except that your model explicitly shows what you did.&lt;/p&gt;&#10;&#10;&lt;p&gt;I don't know that it's always a wise thing to do, but some people do use deletion or dummy strategies for dealing with outliers, in such a fashion, so it's worth knowing about even if you don't generally use it.&lt;/p&gt;&#10;&#10;&lt;p&gt;There may well be some situations where it makes perfect sense to do this.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-05-02T02:16:00.763" Id="96032" LastActivityDate="2014-05-02T02:16:00.763" OwnerUserId="805" ParentId="96029" PostTypeId="2" Score="3" />
  
  
  <row Body="&lt;p&gt;I will try to provide some answers, while also giving the disclaimer that I am the author (Ryan) of one of the books that was mentioned. &lt;/p&gt;&#10;&#10;&lt;p&gt;First, I would also recommend the paper by Lenth that was mentioned, as I always provide my students with that paper in the online courses on sample size determination that I teach. &lt;/p&gt;&#10;&#10;&lt;p&gt;Regarding the costs of books on this subject, the list price of my book is 110 dollars but it can be purchased at one of the &quot;discount&quot; sites (such as half.com) for well under 100 dollars.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I have a copy of the book by Chow, Shao, and Wang, which is at a somewhat high mathematical level and is a good source for biosatisticians and others with sufficient mathematical expertise. One Amazon reviewer has expressed frustration at the number of errors in the book. I've found one or two but it is impossible to write a statistics book that is completely free of errors.   &lt;/p&gt;&#10;&#10;&lt;p&gt;It is true that complex cases, such as mixed models, are generally not covered in books on the subject, but relatively few books have been written on sample size determination.  Cohen's book was for many years the standard book on the subject, but it was published 26 years ago and is thus outdated. If you read Lenth's paper, you will see that he debunks Cohen's use of small, medium, and large effect sizes, referring to them as &quot;shirt sizes&quot;. :-) I agree with Russ Lenth on that, whom I have known for almost 30 years. &lt;/p&gt;&#10;&#10;&lt;p&gt;Much research is needed om sample size determination but this is not a subject that interests statisticians. &lt;/p&gt;&#10;&#10;&lt;p&gt;PASS is the most comprehensive software package for sample size determination and it does have mixed model capability. It uses simulation to determine sample size, even in some cases where analytical results are known. &lt;/p&gt;&#10;&#10;&lt;p&gt;Tom Ryan&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-02T06:15:57.603" Id="96054" LastActivityDate="2014-05-02T06:15:57.603" OwnerUserId="44824" ParentId="91630" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;Using plots of simulated data to get an impression of how to interpret a Q-Q plot as propsed by @Glen_b is an excelent idea. &lt;/p&gt;&#10;&#10;&lt;p&gt;You can also use such simulated curves as a background in your final graph. That way it is easier to compare the diviations from the diagonal in your observed residuals with the kind of variation from the diagonal line one could expect when the residuals were draws from a real normal distribution. See for example the graph below:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/O4QNv.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;The details:&lt;/p&gt;&#10;&#10;&lt;p&gt;I made this graph in Stata. For convenience I used for the plotting position $p=\frac{r-.5}{n}$, the default for &lt;code&gt;qplot&lt;/code&gt;. For those who have Stata and whish to play with it, here is the code (it requires the user written components &lt;code&gt;qplot&lt;/code&gt; and the &lt;code&gt;lean1&lt;/code&gt; scheme, both can be found using &lt;code&gt;findit&lt;/code&gt;):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;// make sure the random draws can be replicated&#10;set seed 12345&#10;&#10;// load some data&#10;sysuse auto, clear&#10;&#10;// do a regression&#10;reg price mpg foreign i.rep78&#10;&#10;// predict the residuals&#10;predict resid, resid&#10;&#10;// create 19 random draws&#10;forvalues i = 1/19 {&#10;    gen resid`i' = rnormal(0,e(rmse))&#10;}&#10;&#10;//create the Q-Q plot&#10;qplot resid? resid?? resid, trscale(invnorm(@)*e(rmse)) ///&#10;    lcolor( `: display _dup(19) &quot;gs12 &quot;' black)         ///&#10;    msymbol(`: display _dup(19) &quot;none &quot;' oh   )         ///&#10;    connect(`: display _dup(19) &quot;l &quot;'    .    )         ///&#10;    lpattern(solid...)                                  ///&#10;    legend(order(20 &quot;observed&quot; 19 &quot;simulated&quot; )         ///&#10;           subtitle(residuals))                         ///&#10;    aspect(1) scheme(lean1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2014-05-02T08:48:54.963" Id="96065" LastActivityDate="2014-05-02T08:48:54.963" OwnerUserId="23853" ParentId="96042" PostTypeId="2" Score="8" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to train a random forest classifier. As predictors, I keep both discrete features and continuous features (the discrete ones including booleans, counters, etc., and the continuous contains floats).&lt;/p&gt;&#10;&#10;&lt;p&gt;Training the model both in &lt;code&gt;R&lt;/code&gt; and in Python's &lt;code&gt;scikit.learn&lt;/code&gt;, I get that the discrete features are always less important (using the importance feature of the model). I wonder if that's a characteristic of my data or perhaps there's some innate characteristic of discrete features.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am not even sure it makes sense to mix discrete and continuous features in the same predictors matrix.&lt;/p&gt;&#10;&#10;&lt;p&gt;Some numbers: a current test with about 30 features (not much, it'll grow larger) yields mean importance 0.035; the continuous features importances range from 0.037 to 0.06 (mean &gt; 0.04) and the discrete features importances range from 0.005 to 0.03 (mean &amp;lt; 0.02).&lt;/p&gt;&#10;&#10;&lt;p&gt;To sum, I have two questions:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;is there something &quot;inherent&quot; in discrete features that makes them &quot;less important&quot; (according to the algorithm for importance calculation)&lt;/li&gt;&#10;&lt;li&gt;does it make sense to mix the feature types?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2014-05-02T11:07:30.623" FavoriteCount="1" Id="96083" LastActivityDate="2014-05-02T11:07:30.623" OwnerUserId="40740" PostTypeId="1" Score="2" Tags="&lt;random-forest&gt;&lt;importance&gt;" Title="Discrete features are less important" ViewCount="50" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;Let's say I design an experiment to test the hypothesis, &quot;Agent Z causes cancer&quot;.  Of $N$ people in the experiment group, $x$ get cancer. Of $N$ people in the control group, $y$ get cancer.  The results are that $x &amp;gt; y$ and $N \gg x$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Pretty standard so far.  I'm interested in a Bayesian interpretation of the results of this experiment, so I come up with the following statement about the &quot;probability that my hypothesis is correct ($P(H_{\rm true})$) given the evidence ($e$)&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;$P(H_{\rm true}|e) = \frac{P(e|H_{\rm true})P(H_{\rm true})}{P(e)}$&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm specifically interested in the quantity $P(e|H_{\rm true})$.  What can be said about that quantity given the information available?  Is there a general way to calculate or estimate it, knowing nothing more about the relationship between Agent Z and cancer?  If not, what more would you need to calculate or estimate it?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-05-02T13:07:16.867" Id="96093" LastActivityDate="2014-05-02T18:42:26.733" LastEditDate="2014-05-02T18:32:14.050" LastEditorUserId="32036" OwnerUserId="44836" PostTypeId="1" Score="2" Tags="&lt;bayesian&gt;&lt;experiment-design&gt;&lt;bayes&gt;" Title="Bayes theorem an experiment design" ViewCount="90" />
  <row AnswerCount="1" Body="&lt;p&gt;Can someone help me interpret those results?&#10;I get very high p-value in my probit model but I do not understand why...&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/S0wAw.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Some details about my variables:&lt;/p&gt;&#10;&#10;&lt;p&gt;confucius = dependent (binary 1=yes, 0=no)&lt;/p&gt;&#10;&#10;&lt;p&gt;totalagri = continuous (tons)&lt;/p&gt;&#10;&#10;&lt;p&gt;bribes = percentage&lt;/p&gt;&#10;&#10;&lt;p&gt;size = continuous (km2)&lt;/p&gt;&#10;&#10;&lt;p&gt;coast = binary&lt;/p&gt;&#10;&#10;&lt;p&gt;agrigdp = percentage&lt;/p&gt;&#10;&#10;&lt;p&gt;impchina, expchina, gdp = continuous (value in $)&lt;/p&gt;&#10;&#10;&lt;p&gt;lifeexp = continuous (years)&lt;/p&gt;&#10;&#10;&lt;p&gt;urbpop = percentage&lt;/p&gt;&#10;&#10;&lt;p&gt;coalprod, nargasprod, oilreserv = continuous&lt;/p&gt;&#10;&#10;&lt;p&gt;diamond = binary&lt;/p&gt;&#10;&#10;&lt;p&gt;cropprod = index&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-05-02T13:57:14.660" Id="96103" LastActivityDate="2014-06-04T07:19:55.283" OwnerUserId="44840" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;p-value&gt;&lt;probit&gt;" Title="High p-values in Probit model" ViewCount="75" />
  <row Body="&lt;p&gt;(&lt;em&gt;Note: by &quot;homogeneity&quot;, I assume you mean &quot;homogeneity of variance&quot;.&lt;/em&gt;)  &lt;/p&gt;&#10;&#10;&lt;p&gt;They are, in essence, two different names for the same assumption, which might be called in more colloquial English &quot;constant variance of the errors&quot; (of course, in practice we do not have access to the true errors, only the residuals, which are what we actually check).  The term &quot;homogeneity of variance&quot; is traditionally used in the ANOVA context, and &quot;homoscedasticity&quot; is used more commonly in the regression context.  But they both mean that the variance of the residuals is the same everywhere.  &lt;/p&gt;&#10;&#10;&lt;p&gt;If you are having trouble understanding homo- / heteroscedasticity, I have several posts about the topic that may be helpful for you:  &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;How to &lt;strong&gt;understand&lt;/strong&gt; what homoscedasticity is, and &lt;strong&gt;check&lt;/strong&gt; for heteroscedasticity: &lt;a href=&quot;http://stats.stackexchange.com/a/52107/7290&quot;&gt;What does having &quot;constant variance&quot; in a linear regression model mean?&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;The &lt;strong&gt;effect&lt;/strong&gt; of heteroscedasticity on statistical power: &lt;a href=&quot;http://stats.stackexchange.com/a/94625/7290&quot;&gt;Efficiency of beta estimates with heteroscedasticty&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;Possible &lt;strong&gt;alternative strategies&lt;/strong&gt; when you have heteroscedasticity: &lt;a href=&quot;http://stats.stackexchange.com/a/91881/7290&quot;&gt;Alternatives to one-way ANOVA for heteroscedastic data&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="6" CreationDate="2014-05-02T14:25:27.607" Id="96107" LastActivityDate="2014-05-02T17:34:25.043" LastEditDate="2014-05-02T17:34:25.043" LastEditorUserId="7290" OwnerUserId="7290" ParentId="96105" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="96116" AnswerCount="1" Body="&lt;p&gt;So I've been working on a problem in my probability class on which I have become stuck. It involves &lt;code&gt;X1 X2 ... Xn ~ Poisson(lambda)&lt;/code&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;1 - We were instructed to show that &lt;code&gt;X_bar&lt;/code&gt; was sufficient.&lt;/p&gt;&#10;&#10;&lt;p&gt;Sol'n: I did this using factorization, and by finding &lt;code&gt;T(x) = Sum(x_i)/n&lt;/code&gt;, which came from the exponent position above my lambda from the likelihood/pdf. It could've also been achieved via the exponential family method.&lt;/p&gt;&#10;&#10;&lt;p&gt;2 - We were then instructed to show that &lt;code&gt;X_bar^2&lt;/code&gt; is approximately normal with mean &lt;code&gt;\lambda^2&lt;/code&gt; and a variance depending on &lt;code&gt;\lambda&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Sol'n: This was achieved using the central limit theorem and the delta method, which ended up giving me &lt;code&gt;N(0, 4*lambda^3)&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;In step 3, however&lt;/strong&gt;, we are asked &quot;to display an approximate pivot based on the result in part #2, and then produce a 95% confidence interval for &lt;code&gt;lambda^2&lt;/code&gt;&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;We were given a hint, namely that &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;sqrt(n)(x_bar^2 - lambda^2) / sqrt (lambda * 4 lambda^2) ~ N(0,1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I have, up until this point, no concept of what to do with a pivot or with this given info...I am looking for a number of degrees of freedom for a poisson table, which I think is wrong, but I've clearly got an issue with understanding what is being asked of me. If anyone could give me a nudge in the right direction, I'd be really grateful.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-02T14:48:54.323" Id="96111" LastActivityDate="2014-05-02T15:52:23.800" LastEditDate="2014-05-02T15:08:02.420" LastEditorUserId="44797" OwnerUserId="44797" PostTypeId="1" Score="2" Tags="&lt;probability&gt;&lt;self-study&gt;&lt;pivot&gt;" Title="Constructing a pivot-based confidence interval" ViewCount="117" />
  
  <row Body="&lt;p&gt;I'm not sure, but I think it's like finding the MLE for a uniform distribution in that you need to account for the fact that the likelihood of $\theta|\textbf{X}$ equals zero if $2\theta$ is less than the maximum of the sample. Then, I think the likelihood (written with indicator functions rather than piecewise) is written as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;$L(\theta|\textbf{x})=[(\frac{3}{4\theta})^{n_1}(\frac{1}{4\theta})^{n-n_1}]I(x_{(1)}\geq0)I(x_{(n)}\leq2\theta)$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $n_1$ is the number of observations between $0$ and $\theta$.&lt;/p&gt;&#10;&#10;&lt;p&gt;If that's right, then I don't think you need to differentiate. You just recognize the function is a decreasing function of $\theta$ over $[\frac{x_{(n)}}{2},\infty)$ and equal to zero for values less than $\frac{x_{(n)}}{2}$. The MLE is therefore $\frac{x_{(n)}}{2}$&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-05-02T15:17:17.600" Id="96115" LastActivityDate="2014-05-02T15:17:17.600" OwnerUserId="44107" ParentId="96110" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;I'm not sure this answers your question directly, but in general, the coefficients between the two approaches will have different interpretations.&#10;In one case (your first listed), it seems you'd like to transform your original data and run a least squares (linear) regression.&#10;In the other case (the second one listed), you'd be taking a generalized linear model approach, using &lt;em&gt;log&lt;/em&gt; as your link function, which is called Poisson regression.&#10;The key difference is in the interpretation of the error, or variance, of the response data (&lt;em&gt;Y&lt;/em&gt;), as a function of the independent data (&lt;em&gt;x&lt;/em&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;If, after transforming your original response data, your errors are close to normally distributed, then perhaps the former approach will work just fine.&#10;On the other hand, if your data is truly (or nearly) Poisson distributed (conditioned on each possible value of &lt;em&gt;x&lt;/em&gt;), then Poisson regression will allow you to estimate variance &lt;em&gt;in the domain of the original independent/predictor data&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;There's already a discussion &lt;a href=&quot;http://stats.stackexchange.com/questions/8505/poisson-regression-vs-log-count-least-squares-regression&quot;&gt;here&lt;/a&gt; touching on some of the related issues.&#10;And for a more general discussion about the differences between pre-transformation and using generalized linear models, &lt;a href=&quot;http://people.stat.sfu.ca/~lockhart/richard/350/08_2/lectures/Heteroscedasticity/web.pdf&quot; rel=&quot;nofollow&quot;&gt;this set of slides&lt;/a&gt; might be helpful (in the context of performing regression analyses on data that don't have nice, normally-distributed variances).&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-05-02T17:44:43.153" Id="96135" LastActivityDate="2014-05-02T17:44:43.153" OwnerUserId="44846" ParentId="96133" PostTypeId="2" Score="2" />
  
  <row AnswerCount="1" Body="&lt;p&gt;When $Y = AX + \varepsilon$ (i.e., $Y$ comes from linear regression model),&#10;$$\varepsilon \sim \mathcal{N}(0, \sigma^2 I) \hspace{1em} \Rightarrow&#10;\hspace{1em} \hat{e} = (I - H) Y \sim \mathcal{N}(0, (I - H) \sigma^2_{})$$&#10;and in that case residuals $\hat{e}_1, \ldots, \hat{e}_n$ are correlated and not independent. But when we do regression diagnostics and want to test the assumption&#10;$\varepsilon \sim \mathcal{N}(0, \sigma^2 I)$, every textbook suggests to use&#10;QQ plots and statistical tests on residuals $\hat{e}$ that were designed to&#10;test whether $\hat{e} \sim \mathcal{N}(0, \sigma^2 I)$ for some $\sigma^2 \in&#10;\mathbb{R}$. &lt;/p&gt;&#10;&#10;&lt;p&gt;How come it doesn't matter for these tests that residuals are correlated, and&#10;not independent? It is often suggested to use standardised residuals:&#10;$$\hat{e}_i' = \frac{\hat{e}_i}{\sqrt{1 - h_{ii}}},$$&#10;but that only makes them homoscedastic, not independent. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;To rephrase the question:&lt;/strong&gt; Residuals from OLS regression are correlated. I understand that in practice, these correlations are so small (most of the time? always?), they can be ignored when testing whether residuals came from normal distribution. My question is, why?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-05-02T18:56:36.883" FavoriteCount="1" Id="96144" LastActivityDate="2015-03-04T08:59:32.040" LastEditDate="2014-05-03T21:21:01.923" LastEditorUserId="44864" OwnerUserId="44864" PostTypeId="1" Score="8" Tags="&lt;regression&gt;&lt;residuals&gt;&lt;non-independent&gt;" Title="Why doesn't correlation of residuals matter when testing for normality?" ViewCount="134" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Is there a way to quantify \ test \ describe how likely it is that high dimensional data come from a single Gaussian or a mixture of Gaussians (with different means \ SDs) or not?&lt;/p&gt;&#10;&#10;&lt;p&gt;What I am thinking of is similar to the idea of a t-test, but when the separation into groups is unknown. &lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-05-02T21:28:39.150" Id="96163" LastActivityDate="2014-05-03T10:21:18.937" LastEditDate="2014-05-02T23:02:36.477" LastEditorUserId="7290" OwnerUserId="29657" PostTypeId="1" Score="0" Tags="&lt;normal-distribution&gt;&lt;mixture&gt;&lt;gaussian-mixture&gt;" Title="Test is data is derived from mixture or just noisy normal" ViewCount="33" />
  
  
  <row Body="&lt;p&gt;You don't want to use the t-test as that ignores trend.  You need to use time series methods specifically &quot;intervention analysis&quot;.  You build a causal variables(dummy variable) that has zeroes before and one when you made the change and on out.  For example 0,0,0,0,0,1,1,1,1.&lt;/p&gt;&#10;&#10;&lt;p&gt;Post your DAILY data to dropbox.com so we can take a look or here.  You need to account for daily fluctations as well using 6 day of the week dummies.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-05-02T21:48:05.793" Id="96165" LastActivityDate="2014-05-02T21:48:05.793" OwnerUserId="3411" ParentId="96012" PostTypeId="2" Score="2" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I have been assigned the following question&lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;On the basis of the Bank of England's 2013 survey of the Financial position of&#10;British households, given in the data file [households], examine the statistical&#10;and econometric relationships &lt;strong&gt;between household debt-to-income ratios and the&#10;respondents background characteristics, in particular: social grade; age-group;&#10;income; saving and housing tenure&lt;/strong&gt;.&#10;Consider the importance of your results with respect to the impact on household&#10;debt-to-income ratios of:&#10;(a) an increase in interest rates; and&#10;(b) a fall in the rate of price inflation.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is is it valid to regress D/Y=f(Y,...) since Y appears on both sides? If not then what would be the way to deal with the problem. I thought perhaps loging the regression so that ln(D) can be regressed on ln(Y) so that an implicit relationship between D/Y and Y can be obtained after the regression.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any help will be much appreciated.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-03T10:01:16.323" Id="96199" LastActivityDate="2014-06-08T14:31:03.943" OwnerUserId="44889" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;econometrics&gt;" Title="Is it possible to have a valid regression with debt to income as the D.V and income as one of the I.V's?" ViewCount="119" />
  <row Body="&lt;p&gt;In a regular t-test, you lose one degree of freedom from having to estimate a single mean while estimating $\sigma$.  In ANOVA, you lose $k$ degrees of freedom from having to estimate $k$ means while estimating the common $\sigma$.  Though you are only comparing two groups in the contrast, you are using the pooled estimate of $\sigma$, which is why you still lose $k$ df.    &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-03T10:34:37.450" Id="96201" LastActivityDate="2014-05-03T10:53:16.580" LastEditDate="2014-05-03T10:53:16.580" LastEditorUserId="44451" OwnerUserId="44451" ParentId="96197" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;I would say no because if you use random noise variables for debt and income and then run such a model, you get a very significant result. E.g. in &lt;code&gt;R&lt;/code&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(1010381)&#10;debt &amp;lt;- rnorm(1000, 1000, 5)&#10;income &amp;lt;- 10^(rnorm(1000, 1, .3))*1000&#10;di &amp;lt;- debt/income&#10;&#10;m1 &amp;lt;- lm(di~income)&#10;summary(m1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;gives $R^2$ of about .7 and a p value with 15 0's.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-03T10:56:00.373" Id="96203" LastActivityDate="2014-05-03T10:56:00.373" OwnerUserId="686" ParentId="96199" PostTypeId="2" Score="1" />
  <row Body="A non-negative continuous probability distribution characterized by one strictly positive parameter." CommentCount="0" CreationDate="2014-05-03T13:44:28.653" Id="96219" LastActivityDate="2014-05-03T14:33:12.440" LastEditDate="2014-05-03T14:33:12.440" LastEditorUserId="34792" OwnerUserId="34792" PostTypeId="4" Score="0" />
  
  <row AcceptedAnswerId="96230" AnswerCount="1" Body="&lt;p&gt;Let $X_{1},..,X_{n}$ be a sample from a poisson$({\lambda})$ distribution. Let the prior be ${\pi}({\lambda})=1/{\sqrt{\lambda}}$. Find the posterior distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;My work: We have $f(x|{\lambda})=\frac{{\lambda}^{x_{1}+..+x_{n}-0.5}e^{-{\lambda}n}}{x_{1}!....x_{n}!}$. To find the marginal density I believe I need to sum this expression through ${\lambda}=0,1,2,......$. How to do this?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-05-03T14:07:05.570" Id="96222" LastActivityDate="2014-05-04T07:01:00.153" OwnerUserId="41856" PostTypeId="1" Score="1" Tags="&lt;self-study&gt;&lt;bayesian&gt;&lt;pdf&gt;&lt;posterior&gt;" Title="Find posterior distribution" ViewCount="105" />
  <row Body="&lt;p&gt;Generally, model building through stepwise regression is a good way to insure biased results. See for example:&lt;/p&gt;&#10;&#10;&lt;p&gt;Steyerberg, E. W., Eijkemans, M. J., and Habbema, J. D. F. (1999). Stepwise selection in small data sets: a simulation study of bias in logistic regression analysis. &lt;em&gt;Journal of clinical epidemiology&lt;/em&gt;, 52(10):935942.&lt;/p&gt;&#10;&#10;&lt;p&gt;Whittingham, M., Stephens, P., Bradbury, R., and Freckleton, R. (2006). Why do we still use stepwise modelling in ecology and behaviour? &lt;em&gt;Journal of Animal Ecology&lt;/em&gt;, 75(5):11821189.&lt;/p&gt;&#10;&#10;&lt;p&gt;If concerned about inflated Type I error from testing so many hypotheses/including so many predictors, consider methods for controlling the &lt;a href=&quot;https://en.wikipedia.org/wiki/Familywise_error_rate&quot; rel=&quot;nofollow&quot;&gt;family-wise error rate&lt;/a&gt; or the &lt;a href=&quot;https://en.wikipedia.org/wiki/False_discovery_rate&quot; rel=&quot;nofollow&quot;&gt;false discovery rate&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-03T15:51:34.767" Id="96227" LastActivityDate="2014-05-03T15:51:34.767" OwnerUserId="44269" ParentId="96224" PostTypeId="2" Score="3" />
  <row AnswerCount="0" Body="&lt;p&gt;I am not sure if such a beast exists, but is there some non-superiority test for a oneway ANOVA of $k$ groups, probably based on a $\Delta^{2}$ equivalence threshold, for $m=\left[k(k-1)\right]/2$ &lt;em&gt;post hoc&lt;/em&gt; pairwise tests for equivalence (using the two one-sided tests procedure) based on an equivalence threshold of $\Delta$? (Where $\Delta$ is in units of the score being tested; my question could be reframed in terms of an equivalence threshold in units of the test statistic distribution, as in $\varepsilon = \Delta/s_{\bar{x}}$.)&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit: turning this into self-study&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;On the one hand, I might argue that the &lt;em&gt;F&lt;/em&gt; statistic is a ratio two variances of the variable being tested, and that since variances are in squared units, that I could simply test something like H$_{0}\text{: }F \ge \frac{\bar{n}\Delta^{2}}{s^{2}_{\text{within}}}$, and reject H$_{0}$ if P$(F \le \frac{\bar{n}\Delta^{2}}{s^{2}_{\text{within}}}) &amp;lt; \alpha$. Is this nave?&lt;/p&gt;&#10;&#10;&lt;p&gt;On the other hand:&lt;/p&gt;&#10;&#10;&lt;p&gt;Relationship between &lt;em&gt;t&lt;/em&gt; and &lt;em&gt;F&lt;/em&gt; tests&lt;/p&gt;&#10;&#10;&lt;p&gt;$\begin{eqnarray}&#10;t_{\nu} &amp;amp; =  \frac{\mathcal{N}\left(0,1\right)}{\sqrt{\chi^{2}_{\nu}/\nu}}\text{, where }\nu=N-1.\\&#10;t^{2}_{\nu} &amp;amp; =  \frac{\mathcal{N}^{2}\left(0,1\right)}{\chi^{2}_{\nu}/\nu}\\&#10; &amp;amp; =  \frac{\chi^{2}_{1}/1}{\chi^{2}_{\nu}/\nu}\\&#10; &amp;amp; =  F_{1,\nu}\\&#10;\end{eqnarray}$&lt;/p&gt;&#10;&#10;&lt;p&gt;TOST for a &lt;em&gt;t&lt;/em&gt; test for equivalence, given symmetric equivalence threshold $\Delta$:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\begin{eqnarray}&#10;t_{1\nu} &amp;amp; = &amp;amp; \frac{\Delta - \mathcal{N}\left(0,1\right)}{\sqrt{\chi^{2}_{\nu}/\nu}}\\&#10; &amp;amp; = &amp;amp; \frac{\Delta}{\sqrt{\chi^{2}_{\nu}/\nu}} - \frac{\mathcal{N}\left(0,1\right)}{\sqrt{\chi^{2}_{\nu}/\nu}}\\&#10;t^{2}_{1\nu} &amp;amp; = &amp;amp; \left(\frac{\Delta}{\sqrt{\chi^{2}_{\nu}/\nu}} - \frac{\mathcal{N}\left(0,1\right)}{\sqrt{\chi^{2}_{\nu}/\nu}}\right)^{2}\\&#10;&amp;amp; = &amp;amp; \left(\frac{\Delta}{\sqrt{\chi^{2}_{\nu}/\nu}}\right)^{2} - \frac{2\Delta\mathcal{N}\left(0,1\right)}{\sqrt{\chi^{2}_{\nu}/\nu}} + \left(\frac{\mathcal{N}\left(0,1\right)}{\sqrt{\chi^{2}_{\nu}/\nu}}\right)^{2}\\&#10;&amp;amp; = &amp;amp; \frac{\Delta^{2}}{\chi^{2}_{\nu}/\nu} - \frac{2\Delta\mathcal{N}\left(0,1\right)}{\sqrt{\chi^{2}_{\nu}/\nu}} + \frac{\mathcal{N}^{2}\left(0,1\right)}{\chi^{2}_{\nu}/\nu}\\&#10;&amp;amp; = &amp;amp; \frac{\Delta}{\sqrt{\chi^{2}_{\nu}/\nu}}\left(\frac{\Delta}{\sqrt{\chi^{2}_{\nu}/\nu}} - 2\mathcal{N}\left(0,1\right)\right) + F_{1,\nu}&#10;\end{eqnarray}$&lt;/p&gt;&#10;&#10;&lt;p&gt;But I feel like I am kinda falling down a rabbit hole with this approach, and don't know how to translate that to $k$ number of groups &gt;2.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-03T16:18:12.680" Id="96229" LastActivityDate="2014-05-21T22:33:33.300" LastEditDate="2014-05-21T22:33:33.300" LastEditorUserId="44269" OwnerUserId="44269" PostTypeId="1" Score="1" Tags="&lt;self-study&gt;&lt;anova&gt;&lt;t-test&gt;&lt;equivalence&gt;&lt;tost&gt;" Title="What ANOVA non-superiority test corresponds to an omnibus set of TOST equivalence tests?" ViewCount="38" />
  <row Body="&lt;p&gt;Hypothesis tests are most commonly framed in terms of null hypotheses of no difference (e.g. two parameters are equal, and their difference is zero, or their ratio is one). In such a case an extreme enough test statistic (i.e. one which is improbable if the the null hypothesis is true) provides evidence that the null hypothesis is false, and one concludes the parameters are &lt;em&gt;different&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;By contrast, one may wish to frame a null hypothesis of &lt;em&gt;difference at least as large as a given level&lt;/em&gt;, and here an extreme enough test statistic provides evidence to reject this null hypothesis, and conclude that the parameters are &lt;em&gt;equivalent&lt;/em&gt; within the given tolerance. With respect to &lt;em&gt;t&lt;/em&gt; and &lt;em&gt;z&lt;/em&gt; type tests, the general form of the negativist null hypothesis is $\text{H}^{-}_{0}\text{: }|\theta| \ge \Delta$, which takes the specific form to two one-sided null hypotheses: $\text{H}^{-}_{01}\text{: }\theta \ge \Delta$ or  $\text{H}^{-}_{02}\text{: }\theta \le -\Delta$. If one rejects $\text{H}^{-}_{01}$, then $\theta$ must be less than $\Delta$, and if one rejects $\text{H}^{-}_{02}$ then $\theta$ must be greater than  $-\Delta$. If one rejects both these one-sided null hypotheses, then $-\Delta &amp;lt; \theta &amp;lt; \Delta$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The two &lt;em&gt;t&lt;/em&gt; test statistics corresponding to these specific null hypotheses are (the corresponding &lt;em&gt;z&lt;/em&gt; test statistics would naturally use $\sigma_{\theta}$):&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;$t_{1} = \frac{\Delta - \theta}{s_{\theta}}$&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;$t_{2} = \frac{\theta + \Delta}{s_{\theta}}$&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;The rejection regions for both these statistics are in the right tail, and both tests must be rejected in order to conclude equivalence. The probability of a Type I error is made by conducting both tests at the $\alpha$ level, rather than the $\alpha/2$ level, because the rejection regions of the null hypotheses are non-overlapping.&lt;/p&gt;&#10;&#10;&lt;p&gt;The equivalence threshold $\Delta$ is expressed in the same units as the measures being tested. However, it may be desirable to express equivalence in terms of the test statistic itself, and this can be done by using $\varepsilon$, where $\varepsilon=\Delta/s_{\theta}$. In this case, $\text{H}^{-}_{0}\text{: }|t| \ge \varepsilon$, so that $\text{H}^{-}_{01}\text{: }t \ge \varepsilon$, $\text{H}^{-}_{02}\text{: }t \le -\varepsilon$, and $t_{1} = \varepsilon - t$ and $t_{2} = t + \varepsilon$, where $t=\theta/s_{\theta}$. Note that if $\varepsilon \le t_{1-\alpha}$, then it is not possible to reject any $\text{H}^{-}_{0}$, because either $t_{1}$ or $t_{2}$ will be less than or equal to zero.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-03T17:00:31.513" Id="96234" LastActivityDate="2014-05-04T07:13:50.723" LastEditDate="2014-05-04T07:13:50.723" LastEditorUserId="44269" OwnerUserId="44269" PostTypeId="5" Score="0" />
  <row Body="&lt;p&gt;For your second question, you want to show that &#10;$$E\left[ (Y-E(Y|X)(f(X)-E(Y|X))\right]=0.$$&#10;Now, if we look at the first term of the product, if we didn't have a conditional expectation, we would have&#10;$$E(Y-E(Y))=E(Y)-E(Y)=0.$$&#10;But by the Law of Total expectation, we know that&#10;$$E(W)=E(E(W|Z)),$$&#10;so you can actually write&#10;$$E(Y-E(Y|X)) = E(E(Y-E(Y|X)|X)) = E(E(Y|X)-E(Y|X)) =E(0)=0.$$&#10;To finish the proof, note that conditional on $X$, the second term is a constant, and therefore the expectation of the product is the product of the expectations:&#10;$$E\left[ (Y-E(Y|X))(f(X)-E(Y|X))|X\right]=E\left[ (Y-E(Y|X))|X\right]\cdot E\left[(f(X)-E(Y|X))|X\right]$$&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-05-03T19:58:08.943" Id="96252" LastActivityDate="2014-05-03T19:58:08.943" OwnerUserId="26501" ParentId="96247" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;This is 3 years late, but still may be relevant for someone...&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $S$ denote a sample of points $x_i \in R^d$ and the set of corresponding labels $y_i \in \{-1,1\}$. We search to find a hyperplane $w$ that would minimize the total hinge-loss:&#10;\begin{equation}&#10;w^* = \underset{w}{\text{argmin }} L^{hinge}_S(w) = \underset{w}{\text{argmin }} \sum_i{l_{hinge}(w,x_i,y_i)}= \underset{w}{\text{argmin }} \sum_i{\max{\{0,1-y_iw\cdot x}\}}&#10;\end{equation}&#10;To find $w^*$ take derivative of the total hinge loss .  Gradient of each component is:&#10;$$&#10;\frac{\partial{l_{hinge}}}{\partial w}=&#10;\begin{cases}&#10;  0  &amp;amp; y_iw\cdot x \geq 1 \\&#10;  -y_ix &amp;amp; y_iw\cdot x &amp;lt; 1&#10;\end{cases}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The gradient of the sum is a sum of gradients.&#10;$$&#10;\frac{\partial{L_S^{hinge}}}{\partial{w}}=\sum_i{\frac{\partial{l_{hinge}}}{\partial w}}&#10;$$&#10;Python example, which uses GD to find hinge-loss optimal separatinig hyperplane follows (its probably not the most efficient code, but it works)&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;import numpy as np&#10;import matplotlib.pyplot as plt&#10;&#10;def hinge_loss(w,x,y):&#10;    &quot;&quot;&quot; evaluates hinge loss and its gradient at w&#10;&#10;    rows of x are data points&#10;    y is a vector of labels&#10;    &quot;&quot;&quot;&#10;    loss,grad = 0,0&#10;    for (x_,y_) in zip(x,y):&#10;        v = y_*np.dot(w,x_)&#10;        loss += max(0,1-v)&#10;        grad += 0 if v &amp;gt; 1 else -y_*x_&#10;    return (loss,grad)&#10;&#10;def grad_descent(x,y,w,step,thresh=0.001):&#10;    grad = np.inf&#10;    ws = np.zeros((2,0))&#10;    ws = np.hstack((ws,w.reshape(2,1)))&#10;    step_num = 1&#10;    delta = np.inf&#10;    loss0 = np.inf&#10;    while np.abs(delta)&amp;gt;thresh:&#10;        loss,grad = hinge_loss(w,x,y)&#10;        delta = loss0-loss&#10;        loss0 = loss&#10;        grad_dir = grad/np.linalg.norm(grad)&#10;        w = w-step*grad_dir/step_num&#10;        ws = np.hstack((ws,w.reshape((2,1))))&#10;        step_num += 1&#10;    return np.sum(ws,1)/np.size(ws,1)&#10;&#10;def test1():&#10;    # sample data points&#10;    x1 = np.array((0,1,3,4,1))&#10;    x2 = np.array((1,2,0,1,1))&#10;    x  = np.vstack((x1,x2)).T&#10;    # sample labels&#10;    y = np.array((1,1,-1,-1,-1))&#10;    w = grad_descent(x,y,np.array((0,0)),0.1)&#10;    loss, grad = hinge_loss(w,x,y)&#10;    plot_test(x,y,w)&#10;&#10;def plot_test(x,y,w):&#10;    plt.figure()&#10;    x1, x2 = x[:,0], x[:,1]&#10;    x1_min, x1_max = np.min(x1)*.7, np.max(x1)*1.3&#10;    x2_min, x2_max = np.min(x2)*.7, np.max(x2)*1.3&#10;    gridpoints = 2000&#10;    x1s = np.linspace(x1_min, x1_max, gridpoints)&#10;    x2s = np.linspace(x2_min, x2_max, gridpoints)&#10;    gridx1, gridx2 = np.meshgrid(x1s,x2s)&#10;    grid_pts = np.c_[gridx1.ravel(), gridx2.ravel()]&#10;    predictions = np.array([np.sign(np.dot(w,x_)) for x_ in grid_pts]).reshape((gridpoints,gridpoints))&#10;    plt.contourf(gridx1, gridx2, predictions, cmap=plt.cm.Paired)&#10;    plt.scatter(x[:, 0], x[:, 1], c=y, cmap=plt.cm.Paired)&#10;    plt.title('total hinge loss: %g' % hinge_loss(w,x,y)[0])&#10;    plt.show()&#10;&#10;if __name__ == '__main__':&#10;    np.set_printoptions(precision=3)&#10;    test1()&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-05-03T20:03:05.543" Id="96253" LastActivityDate="2014-05-03T20:03:05.543" OwnerUserId="44895" ParentId="4608" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;Those look okay to me.&lt;/p&gt;&#10;&#10;&lt;p&gt;The thing that you have to sum is the area of each histogram bar, base times height.&lt;/p&gt;&#10;&#10;&lt;p&gt;The bases there are about 5.723e-15&lt;/p&gt;&#10;&#10;&lt;p&gt;By eye, the height of a typical bar then is about 1.6e13, making a typical bar around 0.09 in area, give or take.&lt;/p&gt;&#10;&#10;&lt;p&gt;Which suggests that the total area will be close to 1&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-03T22:27:36.890" Id="96258" LastActivityDate="2014-05-03T22:27:36.890" OwnerUserId="805" ParentId="96257" PostTypeId="2" Score="4" />
  <row Body="&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;Something quite useful is to use Nagelkerke $R^2$, which is just a generalization of the general R^2 statistic in linear regression. Using the &lt;code&gt;rms&lt;/code&gt; package.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; library(rms)&#10; model &amp;lt;- lrm(y~x)&#10; summary(model)&#10;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Also, you could use cross-validation. That means you test for correct predictions in your original data using a criteria (usually 1 if predict &gt; 0.5) and then calculate a rate. (&gt;80% is usually fine, but that depends on the study).&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;Deviance is a generalization of the residual sum of squares, and can be used to make some hypothesis testing in logistic regression.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;z-value is the statistic $(B_{j}-\hat{B_{j}})/s.e.(\hat{B_{j}})$ which asymptotically converges to a $\mathcal{N}(0,1)$(Under the null hypothesis $B_{j}=0$).&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Significant codes are just a guide. e.g. &lt;code&gt;*&lt;/code&gt; means the associated &lt;em&gt;p&lt;/em&gt;-value is $&amp;lt;0.05$.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2014-05-03T22:47:39.470" Id="96261" LastActivityDate="2014-05-03T23:32:35.517" LastEditDate="2014-05-03T23:32:35.517" LastEditorUserId="22311" OwnerUserId="44613" ParentId="96236" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;Yes, you can do it by re-defining linear regression as an optimization problem (a residual sum of squares cost function for example) and solving it using the constraints you want. So to use the numbers as the example from @Jeremy:&#10;(Note that in this example we are fitting a model without an intercept)&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(1234)&#10;df &amp;lt;- as.data.frame(matrix(rnorm(500), ncol=2))&#10;names(df) &amp;lt;- c(&quot;x&quot;, &quot;y&quot;)&#10;CostFunction &amp;lt;- function(theta){sum( (df$y - theta*df$x)^2)}&#10;theta_0 =1; &#10;theta_opt &amp;lt;- optim(fn= CostFunction, lower=0, par = theta_0, method=&quot;L-BFGS-B&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Ultimately any package you wish to use for constrained regression will do the same thing: formulate a cost function and solve a constrained optimization problem. In the case of ordinary least squares as the one shown here &quot;ordinary&quot; optimizers using &quot;simple&quot; &lt;a href=&quot;https://en.wikipedia.org/wiki/BFGS&quot; rel=&quot;nofollow&quot;&gt;BFGS&lt;/a&gt; variants will do just fine; harder problems (eg. GLMM) will require more exotic beasts like &lt;a href=&quot;https://en.wikipedia.org/wiki/BOBYQA&quot; rel=&quot;nofollow&quot;&gt;BOBYQA&lt;/a&gt;. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-04T01:15:08.107" Id="96270" LastActivityDate="2014-05-04T01:41:20.400" LastEditDate="2014-05-04T01:41:20.400" LastEditorUserId="11852" OwnerUserId="11852" ParentId="96245" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;(UPDATED and corrected version after useful comment). &lt;/p&gt;&#10;&#10;&lt;p&gt;The first passage refers correctly to the &quot;mean&quot; which is a subject of investigation in the context of CLT, but totally incorrectly to the &quot;population mean&quot; -which is not what the CLT examines. The CLT examines, among other things, the &lt;em&gt;sample&lt;/em&gt; mean, viewed as a random variable. So even if it said &quot;the CLT guarantees that the population mean is &lt;em&gt;not&lt;/em&gt; normal ...&quot; it would still be a wrong statement because the CLT does not make statement about the population.&lt;/p&gt;&#10;&#10;&lt;p&gt;The second passage is a great example of devious writing: Each of its two sentences starts with a correct statement, and ends with a wrong statement (attempting to manipulate the perceptions of the reader so as he accepts both).&lt;/p&gt;&#10;&#10;&lt;p&gt;1st sentence&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;(NOT WRONG) &quot;CLT does not guarantee that the population mean is normal&#10;  (NOT WRONG)  &lt;/p&gt;&#10;  &#10;  &lt;p&gt;(WRONG) instead it allows us to treat a population as normal if a&#10;  given population size is large enough (WRONG)&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Again, the CLT does not deal in populations.&lt;/p&gt;&#10;&#10;&lt;p&gt;2nd sentence&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;(CORRECT) Also, the size of the sample will not affect the actual distribution&#10;  of the population (CORRECT),  &lt;/p&gt;&#10;  &#10;  &lt;p&gt;(VAGUE-WRONG) but rather simply gives us a more accurate&#10;  approximation.(VAGUE-WRONG)&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Who&lt;/em&gt; is giving us this approximation? The CLT? or the large sample size? And &quot;more accurate&quot; compared to what other approximation? None other was mentioned. But if again, it implies &quot;approximation of the population distribution&quot; then again it is wrong - &lt;em&gt;the Central Limit Theorem does not deal in populations&lt;/em&gt;. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-05-04T03:38:46.283" Id="96287" LastActivityDate="2014-05-04T13:00:52.240" LastEditDate="2014-05-04T13:00:52.240" LastEditorUserId="28746" OwnerUserId="28746" ParentId="96273" PostTypeId="2" Score="0" />
  
  
  
  <row AcceptedAnswerId="96324" AnswerCount="2" Body="&lt;p&gt;I was attempting the following self-study question. After repeated tries, my answers were at best &lt;strong&gt;0.0944&lt;/strong&gt;, however, the model answer seemed to be otherwise at &lt;strong&gt;0.1889&lt;/strong&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Will appreciate any pointers as I simply can't figure out even after repeated attempts.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Question&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;According to the 2010 census in a city, university-educated employee earned a mean annual income of USD 61,000 &amp;amp; standard deviation of USD4,000. If the incomes are normally distributed and two university-educated employees are randomly selected, what is the probability that one of the them earns more than USD66,000 per year and the other earns less than USD66,000 per year?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;&lt;em&gt;My Attempt&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/FqT0o.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-05-04T05:45:10.903" Id="96298" LastActivityDate="2014-05-04T13:11:47.020" LastEditDate="2014-05-04T06:08:14.887" LastEditorUserId="183" OwnerUserId="37976" PostTypeId="1" Score="0" Tags="&lt;distributions&gt;&lt;self-study&gt;&lt;normal-distribution&gt;" Title="Probability that x is greater than some value and y is less than some value where values come from normal distribution?" ViewCount="71" />
  <row AnswerCount="1" Body="&lt;p&gt;I am currently working on a self-study question relating to exponential distribution. I made an attempt but was not able to make sense of the answer. When I looked up the reference guide, it offered the answer at 0.1068.&lt;/p&gt;&#10;&#10;&lt;p&gt;Appreciate any guidance and advice on where has my method gone wrong. Many thanks.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Question&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;When a truck arrives at a checkpoint, each truck will be checked by customs agents. The times are exponentially distributed with a service rate of 10 trucks per hour. What is the probability that a truck requires less than 15 minutes but more than 10 minutes to be checked?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Attempt&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/DaxrY.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-05-04T05:57:26.553" Id="96300" LastActivityDate="2014-05-04T12:43:07.553" OwnerUserId="37976" PostTypeId="1" Score="0" Tags="&lt;self-study&gt;&lt;exponential&gt;" Title="Self Study Qn on Exponential Distribution" ViewCount="35" />
  <row AnswerCount="0" Body="&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Z2g98.png&quot; alt=&quot;Question&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Can someone help me out here? Using Box-Muller would generate N(0,1) however X ~ N(-1, 4). How do I transform the variables to the distribution of X? And would the pseudocode include the usage of the rejection algorithm too? Appreciate your help!&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-05-04T06:03:11.183" Id="96301" LastActivityDate="2014-05-04T06:44:51.143" LastEditDate="2014-05-04T06:44:51.143" LastEditorUserId="44927" OwnerUserId="44927" PostTypeId="1" Score="1" Tags="&lt;self-study&gt;&lt;monte-carlo&gt;" Title="Box-Muller algorithm and Monte Carlo Integration" ViewCount="43" />
  
  <row Body="&lt;p&gt;Replace lambda value with 1/6 instead of 6 which is the mean. Wrong formula corresponds to wrong result! &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-04T12:43:07.553" Id="96323" LastActivityDate="2014-05-04T12:43:07.553" OwnerUserId="27608" ParentId="96300" PostTypeId="2" Score="0" />
  <row AnswerCount="2" Body="&lt;p&gt;When regression coefficient is nearly 0 (in fact in the real model it's exactly 0), what's the meaning of p value (&amp;lt;0.05) of the coefficient? &lt;/p&gt;&#10;&#10;&lt;p&gt;For example, I did a multiple variable regression with simulated data in R with lm().&lt;/p&gt;&#10;&#10;&lt;p&gt;Generate simulation data with the equation&#10;$$&#10;y=2x_1^2+3x_2^2+3x_1+5&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The terms $x_1x_2$ and $x_2$ coefficients are zero. Using the data to do regression.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;xmesh=mesh(seq(-4,4,0.1),seq(-4,4,0.1))&#10;x1=as.vector(xmesh$x)&#10;x2=as.vector(xmesh$y)&#10;y=2*x1^2+3*x2^2+3*x1+5&#10;model=lm(y~x1+x2+I(x1^2)+I(x2^2)+I(x1*x2)) &#10;summary(model)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The result is :&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Call:&#10;lm(formula = y ~ x1 + x2 + I(x1^2) + I(x2^2) + I(x1 * x2))&#10;&#10;Residuals:&#10;       Min         1Q     Median         3Q        Max &#10;-8.871e-12 -4.500e-15 -7.000e-16  5.700e-15  4.194e-12 &#10;&#10;Coefficients:&#10;              Estimate Std. Error    t value Pr(&amp;gt;|t|)    &#10;(Intercept)  5.000e+00  3.301e-15  1.515e+15  &amp;lt; 2e-16 ***&#10;x1           3.000e+00  7.545e-16  3.976e+15  &amp;lt; 2e-16 ***&#10;x2          -3.348e-15  7.545e-16 -4.438e+00 9.22e-06 ***&#10;I(x1^2)      2.000e+00  3.609e-16  5.542e+15  &amp;lt; 2e-16 ***&#10;I(x2^2)      3.000e+00  3.609e-16  8.314e+15  &amp;lt; 2e-16 ***&#10;I(x1 * x2)  -9.377e-16  3.227e-16 -2.906e+00  0.00367 ** &#10;---&#10;Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1 &#10;&#10;Residual standard error: 1.429e-13 on 6555 degrees of freedom&#10;Multiple R-squared:     1,  Adjusted R-squared:     1 &#10;F-statistic: 2.313e+31 on 5 and 6555 DF,  p-value: &amp;lt; 2.2e-16 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;We can see that the coefficients of term $x_2$ and $x_1x_2$ are nearly 0, and p-value&amp;lt;0.01. I think that lm() did the significance test of coefficient based on t-test with NULL hypothesis $\beta=0$. So p-value&amp;lt;0.05 should mean that the coefficient is significantly different to 0. However the coefficient should be 0 in my model. I am confused. How to interpret these two coefficients' significance?  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Add a new test&lt;/strong&gt;   $y=2x_1^2+3x_1+0.001x_2+5$&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; y2=2*x1^2+3*(x1)+5+0.001*x2&#10;&amp;gt; model3=lm(y2~x1+x2+I(x1^2)+I(x2^2)+I(x1*x2)) &#10;&amp;gt; summary(model3)&#10;&#10;Call:&#10;lm(formula = y2 ~ x1 + x2 + I(x1^2) + I(x2^2) + I(x1 * x2))&#10;&#10;Residuals:&#10;       Min         1Q     Median         3Q        Max &#10;-9.237e-12 -1.700e-15 -1.000e-16  2.200e-15  2.757e-12 &#10;&#10;Coefficients:&#10;              Estimate Std. Error    t value Pr(&amp;gt;|t|)    &#10;(Intercept)  5.000e+00  2.840e-15  1.761e+15   &amp;lt;2e-16 ***&#10;x1           3.000e+00  6.492e-16  4.621e+15   &amp;lt;2e-16 ***&#10;x2           1.000e-03  6.492e-16  1.540e+12   &amp;lt;2e-16 ***&#10;I(x1^2)      2.000e+00  3.105e-16  6.441e+15   &amp;lt;2e-16 ***&#10;I(x2^2)     -2.722e-16  3.105e-16 -8.770e-01    0.381    &#10;I(x1 * x2)  -3.226e-16  2.776e-16 -1.162e+00    0.245    &#10;---&#10;Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1 &#10;&#10;Residual standard error: 1.229e-13 on 6555 degrees of freedom&#10;Multiple R-squared:     1,  Adjusted R-squared:     1 &#10;F-statistic: 1.257e+31 on 5 and 6555 DF,  p-value: &amp;lt; 2.2e-16 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;You can see that in the new test, the coefficients and std Error of term $x_1x_2$ and $x_2^2$ are essentially zero. Their p-value are large enough to accept the null hypothesis that $\beta=0$, it's a good result.&lt;/p&gt;&#10;&#10;&lt;p&gt;How to interpret the p-value of essentially zero coefficients in the two tests? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-04T13:01:33.040" Id="96327" LastActivityDate="2014-05-05T08:11:11.327" LastEditDate="2014-05-05T08:11:11.327" LastEditorUserId="44931" OwnerUserId="44931" PostTypeId="1" Score="3" Tags="&lt;hypothesis-testing&gt;&lt;statistical-significance&gt;&lt;p-value&gt;&lt;regression-coefficients&gt;" Title="How to interpret p value of regression coefficient which is nearly 0?" ViewCount="1309" />
  <row Body="&lt;p&gt;Let's just do this by straightforward combinations. You need a 4, a 5, a 6, and any 2 other cards. $${{{4 \choose 1}{4 \choose 1}{4 \choose 1}}{38 \choose 2}} \over {50 \choose 5}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Evaluating this yields 0.0212, agreeing with your simulated value.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-05-04T14:25:24.327" Id="96340" LastActivityDate="2014-05-04T19:20:30.557" LastEditDate="2014-05-04T19:20:30.557" LastEditorUserId="24073" OwnerUserId="24073" ParentId="96302" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;You are &lt;em&gt;almost&lt;/em&gt; correct.&#10;If you had a larger sample of chicken, your attempt would have been correct. However, even though you report normality, you could still adjust for your smaller sample size by using a different distribution.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;What distribution is used for small sample sizes?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;When you use the appropriate formula for CIs built from that distribution, you will find that you need a slightly different critical value, which will result in a slightly wider CI.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-04T15:00:14.983" Id="96345" LastActivityDate="2014-05-04T15:00:14.983" OwnerUserId="28288" ParentId="96342" PostTypeId="2" Score="4" />
  <row AnswerCount="2" Body="&lt;p&gt;I work in a sports stadium and I would like to run a predictive model on all people who have previously bought tickets to events, to predict the likelihood of them returning to the next event, or indeed any event in the future. &lt;/p&gt;&#10;&#10;&lt;p&gt;We have hundreds and thousands of customers, and approximately 40 historical events in which we can use as binary data on whether or not they attended each event.&lt;/p&gt;&#10;&#10;&lt;p&gt;Does anyone know of a method I can use to run such  a predictive model? I am not really sure where to start, although I believe this can be done.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks very much in advance.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-04T15:17:10.357" Id="96346" LastActivityDate="2014-06-06T19:31:30.493" OwnerUserId="44943" PostTypeId="1" Score="2" Tags="&lt;probability&gt;&lt;predictive-models&gt;&lt;prediction&gt;" Title="Predicting likelihood of attending next event" ViewCount="112" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Overlapping data are often present in time-series analysis and the academic studies mention about Heteroskedasticity and Autocovariance Consistent (HAC) estimators to solve this isssue, such as Newey-West (Harri &amp;amp; Brorsen, 2009). I would like to know if Cochrane-Orcutt estimator could also be employed with overlapping? Thanks!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-04T19:03:39.413" Id="96370" LastActivityDate="2014-05-04T19:03:39.413" OwnerUserId="44958" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;autocorrelation&gt;" Title="Data overlapping" ViewCount="17" />
  <row AcceptedAnswerId="96425" AnswerCount="1" Body="&lt;p&gt;Is there any way to conduct a partial correlation analysis if the DV is dichotomous?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-05-04T20:10:46.107" Id="96379" LastActivityDate="2014-05-05T08:43:04.723" OwnerUserId="40114" PostTypeId="1" Score="1" Tags="&lt;correlation&gt;&lt;partial-correlation&gt;" Title="Partial correlation - scales?" ViewCount="52" />
  
  <row Body="&lt;p&gt;This program simulate you game&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;sim=1000000&#10;prob=matrix(data=0,nrow=13,ncol=1)&#10;for(x in 1:13)&#10;{&#10;  print(x)&#10;  for(iter in 1:sim)&#10;  {&#10;&#10;    #print(paste(iter/sim,x,sep=&quot;   &quot;))&#10;    cards=1:13&#10;    cards=c(cards,cards,cards,cards,cards,cards,cards,cards,cards,cards,cards)# suppose to use more than one deck&#10;    cards=cards[-x]&#10;    y=-1&#10;    while(T)&#10;    {&#10;      y=sample(x=cards,size=1)&#10;      if(y!=x)&#10;        break&#10;&#10;      cards=cards[-which(cards==y)[1]]&#10;    }&#10;    if(x&amp;lt;y)&#10;      prob[x]=prob[x]+1&#10;  }&#10;}&#10;&#10;prob=prob/sim&#10;&#10;rank=1:13&#10;&#10;formula=(13-rank)/12 # your formula&#10;#prob=vector(prob)&#10;&#10;&#10;formula-prob #error between the estimated probability and your formula&#10;&#10; formula-prob&#10;               [,1]&#10; [1,]  0.000000e+00&#10; [2,]  1.516667e-04&#10; [3,]  8.533333e-05&#10; [4,]  3.830000e-04&#10; [5,]  7.036667e-04&#10; [6,]  8.143333e-04&#10; [7,]  1.070000e-04&#10; [8,] -1.005333e-03&#10; [9,]  7.083333e-04&#10;[10,] -5.590000e-04&#10;[11,]  3.986667e-04&#10;[12,]  3.003333e-04&#10;[13,]  0.000000e+00&#10;&amp;gt; &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The error is small so your formula it's probably right&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-05-04T21:17:37.520" Id="96389" LastActivityDate="2014-05-04T21:17:37.520" OwnerUserId="25392" ParentId="96380" PostTypeId="2" Score="0" />
  <row AnswerCount="1" Body="&lt;p&gt;I am running a panel fixed effects regression on 21 countries and 16 years. Its a secondary data taken from OECD website mostly. My model looks like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;\log{(GDP/Labor)}_{i} = \beta_0 +\beta_1 \log{(RnD)}_{i,t-1}+\beta_2 \log{(Human Capital)}_{i,t-1}+\beta_3 \log{(x_j*(RnD_j)}_{t-1}+\epsilon_t&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Where,&lt;br&gt;&#10;Rndi = Research and Development expenditure of country i&lt;br&gt;&#10;Rndj = Research and Development expenditure of country j&lt;br&gt;&#10;HC = Index of Human Capital&lt;/p&gt;&#10;&#10;&lt;p&gt;1). I have following problems with this model:&lt;/p&gt;&#10;&#10;&lt;p&gt;1) all variables have strong trend and they are expected to be non-stationary however, Im-Pesaran-Shin and Levin-Lin-Chu tests are giving contradictory results about non-stationarity so I cannot conclude about non-stationarity. In your opinion, which test is better for panel regressions? Also, is there any test I can do for cointegration in stata for panel regressions in addition to Westlund test? if I dont find evidence for cointeration, do I first difference the data? I lose ALOT of information and everything becomes insignificant if I do that.&lt;/p&gt;&#10;&#10;&lt;p&gt;2) One of the primary goals of my paper is to test for moderation effects of human capital on knowledge spillovers and productivity nexus. Problem is that there is a serious multicollinearity problem. VIF goes as high as 1000! How can I test for moderating effects other than using interactions? Moreover, in some cases I also find strong multicollinearity between my key variables and it really makes everything significant at &amp;lt; 0.001 with R2 less than 50.&lt;/p&gt;&#10;&#10;&lt;p&gt;3) How important is it to include year dummies in regressions? I lose all significance of my variables and things become very bad in terms of signs as well for my core variables when I use year dummies. &lt;/p&gt;&#10;&#10;&lt;p&gt;I will be extremely grateful if you could please advice.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks alot&lt;/p&gt;&#10;&#10;&lt;p&gt;Ali&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-05-04T22:20:18.077" FavoriteCount="1" Id="96397" LastActivityDate="2015-02-21T15:23:16.570" LastEditDate="2014-05-06T16:05:06.070" LastEditorUserId="44969" OwnerUserId="44969" PostTypeId="1" Score="3" Tags="&lt;interaction&gt;&lt;panel-data&gt;&lt;multicollinearity&gt;&lt;fixed-effects-model&gt;&lt;cointegration&gt;" Title="Panel Cointegration, Moderating Effects and Multicollinearity" ViewCount="297" />
  <row AnswerCount="1" Body="&lt;p&gt;I am trying to build a regression model using a neural network. The final cost measure is the mean absolute error (MAE) on the output (one output unit, 200 input units).&lt;/p&gt;&#10;&#10;&lt;p&gt;Right now all my hidden units have rectifier activation. The output unit is just a linear unit with pass-through activation. It seems the network cannot learn efficiently, the error (even on can't find a value that makes the error go down monotonically).&lt;/p&gt;&#10;&#10;&lt;p&gt;I suspect the cost function (L1-norm) might be the culprit. Right now, when taking the gradient, I either pass 1 or -1 depending on predicted value vs actual output value. Is this the right way? (Since L1 is not smooth at 0, would this be the reason why the learning is not smooth/effective?) What is the right way to handle a L1-norm cost function?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks, any help is appreciated!&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-05-04T22:50:50.177" Id="96399" LastActivityDate="2014-06-05T17:29:13.973" LastEditDate="2014-05-04T23:12:50.953" LastEditorUserId="805" OwnerUserId="44971" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;machine-learning&gt;&lt;neural-networks&gt;&lt;gradient-descent&gt;&lt;deep-learning&gt;" Title="L1-norm cost function for Neural Network. (Regression)" ViewCount="242" />
  <row Body="&lt;p&gt;The meaning of standard errors and CIs is predicated on the model being correct. &lt;/p&gt;&#10;&#10;&lt;p&gt;Even assuming that both the class of models is correct and the set of candidate variables is complete, nevertheless a lot of those models will exclude some of the necessary variables, and so &lt;a href=&quot;http://en.wikipedia.org/wiki/Omitted-variable_bias&quot; rel=&quot;nofollow&quot;&gt;even their parameter estimates will be biased&lt;/a&gt; (also see &lt;a href=&quot;http://en.wikipedia.org/wiki/Simpson%27s_paradox&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;, especially the diagram).&lt;/p&gt;&#10;&#10;&lt;p&gt;As a result, the problem is more basic than one of standard errors and width of confidence intervals.&lt;/p&gt;&#10;&#10;&lt;p&gt;What a randomly selected model avoids is the bias introduced by variable selection - bias in both standard errors and estimates, but at the expense of omitted-variable bias, so it's not necessarily preferable.&lt;/p&gt;&#10;&#10;&lt;p&gt;Typical advice leans toward using hold-out samples (such as via cross-validation). If selection and inference are assessed on different subsets of the data, the effect of variable selection bias on inference can be avoided.&lt;/p&gt;&#10;&#10;&lt;p&gt;If that's not possible for some reason, fitting all $k$, but regularizing in some way (such as via shrinkage) would partly mitigate some of the earlier problems.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-05T01:39:57.420" Id="96405" LastActivityDate="2014-05-05T01:39:57.420" OwnerUserId="805" ParentId="92758" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;If your question of interest is &quot;What effect on risk does drug have over placebo?&quot;, then it sounds like you are taking advantage of such matches simply to reduce unobserved sources of between group variability, rather than to make inferences about differences between twins.&lt;/p&gt;&#10;&#10;&lt;p&gt;So your suggestion to measure change in $X$ across the trial period (i.e. $X_{\text{B}}-X_{\text{A}}$) and base your inference on a paired &lt;em&gt;t&lt;/em&gt; test or sign-rank test, would seem to be about right.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you based your inference on $X_{\text{sib drug}}-X_{\text{sib control}}$, and then performed your test (paired &lt;em&gt;t&lt;/em&gt; or signed rank), you would then be inferring that there was a change (or not) in drug-placebo differences in prevalence/levels before and after treatment. Possibly also useful. But aren't clinical trials more often used to characterize effects on risk/change?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-05-05T02:34:52.573" Id="96408" LastActivityDate="2014-05-05T12:43:09.927" LastEditDate="2014-05-05T12:43:09.927" LastEditorUserId="44269" OwnerUserId="44269" ParentId="96406" PostTypeId="2" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm attaching two sets I ran in SAS, regarding spline cubic regression models. &lt;/p&gt;&#10;&#10;&lt;p&gt;I'm new to stats and trying my best to learn but I'm stuck here. How do I get SAS to select outliers based on two or 3 standard deviations from the predicted value using the splines? &lt;/p&gt;&#10;&#10;&lt;p&gt;thanks&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;options ls=75 ps=54 center formdlim =&quot;-&quot;;&#10;data a;&#10;input ID $ totalVol DIM Lame;&#10;if dim&amp;gt;325 then delete;&#10;/*if totalVol='.' then delete;&#10;if totalVol&amp;gt;36 then Vol='totalVol';&#10;if totalVol&amp;lt;-2 then Vol='totalVol';*/&#10;datalines;&#10;6   18.6    117 0&#10;6   8.2 118 0&#10;6   18.4    119 0&#10;6   26.2    120 0&#10;6   18  121 0&#10;6   17.9    122 0&#10;6   18.8    123 0&#10;6   18.3    124 0&#10;6   18.5    125 0&#10;6   20.1    126 0&#10;6   20.2    127 0&#10;6   20.4    128 0&#10;6   8.4 129 0&#10;6   21  130 0&#10;6   8.1 131 0&#10;6   13  132 0&#10;6   13.3    133 0&#10;6   8.7 134 0&#10;6   .   135 0&#10;6   9.3 136 0&#10;6   .   137 0&#10;6   21.3    138 0&#10;6   24.7    139 0&#10;6   18.9    140 0&#10;6   24.1    141 0&#10;6   17.9    142 0&#10;6   19.4    143 0&#10;6   18.9    144 0&#10;6   16.6    145 0&#10;6   22.9    146 0&#10;6   21.4    147 0&#10;6   21.4    148 0&#10;6   19.4    149 0&#10;6   16.7    150 0&#10;6   18.8    151 0&#10;6   19.3    152 0&#10;6   20.6    153 0&#10;6   20.5    154 0&#10;6   20.1    155 0&#10;6   19.5    156 0&#10;6   11.1    157 0&#10;6   19  158 0&#10;6   7.3 159 0&#10;6   20.4    160 0&#10;6   19.6    161 0&#10;6   7   162 0&#10;6   7.7 163 0&#10;6   19.6    164 0&#10;6   7.7 165 0&#10;6   19.6    166 0&#10;6   19.2    167 0&#10;6   8.2 168 0&#10;6   6.7 169 0&#10;6   .   170 0&#10;6   7.8 171 0&#10;6   8.4 172 0&#10;6   8.1 173 0&#10;6   19.2    174 0&#10;6   20.5    175 0&#10;6   17.8    176 0&#10;6   .   177 0&#10;6   11.7    178 0&#10;6   5.8 179 0&#10;6   6.9 180 0&#10;6   12.4    181 0&#10;6   .   182 0&#10;6   14.4    183 0&#10;6   17.2    184 0&#10;6   10  185 0&#10;6   16.6    186 0&#10;6   17.2    187 0&#10;6   5.2 188 0&#10;6   20  189 0&#10;6   16.8    190 0&#10;6   9.7 191 0&#10;6   6.7 192 0&#10;6   8.3 193 0&#10;6   18.3    194 0&#10;6   18.9    195 0&#10;6   20.5    196 0&#10;6   10  197 0&#10;6   21.3    198 0&#10;6   19.3    199 0&#10;6   8.6 200 0&#10;6   8.6 201 0&#10;6   20.3    202 0&#10;6   20.3    203 0&#10;6   .   204 0&#10;6   21.5    205 0&#10;6   9.6 206 0&#10;6   .   207 0&#10;6   .   208 0&#10;6   .   209 0&#10;6   7.2 210 0&#10;6   18.8    211 0&#10;6   18.9    212 0&#10;6   19.4    213 0&#10;6   18.8    214 0&#10;6   22.7    215 0&#10;6   11  216 0&#10;6   .   217 0&#10;6   .   218 0&#10;6   .   219 0&#10;6   .   220 0&#10;6   .   221 0&#10;6   .   222 0&#10;6   7.9 224 0&#10;6   19.8    225 0&#10;6   16.6    226 0&#10;6   17.6    227 0&#10;6   16.5    228 0&#10;6   .   229 0&#10;6   8.6 230 0&#10;6   19.7    231 0&#10;6   18.7    232 0&#10;6   17.7    233 0&#10;6   .   234 0&#10;6   21.6    235 0&#10;6   18.9    236 0&#10;6   17.7    237 0&#10;6   20.4    238 0&#10;6   20.7    240 0&#10;6   18.2    241 0&#10;6   19  242 0&#10;6   19  243 0&#10;6   23.5    244 0&#10;6   18.2    245 0&#10;6   18.5    246 0&#10;6   8.4 247 0&#10;6   17.6    248 0&#10;6   11.2    249 0&#10;6   10.7    250 0&#10;6   20.9    251 0&#10;6   15.3    252 0&#10;6   16.6    253 0&#10;6   6.7 254 0&#10;6   18.7    255 0&#10;6   17.6    256 0&#10;6   17.8    257 0&#10;6   7.5 258 0&#10;6   16.8    259 0&#10;6   16.6    260 0&#10;6   .   261 0&#10;6   7.5 262 0&#10;6   6.6 263 0&#10;6   7   264 0&#10;6   14.7    265 0&#10;6   17.5    266 0&#10;6   17.6    267 0&#10;6   16.5    268 0&#10;6   17.5    269 0&#10;6   17.2    270 0&#10;6   11.6    271 0&#10;6   19.1    272 0&#10;6   15.6    273 0&#10;6   18  274 0&#10;6   17.2    275 0&#10;6   15.1    276 0&#10;6   15.5    277 0&#10;6   21.4    278 0&#10;6   17.2    279 0&#10;6   15.5    280 0&#10;6   17.1    281 0&#10;6   14.7    282 0&#10;6   16.1    283 0&#10;6   16.5    284 0&#10;6   16.3    285 0&#10;6   15.9    286 0&#10;6   15.5    287 0&#10;6   14.7    288 0&#10;6   15.5    289 0&#10;6   17.1    290 0&#10;6   16.6    291 0&#10;6   16.7    292 0&#10;6   6.5 293 0&#10;6   15.3    294 0&#10;6   16.2    295 0&#10;6   6.2 296 0&#10;6   15.3    297 0&#10;6   18.6    298 0&#10;6   14.8    299 0&#10;6   15.6    300 0&#10;6   16.6    301 0&#10;6   19  302 0&#10;6   7   303 0&#10;6   14.8    304 0&#10;6   13  305 0&#10;6   14.6    306 0&#10;6   17.3    307 0&#10;6   14.4    308 0&#10;6   9.1 309 0&#10;6   .   310 0&#10;6   .   311 0&#10;6   .   312 0&#10;6   .   313 0&#10;6   10.1    315 0&#10;6   13.6    316 0&#10;6   14.3    317 0&#10;6   13.6    318 0&#10;6   16  319 0&#10;6   14.5    320 0&#10;6   12.3    321 0&#10;6   13.5    322 0&#10;6   15.3    323 0&#10;6   15.3    324 0&#10;6   6.7 325 0&#10;6   6   326 0&#10;6   7.8 327 0&#10;6   15.6    328 0&#10;6   15.7    329 0&#10;6   7.5 330 0&#10;6   16.4    331 0&#10;6   16.7    360 0&#10;6   14  361 0&#10;6   6.8 362 0&#10;6   15.1    363 0&#10;6   14.4    364 0&#10;6   13.6    365 0&#10;8   34.5    74  1&#10;8   11.4    75  1&#10;8   27.3    76  1&#10;8   27.6    77  1&#10;8   28.2    78  1&#10;8   22  79  1&#10;8   28.4    80  1&#10;8   27.5    81  1&#10;8   30.7    82  1&#10;8   28.9    83  1&#10;8   17.7    84  1&#10;8   31.6    85  1&#10;8   10.4    86  1&#10;8   27.4    87  1&#10;8   14.4    88  1&#10;8   29.6    89  1&#10;8   33.7    90  1&#10;8   15.5    91  1&#10;8   .   92  1&#10;8   13.8    93  1&#10;8   27.2    94  1&#10;8   31.7    95  1&#10;8   28.7    96  1&#10;8   23.8    97  1&#10;8   29.4    98  1&#10;8   33.6    99  1&#10;8   29.5    100 1&#10;8   11.3    101 1&#10;8   27.9    102 1&#10;8   28.9    103 1&#10;8   28.4    104 1&#10;8   27.9    105 1&#10;8   15.4    106 1&#10;8   28.4    107 1&#10;8   20.8    108 1&#10;8   21.6    109 1&#10;8   8.8 110 1&#10;8   21.1    111 1&#10;8   20.9    112 1&#10;8   22.3    113 1&#10;8   23.6    114 1&#10;8   23.1    115 1&#10;8   26.3    116 1&#10;8   14.3    117 1&#10;8   21.6    118 1&#10;8   26.1    119 1&#10;8   8.8 120 1&#10;8   27.1    121 1&#10;8   10.7    122 1&#10;8   26.7    123 1&#10;8   20.3    124 1&#10;8   8.5 125 1&#10;8   8.3 126 1&#10;8   8.4 127 1&#10;8   23.7    129 1&#10;8   8.9 130 1&#10;8   8.4 131 1&#10;8   24.6    132 1&#10;8   17.3    133 1&#10;8   9.4 134 1&#10;8   15.5    135 1&#10;8   7.5 128 1&#10;8   31.2    143 1&#10;8   19  144 1&#10;8   21.1    145 1&#10;8   19.9    146 1&#10;8   8.4 147 1&#10;8   14.7    148 1&#10;8   7.3 149 1&#10;8   7.9 150 1&#10;8   22.9    151 1&#10;8   24.6    152 1&#10;8   32.1    153 1&#10;8   9.4 154 1&#10;8   8.7 155 1&#10;8   9.7 156 1&#10;8   23.3    157 1&#10;8   14.9    158 1&#10;8   31.9    159 1&#10;8   9.8 161 1&#10;8   25.4    162 1&#10;8   13.4    163 1&#10;8   .   164 1&#10;8   .   165 1&#10;8   9.2 136 1&#10;8   23.3    137 1&#10;8   27.3    138 1&#10;8   7.9 139 1&#10;8   18.6    140 1&#10;8   13.2    141 1&#10;8   18.1    142 1&#10;8   .   166 1&#10;8   9.8 167 1&#10;8   26.4    168 1&#10;8   11.4    169 1&#10;8   11.8    170 1&#10;8   27  171 1&#10;8   26  172 1&#10;8   14.1    173 1&#10;8   .   174 1&#10;8   .   175 1&#10;8   .   176 1&#10;8   .   177 1&#10;8   .   178 1&#10;8   .   179 1&#10;8   .   180 1&#10;8   9.7 181 1&#10;8   22.2    182 1&#10;8   27  183 1&#10;8   24.8    184 1&#10;8   24.1    185 1&#10;8   26.4    186 1&#10;8   12  187 1&#10;8   22.7    188 1&#10;8   23.2    189 1&#10;8   22  190 1&#10;8   23  191 1&#10;8   25.2    192 1&#10;8   26  193 1&#10;8   36.1    194 1&#10;8   24  195 1&#10;8   19.5    197 1&#10;8   21.7    198 1&#10;8   14.1    199 1&#10;8   20.6    200 1&#10;8   27.8    201 1&#10;8   22  202 1&#10;8   23.5    203 1&#10;8   23.8    204 1&#10;8   23.4    205 1&#10;8   22.3    206 1&#10;8   23.9    207 1&#10;8   29.1    208 1&#10;8   10.3    209 1&#10;8   21.7    210 1&#10;8   22.7    211 1&#10;8   22.5    212 1&#10;8   22  213 1&#10;8   26.5    214 1&#10;8   27.9    215 1&#10;8   23  216 1&#10;8   19.2    217 1&#10;8   24.5    218 1&#10;8   21.6    219 1&#10;8   22.5    220 1&#10;8   20.6    221 1&#10;8   22.2    222 1&#10;8   22.5    223 1&#10;8   22.6    224 1&#10;8   23.1    225 1&#10;8   21.8    226 1&#10;8   21.8    227 1&#10;8   22.8    228 1&#10;8   19.3    229 1&#10;8   21  230 1&#10;8   21.7    231 1&#10;8   20.9    232 1&#10;8   19  233 1&#10;8   21.2    234 1&#10;8   25.1    235 1&#10;8   20.8    236 1&#10;8   21.3    237 1&#10;8   22.3    238 1&#10;8   20.8    239 1&#10;8   19.9    240 1&#10;8   20.8    241 1&#10;8   19.4    242 1&#10;8   22.3    243 1&#10;8   24.1    244 1&#10;8   19  245 1&#10;8   22.1    246 1&#10;8   21.7    247 1&#10;8   21.9    248 1&#10;8   22.3    249 1&#10;8   9.7 250 1&#10;8   20.2    251 1&#10;8   18.9    252 1&#10;8   19.9    253 1&#10;8   21.3    254 1&#10;8   20.8    255 1&#10;8   21  256 1&#10;8   20.7    257 1&#10;8   18.4    258 1&#10;8   17.7    259 1&#10;8   16.4    275 1&#10;8   17.8    276 1&#10;8   17.1    277 1&#10;8   17.7    278 1&#10;8   20.7    279 1&#10;8   16.5    280 1&#10;8   16.9    281 1&#10;8   17.1    282 1&#10;8   20.8    283 1&#10;8   18.2    284 1&#10;8   9   285 1&#10;8   18.4    286 1&#10;8   17.9    287 1&#10;8   20.7    288 1&#10;8   11.8    261 1&#10;8   18.5    262 1&#10;8   18.9    263 1&#10;8   19  264 1&#10;8   18.1    265 1&#10;8   10.5    266 1&#10;8   .   267 1&#10;8   .   268 1&#10;8   .   269 1&#10;8   .   270 1&#10;8   .   271 1&#10;8   7.2 272 1&#10;8   17.7    273 1&#10;8   17.1    274 1&#10;8   20  260 1&#10;8   5   341 1&#10;8   10  342 1&#10;8   18.4    343 1&#10;8   12.6    344 1&#10;8   18  345 1&#10;8   14.3    346 1&#10;8   5.1 347 1&#10;8   11.8    348 1&#10;8   12  349 1&#10;8   13.8    350 1&#10;8   6.4 351 1&#10;8   8.8 352 1&#10;8   8.4 353 1&#10;8   11.7    354 1&#10;8   8.9 355 1&#10;8   10  356 1&#10;8   11.4    357 1&#10;8   10.3    358 1&#10;8   10.8    359 1&#10;8   10.4    360 1&#10;8   12.4    361 1&#10;8   5.4 362 1&#10;8   13.1    363 1&#10;8   7.6 364 1&#10;8   14.3    365 1&#10;;&#10;&#10;proc print data=a ;&#10;where ID='8';&#10;var ID DIM Vol;&#10;run;&#10;&#10;proc sort data=a out=new;&#10;where ID='8';&#10;by ID;&#10;run;&#10;/*find the outliers for an identified ID*/&#10;proc ucm data=new;&#10;model totalVol;&#10;autoreg;&#10;level plot=smooth;&#10;splinereg DIM nknots=5 degree=3 variance=0 noest;&#10;season length=12 var=0 noest;&#10;estimate plot=panel extradiffuse=325;&#10;outlier maxnum=10;&#10;forecast extradiffuse=325;&#10;run;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and the other code I find was using the same data set but this&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ods graphics on;&#10;proc transreg data=new solve ss2 plots=(transformation obp);&#10;model identity(totalVol) = spline(DIM / degree=3 nknots=40);&#10;&#10;run;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-05-05T02:51:47.973" Id="96409" LastActivityDate="2014-05-05T03:11:24.483" LastEditDate="2014-05-05T03:11:24.483" LastEditorUserId="805" OwnerUserId="44975" PostTypeId="1" Score="0" Tags="&lt;standard-deviation&gt;&lt;sas&gt;&lt;outliers&gt;&lt;splines&gt;" Title="standard deviation set for spline cubic regression model" ViewCount="47" />
  <row Body="&lt;p&gt;Instead of calculating overlapping intervals you calculate the Z-score. This is algorithmically easier to implement, and you will get statistical libraries to help.&lt;/p&gt;&#10;&#10;&lt;p&gt;Take a look : &lt;a href=&quot;https://onlinecourses.science.psu.edu/stat200/node/53&quot; rel=&quot;nofollow&quot;&gt;https://onlinecourses.science.psu.edu/stat200/node/53&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-05T03:04:08.200" Id="96410" LastActivityDate="2014-05-05T03:04:08.200" OwnerUserId="44976" ParentId="38730" PostTypeId="2" Score="0" />
  
  
  <row Body="&lt;p&gt;I am pretty sure it is not the L1 cost function. Neural nets are pretty robust when it comes to only locally differentiable things. To make really sure, you can use the L2 loss and see if it has similar problems. That being said, my experience is that L1 and L2 based objectives find similar solutions most of the time.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here are some things that you should investigate:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Try to initialize the parameters in different ranges (e.g. normally distributed around zero with a standard deviation of 1e-6,1e-5, ..., 1e-1.).&lt;/li&gt;&#10;&lt;li&gt;Reduce the learning rate, e.g. 1e-6,1e-5, ..., 1e-1. (I suppose you are using SGD).&lt;/li&gt;&#10;&lt;li&gt;In case you are using sth more sophisticated, especially second order such as LBFGS, use either stochastic gradient descent, rprop, rmsprop or adadelta. (These methods are not bad per se, but in some cases work horrible. As the other optimizers do in other cases.)&lt;/li&gt;&#10;&lt;li&gt;Have you tried mini batches? That is calculating the loss not on the whole data set or a single sample, but on a group of samples (e.g. 10, 50, 200--depends on your data set size). This can help a lot and is the most underestimated hyper parameters.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2014-05-05T07:33:35.273" Id="96422" LastActivityDate="2014-05-05T08:07:09.263" LastEditDate="2014-05-05T08:07:09.263" LastEditorUserId="2860" OwnerUserId="2860" ParentId="96399" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;In Linoff and Berry's &quot;Data Mining Techniques&quot; they mention reducing the number of categorical variables in a classification model by replacing the variable with the historic response rate.&lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;When building model sets for directed data mining, a powerful transformation is to replace categorical variables with the historical measure of what you are trying to predict. So, historical response rate, historical attrition rate, and historical average customer spend by ZIP code, county, occupation code, or whatever are often more powerful predictors than the original categories themselves.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;Is this valid from a statistical perspective?  &lt;/p&gt;&#10;&#10;&lt;p&gt;Are there any papers that discuss this technique?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-05T08:24:06.403" Id="96426" LastActivityDate="2014-05-05T08:24:06.403" OwnerUserId="29070" PostTypeId="1" Score="0" Tags="&lt;classification&gt;&lt;categorical-data&gt;&lt;data-mining&gt;&lt;data-transformation&gt;&lt;feature-selection&gt;" Title="Replacing categorical variables with historic response rate" ViewCount="23" />
  <row AnswerCount="0" Body="&lt;p&gt;In k-means algorithm, the distance minimization step is equivalent to maximize likelihood: $P(X|\theta)$ or to maximize posterior distribution $P(\theta|X)$?&lt;br/&gt; &#10;I think it's more logical to maximize posterior distribution $P(\theta|X)$ because it corresponds to classify $X$ in a cluster and usually it is posterior distribution $P(\theta|X)$ which is used for that.&#10;If it corresponds to maximize likelihood $P(X|\theta)$ can you explain me why?&lt;br/&gt;&#10;Thank you&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-05-05T09:01:58.063" Id="96430" LastActivityDate="2014-05-05T09:01:58.063" OwnerUserId="44024" PostTypeId="1" Score="0" Tags="&lt;distributions&gt;&lt;maximum-likelihood&gt;&lt;k-means&gt;&lt;posterior&gt;&lt;likelihood&gt;" Title="Square distance and likelihood in k-means" ViewCount="55" />
  
  <row AcceptedAnswerId="96454" AnswerCount="1" Body="&lt;p&gt;I have the expected ratio of four parameters A, B, C and D&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;&#10;         A :   B   :  C   :  D&#10;        3.9 : 12.0 : 24.3 : 59.8&#10;&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I perform three experiments which provides me ratios of A,B,C and D.&#10;As the results show, the ratios obtained in Experiment 1 are more closer to the Expected ratio&#10;Experiment 3 seems to be the worst&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;&#10;                 A :   B   :  C   :  D&#10;Experiment 1    3.9 : 12.0 : 24.3 : 59.8 &lt;br/&gt;&#10;Experiment 2    0.7 :  2.0 : 25.6 : 71.7 &lt;br/&gt;&#10;Experiment 3    3.9 : 29.9 : 53.3 : 7.2 &lt;br/&gt;&#10;&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Please suggest a single metric that can clearly indicate the closeness of values obtained in various experiments (as compared to the Expected ratio)&lt;/p&gt;&#10;&#10;&lt;p&gt;Need to do this for a lot of experiments&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-05-05T10:43:23.980" Id="96445" LastActivityDate="2014-05-05T11:52:33.497" OwnerUserId="44993" PostTypeId="1" Score="0" Tags="&lt;metric&gt;" Title="Need a suitable Metric to indicate closeness between a set of expected vs observed values" ViewCount="22" />
  <row Body="&lt;p&gt;As it turns out, my design is almost exactly that described by &lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0749596X07001398&quot; rel=&quot;nofollow&quot;&gt;Baayen, Davidson and Bates (2008)&lt;/a&gt; (&lt;em&gt;&quot;Mixed-effects modeling with crossed random effects&#10;for subjects and items&quot;&lt;/em&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;Data of this form is handled explicitly by R package &lt;code&gt;lme4&lt;/code&gt; (written by the third author), but not, apparently, by &lt;code&gt;nlme&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;My syntax in the question was (I think) correct: the random-intercept model is fitted by &lt;code&gt;lmer(reaction_time ~ condition + (1|participant) + (1|problem), data=data)&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, I've found a useful overloaded version of the &lt;code&gt;lmer&lt;/code&gt; function in package &lt;a href=&quot;http://cran.r-project.org/web/packages/lmerTest/index.html&quot; rel=&quot;nofollow&quot;&gt;&lt;code&gt;lmerTest&lt;/code&gt;&lt;/a&gt; which provides a number of methods for calculating p values for terms in mixed-models, and package &lt;a href=&quot;http://www.inside-r.org/packages/cran/MuMIn/docs/MuMIn&quot; rel=&quot;nofollow&quot;&gt;&lt;code&gt;MuMin&lt;/code&gt;&lt;/a&gt; provides an implementation of &lt;a href=&quot;http://onlinelibrary.wiley.com/doi/10.1111/j.2041-210x.2012.00261.x/abstract&quot; rel=&quot;nofollow&quot;&gt;Nakagawa and&#10; Schielzeth's (2012)&lt;/a&gt; method of calculating $R^{2}$ for mixed-models (according to &lt;a href=&quot;http://ecologyforacrowdedplanet.wordpress.com/2013/08/27/r-squared-in-mixed-models-the-easy-way/&quot; rel=&quot;nofollow&quot;&gt;this blog&lt;/a&gt;).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-05T11:15:32.873" Id="96449" LastActivityDate="2014-05-05T11:15:32.873" OwnerUserId="42952" ParentId="95105" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;Yes, you should set the null hypothesis to be like that. &lt;/p&gt;&#10;&#10;&lt;p&gt;And in this case, it's a one-side test, since your statistics should be only larger than 2000.&lt;/p&gt;&#10;&#10;&lt;p&gt;You must know which statistics you are testing. &lt;/p&gt;&#10;&#10;&lt;p&gt;If the statistics is the lifetime $T$ of each sample from the population, and it is the mean of the lifetime $\bar T$ you want to testand you have the sample variance $S_T^2$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then we have $\bar T-E[T]=2000\sim t(0,S_T^2/n,n-1)$ as a t-distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;If $\bar T&amp;gt;2000+S_T/\sqrt n\times t(0.95,n-1)$, we conclude $H_1:\bar T&amp;gt; E[T]=2000$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Otherwise we conclude $H_0:\bar T\leq E[T]=2000$&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-05-05T13:40:21.630" Id="96467" LastActivityDate="2014-05-05T13:51:03.770" LastEditDate="2014-05-05T13:51:03.770" LastEditorUserId="44635" OwnerUserId="44635" ParentId="96463" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="96485" AnswerCount="1" Body="&lt;p&gt;If so, why? If not, why not?&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm thinking of an ANOVA context, but if the answer changes depending on context I would also be interested to know why that is.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-05-05T15:13:31.570" Id="96483" LastActivityDate="2014-05-05T21:50:28.203" LastEditDate="2014-05-05T21:09:01.113" LastEditorUserId="7291" OwnerUserId="9162" PostTypeId="1" Score="2" Tags="&lt;anova&gt;&lt;f-statistic&gt;" Title="Is it possible to get a negative F-statistic (e.g. in ANOVA)?" ViewCount="3069" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have estimated the coefficients of the following equation, using the fixed-effect model:&lt;/p&gt;&#10;&#10;&lt;p&gt;$Y_{it}=\alpha _i+ \rho _t + \beta _1 X_{it}+\beta _2 C_i*D_t+\epsilon_{it}$&lt;/p&gt;&#10;&#10;&lt;p&gt;I have observations from 1980 to 2010 for $Y_{it}$ and $X_{it}$.I am interested in the interaction term. The dummy variable $D_t$ is equal to $1$ for years 2000 to 2010 (and $0$ otherwise) and is interacted with the continuous variable $C_i$ which is time-invariant (but varies over the units $i$).&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is: can $\beta _2$ be interpreted as the differentiated effect of $C$ on $Y$ during the period 2000-2010 compared to 1980-2000? &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!     &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-05-05T16:00:36.380" Id="96491" LastActivityDate="2014-05-05T16:00:36.380" OwnerUserId="45011" PostTypeId="1" Score="5" Tags="&lt;categorical-data&gt;&lt;interaction&gt;&lt;panel-data&gt;&lt;fixed-effects-model&gt;" Title="Panel regressions with an interaction term between a time dummy variable and a time invariant variable" ViewCount="548" />
  <row AnswerCount="0" Body="&lt;p&gt;I am doing Rainfall-Runoff modeling. I have 4014 inputs and 4014 outputs. I am confused about the of support vectors. Suppose I have a model having 2000 support vectors and I have another model having 3900 support vectors.  Both perform well, and the second model has slightly better performance. Which is better? What is the relation between number of support vectors and over fitting?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-05T17:10:59.923" Id="96497" LastActivityDate="2014-05-06T12:59:04.710" LastEditDate="2014-05-06T12:59:04.710" LastEditorUserId="615" OwnerUserId="45012" PostTypeId="1" Score="1" Tags="&lt;svm&gt;&lt;libsvm&gt;" Title="Effect of high number of support vectors" ViewCount="40" />
  
  
  
  <row Body="&lt;p&gt;How about a hybrid approach: get aggregate annual sales at monthly frequency, i.e. Jan through Dec, Feb through Jan, Mar through Feb etc.&lt;/p&gt;&#10;&#10;&lt;p&gt;You'll have to adjust for overlapping intervals (e.g. with Newey-West &lt;a href=&quot;http://www.mathworks.com/help/econ/hac.html&quot; rel=&quot;nofollow&quot;&gt;HAC&lt;/a&gt; estimator), but your forecast will produce annual sales directly.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-05-05T20:01:29.510" Id="96515" LastActivityDate="2014-05-05T20:29:28.493" LastEditDate="2014-05-05T20:29:28.493" LastEditorUserId="36041" OwnerUserId="36041" ParentId="96512" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;As an example, look at the relationship between &lt;em&gt;t&lt;/em&gt;, &lt;em&gt;p&lt;/em&gt; and &lt;em&gt;d&lt;/em&gt; in the &lt;em&gt;t&lt;/em&gt;-test.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;p&lt;/em&gt; is defined as the probability of obtaining a test statistic this or more extreme, if the hypothesis under test is true. It is inversely related to both the sample size and the test statistic - in this case, &lt;em&gt;t&lt;/em&gt;. Conversely, &lt;em&gt;t&lt;/em&gt; ~ 1-&lt;em&gt;p&lt;/em&gt; and &lt;em&gt;n&lt;/em&gt; ~ 1-&lt;em&gt;p&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;For correlated samples, &#10;$d_{z} = \frac{t}{\sqrt{n}}$. Then, $d_{z}\sqrt{n} = t$; then, if &lt;em&gt;n&lt;/em&gt; is held constant, as $d_{z}$ increases, &lt;em&gt;t&lt;/em&gt; increases; and since &lt;em&gt;t&lt;/em&gt; ~&lt;em&gt;1-p&lt;/em&gt;, $d_{z}$ ~ 1-&lt;em&gt;p&lt;/em&gt;. Also, if $d_{z}$ is held constant, as &lt;em&gt;n&lt;/em&gt; increases, &lt;em&gt;t&lt;/em&gt; increases, and &lt;em&gt;n&lt;/em&gt; ~ 1-&lt;em&gt;p&lt;/em&gt;.&#10;Colloquially speaking, since both 1-&lt;em&gt;p&lt;/em&gt; and the measures of effect size depend on how extreme the test statistic is, they are correlated.&lt;/p&gt;&#10;&#10;&lt;p&gt;This will apply to other measures of effect size as well, such as the relationship between &lt;em&gt;F&lt;/em&gt;, &lt;em&gt;n&lt;/em&gt; and $\eta^2$ - trivially for those cases where the &lt;em&gt;F&lt;/em&gt;-test converges with the &lt;em&gt;t&lt;/em&gt;-test; or for correlation coefficients, which can also converge with the &lt;em&gt;t&lt;/em&gt;-test.&#10;(I'd be very interested to learn if there are any measures of effect size which do not behave in this way, and by which justification they could be associated with other standardised measures of effect size.)&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;in order to reach significance you need to be trying to prove an effect that is large enough&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Note that an effect size (such as &lt;em&gt;r&lt;/em&gt;) is a descriptive statistic, a property of the sample. It is not inferential. To support a claim about the population, confidence intervals over effect sizes can be used.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-05T20:19:34.813" Id="96519" LastActivityDate="2014-05-06T18:00:22.760" LastEditDate="2014-05-06T18:00:22.760" LastEditorUserId="28288" OwnerUserId="28288" ParentId="96500" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Try this: $p_i\stackrel{iid}{\sim} Be(\alpha,\beta)$ and $p(\alpha,\beta)\propto (\alpha+\beta)^{-5/2}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;I believe the issue you are running into is that $ss\sim Ga(\gamma,\gamma)$ with $\gamma\to 0$ results in the improper $p(ss)\propto 1/ss$ prior. This prior, together with your uniform prior on mu, results in an improper posterior. Despite the fact that you are using a proper prior, it is close enough to this improper prior to cause issues. &lt;/p&gt;&#10;&#10;&lt;p&gt;You might want to take a look at page 110 of &lt;a href=&quot;http://www.stat.columbia.edu/~gelman/book/&quot;&gt;Bayesian Data Analysis (3rd ed)&lt;/a&gt; for a discussion of priors for this model as well as the prior suggested above.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-05-05T21:31:20.233" Id="96527" LastActivityDate="2014-05-05T21:31:20.233" OwnerUserId="40440" ParentId="96481" PostTypeId="2" Score="5" />
  
  <row AcceptedAnswerId="96537" AnswerCount="2" Body="&lt;pre&gt;&lt;code&gt;factanal(x = ~V1 + V2 + V3 + V4 + V5, factors = 2, data = airfoil_self_noise)&#10;&#10;Uniquenesses:&#10;   V1    V2    V3    V4    V5 &#10;0.898 0.005 0.005 0.995 0.394 &#10;&#10;Loadings:&#10;   Factor1 Factor2&#10;V1 -0.319         &#10;V2  0.877  -0.474 &#10;V3          0.997 &#10;V4                &#10;V5  0.754  -0.194 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Here, I have no idea what empty space means in Factor1-V3, Factor1-V4, Factor2-V1, and Factor2-V4.&lt;/p&gt;&#10;&#10;&lt;p&gt;Weren't they supposed to be filled with some numbers, like even with 0?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-05T23:31:13.640" Id="96536" LastActivityDate="2014-05-05T23:46:49.317" OwnerUserId="290" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;factor-analysis&gt;" Title="What does empty output in Factor Analysis means in R?" ViewCount="57" />
  <row AnswerCount="1" Body="&lt;p&gt;So I've been trying to make sense of the clustering algorithm on page 6 of &lt;a href=&quot;http://www.kyb.mpg.de/fileadmin/user_upload/files/publications/attachments/Luxburg07_tutorial_4488%5B0%5D.pdf&quot; rel=&quot;nofollow&quot;&gt;this paper&lt;/a&gt;.  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/wE9PB.png&quot; alt=&quot;Algorithm&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Are the &quot;first&quot; k eigenvalues they refer to the smallest eigenvalues?&lt;/p&gt;&#10;&#10;&lt;p&gt;What are the $y_i$ exactly? I don't see the motivation for using them.&lt;/p&gt;&#10;&#10;&lt;p&gt;If anyone could link other literature or explain how/why the first eigenvectors are being used, that would be great.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-05-05T23:57:58.153" FavoriteCount="1" Id="96539" LastActivityDate="2015-02-27T00:12:44.540" LastEditDate="2014-05-06T03:33:40.297" LastEditorUserId="45034" OwnerUserId="45034" PostTypeId="1" Score="1" Tags="&lt;clustering&gt;&lt;spectral-analysis&gt;" Title="Motivations for Shi-Malik Algorithm" ViewCount="97" />
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a hundred items that I'm performing EFA on, with around 370 complete cases. Using parallel analysis to determine the number of factors to extract, EFA gave 9 factors, all of which make theoretical sense.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, I have 280 incomplete cases, which are missing answers for up to 10% of the items. Missing data is not at random, with a couple of items missing responses for 20-40% of all respondents. Using the &lt;code&gt;R&lt;/code&gt; package &lt;code&gt;missForest&lt;/code&gt;, I imputed the missing data and performed factor analysis on the resulting imputed dataset. This gave me 10 factors, around 5 of which are the same as the previous analysis and 5 'new' factors.&lt;/p&gt;&#10;&#10;&lt;p&gt;What is the protocol when the results of imputed data analysis are different to those of a complete cases analysis? Any advice is much appreciated.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-05-06T09:57:13.320" Id="96576" LastActivityDate="2014-05-06T12:14:05.317" LastEditDate="2014-05-06T12:14:05.317" LastEditorUserId="13554" OwnerUserId="13554" PostTypeId="1" Score="2" Tags="&lt;factor-analysis&gt;&lt;data-imputation&gt;&lt;eda&gt;" Title="What to do when exploratory factor analysis results are different for complete-cases and imputed data?" ViewCount="42" />
  
  <row Body="&lt;p&gt;Georges Monette and I introduced the GVIF in the paper &quot;Generalized collinearity diagnostics,&quot; JASA 87:178-183, 1992 (&lt;a href=&quot;http://www.tandfonline.com/doi/abs/10.1080/01621459.1992.10475190#.U2jkTFdMzTo&quot; rel=&quot;nofollow&quot;&gt;link&lt;/a&gt;). As we explained, the GVIF represents the squared ratio of hypervolumes of the joint-confidence ellipsoid for a subset of coefficients to the &quot;utopian&quot; ellipsoid that would be obtained if the regressors in this subset were uncorrelated with regressors in the complementary subset. In the case of a single coefficient, this specializes to the usual VIF. To make GVIFs comparable across dimensions, we suggested using GVIF^(1/(2*Df)), where Df is the number of coefficients in the subset. In effect, this reduces the GVIF to a linear measure, and for the VIF, where Df = 1, is proportional to the inflation due to collinearity in the confidence interval for the coefficient.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-05-06T13:19:41.517" Id="96593" LastActivityDate="2014-05-06T13:42:02.013" LastEditDate="2014-05-06T13:42:02.013" LastEditorUserId="7290" OwnerUserId="45067" ParentId="70679" PostTypeId="2" Score="3" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Consider a huge dataset of which the following is a characteristic sample:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;stock.id date analyst1 analyst2 analyst3 return&#10;       1 Jan         2       -2        1     10&#10;       2 Feb        -1        2        0    -10&#10;       3 Mar         0        2        0      3&#10;       4 Apr         0        0        0      2&#10;       5 May         1        0        2     -1&#10;       6 Jun         1        0        2     -3&#10;     ...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The &quot;analyst&quot; columns are ratings issued by individual stock analysts and the &quot;return&quot; column is the subsequent return on rated stock.  Here rating 2 means a strong buy and -2 means a strong sell.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am looking for a statistical test or techniques that can tell me how well the analysts performed.  Ideally, I want to come up with scores to indicate which analysts have rated stocks correctly.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-05-06T14:31:43.230" Id="96608" LastActivityDate="2014-05-06T16:38:30.530" LastEditDate="2014-05-06T16:38:30.530" LastEditorUserId="919" OwnerUserId="26339" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;prediction&gt;&lt;rating&gt;&lt;accuracy&gt;" Title="How well do analyst ratings predict stock perfromance?" ViewCount="61" />
  
  
  <row Body="&lt;p&gt;A t-test, like any hypothesis test, can show strong evidence that something is not like it should be. If one fails to prove than, one can't say that the opposite is true. The framework of hypothesis test consider some state as &quot;the natural&quot; state. This state is assumed if there are no evidence against it and cannot be proved. &lt;/p&gt;&#10;&#10;&lt;p&gt;See &lt;a href=&quot;http://www.nizkor.org/features/fallacies/burden-of-proof.html&quot; rel=&quot;nofollow&quot;&gt;Burden of Proof Fallacy&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-06T16:07:24.293" Id="96628" LastActivityDate="2014-05-06T16:07:24.293" OwnerUserId="16709" ParentId="96627" PostTypeId="2" Score="-1" />
  <row Body="&lt;p&gt;It's impossible to say for sure, but my guess is that they simply did pairwise t-tests. They looked at the average responses from the groups that interested them and did t-tests.&lt;/p&gt;&#10;&#10;&lt;p&gt;An option for these data would have been a two-way ANOVA (unbalanced, no doubt) with smoking and exposure group as the factors. There is no evidence from the abstract that the researchers attempted any such a thing. The advantage to such a model would be more degrees of freedom for error and better estimate of error (assuming constant variances!). &lt;/p&gt;&#10;&#10;&lt;p&gt;In my experience, a lot of researchers don't go further than pair-wise t-tests.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-06T17:28:16.750" Id="96641" LastActivityDate="2014-05-06T17:28:16.750" OwnerUserId="14188" ParentId="96607" PostTypeId="2" Score="3" />
  <row AnswerCount="1" Body="&lt;p&gt;I am investigating the long-term relationship of some variables using the R package &lt;code&gt;vars&lt;/code&gt;, but in the output of the &lt;code&gt;cajorls&lt;/code&gt; function I cannot see whether each coefficient is significant.  This information is provided by the &lt;code&gt;cajools&lt;/code&gt; function but for the cointegration equations I need to use the &lt;code&gt;cajorls&lt;/code&gt; function. Does anyone have an idea how to get the information from &lt;code&gt;cajorls&lt;/code&gt;?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-06T17:46:42.407" Id="96645" LastActivityDate="2015-01-28T02:12:35.217" LastEditDate="2014-12-26T11:54:19.307" LastEditorUserId="1739" OwnerUserId="45087" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;time-series&gt;&lt;cointegration&gt;" Title="Finding significance levels for cointegrating coefficients in cajorls" ViewCount="169" />
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;If $X$ and $Y$ be two i.i.d. random variables, then what should $E(X|X&amp;lt;Y)$ essentially look like?&lt;/p&gt;&#10;&#10;&lt;p&gt;P.S.-Is the denominator equal to $\frac12$?&#10;(the two r.v.'s being iid serving as the motivation behind this.)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-06T20:14:52.750" Id="96671" LastActivityDate="2014-05-06T22:40:41.853" OwnerUserId="44431" PostTypeId="1" Score="6" Tags="&lt;conditional-expectation&gt;&lt;iid&gt;" Title="Can someone give a clear-cut idea of $E(X|X&lt;Y)$?" ViewCount="85" />
  <row Body="&lt;p&gt;I would venture that it refers to regular random forests, but the author wants to bring out the distinction between a) the bagging / bootstrapping of the observations used for each tree and b) the random selection of a subset of the input parameters. not sure though.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-06T21:31:59.290" Id="96676" LastActivityDate="2014-05-06T21:31:59.290" OwnerUserId="36680" ParentId="96632" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;It seems you're interested in something along the lines of: &lt;/p&gt;&#10;&#10;&lt;p&gt;$H_0: \mu_1=\mu_2=...=\mu_k$&lt;br&gt;&#10;vs.&lt;br&gt;&#10;$H_1: \mu_j&amp;gt;\mu_1,...,\mu_{j-1},\mu_{j+1},...,\mu_k\quad$ for some $j$&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;There's no great difficulty in constructing a permutation test for this; for example, consider the statistic &quot;largest sample mean minus the mean of the remaining samples&quot;, and then performing a one-tailed test.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;An alternative would be to frame it as a set of one-sided ANOVA contrasts (not orthogonal) of the form $c_i'\mu$ (e.g. $c_3'\,\propto\,(-1, -1,  9, -1, -1, -1, -1, -1, -1, -1)\,$) and test them all one-tailed, presumably with some adjustment for multiple testing.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2014-05-06T22:07:03.143" Id="96680" LastActivityDate="2014-05-07T04:46:45.300" LastEditDate="2014-05-07T04:46:45.300" LastEditorUserId="32036" OwnerUserId="805" ParentId="96672" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;To summarize the bit from @JuhoKokkala that meant the most to me:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;CI's (a range of $\theta$ as a function of $\hat{\theta}$) can be inverted into Acceptance Regions (a range of $\hat{\theta}$ as a function of $\theta$).&lt;/li&gt;&#10;&lt;li&gt;For discrete distributions, the AR's are discrete-valued and thus cannot vary continuously with $\theta$. Thus, discontinuities when a point is added or subtracted from the AR.&lt;/li&gt;&#10;&lt;li&gt;Exact CI's are OK with this. Pearson CI's are only approximately correct (in the limit as $n\rightarrow\infty$).&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2014-05-06T22:32:51.610" Id="96682" LastActivityDate="2014-05-06T22:32:51.610" OwnerUserId="37851" ParentId="96666" PostTypeId="2" Score="0" />
  
  <row AnswerCount="0" Body="&lt;p&gt;In GEE, if you increase the number of clusters would this increase or decrease the standard errors of the regression coefficients? Would the estimated std error of the regression coefficients be smaller or larger than the true std error of the coefficients?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-06T23:14:02.587" Id="96686" LastActivityDate="2014-05-07T06:21:46.040" LastEditDate="2014-05-07T06:21:46.040" LastEditorUserId="3277" OwnerUserId="44281" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;gee&gt;&lt;cluster-sample&gt;" Title="Number of cluster samples in GEE" ViewCount="27" />
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am conducting a survey among long-term care facilities (LTCF) in a city about the prevalence of a certain disease. I have stratified the city by two levels: i) three strata based on geographic boundaries; ii) another two strata for LTCFs' funding (government-funded or private-owned). This gives me six strata. I know the total number of elderly live in these six strata, say the ratio for strata A:B:C:D:E:F is 1:1:1:1:1:1, as my calculated sample size is 3000, then I will take 500:500:500:500:500:500 elderly from the six strata. I randomly pick up LTCF from each stratum and do the survey until the sampling target for that stratum is reached.&lt;/p&gt;&#10;&#10;&lt;p&gt;Things are proceeding as planned, except for one stratum, say stratum A. The response rate for stratum A is a bit too good, so that we have oversampled one stratum to almost 200% (i.e. my sampling target is 500, but now I have more than 900 elderly sampled). Since the 1:1:1:1:1:1 ratio is no longer maintained, I am not sure how should I deal with the extra elderly that have been sampled, I am thinking of picking a simple random sample from stratum A so that only 500 elderly is used for analysis, but it seems to be a waste.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any suggestions or reference? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-07T08:45:16.867" Id="96716" LastActivityDate="2014-05-07T08:53:45.180" LastEditDate="2014-05-07T08:53:45.180" LastEditorUserId="22047" OwnerUserId="588" PostTypeId="1" Score="0" Tags="&lt;stratification&gt;&lt;cluster-sample&gt;&lt;oversampling&gt;" Title="How to deal with oversampling for stratified cluster sampling" ViewCount="60" />
  <row Body="&lt;p&gt;This is how I understood it: &lt;/p&gt;&#10;&#10;&lt;p&gt;We test the hypothesis that $exp(\theta_0) = exp(\hat{\theta})$. As we are applying a function to our variable, we have to use the delta method to calculate the variance: $\sigma_{g(\hat{\theta})}^2={[g'(\hat{\theta})]}^2\sigma_{\hat{\theta}}$, which is in our case equal to $\sigma_{\hat{\theta}}^{2} exp(2\hat{\theta})$ and hence our test statistic is equal to $\frac{{[g(\hat{\theta})-g(\theta_0)]}^2}{\sigma_{g(\hat{\theta})}^2}=\frac{{[exp(\hat{\theta})-exp(\theta_0)]}^2}{\sigma_{\hat{\theta}}^{2}exp(2\hat{\theta})}$ which is chi-squared distributed with one degree of freedom.&lt;/p&gt;&#10;&#10;&lt;p&gt;Unfortunately the calculation of this test statistic is not yet identical as the one of the stata output, so maybe I am still doing something wrong...&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-07T11:48:19.720" Id="96732" LastActivityDate="2014-05-07T11:48:19.720" OwnerUserId="36462" ParentId="95959" PostTypeId="2" Score="2" />
  
  
  <row AcceptedAnswerId="96756" AnswerCount="1" Body="&lt;p&gt;I am building a ML ordered logistic regression. First of all, I really don't know if this is the best way to fit a model to my data, as I am not too confident in ML ordered logit regressions, compared to ML linear regressions. The relevant variables are as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;$Y_{ij}$ = support for the European Union, where 0 = no support, 1 = some support and 2 = much support.&lt;/p&gt;&#10;&#10;&lt;p&gt;$X_1$ = constitutional patriotism as a sumscore of three variables. Possible outcomes are $0,1,...,9$&lt;/p&gt;&#10;&#10;&lt;p&gt;$X_2$ = cultural patriotism as a sumscore of two variables. Possible outcomes are: $0,1,...,6$&lt;/p&gt;&#10;&#10;&lt;p&gt;The ML model consists of individuals nested with seven countries. Apart from the variables above, I have added a lot of control variables. First off, I want to create an interaction term including constitutional patriotism (CON) and cultural patriotism (CUL), where both variables are treated as continuous. The problem though, is that the two scales are not comparable. Hence, I thought about scaling the variables like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;$X_{1 scaled} = X_1/9$ and $X_{2 scaled} = X_2/6$ &lt;/p&gt;&#10;&#10;&lt;p&gt;By doing it this way, I secure that both variables ranges from 0 to 1. Hence, a one unit increase in $X_1$ is comparable to a one unit increase in $X_2$ as they both have minimum = 0 and maximum = 1. &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Do you think it would be more appropriate to standardize the variables instead, so they have mean 0 and variance 1?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Next I will have to choose between ML linear regression and ML ordered logistic regression. As said before, I am not too confident in the practical use of ordered logit. I wonder how much of a difference it makes, taking into account that ML ordered logit adds substantially more complexity to the model. I will, of course, choose a design that both makes sense, and delivers the best fit to the data. If I run a ML linear regression with my ordinal DV, the stadardized residuals are distributed as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ZKgsf.png&quot; alt=&quot;Standardized residuals&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;This seems to be a problem, as the residuals are clearly not normal distributed. &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Does this call for a ordered logit, or could I work around the problem in some other way?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Finally: &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Do you know of any methods to check how well my model fits the data in a ordered logit regression?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I have thought about calculating the predicted distribution of individuals choosing $Y_{ij}=0$, $Y_{ij}=1$ and $Y_{ij}=2$ compared to the actual distribution found in the data. Also, I want to investigate if the ordered logit violates the proportional odds assumption. However, I am not sure how to do this in a ML framework.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any suggestions are much appreciated!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-07T13:42:34.847" Id="96752" LastActivityDate="2014-05-07T14:03:24.060" OwnerUserId="19891" PostTypeId="1" Score="1" Tags="&lt;multilevel-analysis&gt;&lt;residuals&gt;&lt;standardization&gt;&lt;ordered-logit&gt;" Title="Building a ML ordered logit regression model" ViewCount="85" />
  <row AcceptedAnswerId="96773" AnswerCount="1" Body="&lt;p&gt;Are Studentized deleted residuals a form of k-fold cross validation when K=N?  (this question is asked in the context of the discussion &lt;a href=&quot;http://stats.stackexchange.com/questions/96739/what-is-the-632-rule-in-bootsrapping&quot;&gt;here)&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-07T14:40:34.863" Id="96764" LastActivityDate="2014-05-07T15:19:02.527" OwnerUserId="196" PostTypeId="1" Score="1" Tags="&lt;cross-validation&gt;&lt;residuals&gt;" Title="Are studentized deleted residuals a form of k-fold cross validation when K=N?" ViewCount="99" />
  
  
  <row AcceptedAnswerId="96958" AnswerCount="1" Body="&lt;p&gt;In Discovering Statistics Using SPSS 4e, Andy Field writes on p835 that:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;SPSS gives you the choice of two methods for estimating the parameters in&#10;  the analysis: maximum likelihood (ML), which we have encountered&#10;  before, and restricted maximum likelihood (REML). The conventional&#10;  wisdom seems to be that ML produces more accurate estimates of fixed&#10;  regression parameters, whereas REML produces more accurate estimates&#10;  of random variances (Twisk, 2006). As such, the choice of estimation&#10;  procedure depends on whether your hypotheses are focused on the fixed&#10;  regression parameters or on estimating variances of the random&#10;  effects. However, in many situations the choice of ML or REML will&#10;  make only a small difference to parameter estimates. Also, if you want&#10;  to compare models you must use ML.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I'm most interested in the last sentence. Why is it that to compare models you must use ML and not REML?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-07T15:40:33.730" Id="96782" LastActivityDate="2014-05-08T17:38:44.883" OwnerUserId="45005" PostTypeId="1" Score="4" Tags="&lt;maximum-likelihood&gt;&lt;multilevel-analysis&gt;" Title="Why is it necessary to use ML estimation instead of REML to compare multilevel linear models?" ViewCount="142" />
  <row Body="&lt;p&gt;In a panel model, the individual effect terms can be modeled as either random or fixed effects. If the individual effects are correlated with the other regressors in the model, the fixed effect model is consistent and the random effects model is inconsistent. On the other hand, if the individual effects are not correlated with the other regressors in the model, both random and fixed effects are consistent and random effects is efficient.&lt;/p&gt;&#10;&#10;&lt;p&gt;Since the fixed effects model is efficient in both situations, the random and fixed effects estimates ought to be close when both are consistent and distant when random effects is not efficient. Roughly speaking, the hausman test is based on this distance. Therefore, if the distance is large, the null that individual effects are uncorrelated with the other regressors (aka random effects preferred) is rejected. Conversely, if the distance is small, the null is not rejected, and random effects is preferred because it is more efficient.&lt;/p&gt;&#10;&#10;&lt;p&gt;Since you have a large test statistic, the distance is large, and you should reject the null and use fixed effects.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, there is an additional complication. In theory, the matrix V_b - V_B should be positive definite. However, the way the test statistic is constructed, this is not always true in practice. I know there is advice out there for what to do in this situation - you should be cautious about rejecting in these circumstances and look for more information.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-07T15:52:12.190" Id="96783" LastActivityDate="2014-05-07T15:52:12.190" OwnerUserId="44107" ParentId="96775" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="96802" AnswerCount="1" Body="&lt;p&gt;I currently working in a multivariate logistic model but I have a problem regarding the sample size of my observations:&lt;/p&gt;&#10;&#10;&lt;p&gt;-The &quot;success&quot; (1) event group has a sample size of 249 distinct observations&#10;- The &quot;non success&quot; (0) event group has a sample size of 48,957, and it's a significant part of the population, and many times larger than the &quot;success&quot; group.&lt;/p&gt;&#10;&#10;&lt;p&gt;So when I fit a multivariate logistic regression model, two things happen:&#10;- Because of the big sample size, The p-values of the model coefficients become always significant at alpha 1%&#10;- The fitted predicted probability variation between the groups becomes very tiny, even if the independent variables are good predictors.&lt;/p&gt;&#10;&#10;&lt;p&gt;So I was suggested making a bootstrap of the model. So here is my doubt :&lt;/p&gt;&#10;&#10;&lt;p&gt;-Should I do it the more usual way, taking smaller arbitrary size random samples of the complete sample (both groups) with replacement ( or without replacement in this case?)&lt;/p&gt;&#10;&#10;&lt;p&gt;OR&lt;/p&gt;&#10;&#10;&lt;p&gt;-Should I keep the the small &quot;success&quot; group constant and them add an equal number of different &quot;non success&quot; randomly picked cases in each sample. Is this &quot;cheating&quot;?&lt;/p&gt;&#10;&#10;&lt;p&gt;OR&lt;/p&gt;&#10;&#10;&lt;p&gt;-Should I just stay with the original whole sample model, and bear with it's little predictive power (less than 1% fitted predicted difference between the groups ), even when using significant predictors that differentiate the groups.&lt;/p&gt;&#10;&#10;&lt;p&gt;In both cases this is not exactly the usual bootstrap as I wish to make sub-samples of a larger group, the original big sample, and perhaps what I want is not bootstrap at all. My idea is to minimize the discrepancy between the two groups sizes, the inflated p-values, and the small probability fitted difference between groups due to tho fact that the &quot;success&quot; event is rare in my sample. Is this theoretically correct?&lt;/p&gt;&#10;&#10;&lt;p&gt;The final objective is to obtain an &quot;avarage&quot; model with the mean coefficients and use it to calculate the probability of the &quot;non success&quot; cases actualy being &quot;successes&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;Or does anybody has other idea for this question? Feel free to post any suggestion in R language. I know that there is a similar question but I want to know also if my ideas about this are wrong.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-05-07T16:49:35.417" Id="96794" LastActivityDate="2014-05-10T18:21:11.120" OwnerUserId="45128" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;logistic&gt;&lt;sampling&gt;&lt;bootstrap&gt;&lt;resampling&gt;" Title="Logistic regression and discrepant sample sizes between 0/1 groups" ViewCount="111" />
  
  
  
  <row Body="&lt;p&gt;Are you using group as a predictor in your model? If so, the coefficient tells you the difference in magnitude. You could bootstrap CIs for the coefficient to get your answer. A little bit more information on what/how you are doing the analysis would be helpful.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-05-07T18:29:16.163" Id="96815" LastActivityDate="2014-05-07T18:29:16.163" OwnerUserId="44527" ParentId="96801" PostTypeId="2" Score="0" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Does it make sense to compare actual vs. forecast using correlation analysis / see how close $R^2$ is to 1?&lt;/p&gt;&#10;&#10;&lt;p&gt;Does it make sense to use a paired &lt;em&gt;t&lt;/em&gt;-test to test actual vs. forecast to get accuracy of forecast?&lt;/p&gt;&#10;&#10;&lt;p&gt;I know RMSE is widely used, but I can use RMSE only if I have many models to compare.&#10;What if I just have one model/forecast version, and I need to prove to a wider audience that it is a decent/good forecast?&lt;/p&gt;&#10;&#10;&lt;p&gt;Correlation analysis and &lt;em&gt;t&lt;/em&gt;-tests are used by someone where I work, and I am trying to say that's not right, but I don't have a good answer as to why they cannot use correlation and &lt;em&gt;t&lt;/em&gt;-tests for forecast accuracy. &#10;Are they violating any assumptions?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-05-07T19:53:01.917" Id="96824" LastActivityDate="2014-05-07T20:14:49.180" LastEditDate="2014-05-07T20:14:49.180" LastEditorUserId="32036" OwnerUserId="45147" PostTypeId="1" Score="1" Tags="&lt;t-test&gt;&lt;forecasting&gt;&lt;assumptions&gt;&lt;rms&gt;&lt;volatility-forecasting&gt;" Title="Forecast accuracy  can we use correlation and $t$-tests?" ViewCount="68" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;Is there anything wrong with regressing a ratio on itself? For example can we regress the log(savings ratio) on log(income) or the log(debt to income) ratio on log(income)? If not, should we use the log transformation to take income to the RHS and regress log(savings) on log(income)? If using a log transformation is the way to go, how do I interpret my results, given that the savings ratio is what I am interest in, not savings?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-07T21:53:21.727" Id="96836" LastActivityDate="2014-05-07T23:29:43.503" LastEditDate="2014-05-07T22:15:39.827" LastEditorUserId="32036" OwnerUserId="45155" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;econometrics&gt;&lt;lognormal&gt;&lt;ratio&gt;" Title="Regressing a ratio on a component of the ratio" ViewCount="41" />
  
  <row Body="&lt;p&gt;The log of a ratio is the difference of the logs. So log(debt/income)~log(income) = log(debt)-log(income)~log(income). That can't be good. With random data you get a p value with 16 0's:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(123)&#10;income &amp;lt;- 10^(rnorm(100))*10000&#10;debt &amp;lt;- 10^(rnorm(100))*1000&#10;debtincome &amp;lt;- debt/income&#10;m1 &amp;lt;- lm(log(debtincome)~log(income))&#10;summary(m1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;log(savings)~log(income) is at least statistically reasonable; whether it is what you want, I can't say. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-05-07T23:29:43.503" Id="96852" LastActivityDate="2014-05-07T23:29:43.503" OwnerUserId="686" ParentId="96836" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;A Gaussian process gives a probability distribution over functions that pass through the data points. Is there a way to parameterize the Gaussian process to give a probability distribution over closed curves instead of functions? Can the same approach be generalized to give a distribution over surfaces without boundary?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-08T00:18:24.223" FavoriteCount="1" Id="96856" LastActivityDate="2014-05-09T02:21:39.390" OwnerUserId="45164" PostTypeId="1" Score="2" Tags="&lt;machine-learning&gt;&lt;gaussian-process&gt;&lt;geometry&gt;&lt;curves&gt;" Title="Gaussian Process for Closed Curves" ViewCount="35" />
  <row AnswerCount="1" Body="&lt;p&gt;Given a known distribution supported on a finite set of $n$ elements with probabilities $p_1,,p_n$ and an access to an unknown distribution $q$ is it known what is the number of samples from $q$ that is necessary and sufficient to estimate the total variation distance $||pq||_{TV}$ up to an error $\epsilon$ with constant probability?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-08T02:50:44.317" Id="96866" LastActivityDate="2014-08-14T16:17:41.037" OwnerUserId="45169" PostTypeId="1" Score="4" Tags="&lt;probability&gt;&lt;distributions&gt;&lt;mathematical-statistics&gt;" Title="Estimating total variation distance from a given distribution" ViewCount="50" />
  
  
  
  <row Body="&lt;p&gt;In your examples you seem to make the underlying assumption that you are &quot;blind&quot; to how the data you are going to analyze were generated. In other words, whether mean or median is the right tool for say something robust about central tendency of a sample of data largely depends on your (prior) knowledge of the data generating process. That's probably the main reason why most of us dealing with statistics are steadily focused on the same research fields (or their nearest outskirsts). &#10;As a sidelight, in real practice the 7000 value of your in your CASE 2 example would sound more like a mistaken data entry than a genuine outlier.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-08T07:22:14.453" Id="96890" LastActivityDate="2014-05-08T07:22:14.453" OwnerUserId="25292" ParentId="9987" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;In general, both.  When you have an endogenous variable in a regression, all of the coefficient estimators are biased and inconsistent, not just the coefficient estimator for the coefficient on the endogenous variable.  The $(X'X)^{-1}$ matrix spreads the bias around to the other coefficients as well.  This also hints at the very special circumstances under which the bias is not spread around.  If, for example, the endogenous variable is uncorrelated with all the other $X$ variables in the model, then the bias is not spread from the coefficient estimator for the endogenous variable to the coefficient estimators for the other variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; To respond to comment&lt;/p&gt;&#10;&#10;&lt;p&gt;The questioner does not find the answer above convincing.  I'm not sure whether to respond by pushing algebra or by producing a baby monte carlo.  I chose the latter.  The monte carlo is written in &lt;code&gt;R&lt;/code&gt; and appears below.  It's a baby monte carlo because it only&#10;has one replication.  On the other hand, it has 10,000 observations, so the law of large numbers has certainly kicked in.  (Notice what the questioner calls X1 and X2, I call $X_2$ and $X_3$ in the monte carlo).  The equation and true values for the parameters are:&#10;\begin{align}&#10;Y &amp;amp;= \beta_1 + \beta_2X_2 + \beta_3X_3 + \epsilon\\&#10;\beta_1 &amp;amp;=0,\;\beta_2=1,\;\beta_3=1&#10;\end{align}&lt;/p&gt;&#10;&#10;&lt;p&gt;Two things are varied in the monte carlo, the correlation between $X_2$ and $X_3$ and the correlation between $X_3$ and $W_2$, the instrument for $X_2$.  The table below reports all the combinations of these correlations investigated in the monte carlo below.  The&#10;final three columns of the table report the coefficient estimates for $\beta_2$ and&#10;$\beta_3$ using OLS, using two-stage least squares but only instrumenting for $X_2$,&#10;and using TSLS but instrumenting for both $X_2$ and $X_3$.  Asterisked coefficients are &#10;coefficients which were reported as significantly different from their true values at&#10;5% significance.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, the first row reports on a treatment in which the correlation among the $X$s was 0.8 and the correlation between the instrument for $X_2$ (called $W_2$) and $X_3$ is 0.4.  In this case, OLS gave estimates for the betas which were biased upward, estimating them to be 1.22 instead of 1.  These differences were significant. The one-instrument TSLS results had $\beta_2$ estimated at 0.23 and $\beta_3$ estimated at 2.05, both significantly different from their true values of 1.  TSLS with both $X$s instrumented gave coefficients insignificantly different from their true values of 1.&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{array}{rrrrrr}&#10;\text{Tmt #} &amp;amp; cor(X_2,X_3) &amp;amp; cor(X_3,W_2) &amp;amp; \text{OLS} &amp;amp; \text{2SLS-1} &amp;amp; \text{2SLS-2}\\&#10;\hline\\&#10;1 &amp;amp; 0.8 &amp;amp; 0.4 &amp;amp; 1.22^*,1.22^* &amp;amp; 0.23^*,2.05^* &amp;amp; 0.97,1.01\\&#10;2 &amp;amp; 0.0 &amp;amp; 0.4 &amp;amp; 1.45^*,1.45^* &amp;amp; 0.52^*,1.44^* &amp;amp; 0.97,0.99\\&#10;3 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 1.45^*,1.45^* &amp;amp; 0.98,1.44^* &amp;amp; 0.97,0.99\\&#10;4 &amp;amp; 0.8 &amp;amp; 0.0 &amp;amp; 1.22^*,1.22^* &amp;amp; 0.97,1.43^*&amp;amp; 0.97,1.00\\&#10;\end{array}&lt;/p&gt;&#10;&#10;&lt;p&gt;What is shows is that instrumenting only for $X_2$ fails to correct the bias in the coefficient estimator for $X_2$'s coefficient.  This strategy of instrumenting only for $X_2$ only corrects the bias if, in addition, $W_2$ and $X_3$ are uncorrelated---generally a strange and unlikely thing to have happen.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# This program written in response to a Cross Validated question&#10;# http://stats.stackexchange.com/questions/96912/bias-of-more-than-one-endogenous-variables/96915#96915&#10;&#10;# The program is a toy monte carlo.&#10;# It generates two variables, X2 and X3, along with an error term ep.  The&#10;# three of these are mutually correlated, so that both X1 and X2 are going&#10;# to be endogenous in this equation:&#10;# &#10;# Y = b1 + b2*X2 + b3*X3 + ep&#10;# &#10;# Y is generated from that equation.  Then, an instrument for X2, called&#10;# W2, is generated.  The equation above is estimated by 2SLS.&#10;# &#10;# The purpose of the monte carlo is to demonstrate that it is not enough&#10;# to instrument for only one of the endogenous Xs in an equation, even if&#10;# you only care about the coefficient on that variable.  That is, if &#10;# we only instrument for X2 in the above equation, the IV estimator of &#10;# its coefficient is _still_ inconsistent.&#10;&#10;library(sem)&#10;&#10;set.seed(12344321)&#10;&#10;# Set the various basic parameters of the model.&#10;# I am leaving the means of the Xs and Y at zero.  It is easy,&#10;# though pointless, to change this.&#10;var_X2 &amp;lt;- 1&#10;var_X3 &amp;lt;- 1&#10;var_W2 &amp;lt;- 1&#10;var_W3 &amp;lt;- 1&#10;var_ep &amp;lt;- 1&#10;b1 &amp;lt;- 0&#10;b2 &amp;lt;- 1&#10;b3 &amp;lt;- 1&#10;&#10;# Create large sample of normal errors&#10;e1 &amp;lt;- rnorm(10000)&#10;e2 &amp;lt;- rnorm(10000)&#10;e3 &amp;lt;- rnorm(10000)&#10;e4 &amp;lt;- rnorm(10000)&#10;e5 &amp;lt;- rnorm(10000)&#10;&#10;# Verify that they are uncorrelated and N(0,1)&#10;cov(data.frame(e1,e2,e3,e4,e5))&#10;&#10;# To create X2, X3, ep, and W2 I will use the following system&#10;# of equations:&#10;# &#10;#  X2 = (e1 + g22*e2 + g2e*e3) * sqrt(var_X2)/sqrt(1+g22^2+g2e^2)&#10;#  X3 = (e2 + g32*e1 + g3e*e3) * sqrt(var_X3)/sqrt(1+g32^2+g3e^2)&#10;#  ep = sqrt(var_ep)*e3&#10;# &#10;# The &quot;good&quot; variation in X2 comes from e1 and e2.  The &quot;bad,&quot; ie &#10;# endogeneity-causing variation, in X2 comes from e3.  Same for&#10;# X3.  The correlation between X2 and X3 comes from g2e and g3e.&#10;# So, to make the endogeneity worse, increase the absolute value &#10;# of g2e and g3e.&#10;# &#10;# An instrument, W2, for X2 should have none of the bad e3 variation&#10;# in it and should have some of the good e1 and e2 variation in it.&#10;# Same for W3.&#10;# &#10;# W2 = (e4 + h21*e1 + h22*e2) * sqrt(var_W2)/sqrt(1+h21^2+h22^2)&#10;# W3 = (e5 + h31*e1 + h32*e2) * sqrt(var_W3)/sqrt(1+h31^2+h32^2)&#10;# &#10;# The covariance between W2 and X2 (and X3 for that matter) comes from &#10;# h21 and h22.  Thus these parameters controll how good an instrument&#10;# W2 is for X2.  As long as all these parameters are positive (as they&#10;# are below in my example), increasing h22, h23 make W2 a better intrument&#10;# for X2.&#10;&#10;# Setting params which controll endogenity and instrument quality:&#10;g22 &amp;lt;- 0.5&#10;g2e &amp;lt;- 0.5&#10;g32 &amp;lt;- 0.5&#10;g3e &amp;lt;- 0.5&#10;h21 &amp;lt;- 0.75&#10;h22 &amp;lt;- 0.25&#10;h31 &amp;lt;- 0.25&#10;h32 &amp;lt;- 0.75&#10;&#10;# Generating data:&#10;X2 &amp;lt;- (e1 + g22*e2 + g2e*e3) * sqrt(var_X2)/sqrt(1+g22^2+g2e^2)&#10;X3 &amp;lt;- (e2 + g32*e1 + g3e*e3) * sqrt(var_X3)/sqrt(1+g32^2+g3e^2)&#10;ep &amp;lt;- sqrt(var_ep)*e3&#10;W2 &amp;lt;- (e4 + h21*e1 + h22*e2) * sqrt(var_W2)/sqrt(1+h21^2+h22^2)&#10;W3 &amp;lt;- (e5 + h31*e1 + h32*e2) * sqrt(var_W3)/sqrt(1+h31^2+h32^2)&#10;Y  &amp;lt;- b1 + b2*X2 + b3*X3 + ep&#10;&#10;# OK, now the various correlations among these variables should be:&#10;&#10;# cor(X2,X3) = (g22 + g32 + g2e*g3e)/sqrt((1+g22^2+g2e^2)*(1+g32^2+g3e^2))&#10;(g22 + g32 + g2e*g3e)/sqrt((1+g22^2+g2e^2)*(1+g32^2+g3e^2))&#10;cov(X2,X3)&#10;&#10;# cor(X2,ep) = g2e*var_ep/sqrt((1+g22^2+g2e^2)*(var_ep))&#10;g2e*var_ep/sqrt((1+g22^2+g2e^2)*(var_ep))&#10;cov(X2,ep)&#10;&#10;# cor(X3,ep) = g3e*var_ep/sqrt((1+g32^2+g3e^2)*(var_ep))&#10;g3e*var_ep/sqrt((1+g32^2+g3e^2)*(var_ep))&#10;cov(X3,ep)&#10;&#10;# cor(X2,W2) = (h21+h22*g22) / sqrt((1+g22^2+g2e^2)*(1+h21^2+h22^2))&#10;(h21+h22*g22) / sqrt((1+g22^2+g2e^2)*(1+h21^2+h22^2))&#10;cov(X2,W2)&#10;&#10;# A look at the whole variance matrix &amp;amp; correlation matrix:&#10;var(data.frame(Y,X2,X3,W2,W3,ep))&#10;cor(data.frame(Y,X2,X3,W2,W3,ep))&#10;&#10;# OLS should be badly biased:&#10;summary(lm(Y~X2+X3))&#10;&#10;# TSLS using both instruments should be fine:&#10;summary(tsls(Y~X2+X3,~W2+W3))&#10;&#10;# TSLS using only W2 should not be fine:&#10;summary(tsls(Y~X2+X3,~W2+X3))&#10;&#10;&#10;&#10;# How about if we get rid of cross-correlation among the X:&#10;g22 &amp;lt;- -g2e*g3e/2&#10;g32 &amp;lt;- -g2e*g3e/2&#10;&#10;# Maintain the correlation between X3 and W2, though:&#10;(h22+h21*g32) / sqrt((1+g32^2+g3e^2)*(1+h21^2+h22^2))&#10;h22 &amp;lt;- 0.75&#10;(h22+h21*g32) / sqrt((1+g32^2+g3e^2)*(1+h21^2+h22^2))&#10;&#10;&#10;# Generating data:&#10;X2 &amp;lt;- (e1 + g22*e2 + g2e*e3) * sqrt(var_X2)/sqrt(1+g22^2+g2e^2)&#10;X3 &amp;lt;- (e2 + g32*e1 + g3e*e3) * sqrt(var_X3)/sqrt(1+g32^2+g3e^2)&#10;ep &amp;lt;- sqrt(var_ep)*e3&#10;W2 &amp;lt;- (e4 + h21*e1 + h22*e2) * sqrt(var_W2)/sqrt(1+h21^2+h22^2)&#10;W3 &amp;lt;- (e5 + h31*e1 + h32*e2) * sqrt(var_W3)/sqrt(1+h31^2+h32^2)&#10;Y  &amp;lt;- b1 + b2*X2 + b3*X3 + ep&#10;&#10;# A look at the whole variance matrix &amp;amp; correlation matrix:&#10;var(data.frame(Y,X2,X3,W2,W3,ep))&#10;cor(data.frame(Y,X2,X3,W2,W3,ep))&#10;&#10;&#10;# OLS should be badly biased:&#10;summary(lm(Y~X2+X3))&#10;&#10;# TSLS using both instruments should be fine:&#10;summary(tsls(Y~X2+X3,~W2+W3))&#10;&#10;# TSLS using only W2 should not be fine:&#10;summary(tsls(Y~X2+X3,~W2+X3))&#10;&#10;&#10;# How about if we get rid of correlation between W2 and X3 also.&#10;# cor(W2,X3) = (h22+h21*g32)/sqrt((1+g32^2+g3e^2)*(1+h21^2+h22^2))&#10;h22 &amp;lt;- -h21*g32&#10;&#10;# Generating data:&#10;X2 &amp;lt;- (e1 + g22*e2 + g2e*e3) * sqrt(var_X2)/sqrt(1+g22^2+g2e^2)&#10;X3 &amp;lt;- (e2 + g32*e1 + g3e*e3) * sqrt(var_X3)/sqrt(1+g32^2+g3e^2)&#10;ep &amp;lt;- sqrt(var_ep)*e3&#10;W2 &amp;lt;- (e4 + h21*e1 + h22*e2) * sqrt(var_W2)/sqrt(1+h21^2+h22^2)&#10;W3 &amp;lt;- (e5 + h31*e1 + h32*e2) * sqrt(var_W3)/sqrt(1+h31^2+h32^2)&#10;Y  &amp;lt;- b1 + b2*X2 + b3*X3 + ep&#10;&#10;# A look at the whole variance matrix &amp;amp; correlation matrix:&#10;var(data.frame(Y,X2,X3,W2,W3,ep))&#10;cor(data.frame(Y,X2,X3,W2,W3,ep))&#10;&#10;&#10;# OLS should be badly biased:&#10;summary(lm(Y~X2+X3))&#10;&#10;# TSLS using both instruments should be fine:&#10;summary(tsls(Y~X2+X3,~W2+W3))&#10;&#10;# TSLS using only W2 should not be fine:&#10;summary(tsls(Y~X2+X3,~W2+X3))&#10;&#10;&#10;# How about if we get rid of correlation between W2 and X3 but&#10;# go back to having X2 and X3 correlated.&#10;# cor(W2,X3) = (h22+h21*g32)/sqrt((1+g32^2+g3e^2)*(1+h21^2+h22^2))&#10;g22 &amp;lt;- 0.5&#10;g32 &amp;lt;- 0.5&#10;h22 &amp;lt;- -h21*g32&#10;&#10;# Maintain the correlation between X2 and W2, though:&#10;(h21+h22*g22) / sqrt((1+g22^2+g2e^2)*(1+h21^2+h22^2))&#10;h21 &amp;lt;- 0.75&#10;(h21+h22*g22) / sqrt((1+g22^2+g2e^2)*(1+h21^2+h22^2))&#10;&#10;&#10;# Check cor(W3,X3) reasonable&#10;# cor(W3,X3) = (h31*g32+h32)/sqrt((1+h31^2+h32^2)*(1+g32^2+g3e^2))&#10;(h31*g32+h32)/sqrt((1+h31^2+h32^2)*(1+g32^2+g3e^2))&#10;&#10;# Generating data:&#10;X2 &amp;lt;- (e1 + g22*e2 + g2e*e3) * sqrt(var_X2)/sqrt(1+g22^2+g2e^2)&#10;X3 &amp;lt;- (e2 + g32*e1 + g3e*e3) * sqrt(var_X3)/sqrt(1+g32^2+g3e^2)&#10;ep &amp;lt;- sqrt(var_ep)*e3&#10;W2 &amp;lt;- (e4 + h21*e1 + h22*e2) * sqrt(var_W2)/sqrt(1+h21^2+h22^2)&#10;W3 &amp;lt;- (e5 + h31*e1 + h32*e2) * sqrt(var_W3)/sqrt(1+h31^2+h32^2)&#10;Y  &amp;lt;- b1 + b2*X2 + b3*X3 + ep&#10;&#10;# A look at the whole variance matrix &amp;amp; correlation matrix:&#10;var(data.frame(Y,X2,X3,W2,W3,ep))&#10;cor(data.frame(Y,X2,X3,W2,W3,ep))&#10;&#10;&#10;# OLS should be badly biased:&#10;summary(lm(Y~X2+X3))&#10;&#10;# TSLS using both instruments should be fine:&#10;summary(tsls(Y~X2+X3,~W2+W3))&#10;&#10;# TSLS using only W2 should not be fine:&#10;summary(tsls(Y~X2+X3,~W2+X3))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="4" CreationDate="2014-05-08T12:26:01.183" Id="96915" LastActivityDate="2014-05-09T14:55:51.753" LastEditDate="2014-05-09T14:55:51.753" LastEditorUserId="25212" OwnerUserId="25212" ParentId="96912" PostTypeId="2" Score="0" />
  <row Body="&lt;pre&gt;&lt;code&gt;lm(y ~ color + x * color, data = X)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;is redundant because &quot;x * color&quot; is interpreted as &quot;x + color + x:color&quot; where &quot;x:color&quot; is the interaction between x and color.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lm(y ~ x * color, data = X)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;is enough. The model is:&#10;$$y = \beta_0+\beta_1 x+\beta_2 color + \beta_3 x\,color + \varepsilon$$&#10;Because color is a three-level factor, you can read it as:&#10;$$y=\beta_0+\beta_1 x+\beta_{2g}(color = green)+\beta_{2r}(color=red)+\beta_{3g}x(color=green)+\beta_{3r}x(color=red)+\varepsilon$$&#10;i.e.:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;when color is &quot;blue&quot;: $y=13.96+0.97x$;&lt;/li&gt;&#10;&lt;li&gt;when color is &quot;green&quot;: $y=13.96+0.97x+0.25+0.07x=(13.96+0.25)+(0.97+0.07)x$;&lt;/li&gt;&#10;&lt;li&gt;when color is &quot;red&quot;: $y=13.96+0.97x+6.11+0.02x=(13.96+6.11)+(0.97+0.02)x$.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2014-05-08T12:28:30.863" Id="96916" LastActivityDate="2014-05-08T12:28:30.863" OwnerUserId="44965" ParentId="96863" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="96926" AnswerCount="1" Body="&lt;p&gt;I am working on a problem which requires one-class classifier. I am using LIBSVM. I know there are tons of material out there but still I could not find the answer to my query.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;How do I estimate the optimum parameters for the RBF kernel?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Using &lt;code&gt;svm-train -s 2 -t 2 -v 5 train.scale&lt;/code&gt;, I am getting 49% accuracy and my training set has no outlier data. So, does this actually mean 98% accuracy in real world scenario?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="4" CreationDate="2014-05-08T13:16:01.817" Id="96922" LastActivityDate="2014-05-08T13:55:48.597" LastEditDate="2014-05-08T13:55:48.597" LastEditorUserId="25433" OwnerUserId="45206" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;svm&gt;&lt;libsvm&gt;" Title="One class classifier Cross validation" ViewCount="190" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm using SPSS and I'm having touble with deciding what statistical method should be used: there are 3 groups with different participants and treatments and they were measured in three different experimental conditions, but one of the variables is binary (another one is continuous, derived data). Should I use glmm or generalized estimating equations (or something entirely different)? Would it be better to analize the binary variable together with the other one or separately?&#10;Thanks in advance and sorry for my english!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-08T13:40:27.563" Id="96925" LastActivityDate="2014-05-08T13:40:27.563" OwnerUserId="45207" PostTypeId="1" Score="0" Tags="&lt;repeated-measures&gt;&lt;binary&gt;" Title="binary repeated measures with 3 groups and 3 condititions" ViewCount="8" />
  <row Body="&lt;p&gt;Since your data are counts, they can only come in whole numbers.  That is, you could get a count of $5$ orders or $6$ orders, but you could never get $5.5$ orders.  Thus, your data cannot be distributed as Gamma, normal or inverse Gaussian, as these are continuous distributions.  (It is always possible in some case that the fits will be good enough for your purposes, but your goal is to use the distribution to simulate new data, so we can just rule them out.)  &lt;/p&gt;&#10;&#10;&lt;p&gt;For your case, the default first choice would be the &lt;a href=&quot;http://en.wikipedia.org/wiki/Poisson_distribution&quot; rel=&quot;nofollow&quot;&gt;Poisson distribution&lt;/a&gt;.  However, the Poisson is very restrictive, in the sense that the variance must equal the mean.  This can happen in the real world, but rarely does in practice and I highly doubt it would hold for sales.  A distribution that relaxes this restriction / generalizes the Poisson is the &lt;a href=&quot;http://en.wikipedia.org/wiki/Negative_binomial_distribution&quot; rel=&quot;nofollow&quot;&gt;negative binomial&lt;/a&gt;.  This will allow you to estimate the mean and variance separately.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Thus, your first strategy for fitting these distributions and doing so quickly, is to only fit the negative binomial.  The primary function for getting these parameters is &lt;a href=&quot;http://stat.ethz.ch/R-manual/R-devel/library/MASS/html/fitdistr.html&quot; rel=&quot;nofollow&quot;&gt;?fitdistr&lt;/a&gt; in the MASS package.  Subsequently, you can use &lt;a href=&quot;http://stat.ethz.ch/R-manual/R-patched/library/stats/html/NegBinomial.html&quot; rel=&quot;nofollow&quot;&gt;?rnbinom&lt;/a&gt; for simulation.  &lt;/p&gt;&#10;&#10;&lt;p&gt;If speed and size remain a problem, another possibility is to extract random subsamples of manageable size from your data and fit those.  Even if these were a small proportion of the total, the fitted parameters should be very close to the true parameters if the sample is large enough.  &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-05-08T14:24:39.213" Id="96929" LastActivityDate="2014-05-08T14:24:39.213" OwnerUserId="7290" ParentId="96612" PostTypeId="2" Score="2" />
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;Am I correct in saying that by combining all such rules/paths of length 2 from T into a new directed graph G, the strongly connected components can be extracted?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I don't have a citation for you, but I think you've essentially just restated the Apriori algorithm in graph notation (I'm accustomed to a set notation description), so I believe your reasoning is correct.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-08T14:38:00.033" Id="96932" LastActivityDate="2014-05-08T14:38:00.033" OwnerUserId="8451" ParentId="96898" PostTypeId="2" Score="0" />
  
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Given a sample of inter arrival times for an entity at the same location, is there a distribution that can fit such a sample?. I was thinking about poisson, but was concerned about the independence, since the next arrival may be dependent on the previous arrival time. &lt;/p&gt;&#10;&#10;&lt;p&gt;Or is using a time series approach better to capture trend and seasonality?. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-05-08T18:25:54.227" Id="96964" LastActivityDate="2014-05-08T18:25:54.227" OwnerUserId="8849" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;distributions&gt;&lt;poisson&gt;" Title="Distribution for time between arrivals at the same location by the same entity" ViewCount="14" />
  
  
  
  
  <row Body="&lt;p&gt;You could try:  &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Cook, R. D. (1999). &lt;em&gt;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0471193658&quot; rel=&quot;nofollow&quot;&gt;Regression Graphics: Ideas for Studying Regressions through Graphics&lt;/em&gt;.&lt;/a&gt;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;It isn't SPSS-based as far as I know (and for the record I haven't read it), but the substance of the plots that come with regression output should be sufficiently similar from software to software that it shouldn't matter.  &lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;You can also try reading through some of the threads on CV that are categorized under the tags &lt;a href=&quot;/questions/tagged/regression&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;regression&amp;#39;&quot; rel=&quot;tag&quot;&gt;regression&lt;/a&gt; and &lt;a href=&quot;/questions/tagged/data-visualization&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;data-visualization&amp;#39;&quot; rel=&quot;tag&quot;&gt;data-visualization&lt;/a&gt; (e.g., &lt;a href=&quot;http://stats.stackexchange.com/questions/tagged/regression+data-visualization?sort=votes&amp;amp;pageSize=30&quot;&gt;this search&lt;/a&gt;), or even under &lt;a href=&quot;/questions/tagged/spss&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;spss&amp;#39;&quot; rel=&quot;tag&quot;&gt;spss&lt;/a&gt;, &lt;a href=&quot;/questions/tagged/residuals&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;residuals&amp;#39;&quot; rel=&quot;tag&quot;&gt;residuals&lt;/a&gt;, or &lt;a href=&quot;/questions/tagged/interpretation&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;interpretation&amp;#39;&quot; rel=&quot;tag&quot;&gt;interpretation&lt;/a&gt;.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, you should feel free to come back here with specific questions you have.  Just post a plot you are having trouble with and ask how to interpret it.  If CV won't let you post an image because your reputation is too low, you can upload it to &lt;a href=&quot;https://imgur.com/&quot; rel=&quot;nofollow&quot;&gt;imgur.com&lt;/a&gt; and include a link to it in your question.  Someone will add the figure for you.  If the plot is for a homework problem, you should add the &lt;code&gt;[self-study]&lt;/code&gt; tag and explain what you have tried already, but you don't need to add the &lt;code&gt;[self-study]&lt;/code&gt; tag otherwise.  &lt;/p&gt;&#10;" CommentCount="3" CommunityOwnedDate="2014-05-09T01:26:00.270" CreationDate="2014-05-09T01:26:00.270" Id="96997" LastActivityDate="2014-05-09T01:32:49.360" LastEditDate="2014-05-09T01:32:49.360" LastEditorUserId="7290" OwnerUserId="7290" ParentId="96962" PostTypeId="2" Score="2" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am using multi-output support vector regression (MSVR) which predicts multiple outputs at a time. I want to know how can I initially select parameters for my model, and then how to do cross validation.&lt;/p&gt;&#10;&#10;&lt;p&gt;The input &lt;code&gt;x&lt;/code&gt; is 100&amp;times;78 (100 samples, 78 features) and the ouput &lt;code&gt;y&lt;/code&gt; is 78-dimensional.&lt;br&gt;&#10;Please help me with MATLAB code.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-09T04:54:50.637" Id="97004" LastActivityDate="2014-05-09T05:49:48.090" LastEditDate="2014-05-09T05:49:48.090" LastEditorUserId="27403" OwnerUserId="45249" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;multiple-regression&gt;&lt;mathematical-statistics&gt;&lt;matlab&gt;&lt;cross-validation&gt;" Title="Cross validation MATLAB code for multi-output regression" ViewCount="80" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Let $X_1,\dots,X_n$ are i.i.d with distribution function $F$. Let $\hat F_n$ be its empirical distribution function, i.e.,&#10;$$&#10;\hat F_n(x)=\frac1n\sum_{i=1}^n1_{\{X_\le x\}}(x)&#10;$$&#10;where $1_A(x)$ is the characteristic function.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am going to know the order of &#10;$$&#10;T_n=\sum_{i=1}^n\left(\{\Phi^{-1}(\hat F_n(X_i))\}^2-\{\Phi^{-1}(F(X_i))\}^2\right)&#10;$$&#10;where $\Phi$ is the comulative normal standard distribution function.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have feeling that $T_n=o_p(\sqrt n)$ since for all $x\in\mathbb{R}$ I have $\{\Phi^{-1}(\hat F_n(x))\}^2-\{\Phi^{-1}(F(x))\}^2=O_p(1/\sqrt n)$. However, I cannot extend to determine the order of $T_n$ since $\hat F_n$ also depends on $X_i$. Could anyone help me?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-05-09T07:35:17.070" Id="97016" LastActivityDate="2014-05-09T18:09:14.587" LastEditDate="2014-05-09T18:09:14.587" LastEditorUserId="40104" OwnerUserId="40104" PostTypeId="1" Score="0" Tags="&lt;distributions&gt;&lt;normal-distribution&gt;&lt;estimation&gt;&lt;empirical&gt;" Title="Order of difference between Distribution function and Its empirical" ViewCount="24" />
  <row AnswerCount="3" Body="&lt;p&gt;We have three years of data for online visits at a daily level. We want to forecast the daily visits for the next 90 days. What would be the best method to capture weekday seasonality , holiday seasons, and also the drift.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can this be successfully done in R? We are currently using R. We have considered ARIMA but it does not capture seasonality.&lt;/p&gt;&#10;&#10;&lt;p&gt;While converting the data to a time series in R,  what should be the &quot;frequency&quot;?&lt;/p&gt;&#10;&#10;&lt;p&gt;Should we use ARIMA with regressors?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-05-09T08:38:32.333" FavoriteCount="1" Id="97019" LastActivityDate="2015-02-07T14:23:01.663" OwnerUserId="45253" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;forecasting&gt;&lt;arima&gt;" Title="Daily forecasting" ViewCount="279" />
  <row AnswerCount="0" Body="&lt;p&gt;Suppose we have outcome variable Y and two predictors X1 and X2, where X1 and X2 sort of come from a type of category. For example, X1 is the relationship score with friends and X2 relationship score with family. Not the same, but they reflect some thing together (social relationship, in this case).&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to set up a model to test the hypothesis that the diversity/balance of predictors can influence the outcome. Some thing like this: Suppose there are two persons who both have scores of 10 on X1 and X2 total. I suspect that the one with score of 5 on X1 and 5 on X2 will have better outcome than the one with 10 on X1 and 0 on X2. &lt;/p&gt;&#10;&#10;&lt;p&gt;I think a normal model (Y = X1 + X2) or an interaction term (Y = X1 + X2 + X1*X2) seem not to do the job. Maybe there needs to create new variable(s) but I'm not sure which to create.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-05-09T10:13:00.943" Id="97030" LastActivityDate="2014-05-09T10:13:00.943" OwnerUserId="21006" PostTypeId="1" Score="0" Tags="&lt;model&gt;" Title="How to model a multiplicative effect from this hypothesis?" ViewCount="30" />
  
  
  
  
  <row Body="&lt;p&gt;You are right to be concerned about data splitting.  I have found that data splitting is volatile unless $n &amp;gt; 17000$.  The easiest way to ascertain the likely amount of overfitting is to compute the heuristic shrinkage estimator $\hat{\gamma}$ (vanHouwelingen and le Cessie Stat in Med 9:1303; 1990).  Specify $p$, the effective number of parameters &lt;em&gt;examined&lt;/em&gt; against $Y$.  $p$ equals the number of all terms &lt;em&gt;examined&lt;/em&gt;, not just kept in the final model.  This includes main effects, nonlinear terms, interactions, and variables tested then dropped.  Then get the likelihood ratio $\chi^2$ statistic (LR) from your model.  $\hat{\gamma} = \frac{LR - p}{LR}$.  This estimates the slope of the calibration curve were you to apply the model to new data.  $\hat{\gamma} = .9$ would, loosely speaking, indicate 10% overfitting, or 10% of what you learned from the modeling process was just noise. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-09T14:24:10.653" Id="97063" LastActivityDate="2014-05-09T14:24:10.653" OwnerUserId="4253" ParentId="97048" PostTypeId="2" Score="2" />
  
  <row AnswerCount="0" Body="&lt;p&gt;We have three years of data for online visits at a daily level. We want to forecast the daily visits for the next 90 days. What would be the best method to capture weekday seasonality , holiday seasons, and also the drift.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can this be successfully done in R? We are currently using R. We have considered ARIMA but it does not capture seasonality.&lt;/p&gt;&#10;&#10;&lt;p&gt;While converting the data to a time series in R, what should be the &quot;frequency&quot;?&lt;/p&gt;&#10;&#10;&lt;p&gt;Should we use ARIMA with regressors?&lt;/p&gt;&#10;" ClosedDate="2014-05-10T06:09:34.773" CommentCount="0" CreationDate="2014-05-09T08:44:24.653" FavoriteCount="0" Id="97075" LastActivityDate="2014-05-09T15:05:07.203" OwnerDisplayName="user3619640" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;forecasting&gt;" Title="Forecasting daily online visits in r" ViewCount="19" />
  <row AnswerCount="0" Body="&lt;p&gt;I was reading Andrew Ng's &lt;a href=&quot;http://cs229.stanford.edu/notes/cs229-notes12.pdf&quot; rel=&quot;nofollow&quot;&gt;lecture notes&lt;/a&gt; on Reinforcement learning and was trying to understand why policy iteration converged to the optimal value function $V^*$ and optimum policy $\pi^*$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Recall policy iteration is:&lt;/p&gt;&#10;&#10;&lt;p&gt;$&#10;\text{Initialize $\pi$ randomly} \\ &#10;\text{Repeat},\{\\  &#10;\quad Step1: Let \ V := V^{\pi} \text{ \\for the current policy, solve bellman's eqn's and set that to the current V}\\ &#10;\quad Step2: Let \ \pi(s) := argmax_{a \in A} \sum_{s'}P_{sa}(s') V(s')\\&#10;\}&#10;$&lt;/p&gt;&#10;&#10;&lt;p&gt;Why is it that a greedy-algorithm leads to a the optimal policy and the optimal value function? (I know greedy algorithms don't always guarantee that, or might get stuck in local optima's, so I just wanted to see a proof for its optimality of the algorithm). Also, it seems to me that policy iteration is something analogous to clustering or gradient descent. To clustering, because with the current setting of the parameters, we optimize. Similar to gradient descent because it just chooses some value that seems to increase some function. These two methods don't always converge to optimal maximums and I was trying to understand how this algorithm was different from the previous ones I mentioned.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;These are my thoughts so far:&lt;/p&gt;&#10;&#10;&lt;p&gt;Say that we start with some policy $\pi_1$, then after the first step, for that fixed policy we have that:&lt;/p&gt;&#10;&#10;&lt;p&gt;$ V^{\pi_1}(s) = R(s) + \gamma \sum_{s'}P_{s\pi_1(s)}(s')V^{\pi_1}(s')$&lt;/p&gt;&#10;&#10;&lt;p&gt;$V^{(1)} := V^{\pi_1}(s)$&lt;/p&gt;&#10;&#10;&lt;p&gt;Where V^{(1)} is the value function for the first iteration. Then after the second step we choose some new policy $\pi_2$ to increase the value of $V^{\pi_1}(s)$. Now, with the new policy $\pi_2$, if we do do the second step of the algorithm the following inequality holds true:&lt;/p&gt;&#10;&#10;&lt;p&gt;$R(s) + \gamma \sum_{s'}P_{s\pi_1(s)}(s')V^{\pi_1}(s') \leq R(s) + \gamma \sum_{s'}P_{s\pi_2(s)}(s')V^{\pi_1}(s')$&lt;/p&gt;&#10;&#10;&lt;p&gt;Because we choose $\pi_2$ in the second step to increase the value function in the previous step (i.e. to improve $V^{(1)}$. So far, its clear that choosing $\pi_2$ can only increase V^{(1)}, because thats how we choose $\pi_2$. However, my confusion comes in the repeat step because once we repeat and go back to step 1, we actually change things completely because we re-calculate $V^{2}$ for the new policy $\pi_2$. Which gives:&lt;/p&gt;&#10;&#10;&lt;p&gt;$V^{\pi_2}(s) = R(s) + \gamma \sum_{s'}P_{s\pi_2(s)}(s')V^{\pi_2}(s')$&lt;/p&gt;&#10;&#10;&lt;p&gt;but its is NOT:&lt;/p&gt;&#10;&#10;&lt;p&gt;$V^{\pi_1}(s) = R(s) + \gamma \sum_{s'}P_{s\pi_2(s)}(s')V^{\pi_1}(s')$&lt;/p&gt;&#10;&#10;&lt;p&gt;Which seems to be a problem because $\pi_2$ was chosen to improve $V^{(1)}$, and not this new $V^{\pi_2}$. Basically the problem is that $pi_2$ guarantees to improve $R(s) + \gamma \sum_{s'}P_{s\pi_1(s)}(s')V^{\pi_1}(s')$ by doing $\pi_2$ instead of $pi_1$ when the value function is $V^{\pi_1}$. But in the repeat step we change $V^{\pi_1}$ to $V^{\pi_2}$, but I don't see how that guarantees that the value function improves monotonically at each repeat because $\pi_2$ was calculated to improve the value function when the value functions stay at $V^{\pi_1}$, but step 1 changes $V^{\pi_1}$ to $V^{\pi_2}$ (which is bad because I $\pi_2$ only improved the previous value function we had).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-09T16:51:58.903" FavoriteCount="1" Id="97083" LastActivityDate="2014-05-09T20:52:24.247" LastEditDate="2014-05-09T20:52:24.247" LastEditorUserId="37632" OwnerUserId="37632" PostTypeId="1" Score="1" Tags="&lt;reinforcement-learning&gt;" Title="Why does policy iteration algorithm converge to optimal value? (reinforcement learning)" ViewCount="99" />
  
  <row Body="&lt;p&gt;One (standard) interpretation of this question is that you seek a randomized sampling procedure that has at least a 95% chance of including at least one bad apple, should there happen to be a bad apple in the population.  Equivalently, the chance of &lt;em&gt;not&lt;/em&gt; including it cannot exceed 5%.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the worst case, there is only one bad apple in $n=10000$ apples.  The chance this will not be included in a random sample of size $k$ (without replacement) is the number of samples of the $n-1$ good apples divided by the total number of samples of all $n$ apples.  This implies&lt;/p&gt;&#10;&#10;&lt;p&gt;$$0.05 \ge \frac{\binom{n-1}{k}}{\binom{n}{k}} = 1 - \frac{k}{n}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The solution is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$k \ge (1 - 0.05)n,$$&lt;/p&gt;&#10;&#10;&lt;p&gt;requiring a minimum sample of $9500$ in this case.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-05-09T17:52:54.827" Id="97089" LastActivityDate="2014-05-09T17:52:54.827" OwnerUserId="919" ParentId="97086" PostTypeId="2" Score="7" />
  <row AnswerCount="1" Body="&lt;p&gt;I'm trying to figure out what next steps to take.  I created a model and ran OLS on a very large sample of data (over 400000 observations) and got an R-squared value of 0.80.  So the model fit seems really good.  However, I ran a Ramsey RESET test and its test statistic strongly suggested that there were omitted variables.  I'm not sure what do do next.  Do I just throw away the model, saying that the estimates are biased.  Do I keep adding terms until the RESET test no longer suggests omitted variables?  &lt;/p&gt;&#10;&#10;&lt;p&gt;The model is intended to be predictive within and out of the sample and given the model fit to the observed data, is it still truly a good fit, despite there being omitted variables?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-05-09T19:09:11.633" Id="97101" LastActivityDate="2014-11-02T05:16:25.847" LastEditDate="2014-05-09T19:20:24.773" LastEditorUserId="919" OwnerUserId="37594" PostTypeId="1" Score="1" Tags="&lt;model-selection&gt;&lt;bias&gt;" Title="Model fit is High but Ramsey RESET Test suggests omitted variables. What to do?" ViewCount="337" />
  <row Body="&lt;p&gt;What does the $R^2$ do, what your coefficients? &lt;/p&gt;&#10;&#10;&lt;p&gt;As Meehl said, everything is correlated with (almost) everything else - to &lt;em&gt;some&lt;/em&gt; degree. If your variables are of this kind, if there is any, and be it ever so meagre, effect in the population, your &lt;em&gt;p&lt;/em&gt; will decrease as your sample size increases. So maybe your variables are of this normal kind, where some degree of correlation exists. Maybe your variable is statistically significant. However, the more interesting question is probably: are they correlated &lt;em&gt;to an interesting degree&lt;/em&gt;?&#10;And for that, you want to look at your coefficients and $R^2$s, and preferably some estimate of the reliability of these estimates, like their Confidence Intervals.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, you're currently comparing your favourite model to a very boring model; the model that there is &lt;em&gt;nothing&lt;/em&gt; going on. And if you believe Meehl, that hypothesis has a very low &lt;em&gt;a priori&lt;/em&gt; probability, and it isn't so interesting if your model beats it. Instead, you could construct a &lt;em&gt;non-trivial&lt;/em&gt; hypothesis and check if that one is better than your favourite model.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-10T01:02:07.987" Id="97131" LastActivityDate="2014-05-10T01:02:07.987" OwnerUserId="28288" ParentId="97127" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;It's tough without truly understanding what your outcome &lt;code&gt;y&lt;/code&gt; is, but I'll give it my best shot.&lt;/p&gt;&#10;&#10;&lt;p&gt;(1) This sounds like it might be approximated by a &amp;lambda;=1 Poisson distribution. Unless you're seeing it messing up the variance components of the model, I wouldn't worry about it. If it is, I would try transforming it by &lt;code&gt;sqrt(cons_x)&lt;/code&gt; &lt;a href=&quot;http://en.wikipedia.org/wiki/Variance-stabilizing_transformation&quot; rel=&quot;nofollow&quot;&gt;link&lt;/a&gt; and seeing if that works better. Don't forget to check your model for sensibility by plugging in some examples in your final model, varying the # of consultations, and seeing if it makes sense.&lt;/p&gt;&#10;&#10;&lt;p&gt;(2) Sounds like interaction variables need to be added to see the actual effect of consultations on the sex and age.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;y ~ gp_sex + pat_sex + pat_age + cons_x + cons_x*pat_sex + cons_x*pat_age + cons_x*pat_age*pat_sex + ....&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="4" CreationDate="2014-05-10T03:27:06.890" Id="97139" LastActivityDate="2014-05-10T03:27:06.890" OwnerUserId="66775" ParentId="93818" PostTypeId="2" Score="1" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I am trying to provide help to an NGO in the non-profit sector that is running a disease screening program:&lt;/p&gt;&#10;&#10;&lt;p&gt;The program visits thousands of villages a year.  A village has a population (on average around 800), with an unknown number of infected individuals. Each village is visited by the NGO for mass screening. &lt;/p&gt;&#10;&#10;&lt;p&gt;For a given population in a given year: &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;An infected person will attend the NGO screening and get cured with a&#10;probability of S (e.g. 50%)&lt;/li&gt;&#10;&lt;li&gt;An infected person might independently self-diagnose himself, seek&#10;treatment and get cured in a health clinic outside the screening&#10;program with probability P (e.g. 10%) &lt;/li&gt;&#10;&lt;li&gt;An infected person will infect another person per year with probability R (e.g. 50%)  &lt;/li&gt;&#10;&lt;li&gt;An infected person can live for 4 years. &lt;/li&gt;&#10;&lt;li&gt;There might a fixed probability that a new infection will jump over from another village (J)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;These probabilities (S, P, R) will differ per village (within a estimated prior distribution), but are constant per village over time. The NGO has data going back 7 years for each village.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/22AJD.png&quot; alt=&quot;Example data for 2 villages&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The NGO would like to predict the number of infected people in the population for each village in year 8.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am trying to help the NGO, but have struggled to apply statistical learning to this problem, due to the stochastic nature and path-dependency. Most of my approaches try to fit a max. likelihood estimate to a forward projection of a starting population variable in year 0 with sampling of parameters from the prior distributions. I understand we are limited by the many hidden parameters and very limited data per village, but I feel there must be at least some predictive value in the data, but struggle to find the right approach. &lt;/p&gt;&#10;&#10;&lt;p&gt;Hence  I was wondering whether someone can give their perspectives on what is the best approach / are the best tools to perform predictive modelling on such a problem, and help me understand which might be possible in terms of predictive accuracy?&lt;br&gt;&#10;Any tips, links or advice much appreciated;&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-05-10T11:16:32.117" Id="97166" LastActivityDate="2014-05-16T01:13:07.730" LastEditDate="2014-05-10T12:56:29.370" LastEditorUserId="36680" OwnerUserId="36680" PostTypeId="1" Score="3" Tags="&lt;regression&gt;&lt;predictive-models&gt;&lt;model-selection&gt;&lt;epidemiology&gt;" Title="Statistical method for modelling of disease incidence for NGO" ViewCount="236" />
  
  <row Body="&lt;p&gt;&lt;a href=&quot;http://office.microsoft.com/en-gb/excel-help/chisq-test-function-HP010335674.aspx&quot;&gt;Microsoft says&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;code&gt;CHISQ.TEST&lt;/code&gt; returns the probability that a value of the $\chi^2$ statistic at least as high as the value calculated by the above formula could have happened by chance under the assumption of independence.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;This means that &lt;code&gt;CHISQ.TEST&lt;/code&gt; is measuring the upper tail, and so a value near zero from &lt;code&gt;CHISQ.TEST&lt;/code&gt; means that the $\chi^2$ statistic is relatively large, with the observed values not close to the expected values.  The older &lt;code&gt;CHITEST&lt;/code&gt; was similar.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-10T15:58:07.800" Id="97184" LastActivityDate="2014-05-10T15:58:07.800" OwnerUserId="2958" ParentId="97181" PostTypeId="2" Score="5" />
  
  <row AcceptedAnswerId="97209" AnswerCount="1" Body="&lt;p&gt;My question is quite elementary but I need help understanding the basic concepts.&#10;In the following example from the Mathworks &lt;a href=&quot;http://www.mathworks.com/help/stats/princomp.html;jsessionid=7d1614ae9e8aa84acbe93adc16f6&quot; rel=&quot;nofollow&quot;&gt;documentation page&lt;/a&gt;&#10;of the &lt;code&gt;princomp&lt;/code&gt; function&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;load hald;&#10;[pc,score,latent,tsquare] = princomp(ingredients);&#10;pc,latent&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;we get the following values for:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;pc =&#10;&#10;   -0.0678   -0.6460    0.5673    0.5062&#10;   -0.6785   -0.0200   -0.5440    0.4933&#10;    0.0290    0.7553    0.4036    0.5156&#10;    0.7309   -0.1085   -0.4684    0.4844&#10;&#10;latent =&#10;&#10;  517.7969&#10;   67.4964&#10;   12.4054&#10;    0.2372&#10;&#10;score =&#10;&#10;   36.8218   -6.8709   -4.5909    0.3967&#10;   29.6073    4.6109   -2.2476   -0.3958&#10;  -12.9818   -4.2049    0.9022   -1.1261&#10;   23.7147   -6.6341    1.8547   -0.3786&#10;   -0.5532   -4.4617   -6.0874    0.1424&#10;  -10.8125   -3.6466    0.9130   -0.1350&#10;  -32.5882    8.9798   -1.6063    0.0818&#10;   22.6064   10.7259    3.2365    0.3243&#10;   -9.2626    8.9854   -0.0169   -0.5437&#10;   -3.2840  -14.1573    7.0465    0.3405&#10;    9.2200   12.3861    3.4283    0.4352&#10;  -25.5849   -2.7817   -0.3867    0.4468&#10;  -26.9032   -2.9310   -2.4455    0.4116&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Legend:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;latent&lt;/strong&gt; is a vector containing the eigenvalues of the covariance matrix of X.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;pc&lt;/strong&gt; is a p-by-p matrix, each column containing coefficients for one principal component. The columns are in order of decreasing component variance.**&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;score&lt;/strong&gt; is the principal component scores; that is, the representation of X in the principal component space. Rows of SCORE correspond to observations, columns to components.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Can somebody explain whether the values of score are genetrated somehow using the values of pc and if this true, what kind of computation is perfomed ?&lt;/strong&gt;&lt;/p&gt;&#10;" ClosedDate="2014-12-22T16:44:20.197" CommentCount="0" CreationDate="2014-05-10T20:46:24.780" FavoriteCount="1" Id="97203" LastActivityDate="2014-12-22T15:57:52.097" LastEditDate="2014-12-22T15:57:52.097" LastEditorUserId="28666" OwnerUserId="45073" PostTypeId="1" Score="0" Tags="&lt;pca&gt;&lt;matlab&gt;" Title="Question about PCA: how are PC scores generated using PC eigenvectors?" ViewCount="73" />
  
  <row Body="&lt;p&gt;Suppose you are interested in the median $m$.  For the ordered  values, $x_1, x_2, \dots x_n$, compute the estimated &lt;strong&gt;weighted&lt;/strong&gt; proportions $\hat{F}(x_1), \hat{F}(x_2) \dots$, where $\hat{F}(x_i)$ is the estimated proportion of observations $ X $ that are $ \le x_i$.&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{equation}&#10;\hat{F}(x_i)= \frac{\sum_{j=1}^n w_j I(x_j\le x_i)}{\sum_{j=1}^n w_j}&#10;\end{equation}&lt;/p&gt;&#10;&#10;&lt;p&gt;Here $w_j$ is the sampling weight for observation $j$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Imagine that 37 and 41 are successive values of x in the sample such that  $\hat{F}(37) = 0.46$ and that $\hat{F}(41) = 0.53$. Since $F(m)=0.50$, it's obvious that the  estimated median $\hat{m}$  must be between 37 and 41. The value of $\hat{m}$ can be calculated by linear interpolation.&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;\begin{aligned}&#10;\hat{m} &amp;amp;=  37 + \frac{0.50 -0.46}{0.53 -0.46}\times(41-37) \\&#10;&amp;amp; = 37 + \frac{.04}{.07}\times 4 = 39.28&#10;\end{aligned}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that this result depends only on the sampling weights, not on any other aspects of the survey design. The first reference that I know of is Woodruff (1952). Other quantiles are estimated in a similar way.&lt;/p&gt;&#10;&#10;&lt;p&gt;What has been the subject of research is the estimation of confidence intervals for quantiles. Woodruff (1952) contained a method that is based on the asymptotic normality of the sample weighted proportions $\hat{F}$. The method is still widely used. See the quick reference list below.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Some survey packages that estimate quantiles with standard errors/confidence intervals&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;strong&gt;R&lt;/strong&gt;: Tom Lumley's &lt;strong&gt;Survey&lt;/strong&gt; package includes the &lt;strong&gt;svyquantile&lt;/strong&gt;    function.&lt;/li&gt;&#10;&lt;li&gt;&lt;strong&gt;Stata&lt;/strong&gt;: Stas Kolenikov's &lt;strong&gt;epctile&lt;/strong&gt; command (&lt;strong&gt;findit epctile&lt;/strong&gt; in Stata)&lt;/li&gt;&#10;&lt;li&gt;&lt;strong&gt;SUDAAN&lt;/strong&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;strong&gt;SAS SVYMEANS&lt;/strong&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;strong&gt;WesVar&lt;/strong&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Binder DA (1991) Use of estimating functions for interval estimation from complex surveys. Proceedings of the ASA Survey Research Methods Section 1991: 34-42&lt;/p&gt;&#10;&#10;&lt;p&gt;Dorfman A, Valliant R (1993) Quantile variance estimators in complex surveys. Proceedings of the ASA Survey Research Methods Section. 1993: 866-871&lt;/p&gt;&#10;&#10;&lt;p&gt;Francisco, C. A. and Fuller, W. A. 1991. Quantile Estimation With a Complex Survey Design,. The Annals of Statistics, 19: 454469.&lt;/p&gt;&#10;&#10;&lt;p&gt;Shah BV, Vaish AK (2006) Confidence Intervals for Quantile Estimation from Complex Survey Data. Proceedings of the Section on Survey Research Methods.&#10;&lt;a href=&quot;http://www.amstat.org/sections/SRMS/Proceedings/y2012/files/304420_73075.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.amstat.org/sections/SRMS/Proceedings/y2012/files/304420_73075.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Sitter, Randy R, and Changbao Wu. 2001. A note on Woodruff confidence intervals for quantiles. Statistics &amp;amp; probability letters 52, no. 4: 353-358.&lt;/p&gt;&#10;&#10;&lt;p&gt;Woodruff, Ralph S. 1952. Confidence intervals for medians and other position measures. Journal of the American Statistical Association 47, no. 260: 635-646.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-05-11T00:12:58.003" Id="97212" LastActivityDate="2014-05-15T03:40:37.320" LastEditDate="2014-05-15T03:40:37.320" LastEditorUserId="36068" OwnerUserId="36068" ParentId="96439" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;10 random numbers from a &lt;em&gt;standard&lt;/em&gt;  normal will have mean 0 and thus the sum will be about 0, not 5. If you ignore the &quot;standard&quot; part, you will still not be able to pick 10 random numbers from a normal distribution whose sum is &lt;em&gt;exactly&lt;/em&gt; 5. You can get 10 such numbers with sum very close to 5 by e.g. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(123)&#10;vars &amp;lt;- rnorm(10, 0.5, 0.001)&#10;sum(vars)&#10;vars&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and if you make the sd (0.001) even smaller then the sum will tend to be even closer to 5. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-05-11T01:03:05.207" Id="97215" LastActivityDate="2014-05-11T11:10:38.067" LastEditDate="2014-05-11T11:10:38.067" LastEditorUserId="686" OwnerUserId="686" ParentId="97213" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="97226" AnswerCount="1" Body="&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/kCFOf.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;My time series professor passed a comment today, that running this model implies losing k+1 observations. I think I have understood how the observations are wasted, but I am notorious for falsely believing that I have understood something(self-diagnosed). A simple explanation would be appreciated!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-11T04:23:34.897" Id="97225" LastActivityDate="2014-05-11T14:42:02.197" LastEditDate="2014-05-11T04:33:31.583" LastEditorUserId="31152" OwnerUserId="31152" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;arima&gt;&lt;augmented-dickey-fuller&gt;" Title="How does including lags imply losing observations" ViewCount="47" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am in great need of help in order to infer the statistical hypothesis tests performed in &lt;em&gt;Xanthelasmata, arcus corneae, and ischaemic vascular disease and death in general population: prospective cohort study&lt;/em&gt;, BMJ. 2011 (see &lt;a href=&quot;http://www.bmj.com/content/343/bmj.d5497&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem with the paper is that the part of &lt;strong&gt;Statistical analysis&lt;/strong&gt; talks about some tests and hypothesis that researchers use but n the other hand they didn't analyze those at all. I am trying to understand why they use those methods and how the procedure goes.&lt;/p&gt;&#10;&#10;&lt;p&gt;Could you please provide me with some hints?&#10;In your opinion how do you think that the above research was organized?&lt;/p&gt;&#10;&#10;&lt;p&gt;For example: What methods of hypothesis testing are applied by the researchers? &#10;&lt;em&gt;(Justify your answer)&lt;/em&gt; &#10;What type of data should be collected and used in each study above? &#10;What do you think is the result obtained from the application of each of these methods?&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2014-05-11T10:49:00.450" FavoriteCount="2" Id="97252" LastActivityDate="2014-05-11T13:24:08.527" LastEditDate="2014-05-11T13:24:08.527" LastEditorUserId="17230" OwnerUserId="45375" PostTypeId="1" Score="0" Tags="&lt;self-study&gt;&lt;biostatistics&gt;&lt;cox-model&gt;" Title="Inferring the statistical tests performed in a paper" ViewCount="96" />
  <row Body="&lt;p&gt;Firstly if you  are measuring your data in percentages  your outcome variable must be measured in 0s and 1s. (perhaps did pick up an M&amp;amp;M vs didn't???) In this case your data is binary and based on the binomial distribution.  Strictly speaking should be using logistic regression, or a chi squared test not ANOVA.   there maybe cases where using ANOVA is permissible with binary data (e.g where N is high enough) but this is a separate question....  My advice is to consider logistic regression analysis instead or chi squared test.  You can measure the same things as ANOVA using these tests - but they are specifically designed to deal with data measured in 0s and 1s,  rather than data collected with  continous measures.  if you need further help with these tests then you can post questions on this site.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-05-11T10:57:41.283" Id="97253" LastActivityDate="2014-05-11T10:57:41.283" OwnerUserId="45114" ParentId="97234" PostTypeId="2" Score="0" />
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I need to identify the formulas for computing the group differences with Bonferroni corrections and Cohen's d effect size in the following paper (chart on the top of page 6):&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.ncneuropsych.com/research/add-normalize.pdf&quot; rel=&quot;nofollow&quot;&gt;Research Paper&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;What formula was used to do the group comparisons? Also, what were the degrees of freedom?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-11T16:57:02.440" Id="97278" LastActivityDate="2014-05-11T16:57:02.440" OwnerUserId="37743" PostTypeId="1" Score="1" Tags="&lt;anova&gt;&lt;multiple-comparisons&gt;&lt;manova&gt;&lt;bonferroni&gt;&lt;cohens-d&gt;" Title="Identifying formulas from MANOVA in a research paper" ViewCount="32" />
  
  
  <row Body="&lt;p&gt;The residual &lt;em&gt;versus&lt;/em&gt; fitted values plot is useful to illustrate if a model presents &lt;a href=&quot;http://en.wikipedia.org/wiki/Heteroscedasticity&quot; rel=&quot;nofollow&quot;&gt;heteroskedasticity&lt;/a&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;According to the Wikipedia article:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;em&gt;...the presence of heteroscedasticity can invalidate statistical tests of significance that assume that the modelling errors are uncorrelated and normally distributed and that their variances do not vary with the effects being modelled.&lt;/em&gt;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;If one had observed heteroskedasticity, the parameters' standard errors (calculated through t tests) would not make much sense.&lt;/p&gt;&#10;&#10;&lt;p&gt;Your plot seems to be OK, though.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-05-11T21:54:03.990" Id="97307" LastActivityDate="2014-05-11T21:56:04.817" LastEditDate="2014-05-11T21:56:04.817" LastEditorUserId="7290" OwnerUserId="22468" ParentId="97281" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;Either choose some random categories and use the category which gives the best split, or choose some random combinations of categories and use the combination which gives the best split.&lt;/p&gt;&#10;&#10;&lt;p&gt;I think it doesn't really matter which of the two methods you choose since splitting on a combination of categories at a single node can be simulated by splitting on a single category at multiple nodes.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-11T23:44:40.667" Id="97314" LastActivityDate="2014-05-11T23:44:40.667" OwnerUserId="45396" ParentId="76590" PostTypeId="2" Score="0" />
  
  
  <row Body="&lt;p&gt;This is not perfect, but I think it gets the point across.&lt;/p&gt;&#10;&#10;&lt;p&gt;I pick up a book, and say: Look, this book has 3 dimensions, height(smallest), width, and length(largest). We can see that height is the least important dimension, because just a sheet of paper would provide a reasonable representation of the book. Hence, we can reduce dimensionality by going from 3D book to 2D sheet of paper with minimal loss of information because we kept the most important dimensions. And that's exactly what PCA does, it reduces dimensionality, by keeping the most important dimensions, and ignoring the least important ones. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-05-12T02:49:11.717" Id="97328" LastActivityDate="2014-05-12T02:49:11.717" OwnerUserId="11708" ParentId="2691" PostTypeId="2" Score="-1" />
  <row Body="&lt;p&gt;I would say that in general, we want the realized acceptance rate after the burn-in period to correspond to the targeted acceptance rate. Consider for example using an adaptive Metropolis sampler where the scale (or generally covariance matrix) of the proposal distribution is adjusted during the burn-in phase to achieve the targeted acceptance rate. Why would you want the samples during burn-in to be included in the calculation of the acceptance rate though they are discarded and not part of the posterior?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-05-12T07:54:19.020" Id="97341" LastActivityDate="2014-05-12T07:54:19.020" OwnerUserId="44783" ParentId="97337" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;You know that the p-value is $P(t&amp;lt;-|tRatio|)+P(t&amp;gt;|tRatio|)$ and its value is computed by the CDF of a $t$ distribution with the proper number of degrees of freedom, i.e. $n-p$ where $n$ is the number of observations and $p$ the number of parameters.&lt;/p&gt;&#10;&#10;&lt;p&gt;To get the absolute value of tRatio at a given significance you can use the quantile function. Let me use a toy example in R:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; x &amp;lt;- c(1, 2, 3)       # independent variable&#10;&amp;gt; y &amp;lt;- c(2.9, 5.2, 6.9) # dependent variable, n = 3 observations&#10;&amp;gt; linreg &amp;lt;- lm(y ~ x)&#10;&amp;gt; summary(linreg)       # p = 2 parameters&#10;[...]&#10;Coefficients:&#10;            Estimate Std. Error t value Pr(&amp;gt;|t|)  &#10;(Intercept)   1.0000     0.3742   2.673    0.228  &#10;x             2.0000     0.1732  11.547    0.055 .&#10;[...]&#10;&amp;gt; tRatio &amp;lt;- 11.547      # n - p = 1 degree of freedom&#10;&amp;gt; p_value &amp;lt;- pt(-abs(tRatio), 1) +            # P(t &amp;lt; -|tRatio|)&#10;+     pt(abs(tRatio), 1, lower.tail = FALSE)  # P(t &amp;gt; |tRatio|)&#10;&amp;gt; p_value&#10;[1] 0.0549957&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;You can compute the absolute value of tRatio at 5.49957% significance by the quantile function. Since it is a two sided test, you have to compute $F_t^{-1}(pvalue/2)$:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; abs(qt(p_value/2, 1))&#10;[1] 11.547&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;You can compute the absolute value of tRatio at whatever significance by:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; abs(qt(0.05 / 2, 1)) # this is where those vertical lines come from&#10;[1] 12.7062&#10;&amp;gt; abs(qt(0.10 / 2, 1))&#10;[1] 6.313752&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So, if you are looking for 10% significance a tRatio &gt; 6.314 or &amp;lt; -6.314 is enough.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-12T08:51:20.610" Id="97344" LastActivityDate="2014-05-12T08:51:20.610" OwnerUserId="44965" ParentId="65681" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a question that is a kinda puzzling me and hope that I can find some answers here :).&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose I have many signals, let's say four $X_1, X_2, X_3, X_4$ &lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose I have a set of windows $w_1, w_2, \dots , w_n $, in each window each signal follow a Poisson distribution, i.e: $ X_i \sim Poisson(\lambda_i) $.&lt;/p&gt;&#10;&#10;&lt;p&gt;What I want to do is to find windows that show a high value for some signals and not for others, for example they have high value for $X_1, X_2$ and low signal for $X_3, X_4$&lt;/p&gt;&#10;&#10;&lt;p&gt;One idea that come to my mind is to use an ANOVA-like method in which the first group contains $ X_1, X_2$ and the second contains $ X_3, X_4$ and test if the values of group are significantly &lt;strong&gt;bigger&lt;/strong&gt;  or &lt;strong&gt;lower&lt;/strong&gt; than the other.&lt;/p&gt;&#10;&#10;&lt;p&gt;Because the data is not normally distributed I though of using a GLM model how ever it seems that existing glm package in R and python test for equality (hope to be wrong). &lt;/p&gt;&#10;&#10;&lt;p&gt;Does this approach seem reasonable ? if so how to perform this test using R or python statsmodel package. &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance, &lt;/p&gt;&#10;" CommentCount="8" CreationDate="2014-05-12T09:20:55.413" Id="97346" LastActivityDate="2014-05-19T08:45:05.767" LastEditDate="2014-05-19T08:45:05.767" LastEditorUserId="805" OwnerUserId="2325" PostTypeId="1" Score="0" Tags="&lt;hypothesis-testing&gt;&lt;generalized-linear-model&gt;" Title="Check if a group of signals have a significantly lower or higher value from another group using GLM" ViewCount="89" />
  
  <row Body="&lt;p&gt;One practical thing that one would note while performing data clustering is that, in many occasions, the distance (or similarity) between the data instances will not be symmetric. That is,&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Distance (i, j) != Distance (j,i), where i,j belong to {Data Instances} and i != j&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Or even sometimes,&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Distance (i,i) != 0&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Also in some occassions, the distance measures do not satisfy &lt;a href=&quot;http://en.wikipedia.org/wiki/Triangle_inequality&quot; rel=&quot;nofollow&quot;&gt;triangle inequality&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;These type of measures are called non metric distance measures (as they do not satisfy metric axioms).&#10;More information on such non metric could be found &lt;a href=&quot;http://en.wikipedia.org/wiki/Metric_%28mathematics%29&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Many well-known clustering algorithms like K-means, Hierarchical Agglomerative clustering, EM etc. were originally designed to operate on metric distances (some variations of such algorithms work on non metric distances as well). One area where Affinity Propagation (AP) truly stands out is that, AP by design can handle non metric measures! &lt;/p&gt;&#10;&#10;&lt;p&gt;Excerpts from &lt;a href=&quot;http://genes.toronto.edu/affinitypropagation/FreyDueckScience07.pdf&quot; rel=&quot;nofollow&quot;&gt;AP paper&lt;/a&gt; in science 2007:  &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Unlike metric-space clustering techniques such as k-means clustering,&#10;  affinity propagation can be applied to problems where the data do not&#10;  lie in a continuous space. Indeed, it can be applied to problems where&#10;  the similarities are not symmetric  and to problems where the&#10;  similarities do not satisfy the triangle inequality.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="0" CreationDate="2014-05-12T12:21:47.847" Id="97369" LastActivityDate="2014-05-12T14:50:32.240" LastEditDate="2014-05-12T14:50:32.240" LastEditorUserId="40917" OwnerUserId="40917" ParentId="38751" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Based off @jsk's comments I will try to remedy my mistakes:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\begin{align*}&#10;L(\theta|X,G) &amp;amp;= \prod_{j=1}^n \theta e^{-\theta x_j}&#10;\end{align*}$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\begin{align*}&#10;Q(\theta,\theta^i) &amp;amp;= n\log{\theta} - \theta\sum_{j=1}^n \text{E}\left[X_j|G,\theta^i\right]\\&#10;&amp;amp;= n\log{\theta} - \theta\left(\dfrac{\sum_{j=1}^n g_{1j}}{1-e^{-\theta^i}}\right)\left(\dfrac{1}{\theta^i} - e^{-\theta^i}(1+1/\theta^i)\right) - \theta\left(\dfrac{\sum_{j=1}^n g_{2j}}{e^{-\theta^i}(1-e^{-\theta^i})}\right)\left(e^{-\theta^i}(1+1/\theta^i)-e^{-2\theta^i}(2+1/\theta^i)\right) - \theta\left(\dfrac{\sum_{j=1}^n g_{3j}}{e^{-2\theta^i}}\right)\left(e^{-2\theta^i}(2+1/\theta^i)\right)\\&#10;&amp;amp;= n\log{\theta} - \theta N_1 A - \theta N_2 B - \theta N_3 C\\&#10;\dfrac{\partial Q(\theta,\theta^i)}{\partial \theta} &amp;amp;= \dfrac{n}{\theta} - N_1A-N_2B - N_3C \overset{set}{=}0&#10;\end{align*}$&lt;/p&gt;&#10;&#10;&lt;p&gt;solving for $\theta$ we get $\theta^{(i+1)} = \dfrac{n}{N_1A+N_2B+N_3C}$&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-12T12:47:26.510" Id="97372" LastActivityDate="2014-05-12T12:47:26.510" OwnerUserId="17661" ParentId="97324" PostTypeId="2" Score="3" />
  <row AnswerCount="0" Body="&lt;p&gt;I would like to set up an experiment in MATLAB, to predict the class of a set of text instances in a two-class problem (e.g., the text talks/does not talk about &lt;code&gt;basketball&lt;/code&gt;). &lt;/p&gt;&#10;&#10;&lt;p&gt;By doing this, I would like to draw &lt;strong&gt;learning curves&lt;/strong&gt; by varying the number of samples &lt;code&gt;M&lt;/code&gt; and consequently understand which is the best &lt;code&gt;M&lt;/code&gt; to be used to avoid high bias/high variance problems. Consequently, I would like to compute $J_{train}(\theta)$, $J_{test}(\theta)$ and $J_{cv}(\theta)$ (respectively: training error, test error and cross-validation error) on my experiment.&lt;/p&gt;&#10;&#10;&lt;p&gt;I created a &lt;a href=&quot;http://stackoverflow.com/questions/23565826/compute-the-training-error-and-test-error-in-libsvm-matlab&quot;&gt;question&lt;/a&gt; last week, but I guess it was misplaced and maybe it is better to ask it here.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'll try to subdivide my question in different subquestions.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Context&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I looked for some useful documentation on &lt;code&gt;libsvm&lt;/code&gt;, so as to understand how to use its parameters and set up properly the experiment.&#10;Unfortunately, all I can find is a list of random parameters without any further explanation, e.g.:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;-d degree : set degree in kernel function (default 3)&#10;-g gamma : set gamma in kernel function (default 1/num_features)&#10;-r coef0 : set coef0 in kernel function (default 0)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Thus, as an initial action, I went for a classical setting, without specifying any classifier parameter:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;model = svmtrain(trainingSet.labels, featureVectors, '-b 1');&#10;[predict_label, accuracy, prob_estimates] = svmpredict(testSet.labels, testFeatureVectors, model, '-b 1');&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;where the feature vectors are integer vectors of length $L \sim 4000$ (TF-IDF approach, each cell indicates the number of occurrences of the specific word in a text instance).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Smoothness of test error&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I tested the training-testing pipeline specified above by changing the number of samples $M$ that is fed into the classifier in training-testing phase. Then, I extracted the test error:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;testError = accuracy(2);&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;When the number of results in the training set $M$ is increasing, I would expect the test error to change monotonically. However, what I obtain is the following figure (x-axis: $M$, y-axis: training error $J_{test}(\theta)$):&#10;&lt;img src=&quot;http://i.stack.imgur.com/FtSaQ.png&quot; alt=&quot;test error&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Is this a &quot;normal&quot; behavior? As I mentioned above, I was expecting something smoother than this.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thus, I see here different options:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;either I am using the wrong parameters to set up the classifier&lt;/li&gt;&#10;&lt;li&gt;or I misinterpreted the meaning of the output&lt;/li&gt;&#10;&lt;li&gt;or I built wrongly the train-test pipeline&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Is there a more appropriate way of setting up the experiment?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Training error&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;How to compute the error $J_{train}(\theta)$ on the training set?&lt;/p&gt;&#10;&#10;&lt;p&gt;I rerun the prediction on the &lt;strong&gt;training set&lt;/strong&gt; and then extracted the MSE:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;[~, accuracy, ~] = svmpredict(trainingSet.labels, featureVectors, model, '-b 1');&#10;trainError(p) = accuracy(2);&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;However, again, the training error (or what I suppose is the training error) has this shape (which, again, is not monotonic):&#10;&lt;img src=&quot;http://i.stack.imgur.com/EgDUk.png&quot; alt=&quot;training error&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Cross-validation&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;How to use cross-validation and consequently compute the cross-validation error $J_{cv}(\theta)$?&lt;/p&gt;&#10;&#10;&lt;p&gt;I am using:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;[~,accuracy.~] = svmtrain(crossValidationSet.labels, featureVectors);&#10;CVerror = accuracy(2);&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;but again, as above, I don't know whether this makes sense.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Overall result&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Here, I am drawing cross validation error (red) and training error (blue). Not monotonic.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/HQUuv.png&quot; alt=&quot;cross validation error&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-12T14:01:31.947" Id="97383" LastActivityDate="2014-05-13T07:33:00.773" LastEditDate="2014-05-13T07:33:00.773" LastEditorUserId="20869" OwnerUserId="20869" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;classification&gt;&lt;svm&gt;&lt;matlab&gt;&lt;libsvm&gt;" Title="Set up a classification experiment via libsvm (MATLAB)" ViewCount="237" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a generalised linear mixed model with 34 explanatory variables (over 130,000 observations for each). 10 of these variables are different unprotected habitat types, and another 10 are the same habitat types but protected for conservation. (The factor for the random effect E.V. is region).&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to compare whether the coefficients (within this single model) for each habitat type when protected or not protected are significantly different i.e. compare beta(broadleaf.protected) with beta(broadleaf.unprotected), beta(conifer.protected) with beta(conifer.unprotected) etc. to find out whether they have a significantly different effect on my D.V. (visit probability).&lt;/p&gt;&#10;&#10;&lt;p&gt;My explanatory variables are standardised to mean 0 sd 1.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-12T14:32:45.897" Id="97386" LastActivityDate="2014-05-12T15:03:40.407" LastEditDate="2014-05-12T15:03:40.407" LastEditorUserId="27403" OwnerUserId="45427" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;standardization&gt;&lt;glmm&gt;&lt;coefficient&gt;" Title="How to compare pairs of coefficients within a glmm with binomial error" ViewCount="43" />
  <row AnswerCount="2" Body="&lt;p&gt;Consider a random intercept linear model. This is equivalent to GEE linear regression with an exchangeable working correlation matrix. Suppose the predictors are $x_1, x_2,$ and $x_3$ and the coefficients for these predictors are $\beta_1$, $\beta_2$, and $\beta_3$. What is the interpretation for the coefficients in the random intercept model? Is it the same as the GEE linear regression except that it is at the individual level?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-12T15:34:43.740" Id="97393" LastActivityDate="2014-05-12T17:00:13.943" OwnerUserId="44281" PostTypeId="1" Score="9" Tags="&lt;mixed-model&gt;" Title="Random Intercept model vs. GEE" ViewCount="425" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;Am currently using Naive Bayes for a multi labelled text document classification problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;But I would like to know the differences (advantages and disadvantages) of using SkiLearn Naive Bayes or NLTK?&lt;/p&gt;&#10;&#10;&lt;p&gt;SkiLearn seems to have flexibility in parameter setting more than NLTK, but I'm think my thoughts on this being the only difference is quite naive.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also what cases would you select one over the other?&lt;/p&gt;&#10;&#10;&lt;p&gt;Any help appreciated.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-12T22:29:18.943" Id="97436" LastActivityDate="2014-07-24T17:41:41.970" OwnerUserId="45451" PostTypeId="1" Score="0" Tags="&lt;python&gt;&lt;naive-bayes&gt;" Title="Naive Bayes Python implementation differences" ViewCount="221" />
  <row Body="&lt;p&gt;Trying to get to normality is usually a means to an end. Understanding the end will really help in understanding what is the best path forward, and oftentimes whether or not transforming to get to normality is even needed. There are tons of natural distributions that are not normal and do not need to be transformed to normal.&lt;/p&gt;&#10;&#10;&lt;p&gt;The set upper limit of 60 is problematic because it creates a false upper end of the distribution. You may be getting a bad Shapiro-Wilk simply because there are too many 60's in your dataset. I would recommend bootstrapping a right-tail or concatenating the upper and lower ends of your dataset. These are not ideal methods and must all be put in context of what you're trying to do, so as to see if they make sense.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-05-12T23:58:41.073" Id="97449" LastActivityDate="2014-05-12T23:58:41.073" OwnerUserId="66775" ParentId="97445" PostTypeId="2" Score="2" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm working with land surface models. These models basically take a bunch of meteorological forcing data (downward radiation, wind, rain, humidity, etc), and run it through some biogeochemical/physical equations, and spit out a bunch of output data ($CO_2$ update/release, net radiation, evaporation, runoff, etc.). There are some problems with the models, and I'm trying to use clustering methods to split the input variables up into subdomains, so I can look at the responses in the output in a non-linear manner.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is whether it is valid and/or sensible to cluster on &lt;em&gt;structural variables&lt;/em&gt; (e.g. time and space related variables). The input variables, like downward radiation, are &lt;em&gt;highly&lt;/em&gt; dependent on these variables (e.g. the amount of sunshine is largely determined by latitude, time of year, and time of day). So I guess it doesn't make much sense to cluster on both the structural and the input variables. But clustering on the structural variables may offer things that clustering on the input variables doesn't - the ability to categorise model output by time of day, for example. Is there any reason why using structural variables should be avoided? Or is it entirely dependent on the model in question?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-05-13T04:08:32.790" Id="97453" LastActivityDate="2014-05-13T04:08:32.790" OwnerUserId="9007" PostTypeId="1" Score="1" Tags="&lt;clustering&gt;&lt;independence&gt;&lt;predictor&gt;&lt;climate&gt;" Title="Clustering on structural variables?" ViewCount="22" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I use semi-parametric models with linear and non-linear terms in SAS. I want to plot the predictions and confidence limits of the linear terms in the model. However, the output out statement and plots only show predictions and CL's for the non-linear terms (smoothers) in the model. How can I get the predictions and CL's of the observations of the linear terms in the model?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-13T11:02:51.870" Id="97482" LastActivityDate="2014-05-13T11:25:01.390" LastEditDate="2014-05-13T11:25:01.390" LastEditorUserId="45472" OwnerUserId="45472" PostTypeId="1" Score="0" Tags="&lt;sas&gt;&lt;gam&gt;" Title="Predictions and confidence limits for linear terms in GAM model" ViewCount="29" />
  <row AnswerCount="2" Body="&lt;p&gt;I think my question is similar to this one:&#10;&lt;a href=&quot;http://stats.stackexchange.com/questions/82242/how-do-you-plot-an-interaction-between-a-factor-and-a-continous-covariate&quot;&gt;How do you plot an interaction between a factor and a continous covariate?&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;My IV and DV are both scales, and my interaction term has 3 levels. I am trying to look for associations between my IV and DV so I need to be using correlation coefficients rather than means... I think!&lt;/p&gt;&#10;&#10;&lt;p&gt;Basically, someone suggested to me doing a bar chart (because it would show the difference more clearly), with the three interaction levels on the x axis and the IV/DV r on the Y axis. I think that's what they were suggesting but it has completely baffled me! I have no idea how to use r in this way...&lt;/p&gt;&#10;&#10;&lt;p&gt;Any help would be great :)&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT:&#10;Just to clarify, I have done a regression to look for significance, and then also an ANCOVA because it was significant.&#10;But what i'm really asking is for a way to illustrate my findings in a figure.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-05-13T12:04:25.833" Id="97485" LastActivityDate="2014-05-13T16:22:30.973" LastEditDate="2014-05-13T16:22:30.973" LastEditorUserId="45476" OwnerUserId="45476" PostTypeId="1" Score="1" Tags="&lt;data-visualization&gt;&lt;categorical-data&gt;&lt;interaction&gt;&lt;continuous-data&gt;" Title="Plotting an interaction between a continuous IV and DV and an ordinal covariate, can I use a bar chart?" ViewCount="333" />
  
  <row Body="&lt;p&gt;The statement seems right to me. &lt;/p&gt;&#10;&#10;&lt;p&gt;Using sites as a random effect in a mixed model does not consume any degrees of freedom. In contrast, using sites as a dummy variable in the fixed part of the model, would consume a lot of DF (21-1). &lt;/p&gt;&#10;&#10;&lt;p&gt;However, you may want to check if sites have any effect at all. If there is no inner site correlation, you may assume that the observations are independent. In this case you could use all observations within the model without a random effect.&lt;/p&gt;&#10;&#10;&lt;p&gt;Graficcaly, you could check this with a boxplot of the residuals of the model for each site and compare the medians. Or you can compare the AIC of the model with and without the random effect. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-13T13:00:39.110" Id="97496" LastActivityDate="2014-05-13T13:20:42.720" LastEditDate="2014-05-13T13:20:42.720" LastEditorUserId="27403" OwnerUserId="44862" ParentId="62351" PostTypeId="2" Score="0" />
  
  <row AnswerCount="0" Body="&lt;p&gt;What would be the most efficient way to achieve the following task in R:&#10;combine the values of one variable for those observations which match on one index variable and have specific values on a second index variable.&#10;For example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;id1&amp;lt;-c(&quot;a&quot;,&quot;a&quot;,&quot;b&quot;,&quot;b&quot;)&#10;id2&amp;lt;-c (&quot;x&quot;,&quot;y&quot;,&quot;x&quot;,&quot;y&quot;)&#10;variable&amp;lt;-rep(1,4)&#10;d&amp;lt;-as.data.frame(cbind(id1,id2,variable))&#10;d$variable&amp;lt;-as.numeric(as.character(d$variable))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The task is, for the observations (rows) that have the same 'id1', to add the value of 'variable' for the cases with 'id2' equal to 'x' to the value of 'variable' for the cases with 'id2' equal to 'y'.&#10;The best I could come up with is double looping which does the job but is very slow and ugly:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;for (i in 1:nrow(d)){&#10;  if (d$id2[i]=='x') {&#10;        for (j in 1:nrow(d)){&#10;          if (d$id1[i]==d$id1[j] &amp;amp; d$id2[j]=='y'){&#10;            d$variable[j]&amp;lt;-d$variable[i]+d$variable[j]&#10;      } else next  } } else next } &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So what would be the most efficient approach?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-13T13:32:51.417" Id="97500" LastActivityDate="2014-05-13T13:32:51.417" OwnerUserId="39517" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;data-management&gt;" Title="Conditional match and combine data in R" ViewCount="68" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I have an rather open question. In Bayesian statistics you do testing based on some posterior distribution $p(\theta|D)$. E.g. you could try something like $T=P[\theta&amp;gt;0|D]$ and decide based on the threshold $.5$.&lt;/p&gt;&#10;&#10;&lt;p&gt;$T$ is basically some statistics of the data observed - and as such has a sampling distribution. Could I not get frequentists bounds on $T$ that guarantee a certain error niveau using the null? Am I completely mistaken here, is it just useless or does this approach have a tag I could not find? &lt;/p&gt;&#10;&#10;&lt;p&gt;The cool thing would be having a frequentist test proceedure and instead of messing around with 'almost' significant p-values you just report the statistic $T$ itself? Is this related to conditional testing?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-13T14:25:14.683" Id="97511" LastActivityDate="2014-05-14T04:14:38.493" LastEditDate="2014-05-13T14:41:12.167" LastEditorUserId="13680" OwnerUserId="45490" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;&lt;bayesian&gt;&lt;posterior&gt;" Title="What can be learnt from the sampling distribution of posterior probabilities?" ViewCount="71" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I found an approach to calculate a selection correction variable for a Cox regression (Lee 1983. &lt;a href=&quot;http://www.jstor.org/discover/10.2307/1912003?uid=3739776&amp;amp;uid=2129&amp;amp;uid=2&amp;amp;uid=70&amp;amp;uid=4&amp;amp;uid=3739256&amp;amp;sid=21104014221997&quot; rel=&quot;nofollow&quot;&gt;Generalized econometric models with selectivity&lt;/a&gt;. Econometrica 51: 507512.):&#10;$$&#10; = \phi  \frac{ \phi^{-1}   (F_i(t)) }{1 - (F_i(t))}&#10;$$&#10;where $F_i(t)$ is the cumulative hazard function for object $i$ at time $t$,  $\phi$ is the standard normal density function (of the selection correction variable?), and  $\phi^{-1}$ is the inverse of the standard normal distribution function.&lt;/p&gt;&#10;&#10;&lt;p&gt;So far so good. However, I am quite struggling with the calculation in STATA (11). If Lamda is included as covariate, STATA says: &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;flat region resulting in a missing likelihood r(430);&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Does anyone have any idea how to solve this issue or are there any other methods to correct selection bias in Cox proportional hazard model (e.g., Heckman's correction)? Is the two-stage approach applicable and how does it work regarding the use of Cox regression?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-13T14:56:42.127" Id="97518" LastActivityDate="2014-05-13T15:21:05.780" LastEditDate="2014-05-13T15:21:05.780" LastEditorUserId="7290" OwnerUserId="45492" PostTypeId="1" Score="1" Tags="&lt;survival&gt;&lt;stata&gt;&lt;bias&gt;&lt;cox-model&gt;&lt;hazard&gt;" Title="Selection correction variable (non-randomly selected sample / bias) and Cox regression" ViewCount="105" />
  <row Body="&lt;p&gt;In layman's terms, you'll often look for the maximum likelihood and $f(x)$ and $kf(x)$ share the same critical points.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-05-13T15:53:53.300" Id="97529" LastActivityDate="2014-05-13T15:53:53.300" OwnerUserId="44965" ParentId="97515" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I cannot explain the meaning of the quotation, but for &lt;em&gt;maximum-likelihood&lt;/em&gt; estimation, it does not matter whether we choose to find the maximum of &#10;the likelihood function  $L(\mathbf x; \theta)$ (regarded as a function&#10;of $\theta$ or the maximum of &#10;$aL(\mathbf x; \theta)$  where $a$ is some constant.&#10;This is because we are not interested in the maximum value of &#10;$L(\mathbf x; \theta)$ but rather the value $\theta_{\text{ML}}$&#10;where this maximum occurs, and both $L(\mathbf x; \theta)$&#10;and $aL(\mathbf x; \theta)$ achieve their maximum value at the same&#10;$\theta_{\text{ML}}$.  So, multiplicative constants can be ignored.&#10;Similarly, we could choose to consider any monotone function $g(\cdot)$&#10;(such as the logarithm) of the likelihood function $L(\mathbf x; \theta)$, determine&#10;the maximum of $g(L(\mathbf x;\theta))$, and infer the value of&#10;$\theta_{\text{ML}}$ from this.  For the logarithm, the multipliative constant&#10;$a$ becomes the additive constant $\ln(a)$ and this too can be ignored in&#10;the process of finding the location of the maximum: &#10;$\ln(a)+\ln(L(\mathbf x; \theta)$&#10;is maximized at the same point as $\ln(L(\mathbf x; \theta)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Turning to maximum &lt;em&gt;a posteriori&lt;/em&gt; probability (MAP) estimation,&#10;$\theta$ is regarded as a realization of a random variable $\Theta$ with&#10;&lt;em&gt;a priori&lt;/em&gt; density function $f_{\Theta}(\theta)$, &#10;the data $\mathbf x$ is regarded as a &#10;realization of a random variable $\mathbf X$, and the likelihood function is considered to be the value of the  &lt;em&gt;conditional&lt;/em&gt; density &#10;$f_{\mathbf X\mid \Theta}(\mathbf x\mid \Theta=\theta)$ &#10;of $\mathbf X$ conditioned on $\Theta = \theta$; said&#10;conditional density function being evaluated at $\mathbf x$.&#10;The &lt;em&gt;a posteriori&lt;/em&gt; density of $\Theta$ is &#10;$$f_{\Theta\mid \mathbf X}(\theta \mid \mathbf x) &#10;= \frac{f_{\mathbf X\mid \Theta}(\mathbf x\mid \Theta=\theta)f_\Theta(\theta)}{f_{\mathbf X}(\mathbf x)} \tag{1}$$&#10;in which we recognize the numerator as the &lt;em&gt;joint density&lt;/em&gt;&#10;$f_{\mathbf X, \Theta}(\mathbf x, \theta)$ of the data and the parameter&#10;being estimated. The point $\theta_{\text{MAP}}$ where &#10;$f_{\Theta\mid \mathbf X}(\theta \mid \mathbf x)$ attains&#10;its maximum value is the MAP estimate of $\theta$, and,&#10;using the same arguments as in the paragraph, we see that&#10;we can ignore $[f_{\mathbf X}(\mathbf x)]^{-1}$ on the&#10;right side of $(1)$ as a multiplicative constant just&#10;as we can ignore multiplicative constants in &lt;em&gt;both&lt;/em&gt;&#10;$f_{\mathbf X\mid \Theta}(\mathbf x\mid \Theta=\theta)$ and in&#10;$f_\Theta(\theta)$. Similarly when log-likelihoods are being&#10;used, we can ignore additive constants.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-13T16:17:12.263" Id="97533" LastActivityDate="2014-05-13T16:17:12.263" OwnerUserId="6633" ParentId="97515" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;There's some problems with the way you're asking the question, as far as I can tell (or perhaps i'm misreading your question).&lt;/p&gt;&#10;&#10;&lt;p&gt;So, frequentist hypothesis testing says given that the null hypothesis $H_0: \theta \in \Theta_0$ is true (versus $H_1: \theta \in \Theta_1$), reject $H_0$ if the test statistic $T = \frac{\sup_{\theta \in \Theta_1} p(D | \theta)}{\sup_{\theta \in \Theta_0} p(D | \theta)}$ is sufficiently large. Notice that at no point in this is there any indication that $\theta$ is a random variable. The unknown value $\theta$ is considered &lt;strong&gt;fixed&lt;/strong&gt; and unknown. Thus a null hypothesis of $H_0: \theta = 0$ (i.e a single-valued hypothesis) is a reasonable hypothesis to pose.&lt;/p&gt;&#10;&#10;&lt;p&gt;In Bayesian hypothesis testing, there is no notion of assuming $H_0$ to be true - just a prior distribution on $\theta$ that weights the various hypotheses. You base your acceptance/rejection criterion on a ratio depending on $p(\theta|H_0)$ and $p(\theta|H_1)$. Notice that $\theta$ is has the posterior &lt;strong&gt;distribution&lt;/strong&gt; associated to it, and so a statement like $H_0:\theta = 0$ will have posterior probability of 0 if the posterior for $\theta$ is a continuous distribution, and so in such cases, it doesn't make sense to have such a null hypothesis - rather you desire to ask $H_0: \theta \in \Theta_0$ for $\Theta_0$ being small interval containing 0, or something like that. Note that the statements $p(\theta|H_0)$ and $p(\theta|H_1)$ are completely meaningless in frequentist testing.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note also that posterior probabilities in no way correspond to p-values - the posterior probabilities are probabilities on $\theta$ given the &lt;em&gt;fixed&lt;/em&gt; data $D$ while p-values ask about probabilities of the random data $D$ given &lt;em&gt;fixed&lt;/em&gt; $\theta$. These two approaches ask fundamentally different questions. The former asks what you should believe about $\theta$ based on the observed data (and &lt;em&gt;only&lt;/em&gt; the observed data), and the latter asks whether your data is &lt;em&gt;extreme&lt;/em&gt; assuming the null hypothesis is true (i.e. it considers hypothetical data).&lt;/p&gt;&#10;&#10;&lt;p&gt;Your question asks whether you can get some sort of frequentist bounds on the posterior probabilities given that $H_0$ is true. I suppose you can ask how the posterior probabilities would vary if the data were to vary according to the null hypothesis, as a measure of sensitivity of the posterior to the data. But this wouldn't answer the original hypothesis testing question of whether to accept/reject the hypotheses based on the available data.&lt;/p&gt;&#10;&#10;&lt;p&gt;I hope I haven't misinterpreted your question and I hope this even makes sense and is correct...&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-13T17:47:17.333" Id="97544" LastActivityDate="2014-05-13T18:37:11.567" LastEditDate="2014-05-13T18:37:11.567" LastEditorUserId="42728" OwnerUserId="42728" ParentId="97511" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Homework problem asks to compute the principal components of a p-variate multinormal vector $X$ ~ $N(0,\Sigma)$ where $\Sigma$ = $\delta^{2}I_{p} + \sigma ^{2} I_{p}I_{p}^{T}$. I use the PC transformation from our multivariate text where the rows of Y are the PCs and $\Gamma ^{T}$ is matrix of eigenvectors for the decomposition of $\Sigma$: Y = $\Gamma ^{T} (X - \mu)$. I'm stuck in trying to compute the spectral decomposition for $\Sigma$. I know that to compute eigenvalues, it's necessary to take the determinant of $\Sigma$ - $\lambda I_{p}$, where $\Sigma$ is expanded from above. How do I do this?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-13T18:16:33.993" Id="97550" LastActivityDate="2014-05-13T18:16:33.993" OwnerUserId="31534" PostTypeId="1" Score="1" Tags="&lt;self-study&gt;" Title="PC Transformation for multinormal" ViewCount="15" />
  <row AnswerCount="0" Body="&lt;p&gt;I have been doing a LOT of research about this but I can't figure out exactly how to do it. I need to find a problem that can be solved using Monte Carlo Optimisation (it is important that it is an optimisation not a simulation). I have came across - calculating the area of complicated shapes and the TSP problem. Does anybody know if there is any code that uses random number generation (Monte Carlo method) for any of these optimisations (or other optimisations that can be solved by Monte Carlo?) I need to analyze the situation given some code. Thanks&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-05-13T19:44:22.123" Id="97562" LastActivityDate="2014-05-13T19:44:22.123" OwnerUserId="44515" PostTypeId="1" Score="0" Tags="&lt;optimization&gt;&lt;simulation&gt;&lt;monte-carlo&gt;" Title="Monte Carlo Optimisation" ViewCount="44" />
  
  <row Body="&lt;p&gt;I suggest that you use semiparametric regression. This involve regression with splines, splines can be polynomial.&lt;/p&gt;&#10;&#10;&lt;p&gt;Read this book:&#10;&lt;a href=&quot;http://www.stat.tamu.edu/~carroll/semiregbook/&quot; rel=&quot;nofollow&quot;&gt;http://www.stat.tamu.edu/~carroll/semiregbook/&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;There are R packages that can do this for you. For example mgcv.&#10;&lt;a href=&quot;http://cran.r-project.org/web/packages/mgcv/&quot; rel=&quot;nofollow&quot;&gt;http://cran.r-project.org/web/packages/mgcv/&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-14T02:10:05.423" Id="97607" LastActivityDate="2014-05-14T02:10:05.423" OwnerUserId="16596" ParentId="35664" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;The answer is, in general, no.&lt;/p&gt;&#10;&#10;&lt;p&gt;Consider only two random variables, $A_1$ and $A_2$. &lt;/p&gt;&#10;&#10;&lt;p&gt;The constraint $A_1+A_2&amp;lt;c$ is the area below/to the left of a 45-degree line in the $(A_1,A_2)$-plane. &lt;/p&gt;&#10;&#10;&lt;p&gt;The constraints $A_1&amp;lt;b_1$ and $A_2&amp;lt;b_2$ are areas bounded by lines parallel to each of the axes. The events simply don't correspond.&lt;/p&gt;&#10;&#10;&lt;p&gt;In some particular circumstances (including known distributions for the $A$ variables, at the least) you may be able to derive some relationships, but there's no simple general relationship like you hope for.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-05-14T04:26:19.373" Id="97621" LastActivityDate="2014-05-14T06:02:27.407" LastEditDate="2014-05-14T06:02:27.407" LastEditorUserId="805" OwnerUserId="805" ParentId="97616" PostTypeId="2" Score="2" />
  
  
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am using Dredge (MuMln  package) for a general linear model.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the Model Averaged Parameters, what is the Adjusted standard error vs. the 'normal' standard error please? i.e. what is being adjusted for?&lt;/p&gt;&#10;&#10;&lt;p&gt;e.g.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Model-averaged coefficients:&#10;&#10;                Estimate Std. Error **Adjusted SE** z value Pr(&amp;gt;|z|)&#10;(Intercept) 34.21850      1.47217       1.49370     22.909     &amp;lt; 2e-16 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The information I have been able to find so far is:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&quot;Unconditional standard errors are square root of the variance&#10;  estimator, calculated either according to the original equation in&#10;  Burnham and Anderson (2002, equation 4.7), or a newer, revised formula&#10;  from Burnham and Anderson (2004, equation 4). The adjusted standard&#10;  error estimator and confidence intervals with improved coverage are&#10;  returned (see Burnham and Anderson 2002, section 4.3.3).&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;However I do not have access to this book so that is not overly informative.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would be grateful for any suggestion&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;&#10;&lt;p&gt;Emma&#10;EDIT: does no-one have any suggestions?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-14T13:39:03.740" Id="97678" LastActivityDate="2014-05-17T11:24:51.697" LastEditDate="2014-05-17T11:24:51.697" LastEditorUserId="45549" OwnerUserId="45549" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;standard-error&gt;" Title="Adjusted SE vs SE in Dredge output R" ViewCount="83" />
  
  <row Body="&lt;p&gt;They're so close to the same there isn't really any substantial conceptual difference displayed through the different representations. &lt;/p&gt;&#10;&#10;&lt;p&gt;Equation 2 would typically be used in the interest of brevity and for an audience or manuscript focused solely on the SE. Equation 1 would be more useful in most cases of establishing derivation or relationships to underlying principles of variance.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-14T15:55:39.733" Id="97700" LastActivityDate="2014-05-14T15:55:39.733" OwnerUserId="601" ParentId="97690" PostTypeId="2" Score="2" />
  
  
  
  
  <row AcceptedAnswerId="97729" AnswerCount="2" Body="&lt;p&gt;I'm reading from &lt;a href=&quot;http://www.iro.umontreal.ca/~bengioy/papers/ftml.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.iro.umontreal.ca/~bengioy/papers/ftml.pdf&lt;/a&gt; around page 8.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is a quote that I'm not understanding:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&quot;If we include affine operations and their possible composition with sigmoids &#10; in the set of computational elements, linear regression and logistic regression &#10;have depth 1, i.e., have a single level.&quot;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Depth or Depth of Architecture refers &quot;to the depth of that graph, i.e. the longest path from an input node to an output node.&quot;&lt;img src=&quot;http://i.stack.imgur.com/p9dWq.png&quot; alt=&quot;Here is a simple example demonstrating a depth of 4&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I can see why the graph represented in the image has depth of 4. I'm not sure how to think about, say, linear regression in these terms.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-14T18:33:11.140" Id="97725" LastActivityDate="2014-05-14T18:52:04.300" OwnerUserId="40128" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;deep-learning&gt;" Title="Deep learning ... notion of depth" ViewCount="162" />
  
  <row Body="&lt;p&gt;If you're going to do LDA after PCA I would keep the first k components. Don't try and figure out which of the components are important and don't go back to the original parameter space. You can feed in the k dimensional data into your LDA classifier and let it figure out what is important there.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-05-14T19:01:53.787" Id="97732" LastActivityDate="2014-05-14T19:01:53.787" OwnerUserId="41389" ParentId="97724" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="97739" AnswerCount="1" Body="&lt;p&gt;Good day! I'm totally new to SPSS (and the test as well, but after some research I've finally demystified a bit about Mann-Whitney and how to do the test in SPSS. The only problem I'm currently facing is the analysis) And so in this snippet, I want to discover how the researcher came up with this analysis: &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;https://fbcdn-sphotos-g-a.akamaihd.net/hphotos-ak-ash3/t1.0-9/q71/s720x720/10268622_10202139109587436_5776841905445543571_n.jpg&quot; alt=&quot;Valid XHTML&quot;&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;How was he able to come up with a conclusion that there is no significant difference between the examination marks? What specific data in the table made him conclude that? What variables did he look at? If I were to use a Mann-Whitney test and have to interpret the results, where will I look at? What are the factors do I have to consider? Any help would be very much appreciated. :)&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, what's the purpose of the Z and Wilcoxon W? Thank you very much!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-14T19:41:35.893" Id="97737" LastActivityDate="2014-05-14T20:05:51.773" LastEditDate="2014-05-14T19:51:17.687" LastEditorUserId="45520" OwnerUserId="45520" PostTypeId="1" Score="2" Tags="&lt;mann-whitney-u-test&gt;" Title="Mann-Whitney U Test Analysis Help" ViewCount="210" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Most of you will have been entertained by the recent &lt;a href=&quot;http://www.tylervigen.com/&quot; rel=&quot;nofollow&quot;&gt;blog post&lt;/a&gt; showing how random things in the world correlate without, of course, necessarily being causally related.&lt;/p&gt;&#10;&#10;&lt;p&gt;Looking at those graphs, I was wondering in my mind whether, for two vectors X and Y to be correlated, it is the &lt;em&gt;difference&lt;/em&gt; or the &lt;em&gt;ratio&lt;/em&gt; between their respective elements that needs to be relatively constant between elements. In the graphs, the two variables are always on very different scales, and so it is the difference that is constant rather than the ration.&lt;/p&gt;&#10;&#10;&lt;p&gt;Did a quick check in Statistica and turns out that either is sufficient:&#10;&lt;img src=&quot;http://i.stack.imgur.com/SUzm8.png&quot; alt=&quot;enter image description here&quot;&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/IsK06.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Or am I making a mistake here?&lt;/p&gt;&#10;&#10;&lt;p&gt;Looked at the definition of Cov(X,Y) but didn't help me understand better.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-05-14T21:37:12.330" Id="97755" LastActivityDate="2014-05-14T21:43:48.993" OwnerUserId="41307" PostTypeId="1" Score="0" Tags="&lt;correlation&gt;" Title="Does correlation imply a (relatively) constant difference or ratio between the two variables?" ViewCount="43" />
  
  <row Body="&lt;p&gt;I guess I find the answer via looking at the off diagonal of the Fisher information matrix.&lt;/p&gt;&#10;&#10;&lt;p&gt;$P(X=x) = \frac{\Gamma(x + \psi)}{\Gamma(\psi)\Gamma(x+1)} \frac{\psi^\psi\mu^x}{(\psi+\mu)^{(\psi+x)}}$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\log L = n\psi \log\psi - n\log\Gamma(\psi)+\Sigma_{i=1}^n\log\Gamma(x_i+\psi)+\Sigma_{i=1}^nx_i\log\mu-\Sigma_{i=1}^n(\psi+x_i)\log(\psi+\mu) - \Sigma_{i=1}^n \log\Gamma(x_i+1)$&lt;/p&gt;&#10;&#10;&lt;p&gt;$E(-\frac{\partial^2\ell}{\partial\mu\partial\psi}) = E[\Sigma_{i=1}^n\frac{(\psi+\mu)-(\psi+x_i)}{(\psi+\mu)^2}]=0 $&lt;/p&gt;&#10;&#10;&lt;p&gt;The off diagonal of the Fisher information matrix is $0$. This indicates there is no correlation between the two.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-14T22:19:46.283" Id="97762" LastActivityDate="2014-05-15T01:09:22.460" LastEditDate="2014-05-15T01:09:22.460" LastEditorUserId="2126" OwnerUserId="43739" ParentId="94311" PostTypeId="2" Score="1" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have got various time series data for different observations. I want to represent one time series observation by &lt;strong&gt;one aggregated variable&lt;/strong&gt;. The easiest one is to take the &lt;code&gt;mean&lt;/code&gt; of the time series, but &lt;code&gt;mean&lt;/code&gt; does not capture the variations in the observation over time. I thought of computing the distance (&lt;code&gt;dist&lt;/code&gt;) between an arbitrary reference and the time series data.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am curious to know if there is other technique that I could apply to fairly represent the time series observation by &lt;strong&gt;one aggregation variable&lt;/strong&gt;? Is there any R package that I can use?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-05-14T23:42:07.837" Id="97769" LastActivityDate="2014-05-15T11:44:05.450" OwnerUserId="43258" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;time-series&gt;" Title="Aggregating time series data to one variable" ViewCount="64" />
  <row AnswerCount="0" Body="&lt;p&gt;I would like to check whether my rats show motor recovery over time (i.e. a negative trend in errors made). Which test can i best use to see whether my results (motor errors made by the rats) show a linear increase or decrease over time? My results are all non-parameterically distributed. Thanks!&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-05-14T23:55:04.780" Id="97773" LastActivityDate="2014-05-14T23:55:04.780" OwnerUserId="39768" PostTypeId="1" Score="0" Tags="&lt;variance&gt;&lt;error&gt;" Title="How to determine error rate over time?" ViewCount="14" />
  <row AnswerCount="2" Body="&lt;p&gt;I was trying to understand when its better to model some random variable as being distributed by a Poisson Distribution or when being modeled as a Binomial Distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;I was reading the following article:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.umass.edu/wsp/resources/poisson/&quot; rel=&quot;nofollow&quot;&gt;http://www.umass.edu/wsp/resources/poisson/&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;but was not sure if I appreciated it.&lt;/p&gt;&#10;&#10;&lt;p&gt;What kind of structure does your data need, for it to be better approximated by a Poisson rather than a Binomial? What assumptions are necessary to separate when to model by Poisson or by Binomial?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-15T00:50:41.397" FavoriteCount="1" Id="97778" LastActivityDate="2014-05-15T01:30:02.950" OwnerUserId="28986" PostTypeId="1" Score="2" Tags="&lt;distributions&gt;&lt;predictive-models&gt;&lt;model&gt;" Title="When do you know when modeling something with a binomial distribution is better than a Poisson?" ViewCount="265" />
  
  <row Body="&lt;p&gt;First, here are some fake results to consider using &lt;code&gt;set.seed(8)&lt;/code&gt; before sampling &lt;code&gt;Living&lt;/code&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Call:&#10;glm(formula = Living ~ Bay_Dist + Dead_Dist, family = binomial(link = &quot;probit&quot;), &#10;data = mydata)&#10;&#10;Deviance Residuals: &#10;Min       1Q   Median       3Q      Max  &#10;-1.7826  -1.0012   0.7055   0.8691   1.4256  &#10;&#10;Coefficients:&#10;              Estimate Std. Error z value Pr(&amp;gt;|z|)&#10;(Intercept)  1.5650207  1.5138702   1.034    0.301&#10;Bay_Dist    -0.0002185  0.0040025  -0.055    0.956&#10;Dead_Dist   -0.0159294  0.0133800  -1.191    0.234&#10;&#10;(Dispersion parameter for binomial family taken to be 1)&#10;&#10;Null deviance: 13.460  on 9  degrees of freedom&#10;Residual deviance: 11.793  on 7  degrees of freedom&#10;AIC: 17.793&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In this sample it would seem the effects are pretty weak (small estimates, similar in size to their standard errors), but we can't say with any certainty that the effects really are weak without more data. Nothing is crossing any conventional significance thresholds (for whatever those are worth), but this is to be expected with such a small sample. Distance to the nearest dead crown is doing more for the model than distance to the nearest bay tree though (larger &lt;em&gt;z&lt;/em&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;One thing you could do is check product terms (squared/cubed predictors and interactions), but this will cost precious degrees of freedom in a small sample. As for plotting, I'd recommend &lt;code&gt;ggplot2&lt;/code&gt;. I'm somewhat new to that package myself, so I'll offer a very simple plot:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;require(ggplot2);ggplot(mydata,aes(x=Dead_Dist,y=Living))+&#10;geom_point(position=position_jitter(width=0,height=.04))+    #Jittering helps reduce overlap&#10;stat_smooth(method='glm',family=binomial(link='probit'))+&#10;scale_y_continuous('Probability of dead crown')+&#10;scale_x_continuous('Distance to nearest dead crown')&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Dq7PX.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;The blue line is just the model of &lt;code&gt;Living~Dead_Dist&lt;/code&gt;; this version doesn't control &lt;code&gt;Bay_Dist&lt;/code&gt;. The black dots are the observations. I've only jittered them vertically (they're all 0 or 1, and it's not hard to tell which) because their horizontal position is meaningful. The dark area is the 95% &lt;a href=&quot;https://en.wikipedia.org/wiki/Confidence_and_prediction_bands&quot; rel=&quot;nofollow&quot;&gt;confidence band&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-05-15T01:26:36.813" Id="97786" LastActivityDate="2014-05-15T01:26:36.813" OwnerUserId="32036" ParentId="97774" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;First how to interpret the results. In the Probit model, you model the probability of success $\pi=\Phi(x'\beta)$, where $\Phi$ is the cumulative normal distribution. In other words $\Phi^{-1}(\pi)=x'\beta$. So the interpretation of the regression coefficients are a little bit weird! For example let's look at the summary of your model&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; summary(myprobit)&#10;&#10;Call:&#10;glm(formula = Living ~ Bay_Dist + Dead_Dist, family = binomial(link = &quot;probit&quot;), &#10;    data = mydata1)&#10;&#10;Deviance Residuals: &#10;    Min       1Q   Median       3Q      Max  &#10;-1.5314  -0.8864  -0.4853   0.9984   1.6558  &#10;&#10;Coefficients:&#10;             Estimate Std. Error z value Pr(&amp;gt;|z|)&#10;(Intercept)  1.557554   1.509486   1.032    0.302&#10;Bay_Dist    -0.002704   0.003895  -0.694    0.488&#10;Dead_Dist   -0.017399   0.014683  -1.185    0.236&#10;&#10;(Dispersion parameter for binomial family taken to be 1)&#10;&#10;    Null deviance: 13.460  on 9  degrees of freedom&#10;Residual deviance: 11.707  on 7  degrees of freedom&#10;AIC: 17.707&#10;&#10;Number of Fisher Scoring iterations: 5&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Here the estimated coefficient for &lt;code&gt;Dead_Dist&lt;/code&gt; is -0.017399. What does that means? Well based on what I wrote above (i.e. $\Phi^{-1}(\pi)=x'\beta$) it means that if you increase the nearest dead crown by one unit (while keeping Bay_Dist variable unchanged) then $\Phi^{-1}$ of the probability that we observe a dead crown deceases by 0.017399. This is as I said a little bit weird but we need to live with it! &#10;Now lets find the estimated probabilities and try to plot them in 3D.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; p=predict(myprobit, type = &quot;response&quot; )&#10;&amp;gt; library(scatterplot3d)&#10;&amp;gt; scatterplot3d(Bay_Dist,Dead_Dist,p,pch=19) &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Zj9o4.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-15T01:32:39.533" Id="97787" LastActivityDate="2014-05-15T01:43:02.993" LastEditDate="2014-05-15T01:43:02.993" LastEditorUserId="13138" OwnerUserId="13138" ParentId="97774" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;Regularization is designed to combat overfitting, but not aid in gradient descent convergence.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you are minimizing a function $J$ parameterized by vector $\theta$ and where each element of $\theta$ is identified by $\theta_j$, (i.e. minimize $J(\theta)$).&lt;/p&gt;&#10;&#10;&lt;p&gt;Then the basic idea in batch gradient descent is to iterate until convergence by computing a new value of $\theta$ from the previous one in the following way.  Updated each $\theta_j$ simultaneously with the formula.&lt;/p&gt;&#10;&#10;&lt;p&gt;$\theta_j := \theta_j - \alpha\frac{\partial }{\partial \theta_j}J(\theta)$&lt;/p&gt;&#10;&#10;&lt;p&gt;That $\alpha$ term is called the learning rate.  It's arbitrary and if it's very small then the algorithm will converge slowly which will make the algorithm take a long time, but if it's too large, then what can happen is exactly what you are experiencing.  In this case $\theta$ will be updated in the right direction, but will go too far and jump past the minimum or it can even climb out and increase.&lt;/p&gt;&#10;&#10;&lt;p&gt;The remedy is to simply decrease $\alpha$ until it doesn't happen.  A sufficiently small learning rate guarantees that $J(\theta)$ will decrease on every iteration.  The trick is to determine what value of $\alpha$ is a good one that allows fast convergence but avoids non-convergence.  &lt;/p&gt;&#10;&#10;&lt;p&gt;A useful approach is to plot $J(\theta)$ while the algorithm is running to observe how it decreases.  Start with a small value (e.g. 0.01) increase it if it appears to result in slow convergence or decrease it still further in the case of non-convergence.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-15T07:34:59.840" Id="97806" LastActivityDate="2014-05-15T07:34:59.840" OwnerUserId="42362" ParentId="55992" PostTypeId="2" Score="2" />
  
  
  
  
  
  
  
  
  <row AcceptedAnswerId="97892" AnswerCount="2" Body="&lt;p&gt;I'm testing the effect of four different altitudes (0m, 1000m, 2000m and 3000m) on repeated sprint performance (6 sprints at each altitude). &#10;I have 8 test subjects who will be tested at each altitude. That means I'm getting 8x4x6=192 sets of data. &#10;What I want to investigate is the difference in performance at the 4 different altitudes and wether or not there actually IS a difference. How do I best represent this and what would be the relevant statistical tests to run? &#10;To me it is a bit confusing since I have multiple conditions, and I want to compare each condition with the other conditions (but I'm not sure about the best way to do this). &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance for your help. Let me know if I can clarify. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-15T15:49:30.607" Id="97863" LastActivityDate="2014-05-15T21:44:45.143" OwnerUserId="26205" PostTypeId="1" Score="0" Tags="&lt;correlation&gt;&lt;data-visualization&gt;" Title="One group, four conditions and six tests. Testing the effect of altitude" ViewCount="47" />
  <row Body="&lt;p&gt;Essentially, this seems to be a question about estimation. Does Method A provide similar scores as Method B &amp;amp; C do (The second point made by Alexis above)?&lt;/p&gt;&#10;&#10;&lt;p&gt;I would approach this by looking at a graph of Method means (if the distributions seem normal) plotted with confidence intervals. This is a &lt;strong&gt;very simple&lt;/strong&gt; approach, and here's how to do that in R. Note that you would also be interested in the variances of each method: Perhaps your method provides scores that are much more/less variable than other methods. Looking at means will answer the simple question of similarity in the average score between methods.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(dplyr)&#10;library(reshape2)&#10;library(ggplot)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Melt data into long form, then get summaries (using reshape2 and dplyr).  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;data_long = melt(data, id=&quot;Subject&quot;)&#10;methods = group_by(data_long, variable)&#10;d_summary = summarise(methods, mean = mean(value), sd = sd(value), n = n(), se = sd/sqrt(n), ci = qt(.975, n-1) * se)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Then we just plot these values (with ggplot2).&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ggplot(d_summary, aes(x = variable, y = mean)) + geom_pointrange(aes(ymin = mean - ci / 2, ymax = mean + ci / 2))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-05-15T15:56:06.637" Id="97866" LastActivityDate="2014-05-15T15:56:06.637" OwnerUserId="44527" ParentId="97854" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;You may find it easier to use the truncated power basis for cubic regression splines, using the R &lt;code&gt;rms&lt;/code&gt; package.  Once you fit the model you can retrieve the algebraic representation of the fitted spline function using the &lt;code&gt;Function&lt;/code&gt; or &lt;code&gt;latex&lt;/code&gt; functions in &lt;code&gt;rms&lt;/code&gt;.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-05-15T16:52:46.570" Id="97876" LastActivityDate="2014-05-15T16:52:46.570" OwnerUserId="4253" ParentId="97845" PostTypeId="2" Score="2" />
  <row AnswerCount="1" Body="&lt;p&gt;I have a matrix with about 550k elements (2500 x 220) with 100k values known and the rest are unknown.&lt;/p&gt;&#10;&#10;&lt;p&gt;Would it make sense to use matrix completion in this case, or are there too many values which are missing? What should I be careful of when I use matrix completion with it, if there are enough values there?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-05-15T16:54:56.500" Id="97877" LastActivityDate="2014-05-15T19:27:52.570" OwnerUserId="45616" PostTypeId="1" Score="1" Tags="&lt;missing-data&gt;&lt;matrix&gt;" Title="can matrix completion work in the presence of many missing values?" ViewCount="47" />
  
  <row AcceptedAnswerId="111123" AnswerCount="2" Body="&lt;p&gt;I am looking for reference for a high order MA model. Shall be extremely grateful for a reference.&lt;/p&gt;&#10;" ClosedDate="2014-08-13T17:53:16.647" CommentCount="2" CreationDate="2014-05-15T17:45:58.470" Id="97884" LastActivityDate="2014-08-13T16:07:33.737" LastEditDate="2014-08-13T16:07:33.737" LastEditorUserId="28545" OwnerUserId="28545" PostTypeId="1" Score="0" Tags="&lt;matlab&gt;&lt;references&gt;&lt;moving-average&gt;" Title="Reference required for a Moving Average Model" ViewCount="123" />
  <row Body="&lt;p&gt;These questions are on very different levels. Taking them in different order, &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Question 2.&lt;/strong&gt; Chi-square is a Pearson III distribution and the inverse gamma a Pearson V distribution, so some confusion there. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Three-parameter inverse gamma.&lt;/strong&gt; The inverse gamma is usually cited in a two-parameter version. A three-parameter version that seems natural to your problem is in terms of value $-$ minimum value, i.e. a location parameter is added for some minimum size, which on physical grounds would usually be expected to be positive. Adding that third parameter makes the inverse gamma more difficult to fit, unless we are just ad hoc and use the observed minimum value. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Double Pareto.&lt;/strong&gt; I'm not clear exactly what distribution this is. The name of Pareto has been linked to so much, mostly stuff he never touched. Can you cite a formula? &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Practices with landslide data.&lt;/strong&gt; I've found the small fraction [NB!] of the landslide literature I've sampled opaque on exactly how distributions were fitted and cavalier about whether it makes a difference how you do it. I've not noted, for example, much awareness of, or interest in, fitting by maximum likelihood. Frequently (pun intended) people bin the data and then fit density functions by some kind of least squares. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Question 1. Textbook?&lt;/strong&gt; A suitable book in this territory seems elusive. A good book would show SPICE: respecting the Science underlying different distributions, the Probability theory entailed, Inference procedures arising (e.g. hypothesis tests), the Computing (how to do it!) that is crucial, and exhibit Examples based on real data. Even the encyclopedic works of N.L. Johnson and S. Kotz, now passing to a younger generation of authors, notably N. Balakrishnan, is strong on P and I but misses out most of the S, C, and E. That may seem unfair and ungracious given the extraordinary effort that went into them, but there you go. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Question 3. Code.&lt;/strong&gt; I care very much how to program this, but that's off-topic here. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Gratuitous extra.&lt;/strong&gt; Some of the literature seems naive in seeking simple distributions for messy data (often landslide sets are highly heterogeneous e.g. in terms of mechanism). &lt;/p&gt;&#10;" CommentCount="8" CreationDate="2014-05-15T17:50:22.103" Id="97885" LastActivityDate="2014-05-15T17:50:22.103" OwnerUserId="22047" ParentId="97665" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;You need repeated measures analysis. You need one variable for the subject (lets say subject with values 1 to 8) one more variable to express the altitude (lets say altitude with values 0, 1, 2, 3) and one per sprint repeat (lets say sprint1, sprint2, ..., sprint6).&lt;/p&gt;&#10;&#10;&lt;p&gt;So, your data will have 8 variables with 32 values each (8 subjects to 4 different altitudes). Now, it is a nice idea to type in the data in Excel or Calc in the above format and save in a separate file (8 columns and 32 rows).&lt;/p&gt;&#10;&#10;&lt;p&gt;For the statistical analysis: In case you are using SPSS then you need to copy and paste the data from Excel or Calc to SPSS and use the procedure provided in &lt;em&gt;Analyze  General Linear Model  Repeated Measure&lt;/em&gt;. You may find many nice tutorials in the web.&lt;/p&gt;&#10;&#10;&lt;p&gt;In case you are using R project then first type in your data using the following commands&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;subject = as.factor(c(1, 1, 1, 1, 2, 2, 2, 2, .. , 8, 8, 8, 8))&#10;altitude = as.factor(c(0, 1, 2, 3, 0, 1, 2, 3,  .. ,0, 1, 2, 3))&#10;sprint1 = c(time 1 for subject 1 and altitude 0,  ., time 1 for subject 8 and altitude 3)&#10;....&#10;sprint6 = c(time 6 for subject 1 and altitude 0,  ., time 6 for subject 8 and altitude 3)&#10;&#10;dataF = data.frame(subject, altitude, sprint1, sprint2, sprint3, sprint4, sprint5, sprint6)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Then, define the time factor&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;time = factor(rep(&quot;sprint_time_measurement&quot;, 6), levels=c(&quot;sprint1&quot;, &quot;sprint2&quot;, &quot;sprint3&quot;, &quot;sprint4&quot;, &quot;sprint5&quot;, &quot;sprint6&quot;))&#10;time[1] = &quot;sprint1&quot;; time[2] = &quot;sprint2&quot;; time[3] = &quot;sprint3&quot;;time[4] = &quot;sprint4&quot;;time[5] = &quot;sprint5&quot;;time[6] = &quot;sprint6&quot;&#10;idata &amp;lt;- data.frame(time)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And, finally run the analysis with the following commands&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;mod.ok &amp;lt;- lm(cbind(sprint1, sprint2, sprint3, sprint4, sprint5, sprint6) ~  subject * altitude, data=dataF)&#10;library(car)&#10;(av.ok &amp;lt;- Anova(mod.ok, idata=idata, idesign=~time, type = &quot;III&quot;)) &#10;summary(av.ok, multivariate=FALSE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Normally, the above will provide the necessary output for you. The type = &quot;III&quot; option is not necessary, keep it if you need to have similar to the default SPSS output (however keep in mind that this is not necessarily the correct choice)&lt;/p&gt;&#10;&#10;&lt;p&gt;The first thing you will look is for the interaction between the three factors (time, subject, altitude). If not significant interaction appear then look to the partial interactions between each two factors and if no significant interactions appear then look into the effects of each factor separately.&lt;/p&gt;&#10;&#10;&lt;p&gt;If interaction is significant then the rest analysis is a little bit complicated. For SPSS you should use the syntax editor (the procedure described &lt;a href=&quot;http://www.tqmp.org/Content/vol08-1/p001/p001.pdf&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;) while for R, I would suggest (with some caution since I never did that) the &lt;a href=&quot;http://www.bioconductor.org/packages/release/bioc/vignettes/lmdme/inst/doc/lmdme-vignette.pdf&quot; rel=&quot;nofollow&quot;&gt;lmdme&lt;/a&gt; (other suggestions are welcome!)&lt;/p&gt;&#10;&#10;&lt;p&gt;Hope the above will help you. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-05-15T18:11:30.417" Id="97892" LastActivityDate="2014-05-15T21:44:45.143" LastEditDate="2014-05-15T21:44:45.143" LastEditorUserId="27608" OwnerUserId="27608" ParentId="97863" PostTypeId="2" Score="0" />
  <row AnswerCount="2" Body="&lt;p&gt;I have a dataset with a binary (survival) response variable and 3 explanatory variables (&lt;code&gt;A&lt;/code&gt; = 3 levels, &lt;code&gt;B&lt;/code&gt; = 3 levels, &lt;code&gt;C&lt;/code&gt; = 6 levels). In this dataset, the data is well balanced, with 100 individuals per &lt;code&gt;ABC&lt;/code&gt; category. I already studied the effect of these &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;B&lt;/code&gt;, and &lt;code&gt;C&lt;/code&gt; variables with this dataset; their effects are significant.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a subset. In each &lt;code&gt;ABC&lt;/code&gt; category, 25 of the 100 individuals, of which approximately half are alive and half are dead (when less than 12 are alive or dead, the number was completed with the other category), were further investigated for a 4th variable (&lt;code&gt;D&lt;/code&gt;). I see three problems here: &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;I need to weight the data the rare events corrections described in &lt;a href=&quot;http://pan.oxfordjournals.org/content/9/2/137.abstract&quot; rel=&quot;nofollow&quot;&gt;King and Zheng (2001)&lt;/a&gt; to take into account the approximate 50% - 50% is not equal to 0/1 proportion in the bigger sample. &lt;/li&gt;&#10;&lt;li&gt;This non-random sampling of 0 and 1 leads to a different probability for individuals to be sampled in each of the &lt;code&gt;ABC&lt;/code&gt; categories, so I think I have to use true proportions from each category rather than the global proportion of 0/1 in the big sample.&lt;/li&gt;&#10;&lt;li&gt;This 4th variable has 4 levels, and the data are really not balanced in these 4 levels (90% of the data is within 1 of these levels, say level &lt;code&gt;D2&lt;/code&gt;).&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I have read the King and Zheng (2001) paper carefully, as well as &lt;a href=&quot;http://stats.stackexchange.com/questions/6067/does-an-unbalanced-sample-matter-when-doing-logistic-regression&quot;&gt;this CV question&lt;/a&gt; that led me to King and Zheng (2001) paper, and later &lt;a href=&quot;http://stats.stackexchange.com/questions/96999/frequency-weights-rare-events-and-logistic-regression&quot;&gt;this other one&lt;/a&gt; that led me to try the &lt;code&gt;logistf&lt;/code&gt; package (I use R).&#10;I tried to apply what I understood from King and Zheng (2001), but I am not sure what I did is right. I understood there are two methods:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;For the prior correction method, I understood you only correct the intercept. In my case, the intercept is the &lt;code&gt;A1B1C1&lt;/code&gt; category, and in this category survival is 100%, so survival in the big dataset and the subset are the same, and therefore the correction changes nothing. I suspect this method should not apply to me anyway, because I do not have an overall true proportion, but a proportion for each category, and this method ignores that.&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;For the weighting method: I calculated &lt;em&gt;w&lt;sub&gt;i&lt;/sub&gt;&lt;/em&gt;, and from what I understood in the paper: &quot;All&#10;researchers need to do is to calculate &lt;em&gt;w&lt;sub&gt;i&lt;/sub&gt;&lt;/em&gt; in Eq. (8), choose it as the weight in their computer program, and then run a logit model&quot;. So I first ran my &lt;code&gt;glm&lt;/code&gt; as:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;glm(R~ A+B+C+D, weights=wi, data=subdata, family=binomial)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I am not sure I should include &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;B&lt;/code&gt;, and &lt;code&gt;C&lt;/code&gt; as explanatory variables, since I normally expect them to have no effect on survival in this subsample (each category contains about the 50% dead and alive). Anyway, it should not change the output a lot if they are not significant. With this correction, I get a good fit for level &lt;code&gt;D2&lt;/code&gt; (the level with most of individuals), but not at all for others levels of &lt;code&gt;D&lt;/code&gt; (&lt;code&gt;D2&lt;/code&gt; preponderates). See the top right graph:&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/AO3ml.png&quot; alt=&quot;Fits&quot;&gt;&#10;  Fits of a non-weighted &lt;code&gt;glm&lt;/code&gt; model and of a &lt;code&gt;glm&lt;/code&gt; model weighted with &lt;em&gt;w&lt;sub&gt;i&lt;/sub&gt;&lt;/em&gt;. Each dot represents one category. &lt;code&gt;Proportion in the big dataset&lt;/code&gt; is the true proportion of 1 in the &lt;code&gt;ABC&lt;/code&gt; category in the big dataset, &lt;code&gt;Proportion in the sub dataset&lt;/code&gt; is the true proportion of 1 in the &lt;code&gt;ABC&lt;/code&gt; category in the subdataset, and &lt;code&gt;Model predictions&lt;/code&gt; are the predictions of &lt;code&gt;glm&lt;/code&gt; models fitted with the subdataset. Each &lt;code&gt;pch&lt;/code&gt; symbol represents a given level of &lt;code&gt;D&lt;/code&gt;. Triangles are level &lt;code&gt;D2&lt;/code&gt;.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Only later when seeing there is a &lt;code&gt;logistf&lt;/code&gt;, I though this is perhaps not that simple. I am not sure now. When doing &lt;code&gt;logistf(R~ A+B+C+D, weights=wi, data=subdata, family=binomial)&lt;/code&gt;,&#10;I get estimates, but the predict function does not work, and the default model test returns  infinite chi squared values (except one) and all p-values = 0 (except 1).&lt;/p&gt;&#10;&#10;&lt;p&gt;Questions:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Did  I properly understand King and Zheng (2001)? (How far am I from understanding it?)&lt;/li&gt;&#10;&lt;li&gt;In my &lt;code&gt;glm&lt;/code&gt; fits, &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;B&lt;/code&gt;, and &lt;code&gt;C&lt;/code&gt; have significant effects. All this means is that I deparse a lot from the half / half proportions of 0 and 1 in my subset and differently in the different &lt;code&gt;ABC&lt;/code&gt; categories  isn't that right?&lt;/li&gt;&#10;&lt;li&gt;Can I apply King and Zheng's (2001) weighting correction despite the fact that I have a value of tau and a value of $\bar y$ for each &lt;code&gt;ABC&lt;/code&gt; category instead of global values?&lt;/li&gt;&#10;&lt;li&gt;Is it an issue that my &lt;code&gt;D&lt;/code&gt; variable is so unbalanced, and if it is, how can I handle it? (Taking into account I will already have to weight for the rare event correction...Is &quot;double weighting&quot;, i.e. weighting the weights, possible?)&#10;Thanks!&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;: See what happens if I remove A, B and C from the models. I do not understand why there is such differences.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/gCySf.png&quot; alt=&quot;Fits2&quot;&gt;&#10;  Fits without A, B, and C as explanatory variables in models&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="0" CreationDate="2014-05-15T19:08:00.490" Id="97899" LastActivityDate="2014-06-21T13:06:46.397" LastEditDate="2014-05-17T22:32:19.627" LastEditorUserId="45605" OwnerUserId="45605" PostTypeId="1" Score="6" Tags="&lt;logistic&gt;&lt;weighted-regression&gt;&lt;case-control-study&gt;&lt;rare-events&gt;" Title="How to make the rare events corrections described in King and Zheng (2001)?" ViewCount="170" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a two-dimensional data set that I would like to model as the outer product of two vectors (or two unit vectors times a constant). I need to find a way to a) figure out what these two vectors are and b) how well the data fits the model. The caveat is that I need to do this in the presence of possible missing values.&lt;/p&gt;&#10;&#10;&lt;p&gt;I know I can use PCA to find a rank-one matrix approximating my data as well as how good that approximation is, if the matrix is complete. I'm not sure how to do it with missing values.&lt;/p&gt;&#10;&#10;&lt;p&gt;This seems to me like some kind of degenerate form of collaborative filtering, with only one parameter per row/column. I think there has to be an easier way to solve it, though.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-15T19:31:12.373" Id="97906" LastActivityDate="2014-05-15T19:31:12.373" OwnerUserId="45629" PostTypeId="1" Score="1" Tags="&lt;pca&gt;" Title="Approximating a matrix as the outer product of two vectors" ViewCount="29" />
  
  
  <row AnswerCount="3" Body="&lt;p&gt;I understand how to test programs that are either right or wrong. But what if permissible some inaccuracy? How to distinguish a bug in the implementation of bad classifier? Naive solution (baseline)? But it seems that it will help to find the only really bad solution. I think you understand what I mean. &lt;/p&gt;&#10;&#10;&lt;p&gt;What are the ways of testing programs without the correct output?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-15T23:06:28.633" FavoriteCount="3" Id="97927" LastActivityDate="2014-05-21T23:09:26.417" OwnerUserId="28377" PostTypeId="1" Score="2" Tags="&lt;machine-learning&gt;&lt;hypothesis-testing&gt;&lt;model-selection&gt;" Title="How to test a program that uses machine learning" ViewCount="337" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I try to build a multi-linear regression model, several explanatory variables have impact from period1 to period2. I need to do data transformation before modeling.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's simplify the problem first: only one explanatory variable needs to be transformed.&#10;2 data transformation methods are taken into consideration:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;A carry-over rate:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;co &amp;lt;- function(variable, i){&#10;   var1 &amp;lt;- variable&#10;   for (p in (2:length(var1))){&#10;     var1[p] = var1[p-1] * i + var1[p]&#10;   }&#10;   return(var1)&#10;}&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;S-curve:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;sc &amp;lt;- function(variable, j, k){&#10;   var2 &amp;lt;- variable&#10;   for (q in (1:length(var2))){&#10;     var2[q] = 1 + j * exp(k * var2[q])&#10;   }&#10;  return(var2)&#10;}&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;My question is: &lt;strong&gt;how could I get the best values for i, j &amp;amp; k ?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;My initial idea is like this:&#10;I use &lt;code&gt;lm()&lt;/code&gt; and &lt;code&gt;stepAIC()&lt;/code&gt; (under &lt;code&gt;library(MASS)&lt;/code&gt;) to try out all possible combination of these 3 parameters and keep the combination which leads to the highest R square and the lowest AIC (no matter if there are insignificant predictors/coefficients).&#10;(Here, &lt;code&gt;BoxTidwell()&lt;/code&gt; (under &lt;code&gt;library(car)&lt;/code&gt;) is not used since I was told these 2 methods of transformation should be applied in the same time for a variable.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there better methodology to figure out the parameters? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-16T04:13:01.947" Id="97943" LastActivityDate="2014-05-16T05:47:33.923" LastEditDate="2014-05-16T05:47:33.923" LastEditorUserId="45644" OwnerUserId="45644" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;regression&gt;" Title="Try out method to estimate parameters for data transformation in a regression model" ViewCount="36" />
  <row AnswerCount="0" Body="&lt;p&gt;Consider two samples in AB testing with different structure. &lt;/p&gt;&#10;&#10;&lt;p&gt;For example for sample 1 we have:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;city1 - 55%&lt;/li&gt;&#10;&lt;li&gt;city2 - 45%&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;(55% of website visitors, who see design1, are from city1)&lt;/p&gt;&#10;&#10;&lt;p&gt;For sample 2 we have:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;city1 - 47%&lt;/li&gt;&#10;&lt;li&gt;city2 - 53%&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;The idea is to &quot;normalize&quot; both samples so they have same structure.&#10;I try just to multiply: $x_1/0.47*0.55$ but I think its a bad idea.   &lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;I try to search by keywords &quot;sample structure&quot; and &quot;sample composition&quot; but with no result.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;So the question is:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Can I get better result (more stable), if I take into account structure of samples?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;If yes - how to do it?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2014-05-16T04:26:47.213" Id="97944" LastActivityDate="2014-05-16T04:56:44.410" LastEditDate="2014-05-16T04:56:44.410" LastEditorUserId="27403" OwnerUserId="45647" PostTypeId="1" Score="0" Tags="&lt;sampling&gt;&lt;ab-test&gt;" Title="Compare samples with different structure in AB testing" ViewCount="15" />
  <row AnswerCount="1" Body="&lt;p&gt;I'm working with data from different accelerometer hardware. Each hardware has a different maximum range, different resolution at which data changes, and different minimum delay after which a new value is emitted. For example for one hardware, the maximum value it can obtain is 10240, emitted after a minimum of 10000 milliseconds, with an increment of 1.0 each time. For another hardware, the maximum value is 2, emitted after 0 milliseconds and incremented by a minimum of 0.038 each time. How cab I possibly normalize the data? Thanks&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-16T06:24:53.810" Id="97948" LastActivityDate="2014-05-16T12:24:35.537" OwnerUserId="29431" PostTypeId="1" Score="0" Tags="&lt;normalization&gt;" Title="Accelerometer normalization" ViewCount="57" />
  <row AnswerCount="0" Body="&lt;p&gt;I am applying some chaos tests in financial time series in R and I am interested to run a NEGM (Nychka, Ellner, Gallant, McAffrey) Test for a largest Lyapunov exponent based on Jacobian methods. Do you know any package that contains this test, or at least some implementation?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-05-16T13:40:48.260" Id="98957" LastActivityDate="2014-07-10T00:51:24.663" LastEditDate="2014-07-10T00:51:24.663" LastEditorUserId="805" OwnerUserId="45657" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;finance&gt;" Title="NEGM Implementation in R" ViewCount="18" />
  <row Body="&lt;p&gt;The short answer is that there can be very large differences in quantile values depending on how many data points you have and what the spread in the min and max values are. &lt;/p&gt;&#10;&#10;&lt;p&gt;Consider the following example in R:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# Create uniform distribution with 100 points between 0 and 10,000&#10;unif1 &amp;lt;- runif(100,0,10000)&#10;&#10;# Create uniform distribution with 100,000 points between 0 and 10,000&#10;unif2 &amp;lt;- runif(100000,0,10000)&#10;&#10;# Print the quntiles for unif1&#10;quantile(unif1)&#10;&#10;# Print the quntiles for unif2&#10;quantile(unif2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This yields the results&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; quantile(unif1)&#10;   0%       25%       50%       75%      100% &#10;222.9632 2276.7373 4078.1503 6862.3499 9962.6663 &#10;&#10;&amp;gt; quantile(unif2)&#10;   0%          25%          50%          75%         100% &#10;0.3043911 2507.0791144 4998.1681292 7509.9505275 9999.9719788 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;You can clearly see that unif2 is much closer to the true quintile values compared to unif1. I suggest playing a bit with the min and max values and also the number of points to see how these values change. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-05-16T14:33:12.143" Id="98965" LastActivityDate="2014-05-16T14:33:12.143" OwnerUserId="39953" ParentId="97898" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;That's what test sets are for! You need training data with examples &lt;code&gt;x&lt;/code&gt;and corresponding labels &lt;code&gt;y&lt;/code&gt; to begin with. Now instead of using everything for training, take a subset of the data, leave it out of training, and check the quality of the generated predictions &lt;code&gt;y'&lt;/code&gt; with respect to the proper labels &lt;code&gt;x&lt;/code&gt;. See &lt;a href=&quot;http://en.wikipedia.org/wiki/Cross-validation_%28statistics%29&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Cross-validation_(statistics)&lt;/a&gt; for more details.&lt;/p&gt;&#10;&#10;&lt;p&gt;OK you're asking for testing &quot;without the correct output&quot;, but it's hard for me to imagine how you want to train a machine learning classifier without any training data in the first place. That's why I think the trick of using your training data for that purpose is worth mentioning.&lt;/p&gt;&#10;&#10;&lt;p&gt;How well your findings on the test set correlate on the true accuracy on your unlabelled data of course is another question, but you can e.g. do multiple rounds of cross-validation (see Wikipedia link for specific patterns like $k$-fold or leave-one-out), or make sure that the distribution of your unknown data is similar to the training data (if it's not, you might have a problem anyway).&lt;/p&gt;&#10;&#10;&lt;p&gt;UPDATE&#10;To see if your implementation of a specific machine learning algorithm is correct, one way would be to find a paper describing its performance on an openly available dataset, and see if your implementation yields the same results. I've once tried this and got exactly the same performance figures (as in, the same number down to the third digit).&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-05-16T15:17:46.817" Id="98969" LastActivityDate="2014-05-17T17:03:15.553" LastEditDate="2014-05-17T17:03:15.553" LastEditorUserId="979" OwnerUserId="979" ParentId="97927" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Both of the previous answers are wrong. Package GBM uses &lt;code&gt;interaction.depth&lt;/code&gt; parameter as a number of splits it has to perform on a tree (starting from a single node). As each split increases the total number of nodes by 3 and number of terminal nodes by 2 (node $\to$ {left node, right node, NA node}) the total number of nodes in the tree will be $3*N+1$ and the number of terminal nodes $2*N+1$. This can be verified by having a look at the output of &lt;code&gt;pretty.gbm.tree&lt;/code&gt; function.&lt;/p&gt;&#10;&#10;&lt;p&gt;The behaviour is rather misleading, as the user indeed expects the depth to be the depth of the resulting tree. It is not.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-16T15:33:31.053" Id="98974" LastActivityDate="2014-05-16T16:10:28.607" LastEditDate="2014-05-16T16:10:28.607" LastEditorUserId="27403" OwnerUserId="45664" ParentId="16501" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;You should analyze all of the variables you are interested in together.  The relationships between your explanatory variables and the response variable may differ based on whether you are ignoring or controlling for the other variable, and the latter is certainly what you really want to know.  Although written in the context of linear regression, my answer here: &lt;a href=&quot;http://stats.stackexchange.com/a/78830/7290&quot;&gt;Is there a difference between 'controlling for' and 'ignoring' other variables in multiple regression?&lt;/a&gt; will help you understand this better.  &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-05-16T16:13:50.467" Id="98981" LastActivityDate="2014-05-16T16:13:50.467" OwnerUserId="7290" ParentId="98979" PostTypeId="2" Score="3" />
  
  
  
  
  
  <row Body="&lt;p&gt;If condition 2 is (as I think it is):&#10;$$E(x_2\mid x_1,z_2)\ne E(x_2\mid x_1)$$&#10;then it requires that $\theta\ne 0$ in:&#10;$$x_2=\delta_0+\delta_1 x_1+\theta z_2+r$$&#10;i.e. $z_2$ is &lt;em&gt;partially&lt;/em&gt; correlated with $x_2$ once the other exogenous variabile $x_1$ has been netted out (see &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0262232197&quot; rel=&quot;nofollow&quot;&gt;Wooldridge&lt;/a&gt;, 5.1.1)&lt;/p&gt;&#10;&#10;&lt;p&gt;If there is no $x_1$, just $x_2$, this reduces to your condition 2'. You need contidion 2 because, in general, in $y=\beta_0+\beta_1x_1+\beta_2x_2+\cdots+\beta_kx_k+u$, $z$ could be correlated to $x_k$ just because correlated to some $x_j$'s and &lt;em&gt;they&lt;/em&gt;, not $z$, are correlated to $x_k$ (this may happen even if correlation is not always transitive). As an extreme example, if $z=x_1+x_2+\cdots+x_{k-1}$, then by replacing $x_k$ with $z$ you get perfect multicollinearity.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-05-16T20:29:47.883" Id="99007" LastActivityDate="2014-05-16T20:29:47.883" OwnerUserId="44965" ParentId="99002" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="99024" AnswerCount="1" Body="&lt;p&gt;Given i.i.d. draws $x_1,...,x_n$ from $X$, where:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;$X$ has a finite mean $E[X]=\mu &amp;lt; \infty$,&lt;/li&gt;&#10;&lt;li&gt;$X$ is symmetric about its mean, meaning $f_X(\mu+c)=f_X(\mu-c)$ for all $c$,&lt;/li&gt;&#10;&lt;li&gt;The probability density function $f_X$ is not otherwise known. &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Is it possible to prove the following?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Proposition.&lt;/strong&gt; The MLE for the mean of $X$ is the sample mean, $\hat \mu_{MLE}=\bar x = \sum_{i=1}^n x_i$.&lt;/p&gt;&#10;&#10;&lt;p&gt;A proof or a counterexample would be great. I am willing to additionally assume that $X$ has a finite variance $Var[X]=\sigma^2 &amp;lt; \infty$, or other common basic assumptions, if that becomes necessary for the proposition to hold, or if it greatly simplifies the proof.&lt;/p&gt;&#10;&#10;&lt;p&gt;I suspect that it may be possible to use the invariance of the MLE to transformations of the data to prove this, but it might follow from simpler facts about the sample mean.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-05-16T21:38:26.877" FavoriteCount="1" Id="99014" LastActivityDate="2014-05-17T01:14:38.257" OwnerUserId="45689" PostTypeId="1" Score="4" Tags="&lt;distributions&gt;&lt;maximum-likelihood&gt;" Title="MLE for mean of symmetric but otherwise unknown distribution" ViewCount="83" />
  <row Body="&lt;p&gt;Consider the single-parameter &lt;a href=&quot;http://en.wikipedia.org/wiki/Exponential_family&quot; rel=&quot;nofollow&quot;&gt;Exponential Family&lt;/a&gt; of distributions, i.e. distributions whose probability density (or mass) function can be written as&lt;/p&gt;&#10;&#10;&lt;p&gt;$$f(x) = h(x)\cdot \exp{\big\{\eta(\theta)T(x)-A(\theta)\big\}}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The log-likelihood from an i.i.d sample of size $n$ is then&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\tilde L = \sum_{i=1}^n\ln h(x_i) + \eta(\theta)\sum_{i=1}^nT(x_i) - nA(\theta)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and the derivative with respect to $\theta$ is &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac {\partial \tilde L}{\partial \theta}=\eta'(\theta)\sum_{i=1}^nT(x_i)-nA'(\theta) = 0$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\Rightarrow  \frac 1n \sum_{i=1}^nT(x_i) = \frac {A'(\hat \theta_{MLE})}{\eta'(\hat \theta_{MLE})}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;it is obvious from the above that, to arrive at &quot;the sample mean is the MLE for the mean&quot;, the involved functions must have suitable forms.  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Examples where the result holds&lt;/strong&gt;&lt;br&gt;&#10;1) For the Normal distribution (with known variance $\sigma^2$) : $T(x_i) = x_i/\sigma$, $A(\theta)=\mu^2 / 2\sigma^2 \Rightarrow A'(\theta) = \mu / \sigma^2$, $\eta(\theta) = \mu/\sigma\Rightarrow \eta'(\theta) = 1/\sigma$ &lt;/p&gt;&#10;&#10;&lt;p&gt;2) For the Bernoulli(p) distribution,  $T(x_i) = x_i$, $A(\theta) -\ln (1-p)\Rightarrow  A'(\theta) = 1/(1-p) $, $\eta (\theta) = \ln(p/(1-p)\Rightarrow \eta'(\theta) = 1/p(1-p)$&lt;/p&gt;&#10;&#10;&lt;p&gt;In these cases, indeed the MLE for the mean is the sample mean.  It is perhaps easier to find counter-examples, as Whuber hinted.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-05-17T00:40:02.447" Id="99024" LastActivityDate="2014-05-17T01:14:38.257" LastEditDate="2014-05-17T01:14:38.257" LastEditorUserId="28746" OwnerUserId="28746" ParentId="99014" PostTypeId="2" Score="3" />
  <row AnswerCount="0" Body="&lt;p&gt;The life time of a bulb follows exponential distribution. Such types of examples frequently occurs in many areas of science and engineering. Is there any source of  practical data set of two exponential distributed data with a location and scale parameter? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-17T05:39:51.550" Id="99037" LastActivityDate="2014-05-17T08:33:02.037" LastEditDate="2014-05-17T08:33:02.037" LastEditorUserId="27403" OwnerUserId="45589" PostTypeId="1" Score="0" Tags="&lt;dataset&gt;&lt;exponential&gt;" Title="Exponential Distribution Data" ViewCount="39" />
  <row Body="" CommentCount="0" CreationDate="2014-05-17T07:42:10.550" Id="99044" LastActivityDate="2014-05-17T07:42:10.550" LastEditDate="2014-05-17T07:42:10.550" LastEditorUserId="-1" OwnerUserId="-1" PostTypeId="5" Score="0" />
  
  
  
  <row Body="&lt;p&gt;The key point is that the standardization method for Pearson residuals is to divide the difference between observed values $y_i$ and the fitted Poisson mean $\hat\mu_i$ by the &lt;em&gt;theoretical&lt;/em&gt; standard deviation implied by that fitted mean:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$r_i=\frac{y_i - \hat\mu_i}{\sqrt{\hat\mu_i}}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;So if the model is badly mis-specified the assumed relation $\operatorname{Var} \mu_i=\mu_i$ can be wildly inaccurate: you have over-dispersion as @probabilityislogic says; moreover the fitted means are much too large for high-carat stones, indicating the assumed linear relation between the log mean and carat is too simple.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-05-17T14:18:49.587" Id="99076" LastActivityDate="2014-05-17T17:38:22.797" LastEditDate="2014-05-17T17:38:22.797" LastEditorUserId="17230" OwnerUserId="17230" ParentId="99065" PostTypeId="2" Score="6" />
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;The gist of propensity score matching, as I understand it, is as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;You want to estimate the average treatment effect (ATE) of a treatment on some outcome.  However, if you simply calculate the difference between the average outcome of the treated and untreated groups, this may be a biased estimate of ATE if factors that influence the outcome variable also influence the probability of receiving the treatment in the first place.&lt;/p&gt;&#10;&#10;&lt;p&gt;Propensity score matching minimizes this problem by matching treated and untreated observations with similar probabilities of receiving treatment (via logistic regression of treatment status on covariates), and then estimates ATE as the average difference in outcomes among the matched pairs.&lt;/p&gt;&#10;&#10;&lt;p&gt;So far, so good?  This sounds fine conceptually, but where I have trouble is in seeing how the actual mechanics lead to different outcomes for matched as opposed to naive ATE estimation.&lt;/p&gt;&#10;&#10;&lt;p&gt;To illustrate:&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose four individuals, $X_a, X_b, Y_a, Y_b$, where $X$ indicates that the person did not receive the treatment, $Y$ indicates that the person did receive the treatment, the $a$s have similar covariate values to each other, and the $b$s have similar covariate values to each other.&lt;/p&gt;&#10;&#10;&lt;p&gt;And suppose $F(^*)$ denotes the outcome for which you are attempting to estimate the effect of treatment.&lt;/p&gt;&#10;&#10;&lt;p&gt;You first estimate ATE naively, looking at the simple difference in the the average outcome of the treated and the average outcome of the untreated.&lt;/p&gt;&#10;&#10;&lt;p&gt;Naive ATE estimate: $\frac{F(Y_a)+F(Y_b)}2 - \frac{F(X_a)+F(X_b)}2$&lt;/p&gt;&#10;&#10;&lt;p&gt;Next, you estimate ATE by first matching on propensity score.  As mentioned, the subscript indexing each individual reflects covariate values, and so after we run the logistic regression (ignoring sample size issues), we find that $X_a$ and $Y_a$ have similar propensity scores to each other, while $X_b$ and $Y_b$ have similar propensity scores to each other.  We proceed to look at the average difference among these matched pairs.&lt;/p&gt;&#10;&#10;&lt;p&gt;Matched ATE estimate: $\{[F(Y_b)-F(X_b)] + [F(Y_a)-F(X_a)]\}/2$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;The problem is that both the naive ATE estimate and the matched ATE estimate are mathematically equivalent!&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Now I'm sure I've made a mistake in my formulation of the matched ATE estimate.  My question is, where did I go wrong?&lt;/p&gt;&#10;&#10;&lt;p&gt;P.S: I am aware that propensity score matching can also be used to drop observations that don't have suitable matches, but I want to ignore that case because my understanding of propensity score matching is that it should lead to a different estimate than a naive estimation even if all observations are matched.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2014-05-18T00:03:17.360" Id="99117" LastActivityDate="2014-05-18T00:51:48.357" LastEditDate="2014-05-18T00:51:48.357" LastEditorUserId="45732" OwnerUserId="45732" PostTypeId="1" Score="5" Tags="&lt;average&gt;&lt;treatment-effect&gt;&lt;propensity-scores&gt;" Title="Propensity Score Matching  How do the mechanics lead to a different result than unmatched?" ViewCount="120" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;need to do time series analysis(TSA) and forecasting on over 100 customers in Hadoop environment(RHadoop), for each customer, it has its own time series which I can analyze individually, but when I do it in Hadoop, is it possible that each customer's time series might be broken into pieces(i.e it doesn't have continuous time series from begin date to end date) - if possible, is there a way to prevent this from happening?&lt;/p&gt;&#10;&#10;&lt;p&gt;any example code of time series using R in Hadoop is highly appreciated&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-18T05:44:28.953" Id="99131" LastActivityDate="2014-05-18T05:44:28.953" OwnerUserId="41346" PostTypeId="1" Score="0" Tags="&lt;time-series&gt;&lt;rhadoop&gt;" Title="R time series analysis in Hadoop" ViewCount="59" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I have data showing outcomes of some treatment on different people grouped by sex and age (grouped into: infant, child, adult). I need to investigate if there is any difference in outcome among age groups or between sexes.&lt;/p&gt;&#10;&#10;&lt;p&gt;I used:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;model &amp;lt;- aov(data$outcome ~ data$sex*data$age)&#10;summary(model)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and found that neither sex nor age were significant on their own, but the interaction term was significant. &lt;/p&gt;&#10;&#10;&lt;p&gt;I read about the Tukey test so I tried:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;TukeyHSD(model)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and found that none of the differences were significant. Was this the right thing to do? Should my conclusion be that there is no significant difference or is there more I should do?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-18T11:28:55.777" FavoriteCount="1" Id="99150" LastActivityDate="2014-05-18T13:07:41.667" LastEditDate="2014-05-18T11:34:33.393" LastEditorUserId="930" OwnerUserId="40378" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;anova&gt;&lt;categorical-data&gt;&lt;tukey-hsd&gt;" Title="Should we conduct post-hoc tests when there is a significant interaction but no significant main effects?" ViewCount="59" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I'd appreciate reference books/online resources in learning how to apply A/B hypothesis testing for problems of the following type:&lt;/p&gt;&#10;&#10;&lt;p&gt;Given an initial strategy B with $X$ samples yields a success rate of $b$. Now say we are given two new strategies $B_1$ and $B_2$, one trial each with number of samples $X_1$ and $X_2$, and success rates $b_1$ and $b_2$, respectively. Is strategy $B_1$ better than $B_2$, with respect to the success rate $b$ in strategy B?&lt;/p&gt;&#10;&#10;&lt;p&gt;Further, in using measures like $p$-values, how much does our decision of $B_1$ vs $B_2$ change if we have now, say 1000 trials of both $B_1$ and $B_2$ each (so now a total of $1000\cdot X_1$ samples for $B_1$ and $1000\cdot X_2$ for $B_2$)?&lt;/p&gt;&#10;&#10;&lt;p&gt;Aside: how might the Neymann-Pearson lemma apply and help us with this type of decision question &lt;em&gt;in practice&lt;/em&gt;?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-19T01:27:55.430" Id="99215" LastActivityDate="2014-05-19T06:48:27.853" LastEditDate="2014-05-19T06:48:27.853" LastEditorUserId="27403" OwnerDisplayName="Alfred Gardner" PostTypeId="1" Score="0" Tags="&lt;hypothesis-testing&gt;&lt;ab-test&gt;" Title="Hypothesis testing question, references" ViewCount="33" />
  <row Body="&lt;p&gt;It is a matter of &lt;strong&gt;signal-to-noise&lt;/strong&gt;. Euclidean distance, due to the squared terms, is particular sensitive to noise; but even Manhattan distance and &quot;fractional&quot; (non-metric) distances suffer.&lt;/p&gt;&#10;&#10;&lt;p&gt;I found the studies in this article very enlightening:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Zimek, A., Schubert, E. and Kriegel, H.-P. (2012),&lt;br&gt; &lt;strong&gt;A survey on unsupervised outlier detection in high-dimensional numerical data.&lt;/strong&gt;&lt;br /&gt; Statistical Analy Data Mining, 5: 363387. doi: &lt;a href=&quot;http://onlinelibrary.wiley.com/doi/10.1002/sam.11161/abstract&quot;&gt;10.1002/sam.11161&lt;/a&gt;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;It revisits the observations made in e.g. On the Surprising Behavior of Distance Metrics in High Dimensional Space by Aggarwal, Hinneburg and Keim mentioned by @Pat. But it also shows how out synthetic experiments are misleading and that in fact &lt;strong&gt;high-dimensional data &lt;em&gt;can&lt;/em&gt; become easier&lt;/strong&gt;. If you have a lot of (redundant) signal, and the new dimensions add little noise.&lt;/p&gt;&#10;&#10;&lt;p&gt;The last claim is probably most obvious when considering duplicate dimensions. Mapping your data set $x,y \rightarrow x,y,x,y,x,y,x,y,...,x,y$ increases representative dimensionality, but does not at all make Euclidean distance fail. (See also: &lt;a href=&quot;https://en.wikipedia.org/wiki/Intrinsic_dimension&quot;&gt;intrinsic dimensionality&lt;/a&gt;)&lt;/p&gt;&#10;&#10;&lt;p&gt;So in the end, it still depends on your data. If you have a lot of useless attributes, Euclidean distance will become useless. If you could easily embed your data in a low-dimensional data space, then Euclidean distance should also work in the full dimensional space. In particular for &lt;em&gt;sparse&lt;/em&gt; data, such as TF vectors from text, this does appear to be the case that the data is of much lower dimensionality than the vector space model suggests.&lt;/p&gt;&#10;&#10;&lt;p&gt;See also this reply I gave to an earlier question:&#10;&lt;a href=&quot;http://stats.stackexchange.com/a/29647/7828&quot;&gt;http://stats.stackexchange.com/a/29647/7828&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-19T08:13:11.923" Id="99225" LastActivityDate="2014-05-19T08:13:11.923" OwnerUserId="7828" ParentId="99171" PostTypeId="2" Score="19" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a set of data points distributed like a bell-shape curve with varying amplitudes and widths. I need to parameterize these data points and a Gaussian with amplitude, width and offset parameters function is just perfectly suited.  &lt;/p&gt;&#10;&#10;&lt;p&gt;My y-values are defined between at 8 different points between [-2.3561 3.1415] and I am interested fitting a Gaussian that is centered at 0.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;out      = amp.*exp( - (0.5*(X./sd).^2 )) + offset;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Optimizing the likelihood function following an initial estimation of these 3 parameters works generally very good. And this has been already discussed elsewhere in this site.&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem starts when my data points are described by a very shallow bell-shaped profile, looking like a second order polynomial with a small coefficient, so that the highest and lowest points do not have so much of a difference.&lt;/p&gt;&#10;&#10;&lt;p&gt;In these cases the optimization procedure returns extremely high sigma and amplitude values. It basically tries to fit the data with the tip of an amazingly wide Gaussian, which by far exceeds the range of my x-values. These huge fitted values destroy my statistics, because ultimately I would like to do some statistics with these numbers comparing different groups. In these situations, I would like to have a small amplitude and relatively wide width parameter returned from the optimization procedure and this would just do the job good enough.&lt;/p&gt;&#10;&#10;&lt;p&gt;I tried constrained optimization, trying to limit valid values for the parameters. But this also results in values that are densely accumulated on the boundary limits.&lt;/p&gt;&#10;&#10;&lt;p&gt;I was wondering what would be a general approach to tackle this kind of situations? If you have encountered similar situation I would be happy to hear about your experience... &lt;/p&gt;&#10;&#10;&lt;p&gt;Here are some examples of data:&#10;Gray Curve: Initial Estimates&#10;Red Curve: Fitted Gaussian &#10;Blue Curve: Data&lt;/p&gt;&#10;&#10;&lt;p&gt;data_01: This shows a situation where sigma and amp parameters are huge, even though a very small amplitude value would do the job good.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;-2.3562    1.2210&#10;   -1.5708   -1.6224&#10;   -0.7854   -0.1330&#10;         0   -0.1126&#10;    0.7854   -0.0745&#10;    1.5708    1.8017&#10;    2.3562   -0.1533&#10;    3.1416   -0.9268&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/AWkwl.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&#10;" CommentCount="10" CreationDate="2014-05-19T08:46:02.740" Id="99228" LastActivityDate="2014-05-19T09:38:33.417" LastEditDate="2014-05-19T09:38:33.417" LastEditorUserId="37640" OwnerUserId="37640" PostTypeId="1" Score="1" Tags="&lt;optimization&gt;&lt;curve-fitting&gt;" Title="Alternatives to constrained non-linear optimization: a case study with Gaussian curve fitting" ViewCount="86" />
  <row Body="&lt;p&gt;(This would be more appropriate for a comment but I do not have enough reputation to comment.)&lt;/p&gt;&#10;&#10;&lt;p&gt;I think you have to start by defining which kind of impact the measurement of the independent variable is expected to have on the dependent one. Can it be that it is merely a cumulative effect of some of the predictors? Or can it depend on how a predictor or group of predictors change with time? To test the former is much easier than the latter, and I think you need to define what you want to test, i.e., formulate the question you want to answer, before moving any further and starting to search for models.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-05-19T11:49:41.343" Id="99247" LastActivityDate="2014-05-19T11:49:41.343" OwnerUserId="44760" ParentId="99226" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="99998" AnswerCount="1" Body="&lt;p&gt;is forecasting accuracy and trading profitability related? &lt;/p&gt;&#10;&#10;&lt;p&gt;How to explain the existence of the relationship? &lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-05-19T14:26:13.677" Id="99270" LastActivityDate="2014-05-25T13:34:41.337" OwnerUserId="35988" PostTypeId="1" Score="2" Tags="&lt;time-series&gt;&lt;forecasting&gt;&lt;econometrics&gt;&lt;accuracy&gt;" Title="The relationship between trading profitibility and forecasting accuracy" ViewCount="82" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Assuming $A_1, A_2, \ldots, A_n$ are independent exponential random variables (each having the same parameter and, for the sake of simplicity, let's assume the value of the parameter is 1).&#10;Define $B_i = A_i + k$  (where $k$ is a constant of unknown value).&lt;/p&gt;&#10;&#10;&lt;p&gt;What would the maximum likelihood estimate (MLE) of $k$ be, if we're also provided with  a sequence of observations $(b_1, b_2,\ldots, b_n)$?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-05-19T15:42:09.497" Id="99281" LastActivityDate="2014-05-19T20:12:55.937" LastEditDate="2014-05-19T16:23:10.127" LastEditorUserId="45805" OwnerUserId="45805" PostTypeId="1" Score="1" Tags="&lt;self-study&gt;&lt;estimation&gt;&lt;maximum-likelihood&gt;&lt;random-variable&gt;&lt;exponential&gt;" Title="Maximum likelihood estimation for a sequence of observations" ViewCount="75" />
  <row AnswerCount="0" Body="&lt;p&gt;I want to simulate data from the following model:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\textbf{z}_k=\textbf{H}\textbf{x}_k+\textbf{v}_k$  $\textbf{v}_k \sim N(\textbf{0},\textbf{R})$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\textbf{H}$ does not change over time&lt;br&gt;&#10;$\textbf{x}$ is a vector of loadings&lt;br&gt;&#10;$\textbf{R}$ is a diagonal of constants&lt;/p&gt;&#10;&#10;&lt;p&gt;$\textbf{x}_k=\textbf{F}\textbf{x}_{k-1}+(\textbf{I}-\textbf{F}){\mu} + \textbf{w}_k $   $\textbf{w}_k \sim N(\textbf{0},\textbf{Q})$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\textbf{I}$ is the identity matrix&lt;br&gt;&#10;$\mu$ is the vector of mean values of $\textbf{x}$&lt;br&gt;&#10;$\textbf{F}$ is diagonal with the AR(1) params which do not change over time&lt;br&gt;&#10;$\textbf{Q}$ is diagonal with the innovation processes for $\textbf{x}$&lt;/p&gt;&#10;&#10;&lt;p&gt;I have the following code in Matlab&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;nDates=20000; %number of dates&#10;&#10;mats=[1 2 3 4 5 6 7 8 9 10 12 15 20 25 30]'; %maturities &#10;nY=length(mats); %#number of yields&#10;&#10;z=zeros(nY,nDates);  %declare vector for yields&#10;x=zeros(3,nDates); %declare vector for factors&#10;&#10;R=0.00001;   %standard deviation&#10;I=eye(3); %3*3 identity matrix&#10;&#10;v=normrnd(0,R,nY,nDates); %generate residuals&#10;F=[0.9963 0 0; 0 0.9478 0; 0 0 0.774]; %AR(1) matrix&#10;mu=[0.0501; -0.0251;-.0116]; %mean of X&#10;lambda=0.5536; &#10;&#10;q = [0.0026^0.5 0 0;0 0.0027^0.5 0; 0 0 0.0035^0.5];&#10;Q=q*q';&#10;rng('default');  % For reproducibility&#10;r = randn(nDates,3);&#10;w= (r*Q)';     &#10;&#10;&#10;&#10;B= [ones(nY,1),((1-exp(-lambda*mats))./(lambda*mats)),((1-exp(-lambda*mats))./(lambda*mats))-exp(-lambda*mats)];&#10;&#10;X(:,1)=mu;&#10;&#10;for t=2:nDates&#10;x(:,t)=F*(x(:,t-1))+(I-F)*mu+w(:,t);&#10;z(:,t)=B*x(:,t)+v(:,t);&#10;end&#10;&#10;z(:,1)=[];&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;It all seems straightforward enough but what test can I so to ensure that it has been implemented correctly?&lt;/p&gt;&#10;&#10;&lt;p&gt;Ones I have thought of:&lt;br&gt;&#10;Check the correlation of the factors x on their 1 period lags match the values given in matrix F&lt;br&gt;&#10;Check that the variances of v and w are correct&lt;br&gt;&#10;Check that the mean of the simulated variables are correct  &lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to check that the empirical variance of the parameters matches their theoretical equivalent, but I don't know what the theoretical equivalent should be?&lt;/p&gt;&#10;&#10;&lt;p&gt;Please feel free to suggest further tests that will allow me to know for sure if the implementation is correct.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-19T15:47:43.407" FavoriteCount="0" Id="99282" LastActivityDate="2014-05-19T15:47:43.407" OwnerUserId="25973" PostTypeId="1" Score="0" Tags="&lt;time-series&gt;&lt;state-space-models&gt;" Title="Check that state space model implemented correctly" ViewCount="40" />
  <row Body="&lt;p&gt;You really need to find a balance between the science/theory that leads to the data and what the data tells you.  Like others have said, if you let yourself fit any possible transformation (polynomials of any degree, etc.) then you will end up overfitting and getting something that is useless.&lt;/p&gt;&#10;&#10;&lt;p&gt;One way to convince yourself of this is through simulation.  Choose one of the models (linear, exponential, log) and generate data that follows this model (with a choice of the parameters).  If your conditional variance of the y values is small relative to the spread of the x variable then a simple plot will make it obvious which model was chosen and what the &quot;truth&quot; is.  But if you choose a set of parameters such that it is not obvious from the plots (probably the case where an analytic solution is of interest) then analyze each of the 3 ways and see which gives the &quot;best&quot; fit.  I expect that you will find that the &quot;best&quot; fit is often not the &quot;true&quot; fit.&lt;/p&gt;&#10;&#10;&lt;p&gt;On the other hand, sometimes we want the data to tell us as much as possible and we may not have the science/theory to fully determine the nature of the relationship.  The original paper by Box and Cox (JRSS B, vol. 26, no. 2, 1964) discusses ways to compare between several transformations on the y variable, their given set of transformations have linear and log as special cases (but not exponential), but nothing in the theory of the paper limits you to only their family of transformations, the same methodology could be extended to include a comparison between the 3 models that you are interested in.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-19T16:22:43.193" Id="99287" LastActivityDate="2014-05-19T16:22:43.193" OwnerUserId="4505" ParentId="9334" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;Your question sounds very much like you are interested in &lt;em&gt;discrete time event history analysis&lt;/em&gt; (aka &lt;em&gt;discrete time survival analysis&lt;/em&gt;, aka a &lt;em&gt;logit hazard model&lt;/em&gt;) to answer the question whether and when will an event occur?&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, equation 1 gives the logit hazard where discrete time periods (up to period $T$ are indicated $d_{1}, \dots, d_{T}$, and you may condition your model on $p$ number of predictors $X_{1}, \dots, X_{p}$. This gives you a hazard estimate as in equation 2. These equations specify a conditional hazard function with a &lt;em&gt;fully discrete&lt;/em&gt; parameterization of time. Although you could instead specify a conditional hazard function that is constant over time, or is a linear or polynomial function of time period, or even a hybrid of polynomial functions of period plus some discrete time indicators. Your predictors can be constant over time, or time-varying, so I see no reason why you could not also include lagged or differenced functions of the predictors to model auto-correlation.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;$\mathrm{logit}\left(h\left(t,{X_{1t},\dots,X_{pt}}\right)\right) = \alpha_{1}d_{1} + \dots + \alpha_{T}d_{T} + \beta_{1}X_{1t} + \dots + \beta_{p}X_{pt}$&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;$\hat{h}\left(t,{X_{1t},\dots,X_{pt}}\right) = \frac{e^{\hat{\alpha}_{1}d_{1} + \dots + \hat{\alpha}_{T}d_{T} + \hat{\beta}_{1}X_{1t} + \dots + \hat{\beta}_{p}X_{pt}}}{1 + e^{\hat{\alpha}_{1}d_{1} + \dots + \hat{\alpha}_{T}d_{T} + \hat{\beta}_{1}X_{1t} + \dots + \hat{\beta}_{p}X_{pt}}}$&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;One need not use a logit hazard model (indeed one could use probit, complimentary log-log, robit, etc. binomial link functions).&lt;/p&gt;&#10;&#10;&lt;p&gt;If you are using Stata see also the &lt;code&gt;dthaz&lt;/code&gt; package by typing &lt;code&gt;net describe dthaz, from(http://www.doyenne.com/stata)&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Singer, J. and Willett, J. (1993). Its about time: Using discrete-time survival analysis to study duration and the timing of events. &lt;em&gt;Journal of Educational and Behavioral Statistics&lt;/em&gt;, 18(2):155195.&lt;/p&gt;&#10;&#10;&lt;p&gt;Singer, J. D. and Willett, J. B. (2003). &lt;em&gt;Applied longitudinal data analysis: Modeling change and event occurrence&lt;/em&gt;. Oxford University Press, New York, NY.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-19T17:11:07.820" Id="99293" LastActivityDate="2014-05-19T17:11:07.820" OwnerUserId="44269" ParentId="99280" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;The notion of Euclidean distance, which works well in the two-dimensional and three-dimensional worlds studied by Euclid, has some properties in higher dimensions that are contrary to our (maybe just &lt;em&gt;my&lt;/em&gt;) geometric intuition which is also an extrapolation from two and three dimensions.&lt;/p&gt;&#10;&#10;&lt;p&gt;Consider a $4\times 4$ square with vertices at $(\pm 2, \pm 2)$. Draw four&#10;unit-radius circles centered at $(\pm 1, \pm 1)$. These &quot;fill&quot; the square,&#10;with each circle touching the sides of the square at two points, and each&#10;circle touching its two neighbors. For example, the circle centered at&#10;$(1,1)$ touches the sides of the square at $(2,1)$ and $(1,2)$, and&#10;its neighboring circles at $(1,0)$ and $(0,1)$.  Next, draw a small&#10;circle &lt;em&gt;centered at the origin&lt;/em&gt; that touches all four circles. Since&#10;the line segment whose endpoints are the centers of &#10;two osculating circles passes through the point of osculation, It is&#10;easily verified that the small circle has radius $r_2 = \sqrt{2}-1$&#10;and that it touches touches the four larger circles at $(\pm r, \pm r)$.&#10;Note that the small circle is &quot;completely surrounded&quot; by the four&#10;larger circles and thus is also completely inside the square. Note also&#10;that the point $(r_2,0)$ lies on the small circle.&lt;/p&gt;&#10;&#10;&lt;p&gt;Next, consider a $4\times 4 \times 4$ cube with vertices at &#10;$(\pm 2, \pm 2, \pm 2)$. We fill it with $8$ osculating&#10;unit-radius spheres centered at $(\pm 1, \pm 1, \pm 1)$, and &#10;then put a smaller osculating sphere centered at the origin.&#10;Note that the small sphere has radius $r_3 = \sqrt{3}-1$&#10;and the point $(r_3,0,0)$ lies on the surface of the small sphere.&lt;/p&gt;&#10;&#10;&lt;p&gt;Generalizing, we can consider a $n$-dimensional hypercube of side&#10;$4$ and fill it with $2^n$ osculating unit-radius hyperspheres&#10;centered at $(\pm 1, \pm 1, \ldots, \pm 1)$ and then&#10;put a &quot;smaller&quot; osculating sphere of radius&#10;$$r_n = \sqrt{n}-1\tag{1}$$ at the origin.  The point $(r_n,0,0, \ldots, 0)$&#10;lies on this &quot;smaller&quot; sphere.&lt;/p&gt;&#10;&#10;&lt;p&gt;But, notice from $(1)$ that when $n = 4$, $r_n = 1$ and so the&#10;&quot;smaller&quot; sphere has unit radius and thus really does not deserve&#10;the soubriquet of &quot;smaller&quot; for $n\geq 4$. Indeed, it would be better&#10;if we called it the &quot;larger sphere&quot; or just &quot;central sphere&quot;. &#10;Worse yet, when $n &amp;gt; 9$,  we have from $(1)$ that $r_n &amp;gt;2$, and thus the point&#10;$(r_n, 0, 0, \ldots, 0)$ on the central sphere&#10; &lt;em&gt;lies outside the hypercube of side $4$&#10;even though it is &quot;completely surrounded&quot; by the unit-radius hyperspheres&#10;that &quot;fill&quot; the hypercube (in the sense of packing it).&lt;/em&gt;  The central&#10;sphere &quot;bulges&quot; outside the hypercube in high-dimensional space.&#10;I find this very counter-intuitive because my mental translations of&#10; the notion of Euclidean distance to higher dimensions, using&#10;the geometric intuition that I have developed&#10;from the 2-space and 3-space that I am familiar with, do not&#10;describe the reality of high-dimensional space.&lt;/p&gt;&#10;&#10;&lt;p&gt;My answer to the OP's question &quot;Besides, what is 'high dimensions'?&quot;&#10;is $n \geq 9$.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-05-19T19:53:42.173" Id="99306" LastActivityDate="2014-05-19T19:53:42.173" OwnerUserId="6633" ParentId="99171" PostTypeId="2" Score="36" />
  
  
  
  <row Body="&lt;p&gt;You could create (normalised) word vectors for each category (using the number of times the word occurs in that category as the weight). Then calculating the cosine similarity between these category vectors.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-05-20T08:35:36.367" Id="99353" LastActivityDate="2014-05-20T08:35:36.367" OwnerUserId="28077" ParentId="99344" PostTypeId="2" Score="0" />
  
  <row AnswerCount="1" Body="&lt;p&gt;For the development of a new chemical process, a handful experts should be asked a few questions (4 to 6), what values they expect for several experimental parameters, i.e. temperature ranges and how confident they are regarding their guesses on a scale from 1 to 10.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to weight their responses corresponding to the confidence.&lt;/p&gt;&#10;&#10;&lt;p&gt;Some people tend to be very confident with average ratings of 8, other tend to be less confident with ratings of 3 to 4.&lt;/p&gt;&#10;&#10;&lt;p&gt;To avoid such things as the &lt;a href=&quot;https://en.wikipedia.org/wiki/Dunning%E2%80%93Kruger_effect&quot; rel=&quot;nofollow&quot;&gt;Dunning-Kruger effect&lt;/a&gt;, I would like to get a rating of them, what confidence level means what and normalize their responses.&lt;/p&gt;&#10;&#10;&lt;p&gt;I thought of giving them a questionnaire with questions, they know for pretty sure, and others, where they most likely have to guess and take the confidence level their as well. If people give high confidence scores for questions, they could not possibly know, I can assume, that an 8 is more something other would rate with a 5.&lt;/p&gt;&#10;&#10;&lt;p&gt;I now wonder, if such a questionnaire to get the raters tendency in confidence scores already exists, possibly already widely evaluated?&lt;/p&gt;&#10;&#10;&lt;p&gt;I am not sure, if Cross Validated is the right corner of Stack Exchange to post this question, if not I am very sorry; feel free to delete this question then.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-20T10:52:37.930" Id="99368" LastActivityDate="2014-05-25T16:37:11.890" OwnerUserId="37609" PostTypeId="1" Score="1" Tags="&lt;survey&gt;&lt;confidence&gt;" Title="Avoid rater tendency in confidence questionaires" ViewCount="10" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to learn more about how to build ensembles of predictions in R and coming to a roadblock, and am hoping one can offer guidance. &lt;/p&gt;&#10;&#10;&lt;p&gt;I often read about people automatically identifying how they should weight each model through the use of OLS. How do people do this? Do you just insert your prediction from each model as a regressor in the model?&lt;/p&gt;&#10;&#10;&lt;p&gt;E.g., &#10;Final_Prediction = b0 + b1*prediction_from_GBM + b2*prediction_from_SVM + bkxk + e &lt;/p&gt;&#10;&#10;&lt;p&gt;and just fit the line above to combine your predictions? &lt;/p&gt;&#10;&#10;&lt;p&gt;What about when you are predicting class membership.... do you get the probabilities from each model and fit them in a logistic model similarly? &lt;/p&gt;&#10;&#10;&lt;p&gt;Any resources that are R specific or any thoughts / clarifications are greatly appreciated. I have not been able to find anything. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-20T11:33:23.403" Id="99373" LastActivityDate="2014-05-20T11:33:23.403" OwnerUserId="43414" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;ensemble&gt;" Title="How to combine predictions in ensemble" ViewCount="64" />
  <row AcceptedAnswerId="99409" AnswerCount="1" Body="&lt;p&gt;I was wondering if there is an easy way to find sufficient conditions for the following inequality to hold&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;\int f(x,y)^2 \:\mathrm{d}x \:\mathrm{d}y - \int f(x)^2 f(y)^2 \:\mathrm{d}x\:\mathrm{d}y \geq 0,&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $f$ is the density function. Do you have any suggestions?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-05-20T12:16:20.583" Id="99377" LastActivityDate="2014-05-20T18:23:23.117" OwnerUserId="21744" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;probability-inequalities&gt;" Title="Proof of density inequality" ViewCount="112" />
  <row AnswerCount="0" Body="&lt;p&gt;I know the question can sound trivial, but I can't get the difference between simulation and forecasting. If I've understood well, a forecast of $k$ periods ahead, starting from period $T$, is obtained by treating the $k$ steps as missing and proceed with conventional Kalman filtering; that is, to treat the Kalman gain as null. Now I'm thinking, Couldn't it be possible to obtain a similarly valid forecast if I make a simulation of $k$ consecutive values with the initial value &lt;em&gt;equal&lt;/em&gt; to the state value in step $T$? On the whole, which are the defining differences between both methods?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-05-20T13:13:11.393" Id="99382" LastActivityDate="2014-05-20T13:13:11.393" OwnerUserId="43824" PostTypeId="1" Score="0" Tags="&lt;forecasting&gt;&lt;simulation&gt;&lt;kalman-filter&gt;" Title="kalman filter: difference between simulation and forecasting" ViewCount="67" />
  <row Body="&lt;h3&gt;A recommendation: just compute the PSRF separately for each scalar component&lt;/h3&gt;&#10;&#10;&lt;p&gt;The original article by Gelman &amp;amp; Rubin [1], as well as the &lt;em&gt;Bayesian Data Analysis&lt;/em&gt; textbook of Gelman et al. [2], recommends calculating the potential scale reduction factor (PSRF) separately for each scalar parameter of interest. To deduce convergence, it is then required that all PSRFs are close to 1. It does not matter that your parameters are interpreted as random vectors, their components are  scalars for which you can compute PSRFs. &lt;/p&gt;&#10;&#10;&lt;p&gt;Brooks &amp;amp; Gelman [3] have proposed a multivariate extension of the PSRF, which I review in the next section of this answer. However, to quote Gelman &amp;amp; Shirley [4]:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;[...] these methods&#10;  may sometimes represent overkill: individual parameters can be well estimated even while&#10;  approximate convergence of simulations of a multivariate distribution can take a very long time.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;h3&gt;Alternative: multivariate extension by Brooks&amp;amp;Gelman&lt;/h3&gt;&#10;&#10;&lt;p&gt;Brooks &amp;amp; Gelman [3] propose a multivariate extension of the PSRF, where indeed one computes the estimated covariance matrix (your step 4) as a weighted sum of the within-chain ($W$) and between-chain ($B$) covariance matrices (your step 3):&#10;\begin{equation}&#10;\hat{V} = \frac{n-1}{n}W + \frac{1}{n}B,&#10;\end{equation} &#10;where $n$ is the chain length. Then, one needs to define some scalar metric for the distance between the covariance matrices $\hat{V},W$. The authors propose&#10;\begin{equation}&#10;\hat{R} = \max_a \frac{a^T\hat{V}a}{a^TWa} = \frac{n-1}{n} + \left(\frac{m+1}{m}\right)\lambda_1,&#10;\end{equation}&#10;where $m$ is the chain length, the equality is shown in the article with $\lambda_1$ being the largest positive eigenvalue of $W^{-1}\hat{V}/n$. Then, the authors argue that under convergence of the chains, $\lambda_1\rightarrow 0$ and thus with big $n$ this multivariate $\hat{R}$ should converge near 1.&lt;/p&gt;&#10;&#10;&lt;h3&gt;References&lt;/h3&gt;&#10;&#10;&lt;p&gt;[1] Gelman, Andrew, and Donald B. Rubin. &quot;Inference from iterative simulation using multiple sequences.&quot; Statistical Science (1992): 457-472.&lt;/p&gt;&#10;&#10;&lt;p&gt;[2] Gelman, Andrew, et al. Bayesian data analysis. CRC press, 2013. &lt;/p&gt;&#10;&#10;&lt;p&gt;[3] Brooks, Stephen P., and Andrew Gelman. &quot;General methods for monitoring convergence of iterative simulations.&quot; Journal of Computational and Graphical Statistics 7.4 (1998): 434-455.&lt;/p&gt;&#10;&#10;&lt;p&gt;[4] Gelman, Andrew, and Kenneth Shirley. &quot;Inference from simulations&#10;and monitoring convergence&quot;. (Chapter 6 in Brooks, Steve, et al., eds. Handbook of Markov Chain Monte Carlo. CRC Press, 2011.)&lt;/p&gt;&#10;&#10;&lt;p&gt;All articles except the textbook [2] are available at Andrew Gelman's website &lt;a href=&quot;http://www.stat.columbia.edu/~gelman/research/published/&quot; rel=&quot;nofollow&quot;&gt;Andrew Gelman's website&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-20T13:50:29.737" Id="99388" LastActivityDate="2014-05-20T13:50:29.737" OwnerUserId="24669" ParentId="99375" PostTypeId="2" Score="3" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am using the random forest method (regression) to generate a predictor. Normally, I provide a response/answer along with each feature vector when I train my forest. However, for some of my responses, only a range is know and not the exact value. &lt;/p&gt;&#10;&#10;&lt;p&gt;For example (hypothetically), say, the response is the selling price of a house and the feature vector for each house are things like size, shape, location, etc. What happens if, for some of the houses, only a price range is known (i.e., less than \$10,000) and not the actual value (sold for exactly \$20,000). &lt;/p&gt;&#10;&#10;&lt;p&gt;How can/should I encode this information into the response? Maybe these houses should be omitted from training. But what if all houses that sell for less than \$10,000 are only provided as ranges (i.e., &quot;less than \$10,000&quot;, &quot;less than \$4500&quot;, or &quot;less than \$90&quot;). If I omit them then my predictor will be poor at assessing houses that sell for less than \$10,000.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-20T14:18:35.277" Id="99392" LastActivityDate="2014-05-20T14:18:35.277" OwnerUserId="39582" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;random-forest&gt;" Title="Response Range in Random Forest" ViewCount="28" />
  <row AnswerCount="1" Body="&lt;p&gt;My lab teacher asked this question in class, but i find no way to work it out.&lt;/p&gt;&#10;&#10;&lt;p&gt;If I have $n$ points with their uncertainties, I know that they follow a linear expression and I find the best-fit line of them, is there a way to predict how many points will be crossed by the line? &lt;/p&gt;&#10;&#10;&lt;p&gt;I thought that this number should be related to the $\chi^2$ of the points that should be $\sim n$. Someone can help me even if I probably didn't explain myself well? Thanks!&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-05-20T15:08:39.067" Id="99402" LastActivityDate="2014-05-20T16:09:48.890" OwnerUserId="45859" PostTypeId="1" Score="0" Tags="&lt;fitting&gt;&lt;curve-fitting&gt;" Title="Number of points crossed by their best fit line" ViewCount="31" />
  <row AnswerCount="0" Body="&lt;p&gt;There are several approaches, for example, moderated regression analysis and Hunter, Schmidt and Jackson. The HSJ (1982) approach invokes true variance computation based on subsets say male- vs female-led studies. According to Hunter, J. E., Schmidt, F. L. and Jackson (1982) &lt;em&gt;Cumulating Findings Across Studies&lt;/em&gt;, if data are broken into (two) subsets, then there are two ways that a moderator variable will show itself. &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;There should be large differences in the mean effect-size between subsets. &lt;/li&gt;&#10;&lt;li&gt;There should be reduction in variance within subsets. &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Is this a valid approach for identifying the moderator factors or variables?&lt;/p&gt;&#10;&#10;&lt;p&gt;See more in Hunter, J. E., &amp;amp; Schmidt, F. L. (2004). Methods of Meta-Analysis: Correcting Error and Bias in Research Findings (2nd ed.). Thousand Oaks, CA: Sage &lt;/p&gt;&#10;" CommentCount="9" CreationDate="2014-05-20T15:26:43.057" Id="99407" LastActivityDate="2014-05-21T15:04:06.267" LastEditDate="2014-05-21T14:10:06.997" LastEditorUserId="7290" OwnerUserId="10619" PostTypeId="1" Score="0" Tags="&lt;interaction&gt;&lt;meta-analysis&gt;&lt;meta-regression&gt;" Title="Is the approach to moderator analysis advocated by Hunter, Schmidt and Jackson (1982) valid?" ViewCount="91" />
  <row AnswerCount="1" Body="&lt;p&gt;Is cross validation used for model selection only, or can I use cross validation to test the performance accuracy of a neural network?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-20T17:26:56.010" Id="99421" LastActivityDate="2014-05-20T19:56:39.387" LastEditDate="2014-05-20T17:49:52.340" LastEditorUserId="32036" OwnerUserId="45868" PostTypeId="1" Score="0" Tags="&lt;cross-validation&gt;&lt;neural-networks&gt;" Title="Cross validation and neural network performance" ViewCount="20" />
  <row Body="&lt;p&gt;If you have no control group, then you can't do Diff-in-Diff.&lt;/p&gt;&#10;&#10;&lt;p&gt;Since you are interested in estimating the effect of some other variable on an outcome and seeing whether the effect is different during and after the crisis, I think that you would want to run some regression like:&lt;/p&gt;&#10;&#10;&lt;p&gt;$y_{it} = \beta_0 + \beta_1 x_{it} + \beta_2 Crisis_t * x_{it} + \tau_t + c_i + \epsilon_{it}$&lt;/p&gt;&#10;&#10;&lt;p&gt;Here you are interested in the effect of $x_{it}$ on $y_{it}$ and if it is different during the crisis than it is after the crisis (this is equivalent to testing whether $\beta_2 = 0$.  $Crisis_t$ is a dummy variable $=1$ in crisis years (2008) and $=0$ otherwise.&lt;/p&gt;&#10;&#10;&lt;p&gt;$\tau_t$ are dummy variables for each time period.  These should soak up differences in outcomes related to the time period, and are probably important to include.  $c_i$ are individual fixed effects.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-05-20T18:08:23.213" Id="99432" LastActivityDate="2014-05-21T15:00:56.050" LastEditDate="2014-05-21T15:00:56.050" LastEditorUserId="45823" OwnerUserId="45823" ParentId="99420" PostTypeId="2" Score="0" />
  <row AnswerCount="2" Body="&lt;p&gt;&lt;strong&gt;Is there an algorithm that returns the mean probability of response from a multi-dimensional matrix?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, if I have a set of features:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;customerClass = &lt;code&gt;poor&lt;/code&gt; / &lt;code&gt;middle&lt;/code&gt; / &lt;code&gt;rich&lt;/code&gt;&lt;/li&gt;&#10;&lt;li&gt;category = &lt;code&gt;11&lt;/code&gt; / &lt;code&gt;20&lt;/code&gt; / &lt;code&gt;35&lt;/code&gt;&lt;/li&gt;&#10;&lt;li&gt;offerValue = &lt;code&gt;$1&lt;/code&gt; / &lt;code&gt;$2&lt;/code&gt; / &lt;code&gt;$3&lt;/code&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;and a response:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;usesCoupon = FALSE / TRUE&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Assuming I have enough samples, I want to predict the probability for a new sample (customerClass = &lt;code&gt;rich&lt;/code&gt;, category = &lt;code&gt;20&lt;/code&gt;, offerValue = &lt;code&gt;$1&lt;/code&gt;) as the average probability for the population with the same three-dimensional value &lt;code&gt;(rich, 20, $1)&lt;/code&gt;.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-05-20T19:33:09.383" Id="99442" LastActivityDate="2014-05-21T15:45:21.203" OwnerUserId="21720" PostTypeId="1" Score="0" Tags="&lt;algorithms&gt;" Title="which algorithm uses a multidimension array of average probabilities" ViewCount="39" />
  <row Body="&lt;p&gt;For the Poisson model, if the expection for the $i$th observation $Y_i$ is $\mu_i$ its variance is $\mu_i$, &amp;amp; the Pearson residual therefore&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac{y_i-\hat\mu_i}{\sqrt{\hat\mu_i}}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\hat\mu$ is the estimate of the mean. The parametrization of the negative binomial model used in &lt;em&gt;MASS&lt;/em&gt; is explained &lt;a href=&quot;http://stats.stackexchange.com/questions/70619/&quot;&gt;here&lt;/a&gt;. If the expection for the $i$th observation $Y_i$ is $\mu_i$ its variance is $\mu_i + \frac{\mu^2}{\theta}$, &amp;amp; the Pearson residual therefore&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac{y_i-\tilde\mu_i}{\sqrt{\tilde\mu_i+\frac{\tilde\mu'^2}{\theta}}}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\tilde\mu$ is the estimate of the mean. The smaller the value of $\theta$&amp;mdash; i.e. the more extra-Poisson variance&amp;mdash;, the smaller the residual compared to its Poisson equivalent. [But as @whuber has pointed out, the estimates of the means are not the same, $\hat\mu\neq\tilde\mu$, because the estimation procedure weights observations according to their assumed variance. If you were to make replicate measurements for the $i$th predictor pattern, they'd get closer, &amp;amp; in general adding a parameter should give a better fit across all observations, though I don't know how to demonstrate this rigorously. All the same, the population quantities you're estimating are larger if the Poisson model holds, so it shouldn't be a surprise.]&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2014-05-20T19:49:49.033" Id="99445" LastActivityDate="2014-05-21T09:10:13.613" LastEditDate="2014-05-21T09:10:13.613" LastEditorUserId="17230" OwnerUserId="17230" ParentId="99406" PostTypeId="2" Score="4" />
  
  
  
  <row Body="&lt;p&gt;The &lt;code&gt;ltm&lt;/code&gt; package in R does not have effective global model fit statistics available; one could construct the $G^2$ statistic from the complete-data tables in this package, but this statistic behaves extremely poorly in moderate to large sized tests due to extreme levels of data sparseness. However, the &lt;code&gt;mirt&lt;/code&gt; package does have suitable limited information tests of overall model fit (version 1.3.3+). &lt;/p&gt;&#10;&#10;&lt;p&gt;You can call the M2* statistic (Cai and Hansen, 2013) for several polytomous models using the &lt;code&gt;M2()&lt;/code&gt; function, which collapses the highly sparse complete data tables into first- and second-order data tables to test the goodness of fit based on these marginals instead. For instance,&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(mirt)&#10;set.seed(12)&#10;a &amp;lt;- matrix(rlnorm(20))&#10;d &amp;lt;- matrix(rnorm(20*4), 20)&#10;d &amp;lt;- t(apply(d, 1, sort, decreasing=TRUE))&#10;&#10;# simulate 20 items, each with 5 ordered categories&#10;dat &amp;lt;- simdata(a, d, N=1000, itemtype = 'graded') &#10;&#10;# estimate the model&#10;mod &amp;lt;- mirt(dat, 1)&#10;Iteration: 15, Log-Lik: -25545.205, Max-Change: 0.00004&#10;&#10;# call the M2() function&#10;M2(mod)&#10;            M2 df.M2      p.M2    RMSEA.M2    TLI.M2    CFI.M2&#10;stats 117.8272   111 0.3107657 0.007846538 0.9841162 0.9865412&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;For further details about this limited information statistic, see: &lt;/p&gt;&#10;&#10;&lt;p&gt;Cai, L. &amp;amp; Hansen, M. (2013). Limited-information goodness-of-fit testing of hierarchical item factor models. British Journal of Mathematical and Statistical Psychology, 66, 245-276.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-20T21:25:49.577" Id="99464" LastActivityDate="2014-05-23T20:31:25.967" LastEditDate="2014-05-23T20:31:25.967" LastEditorUserId="18152" OwnerUserId="18152" ParentId="95785" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;A textbook I am using (Field's &quot;Discovering Statistics Using SPSS&quot;) declares that &quot;the simplest way&quot; to work things out is to calculate SS&lt;sub&gt;AB&lt;/sub&gt; as SS&lt;sub&gt;BETWEEN&lt;/sub&gt; - SS&lt;sub&gt;A&lt;/sub&gt; - SS&lt;sub&gt;B&lt;/sub&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;This left me wondering if there might be some other way to calculate SS&lt;sub&gt;AB&lt;/sub&gt;. I'm especially, though not exclusively, keen to know if there's a way of calculating it that would be more intuitive.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-05-20T22:38:46.407" FavoriteCount="1" Id="99467" LastActivityDate="2014-05-20T22:42:04.630" LastEditDate="2014-05-20T22:42:04.630" LastEditorUserId="9162" OwnerUserId="9162" PostTypeId="1" Score="1" Tags="&lt;anova&gt;&lt;interaction&gt;" Title="In two-way between-groups ANOVA, is there a way to work out $SS_{AB}$ aside from $SS_{\rm BETWEEN} - SS_A - SS_B$?" ViewCount="24" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm setting up a computer simulation (which I know changes the design of experiments methods) and I can't really determine how to choose which design of experiments (DOE) method to use. I'm a little overwhelmed with all the choices and options and I am a bit unclear on which ones are best for which purposes. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have 17 inputs and maybe 5 outputs. I say maybe because I'm not sure what will be interesting at the end. Ideally, I would like to develop a model (response surface) where given the inputs, it predicts the outputs. &lt;/p&gt;&#10;&#10;&lt;p&gt;But maybe some inputs aren't needed. Or maybe some outputs are not useful. Some of the outputs are independent of others so maybe a single 17 input, 5 output model wouldn't be good and it needs to be split into a few models where some inputs matter and some outputs are coupled. Maybe some inputs aren't important, or maybe some inputs are related -- for example, the ratio of two inputs might determine the response. &lt;/p&gt;&#10;&#10;&lt;p&gt;Ultimately this will probably be a multi-step process where I need to determine the sensitivity to the inputs for each output, or determine the important relationships between inputs (products, ratios, sums, etc) and where the data can be used to generate one or more models of the responses. &lt;/p&gt;&#10;&#10;&lt;p&gt;As always, I need to minimize the number of simulations. I'm using the &lt;a href=&quot;http://dakota.sandia.gov&quot; rel=&quot;nofollow&quot;&gt;Dakota&lt;/a&gt; package and from my reading of the user manual, Monte Carlo methods or Orthogonal Array - Latin Hypercube Sampling may work the best. But what makes one method better than the other for this kind of experiment? Will I be able to determine important inputs, relationships between inputs, and develop models using either one?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-05-21T00:20:55.653" Id="99473" LastActivityDate="2014-05-21T00:20:55.653" OwnerUserId="7970" PostTypeId="1" Score="0" Tags="&lt;experiment-design&gt;" Title="How do I choose which design of experiments method to use?" ViewCount="37" />
  
  
  
  <row Body="&lt;p&gt;The generalised pareto distribution has the limitation that $mu &amp;lt; x$. Thus, the posterior density is capped at x, as COOLSerdash noted. This is intended behaviour, not a bug. &lt;/p&gt;&#10;&#10;&lt;p&gt;The lesser, left-most peak in the data is due to the prior distribution being incorrectly specified with a floor of zero.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-21T02:45:24.917" Id="99483" LastActivityDate="2014-05-21T04:09:23.747" LastEditDate="2014-05-21T04:09:23.747" LastEditorUserId="179" OwnerUserId="179" ParentId="99058" PostTypeId="2" Score="3" />
  
  <row AnswerCount="1" Body="&lt;p&gt;&lt;strong&gt;The case:&lt;/strong&gt; I am investigating the impact of various predictors on the odds of migration using a discrete-time event history model within a multilevel framework. The outcome variable is dichotomous (0=not migrated; 1=migrated). The predictors are largely time varying covariates operating at the individual level (e.g., years of schooling, marital status) and community level (e.g., community wealth index, social networks). Individuals are observed during five years. As such, the multilevel-model structure is as follows: Level 1: years of observation; Level 2: individuals; Level 3: communities. I am estimating these models using multilevel logit models (lme4 package) on stacked data (person-year data file).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;The problem:&lt;/strong&gt; Does anyone know whether there is a limit to the number of time varying covariates that I can safely include in these models. Lets assume that I have 1000 individuals (level 2) that are observed up to 5 years leading to 4500 person year records at level 1 (e.g., about 4.5 records per individual). Those 1000 individuals are located in 25 communities (level 3) (e.g., 40 individuals per community). If I observe these individuals for only 5 years, would that mean that I can only include 5 time varying covariates? Do I run into issues of degrees of freedom if I include 10 time varying covariates? I have read the relevant literature on event history models (e.g., Allison, 1984) and did a thorough search on the internet but can't find any information on this topic. Any information or article/book recommendation on these questions would be highly appreciated! Thanks!&lt;/p&gt;&#10;&#10;&lt;p&gt;Best, Raphael&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-05-21T02:55:27.870" Id="99484" LastActivityDate="2014-05-21T05:51:37.670" OwnerUserId="42121" PostTypeId="1" Score="2" Tags="&lt;survival&gt;&lt;multilevel-analysis&gt;&lt;time-varying-covariate&gt;" Title="Is there a limit to the number of time varying covariates in a discrete event history model" ViewCount="32" />
  <row Body="&lt;p&gt;Time-varying covariates vary at the lowest level of your hierarchy, so one consideration is that you have 4500-ish observations to work with. A second consideration is what is the variability in the covariates's values at each time period? If there's enough variation for each variable, I imagine your would be able to include quite a few more than five. Of course, all the usual caveats about confounding, colinearilty and the like still obtain.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-05-21T05:51:37.670" Id="99490" LastActivityDate="2014-05-21T05:51:37.670" OwnerUserId="44269" ParentId="99484" PostTypeId="2" Score="0" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Is there some theorem that allows us to work out:&lt;/p&gt;&#10;&#10;&lt;p&gt;$Cov\big( f(x+y), f(x)\big)$&lt;/p&gt;&#10;&#10;&lt;p&gt;We now the $Var\big(f(x)\big)$ and $Var\big(f(x+y)\big)$ and also the $Var(x)$, $Var(y)$ and the $Covar(x,y)$&lt;/p&gt;&#10;&#10;&lt;p&gt;Our specific case is the following:&lt;/p&gt;&#10;&#10;&lt;p&gt;What is the $Covar\big(\Phi(\beta_0+\beta_1),\Phi(\beta_0 )\big)$, with $\beta_0$ and $\beta_1$ probit model coefficients with variance and covariance known.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-05-21T09:20:05.457" Id="99508" LastActivityDate="2014-05-24T15:59:05.720" LastEditDate="2014-05-21T10:01:23.340" LastEditorUserId="36462" OwnerUserId="36462" PostTypeId="1" Score="1" Tags="&lt;covariance&gt;&lt;delta-method&gt;" Title="Covariance between f(x+y) and f(x)" ViewCount="97" />
  <row AnswerCount="3" Body="&lt;p&gt;I want to read, study statistically and modify numerical images.&lt;/p&gt;&#10;&#10;&lt;p&gt;During school i used to work with built in functions in MATLAB. &#10;I don't have this possibility anymore.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now i want tout resume my work. I want to know where to begin.&lt;/p&gt;&#10;&#10;&lt;p&gt;What langage should i use (i know some R, python and c) ?&#10;What package would be interesting ?&#10;What book/course should I read to improve my basic knowledge of numerical image, formats, metrics ? (Not affraid of advanced ressources)&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2014-05-22T17:01:03.963" CreationDate="2014-05-21T11:05:08.653" FavoriteCount="1" Id="99517" LastActivityDate="2014-05-21T12:03:12.447" LastEditDate="2014-05-21T11:17:32.350" LastEditorUserId="21846" OwnerUserId="21846" PostTypeId="1" Score="0" Tags="&lt;image-processing&gt;&lt;metric&gt;" Title="Ressources for image processing" ViewCount="31" />
  <row Body="&lt;p&gt;If you are interested in open source, then I would strongly advocate Python. Not so much for the language itself, but for frameworks/libraries: scipy and scikit-learn on the one side, and OpenCV (you have a interface to OpenCV, cv2, which is also integrated with NumPy so you can develop numerical algorithms with nearly as much ease as with MatLab).&lt;/p&gt;&#10;&#10;&lt;p&gt;Maybe the nicest thing is that, since both scipy and cv2 build upon Python and Numpy, you even get to use them at the same time, within the same environment.&lt;/p&gt;&#10;&#10;&lt;p&gt;Together with IPython you have a nice development environment. If you work under Windows check &lt;a href=&quot;http://winpython.sourceforge.net/&quot; rel=&quot;nofollow&quot;&gt;WinPython&lt;/a&gt; or PythonXY. They provide all you need&lt;/p&gt;&#10;" CommentCount="3" CommunityOwnedDate="2014-05-22T17:01:03.963" CreationDate="2014-05-21T12:00:23.473" Id="99524" LastActivityDate="2014-05-21T12:00:23.473" OwnerUserId="17908" ParentId="99517" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;There are several versions of the Simpson diversity index, as explained in &lt;a href=&quot;http://www.countrysideinfo.co.uk/simpsons.htm&quot; rel=&quot;nofollow&quot;&gt;this website&lt;/a&gt;.  I will focus here on this version, which I have stated slightly more precisely to clarify what the sum is over:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$D = \sum_{i=1}^k\frac{n_i(n_i-1)}{N(N-1)}$$&#10;Here $N$ is the total number of individuals within a habitat, and $n_i$ is the number of individuals of the $i$th of $k$ species. Note that, since $N$ is not indexed by $i$, it makes no difference whether the denominator is within the scope of the sum.  We can equally write:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$D =\frac{\sum_{i=1}^kn_i(n_i-1)}{N(N-1)}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The derivation of these formulae is a straightforward application of probability. If two individuals are chosen at random from the habitat, the total number of possible outcomes $O_{tot}$ is:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$O_{tot} = N(N-1)/2$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The division by $2$ is to avoid duplication where the same two individuals are chosen in reverse order.  The outcomes of interest $O_{int}$ are those in which the chosen two individuals belong to the same species. For any one species $i$ the number of pairs $P_i$ of individuals belonging to that species is:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$P_i = n_i(n_i-1)/2$$&lt;/p&gt;&#10;&#10;&lt;p&gt;To find $O_{int}$ we must sum over all species:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$O_{int} = \sum_{i=1}^kP_i = \sum_{i=1}^kn_i(n_i-1)/2$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Dividing $O_{int}$ by $O_{tot}$ and cancelling the divisions by $2$ yields the second of the above formulae for $D$.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-05-21T14:34:19.997" Id="99548" LastActivityDate="2014-05-21T14:42:39.043" LastEditDate="2014-05-21T14:42:39.043" LastEditorUserId="11060" OwnerUserId="11060" ParentId="99509" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;Assume we have 3 annotators, each one of which has assessed the quality of 3 products in a scale from 1 to 7. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ANN  PRODUCT  SCORE&#10;an1  pr1      5&#10;an1  pr2      2&#10;an1  pr3      3&#10;an2  pr1      7&#10;an2  pr2      1&#10;an2  pr3      2&#10;an3  pr1      3&#10;an3  pr2      3&#10;an3  pr3      4&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;We also have a computer model that makes predictions for the same products using a number of features. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;pr1  0.70&#10;pr2  0.25&#10;pr3  0.35&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;There are two ways to calculate the correlation of model's scores with human scores.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;First average the human scores, and then get the correlation with model's scores &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;PRODUCT  ANN.SCORE  MODEL SCORE&#10;pr1      (5+7+3)/3  0.70&#10;pr2      (2+1+3)/3  0.25&#10;pr3      (3+2+4)/3  0.35&#10;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Repeat the model's score for every annotator and product, as follows:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ANN  PRODUCT  ANN.SCORE  MODEL SCORE&#10;an1  pr1      5          0.70&#10;an1  pr2      2          0.25&#10;an1  pr3      3          0.35&#10;an2  pr1      7          0.70&#10;an2  pr2      1          0.25&#10;an2  pr3      2          0.35&#10;an3  pr1      3          0.70&#10;an3  pr2      3          0.25&#10;an3  pr3      4          0.35&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and then get the correlation.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;My question is, which method makes more sense from a statistical point of view? What are the actual differences between the two ways of measuring the correlation? Thank you in advance!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-05-21T15:06:22.617" Id="99558" LastActivityDate="2014-08-07T06:54:36.347" LastEditDate="2014-05-21T16:17:35.643" LastEditorUserId="10840" OwnerUserId="10840" PostTypeId="1" Score="1" Tags="&lt;correlation&gt;&lt;spearman&gt;&lt;spearman-rho&gt;" Title="Evaluating correlation with multiple human annotators" ViewCount="59" />
  <row AnswerCount="0" Body="&lt;p&gt;Let $X_1,\dots,X_n$ are i.i.d. samples from uniform distribution on $(0,1)$. Let $\hat F_n$ be their &lt;em&gt;modified&lt;/em&gt; empirical distribution function defined by&#10;$$&#10;\hat F_n(x)=\frac1{n+2}\left(1+\sum_{i=1}^n1_{\{X_i\le x\}}\right)&#10;$$&#10;for every $x$, where $1_A$ is &lt;a href=&quot;http://en.wikipedia.org/wiki/Characteristic_function&quot; rel=&quot;nofollow&quot;&gt;the characteristic function&lt;/a&gt; of the event $A$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Define $T_n$ by&#10;$$&#10;T_n=\int_0^1\left\{\Phi^{-1}(\hat F_n(u))-u\right\}du&#10;$$&#10;where $\Phi$ is the cummulative standard normal distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;I wish to compute $E(T_n^k)$ for $k\ge 1$. For $k=1$, $E(T_n)$ can be computed as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $X_{(1)}\le \dots\le X_{(n)}\le X_{(n+1)}=1$ be the ordered statistics of our observation. Then&#10;\begin{align}&#10;E(T_n)&amp;amp;=\sum_{i=0}^n\int_0^1\left\{\Phi^{-1}\left(\frac{i+1}{n+2}\right)-u\right\}P\left(X_{(i)}\le u\le X_{(i+1)}\right)du\\&#10;&amp;amp;=\sum_{i=0}^n\int_0^1\left\{\Phi^{-1}\left(\frac{i+1}{n+2}\right)-u\right\}{n\choose i}u^i(1-u)^{n-i}du.&#10;\end{align}&lt;/p&gt;&#10;&#10;&lt;p&gt;But then, how do I compute the second moment of $T_n$, i.e, $E(T_n^2)$? Could anyone help me? Any help will be vastly apprecaiated.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-21T15:15:09.697" Id="99559" LastActivityDate="2014-05-23T19:08:27.427" OwnerUserId="40104" PostTypeId="1" Score="1" Tags="&lt;uniform&gt;&lt;moments&gt;&lt;integral&gt;" Title="Moment of random variable on a integral form" ViewCount="92" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am using R to calculate lagged correlation. Instead of getting lag values -20 to 20, I am getting the following values:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;         [,1]&#10; [1,] -0.83333333&#10; [2,] -0.75000000&#10; [3,] -0.66666667&#10; [4,] -0.58333333&#10; [5,] -0.50000000&#10; [6,] -0.41666667&#10; [7,] -0.33333333&#10; [8,] -0.25000000&#10; [9,] -0.16666667&#10;[10,] -0.08333333&#10;[11,]  0.00000000&#10;[12,]  0.08333333&#10;[13,]  0.16666667&#10;[14,]  0.25000000&#10;[15,]  0.33333333&#10;[16,]  0.41666667&#10;[17,]  0.50000000&#10;[18,]  0.58333333&#10;[19,]  0.66666667&#10;[20,]  0.75000000&#10;[21,]  0.83333333&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I am using the following R code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;indiv_product_data_ts&amp;lt;-ts(indiv_product_data, frequency=12)&#10;x&amp;lt;-indiv_product_data_ts[,2]&#10;y&amp;lt;-indiv_product_data_ts[,3]&#10;x_model&amp;lt;-auto.arima(x)&#10;correlation&amp;lt;-prewhiten(x,y,x.model=x_model,plot=FALSE,lag.max=define_lag)&#10;lag=data.frame(correlation$ccf$lag)&#10;ccf=data.frame(correlation$ccf$acf)&#10;len&amp;lt;-nrow(ccf)&#10;product_list&amp;lt;-rep(product_list[prod_ID],len)&#10;Output&amp;lt;-cbind(product_list,lag,ccf)&#10;colnames(Output)&amp;lt;-c(&quot;BaseProduct&quot;,&quot;lag&quot;,&quot;ccf&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;What could be the reason? I would appreciate any kind of help.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-21T15:50:48.567" Id="99568" LastActivityDate="2014-05-21T17:55:06.953" LastEditDate="2014-05-21T17:55:06.953" LastEditorUserId="32036" OwnerUserId="29500" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;time-series&gt;&lt;correlation&gt;" Title="Not getting correct lag values in output" ViewCount="36" />
  <row AnswerCount="0" Body="&lt;p&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a generative model for a process that can be described as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;y = t(x, w) + e&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $x$ and $y$ observations of a set of random variables which are related by a non-linear transformation function $t$, parameterised by the unknown parameters to be estimated given by $w$. $e$ is the normally distributed error term with 0 mean and a diagonal covariance matrix given by $\sigma^{-1} I$ where $I$ is the identity matrix and $\sigma$ is the global noise precision.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, assuming independence along each pixel of the image, I have the following likelihood term as a product over the individual pixels $i$:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;P(y|x, w, \sigma) = \prod_{i} (\frac{\sigma}{2\pi})^{\frac{1}{2}} \exp^{-\frac{1}{2}e_i \sigma e_i}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Each $w_i$ parameter is a 3-dimensional parameter (for each spatial dimension) to be estimated. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;KL-Divergence and M-Projections:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I am using Expectation propagation (EP) to estimate the posterior distribution and EP has a M-projection step which projects a distribution onto a simpler approximating distribution which in my case is a multivariate normal distribution over my parameters $w$. The way EP works is by exploiting the factored form of the likelihood term i.e. it starts with an approximating distribution $Q$ (multivariate normal over $w$) and then replaces each ith factor by the exact term and projecting this distribution on the current estimate of the Q.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, in this problem I replace one ith term from my likelihood expression to form the distribution to project as: $P(y|x_i, w_i, \sigma) q_{j \neq i}(w_j)$ where the first term is the exact factor and $q_{j \neq i}(w_j)$ is the approximated distribution with the influence of the ith term removed. Let us call this distribution $U$&lt;/p&gt;&#10;&#10;&lt;p&gt;According to the literature, I need to find the moments $E_U[w]$. So, I need to match the first and second order moments  i.e. compute these expectations for my parameters using the distribution $U$. However, I am completely lost as to how to do this. Can someone give me some suggestion on how I should proceed?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-21T16:30:36.203" FavoriteCount="0" Id="99573" LastActivityDate="2014-05-21T16:30:36.203" OwnerUserId="36540" PostTypeId="1" Score="1" Tags="&lt;normal-distribution&gt;&lt;optimization&gt;&lt;approximation&gt;&lt;moments&gt;&lt;kullback-leibler&gt;" Title="computing KL divergence: M projections for arbitrary distributions" ViewCount="59" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I am working with the following sample dataset:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;t1  t2  ntrial  nsuccess&#10; 1       4    1000       4&#10; 1       8    1000       8&#10; 2       4    1000       4&#10; 2       8    1000       8&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Predictors (features) t1 and t2 are categorical: t1 has categories 1 and 2, and t2 has categories 4 and 8. There is no ordering to these categories. In reality, for each combination of t1 and t2, I observe either&#10;a success, or a failure. For example, in first row (t1, t2) = (1, 4) and I recorded 4 successes from 1000 observations.&lt;/p&gt;&#10;&#10;&lt;p&gt;So really the dataset has 4000 rows (call this unrolled binary data), which can be compressed for the purposes of, say, Logistic Regression. Obviously unrolling this data is memory-inefficient, because unrolled data will look something like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;t1  t2  success&#10; 1  4    1&#10; 1  4    1&#10; 1  4    1&#10; 1  4    1&#10; 1  4    0&#10;  ... 996 zeros&#10; 1  4    0&#10; 1  8    1&#10; 1  8    1&#10; ... and so on&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In this toy example, I only have 1000 observations per row, so the data can be unrolled, but in reality my dataset has 10^9 observations, and many features/predictors, so unrolling it is not really feasible.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to know:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Is there an algorithm (in say, R, or Java, etc) which can operate on this dataset directly, without me having to unroll the rows? The algorithm has to be relatively fast and simple to train and predict.&lt;/li&gt;&#10;&lt;li&gt;Is there a cross-validation routine which can also work on this dataset without unrolling the rows?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Here's some starter R code which I tried so far. I fit weighted Logistic Regression (the more observations we have, the lower the variance of predicted quantity, so we have weighted regression on our hands) and Regression Tree.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(boot)&#10;set.seed(1)&#10;input_file = 'data\\test\\test.txt'&#10;# number of cross-validation folds&#10;K = 10&#10;input &amp;lt;- &#10;  read.csv(input_file, header=TRUE, sep='\t', quote=&quot;&quot;, &#10;           colClasses=c(rep('factor', 2), 'numeric', 'numeric'))&#10;# change the contents of the frame&#10;input$nfail = pmax(0, input$ntrial - input$nsuccess)&#10;    # compute the probability of success&#10;    input$prob = input$nsuccess/input$ntrial&#10;&#10;# fit the main model&#10;glm.model =  &#10;  glm(cbind(input$nsuccess, input$nfail) ~ &#10;        input$t1 + input$t2, family = binomial, weights = input$ntrial)&#10;&#10;cost &amp;lt;- &#10;  function(y,yhat) sum(input$ntrial*((y-yhat)^2))/sum(input$ntrial)&#10;# this produces an error, cv.glm doesn't realize rows have to be unrolled&#10;cv.glm(input, glm.model, cost = cost, K=K)$delta[1]&#10;&#10;library(tree)&#10;control = tree.control(sum(input$ntrial))&#10;    tree.model &amp;lt;- &#10;      tree(input$prob ~ input$t1 + input$t2, &#10;       weights = input$ntrial, control = control)&#10;plot(tree.model)&#10;text(tree.model, pretty=0)&#10;# this is too big to fit in memory, R fails&#10;cv.tree(tree.model, K = K)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;For example, if I want to perform 10-fold cross-validation, I have to sample each fold randomly from each of 4000 rows, either sampling a 1 or a 0 (success or failure), and then for each fold I add up the numbers of successes and failures for each particular combination of t1 and t2.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-05-21T18:31:03.033" Id="99591" LastActivityDate="2015-02-19T09:11:00.770" LastEditDate="2014-05-21T19:53:32.873" LastEditorUserId="45944" OwnerUserId="45944" PostTypeId="1" Score="3" Tags="&lt;regression&gt;&lt;logistic&gt;&lt;categorical-data&gt;&lt;cross-validation&gt;&lt;cart&gt;" Title="fit and cross-validate categorical sample data formed from observations" ViewCount="113" />
  
  <row AcceptedAnswerId="99666" AnswerCount="1" Body="&lt;p&gt;I need to implement an algorithm ( or find an implementation) and optimise it using Monte Carlo method. This must be an NP hard such as the Travelling Salesman problem or the Knapsack problem. How can a problem be optimised exactly? Also I need to have the optimum value and compare with that, is there any site that gives the optimum for such problems? (since I can't find anything)&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-05-21T19:28:00.563" Id="99602" LastActivityDate="2014-05-22T09:28:28.040" OwnerUserId="44515" PostTypeId="1" Score="-1" Tags="&lt;optimization&gt;&lt;monte-carlo&gt;" Title="NP hard implementation optimisation using Monte Carlo method" ViewCount="83" />
  <row Body="&lt;p&gt;1 . Let's say $X_{i}$ is the event taking a step.&lt;/p&gt;&#10;&#10;&lt;p&gt;$X_{i} = 1$ with probability $p$ means the person falls&lt;/p&gt;&#10;&#10;&lt;p&gt;$X_{i} = 0$ with probability $1-p$ means the person does not fall&lt;/p&gt;&#10;&#10;&lt;p&gt;I assume that your chance to fall is not correlated with what happened before (for example falling at step 10 will not increase your probability of falling at step 11)&lt;/p&gt;&#10;&#10;&lt;p&gt;Now $A_i$ is the event person falls at steps $i$ and $i+1$, so $A_i = X_i * X_{i+1}$. There are $n-1$ $A_i$ for $n$ $X_i$&lt;/p&gt;&#10;&#10;&lt;p&gt;You are looking for $P\left(\bigcup_{i=1}^{n-1} A_i\right)$ &lt;/p&gt;&#10;&#10;&lt;p&gt;Now let's notice that two $A_i$ are independent if and only if they are not consecutive&lt;/p&gt;&#10;&#10;&lt;p&gt;$P(A_i) = P(X_i \cap X_{i+1}) = p^2$ and $P(A_i \cap A_{i+1}) = P(X_i \cap X_{i+1} \cap X_{i+2}) = p^3$ else if $|i-j| &amp;gt; 1$, $P(A_i \cap A_{j}) = P(A_i) P(A_{j}) = p^4$&lt;/p&gt;&#10;&#10;&lt;p&gt;Then me should try to use de Morgan law&lt;/p&gt;&#10;&#10;&lt;h2&gt;$$\biggl|\bigcup_{i=1}^n A_i\biggr| = \sum_{k = 1}^{n} (-1)^{k+1} \left( \sum_{1 \leq i_{1} &amp;lt; \cdots &amp;lt; i_{k} \leq n} \left| A_{i_{1}} \cap \cdots \cap A_{i_{k}} \right| \right)$$&lt;/h2&gt;&#10;&#10;&lt;p&gt;Anything after this is false&lt;/p&gt;&#10;&#10;&lt;p&gt;So $P\left(\bigcup_{i=1}^{n-1} A_i\right) = \sum_{i=1}^{n-1} P(A_i) - \sum_{i=1}^{n-2}P(A_i \cap A_{i+1})$&lt;/p&gt;&#10;&#10;&lt;p&gt;because $P(A \cup B) = P(A) + P(B) - P(A \cap B)$ and $P(A \cap B) = P(A)P(B)$ if A and B are independent.&lt;/p&gt;&#10;&#10;&lt;p&gt;As &lt;/p&gt;&#10;&#10;&lt;p&gt;$$P\left(\bigcup_{i=1}^{n-1} A_i\right) = (n-1) p^2 - (n-2)p^3$$&lt;/p&gt;&#10;&#10;&lt;p&gt;So with your problem $p = 0.001\% = 0.00001$ and $n = 1000$, $P = 9.9899 * 10^{-8}$&lt;/p&gt;&#10;&#10;&lt;p&gt;2 . The probability that at least someone falls equals 1 minus the probability that nobody falls. So with $n = 10000$, $P_{falling} = 9.9989 * 10^{-7}$&lt;/p&gt;&#10;&#10;&lt;p&gt;As all the events are independent on the person the probability that nobody falls equals the probability that one person does not fall power 1000000.&lt;/p&gt;&#10;&#10;&lt;p&gt;$P_{1000000} = 1 - (1 - P_{falling})^{1000000} = 0.63208$&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-05-21T19:35:43.577" Id="99603" LastActivityDate="2014-05-22T00:13:23.597" LastEditDate="2014-05-22T00:13:23.597" LastEditorUserId="45892" OwnerUserId="45892" ParentId="99589" PostTypeId="2" Score="0" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;For the purpose of this question, please consider me a stats newbie.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm working on a (very fun!) research project which involves estimating a pdf of &quot;personal values&quot; -- i.e. how much a certain person values a certain good they can buy.&lt;/p&gt;&#10;&#10;&lt;p&gt;I wanted to find the functional form of the PDF so that I could generate data according to the distribution I saw in the empirical data I've been working with.&lt;/p&gt;&#10;&#10;&lt;p&gt;But I received a suggestion to follow Maximum Likelihood Estimation, which isn't something I've done in the past. I've been experimenting but my data doesn't seem to likely fit any typical distribution... it's more of a &quot;bactrian camel&quot; distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;So what I'm interested in knowing is:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Is my current approach correct? (see below)&lt;/li&gt;&#10;&lt;li&gt;How might I approach this with Maximum Likelihood Estimation?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Here's what I've done so far:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Constructed a CDF of the data I have (taking them as &quot;valuations&quot;)&lt;/li&gt;&#10;&lt;li&gt;Literally... found the OLS estimator of the CDF, in functional form, so now I have a polynomial estimation of the CDF&lt;/li&gt;&#10;&lt;li&gt;Took the derivative of that function (which should equal the PDF)&lt;/li&gt;&#10;&lt;li&gt;Generated data according to that derivative.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Overall, this approach has performed fairly well. I don't have any metrics to compare my generated data with the actual data -- I don't know any! -- but from comparing the two datasets, they're fairly close in mean and variance.&lt;/p&gt;&#10;&#10;&lt;p&gt;Much appreciated!&lt;/p&gt;&#10;&#10;&lt;p&gt;-- An enthusiastic college kid&lt;/p&gt;&#10;&#10;&lt;p&gt;Edit: Adding details:&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's what my density function looks like. I'd like to approximate a functional form of this, because I need to do two things:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Infer the distribution of data above 150; according to theory, there exist data that is above 150, but it's not in this distribution.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Generate random data (say a sample size of 5,000 -&gt; 10,000)&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/pA3vd.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The distribution I approximated through my take-derivative-of-CDF-estimation looks like this (ignore the improper scaling of y-axis -- this is scaled up)&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Od6tK.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-05-21T21:47:54.873" FavoriteCount="1" Id="99619" LastActivityDate="2015-03-05T12:58:39.400" LastEditDate="2014-05-22T17:52:12.020" LastEditorUserId="45961" OwnerUserId="45961" PostTypeId="1" Score="1" Tags="&lt;estimation&gt;&lt;maximum-likelihood&gt;&lt;pdf&gt;&lt;function&gt;" Title="Generating functional-form PDF from Max Likelihood Estimation" ViewCount="80" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm working with unbalanced panel data using time and firms as IDs and would like to find out how to test for correlation between two panel equations that may be seemingly unrelated.&lt;/p&gt;&#10;&#10;&lt;p&gt;The second question is about any package that allows the use of a Heckman selection model for panel data in Stata.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-05-22T01:52:43.763" Id="99635" LastActivityDate="2015-03-01T07:23:40.030" LastEditDate="2014-05-22T02:45:43.513" LastEditorUserId="32036" OwnerUserId="45968" PostTypeId="1" Score="1" Tags="&lt;multivariate-analysis&gt;&lt;stata&gt;&lt;econometrics&gt;&lt;panel-data&gt;" Title="SUR estimation and Heckman selection model with panel data on Stata?" ViewCount="181" />
  <row AcceptedAnswerId="99646" AnswerCount="1" Body="&lt;p&gt;Should I be concerned about multicollinearity in nonparametric statistics?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-22T02:29:29.590" FavoriteCount="1" Id="99640" LastActivityDate="2014-05-22T02:59:10.617" OwnerUserId="25167" PostTypeId="1" Score="4" Tags="&lt;nonparametric&gt;&lt;multicollinearity&gt;" Title="Is multicollinearity a concern in nonparametric statistics?" ViewCount="156" />
  <row Body="&lt;p&gt;Yes.  Nonparametric vs. parametric is an unrelated issue from multicollinearity.  If you have a multi-variable model (i.e., multiple input variables), then you should potentially be concerned about high levels of collinearity amongst those input variables.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Most of what we think of as prototypical nonparametric analyses are not multi-variable models, though.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-22T02:59:10.617" Id="99646" LastActivityDate="2014-05-22T02:59:10.617" OwnerUserId="7290" ParentId="99640" PostTypeId="2" Score="8" />
  
  <row Body="&lt;p&gt;You have several issues.&lt;/p&gt;&#10;&#10;&lt;p&gt;For one, these three coefficients are not significant, basically, indistinguishable from zeros at 95% confidence (see the t-stats).&lt;/p&gt;&#10;&#10;&lt;p&gt;Second, two of your correlations are also not significant, i.e. indistinguishable from zeros.&lt;/p&gt;&#10;&#10;&lt;p&gt;Third, your correlations are unconditional, i.e. they do not take into account what's going on with other variables. Imagine this, you have two variables: age and sex (1-male, 0- female). Your dependent is salary. So, you compute correlation of salary and sex, and it comes negative. It surprises you. &lt;/p&gt;&#10;&#10;&lt;p&gt;So, you run a regression of salary ~ sex + age. The coefficient on sex comes positive as expected. What's the matter? It turns out in your sample male were younger in average. So, when you run a regression and controlled for age, the sex coefficient came out right.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-05-22T03:31:00.960" Id="99650" LastActivityDate="2014-05-22T03:31:00.960" OwnerUserId="36041" ParentId="99634" PostTypeId="2" Score="2" />
  <row AnswerCount="0" Body="&lt;p&gt;I have already understood that modeling time dependent covariates should be analog to his:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;cfit &amp;lt;- coxph(Surv(tstart, tstop, status) ~ treat + sex + age +&#10;inherit + cluster(id), data=cgd)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In my dataset, however, the dependent variable has no apart category for 'start' en 'stop'. Just a list of random numbers. I'll copy the upper part:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;12&#10;9&#10;27&#10;15&#10;14&#10;2&#10;8&#10;...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The dependent variable is time until infection. This should be written as a function of age, gender, time until prophalctic treatment an time until excision (the last two are time dependent)&lt;/p&gt;&#10;&#10;&lt;p&gt;Does anyone know how I should model this?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-22T06:14:24.363" Id="99655" LastActivityDate="2014-05-22T06:38:45.600" LastEditDate="2014-05-22T06:38:45.600" LastEditorUserId="26338" OwnerUserId="45595" PostTypeId="1" Score="0" Tags="&lt;r&gt;" Title="How model time-dependent covariates?" ViewCount="39" />
  <row AnswerCount="1" Body="&lt;p&gt;In a mixed effect model where the intercept is random effect and the slope is fixed effect (see the code below), I understand the output of &lt;code&gt;summary(glmer(...))&lt;/code&gt;. But I do not understand &lt;code&gt;coef(glmer(...))&lt;/code&gt;; it will output the intercept for each sample. In the example, how are those 100 intercepts estimated? &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;n &amp;lt;- 100&#10;x &amp;lt;- runif(n,2,6)&#10;a &amp;lt;- -3 &#10;b &amp;lt;- 1.5&#10;s &amp;lt;- 1&#10;N &amp;lt;- 8&#10;id &amp;lt;- 1:n&#10;r  &amp;lt;- rnorm(length(x),0,s) # random factor&#10;p &amp;lt;- function(x,a,b) exp(a+b*x)/(1+exp(a+b*x))&#10;y &amp;lt;- rbinom(length(x), N, prob = p(x,a+r,b))&#10;&#10;library(lme4)&#10;model &amp;lt;- glmer(cbind(y,N-y)~x+(1|id),family=binomial)&#10;summary(model)&#10;coef(model) # output here is what I do not understand&#10;&#10;# are they estimates for r?&#10;rr &amp;lt;- coef(model)$id[,1]-summary(model)$coefficients[1,1]&#10;plot(r,rr)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The likelihood of the model, I think, is:&#10;$$&#10;L_{i}=\int_{-\infty}^{\infty}f(y_i|N,a,b,r_{i})g(r_{i}|s)dr_{i}\\&#10;L=\prod_{i=1}^{n} L_{i}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where &lt;em&gt;f&lt;/em&gt; is binomial pmf and &lt;em&gt;g&lt;/em&gt; is normal pdf. So &lt;em&gt;r&lt;/em&gt; should be integrated out. The number of the parameters of this model is 3 (&lt;em&gt;a&lt;/em&gt;, &lt;em&gt;b&lt;/em&gt;, and &lt;em&gt;s&lt;/em&gt;). Although it seems there are 100 intercepts estimated, they are not really considered the parameters of the model, I think. &lt;code&gt;AIC(model)&lt;/code&gt; is &lt;code&gt;-2*logLik(model)+3*2&lt;/code&gt;. I would like to know what method will give those 100 intercepts.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-22T08:02:49.450" Id="99660" LastActivityDate="2014-05-23T07:50:43.470" LastEditDate="2014-05-23T07:50:43.470" LastEditorUserId="44163" OwnerUserId="44163" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;mixed-model&gt;&lt;glmm&gt;&lt;mixed-effect&gt;&lt;glmer&gt;" Title="Coefficients from glmer in R" ViewCount="592" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;It is often stated that the square of the sample correlation $r^2$ is equivalent to the $R^2$ coefficient of determination for simple linear regression. I have been unable to demonstrate this myself and would appreciate a full proof of this fact.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-05-22T10:03:16.127" FavoriteCount="1" Id="99669" LastActivityDate="2015-01-14T03:52:14.723" OwnerUserId="45984" PostTypeId="1" Score="3" Tags="&lt;regression&gt;&lt;correlation&gt;" Title="The equivalence of sample correlation and R statistic for simple linear regression" ViewCount="94" />
  <row Body="&lt;p&gt;The $R^2$ is defined as&#10;$$R^2=\frac{\hat{V}(\hat{y}_i)}{\hat{V}(y_i)}&#10;=\frac{1/(N-1)\sum_{i=1}^N(\hat{y}_i-\bar{y})^2}{1/(N-1)\sum_{i=1}^N(y_i-\bar{y})^2}=\frac{ESS}{TSS}&#10;$$&#10;The squared sample correlation coefficient:&#10;$$r^2(y_i,\hat{y}_i)=\frac{\left(\sum_{i=1}^N(y_i-\bar{y})(\hat{y}_i-\bar{y})\right)^2}{\left(\sum_{i=1}^N(y_i-\bar{y})^2\right)\left(\sum_{i=1}^N(\hat y_i-\bar{y})^2\right)}&#10;$$&#10;is equivalent, as it is easily verified using:&#10;$$\hat V(y_i)=\hat V(\hat y_i)+\hat V(e_i)&#10;$$&#10;(see &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/1119951674&quot; rel=&quot;nofollow&quot;&gt;Verbeek&lt;/a&gt;, 2.4)&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-05-22T10:48:52.173" Id="99677" LastActivityDate="2014-05-22T10:48:52.173" OwnerUserId="44965" ParentId="99669" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="99691" AnswerCount="2" Body="&lt;p&gt;I was wondering if anyone could elaborate more on this statement which I came across whilst reading a book on non-linear mixed effects model. I know that you can have linear, generalised linear and non-linear  mixed effect models. &lt;/p&gt;&#10;&#10;&lt;p&gt;The book says &quot;mixed models are non-linear statistical models, due mainly to the presence of variance parameters&quot;. How do I explain this to someone who understands very little statistics?&lt;/p&gt;&#10;&#10;&lt;p&gt;For instance, how can a linear mixed effect model be non-linear?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-05-22T11:46:50.427" Id="99683" LastActivityDate="2014-05-22T20:52:47.257" LastEditDate="2014-05-22T11:55:37.077" LastEditorUserId="35574" OwnerUserId="35574" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;mixed-model&gt;" Title="Why is a Mixed model a non-linear statistical model" ViewCount="144" />
  
  <row Body="&lt;p&gt;The usual unbiased estimate of $\sigma^2$ is the &lt;em&gt;error mean square&lt;/em&gt;:&#10;$$MSE = \sum_i\sum_j (y_{ij}-\bar y_{i*})^2/(n_T-k), \quad n_T=\sum_i n_i$$&#10;If you don't estimate $\sigma^2$, i.e. if you don't compute $MSE$, then:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;no analysis of variance table is available, since in such tables there are:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;the &lt;em&gt;treatment sum of squares&lt;/em&gt;:&#10;$$SSTR=\sum_i n_i(\bar y_{i*}-\bar y_{**})^2$$&lt;/li&gt;&#10;&lt;li&gt;the explained variance, the &lt;em&gt;treatment mean square&lt;/em&gt;:&#10;$$MSTR=SSTR/(k-1)$$&lt;/li&gt;&#10;&lt;li&gt;the error (within treatment) variability, the &lt;em&gt;error sum of squares&lt;/em&gt;:&#10;$$SSE=\sum_i\sum_j(y_{ij}-\bar{y}_{i*})^2$$&lt;/li&gt;&#10;&lt;li&gt;the unexplained (or residual) variance, the &lt;em&gt;error mean square&lt;/em&gt;:&#10;$$MSE=SSE/(n_T-k)$$&lt;/li&gt;&#10;&lt;li&gt;the overall variability, the &lt;em&gt;total sum of squares&lt;/em&gt;: $$SSTO=SSTR+SSE$$&lt;/li&gt;&#10;&lt;/ul&gt;&lt;/li&gt;&#10;&lt;li&gt;no $F$-test is possible, because an $F$ statistic is defined as&#10;$$F^*=\frac{MSTR}{MSE}$$&lt;/li&gt;&#10;&lt;li&gt;no standard error can be computed, therefore no $t$ statistic, no p-value and no confidence interval, since the standard error is defines as&#10;$$s=\sqrt{\frac{MSE}{n_i}}$$&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;In brief, you don't know if you can trust in your results.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you are looking for a reference, I'd suggest Kutner et al., &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/007310874X&quot; rel=&quot;nofollow&quot;&gt;Applied Linear Statistical Models&lt;/a&gt;, Chapter 16.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-05-22T12:38:07.370" Id="99692" LastActivityDate="2014-05-22T12:38:07.370" OwnerUserId="44965" ParentId="60130" PostTypeId="2" Score="3" />
  <row AnswerCount="1" Body="&lt;p&gt;I did a number of machine learning experiments to predict a binary classification. I measured precision, recall and accuracy.&lt;/p&gt;&#10;&#10;&lt;p&gt;I noticed that my precision is generally quite high, and recall and accuracy are always the same numbers.&lt;/p&gt;&#10;&#10;&lt;p&gt;I used the following definitions:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\text{Precision} = \frac{TP}{(TP + FP)}$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\text{Recall} = \frac{TP}{(TP + FN)}$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\text{Accuracy} = \frac{(TP + TN)}{(P + N)}$&lt;/p&gt;&#10;&#10;&lt;p&gt;I have some difficulties to interpret accuracy and recall. What does it mean if these two number are always the same in my case?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-22T12:41:18.170" Id="99694" LastActivityDate="2014-05-22T13:28:09.483" LastEditDate="2014-05-22T12:44:24.577" LastEditorUserId="26338" OwnerUserId="3908" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;precision-recall&gt;&lt;accuracy&gt;" Title="What does it imply if accuracy and recall are the same?" ViewCount="112" />
  
  <row Body="&lt;p&gt;Statistical calculations can answer this question:  IF you collected many random samples, in what fraction would the mean be as far (or further) from the population mean as you observed? If the answer is tiny, you might suspect that the sample was not random.&lt;/p&gt;&#10;&#10;&lt;p&gt;But statistical calculations cannot answer the question: Was this a random sample? The only way to know that is to find out how the sampling was done. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-22T13:50:27.807" Id="99700" LastActivityDate="2014-05-22T13:50:27.807" OwnerUserId="25" ParentId="99690" PostTypeId="2" Score="5" />
  
  
  
  
  <row Body="&lt;p&gt;You can simulate from an unknown distribution like this pretty easily (and you won't have to specify a distribution):&lt;/p&gt;&#10;&#10;&lt;p&gt;1) Calculate empirical cdf function $\hat{F}$.  In R, see ecdf function.&lt;/p&gt;&#10;&#10;&lt;p&gt;2) Generate vector of random uniforms $u$.  In R, see runif function.&lt;/p&gt;&#10;&#10;&lt;p&gt;3) Calculate $\hat{F}^{-1}(u)$ for each $u$.  See R quantile funtion.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's link for why this works: &lt;a href=&quot;https://en.wikipedia.org/wiki/Inverse_transform_sampling&quot; rel=&quot;nofollow&quot;&gt;https://en.wikipedia.org/wiki/Inverse_transform_sampling&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-05-22T18:02:01.623" Id="99733" LastActivityDate="2014-05-22T18:02:01.623" OwnerUserId="45823" ParentId="99619" PostTypeId="2" Score="0" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a discrete representation of a surface/height-map $z = f(x,y)$ that i want to model as a mixture of gaussians (please take probability distributions out of your mind for a moment). More precisely, &lt;/p&gt;&#10;&#10;&lt;p&gt;$ z = f(x) = \sum\limits_{i=1}^{n_g} a_i \; g(x; c_i, \Sigma_i)$&lt;/p&gt;&#10;&#10;&lt;p&gt;where  &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;$x \in R^d$&lt;/li&gt;&#10;&lt;li&gt;$g(x; c_i, \Sigma_i) = exp\{-\frac{(x-c_i)^T\Sigma_i^{-1}(x-c_i)}{2} \}$,   &lt;/li&gt;&#10;&lt;li&gt;$a_i \geq 0$ is the max-height of the ith gaussian component,   &lt;/li&gt;&#10;&lt;li&gt;$c_i \in R^d$ is the center of the ith gaussian component, and   &lt;/li&gt;&#10;&lt;li&gt;$\Sigma_i$ is the covariance of the i'th gaussian component that determines its support in space and its orientation.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Given the data of the surface, i want to fit this model i.e estimate the parameters $[...,a_i, c_i, \Sigma_i, ...]$ given some reasonable initial guesses by minimizing the least squares objective function given below:&lt;/p&gt;&#10;&#10;&lt;p&gt;$E(a,c,\Sigma) = \sum\limits_{j=1}^N \left(z_j - f(x_j; a, c, \Sigma)\right)^2$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Constraints:&lt;/em&gt; &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;$a_i \geq 0$, &lt;/li&gt;&#10;&lt;li&gt;$\Sigma_i$ should be symmetric ($\Sigma_i = \Sigma_i^T$), and positive-definite ($u^T\Sigma_i u &amp;gt; 0 \; \forall{u} \in R^d$).&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I would like to emphasize that i want to optimize for the full covariance matrices $\Sigma_i$ (even diagonal wont suffice).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Approach so far&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I will describe below the approach i have been venturing on so far but i &lt;em&gt;would really appreciate any other ideas in fitting this model&lt;/em&gt; to data or &lt;em&gt;atleast point me to the right literature&lt;/em&gt; to read (googling takes me to so much literature on fitting a mixture model to a probability distribution using EM). I might have a huge number of components in the mixture. So it will be nice if you could &lt;em&gt;supplement your answers with comments on computational complexity, the quality of the solution (how far from global minimum) and sensitivity to initialization (how close should the initial guesses be)&lt;/em&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;My initial idea is to use iterative minimization algorithms such as the &lt;a href=&quot;http://en.wikipedia.org/wiki/Gauss%E2%80%93Newton_algorithm&quot; rel=&quot;nofollow&quot;&gt;Gauss-Newton&lt;/a&gt; algorithm or the &lt;a href=&quot;http://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm&quot; rel=&quot;nofollow&quot;&gt;LevenbergMarquardt&lt;/a&gt; algorithm.&lt;/p&gt;&#10;&#10;&lt;p&gt;So i started off by deriving the derivatives of the error/objective function $E(a,c,\Sigma)$ with respect to each of its parameters:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\begin{eqnarray}&#10;\frac{\partial E}{\partial a_i} &amp;amp; = &amp;amp; -2\sum\limits_{j=1}^N \left(z_j - f(x_j; a, c, \Sigma)\right) \frac{\partial f(x_j; a, c, \Sigma)}{\partial a_i} \\&#10;&amp;amp; = &amp;amp; -2\sum\limits_{j=1}^N \left(z_j - f(x_j; a, c, \Sigma)\right) g(x_j; c_i, \Sigma_i)&#10;\end{eqnarray}$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\begin{eqnarray}&#10;\frac{\partial E}{\partial c_i} &amp;amp; = &amp;amp; -2\sum\limits_{j=1}^N \left(z_j - f(x_j; a, c, \Sigma)\right) \frac{\partial f(x_j; a, c, \Sigma)}{\partial c_i} \\&#10;&amp;amp; = &amp;amp; -2\sum\limits_{j=1}^N \left(z_j - f(x_j; a, c, \Sigma)\right) \; a_i \frac{\partial g(x_j; c_i, \Sigma_i)}{\partial c_i} \\&#10;&amp;amp; = &amp;amp; -2 \sum\limits_{j=1}^N \left(z_j - f(x_j; a, c, \Sigma)\right) \; a_i \; g(x_j; c_i, \Sigma_i) \; \Sigma_i^{-1} (x_j - c_i)&#10;\end{eqnarray}$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\begin{eqnarray}&#10;\frac{\partial E}{\partial \Sigma_i} &amp;amp; = &amp;amp; -2\sum\limits_{j=1}^N \left(z_j - f(x_j; a, c, \Sigma)\right) \frac{\partial f(x_j; a, c, \Sigma)}{\partial \Sigma_i} \\&#10;&amp;amp; = &amp;amp; -2\sum\limits_{j=1}^N \left(z_j - f(x_j; a, c, \Sigma)\right) \; a_i \frac{\partial g(x_j; c_i, \Sigma_i)}{\partial \Sigma_i} \\&#10;&amp;amp; = &amp;amp; -2 \sum\limits_{j=1}^N \left(z_j - f(x_j; a, c, \Sigma)\right) \; a_i \; g(x_j; c_i, \Sigma_i) \; \frac{\Sigma_i^{-T} (x_j - c_i)(x_j - c_i)^T\Sigma_i^{-T}}{2} &#10;\end{eqnarray}$&lt;/p&gt;&#10;&#10;&lt;p&gt;Now while i minimize the error function $E(a,c,\Sigma)$ using one of the iterative minimization algorithms, &lt;em&gt;i want to enforce that the covariance matrices $\Sigma_i$ are symmetric and positive definite. And i dont know how to enforce that constraint?&lt;/em&gt; (should i try to optimize for elements on one side of the diagonal which is sort of projecting it to the domain of symmetric matrices but not necessarily positive definite?) I am guessing this is an already solved problem atleast reasonably. It would be great if you could point me to the literature for this problem so i can move forward.&lt;/p&gt;&#10;&#10;&lt;p&gt;Alternatively, coming from a computer-vision + machine-learning background, i can't resist but to think about applying the Expectation-Maximization algorithm. Intuitively, if i knew which guassian component each (x,y,z) location belongs to (in a soft sense) then i can compute the parameters of the gaussian components. And, if i know the parameters of the guassian mixture, I can compute the degree to which each (x,y,z) location belongs each component in the mixture. And, Expectation-Maximization algorithms in the end boils down to minimization of an objective function using coordinate descent. So it would be nice if you could &lt;em&gt;point me to any literature you can point me to that takes the EM approach to this problem&lt;/em&gt;.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-05-22T20:36:11.107" FavoriteCount="0" Id="99753" LastActivityDate="2014-05-24T09:57:36.157" LastEditDate="2014-05-23T21:34:42.023" LastEditorUserId="29452" OwnerUserId="29452" PostTypeId="1" Score="6" Tags="&lt;optimization&gt;&lt;least-squares&gt;&lt;nonlinear-regression&gt;&lt;curve-fitting&gt;&lt;gaussian-mixture&gt;" Title="Gaussian mixture regression in higher dimensions" ViewCount="166" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;There seem to be too many points clustered around negative values for all the plots &#10;And while 3 &amp;amp; 4 seem to have random enough patterns, 1 &amp;amp; 2 seems to have negatively sloped trend. &lt;/p&gt;&#10;&#10;&lt;p&gt;If these were to violate linearity and homogeneity assumption, I should stop using the regression model, correct? &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/oVR4u.png&quot; alt=&quot;Residual plot variable 1&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/jY8yb.png&quot; alt=&quot;Residual plot variable 2&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/dpqmX.png&quot; alt=&quot;Residual plot variable 3&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/EKpXn.png&quot; alt=&quot;Residual Plot variable 4&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-23T03:44:04.020" Id="99778" LastActivityDate="2014-05-23T04:53:29.380" LastEditDate="2014-05-23T03:48:41.970" LastEditorUserId="7290" OwnerUserId="46046" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;residuals&gt;&lt;assumptions&gt;" Title="Do these residual plots violate the linearity and homogeneity assumptions for linear regression?" ViewCount="187" />
  <row AnswerCount="0" Body="&lt;p&gt;I plan to fit a GAM or GAMM. There is one categorical variable which I think is important for explaining Y (or Y*), but it is not in my dataset - it is measurable but has not been measured.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can I use a mixture model (GAMM) to compensate for the omission of this variable? If I had this variable, I would have just used a GAM.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can you recommend some software for me to use? Do you have examples?&#10;Is the gamm4 R package &quot;the way to go&quot;?&lt;/p&gt;&#10;&#10;&lt;p&gt;Other details: &#10;Y is count data, so I plan to use a Negative Binomial model or &quot;family&quot;.&#10;The purpose of this taks is to investigate the relationships between the variables.&#10;Although the dataset, with N rows, fits into memory of my machine, a matrix with NxN rows will not fit into memory.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-05-23T04:11:58.420" FavoriteCount="1" Id="99782" LastActivityDate="2014-05-23T04:11:58.420" OwnerUserId="16596" PostTypeId="1" Score="1" Tags="&lt;negative-binomial&gt;&lt;mixed-effect&gt;&lt;gaussian-mixture&gt;&lt;gam&gt;&lt;gamm4&gt;" Title="Can i use a mixture model for when I have an omitted variable?" ViewCount="56" />
  <row AnswerCount="1" Body="&lt;p&gt;I have started learning classification in machine learning. I face two terminologies, one is &quot;single class classification&quot; and a the other is &quot;binary class classification&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am confused about when to use single class classification and when to use binary class classification. My question mainly concentrates on single class classification.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Why do we need single class classification?&lt;/strong&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-23T07:53:02.003" Id="99790" LastActivityDate="2014-05-23T09:48:31.240" LastEditDate="2014-05-23T09:48:31.240" LastEditorUserId="28740" OwnerUserId="12710" PostTypeId="1" Score="2" Tags="&lt;machine-learning&gt;&lt;classification&gt;&lt;terminology&gt;" Title="Why we need single class classification?" ViewCount="77" />
  
  <row Body="&lt;p&gt;It doesn't matter that your data isn't normally distributed if you want to find a confidence interval for the mean, because your sample size is far larger than 30 (Central Limit Theorem).&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-05-23T10:09:43.020" Id="99804" LastActivityDate="2014-05-23T10:09:43.020" OwnerUserId="46055" ParentId="99721" PostTypeId="2" Score="0" />
  
  <row AnswerCount="0" Body="&lt;p&gt;After fitting a multinomial model to my data with the &quot;multinom&quot; function (package nnet), I want to show the effect of selected variables controlling for other variable values. I know that the &quot;effects&quot; package do mainly what I want, but I want to be able to calculate the prediction error (confidence interval) by myself. &#10;Does someone could tell me the methodology and if possible the R code?&#10;I think we should use the delta method, but I'm not sure how to apply it in this case.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is a small example code (based on data available in the effects package)&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(nnet)&#10;library(effects)&#10;mod &amp;lt;- multinom(vote ~ age + gender, data=BEPS)&#10;summary(mod)&#10;&#10;# Call:&#10;# multinom(formula = vote ~ age + gender, data = BEPS)&#10;&#10;# Coefficients:&#10;#                  (Intercept)         age gendermale&#10;# Labour             1.2241862 -0.01562320  0.1682676&#10;# Liberal Democrat   0.4979706 -0.01551381  0.1240998&#10;&#10;# Std. Errors:&#10;#                  (Intercept)         age gendermale&#10;# Labour             0.2277826 0.003830006  0.1204621&#10;# Liberal Democrat   0.2694373 0.004578836  0.1436882&#10;&#10;# Residual Deviance: 3186.266 &#10;# AIC: 3198.266 &#10;&#10;plot(allEffects(mod))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/TbYtq.png&quot; alt=&quot;output of the effect function&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The only thing I need is to be able to calculate the values of errors shown in this graph !&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you in advance,&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-05-15T19:58:42.253" Id="99808" LastActivityDate="2014-05-23T11:24:45.420" OwnerDisplayName="Arnaud" OwnerUserId="28050" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;prediction&gt;&lt;nnet&gt;" Title="Calculate prediction error for a multinomial output like in the &quot;effects&quot; package" ViewCount="144" />
  <row Body="&lt;p&gt;We have studied very similar problems mostly in the retail sector. One needs to recognize that the eventual hourly forecasts need to aggregate/reconcile to the monthly forecast. We have implemented a two-prong approach which uses monthly values to predict hourly values taking into account monthly patterns and hourly effects. These models also incorporate any level shifts or trends that may be found to be statistically significant. Additionally any auto-regressive structure is also incorporated ( previous prices). Care is also taken to identify anomalous data and adjusting for these effects to ensure a robust solution. Known events such as Holidays and other possible causes are also included/tested in a very sophisticated manner. The software package I have helped develop called AUTOBOX implements this solution. I would suggest that you review it ( there is a free 30 day trial ) and if you like the results either acquire it or rewrite it in any language/system that you wish.&#10;Finally one needs to validate that the final model has an error process that has constant variance and that the model's parameters are invariant over time. &lt;/p&gt;&#10;&#10;&lt;p&gt;In closing the recruiter is simply trying to find that employee that can solve a problem that they can't currently solve OR is as creative/ingenious as your are to be able to reach out to subject matter experts and acquire/find an answer. If I were them I would definitely hire you !&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2014-05-23T12:10:52.553" Id="99814" LastActivityDate="2014-05-23T12:10:52.553" OwnerUserId="3382" ParentId="99807" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have three general questions that I am really struggling to answer:&lt;/p&gt;&#10;&#10;&lt;p&gt;a) When we have nested data (e.g. employees nested in departments which are nested in companies which are in specific geographical regions) how data mining methods adjust to nesting?&lt;/p&gt;&#10;&#10;&lt;p&gt;b) Are data mining methods &quot;&quot;immune&quot;&quot; to nested data because of their goal (i.e. predictive analysis)?&lt;/p&gt;&#10;&#10;&lt;p&gt;c)Can we use data mining methods (e.g. neural networks, random forests) as precursors of variable selection before we run multilevel analysis?&lt;/p&gt;&#10;&#10;&lt;p&gt;Please accept my apologies if the general orientations of these questions are inconvenient to you.&#10;Thank you VERY much for your help. I am looking forward to receive your wise views.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-23T12:41:10.103" Id="99817" LastActivityDate="2014-05-23T14:43:21.033" LastEditDate="2014-05-23T14:43:21.033" LastEditorUserId="22478" OwnerUserId="22478" PostTypeId="1" Score="2" Tags="&lt;data-mining&gt;&lt;nested&gt;" Title="Data Mining Methods with Nested Data" ViewCount="41" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have a bunch of raw data values that are dollar amounts and I want to find a confidence interval for a percentile of that data.  Is there a formula for such a confidence interval?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for any help&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-23T14:38:09.333" FavoriteCount="2" Id="99829" LastActivityDate="2014-05-23T19:20:31.723" OwnerUserId="9495" PostTypeId="1" Score="2" Tags="&lt;confidence-interval&gt;&lt;quantiles&gt;&lt;empirical&gt;" Title="Confidence Interval for Percentile for Empirical Distribution" ViewCount="118" />
  <row Body="&lt;p&gt;&lt;em&gt;I'm not sure if I completely follow your thinking, but I can update this if it isn't what you're looking for.&lt;/em&gt;  &lt;/p&gt;&#10;&#10;&lt;p&gt;In general we compare the intra-group variance with the inter-group variance with the &lt;a href=&quot;http://en.wikipedia.org/wiki/Analysis_of_variance&quot; rel=&quot;nofollow&quot;&gt;ANOVA&lt;/a&gt;.  (By the way, the more usual terms are within-group variance and between-group variance.)  The standard use of the ANOVA may not be what you are after, though.  It is used, ultimately, to determine if the means are the same (that's the inter-group variance part).  In addition, it assumes that the group variances (intra-) are the same in order for the check of the inter-group variance to be valid.  &lt;/p&gt;&#10;&#10;&lt;p&gt;If you want to know if the intra-group variances are the same, you can use Levene's test, which is an ANOVA on the absolute differences of each point from its group mean.  In R, the function is &lt;code&gt;leveneTest(obs, team, center=mean)&lt;/code&gt; in the car package (&lt;a href=&quot;http://hosho.ees.hokudai.ac.jp/~kubo/Rdoc/library/car/html/leveneTest.html&quot; rel=&quot;nofollow&quot;&gt;documentation&lt;/a&gt;).  For a slightly more robust version, you can get the absolute value of the differences from the &lt;em&gt;median&lt;/em&gt; and run an ANOVA on them instead.  In that case, it is called the &lt;a href=&quot;http://en.wikipedia.org/wiki/Brown%E2%80%93Forsythe_test&quot; rel=&quot;nofollow&quot;&gt;Brown-Forcythe test&lt;/a&gt;.  That is actually the default for &lt;code&gt;leveneTest&lt;/code&gt; (ironically), so you can just drop the &lt;code&gt;center=mean&lt;/code&gt; part.  I discuss these tests here: &lt;a href=&quot;http://stats.stackexchange.com/a/24024/7290&quot;&gt;Why use Levene's test of equality of variance rather than F ratio?&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;If you believe you have substantial heteroscedasticity, but want to test if your means differ as well, there are a number of methods available.  I discuss them here: &lt;a href=&quot;http://stats.stackexchange.com/a/91881/7290&quot;&gt;Alternatives to one-way ANOVA for heteroscedastic data&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-05-23T14:38:10.447" Id="99830" LastActivityDate="2014-05-23T14:49:18.093" LastEditDate="2014-05-23T14:49:18.093" LastEditorUserId="7290" OwnerUserId="7290" ParentId="99819" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Have you looked at &lt;a href=&quot;https://en.wikipedia.org/wiki/Bland%E2%80%93Altman_plot&quot; rel=&quot;nofollow&quot;&gt;the Wikipedia entry&lt;/a&gt; I linked in your question?&lt;/p&gt;&#10;&#10;&lt;p&gt;You don't plot &quot;the mean of the data&quot;, but for each data point measured in two ways, you plot the difference in the two measurements ($y$) against the average of the two measurements ($x$). Using R and some toy data:&lt;/p&gt;&#10;&#10;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;&amp;gt; set.seed(1)&#10;&amp;gt; measurements &amp;lt;- matrix(rnorm(20), ncol=2)&#10;&amp;gt; measurements&#10;            [,1]        [,2]&#10; [1,] -0.6264538  1.51178117&#10; [2,]  0.1836433  0.38984324&#10; [3,] -0.8356286 -0.62124058&#10; [4,]  1.5952808 -2.21469989&#10; [5,]  0.3295078  1.12493092&#10; [6,] -0.8204684 -0.04493361&#10; [7,]  0.4874291 -0.01619026&#10; [8,]  0.7383247  0.94383621&#10; [9,]  0.5757814  0.82122120&#10;[10,] -0.3053884  0.59390132&#10;&amp;gt; xx &amp;lt;- rowMeans(measurements)        # x coordinate: row-wise average&#10;&amp;gt; yy &amp;lt;- apply(measurements, 1, diff)  # y coordinate: row-wise difference&#10;&amp;gt; xx&#10; [1]  0.4426637  0.2867433 -0.7284346 -0.3097095  0.7272193 -0.4327010  0.2356194  &#10;      0.8410805  0.6985013  0.1442565&#10;&amp;gt; yy&#10; [1]  2.1382350  0.2061999  0.2143880 -3.8099807  0.7954231  0.7755348 -0.5036193  &#10;      0.2055115  0.2454398  0.8992897&#10;&amp;gt; plot(xx, yy, pch=19, xlab=&quot;Average&quot;, ylab=&quot;Difference&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/vdzoe.png&quot; alt=&quot;Bland-Altman plot without limits of agreement&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;To get the limits of agreement (see under &quot;Application&quot; in the Wikipedia page), you calculate the mean and the standard deviation of the differences, i.e., the $y$ values, and plot horizontal lines at the mean $\pm 1.96$ standard deviations.&lt;/p&gt;&#10;&#10;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;&amp;gt; upper &amp;lt;- mean(yy) + 1.96*sd(yy)&#10;&amp;gt; lower &amp;lt;- mean(yy) - 1.96*sd(yy)&#10;&amp;gt; upper&#10;[1] 3.141753&#10;&amp;gt; lower&#10;[1] -2.908468&#10;&amp;gt; abline(h=c(upper,lower), lty=2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/piesA.png&quot; alt=&quot;Bland-Altman plot with limits of agreement&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;(You can't see the upper limit of agreement because the plot only goes up to $y\approx 2.1$.)&lt;/p&gt;&#10;&#10;&lt;p&gt;As to the interpretation of the plot and the limits of agreement, again look to Wikipedia: &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;If the differences within mean  1.96 SD are not clinically important, the two methods may be used interchangeably.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="0" CreationDate="2014-05-23T15:16:11.273" Id="99840" LastActivityDate="2014-05-23T16:45:58.190" LastEditDate="2014-05-23T16:45:58.190" LastEditorUserId="7290" OwnerUserId="1352" ParentId="99835" PostTypeId="2" Score="5" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I would like to state an $\text{ARIMA}\left\{(0,1,4)\times(0,1,1)\right\}$ model (written in format $(p,d,q)\times(P,D,Q)$ model without using the backshift operator.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-05-23T18:58:58.200" Id="99859" LastActivityDate="2014-12-11T03:55:42.833" LastEditDate="2014-12-11T03:55:42.833" LastEditorUserId="17661" OwnerUserId="46081" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;arima&gt;" Title="How do I state an ARIMA(0,1,4)x(0,1,1)12 in terms of Yt etc and not in terms of the backshift operator" ViewCount="63" />
  <row AnswerCount="1" Body="&lt;p&gt;What is the best inter rater agreement test for Likert scale type questions. As far I saw that cronbach alpha is for internal consistency, or it shows how good items are related to describe the main question. I want to measure inter-rater agreement. Is Cronbach Alpha the right metric for doing this?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-05-23T19:02:11.623" Id="99860" LastActivityDate="2015-02-23T17:41:39.427" OwnerUserId="46082" PostTypeId="1" Score="2" Tags="&lt;intraclass-correlation&gt;&lt;agreement-statistics&gt;" Title="Likert scale inter rater agreement" ViewCount="92" />
  <row AnswerCount="0" Body="&lt;p&gt;Here's my situation.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I have a multiple linear regression which I've used to come up with a prediction interval to predict a value y for a given (x1,x2,x3,x4,x5,x6).   It reads something like lower: 30, upper:48.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I also have the same exact thing to predict a value y* at another given (x1*,x2*,x3*,x4*,x5*,x6*).  It reads something like lower:35, upper:51. &lt;/p&gt;&#10;&#10;&lt;p&gt;I want to answer this question:&lt;br&gt;&#10;What is the probability that the value y* is greater than the value y?&lt;/p&gt;&#10;&#10;&lt;p&gt;I think it's a basic question, but I'm not sure. &#10;I could likely come up with this probability if I knew the formula for how the prediction interval is calculated in a multi-variable situation.&lt;br&gt;&#10;Here's what I think should be done, but I wanted to run it by you guys first. &lt;/p&gt;&#10;&#10;&lt;p&gt;Prediction Intervals are based on a t-distribution with (n-6) degrees of freedom (I have a forced 0 y-int).  So I believe the margin of error calculated is then some constant multiplied by the corresponding value from the t-distribution (t_.05/2 with n-6 degrees of freedom).  The &quot;some constant&quot; would be the standard error of this particular estimate. &lt;/p&gt;&#10;&#10;&lt;p&gt;I then just do a basic 2 sample t-test using the point estimate prediction as the means and these constants as the standard errors with my n-6 degrees of freedom.   Is this accurate? &lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a better way?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-05-23T19:11:39.140" Id="99862" LastActivityDate="2014-05-23T19:11:39.140" OwnerUserId="46080" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;multiple-regression&gt;&lt;prediction-interval&gt;" Title="Multiple regression prediction interval comparison" ViewCount="47" />
  <row Body="&lt;p&gt;Depends on what you are looking for.&lt;/p&gt;&#10;&#10;&lt;p&gt;First of all, we have to define what &quot;trusting a model&quot; is. From a mathematical point of view, this mean : &quot;are you using the correct model according to your data ?&quot;. In this case, you should not be using the prediction error to assess the quality of your fit. The only thing we can do is &quot;checking that we have no evidence that the assumptions we made for this model are violated&quot;. In the case of linear regression, we usually check the normality of the error distribution, linearity of the relationship between covariates and response variable, homoscedasticity and the independence of errors.&lt;/p&gt;&#10;&#10;&lt;p&gt;As Glen_b said, models have more than one possible purpose. If you think about prediction performance, I'm pretty sure we can find an example where they are good but the model is not. This case could be very problematic if you think that you are using the correct model. This could lead to wrong prediction for data that are far away from your original data.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are many other things you have to be careful about (i.e. number of observations, variance of your observations, etc).&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-05-23T20:28:48.230" Id="99869" LastActivityDate="2014-05-23T20:28:48.230" OwnerUserId="46087" ParentId="97246" PostTypeId="2" Score="1" />
  
  
  
  <row Body="&lt;p&gt;Your &lt;code&gt;Feature&lt;/code&gt; variable with 64 different feature combinations (or 63 if you don't count the empty one) is equivalent to 6 independent binary variables. So, adding &lt;code&gt;k&lt;/code&gt;, you have 7 factors and 2 responses. If you model your problem that way, your software should provide some useful output display.&lt;/p&gt;&#10;&#10;&lt;p&gt;The amazing insightfulness of data visualization falls off quickly as we show more than a handful of variables. However, if you focus on a few aspects at a time, you can get some benefit.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can look at the two responses by each of the 7 factors independently.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ilER1.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;More options are available if you restrict yourself to one response at a time. You can see two-way interactions with a scatterplot matrix.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/KRKde.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Dots are jittered for the binary factors and color represents the response.&lt;/p&gt;&#10;&#10;&lt;p&gt;A heatmap is one way to see the entire structure, though some patterns are stand out better than others, and you should experiment with the ordering/hierarchy (informed by the model output). &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/W16kV.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In this case, You might notice a couple feature: &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Every other group of 4 columns is darker near the top, which suggests a &lt;code&gt;X4*K&lt;/code&gt; interaction.&lt;/li&gt;&#10;&lt;li&gt;The right half is darker, which suggest X1 may be significant.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;If you estimate those and subtract them out, other patterns may become evident, but it's quite iffy already.&lt;/p&gt;&#10;&#10;&lt;p&gt;To re-emphasize the first point, analytical methods will almost always be more effective for data with this many dimensions. Here is part of the model output for my fake data, which identifies the main effects right away.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Parameter Estimates&#10;&#10;Term               Estimate  Std Error  t Ratio   Prob&amp;gt;|t|&#10;X4                -1.093515   0.054667   -20.00    &amp;lt;.0001&#10;X1                -0.88112    0.054667   -16.12    &amp;lt;.0001&#10;Intercept          1.531147   0.11336     13.51    &amp;lt;.0001&#10;X3                -0.615258   0.054667   -11.25    &amp;lt;.0001&#10;K                  0.100797   0.009028    11.17    &amp;lt;.0001&#10;X4*K              -0.089554   0.009028    -9.92    &amp;lt;.0001&#10;X2                -0.460676   0.054667    -8.43    &amp;lt;.0001&#10;X2*X5              0.3937876  0.054667     7.20    &amp;lt;.0001&#10;X1*X2*X5          -0.377805   0.054667    -6.91    &amp;lt;.0001&#10;X1*X2              0.3277955  0.054667     6.00    &amp;lt;.0001&#10;X2*X3              0.3087438  0.054667     5.65    &amp;lt;.0001&#10;X5                -0.282575   0.054667    -5.17    &amp;lt;.0001&#10;X1*X5              0.2798842  0.054667     5.12    &amp;lt;.0001&#10;X1*X4*X6*K         0.0307494  0.009028     3.41    0.0007&#10;X2*X5*X6           0.1657399  0.054667     3.03    0.0025&#10;X4*X6*K            0.0232485  0.009028     2.58    0.0101&#10;X5*K               0.0213684  0.009028     2.37    0.0181&#10;X3*X5             -0.125324   0.054667    -2.29    0.0220&#10;X1*X2*X3*K        -0.018911   0.009028    -2.09    0.0364&#10;X2*X4*K           -0.017889   0.009028    -1.98    0.0478&#10;X4*X6              0.1036833  0.054667     1.90    0.0581&#10;X1*X6             -0.097084   0.054667    -1.78    0.0760&#10;X1*X4*X5           0.0768364  0.054667     1.41    0.1601&#10;X3*X4*X6*K         0.0124664  0.009028     1.38    0.1676&#10;X2*X3*X6          -0.075022   0.054667    -1.37    0.1702&#10;X1*X3*X4           0.0680795  0.054667     1.25    0.2132&#10;...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-05-24T01:14:21.083" Id="99891" LastActivityDate="2014-05-24T01:14:21.083" OwnerUserId="1191" ParentId="99335" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;Edit:&lt;/p&gt;&#10;&#10;&lt;p&gt;You probably aren't doing anything wrong.  You only have three observations per sample.  Therefore the difference must be very large for a small p-value.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-05-24T05:50:04.063" Id="99898" LastActivityDate="2014-05-26T05:56:34.687" LastEditDate="2014-05-26T05:56:34.687" LastEditorUserId="2310" OwnerUserId="2310" ParentId="99896" PostTypeId="2" Score="0" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I am currently trying to perform a regression on data with one predictor and one covariate.&lt;/p&gt;&#10;&#10;&lt;p&gt;The hypothesis I am trying to test is whether the incidence of pores (a hole through a cell) increases when I stretch the cells. I have three cell lines and three stretch levels (0, 10 and 20%). At each stretch level and for each cell line I have 4 (and sometimes 3). &lt;/p&gt;&#10;&#10;&lt;p&gt;Because I am sampling the occurrence of rare events (typical counts between 0 and 20) over an area, we are looking at a Poisson process. This is the reason why I assume the data is Poisson-distributed and not normally distributed. &lt;/p&gt;&#10;&#10;&lt;p&gt;x1=1x34 vector containing the strain-level for each specimen&#10;x2=1x34 vector containing the cell line fore each specimen&#10;y=1x34 vector containing the pore counts for each specimen&#10;There are 3x3x4-2=34 specimens: cell line 1 is missing one specimen at 10%, cell line 2 is missing one specimen at 20%.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am doing my statistical analysis in MATLAB. Before I came to the conclusion that my data is Poisson distributed, I used ANCOVA with the aoctool function, fitting separate lines through each cell line. The aoctool then calculates an F-statistic and a corresponding p-value.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;[h,atab,ctab,astats]=aoctool2(x1,yT,x2,0.05,'Strain','Pore Count','Cell Line','off','separate lines');&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The aoctool fits the data to the following model:&#10;y = ( + i) + ( + i)x + &#10;where y is the dependent variable,  the general intercept, i the cell line specific intercept,  the general slope, i the cell line specific intercept and  the error term.&lt;/p&gt;&#10;&#10;&lt;p&gt;Fitting the data to this model regresses the pore count with the stretch-level whilst accounting for possible differences between cell lines (groups).&lt;/p&gt;&#10;&#10;&lt;p&gt;Now I want to do the same with the fitglm (or glmfit) function in Matlab but using a Poisson rather than a normal distribution. &#10;However, I have not been able to get the fitglm function to fit separate lines for each cell line. Rather, it takes the cell line as a covariate.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is twofold:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;strong&gt;Is there a difference between using cell line as a covariate or as grouping variable?&lt;/strong&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;strong&gt;How do I adjust the fitglm-input to take into account cell line as a grouping variable?&lt;/strong&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;This is what I have tried so far:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;tbl=table(yT,x1,x2); % enter my data into a 'table' format for the fitglm function&#10;tbl.x2=nominal(tbl.x2); % x2, cell line, is a nominal variable with values 1, 2 or 3&#10;model=fitglm(tbl,'yT ~ (x1+x2)','Distribution','Poisson')&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;this results in&lt;/p&gt;&#10;&#10;&lt;p&gt;Estimated Coefficients:&#10;                   Estimate       SE         tStat       pValue&lt;br&gt;&#10;                   &lt;strong&gt;____&lt;/strong&gt;    &lt;strong&gt;_____&lt;/strong&gt;    &lt;strong&gt;&lt;em&gt;_&lt;/em&gt;&lt;/strong&gt;    &lt;strong&gt;______&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;(Intercept)      2.5086     0.097237     25.798    9.234e-147&#10;x1             0.024509    0.0024828     9.8714    5.5397e-23&#10;x2_2           -0.47372      0.10328    -4.5869     4.499e-06&#10;x2_3           -0.65656      0.10444    -6.2862    3.2532e-10&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Thank you so much for your help!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-24T10:21:17.377" Id="99910" LastActivityDate="2015-02-18T02:01:54.483" LastEditDate="2014-05-24T11:08:10.433" LastEditorUserId="88" OwnerUserId="46105" PostTypeId="1" Score="0" Tags="&lt;generalized-linear-model&gt;&lt;matlab&gt;&lt;group-differences&gt;&lt;poisson-regression&gt;&lt;covariate&gt;" Title="Accounting for groups in GzLM Poisson regression" ViewCount="101" />
  <row AnswerCount="0" Body="&lt;p&gt;I want to compare time spent in shopping between two group people. One group has 1426 cases, whereas the other group only has 96 cases. Which statistical method should I use, t-test or Mann-Whitney test?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-24T11:30:24.130" Id="99913" LastActivityDate="2014-05-24T11:30:24.130" OwnerUserId="46106" PostTypeId="1" Score="0" Tags="&lt;t-test&gt;&lt;mann-whitney-u-test&gt;" Title="statistical method" ViewCount="23" />
  
  <row Body="&lt;p&gt;Your assignment probably is only to manifest your skills using that kinds of regression. So, try applying both and compare results as for the significance of terms (t-statistics of coefficients), adjusted R squared or Bayesian Information Criterion.&#10;However, practically, time series methods would be more appropriate.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-05-24T16:34:45.907" Id="99933" LastActivityDate="2014-05-24T16:34:45.907" OwnerUserId="36545" ParentId="99928" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;You shoud distinguish between probability &lt;em&gt;mass&lt;/em&gt; and probability &lt;em&gt;density&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;A discrete random variable is like a set of little stones: each stone has its own mass (weight). For example, if you toss a regular coin the mass of the &quot;head-stone&quot; is 1/2. The probability &quot;density&quot; function of a discrete variable is actually a probability &lt;em&gt;mass&lt;/em&gt; function.&lt;/p&gt;&#10;&#10;&lt;p&gt;A continuous random variable is like a heap of dust. You may think that a single speck of dust has no mass, but you can calculate the mass of a fistful of dust if you know its volume &lt;em&gt;and&lt;/em&gt; its density: its mass is $volume\times density$.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;dnorm(0)&lt;/code&gt; is just the &lt;em&gt;density&lt;/em&gt; of a (standard) normal variable. When the &lt;em&gt;volume&lt;/em&gt; is zero, i.e. when $Z=0$, the &lt;em&gt;probability mass&lt;/em&gt; is zero just because in $volume\times density$ the first factor is zero. As soon as the volume increases, &lt;em&gt;as soon as you look for the probability mass of a range of values&lt;/em&gt; (a fistful of dust, i.e. an &lt;em&gt;interval&lt;/em&gt;), you can compute a probability mass. For example:&#10;$$\begin{align}P(-0.1&amp;lt;Z&amp;lt;0.1)&amp;amp;=P(Z\in(-0.1,0.1))\\ &amp;amp;=\mathtt{pnorm(0.1)-pnorm(-0.1)}=0.07965567\end{align}$$&#10;Each single value (each speck of dust) has a density, but only intervals like $(-0.1,0.1)$ (fistfuls of dust) have a &quot;mass&quot;. So you can compute the probability &lt;em&gt;mass&lt;/em&gt; of intervals, not of single values. But the probability &lt;em&gt;density&lt;/em&gt; of single values does exist and make sense.&lt;/p&gt;&#10;&#10;&lt;p&gt;The density of a uniform random variable is constant, the density of a normal variable is not. However you can approximate the mass of small intervals. For example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; pnorm(0.1)-pnorm(-0.1)&#10;[1] 0.07965567&#10;&amp;gt; density_at0 &amp;lt;- dnorm(0)&#10;&amp;gt; volume_around0 &amp;lt;- (0.1) - (-0.1)&#10;&amp;gt; volume_around0 * density_at0&#10;[1] 0.07978846&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-05-24T17:48:36.357" Id="99941" LastActivityDate="2014-05-24T18:02:21.363" LastEditDate="2014-05-24T18:02:21.363" LastEditorUserId="44965" OwnerUserId="44965" ParentId="99935" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I have attached an image of a subset of data consisting of snow measurements (D1-D6) taken at unique locations (Site1-Site25) over the course of several years. Each measurement was taken at the same location each time, once a month for 2 years. I'm trying to convert these measurements of snow depth into a single variable for each site to represent which sites consistently have deeper snow. At this point, I don't care about differences in means or anything like that, I just would like a value for each site representing it's &quot;relative snow depth&quot; compared to all other sites. &lt;/p&gt;&#10;&#10;&lt;p&gt;So far I've essentially used a Kruskal-Wallis H-test to determine a mean rank for each site (without then conducting the test) and I'm wondering if that appropriately represents the relative snow depth I'm trying to calculate. If not, does anyone know of a test that may be more appropriate for this data? Thanks!!&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/WTweU.png&quot; alt=&quot;Snow data frame&quot;&gt;&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-05-24T19:56:09.890" Id="99946" LastActivityDate="2014-05-24T19:56:09.890" OwnerUserId="46123" PostTypeId="1" Score="0" Tags="&lt;nonparametric&gt;&lt;ranks&gt;" Title="Is a rank means stat appropriate?" ViewCount="16" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I was taught that I can do a goodness of fit test for binomial model by comparing it to the saturated mode with drop in deviance test. What should I do when my binomial model has over-dispersion? Can I still use the goodness of fit test using drop in deviance?&lt;/p&gt;&#10;&#10;&lt;p&gt;And how do I accomplish this that using R (if the test is different from standard drop-in-deviance)?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-25T00:59:52.950" Id="99960" LastActivityDate="2014-05-25T00:59:52.950" OwnerUserId="40073" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;regression&gt;&lt;goodness-of-fit&gt;&lt;deviance&gt;&lt;quasi-binomial&gt;" Title="How do I obtain the goodness of fit for quasi-binomial model?" ViewCount="43" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I do not have a statistical background but I'm trying to understand bayes theorem using a practical example. In my example I plan on using bayes theorem to calculate how often two words occur next to each other. I essentially have some large text and I plan to use the formula before:&lt;/p&gt;&#10;&#10;&lt;p&gt;Likelihood &quot;this&quot; is followed by &quot;is&quot; = (Probability the word &quot;this&quot; is followed by &quot;is&quot;) * (Probability of the word &quot;this&quot;) / (Probability the word &quot;is&quot;)&lt;/p&gt;&#10;&#10;&lt;p&gt;And I would calculate the probability of a word as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;P(word) = (# number of times &quot;word&quot; occurs) / (total # of words)&lt;/p&gt;&#10;&#10;&lt;p&gt;And lastly, after I calculate this on my initial dataset, I plan on updating my likelihood as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;Likelihood = (old Likelihood) * (new likelihood)&lt;/p&gt;&#10;&#10;&lt;p&gt;Does this sound correct? I would really appreciate any feedback.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-25T02:12:38.473" Id="99964" LastActivityDate="2014-05-25T02:12:38.473" OwnerUserId="18609" PostTypeId="1" Score="0" Tags="&lt;bayes&gt;" Title="Application of Bayes rule - relation between a pair of words" ViewCount="38" />
  
  <row Body="&lt;p&gt;&quot;&lt;em&gt;Just&lt;/em&gt; an artefact&quot; sounds like the effect could be &lt;em&gt;entirely&lt;/em&gt; due to regression to the mean. That is a very strong statement. In fact, Kruger &amp;amp; Dunning already addressed this in section 4.1.3 in their &lt;a href=&quot;http://dx.doi.org/10.1037/0022-3514.77.6.1121&quot; rel=&quot;nofollow&quot;&gt;original paper&lt;/a&gt; (&quot;regression effect&quot; here stands for &quot;regression to the mean&quot;):&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Despite the inevitability of the regression effect, we believe that&#10;  the overestimation we observed was more psychological than&#10;  artifactual. For one, if regression alone were to blame for our&#10;  results, then the magnitude of miscalibration among the bottom&#10;  quartile would be comparable with that of the top quartile.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;And that simply is not the case; their Figure 1 shows that the bottom performers overestimate their performance by more than the top performers underestimate theirs.&lt;/p&gt;&#10;&#10;&lt;p&gt;In addition, Kruger &amp;amp; Dunning explicitly ran additional studies (studies 3 and 4 in their original paper) to address this, and the results from their studies 3 and 4 are consistent with an actual underlying psychological effect.&lt;/p&gt;&#10;&#10;&lt;p&gt;Of course regression to the mean will have an effect in the basic Dunning-Kruger setup, and one could conceive a permutation test approach to estimate how much of the Dunning-Kruger setup is due to it. Nevertheless, my impression is that there is something more there.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-05-25T11:48:58.197" Id="99982" LastActivityDate="2014-05-25T11:48:58.197" OwnerUserId="1352" ParentId="99970" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;To estimate the matrix of own- and cross-price elasticities it is more convenient to use the &lt;code&gt;asclogit&lt;/code&gt; command in Stata. Let's use the example in Cameron and Trivedi (2009) to show you the whole process to get this matrix. The data is on individual choices of whether to fish from the beach, the pier, a private boat or a chartered boat. They have three variables:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;code&gt;income&lt;/code&gt; is each individual's income&lt;/li&gt;&#10;&lt;li&gt;&lt;code&gt;p&lt;/code&gt; is the price of each fishing alternative&lt;/li&gt;&#10;&lt;li&gt;&lt;code&gt;q&lt;/code&gt; is the quantity of fish that an individual can obtain from each fishing alternative&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Note that &lt;code&gt;income&lt;/code&gt; is individual specific (a consumer characteristic) and &lt;code&gt;p&lt;/code&gt; and &lt;code&gt;q&lt;/code&gt; are choice specific (the product characteristics). Now let's first estimate the choice model and then obtain the matrix of own- and cross-price elasticities.&lt;/p&gt;&#10;&#10;&lt;p&gt;Obtain the data set and prepare it for the regression:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;net from http://www.stata-press.com/data/musr&#10;net install musr&#10;net get musr&#10;use mus15data.dta&#10;reshape long d p q, i(id) j(fishmode beach pier private charter) string&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Run the alternative specific clogit (&lt;code&gt;asclogit&lt;/code&gt;) model:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;asclogit d p q, case(id) alternatives(fishmode) casevar(income) basealternative(beach) nolog&#10;&#10;Alternative-specific conditional logit         Number of obs      =       4728&#10;Case variable: id                              Number of cases    =       1182&#10;&#10;Alternative variable: fishmode                 Alts per case: min =          4&#10;                                                              avg =        4.0&#10;                                                              max =          4&#10;&#10;                                                  Wald chi2(5)    =     252.98&#10;Log likelihood = -1215.1376                       Prob &amp;gt; chi2     =     0.0000&#10;&#10;------------------------------------------------------------------------------&#10;           d |      Coef.   Std. Err.      z    P&amp;gt;|z|     [95% Conf. Interval]&#10;-------------+----------------------------------------------------------------&#10;fishmode     |&#10;           p |  -.0251166   .0017317   -14.50   0.000    -.0285106   -.0217225&#10;           q |    .357782   .1097733     3.26   0.001     .1426302    .5729337&#10;-------------+----------------------------------------------------------------&#10;beach        |  (base alternative)&#10;-------------+----------------------------------------------------------------&#10;charter      |&#10;      income |  -.0332917   .0503409    -0.66   0.508     -.131958    .0653745&#10;       _cons |   1.694366   .2240506     7.56   0.000     1.255235    2.133497&#10;-------------+----------------------------------------------------------------&#10;pier         |&#10;      income |  -.1275771   .0506395    -2.52   0.012    -.2268288   -.0283255&#10;       _cons |   .7779593   .2204939     3.53   0.000     .3457992    1.210119&#10;-------------+----------------------------------------------------------------&#10;private      |&#10;      income |   .0894398   .0500671     1.79   0.074    -.0086898    .1875694&#10;       _cons |   .5272788   .2227927     2.37   0.018     .0906132    .9639444&#10;------------------------------------------------------------------------------&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;A negative coefficient for the price means that if the price of one fishing choice increases, then the demand for this choice decreases and the demand for the other choices increases. Conversely, if the quantity of fish caught increases in one choice, the demand for this choice increases and decreases for the other choices.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now the own- and cross price elasticities are&#10;$$&#10;\begin{align}&#10;\frac{\partial p_{ij}}{\partial x_{rik}} &amp;amp;= p_{ij}(1-p_{ij})\beta_r, \qquad j=k \quad\text{(own-price elasticity)} \newline&#10;\frac{\partial p_{ij}}{\partial x_{rik}} &amp;amp;= -p_{ij}p_{ik}\beta_r, \quad \quad \quad j \neq k \quad \text{(cross-price elasticity)}&#10;\end{align}&#10;$$&#10;for the alternative choice $r$ (relative to the baseline choice; here the baseline is &quot;beach&quot;).&lt;/p&gt;&#10;&#10;&lt;p&gt;The own- and cross-price elasticities are easily estimate as:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;estat mfx, varlist(p)&#10;&#10;Pr(choice = beach|1 selected) = .05248806&#10;-------------------------------------------------------------------------------&#10;variable     |   dp/dx   Std. Err.    z     P&amp;gt;|z|  [    95% C.I.    ]       X&#10;-------------+-----------------------------------------------------------------&#10;p            |                                                                 &#10;       beach | -.001249   .000121  -10.29   0.000  -.001487  -.001011    103.42&#10;     charter |  .000609   .000061    9.97   0.000   .000489   .000729    84.379&#10;        pier |  .000087   .000016    5.42   0.000   .000055   .000118    103.42&#10;     private |  .000553   .000056    9.88   0.000   .000443   .000663    55.257&#10;-------------------------------------------------------------------------------&#10;&#10;Pr(choice = charter|1 selected) = .46206853&#10;-------------------------------------------------------------------------------&#10;variable     |   dp/dx   Std. Err.    z     P&amp;gt;|z|  [    95% C.I.    ]       X&#10;-------------+-----------------------------------------------------------------&#10;p            |                                                                 &#10;       beach |  .000609   .000061    9.97   0.000   .000489   .000729    103.42&#10;     charter | -.006243   .000441  -14.15   0.000  -.007108  -.005378    84.379&#10;        pier |  .000764   .000071   10.69   0.000   .000624   .000904    103.42&#10;     private |   .00487   .000452   10.77   0.000   .003983   .005756    55.257&#10;-------------------------------------------------------------------------------&#10;&#10;Pr(choice = pier|1 selected) = .06584968&#10;-------------------------------------------------------------------------------&#10;variable     |   dp/dx   Std. Err.    z     P&amp;gt;|z|  [    95% C.I.    ]       X&#10;-------------+-----------------------------------------------------------------&#10;p            |                                                                 &#10;       beach |  .000087   .000016    5.42   0.000   .000055   .000118    103.42&#10;     charter |  .000764   .000071   10.69   0.000   .000624   .000904    84.379&#10;        pier | -.001545   .000138  -11.16   0.000  -.001816  -.001274    103.42&#10;     private |  .000694   .000066   10.58   0.000   .000565   .000822    55.257&#10;-------------------------------------------------------------------------------&#10;&#10;Pr(choice = private|1 selected) = .41959373&#10;-------------------------------------------------------------------------------&#10;variable     |   dp/dx   Std. Err.    z     P&amp;gt;|z|  [    95% C.I.    ]       X&#10;-------------+-----------------------------------------------------------------&#10;p            |                                                                 &#10;       beach |  .000553   .000056    9.88   0.000   .000443   .000663    103.42&#10;     charter |   .00487   .000452   10.77   0.000   .003983   .005756    84.379&#10;        pier |  .000694   .000066   10.58   0.000   .000565   .000822    103.42&#10;     private | -.006117   .000444  -13.77   0.000  -.006987  -.005246    55.257&#10;-------------------------------------------------------------------------------&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;All the own-price elasticities are negative and the cross-price elasticities are positive which makes sense. The own-price elasticity of &lt;code&gt;-.001249&lt;/code&gt; means that a 1 Dollar increase from the mean of &lt;code&gt;p&lt;/code&gt; (price) of fishing at the beach reduces the probability that beach fishing is chosen by 0.001249 for an individual with mean income and mean &lt;code&gt;q&lt;/code&gt; (fish caught). &lt;br/&gt;&#10;So all elasticities are expressed relative to the mean values of &lt;code&gt;income&lt;/code&gt;, &lt;code&gt;p&lt;/code&gt;, and &lt;code&gt;q&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;The cross-price elasticity &lt;code&gt;.000609&lt;/code&gt; tells you that if the price for fishing from a charter boat increases by 1 Dollar, the probability that beach fishing is chosen increases by &lt;code&gt;.000609&lt;/code&gt;. The result is not in matrix format but you can easily take each &lt;code&gt;dp/dx&lt;/code&gt; block from the results (each of which is a column for your final matrix) and put them together as a matrix of own- and cross-price elasticities where the own-price elasticities are on the principal diagonal and the cross-price elasticities are off diagonal like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;             beach   charter      pier   private&#10;------------------------------------------------&#10;  beach | -.001249   .000609   .000087   .000553&#10;charter |  .000609  -.006243   .000764    .00487&#10;   pier |  .000087   .000764  -.001545   .000694&#10;private |  .000553    .00487   .000694  -.006117&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The fishing choice example seems trivial but it should now be straightforward for you to apply your own demand estimation problem to the code provided. For more information on estimating these price elasticities or the &lt;code&gt;asclogit&lt;/code&gt; command have a look at Cameron and Trivedi (2009) &quot;Microeconometrics Using Stata&quot;.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-05-25T12:52:32.220" Id="99992" LastActivityDate="2014-05-25T22:43:31.097" LastEditDate="2014-05-25T22:43:31.097" LastEditorUserId="26338" OwnerUserId="26338" ParentId="99909" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;Having a quick look at the wikipedia page I'm not sure what the fit is here with MCMC/Metropolis-Hastings algorithm.&lt;/p&gt;&#10;&#10;&lt;p&gt;The models described there look like continuous time Markov chains. If that's right then you can get analytic results, and wouldn't need to resort to MCMC. &lt;/p&gt;&#10;&#10;&lt;p&gt;Essentially the analytic solution is to do the eigendecomposition of Q into $VLV^T$. The pmf of the process at $t$ is, then $p_i(t)=\sum_{i=1}^{4}c_i v_i e^{\lambda_i}$ where the constants, $c_i$ are obtained by solving for some initial probability vector $b$ the equation $Ec=b$.&lt;/p&gt;&#10;&#10;&lt;p&gt;I suspect this has arisen due to some of the terminology being the same. Unless I've misunderstood the question and the wiki reference is just a jumping-off point!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-25T16:41:29.980" Id="100009" LastActivityDate="2014-05-25T16:41:29.980" OwnerUserId="16663" ParentId="28984" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="100767" AnswerCount="5" Body="&lt;p&gt;I have a very large dataset and about 5% random values are missing. These variables are correlated with each other. The following example R dataset is just a toy example with dummy correlated data.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(123)&#10;&#10;# matrix of X variable &#10;xmat &amp;lt;- matrix(sample(-1:1, 2000000, replace = TRUE), ncol = 10000)&#10;colnames(xmat) &amp;lt;- paste (&quot;M&quot;, 1:10000, sep =&quot;&quot;)&#10;rownames(xmat) &amp;lt;- paste(&quot;sample&quot;, 1:200, sep = &quot;&quot;)&#10;#M variables are correlated &#10;&#10;N &amp;lt;- 2000000*0.05 # 5% random missing values &#10;inds &amp;lt;- round ( runif(N, 1, length(xmat)) )&#10;xmat[inds] &amp;lt;- NA &#10;&amp;gt; xmat[1:10,1:10]&#10;         M1 M2 M3 M4 M5 M6 M7 M8 M9 M10&#10;sample1  -1 -1  1 NA  0 -1  1 -1  0  -1&#10;sample2   1  1 -1  1  0  0  1 -1 -1   1&#10;sample3   0  0  1 -1 -1 -1  0 -1 -1  -1&#10;sample4   1  0  0 -1 -1  1  1  0  1   1&#10;sample5  NA  0  0 -1 -1  1  0 NA  1  NA&#10;sample6  -1  1  0  1  1  0  1  1 -1  -1&#10;sample7  NA  0  1 -1  0  1 -1  0  1  NA&#10;sample8   1 -1 -1  1  0 -1 -1  1 -1   0&#10;sample9   0 -1  0 -1  1 -1  1 NA  0   1&#10;sample10  0 -1  1  0  1  0  0  1 NA   0&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Is there a (best) way to impute missing values in this situation? Is the Random Forest algorithm helpful? Any working solution in R would be much appreciated.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edits:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;(1) Missing values are randomly distributed among the variables and samples.As &lt;strong&gt;number of variables&lt;/strong&gt; is &lt;strong&gt;very large&lt;/strong&gt; (here in the example - 10000), while the &lt;strong&gt;number of samples&lt;/strong&gt; is small here in the above dummy example it is about 200. So when we look at any sample over all the variables (10000), there is high chances that there will be missing value at some variable - due to large number of variables. So just deleting the sample is not option. &lt;/p&gt;&#10;&#10;&lt;p&gt;(2) The variable can be treated both as quantitative or qualitative (binary) in the process of imputing. The only judgement is how well we can predict it (accuracy). So predictions like 0.98 instead of 1 might be acceptable rather 0 vs 1 or -1 vs 1. I might need to tradeoff between computing time and accuracy. &lt;/p&gt;&#10;&#10;&lt;p&gt;(3)  The issue I have thinking how overfitting can affect the results as the number of variables are large compared to number of samples. &lt;/p&gt;&#10;&#10;&lt;p&gt;(4) As the total quantity of missing values is about 5% and is random (not concentrated in any variables or samples as precaution was taken to remove the variables or samples that have very high missing values) &lt;/p&gt;&#10;&#10;&lt;p&gt;(5) Making data complete for analysis is first objective and accuracy is secondary. So not too sensitive to accuracy.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-05-25T18:27:57.070" FavoriteCount="9" Id="100020" LastActivityDate="2014-06-01T15:58:40.367" LastEditDate="2014-05-27T22:01:48.980" LastEditorUserId="7244" OwnerUserId="7244" PostTypeId="1" Score="7" Tags="&lt;r&gt;&lt;random-forest&gt;&lt;missing-data&gt;&lt;data-imputation&gt;&lt;multiple-imputation&gt;" Title="How to perform imputation of values in very large number of data points?" ViewCount="386" />
  <row Body="&lt;p&gt;The area of a  circle circle of radius $l$ is equal to $\pi l^2$. It means that a quarter of circle has area $l^2\pi/4$. This means that the square with side the radius of the circle as $area=l^2$. &lt;/p&gt;&#10;&#10;&lt;p&gt;This means that the ratio between the area of a quarter of circle and the area of the square is $\pi/4$.  &lt;/p&gt;&#10;&#10;&lt;p&gt;A point  $(x,y) $ is in the square if $ 0&amp;lt;x&amp;lt;1, 0&amp;lt;y&amp;lt;1$. &#10;and it is in the quarter of circle if  $ 0&amp;lt;x&amp;lt;1, 0&amp;lt;y&amp;lt;1 ,x^2+y^2&amp;lt;1$.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Your integral is so $I((x^2+y^2)&amp;lt;1)P(x,y)= I((x^2+y^2)&amp;lt;1) I(0&amp;lt;x&amp;lt;1)I(0&amp;lt;y&amp;lt;1)$ That is exactly the area described by a quarter of circle&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Wh7mG.gif&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2014-05-25T19:42:11.457" Id="100023" LastActivityDate="2014-05-25T19:42:11.457" OwnerUserId="25392" ParentId="100006" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;There is not a correct answer to this kind of question. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have seen that some people use to write something like : &quot;Lasso selects variables a,b,c and other 100 variables&quot;.  You can also report the table in an appendix. If you think it is very important you can leave the table in the text&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-25T23:16:06.753" Id="100039" LastActivityDate="2014-05-25T23:16:06.753" OwnerUserId="25392" ParentId="100033" PostTypeId="2" Score="3" />
  
  <row AcceptedAnswerId="100745" AnswerCount="1" Body="&lt;p&gt;I have two question about profile log-likelihood. One is of theoretical nature and one is about a plot in R. I illustrate the two questions in a application of profile-likelihood in EVT. I use the package &lt;code&gt;evd&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; data(lisbon)&#10;&amp;gt; fgev(lisbon)&#10;&#10;Call: fgev(x = lisbon) &#10;Deviance: 241.2459 &#10;&#10;Estimates&#10;    loc    scale    shape  &#10;96.0319  12.8526  -0.1988  &#10;&#10;Standard Errors&#10;   loc   scale   shape  &#10;2.6171  1.8346  0.1284  &#10;&#10;Optimization Information&#10;  Convergence: successful &#10;  Function Evaluations: 24 &#10;  Gradient Evaluations: 13 &#10;&#10;&amp;gt; plot(profile(fgev(lisbon)),ci=c(0.95,0.99))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The result is the following plot:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/SrCjF.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;My two question are about this plot.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;theoretical one:&lt;/strong&gt; For log-likelihood profile you can write one of the parameter as a function of the other, i.e. your log-likelihood function $l(\alpha,\beta)=l(\alpha,f(\alpha))$ for some function $f$.&lt;br&gt;&#10;Here I have a log-likelihood function $l(\mu,\sigma,\gamma)$ of three parameters location $\mu$, scale $\sigma$ and shape $\gamma$. In the profile log-likelihood of the location parameter I fix $\sigma$ and $\gamma$ and just vary $\mu$. But which values are used for $\sigma$ and $\gamma$? I don't think it is possible to write $\sigma=f_1(\mu)$ and $\gamma=f_2(\mu)$ for two functions $f_1$ and$ f_2$?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;about the plot&lt;/strong&gt;: In the plot what is indicated by the dashed lines? &lt;/p&gt;&#10;&#10;&lt;p&gt;Many thanks for your help&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-26T10:03:09.417" FavoriteCount="1" Id="100072" LastActivityDate="2014-06-01T07:21:58.530" OwnerUserId="21493" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;profile-likelihood&gt;" Title="How can be profile plots in EVT interpreted and what is the theoretical nature of it?" ViewCount="122" />
  
  <row Body="&lt;ul&gt;&#10;&lt;li&gt;One-sample sign test tests that the &lt;strong&gt;median&lt;/strong&gt; in the population equals the value.&lt;/li&gt;&#10;&lt;li&gt;One-sample Wilcoxon test tests that the distribution in the population &lt;strong&gt;is symmetric around the value&lt;/strong&gt;. More technically, that the &lt;em&gt;sum&lt;/em&gt; of two randomly chosen deviations from the value has equal probability to occure positive or negative. Note that rejecting this null hypothesis does &lt;em&gt;not&lt;/em&gt; preclude the value to be the mean or median of the population. The rejection implies two possible reasons: either the distribution is symmetric about some other value or the distribution is not symmetric at all.&lt;/li&gt;&#10;&lt;li&gt;So if we &lt;em&gt;do assume&lt;/em&gt; symmetric shape of the population distribution then the Wilcoxon tests that the &lt;strong&gt;mean (=median)&lt;/strong&gt; in the population equals the value (it is this test then is the nonparametric alternative to one-sample t-test which assumes normality). If you assume the symmetry and hence you test for mean (=median), then Wilcoxon is more powerful, as a median test, then the more universal sign test above.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="2" CreationDate="2014-05-26T11:48:42.270" Id="100076" LastActivityDate="2014-05-31T11:32:14.390" LastEditDate="2014-05-31T11:32:14.390" LastEditorUserId="3277" OwnerUserId="3277" ParentId="100075" PostTypeId="2" Score="4" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I have three site temperature types: cold, medium, and hot. This is a fixed effect - I am specifically am testing if temperature affects my response variable. I believe temperature type is my 'block'. &lt;/p&gt;&#10;&#10;&lt;p&gt;I will have three sites for each temperature type. I think site is a random effect. Within each site, I will have a factorial design, where two other fixed factors, A and B, are crossed. &lt;/p&gt;&#10;&#10;&lt;p&gt;I am interested in the interaction of all three fixed effect (tempxAxB). &lt;/p&gt;&#10;&#10;&lt;p&gt;From reading my stats book, I think this is a split plot design, but my stats book does not have information about factorial design (the cross of A and B) within blocks. &lt;strong&gt;Is what I just described a split plot design or a three way ANOVA?&lt;/strong&gt; And if it is a funky split plot, could someone point me to an ANOVA table (how to calculate sum of squares) with a similar set up?&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is a schematic of the setup: &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/E60hm.png&quot; alt=&quot;Set up&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-26T14:16:43.903" Id="100087" LastActivityDate="2014-05-26T15:55:14.767" LastEditDate="2014-05-26T14:23:36.083" LastEditorUserId="7290" OwnerUserId="46186" PostTypeId="1" Score="3" Tags="&lt;anova&gt;&lt;experiment-design&gt;&lt;split-plot&gt;" Title="Split plot with factorial design vs three way ANOVA" ViewCount="273" />
  
  
  <row Body="&lt;p&gt;Your system of equations $Ax=b$ has more variables than equations and happens to be consistent.  Thus there are infinitely many solutions.  It appears that there are still infinitely many solutions when you include the nonnegativity constraint.   This behavior isn't at all surprising- why did you expect to get the same answer from both packages when the underlying problem has infinitely many solutions?  Is there some particular answer that you'd like to get, and why?  &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-05-26T18:49:51.857" Id="100107" LastActivityDate="2014-05-26T18:49:51.857" OwnerUserId="8200" ParentId="100093" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;There are still a lot of questions that would need answering if someone wanted to sit down and really plan out a solution, mostly relating to the quality of the inputs / data and the needed accuracy / acceptability of certain types of errors.&lt;/p&gt;&#10;&#10;&lt;p&gt;Based on your description, I would suggest you stop trying to model the problem as a while and start modeling parts of it. &lt;/p&gt;&#10;&#10;&lt;p&gt;Example: you mention 30% of your data is exactly 0.  If you can accurately predict a binary classification problem (output should be exactly zero or non-zero), that would give you a powerful way to make progress. Then you can focus on building a model on the remainder of the data, or further break it down into parts. &lt;/p&gt;&#10;&#10;&lt;p&gt;The other suggestion would be to learn / explore more of the fitting options in R. With the exception of GLM (which is only linear), your used models are all tree based / related. There are many other algorithms that may do better or worse on your data. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-05-26T19:22:58.703" Id="100108" LastActivityDate="2014-05-26T19:22:58.703" OwnerUserId="34874" ParentId="100101" PostTypeId="2" Score="1" />
  <row AnswerCount="3" Body="&lt;p&gt;Im having trouble understanding how to combine parameter uncertainty and interannual variability from various levels in a bootstrapped linear model. Specifically, this model is designed to generate estimates of some phenomenon $Y$ for 0.5 grid cells in a spatial region (e.g, Central America).&lt;/p&gt;&#10;&#10;&lt;p&gt;It looks like this:&#10;$$&#10;    \boldsymbol{Y_{m,y}} = \boldsymbol{\beta_{m,y}X_{m,y}}&#10;$$&#10;where $\boldsymbol{\beta}$ is the vector of parameters being estimated (size 1x3), $\boldsymbol{X}$ is a matrix of observations of land cover fractions (size 3xN), $\boldsymbol{Y}$ is the vector of predicted values (size 1xN), and the subscripts indicate that the model is being parameterized for each month $m$ and year $y$ (e.g., May 2006, June 2006, and May 2007 can all have different parameter estimates). (No intercept, although I dont think that matters for my question.)&lt;/p&gt;&#10;&#10;&lt;p&gt;If we think of the model as $Y_{m,y}=\sum\limits_{i=1}^3{\beta_{i,m,y}X_{i,m,y}}$, then each term $\beta_iX_i$ can be considered a subset of $Y$  i.e., &#10;$Y_i$, the amount of $Y$ that happens on land cover type $i$   in a real physical sense. Thus, there will likely be some correlation among the various parameter estimates. This is embodied/exacerbated by the fact that for each grid cell $g$, $\sum\limits_{i=1}^3{\boldsymbol{X_{g,i,m,y}}} = 1$  that is, the land cover type fractions add up to 1.&lt;/p&gt;&#10;&#10;&lt;p&gt;To create uncertainty bounds for the parameters, I have done 10,000 bootstrapping runs. Lets say there are 500 grid cells in the region. For the first bootstrapping run, I randomly choose 500 grid cells from that region with replacement. I then estimated the parameters for each month and year using that sample set. I then repeated this sampling-fitting procedure 9,999 more times.&lt;/p&gt;&#10;&#10;&lt;p&gt;My goal is to compare the &lt;em&gt;measured/observed&lt;/em&gt; mean annual amount of phenomenon $Y$ with the &lt;em&gt;estimated/modeled&lt;/em&gt; amount. I would like the uncertainty bars around the measured amount (which will take into account just interannual variability in the measured amount) to overlap with the uncertainty bars around the estimated amount (which will take into account both uncertainty in the model parameters as well as interannual variability in the annual estimated amount).&lt;/p&gt;&#10;&#10;&lt;p&gt;For the observations, its easy enough  sum up observed $Y$ for all the months in each year, then find the standard deviation across all the years. I get confused when thinking about how to do this for the estimates, though. The problem becomes even stickier if I want to compare observed vs. estimated annual GLOBAL $Y$ (i.e., across all regions).&lt;/p&gt;&#10;&#10;&lt;p&gt;I know that adding standard deviations in quadrature is going to be key here, but the fact that there are so many levels is confusing me. I'm working in Matlab now and am also fluent in R, but any help you can provide would be much appreciated.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-05-26T20:05:00.547" Id="100110" LastActivityDate="2014-05-28T16:27:22.333" LastEditDate="2014-05-27T17:28:51.760" LastEditorUserId="20363" OwnerUserId="20363" PostTypeId="1" Score="1" Tags="&lt;linear-model&gt;&lt;uncertainty&gt;&lt;variability&gt;" Title="Combining sources of uncertainty/variation in a multi-layered linear model" ViewCount="57" />
  <row Body="&lt;p&gt;If you can calculate the value of $f(\Theta|D)$ for a given $\Theta$, then the problem can be solved in the following way:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Consider the function &#10;$$g(\Theta|k) = 1\;  \mathrm{if}\;  f(\Theta|D) &amp;lt; k, \; 0 \; \mathrm{otherwise}$$&#10;Your integral becomes &#10;$$ \int_M g(\Theta|k)f(\Theta|D) d\Theta $$&lt;/li&gt;&#10;&lt;li&gt;Using MCMC get n samples $S^{(j)}, j = 1\ldots n$ from the posterior $f(\Theta|D)$ , and then approximate the integral as&#10;$$ \frac{1}{n}\sum_{j = 1}^n g(S^{(j)})$$&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;If you don't have a normalized posterior, you can still compute the evidence as&#10;$$ \int_M L(\Theta|D) p(\Theta)d\Theta $$  (by taking samples from the prior and averaging the likelihood over those samples), normalize the posterior and then use the two previous points.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-05-26T20:19:09.123" Id="100111" LastActivityDate="2014-05-26T20:19:09.123" OwnerUserId="29987" ParentId="100097" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm comparing the Chebychev (Weak) Law of Large Numbers (LLN) to the Kolmogorov (Strong) LLN in an econometric textbook and both definitions start off differently. The Chebychev LLN begins with&lt;/p&gt;&#10;&#10;&lt;p&gt;If $x_i, i = 1, . . . , n$ is a sample of of observations such that . . . &lt;/p&gt;&#10;&#10;&lt;p&gt;while the Kolmogorov LLN begins with&lt;/p&gt;&#10;&#10;&lt;p&gt;If $x_i, i = 1, . . . , n$ is a sequence of independently distributed random variable such that . . . &lt;/p&gt;&#10;&#10;&lt;p&gt;I know they refer to convergence in probability and convergence almost surely respectively but what is the significance of using a sample in one and a sequence in the other? Is it harder to prove convergence in a sequence as opposed to a sample?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-26T22:46:57.807" Id="100119" LastActivityDate="2014-05-26T22:46:57.807" OwnerUserId="31188" PostTypeId="1" Score="2" Tags="&lt;sample&gt;&lt;convergence&gt;&lt;wlln&gt;" Title="Difference between sample and sequence in Law of Large Numbers" ViewCount="33" />
  
  
  <row AcceptedAnswerId="100144" AnswerCount="2" Body="&lt;p&gt;What is a good metric for assessing the quality of principal component analysis (PCA)?&lt;/p&gt;&#10;&#10;&lt;p&gt;I performed this algorithm on a dataset. My objective was to reduce the number of features (the information was very redundant). I know the percentage of variance kept is a good indicator of how much information we keep, be are there other information metrics I can use to make sure I removed redundant information and didn't 'lose' such information?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-05-27T07:40:41.980" FavoriteCount="3" Id="100143" LastActivityDate="2014-12-22T15:25:08.583" LastEditDate="2014-12-22T15:25:08.583" LastEditorUserId="28666" OwnerUserId="28183" PostTypeId="1" Score="8" Tags="&lt;machine-learning&gt;&lt;pca&gt;&lt;data-mining&gt;&lt;information-theory&gt;" Title="What are good metrics to assess the quality of a PCA fit, in order to select the number of components?" ViewCount="357" />
  <row AcceptedAnswerId="100156" AnswerCount="1" Body="&lt;p&gt;I know this is basic question, but I have trouble understanding principal component analysis (PCA).&lt;/p&gt;&#10;&#10;&lt;p&gt;PCA can be used for dimensionality reduction. Let's assume we have 100 features that we think are redundant. After applying PCA, we find that 2 principal components explain 90% of the variance. &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;How can we divide the 100 original features into two groups (corresponding to the 2 main PCs?).&lt;/li&gt;&#10;&lt;li&gt;Can the eigenvectors associated to the main PCs be used as 'representatives' of these two groups? &lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="2" CreationDate="2014-05-27T10:04:23.333" Id="100153" LastActivityDate="2014-12-22T15:18:40.823" LastEditDate="2014-12-22T15:18:40.823" LastEditorUserId="28666" OwnerUserId="28183" PostTypeId="1" Score="1" Tags="&lt;pca&gt;" Title="Basic question on PCA: can it be used to divide original features into groups?" ViewCount="58" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;In my textbook, there is a statement mentioned on the topic of linear regression/machine learning, without a proof or rigorious justification, which is simply quoted as,&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Consider a noisy target, $ y = (w^{*})^T \textbf{x} + \epsilon  $, for generating the data, where $\epsilon$ is a noise term with zero mean and $\sigma^2$ variance, independently generated for every example $(\textbf{x},y)$. &lt;strong&gt;The expected error of the best possible linear fit to this target is thus&lt;/strong&gt; $\sigma^2$.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Here my concerns are,&lt;/p&gt;&#10;&#10;&lt;p&gt;How do we know what the best possible linear fit is ? Assuming in some way we estimated best linear fit, is it $y = (w^{*})^T \textbf{x}$ ? If so why can not other parameter $w_2^{*}$ be the best fitting parameter ?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-27T13:03:57.353" Id="100170" LastActivityDate="2014-05-27T15:36:56.000" OwnerUserId="46233" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;machine-learning&gt;" Title="Expected error of best possible linear fit?" ViewCount="22" />
  
  
  
  
  
  <row AcceptedAnswerId="100226" AnswerCount="1" Body="&lt;p&gt;I've come across some past questions asking about using a linear probability model in place of a probit model when the data generating function has uniformly distributed errors. However, I am uncertain about deriving the variance of the linear probability model. The problem is as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/BHGOF.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The first part seems straightforward enough (shown below), though I think I may be missing something fundamental about how the e_i error term arises.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/nrw6s.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;When it comes to calculating the e_i, we clearly must know the distribution first, and I am having a hard time figuring out how we can pin down how this term will be distributed. Any help in the right direction would be appreciated. &lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT: I believe that I worked out the second part of the problem here:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ne3gc.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I also seem to find that, with regards to part (c), White Standard errors will not yield a consistent estimate in this case though I am not sure why. Can anyone provide any intuition about this?&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT II: In fact, it seems White Standard Errors will work in this case. Is this because the e_i estimates are independent of one another?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-05-27T14:11:41.970" Id="100188" LastActivityDate="2014-05-27T17:48:18.710" LastEditDate="2014-05-27T15:14:29.517" LastEditorUserId="27327" OwnerUserId="27327" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;self-study&gt;&lt;mathematical-statistics&gt;" Title="Linear Probability Model with Uniformly Distributed Errors" ViewCount="133" />
  <row Body="&lt;p&gt;In addition to Andy W's answer, the procedure that was suggested to you is similar to the Fixed Effects Vector Decomposition (FEVD) proposed by &lt;a href=&quot;http://pan.oxfordjournals.org/content/15/2/124.short&quot; rel=&quot;nofollow&quot;&gt;Plmber and Troeger (2007)&lt;/a&gt;. It's not quite the same but very alike to their three-step method which goes as follows:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;estimate the unit fixed effects&lt;/li&gt;&#10;&lt;li&gt;decompose the fixed effects into the time-invariant factors and an error term&lt;/li&gt;&#10;&lt;li&gt;estimate 1. again by pooled OLS including the time-invariant variables and the error from 2.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;This procedure was heavily criticized by &lt;a href=&quot;http://pan.oxfordjournals.org/content/19/2/135.abstract&quot; rel=&quot;nofollow&quot;&gt;Greene (2011)&lt;/a&gt; and &lt;a href=&quot;http://pan.oxfordjournals.org/content/19/2/123.full&quot; rel=&quot;nofollow&quot;&gt;Breusch et al. (2011)&lt;/a&gt; so I would be careful with such types of estimation strategies. The point about the lower/higher level effects mentioned by Andy W is one of the set of critique points in these two papers.&lt;/p&gt;&#10;&#10;&lt;p&gt;If it helps you, I have written another post in a related question on &lt;a href=&quot;http://stats.stackexchange.com/a/90759/26338&quot;&gt;how to keep time-invariant variables in fixed effects regressions&lt;/a&gt;. I hope you will find this useful.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-05-27T14:24:48.087" Id="100192" LastActivityDate="2014-05-27T14:24:48.087" OwnerUserId="26338" ParentId="100168" PostTypeId="2" Score="4" />
  <row AnswerCount="1" Body="&lt;p&gt;I have two questions regarding standard multiple regression: &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Why is my shared variance a negative number?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Should I only include the positive semipartial correlations when calculating uniquely explained variance? &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I am trying to calculate the amount of shared variance explained in a regression model with four predictor variables, and this number is coming out negative (-.465).  &lt;/p&gt;&#10;&#10;&lt;p&gt;According to Tabachnick &amp;amp; Fidell, (2001), uniquely explained variance is computed by adding up the squared semipartial correlations.  Shared variance is computed by subtracting the uniquely explained variance from the R square.   &lt;/p&gt;&#10;&#10;&lt;p&gt;My R square value = .325 (R = .570, Adj R square = .295, fwiw)&lt;/p&gt;&#10;&#10;&lt;p&gt;F(4, 91) = 10.941, p = .0005.&lt;/p&gt;&#10;&#10;&lt;p&gt;The semipartial correlation values are (significant predictors indicated by*, from the Part column in SPSS output):&lt;/p&gt;&#10;&#10;&lt;p&gt;.172*&#10;-.174*&#10;.465*&#10;.164&lt;/p&gt;&#10;&#10;&lt;p&gt;I calculated that the predictors collectively uniquely explained 80% of the variance (0.801, which is the sum of POSITIVE semipartial correlation coefficients).  &lt;/p&gt;&#10;&#10;&lt;p&gt;When calculating the shared variance, the figure comes out at -.48 (computed by subtracting the uniquely explained variance from the R square value; .32 - .80 = -.48).  Im not sure whether it is possible to have a negative value for shared variance, where have I gone wrong (if I have)?&lt;/p&gt;&#10;&#10;&lt;p&gt;Any advice would be greatly received.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Many thanks.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-05-27T15:59:30.033" Id="100211" LastActivityDate="2014-08-05T12:29:24.887" OwnerUserId="46247" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;multiple-regression&gt;&lt;variance&gt;&lt;regression-coefficients&gt;" Title="Why is my shared variance negative?" ViewCount="219" />
  <row Body="&lt;p&gt;I have an idea for how to do this (this is Idea 1; see Idea 2 in a different answer). I won't mark this as the correct answer until I get some confirmation, so please let me know if this looks right to you (although make sure you understand what I'm trying to do!). My main point of concern is the way I include covariances in quadrature -- I'm afraid I am being redundant, and therefore the estimates of SD will be too large.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;$cov_b(i,j)$ means &quot;the covariance of $i$ and $j$ across all bootstrapping runs $b$&quot;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Find the mean and SD for a given cover ($c$) in a given month ($m$) of a given year ($y$) in a given region ($r$) across all B bootstrapping runs (e.g., forest in May 2006 in Central America):&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;\overline{Y_{c,m,y,r}} = mean_{b}(Y_{c,m,y,r,b}) = \frac{\sum\limits_{b=1}^B{Y_{c,m,y,r,b}}}{B}&#10;$$&#10;$$&#10;\sigma_{c,m,y,r} = sd_{b}(Y_{c,m,y,r,b}) = \sqrt{\frac{\sum\limits_{b=1}^B{(Y_{c,m,y,r,b}-\overline{Y_{c,m,y,r}})^2}}{B}}&#10;$$&#10;Find the mean and SD of total (i.e., across all 3 covers $c$) Y for a given month ($m$) of a given year ($y$) in a given region ($r$) (e.g., May 2006 in Central America):&#10;$$&#10;\overline{Y_{m,y,r}} = \sum\limits_{c=1}^3{\overline{Y_{c,m,y,r}}}&#10;$$&#10;$$&#10;Y_{m,y,r,b} = \sum\limits_{c=1}^3{Y_{c,m,y,r,b}}&#10;$$&#10;$$&#10;\sigma_{m,y,r} = \sqrt{\sum\limits_{c=1}^3{\sigma^2_{c,m,y,r}}+2\sum\limits_{c1=1}^2{\sum\limits_{c2=2\neq{c1}}^3{cov_b(Y_{c1,m,y,r,b},Y_{c2,m,y,r,b})}}}&#10;$$&#10;Find the mean and SD of total Y for a given month ($m$) in a given region ($r$) (e.g., for all Mays in Central America).&#10;$$&#10;\overline{Y_{m,r}} = mean_y(\overline{Y_{m,y,r}}) = \frac{\sum\limits_{y=1}^N{\overline{Y_{m,y,r}}}}{N}&#10;$$&#10;$$&#10;Y_{m,r,b} = \frac{\sum\limits_{y=1}^N{\sum\limits_{c=1}^3{Y_{c,m,y,r,b}}}}{N}&#10;$$&#10;$$&#10;\sigma_{m,r} = \frac{1}{N}\sqrt{\sum\limits_{y=1}^N{\sigma^2_{m,y,r}}+2\sum\limits_{y1=1}^{N-1}{\sum\limits_{y2=2\neq{y1}}^N{cov_b(Y_{m,y1,r,b},Y_{m,y2,r,b})}}}&#10;$$&#10;Find the mean and SD of annual total Y for a given region ($r$) (e.g., for Central America):&#10;$$&#10;\overline{Y_{r}} = \sum\limits_{m=1}^{12}{\overline{Y_{m,r}}}&#10;$$&#10;$$&#10;\sigma_r = \sqrt{\sum\limits_{m=1}^{12}\sigma^2_{m,r}+2\sum\limits_{m1=1}^{11}{\sum\limits_{m2=2\neq{m1}}^{12}{cov_b(Y_{m1,r,b},Y_{m2,r,b})}}}&#10;$$&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-05-27T16:12:54.493" Id="100212" LastActivityDate="2014-05-28T16:27:22.333" LastEditDate="2014-05-28T16:27:22.333" LastEditorUserId="20363" OwnerUserId="20363" ParentId="100110" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I want to see if this other data I have predicts that this first event will occur.&lt;/p&gt;&#10;&#10;&lt;p&gt;In other words, I have a data field with a record of farts, with columns for day, month, year, and then I have another data field with a record of meals, with columns for date month and year, and also a column for whether the eater is a bean eater.&lt;/p&gt;&#10;&#10;&lt;p&gt;How do I analyze this? It would be easier if this data was in a easy time series format, with dates on the x axis and number of farts on the y axis, but the data only records when a fart occurred, not when a fart didn't occur. Thanks!&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-05-27T18:10:17.707" Id="100234" LastActivityDate="2014-05-27T19:34:03.150" LastEditDate="2014-05-27T19:34:03.150" LastEditorUserId="46257" OwnerUserId="46257" PostTypeId="1" Score="0" Tags="&lt;time-series&gt;&lt;categorical-data&gt;&lt;dataset&gt;&lt;binary-data&gt;" Title="I have data that includes a record of each event and its time" ViewCount="20" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm sure most people are familiar with word grid games like Boggle and the newer digital versions Scramble with Friends and Ruzzle.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Q2ePS.png&quot; alt=&quot;4x4 word grid&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;For anyone not familiar, the idea is to find words by using adjacent tiles.  You start from any cell and try to spell a word by dragging up, down, left, right, or diagonal.&#10;The board doesn't wrap, and you can't reuse letters you've already selected.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm trying to figure out the likelihood of a word appearing on a board given the likelihood of knowing how often individual letters appear.  For example, if I know that the letter A appears 9.8% of the time, what is the probability of seeing the word AA?&lt;/p&gt;&#10;&#10;&lt;p&gt;I know this is fairly simple, but it's been too long since college stats.  (I have two hokey models, but I'd like to hear from the experts.)  I could run a simulation of a million boards and come up with an empirical answer--in fact, someone has--but I'd rather understand why that is.  In order to make things simpler, I'd like to ignore two rules that add to the complexity:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;We can ignore the constraints of how many times a letter can appear on a board.  e.g., don't worry about whether there will be enough E's to make the word ELECTEE.&lt;/li&gt;&#10;&lt;li&gt;We can ignore the fact that letters have to be adjacent.  This means we don't have to figure out the likelihood that a letter is a neighbor of another letter that we need for the word&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;So with that being said, and with the following probabilities&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;A: 0.098&lt;/li&gt;&#10;&lt;li&gt;E: 0.146&lt;/li&gt;&#10;&lt;li&gt;R: 0.079&lt;/li&gt;&#10;&lt;li&gt;S: 0.102&lt;/li&gt;&#10;&lt;li&gt;T: 0.098&lt;/li&gt;&#10;&lt;li&gt;C: 0.021&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;What is the likelihood of a random board containing the word SEE?  What about SET?  TEAR?  What about the rarer CREATES?&lt;/p&gt;&#10;&#10;&lt;p&gt;NOTE: I did search here already and I think this is a similar but different problem than this question: &lt;a href=&quot;http://stats.stackexchange.com/questions/74468/probability-of-drawing-a-given-word-from-a-bag-of-letters-in-scrabble&quot;&gt;Probability of drawing a given word from a bag of letters in Scrabble&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-05-27T18:14:36.517" Id="100236" LastActivityDate="2014-05-27T18:14:36.517" OwnerUserId="46252" PostTypeId="1" Score="1" Tags="&lt;games&gt;" Title="Computing probabilities of letters in a word grid" ViewCount="40" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;In this &lt;a href=&quot;http://stats.stackexchange.com/questions/29719/how-to-determine-best-cutoff-point-and-its-confidence-interval-using-roc-curve-i&quot;&gt;question&lt;/a&gt;, one of the comments discusses the benefits of using Bayes optimum decision rules coupled with loss functions rather than using other performance metrics such as sensitivity.  My attempts at reviewing the biostatistical literature on this topic leads me to different discussions by Pepe, Vickers, Pencina, Cook, and others. I may have missed a portion of literature, but none of these papers and reviews seem to discuss the ideas in this comment. Can anyone suggest good reviews on this topic or any successful application of these ideas in the biostatistical literature. I've already seen some discussion in the machine learning literature, but pointers to relevant papers in this field would also be appreciated.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-27T18:49:09.527" Id="100242" LastActivityDate="2014-05-27T18:49:09.527" OwnerUserId="12548" PostTypeId="1" Score="0" Tags="&lt;classification&gt;&lt;decision-theory&gt;" Title="bayes optimum decisions and loss functions" ViewCount="40" />
  
  <row Body="&lt;p&gt;This is a really interesting question. I'm also looking for the same thing. Actually, there are lot of different ways to deal with it.&lt;/p&gt;&#10;&#10;&lt;p&gt;The first thing, in my opinion, will be to determine what type of missing data you have - missing completely at random (MCAR), missing at random (MAR), or missing not at random ( NMAR). This is difficult and controversial to prove but this &lt;a href=&quot;http://os1.amc.nl/wikistatistiek/images/Clark_-_JCE_2003_Missing_values.pdf&quot; rel=&quot;nofollow&quot;&gt;paper&lt;/a&gt; shows an interesting way to look at MAR data.&lt;/p&gt;&#10;&#10;&lt;p&gt;To deal with multiple imputation R has a few packages:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;code&gt;MICE&lt;/code&gt; (which seems very used), &lt;/li&gt;&#10;&lt;li&gt;&lt;code&gt;randomForest&lt;/code&gt;, &lt;/li&gt;&#10;&lt;li&gt;&lt;code&gt;Hmisc&lt;/code&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;code&gt;Amelia&lt;/code&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;code&gt;mi&lt;/code&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;These are only few of the packages I found so far.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;MICE&lt;/code&gt; also has implemented random forest and a few other methods, like predictive mean matching.&lt;/p&gt;&#10;&#10;&lt;p&gt;This is not much but may help you figure out some things. As soon as I have results or decide with which method I will proceed I'll edit the post.&lt;/p&gt;&#10;&#10;&lt;p&gt;Good luck!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-05-27T20:59:35.383" Id="100256" LastActivityDate="2014-05-28T19:27:54.413" LastEditDate="2014-05-28T19:27:54.413" LastEditorUserId="7244" OwnerUserId="14154" ParentId="100020" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;What you ask for is still an open question in signal processing. Generally, joint diagonalization is not possible for more than 2 general matrices. However, it is possible to find some approximate diagonalizer in a least-square sense. Look at the articles by A. Yeredor for a start.  &lt;/p&gt;&#10;&#10;&lt;p&gt;In some very special cases there is an exact joint diagonalization for a set of matrices. For example, if you have $N$ rank $1$ matrices of dimension $N\times N$ you can do that.  &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-05-27T22:16:04.093" Id="100261" LastActivityDate="2014-05-27T22:37:47.190" LastEditDate="2014-05-27T22:37:47.190" LastEditorUserId="7290" OwnerUserId="46270" ParentId="16790" PostTypeId="2" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;I was discussing about the popularity of some terms and used google trends to conclude in the decrease of their popularity. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.google.com/trends/explore#q=/m/01v0b8,%20/m/02s1yd,%20/m/01v0m4,%20/m/030fmv,%20/m/0l9nq&amp;amp;cmpt=q&quot; rel=&quot;nofollow&quot;&gt;Here&lt;/a&gt; is an exemple of the queries for some of the biggest french engineering schools. I concluded rapidly in a decreasing interest/popularity of these schools.&lt;/p&gt;&#10;&#10;&lt;p&gt;But giving another thought, I wondered if the decrease may have another explanation. Google trends give a relative number, proportional to the total number of queries. Is it possible that the population accessing google has so much evolved in these years that we can't conclude to a decrease in interest ?&#10;I mean that the majority of queries should come from educated/wealthy people and researchers. So with the democratization of internet access the fraction of wealthy/educated people over total should have decreased. Thus explaining a decrease of the querries over total.&lt;/p&gt;&#10;&#10;&lt;p&gt;How can we show this effect ?&#10;How can we remove this effect ?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-05-27T22:39:38.460" Id="100262" LastActivityDate="2014-05-27T22:44:53.213" LastEditDate="2014-05-27T22:44:53.213" LastEditorUserId="21846" OwnerUserId="21846" PostTypeId="1" Score="0" Tags="&lt;untagged&gt;" Title="Google trends data for interest" ViewCount="26" />
  <row AcceptedAnswerId="100276" AnswerCount="4" Body="&lt;p&gt;I've time and again &lt;em&gt;rejected&lt;/em&gt; or &lt;em&gt;failed to reject&lt;/em&gt; the null hypothesis. In the failure to reject case, you conclude that there isn't sufficient evidence for rejection and you &quot;move on&quot; (i.e., either gather more data, end the experiment etc.,)&lt;/p&gt;&#10;&#10;&lt;p&gt;But when you &quot;do&quot; reject the null hypothesis, providing &lt;em&gt;some&lt;/em&gt; evidence for the alternative hypothesis you can't really &quot;prove&quot; that your alternative hypothesis indeed holds true. &lt;/p&gt;&#10;&#10;&lt;p&gt;So, what are the common next steps once you reject the null hypothesis? What tools/techniques does one adopt to &quot;analyze the problem further&quot; to make more the findings more conclusive? What are the logical &quot;next steps&quot; as a statistician warranting further analysis?&lt;/p&gt;&#10;&#10;&lt;p&gt;For example:&lt;/p&gt;&#10;&#10;&lt;p&gt;$H_0: \mu_1 = \mu_0$&lt;/p&gt;&#10;&#10;&lt;p&gt;$H_1: \mu_1 &amp;gt; \mu_0$ (say we know the expected direction)&lt;/p&gt;&#10;&#10;&lt;p&gt;Once we reject the null hypothesis at some level of significance we have &quot;some evidence&quot; for the alternative to be true, but we can't draw that conclusion. If I really want to draw that conclusion conclusively (pardon the double word play) what should I do?&lt;/p&gt;&#10;&#10;&lt;p&gt;I've never pondered this question during my undergrad days but now that I'm doing a fair deal of hypotheses testing I can't help but wonder what's ahead :)&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-05-27T23:44:09.607" FavoriteCount="7" Id="100267" LastActivityDate="2014-05-29T00:05:13.350" LastEditDate="2014-05-28T00:45:44.853" LastEditorUserId="4426" OwnerUserId="4426" PostTypeId="1" Score="17" Tags="&lt;hypothesis-testing&gt;" Title="Now that I've rejected the null hypothesis what's next?" ViewCount="2235" />
  <row AnswerCount="0" Body="&lt;p&gt;I want to test statistical methods for model selection in binary classification problems. In order to do so, I plan to generate data and then use some specific model (i.e. fix parameters for my model) to assign labels for each sample. Finally, I will try to retrieve the right parameters by choosing among several models (they only differ in their parameters, not architecture) the ones that minimize the given criteria.&lt;/p&gt;&#10;&#10;&lt;p&gt;How should I choose/generate the initial data I want to label?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-27T23:59:08.857" Id="100268" LastActivityDate="2014-05-27T23:59:08.857" OwnerUserId="45434" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;model-selection&gt;" Title="how to generate data for model selection in machine learning" ViewCount="36" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;currently finishing the last year of PhD in statistics, we wonder if you could help us with the following.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $T = [0,1]$ and $X = \left( X_{t}, t \in T \right)$ be a gaussian process with mean function $m$ and covariance function $W$. Parentheses in functions are omitted in this notation. We write $X \sim GP(m,W)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;In a first time, suppose that the sample paths $t \mapsto X_{t}$ are continuous a.s. and let $Y$ be the stochastic process defined on $T$ by :&#10;$$Y_{t} = \int_{0}^{t} X(u) du,\mbox{ a.s.}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The question is : what can you say about the law of $Y$ ? In particular, do you believe it to be $GP(m^{\star},W^{\star})$, given by :&#10;$$\begin{array}{ccc}&#10;m^{\star}(t) &amp;amp; = &amp;amp; \int_{0}^{t} m(u)du,\\&#10;W^{\star}(s,t) &amp;amp; = &amp;amp; \int_{0}^{s} \int_{0}^{t} W(u,v)dudv.&#10;\end{array}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;If the preceding statement is true, could you please give us some references?&lt;/p&gt;&#10;&#10;&lt;p&gt;In a second time, suppose that the sample paths $t \mapsto X_{t}$ are differentiable a.s. and let $Z$ be the stochastic process defined on $T$ by :&#10;$$Z_{t} = \frac{dX_{t}}{dt},\mbox{ a.s.}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The question is now : what can you say about the law of $Z$? Do you have any reference where we could find a result?&lt;/p&gt;&#10;&#10;&lt;p&gt;We would be very grateful for any feedback you can give us. Thank you very much.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-28T10:01:09.930" FavoriteCount="1" Id="100314" LastActivityDate="2014-05-28T10:01:09.930" OwnerUserId="46299" PostTypeId="1" Score="1" Tags="&lt;distributions&gt;&lt;normal-distribution&gt;" Title="Calculation of distribution of a gaussian process" ViewCount="35" />
  <row AnswerCount="1" Body="&lt;p&gt;I performed factor analysis on R using &lt;a href=&quot;http://stat.ethz.ch/R-manual/R-patched/library/stats/html/factanal.html&quot; rel=&quot;nofollow&quot;&gt;factanal&lt;/a&gt;. Following advice I found on &lt;a href=&quot;http://ocw.jhsph.edu/courses/statisticspsychosocialresearch/pdfs/lecture8.pdf&quot; rel=&quot;nofollow&quot;&gt;this tutorial&lt;/a&gt;, I chose the number of factors as being the number of principal components that capture 90% of the variability.&#10;I got the following table of results:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;               Factor1 Factor2 Factor3 Factor4 Factor5 Factor6 Factor7 Factor8 Factor9&#10;SS loadings     29.677   9.530   6.340   5.638   4.188   3.636   1.484   0.804   0.651&#10;Proportion Var   0.309   0.099   0.066   0.059   0.044   0.038   0.015   0.008   0.007&#10;Cumulative Var   0.309   0.408   0.474   0.533   0.577   0.615   0.630   0.638   0.645&#10;&#10;Test of the hypothesis that 9 factors are sufficient.&#10;The chi square statistic is 30148.02 on 3732 degrees of freedom.&#10;The p-value is 0 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I tried with one factor and I got a p-value of 0 as well...&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Why is the cumulative var smaller than with 9 PCs for a pca?&lt;/li&gt;&#10;&lt;li&gt;How can I interpret the above table to choose the correct number of factors? (since the p-value is the same for 1 of 9 factors)&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="1" CreationDate="2014-05-28T12:13:48.053" Id="100325" LastActivityDate="2014-05-28T13:07:16.850" OwnerUserId="28183" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;feature-selection&gt;&lt;factor-analysis&gt;" Title="Interpreting results of a factor analysis" ViewCount="73" />
  
  
  
  <row Body="&lt;p&gt;Does the covariance between two points $x$ and $y$ depend only on $x-y$?  If so, then a spectral method is one way to approach the problem.  This is discussed in many textbooks on geostatistics.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-05-28T14:17:50.967" Id="100344" LastActivityDate="2014-05-28T14:17:50.967" OwnerUserId="8200" ParentId="100301" PostTypeId="2" Score="1" />
  
  
  <row AcceptedAnswerId="100368" AnswerCount="2" Body="&lt;p&gt;Unbalanced panels are more common in economic fields, if I want to know the behaviour of firms, what will be the differences using unbalanced data panel. Are there advantages? Does it depend on analysis's period? Or it will be better use balanced panel?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-28T14:35:54.687" Id="100348" LastActivityDate="2014-05-28T17:24:05.207" LastEditDate="2014-05-28T14:41:10.750" LastEditorUserId="46168" OwnerUserId="46168" PostTypeId="1" Score="3" Tags="&lt;panel-data&gt;" Title="Unbalanced data or Balanced data" ViewCount="113" />
  <row AnswerCount="1" Body="&lt;p&gt;We have route-level data (that I cannot share) on monthly bus ridership in New York City, creating a panel $N= 185$, $T=36$. We estimate a fixed effects model and random effects model with R's &lt;code&gt;plm&lt;/code&gt; package. (For this MWE, I use the &lt;code&gt;Grunfeld&lt;/code&gt; investment data, which illustrates the problems I am seeing fairly well).&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(plm)&#10;data(&quot;Grunfeld&quot;)&#10;model_1A &amp;lt;- lm(inv ~ value + capital, data= Grunfeld)&#10;fe.plm &amp;lt;- plm(formula(model_1A), model=&quot;within&quot;, index=c(&quot;firm&quot;, &quot;year&quot;),&#10;              data=Grunfeld)&#10;re.plm &amp;lt;- plm(formula(model_1A), model=&quot;random&quot;, index=c(&quot;firm&quot;, &quot;year&quot;),&#10;              data=Grunfeld)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;A Hausman test indicates that the FE model is preferred, because the estimates differ (note that in the MWE, we fail to reject).&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;phtest(fe.plm, re.plm)&#10;&#10;##  Hausman Test&#10;&#10;##data:  formula(model_1A)&#10;##chisq = 2.3304, df = 2, p-value = 0.3119&#10;##alternative hypothesis: one model is inconsistent&#10;&#10;pbgtest(fe.plm)&#10;&#10;##  Breusch-Godfrey/Wooldridge test for serial correlation in panel models&#10;&#10;##data:  formula(model_1A)&#10;##chisq = 65.0632, df = 20, p-value = 1.14e-06&#10;##alternative hypothesis: serial correlation in idiosyncratic errors&#10;&#10;pbgtest(re.plm)&#10;&#10;##  Breusch-Godfrey/Wooldridge test for serial correlation in panel models&#10;&#10;##data:  formula(model_1A)&#10;##chisq = 69.9495, df = 20, p-value = 1.856e-07&#10;##alternative hypothesis: serial correlation in idiosyncratic errors&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This is where what I think should happen diverges from what many people try to do. My understanding of serial correlation is that it affects the standard errors but not the coefficients. This would suggest to me a serial correlation-robust standard error. For instance (&lt;a href=&quot;http://stats.stackexchange.com/a/60262/10026&quot;&gt;as in this answer&lt;/a&gt;),&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;fe.rse &amp;lt;- sqrt(diag(vcovHC(fe.plm, type=&quot;HC1&quot;, cluster=&quot;group&quot;)))&#10;re.rse &amp;lt;- sqrt(diag(vcovHC(re.plm, type=&quot;HC1&quot;, cluster=&quot;group&quot;)))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;** Why not just use sc-robust standard errors?**&lt;/p&gt;&#10;&#10;&lt;p&gt;But what many authors do instead is include specific AR(1) or ARMA disturbances because Stata makes this easy. For the FE models, we can use &lt;code&gt;gls&lt;/code&gt; from the &lt;code&gt;nlme&lt;/code&gt; package on demeaned data (note: &lt;code&gt;fe.plm&lt;/code&gt; and &lt;code&gt;fe.gls&lt;/code&gt; are virtually identical),&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# within estimator is demeaned&#10;demean &amp;lt;- numcolwise(function(x) x - mean(x))&#10;Grunfeld.dm &amp;lt;- ddply(Grunfeld, .(firm), demean)&#10;Grunfeld.dm$year &amp;lt;- Grunfeld$year&#10;&#10;fe.gls &amp;lt;- gls(update(formula(model_1A), .~.-1), method=&quot;ML&quot;,  data=Grunfeld.dm)&#10;fear.gls &amp;lt;- update(fe.gls,  correlation = corAR1(form = ~ year | firm))&#10;fearma.gls &amp;lt;- update(fe.gls, correlation = corARMA(form = ~ year | firm, &#10;                                                   p=1,q=1))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The RE models can be estimated in with &lt;code&gt;lme&lt;/code&gt; in the &lt;code&gt;nlme&lt;/code&gt; package (again, &lt;code&gt;re.plm&lt;/code&gt; and &lt;code&gt;re.lme&lt;/code&gt; are identical).&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;re.lme &amp;lt;- lme(fixed = formula(model_1A), random = ~ 1|firm, data = mta)&#10;rear.lme &amp;lt;- update(re.lme,  correlation = corAR1(form = ~ year | firm))&#10;rearma.lme &amp;lt;- update(re.lme,  correlation = corARMA(form = ~ year | firm, &#10;                                                    p=1,q=1))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;There are a few things I don't understand about this:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Why do the coefficients change when serial correlation doesn't (shouldn't?) affect estimates?&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Can we still use the Hausman test to select between FE and RE models with autoregressive errors?&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;strong&gt;How can we test for residual autocorrelation? And if it exists, wouldn't we &lt;em&gt;still&lt;/em&gt; need a robust standard error?&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2014-05-28T16:39:18.683" Id="100365" LastActivityDate="2014-08-16T18:13:00.140" LastEditDate="2014-08-16T18:13:00.140" LastEditorUserId="26338" OwnerUserId="10026" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;econometrics&gt;&lt;panel-data&gt;&lt;hausman&gt;" Title="Serial correlation: estimation vs robust SE" ViewCount="498" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am using unit root tests in Stata's xtunitroot command. The data are annual wage replacement rates for unemployment benefits in each of the 50 states for 1977 to 2006. (They vary noticably from state to state and year to year)&lt;/p&gt;&#10;&#10;&lt;p&gt;I am getting contradictory results for tests of a common unit root.&#10;In all cases the tests specify both demean and detrend.&lt;/p&gt;&#10;&#10;&lt;p&gt;the Leven Lui Chin result indicates that I can reject the null of &quot;panels contain unit roots. p&amp;lt;.001 &lt;/p&gt;&#10;&#10;&lt;p&gt;The HT version indicates I cannot reject the null &quot;panels contain unit roots&quot; (p&amp;lt;1.00)&lt;/p&gt;&#10;&#10;&lt;p&gt;The Breitung version tells me that I cannot (quite) reject the null &quot;panel contains unit roots  (p&amp;lt;.08)&lt;/p&gt;&#10;&#10;&lt;p&gt;Any advice?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-28T17:18:38.357" Id="100367" LastActivityDate="2014-05-28T17:18:38.357" OwnerUserId="46322" PostTypeId="1" Score="0" Tags="&lt;stata&gt;&lt;panel-data&gt;&lt;unit-root&gt;" Title="Contradictory xtunitroot results" ViewCount="55" />
  
  <row Body="&lt;p&gt;My knee jerk response is:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;without a measure of goodness the word &quot;better&quot; has no meaning.&lt;/li&gt;&#10;&lt;li&gt;I am trending to &quot;c, none of the above&quot; as my preferred approach&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;The approach to &quot;build&quot;, if you mean build = grow, is to split on the best leaf until a stopping criterion is breached.&lt;/p&gt;&#10;&#10;&lt;p&gt;Like all Machine Learning tools, there are two goals for the tree, and they comprise a measure of goodness for the pruning process: maximize generalization, minimize error in representation.  If you can agree to these two, in a general sense, then I can proceed with an answer.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-28T17:59:31.077" Id="100375" LastActivityDate="2014-05-28T17:59:31.077" OwnerUserId="22452" ParentId="23406" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Answering you question: Cluster Robust is also Heteroskedastic Consistent. &lt;/p&gt;&#10;&#10;&lt;p&gt;I would recommend that you read the &lt;a href=&quot;http://cameron.econ.ucdavis.edu/research/Cameron_Miller_Cluster_Robust_October152013.pdf&quot; rel=&quot;nofollow&quot;&gt;A Practitioner's Guide to Cluster-Robust Inference&lt;/a&gt; which is a nice piece from Colin Cameron on several aspects of clustered/heteroskedastic robust errors.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Page 20 onward should help you out.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-05-28T19:28:24.677" Id="100388" LastActivityDate="2014-05-28T19:28:24.677" OwnerUserId="8528" ParentId="100373" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;Your problem seems tailor-made for some sort of low-rank matrix completion. Try using the &lt;code&gt;impute.svd()&lt;/code&gt; function from the &lt;a href=&quot;http://cran.r-project.org/web/packages/bcv/bcv.pdf&quot; rel=&quot;nofollow&quot;&gt;&lt;code&gt;bcv&lt;/code&gt; package&lt;/a&gt;. I would suggest using a small rank (the argument &lt;code&gt;k&lt;/code&gt;) -- something like 5. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-28T21:30:50.117" Id="100400" LastActivityDate="2014-05-29T03:45:54.987" LastEditDate="2014-05-29T03:45:54.987" LastEditorUserId="19762" OwnerUserId="7690" ParentId="100020" PostTypeId="2" Score="1" />
  
  
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I am running a glm in R on data with quite many predictors (~50), both initially continuous and factors. The response is binary and the volume of the data is OK (~100K rows), in order to model non-linear relationships, I convert the continuous variables to factors as well. In the end this results in some levels of some variables being insignificant. For example, variable &quot;age&quot; is binned into 20-30, 40-50, 50-60, 60+ and only the first two are significant.&lt;/p&gt;&#10;&#10;&lt;p&gt;All this results in a reasonable model with AUC 0.86 and residuals looking random with a small bias. &lt;/p&gt;&#10;&#10;&lt;p&gt;I understand that R converts each factor level to a binary variable and then runs the regression, and I'd still like to improve the model. Would you please help me understand if: &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;converting all the variables' factor levels to binary myself,&lt;/li&gt;&#10;&lt;li&gt;selecting only the variables that were significant in the initial model&lt;/li&gt;&#10;&lt;li&gt;then re-fitting the model with the new (reduced) variable set&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;sound like a good idea?&lt;/p&gt;&#10;&#10;&lt;p&gt;Or should I continue to use predict() on the model with many insignificant coefficients for factor levels happily as before?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-29T08:59:22.733" Id="100453" LastActivityDate="2014-06-02T09:19:10.580" OwnerUserId="21674" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;generalized-linear-model&gt;&lt;regression-coefficients&gt;&lt;binning&gt;" Title="Should the final R glm include only significant levels of factors" ViewCount="419" />
  <row Body="&lt;p&gt;Checking if your time series is stationary or not, see this &lt;a href=&quot;http://stats.stackexchange.com/questions/27332/how-to-know-if-a-time-series-is-stationary-or-non-stationary&quot;&gt;post&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Checking which ARMA/ARIMA model to fit, see this &lt;a href=&quot;http://people.duke.edu/~rnau/411arim3.htm&quot; rel=&quot;nofollow&quot;&gt;website&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-05-29T10:13:42.883" Id="100465" LastActivityDate="2014-05-29T10:13:42.883" OwnerUserId="46087" ParentId="100459" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;The simplest thing would be to look at the difference between your price and each vendor's price (in absolute value terms) and see which is highest each month. That gives you one rating for each vendor each month for each comparison you want to make (where comparisons would be &quot;all&quot;, &quot;equity&quot;, &quot;bond&quot; etc).&lt;/p&gt;&#10;&#10;&lt;p&gt;You could graph these differences over time to see if they change. &lt;/p&gt;&#10;&#10;&lt;p&gt;Slightly more complex, you could set up a regression model for each month:&lt;/p&gt;&#10;&#10;&lt;p&gt;AVdiff ~ vendor + category&lt;/p&gt;&#10;&#10;&lt;p&gt;where AVdiff is the absolute value of the difference between prices and the other two are dummy variables. That would give you one model per month.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are ways to try to account for multiple months at once (e.g. multi-level models, generalized estimating equations) but I don't think they are right here as your main interest seems to be in what would be the random effect (vendor) not in change over time. &lt;/p&gt;&#10;&#10;&lt;p&gt;Time series models may be appropriate; I do not know enough about these to give a solid recommendation one way or the other, but usually they require quite long series and you may not have enough months. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-29T13:05:00.170" Id="100486" LastActivityDate="2014-05-29T13:05:00.170" OwnerUserId="686" ParentId="100483" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;Hi guys I am doing some model selection and need to estmiate both observed and expected Fisher Information Matrix to calculate this &lt;a href=&quot;http://myweb.uiowa.edu/cavaaugh/ms_lec_4_ho.pdf&quot; rel=&quot;nofollow&quot;&gt;link&lt;/a&gt;.&#10;How do you estimate that expected one numerically? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-29T13:28:28.997" Id="100488" LastActivityDate="2015-03-04T06:49:49.373" LastEditDate="2014-05-30T09:52:47.607" LastEditorUserId="45986" OwnerUserId="45986" PostTypeId="1" Score="3" Tags="&lt;fisher-information&gt;" Title="what is the easy way to estimate expected Fisher information matrix for a given functional model?" ViewCount="85" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am using SAS to do clustering analysis on a huge dataset. Since my dataset contains various types of variables, I am confused about the appropriate method to perform the analysis. Here are my questions:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;How to deal with nominal variables?&lt;/li&gt;&#10;&lt;li&gt;I standardize the data to be analyzed using &lt;code&gt;PROC STANDARD (mean=0 std=1)&lt;/code&gt; before doing &lt;code&gt;PROC CLUSTER&lt;/code&gt;. However, the result will be sill based on binary variables, meaning the clusters I got are heavily divided based on binary variables used, such as gender. How can I deal with this issue?&lt;/li&gt;&#10;&lt;li&gt;If I make some variables have a larger std, will these variables have larger effect on the cluster generated, i.e., make my clusters more dependent on these variables?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="1" CreationDate="2014-05-29T15:53:26.917" Id="100506" LastActivityDate="2014-05-29T15:58:13.933" LastEditDate="2014-05-29T15:58:13.933" LastEditorUserId="7290" OwnerUserId="46390" PostTypeId="1" Score="0" Tags="&lt;clustering&gt;&lt;sas&gt;" Title="How to cluster with binary, nominal, ordinal and continuous variables?" ViewCount="119" />
  
  <row Body="&lt;p&gt;You are trying to get order statistics in a non-order statistic framework. &lt;/p&gt;&#10;&#10;&lt;p&gt;You could change your input vector from being a 5x1 vector of integers to being a 10x1 vector comprised of the first 5x1 vector that is then concatenated to the rank of each value.  Easy idea to think, hard to write in words.  Code might help, see below.&lt;/p&gt;&#10;&#10;&lt;p&gt;I use the following MATLAB code to make a sample vector:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x1=randint(5,1,[0,12])&#10;[~,~,x2]=unique(x1)&#10;X=[x1;x2]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;It looks like you are trying to wrap sorting and order-based comparison in a classic one-off algebraic expression instead of a loop.  The problem is that sorting is an NP-easy (non-polynomial easy) task.  the P vs. NP part of the problem means it is unlikely that there exists an analytic expression for the NN to converge to.  The NN learns from data and approaches an analytic expression, but if the expression does not exist then you have problems.  By preprocessing the order statistic you then return the problem to be within what the NN (assuming it is well behaved and properly related in topology to the problem) is capable of approximating.&lt;/p&gt;&#10;&#10;&lt;p&gt;Best of luck.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-29T18:03:04.010" Id="100521" LastActivityDate="2014-05-29T18:03:04.010" OwnerUserId="22452" ParentId="62920" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Let $f(k,n)$ be the chance that one or more cookies in a batch of $k$ cookies with $n$ raisins (well and randomly mixed) in the dough will have no raisins.  Consider the $1/k$ portion of the dough that will end up in the first cookie.  Complete randomness of the mixing means that each of the $n$ raisins will independently have a $1/k$ chance of ending up in that first cookie.  Therefore the distribution of the number of raisins in that cookie is Binomial, with parameters $1/k$ and $n$.  Let &lt;/p&gt;&#10;&#10;&lt;p&gt;$$p(i, 1/k, n) = k^{-n}\binom{n}{i} (k-1)^{n-i}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;designate the chance that the cookie gets exactly $i$ raisins ($0 \le i \le n$).&lt;/p&gt;&#10;&#10;&lt;p&gt;The events $i=0, i=1, \ldots, i=n$ partition all the possibilities, so their probabilities add.  If $i=0$, we have found a cookie with no raisins (and the chance of that happening equals the chance that all $n$ raisins end up in the remaining $1-1/k$ of the dough: $(1-1/k)^n$).  Otherwise, we continue to examine the remaining cookies.  The assumption of completely random mixing means that when the first cookie has $i$ raisins, the remaining $n-i$ raisins are randomly and independently mixed into the remaining $1-1/k$ of the dough, from which $k-1$ cookies will be made.  Consequently, when $k\ge 1$ and $n\ge 0,$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$f(k,n) = (1-1/k)^n + \sum_{i=1}^n p(i, 1/k, n) f(k-1, n-i).$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The initial conditions are $f(0,n)=0$ (when there are no cookies, there is no chance for a cookie to have no raisins), $f(1,n)=0$ for $n\ge 1$ (when there is one cookie and one or more raisins, the cookie must have some raisins), and $f(k,0) = 1$ (when there are no raisins and some cookies, at least one cookie will have no raisins).  These uniquely determine the solution, which one can check is &lt;/p&gt;&#10;&#10;&lt;p&gt;$$f(k,n) = k^{-n} \sum _{i=1}^{k-1} (-1)^{i+k-1} i^n \binom{k}{i}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The value $f(100,400)$ is approximately $0.847695$.  To check this, I ran a simulation of $10^5$ batches of cookie dough using a multinomial random number generator in &lt;code&gt;R&lt;/code&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#&#10;# Specify the simulation parameters.&#10;#&#10;n.raisins &amp;lt;- 400&#10;n.cookies &amp;lt;- 100&#10;n.iter &amp;lt;- 10^5&#10;#&#10;# Run the simulation.  (Takes about 1 second.)&#10;#&#10;set.seed(17)&#10;sim &amp;lt;- rmultinom(n.iter, n.raisins, rep(1, n.cookies))&#10;#&#10;# Find the proportion of batches with at least one raisin-free cookie.&#10;#&#10;p.hat &amp;lt;- mean(apply(sim, 2, min)==0)&#10;#&#10;# Report the value with its standard error and a Z-statistic for a hypothetical result.&#10;#&#10;p.se &amp;lt;- sqrt(p.hat*(1-p.hat) / n.iter)&#10;z &amp;lt;- round((0.847695 - p.hat) / p.se, 3)&#10;decimals &amp;lt;- ceiling(-log(p.se, 10)) + 1&#10;cat(&quot;Estimated chance is &quot;, round(p.hat, decimals), &#10;    &quot; (&quot;, round(p.se, decimals), &quot;); Z = &quot;, z, &quot;\n&quot;, sep=&quot;&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The output is&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Estimated chance is 0.8496 (0.0011); Z = -1.694&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;This is not significantly different from the theoretical value, but it is hugely different from &lt;a href=&quot;http://www.umass.edu/wsp/resources/poisson/answer07.html&quot; rel=&quot;nofollow&quot;&gt;the answer&lt;/a&gt; of &quot;$0.0183$&quot;.  The answer's explanation suggests an entirely different interpretation of the question: it estimates the chance that &lt;em&gt;one designated cookie&lt;/em&gt;, having a Poisson distribution of raisins with a mean of $4$, will have no raisins.  But that's clearly not what the question asks.  In fact, using this approximate answer, we could compute the chance that at least one of $100$ &lt;em&gt;independent&lt;/em&gt; such cookies has no raisins: it equals $0.8425338$, computed by the &lt;code&gt;R&lt;/code&gt; expression &lt;code&gt;1 - (1-dpois(0, 4))^100&lt;/code&gt;.  This is an &lt;em&gt;adequate&lt;/em&gt; approximation, but even the foregoing simulation can determine it's not completely correct (the Z-value is -6.26, which is highly significant).&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;The answer to (7a) is obtained by finding the smallest $n$ for which $f(100,n)\le 1 - 0.99$. A quick line search finds &lt;/p&gt;&#10;&#10;&lt;p&gt;$$f(100, 915) = 0.0100976;\quad f(100, 916) = 0.00999707.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Therefore the answer is $916$.  Changing &lt;code&gt;n.raisins&lt;/code&gt; in the simulation to &lt;code&gt;916&lt;/code&gt; and re-running it gives the output&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Estimated chance is 0.00954 (0.00031)&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;That corroborates this answer.&lt;/p&gt;&#10;&#10;&lt;p&gt;The Poisson approximation gives $921$ as the answer:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;1 - (1-dpois(0, 920:921/100))^100&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;[1] 0.010053572 0.009954032&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Incidentally, this approximation has a simple formula: compute&lt;/p&gt;&#10;&#10;&lt;p&gt;$$-k \log(1 - 0.99^{1/k})$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and round it up to the nearest integer.  This in turn will be close to $k\left(\log(k) - \log(1-0.99)\right).$  These useful and simple formulas show where the value of the Poisson approximation lies: it replaces a long complicated sum for the exact answer by a readily-understood, easily-computed value that more clearly shows how the answer behaves in terms of $n$ and $k$.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-05-29T18:18:38.830" Id="100522" LastActivityDate="2014-05-29T21:27:34.140" LastEditDate="2014-05-29T21:27:34.140" LastEditorUserId="919" OwnerUserId="919" ParentId="100011" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Estimating that error correctly will be tricky.  But I would like to suggest it is more important first to find a better procedure to subtract background.  Only once a good procedure is available would it be worthwhile analyzing the amount of error.&lt;/p&gt;&#10;&#10;&lt;p&gt;In this case, using the mean is biased upwards by the contributions from the signal, which look large enough to be important.  Instead use a more robust estimator.  A simple one is the &lt;em&gt;median.&lt;/em&gt;  Moreover, a little more can be squeezed out of these data by adjusting the columns as well as the rows at the same time.  This is called &quot;median polish.&quot;  It is very fast to carry out and is available in some software (such as &lt;code&gt;R&lt;/code&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;These figures of simulated data with 793 rows and 200 columns show the result of adjusting background with median polish.  (Ignore the labels on the y-axis; they are an artifact of the software used to display the data.)&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/MPcxD.png&quot; alt=&quot;Figure 1&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;A very slight bias is still evident in the adjusted data: the top and bottom quarters, where the signal is not present in any column, are slightly greener than the middle half.  However, by contrast, merely subtracting row means from the data produces obvious bias:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/15x1q.png&quot; alt=&quot;Figure 2&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Scatterplots (not shown here, but produced by the code) of actual background against estimated background confirm the superiority of median polish.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, this is somewhat an unfair comparison, because to compute background you have previously selected columns believed not to have a signal.  But there are problems with this:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;If there are low-level signals present in those areas (which you haven't seen or expected), they will bias the results.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Only a small subset of the data has been used, magnifying the estimation error in background.  (Using only one-tenth of the available columns approximately triples the error in estimating background compared to using the nine-tenths of columns that appear to have little or no signal.)&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Furthermore, even when you are confident that some columns do not contain signals, you can still apply median polish to those columns.  This will protect you from unexpected violations of your expectations (that these are signal-free areas).  Moreover, this robustness will allow you to broaden the set of columns used to estimate background, because if you inadvertently include a few with some signal, they will have only a negligible effect.&lt;/p&gt;&#10;&#10;&lt;p&gt;Additional processing to identify isolated outliers and to estimate and extract the signal can be done, perhaps in the spirit of &lt;a href=&quot;http://stats.stackexchange.com/a/100399/919&quot;&gt;my answer to a recent related question&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;R&lt;/code&gt; code:&lt;/p&gt;&#10;&#10;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;#&#10;# Create background.&#10;#&#10;set.seed(17)&#10;i &amp;lt;- 1:793&#10;row.sd &amp;lt;- 0.08&#10;row.mean &amp;lt;- log(60) - row.sd^2/2&#10;background &amp;lt;- exp(rnorm(length(i), row.mean, row.sd))&#10;k &amp;lt;- sample.int(length(background), 6)&#10;background[k] &amp;lt;- background[k] * 1.7&#10;&#10;par(mfrow=c(1,1))&#10;plot(background, type=&quot;l&quot;, col=&quot;#000080&quot;)&#10;#&#10;# Create a signal.&#10;#&#10;j &amp;lt;- 1:200&#10;f &amp;lt;- function(i, j, center, amp=1, hwidth=5, l=0, u=6000) {&#10;  0.2*amp*outer(dbeta((i-l)/(u-l), 3, 1.1), pmax(0, 1-((j-center)/hwidth)^4))&#10;}&#10;#curve(f(x, 10, center=10), 0, 6000)&#10;#image(t(f(i,j, center=100,u=600)), col=c(&quot;White&quot;, rainbow(100)))&#10;&#10;u &amp;lt;- 600&#10;signal &amp;lt;- f(i,j, center=10, amp=110, u=u) +&#10;  f(i,j, center=90, amp=90, u=u) +&#10;  f(i,j, center=130, amp=80, u=u)&#10;#&#10;# Combine signal and background, both with some iid multiplicative error.&#10;#&#10;ccd &amp;lt;- outer(background, j, function(i,j) i) * exp(rnorm(length(signal), sd=0.05)) + &#10;  signal * exp(rnorm(length(signal), sd=0.1))&#10;ccd &amp;lt;- matrix(pmin(120, ccd), nrow=length(i))&#10;#image(j, i, t(ccd), col=c(rep(&quot;#f8f8f8&quot;,20), rainbow(100)),main=&quot;CCD&quot;)&#10;#&#10;# Compute background via row means (not recommended).&#10;# (Returns $row and $overall to match the values of `medpolish`.)&#10;#&#10;mean.subtract &amp;lt;- function(x) {&#10;  row &amp;lt;- apply(x, 1, mean)&#10;  overall &amp;lt;- mean(row)&#10;  row &amp;lt;- row - overall&#10;  return(list(row=row, overall=overall))&#10;}&#10;#&#10;# Estimate background and adjust the image.&#10;#&#10;fit &amp;lt;- medpolish(ccd)&#10;#fit &amp;lt;- mean.subtract(ccd)&#10;ccd.adj &amp;lt;- ccd - outer(fit$row, j, function(i,j) i)&#10;    image(j, i, t(ccd.adj), col=c(rep(&quot;#f8f8f8&quot;,20), rainbow(100)), &#10;          main=&quot;Background Subtracted&quot;)&#10;    plot(fit$row + fit$overall, type=&quot;l&quot;, xlab=&quot;i&quot;)&#10;    plot(background, fit$row)&#10;#&#10;# Plot the results.&#10;#&#10;require(raster)&#10;show &amp;lt;- function(y, nrows, ncols, hillshade=TRUE, aspect=1, ...) {&#10;  x &amp;lt;- apply(y, 2, rev)&#10;  x &amp;lt;- raster(x, xmn=0, xmx=ncols, ymn=0, ymx=nrows*aspect)&#10;  crs(x) &amp;lt;- &quot;+proj=lcc +ellps=WGS84&quot;&#10;  if (hillshade) {&#10;    slope &amp;lt;- terrain(x, opt='slope')&#10;    aspect &amp;lt;- terrain(x, opt='aspect')&#10;    hill &amp;lt;- hillShade(slope, aspect, 10, 60)&#10;    plot(hill, col=grey(0:100/100), legend=FALSE, ...)&#10;    alpha &amp;lt;- 0.5; add &amp;lt;- TRUE&#10;  } else {&#10;    alpha &amp;lt;- 1; add &amp;lt;- FALSE&#10;  }&#10;  plot(x, col=rainbow(127, alpha=alpha), add=add, ...)&#10;}&#10;&#10;par(mfrow=c(1,2))&#10;asp &amp;lt;- length(j)/length(i) * 6/8&#10;show(ccd, length(i), length(j), aspect=asp, main=&quot;Raw Data&quot;)&#10;show(ccd.adj, length(i), length(j), aspect=asp, main=&quot;Adjusted Data&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="3" CreationDate="2014-05-29T18:43:16.350" Id="100524" LastActivityDate="2014-05-29T18:43:16.350" OwnerUserId="919" ParentId="100484" PostTypeId="2" Score="7" />
  <row AnswerCount="1" Body="&lt;p&gt;Defining $Z=f(x,y)$ in interval $[q,1][q,1]$,$x,y$ are all i.i.d. uniform.&lt;br&gt;&#10;I know that, if $f(x,y)$ is non-increasing or non-decreasing for both $x$ and $y$, the median of $z$ is $f((1-q)/2,(1-q)/2)$. But if $f(x,y)$ is non-increasing or non-decreasing for only $x$, and it is not non-increasing or non-decreasing for $y$.&lt;/p&gt;&#10;&#10;&lt;p&gt;How can I get the median of $Z$?&lt;/p&gt;&#10;&#10;&lt;p&gt;The figure of $Z$ is as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/skWgim.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-29T19:24:04.287" Id="100530" LastActivityDate="2014-05-29T22:50:56.677" LastEditDate="2014-05-29T22:50:56.677" LastEditorUserId="22468" OwnerUserId="43780" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;median&gt;" Title="How to get the median of Z=f(x,y)?" ViewCount="58" />
  
  <row Body="&lt;p&gt;In &quot;vintage&quot; WinBugs this may work:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;mu.alpha ~ dnorm(0, 0.001)&#10;mu.beta  ~ dnorm(0, 0.001)&#10;&#10;sigma.squared.alpha ~ dgamma(0.001, 0.001)&#10;sigma.squared.beta  ~ dgamma(0.001, 0.001)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-05-29T22:12:35.943" Id="100547" LastActivityDate="2014-05-29T22:12:35.943" OwnerUserId="9394" ParentId="94712" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;If I'm interpreting your question correctly, here's my response:&lt;/p&gt;&#10;&#10;&lt;p&gt;The overlap doesn't really tell you much about the &quot;similarities&quot; or &quot;differences&quot; of the PDFs - they're either the same, or different as Glen_b says. The PDFs are really just giving you information about the associated variable in question. If the area of overlap is .05, then that means there's a 5% chance that variable A will fall in that range and a 5% chance that variable B will fall in that range. Assuming the two variables are independent, there would be a .25% chance that they would both fall in that range simultaneously.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you're interested in comparing the two variables, then you should be able to use the PDFs to generate means and variances for your two samples and use an appropriate statistical test to compare them.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-05-29T22:41:27.043" Id="100551" LastActivityDate="2014-05-29T22:41:27.043" OwnerUserId="45561" ParentId="100478" PostTypeId="2" Score="0" />
  
  <row AnswerCount="0" Body="&lt;p&gt;My data set consist of a large set of events. Each event occurs at an arbitrary location and lasts for an arbitrary but limited amount of time.&#10;Since I have no experience with spatio-temporal modeling, can you give my a starting point on how to build a model for such data which allows me to predict where events are likely to occur in the future and how long they will likely last? Is this possible after all?&lt;/p&gt;&#10;" ClosedDate="2014-05-30T14:44:28.647" CommentCount="1" CreationDate="2014-05-30T07:13:13.297" FavoriteCount="1" Id="100573" LastActivityDate="2014-05-30T07:13:13.297" OwnerUserId="17025" PostTypeId="1" Score="1" Tags="&lt;predictive-models&gt;&lt;spatio-temporal&gt;" Title="How to pedict spatio-temporal events?" ViewCount="33" />
  <row AnswerCount="1" Body="&lt;p&gt;I have a large dataset from which I want to perform a bayesian probit regression using Gibbs sampling &lt;a href=&quot;http://www.jstor.org/discover/10.2307/2290350?uid=3738016&amp;amp;uid=2&amp;amp;uid=4&amp;amp;sid=21104089937577&quot; rel=&quot;nofollow&quot;&gt;1&lt;/a&gt;. Since the dataset has one milion rows, and variables from a truncated normal must be sampled [2], the whole process is too slow. I would consider variational or parallel approaches, but before going that far I would like to know whether I could randomly sample (with replacement) from my dataset at every Gibbs iteration, so that I have less instances to learn from at every step.&lt;/p&gt;&#10;&#10;&lt;p&gt;My intuition is that even if I change the samples, I would not be changing the probability density and therefore the Gibbs sample should not notice the trick. Am I right? Are there some references of people having done this?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.jstor.org/discover/10.2307/2290350?uid=3738016&amp;amp;uid=2&amp;amp;uid=4&amp;amp;sid=21104089937577&quot; rel=&quot;nofollow&quot;&gt;1&lt;/a&gt; Albert and Chib 1993. &lt;a href=&quot;http://www.jstor.org/discover/10.2307/2290350?uid=3738016&amp;amp;uid=2&amp;amp;uid=4&amp;amp;sid=21104089937577&quot; rel=&quot;nofollow&quot;&gt;Bayesian Analysis of Binary and Polychotomous Response Data&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;[2] I use python and the function scipy.stats.truncnorm&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-05-30T10:55:52.113" Id="100591" LastActivityDate="2015-02-26T03:01:30.263" OwnerUserId="36312" PostTypeId="1" Score="0" Tags="&lt;sampling&gt;&lt;bootstrap&gt;&lt;mcmc&gt;&lt;large-data&gt;&lt;gibbs&gt;" Title="Can I subsample a large dataset at every MCMC iteration?" ViewCount="145" />
  
  
  <row Body="&lt;p&gt;The &lt;strong&gt;&lt;em&gt;t&lt;/em&gt;&lt;/strong&gt;-distribution and &lt;strong&gt;&lt;em&gt;F&lt;/em&gt;&lt;/strong&gt;-distribution are different because of the two degrees of freedom (&lt;strong&gt;&lt;em&gt;dof&lt;/em&gt;&lt;/strong&gt;) values that go into the latter, as opposed to one &lt;strong&gt;&lt;em&gt;dof&lt;/em&gt;&lt;/strong&gt; value for the former.  The PDF quickly degenerates for high &lt;strong&gt;&lt;em&gt;F&lt;/em&gt;&lt;/strong&gt; values because of the low probability one would see such a large &lt;strong&gt;&lt;em&gt;F&lt;/em&gt;&lt;/strong&gt; value jointly across one of the &lt;strong&gt;&lt;em&gt;dof&lt;/em&gt;&lt;/strong&gt; (the one usually measuring the number of independent variables minus one).  For the &lt;strong&gt;&lt;em&gt;t&lt;/em&gt;&lt;/strong&gt; distribution, one doesn't need to consider this second &lt;strong&gt;&lt;em&gt;dof&lt;/em&gt;&lt;/strong&gt; factor.  Some lecture slides covering this are on: &lt;a href=&quot;http://statisticalideas.blogspot.com/p/test.html&quot; rel=&quot;nofollow&quot;&gt;Statistical Ideas&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-30T11:59:33.417" Id="100598" LastActivityDate="2014-05-30T11:59:33.417" OwnerUserId="46351" ParentId="100594" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;One answer lies in &lt;a href=&quot;http://boulder.research.yale.edu/Boulder-2010/ReadingMaterial-2010/Troyer/Article.pdf&quot; rel=&quot;nofollow&quot;&gt;this article&lt;/a&gt;, specifically, Eq. (17), (18) and section D where a method is proposed to compute $\tau_A$ in Eq. (18).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-05-30T12:43:03.627" Id="100601" LastActivityDate="2014-05-30T12:43:03.627" OwnerUserId="12100" ParentId="68366" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="100615" AnswerCount="2" Body="&lt;p&gt;I have a classification dataset where roughly 20% (maybe more) of the labels are incorrect. There is no way to know which labels are incorrect and no way to eliminate them in the future when further data is collected. &lt;/p&gt;&#10;&#10;&lt;p&gt;A method I see in dealing with this is to train an ensemble of classifiers and then take only the training data which matches the ensemble majority vote. &lt;/p&gt;&#10;&#10;&lt;p&gt;Are there any other algorithms/methods that are more resilient to data that is not labeled 100% correctly? Can we even treat this data as supervised learning? Is there anyway to trust the trained model or performance metrics such as accuracy and F1 score?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you for the help.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-30T13:21:39.227" FavoriteCount="0" Id="100606" LastActivityDate="2014-05-30T14:50:21.553" LastEditDate="2014-05-30T13:46:06.713" LastEditorUserId="39953" OwnerUserId="39953" PostTypeId="1" Score="2" Tags="&lt;classification&gt;&lt;algorithms&gt;" Title="Classification with mislabeled data" ViewCount="86" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I want to do a discriminant analysis for my study. This study consist of one dependent variable and 8 independent variables. The dependent variable is categorical and has 2 groups. The independent variable are in the form of likert scale. Each independent variable has many items in it in the form of a likert scale. How do we combine all these items to form a independent variable. For example: One independent variable overconfidence has many items and these items are in the form of 5 point likert scale. How can we combine all these items to form the factor overconfidence. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-05-30T15:22:03.970" Id="100618" LastActivityDate="2014-05-30T15:53:23.773" LastEditDate="2014-05-30T15:53:23.773" LastEditorUserId="37613" OwnerUserId="46448" PostTypeId="1" Score="0" Tags="&lt;likert&gt;&lt;discriminant-analysis&gt;" Title="Discriminant analysis" ViewCount="91" />
  <row Body="&lt;p&gt;Use time series analysis.&lt;/p&gt;&#10;&#10;&lt;p&gt;In time series data, a large number of dimensions is common, and usually not something to worry about, if you use appropriate methods. Because the intrinsic dimensionality of the data is much lower.&lt;/p&gt;&#10;&#10;&lt;p&gt;I.e. don't use Euclidean distance. Don't use low-dimensional methods such as k-means. Use something that &lt;em&gt;understands time&lt;/em&gt;.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-05-30T15:51:24.927" Id="100623" LastActivityDate="2014-05-30T15:51:24.927" OwnerUserId="7828" ParentId="100334" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="100629" AnswerCount="1" Body="&lt;p&gt;I would like to ask the difference between the normal distribution and the multinomial distribution because I don't know when to use each of them.&#10;I know the normal distribution is used for continuous probability, and the multinomial distribution is used for probabilities of &lt;em&gt;K&lt;/em&gt; kinds of categories.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can anyone give me some examples of each to make me understand them more clearly?&#10;Thanks.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-05-30T16:09:58.943" Id="100626" LastActivityDate="2014-05-30T16:35:18.043" LastEditDate="2014-05-30T16:30:32.060" LastEditorUserId="32036" OwnerUserId="46447" PostTypeId="1" Score="0" Tags="&lt;normal-distribution&gt;&lt;multinomial&gt;" Title="Difference between normal distribution and multinomial distribution" ViewCount="50" />
  <row Body="&lt;p&gt;The &lt;a href=&quot;http://en.wikipedia.org/wiki/Probability_distribution#Common_probability_distributions&quot; rel=&quot;nofollow&quot;&gt;Wikipedia page on Distributions&lt;/a&gt; has a list of many common distributions along with short descriptions and links to more details for each of the listed distributions.  Perusing that list may help.  There are a lot more distributions than just the 2 that you mentioned.&lt;/p&gt;&#10;&#10;&lt;p&gt;The multinomial is used when you have a finite number (usually small) of classes/groups where ordering does not matter.  It can be used to describe things like favorite color on a survey, or the type of car that passes a given point next.&lt;/p&gt;&#10;&#10;&lt;p&gt;The normal distribution is used for continuous data, values that theoretically can take on an infinite number of values if we measured precisely enough (though in practice we will round to a finite subset).  Things that result as a combination of many small effects tend to work well with the normal distribution, for example heights in a homogeneous population are determined by several genetic and environmental factors and tend to be reasonably close to normally distributed.  Sample means are also a commonly treated as normally distributed (or at least close enough).  &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-05-30T16:35:18.043" Id="100629" LastActivityDate="2014-05-30T16:35:18.043" OwnerUserId="4505" ParentId="100626" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Since the output of Holt.Winters forecast isn't a table, you need to create two separate &lt;code&gt;write.table()&lt;/code&gt; statements and append them.  The first being a matrix of &lt;code&gt;col.names&lt;/code&gt; and the second being your forecast(s).&lt;/p&gt;&#10;&#10;&lt;p&gt;Try something like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;colnames &amp;lt;- c(&quot;CustomerID&quot;, &quot;Month&quot;, &quot;Point Forecast&quot;, &quot;Lo.80&quot;,&quot;Hi.80&quot;,&quot;Lo.95&quot;,&quot;Hi.95&quot;)&#10;titles   &amp;lt;- matrix(colnames, nrow=1, ncol=7)&#10;write.table(titles,    &quot;\pathname.csv&quot;, sep=&quot;,&quot;, row.names=FALSE, col.names=FALSE)&#10;write.table(forecasts, &quot;\pathname.csv&quot;, sep=&quot;,&quot;, append=TRUE,     col.names=FALSE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2014-05-30T17:27:22.147" Id="100634" LastActivityDate="2014-05-30T17:55:23.983" LastEditDate="2014-05-30T17:55:23.983" LastEditorUserId="7290" OwnerUserId="46453" ParentId="76466" PostTypeId="2" Score="1" />
  
  
  
  <row Body="&lt;p&gt;First, go &lt;a href=&quot;http://stats.stackexchange.com/questions/2492/is-normality-testing-essentially-useless&quot;&gt;here&lt;/a&gt; and read the whole exchange.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then, consider that in orthodox hypothesis testing, we never &lt;em&gt;accept&lt;/em&gt; a hypothesis - such as the hypothesis of equal variances - we only &lt;em&gt;fail to reject&lt;/em&gt; such a hypothesis. Your situation is actually a good example for why this is so. Remember that in the single case, the &lt;em&gt;p&lt;/em&gt; value is best seen a gradual measure of the probability of the data conditional on H0 being true.&lt;/p&gt;&#10;&#10;&lt;p&gt;In this specific situation, the &lt;em&gt;p&lt;/em&gt; value tells you that if drawing samples from two groups with the same parameter, only in about 8% of cases would the difference in the parameter be more extreme than in your situation. In other words, the value in question is quite different in this sample; however, your sample is too small to justify the confident rejection of the hypothesis that they were still drawn from populations where the parameter is identical.&#10;In other words, had your sample been only a little larger, a difference this big would have justified a rejection of the null hypothesis. As they say, surely God loves the &lt;em&gt;p&lt;/em&gt;=0.049 as much as the 0.051; and &lt;em&gt;p&lt;/em&gt; = 0.08 is hardly evidence in favour of H0.&lt;/p&gt;&#10;&#10;&lt;p&gt;So while your test does not justify the rejection of H0 at 95% confidence (or 5% alpha rate), it is far from good positive evidence &lt;em&gt;for&lt;/em&gt;  it. In fact, &lt;a href=&quot;http://www.tandfonline.com/doi/abs/10.1080/00031305.1990.10475752&quot; rel=&quot;nofollow&quot;&gt;the F test is known to be not especially good at detecting that which the &lt;em&gt;t&lt;/em&gt; test is vulnerable to in the first place! On the other hand, the &lt;em&gt;t&lt;/em&gt; test is known to be highly robust to deviations from equal variances&lt;/a&gt;.&#10;However, this holds only while sample sizes are equal. In your case, they are &lt;em&gt;highly&lt;/em&gt; unequal, so at a &lt;em&gt;p&lt;/em&gt; value of .08, indicating little confidence in the H0 (regardless of any arbitrary alpha level), I would be somewhat concerned.&lt;/p&gt;&#10;&#10;&lt;p&gt;So if you want to make sure, you have two things ahead of you. First, visualise the sample distributions by inspecting QQplots, histograms and/or similar methods. Then, perform a test more robust to differences in variances, such as &lt;a href=&quot;http://en.wikipedia.org/wiki/Welch%27s_t-test&quot; rel=&quot;nofollow&quot;&gt;Welch's &lt;em&gt;t&lt;/em&gt; test&lt;/a&gt;, and see if its result is in disagreement with the &lt;em&gt;t&lt;/em&gt; test. If the samples appear reasonably similar, and the two tests deliver similar results, you're good to go. If not, well, you already got the robust test calculated, haven't you?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-30T22:54:20.253" Id="100658" LastActivityDate="2014-05-30T22:54:20.253" OwnerUserId="28288" ParentId="100652" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="100736" AnswerCount="1" Body="&lt;p&gt;In ordinary least squares regression (OLS), if the plot of the residuals against the fitted values form a horizontal line around 0, then we can say that the dependent variable is linearly related to the independent variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;I had thought that this is true because $E(y_i - \hat{y}_I)=0$ when the dependent variable is linearly related to the independent variable, see &lt;a href=&quot;http://stats.stackexchange.com/questions/100653/how-to-do-ordinary-least-squares-ols-when-the-observations-are-not-linear&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, suppose:&lt;/p&gt;&#10;&#10;&lt;p&gt;$y_i = \alpha + \sin(x_i) + \epsilon_i$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then $E(y_i - \hat{y}_i)$ is still 0, see &lt;a href=&quot;http://stats.stackexchange.com/questions/100653/how-to-do-ordinary-least-squares-ols-when-the-observations-are-not-linear&quot;&gt;here&lt;/a&gt; but then the plot of its residuals against its fitted value is no longer a horizontal line around 0, as this R code shows:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;n &amp;lt;- 10^3&#10;df &amp;lt;- data.frame(x=runif(n, 1, 10))&#10;df$mean.y.given.x &amp;lt;- sin(df$x)&#10;df$y &amp;lt;- df$mean.y.given.x + rnorm(n)&#10;model &amp;lt;- lm(y ~ x, data=df)&#10;plot(predict(model, newdata=df), residuals(model))&#10;abline(a=0,b=0,col='blue')&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/kyBwt.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;So my question is, which assumption(s) of OLS that causes the plot of the residuals and the fitted value to be a horizontal line around 0 and why/how is it true? &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-05-31T10:43:18.857" Id="100682" LastActivityDate="2014-09-17T00:26:16.523" LastEditDate="2014-09-17T00:26:16.523" LastEditorUserId="805" OwnerUserId="31661" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;regression&gt;&lt;linear-model&gt;&lt;least-squares&gt;" Title="Why is the plot of residuals against fitted values a horinzontal line when the dependent variable is linearly related to the indenpendent variable?" ViewCount="213" />
  <row Body="&lt;p&gt;Start with this:&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{eqnarray*}&#10;SST &amp;amp; = &amp;amp; \sum_{i=1}^{n}(y_{i}-\bar{y})^{2}\\&#10; &amp;amp; = &amp;amp; \sum_{i=1}^{n}((y_{i}-\hat{y}_{i})+(\hat{y}_{i}-\bar{y}))^{2}\\&#10; &amp;amp; = &amp;amp; \underset{SSE}{\underbrace{\sum_{i=1}^{n}(y_{i}-\hat{y}_{i})^{2}}}+\underset{SSR}{\underbrace{\sum_{i=1}^{n}(\hat{y}_{i}-\bar{y})^{2}}}+2\sum_{i=1}^{n}(y_{i}-\bar{y}_{i})(\hat{y}_{i}-\bar{y}).&#10;\end{eqnarray*}&#10;It can be shown that $2\sum_{i=1}^{n}(y_{i}-\bar{y}_{i})(\hat{y}_{i}-\bar{y})=0.$&lt;/p&gt;&#10;&#10;&lt;p&gt;Lets think about how to do that last step. First note that $\hat{y}_{i}=\hat{\beta}_{0}+\hat{\beta}_{1}x_{i}=\bar{y}+\hat{\beta}_{1}(x_{i}-\bar{x})$&#10;and so $(\hat{y}_{i}-\bar{y})=\hat{\beta}_{1}(x_{i}-\bar{x})$. Furthermore,&#10;we can write $(y_{i}-\hat{y}_{i})=(y_{i}-\bar{y})-\hat{\beta}_{1}(x_{i}-\bar{x}).$&#10;Then&#10;\begin{eqnarray*}&#10;\sum_{i=1}^{n}(y_{i}-\bar{y}_{i})(\hat{y}_{i}-\bar{y}) &amp;amp; = &amp;amp; \hat{\beta}_{1}\sum_{i=1}^{n}(x_{i}-\bar{x})(y_{i}-\bar{y})-\hat{\beta}_{1}^{2}\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}\\&#10; &amp;amp; = &amp;amp; \left(\dfrac{Q_{xy}}{Q_{xx}}\right)Q_{xy}-\left(\dfrac{Q_{xy}}{Q_{xx}}\right)^{2}Q_{xx}\\&#10; &amp;amp; = &amp;amp; 0&#10;\end{eqnarray*}&#10; because $\hat{\beta}_{1}=Q_{xy}/Q_{xx}.$&lt;/p&gt;&#10;&#10;&lt;p&gt;Also I used the standard sum of square notation, e.g. $Q_{xx}=\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}$&#10;etc&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-05-31T12:47:02.303" Id="100693" LastActivityDate="2014-05-31T12:47:02.303" OwnerUserId="30494" ParentId="100686" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;I'm not a mathematician, nor do I use statistics.&lt;/p&gt;&#10;&#10;&lt;p&gt;As a scientist, though, any conclusions you can draw from the data will be far more robust if you use all five methods of instruction on all five groups of students during different lessons.&lt;/p&gt;&#10;&#10;&lt;p&gt;1) This will be more equitable to all the students if it turns out one method is particularly bad or good.&lt;/p&gt;&#10;&#10;&lt;p&gt;2) You will then have exactly one basic group, with the same general deviation, and can then run the appropriate statistical technique and draw more accurate conclusions as to cause and effect.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any teaching method that is beneficial to one student is almost guaranteed to be neutral or harmful to another student relative to that student's preferred methods (i.e. individual learning, mentor, group approaches).  Regardless of &quot;learning style&quot;.  So please try to factor this in to your analysis.  Schooling is the only form of psychological experimentation in which the experimenters don't need informed consent from the subjects.  This alone carries with it a profound ethical obligation.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-31T16:58:58.970" Id="100707" LastActivityDate="2014-05-31T16:58:58.970" OwnerUserId="46486" ParentId="99283" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;Since the question is ill-defined, we should guess about the real situation. My five cents are the follows. &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;First of all, the arrangement of data is irrelevant. However, when using cross-validation methods that imply elements of randomness (k-fold CV for example), then each run will produce different, although close results. This is the first possible reason. To check if this is the case, you may carry out leave-one-out CV the same way as you have done your method of CV. If leave-one-out CV produce the same results after shuffling, then you have trapped into the described issue, but all is OK - that is a feature of stochastic methods of CV. If LOO-CV results also differ, then check point 2:&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The second possible reason is that when you are permuting training data, you change only &lt;em&gt;X&lt;/em&gt; matrix, but the &lt;em&gt;y&lt;/em&gt; vector remains the same. That is an error. The correspondence between rows in &lt;em&gt;X&lt;/em&gt; and elements in &lt;em&gt;y&lt;/em&gt; should not be violated. Shuffle them together as united matrix and then split back, or save shuffling indices and permute &lt;em&gt;y&lt;/em&gt; according to this indices. &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="2" CreationDate="2014-05-31T17:27:10.947" Id="100708" LastActivityDate="2014-05-31T17:27:10.947" OwnerUserId="8165" ParentId="100688" PostTypeId="2" Score="0" />
  <row AnswerCount="2" Body="&lt;p&gt;I'm not well-versed in statistics at all. I'm currently working on an implementing an optimization algorithm, and am having a bit of trouble on this part.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have an array of $n$ elements $A = \left\{a_{i}\right\}$ where $a_{i}\in\mathbb{R}$ and $1 \le i \le n$. Furthermore, I have an array of probabilities given by $P = \left\{p_{i}\right\}$ with $p_{i}\in[0,1]$. (Also, $\sum_{i} p_{i} = 1$ of course) If it matters, the probability distribution is triangular with $i=1$ having the highest probability and $i=n$ having the smallest probability.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now I want to choose $q &amp;lt; n$ distinct elements from $A$ using the probabilities of selection from $P$. I'm currently coding the algorithm in MATLAB, but I'm not sure how to go about choosing the $q$ elements.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-05-31T17:58:18.650" Id="100709" LastActivityDate="2015-02-26T00:01:29.627" LastEditDate="2014-05-31T18:44:08.410" LastEditorUserId="46487" OwnerUserId="46487" PostTypeId="1" Score="2" Tags="&lt;probability&gt;&lt;distributions&gt;" Title="Choosing elements of an array based on given probabilities for each element" ViewCount="100" />
  
  <row Body="&lt;p&gt;The outputs are different for a variety of reasons, not the least of which is that they test different things in different ways. Further, not every regression and ANOVA output tests the same thing. It looks like those are R outputs so I'll focus on them in particular rather than generalize your question much.&lt;/p&gt;&#10;&#10;&lt;p&gt;Your regression output is the coefficients of tests of each of the values against a specific value ConnectivityLOW:SusceptibilityLOW (CLOW:SLOW). It is the unique contribution of each item, removing the variance accounted for by other items simultaneously. Keep in mind that because an interaction is included you're not seeing the overall, or average, main effects but only their effect when the other value is 0. These may (looks like would) be different values if the interaction was removed from the model and you examined the main effects in isolation but this usually isn't much of an issue with categorical predictors. The coefficient for the interaction isn't as obvious because it's the test between of (CHIGH:SLOW - CLOW:SLOW) - (CHIGH:SHIGH - CLOW-SLOW). With a simple 2x2 design it's relatively easy to generate one number to represent the interaction but there isn't one for more complex interactions and you cannot trust the output of the regression coefficients for more complex situations, whether significant or not, because they test simpler effects than the full interaction. So, in this case you could use the interaction coefficient.&lt;/p&gt;&#10;&#10;&lt;p&gt;The ANOVA tells you variance accounted for by each variable with each one added in sequence to the model. If there were no correlations among your predictor variables this would have p-values very similar to (identical to) your regression coefficients for main effects. Your effect of C is an independent effect of C not accounting for S. The effect of S is the effect of S with C already entered into the model and therefore has more in common with the coefficient for S in the table of regression coefficients. The interaction is a test of the explanatory power of the subtraction score mentioned above and will have a very similar outcome to that direct test. But more importantly, it will be a test of the complete interaction in more complex designs rather than show tests of simpler components of the interaction. It's your arbiter of whether the interaction is significant.&lt;/p&gt;&#10;&#10;&lt;p&gt;That's the answer for your example but you really need to study regression a bit to better understand this for situations with with continuous variables as well and more complex designs.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-31T21:15:31.530" Id="100720" LastActivityDate="2014-06-01T02:52:50.577" LastEditDate="2014-06-01T02:52:50.577" LastEditorUserId="601" OwnerUserId="601" ParentId="100670" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;On page 3 the statistical approach is spelled out. &lt;/p&gt;&#10;&#10;&lt;p&gt;They do a Poisson auto-regression of goals scored for each team with 6 explanatory variables.  The autoregressive portion only goes back 10 games for the team in question and 5 for the opposing team. Perhaps you are not familiar with autoregression.&lt;/p&gt;&#10;&#10;&lt;p&gt;Autoregression means that a regression at $t$ is performed using previous values (in this case goals) going back some number of timesteps $p$. A simple linear autoregression would look like this. &lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;X_t=c + \sum^p_{i=1}\beta X_{t-i} + \epsilon_t&#10;$$ &lt;/p&gt;&#10;&#10;&lt;p&gt;What is commonly referred to as Poisson regression is often linear so in that case the $X_t$ would be replaced with $log(X_t)$. The Poisson regression used isn't stated however and may be non-linear. Regardless the principle of autoregression would be true no matter what model they used; the values from previous timesteps are used as predictive variables for the current timestep.&lt;/p&gt;&#10;&#10;&lt;p&gt;For this model $p=10$ for the team of interest and $p=5$ for the opposing team. The fact that the data goes back to 1960 only means that there is some $k$ for which the $k-10$th game happened in 1960 and in addition there is a $k+10$th game that is not in 1960 (probably 1961). &lt;/p&gt;&#10;&#10;&lt;p&gt;The model is not claiming that an event that happened 54 years ago will affect the outcome of the World Cup this year but rather they are offering the premise that there is a pattern that was true in 1960 that involved the 10 previous games at most and that this pattern is still true today.&lt;/p&gt;&#10;&#10;&lt;p&gt;Whether or not this premise is fishy is perhaps a more sophisticated question. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-31T21:18:37.973" Id="100721" LastActivityDate="2014-05-31T21:39:31.360" LastEditDate="2014-05-31T21:39:31.360" LastEditorUserId="13950" OwnerUserId="13950" ParentId="100715" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I understand that the $x's$ are also random, in which case what follows is to be understood &quot;conditional on the $x$'s&quot;. &lt;/p&gt;&#10;&#10;&lt;p&gt;There are conceivably &lt;em&gt;four&lt;/em&gt; &quot;cases&quot; here ($f_{\gamma}(\gamma)$ is the probability density function of $\gamma$),&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \begin{align} &#10;&amp;amp; \prod_{i=1}^{n}P_i\left(\lambda[x,E(\gamma)]\right) = \prod_{i=1}^{n}P_i\left[\lambda\left(x,\int\gamma f_{\gamma}(\gamma)\mathrm{d}\gamma\right)\right] &amp;amp;\qquad [1]\\ &#10;&amp;amp; \prod_{i=1}^{n}P_i(E[\lambda(x,\gamma)]) = \prod_{i=1}^{n}P_i\left[\int f_{\gamma}(\gamma)\lambda(x,\gamma)\mathrm{d}\gamma\right] &amp;amp;\qquad [2]\\&#10;&amp;amp; \prod_{i=1}^{n}E\left(P_i[\lambda(x,\gamma)]\right)=\prod_{i=1}^{n}\left(\int f_{\gamma}(\gamma)P_i[\lambda(x,\gamma)]\mathrm{d}\gamma\right) &amp;amp;\qquad [3] \\&#10;&amp;amp; E\left[\prod_{i=1}^{n}P_i[\lambda(x,\gamma)]\right] = \int f_{\gamma}(\gamma)\left(\prod_{i=1}^{n}P_i[\lambda(x,\gamma)]\right)\mathrm{d}\gamma &amp;amp;\qquad [4]&#10;\end{align}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Since the above are conditional on the $x'$s, the expected value is taken with respect to $\gamma$ only.&lt;/p&gt;&#10;&#10;&lt;p&gt;Moving from $[1]$ to $[4]$, what we are doing is gradually &lt;em&gt;increasing the extent to which we are allowing the uncertainty in $\gamma$ to interact with the structure&lt;/em&gt;.  &lt;/p&gt;&#10;&#10;&lt;p&gt;In $[1]$, we &quot;sweep away&quot; this uncertainty by using, instead of $\gamma$, a centrality measure for it, $E(\gamma)$, and then calculate all consecutive interactions (trough the function $\lambda ()$, through the $P_i$'s, and then through $\prod$), using just this centrality measure.  &lt;/p&gt;&#10;&#10;&lt;p&gt;In $[2]$, we allow the uncertainty in $\gamma$ to interact with the function $\lambda()$, by permitting $\lambda ()$ to take the various values it takes for each possible value of $\gamma$, and &lt;em&gt;then&lt;/em&gt; averaging. But after that, the uncertainty stops affecting the expression, since it has been averaged out.&lt;/p&gt;&#10;&#10;&lt;p&gt;In $[3]$, we allow the uncertainty in $\gamma$ to interact directly with the composite function  $P_i\circ \lambda$: for each possible value of $\gamma$ we calculate the magnitude $P_i[x,\lambda(\gamma)]$, and then averaging over it.  &lt;/p&gt;&#10;&#10;&lt;p&gt;In other words, each expression is affected by a different random quantity.&lt;/p&gt;&#10;&#10;&lt;p&gt;Depending on the actual functional forms employed, these three expressions in general give different results. Only if the &lt;em&gt;affine&lt;/em&gt; property is preserved, they are equivalent:&#10;If, say, $\lambda(x,\gamma) = a_1 + h(x)\gamma$ and $P_i = a_2+b_1\lambda$, then , conditional on the $x$'s, the three expressions will give identical results because of the linearity property of the expected value operator.  &lt;/p&gt;&#10;&#10;&lt;p&gt;But if non-affine forms are involved the equality breaks. Why? Because with non-linearity, each possible value of $\gamma$ will affect differently &quot;itself&quot; and $\lambda$ ($[1]$ against $[2]$), and also it will affect differently $\lambda$ and $P_i \circ \lambda$ ($[2]$ against $[3]$). If, say, $\lambda(x,\gamma) = x+ \gamma^2$, $\lambda$ is essentially not a function of $\gamma$ but a function &lt;em&gt;of a non-linear function&lt;/em&gt; of $\gamma$. In other words, non-linearity &quot;implies&quot; another function composition. In this example, if nevertheless $P_i$ is an affine function of $\lambda$, then while $[1]$ is not in general equal to $[2]$, still, $[2]$ is equal to $[3]$.    &lt;/p&gt;&#10;&#10;&lt;p&gt;The same reasoning applies to $[4]$: in it, the uncertainty in $\gamma$ is allowed to interact withe the whole product, before averaging it out. You should not be confused with the case where &quot;the $P_i$'s are independent&quot;. The question is &quot;independent with respect to what?&quot; Presumably, with respect to the $x's$. But they are not independent with respect to $\gamma$ -and here our issue is the uncertainty in $\gamma$.  &lt;/p&gt;&#10;&#10;&lt;p&gt;If the $P_i$'s are independent with respect to the $x$'s (and you indicate that this indeed holds), then $[4]$ is the average (always with respect to $\gamma$) joint probability, while $[3]$ is the product of the average marginal probabilities. $[4]$ acknowledges fully the uncertainty in $\gamma$, while $[3]$ gives you the joint probability when the uncertainty in $\gamma$ has collapsed in its expected value.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-05-31T21:39:07.887" Id="100722" LastActivityDate="2014-05-31T21:39:07.887" OwnerUserId="28746" ParentId="100679" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I believe that you can. Your model would imply that the proportion feeding depends on the size of the flock; and furthermore, proportions from large flocks should be trusted more than proportions from smaller flocks (that's the impact of the weight parameter). &lt;/p&gt;&#10;&#10;&lt;p&gt;Another option would be to use a beta-binomial model. See the betareg package. It allows for clusters (flocks in this case) of different sizes.&lt;/p&gt;&#10;&#10;&lt;p&gt;My main concern is with the model you have chosen. If flock size affects the proportion feeding, then presumably the birds are affecting each other, so responses are no longer independent. But perhaps your model will fit approximately well.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-05-31T21:41:09.100" Id="100723" LastActivityDate="2014-05-31T21:41:09.100" OwnerUserId="14188" ParentId="100700" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;I think I agree, drag/drop approach for data mining is not want you need. I think you need something similar to Python-like scripting language. If you think about such approach to statistics, you can try &lt;a href=&quot;http://jwork.org/scavis/&quot; rel=&quot;nofollow&quot;&gt;ScaVis program&lt;/a&gt; that use Python for statistics. Another option is SciPy or similar. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-01T01:26:06.970" Id="100734" LastActivityDate="2014-06-01T01:26:06.970" OwnerUserId="46499" ParentId="45292" PostTypeId="2" Score="1" />
  
  
  <row AcceptedAnswerId="100786" AnswerCount="1" Body="&lt;p&gt;What is the PDF of the difference of two i.i.d Laplace distributed random variables?&lt;/p&gt;&#10;&#10;&lt;p&gt;I know that the difference of two i.i.d Normal variables is still the Normal distribution. Since the properties of the Laplace distribution are similar to the Normal distribution, I am guessing that the difference is also the Laplace distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have tried to solve this using Mathematica. Neither &lt;code&gt;PDF&lt;/code&gt; nor &lt;code&gt;Integrate&lt;/code&gt; functions give a correct answer. Thank you!&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-06-01T07:59:41.660" FavoriteCount="1" Id="100746" LastActivityDate="2014-06-01T20:55:42.070" LastEditDate="2014-06-01T09:03:21.060" LastEditorUserId="32036" OwnerUserId="46507" PostTypeId="1" Score="2" Tags="&lt;pdf&gt;&lt;laplace-distribution&gt;" Title="Difference between two i.i.d Laplace distributions?" ViewCount="157" />
  <row AnswerCount="0" Body="&lt;p&gt;In one of the slides of a statistics course that I followed the following about using drop1 or ANCOVA is stated.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Using drop1 the p-values shown are p-values for deleting one variable at a time from the full model, whereas the p-values in the output of anova are sequential, as in a step-up strategy. This problem does not arise in ANOVA or linear regression, only in ANCOVA and mixed models.)&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I looked at it for 20 minutes, and cannot understand that if this is true why someone would still use the command &lt;code&gt;anova()&lt;/code&gt; in R anyway.&lt;/p&gt;&#10;&#10;&lt;p&gt;Does anyone have an idea?&lt;/p&gt;&#10;&#10;&lt;p&gt;The reason why I ask this question here is because the subject was given a few months ago.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-06-01T12:20:37.890" Id="100751" LastActivityDate="2014-06-01T12:20:37.890" OwnerUserId="46509" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;ancova&gt;" Title="R: What is the difference between ANCOVA and drop1?" ViewCount="49" />
  <row AnswerCount="0" Body="&lt;p&gt;A question states: $X$ is the vector of regressors stacked for 30 observations and $Rank(X)=5$. There are no lags of $y_t$ in the set $X_t$. Using the Durbin-Watson statistic, test the null hypothesis of no autocorrelation at a 5% significance level. &lt;/p&gt;&#10;&#10;&lt;p&gt;They then execute the DW test with $k=4$. As far as I know, this is the amount of regressors right? How can it be 4 (and not 5) when the matrix rank of $X$ is 5?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-01T12:46:00.820" Id="100753" LastActivityDate="2014-06-01T12:46:00.820" OwnerUserId="46510" PostTypeId="1" Score="1" Tags="&lt;econometrics&gt;&lt;matrix&gt;" Title="Matrix rank and amount of regressors" ViewCount="15" />
  <row Body="&lt;p&gt;Though I'm not an expert in survival analysis, I put here my suggestions and hope they will be helpful.&lt;/p&gt;&#10;&#10;&lt;p&gt;First of all, selection of variables looking at their p-values is a wrong way, especially when the model is aimed to make statistical inferences. You can read about that in multiple sources searching for &quot;stepwise regression drawbacks&quot;. The selection of variables should be based on your domain-specific knowledge. All  variables which are relevant (on your opinion) should be present, no matter whether their influence is significant or not. In such way you will report the effect of Sit adjusted for the list of used variables, and that is right. It seems that your research is exploratory but not confirmatory. In such case while interpreting the results, you'd better make emphasis on the sizes of effect (model coefficients, odds ratios or risk ratios) rather then on p-values.&lt;/p&gt;&#10;&#10;&lt;p&gt;As for violation of proportionality assumption: taking into consideration the interaction between Sit and time, you are incorporating linear dependence of Sit on time into the model. So if the true relationship between Sit and time is really close to linear, then proportionality assumption will be held. Thus all model diagnostics methods remains relevant.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-06-01T14:49:34.047" Id="100760" LastActivityDate="2014-06-01T14:49:34.047" OwnerUserId="8165" ParentId="99932" PostTypeId="2" Score="0" />
  
  <row AcceptedAnswerId="101171" AnswerCount="1" Body="&lt;p&gt;I am trying to generate a simulated data matrix that is correlated by both observation and variable directions. So far I know how to do this for &lt;strong&gt;variable x variable&lt;/strong&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/NgP8j.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; # correlated matrix between variables &#10;    n = 200&#10;    p = 100 &#10;    CRMt &amp;lt;- matrix(NA, nrow = p, ncol = p)&#10;    diag(CRMt) &amp;lt;- 1&#10;    CRMt[upper.tri (CRMt, diag = FALSE)] &amp;lt;- 0.5&#10;    CRMt[lower.tri (CRMt, diag = FALSE)] &amp;lt;- 0.5&#10;&#10;L = chol(CRMt)# Cholesky decomposition&#10;p = dim(L)[1]&#10;&#10;set.seed(999)&#10;M = t(L) %*% matrix(rnorm(p*n), nrow=p, ncol=n)&#10;M1 &amp;lt;- t(M)&#10;rownames(M1) &amp;lt;- paste(&quot;S&quot;, 1:200, sep = &quot;&quot;) &#10;colnames(M1) &amp;lt;- paste(&quot;M&quot;, 1:100, sep = &quot;&quot;)&#10;cor(M1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now say I want to create a data matrix that also follows the following &lt;strong&gt;observation x observation&lt;/strong&gt; correlation matrix.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/XkDuZ.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;OCRMt &amp;lt;- matrix(NA, nrow = n, ncol = n)&#10;diag(OCRMt) &amp;lt;- 1&#10;OCRMt[upper.tri (OCRMt, diag = FALSE)] &amp;lt;- 0.3&#10;OCRMt[lower.tri (OCRMt, diag = FALSE)] &amp;lt;- 0.3&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;How can I do this ? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-01T16:42:22.120" FavoriteCount="2" Id="100770" LastActivityDate="2014-06-04T18:19:29.583" LastEditDate="2014-06-04T18:19:29.583" LastEditorUserId="88" OwnerUserId="7397" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;correlation&gt;&lt;matrix&gt;" Title="Generating a correlated data matrix where both observations and variables are correlated" ViewCount="241" />
  <row Body="&lt;p&gt;The Weibull distribution (or Weibull probability density function - pdf) with two parameters has the shape parameter ($k$) and the scale parameter ($\lambda$).&lt;/p&gt;&#10;&#10;&lt;p&gt;The shape parameter models the format (shape) of the distribution. If $k$ = 1 the Weibull distribution reduces to an exponential distribution. If $k$ is between 3 and 4, the Weibull pdf is similar to a normal distribution. See the following picture, which simulates a Weibull pdf with a fixed scale parameter ($\lambda$ = 20) and varies the shape parameter ($k$) from 0.5 to 2, 3.5 and 8:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ihxwz.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In the study of Liu et al. (2010), the authors fitted 205,873 Weibull distributions (one for each of the 205,873 URLs investigated) using dwell time data.&lt;br&gt;&#10;The picture 3b is an empirical cumulative density function (eCDF) regarding the shape parameter for all the 205,873 Weibull pdfs obtained. That figure shows that in 98.5% of these 205,873 Weibull distributions, the shape parameter ($k$) was smaller than 1. This means that the dwell time frequency drops exponentially.&lt;br&gt;&#10;This is not my area of expertise, but if understood a little bit about that article, the authors explain this behavior claiming that the first thing a user does in a web page is to scroll it down to see if there is something interesting there. In this phase there is a huge frequency (or probability density) of users abandoning the website. If the webpage survives this phase of screening, than the dwell time behavior will be more smooth (makes sense to me). &lt;/p&gt;&#10;&#10;&lt;p&gt;The Weibull scale parameter ($\lambda$) is the 63.2 percentile of the distribution (McCool, 2012). This means that if, for example, given a Weibull distribution with $\lambda$ = 2, 63.2% of the observed values will be smaller than 2. The following picture shows how the scale parameter varies, holding the shape parameter constant ($k$ = 3.5). &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/2gs03.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In the Liu et al. (2010) picture 3a, they show that 80% of the 205,873 fitted Weibull pdfs, had the scale parameter smaller than 70 seconds. This means that in those 80% URLs the dwell time value was smaller than 70 seconds in 63.2% of all observations.&lt;/p&gt;&#10;&#10;&lt;p&gt;I hope this can help you start understanding the Weibull pdf parameters.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;McCool, J.I. Using the Weibull Distribution: Reliability, Modeling and Inference. Hoboken: John Wiley., 2012. 368p.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Liu, C.; White, R. W.; Dumais, S. Understanding Web Browsing Behaviors through&#10;Weibull Analysis of Dwell Time. SIGIR10. Geneva, Switzerland. 2010.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Code in R, to reproduce pictures in this answer.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;require(MASS)&#10;&#10;# Weibull shape&#10;windows()&#10;i=0.5&#10;set.seed(29)&#10;x=rweibull(1000,shape=i,scale=20)&#10;weib = fitdistr(na.omit(x),densfun=dweibull,start=list(scale=1,shape=1))&#10;plot(curve(dweibull(x,shape=weib$estimate[2],scale=weib$estimate[1]),from=0.1,to=40),ylim=c(0,0.15),type=&quot;l&quot;,lty=4, xlab=&quot;dwell time (t)&quot;,ylab = &quot;density&quot;, cex.lab=1.35, cex.axis=1.25)&#10;rm(x,weib,i)&#10;&#10;for (i in c(2,3.5,8)){&#10;x=rweibull(1000,shape=i,scale=20)&#10;weib = fitdistr(na.omit(x),densfun=dweibull,start=list(scale=1,shape=1))&#10;curve(dweibull(x,shape=weib$estimate[2],scale=weib$estimate[1]),from=0.1,to=40,lty=i, add=T)&#10;rm(i,x,weib)&#10;}&#10;&#10;text(4,0.1,expression(paste(&quot;k = 0.5&quot;)),cex=1.15)&#10;text(1.85,0.02,expression(paste(&quot;k = 2&quot;)),cex=1.15)&#10;text(18.95,0.055,expression(paste(&quot;k = 3.5&quot;)),cex=1.15)&#10;text(19.6,0.115,expression(paste(&quot;k = 8&quot;)),cex=1.15)&#10;&#10;dev.off()&#10;&#10;# Weibull scale&#10;windows()&#10;i=5&#10;set.seed(29)&#10;x=rweibull(1000,shape=3.5,scale=i)&#10;weib = fitdistr(na.omit(x),densfun=dweibull,start=list(scale=1,shape=1))&#10;plot(curve(dweibull(x,shape=weib$estimate[2],scale=weib$estimate[1]),from=0.1,to=40),ylim=c(0,0.3),type=&quot;l&quot;,lty=4, xlab=&quot;dwell time (t)&quot;,ylab = &quot;density&quot;, cex.lab=1.35, cex.axis=1.25)&#10;rm(x,weib,i)&#10;&#10;for (i in c(10,15,20)){&#10;x=rweibull(1000,shape=3.5,scale=i)&#10;weib = fitdistr(na.omit(x),densfun=dweibull,start=list(scale=1,shape=1))&#10;curve(dweibull(x,shape=weib$estimate[2],scale=weib$estimate[1]),from=0.1,to=40,lty=i, add=T)&#10;rm(i,x,weib)&#10;}&#10;&#10;text(4.75,0.27,expression(paste(lambda,&quot; = 5&quot;)),cex=1.15)&#10;text(9.62,0.15,expression(paste(lambda,&quot; = 10&quot;)),cex=1.15)&#10;text(14.65,0.1,expression(paste(lambda,&quot; = 15&quot;)),cex=1.15)&#10;text(19.65,0.079,expression(paste(lambda,&quot; = 20&quot;)),cex=1.15)&#10;&#10;dev.off()&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="5" CreationDate="2014-06-01T17:56:04.217" Id="100777" LastActivityDate="2014-06-02T17:48:58.827" LastEditDate="2014-06-02T17:48:58.827" LastEditorUserId="22468" OwnerUserId="22468" ParentId="100752" PostTypeId="2" Score="6" />
  <row AnswerCount="2" Body="&lt;p&gt;I was hoping to get some help. In understand how to compute an exact numerical solution (&lt;a href=&quot;http://www.cs.berkeley.edu/~jordan/courses/260-spring10/lectures/lecture5.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.cs.berkeley.edu/~jordan/courses/260-spring10/lectures/lecture5.pdf&lt;/a&gt;) for the following Bayesian model:&#10;$$ \tau \sim Ga(\alpha, \beta)$$&#10;$$\mu \sim N(m ,p)$$&#10;$$Y_i \sim N(\mu, \tau)$$&#10;where the data is $Y_i$.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am performing a meta-analysis where my data are $Y_i$ from each study, and $\tau_i$ from each study. The problem is described by a hierarchical model:&#10;$$ \tau \sim Ga(\alpha, \beta)$$&#10;$$\mu \sim N(m ,p)$$&#10;$$\theta_i \sim N(\mu, \tau)$$&#10;$$Y_i \sim N(\theta_i, \tau_i)$$&#10;Is there any way to solve this model analytically? Any help/suggestions would be greatly appreciated.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-06-01T18:17:56.607" Id="100780" LastActivityDate="2014-06-02T19:29:09.483" OwnerUserId="18098" PostTypeId="1" Score="1" Tags="&lt;bayesian&gt;&lt;hierarchical-bayesian&gt;&lt;conjugate-prior&gt;" Title="Bayesian Hierarchical Model - Exact Conjugate Solution?" ViewCount="124" />
  <row Body="&lt;p&gt;A toolbox can be a cage.  If you are doing something new it can be instructive to write it without the toolbox the first time.&lt;/p&gt;&#10;&#10;&lt;p&gt;The points in the 512 (or whatever) dimensional space might be able to be mapped to any subset of it.  Without knowing something about the data, it could throw away all but 1/512th of the information to map to a 2d space.&lt;/p&gt;&#10;&#10;&lt;p&gt;So not all spaces are created equal.  Some are planes oriented with the axes (stunningly convenient but equally rare) but others are surfaces of spheres, or follow differential equations.  Some may have projections to 2d surfaces for which we currently have no formalism, but future developments open them up to approach.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you do not have a clean way to do the projection, you are going to have to throw away information.&lt;/p&gt;&#10;&#10;&lt;p&gt;The quick way, as @ttnphns suggested, to determine if you have plane-ish clouds of data, and throw away the non-informative dimensions perpendicular to those planes, is PCA (principle component analysis).&lt;/p&gt;&#10;&#10;&lt;p&gt;Here are some links:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://www.mathworks.com/help/stats/pca.html&quot; rel=&quot;nofollow&quot;&gt;http://www.mathworks.com/help/stats/pca.html&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://www.mathworks.com/help/stats/princomp.html&quot; rel=&quot;nofollow&quot;&gt;http://www.mathworks.com/help/stats/princomp.html&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://www.cs.princeton.edu/picasso/mats/PCA-Tutorial-Intuition_jp.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.cs.princeton.edu/picasso/mats/PCA-Tutorial-Intuition_jp.pdf&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://web.ipac.caltech.edu/staff/fmasci/home/statistics_refs/PrincipalComponentAnalysis.pdf&quot; rel=&quot;nofollow&quot;&gt;http://web.ipac.caltech.edu/staff/fmasci/home/statistics_refs/PrincipalComponentAnalysis.pdf&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Best of luck.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-01T19:09:45.637" Id="100782" LastActivityDate="2014-06-01T19:09:45.637" OwnerUserId="22452" ParentId="51009" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="100804" AnswerCount="1" Body="&lt;p&gt;I am trying to apply a simple Naive Bayes or SVM (libSVM) algorithms to a large data set, which I've constructed as an .arff file.&lt;/p&gt;&#10;&#10;&lt;p&gt;The number of features in my set is ~180k and there are ~6k examples. Also there are 8 classification classes. The data is of size ~3.2GB.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am working with Weka's Java API and Eclipse, I am increasing JVM's memory to the maximum, but I am always getting a heap space error.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am on a MacBook Pro, 2.3 GHz Intel Core i5, 4GB 1333 MHz DDR3.&lt;/p&gt;&#10;&#10;&lt;p&gt;Do I need to find another machine to work with or is it possible that I am having an memory leaks programmatically?&lt;/p&gt;&#10;" ClosedDate="2014-06-02T07:18:28.643" CommentCount="1" CreationDate="2014-06-01T19:29:47.040" Id="100783" LastActivityDate="2014-06-01T22:48:24.953" OwnerUserId="44513" PostTypeId="1" Score="0" Tags="&lt;dataset&gt;&lt;large-data&gt;&lt;weka&gt;&lt;ram&gt;" Title="Memory consumption in NLP tasks" ViewCount="40" />
  <row AnswerCount="1" Body="&lt;p&gt;Say I have the following college enrollment data:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;California&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;In 1975, 188,234 men and 156,612 women were enrolled in California colleges.&lt;br&gt;&#10;In 2005, 194,416 men and 201,334 women were enrolled in California colleges. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Texas&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;In 1975, 132,261 men and 98,554 women were enrolled in Texas colleges.&lt;br&gt;&#10;In 2005, 140,082 men and 153,305 women were enrolled in Texas colleges. &lt;/p&gt;&#10;&#10;&lt;p&gt;Consider, for each state, the proportion of those enrolled in colleges who were female.&lt;/p&gt;&#10;&#10;&lt;p&gt;How would I go about computing the p-value for the null hypothesis that the 19752005 increase in this proportion was the same in Texas as in California? &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-06-01T22:20:54.307" Id="100799" LastActivityDate="2014-06-18T03:07:36.457" LastEditDate="2014-06-18T03:07:36.457" LastEditorUserId="32036" OwnerUserId="46481" PostTypeId="1" Score="0" Tags="&lt;hypothesis-testing&gt;" Title="Testing for equality of differences in proportions" ViewCount="38" />
  
  <row Body="&lt;p&gt;The book&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Diggle, P., Heagerty, P., Liang, K. Y., &amp;amp; Zeger, S. (2002). Analysis of longitudinal data. Oxford University Press.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;is a classic. You can't go wrong starting with that.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-02T05:32:28.860" Id="100824" LastActivityDate="2014-06-02T05:32:28.860" OwnerUserId="30201" ParentId="100729" PostTypeId="2" Score="1" />
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;My study involves studying the seasonal variations (premonsoon, monsoon, postmonsoon) in river at 5 different sampling points. The water quality study involved eight water parameters. To check the reliability of data, I had run reliability analysis and got Cronbach's value = 0.187..which is very very low. &#10;But my question is, is it necessary to run reliability analysis for every data? The data I am handling is completely scientific kind of data..which can show inconsistent results..yet is the standard reliability test necessary?&#10;If yes, please advice on what can be done about this as the shifting of data etc was of no use. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-02T10:50:28.983" Id="100852" LastActivityDate="2014-06-02T10:50:28.983" OwnerUserId="46549" PostTypeId="1" Score="0" Tags="&lt;spss&gt;" Title="Is use of reliability analysis always necessary in data handling?" ViewCount="15" />
  
  
  <row Body="&lt;p&gt;It looks like a Weibull distribution is appropriate, as seen &lt;a href=&quot;http://research.microsoft.com/en-us/um/people/ryenw/papers/LiuSIGIR2010.pdf&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-02T12:44:02.640" Id="100863" LastActivityDate="2014-06-02T12:44:02.640" OwnerUserId="46102" ParentId="99902" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;As you bootstrap you assume that the new bootstrapped distribution is equivalent to the original distribution. If $n_1&amp;gt;n_0$ then you are forced to draw repeated values of $n_0$, which leads to several problems. First of all, the bootstrapped distribution will only have de facto $n_0$ values, the rest being copies, and these copies will only lead to a stronger weighting of these -- randomly drawn -- value when doing e.g. a correlation analysis. Then there is the issue on how your tests might require non-correlated data, as referred by @nic.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you want to really attain $n_1$ real independent values, I think you have to analyse the $n_0$ values, understand which distribution they are likely to follow and then draw $n_1$ values from this &lt;em&gt;assumed&lt;/em&gt; distribution. It is certainly not as generic, but, IMO, is more transparent and you will not suffer from the issues mentioned above.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-02T15:14:36.827" Id="100879" LastActivityDate="2014-06-02T15:14:36.827" OwnerUserId="44760" ParentId="59131" PostTypeId="2" Score="0" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;This question is related to &lt;a href=&quot;http://stats.stackexchange.com/questions/64293/understanding-metropolis-hastings-with-asymmetric-proposal-distribution&quot;&gt;Understanding Metropolis-Hastings with asymmetric proposal distribution&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm trying to draw sample paths from a 2-state markov chain using M-H. Unfortunately, I've only studied the Gaussian (random walk) proposal distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a generally accepted (best) proposal distribution to use when drawing candidates from a discrete distribution? Given the prior estimates of the transition matrix, should I just compute the stationary distribution and draw unconditionally from them, thereby making it an independent chain? Or is it more typical to draw from a pre-specified symmetric transition matrix for the 2 states? Or is there another method which tends to work better?&lt;/p&gt;&#10;&#10;&lt;p&gt;Note: I know that there are other ways to estimate the evolution of the 2 states. However, I use M-H in other steps of the model I'm working with&gt; Therefore, I'd like to use it to estimate the time series of the 2-state process as well, if at all possible.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-02T15:29:59.890" Id="100882" LastActivityDate="2014-06-02T15:29:59.890" OwnerUserId="41762" PostTypeId="1" Score="0" Tags="&lt;mcmc&gt;&lt;metropolis-hastings&gt;" Title="Proper proposal distribution in Metropolis-Hastings for a 2-state discrete process" ViewCount="87" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a collection of benchmarks that we run against different implementations of a system.  Is there a generally-accepted way to make comparisons between multiple sets of benchmark data?  Today, we take the data for system $a$ (with data points $a_1, a_2, a_3,...$) and the data for system $b$ (with data points $b_1, b_2, b_3,...$) and take the geomean of the speedup (e.g. $\frac{a_1}{b_1}$) for each point. Is this a valid approach?&lt;/p&gt;&#10;&#10;&lt;p&gt;In this case, it is safe to assume that the data points have equal importance.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-02T15:53:41.030" Id="100885" LastActivityDate="2014-06-02T16:20:06.513" LastEditDate="2014-06-02T16:20:06.513" LastEditorUserId="26338" OwnerUserId="46562" PostTypeId="1" Score="0" Tags="&lt;performance&gt;" Title="Best way to report &quot;average speedup&quot;?" ViewCount="67" />
  
  
  <row Body="&lt;p&gt;For the Stata commands in this answer let me collect your variables in a local: &lt;br/&gt;&#10;&lt;code&gt;local xlist sse01 wartosc_sr_trw_per_capita zatr_przem_bud podm_gosp_na_10tys_ludn proc_ludn_wiek_prod ludnosc_na_km2&lt;/code&gt; &lt;br/&gt;&#10;So now you can always call all the variables with `xlist'&lt;/p&gt;&#10;&#10;&lt;p&gt;1) There are two commands that you can use after your fixed effects regression. &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;code&gt;xttest2&lt;/code&gt; performs a Breusch-Pagan LM test with the null hypothesis of no dependence between the residuals. This is a test for contemporaneous correlation. Not rejecting the null means that the test did not detect any cross-sectional dependence in your residuals. &lt;/li&gt;&#10;&lt;li&gt;&lt;code&gt;xttest3&lt;/code&gt; performs a modified version of the Wald test for groupwise heteroscedasticity. The null hypothesis is homoscedasticity.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;You can install both commands by typing &lt;code&gt;ssc instal xttest2&lt;/code&gt; and &lt;code&gt;ssc instal xttest3&lt;/code&gt;. If you detect correlations between your residuals you can correct for this with the robust option:&#10;&lt;br/&gt;&lt;code&gt;xtreg st_bezr 'xlist', fe robust&lt;/code&gt;&lt;br/&gt; &lt;br/&gt;&#10;To test for autocorrelation you can apply a Lagrange Multiplier test via &lt;code&gt;xtserial&lt;/code&gt;:&lt;br/&gt;&#10;&lt;code&gt;xtserial st_bezr 'xlist'&lt;/code&gt;&lt;br/&gt;&#10;The null hypothesis is no serial correlation. To correct for both serial correlation and heteroscedasticity you can use the cluster option with your id variable:&lt;br/&gt;&#10;&lt;code&gt;xtreg st_bezr 'xlist', fe cluster(id)&lt;/code&gt;&#10;&lt;br/&gt;&lt;br/&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;2) For the normality test for the residuals: you can obtain the residuals via the predict command &lt;code&gt;predict res, e&lt;/code&gt; after your fixed effects regression. For visual inspection you can use:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;code&gt;kdensity res, normal&lt;/code&gt; (plots the distribution of the residuals and compares it to a normal)&lt;/li&gt;&#10;&lt;li&gt;&lt;code&gt;pnorm res&lt;/code&gt; (plots a standardized normal probability plot) &lt;/li&gt;&#10;&lt;li&gt;&lt;code&gt;qnorm res&lt;/code&gt; (plots the quantiles of the residuals against the quantiles of a normal distribution)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;With pnorm you can see if there is non-normality in the middle of the distribution and qnorm shows you any non-normality in the tails. A formal test can be obtained by &lt;code&gt;swilk res&lt;/code&gt;. The null hypothesis is that the residuals are normally distributed. Generally, non-normality is not a too big concern but it matters for inference. You can again correct for this with the robust option.&lt;/p&gt;&#10;&#10;&lt;p&gt;3) Having &lt;code&gt;corr(u_i, Xb)  = -0.9961&lt;/code&gt; means that the fixed effects are strongly correlated with your explanatory variables, so you did well by controlling for these fixed effects. A strong correlation of this type usually indicates that pooled OLS or random effects will not be suitable for your purpose because both of these models assume that the correlation between $u_i$ and $X\beta$ is zero.&lt;/p&gt;&#10;&#10;&lt;p&gt;4) Generally yes but it depends what you want to estimate or how you can treat your data, i.e. whether your variables are random variables or not. &lt;a href=&quot;http://stats.stackexchange.com/questions/34642/difference-between-panel-data-mixed-model&quot;&gt;Here&lt;/a&gt; is an excellent explanation for the difference between mixed effects and panel data models by @mpiktas which will surely help you.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-02T17:16:02.070" Id="100901" LastActivityDate="2014-06-02T17:16:02.070" OwnerUserId="26338" ParentId="100728" PostTypeId="2" Score="5" />
  
  <row Body="&lt;p&gt;It seems peculiar that such a complex model was selected instead of a more simple linear model. However, if you insist on using that model then notice that &lt;em&gt;NumberOfDifferentKindsOfFruits&lt;/em&gt; should be strictly greater than 0.&lt;/p&gt;&#10;&#10;&lt;p&gt;Further:&lt;/p&gt;&#10;&#10;&lt;p&gt;If &lt;em&gt;NumberOfDifferentKindsOfFruits&lt;/em&gt; equals 1 and &lt;em&gt;NumberOfCustomer&lt;/em&gt; equals 0 the expected average price will be -7.095.&lt;/p&gt;&#10;&#10;&lt;p&gt;If the natural logarithm of &lt;em&gt;NumberOfDifferentKindsOfFruits&lt;/em&gt; increases by one (that is to say that &lt;em&gt;NumberOfDifferentKindsOfFruits&lt;/em&gt; is almost tripled) and all other things remain the same, the expected average price will decrease with 9.471.&lt;/p&gt;&#10;&#10;&lt;p&gt;If the &lt;em&gt;square root&lt;/em&gt; of the &lt;em&gt;NumberOfCustomer&lt;/em&gt; is increased by one and all other variables remain the same, the average expected price will increase with 53.942. Notice, that the last remark is not easily to be understood in terms of the single &lt;em&gt;NumberOfCustomer&lt;/em&gt; variable without square root.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hope that the above will help you&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-06-02T20:38:40.673" Id="100925" LastActivityDate="2014-06-02T20:38:40.673" OwnerUserId="27608" ParentId="100846" PostTypeId="2" Score="0" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I need to make a simulation to calculate the integral of a pdf $f(x|D)$ over a region $T$.That is, I need to evaluate&#10;$$\int_T f(x|D)\; dx$$&lt;/p&gt;&#10;&#10;&lt;p&gt;to do this, I just defined the function $g(x)=1$ if $x \in T$ and $g(x)=0$ if $x$ is not in $T$. So the integral that I want to evaluate is&#10;$$\int_{D(f)} g(x)f(x|D)\;dx$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $D(f)$ is the domain of $f$, right? The Monte Carlo estimative of my integral is given by&#10;$$\bar{I}=\frac{1}{N} \sum_{i=1}^N g(x_i), x_i \sim f$$&lt;/p&gt;&#10;&#10;&lt;p&gt;right? And the Monte Carlo Standard Error is &#10;$$SE= \sqrt{\frac{1}{N}\sum_{i=1}^N(g(x_i)-\bar{I})^2}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I made a Python function to calculate this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;def MCSE(g, estimativa):&#10;&#10;#Size of the vector g (which is composed of 0 and 1)&#10;N = int(len(g))&#10;#Auxiliar vector vet&#10;vet = []&#10;for i in range(N):&#10;    vet.append(float((g[i]-estimativa)**2))&#10;#Calculate the standard error and return&#10;SE = sqrt(np.mean(vet))&#10;return SE&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Is my function correct? Because when I estimate the integral and it gives a value close to the real value of the integral (which I know from other ways), it is giving values close to 0.5, which doesn't seems correct.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-06-02T23:23:59.607" FavoriteCount="1" Id="100942" LastActivityDate="2014-06-02T23:23:59.607" OwnerUserId="35224" PostTypeId="1" Score="0" Tags="&lt;standard-deviation&gt;&lt;python&gt;&lt;standard-error&gt;&lt;mcmc&gt;" Title="MCMC Standard error" ViewCount="44" />
  
  
  
  <row AcceptedAnswerId="100997" AnswerCount="1" Body="&lt;p&gt;I've standardized all the variables (even the response variable) and then I've split my data into a training and test part.  And for example, I've got the following model based on my TRAINING set:&#10;y = 0.5x_1 - 0.2x_2&lt;/p&gt;&#10;&#10;&lt;p&gt;Now I shall get values for y like -0.4,0.7,... but I want to say something about the original response variable. What can I do to get this?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-03T11:48:03.477" Id="100992" LastActivityDate="2014-06-03T15:26:28.073" LastEditDate="2014-06-03T15:26:28.073" LastEditorUserId="4253" OwnerUserId="35223" PostTypeId="1" Score="1" Tags="&lt;interpretation&gt;&lt;standardization&gt;&lt;gam&gt;&lt;regression-strategies&gt;" Title="Interpretation with training and test set with standardized variables" ViewCount="78" />
  <row AnswerCount="0" Body="&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Sgd9z.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I am having trouble getting started with this question. Is there a formula for finding point predictions and prediction intervals? Any advice or hints would be great!  &lt;/p&gt;&#10;&#10;&lt;p&gt;Let $\varepsilon$ be defined as white noise.&lt;/p&gt;&#10;&#10;&lt;p&gt;Does $X_{81}$= 0.8*$X_{80}$ -0.2* $\varepsilon_{80}$?&lt;br&gt;&#10;Does $\varepsilon_{81}$ =$X_{81}$-0.8*$X_{80}$- 0.2*$\varepsilon_{80}$?&lt;/p&gt;&#10;&#10;&lt;p&gt;Does $X_{82}$ = 0.8*$X_{81}$ -0.2*$\varepsilon_{81}$?&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-06-03T11:54:10.360" FavoriteCount="1" Id="100993" LastActivityDate="2014-06-03T13:30:35.660" LastEditDate="2014-06-03T13:30:35.660" LastEditorUserId="46636" OwnerUserId="46636" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;self-study&gt;" Title="Point predictions and prediction intervals" ViewCount="69" />
  <row Body="&lt;p&gt;It is first best to settle on the meaning of fixed and random effects &lt;a href=&quot;http://andrewgelman.com/2005/01/25/why_i_dont_use/&quot; rel=&quot;nofollow&quot;&gt;[link]&lt;/a&gt;, and then move forward from that point. &lt;/p&gt;&#10;&#10;&lt;p&gt;To explicitly model the cluster level (i.e., hospitals) differences seems sensible within a GEE or Multilevel framework. It is also my understanding that sample size needs to be considered at all cluster levels in any multilevel model &lt;a href=&quot;http://www.stats.ox.ac.uk/~snijders/PowerSampleSizeMultilevel.pdf&quot; rel=&quot;nofollow&quot;&gt;[link]&lt;/a&gt;, so you might need to do a quick Monte Carlo study to see if you even have power at the hospital level to pursue what was suggested to you. It has been my experience that reviewers will push, push, push the investigation of random effects in a multilevel model with little consideration of power at the higher levels, so hopefully this isn't a case of that kind of behavior.&lt;/p&gt;&#10;&#10;&lt;p&gt;I've handled this issue with small sample sizes at level 2 and 3 in a structural equation modeling framework, and tested for invariance between my level 2 and 3 groupings. This allowed me to examine the differences &lt;strong&gt;only&lt;/strong&gt; as &lt;strong&gt;fixed effects&lt;/strong&gt;. That might not be possible of you, but you should know that there are ways to tackle this problem from multiple angles, and your original solution might a different angle, with caveats... but you'll need to see what others say. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-03T13:25:46.657" Id="101001" LastActivityDate="2014-06-03T13:25:46.657" OwnerUserId="46638" ParentId="100847" PostTypeId="2" Score="0" />
  
  <row AcceptedAnswerId="101017" AnswerCount="1" Body="&lt;p&gt;I understand that it's instead correct to cross-validate using new data. Why is it &#10;so? It is just that a model will tend to fit the data set that was used to created it better than another randomly sampled set of data?&lt;/p&gt;&#10;&#10;&lt;p&gt;Could it ever be justified to use the same data for EFA and CFA?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-06-03T14:11:57.457" FavoriteCount="1" Id="101006" LastActivityDate="2014-06-03T17:34:22.117" LastEditDate="2014-06-03T14:23:00.447" LastEditorUserId="9162" OwnerUserId="9162" PostTypeId="1" Score="2" Tags="&lt;cross-validation&gt;&lt;factor-analysis&gt;&lt;confirmatory-factor&gt;" Title="Why is it wrong to discover factors using EFA then use CFA on the same data to confirm that factor model?" ViewCount="410" />
  
  <row Body="&lt;p&gt;Some Exploratory Data Analysis can help you to get some hints about the data you're dealing with. &#10;See: &lt;a href=&quot;http://en.wikipedia.org/wiki/Exploratory_data_analysis&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Exploratory_data_analysis&lt;/a&gt;&#10;Or, for some exploratory data analysis techniques, take a look: &lt;a href=&quot;https://www.coursera.org/course/exdata&quot; rel=&quot;nofollow&quot;&gt;https://www.coursera.org/course/exdata&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-06-03T15:26:57.987" Id="101021" LastActivityDate="2014-06-03T15:26:57.987" OwnerUserId="44469" ParentId="101012" PostTypeId="2" Score="2" />
  
  <row AcceptedAnswerId="101060" AnswerCount="2" Body="&lt;p&gt;I had thought that if there is an interaction between 2 independent variables, then it is always the case that there will be main effect for both the independent variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, I found out that this is not the case after reading this &lt;a href=&quot;http://courses.washington.edu/smartpsy/interactions.htm&quot; rel=&quot;nofollow&quot;&gt;site&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can I have some practical examples where there is interaction and:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;no main effects for both independent variables.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;no main effect for one of the independent variables only.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="2" CreationDate="2014-06-03T17:28:27.737" FavoriteCount="1" Id="101037" LastActivityDate="2015-02-05T15:28:49.610" LastEditDate="2015-02-05T15:28:49.610" LastEditorUserId="36515" OwnerUserId="31661" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;interaction&gt;" Title="I need some examples of data where there is an interaction and with/without main effects" ViewCount="63" />
  <row AnswerCount="0" Body="&lt;p&gt;If after a &lt;code&gt;bartlett.test(split(x,list(f1,f2)))&lt;/code&gt; I get a very significant &lt;em&gt;p-value&lt;/em&gt;, but my sample size is huge -- millions of measurements -- is it justified to discard the homogeneity of variance in case of a significant ANOVA?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-06-03T18:13:40.500" Id="101039" LastActivityDate="2014-06-03T18:13:40.500" OwnerUserId="44939" PostTypeId="1" Score="0" Tags="&lt;anova&gt;&lt;assumptions&gt;" Title="Discarding the violation of homogeneity of variance in ANOVA" ViewCount="44" />
  
  <row Body="&lt;p&gt;You're interpreting the individual points on the graph and calling that the interaction but it's not. Taking the example you provided, imagine how your description of the interaction would go if the main effect of A were much larger. Or perhaps if it was much smaller, or even 0. Your description would change but that main effect should be independent of the interaction. Therefore, your description is of the data but not the interaction per se. &lt;/p&gt;&#10;&#10;&lt;p&gt;You need to subtract out main effects to see just the interaction. Once you do that then ALL 2x2 interactions look like the last one on the page you reference, a symmetric &quot;X&quot;. For example, in the linked document there is a data set&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    A1 A2&#10;B1   8 24&#10;B2   4  6&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;There are clearly main effects in the rows and columns. If those are removed you can then see the interaction (think of the matrices below being operated on simultaneously).&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;8 24 -  10.5 10.5 -  5.5  5.5 -  -4.5 4.5 =  -3.5  3.5&#10;4  6    10.5 10.5   -5.5 -5.5    -4.5 4.5     3.5 -3.5&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;(The subtracted matrices above can be calculated as the deviations from the grand mean expected based on the marginal means. The first matrix is the grand mean, 10.5. The second is based on the deviation of row means from the grand mean. The first row is 5.5 higher than the grand mean, etc.)&lt;/p&gt;&#10;&#10;&lt;p&gt;After the main effects are removed then the interaction can be described in effect scores from the grand mean or the reversing difference scores. An example of the latter for the example of above would be, &quot;the interaction is that the effect of B at A1 is 7 and the effect of B at A2 is -7.&quot; This statement remains true regardless of the magnitudes of the main effects. It also highlights that the interaction is about the differences in effects rather than the effects themselves.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now consider the various graphs at your link. Deep down, the interaction is the same shape as described above and in graph 8, a symmetric X. In that case the effect of B is in one direction at A1 and the other direction at A2 (note that your use of increasing A in your description suggests you know A isn't categorical). All that's happening when the main effects are added is that those shift around the final values. If you're just describing the interaction then the one for 8 is good for all of the ones where the interaction is present. However, if your plan is to describe the data then the best way is to just describe the effects and difference in effects. For example, for graph 7 it might be:&#10;  &quot;Both main effects increase from level 1 to 2, however the interaction causes a pattern of data where there is no effect of B at A1 and a positive effect at A2.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;That's a concise accurate description of the data, data where an interaction is present, that contains no actual description of the interaction per se. It's a description of how the main effects are modified by the interaction. Which should be sufficient when no numbers are supplied.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-03T20:21:44.567" Id="101052" LastActivityDate="2014-06-13T14:35:30.493" LastEditDate="2014-06-13T14:35:30.493" LastEditorUserId="601" OwnerUserId="601" ParentId="101029" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;You could possibly convert this into a logistic regression and use the deviance from that.  For example if you fit the quantile regression to find the 75th percentile then you are predicting that 75 percent of values will be below the predicted value.  For each data point in the holdout set find its predicted 75th percentile value and code it as a &quot;success&quot; if the actual value is less than the predicted percentile and a failure if it is greater.  Then compute the deviance for a logistic model with all predictions being 0.75 (you can do this with the R &lt;code&gt;glm&lt;/code&gt; function fitting only an &lt;code&gt;offset&lt;/code&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;If you have predicted multiple quantiles than you would need to work out the deviance for the multinomial, e.g. if you calculate the 25th and 75th percentiles then each point will fall into 1 of 3 categories: below 25th, between 25th and 75th, and above 75th.  The probabilities (under a proper fit) of the 3 groups would be 0.25, 0.5, and 0.25.  I believe the deviance in this case is just the product of the given probabilities for each data point (if a data point is between then you use 0.5, below (or above) use 0.25) and multiply those all together (or sum their logs for the log deviance).  Smaller deviances would be a better fit, larger deviances are a worse fit.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-03T21:35:05.493" Id="101056" LastActivityDate="2014-06-03T21:35:05.493" OwnerUserId="4505" ParentId="101046" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;The significance of the main effects are going to depend on how your data is coded.&lt;/p&gt;&#10;&#10;&lt;p&gt;One real example that I deal with regularly where there is potential for significant interactions and non-significant main effects is:&lt;/p&gt;&#10;&#10;&lt;p&gt;Predictor 1, group: 0-control 1-treatment&lt;/p&gt;&#10;&#10;&lt;p&gt;Predictor 2, time: 0-pre intervention 1-post intervention&lt;/p&gt;&#10;&#10;&lt;p&gt;So the main effect on group (predictor 1) is measuring the difference between the treatment and control groups before any intervention takes place.  If subjects were randomly assigned to group then this should not be significant.  Even in an observational study without randomization this could be non-significant if the groups were chosen to be similar.&lt;/p&gt;&#10;&#10;&lt;p&gt;The main effect on time measures the change in the response between the pre intervention time period and the post intervention time period.  If the response is not changing over time (due to outside effects or contamination from treatment group) then this should be non-significant as well.  When it is not 0 (it is significant) it is an indication of any time effect/drift.&lt;/p&gt;&#10;&#10;&lt;p&gt;The interaction is then what is of interest, it is the change in the treatment group after intervention adjusting out any effect of differences between the groups to begin with and any drift over time.&lt;/p&gt;&#10;&#10;&lt;p&gt;In an ideal situation both main effects will be 0 and the interaction will not be.  But there are cases where either main effect could be non-0.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that in the ideal situation (both groups identical at baseline, no change in control group over time) if we code the control group as -1 instead of 0 and the pre time as -1 instead of 0, then the main effect definitions change and the expectation of them being 0 is no longer the case.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-03T21:49:26.907" Id="101060" LastActivityDate="2014-06-03T21:49:26.907" OwnerUserId="4505" ParentId="101037" PostTypeId="2" Score="2" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm doing a Support Vector Regression with about 70k samples with 500 features each. I'm using sklearn implementation of SVR and my input for the train set is a sparse matrix.&#10;But, for my surprise, the training with Linear Kernel is taking a huge amount of time, a lot more than the training with RBF Kernel.&lt;/p&gt;&#10;&#10;&lt;p&gt;Does someone have a hint of why that happens? For me it seems that this is against intuition, given that RBF Kernel distances seem more expensive to calculate.  &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-06-04T08:26:17.557" Id="101086" LastActivityDate="2014-10-13T18:15:22.630" LastEditDate="2014-06-04T08:50:06.803" LastEditorUserId="44469" OwnerUserId="44469" PostTypeId="1" Score="0" Tags="&lt;svm&gt;&lt;nonlinear-regression&gt;&lt;kernel&gt;&lt;computing&gt;" Title="Linear Kernel taking more time to train than RBF Kernel (SVR)" ViewCount="81" />
  <row AnswerCount="0" Body="&lt;p&gt;I am playing with &lt;code&gt;scikit-learn&lt;/code&gt; to find the &lt;code&gt;tf-idf&lt;/code&gt; values. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have a set of &lt;code&gt;documents&lt;/code&gt; like:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;D1 = &quot;The sky is blue.&quot;&#10;D2 = &quot;The sun is bright.&quot;&#10;D3 = &quot;The sun in the sky is bright.&quot;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I want to create a matrix like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;   Docs      blue    bright       sky       sun&#10;   D1 tf-idf 0.0000000 tf-idf 0.0000000&#10;   D2 0.0000000 tf-idf 0.0000000 tf-idf&#10;   D3 0.0000000 tf-idf tf-idf tf-idf&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So, my code in &lt;code&gt;Python&lt;/code&gt; is:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;import nltk&#10;import string&#10;&#10;from sklearn.feature_extraction.text import TfidfVectorizer&#10;from nltk.corpus import stopwords&#10;&#10;train_set = [&quot;sky is blue&quot;, &quot;sun is bright&quot;, &quot;sun in the sky is bright&quot;]&#10;stop_words = stopwords.words('english')&#10;&#10;transformer = TfidfVectorizer(stop_words=stop_words)&#10;&#10;t1 = transformer.fit_transform(train_set).todense()&#10;print t1&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The result matrix I get is:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;[[ 0.79596054  0.          0.60534851  0.        ]&#10; [ 0.          0.4472136   0.          0.89442719]&#10; [ 0.          0.57735027  0.57735027  0.57735027]]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;If I do a hand calculation then the matrix should be:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;            Docs  blue      bright       sky       sun&#10;            D1    0.2385    0.0000000  0.0880    0.0000000&#10;            D2    0.0000000 0.0880     0.0000000 0.0880&#10;            D3    0.0000000 0.058      0.058     0.058 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I am calculating like say &lt;code&gt;blue&lt;/code&gt; as &lt;code&gt;tf&lt;/code&gt; = &lt;code&gt;1/2 = 0.5&lt;/code&gt; and &lt;code&gt;idf&lt;/code&gt; as &lt;code&gt;log(3/1) = 0.477121255&lt;/code&gt;. Therefore &lt;code&gt;tf-idf = tf*idf = 0.5*0.477 = 0.2385&lt;/code&gt;. In this way, I am calculating the other &lt;code&gt;tf-idf&lt;/code&gt; values. Now, I am wondering, why I am getting different results in the matrix of hand calculation and in the matrix of Python? Which gives the correct results? Am I doing something wrong in hand calculation or is there something wrong in my Python code?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-04T08:42:12.117" FavoriteCount="1" Id="101089" LastActivityDate="2014-06-04T08:42:12.117" OwnerUserId="46059" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;python&gt;&lt;matrix&gt;" Title="Difference in values of tf-idf matrix using scikit-learn and hand calculation" ViewCount="148" />
  <row Body="&lt;p&gt;Your approach is statistically valid.&lt;/p&gt;&#10;&#10;&lt;p&gt;With that being said, you may also want to split the first 10 months into a training and validation set. Build a logistic regression model using only the training set or perform &lt;a href=&quot;http://en.wikipedia.org/wiki/Cross-validation_%28statistics%29&quot; rel=&quot;nofollow&quot;&gt;cross-validation&lt;/a&gt; to obtain the lowest misclassification error rate in the validation set. This way you are less likely to overfit the first 10 months of data, which will likely lead to better predictions in the 11th month (your validation set), the 12 month (your test set), and most importantly, future months. As with any predictive model, it will be important to see how the model holds up over time!&lt;/p&gt;&#10;&#10;&lt;p&gt;Please let me know if you need any further help! Happy modeling!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-06-04T11:55:38.900" Id="101103" LastActivityDate="2014-06-04T11:55:38.900" OwnerUserId="29068" ParentId="101066" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have an implementation that solves the $C$-SVM optimization problem. Is it possible to use this algorithm to create a $\nu$-SVM?&lt;/p&gt;&#10;&#10;&lt;p&gt;I know there is a connection saying &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;$\nu$-SVM leads to $\rho &amp;gt; 0$, then $C$-SVM with $C$ set a &gt; priori to $1/\rho$ leads to the same decision function. (&lt;a href=&quot;http://sci2s.ugr.es/keel/pdf/algorithm/articulo/NewSVM.pdf&quot; rel=&quot;nofollow&quot;&gt;New Support Vector Algorithms&lt;/a&gt;)&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;But can I use a given $\nu$ to derive the parameter $C$ and train the $C$-SVM?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-04T11:58:19.943" Id="101104" LastActivityDate="2014-06-04T11:58:19.943" OwnerUserId="46698" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;svm&gt;" Title="$\nu$ SVM in terms of C-SVM" ViewCount="73" />
  <row AnswerCount="1" Body="&lt;p&gt;I have a data on 7 nutrients (intake of 150 people - 75 men and 75 women). Can i do factor analysis to derive the nutrient intake pattern? If so shouldn't each nutrient be adjusted with the total energy intake?&#10;I would like to do a nutrient intake pattern analysis with the data that I have. Intakes are available for protein, fat, carbohydrates, calcium, iron, vitamin A and vitamin C. Only protein and fat seems to be normally distributed. What should I do?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-06-04T12:00:25.107" FavoriteCount="0" Id="101105" LastActivityDate="2014-06-30T17:08:28.723" LastEditDate="2014-06-04T12:15:29.720" LastEditorUserId="46697" OwnerUserId="46697" PostTypeId="1" Score="-1" Tags="&lt;epidemiology&gt;" Title="energy adjusted nutrient intake" ViewCount="40" />
  
  
