  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I know the &lt;code&gt;glmnet()&lt;/code&gt; function &lt;em&gt;cannot&lt;/em&gt; exclude the intercept by users, but does anyone know how to derive the fit without intercept from the &lt;code&gt;glmnet()&lt;/code&gt;? &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-08-03T16:22:22.110" FavoriteCount="1" Id="33621" LastActivityDate="2013-07-07T07:14:57.120" LastEditDate="2012-08-04T10:48:10.767" LastEditorUserId="930" OwnerUserId="13064" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;lasso&gt;&lt;glmnet&gt;" Title="How to use glmnet without the intercept?" ViewCount="1004" />
  
  <row Body="&lt;p&gt;+1 to both @JoelW. &amp;amp; @MichaelChernick.  I want to add a detail to @JoelW.'s answer.  He notes that &quot;we almost never have a direct estimate of the SEM&quot;, which is essentially true, but it's worth explicitly recognizing a caveat to that statement.  Specifically, when a study compares multiple groups / treatments (for example, placebo vs. standard drug vs. new drug), an ANOVA is typically used to see if they are all equal.  The null hypothesis is that each group has been drawn from the same population, and thus, all three means are estimates of the population mean.  That is, the null hypothesis in a standard ANOVA assumes that you &lt;em&gt;do have&lt;/em&gt; a direct estimate of the SEM.  Consider the equation for the variance of the sampling distribution of means:&#10;$$&#10;\sigma^2_{\bar x}=\frac{\sigma^2_{pop}}{n_j},&#10;$$&#10;where $\sigma^2_{pop}$ is the population variance, and $n_j$ is the number of groups.  Although we don't usually perform the calculations in this way, we &lt;em&gt;could&lt;/em&gt; simply use standard formulas to plug in estimated values, and with minimal algebraic reshuffling, form the $F$ statistic like so:&#10;$$&#10;F=\frac{n_j\times s^2_{\bar x}}{s^2_{\text{pooled within group}}}&#10;$$&#10;In this case, we really would be using the standard formula (only applied over the group means), that is:&#10;$$&#10;s^2_{\bar x}=\frac{\sum_{j=1}^{n_j}(\bar x_j-\bar x_.)^2}{n_j-1},&#10;$$&#10;with $x_.$ being the mean of the group means.  &lt;/p&gt;&#10;&#10;&lt;p&gt;In that we typically believe the null hypothesis is not true, @JoelW.'s point is right, but I work through this point, because I think the clarity it affords is helpful for understanding these issues.  &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-08-03T18:43:45.700" Id="33627" LastActivityDate="2015-03-04T17:23:51.747" LastEditDate="2015-03-04T17:23:51.747" LastEditorUserId="7290" OwnerUserId="7290" ParentId="33547" PostTypeId="2" Score="4" />
  <row AnswerCount="0" Body="&lt;p&gt;I have one IV ($A$) and two DVs ($B$,$C$). $A$ is a binary, experimentally manipulated variable. Each subject has two scores on $B$ and two scores on $C$, corresponding to the experimental manipulations $A$ and ~$A$.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to say that $A$ causes an increase in $B$, and increases in $B$ lead to an increase in $C$. $A$ should lead to increases in $C$, if and only if it leads to increases in $B$ (full mediation).&lt;/p&gt;&#10;&#10;&lt;p&gt;Paired t-tests confirm $B$ is higher for $A$ vs ~$A$, and $C$ is higher for $A$ vs ~$A$. $\Delta B$ is highly correlated with $\Delta C$. I'm relying on difference scores under the assumption that $B$ and $C$ are not necessarily correlated because of individual differences in the baseline of $B$.&lt;/p&gt;&#10;&#10;&lt;p&gt;How can I test for mediation here? Since $A$ is binary, there is no &quot;$\Delta A$&quot; with which to run a regression analysis. I'm not sure where to go next; any help is appreciated!&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-08-03T22:55:26.543" Id="33643" LastActivityDate="2012-08-04T07:58:01.830" LastEditDate="2012-08-04T07:58:01.830" LastEditorUserId="8507" OwnerUserId="1977" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;mediation&gt;&lt;binary&gt;" Title="Mediation analysis using binary IV and difference scores" ViewCount="113" />
  
  <row Body="&lt;p&gt;One has $\frac{Y}{Z}=\max\left\{\frac{X}{1-X},\frac{1-X}{X}\right\}$. Therefore $\frac{Y}{Z} \leq t$ $\iff$ $\frac{X}{1-X} \leq t$ and $\frac{1-X}{X} \leq t$. After a little bit of algebra $\frac{X}{1-X} \leq t$ $\iff$ $X \leq \frac{t}{1+t}$ and  $\frac{1-X}{X} \leq t$ $\iff$ $X \geq \frac{1}{1+t}$. So finally the probability that  $\frac{Y}{Z} \leq t$ is the Lebesgue measure of the interval $\left[\frac{1}{1+t}, \frac{t}{1+t}\right]$.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-04T06:56:47.417" Id="33658" LastActivityDate="2012-08-04T06:56:47.417" OwnerUserId="8402" ParentId="33656" PostTypeId="2" Score="6" />
  
  <row Body="&lt;p&gt;When measuring clustering in time I don't think squared distance in time makes much sense at all. A rate in terms of events per unit time seems more reasonable to me.  Periods with high rates are the event cluster intervals.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-04T14:01:23.977" Id="33669" LastActivityDate="2012-08-04T14:01:23.977" OwnerUserId="11032" ParentId="33651" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;Obviously if the methods use different models you will get different answers. Subtracting off the intercept terms does not lead to the model without the intercept because the best fitting coefficients will change and you do not change them the way you are approaching it. You need to fit the same model with both methods if you want the same or nearly the same answers.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-08-04T16:01:46.223" Id="33677" LastActivityDate="2012-08-04T16:01:46.223" OwnerUserId="11032" ParentId="33674" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;In case someone really &lt;em&gt;does&lt;/em&gt; want the mean survival time as originally asked, it's $e^{\mu+{\sigma^2\over 2}}$.  (In fact, the original poster should carefully consider whether they want the mean or the median for their use of the resulting number.  For the example given with $\sigma=1.1$, the mean is almost twice the median.)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-04T16:28:33.643" Id="33679" LastActivityDate="2012-08-04T20:27:23.133" LastEditDate="2012-08-04T20:27:23.133" LastEditorUserId="13077" OwnerUserId="13077" ParentId="33664" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;So, under $H_0$ &lt;/p&gt;&#10;&#10;&lt;p&gt;$[(2-\lambda)\bar X+(1-2\lambda)\bar Y+(1+\lambda)\bar Z]$~$N(0,\frac{[(2-\lambda)^2+(1-2\lambda)^2+(1+\lambda)^2]\sigma ^2}{n})$ Now estimate $\sigma ^2$ by the pooled sample variance $s$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that $\bar X$~$N(m_1,\frac{\sigma^2}{n})$,$\bar Y$~$N(m_2,\frac{\sigma^2}{n})$ and $\bar Z$~$N(m_1-m_2,\frac{\sigma^2}{n})$ Similarly $\frac{(n-1)s_1^2}{\sigma ^2}$~$\chi ^2_{n-1}$,$\frac{(n-1)s_2^2}{\sigma ^2}$~$\chi ^2_{n-1}$ and $\frac{ns_3^2}{\sigma ^2}$~$\chi ^2_{n}$ independently.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hence $[(2-\lambda)\bar X+(1-2\lambda)\bar Y+(1+\lambda)\bar Z]$~$N(0, {6\sigma ^2(\lambda ^2-\lambda +1)}/n)$ and $\frac{(3n-2)s^2}{\sigma ^2}$~$\chi ^2_{3n-2}$&lt;/p&gt;&#10;&#10;&lt;p&gt;So,$\dfrac{\frac{[(2-\lambda)\bar X+(1-2\lambda)\bar Y+(1+\lambda)\bar Z]\sqrt  n}{\sigma\sqrt {6(\lambda ^2-\lambda +1)}}}{\sqrt {\frac{(3n-2)s^2}{\sigma^2}/(3n-2)}}$~$t_{3n-2}$&lt;/p&gt;&#10;&#10;&lt;p&gt;We get the test statistics $$T=\frac{[(2-\lambda)\bar X+(1-2\lambda)\bar Y+(1+\lambda)\bar Z]\sqrt n}{s\sqrt{6(\lambda ^2-\lambda +1)}}$$&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-04T16:54:54.210" Id="33681" LastActivityDate="2013-03-24T19:00:04.893" LastEditDate="2013-03-24T19:00:04.893" LastEditorUserId="12710" OwnerUserId="12710" ParentId="33675" PostTypeId="2" Score="3" />
  <row AnswerCount="0" Body="&lt;p&gt;Let $\Delta_{K}$ be the probability simplex of dimension $K-1$, i.e. $x \in \Delta_{K}$ is such that $x_i \ge 0$ and $\sum_i x_i = 1$.&lt;/p&gt;&#10;&#10;&lt;p&gt;What distributions which are frequently (or well-known, or defined in the past) over $\Delta_{K}$ exist?&lt;/p&gt;&#10;&#10;&lt;p&gt;Clearly, there is the Dirichlet distribution and the logistic normal distribution. Are there any other distributions which come up naturally in this context?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-04T20:57:12.377" Id="33685" LastActivityDate="2014-03-31T15:27:52.787" LastEditDate="2014-03-31T15:27:52.787" LastEditorUserId="919" OwnerUserId="13002" PostTypeId="1" Score="4" Tags="&lt;distributions&gt;&lt;multinomial&gt;" Title="What are some distributions over the probability simplex?" ViewCount="337" />
  
  <row AnswerCount="4" Body="&lt;p&gt;I am an ecology student and have to deal with 10 or 20 field variables, including species frequencies. I need to screen out what variables are most important in the occurrence of a bird species. What book would tell me the methods to do this?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-08-04T23:29:20.043" FavoriteCount="2" Id="33686" LastActivityDate="2013-03-03T11:58:48.747" LastEditDate="2013-03-03T11:58:48.747" LastEditorUserId="88" OwnerUserId="13080" PostTypeId="1" Score="4" Tags="&lt;model-selection&gt;&lt;references&gt;&lt;ecology&gt;&lt;canonical-correlation&gt;" Title="Books about model selection in ecology" ViewCount="324" />
  <row Body="&lt;p&gt;No book is going to tell you which variable to include and which to exclude. You should have done necessary background research before doing your fieldwork to get an idea of which variables to measure. You could have based those variables on the species life history and/or previous research. Once variables were selected, it is good practice to do a lot of hard thinking about the ecology of the species and develop some &lt;em&gt;a priori&lt;/em&gt; models.&lt;/p&gt;&#10;&#10;&lt;p&gt;Are you trying to model the probability of occurrence for a species? Often researchers assume that a species will be detected if it is at a site.  However, detection is never certain. Therefore, we can’t assume detection, non-detection data represent presence-absence data. Further, this imperfect detectability problem also imposes constrains on count data as well.  Fortunately, there are all sorts of survey techniques that allow us to collect data that can be used to model and correct for the incomplete detection problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are a new set of techniques based in hierarchical modeling that allow one to untangle the detection process from the occurrence/abundance process. Basically, these models use ancillary data (i.e., distance from transect, time to detection, etc.) or information from repeated surveys (i.e., a series of detection, non-detection or count frequency data at each survey site) to model the detection process. These models provide a framework for modeling both the detection process and the intensity of abundance or occupancy as a latent variable. One of the greatest advantages of the hierarchical modeling framework is the ability to separately model the abundance and detection processes. This allows the effects of a given covariate, on either process, to be disentangled. These techniques result in spatially-explicit models of abundance or occurrence. Such models are attractive because they can be used to understand ecological relationships among animal abundance or occurrence and environmental conditions while accounting for imperfect detection.  For species management or conservation purposes, spatially-explicit models of abudance or occurrence can be very valuable.  The naive approach is to ignore the problem of incomplete detection and potentially draw wrong inferences about the species' ecological relationships to environmental conditions.&lt;/p&gt;&#10;&#10;&lt;p&gt;A good place to start learning about these types of models is &lt;code&gt;R Package unmarked&lt;/code&gt;; see &lt;a href=&quot;https://sites.google.com/site/unmarkedinfo/home&quot; rel=&quot;nofollow&quot;&gt;unmarked google site&lt;/a&gt; and &lt;a href=&quot;https://groups.google.com/forum/?fromgroups#!forum/unmarked&quot; rel=&quot;nofollow&quot;&gt;unmarked google group&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Good Luck.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-05T01:45:28.703" Id="33691" LastActivityDate="2013-03-03T07:40:48.807" LastEditDate="2013-03-03T07:40:48.807" LastEditorUserId="12318" OwnerUserId="12318" ParentId="33686" PostTypeId="2" Score="8" />
  
  
  <row Body="&lt;p&gt;We note that since each $X_i$ has mean 0 and variance 1, $(\sum_i X_i^2)/n$ converges to 1 a.s.&lt;/p&gt;&#10;&#10;&lt;p&gt;But if &#10;$$&#10;\lim_{n\to\infty} \mathbb P\Big( \sum_i X_i^2 &amp;lt; a \Big) &amp;gt; 0 \,,&#10;$$ then with positive probability &#10;$$&#10;\frac{1}{n}\sum_i X_i^2&#10;$$ goes to 0, which is a contradiction because if the sum have a positive probability of staying finite then dividing by n we have $(\sum_i X_i^2)/n$ converging to 0 with positive probability contradicting convergence to 1 almost surely.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2012-08-05T12:53:16.853" Id="33710" LastActivityDate="2012-08-06T10:48:26.977" LastEditDate="2012-08-06T10:48:26.977" LastEditorUserId="11032" OwnerUserId="11032" ParentId="33698" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Company tweets do vary with time and so have time dependence as well as dependence on covariates.  But it is possible to model covariates in time along with an ARMA structure in such a model. Instead of using them continuously you choose to create categories.  There is nothing wrong with that (except that some quantitative information is lost). I would keep track of these covariates as a function of time as recent news reports will have more influence and relevance than old ones. Treat the problem as a time series forecasting problem rather than as a simple regression problem.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-05T14:01:24.817" Id="33718" LastActivityDate="2012-08-05T14:01:24.817" OwnerUserId="11032" ParentId="33706" PostTypeId="2" Score="1" />
  
  
  <row Body="A branch of mathematics/statistics used to determine the information carrying capacity of a channel, whether one that is used for communication or one that is defined in an abstract sense.  Entropy is one of the measures by which information theorists can quantify the uncertainty involved in predicting a random variable." CommentCount="0" CreationDate="2012-08-05T16:51:50.940" Id="33729" LastActivityDate="2012-08-05T17:02:04.187" LastEditDate="2012-08-05T17:02:04.187" LastEditorUserId="3826" OwnerUserId="3826" PostTypeId="4" Score="0" />
  
  
  
  
  <row Body="&lt;p&gt;You can definitely use glm to fit this model. In glm, you can specify family(nbinomial $\#_{k}$) and then search for a $\#_{k}$ that makes the deviance-based dispersion equal to 1. However, you can also use &lt;code&gt;family(nbinomial ml)&lt;/code&gt; to estimate $\#_{k}$ with maximum likelihood, which should report the same value as nbreg. On the other hand, nbreg will also give you a confidence interval. The nb link function is $\eta=\ln \frac{\mu}{\mu +k}$, where $k=1$ if you specify &lt;code&gt;family(binomial)&lt;/code&gt; without the $\#_{k}$ parameter. &lt;/p&gt;&#10;&#10;&lt;p&gt;To get your residuals, run your glm command first. Then type &lt;code&gt;predict resid1, pearson&lt;/code&gt;. Do that for the other 5 specifications to get resid2-resid6. I am not sure what you mean by aggregate, but you can export the residuals (and an id) as a csv file with &lt;code&gt;outsheet idvar resid1-resid6 using &quot;C:/pearson_resids&quot;, comma&lt;/code&gt;.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-08-06T16:08:30.440" Id="33771" LastActivityDate="2012-08-06T16:21:04.173" LastEditDate="2012-08-06T16:21:04.173" LastEditorUserId="7071" OwnerUserId="7071" ParentId="33768" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;Let me propose an answer - I am happy to hear constructive criticism:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Merge measurements of technical replicates (taking the union)&#10;&lt;ul&gt;&#10;&lt;li&gt;I assume that the between-replicates variation is about the same as the within-replicate variation (for any variable).&lt;/li&gt;&#10;&lt;/ul&gt;&lt;/li&gt;&#10;&lt;li&gt;Apply statistical tests of difference of mean for the combined data for each biological replicate and each variable. Apply multiplicity correction.&lt;/li&gt;&#10;&lt;li&gt;For variables detected in both biological replicate sets:&#10;&lt;ul&gt;&#10;&lt;li&gt;if direction of change is in both direction:&#10;&lt;ul&gt;&#10;&lt;li&gt;calculate combined p-values using &lt;a href=&quot;http://en.wikipedia.org/wiki/Fisher%27s_method&quot; rel=&quot;nofollow&quot;&gt;Fisher's method&lt;/a&gt; &lt;/li&gt;&#10;&lt;/ul&gt;&lt;/li&gt;&#10;&lt;li&gt;else, set p-value to 1&lt;/li&gt;&#10;&lt;/ul&gt;&lt;/li&gt;&#10;&lt;li&gt;For variables seen in only one biological replicate set, use its original p-value (just penalize but don't remove them)&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Do I miss any assumptions such that you would deem this procedure unsound? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-06T16:25:03.733" Id="33773" LastActivityDate="2012-08-06T16:25:03.733" OwnerUserId="1829" ParentId="33422" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;Wikipedia has a page that &lt;a href=&quot;http://en.wikipedia.org/wiki/List_of_probability_distributions&quot;&gt;lists many probability distributions&lt;/a&gt; with links to more detail about each distribution.  You can look through the list and follow the links to get a better feel for the types of applications that the different distributions are commonly used for.&lt;/p&gt;&#10;&#10;&lt;p&gt;Just remember that these distributions are used to model reality and as Box said: &quot;all models are wrong, some models are useful&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here are some of the common distributions and some of the reasons that they are useful:&lt;/p&gt;&#10;&#10;&lt;p&gt;Normal: This is useful for looking at means and other linear combinations (e.g. regression coefficients) because of the CLT.  Related to that is if something is known to arise due to additive effects of many different small causes then the normal may be a reasonable distribution, for example many biological measures are the result of multiple genes and multiple environmental factors and therefor are often approximately normal.&lt;/p&gt;&#10;&#10;&lt;p&gt;Gamma: Right skewed and useful for things with a natural minimum at 0.  Commonly used for elapsed times and some financial variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;Exponetial: special case of the Gamma.  It is memoryless and scales easily.&lt;/p&gt;&#10;&#10;&lt;p&gt;Chi-squared ($\chi^2$): special case of the Gamma.  Arrises as sum of squared normal variables (so used for variances).&lt;/p&gt;&#10;&#10;&lt;p&gt;Beta:  Defined between 0 and 1 (but could be transformed to be between other values), useful for proportions or other quantities that must be between 0 and 1.&lt;/p&gt;&#10;&#10;&lt;p&gt;Binomial:  How many &quot;successes&quot; out of a given number of independent trials with same probability of &quot;success&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Poisson: Common for counts.  Nice properties that if the number of events in a period of time or area follows a poisson, then the number in twice the time or area still follows the poisson (with twice the mean), this works for adding poissons or scaling with values other than 2.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that if events occur over time and the time between occurances follows an exponential then then number that occure in a time period follows a poisson.&lt;/p&gt;&#10;&#10;&lt;p&gt;Negative Binomial:  Counts with minimum 0 (or other value depending on which version) and no upper bound.  Conceptually it is the number of &quot;failures&quot; before k &quot;successes&quot;.  The negative binomial is also a mixture of poisson variables whose means come from a a gamma distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;Geometric: special case for negative binomial where it is the number of &quot;failures&quot; before the 1st &quot;success&quot;.  If you truncate (round down) an exponential variable to make it discrete, the result is geometric.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-08-06T21:18:32.883" Id="33797" LastActivityDate="2012-08-07T18:50:41.857" LastEditDate="2012-08-07T18:50:41.857" LastEditorUserId="4505" OwnerUserId="4505" ParentId="33776" PostTypeId="2" Score="10" />
  
  
  <row AcceptedAnswerId="33978" AnswerCount="2" Body="&lt;p&gt;Given that the posterior estimate of $\sigma'^{2}$of a normal likelihood and an inverse gamma prior on $\sigma^2$ is:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\sigma'^{2}\sim\textrm{IG}\left(\alpha + \frac{n}{2}, \beta +\frac{\sum_{i=1}^n{(y_i-\mu)^2}}{2}\right)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;which is equivalent to &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\sigma'^{2}\sim\textrm{IG}\left( \frac{n}{2}, \frac{n\sigma^2}{2}\right)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;since a weak $\textrm{IG}(\alpha, \beta)$ prior on $\sigma^2$ removes $\alpha$ and $\beta$ from eqn 1:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\sigma'^{2}\sim\textrm{IG}\left( \frac{n}{2}, \frac{\sum_{i=1}^n{(y_i-\mu)^2}}{2}\right)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;It is apparent that the posterior estimate of $\sigma^2$ is a function of the sample size and the sum of squares of the likelihood. But what does this mean? There is a derivation on &lt;a href=&quot;http://en.wikipedia.org/w/index.php?title=Normal-gamma_distribution&quot; rel=&quot;nofollow&quot;&gt;Wikipedia&lt;/a&gt; that I don't quite follow.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have the following questions&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Can I get to this second equation without invoking Bayes' rule? I am curious if there is something inherent in the parameters of an IG that is related to the mean and variance independent of the normal likelihood.&lt;/li&gt;&#10;&lt;li&gt;Can I use the sample size and standard deviation from a previous study to estimate an informed prior on $\sigma^2$, and then update the prior with new data? This seems straightforward, but I can not find any examples of doing so, or rationale why this would be a legitmate approach - other than what can be seen in the posterior.&lt;/li&gt;&#10;&lt;li&gt;Is there a popular probability or statistics textbook that I can consult for further explanation?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="4" CreationDate="2012-08-07T00:07:56.477" Id="33808" LastActivityDate="2012-08-10T07:18:37.850" LastEditDate="2012-08-09T11:03:58.380" LastEditorUserId="10849" OwnerUserId="2750" PostTypeId="1" Score="4" Tags="&lt;bayesian&gt;&lt;prior&gt;&lt;conjugate-prior&gt;" Title="How is the inverse gamma distribution related to $n$ and $\sigma$?" ViewCount="1607" />
  
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I have an exam in two days and can't figure this out at all:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;In the local cola canning factory, the mean fill of cans is set at 300&#10;  milliliters but there is concern that the population mean fill may not&#10;  in fact be 300ml. Assume that the standard deviation of the amount&#10;  of cola in a random can is 1. 2 ml. A random sample of 100 cans showed&#10;  a mean fill of x̄  299. 64 ml.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;Is there evidence at the 1% significance level that the population&#10;  mean fill differs from 300 ml? To answer this question, carry out in&#10;  detail a z-test but do not use the p-value method.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;Calculate the p-value of the test in (i) above and explain why your&#10;  decision would be the same as that which you reached in (i) if you had&#10;  been asked to conduct the test in (i) by the p-value method.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="2" CreationDate="2012-08-07T12:14:17.617" Id="33839" LastActivityDate="2013-01-04T15:17:55.383" LastEditDate="2012-08-07T12:21:33.463" LastEditorUserId="1205" OwnerUserId="13127" PostTypeId="1" Score="1" Tags="&lt;self-study&gt;&lt;standard-deviation&gt;&lt;p-value&gt;" Title="Calculating if population mean differs from a given value" ViewCount="159" />
  <row Body="&lt;p&gt;I think most of your questions are really about the general modeling strategy to be used in data analysis:  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;1.&lt;/strong&gt; I think you should &lt;em&gt;always&lt;/em&gt; customize your modeling strategy based on what your substantive concerns are.  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;2 / 3.&lt;/strong&gt; The best general approach is to include all of the variables that you care about into one larger model, rather than fitting separate models to different situations based on subsets of the data.  There are two main reasons for this:  &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;If there is a difference in the response depending on whether a particular covariate has a given value or not, that can be properly tested within a larger model.  Moreover, testing &lt;em&gt;interactions&lt;/em&gt; (whether the effect of another covariate depends on the value of this one) can also be properly tested.  &lt;/li&gt;&#10;&lt;li&gt;Your model can borrow information from the cases where a covariate takes on the other value to more precisely estimate all of the effects included.  For example, you will get a better estimate of the intercept when all the data (e.g., $male$ vs. $female$) are included, simply because there's more data to use for the estimate.  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;(Note that there are some lower-frequency cases that appear to diverge from this advice.  One I can think of concerns &lt;a href=&quot;http://en.wikipedia.org/wiki/Survival_analysis&quot; rel=&quot;nofollow&quot;&gt;survival analyses&lt;/a&gt;; the commonly used &lt;a href=&quot;http://en.wikipedia.org/wiki/Proportional_hazards_models&quot; rel=&quot;nofollow&quot;&gt;Cox model&lt;/a&gt; assumes proportional hazard, and when a covariate violates this assumption, a &lt;em&gt;stratified&lt;/em&gt; approach can be used.  Even in this case, however, all strata are fit together, so that the estimates 'borrow strength' across the strata. I'm hard pressed to think of a situation where you'd want to break up your data and fit several smaller models independently.)&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Relative to this specific case:  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;2.&lt;/strong&gt; I don't think you should do a standard ANOVA here, as your response variable is binary.  Your best bet is to use a logistic regression model.  (For more information about GLiM's, you may find the answer I wrote &lt;a href=&quot;http://stats.stackexchange.com/questions/20523/difference-between-logit-and-probit-models/30909#30909&quot;&gt;here&lt;/a&gt; to be helpful, although it was written in a different context.)  &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-08-07T15:06:06.873" Id="33851" LastActivityDate="2012-08-07T15:27:53.337" LastEditDate="2012-08-07T15:27:53.337" LastEditorUserId="7290" OwnerUserId="7290" ParentId="33849" PostTypeId="2" Score="5" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm trying to calculate logistic regression coefficients by defining the log-likelihood function and using maximum likelihood.&lt;/p&gt;&#10;&#10;&lt;p&gt;In some cases when the initial (start) values I gave to the maximum likelihood were not correct I got wrong results for the logistic regression (different from the ones I get when using &lt;code&gt;glm&lt;/code&gt; for example).&lt;/p&gt;&#10;&#10;&lt;p&gt;Given the input data and y values, what should be the optimum initial values for logistic regression (or, in other words, what are the values that are being used in &lt;code&gt;glm&lt;/code&gt;)?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-08-07T16:09:51.133" Id="33857" LastActivityDate="2013-06-03T11:34:40.243" LastEditDate="2013-06-03T11:31:39.107" LastEditorUserId="22047" OwnerUserId="5497" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;logistic&gt;&lt;maximum-likelihood&gt;" Title="Initial values for logistic regression using maximum likelihood" ViewCount="759" />
  <row AnswerCount="2" Body="&lt;p&gt;&lt;strong&gt;Q1:&lt;/strong&gt; Is there any Arima (p,d,q) model that can forecast interventions (pulse) itself? I know that I can use &lt;code&gt;xreg&lt;/code&gt; or even &lt;code&gt;xtransf&lt;/code&gt; arguments as the covariates to include the intervention over the observed time series.&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem is that I don’t know the new values of the covariates to include in the predict function to do the forecasts.  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Q2:&lt;/strong&gt; Can we simulate (estimate) these new covariates? If yes, how? I think that depends on the probability of having pulse. But how we can estimate these probabilities?  &lt;/p&gt;&#10;&#10;&lt;p&gt;To be more precise, I want to develop a time series model that can capture the interventions and also be able to forecast them! Is that even possible? Maybe I am asking too much :) I appreciate any comment, suggestion or reference.&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2012-08-07T16:35:27.303" Id="33859" LastActivityDate="2012-09-08T21:01:24.993" LastEditDate="2012-08-07T16:39:41.383" LastEditorUserId="7290" OwnerUserId="13138" PostTypeId="1" Score="2" Tags="&lt;time-series&gt;&lt;forecasting&gt;&lt;arima&gt;&lt;intervention-analysis&gt;" Title="Forecasting Interventions (pulse) with ARIMA Model" ViewCount="586" />
  
  <row AcceptedAnswerId="33891" AnswerCount="1" Body="&lt;p&gt;Identify the following models as linear or non-linear. In case of a non-linear model, reduce the model into a linear model by a suitable transformation.&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\eqalign{&#10;(a)\quad&amp;amp;y=\beta_0+\beta_1 x+\beta_2 x^2+e \\&#10;(b)\quad&amp;amp;y=\frac{x}{\beta_0+\beta_1 x}+e\\&#10;(c)\quad&amp;amp;y=\frac{\exp(\beta_0+\beta_1 x)}{1+\exp(\beta_0+\beta_1 x)}+e&#10;}$$ &lt;/p&gt;&#10;&#10;&lt;p&gt;where $e\sim\mathcal{N}(0,\sigma^2).$&lt;/p&gt;&#10;&#10;&lt;p&gt;I know that the model of $(a)$ is linear. I think $(b)$ &amp;amp; $(c)$ are non-linear.How can I make them linear models?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-08-07T19:14:55.930" FavoriteCount="2" Id="33876" LastActivityDate="2012-08-07T22:13:50.933" LastEditDate="2012-08-07T22:13:50.933" LastEditorUserId="919" OwnerUserId="12710" PostTypeId="1" Score="0" Tags="&lt;self-study&gt;&lt;data-transformation&gt;&lt;linear-model&gt;" Title="How to identify models as linear or non-linear?" ViewCount="168" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I want to have R display the data it gives me from the &lt;code&gt;summary()&lt;/code&gt; function in a table so I can easily share this. I am currently just doing &lt;code&gt;summary()&lt;/code&gt; in the console and then taking a screenshot, but I would rather have this generated as a nice table just like all of my graphs are. Any ideas? &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-08-07T21:08:25.797" FavoriteCount="3" Id="33887" LastActivityDate="2012-08-08T04:44:40.750" LastEditDate="2012-08-08T01:10:35.503" LastEditorUserId="930" OwnerUserId="13147" PostTypeId="1" Score="7" Tags="&lt;r&gt;&lt;dataset&gt;&lt;summary-statistics&gt;&lt;tables&gt;" Title="How to generate nice summary table?" ViewCount="7880" />
  
  <row Body="&lt;p&gt;&lt;code&gt;-estat summarize-&lt;/code&gt; will give summaries of the sample used in the most recent estimation command. See &lt;code&gt;-help postestimation-&lt;/code&gt;, or the &lt;code&gt;postestimation&lt;/code&gt; help for the specific command you are using.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-08-08T01:40:14.250" Id="33901" LastActivityDate="2012-08-08T21:01:25.953" LastEditDate="2012-08-08T21:01:25.953" LastEditorUserId="930" OwnerUserId="7644" ParentId="33899" PostTypeId="2" Score="0" />
  
  <row AnswerCount="0" Body="&lt;p&gt;While I have successfully used R for Cluster Analysis and Correspondence Analysis, I'm new to the use of R-project for Choice-Based Conjoint. &lt;/p&gt;&#10;&#10;&lt;p&gt;Per recommendations in &lt;a href=&quot;http://stats.stackexchange.com/questions/9283/conjoint-packages-for-r&quot;&gt;this post&lt;/a&gt;, I'm looking at the following [R] Packages for purposes of analyzing choice-based conjoint data:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;ChoiceModelR&lt;/li&gt;&#10;&lt;li&gt;mlogit&lt;/li&gt;&#10;&lt;li&gt;AlgDesign for constructing choice sets&lt;/li&gt;&#10;&lt;li&gt;prefmod for analysing paired comparison data&lt;/li&gt;&#10;&lt;li&gt;conf.design for constructing factorial designs&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;For reference purposes, I'm curious whether there is a sample R-project session showing the use of one or more of these packages to:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Specify attributes and attributes levels&lt;/li&gt;&#10;&lt;li&gt;Analyze choice-based conjoint data to list out respondent-level utility values&lt;/li&gt;&#10;&lt;li&gt;Simulate market share&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Has anyone seen a sample R-project session of this nature? Or can someone who has created such a session post it?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-08-08T04:23:20.990" FavoriteCount="1" Id="33909" LastActivityDate="2012-08-08T06:33:37.877" LastEditDate="2012-08-08T06:33:37.877" LastEditorUserId="8507" OwnerUserId="11781" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;conjoint-analysis&gt;" Title="Sample session for analysis of conjoint data in R" ViewCount="346" />
  
  <row Body="&lt;p&gt;Adding duplicate data points will not even make the parameters estimable.  Just think of it this way.  It takes two points to fit a straight line and with two points you get a perfect fit.  You have no information to estimate the variability in the determination of those points.  Generally we assume that x is known and y is observed with some random error.  Now repeat those two points 5 times or even 100 times.  It does not matter.  You gain no information about the variability of y.  All the additional points are artificial and you still get exactly ythe same line with exactly the same fit.  There is not new information.  There would be if the new observations were really independent.  They the data would be suggesting that y is observed with no error.  Yet you would still want intermediate values for x so as to see if the function really is linear.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Even if you have more points than variables that does not really solve your problem.  Yes it makes the parameters estimable and you may even have an estimate of the random error component from the residuals.  But the estimates will not be accurate.  You should have a lot more observations than variables in order to obtain a good model fit.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-08-08T16:42:29.150" Id="33935" LastActivityDate="2012-08-08T16:42:29.150" OwnerUserId="11032" ParentId="33931" PostTypeId="2" Score="8" />
  
  <row Body="&lt;p&gt;You should read up on copulas! That will give you a way to construct as many counterexamples as you wish. You can start with&#10;&lt;a href=&quot;http://en.wikipedia.org/wiki/Copula_(probability_theory&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Copula_(probability_theory&lt;/a&gt;)&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-08-08T21:04:01.660" Id="33951" LastActivityDate="2012-08-08T21:04:01.660" OwnerUserId="11887" ParentId="33354" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;Yes, you can compare.&lt;/p&gt;&#10;&#10;&lt;p&gt;And where did you get the idea that a not-significant correlation can't be interpreted?&lt;/p&gt;&#10;&#10;&lt;p&gt;Nor does &quot;not significant&quot; mean &quot;not different from 0&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;Not significant&quot; means the following and nothing more:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;If, in the population from which this sample was taken, the true correlation was 0, it would not be unlikely to get a correlation as large as this&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Any correlation that is not 0 is different from 0. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-09T10:56:13.680" Id="33973" LastActivityDate="2012-08-09T10:56:13.680" OwnerUserId="686" ParentId="33969" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;At the CPU level, the result of adding a bunch of numbers up can depend on the order you do it. I had this happen to me when doing some OpenMP computations. The result wasn't constant, even though there was no randomness in the program. What was happening was that the order in which the OpenMP threads finished was variable, and so the sum computed was being done in a different order, and giving a different result. I eventually tracked this down to a minimal C example that exhibited the behaviour - add the vector up from the first to the last and you got a different answer to if you added it up from the last to the first.&lt;/p&gt;&#10;&#10;&lt;p&gt;Although the differences were tiny, probably one part in 10^10, it had a large effect on my eventual output. This was a maximisation problem, and the tiny difference in slope was sending the maximiser to a different point in the space. After 100 iterations, the maximiser could well be in a very different place just because of the different ordering of an addition in one case. And once the system has moved, there's no way back (especially when the likelihood surface is flat...). Its a sensitive dependence on initial conditions (chaotic) situation.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, even if your programs are implementing the same algorithm correctly, they may still get two different answers. Both are right in some sense, both are wrong in some sense.&lt;/p&gt;&#10;&#10;&lt;p&gt;Whether they are significantly different is what's important. Don't sweat the 0.00001 of a probability difference...&lt;/p&gt;&#10;&#10;&lt;p&gt;Of course with R you can look at the source code and find out how it does it...&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-08-09T18:08:32.433" Id="34009" LastActivityDate="2012-08-09T18:08:32.433" OwnerUserId="1549" ParentId="33987" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;Factor scores are certainly continuous. Whether that makes them interval is an interesting question, but I think they can be treated as interval for most purposes.&lt;/p&gt;&#10;&#10;&lt;p&gt;It's an interesting question because Steven's classification says that something is interval if intervals are the same. Is the difference between a factor score of 1 and 2 the same as between 0 and 1?  Well, it depends on what your definition of &quot;same&quot; is.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-09T20:16:52.757" Id="34018" LastActivityDate="2012-08-09T20:16:52.757" OwnerUserId="686" ParentId="34014" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;The approach is just one of many possible ways to adjust for multiplicity.  You could also apply bootstrap p-value adjustment ala Westfall and Young.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-10T01:24:02.517" Id="34029" LastActivityDate="2012-08-14T10:52:12.540" LastEditDate="2012-08-14T10:52:12.540" LastEditorDisplayName="user10525" OwnerUserId="11032" ParentId="19073" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Cascade-Correlation Neural Networks adjust their structure by adding hidden nodes during the training process, so this may be a place to start. Most of the other work I've seen that automatically adjusts the number of layers, number of hidden nodes, etc, of a neural network use evolutionary algorithms. &lt;/p&gt;&#10;&#10;&lt;p&gt;Unfortunately, this work is out of my area so I can't recommend any particular papers or references to help you get started. I can tell you that I haven't seen any work which tries to jointly optimize network structure and parameters simultaneously within the deep learning community. In fact, most deep learning architectures are based on greedily learning a single layer at a time, thus making even online learning of deep neural networks a rather untouched area (the work of Martens et al. on Hessian Free Optimization being a notable exception). &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-08-10T13:36:41.127" Id="34053" LastActivityDate="2012-08-10T13:36:41.127" OwnerUserId="6248" ParentId="33967" PostTypeId="2" Score="6" />
  
  
  
  
  <row AcceptedAnswerId="34079" AnswerCount="2" Body="&lt;p&gt;I have a timeseries dataset holding stock data for a large set of companies. Assume the following subset, where &lt;code&gt;obsDay&lt;/code&gt; is the observation day (148 days in reality) and &lt;code&gt;weekDay&lt;/code&gt; the day of the week (1 = monday, 2 = tuesday, ...):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;company  sector  obsDay weekDay  stockPrice&#10;-------------------------------------------&#10;1        15      1      3        10.40&#10;1        15      2      4         9.42&#10;1        15      3      5         9.66&#10;1        15      4      1        11.00&#10;1        15      5      2        10.21&#10;2        10      1      3        43.55&#10;2        10      2      4        43.50&#10;2        10      3      5        40.31&#10;2        10      4      1        48.43&#10;2        10      5      2        43.00&#10;3        20      1      3        10.00&#10;3        20      2      4        11.00&#10;3        20      3      5        12.00&#10;3        20      4      1        13.00&#10;3        20      5      2        14.00&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In an OLS regression, I would like to see the effect, for instance, of day of the week and industry sector, on stockprice. It is suggested that I would then have to make dummies for each variable, as follows (for sector):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;company  sector  sector15  sector10  sector20 obsDay weekDay  stockPrice&#10;------------------------------------------------------------------------&#10;1        15      1         0         0        1      3        10.40&#10;1        15      1         0         0        2      4         9.42&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;(and the same for weekDay). But with the many variables I have, this is making the dataset hard to read. Can't SPSS create dummy variables from the &lt;code&gt;sector&lt;/code&gt; and &lt;code&gt;weekDay&lt;/code&gt; categories on the fly when performing the OLS?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-08-10T16:09:33.390" Id="34070" LastActivityDate="2012-08-10T19:45:01.507" OwnerUserId="10776" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;spss&gt;&lt;categorical-data&gt;&lt;least-squares&gt;" Title="SPSS dummy variables in OLS" ViewCount="1447" />
  <row AcceptedAnswerId="34173" AnswerCount="1" Body="&lt;p&gt;I have two continuous predictor variables to predict a dichotomous variable. In addition i have constructed two (interaction) models, based on domain knowledge which use both variables to predict the third.&lt;/p&gt;&#10;&#10;&lt;p&gt;So now i want to compare these predictors, using R. I get these result on the wilcox test:&#10;W = 36655, p-value = 3.896e-09 (single predictor)&#10;W = 29680.5, p-value &amp;lt; 2.2e-16 (model)&lt;/p&gt;&#10;&#10;&lt;p&gt;But i'm wondering if the area under the ROC curve would not be a good (better?) measure then the wilcox or the t-test.&#10;To get a general idea of how well the continuous predictor separates the two groups i plot the two histograms together. But the differences are small and besides, i'd like a 'grade' to tell me which is better and by how much.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-08-10T16:27:05.963" Id="34073" LastActivityDate="2012-08-12T11:40:28.287" OwnerUserId="11151" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;continuous-data&gt;&lt;roc&gt;&lt;wilcoxon&gt;&lt;predictor&gt;" Title="Comparing continuous predictors for a dichotomous variable" ViewCount="194" />
  
  
  
  <row Body="&lt;p&gt;You want to use sampling to say something about whether or not 100% of the population has a certain property?  To get 95% confidence that 100% of the population has the property, you have to sample at least 95% of the population...&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-11T00:14:24.977" Id="34104" LastActivityDate="2012-08-11T00:14:24.977" OwnerUserId="13214" ParentId="34081" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;I am not aware of any &lt;code&gt;R&lt;/code&gt; package that can do this.  You could possibly coerce the &lt;code&gt;lme4&lt;/code&gt; or &lt;code&gt;nlme&lt;/code&gt; packages to do it, if you wrote your own functions for that particular family.  Another (related) possibility is to use AD Model Builder (&lt;a href=&quot;http://www.admb-project.org/examples/survival-analysis/weibull-regression-with-censoring/kidney.tpl/view&quot; rel=&quot;nofollow&quot; title=&quot;ADMB&quot;&gt;ADMB&lt;/a&gt;).  This is a flexible optimizer and forms the basis of the &lt;code&gt;glmmADMB&lt;/code&gt; package in &lt;code&gt;R&lt;/code&gt; (which although it implements quite a few families, does not implement the Weibull distribution).  &lt;/p&gt;&#10;&#10;&lt;p&gt;Perhaps you can explain why this &lt;em&gt;must&lt;/em&gt; use a frequentist approach?  If you really want that, another more &quot;canned&quot; approach could possibly be done in SAS using &lt;code&gt;proc nlmixed&lt;/code&gt; which allows random effects models where you specify your own (log) likelihood function.  I work out an extensive example of this for random coefficient zero-inflated Poisson models &lt;a href=&quot;http://www.ats.ucla.edu/stat/sas/faq/zip_nlmixed.htm&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.  That might not quite work as I have since updated the dataset that is based on (it was a simulated multilevel dataset).&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-08-11T01:24:34.390" Id="34108" LastActivityDate="2012-08-11T01:24:34.390" OwnerUserId="12521" ParentId="34088" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;I am not sure I follow your quartile models but within each model set you can compare models using AIC wieght ($w_i$) for each model $i$:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;w_i = \frac{e^{(-0.5 * \Delta AIC_i)}}{\Sigma e^{(-0.5*\Delta AIC_i)}}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;which can be interpreted as the weight of evidence for each model.&lt;/p&gt;&#10;&#10;&lt;p&gt;If each variable is equally represented in the model set, we can also examine the relative importance of each variable by summing all the AIC wieghts for the models that contian variable $j$ where $j = 1,...,J$ variables.  Call this sum the &quot;importance weight.&quot;  The ratio of imporance weights for two variables gives you an idea of how plausible a variable is.  For example, population density might have an importance weight of 0.9 and distance to forest edge might have an importance weight of 0.3.  Therefore, we could say population density is 3 times more plausible than distance to forest edge.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-11T04:43:24.650" Id="34115" LastActivityDate="2012-08-11T04:43:24.650" OwnerUserId="12318" ParentId="33879" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Yes, It is possible, you may send details so can do specific answer. I will suggest you to used faisalconjoint package of R, It is useful for all type of conjoint data, and testes with many published data and studies.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-11T05:45:52.070" Id="34117" LastActivityDate="2012-08-11T05:45:52.070" OwnerUserId="12777" ParentId="34044" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;The Rasch measurement model is the simplest of the item response theory (IRT) family of measurement models. Due to the simplicity of the model, it allows smaller sample sizes to arrive at stable parameter estimates.  The Rasch model will analyze data that are: 1) dichotomous (right or wrong), 2) polytomous (rating scale or partial credit items, and 3) many-faceted (e.g., raters, items, and students).  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-11T09:05:35.233" Id="34121" LastActivityDate="2013-05-09T18:57:36.250" LastEditDate="2013-05-09T18:57:36.250" LastEditorUserId="805" OwnerUserId="930" PostTypeId="5" Score="0" />
  
  
  <row Body="&lt;p&gt;If the positively-correlated  regressor  is the only  regressor  in a linear model then its coefficient should be positive.  &lt;/p&gt;&#10;&#10;&lt;p&gt;If there are several  regressors and they are not independent then you can see the effect you are asking about.  Read about &lt;a href=&quot;http://en.wikipedia.org/wiki/Confounding&quot;&gt;confounding&lt;/a&gt; for some explanation&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-11T23:58:13.583" Id="34155" LastActivityDate="2012-08-11T23:58:13.583" OwnerUserId="2958" ParentId="34151" PostTypeId="2" Score="7" />
  <row Body="&lt;p&gt;I only add remark about modeling the shape of daily average number of users per minute - you can decompose that curve using sum of two gaussian functions, such sum will have 6 free parameters to fit.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Gaussian_function&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Gaussian_function&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;or father sum of one rescaled gamma/beta distribution function and gaussian function. Gamma or beta seems to be batter choice for that first very flat peak around 14:00.&lt;/p&gt;&#10;&#10;&lt;p&gt;And ahter fitting sum of two functions you can observ how they parameters change depending on the day of weak.&lt;/p&gt;&#10;&#10;&lt;p&gt;For fitting you can try BGFS algorithm, but the problem will be finding staring values of parametres, but in tis case it can be resolved very easy - just cut daily profiles of number of users into two sets - first for &lt;code&gt;10:00-15:00&lt;/code&gt; and second &lt;code&gt;15:00-2:00&lt;/code&gt;m then fit one gaussian or rather gama/beta to first set and one gaussian function to second one, then use fitted parametres as starting values for BFGS for fitting sum of two choosen distributions.&lt;/p&gt;&#10;&#10;&lt;p&gt;But I don't see the point of modeling daily average numbers of users.&#10;The written-on-the-knee model for forecasting number of users is to take weighted average of number of users and daily profiles of average number of users...&#10;I can write more in this topis if you want.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-12T14:03:34.183" Id="34179" LastActivityDate="2012-08-12T14:11:03.877" LastEditDate="2012-08-12T14:11:03.877" LastEditorUserId="4908" OwnerUserId="4908" ParentId="26694" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;The BIC is justified from the Bayesian viewpoint but you din't need to be taking a Bayesian approach to use it.  It is very similar to AIC.  The difference is in the form of the penalty function on the number of parameterrs used. AIC was introduced by Akaike in the mid 1970s.  BIC was introduced by Gideon Schwarz in 1978.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-08-13T00:17:52.700" Id="34190" LastActivityDate="2012-08-13T00:17:52.700" OwnerUserId="11032" ParentId="22445" PostTypeId="2" Score="0" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a problem with a multiple regression I performed:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;model without constant term;&lt;/li&gt;&#10;&lt;li&gt;one dependent continuous variable;&lt;/li&gt;&#10;&lt;li&gt;first set of dummies: derived from 2 continuous variables, I used the median value of them as a threshold to derive two binary variables; from these two binaries, I derived 4 dummies, one for each combination (10, 01, 00, 11);&lt;/li&gt;&#10;&lt;li&gt;second set of dummies: 3 dummies derived from one categorical variable;&lt;/li&gt;&#10;&lt;li&gt;two continuous variables.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;This model has a r-squared value of 98% (and similar adjusted r squared): I think it is too high, but I don't know how to interpret it correctly and assess its eventual validity; I know that r squared tend to increase with the number of explanatory variables, but I don't know if the number of dummies has an influence in its value and validity as an indicator of a good regression.&#10;Moreover, this model present high VIF values, indicating collinearity: are these measures still valid or not?&lt;/p&gt;&#10;&#10;&lt;p&gt;I have to say I have also tested the model with constant term (and $k-1$ and $n-1$ dummies), which has a very low r squared (around 10%) but no collinearity problems: I would use this model if only I could separate the effect of the two reference dummies on the constant term (and I don't know how to do it).&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-08-13T09:54:15.527" FavoriteCount="1" Id="34209" LastActivityDate="2012-08-13T10:54:40.257" LastEditDate="2012-08-13T10:54:40.257" LastEditorUserId="930" OwnerUserId="13183" PostTypeId="1" Score="0" Tags="&lt;multiple-regression&gt;&lt;multicollinearity&gt;&lt;r-squared&gt;" Title="How to interpret R-squared in multiple regression with more sets of dummies and continuous variables?" ViewCount="2187" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I am trying to figure out the best way to test my data for differences in R. The data I have look like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;domain               variable    lifecycle&#10;162                      CSP1         5&#10;162               old Species        10&#10;162                        BD        16&#10;162                       DBH        14&#10;162                    Height        16&#10;162           Height comments        16&#10;162            Other comments         4&#10;162                      Year         5&#10;123                   species        12&#10;123                  den bran        14&#10;123                  den core        14&#10;191                      CSP2         4&#10;191                   C kg ha         5&#10;185                  location         5&#10;185                   biomass         6            &#10; 82                   CSP num         6 &#10; 82                  depth lb         8&#10; 82                       C t         7&#10;190                     Layer         8     &#10;190                Dry weight         9&#10;204                 item plot         5&#10;204               item volume         7&#10;204 within central plot small         7&#10;205                      plot         3&#10;205        successional stage         3&#10;205            rarefy hundred         3&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now I want to use the &quot;variable&quot; column as factorial group and the &quot;lifecycle&quot; as the characteristiom to compare. Because the data is not normally distributed and I have more than two groups to compare I chose a Kruskal-Wallis test, which shows no significant differences.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Kruskal-Wallis rank sum test  &#10;&#10;data:  lifecycle by variable&#10;Kruskal-Wallis chi-squared = 25, df = 25, p-value = 0.4624&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;My question is now if I can use the Kruskal-Wallis in this case to find differences in &quot;lifecycle&quot; for the &quot;variable&quot; groups, or is the sample size to small with only one value per group? &lt;/p&gt;&#10;&#10;&lt;p&gt;And if it is not usable here in this case what would be a better solution?&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT:&lt;/p&gt;&#10;&#10;&lt;p&gt;I did the Kruskal-Wallis now with the domain as grouping factor. Which gives me a significant difference. The interpretation would be that there are domains with significant more or less impact than other. &lt;/p&gt;&#10;&#10;&lt;p&gt;After that I performed a post-hoc test to see between which of the domains the differences occur. But supprisingly the paired wilcoxon test with bonferroni correction showed up no significant grup differences.&#10;How could this be?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-08-13T14:05:54.407" Id="34225" LastActivityDate="2012-08-15T13:44:43.317" LastEditDate="2012-08-14T10:07:30.260" LastEditorDisplayName="user13261" OwnerDisplayName="user13261" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;kruskal-wallis&gt;" Title="Test data for differences" ViewCount="507" />
  <row Body="&lt;p&gt;Yes logistic regression or a loglinear model seem to be the approaches to take to this problem.  The binary outcome is whether or not the respondent stayed drug free.  All the variables that are potential confounders should be included in the model and you should also look at the interaction terms between the confounders and the treatments.  If the treatment effect remains significant when including all these other variables then it would be safe to conclude that there is a real treaatment effect.&lt;/p&gt;&#10;" CommentCount="11" CreationDate="2012-08-13T14:31:03.590" Id="34227" LastActivityDate="2012-08-13T14:31:03.590" OwnerUserId="11032" ParentId="34224" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;Yes, both of your points make sense. To see a derivation of two different flavors of diff-in-diff models, you can see &lt;a href=&quot;http://cgibbons.berkeley.edu/Courses/ECON140_S11/DIDSlides.pdf&quot; rel=&quot;nofollow&quot;&gt;my lecture slides on the topic&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-13T16:09:58.683" Id="34236" LastActivityDate="2012-08-13T16:09:58.683" OwnerUserId="401" ParentId="34177" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;It seems to me you are actually looking for a regression models with &lt;a href=&quot;http://en.wikipedia.org/wiki/Influence_function_%28statistics%29#M-estimators&quot; rel=&quot;nofollow&quot;&gt;re-descending loss function&lt;/a&gt; (&quot;far away points are weighted less than close ones&quot;) loss function. Such loss functions --for example the Tukey biweight-- lead to highly non-convex optimization problems, meaning that there are, potentially, a finite but factorial-order increasing number of potential solution. Clearly, which ever one you will pick up will depend on your &quot;starting weights&quot; (you initial guess about which observations belong to the model you are looking for).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;R&lt;/code&gt; has some facilities to fit such models. An example is the &lt;code&gt;lmrob&lt;/code&gt; function in the &lt;code&gt;robustbase&lt;/code&gt; package. For illustrative reasons I use a univariate example below but in principle the logic extends to higher dimensions. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(robustbase)&#10;#constructing the data --one group at a time&#10;x1&amp;lt;-cbind(1,rnorm(100,0,5))&#10;x2&amp;lt;-cbind(1,rnorm(100,0,5))&#10;b1&amp;lt;-c(-3,2)&#10;b2&amp;lt;-c(4,-2)&#10;y1&amp;lt;-x1%*%b1+rnorm(100)&#10;y2&amp;lt;-x2%*%b2+rnorm(100)&#10;#mixing the data&#10;x&amp;lt;-rbind(x1,x2)&#10;y&amp;lt;-c(y1,y2)&#10;sam1&amp;lt;-sample(1:100,3)  &#10;bet1&amp;lt;-lm(y[sam1]~x[sam1,]-1)&#10;ini1&amp;lt;-list(coefficients=as.numeric(coef(bet1)),scale=sd(bet1$resid))&#10;ctrl&amp;lt;-lmrob.control()&#10;mod1&amp;lt;-lmrob(y~x-1,init=ini1)&#10;&#10;plot(x[,2],y,pch=16)&#10;points(x[sam1,2],y[sam1],col=&quot;red&quot;,pch=16)&#10;abline(mod1$coef,col=&quot;red&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Ibquk.png&quot; alt=&quot;the red points mark the initial guess of $d+1$ points from the same relationship. The red line is the final fitted regression function&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, as I mentioned, fitting these types of model is actually a highly non-convex problem. The final solutions will depend on the quality of your starting points. In the example above, my &quot;guess&quot; that the original points (marked in red in the plot above) belonged to the same relationship was correct. In general, specially in higher dimensions, this is a highly questionable assumption. None-withstanding this, if your initial guess is correct, the biweight loss function enabled an appreciable gain over the naive solution (the OLS line passing by the 3 initial points). &lt;/p&gt;&#10;&#10;&lt;h1&gt;EDIT:&lt;/h1&gt;&#10;&#10;&lt;p&gt;I forgot to mention that you have a parameter that determines the width of the area for which the bi-weight gives non-zero weights.  This is the&#10; &lt;code&gt;tuning.psi&lt;/code&gt; parameter of the &lt;code&gt;lmrob.control&lt;/code&gt; object.&lt;/p&gt;&#10;&#10;&lt;p&gt;There is a tradeoff between efficiency and the width of the biweight: the wider the biweight, the more points you effectively use, the more efficiency you get. A limiting case is a biweight that never re-descends (maximal width) which would give you the OLS solution. Adjusting the width is done by:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ctrl$tuning.psi&amp;lt;-1.35&#10;mod1&amp;lt;-lmrob(y~x-1,init=ini1,control=ctrl)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;If you set the &lt;code&gt;ctrl$tuning.psi&lt;/code&gt; too small, you will get convergence problems. This can be solved by increasing the &lt;code&gt;max.it&lt;/code&gt; value of the control object:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ctrl$max.it&amp;lt;-500&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;There is a whole theory on optimal values of the tunning constant, but it only applies if the sub-population you are targeting has more than half of the observations. I gather this is not the case you are concerned with. If this is the case, I think it is best is to play with to get a handle on it. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-08-13T17:33:52.653" Id="34241" LastActivityDate="2012-08-14T12:31:34.117" LastEditDate="2012-08-14T12:31:34.117" LastEditorUserId="603" OwnerUserId="603" ParentId="34237" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;There are a number of modelling options to account for a non-constant variance, for example ARCH (and GARCH, and their many extensions) or stochastic volatility models.&lt;/p&gt;&#10;&#10;&lt;p&gt;An ARCH model extend ARMA models with an additional time series equation for the square error term. They tend to be pretty easy to estimate (the fGRACH R package for example).&lt;/p&gt;&#10;&#10;&lt;p&gt;SV models extend ARMA models with an additional time series equation (usually a AR(1)) for the log of the time-dependent variance. I have found these models are best estimated using Bayesian methods (OpenBUGS has worked well for me in the past).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-13T18:24:24.867" Id="34246" LastActivityDate="2012-08-13T18:24:24.867" OwnerUserId="13267" ParentId="34229" PostTypeId="2" Score="6" />
  <row AcceptedAnswerId="34267" AnswerCount="1" Body="&lt;p&gt;Last month I asked this question &lt;a href=&quot;http://stats.stackexchange.com/questions/31597/graphing-a-probability-curve-for-a-logit-model-with-multiple-predictors&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;After thinking about it recently, I was wondering if it makes sense to think about logit probabilities in that regards. Since the predictor of a coefficient shows the log odds change in the response variable independent of all other predictors, we would expect that plotting bid vs pr(outcome), with the curve representing a different predictor is simply not useful. So if the coefficient for variable x is 0.5, that would be the log odds change regardless of the values for y, z, or f. Therefore, I'm wondering if it makes sense to make such a graph.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Am I thinking about logistic regression correctly? Since logit coefficients are independent of the other predictors, wouldn't a plot like that be largely &quot;useless.&quot;&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;If that is the case, what should be the main use for predicted probabilities when using logit models?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Just some sample code if you wish: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;df=data.frame(income=c(5,5,3,3,6,5),&#10;              won=c(0,0,1,1,1,0),&#10;              age=c(18,18,23,50,19,39),&#10;              home=c(0,0,1,0,0,1))&#10;str(df)&#10;&#10;md1 = glm(factor(won) ~ income + age + home, &#10;          data=df, family=binomial(link=&quot;logit&quot;))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-08-13T21:57:37.877" Id="34263" LastActivityDate="2012-08-14T00:30:54.873" LastEditDate="2012-08-13T23:21:25.373" LastEditorUserId="3310" OwnerUserId="3310" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;logistic&gt;" Title="Predicted Probabilities for Logit Models" ViewCount="448" />
  <row AnswerCount="2" Body="&lt;p&gt;It seems when I glance around here at the fashionable learning algorithms, things like neural networks, boosted trees, support vector machines, random forests, and friends are promoted for supervised learning problems. Dirichlet processes and their ilk seem to be mentioned mostly in unsupervised learning problems, such as document or image clustering. I do see them get used for regression problems, or as general purpose priors when one wants to do Bayesian statistics in a nonparametric or semiparametric way (e.g. as a flexible prior on the distribution of random effects in certain models) but my limited experience suggests that this doesn't come as much from the machine learning crowd as it does from more traditional statisticians. I've done a small amount of googling on this and I've found a few definitive uses in machine learning of DPs for supervised learning, but it seems like whenever I need to look up something about (say) hierarchical DPs, whatever paper I find the answer in is using it for unsupervised learning.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, are Dirichlet processes and their cousins most effective as priors for flexible clustering models? Are they not competitive with boosting, SVMs, and neural networks for supervised learning problems? Are they useful only in certain situations for these problems? Or is my general impression incorrect?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-08-14T02:51:27.357" FavoriteCount="3" Id="34272" LastActivityDate="2012-11-19T08:51:26.107" OwnerUserId="5339" PostTypeId="1" Score="6" Tags="&lt;dirichlet-process&gt;" Title="Dirichlet processes for supervised learning?" ViewCount="313" />
  
  <row Body="&lt;p&gt;Consider, a classic example of the following rule: &lt;/p&gt;&#10;&#10;&lt;p&gt;IF (patient is pregnant) THEN (patient is female). &lt;/p&gt;&#10;&#10;&lt;p&gt;This rule is very accurate and comprehensible, but it is not interesting, since it represents the obvious. &#10;Another Example from real world data set,&lt;/p&gt;&#10;&#10;&lt;p&gt;IF (used_seat_belt = ‘yes’) THEN (injury = ‘no’).......................................................(1)&lt;/p&gt;&#10;&#10;&lt;p&gt;IF ((used_seat_belt = ‘yes’) Λ (passenger = child)) THEN (injury = ‘yes’)...............(2)&lt;/p&gt;&#10;&#10;&lt;p&gt;Rule (1) is a general and an obvious rule. But rule (2) contradicts the knowledge represented by rule (1) and so the user's belief. This kind of knowledge is unexpected from users preset beliefs and it is always interesting to extract this interesting (or surprising) knowledge from data sets.&#10; “Unexpectedness” means knowledge which is unexpected from the beliefs of users i.e. A decision rule is considered to be interesting (or surprising) if it represents knowledge that was not only previously unknown to the users but also contradicts the original beliefs of the users.&lt;/p&gt;&#10;&#10;&lt;p&gt;I hope, these examples may help you to understand the concept more clearly.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;&lt;br&gt;&#10;Yes, firstly, discover the general rules and then discover exceptions to these general rules.&#10;For example,&lt;/p&gt;&#10;&#10;&lt;p&gt;A general rule : If &lt;em&gt;bird&lt;/em&gt; then &lt;em&gt;fly&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;However, there are few exceptional birds like emu and penguin that do not fly. It would definitely be valuable to discover such exceptions along with the rule, making the rule more accurate, comprehensible as well as interesting.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-08-14T07:44:39.867" Id="34274" LastActivityDate="2012-08-16T21:34:15.513" LastEditDate="2012-08-16T21:34:15.513" LastEditorUserId="930" OwnerUserId="13287" ParentId="18855" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;Yes-dependent on where the optimal cross-validated value of $\lambda$ lies-which is dependent on the true joint distribution that generates the features and the response (interpret as problem dependent) and the class of functions $\Omega$ such that $f(.) \subseteq \Omega$ (interpret as the kind of model that is being used to fit).  As you move from underfitting to the optimal point, the test-error and training-error continue to decrease. Then you reach the optimal $\lambda^*$, after which the Test-error begins to increase, while the training-error continues to decrease- indicating an over-fitting - that is not generalizing the model enough, so as to perform optimally on out-of-sample (unseen) data.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-14T14:25:06.153" Id="34298" LastActivityDate="2012-08-14T14:25:06.153" OwnerUserId="6897" ParentId="34291" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;Pills against the &quot;megaphone effect&quot; include (among others):&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Use log or square root transform  $Y$. This is not exact but sometimes it tames the widening.&lt;/li&gt;&#10;&lt;li&gt;Use &lt;a href=&quot;http://en.wikipedia.org/wiki/Least_squares#Weighted_least_squares&quot;&gt;weighted least square regression&lt;/a&gt;. In this approach, each observation is given its own variance factor. &lt;a href=&quot;http://stackoverflow.com/questions/6375650/function-for-weighted-least-squares-estimates&quot;&gt;This answer&lt;/a&gt; shows how to use WLSR in R (for instance if the variance of the residuals is proportional to the means, you can provide as weights the inverse of the fitted value in the unweighted model).&lt;/li&gt;&#10;&lt;li&gt;Use robust regression. The funciton &lt;code&gt;rlm()&lt;/code&gt; in the &lt;code&gt;MASS&lt;/code&gt; package of R does M-estimation, which is supposed to be robust to inequality of variances.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="1" CreationDate="2012-08-14T22:41:18.280" Id="34328" LastActivityDate="2012-08-14T22:41:18.280" OwnerUserId="10849" ParentId="34325" PostTypeId="2" Score="11" />
  <row Body="&lt;p&gt;I don't entirely understand question 1. Are you asking when to use numeric versus factor values? Factor values should be used for categorical data (discrete units that are not in any specific order), numeric should be used for continuous, ratio, or (some) interval level data. In the equation above, age should be numeric, home (if dichotomous) won't matter if it is factor or not, and income would likely be factor (though you could make a reasonable interpretation with numeric if the factors are equally spaced and distributed).&lt;/p&gt;&#10;&#10;&lt;p&gt;To know if you should be using factors, consider the following question: does a partial count (e.g. 0 &amp;lt; x &amp;lt; 1) make sense as a result? Treating non-numeric data as numeric is what gives us our famous 2.4 children.&lt;/p&gt;&#10;&#10;&lt;p&gt;For question 2, if you have a sample that is limited to the two genders (e.g. all respondents are male or female) you'll shouldn't be able to get a coefficient for male and female from the equation. One of them will be the reference variable, meaning that it is represented as part of the constant. So, the effect of being male in your equation would be:&lt;/p&gt;&#10;&#10;&lt;p&gt;y=bx(male) + BX(other covariates) + a(constant) + e. The result for male should be the effect of being male controlling for other covariates. If you take male out and put in female, the number should be of the same magnitude but in the other direction (assuming your model does not allow for any interaction between the covariates).&lt;/p&gt;&#10;&#10;&lt;p&gt;c.f. &lt;a href=&quot;http://www.ats.ucla.edu/stat/mult_pkg/whatstat/nominal_ordinal_interval.htm&quot; rel=&quot;nofollow&quot;&gt;http://www.ats.ucla.edu/stat/mult_pkg/whatstat/nominal_ordinal_interval.htm&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-14T23:06:27.063" Id="34329" LastActivityDate="2012-08-14T23:06:27.063" OwnerUserId="13066" ParentId="34319" PostTypeId="2" Score="2" />
  <row AnswerCount="0" Body="&lt;p&gt;Using the survival package in R I am trying to calculate 95% confidence interval (CI) of the proportional hazards model coefficient. Normally this is pretty simple to do with the survival package.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; confint(cch(Surv(time, event) ~ a + b + c, ...))&#10;&#10;         2.5 %    97.5 %&#10;a   -0.4216482 0.1225872&#10;b    0.2624281 1.3339170&#10;c    0.3344311 1.2224415&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;However when I try to do this in a univariable setting, the confint function fails to return anything, except a matrix with no rows.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; confint(cch(Surv(time, event) ~ a, ...))&#10;&#10;         2.5 %    97.5 %&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Is this a bug in R or am I missing something else? If this is a bug how can I calculate these CIs 'by hand'?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-08-14T23:26:34.153" Id="34330" LastActivityDate="2012-08-14T23:26:34.153" OwnerUserId="7037" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;confidence-interval&gt;&lt;survival&gt;&lt;cox-model&gt;&lt;case-cohort&gt;" Title="R CCH CI Univariable Analysis" ViewCount="72" />
  
  
  
  
  <row Body="&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;Now that you edited your question, I should add that the procedure of Tonry &amp;amp; Davis that I describe below is pretty general. In fact, appart from reducing the squared difference between the convoluted, shifted and scaled template and the observed spectrum (which can be thought of as a Maximum Likelihood estimator of the parameters with equal errors, which can be accounted for [see my comments]) the only part where they make strong assumptions is on the shape of the convolution of the template and on the shape of the largest peak in the CCF. You described that your objects may have different shapes; have you tried actually plotting the shape of the CCF? Maybe an extended gaussian or a Moffat function are more suitable choices in your case in order to model the 2D CCF peak. Analytically tracking the shape of the CCF, I think, is the easier path; this would allow you to analytically track the errors on the CCFs.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;My original answer:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;At least in astronomy we &quot;have&quot; a standard way of doing this, and it's the method of &lt;a href=&quot;http://articles.adsabs.harvard.edu/cgi-bin/nph-iarticle_query?1979AJ.....84.1511T&amp;amp;data_type=PDF_HIGH&amp;amp;whole_paper=YES&amp;amp;type=PRINTER&amp;amp;filetype=.pdf&quot; rel=&quot;nofollow&quot;&gt;Tonry &amp;amp; Davis (1979)&lt;/a&gt;. The part that may interest you in that paper is Section III, but let me explain to you a little of the background of their method in order to have an easier reading :-).&lt;/p&gt;&#10;&#10;&lt;p&gt;The idea in astronomy is that you have a source that emits certain flux (energy per meter$^2$ per second) at different wavelengths, say, $f_{\text{source}}(\lambda)$: this is called the &lt;strong&gt;spectrum of the source&lt;/strong&gt;. However, because the objects that Tonry &amp;amp; Davis measure (galaxies, by the way) are receding from our point of view in our galaxy, all the known features of this object (say, bumps at certain wavelengths because of atomic absorption or emission) are generally shifted towards the red (i.e., we see them at &lt;strong&gt;longer&lt;/strong&gt; wavelengths. For example, if we expected to see a bump at $\lambda=4500$, we may actually see it at $\lambda_{\text{source}}=4510$). Their work, then, is focused on calculating this wavelength shift and, of course, measuring the error on this shift. &lt;/p&gt;&#10;&#10;&lt;p&gt;Note that in their work, Tonry &amp;amp; Davis make a conversion between wavelength and bins (because we bin this flux as a function in wavelength in pixels; both to have higher signal to noise ratio and because a CCD camera is the best instrument to date for measuring flux) and from there they measure this wavelength shift. Maybe in your case you don't actually need this conversion between the bins Tonry &amp;amp; Davis talk about and wavelength, so you may want to change equation (1) in their paper to suit your needs. Another important feature of this paper is that they weight any deviations from zero quadratically, because these &quot;bumps&quot; I was talking about in the first paragraph of this answer are clearly more important. However, you can modify their $\chi^2$ reduction scheme to suit your needs ;-). &lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, note that &quot;all they do&quot; is to approximate the largest peak in the Cross-Correlation Function (CCF) by a gaussian, and from there do the error analysis. In practice, I've seen this approximation to work pretty well, but always check the shape of your CCF, just in case.&lt;/p&gt;&#10;&#10;&lt;p&gt;PS: I didn't post this answer on that post you cited because JBWhitmore appeared to work on astronomy (or be an astronomer), and almost every astronomer I know knows the paper of Tonry &amp;amp; Davis, so I thought he was searching for something else.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-08-15T16:40:56.557" Id="34372" LastActivityDate="2012-08-21T20:48:18.090" LastEditDate="2012-08-21T20:48:18.090" LastEditorUserId="9174" OwnerUserId="9174" ParentId="34362" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Most often this is because you did not tell R that label is a categorical variable.  The &lt;code&gt;read.csv&lt;/code&gt; function tries to guess what type to use for each column and if it looks like a numeric column then that is what it will use.  You can check how R stores the variable using the &lt;code&gt;str&lt;/code&gt; function.  You can force &lt;code&gt;read.csv&lt;/code&gt; to read the variable as a factor (or numeric, or ...) using the &lt;code&gt;colClasses&lt;/code&gt; argument.  Or you can change labels to a factor after reading it in using the &lt;code&gt;factor&lt;/code&gt; function.&lt;/p&gt;&#10;&#10;&lt;p&gt;If this is not the case then we need more information about your data.  The results from running &lt;code&gt;str&lt;/code&gt; on your data frame would probably be useful.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-15T17:10:29.850" Id="34375" LastActivityDate="2012-08-15T17:10:29.850" OwnerUserId="4505" ParentId="34363" PostTypeId="2" Score="3" />
  
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;... data are evaluated as they are collected, and further sampling is stopped in accordance with a pre-defined stopping rule as soon as significant results are observed. &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;--http://en.wikipedia.org/wiki/Sequential_analysis&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-15T18:19:49.387" Id="34385" LastActivityDate="2012-08-15T18:19:49.387" LastEditDate="2012-08-15T18:19:49.387" LastEditorUserId="919" OwnerUserId="919" PostTypeId="5" Score="0" />
  <row Body="Hypothesis testing without a fixed sample size." CommentCount="0" CreationDate="2012-08-15T18:19:49.387" Id="34386" LastActivityDate="2012-08-15T18:19:49.387" LastEditDate="2012-08-15T18:19:49.387" LastEditorUserId="919" OwnerUserId="919" PostTypeId="4" Score="0" />
  
  
  
  
  <row Body="&lt;p&gt;You will have to do it by hand. The &lt;code&gt;forecast.stl()&lt;/code&gt; function fits an ARIMA model to the de-seasonalized data, forecasts it, and then re-seasonalizes the forecasts. It is not set up to handle model regressors. If you include an &lt;code&gt;xreg&lt;/code&gt; term it will use it when fitting the model, but then it will complain when forecasting because it won't know how to deal with future regressors. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;forecast.stl()&lt;/code&gt; is a very simple function and could easily be modified to handle regressors. All you need is one extra argument for future regressors, and then change the line that calls &lt;code&gt;forecast()&lt;/code&gt; with the fitted ARIMA model to include the future regressors.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-08-16T01:04:31.543" Id="34411" LastActivityDate="2012-08-16T01:04:31.543" OwnerUserId="159" ParentId="34226" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Just thought I'd add an example of how to calculate the &#10;normalising constant.&#10;If you know the beta integral, then its easier to use that for direct integration.  With a change of variables in the usual definition you get&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\int_{L}^{U}(x-L)^{a-1}(U-x)^{b-1}dx=(U-L)^{a+b-1}B(a,b)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The change in variables is $t=\frac{x-L}{U-L}$ and you get back to the standard definition of the beta integral.  To apply this to the calculation of Z we must first determine the limits of integration.  This is simple for the simplex as the parameters must all be positive and sum to 1.  So we have&#10; $$0\leq\theta_1\leq 1$$ &#10;$$0\leq\theta_i\leq 1-\sum_{j=1}^{i-1}\theta_j\;\;\; i=2,\dots,n-1$$&#10;$$\theta_n=1-\sum_{j=1}^{n-1}\theta_j $$&lt;/p&gt;&#10;&#10;&lt;p&gt;This assumes that we integrate in the order $\theta_n,\theta_{n-1},\dots,\theta_1$.  The order of integration doesn't matter, but this order is easier to write down.The first integral is a substitution so we have for the second integral.&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\int_{0}^{1-\sum_{j=1}^{n-2}\theta_j}\left[\prod_{k=1}^{n-2}\theta_{k}^{\alpha_k-1}\right]\theta_{n-1}^{\alpha_{n-1}-1}\left( 1-\sum_{j=1}^{n-2}\theta_j - \theta_{n-1}\right)^{\alpha_n-1}d\theta_{n-1}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;This is of the form of the transform beta integral with $L=0$ and $U= 1-\sum_{j=1}^{n-2}\theta_j $  hence we get:&#10; $$\left[\prod_{k=1}^{n-2}\theta_{k}^{\alpha_k-1}\right]B(\alpha_n,\alpha_{n-1})\left( 1-\sum_{j=1}^{n-2}\theta_j \right)^{\alpha_n+\alpha_{n-1}-1}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Now we apply this again to the integral over $\theta_{n-2}$.  It is another transformed beta integral but with $U= 1-\sum_{j=1}^{n-3}\theta_j$.  Hence we get&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\left[\prod_{k=1}^{n-3}\theta_{k}^{\alpha_k-1}\right]B(\alpha_n,\alpha_{n-1}) B(\alpha_n+\alpha_{n-1},\alpha_{n-2}) \left( 1-\sum_{j=1}^{n-3}\theta_j \right)^{\alpha_n+\alpha_{n-1}+\alpha_{n-2}-1}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;It is now straight forward to repeatedly apply this and you get&lt;/p&gt;&#10;&#10;&lt;p&gt;$$Z= B(\alpha_n,\alpha_{n-1}) B(\alpha_n+\alpha_{n-1},\alpha_{n-2}) B(\alpha_n +\alpha_{n-1}+\alpha_{n-2} ,\alpha_{n-3}) \dots B(\alpha_n+\dots+\alpha_{2},\alpha_1)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;If you plug in the relation between the beta and gamma integrals $B(a,b)=\frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}$ you get the correct normalising constant.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-16T01:20:09.770" Id="34412" LastActivityDate="2012-08-16T05:06:57.803" LastEditDate="2012-08-16T05:06:57.803" LastEditorUserId="2392" OwnerUserId="2392" ParentId="34379" PostTypeId="2" Score="4" />
  <row AnswerCount="0" Body="&lt;p&gt;Continuing on my exploration of the log-normal distribution, I'm working on reimplementing some code originally written for a Weibull/Exponential model for a log-normal model. Among the things it does is use SAS's PROX NLMIXED to hard-calculate some likelihoods. While this is technically SAS code, I think it's pretty readable in a code-agnostic fashion.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, the Weibull Version:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;*Linear Predictor of Event A&#10;lam=exp(-(f0*alpha+f1*alpha*X));&#10;&#10;*Density of Event A;&#10;ff1=alpha*lam*t1**(alpha-1)*exp(-lam*t1**alpha);&#10;ff2=alpha*lam*t2**(alpha-1)*exp(-lam*t2**alpha);&#10;&#10;*Survival function of Event A;&#10;sf1=exp(-lam*t1**alpha);&#10;sf2=exp(-lam*t2**alpha);&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The two survival functions are just at two different points in time for interval censoring.&lt;/p&gt;&#10;&#10;&lt;p&gt;I've implemented, I believe, the appropriate survival and density functions for a Log-Normal as follows:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;*Density of EventA;&#10;ff1 = exp(-0.5*((log(t1)-mu)/sigma)**2)/((t1*(2*CONSTANT('PI'))**0.5)*sigma);&#10;ff2 = exp(-0.5*((log(t2)-mu)/sigma)**2)/((t2*(2*CONSTANT('PI'))**0.5)*sigma);&#10;&#10;*Survival function of EventA;&#10;sf1= 1 - CDF('Normal',((log(t1)-mu)/sigma));&#10;sf2= 1 - CDF('Normal',((log(t2)-mu)/sigma));&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I guess the first part of the question is does that appear correct? My second question is where I'm getting caught up, actually defining the linear predictor for the log normal. I think it should be something along the form of:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;mu = exp(f0+f1*X)&lt;/code&gt; ala the Weibull above and with sigma estimated outside the linear predictor, but I'm not positive. Can someone point me in the right direction?&lt;/p&gt;&#10;&#10;&lt;p&gt;Edit: The goal PDF once the syntax errors are fixed is:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/us46d.gif&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2012-08-16T03:08:01.263" Id="34414" LastActivityDate="2012-08-16T19:18:11.237" LastEditDate="2012-08-16T19:18:11.237" LastEditorUserId="88" OwnerUserId="5836" PostTypeId="1" Score="4" Tags="&lt;sas&gt;&lt;survival&gt;&lt;lognormal&gt;" Title="Specifying the linear predictor, survival function and PDF of a log-normal survival distribution" ViewCount="214" />
  <row AcceptedAnswerId="65277" AnswerCount="1" Body="&lt;p&gt;I've learned how to use some packages like &lt;code&gt;neuralnet&lt;/code&gt; and &lt;code&gt;caret&lt;/code&gt; to create models based on designed experiments and am getting to the point where I think my models are relatively decent.&lt;/p&gt;&#10;&#10;&lt;p&gt;There's a program called &lt;a href=&quot;http://www.3ds.com/products/simulia/portfolio/isight-simulia-execution-engine/overview/&quot; rel=&quot;nofollow&quot;&gt;ISight&lt;/a&gt; that some others at my company use for radial basis function analysis. I prefer R for a bunch of reasons, but one of the features it has is an &quot;optimizer.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;This lets you input a desired value for your response(s) and it chugs away and produces it's best suggestion of what input variable values will achieve this output. It also allows you to weight each output response importance (if you have multiple responses) and will drive toward a weighted/balanced response set based on your ranking.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm wondering if there's something like this for R?&lt;/p&gt;&#10;&#10;&lt;p&gt;I could just generate a huge data frame of various combinations, run it through the &lt;code&gt;predict&lt;/code&gt; functions of my models and then look for the responses around my target value, which will tell me at what approximate input values I can achieve that value?&lt;/p&gt;&#10;&#10;&lt;p&gt;That will probably be fine for one response, but when I start trying to model multiple responses it will be much more difficult to figure out which input levels optimize across the whole array of desired output values.&lt;/p&gt;&#10;&#10;&lt;p&gt;Feel free to provide any terminology used to describe this. I'm new to the area and only know that ISight calls it the &quot;optimizer.&quot; My google searches may have been poor due to not knowing what to call what I'm looking for.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-08-16T03:23:37.213" Id="34415" LastActivityDate="2013-07-23T15:27:31.030" OwnerUserId="9378" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;predictive-models&gt;&lt;neural-networks&gt;&lt;optimization&gt;" Title="Optimization of models (ANN, radial basis, etc.) in R to target predictor levels to produce a desired response" ViewCount="197" />
  
  <row Body="&lt;p&gt;Finite moving average processes have infinite autoregressive representations if they satisfy the invertibility conditions given in Box, Jenkins and Reinsel (1994) page 70. Mixed models such as ARMA(1,1) have both infinite moving average and infinite autoregressive representations if they are stationary and satisfy invertibility conditions. This is also shown in Box, Jenkins and Reinsel (1994) pp. 77-78 where you can see how the representations are constructed.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-16T14:59:07.067" Id="34447" LastActivityDate="2012-08-16T14:59:07.067" OwnerUserId="11032" ParentId="34446" PostTypeId="2" Score="1" />
  
  
  <row AcceptedAnswerId="34473" AnswerCount="2" Body="&lt;p&gt;I did not see that explicitly mentioned, even though I think it is correct.&lt;/p&gt;&#10;&#10;&lt;p&gt;Isn't the exchangeability assumption the most common assumption about examples in the Bayesian setting?&lt;/p&gt;&#10;&#10;&lt;p&gt;I am thinking of a model of the form $p(x_1,\ldots,x_n,\theta) = p(\theta) \prod_{i=1}^n p(x_i \mid \theta)$, where $p(\theta)$ is a prior. By deFinetti's theorem, I think this means that the $x_i$ exchangeable.&lt;/p&gt;&#10;&#10;&lt;p&gt;Having a prior this way is very common in the Bayesian setting. Hence my conclusion. Is there anything wrong with my reasoning?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-08-16T16:55:20.493" Id="34459" LastActivityDate="2012-08-16T19:27:23.620" LastEditDate="2012-08-16T19:18:31.893" LastEditorUserId="88" OwnerUserId="13002" PostTypeId="1" Score="2" Tags="&lt;bayesian&gt;" Title="Bayesian models and exchangeability" ViewCount="183" />
  <row Body="&lt;p&gt;No, I think your reasoning is right.  Exchangeability was a veryn important property to deFinetti in his development of probability theory (which is Bayesian).  It also is important regarding permutation tests.  Often in doing statistical inference we assume observations are independent and identically distributed and this of course implies exchangeability.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-08-16T17:03:36.127" Id="34460" LastActivityDate="2012-08-16T17:03:36.127" OwnerUserId="11032" ParentId="34459" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;glmnet (in the package of the same name) can handle multinomial outcomes if you pass it the argument family=&quot;multinomial&quot;. The glmnet help also has more information about how it fits multinomial models and how to do grouped lasso penalization.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-08-17T00:28:16.190" Id="34491" LastActivityDate="2012-08-17T00:28:16.190" OwnerUserId="12379" ParentId="34445" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;SPSS removes cases &lt;a href=&quot;http://en.wikipedia.org/wiki/Listwise_deletion&quot; rel=&quot;nofollow&quot;&gt;list-wise&lt;/a&gt; by default, and in my experience this is the case for the majority of statistical procedures. So if a case is missing data for any of the variables in the analysis it will be dropped entirely from the model. For generating correlation matrices or linear regression you &lt;em&gt;can&lt;/em&gt; exclude cases pair-wise if you want (I'm not sure if that is ever really advised), but for logistic and generalized linear model regression procedures this isn't an option. Hence you may want to look at techniques for imputing missing data.&lt;/p&gt;&#10;&#10;&lt;p&gt;Below are some resources I came up quickly for missing data analysis in SPSS;&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;User &lt;a href=&quot;http://stats.stackexchange.com/users/3277/ttnphns&quot;&gt;ttnphns&lt;/a&gt; has a macro for hot-deck imputation on his &lt;a href=&quot;http://rivita.ru/spssmacros_en.shtml&quot; rel=&quot;nofollow&quot;&gt;web site&lt;/a&gt;. I also see &lt;a href=&quot;http://www.afhayes.com/spss-sas-and-mplus-macros-and-code.html&quot; rel=&quot;nofollow&quot;&gt;Andrew Hayes&lt;/a&gt; has a macro for hot-deck imputation.&lt;/li&gt;&#10;&lt;li&gt;Raynald Levesque's site has a set of example syntax implementations of various &lt;a href=&quot;http://spsstools.net/SampleSyntax.htm#WorkingWithMissingValues&quot; rel=&quot;nofollow&quot;&gt;missing values procedures&lt;/a&gt;. Including another implementation of hot-deck imputation!&lt;/li&gt;&#10;&lt;li&gt;SPSS has various tools in-built for imputing missing values. See the commands &lt;code&gt;MVA&lt;/code&gt;, &lt;code&gt;RMV&lt;/code&gt;, and &lt;code&gt;MULTIPLE IMPUTATION&lt;/code&gt;. See the &lt;a href=&quot;http://publib.boulder.ibm.com/infocenter/spssstat/v20r0m0/index.jsp?topic=%2Fcom.ibm.spss.statistics.help%2Fidh_miss.htm&quot; rel=&quot;nofollow&quot;&gt;Missing Values Analysis&lt;/a&gt; section in the HELP documentation.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I'm not quite sure what is available in base and what are available as add-ons. I believe the &lt;code&gt;MULTIPLE IMPUTATION&lt;/code&gt; &lt;strike&gt;command is an add-on, but the others are part of the base package.&lt;/strike&gt; and the &lt;code&gt;MVA&lt;/code&gt; commands are add-ons, but the &lt;code&gt;RMV&lt;/code&gt; procedure is part of the base package. &lt;/p&gt;&#10;&#10;&lt;p&gt;For more general questions about missing data analysis, peruse the tag &lt;a href=&quot;/questions/tagged/missing-data&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged 'missing-data'&quot; rel=&quot;tag&quot;&gt;missing-data&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-17T14:49:24.483" Id="34519" LastActivityDate="2012-08-17T15:05:45.300" LastEditDate="2012-08-17T15:05:45.300" LastEditorUserId="1036" OwnerUserId="1036" ParentId="34494" PostTypeId="2" Score="3" />
  <row AnswerCount="3" Body="&lt;p&gt;Could you give me some clarification about data mining and artificial intelligence algorithms? What mathematics base they used for? Could you give me starting point, in mathematics, to understand these types of algorithms? &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-08-17T07:27:49.003" FavoriteCount="1" Id="34537" LastActivityDate="2012-08-18T09:34:30.290" LastEditDate="2012-08-17T21:08:14.797" LastEditorUserId="930" OwnerDisplayName="Zagorulkin Dmitry" PostTypeId="1" Score="11" Tags="&lt;data-mining&gt;&lt;algorithms&gt;&lt;references&gt;" Title="Mathematics base for data mining and artificial intelligence algorithms" ViewCount="357" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I understand that there is a function in R called &lt;code&gt;poly()&lt;/code&gt; that can generate orthogonal polynomials--useful for applying on input variables before running a predictive model.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is that what is the role of categorical variables when we generate polynomials? Are they to be excluded?&lt;/p&gt;&#10;&#10;&lt;h2&gt;Update:&lt;/h2&gt;&#10;&#10;&lt;p&gt;Dan, Thank you for your kind response. I'm not sure I understand it completely - let me explain the query in more detail. I'm trying to run logistic regression using glmnet on &lt;a href=&quot;http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic3.xls&quot; rel=&quot;nofollow&quot;&gt;Titanic dataset&lt;/a&gt;. &lt;BR/&gt; Let us assume shortened set of columns:&lt;ul&gt;* class(factor with three levels 1, 2 ,3), &lt;br/&gt;* sex(factor: male, female), &lt;br/&gt;* Age (integer), &lt;br/&gt;*survived(factor &amp;amp; target variable 0 or 1).&lt;/ul&gt; The questions is it meaningful to create polynomial features based on these factors? e.g. class. If yes could you pls explain what it means? &lt;BR&gt; I've seen examples with numeric input variables, where one can pass the entire input set to the poly() function and get polynomial features as output. &lt;br/&gt; Your response is highly appreciated.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-08-17T18:07:16.650" Id="34549" LastActivityDate="2012-08-18T19:50:17.533" LastEditDate="2012-08-18T19:50:17.533" LastEditorUserId="13411" OwnerDisplayName="Seth" OwnerUserId="13411" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;regression&gt;&lt;categorical-data&gt;&lt;polynomial&gt;" Title="What is the role of a categorical predictor in polynomial regression?" ViewCount="620" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;On the Mathematics site an OP who is just learning statistics gave his description of the difference between linear regression and ANOVA and asked if his interpretation was correct.  I responded that linear regression considers how a set of covariates relate to a response in a functional form (could have added &quot;that is linear in the parameters&quot;) whereas ANOVA categorizes the response into a class or classes of group(s) and tests for a difference between group means.  A member downvoted my answer saying that ANOVA can include continuous predictors as well.  His own answer indicated that he was considering the term ANOVA to mean the testing of significant terms from the decomposition of variance in the general linear model. We both gave descriptions of linear regression that agreed.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is:  &quot;What do you think is the best answer?  His answer?  My answer?  An explanation providing the two meanings of ANOVA?  Something else?&lt;/p&gt;&#10;&#10;&lt;p&gt;Link: &lt;a href=&quot;http://math.stackexchange.com/questions/183704&quot;&gt;http://math.stackexchange.com/questions/183704&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="16" CreationDate="2012-08-18T12:34:14.470" FavoriteCount="1" Id="34582" LastActivityDate="2012-12-04T21:18:20.267" LastEditDate="2012-08-18T14:55:35.743" LastEditorUserId="11032" OwnerUserId="11032" PostTypeId="1" Score="7" Tags="&lt;regression&gt;&lt;anova&gt;" Title="Linear regression vs analysis of variance: how to explain the difference?" ViewCount="2934" />
  <row AnswerCount="0" Body="&lt;p&gt;I am looking for some thoughts on how best to approach a dataset and pick the right model for predicting the second hand price of widgets. &lt;/p&gt;&#10;&#10;&lt;h2&gt;Some background on the properties of these widgets:&lt;/h2&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;The widgets come in different models, sizes, colors and a few other properties that are categorizable, along with the level of wear &amp;amp; tear that is unique for each widget (but can be measured accurately).&lt;/li&gt;&#10;&lt;li&gt;The value of widgets depreciates over time (it depreciation profile is similar to 1/x) up until some fixed value which is equal to their scrap value. The scrap value is not constant over time but &gt; 0. &lt;/li&gt;&#10;&lt;li&gt;The widgets’ prices are observed once in a time period and recorded along with other properties for each widget. The price is not updated again. &lt;/li&gt;&#10;&lt;li&gt;There is constant stream of new widgets hitting the market and the data comes pretty uniformly so the number of new widgets coming to the second hand market is pretty stable at say some k widgets per day s.t. the stock in time t+10 would be stock at time t plus 10*k.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;h2&gt;Questions on how to handle the data:&lt;/h2&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Because the price is observed only once, what should be the approach to setting upper limit as to how old the price data can be to be taken into consideration in the analysis? What kind of preliminary analysis should be used to determine such cutoffs? &lt;/li&gt;&#10;&lt;li&gt;Also, should there be any weight put on the recency of the price data point in modeling the price?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;h2&gt;How to choose the best model?&lt;/h2&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;What things should be considered about the underlying data before choosing the right model (treating outliers etc)?&lt;/li&gt;&#10;&lt;li&gt;Is multiple variable regression the right way to approach this data or is there already an establish method of approaching such cases of price prediction?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2012-08-18T12:49:27.860" FavoriteCount="1" Id="34583" LastActivityDate="2012-08-18T12:57:55.977" LastEditDate="2012-08-18T12:57:55.977" LastEditorUserId="7290" OwnerUserId="13382" PostTypeId="1" Score="3" Tags="&lt;dataset&gt;&lt;predictive-models&gt;&lt;model-selection&gt;&lt;multiple-regression&gt;" Title="Choosing the best model to predict second hand widget prices" ViewCount="96" />
  
  <row AnswerCount="3" Body="&lt;p&gt;There's a way to do survival analysis of two (or more I suppose) mutually exclusive competing risks as a mixture of two different survival curves. Something like what you see in A.C. Ghani et al. &lt;em&gt;&lt;a href=&quot;http://aje.oxfordjournals.org/content/162/5/479.long&quot;&gt;Methods for Estimating the Case Fatality Ratio for a Novel, Emerging Infectious Disease&lt;/a&gt;&lt;/em&gt;. American Journal of Epidemiology (2005) Vol. 162, No. 5&lt;/p&gt;&#10;&#10;&lt;p&gt;What I'm looking for is a package that would help produce something like this figure:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/huf3p.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Where the survival curve of one outcome, and 1-the survival curve of the other outcome will eventually meet at a particular point that is the mixture of the two outcomes.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-08-19T02:13:04.450" FavoriteCount="1" Id="34610" LastActivityDate="2014-11-07T07:27:47.453" LastEditDate="2012-08-19T02:28:28.353" LastEditorUserId="11032" OwnerUserId="5836" PostTypeId="1" Score="6" Tags="&lt;r&gt;&lt;sas&gt;&lt;survival&gt;&lt;kaplan-meier&gt;" Title="R packages (or SAS code) to produce two simultaneous Kaplan-Meier curves?" ViewCount="722" />
  
  <row AcceptedAnswerId="34618" AnswerCount="3" Body="&lt;p&gt;I am learning right now about regression analysis and the analysis of variance.&lt;/p&gt;&#10;&#10;&lt;p&gt;In regression analysis you have one variable fixed and you want to know how the variable goes with the other variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;In analysis of variance you want to know for example: If this specific animal food influences the weight of animals... SO one fixed var and the influence on the others...&lt;/p&gt;&#10;&#10;&lt;p&gt;Is that right or wrong, pls help me... &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-08-17T16:29:23.543" FavoriteCount="3" Id="34616" LastActivityDate="2013-05-24T18:40:03.210" OwnerDisplayName="maximus" OwnerUserId="13561" PostTypeId="1" Score="10" Tags="&lt;regression&gt;" Title="Difference between regression analysis and analysis of variance?" ViewCount="2820" />
  <row Body="&lt;p&gt;If you expect the estimate from simulated data to be equal to the value of the parameter form the simulated distribution you are naive.  That will never happen.  This is not a question of sensitivity to data.  It is the natural variability of estimates.  The estimate have a distribution with a mean and a variance.  So although I don't know the specifics of the program you are using, it would not be unusual to see the program provide a confidence interval along with the estimate to provide an idea of the uncertainty in the estimate.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now maximum likelihood is based on a parametric form of the distribution ( the negative exponential in your case).  So if the data appear to come from a distribution that is not exponential the estimate could be sensitive to the characteristics of the data set.  As an illustrative example, the sample mean is the mle for the population mean of a normal distribution.  But if the data contains a very large positive or negative observation the result will be very sensitive to that outlier.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2012-08-19T11:49:41.383" Id="34623" LastActivityDate="2012-08-19T13:41:22.793" LastEditDate="2012-08-19T13:41:22.793" LastEditorUserId="11032" OwnerUserId="11032" ParentId="34612" PostTypeId="2" Score="0" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I've been given a task with the following question:  &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Investigate whether or not the type or treatment (0 or 1) has an impact on the result (0 or 1)  &lt;/li&gt;&#10;&lt;li&gt;The same as 1), but keeping in mind another variable which is either 0,1 or 2 may be a confounder for the result.  &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;How would I check whether or not it has an impact? For question one, I think I can use a chi-squared test, is this correct? And for question two, I'm not sure how to include the confounder in the test.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-08-19T19:05:47.183" Id="34644" LastActivityDate="2015-02-09T03:48:56.513" LastEditDate="2012-08-19T19:19:16.927" LastEditorUserId="7290" OwnerUserId="13260" PostTypeId="1" Score="0" Tags="&lt;self-study&gt;&lt;chi-squared&gt;&lt;confounding&gt;" Title="Which test to use to check if a possible confounder impacts a 0 / 1 result?" ViewCount="405" />
  <row Body="&lt;p&gt;I do not think sampling should be adjusted because of chance imbalances.  Adjusting creates complications that can be worse than any problem you think you might solve.  If in the end you have covariate imbalances there are ways to adjust for them.  See this book by Vance Berger for example.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0470863625&quot; rel=&quot;nofollow&quot;&gt;Selection Bias and Covariate Imbalances In Randomized Clinical Trials&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-08-20T01:44:39.197" Id="34664" LastActivityDate="2012-08-20T01:44:39.197" OwnerUserId="11032" ParentId="34663" PostTypeId="2" Score="2" />
  
  <row AnswerCount="2" Body="&lt;p&gt;If I've understood correctly, the Benjamini-Hochberg (BH) correction is used to correct for the rate of false discoveries (FDR) when testing a collection of $m$ random variables, $\{X_1, \ldots,X_m\}$ against $m$ null hypotheses $\{H_0^1, \ldots, H_0^m\}$, of which $k\leq m$ can be true.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now consider a situation where you have a set of random variables that are &lt;em&gt;sorted&lt;/em&gt;. For example, $\{Y_1,\ldots,Y_m\}$ with $Y_m\leq\ldots \leq Y_1$ and each $Y_i\sim F_i$ (i.e., distributed according to $F_i$). An example of such a scenario would be the eigenvalues of a random matrix. Suppose now, that given a single null hypothesis $H_0$, and test statistic $T_i$, the $Y_i$'s are tested in descending order — i.e., $Y_1$, then $Y_2$, and so on, until $H_0$ is false. &lt;/p&gt;&#10;&#10;&lt;p&gt;Is the BH correction also applicable here or is it a fundamentally different scenario? Or does the question of controlling FDR not arise at all?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-08-20T05:02:19.303" FavoriteCount="1" Id="34678" LastActivityDate="2013-03-14T10:39:06.290" LastEditDate="2013-01-18T10:26:08.110" LastEditorUserId="930" OwnerDisplayName="user13443" PostTypeId="1" Score="5" Tags="&lt;hypothesis-testing&gt;&lt;multiple-comparisons&gt;&lt;p-value&gt;" Title="On the applicability of Benjamini-Hochberg" ViewCount="341" />
  <row AnswerCount="1" Body="&lt;p&gt;I have a question regarding PU-Learning, which deals with learning from positive-labeled (no labeled negative examples) and positive/negative-unlabeled data. &lt;/p&gt;&#10;&#10;&lt;p&gt;Particularly, my question is about the paper &lt;a href=&quot;http://cseweb.ucsd.edu/~elkan/posonly.pdf&quot; rel=&quot;nofollow&quot;&gt;Learning classifiers from only Positive and Unlabeled data&lt;/a&gt;. This paper converts a non-traditional classifier $g(x)$ which is learned from labeled/unlabeled datasets and outputs the probability of an example of being labeled, into a traditional classifier $f(x)$ which outputs the probability of an example to be positive. I have implemented the first proposed approach (section 2, &quot;learning a traditional classifier from nontraditional input&quot;). However, the probability $f(x)=g(x)/p(s=1|y=1)$ becomes greater than 1 for several examples (this should not be greater than 1, since it is a probability)&lt;/p&gt;&#10;&#10;&lt;p&gt;For learning the non-traditional classifier $g(x)$, I am using a non-traditional dataset composed of  &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;100,000 labeled examples randomly chosen from the whole labeled data, and  &lt;/li&gt;&#10;&lt;li&gt;100,000 unlabeled examples randomly chosen from the whole unlabeled data.  &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;And for estimating $p(s=1|y=1)$--with the first proposed estimator--I am using a validation dataset composed of  30,000 labeled instances randomly chosen from a separate labeled dataset.&lt;/p&gt;&#10;&#10;&lt;p&gt;According to the paper, the probability $f(x)$ is guaranteed to be well-formed if (1) and (2) overlap in the example space. In my case, (1) and (2) have a 20% of overlapped examples, but I am still getting non well-formed probabilities (&gt;1).&lt;/p&gt;&#10;&#10;&lt;p&gt;How could I achieve a well-defined probability for $f(x)$?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT 8/21&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;According to the paper, $g(x)$ must be a classifier that produces correct probabilities as its output, like Logistic Regression, or a calibrated classifier such as Naive Bayes/SVM, in order to get the approach to work. I am simply using Maximum Entropy classifier (from the NLTK package), also known as Logistic Regression. Therefore I guess there should not be any problem with this.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Although the paper states that the labeled (1) and unlabeled (2) datasets for training $g(x)$ are &quot;samples from overlapping regions in feature space&quot;, such datasets seem to be disjoint (see experiment from Section 5, P and U are disjoint). I have tried with disjoint labeled and unlabeled datasets as well, however I am still getting probabilities over one for $f(x)$.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Balancing datasets does not make any difference, neither. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;I have tried the second estimator proposed(e2). Still getting probabilities over 1 for $f(x)$. &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT 8/29&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;In section 5, which describes an example with real-world data, we have a set P of labeled+positive examples from the database TCDB, and a set U of unlabeled examples randomly sampled from the database SwissProt, being P and U disjoint. Then they use P,U to learn a non traditional classifier. I think that this approach can be applied to my problem, since I also have a set of positive examples, and a set of unlabeled examples randomly sampled. What do you think ?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;I have tried the third estimator proposed(e3). Since it takes the maximum probability of the examples from the positive-labeled set (in my case around 0.98), I am no longer getting probabilities over one. However, since this estimate is only based in one example and not the average between all examples, therefore this does not look like a good estimator to me. Any thoughts regarding the validity of this estimator?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-20T08:29:15.323" Id="34688" LastActivityDate="2012-09-04T08:31:18.030" LastEditDate="2012-09-04T08:31:18.030" LastEditorUserId="13600" OwnerUserId="13600" PostTypeId="1" Score="6" Tags="&lt;machine-learning&gt;&lt;probability&gt;&lt;classification&gt;&lt;unsupervised-learning&gt;&lt;nlp&gt;" Title="Getting probabilities over 1 in positive and unlabeled learning" ViewCount="300" />
  
  
  <row Body="&lt;p&gt;I think there are several challenges to consider.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;In terms of how to visualize&lt;/strong&gt;, the most accurate would be to use a mosaic plot, or a stacked barplot (which are practically the same in this case, but it might be easier to find a stacked barplot in excel or SPSS than the mosaic plot).&lt;/p&gt;&#10;&#10;&lt;p&gt;It might also be helpful to change the likert scale to a numerical (1-5) scale, and have a boxplot of each of the 4 categories of your second question.  Since boxplots are based on percentiles, the meaning of the boxplot can be somewhat consistent (depending on how the quantiles are calculated when dealing with mid points) with the type of data you present.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;In terms of how to analyse&lt;/strong&gt;, there are different questions you can ask.  The simplest will be &quot;is there a correlation between the two?&quot;, that can easily be answered using the pearson correlation on the ranking of the numerical values of your scales.  This correlation will actually give you the Spearman correlation measure (the correlation of the ranks).  The ranking is important for cases where you will have ties (for example, the vector:  1,2,2,4  should actually become:  1,2.5,2.5,3).&lt;/p&gt;&#10;&#10;&lt;p&gt;The wilcoxon test is relevant if you want to answer the question if the ranks of one measure is different than the other measure.  But from your question, it doesn't sound like an interesting question.  You can also use the Chi-square test for a similar question, but it's power will probably be smaller.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-20T10:59:44.353" Id="34694" LastActivityDate="2012-08-20T10:59:44.353" OwnerUserId="253" ParentId="33842" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Based on your follow up comment it sounds like you are trying to estimate the coverage probability of a confidence interval when you assume constant error variance when the true error variance is not constant. &lt;/p&gt;&#10;&#10;&lt;p&gt;The way I think about this is that, for each run, the confidence interval either covers the true value or it doesn't. Define an indicator variable: &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ &#10;Y_i &#10;= \begin{cases}&#10;1 &amp;amp; {\rm if \ the \ interval \ covers} \\ &#10;0 &amp;amp; {\rm if \ it \ does \ not }&#10;\end{cases}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Then the coverage probability you're interested in is $E(Y_i) = p$ which you can estimate by the sample proportion which I think is what you're proposing. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;&lt;strong&gt;How do I set the number of iteration runs?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;We know that the variance of a Bernoulli trial is $p(1-p)$, and your simulations will generate IID bernoulli trials, therefore the variance of your simulation based estimate of $p$ is $p(1-p)/n$, where $n$ is the number of simulations. You can choose $n$ to shrink this variance as much as you want. It is a fact that $$p(1-p)/n \leq 1/4n$$ &lt;/p&gt;&#10;&#10;&lt;p&gt;So, if you want the variance to be less than some pre-specified threshold, $\delta$, then you can ensure this by choosing $n \geq 1/4\delta$. &lt;/p&gt;&#10;&#10;&lt;p&gt;In a more general setting, if you're trying to investigate properties of the sampling distribution of an estimator by simulation (e.g. it's mean and variance) then you can choose your number of simulations based on how much precision you want to achieve in an analogous fashion to that described here. &lt;/p&gt;&#10;&#10;&lt;p&gt;Also note that, when the mean (or some other moment) of a variable is the object of interest, as it is here, you can construct a confidence interval for it based on the simulations using the normal approximation (i.e. the central limit theorem), as discussed in MansT's nice answer. This normal approximation is better as the number of samples grows, so, if you plan on constructing a confidence interval by appealing to the central limit theorem, you will want $n$ to be large enough for that to apply. For the binary case, as you have here, it appears this approximation is good even when $np$ and $n(1-p)$ are pretty moderate - say, $20$. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Is it true that larger than necessary replications may result in spurious biases? If so, how is that?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;As I mentioned in a comment - this depends on what you mean by spurious. Larger numbers of simulations will not produce bias in the statistical sense, but it may reveal an unimportant bias that is only noticeable with an astronomically large sample size. For example, suppose the true coverage probability of the misspecified confidence interval were $94.9999\%$. Then, this isn't really a problem in a practical sense, but it you may only pick up this difference if you ran a ton of simulations.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-20T15:02:11.600" Id="34710" LastActivityDate="2012-08-20T15:12:50.087" LastEditDate="2012-08-20T15:12:50.087" LastEditorUserId="4856" OwnerUserId="4856" ParentId="34706" PostTypeId="2" Score="8" />
  
  <row Body="&lt;p&gt;I believe the answer is yes.&lt;/p&gt;&#10;&#10;&lt;p&gt;One could think of the &quot;similarity&quot; between two variables to be measured by (say) correlation.  And for the p-value to be the significance of the correlation being different than 0.  In such a case, a small p-value (nearing zero) is one that reflects a large distance (&quot;difference&quot;) between the variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;You could turn the p-values into z scores (where the &quot;distance&quot; of them will have to &quot;usual&quot; direction), and see if the methods you mentioned will make sense on that...&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-20T16:30:58.997" Id="34716" LastActivityDate="2012-08-20T16:30:58.997" OwnerUserId="253" ParentId="34669" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;An experiment requires a control, but it doesn't need to be structured as &quot;end result of one randomly assigned group&quot; vs. &quot;end result of another randomly assigned group&quot;. You are concerned that the composition of the groups is unbalanced on one particular attribute that you believe has an outsized influence on the end result; perhaps a different structure would make this irrelevant. The experiment may be set up as a before-and-after for each teaching method group, or it can be a regression where one independent variable is the teaching method. If you have really strong reasons to believe that this one particular attribute outweighs all others, you could even take your population of participants and separate them into groups according to education level, and then within each group randomly assign participants to the control and test groups.&lt;/p&gt;&#10;&#10;&lt;p&gt;A risk is that while you've picked out one key driver of different outcomes, there may be others lurking in there you haven't detected, and which you might be subverting. &lt;/p&gt;&#10;" CommentCount="9" CreationDate="2012-08-20T20:23:07.110" Id="34739" LastActivityDate="2012-08-20T20:23:07.110" OwnerUserId="3331" ParentId="34663" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="34768" AnswerCount="2" Body="&lt;p&gt;I've asked a few questions here before regarding my thesis. Although I try my best to follow-up on your suggestions, my statistical knowledge is limited but I try my utmost. Adding a predictive model to your thesis is not required (since it isn't taught during the studies) but my thesis coach insists. So I've just let SPSS dictate the best-fitting ARIMA model for my thesis. &lt;/p&gt;&#10;&#10;&lt;p&gt;Basically, I have taken some internet data (&lt;code&gt;hbVol0LN&lt;/code&gt; is number of tweets, &lt;code&gt;hbBullQuality0&lt;/code&gt; is the ratio for postive against negative tweets, etc.) for 100 companies over 103 days. Here, the dependent variable is the return of the stock of each of those 100 companies per day. I already performed an OLS (although it has been pointed out that this is not the ideal model for my research, it is accepted by my coach), but now I believe this ARIMA model should hold the predictive value of the data.&lt;/p&gt;&#10;&#10;&lt;p&gt;It is very hard to find annotated ARIMA output online, or a paper which describes the output in a way I can understand. Could you perhaps give me some insights of what this output is telling me? Any help at all is greatly appreciated.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you're having a hard time reading the graph, here's the full-size one: &lt;a href=&quot;http://i.stack.imgur.com/H42YP.png&quot; rel=&quot;nofollow&quot;&gt;http://i.stack.imgur.com/H42YP.png&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Again, I cannot express how frustrating it is for somebody who has hardly had to do any statistics during his studies, having to produce a predictive model. Therefore, really, any help is appreciated.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/H42YP.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2012-08-20T20:39:51.520" Id="34741" LastActivityDate="2012-10-17T09:05:53.140" LastEditDate="2012-08-20T21:20:54.770" LastEditorUserId="11032" OwnerUserId="10776" PostTypeId="1" Score="0" Tags="&lt;spss&gt;&lt;predictive-models&gt;&lt;arima&gt;" Title="Could somebody explain to me what this ARIMA model output says?" ViewCount="6333" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I am developing a questionnaire to measure four factors which constitute spirituality, and I would like to ask the following question: &lt;/p&gt;&#10;&#10;&lt;p&gt;Are data transformations on non-normal data necessary for an exploratory factor analysis when using the principal axis factoring extraction method?&lt;/p&gt;&#10;&#10;&lt;p&gt;I finished screening my data yesterday, and I found that 3 out of 20 questions are positively skewed while 1 out of 20 is negatively skewed (Question 6 = 4.88, Question 9 = 7.22, Question 12 = 11.11, Question 16 = -6.26). I also found that 1 of the questions (out of 20) is leptokurtic (Question 12 = 12.21). &lt;/p&gt;&#10;&#10;&lt;p&gt;I chose the principal axis factoring extraction method because I read that it is used on &quot;severely non-normal data&quot; while maximum-likelihood is used on normal data, but:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;How would I know if my data is &quot;severely&quot; non-normal? &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;If my data is &quot;severely non-normal&quot;, does this mean that I can leave the data as it is now (not transform it) and analyze it using the principal axis factoring extraction method? Or do I need to transform the data before proceeding with the EFA? &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;If I do need to transform the data, what transformations would I use for positively skewed, negatively skewed, and leptokurtic items?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2012-08-21T02:53:55.587" Id="34753" LastActivityDate="2012-08-21T07:58:25.430" LastEditDate="2012-08-21T07:57:12.077" LastEditorUserId="930" OwnerUserId="12750" PostTypeId="1" Score="5" Tags="&lt;factor-analysis&gt;&lt;skewness&gt;&lt;kurtosis&gt;&lt;exploratory-analysis&gt;" Title="Are data transformations on non-normal data necessary for an exploratory factor analysis when using the principal axis factoring extraction method?" ViewCount="1875" />
  
  
  
  <row Body="&lt;p&gt;Adding quadratic terms would help if the mean varied that way but the variability is in the variance in your case.  Since it is the covariates that cause the change, a form of variance function estimation involving those covariates would be the approach I recommend.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-08-21T11:23:02.767" Id="34764" LastActivityDate="2012-08-21T13:42:03.207" LastEditDate="2012-08-21T13:42:03.207" LastEditorUserId="11032" OwnerUserId="11032" ParentId="34762" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="34774" AnswerCount="1" Body="&lt;p&gt;Let $R_{it}$ and $R_{mt}$ be daily stock returns for some company $i$ and the daily market index returns (respectively), with $i \in \{1,...,N\}$ and $t \in \{1,...,200\}$. It is common to have $R_{it}$ as the response and $R_{mt}$ as the predictor. &lt;/p&gt;&#10;&#10;&lt;p&gt;Let a severe financial crisis occur at $t=150$ and this crisis lasts until $t=200$. To model whether the stock fell more than the market index, we estimate with OLS $\forall i$:&lt;/p&gt;&#10;&#10;&lt;p&gt;$R_{it} = \alpha_i + \beta_i R_{mt} + \gamma_i D_t + \epsilon_{it}$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $D_t$ equals 1 over the crisis and 0 otherwise. &lt;/p&gt;&#10;&#10;&lt;p&gt;What I want to know, broadly, is &quot;Which stocks were the fastest to fall?&quot;. I have ideas for sub-sample analysis that can tackle this question, but I would prefer an econometric approach. &lt;/p&gt;&#10;&#10;&lt;p&gt;One way to do this would be to truncate the sample to $t \in \{1,...,160\}$ and look for the most negative $\gamma_i$'s. However, I'm looking for an econometric result that will identify the stocks that (i) fell over the entire crisis period, but (ii) had a very large proportion of their fall in the first week or so of the crisis. I also want this paramaterized so that I can use it as a dependent variable in a cross sectional regression analysis to explain this phenomena. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-08-21T12:52:58.423" Id="34771" LastActivityDate="2012-08-21T14:10:29.330" LastEditDate="2012-08-21T14:10:29.330" LastEditorDisplayName="user13253" OwnerDisplayName="user13253" PostTypeId="1" Score="0" Tags="&lt;finance&gt;" Title="Determining which stocks fall first in a stock market crash" ViewCount="104" />
  <row Body="&lt;p&gt;Could you just use a pie chart? Each category would be one piece of the pie and it's size would be the proportion it represents of the total.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-08-21T13:32:32.040" Id="34777" LastActivityDate="2012-08-21T14:33:37.057" LastEditDate="2012-08-21T14:33:37.057" LastEditorUserId="-1" OwnerDisplayName="Roger" ParentId="34776" PostTypeId="2" Score="0" />
  
  
  
  <row Body="&lt;p&gt;I would like to suggest that this phenomenon (of a non-significant overall test despite a significant individual variable) can be understood as a kind of aggregate &quot;masking effect&quot; and that although it conceivably could arise from multicollinear explanatory variables, it need not do that at all.  It also turns out not to be due to multiple comparison adjustments, either.  Thus this answer is adding some qualifications to the answers that have already appeared, which on the contrary suggest that either multicollinearity or multiple comparisons should be looked at as the culprits.&lt;/p&gt;&#10;&#10;&lt;p&gt;To establish the plausibility of these assertions, let's generate a collection of &lt;em&gt;perfectly orthogonal&lt;/em&gt; variables--just as non-collinear as possible--and a dependent variable that explicitly is determined solely by the first of the explanands (plus a good amount of random error independent of everything else).  In &lt;code&gt;R&lt;/code&gt; this can be done (reproducibly, if you wish to experiment) as&lt;/p&gt;&#10;&#10;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(17)&#10;p &amp;lt;- 5 # Number of explanatory variables&#10;x &amp;lt;- as.matrix(do.call(expand.grid, lapply(as.list(1:p), function(i) c(-1,1))))&#10;y &amp;lt;- x[,1] + rnorm(2^p, mean=0, sd=2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;It's unimportant that the explanatory variables are binary; what matters is their orthogonality, which we can check to make sure the code is working as expected, which can be done by inspecting their correlations.  Indeed, &lt;strong&gt;the correlation matrix is interesting&lt;/strong&gt;: the small coefficients suggest &lt;code&gt;y&lt;/code&gt; has little to do with any of the variables except the first (which is by design) and the off-diagonal zeros confirm the orthogonality of the explanatory variables:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; cor(cbind(x,y))&#10;     Var1  Var2  Var3   Var4  Var5      y&#10;Var1 1.00 0.000 0.000  0.000  0.00  0.486&#10;Var2 0.00 1.000 0.000  0.000  0.00  0.088&#10;Var3 0.00 0.000 1.000  0.000  0.00  0.044&#10;Var4 0.00 0.000 0.000  1.000  0.00 -0.014&#10;Var5 0.00 0.000 0.000  0.000  1.00 -0.167&#10;y    0.49 0.088 0.044 -0.014 -0.17  1.000&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Let's run a series of regressions&lt;/strong&gt;, using only the first variable, then the first two, and so on.  For brevity and easy comparison, in each one I show only the line for the first variable and the overall F-test:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt;temp &amp;lt;- sapply(1:p, function(i) print(summary(lm(y ~ x[, 1:i]))))&#10;&#10;#              Estimate Std. Error t value Pr(&amp;gt;|t|)   &#10;1  x[, 1:i]       0.898      0.294    3.05   0.0048 **&#10;F-statistic: 9.29 on 1 and 30 DF,  p-value: 0.00478 &#10;&#10;2  x[, 1:i]Var1    0.898      0.298    3.01   0.0053 **&#10;F-statistic: 4.68 on 2 and 29 DF,  p-value: 0.0173 &#10;&#10;3  x[, 1:i]Var1   0.8975     0.3029    2.96   0.0062 **&#10;F-statistic: 3.05 on 3 and 28 DF,  p-value: 0.0451 &#10;&#10;4  x[, 1:i]Var1   0.8975     0.3084    2.91   0.0072 **&#10;F-statistic: 2.21 on 4 and 27 DF,  p-value: 0.095 &#10;&#10;5  x[, 1:i]Var1   0.8975     0.3084    2.91   0.0073 **&#10;F-statistic: 1.96 on 5 and 26 DF,  p-value: 0.118&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Look at how (a) the significance of the first variable barely changes, (a') the first variable remains significant (p &amp;lt; .05) even when adjusting for multiple comparisons (&lt;em&gt;e.g.&lt;/em&gt;, apply Bonferroni by multiplying the nominal p-value by the number of explanatory variables), (b) the coefficient of the first variable barely changes, but (c) the overall significance grows exponentially, quickly inflating to a non-significant level.&lt;/p&gt;&#10;&#10;&lt;p&gt;I interpret this as demonstrating that &lt;strong&gt;including explanatory variables that are largely independent of the dependent variable can &quot;mask&quot; the overall p-value of the regression.&lt;/strong&gt;  When the new variables are orthogonal to existing ones and to the dependent variable, they will not change the individual p-values.  (The small changes seen here are because the random error added to &lt;code&gt;y&lt;/code&gt; is, by accident, slightly correlated with all the other variables.)  One lesson to draw from this is that &lt;strong&gt;parsimony is valuable&lt;/strong&gt;: using as few variables as needed can strengthen the significance of the results.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am &lt;em&gt;not&lt;/em&gt; saying that this is necessarily happening for the dataset in the question, about which little has been disclosed. But knowledge that this masking effect &lt;em&gt;can&lt;/em&gt; happen should inform our interpretation of the results as well as our strategies for variable selection and model building.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-08-21T20:48:37.000" Id="34813" LastActivityDate="2012-08-21T21:57:21.913" LastEditDate="2012-08-21T21:57:21.913" LastEditorUserId="919" OwnerUserId="919" ParentId="24720" PostTypeId="2" Score="9" />
  <row Body="&lt;p&gt;The following is a tutorial that may help you:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://ww2.coastal.edu/kingw/statistics/R-tutorials/independ.html&quot; rel=&quot;nofollow&quot;&gt;http://ww2.coastal.edu/kingw/statistics/R-tutorials/independ.html&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-08-21T21:13:23.387" Id="34814" LastActivityDate="2012-08-21T21:13:23.387" OwnerUserId="13209" ParentId="34809" PostTypeId="2" Score="0" />
  
  
  
  <row Body="&lt;p&gt;The way the bootstrap is applied to estimate prediction error rate is to use the bootstrap samples to estimate the bias of the resubstitution estimate of error rate and adjust it.  Various bootstrap estimators have been devised and shown to work better than the leave-one-out version of cross-validation.  For most population distributions the version called 632+ is the best.  Details about the methods and the literature on it can be found in my book &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0471756210&quot; rel=&quot;nofollow&quot;&gt;Bootstrap Methods&lt;/a&gt; or this book &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0471691151&quot; rel=&quot;nofollow&quot;&gt;by McLachlan&lt;/a&gt;.  This started with Efron 1983 JASA paper &quot;Estimating the error rate of a prediction rule: Improvements on Cross-validation &quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;This approach is preferable to the method you have planned.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-08-22T10:41:56.680" Id="34840" LastActivityDate="2012-08-22T10:41:56.680" OwnerUserId="11032" ParentId="34831" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a logistic model that I've built with the &lt;code&gt;nls&lt;/code&gt; function in R. I want to use Bayesian model averaging for variable selection, but I can't find a package for that in R. Are there any suitable packages? If not, is it possible to make a not too complicated script for it? &lt;/p&gt;&#10;&#10;&lt;p&gt;Data example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;y&amp;lt;-sample(c(1,0),100,replace=T)&#10;&#10;var1&amp;lt;-sample(c(1,0),100,replace=T)&#10;&#10;var2&amp;lt;-sample(c(1,0),100,replace=T)&#10;&#10;var3&amp;lt;-sample(c(1,0),100,replace=T)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The model:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Sw&amp;lt;- function(y1, N1,N2,N3) {&#10;&#10;   SA &amp;lt;- nls(y1~exp(c+(a1*N1)+(a2*N2)+(a3*N3))/(1+exp(c+(a1*N1)+(a2*N2)+(a3*N3)))&#10;    ,start=list(a1=-0.2,a2=-0.2,a3=-0.2,c=0.2))&#10;   SA   &#10;}&#10;&#10;&#10;&#10;model &amp;lt;- Sw(y, var1,var2,var3)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;How would I do Bayesian model averaging on this? I have 190 observations, where about 70 are &lt;code&gt;1&lt;/code&gt;s and 120 are &lt;code&gt;0&lt;/code&gt;s. I have 13 variables in total.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2012-08-22T11:04:43.013" Id="34843" LastActivityDate="2012-08-23T10:47:55.997" LastEditDate="2012-08-23T10:47:55.997" LastEditorUserId="3826" OwnerUserId="13173" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;logistic&gt;&lt;bayesian&gt;&lt;nls&gt;" Title="Bayesian model averaging in R" ViewCount="388" />
  
  <row AnswerCount="2" Body="&lt;p&gt;How can I find out if the differences in length and width of cut marks left in fabric are significantly different between groups (knife types)?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Additional Information&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Groups:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;KnifeType1 n=15&#10;KnifeType2 n=15&#10;KnifeType3 n=11&#10;KnifeType4 n=15&#10;KnifeType5 n=6&#10;KnifeType6 n=4&#10;KnifeType7 n=4&#10;KnifeType8 n=5&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I want to test length and width separately, and all measurements are in mm.&lt;/p&gt;&#10;&#10;&lt;p&gt;The data is not normally distributed, and the groups have very different variances; do I have to rely on Welch's t test?&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm trying to determine if the lengths and widths of cut marks would help you determine what type of knife was used-possibly by seeing if the mean lengths and widths are sig different from each other. I'm using SPSS, and have a year of undergraduate stats.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt; I read that Welch's t test was pretty robust agains nonnormality, would it be okay to use that one and state that the normality assumption was violated?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-08-22T14:12:35.603" Id="34851" LastActivityDate="2012-10-27T17:53:40.640" LastEditDate="2012-10-27T17:53:40.640" LastEditorUserId="930" OwnerUserId="13451" PostTypeId="1" Score="1" Tags="&lt;statistical-significance&gt;&lt;spss&gt;&lt;nonparametric&gt;&lt;hypothesis-testing&gt;" Title="Test to determine significance for non normal data with different variance and unequal n?" ViewCount="643" />
  <row Body="&lt;p&gt;Have you looked at the paper: F. Goreaud and R. Pélissier, “On explicit formulas of edge effect correction for Ripley's K-function,” Journal of Vegetation Science, vol. 10, no. 3, pp. 433–438, Feb. 2009.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-22T15:54:19.560" Id="34865" LastActivityDate="2012-08-22T15:54:19.560" OwnerUserId="3591" ParentId="27387" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;Linear regression can accommodate non-straight-line relationships between IVs and the DV through various transformations of variables, addition of polynomial terms and so on.  &lt;/p&gt;&#10;&#10;&lt;p&gt;That is a model like&lt;/p&gt;&#10;&#10;&lt;p&gt;$y = b_0 + b_1x_1^2 + b_2x_1 + b_3x_3^5$&lt;/p&gt;&#10;&#10;&lt;p&gt;is a linear model.  But a model such as&lt;/p&gt;&#10;&#10;&lt;p&gt;$y = b_0 + 2^{b_1x_1}$&lt;/p&gt;&#10;&#10;&lt;p&gt;is not. &lt;/p&gt;&#10;&#10;&lt;p&gt;If the data are &lt;em&gt;really&lt;/em&gt; nonlinear, then the choice of model depends partly on what you know about the relationships. If you don't know much, a spline regression may work well. &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-08-22T18:24:25.880" Id="34884" LastActivityDate="2012-08-22T19:31:13.280" LastEditDate="2012-08-22T19:31:13.280" LastEditorUserId="686" OwnerUserId="686" ParentId="34881" PostTypeId="2" Score="5" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a large dataset that is weighted due to:  &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;probability proportional to size sampling, &amp;amp;  &lt;/li&gt;&#10;&lt;li&gt;disproportionate stratification.  &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I would like to compare the means for two different domains within the same sample, using the weighted means and variance estimates. I believe I could proceed using the Delta approximation of variance of a function of random variables:&#10;$$&#10;{\rm V}(\mu_1 - \mu_2) = {\rm V}(\mu_1) + {\rm V}(\mu_2) - 2{\rm Cov}(\mu_1, \mu_2)&#10;$$&#10;but I cannot find any precedent, nor a different solution, to this problem in the literature. Nor could I find any other questions on this forum that address this specific problem. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-08-22T18:47:15.380" Id="34885" LastActivityDate="2012-08-22T23:33:37.970" LastEditDate="2012-08-22T23:33:37.970" LastEditorUserId="5739" OwnerUserId="13508" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;&lt;sampling&gt;&lt;survey&gt;&lt;weighted-mean&gt;" Title="Hypothesis test for difference between domain means with weighted data" ViewCount="148" />
  <row Body="&lt;p&gt;I dont' know if this suggestion might be too advanced, but if you want to model &lt;em&gt;duration&lt;/em&gt; (i.e., time until cessation), the appropriate approach is &lt;a href=&quot;http://en.wikipedia.org/wiki/Survival_analysis&quot; rel=&quot;nofollow&quot;&gt;survival analysis&lt;/a&gt;.  Most likely, the &lt;a href=&quot;http://en.wikipedia.org/wiki/Proportional_hazards_models&quot; rel=&quot;nofollow&quot;&gt;Cox proportional hazards model&lt;/a&gt; is best.  &lt;/p&gt;&#10;&#10;&lt;p&gt;With regard to non-linear relationships, @PeterFlom is giving you good advice that transformations (such as squared terms) and splines can help.  &lt;/p&gt;&#10;" CommentCount="14" CreationDate="2012-08-22T18:48:12.500" Id="34886" LastActivityDate="2012-08-22T18:48:12.500" OwnerUserId="7290" ParentId="34881" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;I don't think there is a simple expression for the pdf. If there were, then there would be a simple expression for the usual $\arctan$, and of the ratio between two (noncentered) normal distributions. The latter is studied in papers like Marsaglia (1965, &lt;a href=&quot;http://www.jstatsoft.org/v16/i04/paper&quot; rel=&quot;nofollow&quot;&gt;2006&lt;/a&gt;) and Cedilnik et al (&lt;a href=&quot;http://mrvar.fdv.uni-lj.si/pub/mz/mz1.1/cedilnik.pdf&quot; rel=&quot;nofollow&quot;&gt;2004&lt;/a&gt;). &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-08-22T20:27:43.747" Id="34892" LastActivityDate="2012-08-22T20:27:43.747" OwnerUserId="11981" ParentId="34889" PostTypeId="2" Score="3" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm very keen on sports and am just starting to understand how I can start to apply maths to sports related problems and issues. I'm keen to get some pointers in the right direction for the linear regressions I have run, and (linear) weights I am trying to derive for goals scored in the English Premiership.&lt;/p&gt;&#10;&#10;&lt;p&gt;Sabremetrics (http://en.wikipedia.org/wiki/Sabremetrics) style studies have been carried out on various sports but have never really caught on for football/soccer. To try and assist this, a club recently released the most comprehensive data set ever released for a whole season (over 200 variables for every player in all 20 teams) to try and aid analytical studies into the game. In short as it is relatively easy to do (on a basic level with Excel) I was keen to run regressions on the data to try and determine linear weights for defining goals scored in a predictive model (so you could effectively value a player - e.g. above or below the models prediction or make projections for what they could score in a season or who was more valuable).&lt;/p&gt;&#10;&#10;&lt;p&gt;I grouped the data so it gave season long data for all 538 players who played in the 2011/12 season (instead of by individual game). I then cut out all players who played less than 1000 minutes (about 25% of the season), and scored less than 5 goals (to try and remove potential outliers). This reduced the data set to only 65 players.&lt;/p&gt;&#10;&#10;&lt;p&gt;I then went through the variables and removed any variables that were not relevant (e.g. number of goalkeeper punches, wouldn't govern how many goals a player would score - as the player would have a value of 0 for this as he is an outfield player and not a goalkeeper). I then started running regressions and cut out every variable with a p value greater than 0.05 (e.g. statistically insignificant). This has then left me with a variety of different options with r2 values between 0.84 and 0.89 (depending on slight changes to variables).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://img253.imageshack.us/img253/7638/revcreg6r2896.jpg&quot; rel=&quot;nofollow&quot;&gt;http://img253.imageshack.us/img253/7638/revcreg6r2896.jpg&lt;/a&gt; (was the last regression I ran before running it with solely significant variables - highest r2 value at 0.896)&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://img820.imageshack.us/img820/8695/revcreg7ar2848.jpg&quot; rel=&quot;nofollow&quot;&gt;http://img820.imageshack.us/img820/8695/revcreg7ar2848.jpg&lt;/a&gt; (was taking the regression above and dropping &quot;big chances&quot; making all variables significant and giving an r2 of 0.848)&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://img4.imageshack.us/img4/4255/revcreg7br2868.jpg&quot; rel=&quot;nofollow&quot;&gt;http://img4.imageshack.us/img4/4255/revcreg7br2868.jpg&lt;/a&gt; (was adding &quot;big chances&quot; back in, as it had been significant when I was going through all of the variables and was only marginal insignificant at 0.16 in the first image - gives an r2 of 0.868).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://img594.imageshack.us/img594/7743/revdreg7cr2881.jpg&quot; rel=&quot;nofollow&quot;&gt;http://img594.imageshack.us/img594/7743/revdreg7cr2881.jpg&lt;/a&gt; (was adding &quot;attempts from set play on target&quot; back in as it had a similar p value to &quot;big chances&quot; - gives an r2 of 0.881).&lt;/p&gt;&#10;&#10;&lt;p&gt;All of the above has given me a number of questions that I would be grateful for some advice on:&lt;/p&gt;&#10;&#10;&lt;p&gt;Is what I am trying to do flawed (e.g. am I wrong to try and apply regression analysis to this)?&lt;/p&gt;&#10;&#10;&lt;p&gt;Is an r2 in excess of 0.84 satisfactory to accept and take forward to test on a larger data set over more years?&lt;/p&gt;&#10;&#10;&lt;p&gt;The &quot;shots on target including goals&quot; variable includes goals scored - so can I use it an independent variable to try and determine &quot;goals&quot; scored - which I am treating as the dependent variable? (the same would also apply to &quot;attempts from open play on target&quot; but oddly this comes out as a negative coefficient where as the shots on target which effectively includes the same data comes out as positive).&lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;big chances&quot; (e.g. big chances to score) was significant through the first couple of regressions I ran, before coming out with a p-value of 0.16 in the first regression above - it was significant so should I still treat it as significant (as including it results in a better r2)?&lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;fouls won penalty&quot; (e.g. foul committed on the player that results in a penalty) is significant but I would expect this to be a positive coefficient (assuming the player took and scored the penalty). Given the lack of clarification on the variable (I just can't see that this should be strongly negative) should I remove this?&lt;/p&gt;&#10;&#10;&lt;p&gt;Hopefully I have been fairly clear on what I've been trying to do, and I would be grateful for any nudges in the right direction and advice imparted.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks,&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-22T22:21:10.730" Id="34905" LastActivityDate="2012-08-23T02:55:58.877" OwnerUserId="8812" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;linear-model&gt;&lt;games&gt;" Title="Attempting to define linear weights for Goals scored in the English Premiership" ViewCount="305" />
  
  
  <row Body="&lt;p&gt;If you just want to be able to write down the probability mass function, you have a lot of flexibility, basically because you can repeatedly use integration by parts. As long as you can integrate the distribution of $X$ repeatedly to get closed form expressions, you get at worst a double sum of closed form expressions for the pmf of the compound distribution. I think you often get single sums, so maybe there is an even simpler way of expressing these.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, let $\text{pdf}_X(x) = -\log x$ on $[0,1]$. &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \int_0^1 {N\choose k}x^k (1-x)^{N-k} (-\log x) ~dx \\\ = \frac{1}{N+1}\sum_{i=k}^{N} \frac1{i+1}.$$&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-23T02:18:05.443" Id="34915" LastActivityDate="2012-08-23T02:18:05.443" OwnerUserId="11981" ParentId="34900" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;If a classifier predicts a certain class with a probability, that number can be used as a proxy for the degree of confidence in that classification. Not to be confused with confidence intervals. For example if classifier P predicts two cases as +1 &amp;amp; -1 with probability 80% &amp;amp; 60% then it is correct to say that it is more sure of the +1 classification than the -1 classification. The variance as measured by p(1-p) is also a good measure of uncertainty. Note, the baseline confidence is 50% not 0.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-23T04:01:58.173" Id="34918" LastActivityDate="2012-08-23T04:01:58.173" OwnerUserId="13516" ParentId="34823" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;In general, structural equation modelling (SEM) with all observed variables is typically called &lt;a href=&quot;http://en.wikipedia.org/wiki/Path_analysis_%28statistics%29&quot;&gt;path analysis&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;One of the main motivations for SEM is to attempt to model relationships between latent variables. By including items rather than the composite score and modelling items as indicators of a latent variable you are able to assess relationships between latent variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;In particular, with items rather than the composite score&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;you can assess your measurement model&lt;/li&gt;&#10;&lt;li&gt;you can get an estimate of relationships between latent variables (i.e., adjusting for measurement error).&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Various middle grounds also exist including:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://jeromyanglim.blogspot.com.au/2009/09/item-parcelling-in-confirmatory-factor.html&quot;&gt;item parcelling&lt;/a&gt;: i.e., you create two or more parcels of items from your 11 items, and include these parcels as observed variables for a latent variable.&lt;/li&gt;&#10;&lt;li&gt;incorporate error of measurement into the model with observed variables.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;It is not &quot;invalid&quot; to include a composite variable in SEM. However, it is in some sense invalid to say that inferences based on the observed composite variable are representative of the relationship between theorised latent variables. Most of the time, you'd want to adopt one of the other approaches (i.e., including items, including item parcels, or include measurement error).&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-08-23T07:37:11.597" Id="34922" LastActivityDate="2012-08-23T07:43:37.640" LastEditDate="2012-08-23T07:43:37.640" LastEditorUserId="183" OwnerUserId="183" ParentId="34919" PostTypeId="2" Score="6" />
  
  <row AcceptedAnswerId="34949" AnswerCount="2" Body="&lt;p&gt;I'm looking for a statistical test or tests to compare the effects of different 'treatments' on a single 'individual' over time. The single 'individual' could be one person or a group acting as a single entity (e.g. a team, a company, etc.). A simple example is comparing the effects of two different exercise regimes on one person's weight loss.&lt;/p&gt;&#10;&#10;&lt;p&gt;The main concern I have with using a 'standard' statistical test (in this case, something like a t-test, Mann-Whitney U test or Kolmogorov-Smirnov Test seems appropriate) is the obvious non-independence of the data.&lt;/p&gt;&#10;&#10;&lt;p&gt;I could check for independence by running some form of ABAB study (i.e. alternating the regimes several times to check that they are not interacting). This seems to be the approach that the author of this &lt;a href=&quot;http://escholarship.org/uc/item/2xc2h866&quot; rel=&quot;nofollow&quot;&gt;'Self Experimentation'&lt;/a&gt; paper (Roberts, S.) seems to take. He then uses repeated chi-squared tests (first A against first B, second A against first B, etc.). Is it a valid statistical approach to perform multiple tests on the same data or does this increase the chance of type I errors &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2014474/&quot; rel=&quot;nofollow&quot;&gt;as indicated in this paper&lt;/a&gt;?&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The more often one analyses the accumulating data, the greater the&#10;  chance of eventually and wrongly detecting a difference, thereby&#10;  drawing incorrect conclusions from the trial.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Should I worry about the non-independence? Can I simply perform multiple tests against the same data? Is there a specialised test for this kind of experiment?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-08-23T09:00:01.680" Id="34927" LastActivityDate="2012-08-23T15:10:20.120" LastEditDate="2012-08-23T14:10:16.310" LastEditorUserId="7290" OwnerUserId="13465" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;hypothesis-testing&gt;&lt;non-independent&gt;&lt;methodology&gt;" Title="Statistical test to compare multiple treatments on the same 'individual' over time" ViewCount="976" />
  <row AnswerCount="0" Body="&lt;p&gt;I have fitted a Poisson model to my data in R which includes two factors as independent variables. Each factor has 5 levels. I have used the  &lt;code&gt;contrasts = list(FactorA='contr.sum', FactorB='contr.sum')&lt;/code&gt;command to change the constraints of my model. So, the parameter estimates for each factor sum to 1. &lt;/p&gt;&#10;&#10;&lt;p&gt;My problem is that I find it very difficult to understand what exactly the p-values of the estimates mean. For instance, if the p-value for one level of &lt;code&gt;FactorA&lt;/code&gt; is significant, does this mean that it's generally significant for explaining the response variable or only in comparison to the other levels of the same factor?&lt;/p&gt;&#10;&#10;&lt;p&gt;Are the constraints that I used involved in this interpretation that I have to make for the significance of each level?&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, I do not get any output for the last level of each factor. Is there any way to interpret those two levels?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-08-23T09:25:32.667" Id="34929" LastActivityDate="2012-08-23T13:10:33.397" LastEditDate="2012-08-23T13:10:33.397" LastEditorUserId="919" OwnerUserId="13527" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;categorical-data&gt;&lt;generalized-linear-model&gt;&lt;p-value&gt;" Title="How to interpret p-values for factors" ViewCount="534" />
  <row Body="&lt;p&gt;I remember Lasso regression doesn't perform very well when $n \leq p$, but I'm not sure. I think in this case Elastic Net is more appropriate for variable selection.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-08-23T13:46:45.163" Id="34942" LastActivityDate="2013-12-13T11:53:23.757" LastEditDate="2013-12-13T11:53:23.757" LastEditorUserId="17230" OwnerUserId="13531" ParentId="34769" PostTypeId="2" Score="1" />
  
  
  
  <row AnswerCount="2" Body="&lt;p&gt;Sorry if Im missing something very obvious here but I am new to mixed effect modelling. &lt;/p&gt;&#10;&#10;&lt;p&gt;I am trying to model a binomial presence/absence response as a function of percentages of habitat within the surrounding area. My fixed effect is the percentage of the habitat and my random effect is the site (I mapped 3 different farm sites). &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;glmmsetaside&amp;lt;-lmer(treat~setas+(1|farm),family=binomial,data=territory)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;When &lt;code&gt;verbose=TRUE&lt;/code&gt;:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;0:     101.32427: 0.333333 -0.0485387 0.138083&lt;/code&gt;&lt;br&gt;&#10;&lt;code&gt;1:     99.797113: 0.000000 -0.0531503 0.148455&lt;/code&gt;&lt;br&gt;&#10;&lt;code&gt;2:     99.797093: 0.000000 -0.0520462 0.148285&lt;/code&gt;&lt;br&gt;&#10;&lt;code&gt;3:     99.797079: 0.000000 -0.0522062 0.147179&lt;/code&gt;&lt;br&gt;&#10;&lt;code&gt;4:     99.797051: 7.27111e-007 -0.0508770 0.145384&lt;/code&gt;&lt;br&gt;&#10;&lt;code&gt;5:     99.797012: 1.45988e-006 -0.0495767 0.141109&lt;/code&gt;&lt;br&gt;&#10;&lt;code&gt;6:     99.797006: 0.000000 -0.0481233 0.136883&lt;/code&gt;&lt;br&gt;&#10;&lt;code&gt;7:     99.797005: 0.000000 -0.0485380 0.138081&lt;/code&gt;&lt;br&gt;&#10;&lt;code&gt;8:     99.797005: 0.000000 -0.0485387 0.138083&lt;/code&gt;  &lt;/p&gt;&#10;&#10;&lt;p&gt;My output is this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Generalized linear mixed model fit by the Laplace approximation &#10;Formula: treat ~ setasidetrans + (1 | farm) &#10;&#10;AIC   BIC logLik deviance&#10;105.8 112.6  -49.9     99.8&#10;Random effects:&#10; Groups Name        Variance Std.Dev.&#10;farm   (Intercept)  0        0  &#10;Number of obs: 72, groups: farm, 3&#10;&#10;Fixed effects:&#10;Estimate Std. Error z value Pr(&amp;gt;|z|)&#10;(Intercept)   -0.04854    0.44848  -0.108    0.914&#10;setasidetrans  0.13800    1.08539   0.127    0.899&#10;&#10;Correlation of Fixed Effects:&#10;            (Intr)&#10;setasidtrns -0.851&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I basically do not understand why my random effect is 0? Is it because the random effect only has 3 levels? I dont see why this would be the case. I have tried it with lots of different models and it always comes out as 0.&lt;/p&gt;&#10;&#10;&lt;p&gt;It cant be because the random effect doesn't explain any of the variation because I know the habitats are different in the different farms.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you very much in advance for your help.&lt;/p&gt;&#10;&#10;&lt;p&gt;-&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is an example set of data using &lt;code&gt;dput&lt;/code&gt;:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;list(territory = c(1, 7, 8, 9, 10, 11, 12, 13, 14, 2, 3, 4, 5, &#10;6, 15, 21, 22, 23, 24, 25, 26, 27, 28, 16, 17, 18, 19, 20, 29, &#10;33, 34, 35, 36, 37, 38, 39, 40, 30, 31, 32, 41, 45, 46, 47, 48, &#10;49, 50, 51, 52, 42, 43, 44, 53, 55, 56, 57, 58, 59, 60, 61, 62, &#10;54, 63, 65, 66, 67, 68, 69, 70, 71, 72, 64), treat = c(1, 1, &#10;1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, &#10;0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, &#10;0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, &#10;0, 0, 0, 0, 0, 0, 0), farm = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, &#10;1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, &#10;2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, &#10;3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3), &#10;    built = c(5.202332763, 1.445026852, 2.613422283, 2.261705833, &#10;    2.168842186, 1.267473928, 0, 0, 0, 9.362387965, 17.55433115, &#10;    4.58020626, 4.739300829, 8.638442377, 0, 1.220760647, 7.979990338, &#10;    13.30789514, 0, 8.685544976, 3.71617163, 0, 0, 6.802926951, &#10;    8.925512803, 8.834006678, 4.687723044, 9.878232478, 8.097800267, &#10;    0, 0, 0, 0, 5.639651095, 9.381654651, 8.801754791, 5.692392532, &#10;    3.865304919, 4.493438554, 4.826277798, 3.650995554, 8.20818417, &#10;    0, 8.169597157, 8.62030666, 8.159474015, 8.608979238, 0, &#10;    8.588288678, 7.185700856, 0, 0, 3.089524893, 3.840381223, &#10;    31.98103158, 5.735501995, 5.297691011, 5.17141191, 6.007539933, &#10;    2.703345394, 4.298077606, 1.469986793, 0, 4.258511595, 0, &#10;    21.07029581, 6.737664009, 14.36176373, 3.056631919, 0, 32.49289428, &#10;    0)&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;It goes on with around 10 more columns for different types of habitat (like built, setaside is one of them) with percentages in it.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-08-23T19:19:24.337" Id="34969" LastActivityDate="2012-08-23T22:40:13.460" LastEditDate="2012-08-23T20:50:17.993" LastEditorUserId="13540" OwnerUserId="13540" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;lmer&gt;&lt;random-effects-model&gt;&lt;mixed-effect&gt;" Title="Random effect equal to 0 in lmer in R" ViewCount="2100" />
  <row AcceptedAnswerId="38946" AnswerCount="1" Body="&lt;p&gt;I was trying to build a classifier for a set of documents using a support vector machine. I choose to build the feature space using term occurrence. While experimenting, I found the following scenario:&#10;When removing stop words, the svm-based classifier was successfully built; otherwise, when keeping stop words, the SVM just could not be built and I got an error message “no support vector can be found”.&#10;I am very confused about this scenario. What might be the possible reason for this scenario?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-08-23T20:26:52.993" Id="34972" LastActivityDate="2012-10-08T19:32:42.453" LastEditDate="2012-08-26T13:37:05.583" LastEditorUserId="11032" OwnerUserId="6720" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;classification&gt;&lt;svm&gt;&lt;text-mining&gt;&lt;kernel&gt;" Title="Possible reason for failing to build a support vector machine" ViewCount="169" />
  <row Body="&lt;p&gt;Looks like there was probably no effect due to Farm built in from the experimental design; each farm has exactly half treated and half not.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; xtabs(~treat+farm, territory)&#10;     farm&#10;treat  1  2  3&#10;    0 14 12 10&#10;    1 14 12 10&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;It can also be instructive to fit farm as a fixed effect and see what happens; we see that the Farm effect is very, very small compared with the built effect, so I wouldn't be too surprised that the fitted variance in the mixed model is zero.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; m2&amp;lt;-glm(treat~built+factor(farm),family=binomial,data=territory)&#10;&amp;gt; library(car)&#10;&amp;gt; Anova(m2)&#10;&#10;Analysis of Deviance Table (Type II tests)&#10;&#10;Response: treat&#10;             LR Chisq Df Pr(&amp;gt;Chisq)&#10;built         0.50685  1     0.4765&#10;factor(farm)  0.02008  2     0.9900&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="3" CreationDate="2012-08-23T21:36:43.943" Id="34974" LastActivityDate="2012-08-23T21:36:43.943" OwnerUserId="3601" ParentId="34969" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;Did you see the &lt;a href=&quot;http://books.nips.cc/papers/files/nips19/NIPS2006_0137.pdf&quot; rel=&quot;nofollow&quot;&gt;Shimazaki-Shinomoto&lt;/a&gt; method?&lt;/p&gt;&#10;&#10;&lt;p&gt;Although it seems to be computationally expensive, it may give you good results. It's worth giving it a try if computational time is not your problem. There are some implemantations of this method in java, MATLAB, etc, in the website below (Stack doesn't allow me to link you directly), which runs fast enough.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;http://176.32.89.45/~hideaki/res/histogram.html&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Best of luck!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-23T21:48:47.273" Id="34976" LastActivityDate="2012-08-23T21:48:47.273" OwnerUserId="13542" ParentId="798" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;This smells like a &lt;a href=&quot;http://dx.doi.org/10.1214/aoms/1177728066&quot; rel=&quot;nofollow&quot;&gt;Kiefer &amp;amp; Wolfowitz (1956)&lt;/a&gt; inconsistency of the MLE problem with many variance parameters (and, really, many location parameters, which was considered before them, but they mention it, too). I don't think that your variance estimates are worth much with 40 observations; and AIC/BIC are not worth anything at all in your situation, as they explicitly assume large samples (40 could be reasonably large for a mean of i.i.d. data with symmetric distribution, but not large enough for anything else) and i.i.d. data (which you obviously don't have). So to me, you cannot justify much, except switching to the GLS. Modeling the variance usually brings more problems than it solves (first and foremost, specification errors lead to disasters in the variance components and the standard errors of your main model), unless your variability changes by a factor of a couple of order of magnitude, and cannot be reasonably ignored.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-08-23T23:01:28.287" Id="34980" LastActivityDate="2012-08-27T00:21:24.860" LastEditDate="2012-08-27T00:21:24.860" LastEditorUserId="5739" OwnerUserId="5739" ParentId="34932" PostTypeId="2" Score="3" />
  <row AnswerCount="2" Body="&lt;p&gt;I'm working on an experiment in which we try to sort events into three categories based on roughly 20 inputs using a feed-forward neural network (trained via backpropagation). Unfortunately, many variables which offer good discrimination become meaningless in some situations. So far, the procedure has been to set meaningless variables to some placeholder value which lies outside the normal range for that variable. &lt;/p&gt;&#10;&#10;&lt;p&gt;But that's roughly half the inputs in something like half the events. The classification isn't as strong as we'd like, so the options I've come up with are: &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Split the events based on whether the unreliable variables are present, train several neural nets on the sub-samples. The classified samples would then be merged. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Switch to some other classifier (boosted decision trees are very popular in my field) which handles missing information more naturally. &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Is one of these approaches obviously superior? Is there some other obvious option I'm missing? &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-08-24T07:26:13.103" FavoriteCount="1" Id="34993" LastActivityDate="2012-08-24T18:51:47.823" LastEditDate="2012-08-24T08:39:53.087" LastEditorUserId="2116" OwnerUserId="10457" PostTypeId="1" Score="5" Tags="&lt;multivariate-analysis&gt;&lt;neural-networks&gt;" Title="Supervised classifier for events with missing data" ViewCount="81" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I want to do an ordinal logistic regression in R without the proportionality odds assumption. I know this can be done directly using &lt;code&gt;vglm()&lt;/code&gt; function in &lt;code&gt;R&lt;/code&gt; by setting &lt;code&gt;parallel=FALSE&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;But my problem is how to fix a particular set of coefficients in this regression setup? For example, say the dependent variable $Y$ is discrete and ordinal and can take values $Y = 1$, $2$, or $3$. If the regressors are $X_{1}$ and $X_{2}$, then the regression equations are&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \begin{aligned} &#10;{\rm logit} \big( P(Y \leq 1) \big) &amp;amp;= \alpha_{1} + \beta_{11}X_{1} + \beta_{12}X_{2} \\&#10;{\rm logit}\big(P(Y \leq 2) \big) &amp;amp;= \alpha_{2} + \beta_{21}X_{1} + \beta_{22}X_{2} &#10;\end{aligned} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to set $\beta_{11}$ and $\beta_{22}$ to $1$. Please let me know how can I achieve this. Also if &lt;code&gt;R&lt;/code&gt; can't do this, could you also please let me know if I can achieve this in any other statistical software?&lt;/p&gt;&#10;" CommentCount="11" CreationDate="2012-08-24T10:38:18.777" FavoriteCount="5" Id="34997" LastActivityDate="2014-02-02T12:32:10.537" LastEditDate="2012-08-24T14:26:56.570" LastEditorUserId="4856" OwnerUserId="13553" PostTypeId="1" Score="10" Tags="&lt;r&gt;&lt;regression&gt;&lt;logistic&gt;" Title="How to fix a coefficient in an ordinal logistic regression without proportional odds assumption in R?" ViewCount="642" />
  
  
  <row AcceptedAnswerId="35023" AnswerCount="1" Body="&lt;p&gt;I am testing out 3 modeling approaches for malnutrition in children.  Theoretically, distal determinants (education,poverty) operate through proximal determinants (water, sanitation) in determining malnutrition rates.  The three logistic models, where stunting is a binary indicator for malnutrition, are:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;// Proximal determinants only: both binary indicators&#10;stunting ~ water + sanitation&#10;&#10;// Distal determinants only: both categorical indicators&#10;stunting ~ i.education + i.poverty&#10;&#10;// Both proximal and distal determinants&#10;stunting ~ water + sanitation + i.education + i.poverty&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I am surprised to find that the r-squared value of the second model is higher than the third model, as calculated by the correlation between the predicted and actual values (stata):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;predict predicted, xb&#10;corr predicted stunting&#10;local rsq = r(rho)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;While I expected the strength of the relationship and statistical significance of the more proximal causes to decrease (as they were soaked up by the distal causes), I expected the combined model to have higher explanatory power (as measured by r-squared).  Does anyone have any explanation as to why the second model has the most explanatory power?  Let me know if I can provide additional information for answering this question. &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-08-24T16:10:55.617" Id="35019" LastActivityDate="2012-08-25T23:24:04.290" LastEditDate="2012-08-25T23:24:04.290" LastEditorUserId="88" OwnerUserId="13378" PostTypeId="1" Score="4" Tags="&lt;regression&gt;&lt;logistic&gt;&lt;interpretation&gt;&lt;r-squared&gt;" Title="Fewer variables have higher R-squared value in logistic regression" ViewCount="446" />
  
  <row Body="&lt;p&gt;The answer works in the same way as jbowman's (now deleted answer) except that X$_1$ and X$_2$ are replaced by M$_1$ and M$_2$ where M$_1$ has the distribution of the maximum of the n$_1$ samples from group 1 and M$_2$ is the maximum of the n$_2$ samples from group 2.  So now we have to replace the distribution of the difference of two independent normals to the distribution of the difference between the maxima for the two normals.  Unfortunately the exact distribution for the maximum is not simple but the answer is to calculate the probability that M$_1$-M$_2$&gt;0. Actually for M$_1$ it is F$_1$$^n$(x) and F$_2$$^n$(x) for M$_2$ where F$_1$ and F$_2$ are the respective normal distributions for group 1 and group 2.  This is explained below in my comment. &lt;/p&gt;&#10;&#10;&lt;p&gt;As a further note asymptotically M$_1$ and M$_2$ can be appropriately normalized to have their distributions of the form of a Gumbel distribution (i.e. F(x) =exp(-e$^-$$^x$)). For large n$_1$ and n$_2$ that could be useful in getting an approximate answer to the desired probability.&lt;/p&gt;&#10;&#10;&lt;p&gt;As requested by the OP here are some links to books on extreme value theory at the amazon.com site.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0486436047&quot; rel=&quot;nofollow&quot;&gt;Gumbel&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0387907319&quot; rel=&quot;nofollow&quot;&gt;Leadbetter et al&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0471021482&quot; rel=&quot;nofollow&quot;&gt;Galambos&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/1849968748&quot; rel=&quot;nofollow&quot;&gt;Coles&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0387239464&quot; rel=&quot;nofollow&quot;&gt;deHaan&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-08-24T22:24:17.480" Id="35046" LastActivityDate="2012-08-26T23:04:19.123" LastEditDate="2012-08-26T23:04:19.123" LastEditorUserId="11032" OwnerUserId="11032" ParentId="35040" PostTypeId="2" Score="4" />
  <row AcceptedAnswerId="35051" AnswerCount="1" Body="&lt;p&gt;Recently I am studying a probability problem related to the multivariate hypergeometric distribution. The problem is stated as:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Given well-mixed $n$ balls of $m$ colors, and assume that $n_i$ is the number of balls with color $i$, where $i \in {1, ..., m}$. So we have $\sum_{i=1}^{m}n_i = n$&lt;/p&gt;&#10;  &#10;  &lt;p&gt;Do $k$ random draws without replacement. What's the probability that the $k$-th draw is a color which has been drawn from the previous $(k-1)$ draws?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;My naive solution is like this: the total possible ball sequences for $i$ draws would be:&lt;/p&gt;&#10;&#10;&lt;p&gt;$P(n, k)$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $P(n, k)$ is the permutation of $k$ items from $n$ items.&lt;/p&gt;&#10;&#10;&lt;p&gt;And among these sequences, only the ones with at least one occurrence of the last ball's color satisfy the requirement. So the possible sequences would be the all sequences except for the cases where none of the $(k-1)$ draws contain the last ball's color:&lt;/p&gt;&#10;&#10;&lt;p&gt;$P(n, k) - \sum_{i=1}^{m}n_i*P(n-n_i, k-1)$&lt;/p&gt;&#10;&#10;&lt;p&gt;So the probability can be calculated as:&lt;/p&gt;&#10;&#10;&lt;p&gt;$1 - {{\sum_{i=1}^{m}n_i*P(n-n_i, k-1)}\over{P(n, K)}}$&lt;/p&gt;&#10;&#10;&lt;p&gt;Now my questions are:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Is my naive solution correct, or there are cases missing or overlapping?&lt;/li&gt;&#10;&lt;li&gt;I tried to write a computer program to calculate the probability given $n, m$ and $k$, however it turns out to be very slow for large inputs, thanks to the permutations. Is there any simpler (so fast on computation) formula for this probability, like a closed form formula?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Any hints would be appreciated, and let me know if the question is too hard to understand~&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-24T22:51:59.180" Id="35048" LastActivityDate="2012-08-24T23:56:02.273" LastEditDate="2012-08-24T23:56:02.273" LastEditorUserId="13571" OwnerUserId="13571" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;multivariate-analysis&gt;" Title="Draw balls from a set of color balls: the probability of drawing a color seen before?" ViewCount="139" />
  
  
  
  
  <row Body="&lt;p&gt;I don't know STATA but an interaction term is usually simply the product of the two terms involved. Since both your independent variables are binary, you can code each of them 0 for no and 1 for yes, then the interaction term will be 1 if both of them are &quot;yes&quot; and 0 otherwise.&lt;/p&gt;&#10;&#10;&lt;p&gt;After you have output, you can interpret it. There will be a parameter for each independent variable and their interaction. The main effect for water is the effect for water when insurance = 0; the main effect for insurance is the effect for insurance when water = 0. To get the effect of each variable when the other = 1, you have to add the interaction term.  Since it is a logistic regression, these effects will be on the logit; alternatively, you can find odds ratios, but there will no longer be 1 OR for each independent variable - you will have to specify the level of the other. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-08-25T10:52:12.283" Id="35075" LastActivityDate="2012-08-25T10:52:12.283" OwnerUserId="686" ParentId="35074" PostTypeId="2" Score="2" />
  <row AnswerCount="1" Body="&lt;p&gt;Does anyone know of an explicit matrix expression for the covariance of a linear and quadratic form? That is,&lt;/p&gt;&#10;&#10;&lt;p&gt;$\mathrm{Cov}[\mathbf{a' y},\mathbf{y' Hy}]$&#10;where $\mathbf{y}\sim \mathcal N(\boldsymbol{\mu},\boldsymbol{\Sigma})$.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm particularly interested in the case where &#10;$\boldsymbol{\mu}=\mathbf{0}$, and I think this simplifies (without the normal assumption) to&lt;/p&gt;&#10;&#10;&lt;p&gt;$\mathbb E[(\mathbf{a'y})(\mathbf{y'Hy})]$. Since this involves cubic terms it probably isn't going to be simple.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-08-25T14:55:54.937" FavoriteCount="1" Id="35084" LastActivityDate="2012-12-12T14:09:29.017" LastEditDate="2012-12-12T14:09:29.017" LastEditorUserId="2970" OwnerUserId="13586" PostTypeId="1" Score="6" Tags="&lt;normal-distribution&gt;&lt;multivariate-analysis&gt;&lt;covariance&gt;" Title="Covariance of a linear and quadratic form of a multivariate normal" ViewCount="647" />
  <row AcceptedAnswerId="35087" AnswerCount="1" Body="&lt;p&gt;Does Adaboost ensure that resultant accuracy is more than or at least equal to current accuracies?&lt;/p&gt;&#10;&#10;&lt;p&gt;What happens if &lt;strong&gt;Classifier A&lt;/strong&gt; performs badly and the weights are accordingly updated and the next &lt;strong&gt;Classifier B&lt;/strong&gt; performs very well (better than &lt;strong&gt;Classifier A&lt;/strong&gt;) on the data. How does &lt;strong&gt;Adaboost&lt;/strong&gt; handle this problem?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-25T15:03:42.110" Id="35085" LastActivityDate="2012-08-25T15:30:21.570" LastEditDate="2012-08-25T15:11:48.493" LastEditorUserId="11032" OwnerUserId="7918" PostTypeId="1" Score="0" Tags="&lt;classification&gt;&lt;boosting&gt;&lt;ensemble&gt;" Title="Accuracy of classifiers with Adaboost" ViewCount="153" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am a medical graduate trying to analyze large data and write a paper about it. Can any one explain what should I included in the article about the Cox-regression model.&lt;/p&gt;&#10;&#10;&lt;p&gt;HJP&lt;/p&gt;&#10;" ClosedDate="2012-08-26T07:34:30.840" CommentCount="0" CreationDate="2012-08-25T21:16:56.553" FavoriteCount="0" Id="35100" LastActivityDate="2012-08-25T21:56:37.203" LastEditDate="2012-08-25T21:53:51.917" LastEditorUserId="686" OwnerUserId="13591" PostTypeId="1" Score="1" Tags="&lt;cox-model&gt;" Title="What to report and how to report Cox-regression survival analysis in a research paper" ViewCount="33" />
  
  <row Body="&lt;p&gt;The standard deviation is the square root of the variance.&lt;/p&gt;&#10;&#10;&lt;p&gt;The standard deviation is expressed in the same units as the mean is, whereas the variance is expressed in squared units, but for looking at a distribution, you can use either just so long as you are clear about what you are using. For example, a Normal distribution with mean = 10 and sd = 3 is exactly the same thing as a Normal distribution with mean = 10 and variance = 9.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-08-26T12:37:05.710" Id="35124" LastActivityDate="2012-08-26T12:37:05.710" OwnerUserId="686" ParentId="35123" PostTypeId="2" Score="15" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a model with two variables $X_1$ and $X_2$.&lt;/p&gt;&#10;&#10;&lt;p&gt;In general, I would like to create a Gibbs sampler based on the conditionals $p(X_1 | X_2)$ and $p(X_2 | X_1)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, computing either $p(X_2 | X_1)$ or $p(X_1 | X_2)$ is intractable. Though, they can be computed up to a normalization constant.&lt;/p&gt;&#10;&#10;&lt;p&gt;Fortunately, there is a function $f$ such that $p(X_2 | f(X_1))$ and $p(X_1| f(X_2))$ is tractable.&lt;/p&gt;&#10;&#10;&lt;p&gt;I wanted to know if the following is a valid sampler that has the guaranteed MCMC convergence.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;randomly initialize $x_1$ and $x_2$&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;until convergence is reached, perform the following:&lt;/p&gt;&#10;&#10;&lt;p&gt;2a. perform &lt;em&gt;one&lt;/em&gt; Hastings step to get a new value for $x_1$ (i.e. one accept/reject step) for $p(X_1 | X_2)$ based on the proposal $p(X_1 | f(X_2))$.&lt;/p&gt;&#10;&#10;&lt;p&gt;2b. perform &lt;em&gt;one&lt;/em&gt; Hastings step analogously for $x_2$.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I am just not sure whether in order to get guaranteed convergence, 2a and 2b have to be run until convergence too (instead of taking one accept/reject step).&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-08-27T00:53:20.553" Id="35146" LastActivityDate="2012-08-27T08:55:05.117" LastEditDate="2012-08-27T08:55:05.117" LastEditorUserId="2116" OwnerUserId="13002" PostTypeId="1" Score="1" Tags="&lt;sampling&gt;&lt;mcmc&gt;" Title="Is the following a valid MCMC sampler?" ViewCount="82" />
  <row Body="&lt;p&gt;One way to handle this is by &quot;binning&quot; -- the domain of the real value is binned into a finite set of intervals, and then each example is placed at the relevant bin.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can read more about it in the following Wikipedia page:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Naive_Bayes_classifier&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Naive_Bayes_classifier&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;starting at &quot;Another common ...&quot;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-27T03:11:45.437" Id="35147" LastActivityDate="2012-08-27T03:11:45.437" OwnerUserId="13613" ParentId="35144" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;&lt;strong&gt;RColorBrewer&lt;/strong&gt; has not been mentioned here, I use it often for plotting if I need color schemes&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2012-08-27T07:28:03.990" CreationDate="2012-08-27T07:28:03.990" Id="35154" LastActivityDate="2012-08-27T07:28:03.990" OwnerUserId="9866" ParentId="73" PostTypeId="2" Score="2" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm lost. If I estimated the a Gaussian mixture model, with a shared diagonal covariance, will the Fisher information of the means be $\Sigma^{-1}$ ?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-08-27T12:59:57.483" Id="35164" LastActivityDate="2012-08-27T12:59:57.483" OwnerUserId="13624" PostTypeId="1" Score="3" Tags="&lt;estimation&gt;&lt;information-theory&gt;&lt;mixture&gt;" Title="Calculating the Fisher Information of bivariate normal" ViewCount="196" />
  
  <row Body="&lt;p&gt;&lt;strong&gt;First&lt;/strong&gt;, consider the timer you are using for observations as compared to the resolution/accuracy you need. If you are attempting to optimize routines that render in less than a second on average, the standard .NET and JAVA timers will present as much noise as information. If you're digging for optimization for longer running routines, greater than a second, the amount of observation error is quite reasonable regardless of the timer used.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Second&lt;/strong&gt;, you need to first (apriori) establish the &quot;level of significance&quot; or power used in your analysis. This is the threshold you will use to 'reject' or 'fail to reject' your null hypothesis. A commonly used significance level is 5%, so I'll refer to that. In that context, you are asking what sample size provides a 5% significance level around your estimation of the mean.&lt;/p&gt;&#10;&#10;&lt;p&gt;You already know how to compute the average of your observations, I'll refer to it as $\tilde{x}$. Calculate the sample standard deviation as well; I refer to it as σ. The amount of &quot;confidence&quot; you have in your estimation of $\tilde{x}$ is a function of the standard deviation of your observations. Most often this is called &quot;standard error&quot; and you calculate it as $\frac{\sigma}{\sqrt{N}}$ where $N$ is the number of observations.&lt;/p&gt;&#10;&#10;&lt;p&gt;With an apriori estimate of σ (from a control group, prior experiment, etc), you can answer this question two ways: &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;What is the estimate of the sample mean with 95% confidence? &lt;/p&gt;&#10;&#10;&lt;p&gt;$\tilde{x}-\frac{2\sigma}{\sqrt{N}}$ and $\tilde{x}+\frac{2\sigma}{\sqrt{N}}$&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;What is the minimum number of samples I need to estimate the sample mean with 95% confidence that is $W$ units in width?&lt;/p&gt;&#10;&#10;&lt;p&gt;$N$ = $(\frac{4\sigma}{W})^2$ &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Hope that helps.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-27T15:08:04.563" Id="35170" LastActivityDate="2013-10-27T17:19:51.043" LastEditDate="2013-10-27T17:19:51.043" LastEditorUserId="11939" OwnerUserId="11939" ParentId="35165" PostTypeId="2" Score="6" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am a graduate student and have a question regarding generalized estimating equation analysis. When conducting across day analysis with GEE, with variable a on day $x$ predicting variable $b$ on day $x+1$, is it necessary to control for previous day's values for variable $b$, or is this correlation already accounted for with the GEE technique? &#10;Your comments and references would be much appreciated.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-27T16:32:44.830" Id="35178" LastActivityDate="2014-04-25T14:03:46.503" LastEditDate="2014-04-25T14:03:46.503" LastEditorUserId="26338" OwnerUserId="13633" PostTypeId="1" Score="3" Tags="&lt;generalized-linear-model&gt;&lt;gee&gt;" Title="Control for previous day values when doing across day analysis with generalized estimating equations?" ViewCount="129" />
  <row AcceptedAnswerId="35187" AnswerCount="2" Body="&lt;p&gt;I possess a basic understanding of random vs. fixed effects, and how to code random effects models in SAS. However, I'm having trouble wrapping my head around the derivation of random effects terms, and how a random intercept model, for example, can describe the variation in $k$ intercepts with a single parameter ($\sigma^2$ for a normal dbn) rather than $k-1$ parameters, which can amount to a huge saving in degrees of freedom. Isn't that cheating? ;) &lt;/p&gt;&#10;&#10;&lt;p&gt;The old school technique would be to use the maximum likelihood method to solve for $k-1$ parameters for each category.  &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;How does a random intercept model avoid this by using a single parameter?  &lt;/li&gt;&#10;&lt;li&gt;Isn't the end result the same--both random and fixed effect models will estimate $k-1$ intercept terms?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="1" CreationDate="2012-08-27T17:17:54.407" Id="35184" LastActivityDate="2012-08-27T19:00:45.307" LastEditDate="2012-08-27T17:36:08.643" LastEditorUserId="7290" OwnerUserId="13634" PostTypeId="1" Score="5" Tags="&lt;random-effects-model&gt;&lt;fixed-effects-model&gt;&lt;degrees-of-freedom&gt;" Title="Why is it valid to account for k-1 intercepts w/ only 1 random intercept parameter?" ViewCount="175" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm estimating several inverse covariance matrices of a set of measurements across different subpopulations using an wishart prior in jags/rjags/R. &lt;/p&gt;&#10;&#10;&lt;p&gt;Instead of specifying a scale matrix and degrees of freedom on the inverse covariance matrix prior (the wishart distribution), I would like to use a hyperprior on the scale matrix and degrees of freedom, so they can be estimated from the variation across subpopulations.&lt;/p&gt;&#10;&#10;&lt;p&gt;I haven't found much literature on hyperpriors for the scale matrix and degrees of freedom. Most of the literature seems to stop the hierarchy at the choice of the prior to the covariance/inverse covariance and/or are focused on estimating a single covariance matrix rather than several covariance matrices across different populations.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any suggestions as to how to go about this - what are the recommended hyperprior distributions to use for the scale matrix and degrees of freedom of the wishart distribution? Is there some literature on this that I'm missing? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-27T20:36:59.647" FavoriteCount="3" Id="35198" LastActivityDate="2012-09-19T04:33:03.363" LastEditDate="2012-09-19T04:33:03.363" LastEditorUserId="5739" OwnerUserId="4733" PostTypeId="1" Score="6" Tags="&lt;bayesian&gt;&lt;covariance&gt;&lt;prior&gt;&lt;wishart&gt;&lt;hierarchical-bayesian&gt;" Title="Hyperprior distributions for the parameters (scale matrix and degrees of freedom) of a wishart prior to an inverse covariance matrix" ViewCount="734" />
  
  <row AnswerCount="1" Body="&lt;p&gt;The algorithm mentioned on &lt;a href=&quot;http://en.wikipedia.org/wiki/Exponential_smoothing&quot; rel=&quot;nofollow&quot;&gt;Wikipedia&lt;/a&gt; has a line $$s_t = \alpha \frac{x_t}{c_{t-L}} + (1-\alpha) (s_{t-1} + b_{t-1})$$.&lt;/p&gt;&#10;&#10;&lt;p&gt;For $1 &amp;lt; t &amp;lt; L$ how should we interpret $c_{t-L}$?  On that interval, should we replace it with $c_t$?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-28T02:35:27.217" Id="35215" LastActivityDate="2012-08-28T11:36:50.130" LastEditDate="2012-08-28T11:36:50.130" LastEditorUserId="88" OwnerUserId="8067" PostTypeId="1" Score="2" Tags="&lt;time-series&gt;&lt;forecasting&gt;" Title="Clarification in definition of Holt-Winters algorithm" ViewCount="191" />
  
  
  <row Body="&lt;p&gt;Following on @Procrastinator's astute &lt;a href=&quot;http://stats.stackexchange.com/questions/35247/adk-test-sample-size#comment70512_35247&quot;&gt;comment&lt;/a&gt;, it may be that &lt;a href=&quot;http://rgm2.lab.nig.ac.jp/RGM2/func.php?rd_id=adk%3aadk.test&quot; rel=&quot;nofollow&quot;&gt;?adk.test&lt;/a&gt; is not the optimal function for you to be using here.  The &lt;a href=&quot;http://cran.r-project.org/web/packages/fitdistrplus/index.html&quot; rel=&quot;nofollow&quot;&gt;fitdistrplus package&lt;/a&gt; offers a number of functions that will better serve your goals of determining which distribution is best.  You should read its &lt;a href=&quot;http://cran.r-project.org/web/packages/fitdistrplus/vignettes/intro2fitdistrplus.pdf&quot; rel=&quot;nofollow&quot;&gt;vignette&lt;/a&gt;.  You can get the A-D test (as well as Kolmogorov-Smirnov, and Cramer-Von Mises) via:  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;fit = fitdist(x, &quot;&amp;lt;some_dist_type&amp;gt;&quot;)&#10;gofstat(fit, print.test=T)  &#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="3" CreationDate="2012-08-28T15:08:17.257" Id="35254" LastActivityDate="2012-08-28T15:08:17.257" OwnerUserId="7290" ParentId="35247" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;If 2 knife types have the exact same mean, but very different variances then you would still have information useful to classification, if you are seeing a cut that lies far from the mean relative to the small variance, but reasonble from the large variance then it seems much more likely to have come from the knife with the larger variance.  So focusing on differences is means when there are other differences is probably not the best approach.&lt;/p&gt;&#10;&#10;&lt;p&gt;You should look into classification analysis, possibly K nearest neighbors methods, or a Bayesian approach (using either the distribution that you believe fits the data, or a smoothed approximation like a logspline estimate).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-28T16:05:36.803" Id="35257" LastActivityDate="2012-08-28T16:05:36.803" OwnerUserId="4505" ParentId="34851" PostTypeId="2" Score="1" />
  
  
  
  
  <row Body="&lt;p&gt;(Note: I've changed your $\xi$ for $x$.)&lt;/p&gt;&#10;&#10;&lt;p&gt;For a random variable $X$ with density $p$, if you have constraints&#10;$$&#10;  \int G_i(x)\,p(x)\,dx=c_i \, ,&#10;$$&#10;for $i=1,\dots,n$, the maximum entropy density is&#10;$$&#10;  p_0(x)=A\exp\left(\sum_{i=1}^n a_iG_i(x)\right) \, ,&#10;$$&#10;where the $a_i$'s are determined from the $c_i$'s, and $A$ is a normalization constant. &lt;/p&gt;&#10;&#10;&lt;p&gt;In this context, the Gaussian approximation (&quot;near-gaussianity&quot;) means two things:&lt;/p&gt;&#10;&#10;&lt;p&gt;1) You accept to introduce two new constraints: the mean of $X$ is $0$ and the variance is $1$ (say);&lt;/p&gt;&#10;&#10;&lt;p&gt;2) The corresponding $a_{n+2}$ (see bellow) is much bigger than the other $a_i$'s.&lt;/p&gt;&#10;&#10;&lt;p&gt;These additional constraints are represented as&#10;$$&#10;  G_{n+1}(x)=x \, , \qquad c_{n+1}=0 \, ,&#10;$$&#10;$$&#10;  G_{n+2}(x)=x^2 \, , \qquad c_{n+2}=1 \, ,&#10;$$&#10;yielding&#10;$$&#10;  p_0(x)=A\exp\left(a_{n+2}x^2 + a_{n+1}x + \sum_{i=1}^n a_iG_i(x)\right) \, ,&#10;$$&#10;which can be rewritten as (just &quot;add zero&quot; to the exponent)&#10;$$&#10;  p_0(x)=A\exp\left(\frac{x^2}{2} - \frac{x^2}{2} + a_{n+2}x^2 + a_{n+1}x + \sum_{i=1}^n a_iG_i(x)\right) \, ,&#10;$$&#10;leading to what you want:&#10;$$&#10;  p_0(x)=A'\,\phi(x)\exp\left(a_{n+1}x + \left(a_{n+2}+\frac{1}{2}\right)x^2 + \sum_{i=1}^n a_iG_i(x)\right) \, ;&#10;$$&#10;ready to be Taylor expanded (using the second condition of the Gaussian approximation).&lt;/p&gt;&#10;&#10;&lt;p&gt;Doing the approximation like a Physicist (which means that we don't care about the order of the error term), using $\exp(t)\approx 1+t$, we have the approximate density&#10;$$&#10;  p_0(x) \approx A'\,\phi(x)\left(1+a_{n+1}x + \left(a_{n+2}+\frac{1}{2}\right)x^2 + \sum_{i=1}^n a_iG_i(x)\right) \, .&#10;$$&#10;To finish, we have to determine $A'$ and the values of the $a_i$'s. This is done imposing the conditions&#10;$$&#10;  \int p_0(x)\,dx=1 \, , \qquad \int x \,p_0(x)\,dx=0 \, , \qquad \int x^2 \,p_0(x)\,dx=1&#10;$$&#10;$$&#10;\int G_i(x)\, p_0(x)\,dx=c_i \, , \quad i=1,\dots,n \, ,&#10;$$&#10;to obtain a system of equations, whose solution gives $A'$ and the $a_i$'s.&lt;/p&gt;&#10;&#10;&lt;p&gt;Without imposing additional conditions on the $G_i$'s, I don't believe that there is a simple solution in closed form.&lt;/p&gt;&#10;&#10;&lt;p&gt;P.S. Mohammad clarified during a chat that with additional orthogonality conditions for the $G_i$'s we can solve the system.&lt;/p&gt;&#10;" CommentCount="12" CreationDate="2012-08-28T22:58:12.600" Id="35290" LastActivityDate="2012-08-29T23:45:43.407" LastEditDate="2012-08-29T23:45:43.407" LastEditorUserId="9394" OwnerUserId="9394" ParentId="35284" PostTypeId="2" Score="10" />
  <row Body="&lt;p&gt;There are at least two major sources of overfitting you might wish to consider.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Overfitting from an algorithm which has inferred too much from the available training samples. This is best guarded against empirically by using a measure of the generalisation ability of the model. Cross validation is one such popular method.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Overfitting because the underlying distribution is undersampled. Usually there is little that can be done about this unless you can gather more data or add domain knowledge about the problem to your model.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;With 120 samples and a large number of features you are very likely to fall foul of 2 and may also be prone to 1.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can do something about 1 by careful observation of the effect of model complexity on the test and training errors.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-08-29T08:37:37.660" Id="35306" LastActivityDate="2012-08-29T08:37:37.660" OwnerUserId="10065" ParentId="35276" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;In practice, the reason that SVMs tend to be resistant to over-fitting, even in cases where the number of attributes is greater than the number of observations, is that it uses regularization.  They key to avoiding over-fitting lies in careful tuning of the regularization parameter, $C$, and in the case of non-linear SVMs, careful choice of kernel and tuning of the kernel parameters.&lt;/p&gt;&#10;&#10;&lt;p&gt;The SVM is an approximate implementation of a bound on the generalization error, that depends on the margin (essentially the distance from the decision boundary to the nearest pattern from each class), but is independent of the dimensionality of the feature space (which is why using the kernel trick to map the data into a very high dimensional space isn't such a bad idea as it might seem).  So &lt;strong&gt;in principle&lt;/strong&gt; SVMs should be highly resistant to over-fitting, but &lt;strong&gt;in practice&lt;/strong&gt; this depends on the careful choice of $C$ and the kernel parameters.  Sadly, over-fitting can also occur quite easily when tuning the hyper-parameters as well, which is my main research area, see&lt;/p&gt;&#10;&#10;&lt;p&gt;G. C. Cawley and N. L. C. Talbot, Preventing over-fitting in model selection via Bayesian regularisation of the hyper-parameters, Journal of Machine Learning Research, volume 8, pages 841-861, April 2007. (&lt;a href=&quot;http://jmlr.csail.mit.edu/papers/v8/cawley07a.html&quot;&gt;www&lt;/a&gt;)&lt;/p&gt;&#10;&#10;&lt;p&gt;and&lt;/p&gt;&#10;&#10;&lt;p&gt;G. C. Cawley and N. L. C. Talbot, Over-fitting in model selection and subsequent selection bias in performance evaluation, Journal of Machine Learning Research, 2010. Research, vol. 11, pp. 2079-2107, July 2010. (&lt;a href=&quot;http://jmlr.csail.mit.edu/papers/v11/cawley10a.html&quot;&gt;www&lt;/a&gt;)&lt;/p&gt;&#10;&#10;&lt;p&gt;Both of those papers use kernel ridge regression, rather than the SVM, but the same problem arises just as easily with SVMs (also similar bounds apply to KRR, so there isn't that much to choose between them in practice).  So in a way, SVMs don't really solve the problem of over-fitting, they just shift the problem from model fitting to model selection.&lt;/p&gt;&#10;&#10;&lt;p&gt;It is often a temptation to make life a bit easier for the SVM by performing some sort of feature selection first.  This generally makes matters worse, as unlike the SVM, feature selection algorithms tend to exhibit more over-fitting as the number of attributes increases.  Unless you want to know which are the informative attributes, it is usually better to skip the feature selection step and just use regularization to avoid over-fitting the data.&lt;/p&gt;&#10;&#10;&lt;p&gt;In short, there is no inherent problem with using an SVM (or other regularised model such as ridge regression, LARS, Lasso, elastic net etc.) on a problem with 120 observations and thousands of attributes, &lt;em&gt;provided the regularisation parameters are tuned properly&lt;/em&gt;.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-08-29T11:20:26.490" Id="35311" LastActivityDate="2012-08-29T11:20:26.490" OwnerUserId="887" ParentId="35276" PostTypeId="2" Score="10" />
  <row Body="&lt;p&gt;Not quite. Factor analysis operates with the second moments, and really hopes that the data are Gaussian so that the likelihood ratios and stuff like that is not affected by non-normality. ICA, on the other hand, is motivated by the idea that when you add things up, you get something normal, due to CLT, and really hopes that the data are non-normal, so that the non-normal components can be extracted from them. To exploit non-normality, ICA tries to maximize the fourth moment of a linear combination of the inputs. If anything, ICA should be compared to PCA, which maximizes the second moment (variance) of a standardized combination of inputs.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-29T13:34:55.757" Id="35321" LastActivityDate="2012-08-29T13:34:55.757" OwnerUserId="5739" ParentId="35319" PostTypeId="2" Score="11" />
  
  <row Body="&lt;p&gt;A feed-forward neural network (typically multi-layer) is a type of supervised learner that will adjust the network weights on its input and internal nodes, in an iterative manner, in order to minimize errors between predicted and actual target variables.  It commonly uses stochastic gradient descent (sometimes called error back propagation) over many iterations in order to find a local minimum of the error response and optimize the network weights accordingly. &lt;/p&gt;&#10;&#10;&lt;p&gt;The basic idea behind stochastic gradient descent is to start by randomizing the weights, then adjust them by iterating through several passes and updating the weights in a direction that moves the total error between target and predicted errors towards the local minimum error of the gradient surface.  In practice, a tradeoff is found between optimizing a training set against a validation set, in order to reduce the problem of over-fitting.&lt;/p&gt;&#10;&#10;&lt;p&gt;Lastly, the input (time series or otherwise) often needs to be transformed in order to create a stationary series that is also bounded (amplitude wise) between the input range of the NN layer transfer function(s)(typically, 0 to 1 or -1 to 1).&lt;/p&gt;&#10;&#10;&lt;p&gt;Once the weights have been trained, the model can be stored and used to process additional new time series data, much like a typical linear regression based model.&lt;/p&gt;&#10;&#10;&lt;p&gt;An example illustration of using a NN to predict finanacial time series data, using Weka, is posted here:&#10;&lt;a href=&quot;http://intelligenttradingtech.blogspot.com/2010/01/systems.html&quot;&gt;http://intelligenttradingtech.blogspot.com/2010/01/systems.html&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;A good text comparing financial AR based models against NN models is, &quot;Applied Quantitative Methods for Trading and Investment,&quot; Christian Dunis et.al.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-29T20:54:34.653" Id="35338" LastActivityDate="2012-08-29T21:24:33.350" LastEditDate="2012-08-29T21:24:33.350" LastEditorUserId="13519" OwnerUserId="13519" ParentId="29519" PostTypeId="2" Score="6" />
  
  
  <row Body="&lt;p&gt;I have found the Nelder-Mead simplex method very handy for optimizing the hyper-parameters of kernel machines.  It doesn't require gradient information and it is almost as fast as gradient descent.  If you are using MATLAB it is the fminsearch function in the optimisation toolbox, or alternatively you can download my implementation &lt;a href=&quot;http://theoval.cmp.uea.ac.uk/~gcc/matlab/#optim&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would not advise using algorithms such as simulated annealing or genetic algorithms as it is very easy to over-fit the model selection criterion, even if you only have a couple of hyper-parameters to tune, and such methods that optimise the criterion very aggressively are likely to encounter this pitfall.  This is actually a benefit of grid-search - it doesn't optimise the model selection criterion too closely and this helps avoid over-fitting. &lt;/p&gt;&#10;&#10;&lt;p&gt;Don't pay any attention to the difference between trainings set and test set performance, it generally doesn't mean much (in fact I would recommend not looking at training set performance at all unless you suspect something is going wrong).&lt;/p&gt;&#10;&#10;&lt;p&gt;Make sure you use a completely separate test set for evaluating the final performance of the model.  If you tune the hyper-parameters using  the test set, you will get a (possibly highly) optimistic performance estimate.  Use a training-validation-test partitioning scheme and optimise the hyper-parameters using the validation set and measure performance on the test set.  I use nested cross-validation.&lt;/p&gt;&#10;&#10;&lt;p&gt;As I understand it, cross-validation is O.K. for time-series data, provided the data in each partition are essentially statistically independent (e.g. the time period covered by each partition is much longer than the effects of autocorrelation in the time series etc.).  However, I am no time-series expert, so &lt;em&gt;caveat lector&lt;/em&gt;!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-30T09:16:08.687" Id="35355" LastActivityDate="2012-08-30T09:16:08.687" OwnerUserId="887" ParentId="22414" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;The $p(x)$ you are using is the normal probability density.  It is not a probability and can be greater than $1$.  Bayes classification is determined by finding the class with the highest probability density when costs for the error types are the same.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-08-30T12:08:14.500" Id="35365" LastActivityDate="2012-08-30T12:47:51.720" LastEditDate="2012-08-30T12:47:51.720" LastEditorDisplayName="user10525" OwnerUserId="11032" ParentId="35364" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;No the 616 does not constitute a random sample from the population.  It is a specific subset that answered yes to question 1a. Is quality B only assessed on those 616 or is it also studied for the remaining 2415?  There is no simple answer to your question. You could bootstrap the 3031 and for each bootstrap sample you will get a subset of the 616 that answered yes to Q1a and from that you will have a subset of 34 with quality B.  The bootstrap will give you a distribution of percentages from which you could construct a bootstrap confidence interval.  Sorry to make it complicated but I don't see an easy way.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2012-08-30T19:27:35.377" Id="35388" LastActivityDate="2012-08-30T19:27:35.377" OwnerUserId="11032" ParentId="35387" PostTypeId="2" Score="-2" />
  <row Body="&lt;p&gt;If you happen across Pierre Baldi and Laurent Itti (2010) “Of bits and wows: A Bayesian theory of surprise with applications to attention” Neural Networks 23:649-666, you will find Equation 73 gives a KL divergence between two gamma pdfs. Take care, though, it looks like the formula is mis-printed. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-30T19:54:11.137" Id="35391" LastActivityDate="2012-08-30T19:54:11.137" OwnerUserId="13315" ParentId="11646" PostTypeId="2" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a problem that consists of finding the optimal solution based on the following criteria:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Logic for identifying that event A has occurred (i.e. &quot;find&quot; logic that most accurately categorises an A type event)&lt;/li&gt;&#10;&lt;li&gt;Logic for identifying that event B has occurred  (i.e. &quot;find&quot; logic that most accurately categorises a B type event)&lt;/li&gt;&#10;&lt;li&gt;Transition states (finding a parsimonious number of states in which the system can be said to occupy)&lt;/li&gt;&#10;&lt;li&gt;State transition logic (Which state to transition to based on an event)&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I have been wrestling with this problem for over a year now, and I have managed to identify that the problem decomposes into combining the four sub problems above into one that produces the optimal solution.&lt;/p&gt;&#10;&#10;&lt;p&gt;For the purpose of brevity, assume that I have an objective function (or fitness evaluation function). The fitness function will return a score based on how a proposed solution performs with new data.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question has been stated with an unintentional bias to GA, but thats simply because that is the approach that (to my perhaps naive mind), appears to be the most promising line of enquiry.&lt;/p&gt;&#10;&#10;&lt;p&gt;I think GP may be useful here (though I have no experience with it), because as requirement 1,2 and 4 suggest, the &quot;optimal&quot; logic to use is not yet known. As an aside, I have &lt;em&gt;some&lt;/em&gt; ideas about what possible solutions using this logic are likely to entail. It's just that I'm not sure which approach would produce the &quot;fittest&quot; result (pardon the GA terminology, I don't mean to bias the answers).&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is that given such an &quot;optimisation&quot; problem, what is the best approach for solving it - GA, GP, ML (or some combination of all three)?&lt;/p&gt;&#10;&#10;&lt;p&gt;More (equally?) importantly, what are the practical steps I need to take to implement a solution that can help me solve this problem?&lt;/p&gt;&#10;&#10;&lt;p&gt;Last but not the least (it may be too early to start talking about the implementation details), but I would like to implement the solution using Python, if possible. However, I can also use R, Octave etc if there are existing packages/toolkits etc to prevent me from reinventing the wheel.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-08-31T09:06:04.673" Id="35417" LastActivityDate="2012-08-31T10:42:59.650" LastEditDate="2012-08-31T10:42:59.650" LastEditorUserId="11032" OwnerUserId="8333" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;machine-learning&gt;&lt;modeling&gt;&lt;optimization&gt;&lt;genetic-algorithms&gt;" Title="Genetic algorithms, genetic programming or machine learning algorithms for solving this problem" ViewCount="335" />
  <row AnswerCount="1" Body="&lt;p&gt;I have developed a scoring system using logistic regression. The score ranges between 0 and 6 (using integers) and predicts death. It does not use a conventional regression formula and thus I am not able to calculate a precise value of the predicted risk of dying. An example of the score could look like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;Score&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Dead&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Alive&lt;br&gt;&#10;0&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;0&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;101&lt;br&gt;&#10;1&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;911&lt;br&gt;&#10;2&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;672&lt;br&gt;&#10;3&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;291&lt;br&gt;&#10;4&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;8&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;78&lt;br&gt;&#10;5&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;10&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;60&lt;br&gt;&#10;6&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;5&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;4&lt;br&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I know that I have to use Pearson's goodness of fit to test the goodness-of-fit and have three cohorts, a development cohort and two independent validation cohorts. &lt;/p&gt;&#10;&#10;&lt;p&gt;My question is: How do I calculate the Pearson Goodness-of-fit test in each cohort? In the development cohort, what would be my expected mortality? In the validation cohorts, I guess that I could use the observed mortality in the development cohort as expected mortality. &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance!&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-08-31T11:57:26.550" Id="35422" LastActivityDate="2012-08-31T13:51:39.873" OwnerUserId="13734" PostTypeId="1" Score="1" Tags="&lt;logistic&gt;&lt;goodness-of-fit&gt;&lt;validation&gt;&lt;pearson&gt;" Title="Validation of logistic regression - goodness of fit (pearson)" ViewCount="602" />
  
  
  <row Body="&lt;p&gt;It sounds like you're talking about wavelet analysis? R packages include &lt;code&gt;wavelets&lt;/code&gt;, &lt;code&gt;waveslim&lt;/code&gt;, &lt;code&gt;wmtsa&lt;/code&gt;, or &lt;code&gt;rwt&lt;/code&gt;. Not sure which have nice graphing capabilities. Package &lt;code&gt;dlpR&lt;/code&gt; is from a specific field, but I have to say its &lt;code&gt;wavelet.plot&lt;/code&gt; plot (use its &lt;code&gt;morlet&lt;/code&gt; function to process the data) is quite nice.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-08-31T16:45:58.943" Id="35449" LastActivityDate="2012-08-31T16:45:58.943" OwnerUserId="1764" ParentId="35434" PostTypeId="2" Score="4" />
  <row AcceptedAnswerId="35451" AnswerCount="1" Body="&lt;p&gt;I have the following data in R&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x &amp;lt;- c(0.1,0.2,0.3,0.6,0.8,0.9,1)&#10;y &amp;lt;- c(90,96,97.7,99.3,99.65,99.95,100)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I'm trying to find a logarithmic equation that best fits these points. I'm not sure what the equation would look like, but probably something like one of these&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;a*log(x)+b&lt;/li&gt;&#10;&lt;li&gt;(a*log(x)+b)/(log(x)+c)&lt;/li&gt;&#10;&lt;li&gt;a*log(b*x)+c&lt;/li&gt;&#10;&lt;li&gt;etc&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;What kind of curve do you think best fits this data. And how can I find out?&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2012-08-31T17:08:16.763" Id="35450" LastActivityDate="2012-08-31T17:17:53.647" OwnerUserId="8627" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;modeling&gt;" Title="Model some logarithmic looking data in R" ViewCount="114" />
  <row Body="&lt;p&gt;I've outlined some of the approaches above, and giving it some thought, I've gone with what I'm choosing and why.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Sampling with (or without) replacement (&lt;a href=&quot;http://stats.stackexchange.com/questions/35174/how-to-handle-sets-with-less-than-k-elements-when-using-a-single-hash-function#comment70396_35174&quot;&gt;suggested by Anony-Mousse&lt;/a&gt;)&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I'm not a fan of this.  This could very easily produce values that produce similarities that are higher than they actually are, which &lt;em&gt;could&lt;/em&gt; (not always) lead to a flawed prediction/outcome.  I'm personally trying to avoid false positives as much as possible, and while a bad hash function can certainly lead to false positives, I'd rather not introduce the possibility of &lt;em&gt;more&lt;/em&gt; if I can avoid it.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Taking intersection of $h_k(S_1)$ and $h_{k}(S_2)$ where $min(|S_1|, |S_2|) &amp;lt; k$&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;While this was what I was leaning towards, I decided against this.  The key was in looking at the error estimate, $1 \over \sqrt{k}$ (same as the &lt;a href=&quot;http://en.wikipedia.org/wiki/MinHash#Variant_with_many_hash_functions&quot; rel=&quot;nofollow&quot;&gt;multiple hash function&lt;/a&gt; version).&lt;/p&gt;&#10;&#10;&lt;p&gt;Assuming that $k_1 = min(|S_1|,|S_2|)$, the error estimate for that hash comparison would be different than if $k$ samples were taken.  &lt;/p&gt;&#10;&#10;&lt;p&gt;While it would be accurate for &lt;em&gt;that&lt;/em&gt; comparison, it would be like comparing apples and oranges, as the expected error for all the other sets with $k$ elements is different.  This is unacceptable.&lt;/p&gt;&#10;&#10;&lt;p&gt;The other approach:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Void any sets that don't have more than $k$ elements for comparison. &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I've chosen to produce an error when this happens.  Given the reasons stated in the first option, I'd rather have an error than return an &lt;em&gt;erroneous value that I presume is correct&lt;/em&gt;.  &lt;/p&gt;&#10;&#10;&lt;p&gt;The error indicates to me that my selection of $k$ needs adjustment (or I need to take another look at the data) if I want to be able to process sets that small.&lt;/p&gt;&#10;&#10;&lt;p&gt;Whether it's an error or skipping the comparison of those sets, the point is the same &lt;strong&gt;don't produce an error that's assumed to be correct&lt;/strong&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-08-31T17:57:12.677" Id="35455" LastActivityDate="2012-08-31T22:23:05.153" LastEditDate="2012-08-31T22:23:05.153" LastEditorUserId="740" OwnerUserId="740" ParentId="35174" PostTypeId="2" Score="1" />
  
  
  
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I made 100 comparisons by applying chi-square test (tajima relative rate test) and got 100 p values. Do I need P-value adjustment? &lt;/p&gt;&#10;&#10;&lt;p&gt;If yes then which one correction will be suitable?&lt;/p&gt;&#10;&#10;&lt;p&gt;I have explored internet and also on this forum but didnt get answer of this particular question.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-09-01T08:59:05.710" Id="35493" LastActivityDate="2012-09-01T12:02:17.333" OwnerUserId="11889" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;&lt;multiple-comparisons&gt;&lt;p-value&gt;" Title="Do I need P value Adjustment for 100 pairwise comparisons?" ViewCount="240" />
  
  <row AnswerCount="0" Body="&lt;p&gt;When splitting attributes while constructing a decision tree, i can use information gain or information gain ratio to try and determine the best value to split the tree on.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;I'd use information gain when I have smaller numbers of distinct values (mostly nominal and ordinal values that are not continuous)&lt;/li&gt;&#10;&lt;li&gt;I'd use information gain ratio for attributes which have large numbers of distinct values (mainly continuous values)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;That said, do I necessarily have to choose &lt;em&gt;one&lt;/em&gt; method for use with all attributes across the entire tree?&lt;/p&gt;&#10;&#10;&lt;p&gt;My gut feeling is no, because information gain or information gain ratio for comparison to other attributes is not a relevant comparison; it's only relevant for comparison to a different value of the same attribute for the purposes of producing a split.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm thinking that one might be able to go so far as to swap them for different splits in the tree, depending on the training data. If the subset of values for that attribute is small compared to previous splits where the distinct values were large, one could use the value that's more biased to producing a more useful result.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-09-01T14:15:08.993" FavoriteCount="2" Id="35511" LastActivityDate="2012-09-01T14:39:37.810" LastEditDate="2012-09-01T14:39:37.810" LastEditorUserId="11032" OwnerUserId="740" PostTypeId="1" Score="5" Tags="&lt;cart&gt;&lt;entropy&gt;" Title="Information gain and information gain ratio: Do I have to pick just one?" ViewCount="838" />
  <row Body="&lt;p&gt;These are statistical models,  So, of course an error term is assumed.  Most likely it is an additive error term.  The book may not make it explicit but the fact that it is not shown in the equation should not be interpreted to mean that no error term is assumed.  The author probably thinks that the error term is implicitly assumed.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-09-01T14:46:00.733" Id="35517" LastActivityDate="2012-09-01T14:46:00.733" OwnerUserId="11032" ParentId="35513" PostTypeId="2" Score="0" />
  
  
  
  <row Body="&lt;p&gt;There is a statistics one course  starting up tomorrow at coursera. I guess it's much like the one at udacity, though I haven't followed that one. &#10;It will be with small assignments throughout the course. 75,000 have already signed up. &lt;/p&gt;&#10;&#10;&lt;p&gt;It's not a book, but it takes you directly back to the classroom again :-)&#10;Here's the link: &lt;a href=&quot;https://www.coursera.org/course/stats1&quot; rel=&quot;nofollow&quot;&gt;https://www.coursera.org/course/stats1&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2012-09-02T15:57:49.703" CreationDate="2012-09-02T02:36:11.577" Id="35548" LastActivityDate="2012-09-02T02:36:11.577" OwnerUserId="9154" ParentId="35544" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;You can rank them in many ways.&lt;/p&gt;&#10;&#10;&lt;p&gt;Whether you can validly take the mean is an interesting question. Taking the mean implies that the gap between each level on the Likert scale is the same, in some sense, as the other gaps. That is, the difference between (say) &quot;strongly disagree&quot; and &quot;disagree&quot; is the same as that between &quot;disagree&quot; and &quot;neutral&quot;. Taking the median makes no such assumption, but with only 5 levels, the medians may not vary much. You could also rank them by something like &quot;% of people saying 5&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Given that you have only 70 firms, you may want to look at each one in more detail. &lt;/p&gt;&#10;&#10;&lt;p&gt;You may also want to see if the reasons cluster somehow - do people who rate one variable highly also rate another one highly?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-09-02T12:22:12.020" Id="35567" LastActivityDate="2012-09-02T12:22:12.020" OwnerUserId="686" ParentId="35566" PostTypeId="2" Score="2" />
  
  
  
  
  
  
  
  
  <row AcceptedAnswerId="35635" AnswerCount="1" Body="&lt;p&gt;So say I am testing the predictive value of a predictor towards a &quot;DV&quot; within a survey data-set, between two time-points. Say also that I have measurements of both variables at all time points and that I can see that the DV and the predictor correlate at baseline. If I use linear regression or pearson correlation to evaluate the associaton between the predictor at baseline and the DV at a later time-point, do I need to correct for the correlation at baseline? I figure that if I do not correct, somehow, for this initial correlation I am not only testing prediction of the DV but also the test-retest-reliability of the predictor. Hope I am making sense.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-09-03T17:12:19.797" Id="35617" LastActivityDate="2012-09-05T06:57:03.073" OwnerUserId="13808" PostTypeId="1" Score="4" Tags="&lt;predictive-models&gt;" Title="How to correct for correlation at baseline between predictor and &quot;DV&quot;?" ViewCount="374" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;If my data follows a normal distribution I can standardize it for a machine learning algorithm, e.g. logistic regression, by subtracting the mean and dividing the result by the standard deviation.&lt;/p&gt;&#10;&#10;&lt;p&gt;But what should I do with data that is power law/Pareto distributed, e.g. words in a corpus? How should data in this case be standardized? Or should I prefer simple scale normalizatiom?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-09-03T20:37:50.570" Id="35628" LastActivityDate="2012-09-03T21:14:07.447" LastEditDate="2012-09-03T21:14:07.447" LastEditorUserId="88" OwnerUserId="8116" PostTypeId="1" Score="2" Tags="&lt;machine-learning&gt;&lt;logistic&gt;&lt;standardization&gt;" Title="Standardize/normalize power law distribution for machine learning" ViewCount="273" />
  <row AcceptedAnswerId="35671" AnswerCount="1" Body="&lt;p&gt;Here's a Q-Q plot for my sample (notice the logarithmic Y axis); $n = 1000$:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/QlMGU.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;br&gt;&#10;As pointed out by whuber, this indicates that the underlying distribution is left-skewed (the right tail is shorter).&lt;/p&gt;&#10;&#10;&lt;p&gt;Using &lt;code&gt;shapiro.test&lt;/code&gt; (on the log-transformed data) in R, I get a test statistic of $W = 0.9718$ and a p-value of $5.172\cdot10^{-13}$, which means that we formally &lt;strong&gt;reject&lt;/strong&gt; the null hypothesis $H_0 : \text{the sample is normal distributed}$ at the 95% confidence level.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is: Is this good enough in practice for further analysis assuming (log-)normality? In particular, I would like to calculate confidence intervals for the means of similar samples using the approximate method by Cox and Land (described in the paper: Zou, G. Y., cindy Yan Huo and Taleban, J. (2009). Simple confidence intervals for lognormal means and their differences with environmental applications. Environmetrics 20, 172–180):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ci &amp;lt;- function (x) {&#10;        y &amp;lt;- log(x)&#10;        n &amp;lt;- length(y)&#10;        s2 &amp;lt;- var(y)&#10;        m &amp;lt;- mean(y) + s2 / 2&#10;        z &amp;lt;- qnorm(1 - 0.05 / 2) # 95%&#10;        #z &amp;lt;- qnorm(1 - 0.10 / 2) # 90%&#10;        d &amp;lt;- z * sqrt(s2 / n + s2 * s2 / (2 * (n - 1)))&#10;&#10;        return(c(exp(m - d), exp(m + d)))&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I've noticed that the confidence intervals tend to be centered around a point which is slightly above the actual sample mean. For example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; mean(x)&#10;[1] 82.3076&#10;&amp;gt; y &amp;lt;- log(x)&#10;&amp;gt; exp(mean(y) + var(y) / 2)&#10;[1] 91.22831&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I think these two values should be the same under $H_0$.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-09-03T21:02:23.443" FavoriteCount="1" Id="35630" LastActivityDate="2012-09-05T21:24:05.237" LastEditDate="2012-09-05T21:24:05.237" LastEditorUserId="7290" OwnerUserId="13799" PostTypeId="1" Score="11" Tags="&lt;interpretation&gt;&lt;lognormal&gt;&lt;qq-plot&gt;" Title="Can I assume (log-)normality for this sample?" ViewCount="528" />
  <row AnswerCount="0" Body="&lt;p&gt;Say you have a longitudinal data set with 5 measurement points, which begins at 200-300 participants and declines to about 50. There is an intervention between time-point 2 and 3. There's 3 continuous IVs and 1 continuous DV measured at all timepoints. One group. The key interest is the predictive value of the IVs as far into the future as possible, not the effect of the intervention. What alternative types of modeling are applicable? Do you do a bunch of regressions of the IVs on the DV between all interesting 2 timepoint-spans? Or is there some more effective type of modeling which extracts the same information in a more compact manner using less analyses? Is there any idea to do anything but a linear model given the low number of time-points? &lt;/p&gt;&#10;&#10;&lt;p&gt;I am thankful for any answers.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2012-09-03T23:03:25.437" Id="35637" LastActivityDate="2012-09-03T23:44:13.467" LastEditDate="2012-09-03T23:44:13.467" LastEditorUserId="13808" OwnerUserId="13808" PostTypeId="1" Score="1" Tags="&lt;repeated-measures&gt;&lt;longitudinal&gt;" Title="Looking for pointers on how/if to model a &quot;low t and high n&quot; longitudinal data set to evaluate outcome-predictors" ViewCount="55" />
  
  <row AnswerCount="0" Body="&lt;p&gt;A variable was categorized into 10 equally spaced intervals from a continuous variable which was originally in proportion. Now, we have to use this variable in CATREG procedure. If we choose the optimal scaling level to be 'ordinal' then quantification for the last seven categories are the same. That means the observations that originally had values ranging from 0.31 to 1.00, now receive the same quantified value after optimal scaling. If we use 'numeric' as the quantification level instead then different categories receive different quantification (as it should be). &lt;/p&gt;&#10;&#10;&lt;p&gt;But is using 'numeric' as the optimal scaling level valid here? Although the variable was numeric originally, but later that was transformed to an equally spaced 10-category variable. Note that, using 'nominal' gives quite different quantification, but some of the category quantification is still the same. I guess keeping in mind the nature of the original variable, different categories should not receive the same quantification. Could someone please clarify this?    &lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-09-04T02:14:12.777" Id="35644" LastActivityDate="2012-09-04T02:14:12.777" OwnerUserId="12603" PostTypeId="1" Score="0" Tags="&lt;optimal-scaling&gt;" Title="Optimal scaling level for variables categorized from continuous data" ViewCount="254" />
  <row AcceptedAnswerId="35654" AnswerCount="2" Body="&lt;p&gt;&lt;em&gt;Previously I asked this question to R-help (finance group) however, could not get a satisfactory reply (probably this is not directly related to R.) Here is my question:&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there any possible way to get 2 continuous distributions (defined on the real line) such that, both distributions have same 1% &amp;amp; 5% quantiles (or, any one of these two. However to me this is a weak condition!), however those distributions are notably different in shape, location etc.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can somebody please point me any example of such 2 distribution?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-09-04T05:54:51.083" Id="35649" LastActivityDate="2012-09-04T08:29:26.233" LastEditDate="2012-09-04T06:02:26.347" LastEditorUserId="183" OwnerUserId="5738" PostTypeId="1" Score="2" Tags="&lt;distributions&gt;" Title="Two continuous distributions with same 1% &amp; 5% quantiles however  distributions are notably different in shape, location etc?" ViewCount="120" />
  <row Body="&lt;p&gt;Take two distribution functions $F_1$, $F_2$ (increasing functions from $\mathbb{R} \to [0,1]$...) such that $F_1(0.01)=F_2(0.01)$ and $F_1(0.05)=F_2(0.05)$. There are infinitely many possibilities !&lt;/p&gt;&#10;&#10;&lt;p&gt;For instance you can fix some values for the $1\%$ and $5\%$ quantiles and use the optim() R function to determine the parameters of a normal distribution which fits these values and on the other hand determine the parameters of a log-normal distribution which fits these values. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-09-04T07:03:03.970" Id="35652" LastActivityDate="2012-09-04T08:29:26.233" LastEditDate="2012-09-04T08:29:26.233" LastEditorUserId="8402" OwnerUserId="8402" ParentId="35649" PostTypeId="2" Score="3" />
  
  
  
  <row AcceptedAnswerId="35670" AnswerCount="1" Body="&lt;p&gt;I have an experiment which is executed on hundreds of computers distributed all over the world that measures the occurences of certain events. The events each depend on one another so I can order them in increasing order and then calculate the time difference.&lt;/p&gt;&#10;&#10;&lt;p&gt;The events should be exponentially distributed but when plotting a histogram this is what I get:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Oskbm.png&quot; alt=&quot;Histogram of events&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The imprecision of the clocks at the computers causes some of the events to be assigned a timestamp earlier than that of the event they depend on.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm wondering whether the clock synchronization can be blamed for the fact that the peak of the PDF is not at 0 (that they shifted the whole thing to the right)?&lt;/p&gt;&#10;&#10;&lt;p&gt;If the clocks differences are normally distributed, can I just assume that the effects will compensate for one another and thus just use the calculated time diff?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-09-04T13:21:13.203" Id="35666" LastActivityDate="2012-09-05T06:48:16.563" OwnerUserId="13714" PostTypeId="1" Score="10" Tags="&lt;error&gt;&lt;measurement-error&gt;&lt;exponential&gt;" Title="Correcting for normally distributed clock inprecision" ViewCount="124" />
  
  
  <row Body="&lt;p&gt;Yes. Your data $\bf must$  follow the distribution under which you will make your estimations or under which you will run you ML estimator. If this is not the case your ML will not be ML for that data. You should transform your data or create a data which follows exponential distribution.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2012-07-15T12:08:20.217" Id="35690" LastActivityDate="2012-07-15T12:08:20.217" OwnerDisplayName="Seyhmus Güngören" OwnerUserId="13961" ParentId="35689" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;Multicolinearity is not generally a problem for SVMs.  Ridge regression is often used where multicolinearity is an issue, as the regularisation term resolves the invertibility issue by adding a ridge.  The SVM uses the same regularisation term as ridge regression does, but with the hinge loss in place of the squared error.&lt;/p&gt;&#10;&#10;&lt;p&gt;Ridge regression has a link with PCA &lt;a href=&quot;http://tamino.wordpress.com/2011/02/12/ridge-regression/&quot; rel=&quot;nofollow&quot;&gt;as explained by Tamino&lt;/a&gt;, essentially it penalises principal components with large eigenvalues less than components with low eigenvalues, so it is a bit like having a soft selection of the most important PCs, rather than a hard (binary) selection.&lt;/p&gt;&#10;&#10;&lt;p&gt;The important thing is to make sure that the regularisation parameter, C, and any kernel parameters are tuned correctly and carefully, preferably by minimising an appropriate cross-validation based model selection criterion.  This really is the key to getting good results with an SVM (and kernel methods in general).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-09-05T10:54:03.293" Id="35712" LastActivityDate="2012-09-05T10:54:03.293" OwnerUserId="887" ParentId="35708" PostTypeId="2" Score="3" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm doing statistical analysis on a psychology study I ran a short while ago, and I'm starting to think that I've designed myself into a corner.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the study, we measured participants' pointing responses after traveling in a virtual environment. There were three factors, each nested within another:&lt;/p&gt;&#10;&#10;&lt;p&gt;Physical rotation: 2 levels&#10;Visuals: 3 levels&#10;Turning angle: 10 levels&lt;/p&gt;&#10;&#10;&lt;p&gt;Each participant was subjected to all variations of these treatments, so they ran 60 trials each. However, due to this large number of factors, we restricted the randomization of the order of treatments for each participant. In picture form, our design looks like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/zEITj.png&quot; alt=&quot;Split-split-split plot design&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;M represents the physical rotation condition, V is the visual condition, and A is the turning angle condition. Each table represents one full session of 60 trials for a participant assigned to that group. The numbers in the cells represent the order in which participants assigned to that group perform trials. We ran 36 participants in total, and assigned each randomly to a group so that each of the four groups had 9 participants.&lt;/p&gt;&#10;&#10;&lt;p&gt;This is a bit complex, so please let me know if I can explain something better. The groups represent the four different permutations of the 60 conditions we decided on, to make the best out of our limited number of participants.&lt;/p&gt;&#10;&#10;&lt;p&gt;Group 1:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;M1|V1&lt;/li&gt;&#10;&lt;li&gt;M2|V1&lt;/li&gt;&#10;&lt;li&gt;M1|V2&lt;/li&gt;&#10;&lt;li&gt;M2|V2&lt;/li&gt;&#10;&lt;li&gt;M1|V3&lt;/li&gt;&#10;&lt;li&gt;M2|V3&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Group 2:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;M1|V3&lt;/li&gt;&#10;&lt;li&gt;M2|V3&lt;/li&gt;&#10;&lt;li&gt;M1|V2&lt;/li&gt;&#10;&lt;li&gt;M2|V2&lt;/li&gt;&#10;&lt;li&gt;M1|V1&lt;/li&gt;&#10;&lt;li&gt;M2|V1&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Group 3:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;M2|V1&lt;/li&gt;&#10;&lt;li&gt;M1|V1&lt;/li&gt;&#10;&lt;li&gt;M2|V2&lt;/li&gt;&#10;&lt;li&gt;M1|V2&lt;/li&gt;&#10;&lt;li&gt;M2|V3&lt;/li&gt;&#10;&lt;li&gt;M1|V3&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Group 4:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;M2|V3&lt;/li&gt;&#10;&lt;li&gt;M1|V3&lt;/li&gt;&#10;&lt;li&gt;M2|V2&lt;/li&gt;&#10;&lt;li&gt;M1|V2&lt;/li&gt;&#10;&lt;li&gt;M2|V1&lt;/li&gt;&#10;&lt;li&gt;M1|V1&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Within each of the six blocks, participants completed all ten turning angle conditions, in random order.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, we have one factor that is randomized (turning angle), one that is alternated (movement condition) and one that is ordered (visual condition). Each of these is nested within another, making for a split-split-split plot design.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, what we are analyzing is how the pointing responses differ between conditions, and between participants. I have read everything I could find on split-plots, and even found some references to split-split-split plots, but I am still very unsure of how to actually crunch the numbers in R or SPSS.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would be very grateful for any advice, hints or references to useful reading material that could help me in this. Please ask if you have any questions, or if I can explain something more clearly.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-09-05T13:14:52.193" Id="35715" LastActivityDate="2012-09-05T13:24:12.750" LastEditDate="2012-09-05T13:24:12.750" LastEditorUserId="12797" OwnerUserId="12797" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;anova&gt;&lt;spss&gt;&lt;split-plot&gt;" Title="Split-split-split plot cross-over design... is this even possible?" ViewCount="526" />
  
  <row AnswerCount="3" Body="&lt;p&gt;Discrimination parameters in two-parameter model from &lt;a href=&quot;http://en.wikipedia.org/wiki/Item_response_theory&quot; rel=&quot;nofollow&quot;&gt;IRT&lt;/a&gt; are usually considered item parameters.&lt;/p&gt;&#10;&#10;&lt;p&gt;But I've come to doubt it.&#10;Think about psychophysics; for example, detecting luminance.&#10; I don't think anyone would say that discrimination (i.e., slope) parameter applies to luminance itself.&#10; It comes from human perception, so I figure it's a person parameter and it could be &#10; different for each person.&lt;/p&gt;&#10;&#10;&lt;p&gt;What about test situation? Is it without any doubt that &#10; the discrimination parameter is considered as an item parameter?&#10; I doubt it is, especially given the fact that there is room for differential item functioning (&lt;a href=&quot;http://en.wikipedia.org/wiki/Differential_item_functioning&quot; rel=&quot;nofollow&quot;&gt;DIF&lt;/a&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;So I'm concerned with the fact there's any theoretical basis for saying that&#10; discrimination parameters are item-parameters.&lt;/p&gt;&#10;&#10;&lt;p&gt;I think I could model person-specific discrimination parameter model, in which&#10; item-discrimination parameter is determined by person-specific discrimination parameters in the&#10; group. That would justify DIF. &lt;/p&gt;&#10;&#10;&lt;p&gt;So, my question would sum up like this:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Is that if there's any theoretical/empirical basis for saying that&#10;discrimination parameters are item-parameters? How can one be sure of it?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Is there any related reference for &#10;person-discrimination parameter model mentioned above?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2012-09-05T16:27:28.643" FavoriteCount="1" Id="35724" LastActivityDate="2014-02-14T15:05:37.723" LastEditDate="2012-09-05T21:25:57.207" LastEditorUserId="930" OwnerUserId="11242" PostTypeId="1" Score="4" Tags="&lt;psychometrics&gt;&lt;irt&gt;" Title="Are discrimination parameters in two-parameter IRT models only specific to items?" ViewCount="282" />
  
  
  <row Body="&lt;p&gt;Vector auto-regression generalizes time-series methods allowing multiple time series to be analyzed together.  Using standard (i.e., Box-Jenkins) time-series methods, a series can be modeled such that a current value is, in part, a function of previous values (called lags).  Vector auto-regression allows a series to also be partly a function of the lags of a second (or more) series, and the other series to be partly a function of the lags of the first series, simultaneously. &lt;/p&gt;&#10;&#10;&lt;p&gt;One important use of VAR is testing for Granger causality.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-09-05T18:47:58.683" Id="35733" LastActivityDate="2013-03-31T20:38:07.307" LastEditDate="2013-03-31T20:38:07.307" LastEditorUserId="7290" OwnerUserId="7290" PostTypeId="5" Score="0" />
  
  <row Body="&lt;p&gt;I don't think it is a good idea to drop the main effects given that the uplift models try to model the second order incremental effect on the top of the main effects (the response variable is not incremental since a particular subject cannot be in both test and in control in the same time.) &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;The True Lift Model - A Novel Data Mining Approach to Response Modeling in Database Marketing&lt;/em&gt; by Victor Lo includes a simple example of using logistic regression - it includes both the main effect and the interaction terms.&lt;/p&gt;&#10;&#10;&lt;p&gt;The step-wise variable selection that you are implying is possible but I believe there are better methods based on regularization (elastic net etc.) As for including higher order interactions, why not? - they are just additional variables. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Real-World Uplift Modelling with Significance-Based Uplift Trees&lt;/em&gt; by Radcliffe and Surry states that variable selection is very important so I guess one should be careful with using higher order interactions.&lt;/p&gt;&#10;&#10;&lt;p&gt;If I were to try this I would first build a good quality main effect model (based on the control dataset only by utilizing regularization, interactions etc.), then I would throw in the test/control interaction term and re-fit the model on the full training dataset (using the same tuning hyper-parameters)&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-09-05T22:46:58.997" Id="35748" LastActivityDate="2012-09-05T22:46:58.997" OwnerUserId="6129" ParentId="35018" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;There is a little restoring force around the mean in a hypergeometric distribution. If your first few balls have been black more than average, then the probability that the next ball is white is a little higher than if the first few balls had been white more than average. This restoring force differs when you sample $100$ balls from $2000$ versus $50$ from $1000$ twice. As an extreme, you could sample $1$ from $20$ $100$ times, and then there would be no restoring force.&lt;/p&gt;&#10;&#10;&lt;p&gt;One measure of the spread of a distribution is the variance. The variance when you draw n balls out of w+b is &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Hypergeometric_distribution&quot; rel=&quot;nofollow&quot;&gt;$$ \frac{n (w+b-n) w b }{(w+b)^2(w+b-1)}.$$&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;For $n=100, w=100, b=1900,$ the variance is $\frac{9025}{1999} = 4.51476.$&lt;/p&gt;&#10;&#10;&lt;p&gt;For $n=50, w=50, b=950,$ the variance is $\frac{9025}{3996}$. When you do this twice independently and add the results, you get twice the variance, $\frac{9025}{1998} = 4.51702.$&lt;/p&gt;&#10;&#10;&lt;p&gt;For $n=1, w=1, b=19,$ the variance is $\frac{19}{400}.$ If you do this $100$ times independently and add the results, you get $100$ times the variance, $\frac{19}{4} = \frac{9025}{1900} = 4.75.$&lt;/p&gt;&#10;&#10;&lt;p&gt;The difference between $\frac{9025}{1999}$ and $\frac{9025}{1998}$ is not very large, and you won't easily tell the difference by eye-balling results. There is a larger difference if you look at the exact probabilities of events far from the mean. &lt;/p&gt;&#10;&#10;&lt;p&gt;$0$ is not very far from the mean ($5$), so the probabilities are not much different. The probability that you get $0$ white out of $100$ when everything is put together is $0.0051735.$ The probability that you get $0$ white out of $50$ twice is $0.0051707.$ The probability that you get $0$ out of $1$ $100$ times is $0.00592053.$&lt;/p&gt;&#10;&#10;&lt;p&gt;$40$ is farther from the mean, and there is a larger proportional difference between the probabilities. The chance that you get $40$ out of $100$ is $3.12262 \times 10^{-29}.$ The chance to get a total of $40$ when you take $50$ from each half is $3.35434 \times 10^{-29},$ about $7\%$ larger. The chance to get $40$ if you draw $1$ ball out of each group of $20$ is $5.75970 \times 10^{-26} = 5759.70 \times 10^{-29},$ almost $2000$ times as large. Even that far from the mean, you don't see a large difference between drawing $100$ out of $2000$ and drawing $50$ from $1000$ twice, but there is a larger difference with drawing $1$ out of $20$ $100$ times. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-09-05T23:44:34.323" Id="35749" LastActivityDate="2012-09-05T23:44:34.323" OwnerUserId="11981" ParentId="35745" PostTypeId="2" Score="3" />
  
  <row AnswerCount="0" Body="&lt;p&gt;$Y^n$ are the observations of our HMM, where $Y_i=a_i$ is a single observation, where $a_i \epsilon \{0,1\}$. For example, $Y^n = k^n$ where $k^n=\{0,1,1,0\}$&lt;/p&gt;&#10;&#10;&lt;p&gt;$X^n$ are the actual states of our HMM, of which we know some (any) $X_i$.&lt;/p&gt;&#10;&#10;&lt;p&gt;How can I both 1.intuitively and 2.carefully (using actual math) reason about the following fact: If we have $Y^n = k^n$ where $k^n=\{0,1,1,0\}$ and &lt;strong&gt;any&lt;/strong&gt; $X_i$ then we know the entire $X^n$. &lt;/p&gt;&#10;&#10;&lt;p&gt;I think it has something to do with the Markov implication that &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Conditioned on the present, the past and the future are independent&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;but I'm not very clear on how to proceed. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-09-06T01:40:20.460" Id="35753" LastActivityDate="2012-09-06T19:19:04.573" LastEditDate="2012-09-06T19:19:04.573" LastEditorUserId="11687" OwnerUserId="11687" PostTypeId="1" Score="2" Tags="&lt;self-study&gt;&lt;markov-process&gt;&lt;hidden-markov-model&gt;" Title="In a hidden Markov model, how do all observations and one state give you all states?" ViewCount="109" />
  
  
  
  <row Body="&lt;p&gt;You're testing the interaction between a 2-level repeated measures variable and a quantitative between-subjects variable.  I gave a rather extensive answer for how to analyze repeated measures models in R a few days ago &lt;a href=&quot;http://stats.stackexchange.com/questions/35590/linear-regression-with-repeated-measures-in-r/35632#35632&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-09-06T13:01:16.437" Id="35784" LastActivityDate="2012-09-06T13:01:16.437" OwnerUserId="11091" ParentId="35778" PostTypeId="2" Score="1" />
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have gene expression data in two samples like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Gene           s1   s2&#10;C9orf152    2.96295 2.99861&#10;RPS11       11.2968 11.0775&#10;ELMO2       8.16786 7.6039&#10;CREB3L1     6.24832 6.69416&#10;PNMA1       8.11559 7.90189&#10;MMP2        8.32177 7.06863&#10;TMEM216     5.48133 5.68662&#10;C10orf90    2.91206 3.11626&#10;ZHX3        5.47916 5.31362&#10;ERCC5       5.94527 5.92997&#10;GPR98       3.09796 3.24092&#10;RXFP3       5.17484 5.14564&#10;CTAGE10P    2.71189 2.60001&#10;APBB2       7.16389 7.40401&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now I want to know if any test can figure out the differential expressed genes between &lt;code&gt;S1&lt;/code&gt; and &lt;code&gt;S2&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;T-test can only give one p-value between two samples, right? I want to figure out each p-value for each gene between &lt;code&gt;s1&lt;/code&gt; and &lt;code&gt;s2&lt;/code&gt;.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2012-09-06T22:30:27.277" Id="35814" LastActivityDate="2012-09-06T23:52:03.957" LastEditDate="2012-09-06T23:52:03.957" LastEditorUserId="11032" OwnerUserId="13866" PostTypeId="1" Score="0" Tags="&lt;multiple-comparisons&gt;&lt;group-differences&gt;" Title="Differential expression gene analysis" ViewCount="183" />
  
  <row AcceptedAnswerId="35832" AnswerCount="2" Body="&lt;blockquote&gt;&#10;  &lt;p&gt;regression analysis helps one understand how the typical value of the dependent variable changes...&#10;   -- &lt;a href=&quot;http://en.wikipedia.org/wiki/Regression_analysis&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Regression_analysis&lt;/a&gt;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;What does this mean? What &quot;typical value&quot; is it talking about? What kind of change is it looking for? Positive or negative or what? Change in what? Percentage? I'm confused. &lt;/p&gt;&#10;&#10;&lt;p&gt;Lessons learnt:&lt;br&gt;&#10;* greened answer is good in that it had concrete clearness&lt;br&gt;&#10;* wikipedia documents a ton of info, most of which are unreadable and unusable&lt;br&gt;&#10;* most (80%) ppl, while im sure good meaning, just dont know how to give good answers&lt;br&gt;&#10;* and only human... have so many flaws, and thereby closes good questions  &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-09-07T03:09:34.030" Id="35827" LastActivityDate="2012-10-07T08:23:26.493" LastEditDate="2012-10-07T08:23:26.493" LastEditorUserId="13898" OwnerUserId="13898" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;basic-concepts&gt;" Title="How does regression analysis help one understand how the typical value of the dependent variable change?" ViewCount="479" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to load multiple csv files (each file having near about 37 lacks records). Then I have to merge/join these data files (overall 2.5 crores of records in a single data set). But sometimes I am getting error due to out of memory. What's the maximum file size that can R read through read.csv or read.table ? How to overcome these issues ? Please suggest.&lt;/p&gt;&#10;" ClosedDate="2012-09-07T12:25:49.207" CommentCount="2" CreationDate="2012-09-07T09:37:12.927" Id="35852" LastActivityDate="2012-09-07T09:37:12.927" OwnerUserId="13882" PostTypeId="1" Score="1" Tags="&lt;r&gt;" Title="Maximum File Size that R can read" ViewCount="2427" />
  <row AnswerCount="3" Body="&lt;p&gt;I have a panel of daily stock prices $\{Y_{it}\}$, $t \in \{1,...,1000\}$ and some event that occurs at $t=700$ which causes the average stock price to decrease by about 10% over the next 15 days.&lt;/p&gt;&#10;&#10;&lt;p&gt;How do I answer the question &quot;which stocks fell first&quot;? I want to know of the characteristics of stocks that tended to be correlated with very early falls (like, first 1 or 2 days). I have many ideas for sub-sample analysis but none that I want to use involving econometrics. &lt;/p&gt;&#10;&#10;&lt;p&gt;Broad econometric-based ideas welcome.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can survival analysis be used here? &lt;/p&gt;&#10;&#10;&lt;p&gt;Note; I did make an earlier post on this issue that was answered, but I constrained the question way too much and didn't get what I wanted. &lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT: &lt;em&gt;Please don't absolutely answer the question (up to model specification). I just want ideas/suggestions of areas that might be good and a very general approach/suggestion. Then I want to develop it all myself from there.&lt;/em&gt; &lt;/p&gt;&#10;" CommentCount="7" CreationDate="2012-09-07T14:42:23.380" Id="35864" LastActivityDate="2012-09-13T19:12:13.933" LastEditDate="2012-09-13T18:04:14.280" LastEditorUserId="10849" OwnerDisplayName="user13253" PostTypeId="1" Score="3" Tags="&lt;finance&gt;" Title="Which stocks fell first after some event?" ViewCount="279" />
  
  <row Body="&lt;p&gt;$(\Sigma-\lambda I)y=0$ The characteristic Equation&lt;/p&gt;&#10;&#10;&lt;p&gt;$ =&amp;gt;y'(\Sigma-\lambda I)y=0$ Pre-Multipling $y'$&lt;/p&gt;&#10;&#10;&lt;p&gt;$=&amp;gt;y'\Sigma y=y'\lambda y$&lt;/p&gt;&#10;&#10;&lt;p&gt;$=&amp;gt;\lambda=\frac{y'\Sigma y}{y'y}=\frac{Var(y'(X-\mu))}{\sum y_i^2} \geq 0$ Asuming $\sum y_i^2\neq 0$&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-09-07T19:47:28.903" Id="35881" LastActivityDate="2012-09-07T19:47:28.903" OwnerUserId="12710" ParentId="35861" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I understand heterogeneity to be any difference between individuals. Observed heterogeneity usually consists of the covariates and unobserved heterogeneity consists of any unobserved difference like ability or effort.&lt;/p&gt;&#10;&#10;&lt;p&gt;Endogeneity refers to the relationship between the observed and unobserved variables, namely that they are dependent on one another.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-09-07T20:26:50.313" Id="35882" LastActivityDate="2012-09-08T05:31:18.403" LastEditDate="2012-09-08T05:31:18.403" LastEditorUserId="3826" OwnerUserId="13918" ParentId="10890" PostTypeId="2" Score="2" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;Let $f$ be a nonnegative function. I am interested in finding $z \in [0,1]$ such that&#10;$$ \int_0^{z} f(x)\,dx = \frac{1}{2}\int_0^1 f(x)\,dx$$ The caveat: all I can do is sample $f$ at points in $[0,1]$. I can, however, choose the locations where I sample $f$ randomly, if I so desire.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Questions:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Is it possible to obtain an unbiased estimate of $z$ after finitely many samples? If so, what is the smallest posssible variance of such an estimate after $k$ samples?&lt;/li&gt;&#10;&lt;li&gt;If not, what procedures are available to estimate $z$, and what are their associated convergence times. &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;As pointed out by Douglas Zare in the comments, this can be very difficult to do if the function is close to zero or very large. Fortunately, the function I need to use this for is bounded from above and below, so lets suppose that $1 \leq f(x) \leq 2$. Moreover, we can also assume that $f$ is Lipschitz or even differentiable if that helps. &lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-09-07T22:50:58.787" FavoriteCount="1" Id="35892" LastActivityDate="2012-09-26T02:18:33.677" LastEditDate="2012-09-11T03:11:30.500" LastEditorUserId="2970" OwnerUserId="13923" PostTypeId="1" Score="8" Tags="&lt;sampling&gt;&lt;monte-carlo&gt;&lt;quantiles&gt;&lt;quasi-monte-carlo&gt;" Title="Solving a simple integral equation by random sampling" ViewCount="518" />
  
  <row Body="&lt;p&gt;Answer:&lt;/p&gt;&#10;&#10;&lt;p&gt;The rate of x is incorrectly specified as &lt;code&gt;mu=exp(g0+g1*x)&lt;/code&gt;. Once this is corrected to &lt;code&gt;mu = (g0+g1*x)&lt;/code&gt; LIFEREG and NLMIXED arrive at the same answer.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-09-08T06:33:25.963" Id="35906" LastActivityDate="2012-09-08T06:33:25.963" OwnerUserId="5836" ParentId="35905" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;The first option does not take account of the reduced variance that you get from the sample   The first option gives you five lower 95% confidence bounds for the mean based on a sample of size 1 in each case.  Combining them by averaging does not create a bound that you can interpret as a lower 95% bound.  No one would do that.  The second option is what is done. The average of the five independent observations has a variance smaller by a factor of 6 than the variance for a single sample.  It therefore gives you a much better lower bound than any of the five you calculated the first way.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also if the X$_i$ can be assumed to be iid normal then T will be normal.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-09-08T18:41:52.720" Id="35931" LastActivityDate="2012-09-08T18:41:52.720" OwnerUserId="11032" ParentId="35927" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;Is there a generic answer as to how the log transform affects the Pearson correlation between two variables, i.e., is $\rho(\log(x),\log(y))  &amp;lt; \rho(x,y)$ always or the other way around and why?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-09-08T23:35:58.297" FavoriteCount="1" Id="35941" LastActivityDate="2012-09-09T02:20:58.780" LastEditDate="2012-09-09T02:20:58.780" LastEditorUserId="2970" OwnerUserId="13949" PostTypeId="1" Score="3" Tags="&lt;correlation&gt;&lt;pearson&gt;" Title="Effect of log transform on Pearson correlation" ViewCount="529" />
  <row AcceptedAnswerId="37458" AnswerCount="1" Body="&lt;p&gt;I had heard that there is a body of literature devoted to the following problem:&lt;/p&gt;&#10;&#10;&lt;p&gt;You have a dataset and you produced a good predictive model for it. Now you have a different dataset, derived from different instruments, or different data sources, but similar enough that you can hope to scale one to the other so that you can use the predictive model on it.&lt;/p&gt;&#10;&#10;&lt;p&gt;Alternately, you do not have enough data to produce a model and hope that combining different datasets by scaling one to the other will help you reach the quantity of data you need.&lt;/p&gt;&#10;&#10;&lt;p&gt;First off what is this problem called so that I can search it more effectively? And also does anyone know of a good recent survey of techniques for this, either in book or paper form? I currently have access to most academic journals so those links work for me as well.&lt;/p&gt;&#10;&#10;&lt;p&gt;Say I have dataset A that I have a model for, dataset B occupies the same database schema but is from a difference source with different factors that are not included in my feature set.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Initially my intuition was to construct a QQplot and fit lines (or curves) to features that I thought should be similar. If the difference in the way feature 1 from A increases is similar to the way feature 1 from B increases but with a constant factor then fitting a line can reveal this factor. If the difference was exponential or logarithmic then I could scale using a fitted function. In this way I could constrain the way one variable increased to fit how another variable increased.&lt;/p&gt;&#10;&#10;&lt;p&gt;However this is just my intuition. I can certainly test it for overfitting but when I had heard that there was a lot of literature devoted to this subject then it seemed as though I should learn a few ways in which I could question my assumptions. It would probably be good for me to review the literature. &lt;/p&gt;&#10;&#10;&lt;p&gt;Does anyone know what the tag for this literature might be?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-09-09T02:40:15.767" Id="35944" LastActivityDate="2014-06-15T20:08:33.737" LastEditDate="2012-09-11T00:52:02.407" LastEditorUserId="13950" OwnerUserId="13950" PostTypeId="1" Score="3" Tags="&lt;dataset&gt;&lt;terminology&gt;" Title="Fitting a dataset into another dataset" ViewCount="210" />
  
  <row Body="&lt;p&gt;I've not read the book, but as stated the criticism seems pretty unreasonable to me.  If extreme events are important, then statistics has appropriate tools in the toolbox, such as extreme value theory, and a good statistician will know how to use them (or at least find out how to use them and will be sufficiently engaged with the purpose of the analysis to look).  The criticism seems to be &quot;statistics is bad because there are bad statisticians that only know about normal distributions&quot;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-09-09T13:06:39.203" Id="35957" LastActivityDate="2012-09-09T13:06:39.203" OwnerUserId="887" ParentId="35956" PostTypeId="2" Score="15" />
  <row Body="&lt;p&gt;Tsochantaridis et. al. discuss this problem in &quot;Large Margin Methods for Structured and Interdependent Output Variables.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;They compare a number of structured max-margin formulations and observe that the &quot;slack rescaling&quot; formulation is scale invariant while the &quot;margin rescaling&quot; formulation is not (Section 2.2.5). So the $\Delta(y_i,y)$ function in margin scaling formulation does need to be adjusted according to the scaling of the feature function.&lt;/p&gt;&#10;&#10;&lt;p&gt;Feature function scaling, though, is probably not a real problem in practice because the feature function is usually used to decompose a large output space and wouldn't be used to hide arbitrary scaling factors.&lt;/p&gt;&#10;&#10;&lt;p&gt;The factor of $.5$ that you mention may be due to mapping the standard two-class svm formulation to the structured formulation. The feature mapping portion of the constraint in the structured problem can be written as:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;w_{y_i} \cdot \Psi(x,y_i) - w_{\neg y_i} \cdot \Psi(x,\neg y_i)\\&#10;\quad=(w_{y_i} \cdot x) y_i - (w_{\neg y_i} \cdot x) \neg y_i\\&#10;\quad=(w_{y_i} + w_{\neg y_i}) \cdot x \; y_i&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;If you assume that $w_{y_i}=w_{\neg y_i}$ and the equivalent regularizer from the standard 2-class svm is $||w||^2\dot = ||w_{y_i} + w_{\neg y_i}||^2$, then this would penalize the weights twice as much as the structured regularizer, $||w_{y_i}||^2 + ||w_{\neg y_i}||^2$. Hence, you would want the value of the structured constraint to be $(w_{y_i} + w_{\neg y_i}) \cdot x\;y_i/2\ge 1$ (if you assume a 0-1 loss).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-09-09T15:29:11.850" Id="35965" LastActivityDate="2012-09-11T14:21:56.867" LastEditDate="2012-09-11T14:21:56.867" LastEditorUserId="9595" OwnerUserId="9595" ParentId="34600" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;The function &lt;code&gt;predictSurvProb&lt;/code&gt; in the &lt;code&gt;pec&lt;/code&gt; package can give you absolute risk estimates for new data based on an existing cox model if you use R.&lt;/p&gt;&#10;&#10;&lt;p&gt;The mathematical details I cannot explain.&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT: The function provides survival probabilities, which I have so far taken as 1-(Event probability).&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-09-10T13:27:12.903" Id="36016" LastActivityDate="2012-09-11T12:39:33.150" LastEditDate="2012-09-11T12:39:33.150" LastEditorUserId="10064" OwnerUserId="10064" ParentId="36015" PostTypeId="2" Score="6" />
  
  
  
  
  
  <row AcceptedAnswerId="36059" AnswerCount="1" Body="&lt;p&gt;I have some trouble showing sufficiency for largest order statistic ${x}_{n}$.&#10;This is from &lt;a href=&quot;http://www.springerlink.com/content/978-0-387-98502-2&quot; rel=&quot;nofollow&quot;&gt;Casella's text&lt;/a&gt;, problem 1.6.3.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let ${p}_{\theta}$ be a density function.&lt;br&gt;&#10;${p}_{\theta}{x}=c({\theta})f(x)$ for $0&amp;lt;x&amp;lt;\theta$.  &lt;/p&gt;&#10;&#10;&lt;p&gt;If ${X}_{1},{X}_{2},....{X}_{n}$ are iid with density ${p}_{\theta}$, show that ${X}_{(n)}$ is sufficient for $\theta$.&lt;/p&gt;&#10;&#10;&lt;p&gt;I understand that by the definition of sufficiency, if the summary statistic, T, is independent of the parameter $\theta$, for all t, then it is sufficient.&lt;/p&gt;&#10;&#10;&lt;p&gt;How do I actually show that?  It seems obvious that $c(\theta)$ and $f(x)$ will not get involved with each other.  And there is not an explicit formula for me to work with, like normal or student t.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2012-09-10T19:20:25.080" Id="36047" LastActivityDate="2012-09-29T19:13:27.093" LastEditDate="2012-09-29T19:13:27.093" LastEditorUserId="686" OwnerUserId="13985" PostTypeId="1" Score="1" Tags="&lt;self-study&gt;&lt;inference&gt;&lt;order-statistics&gt;" Title="How to show order statistic is sufficient" ViewCount="936" />
  <row Body="&lt;p&gt;I read the Black Swan a couple of years ago.  The Black Swan idea is good and the attack on the ludic fallacy (seeing things as though they are dice games, with knowable probabilities) is good but statistics is outrageously misrepresented, with the central problem being the wrong claim that all statistics falls apart if variables are not normally distributed.  I was sufficiently annoyed by this aspect to write Taleb the letter below:&lt;/p&gt;&#10;&#10;&lt;p&gt;Dear Dr Taleb&lt;/p&gt;&#10;&#10;&lt;p&gt;I recently read &quot;The Black Swan&quot;.  Like you, I am a fan of Karl Popper, and I found myself agreeing with much that is in it.  I think your exposition of the ludic fallacy is basically sound, and draws attention to a real and common problem.  However, I think that much of Part III lets your overall argument down badly, even to the point of possibly discrediting the rest of the book.  This is a shame, as I think the arguments with regard to Black Swans and &quot;unknown unknowns&quot; stand on their merits without relying on some of the errors in Part III.&lt;/p&gt;&#10;&#10;&lt;p&gt;The main issue I wish to point out - and seek your response on, particularly if I have misunderstood issues - is your misrepresentation of the field of applied statistics.  In my judgement, chapters 14, 15 and 16 depend largely upon a straw man argument, misrepresenting statistics and econometrics.  The field of econometrics that you describe is not the one that I was taught when I studied applied statistics, econometrics, and actuarial risk theory (at the Australian National University, but using texts that seemed pretty standard).  The issues that you raise (such as the limitations of Gaussian distributions) are well and truly understood and taught, even at the undergraduate level.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, you go to some lengths to show how income distribution does not follow a normal distribution, and present this as an argument against statistical practice in general.  No competent statistician would ever claim that it does, and ways of dealing with this issue are well established.  Just using techniques from the very most basic &quot;first year econometrics&quot; level, for example, transforming the variable by taking its logarithm would make your numerical examples look much less convincing.  Such a transformation would in fact invalidate much of what you say, because then the variance of the original variable does increase as its mean increases.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I am sure there are some incompetent econometricians who do OLS regressions etc with an untransformed response variable the way you say, but that just makes them incompetent and using techniques which are well established to be inappropriate.  They would certainly have been failed even in undergraduate courses, which spend much time looking for more appropriate ways of modelling variables such as income, reflecting the actual observed (non-Gaussian) distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;The family of Generalized Linear Models is one set of techniques developed in part to get around the problems you raise.  Many of the exponential family of distributions (eg Gamma, Exponential, and Poisson distributions) are assymetrical and have variance that increases as the centre of the distribution increases, getting around the problem you point out with using the Gaussian distribution.  If this is still too limiting, it is possible to drop a pre-existing &quot;shape&quot; altogether and simply specify a relationship between the mean of a distribution and its variance (eg allowing the variance to increase proportionately to the square of the mean), using the &quot;quasi-likelihood&quot; method of estimation.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Of course, you could argue that this form of modelling is still too simplistic and an intellectual trap that lulls us into thinking the future will be like the past.  You may be correct, and I think the strength of your book is to make people like me consider this.  But you need different arguments to those that you use in chapters 14-16.  The great weight you place on the fact that the variance of the Gaussian distribution is constant regardless of its mean (which causes problems with scalability), for instance, is invalid.  So is your emphasis on the fact that real-life distributions tend to be assymetric rather than bell-curves.&lt;/p&gt;&#10;&#10;&lt;p&gt;Basically, you have taken one over-simplification of the most basic approach to statistics (naïve modelling of raw variables as having Gaussian distributions) and shown, at great length, (correctly) the shortcomings of such an oversimplified approach.  You then use this to make the gap to discredit the whole field.  This is either a serious lapse in logic, or a propaganda technique.  It is unfortunate because it detracts from your overall argument, much of which (as I said) I found valid and persuasive.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would be interested to hear what you say in response.  I doubt I am the first to have raised this issue.&lt;/p&gt;&#10;&#10;&lt;p&gt;Yours sincerely&lt;/p&gt;&#10;&#10;&lt;p&gt;PE&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-09-10T19:36:32.597" Id="36051" LastActivityDate="2012-09-10T19:36:32.597" OwnerUserId="7972" ParentId="35956" PostTypeId="2" Score="36" />
  <row AcceptedAnswerId="36063" AnswerCount="1" Body="&lt;p&gt;I have a database of workers, customers, and jobs, and I want to analyze my data to see if a given worker is performing well above or below the mean. I've come up with something of a solution, and I'm interested in hearing if there are any flaws in my current approach. &lt;/p&gt;&#10;&#10;&lt;p&gt;My goal is to see if a given worker has a lower than average score when it comes to converting first-time jobs into recurring jobs.&lt;/p&gt;&#10;&#10;&lt;p&gt;I do this by taking all first-time jobs that a given worker has been assigned to, and I then check to see if any subsequent jobs exist under the same customer. If there were subsequent jobs the worker gets a 1, otherwise they get a 0. This gives me something that looks like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Worker 1 [48 total records (first-time jobs)]&#10;0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,&#10;--&#10;Worker 2 [56 total records (first-time jobs)]&#10;1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I then take this data and calculate the mean. I do this by counting the total number of first-time jobs in my system (2,925), as well as the sum of the 1's and 0's. This gives me a mean of 0.38 (so 38% of all first-time jobs typically become recurring jobs). I then calculate the standard deviation, which in this case is 0.13.&lt;/p&gt;&#10;&#10;&lt;p&gt;I then look at each worker who has completed a minimum of 15 first-time jobs. This is so that I only analyze workers with a sufficiently large sample size, in order to increase confidence in the results.&lt;/p&gt;&#10;&#10;&lt;p&gt;I then come up with a mean score for each of these workers (the sum of the 1's and 0's, divided by the total number of first-time jobs). Finally, I convert this into a z score, which I do by subtracting the mean (for all the data) from the worker's mean score. I then divide the result by the standard deviation to obtain the individual z score. Finally, I put the results in a bar chart, which looks like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/e6RG6.gif&quot; alt=&quot;bar chart&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The dark bars are for any result with a z-score above 1 or below -1.&lt;/p&gt;&#10;&#10;&lt;p&gt;My questions are as follows:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Is this the correct approach, given my goals?&lt;/li&gt;&#10;&lt;li&gt;Does it make sense to limit this analysis to workers with a minimum of 15 records? The lower the threshold, the sooner I can perform this valuable analysis, but I want to make sure I'm using a large enough sample size to avoid problems.&lt;/li&gt;&#10;&lt;li&gt;If 15 is a good minimum, should I also use that as the maximum? For example, if Worker 1 has 15 records, and Worker 2 has 35 records, should I only analyze the last 15 of Worker 2's records? Or is it better to include all the data available?&lt;/li&gt;&#10;&lt;li&gt;Finally, what z-scores should I consider significant? Right now I am focusing on anything greater than 1, but is that too low?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I greatly appreciate any input. Thank you.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-09-10T20:46:18.313" Id="36058" LastActivityDate="2012-09-10T21:15:09.177" LastEditDate="2012-09-10T20:53:52.690" LastEditorUserId="13357" OwnerUserId="13357" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;&lt;statistical-significance&gt;" Title="Determine if a person differs from typical performance" ViewCount="90" />
  
  <row Body="&lt;p&gt;Yes, the parameters can be „overfitted” onto training and test set during crossvalidation or bootstrapping. However, there are some methods to prevent this.&#10;First simple method is, you divide your dataset into 3 partitions, one for testing (~20%), one for testing optimized parameters (~20%) and one for fitting the classifier with set parameters. It is only possible if you have quite large dataset. In other cases double crossvalidation is suggested.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;i&gt;Romain François and Florent Langrognet, &quot;Double Cross Validation for Model Based Classification&quot;, 2006&lt;/i&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-09-11T13:50:49.747" Id="36094" LastActivityDate="2012-09-11T13:50:49.747" OwnerUserId="14001" ParentId="29354" PostTypeId="2" Score="3" />
  
  
  
  <row Body="&lt;p&gt;An article that may be of interest is:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&quot;Extended nearest shrunken centroid classification: A new method for&#10;  open-set authorship attribution of texts of varying sizes&quot;, Schaalje,&#10;  Fields, Roper, and Snow.  Literary and Linguistic Computing, vol. 26,&#10;  No. 1, 2011.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Which takes a method for attributing a text to a set of authors and extends it to use the possibility that the true author is not in the candidate set.  Even if you don't use the NSC method, the ideas in the paper may be useful in thinking about how to proceed.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-09-11T17:31:36.590" Id="36114" LastActivityDate="2012-09-11T17:31:36.590" OwnerUserId="4505" ParentId="35917" PostTypeId="2" Score="0" />
  
  
  
  
  
  <row Body="&lt;p&gt;Try &lt;a href=&quot;http://www.foolabs.com/xpdf/download.html&quot; rel=&quot;nofollow&quot;&gt;xpdf&lt;/a&gt;. It does a good job of extracting text from a pdf document.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-09-12T15:45:52.450" Id="36169" LastActivityDate="2012-09-12T15:45:52.450" OwnerUserId="14033" ParentId="36166" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;The point $[E(X), E(Y)]$ does always fall on the least squares regression line when fitting $Y=AX+B$.  where by $E(X)$ and $E(Y)$ we mean the sample averages. In your notation it should be $E(Y\vert X=E(X))=E(Y)$. This is an interesting property of the least squares estimate.  Given $Y_i=A X_i + B +e_i$ is the model with $i=1,2,...,n$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The least squares estimates for A and B are obtained by taking partial derivatives of $\sum e_i^2$ with respect to $A$ and $B$ and setting them equal to zero.  This leads to two equations in two unknowns and one of the equations reduces to $Y_b=A X_b +B$, where $X_b$ and $Y_b$ are the sample means for $X$ and $Y$ respectively.&lt;/p&gt;&#10;&#10;&lt;p&gt;To answer the new question from the edit:  You do not add the overall mean nutrition intake for every subject because Their total caloric intake is not always at the mean total caloric intake for the individual subject.  All we said with the first result was that if your total caloric intake is at the sample mean then the expected nutricianal intake would be at the sample mean.  But the authors want to adjust each individual based on his own total caloric intake.&lt;/p&gt;&#10;" CommentCount="15" CreationDate="2012-09-12T19:13:40.407" Id="36182" LastActivityDate="2012-09-13T16:06:03.710" LastEditDate="2012-09-13T16:06:03.710" LastEditorUserId="11032" OwnerUserId="11032" ParentId="36176" PostTypeId="2" Score="-2" />
  
  <row Body="&lt;h3&gt;Why your first thoughts led you astray:&lt;/h3&gt;&#10;&#10;&lt;p&gt;When you take the SVD of a matrix, $U$ and $V$ are &lt;em&gt;unitary&lt;/em&gt; (orthogonal). So, while it is true that $SA = SU \Sigma V^{T}$, that is &lt;em&gt;not&lt;/em&gt; (generally) the SVD of $SA$. Only if $S$ is unitary (which in the case of a smoothing matrix, it's not) would it be true that $U' = SU$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there any elegant, symbolic way of relating the two SVDs? I can't find one. However, your smoothing matrix is a Toeplitz matrix. It's possible that such matrices have some special properties that might make for a more fruitful analysis. If you figure something out, please share with the rest of us.&lt;/p&gt;&#10;&#10;&lt;h3&gt;The case of extreme smoothing:&lt;/h3&gt;&#10;&#10;&lt;p&gt;One way to think about smoothing is a continuum from no smoothing to the extreme where we smooth each column to its mean value. Now, in that extreme case, the matrix would have a rank of 1, and there would only be one non-zero singular value. Let's look at the SVD:&lt;/p&gt;&#10;&#10;&lt;p&gt;$&#10;\left[ \begin{matrix}&#10;  \uparrow &amp;amp; \uparrow &amp;amp; &amp;amp; \uparrow \\&#10;  \mu_1 &amp;amp; \mu_2 &amp;amp; ... &amp;amp; \mu_m \\&#10;   \downarrow &amp;amp; \downarrow &amp;amp; &amp;amp; \downarrow &#10;\end{matrix} \right]&#10;= \left[ \begin{matrix}&#10;  \boldsymbol{\mu} \\&#10;  \boldsymbol{\mu} \\&#10;  ... \\&#10;\end{matrix} \right]&#10;= \mathbf{1} \boldsymbol{\mu}^T&#10;= \dfrac{\mathbf{1}}{\sqrt{n}} \left[ \|\boldsymbol{\mu}\| \sqrt{n} \right] \dfrac{\boldsymbol{\mu}^T}{\|\boldsymbol{\mu}\|}&#10;$&lt;/p&gt;&#10;&#10;&lt;p&gt;The last equation represents the truncated SVD. Note that the left and right vectors are of length 1. You can expand $\frac{\mathbf{1}}{\sqrt{n}}$ into an orthogonal matrix. Similarly for $\frac{\boldsymbol{\mu}}{\|\boldsymbol{\mu}\|}$. Then just zero-pad the middle matrix, and you've got the full SVD.&lt;/p&gt;&#10;&#10;&lt;h3&gt;Intermediate smoothing&lt;/h3&gt;&#10;&#10;&lt;p&gt;Presumably you're not going to do such extreme smoothing. So, what does this mean for you? As we broaden the smoothing, the spectrum gradually squishes down to a single value. For instance, in my simulations*:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/sK4Fp.gif&quot; alt=&quot;Spectrum Normal&quot;&gt; &lt;img src=&quot;http://i.stack.imgur.com/1bIBK.gif&quot; alt=&quot;Spectrum Ortho&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;As suggested by the derivation above, $U'_1$ will approach the normed 1-vector, and $V'_1$ will approach the normed mean-vector. But what about the other vectors?&lt;/p&gt;&#10;&#10;&lt;p&gt;As their corresponding singular values shrink, the other $U'_i$'s and $V'_i$'s will vary ever more wildly until they're just arbitrary choices for bases of the subspaces orthogonal to $U'_1$ and $V'_1$. That is to say, the'll just become noise.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you need some intuition for why they're &quot;just noise&quot;, consider that $SA$ is a weighted sum of dyads: $\displaystyle \sum \sigma_i U'_i V'^T_i $. We could completely change the directions of $U'_i$ and $V'_i$, and it will only affect the entries of $SA$ by less than $\sigma_i$.&lt;/p&gt;&#10;&#10;&lt;h3&gt;Another visualization&lt;/h3&gt;&#10;&#10;&lt;p&gt;Here's another way to look at column smoothing. Picture each row in the matrix as a point in $m$-space. As we smooth the columns, each point will get closer to the previous and next point. As a whole, the point cloud shrinks down†:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/8YPqY.gif&quot; alt=&quot;Row point cloud&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Hope this helps!&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;sub&gt;[ * ]: I defined a family of increasingly broad smoothers. Roughly speaking, I took the kernel [1/4, 1/2, 1/4], convolved it $z$ times, clipped it to $d$ dimensions, and normalized so it summed to 1. Then I graphed the progressive smoothing of a random orthogonal and a random normal matrix.&lt;/sub&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;sub&gt;[ † ]:  Smoothers generated in the same way. $A$ is constructed as a series of points in $2$-space that look interesting.&lt;/sub&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-09-12T21:30:51.023" Id="36196" LastActivityDate="2014-10-07T17:33:58.890" LastEditDate="2014-10-07T17:33:58.890" LastEditorUserId="13669" OwnerUserId="13669" ParentId="28142" PostTypeId="2" Score="7" />
  <row AnswerCount="1" Body="&lt;p&gt;I have a study in which 60 subjects took both drug A on one day and drug B on another.  The order of drug administration was counterbalanced, such that 30 subjects took drug A on the first day and 30 took drug B on the first day.  (Each subject then received the other drug on the second day.)  Our dependent variable was a continuous measure of performance on a behavioral task.  I would like to evaluate whether drug condition affects behavioral performance, but I suspect that there may be an effect of the order of drug administration.&lt;/p&gt;&#10;&#10;&lt;p&gt;I believe the goal would be to treat subject as a random effect, drug condition as a within-subject repeated measure, and order of drug administration as a between-subjects measure.  Would a split-plot ANOVA be appropriate? My data happen to be in Matlab; as a bonus, would anyone know of a way to do so in Matlab? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-09-12T22:49:34.413" Id="36203" LastActivityDate="2012-09-13T01:47:49.983" LastEditDate="2012-09-13T01:47:49.983" LastEditorUserId="7290" OwnerUserId="14042" PostTypeId="1" Score="2" Tags="&lt;anova&gt;&lt;repeated-measures&gt;&lt;matlab&gt;" Title="Would a split-plot ANOVA be appropriate to look for order effects in my data?" ViewCount="186" />
  <row AnswerCount="2" Body="&lt;p&gt;Can you suggest some pointers on how I can manipulate my data to be 'smoother'? Any algorithms, or techniques that would be useful in this aspect?&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Updates:&lt;/p&gt;&#10;&#10;&lt;p&gt;In response to the comments asking for more info:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;The data does not follow a known distribution as far as I know.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;It consists of 10+ different time series, which might have some relation to each other.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;I have another sample time series, to which I am trying to maximize the correlation of this one to that. The most important thing is to maximize correlation and hopefully preserve the shape of the aggregated 10+ time series combined together.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;I have  tried rectangular smoothing (3-point) and triangular smoothing (3-point) but they both seem worse than not smoothing at all. Triangular smoothing is however better than rectangular smoothing.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;I am reading something on Savitzky-Golay and am going to try that next, and see if I can obtain a better result.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;My question is - what other smoothing filter/techniques should I try?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="8" CreationDate="2012-09-13T05:49:33.800" Id="36219" LastActivityDate="2013-03-13T04:37:47.217" LastEditDate="2012-09-14T01:49:34.167" LastEditorUserId="919" OwnerUserId="14052" PostTypeId="1" Score="3" Tags="&lt;time-series&gt;&lt;smoothing&gt;" Title="I have a very fuzzy data set, what can I do to 'smooth' it?" ViewCount="369" />
  <row Body="&lt;p&gt;For smoothing you can use: Moving average filter, Savitzky-Golay filter, Gaussian filter, Kaiser window, Continuous Wavelet Transform, Discrete Wavelet Transform etc...&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-09-13T12:04:09.637" Id="36229" LastActivityDate="2012-09-13T12:04:09.637" OwnerUserId="10891" ParentId="36219" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Usually, each row is an &quot;observation&quot; (in your case image), and each column is a variable (in your case pixel value).  Therefore, you should center and scale the columns before doing PCA.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, lots of good PCA libraries already exist, such as &lt;a href=&quot;http://scikit-learn.org/dev/modules/generated/sklearn.decomposition.PCA.html&quot; rel=&quot;nofollow&quot;&gt;sklearn.decomposition.PCA&lt;/a&gt;, which can save you a lot of effort re-inventing the wheel.  But if you persist in implementing PCA yourself you should probably do so via &lt;code&gt;svd&lt;/code&gt; rather than via the co-variance matrix.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-09-13T20:17:12.997" Id="36256" LastActivityDate="2012-09-13T20:17:12.997" OwnerUserId="2817" ParentId="36254" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;You left out some pieces that need to be assumed.  First of all what is your data to get the estimates of thee proportions? Do you have an experiment where in each trial a category occurs and you have a sequence of trials? If so the probabilities for the list of categories will sum to 1.  How many trials n did you run to get the estimates?&lt;/p&gt;&#10;&#10;&lt;p&gt;For purposes of supplying an intelligent answer that could be what you are looking for I will make the assumption that you have k categories and look at the cases where (1) n is very large and (2) n is fairly small.&lt;/p&gt;&#10;&#10;&lt;p&gt;In either case under my assumptions the joint probability distribution for number of occurrences of each category in n trials is multinomial.  There are k-1 parameters (category probabilities) since you have the constraint that the sum of all the category probailities equals 1. So for example the last category has its probability determined given the probailities fro the othe k-1 categories.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now category i has probability denoted by p$-i$ and keep in mind that p$_k$ =1 - sum of all the other p$_i$s. Suppose you want to compare category 1 with category 2.  The number of cases in category 1 is marginally binomial with parameters n and p$_1$.  Similarly the number of cases in category 2 is marginally binomial with parameters n and p$_2$.  In case (1) when n is very large you can approximate the difference in the two proportions by a normal distribution.  The trick though is that even though you can estimate the variance of the estimates of p$_1$ and p$_2$ the variance for the difference is not the sum of the variances. The estimates are negatively correlated and hence their covariance is negative. For the variance of the difference you need to subtract 2 times the covariance of the estimates.  Since the covariance of the estimates is negative this means the variance will be larger than what you would get in the case of independent sets of trials. &lt;/p&gt;&#10;&#10;&lt;p&gt;The joint distribution of the number in category 1, the number in category two and the number in neither 1 nor 2 (n - the sum of the other two) is trinomial.  It's distribution depends on p$_1$ and p$_2$ and the covariance can be expressed as a function of p$_1$ and p$_2$.  Those probabilties are estimated as # of category occurrences/n.  So we can estimate the covariance by plugging these estimates into the function.  Then we can estimate the variance of the difference and normal to have variance approximately 1. Under the null hypothesis that p$_1$ = p$_2$ the normalized statistic will have mean 0 and will be approximately a standard normal.  This distirbution can then be used for the test.&lt;/p&gt;&#10;&#10;&lt;p&gt;In case (2) things are more complicated because the normal approximation may not be good.  In that case an exact test must be performed based on the exact trinomial distribution mentiioned earlier. &lt;a href=&quot;http://en.wikipedia.org/wiki/Multinomial_distribution&quot;&gt;This&lt;/a&gt; wikipedia reference gives all the details you need regarding the multinomial distirbution including the covariance which is -n p$_1$ p$_2$ for p$_1$ and p$_2$ that is need for the standard error of the difference estimate.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-09-13T20:36:25.130" Id="36258" LastActivityDate="2012-09-13T21:01:19.493" LastEditDate="2012-09-13T21:01:19.493" LastEditorUserId="11032" OwnerUserId="11032" ParentId="36253" PostTypeId="2" Score="5" />
  <row AnswerCount="2" Body="&lt;p&gt;In a predictive model, I have standardized variables as predictors. Say I have to rescore the model on fresh data at some point in the future: do I use the means/stds as they were when I built the model to center and scale the new data, or do I use the means/stds as they are with the data I'm scoring.&lt;/p&gt;&#10;&#10;&lt;p&gt;My take is to use the means/stds of the data I'm scoring, since I want the standardized variables to reflect distributions as they are at the time of scoring.&lt;/p&gt;&#10;&#10;&lt;p&gt;Pros &amp;amp; cons of original means/stds vs. current means/stds?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-09-13T23:28:54.853" Id="36266" LastActivityDate="2012-10-01T00:02:57.050" LastEditDate="2012-09-14T00:51:06.687" LastEditorUserId="12318" OwnerUserId="14075" PostTypeId="1" Score="5" Tags="&lt;distributions&gt;&lt;predictive-models&gt;&lt;summary-statistics&gt;&lt;standardization&gt;" Title="Predictive model &amp; standardized variables" ViewCount="180" />
  
  
  <row AnswerCount="0" Body="&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;strong&gt;Possible Duplicate:&lt;/strong&gt;&lt;br&gt;&#10;  &lt;a href=&quot;http://stats.stackexchange.com/questions/1142/simple-algorithm-for-online-outlier-detection-of-a-generic-time-series&quot;&gt;Simple algorithm for online outlier detection of a generic time series&lt;/a&gt;  &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&#10;&#10;&lt;p&gt;How could I get rid of sparky data in a descrete data set, but in a &quot;smoother out&quot; manner?&lt;/p&gt;&#10;&#10;&lt;p&gt;Take for instance&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/vb44Q.png&quot; rel=&quot;nofollow&quot;&gt;http://i.stack.imgur.com/vb44Q.png&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;There are two sparks, at 20000, but the next one at 600 is also considered a spark.&lt;/p&gt;&#10;&#10;&lt;p&gt;I've managed to get the very high ones to zero, by&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;a = 2&#10;b = 5&#10;beta_dist = RealDistribution('beta', [a, b])&#10;f(x) = x / 19968&#10;normalized_insertions = [f(i) for i in insertions]&#10;&#10;insertions_pairs = [(i, beta_dist.distribution_function(i)) for i in normalized_insertions]&#10;plot_b = beta_dist.plot()&#10;&#10;show(list_plot(insertions_pairs)+plot_b)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;No idea how to go about the lower ones. The maximul should be reached at 100, perhaps the parameters for the beta distribution need a little more twiddling?&lt;/p&gt;&#10;&#10;&lt;p&gt;Currently, it looks like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/7hKHT.png&quot; rel=&quot;nofollow&quot;&gt;http://i.stack.imgur.com/7hKHT.png&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;If possible, use sage as a reference for your explanations.&lt;/p&gt;&#10;" ClosedDate="2012-09-14T15:48:41.493" CommentCount="3" CreationDate="2012-09-14T01:43:02.770" Id="36271" LastActivityDate="2012-09-14T14:03:41.593" LastEditDate="2012-09-14T14:03:41.593" LastEditorUserId="14078" OwnerUserId="14078" PostTypeId="1" Score="0" Tags="&lt;outliers&gt;&lt;signal-processing&gt;&lt;computational-statistics&gt;" Title="Getting rid of spikes in sample data" ViewCount="38" />
  <row AcceptedAnswerId="36296" AnswerCount="1" Body="&lt;p&gt;I want to understand &lt;a href=&quot;http://www.google.co.ve/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;ved=0CCYQFjAA&amp;amp;url=http://people.csail.mit.edu/menze/papers/menze_10_generative.pdf&amp;amp;ei=kKxSULuOFo-C8ASaq4GADQ&amp;amp;usg=AFQjCNHvGj6fo8fvScCTK5vd2IjsOwXyAg&amp;amp;sig2=q0MyFzQ1r9awzT-sRYMuog&quot; rel=&quot;nofollow&quot;&gt;this paper&lt;/a&gt; on brain tumour segmentation.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/RrCKP.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;How is this equation read?&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm guessing $q_i(t_i)$ represents the likelihood of tumour on voxel i.Is q usually used to represent likelihoods?&lt;/p&gt;&#10;&#10;&lt;p&gt;What does the equal sign with the triangle mean?&lt;/p&gt;&#10;&#10;&lt;p&gt;I understand what each term on the conditional probability represents, but I don't get why it's called &lt;em&gt;proportional&lt;/em&gt; to the product of the summation. I'm looking for a reading reference here. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-09-14T04:29:16.907" Id="36278" LastActivityDate="2012-10-14T14:02:46.340" LastEditDate="2012-09-14T05:05:07.230" LastEditorUserId="7290" OwnerUserId="13406" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;bayesian&gt;&lt;maximum-likelihood&gt;&lt;references&gt;" Title="How is this equation read?" ViewCount="131" />
  <row AnswerCount="1" Body="&lt;p&gt;I am running LDA from &lt;a href=&quot;http://psiexp.ss.uci.edu/research/programs_data/toolbox.htm&quot; rel=&quot;nofollow&quot;&gt;Mark Steyver's MATLAB Topic Modelling toolkit&lt;/a&gt; on a few Apache Java open source projects. I have taken care of stop word removal (for e.g. words such Apache, java keywords are marked as stopwords) and tokenization. I find that perplexity on test data always decreases with increasing number of topics. I tried different values of &lt;code&gt;ALPHA&lt;/code&gt; but no difference.&lt;/p&gt;&#10;&#10;&lt;p&gt;I need to find optimal number of topics and for that perplexity plot should reach a minimum. Please suggest what may be wrong.&lt;/p&gt;&#10;&#10;&lt;p&gt;Definition and details regarding calculation of perplexity of a topic model is explained in &lt;a href=&quot;http://stats.stackexchange.com/questions/18167/how-to-calculate-perplexity-of-a-holdout-with-latent-dirichlet-allocation&quot;&gt;this post&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; I played with hyperparameters alpha and beta and now perplexity seems to reach a minimum. It is not clear to me as to how these hyperparameters affect perplexity. Initially I was plotting results till 200 topics without any success. Now on the same range minimum is reached at around 50-60 topics (which was my intuition) after modifying hyperparameters.&#10;Also, as &lt;a href=&quot;https://lists.cs.princeton.edu/pipermail/topic-models/2011-April/001281.html&quot; rel=&quot;nofollow&quot;&gt;this post&lt;/a&gt; notes, you bias optimal number of topics according to specific values of hyperparameters.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-09-14T05:22:44.017" Id="36279" LastActivityDate="2012-10-18T18:02:53.243" LastEditDate="2012-09-15T02:13:57.167" LastEditorUserId="13915" OwnerUserId="13915" PostTypeId="1" Score="2" Tags="&lt;machine-learning&gt;&lt;topic-models&gt;&lt;hyperparameter&gt;" Title="Trouble minimizing perplexity in LDA" ViewCount="1216" />
  
  
  
  <row Body="&lt;p&gt;There is nothing wrong with using categorical independent variables, but they are typically broken into indicator or &quot;dummy&quot; variables, one for each level that the categorical variable can take on. But one category's dummy variable needs to be left out of the model due to the &quot;dummy variable trap&quot; identification problem. For more information, see &lt;a href=&quot;http://stats.stackexchange.com/questions/8135/dummy-variable-trap-issues/8165#8165&quot;&gt;this question&lt;/a&gt;, for example.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-09-14T14:54:04.993" Id="36304" LastActivityDate="2012-09-14T14:54:04.993" OwnerUserId="401" ParentId="36303" PostTypeId="2" Score="3" />
  <row AnswerCount="0" Body="&lt;pre&gt;&lt;code&gt;Fixed effects: Ratio ~ ADF + CP + FCM + DMI + DIM &#10;                 Value  Std.Error  DF   t-value p-value&#10;(Intercept)  3.1199808 0.16237303 158 19.214896  0.0000&#10;ADF         -0.0265626 0.00406990 158 -6.526603  0.0000&#10;CP          -0.0534021 0.00539108 158 -9.905636  0.0000&#10;FCM         -0.0149314 0.00353524 158 -4.223598  0.0000&#10;DMI          0.0072318 0.00498779 158  1.449894  0.1491&#10;DIM         -0.0008994 0.00019408 158 -4.634076  0.0000&#10;&#10;Correlation: &#10;    (Intr) ADF    CP     FCM    DMI   &#10;ADF -0.628                            &#10;CP  -0.515  0.089                     &#10;FCM -0.299  0.269 -0.203              &#10;DMI -0.229 -0.145  0.083 -0.624       &#10;DIM -0.113  0.127 -0.061  0.010 -0.047&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I obtained these value from &lt;code&gt;lme&lt;/code&gt; function for mixed model. However, I am wondering why the values for correlation of fixed effect provided here differ from what I obtain using &lt;code&gt;cor.test&lt;/code&gt; function (Pearson correlation)&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-09-14T11:21:04.123" FavoriteCount="1" Id="36307" LastActivityDate="2013-09-04T14:36:21.537" LastEditDate="2013-09-04T14:36:21.537" LastEditorUserId="21599" OwnerDisplayName="user12714" OwnerUserId="12714" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;correlation&gt;&lt;mixed-model&gt;&lt;lme&gt;&lt;nlme&gt;" Title="Why is correlation obtained from nlme different from Pearson correlation?" ViewCount="193" />
  <row AcceptedAnswerId="36313" AnswerCount="1" Body="&lt;p&gt;I have location data from 5 years (&gt;30,000 points).  Each location is given a classification name (in my case vegetation classes).  These vegetation classes were assigned by intersecting the locations with vegetation maps representing 5 different years.  The vegetation categories changed due to clear cut practices. Basically, some old forest areas in 2007 became clear cut in 2008, and others in 2009 became clear cuts in 2010 etc. Also, some old clear cuts became young forests because they grew back.  What I want to know is, if I used just 1 map (say 2007) - how many wrong classifications will I have per year after this?  Because what was once forest in 2007 could be clear cut in 2009.  Knowing that the location from year 2009 &lt;em&gt;is&lt;/em&gt; a clear cut, using the map from 2007 - was it misclassified?  I will want to test this for each year.  Basically, is there a map that minimizes miss-classification of points.  &lt;/p&gt;&#10;&#10;&lt;p&gt;In the data set for each XY location, I have Year location was collected, clearcut07, clearcut08, clearcut09..., Names07, Names08, Names09....  &lt;/p&gt;&#10;&#10;&lt;p&gt;The clearcut07 is binary indicator of 1 or 0, if was really a clear cut that year. the Names07 are the categorical names for all locations (all 5 years of points - if they were to be classified from that year).  So, all locations obtained are given the attributes from the 2007 map, then the 2008 map then 2009, etc. &lt;/p&gt;&#10;&#10;&lt;p&gt;I need a summary of counts of locations within each category.  A count of locations that were properly classified as clear cuts from each year of the data.  A count of locations from data collected, for example, in 2008 that were classified as clear cuts from each map? And a count of locations that were incorrectly classified as clear cuts from each map when I know the correct classification.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-09-13T17:04:23.130" Id="36310" LastActivityDate="2013-01-10T08:30:38.533" LastEditDate="2012-09-15T19:15:33.787" LastEditorUserId="930" OwnerDisplayName="Kerry" OwnerUserId="14125" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;time-series&gt;&lt;summary-statistics&gt;" Title="How to compare yearly categorical data? " ViewCount="175" />
  
  
  
  
  
  
  <row AcceptedAnswerId="36358" AnswerCount="2" Body="&lt;p&gt;I have a dataset with which I would like to compare the effect of species and habitat on movement rate, with pairwise comparisons. I would also like to include the effect of individual  (as a random factor?) - this random factor is the part I don't know how to do, at least not in the framework of &lt;code&gt;Anova()&lt;/code&gt;.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Here's a subset of the data:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;species  &amp;lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;a&quot;, &quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;a&quot;, &quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;a&quot;, &quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &#10;              &quot;a&quot;, &quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;a&quot;)&#10;habitat  &amp;lt;- c(&quot;x&quot;, &quot;x&quot;, &quot;x&quot;, &quot;y&quot;, &quot;y&quot;, &quot;y&quot;, &quot;x&quot;, &quot;x&quot;, &quot;y&quot;, &quot;z&quot;, &quot;y&quot;, &quot;y&quot;, &quot;z&quot;, &quot;z&quot;, &quot;x&quot;, &#10;              &quot;x&quot;, &quot;y&quot;, &quot;y&quot;, &quot;z&quot;, &quot;z&quot;)&#10;mvt.rate &amp;lt;- c(6, 5, 7, 8, 9, 4, 3, 5, 6, 9, 3, 6, 6, 7, 8, 9, 5, 6, 7, 8)&#10;ind      &amp;lt;- as.factor(c(1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4))&#10;data1    &amp;lt;- data.frame(species, habitat, mvt.rate, ind)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Currently, I'm simply running a two-way ANOVA with pairwise comparisons, without considering the effect of individual, like so:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;fit &amp;lt;- lm(mvt.rate ~ habitat + species, data=data1)&#10;require(car)&#10;Anova(fit, type=&quot;III&quot;)&#10;require(agricolae)&#10;    #pairwise comparison of habitats&#10;comparison.hab &amp;lt;- HSD.test(fit, &quot;habitat&quot;, group=TRUE)                     &#10;    #pairwise comparison of species&#10;comparison.sp &amp;lt;- HSD.test(fit, &quot;species&quot;, group=TRUE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In the dataset, each row represents a movement, and in many cases, individuals make several (non-independent) movements - I am currently not considering this non-independence of mvt.rate and individual.  I believe the correct way to do this is to consider individual as a random variable, but I'm not entirely sure.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-09-15T13:38:41.193" FavoriteCount="1" Id="36351" LastActivityDate="2012-09-16T12:25:14.940" LastEditDate="2012-09-16T12:25:14.940" LastEditorUserId="930" OwnerUserId="14113" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;anova&gt;&lt;multiple-comparisons&gt;&lt;random-effects-model&gt;" Title="How to run a two-way ANOVA with a random variable followed by pairwise comparisons?" ViewCount="1021" />
  
  
  
  <row Body="&lt;p&gt;To evaluate the quality of the forecasts, I would first evaluate the quality of the residuals , as you suggested. Take the residuals from each panel separtely into an analytical engine that would test for any ARIMA structure , any anomolous values suggesting pulses,seasonal pulses, level shifts and/or local time trends, any change in model paramters over time and any changes in variance suggesting heterocedasticity. Note that heteroscedasticity can be rectified by weighted least squares, power transformations or Garch . If the analytical engine reports none of these violations , you should be good to go, otherwise the residuals are flawed and your forecasts are subsequently flawed.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-09-16T11:51:18.180" Id="37380" LastActivityDate="2012-09-16T11:51:18.180" OwnerUserId="3382" ParentId="23746" PostTypeId="2" Score="1" />
  <row Body="&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;You have defined the model based on the defined times to events in your question. You have two point processes with censoring times.  The time to events [IN events and OUT events] is defined in your post.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The time 2T just amounts to a censoring time which is handled routinely in survival analysis if you just assume the OUT event occurs beyond 2T but since you are assuming the OUT event does occur at 2T if not before that amounts to truncating its distribution.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Since the time between out events is exponential the lack of memory property says that given the in event occurs at time t$_a$ does not change the distribution of the time to the next event. So the time from t$_a$ to the next OUT event is still exp(λ).  But given that the out event is truncated at time 2T the actual distribution that you want is &#10;V = min(t$_1$, t$_2$) where t$_1$ is distributed &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;exp(λ) and t$_2$=2T-t$_a$.  This just makes the probability &lt;/p&gt;&#10;&#10;&lt;p&gt;given t$_a$ that V=2T-t$_a$ is the probability that an exp(λ) random variable E is greater &lt;/p&gt;&#10;&#10;&lt;p&gt;than 2T-t$_a$. and it follows the exp(λ) density when &lt;/p&gt;&#10;&#10;&lt;p&gt;0 &amp;lt; E &amp;lt; 2T-t$_a$. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-09-16T11:51:53.420" Id="37381" LastActivityDate="2012-10-08T17:04:39.197" LastEditDate="2012-10-08T17:04:39.197" LastEditorUserId="11032" OwnerUserId="11032" ParentId="37371" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="37436" AnswerCount="1" Body="&lt;p&gt;I have used 20 variables to extract 4 factors in a factor analysis. Now I want to use the extracted factors (factor scores) as IVs on an outcome variable. Besides I want to include 4 other variables as IVs in the regression which were not used in the factor analysis. That means I want to regress 4 factors and 4 variables (from outside of the FA) as IVs on a DV. Is there any problem in it statistically? &lt;/p&gt;&#10;&#10;&lt;p&gt;Secondly, if I want to run a separate regression on another DV by using only 2 of the factors (factor scores) and 3 other variables as IVs, is there any problem?&lt;/p&gt;&#10;&#10;&lt;p&gt;Just to know, if I extracted 4 components from a PCA (Principal Component Analysis) instead, what would be the answers to the same two questions? &lt;/p&gt;&#10;&#10;&lt;p&gt;Grateful for any help.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-09-16T13:33:41.263" FavoriteCount="1" Id="37385" LastActivityDate="2012-09-17T17:11:00.690" OwnerUserId="12603" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;pca&gt;&lt;factor-analysis&gt;" Title="About regressing some variables along with some factors on a DV after a factor analysis" ViewCount="102" />
  
  
  <row Body="&lt;p&gt;Suppose that you have $X_1,\dots,X_n$ random variables (whose values will be observed in your experiment) that are conditionally independent, given that $\Theta=\theta$, with conditional densities $f_{X_i\mid\Theta}(\,\cdot\mid\theta)$, for $i=1,\dots,n$. This is your (postulated) statistical (conditional) model, and the conditional densities express, for each possible value $\theta$ of the (random) parameter $\Theta$, your uncertainty about the values of the $X_i$'s, &lt;strong&gt;before&lt;/strong&gt; you have access to any real data. With the help of the conditional densities you can, for example, compute conditional probabilities like&#10;$$&#10;  P\{X_1\in B_1,\dots,X_n\in B_n\mid \Theta=\theta\} = \int_{B_1\times\dots\times B_n} \prod_{i=1}^n f_{X_i\mid\Theta}(x_i\mid\theta)\,dx_1\dots dx_n \, , &#10;$$&#10;for each $\theta$.&lt;/p&gt;&#10;&#10;&lt;p&gt;After you have access to an actual sample $(x_1,\dots,x_n)$ of values (realizations) of the $X_i$'s that have been observed in one run of your experiment, the situation changes: there is no longer uncertainty about the observables $X_1,\dots,X_n$. Suppose that the random $\Theta$ assumes values in some parameter space $\Pi$. Now, you define, for those known (fixed) values $(x_1,\dots,x_n)$ a function&#10;$$&#10;  L_{x_1,\dots,x_n} : \Pi \to \mathbb{R} \, &#10;$$&#10;by&#10;$$&#10;  L_{x_1,\dots,x_n}(\theta)=\prod_{i=1}^n f_{X_i\mid\Theta}(x_i\mid\theta) \, .&#10;$$&#10;Note that $L_{x_1,\dots,x_n}$, known as the &quot;likelihood function&quot; is a function of $\theta$. In this &quot;after you have data&quot; situation, the likelihood $L_{x_1,\dots,x_n}$ contains, for the particular conditional model that we are considering, all the information about the parameter $\Theta$ contained in this particular sample $(x_1,\dots,x_n)$. In fact, it happens that $L_{x_1,\dots,x_n}$ is a sufficient statistic for $\Theta$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Answering your question, to understand the differences between the concepts of conditional density and likelihood, keep in mind their mathematical definitions (which are clearly different: they are different mathematical objects, with different properties), and also remember that conditional density is a &quot;pre-sample&quot; object/concept, while the likelihood is an &quot;after-sample&quot; one. I hope that all this also help you to answer why Bayesian inference (using your way of putting it, which I don't think is ideal) is done &quot;using the likelihood function and not the conditional distribution&quot;: the goal of Bayesian inference is to compute the posterior distribution, and to do so we condition on the &lt;strong&gt;observed&lt;/strong&gt; (known) data.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-09-17T02:04:02.017" Id="37409" LastActivityDate="2012-09-17T23:09:20.427" LastEditDate="2012-09-17T23:09:20.427" LastEditorUserId="9394" OwnerUserId="9394" ParentId="37406" PostTypeId="2" Score="7" />
  <row AnswerCount="0" Body="&lt;p&gt;I've came across a method in research communication, where different percentages of variation were calculated for a collection of ratio values for some proteins. As seen in the figure, the frequency of (protein) ratios falling within a certain percentage of variation is plotted against the percent variation itself. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/5u3DR.jpg&quot; alt=&quot;http://i.stack.imgur.com/RgXkC.jpg&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Can anyone tell me how this percent variation is calculated?   &lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-09-17T04:47:48.113" Id="37410" LastActivityDate="2012-09-17T16:51:09.820" LastEditDate="2012-09-17T16:51:09.820" LastEditorUserId="7290" OwnerUserId="14137" PostTypeId="1" Score="0" Tags="&lt;frequency&gt;&lt;coefficient-of-variation&gt;" Title="How to calculate percent variation" ViewCount="391" />
  <row Body="&lt;p&gt;First, to get an idea, are your matrices of dimension 105 x 105, corresponding with the applications that you mention? When you say 'stay in state Y' does that mean stick around application Y?&lt;/p&gt;&#10;&#10;&lt;p&gt;Then, I would assume that outcomes such as &quot;Processes in cluster A tend to stay in state Y once they get there, which is not true for processes in other clusters&quot; are a bit too fine-grained with just 10 clusters. Have you tried a clustering of the application domain -- if I understand correctly you could cluster the 105 applications based on user behaviour. Next, have you looked at simple presence of users rather than transition, i.e. look at profiles of users across the 105 applications? It sounds as if you could use Pearson coefficient between user profiles; either on clusters of applications, or on the applications themselves. This could perhaps be extended towards transitions between applications, but currently I feel there is a huge mismatch between the number of clusters and the type of outcome you are interested in.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-09-17T09:19:51.300" Id="37415" LastActivityDate="2012-09-17T09:19:51.300" OwnerUserId="4495" ParentId="37390" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;This definition of random assignment seems to be assigning with equal probability.  To assign 0 weight any of the possible assignments could create bias and should be considered a nonrandom assignemnt by any definition.  However sampling with unequal nonzero weights can be an acceptable procedure (e.g. sampling randomly proportional to size or stratified random sampling with unequal samples per stratum are survey sampling examples).  They fit into a more general definition of random sampling. If one is estimating a mean a weighted average can be used to get a unbiased estimate of the population mean.  By excluding a possible outcome you change the population and it is not apprpriate to draw an inference to the large population that you chose not to sample from.  Also because potential samples were excluded it is impossible to make a weighted adjustment to guarantee an unbiased estimation for the mean of the unrestricted population.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-09-17T12:59:21.913" Id="37426" LastActivityDate="2012-09-17T12:59:21.913" OwnerUserId="11032" ParentId="37422" PostTypeId="2" Score="3" />
  
  
  
  
  <row Body="&lt;p&gt;If you don't have a valid instrument, then the bias can actually be worse using TSLS/IV as compared to the standard linear model. If you have a valid instrument, then your bias will be 0 on average, but your standard errors will be larger. For some information on these cases, see &lt;/p&gt;&#10;&#10;&lt;p&gt;Bound, John,  David A. Jaeger, and Regina M. Baker. 1995 &quot;Problems with Instrumental Variables Estimation When the Correlation Between the Instruments and the Endogeneous Explanatory Variable is Weak.&quot; &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt;. 90(430): 443--450. [&lt;a href=&quot;http://www.jstor.org/stable/2291055&quot; rel=&quot;nofollow&quot;&gt;jstor&lt;/a&gt;]&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-09-17T20:57:19.563" Id="37446" LastActivityDate="2012-09-17T20:57:19.563" OwnerUserId="401" ParentId="37441" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Try spectral entropy; the paper by Vierto-Oja et al (&quot;Description of the Entropy(tm) algorithm as applied in the Datex-Ohmeda S/5(tm) Entropy Module&quot;, H. Viertio-Oja, et al, Acta Anaesthesiol Scandinavica, 2004, 48:154-161.) gives a good definition. &#10;Basically, a more rhythmic signal will display lower spectral entropy: a pure sine wave will have 0 entropy while white noise has a value of 1.&#10;Snoring should be more rhythmic than speech and this variable may discriminate.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-09-17T21:48:10.370" Id="37452" LastActivityDate="2012-10-01T09:11:34.953" LastEditDate="2012-10-01T09:11:34.953" LastEditorUserId="11030" OwnerUserId="11030" ParentId="31590" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;$\newcommand{\var}{\operatorname{var}}$&#10;Variances are additive: for independent random variables $X_1,\ldots,X_n$,&#10;$$&#10;\var(X_1+\cdots+X_n)=\var(X_1)+\cdots+\var(X_n).&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Notice what this makes possible: Say I toss a fair coin 900 times.  What's the probability that the number of heads I get is between 440 and 455 inclusive?  Just find the expected number of heads ($450$), and the variance of the number of heads ($225=15^2$), then find the probability with a normal (or Gaussian) distribution with expectation $450$ and standard deviation $15$ is between $439.5$ and $455.5$.  Abraham de Moivre did this with coin tosses in the 18th century, thereby first showing that the bell-shaped curve is worth something.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-09-18T01:41:14.847" Id="37465" LastActivityDate="2012-09-18T01:41:14.847" OwnerUserId="5176" ParentId="118" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;You are not correct in the way that in the frequentist framework parameters (in your case probability\proportion) are NOT a random variable. However, if you assume Bayesian framework, you can safely work with parameters as random variables and to obtain probabilities that some parameters exceeds some value.&#10;You have your sampling distribution $$X|p,N \sim B(N,p)$$&#10;Assume that you observed sample is $X_{1},...,X_{t}$. Then, the likelihood function is proportional to $$p^{r}(1-p)^{N-r},$$&#10;where $r= \Sigma X_{k}$. Now you need prior distribution on the proportion parameter. The usual choise in this problem is to use beta distribution $B(\alpha, \beta)$. Then you posterior (due to the conjugacy property) will also be a beta distribution with updated parameters $B(\alpha +r, \beta +N-r)$ or &#10;$$p|X,N \sim B(\alpha +r, \beta +N-r)$$&#10;Now you can calculate the probabilities that you want, though it is not straightforward, since it requires to calculate incomplete Beta function. Another aspect is that I dont see any way how to calculate the number of experiments, since all statistical information colapses to one sum $r$.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2012-09-18T10:06:44.090" Id="37482" LastActivityDate="2012-09-18T10:06:44.090" OwnerUserId="9343" ParentId="37481" PostTypeId="2" Score="2" />
  
  
  
  <row Body="&lt;p&gt;You might want to have a look at the implementation of &lt;a href=&quot;http://mallet.cs.umass.edu/topics.php&quot; rel=&quot;nofollow&quot;&gt;LDA in Mallet&lt;/a&gt;, which can do hyperparameter optimization as part of the training. Mallet also uses asymmetric priors by default, which according to &lt;a href=&quot;http://people.cs.umass.edu/~wallach/publications/wallach09rethinking.pdf&quot; rel=&quot;nofollow&quot;&gt;this paper&lt;/a&gt;, leads to the model being much more robust against setting the number of topics too high. In practice this means you don't have to specify the hyperparameters, and can set number of topics pretty high without negatively affecting results.&lt;/p&gt;&#10;&#10;&lt;p&gt;In my experience hyperparameter optimization and asymmetric priors gave significantly better topics than without it, but I haven't tried the Matlab Topic Modelling toolkit.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-09-18T17:59:50.107" Id="37507" LastActivityDate="2012-09-18T17:59:50.107" OwnerUserId="7961" ParentId="36279" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;Answer to the first part. -3.065293e-17 is R-speak for 0. I am not sure what model you have fit, but if it's a basic regression, the residuals are forced, in theory, to sum to zero. But in the world of floating point arithmetic, which is what R is doing, you don't always get 0 ... rather, something very teensy.&lt;/p&gt;&#10;&#10;&lt;p&gt;Second part. I am not sure that I understand your question. cor() takes 2 vectors, as in cor(x,y). You need to give it your residuals and your income. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-09-19T00:44:02.333" Id="37528" LastActivityDate="2012-09-19T00:44:02.333" OwnerUserId="14188" ParentId="37522" PostTypeId="2" Score="1" />
  
  
  
  <row Body="&lt;p&gt;I am not sure if I got your question right, &#10;but if you already have a number of exact factors,&#10;I guess you can use chi-squared test &#10;to see if the factor loading of your concern is significant&#10;as we do in Multiple Regression.&lt;/p&gt;&#10;&#10;&lt;p&gt;So here I assume you know in advance the exact value of factors &#10;and the criterion variable, it's much like multiple regression.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you have multiple criterion variables, then you might want to test&#10;if the factor loadings for a specific factor is significantly different for&#10;(0,0,0, ...,0). We can approach this problem with Multiple Comparison or multivariate viewpoint.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-09-19T06:30:07.777" Id="37554" LastActivityDate="2012-09-19T06:30:07.777" OwnerUserId="14191" ParentId="3898" PostTypeId="2" Score="0" />
  <row AnswerCount="1" Body="&lt;p&gt;I am trying to do a meta-analysis using Hedges' G as the effect size. &lt;/p&gt;&#10;&#10;&lt;h3&gt;Questions&lt;/h3&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Some of the studies have different groups but the same control group. How does this change the computation and pooling of effect sizes?&lt;/li&gt;&#10;&lt;li&gt;How do I calculate a weighted effect size for each study so that I can perform moderator effect analysis using SPSS?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="3" CreationDate="2012-09-19T07:36:15.807" FavoriteCount="1" Id="37557" LastActivityDate="2012-09-19T16:29:48.297" LastEditDate="2012-09-19T16:29:48.297" LastEditorUserId="11032" OwnerUserId="14192" PostTypeId="1" Score="5" Tags="&lt;spss&gt;&lt;meta-analysis&gt;&lt;effect-size&gt;" Title="How to calculate weighted Hedges' g effect size in meta-analysis when some effect sizes share a control group?" ViewCount="828" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have chemical compositions of individuals collected in 6 different sites in monthly intervals over a period of 6 months. &lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to determine whether there is significant variation among sites as well as among months within sites. I was advised to use a repeated measures ANOVA to address this. However my design is unbalanced and has missing values. So, for example not all the sites are represented at all times and the number of individuals per site and time also varies. &lt;/p&gt;&#10;&#10;&lt;p&gt;Is it possible to do a repeated measures ANOVA with this dataset? Should I use the lmer function from the lme4 package?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-09-19T13:23:15.553" Id="37577" LastActivityDate="2012-09-19T13:58:00.003" LastEditDate="2012-09-19T13:58:00.003" LastEditorUserId="2116" OwnerUserId="14198" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;repeated-measures&gt;" Title="Repeated measures ANOVA, unbalanced and missing values in R" ViewCount="478" />
  <row Body="&lt;p&gt;There is a nice theory of what kind of distributions can be limiting distributions of sums of random variables. The nice resource is the following &lt;a href=&quot;http://books.google.lt/books/about/Limit_Theorems_of_Probability_Theory.html?id=4LkdSaI4xXMC&amp;amp;redir_esc=y&quot; rel=&quot;nofollow&quot;&gt;book&lt;/a&gt; by Petrov, which I personally enjoyed immensely. &lt;/p&gt;&#10;&#10;&lt;p&gt;It turns out, that if you are investigating limits of this type &#10;$$\frac{1}{a_n}\sum_{i=1}^nX_n-b_n, \quad (1)$$ where $X_i$ are independent random variables, the distributions of limits are only certain distributions. &lt;/p&gt;&#10;&#10;&lt;p&gt;There is a lot of mathematics going around then, which boils to several theorems which completely characterizes what happens in the limit. One of such theorems is due to Feller:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; Let $\{X_n;n=1,2,...\}$ be a sequence of independent random variables, $V_n(x)$ be the distribution function of $X_n$, and  $a_n$ be a sequence of positive constant. In order that &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\max_{1\le k\le n}P(|X_k|\ge\varepsilon a_n)\to 0, \text{ for every fixed } \varepsilon&amp;gt;0$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\sup_x\left|P\left(a_n^{-1}\sum_{k=1}^nX_k&amp;lt;x\right)-\Phi(x)\right|\to 0$$&lt;/p&gt;&#10;&#10;&lt;p&gt;it is necessary and sufficient that&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\sum_{k=1}^n\int_{|x|\ge \varepsilon a_n}dV_k(x)\to 0 \text{ for every fixed }\varepsilon&amp;gt;0,$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$a_n^{-2}\sum_{k=1}^n\left(\int_{|x|&amp;lt;a_n}x^2dV_k(x)-\left(\int_{|x|&amp;lt;a_n}xdV_k(x)\right)^2\right)\to 1$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and &lt;/p&gt;&#10;&#10;&lt;p&gt;$$a_n^{-1}\sum_{k=1}^n\int_{|x|&amp;lt;a_n}xdV_k(x)\to 0.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;This theorem then gives you an idea of what $a_n$ should look like. &lt;/p&gt;&#10;&#10;&lt;p&gt;The general theory in the book is constructed in such way that norming constant is restricted in any way, but final theorems which give &lt;strong&gt;necessary and sufficient&lt;/strong&gt; conditions, do not leave any room for norming constant other than $\sqrt{n}$.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-09-19T13:53:47.700" Id="37582" LastActivityDate="2012-09-19T17:17:46.833" LastEditDate="2012-09-19T17:17:46.833" LastEditorUserId="2116" OwnerUserId="2116" ParentId="36113" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="39107" AnswerCount="3" Body="&lt;p&gt;The second question is that I found in a discussion somewhere on the web talking about &quot;supervised clustering&quot;, as far as I know, clustering is unsupervised, so what is exactly the meaning behind &quot;supervised clustering&quot; ? What is the difference with respect to &quot;classification&quot; ?&lt;/p&gt;&#10;&#10;&lt;p&gt;There are many links talking about that:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.cs.uh.edu/docs/cosc/technical-reports/2005/05_10.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.cs.uh.edu/docs/cosc/technical-reports/2005/05_10.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://books.nips.cc/papers/files/nips23/NIPS2010_0427.pdf&quot; rel=&quot;nofollow&quot;&gt;http://books.nips.cc/papers/files/nips23/NIPS2010_0427.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://engr.case.edu/ray_soumya/mlrg/supervised_clustering_finley_joachims_icml05.pdf&quot; rel=&quot;nofollow&quot;&gt;http://engr.case.edu/ray_soumya/mlrg/supervised_clustering_finley_joachims_icml05.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.public.asu.edu/~kvanlehn/Stringent/PDF/05CICL_UP_DB_PWJ_KVL.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.public.asu.edu/~kvanlehn/Stringent/PDF/05CICL_UP_DB_PWJ_KVL.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.machinelearning.org/proceedings/icml2007/papers/366.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.machinelearning.org/proceedings/icml2007/papers/366.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.cs.cornell.edu/~tomf/publications/supervised_kmeans-08.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.cs.cornell.edu/~tomf/publications/supervised_kmeans-08.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume6/daume05a/daume05a.pdf&quot; rel=&quot;nofollow&quot;&gt;http://jmlr.csail.mit.edu/papers/volume6/daume05a/daume05a.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;etc ...&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-09-19T14:40:21.963" FavoriteCount="6" Id="37587" LastActivityDate="2012-10-12T06:12:30.527" LastEditDate="2012-10-10T12:55:56.247" LastEditorUserId="88" OwnerUserId="8114" PostTypeId="1" Score="8" Tags="&lt;clustering&gt;&lt;classification&gt;&lt;unsupervised-learning&gt;&lt;statistical-learning&gt;" Title="Supervised clustering or classification?" ViewCount="5278" />
  
  <row Body="&lt;p&gt;Similar to &lt;a href=&quot;http://stats.stackexchange.com/a/37480/2485&quot;&gt;steffen's suggestion&lt;/a&gt; of RapidMiner, you might want to consider &lt;a href=&quot;http://en.wikipedia.org/wiki/Weka_%28machine_learning%29&quot; rel=&quot;nofollow&quot;&gt;Weka&lt;/a&gt;.  It may be geared more specifically to machine learning than you are hoping for though.  It has lots of algorithms for tasks like clustering, classification, and regression.  Weka has a GUI, but it can also be used as a software library as well.  I've seen histograms in the GUI but I'm not sure if it's easy to reuse them through the library or not.&lt;/p&gt;&#10;" CommentCount="1" CommunityOwnedDate="2014-01-17T19:50:11.640" CreationDate="2012-09-19T15:41:56.753" Id="37593" LastActivityDate="2012-09-19T15:41:56.753" OwnerUserId="2485" ParentId="37466" PostTypeId="2" Score="1" />
  
  
  
  
  <row Body="&lt;h2&gt;Hull-White/Vasicek Model: dX(t) = 3*(2-x)*dt+ 2*dw(t)&lt;/h2&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; library(Sim.DiffProc)&#10;&amp;gt; drift &amp;lt;- expression( (3*(2-x)) )&#10;&amp;gt; diffusion &amp;lt;- expression( (2) )&#10;&amp;gt; snssde(N=1000,M=1,T=1,t0=0,x0=10,Dt=0.001,drift,diffusion,Output=FALSE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Multiple trajectories of the OU process by Euler Scheme&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; snssde(N=1000,M=50,T=1,t0=0,x0=10,Dt=0.001,drift,diffusion,Output=FALSE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;You can also use the package &lt;a href=&quot;http://cran.r-project.org/web/packages/Sim.DiffProcGUI/index.html&quot; rel=&quot;nofollow&quot;&gt;Sim.DiffProcGUI&lt;/a&gt; (Graphical User Interface for Simulation of Diffusion Processes).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-09-19T19:45:09.830" Id="37616" LastActivityDate="2012-09-20T12:40:43.397" LastEditDate="2012-09-20T12:40:43.397" LastEditorUserId="930" OwnerUserId="14207" ParentId="16416" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;I will assume that the number of occurrences for A, B, C is marginally binomial for each event type.  For large n the normal approximation with the continuity correction can work as an approximate confidence interval.  If the true proportion is close to either 0 or 1 the normal approximation will be less accurate and could have lower endpoint negative in the case p is close to 0 and the upper bound could go above 1 when p is close to 1.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now the Clopper-Pearson method provides a relatively fast method for calculating exact binomial confidnce intervals for any n.  So I would suggest going with the exact method. For a good reference, look at &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0471887692&quot; rel=&quot;nofollow&quot;&gt;Statistical Intervals by Hahn and Meeker&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-09-19T21:10:36.740" Id="37622" LastActivityDate="2012-09-19T21:10:36.740" OwnerUserId="11032" ParentId="37612" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="37653" AnswerCount="1" Body="&lt;p&gt;I am turning my work from medical statistics into biology statistics (fishery). I am wondering if there is a big differences in methodology or focus between these two fields? In medical field, we do a lot clinical trials or patient cohort studies, and we try to find associations by applying different types of significance tests and regressions. In the case of biology, there is very limited control of the sampling method. I have a feeling that the statistician's main focus would be to find the best way to sampling the data. Since I am really fresh in this field, can someone give me some ideas about the main focus, common methodologies in biology statistics? Is it really different from medical statistics? Do you have any nice books to recommend?&#10;Thanks  &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-09-19T21:19:58.857" FavoriteCount="1" Id="37624" LastActivityDate="2013-08-20T00:02:54.840" LastEditDate="2013-08-20T00:02:54.840" LastEditorUserId="22468" OwnerUserId="13702" PostTypeId="1" Score="3" Tags="&lt;references&gt;&lt;biostatistics&gt;&lt;methodology&gt;" Title="The biggest difference between medical statistics and biological statistics" ViewCount="236" />
  <row Body="&lt;p&gt;Before looking on QQplots of residuals, you should assess the quality of fit, by plotting residuals against the predictors in the model (and possibly, also againt other variables you hav ewhich you did not use). Nonlinearity should show up in this plots. If the effect of varoble $x$ really is linear, you expect the plot of residuals against $x$ to be $horizontal&quot;, without visible structure:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;                                                                   *&#10;*                 *&#10;      *                               *&#10;        *   &#10;                                                  *&#10;--------------------------------------*------------------------------x&#10;   *     &#10;           *&#10;&#10;                                     *&#10;       *                                                    *&#10;                                *&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;That is, a random horizontal &quot;blob&quot; of points, centered around the line resid=0.&lt;/p&gt;&#10;&#10;&lt;p&gt;If the effect is non-linear, you expect to see some curvature in this plot.&#10;(and, please, ignore the QQplots until you got non-linearities sorted out, using plots as above!)&lt;/p&gt;&#10;&#10;&lt;p&gt;You should also think about possible interactions (modelled usually by product terms), that is, the effect of one variable depends on the levels of another, (If all your three variables have high values at the same time, maybe that shows some particularly difficult patient? If so, interactions could be needed).&lt;/p&gt;&#10;&#10;&lt;p&gt;If you go for some nonlinear model, after having tried for interactions and transformations (did you try log(Cost)?) Did you try some box-cox-transformations?  Since you have multiple regression, I dont think that loess is what you need, you should look for gam (generalized additive models, SAS should have that, in R it is in package mgcv).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-09-19T21:26:39.780" Id="37627" LastActivityDate="2012-09-19T21:26:39.780" OwnerUserId="11887" ParentId="37620" PostTypeId="2" Score="5" />
  
  <row AcceptedAnswerId="37652" AnswerCount="1" Body="&lt;p&gt;As I understand it, one of the main goal of the chi-squared test on a contingency table is to determine if the link between lines and columns of the table is &quot;more&quot; than the sampling bias and the random fluctuations it can generate.&lt;/p&gt;&#10;&#10;&lt;p&gt;So my question is : if a contingency table contains the whole surveyed population, there is no more sampling bias. So does it make sense to apply a chi-squared test on this table, or do we just have to look at line and column percentages without worrying to test against the null hypothesis ?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance for any hints !&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-09-20T06:58:49.690" Id="37651" LastActivityDate="2012-09-20T07:00:26.660" OwnerUserId="11707" PostTypeId="1" Score="6" Tags="&lt;sampling&gt;&lt;chi-squared&gt;" Title="Does it make sense to apply a chi-squared test on a contingency table when the whole population has been surveyed?" ViewCount="131" />
  <row Body="&lt;p&gt;A couple things come to mind. As a disclaimer, my exposure to the statistics of Biology come from undergrad, and the occasional times my research overlaps with Ecologists.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;First, &quot;biological statistics&quot; is a staggeringly broad definition. What might be routine for the microbiology people may be impossible for the population ecologists, for example.&lt;/li&gt;&#10;&lt;li&gt;As mentioned by Michael Chernick, there may be some sampling methods that fisheries research relies on that don't generally show up in clinical or population-based health research. Capture-Recapture comes to mind as one. The sheer difficulty in observing animals, who by their nature tend to be poor research subjects, and sample size issues. To take a somewhat extreme example, if one is doing Black Rhino research, your maximum N will be 4,240. But even for more abundant species, sampling large numbers can be extremely difficult.&lt;/li&gt;&#10;&lt;li&gt;Multi-species analysis is generally of greater interest to Biologists than medical researchers. While one could make an argument that some infectious disease research is multi-species analysis, attempting to estimate species diversity, predator-prey relationships, ecological networks etc. are fairly field specific.&lt;/li&gt;&#10;&lt;li&gt;While spatial statistics are becoming a thing in population health research, they're already a major aspect of biological research.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I suspect one of the hardest transitions will just be figuring out what counts as &quot;standard&quot; in the field in terms of analysis.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-09-20T07:18:36.427" Id="37653" LastActivityDate="2012-09-20T07:18:36.427" OwnerUserId="5836" ParentId="37624" PostTypeId="2" Score="1" />
  
  
  
  
  
  
  
  <row AcceptedAnswerId="37724" AnswerCount="3" Body="&lt;p&gt;I would like to make a prediction for a (new) subject to have a certain outcome given the historical data and the model:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;glm(outcome ~ age + treatment + history, family=binomial, ...) &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;however in the historical data that will be fitted by the model, I have some sort of repeated measurements on some of the subjects (and I don't know if repeated measures is the appropriate term to be used here, hence using lmer etc is doubtful); example:&lt;br&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;subject_ID    age    treatment    history    outcome&#10;S_1           33      T_1         H_1        0&#10;S_2           27      T_2         H_2        1&#10;S_2           27      T_3         H_2        1&#10;S_3           56      T_1         H_11       0&#10;etc...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In this example subject_2 (S_2) has two rows because he had simultaneously two different treatments at the same time. could a logistic regression still be used or should cases like subject_2 be removed from the analysis?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-09-20T23:17:51.470" Id="37714" LastActivityDate="2012-09-21T10:40:56.003" LastEditDate="2012-09-21T10:40:56.003" LastEditorUserId="2116" OwnerUserId="14247" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;regression&gt;&lt;repeated-measures&gt;" Title="Logistic regression with repeated measures?" ViewCount="586" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have several study sites, 4 from 2009, 2 from 2010, and 5 from 2011, for a total of 11 sites. At each site, I have 3 quadrats (defined unit of space), and within each quadrat I placed 25 live oysters (75 then at each site). After 1 year, I went back to see how many I could find alive and dead, and then was able to calculate how many went missing (washed away by waves, animal ate, person took, etc). So what I do not know, is what the proper test is to run to compare between the different sites.&lt;/p&gt;&#10;&#10;&lt;p&gt;Someone once told me to do a contingency table analysis, another told me chi square, and also regression. If you have any suggestions or help that would be great. Thanks, Brian&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-09-21T03:37:27.163" Id="37723" LastActivityDate="2012-09-21T16:28:46.377" LastEditDate="2012-09-21T10:42:17.767" LastEditorUserId="686" OwnerDisplayName="user14251" PostTypeId="1" Score="2" Tags="&lt;chi-squared&gt;&lt;survival&gt;&lt;missing-data&gt;&lt;contingency-tables&gt;" Title="How do I correctly analyze my live, dead, missing individuals between sites?" ViewCount="172" />
  <row Body="&lt;p&gt;If all of those repetitions are exactly like you've described here for the subject &lt;code&gt;S_2&lt;/code&gt; (i.e., simultaneous application of treatments), I would collapse these two lines together and fill in the treatment as an additional level of the treatment, namely the interaction of treatments &lt;code&gt;T_2&lt;/code&gt; and &lt;code&gt;T_3&lt;/code&gt;.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-09-21T04:06:33.033" Id="37724" LastActivityDate="2012-09-21T04:06:33.033" OwnerUserId="5739" ParentId="37714" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;@StasK gives a good method. Another possibility is to have one variable called &quot;treatment&quot; but have values for it that include multiple treatments - e.g. if there are three possible treatments, the new variable could have levels of none, t1, t2, t3, t12, t13, t23 and t123.&lt;/p&gt;&#10;&#10;&lt;p&gt;I think the results ought to match with the interaction approach, but they may be easier to interpret. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-09-21T10:20:50.937" Id="37735" LastActivityDate="2012-09-21T10:20:50.937" OwnerUserId="686" ParentId="37714" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;You need $\rho$ to account for how correlated the variables are. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you had actual data, then $\rho$ would be output. But here MrProf is simulating data, so it is input. Just like, if you had actual data, $\mu$ and $\sigma$ would be output, but in order to simulate data, they are input. &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-09-21T11:59:01.950" Id="37739" LastActivityDate="2012-09-21T11:59:01.950" OwnerUserId="686" ParentId="37738" PostTypeId="2" Score="4" />
  <row AnswerCount="3" Body="&lt;p&gt;I have been burdened with the task of coming up with a forecast plan for my company.  I have no experience and am VERY new to the whole forecasting scene.  As of right now, my company has no plans of investing in any forecasting software so my only tool is Excel.  I've tried to do some research online myself and it seems that this triple smoothing method would be a great asset, but I'm a little confused and I guess I don't really understand the equations.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Below I have provided 3 years worth of sales for one item. We forecast in periods (4 weeks = 1 period). So there are 13 periods in one year. When we forecast, we have to forecast out 6 periods into the future, please help me use the triple smoothing technique to accomplish this.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Period 10 2009  69,088&#10;Period 11 2009  83,400&#10;Period 12 2009  75,735&#10;Period 13 2009  79,526&#10;Period 01 2010  81,005&#10;Period 02 2010  94,013&#10;Period 03 2010  90,567&#10;Period 04 2010  94,568&#10;Period 05 2010  101,687&#10;Period 06 2010  93,540&#10;Period 07 2010  84,249&#10;Period 08 2010  91,280&#10;Period 09 2010  78,531&#10;Period 10 2010  89,465&#10;Period 11 2010  83,341&#10;Period 12 2010  87,106&#10;Period 13 2010  65,636&#10;Period 01 2011  79,632&#10;Period 02 2011  89,722&#10;Period 03 2011  87,483&#10;Period 04 2011  99,228&#10;Period 05 2011  113,215&#10;Period 06 2011  96,057&#10;Period 07 2011  95,475&#10;Period 08 2011  92,466&#10;Period 09 2011  103,529&#10;Period 10 2011  94,515&#10;Period 11 2011  76,146&#10;Period 12 2011  81,736&#10;Period 13 2011  80,174&#10;Period 01 2012  81,437&#10;Period 02 2012  102,695&#10;Period 03 2012  120,775&#10;Period 04 2012  97,058&#10;Period 05 2012  119,921&#10;Period 06 2012  102,311&#10;Period 07 2012  109,498&#10;Period 08 2012  110,318&#10;Period 09 2012  98,103&#10;&#10;Period 10 2012  &#10;Period 11 2012  &#10;Period 12 2012  &#10;Period 13 2012  &#10;Period 01 2013  &#10;Period 02 2013  &#10;Period 03 2013  &#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2012-09-21T15:57:23.923" FavoriteCount="2" Id="37753" LastActivityDate="2013-08-24T13:51:20.437" LastEditDate="2012-09-21T16:22:50.600" LastEditorUserId="7290" OwnerUserId="14263" PostTypeId="1" Score="4" Tags="&lt;forecasting&gt;&lt;excel&gt;&lt;exponential-smoothing&gt;" Title="How to use triple exponential smoothing to forecast in Excel" ViewCount="5410" />
  
  <row AcceptedAnswerId="37780" AnswerCount="3" Body="&lt;p&gt;Random assignment is valuable because it ensures independence of treatment from potential outcomes.  That is how it leads to unbiased estimates of the average treatment effect.  But other assignment schemes can also systematically ensure independence of treatment from potential outcomes.  So why do we need random assignment?  Put another way, what is the advantage of random assignment over nonrandom assignment schemes that also lead to unbiased inference?&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $\mathbf{Z}$ be a vector of treatment assignments in which each element is 0 (unit not assigned to treatment) or 1 (unit assigned to treatment).  In a JASA article, &lt;a href=&quot;http://people.ucsc.edu/~cdobkin/Classes/Reza/Identification%20of%20Causal%20Effects%20Using%20Instrumental%20Variables%20%28Angrist%29.pdf&quot;&gt;Angrist, Imbens, and Rubin (1996, 446-47)&lt;/a&gt; say that treatment assignment $Z_i$ is random if $\Pr(\mathbf{Z} = \mathbf{c}) = \Pr(\mathbf{Z} = \mathbf{c'})$ for all $\mathbf{c}$ and $\mathbf{c'}$ such that $\iota^T\mathbf{c} = \iota^T\mathbf{c'}$, where $\iota$ is a column vector with all elements equal to 1.&lt;/p&gt;&#10;&#10;&lt;p&gt;In words, the claim is that assignment $Z_i$ is random if any vector of assignments that includes $m$ assignments to treatment is as likely as any other vector that includes $m$ assignments to treatment.&lt;/p&gt;&#10;&#10;&lt;p&gt;But, to ensure independence of potential outcomes from treatment assignment, it suffices to ensure that each unit in the study has equal probability of assignment to treatment.  And that can easily occur even if most treatment assignment vectors have &lt;em&gt;zero&lt;/em&gt; probability of being selected.  That is, it can occur even under nonrandom assignment.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Here is an example. We want to run an experiment with four units in which exactly two are treated.  There are six possible assignment vectors:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;1100&lt;/li&gt;&#10;&lt;li&gt;1010&lt;/li&gt;&#10;&lt;li&gt;1001&lt;/li&gt;&#10;&lt;li&gt;0110&lt;/li&gt;&#10;&lt;li&gt;0101&lt;/li&gt;&#10;&lt;li&gt;0011&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;where the first digit in each number indicates whether the first unit was treated, the second digit indicates whether the second unit was treated, and so on.&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose that we run an experiment in which we exclude the possibility of assignment vectors 3 and 4, but in which each of the other vectors has equal (25%) chance of being chosen.  This scheme is not random assignment in the AIR sense.  But in expectation, it leads to an unbiased estimate of the average treatment effect.  And that is no accident.  Any assignment scheme that gives subjects equal probability of assignment to treatment will permit unbiased estimation of the ATE.&lt;/p&gt;&#10;&#10;&lt;p&gt;So: why do we need random assignment in the AIR sense?  My argument is rooted in randomization inference; if one thinks instead in terms of model-based inference, does the AIR definition seem more defensible?&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2012-09-22T01:26:05.203" Id="37775" LastActivityDate="2012-09-23T16:24:23.570" LastEditDate="2012-09-22T11:31:57.533" LastEditorUserId="2116" OwnerUserId="9618" PostTypeId="1" Score="8" Tags="&lt;experiment-design&gt;&lt;econometrics&gt;&lt;causal-inference&gt;&lt;instrumental-variables&gt;&lt;randomization&gt;" Title="Random assignment: why bother?" ViewCount="787" />
  
  
  
  
  
  
  <row Body="&lt;p&gt;I assume you have the estimated parameters in his model so that given a data point you can make a prediction.  Then you could do predictions with both models and compare them. I see no problem with doing this as long as the comparison is meaningful (in this case is the test data set of the type that both models were built to predict.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am not familiar with mlogit so I cannot tell you if and how you can do it with that particular logistic regression program.  But many software products have fit and predict options where if you want to predict a new observation you just supply the model formula.&lt;/p&gt;&#10;&#10;&lt;p&gt;Aside from that it seems that it would be easy for you to program it yourself by just plugging in the data point to the model formula and computing the predicted logit or through the inverse transformation the predicted probability p.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-09-22T13:30:28.940" Id="37798" LastActivityDate="2012-09-22T13:30:28.940" OwnerUserId="11032" ParentId="37702" PostTypeId="2" Score="0" />
  
  <row AcceptedAnswerId="37808" AnswerCount="1" Body="&lt;p&gt;I have a GLMM of the form: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lmer(present? ~ factor1 + factor2 + continuous + factor1*continuous + &#10;                (1 | factor3), family=binomial)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;When I use &lt;code&gt;drop1(model, test=&quot;Chi&quot;)&lt;/code&gt;, I get different results than if I use &lt;code&gt;Anova(model, type=&quot;III&quot;)&lt;/code&gt; from the car package or &lt;code&gt;summary(model)&lt;/code&gt;. These latter two give the same answers. &lt;/p&gt;&#10;&#10;&lt;p&gt;Using a bunch of fabricated data, I have found that these two methods normally do not differ. They give the same answer for balanced linear models, unbalanced linear models (where unequal n in different groups), and for balanced generalised linear models, but not for balanced generalised linear mixed models. So it appears that only in cases where random factors are included does this discord manifest.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Why is there a discrepancy between these two methods?  &lt;/li&gt;&#10;&lt;li&gt;When using GLMM should &lt;code&gt;Anova()&lt;/code&gt; or &lt;code&gt;drop1()&lt;/code&gt; be used?  &lt;/li&gt;&#10;&lt;li&gt;The difference between these two is rather slight, at least for my data. Does it even matter which is used?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2012-09-22T16:01:48.237" FavoriteCount="4" Id="37805" LastActivityDate="2012-09-24T09:06:04.973" LastEditDate="2012-09-22T16:07:08.103" LastEditorUserId="7290" OwnerUserId="14283" PostTypeId="1" Score="7" Tags="&lt;r&gt;&lt;anova&gt;&lt;glmm&gt;" Title="Why do Anova( ) and drop1( ) provided different answers for GLMMs?" ViewCount="2237" />
  
  <row AcceptedAnswerId="37831" AnswerCount="1" Body="&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;strong&gt;Possible Duplicate:&lt;/strong&gt;&lt;br&gt;&#10;  &lt;a href=&quot;http://stats.stackexchange.com/questions/30159/is-it-possible-to-have-a-pair-of-gaussian-random-variables-for-which-the-joint-d&quot;&gt;Is it possible to have a pair of Gaussian random variables for which the joint distribution is not Gaussian?&lt;/a&gt;  &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&#10;&#10;&lt;p&gt;In the &lt;a href=&quot;http://en.wikipedia.org/wiki/Multivariate_normal_distribution&quot; rel=&quot;nofollow&quot;&gt;Wikipedia entry&lt;/a&gt; on the multivariate normal distribution, it says that one definition &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;is that a random vector is said to be &lt;em&gt;k&lt;/em&gt;-variate normally distributed&#10;  if every linear combination of its &lt;em&gt;k&lt;/em&gt; components has a univariate&#10;  normal distribution.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;However, since it's also true that any linear combination of normally distributed variables is itself normal, does this mean that any vector of univariate random normals is itself multivariate normal?  Is there ever any situation where a vector of random normals is &lt;strong&gt;not&lt;/strong&gt; multivariate normal?&lt;/p&gt;&#10;&#10;&lt;h2&gt;Update&lt;/h2&gt;&#10;&#10;&lt;p&gt;What I should have said was &quot;any linear combination of &lt;strong&gt;independent&lt;/strong&gt; normally distributed variables is itself normal.&quot;  All the answers below are good examples of variables that are not independent and thus not multivariate normal.  So I should rephrase my question to be:  is there ever any situation where a vector of &lt;strong&gt;independent&lt;/strong&gt; random normals is &lt;strong&gt;not&lt;/strong&gt; multivariate normal?  I'm leaving the headline as is, to reflect history and the nature of the answers to this question, but I will alter it if you think I should.  Sorry for any confusion I may have caused.&lt;/p&gt;&#10;" ClosedDate="2012-09-23T21:15:12.630" CommentCount="11" CreationDate="2012-09-23T05:58:42.437" Id="37829" LastActivityDate="2012-09-23T18:46:58.330" LastEditDate="2012-09-23T18:46:58.330" LastEditorUserId="8462" OwnerUserId="8462" PostTypeId="1" Score="3" Tags="&lt;probability&gt;&lt;normal-distribution&gt;&lt;multivariate-analysis&gt;" Title="Is a vector of normal random variables ever -not- multivariate normal" ViewCount="553" />
  
  <row AcceptedAnswerId="37849" AnswerCount="1" Body="&lt;p&gt;I am confused about the notation in my Stochastic Processes class, and I can't find a place in the textbook that explicitly defines this notation. He uses $E$ to mean simple expectation i.e. for tail sum formula for expectation it is written $$EX = \sum_{k = 1}^\infty P(X \ge k)$$ Then in the next paragraph, he writes that the probability of returning at least $k$ times $\{N(y) \ge k\}$ is $$E_xN(y) = \sum_{k = 1}P(N(y) \ge k) = \cdots = {\rho_{xy} \over 1 - \rho_{yy}}$$ where I added the &lt;code&gt;\cdots&lt;/code&gt; because the calculations aren't relevant. &lt;/p&gt;&#10;&#10;&lt;p&gt;As a direct consequence of this, I am confused about what the &quot;limiting fraction of time we spend in each state&quot; theorem is saying. It goes as follows: &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;strong&gt;Theorem 1.21 (Asymptotic Frequency)&lt;/strong&gt;. Suppose our Markov Chains is irreducible and all states are recurrent. If $N_n(y)$ is the number of visits to $y$ up to time $n$, then $${N_n(y) \over n} \to {1 \over E_yT_y}$$&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;where $T_y$ is defined to be $T_y^1$ for $$T_y^k = \min\{n &amp;gt; T_y^{k-1}: X_n = y\}$$ i.e. the time of first return to $y$. &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-09-23T16:46:17.063" Id="37847" LastActivityDate="2012-09-23T17:34:12.963" OwnerUserId="13022" PostTypeId="1" Score="2" Tags="&lt;stochastic-processes&gt;" Title="What does $E_yT_y$ mean?" ViewCount="93" />
  
  
  
  <row Body="&lt;p&gt;I found that using the original parameters that I used to setup the problem, the moving average was performing better, but when I started playing with the parameters that defined my dynamic model I found the Kalman Filter was performing much better.  Now that I have something setup to see the effects the parameters play I think I will gain a better intuition on what exactly is happening.  Thank you to those who replied and sorry if my question was/is vague.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-09-24T03:23:40.750" Id="37882" LastActivityDate="2012-09-24T03:23:40.750" OwnerUserId="14131" ParentId="37387" PostTypeId="2" Score="2" />
  
  <row AnswerCount="0" Body="&lt;p&gt;In the past I have trained Hidden Markov Models (HMMs) for sequence recognition with the Baum-Welch algorithm such that if there were $n$ classes, I would use training data $D_i$ for a class $i$ to generate a single HMM to recognize class $i$.&lt;/p&gt;&#10;&#10;&lt;p&gt;I was reading around and saw that there was a way to create one large HMM and train it using $D = \cup_{i=1}^n D_i$ such that I could recognize the class through observing the optimal state sequence (obtained through the Viterbi algorithm) to determine the class.&lt;/p&gt;&#10;&#10;&lt;p&gt;I believe this is called training through Viterbi realignment, but I am not sure. Any help on the matter would be appreciated. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-09-24T16:30:08.090" Id="37909" LastActivityDate="2012-09-24T18:06:22.547" LastEditDate="2012-09-24T18:06:22.547" LastEditorUserId="562" OwnerUserId="562" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;hidden-markov-model&gt;&lt;sequence-analysis&gt;" Title="Viterbi realignment" ViewCount="70" />
  <row AnswerCount="3" Body="&lt;p&gt;From my model, I'm asked to determine which variables are statistically significant. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;fitted.model &amp;lt;- lm(spending ~ sex + status + income, data=spending)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;My results were as follows:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Coefficients:&#10;                Estimate  Std. Error t value   Pr(&amp;gt;|t|)    &#10;(Intercept)    22.55565   17.19680   1.312   0.1968    &#10;sex         **-22.11833**  8.21111  -2.694   0.0101 *  &#10;status          0.05223    0.28111   0.186   0.8535    &#10;income          4.96198    1.02539   4.839 1.79e-05 ***&#10;verbal         -2.95949    2.17215  -1.362   0.1803 &#10;&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 &#10;&#10;Residual standard error: 22.69 on 42 degrees of freedom&#10;Multiple R-squared: 0.5267, Adjusted R-squared: 0.4816 &#10;F-statistic: 11.69 on 4 and 42 DF,  p-value: 1.815e-06.&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Do I have to look at the last column? If so, then &lt;code&gt;sex&lt;/code&gt; and &lt;code&gt;income&lt;/code&gt; would be statistically significant.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-09-24T17:22:43.440" FavoriteCount="1" Id="37912" LastActivityDate="2013-02-13T13:23:16.683" LastEditDate="2012-09-24T21:08:03.050" LastEditorUserId="930" OwnerUserId="14009" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;regression&gt;&lt;statistical-significance&gt;&lt;self-study&gt;" Title="How to determine which variables are statistically significant in multiple regression?" ViewCount="23180" />
  <row Body="&lt;p&gt;I would recommend to read the free online textbook by Rob J Hyndman and George Athanasopoulos:&#10;&lt;a href=&quot;http://otexts.com/fpp/&quot; rel=&quot;nofollow&quot;&gt;http://otexts.com/fpp/&lt;/a&gt;.&#10;There you find R code and a package to do all those things.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-09-25T08:50:54.183" Id="37948" LastActivityDate="2012-09-25T08:50:54.183" OwnerUserId="12147" ParentId="37908" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;The log transformed data looks not bad:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;trans &amp;lt;- log10(resids-1.0001*min(resids))&#10;qqnorm(trans)&#10;qqline(trans)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/aGoAr.png&quot; alt=&quot;qq-plot of log transformed data&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;If that is appropriate depends on the science behind the data.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-09-25T10:04:26.810" Id="37953" LastActivityDate="2012-09-25T10:56:28.410" LastEditDate="2012-09-25T10:56:28.410" LastEditorUserId="11849" OwnerUserId="11849" ParentId="37911" PostTypeId="2" Score="0" />
  <row AnswerCount="2" Body="&lt;p&gt;I am used to performing power analysis with unweighted data. In R, I use the pwr package, which is based on Cohen (1988).&lt;/p&gt;&#10;&#10;&lt;p&gt;How would you do power analysis on weighted survey data? I don't think you could use the same calculations. The weights would probably somehow have to figure into it? Is there a reference that discusses this?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-09-25T17:16:30.557" Id="37969" LastActivityDate="2012-09-25T21:26:49.453" OwnerUserId="7169" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;power-analysis&gt;&lt;weighted-sampling&gt;" Title="Power analysis with weighted survey data" ViewCount="323" />
  
  <row Body="&lt;p&gt;You are right.&lt;/p&gt;&#10;&#10;&lt;p&gt;The loss you optimize when training a neural network is basically the sum over your some function of the rows in your training set. That loss acts as a proxy called the &quot;empirical distribution&quot; for the true underlying distribution which you do not have access to.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, it might not hurt: if you have lots of data (since you generate it) you can train a really large network. In the limit, you have an infinitely large network with infinite training data - and the network can just remember all inputs and thus predict them perfectly. This sounds rather theoretical, but I have had cases where some kind of fixing the empirical distribution to somewhat resemble the true distribution more (because of prior knowedge) have not helped at all. Actually, the error on held out test data was roughly the same for both training sets.&lt;/p&gt;&#10;&#10;&lt;p&gt;So if it hurts your specific training problem depends on the data and the problem.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-09-25T18:31:24.447" Id="37972" LastActivityDate="2012-09-25T18:31:24.447" OwnerUserId="2860" ParentId="37967" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;What you are seeing is called &quot;Regression towards the Mean&quot; and is completely expected.  Any time that there is variability in the data (and yours looks like it has a bunch) then the prediction values will on average be between the overall mean and the observed values.  The plot you created, of the outcome vs the predicted values is not commonly done, for the reasons that you are seeing, it tends to confuse more than enlighten.  It is more common to plot the predicted values against the residuals as this plot will show more randomness if the model is reasonable.  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit to address comment below&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;csgillespie's example has been criticized for only including 1 predictor when the original question included 4.  Here is some quick R code that can be run to show the same patterns with 4 predictors:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# simulated data, no relationship&#10;df1 &amp;lt;- data.frame(y=rnorm(100), x1=rnorm(100), x2=rnorm(100),&#10;        x3=rnorm(100), x4=rnorm(100))&#10;&#10;fit1 &amp;lt;- lm( y ~ ., data=df1 )&#10;#plot(df1$y, fitted(fit1), asp=1)&#10;    scatter.smooth(df1$y, fitted(fit1), asp=1)&#10;abline(0,1)&#10;abline(h=mean(fitted(fit1)), col='lightgrey')&#10;plot(df1$y, resid(fit1))&#10;abline(h=0)&#10;&#10;plot(fitted(fit1), resid(fit1))&#10;abline(h=0)&#10;&#10;# simulated data, relationship&#10;library(MASS)&#10;df2 &amp;lt;- as.data.frame( mvrnorm(100, mu=1:5, Sigma= matrix(.7,5,5)+diag(rep(.3,5))))&#10;names(df2) &amp;lt;- c('y','x1','x2','x3','x4')&#10;&#10;fit2 &amp;lt;- lm( y ~ ., data=df2 )&#10;#plot(df2$y, fitted(fit2), asp=1)&#10;    scatter.smooth(df2$y, fitted(fit2), asp=1)&#10;abline(0,1)&#10;abline(h=mean(fitted(fit2)), col='lightgrey')&#10;plot(df2$y, resid(fit2))&#10;abline(h=0)&#10;&#10;plot(fitted(fit2), resid(fit2))&#10;abline(h=0)&#10;&#10;&#10;# real data&#10;fit3 &amp;lt;- lm( Murder~Population+Income+Illiteracy+Frost, data=as.data.frame(state.x77))&#10;&#10;scatter.smooth( state.x77[,'Murder'], fitted(fit3), asp=1)&#10;abline(0,1)&#10;abline(h=mean(fitted(fit3)), col='lightgrey')&#10;plot(state.x77[,'Murder'], resid(fit3))&#10;abline(h=0)&#10;&#10;plot(fitted(fit3), resid(fit3))&#10;abline(h=0)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Notice that the plots look very similar to those in the original question.&lt;/p&gt;&#10;&#10;&lt;p&gt;Notice also that in the plot of the original outcome vs. the fitted values that the points (and more so their trend) tend to fall between the $y=x$ line and the mean line.  This is the idea of regression towards the mean as originally described by Galton &lt;a href=&quot;http://en.wikipedia.org/wiki/Regression_toward_the_mean#History&quot; rel=&quot;nofollow&quot;&gt;See Here&lt;/a&gt;.  The points are not randomly scattered about the $y=x$ line like the original poster assumed and what would happen without the regression towards the mean, rather they follow a linear trend along a line that represents proportianality between the $y=x$ line and the overall mean line, just as predicted by Galton.  The term regression towards the mean (and variants) is sometimes used for other concepts (some closer related to the original than others) as can be seen in the above article, and that may be where some of the confusion comes from.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-09-25T19:50:30.543" Id="37974" LastActivityDate="2012-09-26T17:53:46.353" LastEditDate="2012-09-26T17:53:46.353" LastEditorUserId="4505" OwnerUserId="4505" ParentId="37973" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;If you assume your events occurs in a poisson process, then the waiting time until an event is exponentially distributed with density function&#10;$$&#10;    f_Y(y) = \lambda e^{-\lambda y} \qquad \text{for}~~ y&amp;gt;0&#10;$$&#10;($\lambda&amp;gt;0$, the expectation of $Y$ is $1/\lambda$). Then the probability of an event in no more time than $a$ years is (a simple integral)&#10;$P(Y \le a)=1-e^{-\lambda a}$. Calculate this with $a=1$ and then with $a=2$ and you get your correction factor. &lt;/p&gt;&#10;&#10;&lt;p&gt;You must of course think about IF this is a defensible assumption!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-09-25T23:06:42.860" Id="37991" LastActivityDate="2012-09-25T23:06:42.860" OwnerUserId="11887" ParentId="37709" PostTypeId="2" Score="1" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I've been trying to read up on Poisson regression models, and it looks like it is possible to estimate such a model with a binary outcome. This has come up before on this site &lt;a href=&quot;http://stats.stackexchange.com/questions/18595/poisson-regression-to-estimate-relative-risk-for-binary-outcomes&quot;&gt;here&lt;/a&gt; (and somewhat &lt;a href=&quot;http://stats.stackexchange.com/questions/11096/how-to-interpret-coefficients-in-a-poisson-regression&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;http://stats.stackexchange.com/questions/24821/interpreting-coefficients-for-poisson-regression&quot;&gt;there&lt;/a&gt;). I am still a bit confused about how to interpret the coefficients when the outcome is binary, and how to specify an offset to facilitate this interpretation. Let's assume that $E[y|x]=\exp(a+\beta x + \gamma d)$, where $x$ is continuous, and $d$ and $y$ are binary. Let's say my random sample is $N=10,000$ and $y=1$ happens 10% of the time and $d=1$ 25% of the time. &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;I believe (but would like to verify) that for small values of $\beta$, I can interpret it as an elasticity, so if $\hat \beta=.05$, that's a ~5% increase in $\Pr (y)=1$ for an additional unit of $x$. When $\beta$ is larger, it is more accurate to exponentiate it, so if $\hat \beta=.5$, that corresponds to a 65% increase in $\Pr (y=1)$. With binary $d$ going from 0 to 1, the marginal effect is $\exp(\hat \gamma)-1,$ so for $\hat \gamma=0.3$, we would get a 35% increase.&lt;/li&gt;&#10;&lt;li&gt;Can I interpret $\exp(\alpha)$ as the baseline probability for those with $d=0$ and $x=0$?&lt;/li&gt;&#10;&lt;li&gt;If the outcome $y$ was actual counts or even continuous, I can just change the end to read &quot;increase in $y$&quot; rather than  &quot;increase in $\Pr (y=1)$&quot;.          &lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="5" CreationDate="2012-09-26T03:34:28.633" Id="38004" LastActivityDate="2013-01-17T12:27:51.943" LastEditDate="2013-01-17T12:27:51.943" LastEditorUserId="17230" OwnerUserId="7071" PostTypeId="1" Score="3" Tags="&lt;regression&gt;&lt;poisson&gt;&lt;binary-data&gt;" Title="Poisson regression for binary data" ViewCount="698" />
  <row AcceptedAnswerId="38011" AnswerCount="1" Body="&lt;p&gt;I read Nonlinear Dimensionality Reduction by Lee and Verleysen [&lt;a href=&quot;http://books.google.si/books?id=o_TIoyeO7AsC&amp;amp;lpg=PA8&amp;amp;ots=CMId9rg3Iu&amp;amp;dq=finite%20eight%20order%20moment%20identically%20distributed&amp;amp;pg=PA8#v=onepage&amp;amp;q=finite%20eight%20order%20moment%20identically%20distributed&amp;amp;f=false&quot;&gt;Google Books&lt;/a&gt;] and came across the following theorem (p. 8):&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Let $\mathbf{y}$ be a $D$-dimensional vector $[y_1, \ldots, y_d, \ldots,y_D]'$; all components $y_d$ of the vector are i.i.d. with a finite eighth order moment.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Can anyone please explain what the authors mean by &quot;finite eighth order moment&quot;?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-09-26T06:10:49.287" Id="38010" LastActivityDate="2012-09-26T10:54:40.777" LastEditDate="2012-09-26T10:54:40.777" LastEditorUserId="11032" OwnerUserId="609" PostTypeId="1" Score="5" Tags="&lt;distributions&gt;&lt;mathematical-statistics&gt;&lt;moments&gt;" Title="Eighth order moment" ViewCount="117" />
  <row AnswerCount="2" Body="&lt;p&gt;When regressing income ($Y$) on age ($X$) moderated by gender ($Z$), I not only find significant effects for age ($X$), age squared ($X^2$), gender ($Z$), the interaction of age and gender ($XZ$), and the interaction of squared age and gender ($X^2Z$). Could anyone help me with how to interpret these results? Specifically, how can I calculate simple slopes for the interaction effects of age squared and gender in SPSS?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-09-26T09:33:48.960" Id="38019" LastActivityDate="2013-03-15T08:29:24.823" LastEditDate="2012-09-27T00:13:04.920" LastEditorUserId="7290" OwnerUserId="14391" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;interaction&gt;&lt;interpretation&gt;&lt;quadratic-form&gt;" Title="Interpretation for simple slope analysis for curvilinear regression with interaction effects" ViewCount="518" />
  <row Body="&lt;p&gt;L1-regularized logistic regression.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;It is computationally fast.&lt;/li&gt;&#10;&lt;li&gt;It has an intuitive interpretation.&lt;/li&gt;&#10;&lt;li&gt;It has only one easily understandable hyperparameter that can be automatically tuned by cross-validation, which often is a good way to go.&lt;/li&gt;&#10;&lt;li&gt;Its coefficients are piecewise linear and their relation to the hyperparameter is &#10;instantly and easily visible in a simple plot.&lt;/li&gt;&#10;&lt;li&gt;It is one of the less dubious methods for variable selection.&lt;/li&gt;&#10;&lt;li&gt;Also it has a really cool name.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="1" CommunityOwnedDate="2012-09-26T10:10:10.707" CreationDate="2012-09-26T10:10:10.707" Id="38021" LastActivityDate="2012-09-26T10:10:10.707" OwnerUserId="10064" ParentId="258" PostTypeId="2" Score="3" />
  
  
  
  <row Body="&lt;p&gt;I think this is a valid procedure for feature selection which is no more prone to overfitting than other feature selection procedures. The problem with this procedure is that it has large computational complexity and barely can be used for real data sets.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-09-26T16:38:59.683" Id="38046" LastActivityDate="2012-09-26T16:38:59.683" OwnerUserId="4337" ParentId="38038" PostTypeId="2" Score="0" />
  
  <row AnswerCount="3" Body="&lt;p&gt;I have some data on a sample of $n=1776$ hospitals. For each hospital there is a total number of patients (&lt;code&gt;patients&lt;/code&gt;), and a number of patients diagnosed with a particular condition (&lt;code&gt;diagnosed&lt;/code&gt;). Do I take the mean of this proportion,&lt;code&gt;diagnosed&lt;/code&gt;/&lt;code&gt;patients&lt;/code&gt;, for all hospitals in the sample, $\hat{\mu}$, and calculate a 95% confidence interval as $\hat{\mu} \pm 1.96\sigma / \sqrt{n}$ or as $\hat{\mu} \pm 1.96  \sqrt{\hat{\mu}(1-\hat{\mu})/n}$ ? Or.... ?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;[Following comments from whuber]. Additionally, the data are broken down into 2 age groups (young and old) and 3 risk scores. That is, all 1776 hospitals have total numbers of patients as follows:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;               younger patients       older patients             &#10;&#10;Low risk            A                      D&#10;&#10;Medium risk         B                      E&#10;&#10;High risk           C                      F&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;...and similarly for the numbers of patients with the condition. &lt;/p&gt;&#10;&#10;&lt;p&gt;So, for each combination of age group and risk score, I would like to estimate the mean prevalence and a confidence interval for it.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is some summary of the data&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Risk   age    mean   sd      n&#10;1      u50    0.37   0.19    1776&#10;2      u50    0.49   0.25    1776&#10;3      u50    0.54   0.26    1776&#10;1      o50    0.45   0.36    1776&#10;2      o50    0.52   0.42    1776&#10;3      o50    0.67   0.41    1776&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="3" CreationDate="2012-09-26T20:28:33.607" Id="38057" LastActivityDate="2012-09-28T13:35:39.103" LastEditDate="2012-09-26T23:01:11.753" LastEditorUserId="88" OwnerUserId="11405" PostTypeId="1" Score="5" Tags="&lt;confidence-interval&gt;" Title="Confidence intervals for proportions (prevalence)" ViewCount="687" />
  
  <row AcceptedAnswerId="38084" AnswerCount="1" Body="&lt;p&gt;Coming from a rigorous background in analysis and modern probability theory, I find Bayesian statistics straightforward and easy to understand, and frequentist statistics incredibly confusing and unintuitive. It seems that frequentists are really doing bayesian statistics, except with &quot;secret priors&quot; that aren't well motivated or carefully defined.&lt;/p&gt;&#10;&#10;&lt;p&gt;On the other hand, a lot of great statisticians who understand both perspectives ascribe to the frequentist perspective, so there must be something there that I just don't understand. Rather than giving up and declaring myself a Bayesian, I'd like to learn more about the frequentist perspective to try to really &quot;grok&quot; it.&lt;/p&gt;&#10;&#10;&lt;p&gt;What are some good references for learning frequentist statistics from a rigorous perspective? Ideally I'm looking for definition-theorem-proof type books, or perhaps hard problem sets that, by solving them, I would gain the right mindset. I've read a lot of the more &quot;philosophical stuff&quot; one might find searching the internet - wiki pages, random pdfs from .edu/~randomprof sites, etc - and it hasn't helped.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-09-27T06:24:19.730" FavoriteCount="2" Id="38082" LastActivityDate="2012-09-27T06:56:08.320" OwnerUserId="14415" PostTypeId="1" Score="7" Tags="&lt;references&gt;&lt;frequentist&gt;&lt;intuition&gt;" Title="Frequentist statistics references for someone well versed in modern probability theory" ViewCount="287" />
  <row Body="&lt;p&gt;First off, you didn't (I am sure) find &quot;no effect&quot; in any case for any effect; you found no &lt;em&gt;significant&lt;/em&gt; effect. The key thing to look at is the effect size (parameter estimate), with and without the interaction, and see how it changes.&lt;/p&gt;&#10;&#10;&lt;p&gt;Second, certainly adding an interaction can change a main effect - it can make it larger or smaller. When you have an interaction in the model, say $X_1*X_2$, then the main effect for $X_1$ is the effect of change in $X_1$ when $X_2 = 0$ (similarly, the main effect for $X_2$ is the effect of change in $X_2$ when $X_1 = 0$).&lt;/p&gt;&#10;&#10;&lt;p&gt;Since your IV has 2 levels, let's use (as an example) that it is sex, coded 0 for male and 1 for female. Then the effect of the covariate when the interaction is in the model is the effect of the covariate for males. If it is smaller then the main effect of the covariate without the interaction, that means the effect of the covariate is smaller for males than for females. &lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-09-27T10:29:00.903" Id="38096" LastActivityDate="2012-09-27T10:29:00.903" OwnerUserId="686" ParentId="38091" PostTypeId="2" Score="2" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;Let $U \subset \mathbb{R}^n$ be a vector space with $\dim(U)=d$. A standard normal distribution on $U$ is the law of a random vector $X=(X_1, \ldots, X_n)$ taking values in $U$ and such that the coordinates of $X$ in one ($\iff$ in any) orthonormal basis of $U$ is a random vector made of $d$ independent standard normal distributions ${\cal N}(0, 1)$. &lt;/p&gt;&#10;&#10;&lt;p&gt;When reading &lt;a href=&quot;http://stats.stackexchange.com/questions/38086/what-will-be-the-effect-on-the-d-f-of-chi2-distribution#comment74961_38086&quot;&gt;this question&lt;/a&gt; I asked myself the following question. Let $Y=(Y_1, \ldots, Y_n)$ be a standard normal distribution on $\mathbb{R}^n$. Is is true that the conditional distribution of $Y$ given $Y \in U$ is the standard normal distribution on $U$ ?&lt;/p&gt;&#10;&#10;&lt;p&gt;The squared norm ${\Vert X \Vert}^2$ of $X$ has a chi-square distribution $\chi^2_d$. Thus, if this is true, that would explain @Argha's claim.&lt;/p&gt;&#10;&#10;&lt;p&gt;Sorry if the LaTeX is mistyped, I don't see the LaTeX rendering :(&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT 01/10/2012:&lt;/strong&gt; Ok I see. Write $y=u+v$ the orthogonal decompostion of $y$ in $U\oplus U^\perp$. Then $$\Pr(Y\in \mathrm{d}y \cap Y \in U)=\Pr(P_U Y \in \mathrm{d}u)$$. That shows that $(Y \mid Y \in U) \sim P_U Y$. This is little bit heuristic but morally correct. Finally it is clear from the definition that $P_U Y$ is standard normal on $U$.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-09-27T13:21:54.893" Id="38111" LastActivityDate="2014-02-27T11:41:09.377" LastEditDate="2012-10-02T05:08:56.953" LastEditorUserId="8402" OwnerUserId="8402" PostTypeId="1" Score="4" Tags="&lt;normal-distribution&gt;" Title="Standard normal distribution on a subspace" ViewCount="356" />
  
  
  <row Body="&lt;p&gt;First, I think you need to have a clear definition of your dependent variable. How are you going to quantify the product mix? &lt;/p&gt;&#10;&#10;&lt;p&gt;For example, if you only wanted to compare CDs and another product, your dependent variable could be defined as the ratio of one to the other. I don't see how you could discuss the mix of all of the products at once.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, I think this is more of a time series problem, not a cross sectional logistic regression problem. You might want to look at vector autoregressive models (VAR) to model how the mix changes over time.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-09-27T16:41:08.307" Id="38131" LastActivityDate="2012-09-27T16:41:08.307" OwnerUserId="10500" ParentId="38126" PostTypeId="2" Score="1" />
  
  <row AnswerCount="2" Body="&lt;p&gt;As a validation study, I use two libsvm-based svm classifier against the same data set.&#10;One classifier is libsvm implementation in Rapidminer. Another classifier is Libsvm itself. Both of them assume the same parameter setting.  However, the prediction score of these two classifiers are different.  What might be reason to cause this kind of difference?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-09-27T18:17:56.453" FavoriteCount="1" Id="38139" LastActivityDate="2013-08-09T08:16:41.690" OwnerUserId="6720" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;svm&gt;&lt;libsvm&gt;&lt;rapidminer&gt;" Title="Different prediction score for two SVM-based classifiers" ViewCount="423" />
  
  <row Body="&lt;p&gt;PeterFlom is right that the information you have is not enough to answer the question.  However when you ask what do I need to know I could say that if a divine spirit told you&#10;P(AUB)=.6 then since P(A)=.2 and P(B)=.3 then the desired answer .1.  It comes from the well known formula in probability&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;For any two event A and B P(AUB)=P(A)+P(B)-P(A∩B). So P(A∩B)=P(A)+P(B)-P(AUB)&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;So you only need to know P(AUB). This also makes it clear why you can't solve with the information at hand &lt;strong&gt;You do need to know P(AUB)&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I am adding to my answer because the OP mentioned that his problem is considering information where A=[set of radiologists in the sample space] and B=[set of members if the sample space with cancer] and he wants to understand what is the probability that a member of the cancer set that also is a radiologist will have cancer.  He states that he thinks that the probability that a cancer patient who is a radiologist would have a probability of 0.1 for having cancer. He thinks that is too low and presumably even 0.2 might seem too low as well.  I have 2 responses to that.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;1. It seems that the answer to that question is really asking for P[B|A]&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;P[B|A]=P{A∩B)/P(A).&lt;/strong&gt;  This does not have the same bounds as &lt;strong&gt;P(A∩B)&lt;/strong&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;For this problem &lt;strong&gt;P(B|A)&lt;/strong&gt; is not known either since we do not know &lt;strong&gt;P(A∩B)&lt;/strong&gt;.&#10;In this case P[B|A]=P(A∩B)/0.2 =&lt;strong&gt;5 P(A∩B)&lt;/strong&gt;.&#10;So P[B|A] has a lower bound of 0 and an upper bound of 5(.2)=1!&#10;So P[B|A] ** can be any probability regardless of what P(A) and P(B) are!**&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;2.  Why is a value less than 0.2 plausible?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;This is just a theory, but given no information about the smoker's profession the probability he/she has cancer is 0.2.  Now radiologist understand the dangers of cancer better than most smokers who are not radiologists.  So the radiologists that smoke might tend to be light smokers. Now light smokers are less likely to have cancer because smokers with cancer predominantly have lung cancer and light smokers are less likely to have lung cancer than the moderate or heavy smokers. &lt;/p&gt;&#10;&#10;&lt;p&gt;Initially it was not clear that the OP wanted P(B|A) to me.  After explaining the problem I think that it is because he is asking for the probability of cancer when you know the individual is a radiologist.&#10;.&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2012-09-27T21:38:57.337" Id="38149" LastActivityDate="2012-09-28T09:33:08.883" LastEditDate="2012-09-28T09:33:08.883" LastEditorUserId="11032" OwnerUserId="11032" ParentId="38146" PostTypeId="2" Score="-1" />
  <row Body="&lt;p&gt;How about a mixed effects model of some sort where&lt;/p&gt;&#10;&#10;&lt;p&gt;$p(d_{s,i,t})=f(\beta_s+\alpha_{s,t}+\gamma_t+\epsilon_i)$&lt;/p&gt;&#10;&#10;&lt;p&gt;Where $\beta_s$ is a fixed state effect estimated from the previous elections or polls, $\alpha_{s,t}$ is a random state effect for this particular poll at this particular time, $\gamma_t$ is the overall mood of the nation at the time of this poll.  You would estimate the $\beta_s$ from the previous poll or election data.  $\epsilon_i$ is a random variable reflecting individual randomness.&lt;/p&gt;&#10;&#10;&lt;p&gt;You would need some serious constraints on the $\alpha_{s,i}$ to make this estimatable, in fact you might want to set them all to zero, but I've left them in for completeness in case there's a clever way to use them.  More generally, there's a range of possible ways of finishing this off, constraining the model and estimating parameters.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Once you've fit the model you can use it with the full population data to estimate the total result.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-09-27T23:47:31.377" Id="38154" LastActivityDate="2012-09-27T23:47:31.377" OwnerUserId="7972" ParentId="38143" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="38190" AnswerCount="2" Body="&lt;p&gt;In epidemiology, we often deal with lots of factors associated with disease. This multitude of factors makes plots of effect sizes (often hazard ratios) of individual factors confusing. Accepting some oversimplification, factors can for the most part be reasonably assigned to some group like nutritional, lifestyle, social, psychological, demographic, and maybe even subgroups like e.g. macro-/micronutrients for nutritional factors.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am searching for a plot that gives an overview of the effects within these groups while facilitating a visual comparison of size and distribution of effects within and between groups.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have tried &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;small multiples of histograms/smoothed density estimates and parallel boxplots of HRs in each group&lt;/li&gt;&#10;&lt;li&gt;forest-plot-like plots showing effects sizes and their CIs as points and horizontal segments and groups separated by horizontal lines&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;These ad-hoc approaches are not optimal. The first discards most of the information on the level of the individual factor, the second is too detailed on that level. Neither provides real subgrouping. Neither provides an informative visual comparison of distribution AND size of effects within groups.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there something better?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-09-28T07:23:51.357" Id="38165" LastActivityDate="2012-11-28T01:02:46.133" OwnerUserId="10064" PostTypeId="1" Score="1" Tags="&lt;data-visualization&gt;&lt;effect-size&gt;" Title="What are good ways of visualizing many effects ordered in groups and subgroups?" ViewCount="154" />
  
  
  <row Body="&lt;p&gt;My general preference, when comparing a more complex model (here, NB) to a less complex one (here Poisson) is not to rely on any statistical test, but to run both and see if the predicted values are substantially different. (And what 'substantially' means is dependent on the field you are working in). If they are, then prefer the more complex model. If not, the simpler.&lt;/p&gt;&#10;&#10;&lt;p&gt;This allows us not to rely on arbitrary cutoffs; it requires us to employ judgement. Those are, in my opinion, good things. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-09-28T11:08:23.853" Id="38178" LastActivityDate="2012-09-28T11:08:23.853" OwnerUserId="686" ParentId="38177" PostTypeId="2" Score="3" />
  
  <row AcceptedAnswerId="56405" AnswerCount="2" Body="&lt;p&gt;I'm a new user of mixed models and through the material I've been reading there are always probability values (p&gt;t) or (p&gt;z) that estimate the importance of a level of a factor in the model. However, when using the &lt;code&gt;lmer()&lt;/code&gt; function in R, which supposedly gives you those probabilities, I simply don't find them. Here is the output: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Linear mixed model fit by REML &#10;Formula: Temp ~ depth + (1 | locality) &#10;   Data: qminmatrix &#10;   AIC   BIC logLik deviance REMLdev&#10; 561.3 581.3 -273.7    551.5   547.3&#10;Random effects:&#10; Groups   Name        Variance Std.Dev.&#10; locality (Intercept) 4.7998   2.1909  &#10; Residual             4.0433   2.0108  &#10;Number of obs: 128, groups: locality, 4&#10;&#10;Fixed effects:&#10;            Estimate Std. Error t value&#10;(Intercept)  22.0103     1.1500  19.140&#10;depth1        1.9564     0.6832   2.864&#10;depth10       2.6624     0.5756   4.625&#10;depth5        3.0209     0.4932   6.125&#10;depthWS      -2.2585     0.5444  -4.149&#10;&#10;Correlation of Fixed Effects:&#10;        (Intr) depth1 dpth10 depth5&#10;depth1  -0.157                     &#10;depth10 -0.175  0.189              &#10;depth5  -0.213  0.313  0.458       &#10;depthWS -0.191  0.334  0.373  0.441&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="4" CreationDate="2012-09-28T13:44:35.193" FavoriteCount="2" Id="38188" LastActivityDate="2014-02-26T09:19:59.373" LastEditDate="2012-11-23T15:07:29.330" LastEditorUserId="919" OwnerUserId="14465" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;p-value&gt;&lt;lmer&gt;" Title="Can't find p-values in the output from lmer() in the lm4 package in R" ViewCount="2174" />
  <row AnswerCount="0" Body="&lt;p&gt;For two $n$ dimensional multivariate normal distributions $X_{1}\sim N\left(\mu_{1},\Sigma_{1}\right)$&#10; and $X_{2}\sim N\left(\mu_{2},\Sigma_{2}\right)$, the Kullback-Leibler distance is given by $$KL=\frac{1}{2}\left(tr\left(\Sigma_{2}^{-1}\Sigma_{1}\right)+\left(\mu_{2}-\mu_{1}\right)'\Sigma_{1}^{-1}\left(\mu_{2}-\mu_{1}\right)-ln\left(\frac{det\left(\Sigma_{1}\right)}{det\left(\Sigma_{2}\right)}\right)-n\right)&#10; $$&lt;/p&gt;&#10;&#10;&lt;p&gt;When $\Sigma_{1}=\Sigma_{2}$, the above will be similar to half the Mahalanobis distance squared. The Mahalanobis distance squared is chi-square distributed with $n-1$ degrees of freedom, so I imagine that is part of the answer.&lt;/p&gt;&#10;&#10;&lt;p&gt;When the parameters of $KL$ are replaced with estimates from on a random sample, is it possible to derive the distribution in the more general case when $\Sigma_{1}$ is not necessarily equal to $\Sigma_{2}$?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-09-28T15:30:41.477" Id="38198" LastActivityDate="2012-09-28T20:41:59.797" LastEditDate="2012-09-28T20:41:59.797" LastEditorUserId="11623" OwnerUserId="11623" PostTypeId="1" Score="3" Tags="&lt;distributions&gt;&lt;kullback-leibler&gt;" Title="Sample distribution for Kullback-Leibler distance" ViewCount="151" />
  
  <row AcceptedAnswerId="38202" AnswerCount="2" Body="&lt;p&gt;This might be a question for the programmers, but I thought I would ask here first.  Im comparing browsing pressure on plants between sites.  Ive a value to indicate Browsing Pressure and count data of trees that have been damaged between locations.  Ive been using Crawleys R example (page 574) regarding sex ratios and my issue is with the lines command to plot the information.  I can't get a fitted regression line to the points to show up for my data.  At least not using the commands Crawley suggested.  Ive included Crawley's script for quick reference:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;density = c(1,4,10,22,55,121,210,444)&#10;females=c(1,3,7,18,22,41,52,79)&#10;males=c(0,1,3,4,33,80,158,365)&#10;numbers=as.data.frame(cbind(density,females,males))&#10;attach(numbers)&#10;par(mfrow=c(1,2))&#10;p&amp;lt;-males/(males+females)&#10;plot(log(density),p,ylab=&quot;Proportion male&quot;)&#10;y&amp;lt;-cbind(males,females)&#10;model&amp;lt;-glm(y~log(density),binomial)&#10;xv&amp;lt;-seq(0,6,0.1)&#10;plot(log(density),p,ylab=&quot;Proportion male&quot;)&#10;lines(xv,predict(model,list(density=exp(xv)),type=&quot;response&quot;))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;My Script:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;data&amp;lt;-####see the dput() data below to insert here&#10;attach(data)&#10;p1&amp;lt;-MS1/(MS1+M1)&#10;y&amp;lt;-cbind(MS1,M1)&#10;GM1&amp;lt;-glm(y~BPT,family=binomial (logit),data=data)&#10;summary(GM1)&#10;plot(BPT,p1,col=&quot;black&quot;,pch=1,main=&quot;Relationship a&quot;,xlab=&quot;Browsing pressure&quot;,&#10;ylab=&quot;Moose Damage Survey&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This is where I am having difficulties interpreting Crawley's &lt;code&gt;lines&lt;/code&gt; command.  I can create a sequence for my data like his xv values - mine go from 0 to 0.7.&lt;br&gt;&#10;&lt;code&gt;xv&amp;lt;-seq(0,0.7,0.01)&lt;/code&gt; But as Im not sure what is happening in the command line, blindly inserting my data results in no line showing on my graph.  I have some success with &lt;code&gt;regLine&lt;/code&gt; from the &lt;code&gt;car&lt;/code&gt; library, but its not a line fit to my data.  Any help explaining why this works for Crawleys but not my data would be appreciated&lt;/p&gt;&#10;&#10;&lt;p&gt;My DATA:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;structure(list(S = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, &#10;14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, &#10;30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, &#10;46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60), &#10;    BPT = c(0.195778643105884, 0.0651216427326909, 0.0199432997190648, &#10;    0.255717971810655, 0.21031730503132, 0.0380060076389563, &#10;    0.0592940237882193, 0.00425756881206537, 0.107351336677244, &#10;    0.353626952501855, 0.134358354834345, 0.0774003112699534, &#10;    0.0571275803730281, 0.102480049705623, 0.506048974065245, &#10;    0.637795327349053, 0, 0.00825369912299262, 0.123063816485164, &#10;    0.244214078077056, 0, 0.231982659809539, 0.0265824936284919, &#10;    0, 0, 0, 0.197470272555566, 0, 0.0481572367104686, 0.339072491473947, &#10;    0.0640004726042278, 0.099465609734403, 0, 0.153951519788959, &#10;    0.103791449528225, 0.22066571012681, 0.101286659375451, 0.0210055739071239, &#10;    0, 0, 0.00811999120846242, 0.0576334384710714, 0.0514027186783361, &#10;    0.00907843357263548, 0.0785740169740806, 0.0801612249152947, &#10;    0.0439884643248279, 0.189608233745472, 0.434349352638054, &#10;    0.188190501082836, 0.016381369054353, 0.215189432724754, &#10;    0.0948563822493123, 0, 0.0111745795351048, 0.0289751701436048, &#10;    0.0962515835377456, 0.199970627051051, 0.0306277369169669, &#10;    0), M1 = c(51, 123, 137, 23, 69, 98, 80, 59, 84, 87, 63, &#10;    88, 64, 75, 30, 19, 86, 152, 55, 22, 115, 66, 75, 124, 87, &#10;    179, 39, 66, 117, 37, 59, 103, 57, 65, 83, 68, 87, 104, 134, &#10;    89, 52, 61, 65, 75, 71, 97, 45, 37, 86, 82, 36, 139, 40, &#10;    56, 63, 64, 37, 79, 70, 121), M2 = c(108, 195, 255, 31, 104, &#10;    157, 135, 88, 117, 152, 96, 124, 102, 118, 37, 25, 135, 277, &#10;    97, 47, 174, 121, 134, 196, 163, 270, 75, 123, 214, 46, 109, &#10;    156, 92, 118, 127, 112, 134, 152, 194, 132, 107, 101, 105, &#10;    140, 107, 163, 87, 60, 122, 122, 66, 237, 66, 95, 108, 95, &#10;    50, 128, 128, 189)), .Names = c(&quot;S&quot;, &quot;BPT&quot;, &quot;M1&quot;, &quot;M2&quot;), row.names = c(NA, &#10;-60L), class = &quot;data.frame&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2012-09-28T16:24:47.807" Id="38201" LastActivityDate="2012-09-28T17:07:26.443" OwnerUserId="14125" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;data-visualization&gt;&lt;generalized-linear-model&gt;&lt;scatterplot&gt;" Title="Problems plotting GLM data of binomial proportional data" ViewCount="1639" />
  <row Body="&lt;p&gt;I would suggest a couple of links: &lt;/p&gt;&#10;&#10;&lt;p&gt;1) &lt;a href=&quot;http://stackoverflow.com/questions/10183000/r-function-for-a-vector&quot;&gt;Shrink number of levels of a factor variable&lt;/a&gt;&#10; is a link to a question on &lt;code&gt;stackoverflow&lt;/code&gt; to deal with a similar issue while using the &lt;code&gt;randomForest&lt;/code&gt; package.  Specifically it deals with using only the most frequently occurring levels and assigning a new level to all other, less frequently occurring levels.  &lt;/p&gt;&#10;&#10;&lt;p&gt;The idea for it came from here: &lt;a href=&quot;http://jmlr.csail.mit.edu/proceedings/papers/v7/miller09/miller09.pdf&quot; rel=&quot;nofollow&quot;&gt;2009 KDD Cup Slow Challenge&lt;/a&gt;.  The data for this competition had lots of factors with lots of levels and it discusses some of the methods they used to pare the data down from 50,000 rows by 15,000 columns to run on a 2-core/2GB RAM laptop.&lt;/p&gt;&#10;&#10;&lt;p&gt;My last suggestion would be to look at running the problem, as suggested above, in parallel on a hi-CPU Amazon EC2 instance.  &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-09-28T19:39:38.333" Id="38212" LastActivityDate="2012-09-28T19:39:38.333" OwnerUserId="7411" ParentId="37370" PostTypeId="2" Score="3" />
  
  <row AcceptedAnswerId="44565" AnswerCount="2" Body="&lt;p&gt;I am working with linear SVM (Using SVMlight) and I'm assisting to a weird phenomenon.&lt;/p&gt;&#10;&#10;&lt;p&gt;The training algorithm weighted some features 0. Does that means such features are irrelevant for the classification?&lt;/p&gt;&#10;&#10;&lt;p&gt;Looking at the dataset I found that the vectors containing such features belongs only to a category. And that to me sound like the features are very relevant to classify a new observation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Moreover I have examples of other features belonging to a single category that have a non zero weight and that confuses me even more.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is that behavior correct or I am missing something?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-09-28T19:46:14.750" Id="38213" LastActivityDate="2012-11-28T04:57:40.287" LastEditDate="2012-09-28T20:24:33.817" LastEditorUserId="88" OwnerUserId="10841" PostTypeId="1" Score="0" Tags="&lt;svm&gt;" Title="Meaning of 0-weighted linear weights in SVM" ViewCount="148" />
  <row AcceptedAnswerId="38261" AnswerCount="1" Body="&lt;p&gt;Let ${\bf X} = (X_1, \dots, X_n)$ denote the log-intensities of all $n = 291$ tropical cyclones from the north Atlantic basin between 1980 and 2006. Log-intensity is defined as the logarithm of its maximum sustained windspeed (measured in knots). A tropical cyclone is a category 5 hurricane if its maximum sustained windspeed is 137 knots or more, i.e. if its log-intensity is 4.92 or more. We are interested in the probability $p \in [0,1]$ of a tropical cyclone attaining category 5 strength. Consider the following model: $X_i \overset{{\rm iid}}{\sim} \mathsf{Normal}(\mu, \sigma^2)$, where $\sigma = 0.33, \mu \in (-\infty, \infty)$. Under this model $p = \Phi\left({\mu - 4.92 \over \sigma}\right)$ where $\Phi(x)$ denotes the standard normal CDF. Find the 95% ML confidence interval for $p$ based on the following summaries of the data $$\bar x = 4.21 \quad \sqrt{n} = 17.06 \quad \#\{x_i \ge 4.92\} = 9$$&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;My Work&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The formula for an ML interval for $X_i \overset{{\rm iid}}{\sim} \mathsf{Normal}(\mu, \sigma^2)$ with known $\sigma$ is $B_c(x) = [\bar X \mp c{\sigma \over \sqrt{n}}]$. Calculating along these lines, for $c = 1.96$ as the $z$ value that leaves $0.05$ probability in the two tails combined of the normal pdf, I get $B_{1.96}(x) = [4.172, 4.247]$, but when I try to evaluate the endpoints of this interval for $p$ given that $\Phi(x)$ is a monotone transformation, I get nonsense answers. What am I doing wrong?&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, this will never be handed in for any class, I simply came across this problem and wondered how it could be solved. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-09-28T23:41:25.227" Id="38230" LastActivityDate="2012-09-29T16:02:27.520" LastEditDate="2012-09-29T04:19:22.063" LastEditorUserId="13022" OwnerUserId="13022" PostTypeId="1" Score="2" Tags="&lt;confidence-interval&gt;&lt;maximum-likelihood&gt;" Title="How do I find a 95% percent ML confidence interval of the probability of category 5 strength" ViewCount="207" />
  
  
  
  <row Body="&lt;p&gt;The &quot;uniform&quot; in uniform prior doesn't just mean that hits and misses are equally likely. It means that you assume that you have a probability measure on the rates $[0,1]$ and this measure is the uniform measure. For example, it means the chance the true rate is between $0.9$ and $1.0$ is $0.1$. There are other measures on $[0,1]$ which have mean value $1/2$.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, if you start with the uniform prior and then observe $1$ hit and $1$ miss, the updated distribution is more concentrated around a rate of $1/2$ than the uniform prior. Instead of a probability of $\Delta r $ for the rate to be between $r$ and $r+\Delta r$, you would estimate the probability to be about $6 r (1-r) \Delta r$. (The factor of $6$ makes the total measure $1$.) This new distribution would also predict that the probability of getting a hit next time is $1/2$. If you observe an additional $h$ hits and $m$ misses, the expected probability of a hit is &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac{h+2}{h+m+4}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;This is closer to $1/2$ than $\frac{h+1}{h+m+2}$. This agrees with the fact that you started with a distribution which was more concentrated near $1/2$. Of course, if you include that first hit and miss then there have been $h+1$ hits and $m+1$ misses in total. &lt;/p&gt;&#10;&#10;&lt;p&gt;The distribution you get from the uniform prior by observing some number of hits and misses is called a &lt;a href=&quot;http://en.wikipedia.org/wiki/Beta_distribution&quot;&gt;beta distribution&lt;/a&gt;. The uniform distribution on $[0,1]$ is $\text{Beta}(1,1)$. Beta distributions have the nice property that if you update a beta distribution with an observation, the result is still in that family. From one observation, the $\text{Beta}(a,b)$ distribution is updated either to $\text{Beta}(a+1,b)$ or $\text{Beta}(a,b+1)$. The mean of a $\text{Beta}(a,b)$ distribution is $\frac{a}{a+b}$. It is perfectly reasonable to have a prior distribution which is not a beta distribution, but the answers might not come out as nicely.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;If I'm very confident that the hit rates are 50/50, I could add 2 instead of 1 &#10;to the hits and misses. Or if I'm less confident, I could add 1/2.&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;That might or might not agree with confidence. That is about whether you think it is likely that the rate is close to $1/2$. In some situations, you may believe the rate should be close to $0$ or close to $1$, and the first trial should be very informative. You might believe that half of your students know how to solve a type of problem, and that if you give a random student $5$ of these problems, the student is very likely to solve all $5$ or fail to solve any. This might be approximated by a $\text{Beta}(\epsilon,\epsilon)$ distribution so that after $h$ correct and $m$ incorrect, the average value is &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac{h+\epsilon}{h+m+2\epsilon},$$ &lt;/p&gt;&#10;&#10;&lt;p&gt;which is close to $0$ or $1$ after one observation.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-09-29T08:31:47.663" Id="38253" LastActivityDate="2012-09-29T08:31:47.663" OwnerUserId="11981" ParentId="38249" PostTypeId="2" Score="5" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;In a paper looking at hand movements and time taken to complete a task it appears that the ratio of movements over time is constant (i.e., movement and time are highly correlated) yet they say in the paper they can control independently for both variable - how is this possible?&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;There is a strong relationship between the time taken and number of&#10;  hand movements made (Spearman coefficient 0.79, P &amp;lt;0.01). This has&#10;  been demonstrated with ICSAD before. Therefore, why not just time the&#10;  procedure with a stopwatch? This is answered when we apply partial&#10;  correlation coefficient tests. When controlling for time, the number&#10;  of movements made significantly compares with surgical experience and&#10;  global score (correlation coefficient −0.44 and 0.56, respectively, P&#10;  &amp;lt;0.01 for both). However, when controlling for movement, the time&#10;  taken had no such relationship with experience and global rating&#10;  (correlation coefficient −0.02, P = 0.9; 0.10, P = 0.8, respectively),&#10;  suggesting that operative speed is secondary to economy of hand&#10;  movement.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Datta V, Chang A, Mackay S, Darzi A. The relationship between motion analysis and surgical technical assessments. &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/12135725&quot; rel=&quot;nofollow&quot;&gt;Am J Surg. 2002 Jul;184(1):70-3.&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-09-29T13:59:50.677" Id="38269" LastActivityDate="2013-11-07T17:56:58.247" LastEditDate="2012-10-06T20:12:04.727" LastEditorUserId="7290" OwnerUserId="14490" PostTypeId="1" Score="0" Tags="&lt;correlation&gt;&lt;statistical-control&gt;" Title="controlling for a highly correlated variable" ViewCount="202" />
  
  
  <row AcceptedAnswerId="38291" AnswerCount="1" Body="&lt;p&gt;$$SE = \frac{SD}{\sqrt{N}}$$&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;If N is the entire population, I would expect SE to be zero.&lt;/li&gt;&#10;&lt;li&gt;If N is equal to population size - 1, I would expect SE to be lower than if N is a small fraction of the population size, yet SE is the same.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;So why isn't population size part of SE?&lt;/p&gt;&#10;&#10;&lt;p&gt;While a mathematical explanation is great, I'd really like to get a gut feel intuition of the reason.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-09-29T21:21:34.653" FavoriteCount="1" Id="38290" LastActivityDate="2012-09-29T22:27:01.473" OwnerUserId="14498" PostTypeId="1" Score="5" Tags="&lt;standard-error&gt;" Title="Why does standard error not involve population size?" ViewCount="491" />
  
  <row AcceptedAnswerId="38338" AnswerCount="2" Body="&lt;p&gt;I took the original Stanford AI course and got an immense amount out of it. I got a poor score since I did not have a great deal of time to work out all the problems.. But what I really enjoyed about the course was getting a grasp of the new techiques, way beyond the conventional statitistcs..&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm looking for a way to get to the next level. The course really did not provide a good reference for reviewing the subject. The big book by the course authors (Artificial Intelligence: A Modern Approach by Norvig and Russell) (which I have) is too complex for me. &lt;/p&gt;&#10;&#10;&lt;p&gt;I try to do all my work in Python and I am making some headway in getting the tools, Sci-py, scikit learn, NLTK. Pandas (am currently reading Data Analysis with Python, by Mckinney, excellent). and more.. But I'm in need of references to help me leap from the conceptual level to actual implementation , including proper tool selection and problem selection and definition, that is, sort of a cook book approach.&lt;/p&gt;&#10;&#10;&lt;p&gt;This question may be too open ended,, but it expresses my dilemma. The whole space is quite open ended, and I'm looking for references to help me navigate through it.&lt;/p&gt;&#10;&#10;&lt;p&gt;What references (accessible) might you suggest?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-09-30T16:32:58.167" Id="38323" LastActivityDate="2014-04-23T23:24:32.133" LastEditDate="2012-09-30T22:03:59.097" LastEditorUserId="1963" OwnerUserId="1963" PostTypeId="1" Score="5" Tags="&lt;machine-learning&gt;&lt;references&gt;" Title="Looking for a good followup to the Stanford AI course" ViewCount="189" />
  
  
  <row Body="&lt;p&gt;The quantiles of a distribution refer to points on its &lt;a href=&quot;http://stats.stackexchange.com/questions/tagged/cdf&quot;&gt;cumulative distribution function&lt;/a&gt;. Some common quantiles are quartiles and percentiles. Quantiles are used in quantile-normal and quantile-quantile plots, which can be useful for examining the shape of distributions. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-01T00:26:13.473" Id="38343" LastActivityDate="2012-10-01T00:32:36.400" LastEditDate="2012-10-01T00:32:36.400" LastEditorUserId="686" OwnerUserId="686" PostTypeId="5" Score="0" />
  <row AnswerCount="1" Body="&lt;p&gt;$X_{1}$ and $X_{2}$ are independent, chi-square distributed random variables. I've managed to show that $Z=X_{1}+X_{2}$ is independent of $\frac{X_{1}}{Z}$. How do I conclude then that $Z=X_{1}+X_{2}$ is independent of $(\frac{X_{1}}{Z},\frac{X_{2}}{Z})$? &lt;/p&gt;&#10;" CommentCount="11" CreationDate="2012-10-01T01:15:51.330" FavoriteCount="0" Id="38347" LastActivityDate="2013-01-06T21:39:16.463" LastEditDate="2012-10-01T14:10:47.087" LastEditorUserId="919" OwnerUserId="14515" PostTypeId="1" Score="3" Tags="&lt;self-study&gt;&lt;independence&gt;" Title="Showing independence" ViewCount="112" />
  
  
  
  <row Body="&lt;p&gt;Technically speaking, the classical statistics (the one which talks about significance, as opposed to e.g. Bayesian Statistics) requires the populations to be infinite regardless it may or may not make sense in the real world. &lt;/p&gt;&#10;&#10;&lt;p&gt;In your case, (whether you take bootstrap or straightforward approach) you are in fact taking into consideration infinite number of parallel realizations of your company based upon eighter normal model or the explicit bootstrap realizations. The significance tells you, that among this population a given relationship exists (with the given type I error).&#10;It is a leap of thought, the one that is commonly done in this field. [See: e.g. Bostad W.M. - Introduction to Bayesian statistics (2007), p. 5] &lt;/p&gt;&#10;&#10;&lt;p&gt;On a side, note that any classical statistical test assumes also that the variables come from i.i.d. distributions (independent and identically distributed). This may be problem in organization where exist many interpersonal mechanisms and interactions which effectively change (may it be unify or diversify) the way people answer your question(aire). So you should be aware, that however small, your sample may still be overreplicated.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-01T11:13:43.653" Id="38377" LastActivityDate="2012-10-01T11:13:43.653" OwnerUserId="10069" ParentId="38308" PostTypeId="2" Score="0" />
  
  
  
  
  <row AcceptedAnswerId="38407" AnswerCount="3" Body="&lt;p&gt;In a recent &lt;a href=&quot;http://dx.doi.org/10.1080/17470218.2012.711335&quot;&gt;paper &lt;/a&gt;, Masicampo and Lalande (M-L) collected a large number of p-values published in many different studies. They observed a curious jump in the histogram of the p-values right at the canonical critical level of 5%.&lt;/p&gt;&#10;&#10;&lt;p&gt;There is a nice discussion about this M-L Phenomena on Prof. Wasserman's blog: &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://normaldeviate.wordpress.com/2012/08/16/p-values-gone-wild-and-multiscale-madness/&quot;&gt;http://normaldeviate.wordpress.com/2012/08/16/p-values-gone-wild-and-multiscale-madness/&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;On his blog, you will find the histogram:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/EFtVY.png&quot; alt=&quot;Histogram of published p-values&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Since the 5% level is a convention and not a law of nature, &lt;strong&gt;what causes this behavior of the empirical distribution of published p-values?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Selection bias, systematic &quot;adjustment&quot; of p-values just above the canonical critical level, or what?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-10-01T17:47:18.610" FavoriteCount="10" Id="38403" LastActivityDate="2012-10-02T07:32:54.377" LastEditDate="2012-10-02T02:13:07.433" LastEditorUserId="183" OwnerUserId="9394" PostTypeId="1" Score="23" Tags="&lt;statistical-significance&gt;&lt;p-value&gt;&lt;meta-analysis&gt;" Title="What causes the discontinuity in the distribution of published p-values at p &lt; .05?" ViewCount="1081" />
  
  <row AcceptedAnswerId="107889" AnswerCount="1" Body="&lt;p&gt;Can anyone help with the understanding of this notation (and idea) from the vignette for &lt;a href=&quot;http://cran.r-project.org/web/packages/gbm/vignettes/gbm.pdf&quot; rel=&quot;nofollow&quot;&gt;GBM&lt;/a&gt; in R?&lt;/p&gt;&#10;&#10;&lt;p&gt;It starts with the following:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/iUUr2.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Question 1&lt;/strong&gt;: I believe this is simply saying that we are looking for the function f(x) that minimizes the average value of the loss function (and that expected value is over all values of X and Y)? &lt;/p&gt;&#10;&#10;&lt;p&gt;Then this is shown:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/fJ32d.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Question 2&lt;/strong&gt;: What are these two steps?&lt;/p&gt;&#10;&#10;&lt;p&gt;Is the first of these two equivalent to the one above by a law of expectations? And is it saying that we are seeking the expected value of the loss function over y, considering x fixed? Then the outer part of the expression is averaging over all values of x?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-01T18:19:27.683" FavoriteCount="2" Id="38405" LastActivityDate="2014-07-14T14:46:46.393" LastEditDate="2012-10-02T19:31:32.833" LastEditorUserId="88" OwnerUserId="2040" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;machine-learning&gt;&lt;expected-value&gt;&lt;loss-functions&gt;" Title="Notation in GBM package vignette: expected value of loss functions" ViewCount="159" />
  
  <row Body="&lt;p&gt;(1) As already mentioned by @PeterFlom, one explanation might be related to the &quot;file drawer&quot; problem. (2) @Zen also mentioned the case where the author(s) manipulate(s) the data or the models (e.g. &lt;a href=&quot;http://en.wikipedia.org/wiki/Data_dredging&quot;&gt;data dredging&lt;/a&gt;). (3) However, we do not test hypotheses on a purely random basis. That is, hypotheses are not chosen by chance but we have (more or less strong) theoretical assumption.&lt;/p&gt;&#10;&#10;&lt;p&gt;You also might be interested in the works of Gerber and Malhotra who recently have conducted research in that area applying the so called &quot;caliper test&quot;: &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://www.stanford.edu/~neilm/qjps.pdf&quot;&gt;Do Statistical Reporting Standards Affect What Is Published? Publication Bias in Two Leading Political Science Journals &lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://smr.sagepub.com/content/37/1/3.abstract&quot;&gt;Publication Bias in Empirical Sociological Research: Do Arbitrary Significance Levels Distort Published Results?&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;You also might be interested in this special issue edited by Andreas Diekmann: &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://books.google.de/books?id=vzJlczjAz4sC&amp;amp;lpg=PA652&amp;amp;ots=cicT-FPxqg&amp;amp;dq=malhotra%20caliper%20test&amp;amp;pg=PA592#v=onepage&amp;amp;q=malhotra%20caliper%20test&amp;amp;f=false&quot;&gt;Methodological Artefacts, Data Manipulation and Fraud in Economics and Social Science&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2012-10-01T18:37:09.930" Id="38407" LastActivityDate="2012-10-01T19:08:17.137" LastEditDate="2012-10-01T19:08:17.137" LastEditorUserId="11032" OwnerUserId="307" ParentId="38403" PostTypeId="2" Score="13" />
  <row AcceptedAnswerId="38945" AnswerCount="2" Body="&lt;p&gt;I built a SVM-based classifier against a data set, the precision is about 66% and the recall is about 88%. Generally, what are the options to tune the parameter that can increase the precision?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-01T19:17:02.393" FavoriteCount="1" Id="38412" LastActivityDate="2013-09-23T17:11:58.403" OwnerUserId="3026" PostTypeId="1" Score="3" Tags="&lt;machine-learning&gt;&lt;data-mining&gt;&lt;svm&gt;&lt;text-mining&gt;&lt;libsvm&gt;" Title="The general approaches for improving a SVM-based classifier which is low precision and high recall" ViewCount="561" />
  <row AnswerCount="0" Body="&lt;p&gt;It is common in regression to see $R^2$ formulated as follows: $$R^2\equiv 1 - {SS_{\rm err}\over SS_{\rm tot}},$$ where $SS_\text{err}=\sum_i (y_i - f_i)^2$ and $SS_\text{tot}=\sum_i (y_i-\bar{y})^2$.  This exact formulation seems to make the most sense if the goal of the regression is indeed to minimize the residual sum of squares.  However, sometimes other, more exotic loss functions are more appropriate for a variety of reasons, and some predictive modeling algorithms might be tried to minimize this loss function.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to define my $R^2$-like measure in the following way: $$R^2= 1 - \frac{\sum_il(y_i, f_i)}{\sum_il(y_i, \bar{y})},$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $l(y_i, f_i)$ denotes the &quot;loss&quot; occurred in some sense if $f_i$ is predicted but the actual answer is $y_i$.  In the case of OLS regression, $l(y_i, f_i) = (y_i - f_i)^2$, but this allows for other predictive models based on least absolute deviations or even more exotic functions based upon the problem domain.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is this valid to do?  Is anything gained with this more general formulation of $R^2$?  Is it still valid to call it $R^2$?  Does this add any useful interpretations, or is it more likely to just be confusing?  Should the traditional $R^2$ be reported as well?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-10-01T21:14:13.740" FavoriteCount="1" Id="38422" LastActivityDate="2012-10-01T21:14:13.740" OwnerUserId="2485" PostTypeId="1" Score="1" Tags="&lt;predictive-models&gt;&lt;goodness-of-fit&gt;&lt;r-squared&gt;&lt;loss-functions&gt;" Title="Can I define an $R^2$-like measure in this way when predicting with exotic loss functions?" ViewCount="68" />
  <row Body="&lt;p&gt;Sure you can, e.g.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(38821010)&#10;&#10;x &amp;lt;- rnorm(100)&#10;y &amp;lt;- 5*x + 3&#10;&#10;var(y)/mean(y)&#10;var(x)/mean(x)&#10;&#10;cor(x,y)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;as an example. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-01T21:43:14.953" Id="38426" LastActivityDate="2012-10-01T21:43:14.953" OwnerUserId="686" ParentId="38424" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;I gave it some thought and I think you could use the group standard error formula from the unpaired t test and factor within it the coefficient of GDP (let's call it A) to estimate stock return: &lt;/p&gt;&#10;&#10;&lt;p&gt;group standard error = SQRT(A^2*2%^2 + 5%^2).&lt;br&gt;&#10;Here A is the coefficient of GDP to estimate stock return. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-01T21:56:25.107" Id="38428" LastActivityDate="2012-10-01T22:28:25.443" LastEditDate="2012-10-01T22:28:25.443" LastEditorUserId="1329" OwnerUserId="1329" ParentId="38423" PostTypeId="2" Score="0" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Let $X$ by the joint distribution of the random variables $A$, $B$, $C$, and $D$. Let $(A \perp B) \mid (C, D)$ and $(C \perp D) \mid (A, B)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/yrGho.png&quot; alt=&quot;Four Node Markov Network&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I understand that this distribution should factor over the four pairwise cliques.&lt;/p&gt;&#10;&#10;&lt;p&gt;$\Pr(X) = \frac{1}{Z}\phi_1(A,D)\phi_2(A,C)\phi_3(C,B)\phi_4(D,B)$&lt;/p&gt;&#10;&#10;&lt;p&gt;However, I'd like to see how, starting with $\Pr(X)$, we could factor the distribution into appropriate functions. I've been knocking my head against this most of the day, so any hints appreciated.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-10-01T22:05:04.207" FavoriteCount="3" Id="38429" LastActivityDate="2014-12-01T07:14:19.167" LastEditDate="2012-10-02T19:22:13.980" LastEditorUserId="88" OwnerUserId="82" PostTypeId="1" Score="3" Tags="&lt;probability&gt;&lt;graphical-model&gt;" Title="Factoring simple Markov network" ViewCount="163" />
  
  
  
  <row AnswerCount="1" Body="&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;strong&gt;Possible Duplicate:&lt;/strong&gt;&lt;br&gt;&#10;  &lt;a href=&quot;http://stats.stackexchange.com/questions/38118/parallel-lines-on-residual-vs-fitted-plot&quot;&gt;Parallel lines on residual vs fitted plot&lt;/a&gt;  &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&#10;&#10;&lt;p&gt;I'm regressing the time it took for an event to happen on another, normally-distributed predictor.&lt;/p&gt;&#10;&#10;&lt;p&gt;When I plot the fitted values against the residuals, the data line up in a series of parallel lines (with negative slopes).&lt;/p&gt;&#10;&#10;&lt;p&gt;The Q-Q plot is sigmoidal, but not too too bad.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any suggestions for what I should do to my data?&lt;/p&gt;&#10;&#10;&lt;p&gt;Transform the response variable? Use a generalized linear model (glm) instead? If the latter, what family of error distribution should I use?&lt;/p&gt;&#10;&#10;&lt;p&gt;Many, many thanks!&lt;/p&gt;&#10;" ClosedDate="2012-10-02T05:07:56.903" CommentCount="0" CreationDate="2012-10-02T04:02:04.667" Id="38446" LastActivityDate="2012-10-02T05:06:39.500" LastEditDate="2012-10-02T04:12:59.493" LastEditorUserId="7344" OwnerUserId="7344" PostTypeId="1" Score="0" Tags="&lt;regression&gt;" Title="What do parallel lines in a fitted values vs. residual plot mean? How should I transform my data?" ViewCount="469" />
  <row Body="&lt;p&gt;One argument that is missing so far is the flexibility of data analysis known as researchers degrees of freedom. In every analysis there are many decisions to be made, where to set the outlier criterion, how to transform the data, and ...&lt;/p&gt;&#10;&#10;&lt;p&gt;This was recently raised in an influential article by Simmons, Nelson and Simonsohn:&lt;/p&gt;&#10;&#10;&lt;p&gt;Simmons, J. P., Nelson, L. D., &amp;amp; Simonsohn, U. (2011). &lt;a href=&quot;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1850704&quot;&gt;False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant.&lt;/a&gt; &lt;em&gt;Psychological Science&lt;/em&gt;, 22(11), 1359 –1366. doi:10.1177/0956797611417632&lt;/p&gt;&#10;&#10;&lt;p&gt;(Note that this is the same Simonsohn responsible for some recently detected cases of data fraud in Social Psychology, e.g., &lt;a href=&quot;http://www.nature.com/news/the-data-detective-1.10937&quot;&gt;interview&lt;/a&gt;, &lt;a href=&quot;http://blogs.nature.com/news/2012/07/data-detective-makes-his-fraud-busting-algorithm-public.html&quot;&gt;blog-post&lt;/a&gt;)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-02T07:32:54.377" Id="38453" LastActivityDate="2012-10-02T07:32:54.377" OwnerUserId="442" ParentId="38403" PostTypeId="2" Score="9" />
  
  <row Body="&lt;p&gt;Your data can be easly modeled using a seaonal model of the form &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    Y(T) =  168.16                                                                         &#10;   +[X1(T)][(+ 28.8257)]                 :PULSE           2012/  3&#10;   +[X2(T)][(- 14.3322)]                 :PULSE           2010/ 13&#10;   +[X3(T)][(+ 15.0558)]                 :PULSE           2011/  9&#10;   +[X4(T)][(+ 13.6610)]                 :PULSE           2012/  8&#10;  +     [(1-  .945B** 13)]**-1  [A(T)]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Note that this is simply an equation which uses .945 * the value 13 periods ago and can be restated as y= 9.3 + .945*y(t-13) . Analysis suggested 4 unusual points which you might want to focus on to identify any omitted &quot;information/cause series&quot; like promotion/price actrivity.&lt;/p&gt;&#10;&#10;&lt;p&gt;A plot of the actual fit and forecasts &lt;img src=&quot;http://i.stack.imgur.com/6NV1K.jpg&quot; alt=&quot;enter image description here&quot;&gt; . In my opinion the reason that Peter's holt-winter's additive seaonal model didn't capture the seasonality is his model was deterministic in nature not adaptive. Sometimes a deterministic model is appropriate, sometimes it is not . The data will tEll you which model is appropriate.  In addition his model.procedure believed the 4 questionable data points rather than challenging them for &quot;consistency wirt expectations&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;The forecasts for the next 7 periods are&lt;/p&gt;&#10;&#10;&lt;p&gt;98.59&#10;81.24&#10;86.52&#10;85.05&#10;86.24&#10;106.32&#10;96.17&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/l0Iqi.jpg&quot; alt=&quot;enter image description here&quot;&gt; . the r-square for the model is .754 with an MSE of 56.7 . This automatic analysis was obtained using AUTOBOX a program that I have helped develop. Improved forecasting accuracy can save money. Hope this helps.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-02T11:32:56.663" Id="38464" LastActivityDate="2012-10-02T12:29:27.000" LastEditDate="2012-10-02T12:29:27.000" LastEditorUserId="3382" OwnerUserId="3382" ParentId="37753" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="38499" AnswerCount="1" Body="&lt;p&gt;I am using Random Forests in Matlab for regression. After educating my model on train data, I want to get MSE on test data not used in training. I do that two ways:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;call &lt;code&gt;predict&lt;/code&gt; and directly calculate MSE using predicted and actual values&lt;/li&gt;&#10;&lt;li&gt;call &lt;code&gt;error&lt;/code&gt; and use built in TreeBagger functionality to do the same task.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;In first case I get 10 times bigger result. Why? The only explanation I have, is that built in function somehow discounts outliers in prediction, but I am not sure how exactly it is done.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can somebody, please, explain all this to me.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-02T13:38:42.247" Id="38470" LastActivityDate="2012-10-02T19:30:10.510" LastEditDate="2012-10-02T19:23:15.010" LastEditorUserId="88" OwnerUserId="11116" PostTypeId="1" Score="2" Tags="&lt;matlab&gt;&lt;random-forest&gt;" Title="Matlab RandomForest prediction error calculation" ViewCount="675" />
  <row AnswerCount="5" Body="&lt;p&gt;I work mainly with non-statisticians in fields such as medicine, social sciences and education.&lt;/p&gt;&#10;&#10;&lt;p&gt;Whether I am consulting with graduate students, helping researchers with articles or reviewing articles for journals, I often have the problem that someone (client, author, dissertation committee, journal editor) wants to use some relatively well-known technique when it is either entirely inappropriate or when better but lesser-known methods exist. Often, I will explain the alternative technique but then be told &quot;everybody does it the other way&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'd be interested in how others deal with this sort of difficulty.  &lt;/p&gt;&#10;&#10;&lt;p&gt;ADDITIONS &lt;/p&gt;&#10;&#10;&lt;p&gt;@MichaelChernick suggested I could share some stories, so I will&lt;/p&gt;&#10;&#10;&lt;p&gt;Currently I am working with one person who is duplicating a previous paper and adding one independent variable to see if it helps. The previous paper is, frankly, terrible. It treats dependent data as if they were independent; it is tremendously overfit and there are other problems too. Yet he (my client) submitted an earlier version as a dissertation and not only got his degree but was widely praised for the research.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Many&lt;/em&gt; times I have tried to convince people not to dichotomize variables. This comes up very often in medicine. I patiently point out that dicohotomizing (say) birthweight into low and normal (usually at 2,500 g) means treating a 2,499 g baby as just like a 1,400 g one; but treating the 2,501 gram baby quite differently. The clinician agrees with me that this is silly. Then says to do it that way.&lt;/p&gt;&#10;&#10;&lt;p&gt;I had a graduate student client long ago whose committee &lt;em&gt;insisted&lt;/em&gt; on a cluster analysis. The student did not understand the method, the method did not answer useful questions, but that's what the committee wanted, so that's what they got.&lt;/p&gt;&#10;&#10;&lt;p&gt;The entire field of statistical graphics is one where, for many, &quot;this is how grandpa did it&quot; is enough.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then there are people who seem to just push buttons. I remember one presentation (not by someone I helped!) who had taken an entire questionnaire and factor analyzed it. One of the variables she included was ID number!&lt;/p&gt;&#10;&#10;&lt;p&gt;Oy. &lt;/p&gt;&#10;" CommentCount="6" CommunityOwnedDate="2012-10-02T16:35:10.420" CreationDate="2012-10-02T14:18:02.057" FavoriteCount="7" Id="38471" LastActivityDate="2012-10-03T12:49:38.900" LastEditDate="2012-10-02T21:01:34.247" LastEditorUserId="686" OwnerUserId="686" PostTypeId="1" Score="23" Tags="&lt;consulting&gt;" Title="Strategies for introducing advanced statistics to various audiences" ViewCount="719" />
  <row Body="" CommentCount="0" CreationDate="2012-10-02T14:32:01.977" Id="38472" LastActivityDate="2012-10-02T14:32:01.977" LastEditDate="2012-10-02T14:32:01.977" LastEditorUserId="-1" OwnerUserId="-1" PostTypeId="5" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;I want to analyze the determinants of credit constraints of a firm. I have information for both formal and informal credit. I have 6 categories of credit-constraint statuses of a firm for formal credit, while I only have 2 categories for informal credit. As credit constraint status is determined simultaneously, and formal and informal credit market are interrelated, I want to estimate the determinants of credit constraints for formal and informal credit market at the same time. Bivariate logit/probit model may be too simplistic. Is there extension of bivariate logit model or some kind of multivariate multinomial logit model in Stata?  Thank you very much in advance.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-10-02T15:12:16.253" Id="38476" LastActivityDate="2014-03-30T23:05:47.130" LastEditDate="2014-03-30T23:05:47.130" LastEditorUserId="26338" OwnerUserId="14562" PostTypeId="1" Score="3" Tags="&lt;stata&gt;&lt;multinomial&gt;&lt;logit&gt;" Title="Combination of multinomial logit model and logit model" ViewCount="87" />
  <row Body="&lt;p&gt;I believe you can find Matlab very useful for your needs. &lt;br&gt;Here you can find a &lt;a href=&quot;http://www.mathworks.com/company/events/webinars/wbnr33692.html&quot; rel=&quot;nofollow&quot;&gt;video tutorial&lt;/a&gt; that explains how to handle large data sets AND explains the new features for the x64 versions regarding memory allocation etc.&lt;br&gt;&#10;Also Matlab is very well suited for handling &lt;a href=&quot;http://www.mathworks.com/help/matlab/math/sparse-matrix-operations.html&quot; rel=&quot;nofollow&quot;&gt;sparse matrices&lt;/a&gt; (and any type of matrices), contains several built-in functions and user provided functions for handling centroids, and so on.&lt;br&gt;&#10;Just my 2 cents.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-02T15:15:42.983" Id="38477" LastActivityDate="2012-10-02T15:15:42.983" OwnerUserId="14525" ParentId="10122" PostTypeId="2" Score="0" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am running regression analyses in matlab with glmfit and calculate significance of contrasts of parameter estimates by using standard errors. Somehow however, the results I get are not consistent with SPSS results, so I'm wondering if I am doing something wrong.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hence 2 Questions:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Can somebody point me to a source where the equations/formulas for of t-tests and F tests for contrasts of parameter estimates is described?&lt;/li&gt;&#10;&lt;li&gt;It would also be great if somebody could have a look of my code, to see if there is maybe something obviously wrong (see below).&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;h3&gt;code&lt;/h3&gt;&#10;&#10;&lt;p&gt;Note: as this is matlab code, matrix algebra applies.&lt;/p&gt;&#10;&#10;&lt;p&gt;1) Calculate sigma square from residuals and degrees of freedom (stats.residp is a vector):&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;sigma_squared = (stats.residp'*stats.residp)/stats.dfe;&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;2) Get the variance for a contrast through multiplication with covariance of regressors (stats.covb) and contrast vector (c, which is actually a matrix with many contrasts):&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;var_c = sigma_squared*c'*stats.covb*c;&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;3) Get the standard error by taking the square root of the diagonal of the matrix of contrast-variances:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;se_c = sqrt(diag(var_c));&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;4) Calculate t-statistic (c,b,se_c are vectors; ./ makes the division an element by element division):&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;t = (c'*b)./se_c;&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;5) Get the p value by plugging the t statistic in the cumulative student t distribution with the right degrees of freedom:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;p = 2*((1-tcdf(t,stats.dfe)).*(t&amp;gt;0) + (tcdf(t,stats.dfe)).*(t&amp;lt;0));&lt;/code&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-02T16:26:34.533" Id="38483" LastActivityDate="2012-10-04T12:11:04.017" LastEditDate="2012-10-04T12:11:04.017" LastEditorUserId="14565" OwnerUserId="14565" PostTypeId="1" Score="2" Tags="&lt;multiple-regression&gt;&lt;matlab&gt;&lt;t-test&gt;&lt;standard-error&gt;&lt;contrasts&gt;" Title="Calculating standard errors for contrasts of parameter estimates of a (multiple) linear regression" ViewCount="720" />
  <row Body="&lt;p&gt;Thanks for this nice question Peter.  I work at a medical research institution and deal with physicians who do research and publish in the medical journals.  Often they are more interested in getting their paper published than &quot;doing the statistics completely right&quot;.  So when I propose an unfamilar technique they will point to a similar paper and say &quot;look they did it this way and got their results published.&quot;  &lt;/p&gt;&#10;&#10;&lt;p&gt;There is a problem I think when the published paper is really bad and has mistakes.  It is difficult to argue even though I have a great reputation. Some docs have big egos and think they can learn almost anything.  So they think they understand the statistics when they don't and can be insistent.  It can get frustrating.  When it is a t test and Wilcoxon is more appropriate I get them to do a Wilk Shapiro test and if normality is rejected we include both methods and explain why Wilcoxon is better.  I sometimes can convince them and often they depend on me for statistics, so I have a little more clout then a general consultant might have.&lt;/p&gt;&#10;&#10;&lt;p&gt;I also ran into a situation where I did Kaplan-Meier curves for them and we used the log rank test but Wilcoxon gave a different result.  It was hard for me to decide and in such situations I think it is best to present both methods and explain why they differ.  The same goes for using Peto vs Greenwood confidence intervals for the survival curve. Explaining the Cox proportion hazard assumption can be difficult and they often misinterpret odds ratios and relative risk.&lt;/p&gt;&#10;&#10;&lt;p&gt;There is no simple answer.  I had a boss here who was a top medical researcher in cardiology and he sometimes referees for journals.  He was looking at a paper that dealt with diagnosis and used AUC as a measure.  He had never seen an AUC curve before and came to me to see if I thought it was valid.  He had doubts.  It turned out to be appropriate and I explained it to him as best I could.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have tried to lecture on biostatistics to physicians and have taught biostatistics in public health schools.  i try to do it better than others have and produced a book for health science majors introductory course in 2002 with an epidemiologist as coauthor.  Wiley wants me to do a second edition now.  In 2011 I published a more concise book that I tried to cover just the essentials so that busy MDs might take the time to reasd it and reference it. That is how I deal with it.  Maybe you can share your stories with us.&lt;/p&gt;&#10;" CommentCount="2" CommunityOwnedDate="2012-10-02T16:35:10.420" CreationDate="2012-10-02T16:28:33.317" Id="38484" LastActivityDate="2012-10-02T16:28:33.317" OwnerUserId="11032" ParentId="38471" PostTypeId="2" Score="6" />
  
  
  <row AcceptedAnswerId="38512" AnswerCount="1" Body="&lt;p&gt;Suppose I'm given the mean and one quantile (e.g. the 20% quantile) of a random variable $x$, and I want to find the parameters $\alpha$ and $\beta$ of a Beta distribution that has the same mean and quantile.  Is there an efficient way to do it?  &lt;/p&gt;&#10;&#10;&lt;p&gt;Using just the mean, I know that since $\bar{x} = \frac{\alpha}{\alpha+\beta}$, we have $\beta = \frac{\alpha}{\bar{x}} - \alpha$.  So we only really have one parameter to estimate.  But I'm unsure how to use the quantile information to take the next step.  Maybe there's something I can do with the Incomplete Beta when I know the ratio $\frac{\beta}{\alpha} = \frac{1-\bar{x}}{\bar{x}}$?&lt;/p&gt;&#10;&#10;&lt;p&gt;I have access to &lt;code&gt;R&lt;/code&gt; myself, so I could use a numerical optimizer for this, but ideally I need a method that can be carried out in Excel in someone else's environment.  Excel does have &lt;code&gt;BETA.DIST()&lt;/code&gt; and &lt;code&gt;BETA.INV()&lt;/code&gt; functions available.  A look-up table would be fine, but a closed-form formula would be better if it's possible.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-02T19:38:44.773" Id="38501" LastActivityDate="2014-06-06T17:22:48.627" OwnerUserId="1434" PostTypeId="1" Score="6" Tags="&lt;r&gt;&lt;excel&gt;&lt;quantiles&gt;&lt;beta&gt;" Title="Beta distribution from mean and quantile" ViewCount="702" />
  <row Body="&lt;p&gt;You're not doing anything wrong, the two functions are making different underlying assumptions about the distribution of the data. Your first implementation is assuming multivariate normal, and the 2nd a multivariate t-distribution (see ?cov.trob in package MASS). The effect is easier to see if you pull out one group:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#pull out group 1&#10;pick = group ==1&#10;p3 &amp;lt;- qplot(data=df[pick,], x=x, y=y)&#10;tl = with(df[pick,], &#10;     ellipse(cor(x, y),scale=c(sd(x),sd(y)),&#10;             centre=c(mean(x),mean(y))))&#10;p3 &amp;lt;- p3 + geom_path(data=as.data.frame(tl), aes(x=x, y=y))&#10;p3 &amp;lt;- p3 + stat_ellipse(level=0.95)&#10;p3 # looks off center&#10;p3 &amp;lt;- p3 + geom_point(aes(x=mean(x),y=mean(y),size=2,color=&quot;red&quot;))&#10;p3&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So although it is close to the same center and orientation they are not the same. You can come close to the same size ellipse by using &lt;code&gt;cov.trob()&lt;/code&gt; to get the correlation and scale for passing to &lt;code&gt;ellipse()&lt;/code&gt;, and using the t argument to set the scaling equal to an f-distribution as &lt;code&gt;stat_ellipse()&lt;/code&gt; does.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;tcv = cov.trob(data[pick,2:3],cor=TRUE)&#10;tl = with(df[pick,], &#10;          ellipse(tcv$cor[2,1],scale=sqrt(diag(tcv$cov)),&#10;                  t=qf(0.95,2,length(x)-1),&#10;                  centre=tcv$center))&#10;p3 &amp;lt;- p3 + geom_path(data=as.data.frame(tl), aes(x=x, y=y,color=&quot;red&quot;))&#10;p3&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;but the correspondence still isn't exact. The difference must be arising between using the cholesky decomposition of the covariance matrix and creating the scaling from the correlation and the standard deviations. I'm not enough of a mathematician to see exactly where the difference is. &lt;/p&gt;&#10;&#10;&lt;p&gt;Which one is correct? That's up to you to decide! The &lt;code&gt;stat_ellipse()&lt;/code&gt; implementation will be less sensitive to outlying points, while the first will be more conservative. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-10-02T20:44:20.103" Id="38506" LastActivityDate="2012-10-02T20:44:20.103" OwnerUserId="12258" ParentId="38117" PostTypeId="2" Score="8" />
  
  <row Body="k-means is a family of cluster analysis methods in which you specify the number of clusters you expect. This is as opposed to hierarchical cluster analysis methods. " CommentCount="0" CreationDate="2012-10-02T21:55:57.097" Id="38517" LastActivityDate="2012-10-02T23:20:55.413" LastEditDate="2012-10-02T23:20:55.413" LastEditorUserId="686" OwnerUserId="686" PostTypeId="4" Score="0" />
  <row AnswerCount="1" Body="&lt;p&gt;I got completely different results from lmer() and lme()! Just look at the coefficients' std.errors. Completely different in both cases. Why is that and which model is correct?&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; mix1c = lmer(logInd ~ 0 + crit_i + Year:crit_i + (1 + Year|Taxon), data = datai)&#10;&amp;gt; mix1d = lme(logInd ~ 0 + crit_i + Year:crit_i, random = ~ 1 + Year|Taxon, data = datai)&#10;&amp;gt; &#10;&amp;gt; summary(mix1d)&#10;Linear mixed-effects model fit by REML&#10; Data: datai &#10;       AIC      BIC    logLik&#10;  4727.606 4799.598 -2351.803&#10;&#10;Random effects:&#10; Formula: ~1 + Year | Taxon&#10; Structure: General positive-definite, Log-Cholesky parametrization&#10;            StdDev       Corr  &#10;(Intercept) 9.829727e-08 (Intr)&#10;Year        3.248182e-04 0.619 &#10;Residual    4.933979e-01       &#10;&#10;Fixed effects: logInd ~ 0 + crit_i + Year:crit_i &#10;                 Value Std.Error   DF   t-value p-value&#10;crit_iA      29.053940  4.660176   99  6.234515  0.0000&#10;crit_iF       0.184840  3.188341   99  0.057974  0.9539&#10;crit_iU      12.340580  5.464541   99  2.258301  0.0261&#10;crit_iW       5.324854  5.152019   99  1.033547  0.3039&#10;crit_iA:Year -0.012272  0.002336 2881 -5.253846  0.0000&#10;crit_iF:Year  0.002237  0.001598 2881  1.399542  0.1618&#10;crit_iU:Year -0.003870  0.002739 2881 -1.412988  0.1578&#10;crit_iW:Year -0.000305  0.002582 2881 -0.118278  0.9059&#10; Correlation: &#10;             crit_A crit_F crit_U crit_W cr_A:Y cr_F:Y cr_U:Y&#10;crit_iF       0                                              &#10;crit_iU       0      0                                       &#10;crit_iW       0      0      0                                &#10;crit_iA:Year -1      0      0      0                         &#10;crit_iF:Year  0     -1      0      0      0                  &#10;crit_iU:Year  0      0     -1      0      0      0           &#10;crit_iW:Year  0      0      0     -1      0      0      0    &#10;&#10;Standardized Within-Group Residuals:&#10;        Min          Q1         Med          Q3         Max &#10;-6.98370498 -0.39653580  0.02349353  0.43356564  5.15742550 &#10;&#10;Number of Observations: 2987&#10;Number of Groups: 103 &#10;&amp;gt; summary(mix1c)&#10;Linear mixed model fit by REML &#10;Formula: logInd ~ 0 + crit_i + Year:crit_i + (1 + Year | Taxon) &#10;   Data: datai &#10;  AIC  BIC logLik deviance REMLdev&#10; 2961 3033  -1469     2893    2937&#10;Random effects:&#10; Groups   Name        Variance   Std.Dev. Corr   &#10; Taxon    (Intercept) 6.9112e+03 83.13360        &#10;          Year        1.7582e-03  0.04193 -1.000 &#10; Residual             1.2239e-01  0.34985        &#10;Number of obs: 2987, groups: Taxon, 103&#10;&#10;Fixed effects:&#10;               Estimate Std. Error t value&#10;crit_iA      29.0539403 18.0295239   1.611&#10;crit_iF       0.1848404 12.3352135   0.015&#10;crit_iU      12.3405800 21.1414908   0.584&#10;crit_iW       5.3248537 19.9323887   0.267&#10;crit_iA:Year -0.0122717  0.0090916  -1.350&#10;crit_iF:Year  0.0022365  0.0062202   0.360&#10;crit_iU:Year -0.0038701  0.0106608  -0.363&#10;crit_iW:Year -0.0003054  0.0100511  -0.030&#10;&#10;Correlation of Fixed Effects:&#10;            crit_A crit_F crit_U crit_W cr_A:Y cr_F:Y cr_U:Y&#10;crit_iF      0.000                                          &#10;crit_iU      0.000  0.000                                   &#10;crit_iW      0.000  0.000  0.000                            &#10;crit_iA:Yer -1.000  0.000  0.000  0.000                     &#10;crit_iF:Yer  0.000 -1.000  0.000  0.000  0.000              &#10;crit_iU:Yer  0.000  0.000 -1.000  0.000  0.000  0.000       &#10;crit_iW:Yer  0.000  0.000  0.000 -1.000  0.000  0.000  0.000&#10;&amp;gt; &#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="3" CreationDate="2012-10-02T23:59:12.963" FavoriteCount="3" Id="38524" LastActivityDate="2012-10-05T02:14:22.047" OwnerUserId="5509" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;mixed-model&gt;&lt;lmer&gt;&lt;lme&gt;" Title="Completely different results from lmer() and lme()" ViewCount="1269" />
  
  
  
  
  <row AcceptedAnswerId="38691" AnswerCount="1" Body="&lt;p&gt;I have a Model Y= X+e and need the density of X. The deamer package deconvolves the density for X, but if I use the simpsons rule to integrate this density, I get values which are above 1. &#10;The following example gives me a density which integral is 1.173454:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(deamer) # deconvolution&#10;library(Bolstad) # simpson rule&#10;&#10;# The Y's I have are inv-Weibull distributed and the error's are inv-normal distributed.&#10;# As the deconvolution of those would take a long time, i used runif in my example&#10;# to simplify the problem. The following uncommented lines are what I like to deconvolve:&#10;&#10;#library(actuar) # for rinvweibull&#10;#y &amp;lt;- rinvweibull(30000, shape=5.53861156, scale=488)/1000&#10;#y &amp;lt;- y[y&amp;lt;1.5]&#10;#e &amp;lt;- 1/rnorm(30000, mean=0.0023853421, sd=0.0004784688)/1000&#10;#e &amp;lt;- e[e&amp;lt;1.5]&#10;#decon &amp;lt;- deamerSE(y, error=e, from=-0.1, to=0.3)&#10;&#10;y &amp;lt;- runif(1000, min = 0.8, max=1.2)&#10;e &amp;lt;- runif(1000, min = 0.1, max=0.5)&#10;decon &amp;lt;- deamerSE(y, error=e, from=0.4, to=1)&#10;plot(decon)&#10;&#10;# following line gives me integral of density (with simpsons rule)&#10;sintegral(decon$supp, decon$f)$value&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I am not sure if this is just an estimation error or if I should consider downscaling the density so that the integrated density is 1:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# Downscaling&#10;yValsScaled &amp;lt;- decon$f/area&#10;    plot(decon$supp, yValsScaled, type=&quot;l&quot;)&#10;(areaScaled &amp;lt;- sintegral(decon$supp, yValsScaled)$value)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;What do you think?&lt;/p&gt;&#10;&#10;&lt;p&gt;btw: If you use a larger intervall with the &lt;i&gt;from&lt;/i&gt; and &lt;i&gt;to&lt;/i&gt; argments in &lt;i&gt;deamerSE&lt;/i&gt; function, the integrated density will be even greater (because of the periodicity of the density). &#10;Usually I thought that with deamerSE I would get a density which integral (from -inf to inf) is approximately 1. Therefore I thougt that integrating the density using a smaller intervall (e.g. with &lt;i&gt;from=0.4&lt;/i&gt; and &lt;i&gt;to=1&lt;/i&gt; in the deamerSE function) should give me a density which integral is less than 1. &#10;But as you can see it doesn't. So I am rather confused.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-03T10:15:00.993" Id="38547" LastActivityDate="2012-10-04T23:01:00.103" LastEditDate="2012-10-04T09:15:59.307" LastEditorUserId="13525" OwnerUserId="13525" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;pdf&gt;&lt;density&gt;&lt;integral&gt;" Title="Is output of Deamer deconvolution not a density?" ViewCount="214" />
  <row Body="&lt;p&gt;It looks like you want to make a &lt;a href=&quot;http://en.wikipedia.org/wiki/Confidence_interval&quot; rel=&quot;nofollow&quot;&gt;confidence interval&lt;/a&gt;. If that's not what you are trying to do, and you just want to report the standard deviation, than it is fine to write&lt;/p&gt;&#10;&#10;&lt;p&gt;$1.23 \pm 0.52$&lt;/p&gt;&#10;&#10;&lt;p&gt;with out the $\sigma$, as long as you make it clear that you are reporting the standard deviation and &lt;em&gt;not&lt;/em&gt; a confidence interval, since many readers will assume this notation refers to a confidence interval. I've never seen the notation &lt;/p&gt;&#10;&#10;&lt;p&gt;$1.23 \pm 0.52\sigma $&lt;/p&gt;&#10;&#10;&lt;p&gt;used for this before, but it's possible it could be normal in your field. Unlikely though.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-03T12:36:08.150" Id="38557" LastActivityDate="2012-10-03T12:36:08.150" OwnerUserId="6446" ParentId="38556" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;If you're comparing the correlation of two variables, just report the Spearman correlation coefficient. The Spearman correlation is recommended over Pearson correlation for this type of data: &lt;a href=&quot;http://stats.stackexchange.com/questions/8071/how-to-choose-between-pearson-and-spearman-correlation&quot;&gt;How to choose between Pearson and Spearman correlation?&lt;/a&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you want to know how multiple variables impact the answer to one of the binary questions, do a logit or probit model. I've never used SPSS, but I'm sure there's a menu option for it. Select the binary answer as the dependent variable in the model.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-03T12:40:26.213" Id="38558" LastActivityDate="2012-10-03T12:40:26.213" OwnerUserId="10500" ParentId="38548" PostTypeId="2" Score="2" />
  
  <row AcceptedAnswerId="38586" AnswerCount="1" Body="&lt;p&gt;If a continuous random variable $X$ has a symmetric distribution around 0, what is the conditional distribution of $X$ given $|X|$? &lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-10-03T13:33:00.827" Id="38567" LastActivityDate="2012-10-04T03:50:38.917" LastEditDate="2012-10-04T03:50:38.917" LastEditorUserId="2970" OwnerUserId="14604" PostTypeId="1" Score="4" Tags="&lt;self-study&gt;&lt;conditional-probability&gt;" Title="Conditional distribution of $X$ given $|X|$" ViewCount="210" />
  <row AcceptedAnswerId="38579" AnswerCount="1" Body="&lt;p&gt;I have created a Rscript for illustrating my question:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(epicalc)&#10;&#10;temp.df &amp;lt;- data.frame(x=1:100,y=log(1:100))&#10;set.seed(1)&#10;temp.df[,&quot;y&quot;] &amp;lt;- temp.df[,&quot;y&quot;]*sample((10000:10100)/10000,size=100,replace=TRUE)&#10;temp.df[temp.df[,&quot;x&quot;]&amp;lt;=15,&quot;x_gp&quot;] &amp;lt;- &quot;&amp;lt;=15&quot;&#10;temp.df[temp.df[,&quot;x&quot;]&amp;gt;15,&quot;x_gp&quot;] &amp;lt;- &quot;&amp;gt;15&quot;&#10;temp.df[,&quot;x_gp&quot;] &amp;lt;- factor(temp.df[,&quot;x_gp&quot;],levels=c(&quot;&amp;lt;=15&quot;,&quot;&amp;gt;15&quot;))&#10;&#10;glm.null &amp;lt;- glm(y~NULL,data=temp.df)&#10;glm.01 &amp;lt;- glm(y~x,data=temp.df)&#10;glm.02 &amp;lt;- glm(y~log(x),data=temp.df)&#10;glm.03 &amp;lt;- glm(y~x*x_gp,data=temp.df)&#10;&#10;lrtest(glm.null,glm.01)&#10;lrtest(glm.null,glm.02)&#10;lrtest(glm.null,glm.03)&#10;lrtest(glm.01,glm.02)&#10;lrtest(glm.02,glm.03)&#10;lrtest(glm.01,glm.03)&#10;&#10;(logLik(glm.02)*-2)-(logLik(glm.03)*-2)&#10;&#10;plot(temp.df[,c(&quot;x&quot;,&quot;y&quot;)],col=&quot;black&quot;,pch=15)&#10;points(temp.df[,&quot;x&quot;],predict(glm.01),col=&quot;blue&quot;,pch=20)&#10;points(temp.df[,&quot;x&quot;],predict(glm.02),col=&quot;red&quot;)&#10;points(temp.df[,&quot;x&quot;],predict(glm.03),col=&quot;green&quot;,pch=16)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The dataframe is describing a log-linear relationship. But in reality, maybe it is not log-linear, even if it is log-linear, it may not be preferred to use &lt;code&gt;log&lt;/code&gt;, as it may be more difficult to understand.&lt;/p&gt;&#10;&#10;&lt;p&gt;I thought of &quot;bending&quot; the linear regression into two &quot;segments&quot; (&lt;code&gt;glm.03&lt;/code&gt;) using the interaction term. I am aware that it is not as good as &lt;code&gt;log&lt;/code&gt; in my example but already a lot better than using a straight line (&lt;code&gt;glm.02&lt;/code&gt;) alone.&lt;/p&gt;&#10;&#10;&lt;p&gt;I wonder if my method will be too &lt;code&gt;unorthodox&lt;/code&gt; in the field of public health, so can anyone suggest other ways for describing a log-linear-like relation, without using &lt;code&gt;log&lt;/code&gt;? Thanks.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-03T15:18:14.827" Id="38577" LastActivityDate="2012-10-03T15:42:06.843" LastEditDate="2012-10-03T15:42:06.843" LastEditorUserId="88" OwnerUserId="588" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;log-linear&gt;" Title="Using interaction term for describing non-linear relationship in regression model" ViewCount="172" />
  <row Body="&lt;p&gt;To answer your question in comments.  If you were doing a bootstrap instead of a simulation the bootstrap would suggest something like option 1.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you want to accept the assumption that the xs are fixed and known which is an important assumption for applying OLS then option 1 demonstrates the performance when the assumption of known x is correct.  But suppose you are interested in seeing the sensitivity to the assumption of fixed x.  Then you want to simulate with random errors generated for both x and y.  That would not be option 1 but it would not be option 2 either.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-10-03T17:17:16.460" Id="38585" LastActivityDate="2012-10-03T17:17:16.460" OwnerUserId="11032" ParentId="38534" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="38596" AnswerCount="3" Body="&lt;p&gt;Hi I'm taking a graduate course in Statistics and we've been covering Test statistics, and other concepts. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, I am often able to apply the formulas and develop a sort-of intuition on how stuff works but I am often left with a feeling that perhaps if I backed up my study with simulated experiments I will develop better intuition into the problems at hand.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, I have been thinking of writing simple simulations to better&#10;understand some of the concepts we discuss in class. Now I could use say Java to:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Produce a random population with a normal mean and standard deviation. &lt;/li&gt;&#10;&lt;li&gt;Then take a small sample and try to try to empirically calculate Type-I and Type-II errors.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Now the questions I have are:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Is this a legitimate approach to develop intuition?&lt;/li&gt;&#10;&lt;li&gt;Is there software to do this (&lt;code&gt;SAS&lt;/code&gt;?, &lt;code&gt;R&lt;/code&gt;?) &lt;/li&gt;&#10;&lt;li&gt;is this a discipline in Statistics that deals with such programming: experimental statistics?, computational statistics? simulation?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="7" CreationDate="2012-10-03T19:18:33.210" FavoriteCount="2" Id="38593" LastActivityDate="2012-10-03T21:37:27.280" LastEditDate="2012-10-03T21:37:27.280" LastEditorUserId="88" OwnerUserId="14163" PostTypeId="1" Score="8" Tags="&lt;r&gt;&lt;hypothesis-testing&gt;&lt;sas&gt;&lt;simulation&gt;&lt;computational-statistics&gt;" Title="Using computer simulations to better understand statistical concepts at the graduate level" ViewCount="589" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm using pLSA (Probabilistic Latent Semantic Analysis). I'm trying to estimate the best value for some parameters with a process of k-fold cross-validation.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have noticed that the model fitting is strongly influenced by the initialization. In order to choose the &quot;best&quot; initialization i'm using the trivial method of multiple random initializations.&lt;/p&gt;&#10;&#10;&lt;p&gt;Does someone know if do it exist a better way?Or maybe some good practices?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-03T20:34:58.893" Id="38600" LastActivityDate="2012-10-03T20:34:58.893" OwnerUserId="10669" PostTypeId="1" Score="2" Tags="&lt;machine-learning&gt;&lt;lsa&gt;" Title="Initialization in pLSA" ViewCount="96" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I have my jags output object. In order to understand how MCMC coda chains work, I tried to see if first iteration in each MCMC chain is equal to the initial values supplied. And it is different! The initial value is not there! Is it an error?&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that I specified burnin = 0 for this purpose.&lt;/p&gt;&#10;&#10;&lt;p&gt;How I ran jags:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;inits = function () { list(&#10;    alpha = rnorm(no_crit, 0, 10000),&#10;    beta = rnorm(no_crit, 0, 10000)&#10;    ,eps_tau = 7.9&#10;    ,gamma_tau = 3.1&#10;    ,delta_tau = 213 &#10;) &#10;}&#10;&#10;params = c(&quot;alpha&quot;, &quot;beta&quot;, &quot;eps_tau&quot;, &quot;gamma_tau&quot;, &quot;delta_tau&quot;)&#10;&#10;ni &amp;lt;- 5000&#10;nt &amp;lt;- 8&#10;nb &amp;lt;- 0&#10;nc &amp;lt;- 3&#10;&#10;out &amp;lt;- R2jags::jags(win.data, inits, params, &quot;model.txt&quot;,&#10;    nc, ni, nb, nt,  &#10;    working.directory = paste(getwd(), &quot;/tmp_bugs/&quot;, sep = &quot;&quot;)&#10;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;After the jags computation finished, I dumped the first iteration from each MCMC coda chain:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; mm = as.mcmc(out)&#10;&amp;gt; mm[1, c(&quot;delta_tau&quot;, &quot;eps_tau&quot;)]&#10;&amp;gt; mm[1, c(&quot;delta_tau&quot;, &quot;eps_tau&quot;)]&#10;[[1]]&#10;   delta_tau      eps_tau &#10;4426.7716020    0.4825011 &#10;&#10;[[2]]&#10;   delta_tau      eps_tau &#10;4811.3174529    0.5240721 &#10;&#10;[[3]]&#10;   delta_tau      eps_tau &#10;4406.2672016    0.5351576 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;As you can see, the first iteration in all these chains is different from what I supplied as initial values (eps_tau = 7.9, delta_tau = 213).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-04T00:13:56.753" Id="38613" LastActivityDate="2013-08-27T23:15:11.057" OwnerUserId="5509" PostTypeId="1" Score="5" Tags="&lt;r&gt;&lt;mcmc&gt;&lt;jags&gt;&lt;winbugs&gt;&lt;coda&gt;" Title="First iteration in MCMC coda chain is different from initial values" ViewCount="268" />
  <row AnswerCount="0" Body="&lt;p&gt;I have developed three a-priori candidate models that represent factors at three separate spatial scales. A factor in one model is correlated with a factor in another model. (there are debates in the literature about which factor is most influencing the independent variable, and I  do not want to draw conclusions about one factor without giving consideration to another.)&lt;/p&gt;&#10;&#10;&lt;p&gt;But I also learned that I need to create global model that uses all factors from all models. So, is it valid for me to create a global model that has correlated variables? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-04T03:29:46.183" Id="38619" LastActivityDate="2012-10-04T09:16:24.730" LastEditDate="2012-10-04T09:16:24.730" LastEditorUserId="88" OwnerUserId="14614" PostTypeId="1" Score="1" Tags="&lt;multicollinearity&gt;&lt;model&gt;" Title="Correlated factors in a global model" ViewCount="38" />
  <row Body="&lt;p&gt;I do not know anything about NLMEFITSA but quite a lot about mixed effects modelling. The covariance matrix for beta (psi) is a measure of how the fixed effects parameters are spread over the population whereas covb is a measure of the uncertainty of the parameters.&lt;/p&gt;&#10;&#10;&lt;p&gt;Consider a one-parameter model for simplicity. If the single parameter b is assumed to be normally distributed over the population beta is its estimated mean and psi its variance (over the population). covb on the other hand is a measure of the uncertainty of the estimated value beta itself, thus an uncertatinty measure of the mean. I'm not sure how the program works, but also the estimated value of the covariance parameter psi is of course uncertain and covb might also inlude an estimate of the uncertainty of psi (making covb a 2x2 matrix in the one-parameter case). The matrix covb is usually computed by using Fisher information theory.&lt;/p&gt;&#10;&#10;&lt;p&gt;Google &quot;fisher information matrix&quot; AND covariance, or something similar, for more information about parameter uncertainty in likelihood methods.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-10-04T06:02:30.500" Id="38623" LastActivityDate="2012-10-04T06:02:30.500" OwnerUserId="12415" ParentId="38610" PostTypeId="2" Score="1" />
  <row AnswerCount="3" Body="&lt;p&gt;I am testing a new kind of medicine against an older medicine in a very limited population. All the test subjects will first recieve the older, ineffective medicine. They will only recieve the new medicine if the old medicine was ineffective.&lt;/p&gt;&#10;&#10;&lt;p&gt;My data so far: None of 4 test subjects were cured by the old medicine, so all were given the new medicine. All 4 test subjects were cured by the new medicine. There are only two outcomes: cure or no cure.&lt;/p&gt;&#10;&#10;&lt;p&gt;I guess I cannot use a Fisher's exact test for this study, as the data are paired. Which test(s) could I use for this kind of experiment?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-04T09:41:59.937" Id="38632" LastActivityDate="2012-10-04T13:43:09.960" LastEditDate="2012-10-04T13:38:51.070" LastEditorUserId="25" OwnerUserId="14638" PostTypeId="1" Score="7" Tags="&lt;model-selection&gt;&lt;biostatistics&gt;" Title="Which kind of statistical exact test should I use?" ViewCount="159" />
  
  <row Body="&lt;p&gt;Peter Flom's answer makes a good point, but strictly speaking, the situation is more complicated than that. Treatment 2 is administered conditionally on the failure of treatment 1. Suppose the probability of success with treatment 1 is $p_1$, (respectively $p_2$ for treatment 2.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Each patient who receives treatment 2 and is cured thereby contributes $(1-p_1)p_2$ to the likelihood. With 4 patients, that gives $(1-p_1)^4 p_2^4$.&lt;/p&gt;&#10;&#10;&lt;p&gt;You could then maximize the likelihood for $p_1$ and $p_2$ and compare, via a likelihood ratio test to the likelihood when $p_1=p_2$.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, in your case, since no one was cured by the old treatment, MLE's for $p_1$ and $p_2$ are 0 and 1 respectively and that test will breakdown. But there is another way:&lt;/p&gt;&#10;&#10;&lt;p&gt;Assuming $p_1=p_2$, the MLE for the NULL model is $p=0.5$. The probability of getting what you got, or worse (only there isn't any worse) is then $0.5^8=0.004$.&lt;/p&gt;&#10;&#10;&lt;p&gt;However 4 patients is pretty small ... and from a design perspective, I would like to see a randomized trial. Or at least a cross-over. I don't know what the illness is, but if it were the common cold, that's exactly the result you would get if you applied two bogus treatments. The first would fail, and the patient would get better in time.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-10-04T13:05:25.123" Id="38649" LastActivityDate="2012-10-04T13:05:25.123" OwnerUserId="14188" ParentId="38632" PostTypeId="2" Score="12" />
  
  <row Body="&lt;p&gt;Your proposed general approach - using latent partitions to assign different data points to different base classifiers - is a well-researched approach toward classification. &lt;/p&gt;&#10;&#10;&lt;p&gt;The reason these methods are not widely used is likely because they are relatively complicated and have longer running times than logistic regression or SVMs. In many cases, it seems that they can lead to better classification performance.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here are some references:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;Shahbaba, B. and Neal, R. &quot;Nonlinear models using Dirichlet process&#10;mixtures&quot;&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Zhu, J. and Chen, N. and Xing, E.P. &quot;Infinite Latent SVM for&#10;Classification and Multi-task Learning&quot; &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Rasmussen, C.E. and Ghahramani, Z. &quot;Infinite mixtures of Gaussian&#10;process experts&quot;&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Meeds, E. and Osindero, S. &quot;An alternative infinite mixture of&#10;Gaussian process experts&quot;&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2012-10-04T14:38:33.060" Id="38661" LastActivityDate="2012-10-04T14:38:33.060" OwnerUserId="9595" ParentId="31440" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;To determine causation you need to perform a randomization test. You take your test subjects, and randomly choose half of them to have quality A and half to not have it. You then see if there is a statistically significant difference in quality B between the two groups.&lt;/p&gt;&#10;&#10;&lt;p&gt;It is important that you do the randomization &lt;em&gt;before&lt;/em&gt; you do any measurement. In particular, if you are given a data set with $A$ and $B$ already measured, then it is impossible to determine causation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that it may be impossible to do the randomization test that you want to do. For example, how could you test if being tall causes you to weigh more? Certainly there is a correlation between height and weight, but you can't randomly assign one group of people to a 'tall' group and one to a 'short' group. In this case, the randomization test can't be done.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-06-26T10:07:45.107" Id="38673" LastActivityDate="2012-06-26T10:07:45.107" OwnerDisplayName="Chris Taylor" OwnerUserId="2425" ParentId="38672" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Since originally posing this question, I have come to the conclusion that mixed-effect models with subjects as the random blocking factor are the practical solution to this problem, i.e. option #2 in my original post. If the &lt;code&gt;random&lt;/code&gt; argument to &lt;code&gt;lme&lt;/code&gt; is set to &lt;code&gt;~1|ID&lt;/code&gt; (where &lt;code&gt;ID&lt;/code&gt; identifies observations coming from the same test subject) then a random intercept model is fitted. If it is set to &lt;code&gt;~TIME|ID&lt;/code&gt; then a random slope and intercept model is fitted. Any right-sided formula containing variables that vary within the same individual can be placed between the &lt;code&gt;~&lt;/code&gt; and the &lt;code&gt;|ID&lt;/code&gt;, but overly complicated formulas will result in a saturated model and/or various numerical errors. Therefore, one can use a likelihood ratio test (&lt;code&gt;anova(myModel, update(myModel,random=~TIME|ID))&lt;/code&gt;) to compare a random intercept model to a random slope and intercept model or other candidate random effect models. If the difference in fit is not significant then stick with the simpler model. It was overkill for me to go into random trig functions in my original post.&lt;/p&gt;&#10;&#10;&lt;p&gt;The other issue I raised was one of model selection. It seems like people don't like model selection of any kind, but nobody has any practical alternatives. If you blindly believe the researcher who collected the data about what explanatory variables are and are not relevant, you will often be blindly accepting their untested assumptions. If you take into account every possible bit of information, you will often end up with a saturated model. If you arbitrarily choose a particular model and variables because they're easy, you will again be accepting untested assumptions, this time your own.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, in summary, for repeated measures it's &lt;code&gt;lme&lt;/code&gt; models followed by trimming via &lt;code&gt;MASS:::stepAIC&lt;/code&gt; or &lt;code&gt;MuMIn:::dredge&lt;/code&gt; and/or &lt;code&gt;nlme:::anova.lme&lt;/code&gt; until and unless someone has a better idea.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'll leave this self-answer up for a while before accepting it to see if anybody has any rebuttals. Thanks for your time, and if you're reading this because you have the same sort of question I have, good luck and welcome to semi-uncharted territory.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-04T19:37:35.703" Id="38681" LastActivityDate="2012-10-04T19:37:35.703" OwnerUserId="4829" ParentId="11413" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;why not simply use the correlation coefficient between the predicted click probability and the click event (0 or 1)? Higher the correlation, better the algorithm.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-10-05T03:48:56.337" Id="38706" LastActivityDate="2012-10-05T03:48:56.337" OwnerUserId="14663" ParentId="28852" PostTypeId="2" Score="1" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Let ${(x_1,x_2,z_1,z_2)}$ be real-valued vectors of equal length with &lt;/p&gt;&#10;&#10;&lt;p&gt;$${\hat{r}_p(x_1,x_2) \approx \hat{r}_p(z_1,z_2) &amp;gt; 0}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$${\hat{r}_p(x_1,z_1) \approx \hat{r}_p(x_2,z_2) &amp;gt; 0}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\hat{r}_p$ denotes the estimated Pearson correlation coefficient.&#10;Now perform two simple linear regressions&lt;/p&gt;&#10;&#10;&lt;p&gt;$$x_1 = \hat\beta_{0,1}+\hat\beta_{1,1}z_{1}+\hat{e}_1$$&#10;$$x_2 = \hat\beta_{0,2}+\hat\beta_{1,2}z_{2}+\hat{e}_2$$&lt;/p&gt;&#10;&#10;&lt;p&gt;(The coefficients have two subscripts to make clear that these two regression equations do not share their coefficients.)&lt;/p&gt;&#10;&#10;&lt;p&gt;The purpose of the regressions is to extract the residuals $\hat{e}_i$ as versions of the $x_i$ adjusted for/uncorrelated with the corresponding $z_i$.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is whether general statements can be made about the magnitude of ${\hat{r}_p(\hat{e}_1,\hat{e}_2)}$ in relation to that of ${\hat{r}_p(x_1,x_2)}$, that is, is the estimated correlation between the $x_i$ expected to grow or shrink by &quot;adjusting for&quot; (removing linear correlation with) the $z_i$?&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2012-10-05T07:04:22.180" Id="38712" LastActivityDate="2012-10-08T16:45:54.107" LastEditDate="2012-10-08T16:45:54.107" LastEditorUserId="10064" OwnerUserId="10064" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;correlation&gt;" Title="Magnitude of Pearson correlation after adjustment by simple linear regression" ViewCount="162" />
  <row Body="&lt;p&gt;&lt;em&gt;&lt;strong&gt;Is there any way of doing this without expanding out $E[(X-E[X])(aA+......)]$?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Yes. There is a property of covariance called &lt;em&gt;bilinearity&lt;/em&gt; which is that the covariance of a linear combination &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ {\rm cov}(aX + bY, cW + dZ) $$&lt;/p&gt;&#10;&#10;&lt;p&gt;(where $a,b,c,d$ are constants and $X,Y,W,Z$ are random variables) can be decomposed as &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ &#10;ac\cdot {\rm cov}(X,W) + &#10;ad\cdot {\rm cov}(X,Z) + &#10;bc\cdot {\rm cov}(Y,W) + &#10;bd\cdot {\rm cov}(Y,Z) $$&lt;/p&gt;&#10;&#10;&lt;p&gt;In the example you've given, you can use this property to write $\textrm{cov}(X,aA + bB + cC + dD)$ as &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ a\ {\rm cov}(X, A) +b\ {\rm cov}(X, B) +c\ {\rm cov}(X, C) +d\ {\rm cov}(X, D) $$&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-10-05T12:08:50.617" Id="38722" LastActivityDate="2012-10-05T12:49:57.067" LastEditDate="2012-10-05T12:49:57.067" LastEditorUserId="919" OwnerUserId="4856" ParentId="38721" PostTypeId="2" Score="7" />
  <row AnswerCount="1" Body="&lt;p&gt;If $x$ and $y$ are two series, is there any relation between $\sum{(x,y)}$ that can be expressed in terms of mean of these two. Specifically, I want to know if any sort of relation exists between $\sum(x,y)$ and $\sum{(\bar{x}, \bar{y})}$.&lt;/p&gt;&#10;" ClosedDate="2012-10-07T18:52:42.427" CommentCount="4" CreationDate="2012-10-05T12:59:07.700" Id="38724" LastActivityDate="2012-10-05T14:57:31.247" LastEditDate="2012-10-05T13:13:07.177" LastEditorUserId="9249" OwnerUserId="14677" PostTypeId="1" Score="0" Tags="&lt;mean&gt;&lt;summations&gt;" Title="Sum(XY) in terms of Xbar and Ybar" ViewCount="1092" />
  
  
  <row AcceptedAnswerId="38766" AnswerCount="2" Body="&lt;p&gt;In terms of &lt;code&gt;input -&amp;gt; [process] -&amp;gt; output&lt;/code&gt; what are features that distinguish clustering, blind signal separation and dimensionality reduction?&lt;/p&gt;&#10;&#10;&lt;p&gt;From &lt;a href=&quot;http://en.wikipedia.org/wiki/Unsupervised_learning&quot;&gt;this Wikipedia article&lt;/a&gt;, the implication is that there are two types of unsupervised learning:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;clustering; and &lt;/li&gt;&#10;&lt;li&gt;blind signal separation&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I have never heard of the term &lt;em&gt;blind signal separation&lt;/em&gt; before. How does it differ from clustering and how does dimensionality reduction underpin this?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-05T18:00:18.913" FavoriteCount="1" Id="38749" LastActivityDate="2012-10-07T09:59:57.480" LastEditDate="2012-10-06T09:25:51.580" LastEditorUserId="14640" OwnerUserId="13402" PostTypeId="1" Score="4" Tags="&lt;clustering&gt;&lt;pca&gt;&lt;dimensionality-reduction&gt;&lt;unsupervised-learning&gt;" Title="What are features that distinguish clustering, blind signal separation and dimensionality reduction?" ViewCount="233" />
  
  <row Body="&lt;p&gt;&lt;a href=&quot;http://www.stat.cmu.edu/~larry/&quot; rel=&quot;nofollow&quot;&gt;Larry A. Wasserman&lt;/a&gt;'s &lt;a href=&quot;http://normaldeviate.wordpress.com/&quot; rel=&quot;nofollow&quot;&gt;Normal Deviate&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2012-10-06T09:36:10.793" CreationDate="2012-10-06T09:36:10.793" Id="38768" LastActivityDate="2012-10-06T09:36:10.793" OwnerUserId="4864" ParentId="29386" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;What removal of outliers will do to a regression line depends, of course, on where the outliers are. So, rather than use &quot;normally falls&quot; I would be more specific. And it might be clearer if you talk about addition of outliers than removal of them. But I think you are basically correct. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-06T12:47:32.347" Id="38778" LastActivityDate="2012-10-06T12:47:32.347" OwnerUserId="686" ParentId="38765" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I'm not sure a simple gaussian linear model is relevant because of the nature of the depednant variable. It seems to me that a test score is an &quot;ordinal&quot; variable (i.e. a discrete variable with ordered categories). I would therefore look into methods for ordinal regression instead of simple linear regression like anova. Such methods exist in all standard software.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2012-10-06T17:13:43.210" Id="38790" LastActivityDate="2012-10-06T17:13:43.210" OwnerUserId="14661" ParentId="38787" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;Silhouette statistic is computed for every object from the set of objects being clustered (what is objects in your case - probes?). Sole objects (objects remained unclustered) in the solution receive silhouette value 0. This of course affects the average silhouette value. You might want to consider quality of clustering only among those objects that &lt;em&gt;were&lt;/em&gt; clustered. So, set silhouette value for sole objects to missing value rather than 0 before averaging. This trick implies that sole objects are treated as noise points only and not as clusters on their own. Please be aware I'm not R user and therefore can't comment on clara function.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-10-07T09:03:25.193" Id="38821" LastActivityDate="2012-10-07T09:03:25.193" OwnerUserId="3277" ParentId="38810" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;My Nonparametrics: Statistical Methods based on Ranks (Lehmann, 1975 edition) gives the formula as:&lt;/p&gt;&#10;&#10;&lt;p&gt;$p(r=2k) = 2\frac{{n-1\choose{k-1}}^2}{2n\choose{n}}$&lt;/p&gt;&#10;&#10;&lt;p&gt;where the number of runs is counted as both the runs up and down.  Asymptotically, &lt;/p&gt;&#10;&#10;&lt;p&gt;$ \frac{r - n}{\sqrt{n}} \sim \text{N}(0,1)$&lt;/p&gt;&#10;&#10;&lt;p&gt;An alternative procedure is of course to do a permutation test; generate 10,000 or so random sequences by randomly permuting the collection of data points, and calculate the test statistic for each sequence.  You can then compare your calculated test statistic on the original sequence with the collection of randomly-generated test values to get a p-value.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that this does not account for the possibility of ties, which is a definite possibility in the OP's case, as each byte can only take on one of 256 possible values.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-10-07T14:52:17.760" Id="38833" LastActivityDate="2012-10-07T22:24:00.563" LastEditDate="2012-10-07T22:24:00.563" LastEditorUserId="7555" OwnerUserId="7555" ParentId="38829" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have conducted an experiment with one group of subjects ($n=14$). I have two within-subjects factors and one dependent variable (called Error). The factor &quot;Type&quot; has 2 levels (Baseline, Operant); the factor &quot;Event&quot; has two levels (Action, Tone). The hypothesis is that there is a significant interaction Type*Event, specifically because:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;$Error_{(Baseline-Action)}-Error_{(Operant-Action)}\leq 0$&lt;/li&gt;&#10;&lt;li&gt;$Error_{(Baseline-Tone)}-Error_{(Operant_Tone)}\geq0$&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;So, I first do a two-way repeated measures ANOVA. In R:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;aov1 &amp;lt;- aov(Error~Type*Event+Error(subj/(Type*Event)), data=my_data)&#10;summary(aov)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and the interaction is significant. Now I have to do a multiple comparison. I think it is possible to use both a pairwise t test:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# 'inter' is a column created by interaction(my_data$event, my_data$type)&#10;pairwise.t.test(my_data$Error, my_data$inter, paired=T, p.adj=&quot;fdr&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and a Tukey test:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;model &amp;lt;- lme(value~inter, random=~1|subj/(inter), data=my_data)&#10;comp &amp;lt;- glht(model, linfct=mcp(te=&quot;Tukey&quot;),type=&quot;none&quot;)&#10;summary(comp)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The pairwise confirms my hypothesis 1 and 2, but the Tukey doesn't (it return a significant p-value only for the hypothesis 2). However, I don't know if I'm mistaken something with the commands for the Tukey.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;My questions are:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;strong&gt;is this modus operandi correct in my case?&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;strong&gt;on what basis I can choose which among pairwise t test and Tukey's test is the most appropriate? Are there other alternatives?&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT.&lt;/strong&gt; What is the correct procedure for computing effect size from the output of glht (which give z-scores) and from a pairwise comparison t test (which doens't return the t values)?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-10-07T18:16:10.397" Id="38844" LastActivityDate="2014-05-10T19:36:24.860" LastEditDate="2014-05-10T19:36:24.860" LastEditorUserId="26338" OwnerUserId="12540" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;anova&gt;&lt;post-hoc&gt;" Title="pairwise t test or Tukey's test?" ViewCount="1492" />
  <row Body="&lt;p&gt;The model is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac{1}{\sqrt{Y}} = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p + \varepsilon$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $Y$ is the original outcome, the $X_i$ are the explanatory variables, the $\beta_i$ are the coefficients, and $\varepsilon$ are iid, mean-zero error terms.  Writing $b_i$ for the estimated value of $\beta_i$, we see that a one-unit change in $X_i$ adds $b_i$ to the right hand side.  Starting from any baseline set of values $(x_1, \ldots, x_p)$, this induces a change in predicted values from $\widehat{1/\sqrt{y}} = b_0 + b_1 x_1 + \cdots + b_p x_p$ to $\widehat{1/\sqrt{y'}} = b_0 + b_1 x_1 + \cdots + b_p x_p + b_i$.  Subtracting the first equation from the second gives&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac{1}{\sqrt{\hat{y'}}} - \frac{1}{\sqrt{\hat{y}}} = b_i.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Solving for $\hat{y'}$ gives&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\hat{y'} = \frac{\hat{y}}{1 + 2b_i\sqrt{\hat{y}} + b_i^2 \hat{y}}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;One may stop here, but often we seek simpler expressions: the behavior of this one might not be any easier to understand than the original model.  Simplification can be achieved provided  $b_i$ is very small.  If necessary, we can contemplate a tiny change in $X_i$, say by an amount $\delta$, which would replace $b_i$ in the preceding equation by $\delta b_i$.  Using a sufficiently small value of $\delta$ will assure the denominator is close to $1$.  When it is,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac{\hat{y}}{1 + 2\delta b_i\sqrt{\hat{y}} + \delta^2 b_i^2 \hat{y}} \approx \hat{y}(1 - 2\delta b_i\sqrt{\hat{y}} - \delta^2 b_i^2 \hat{y}),$$&lt;/p&gt;&#10;&#10;&lt;p&gt;whence the change in predicted values is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\hat{y'} - \hat{y} \approx -\delta (2b_i\sqrt{\hat{y}} + \delta b_i^2 \hat{y}).$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Taking $\delta$ to be so small that $\delta b_i^2 \hat{y} \ll 2 b_i\sqrt{\hat{y}}$ allows us to drop the second term in the right hand side.  That is, &lt;strong&gt;for very tiny changes, the predicted outcome changes by $-(2b_i\sqrt{\hat{y}})$ times the amount of change in $x_i$.&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;h3&gt;Comments&lt;/h3&gt;&#10;&#10;&lt;p&gt;The appearance of the negative sign indicates that an &lt;em&gt;increase&lt;/em&gt; in $X_i$ will &lt;em&gt;decrease&lt;/em&gt; $Y$ when $b_i$ is positive and &lt;em&gt;increase&lt;/em&gt; $Y$ when $b_i$ is negative.  Normally, we avoid this (potentially confusing) sign reversal by using $-1/\sqrt{Y}$ instead of $1/\sqrt{Y}$ when making a reciprocal square root transformation (or any other transformation that reverses the order of numbers).&lt;/p&gt;&#10;&#10;&lt;p&gt;This solution method is always applicable no matter how $Y$ is re-expressed, but it can lead to complicated algebra for other transformations of $Y$.  Those who know the basics of differential calculus will recognize that all we're doing here is approximating the change in $\hat{y}$ to first order using its derivative with respect to $x_i$, so they will be able to avoid most of the algebraic manipulations.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-07T18:45:47.333" Id="38846" LastActivityDate="2012-10-07T18:45:47.333" OwnerUserId="919" ParentId="38750" PostTypeId="2" Score="2" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am learning sentiment analysis to apply it to twitter real time data to predict user's mood. I ponder about using which alternative way to do that data mining job.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Use all words to process and create a model by SVM or such classifier&lt;/li&gt;&#10;&lt;li&gt;Get vast lexicon list that is obtained form a outsource and use it as a vocabulary and just use these lexicons as a features on classifier.&lt;/li&gt;&#10;&lt;li&gt;Just use lexicons and instead of using a model emerged by a classifier, take the real time value and look at frequencies of lexicons at the tweets and rank. (this is not the way for me but I want to see the public wisdom about)&lt;/li&gt;&#10;&lt;li&gt;Or use any other method.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;As a sub question, If predicting mood will be complicated for me I will just take care of the binary classification as positive or negative feelings. Which method will work best in that case?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-07T23:20:54.003" FavoriteCount="1" Id="38873" LastActivityDate="2012-10-11T11:08:29.643" LastEditDate="2012-10-11T11:08:29.643" LastEditorUserId="8413" OwnerUserId="14289" PostTypeId="1" Score="1" Tags="&lt;data-mining&gt;&lt;natural-language&gt;&lt;information-retrieval&gt;&lt;sentiment-analysis&gt;" Title="Using sentiment lexicons or all words processing for sentiment analysis?" ViewCount="123" />
  <row Body="&quot;post-hoc&quot; refers to analyses that are decided upon after the data has been collected, as opposed to &quot;a priori&quot;. " CommentCount="0" CreationDate="2012-10-07T23:55:47.780" Id="38878" LastActivityDate="2012-10-07T23:56:09.427" LastEditDate="2012-10-07T23:56:09.427" LastEditorUserId="686" OwnerUserId="686" PostTypeId="4" Score="0" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;&lt;strong&gt;Question 1:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose that data is modelled by a mixture of K probability distributions which are actually Gaussians. $P(x_i|\theta_j)$ is the probability density of the j'th cluster, for which the parameters that we want to learn are $\theta_j = \{\mu_j, \sigma_j\}$. The probability that data-point $x_i$ belongs to cluster $c_j$ (i.e. to the j'th mixture) is $P(x_i \in c_j | x_i, \theta_j) = \frac{P(x_i|\theta_j)}{\sum_l P(x_i|\theta_l)}$. Thus the algorithm consists of take some initial values for the parameters of each mixture $\theta_j$ (how do we choose this initial parameter values ?). Then we repeat the flowing two steps many times:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Compute $P(x_i \in c_j | x_i, \theta_j)$ for each data-point i =&#10;0,..,N, in a mixture j = 1,..,K. Thus we will get a matrix M of N x&#10;K.&lt;/li&gt;&#10;&lt;li&gt;Optimize the parameters $\theta_j$ of each cluster j. (how do we&#10;optimize/update them ? does this mean just trying with other&#10;parameter values ?)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I'm confused on how to choose the parameters of each cluster and how to optimize/update them (the second step).&lt;/p&gt;&#10;&#10;&lt;p&gt;So if I understand, we should repeat the following code algorithm until convergence:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;X = {x1, ..., xn} set of data-points&#10;M = {m1, ..., mk} set of k means choosen randomly from X&#10;S = {s1, ..., sk} set of k covariance matrix, each si is choosen to be the identity matrix&#10;&#10;for i from 1 to n&#10;{&#10;    for j from 1 to k&#10;    {&#10;        compute Pij = pdf(xi | mj, sj) / SUM_l pfd(xi | ml, sl)&#10;    }&#10;}&#10;&#10;for i from 1 to n&#10;{&#10;    for j from 1 to k&#10;    {&#10;        mj = mj + (Pij * xi) / SUM_l Plj // update the mean&#10;    }&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Question 2:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there an incremental version of a Gaussian mixture model which is incremental (the data-points are processed sequentially)? That is, without &quot;repeating&quot; the 2 steps of the EM algorithm &quot;until convergence&quot;.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-10-08T07:58:20.830" Id="38894" LastActivityDate="2015-03-02T21:37:31.513" LastEditDate="2012-11-09T14:12:55.447" LastEditorUserId="8114" OwnerUserId="8114" PostTypeId="1" Score="1" Tags="&lt;bayesian&gt;&lt;clustering&gt;&lt;mixed-model&gt;&lt;normal-distribution&gt;&lt;online&gt;" Title="An incremental Gaussian mixture model" ViewCount="332" />
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Am looking for pointers to publicly(online) available high-dimensional regression datasets for evaluating my research work. &lt;/p&gt;&#10;&#10;&lt;p&gt;By high-dimensional, am looking for regression datsets with the number of predictor variables (features) ranging anywhere from 50 to 2000.&lt;/p&gt;&#10;&#10;&lt;p&gt;Most of the regression datasets on statLib(CMU) and UCI-ML repository have datasets with the number of predictor variables $&amp;lt;$ 50.&lt;/p&gt;&#10;" ClosedDate="2012-10-08T15:37:37.057" CommentCount="2" CreationDate="2012-10-08T15:09:21.573" FavoriteCount="1" Id="38928" LastActivityDate="2012-10-08T15:14:47.730" LastEditDate="2012-10-08T15:14:47.730" LastEditorUserId="6897" OwnerUserId="6897" PostTypeId="1" Score="5" Tags="&lt;regression&gt;&lt;machine-learning&gt;&lt;dataset&gt;&lt;predictive-models&gt;&lt;cross-validation&gt;" Title="High-dimensional Regression Datasets" ViewCount="618" />
  <row Body="&lt;p&gt;Recapture rate and survival rate are the two components of return rate. The return rate observed during a mark–recapture study is a product of the probability of survival (survival rate) and the probability of recapturing an animal that is alive (recapture&#10;rate). Recapture rate is the number of marked individuals recaptured divided by the total number of individuals captured in the second visit.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-10-08T16:26:22.333" Id="38930" LastActivityDate="2012-10-08T17:44:16.680" LastEditDate="2012-10-08T17:44:16.680" LastEditorUserId="12603" OwnerUserId="12603" ParentId="38904" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;I've used &lt;a href=&quot;http://svmlight.joachims.org&quot; rel=&quot;nofollow&quot;&gt;SVMLight&lt;/a&gt; for a similar text classification problem!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-08T19:48:45.093" Id="38950" LastActivityDate="2012-10-08T19:48:45.093" OwnerUserId="8580" ParentId="15181" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I use Information Gain (also known as Mutual Information). My advisor and I regularly use the approach described in this paper &lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S1067502707002630&quot; rel=&quot;nofollow&quot;&gt;Cohen, 2008&lt;/a&gt; for analyzing features for classification by SVM.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-08T20:12:13.637" Id="38954" LastActivityDate="2012-10-08T20:12:13.637" OwnerUserId="8580" ParentId="38831" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;I'd recommend &lt;a href=&quot;http://en.wikipedia.org/wiki/Collaborative_filtering&quot; rel=&quot;nofollow&quot;&gt;collaborative filtering&lt;/a&gt; for this! It's used all the time for similar problems.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-10-08T22:25:10.520" Id="38956" LastActivityDate="2012-10-08T22:25:10.520" OwnerUserId="8580" ParentId="38893" PostTypeId="2" Score="0" />
  
  
  
  <row AcceptedAnswerId="95786" AnswerCount="2" Body="&lt;p&gt;I need to find an appropriate statistical test (likelihood ratio test, t-test, etc.) on the following: Let $\{X_i;Y_i\}^n_{i=1}$ be an i.i.d. sample of a random vector $(X;Y)$ and assume that $\bigl( \begin{smallmatrix} &#10;  Y\\&#10;  X&#10;\end{smallmatrix} \bigr)$~$N$ $\left[\bigl( \begin{smallmatrix} &#10;  \mu_1\\&#10;  \mu_2&#10;\end{smallmatrix} \bigr),&#10;\bigl( \begin{smallmatrix} &#10;  1 &amp;amp; .5\\&#10;  .5 &amp;amp; 1 &#10;\end{smallmatrix} \bigr) \right]$. The hypotheses are:&#10;$H_0=\mu_1+\mu_2\le 1$; &#10;$H_1=\mu_1+\mu_2\gt 1$&lt;/p&gt;&#10;&#10;&lt;p&gt;By looking at this information, how do I know which test is the most appropriate? Is it because the data is i.i.d. I can simply take a likelihood ratio test? A good explanation on what test is more appropriate than another one would be greatly appreciated. This would definitely clear my mind.  &lt;/p&gt;&#10;" CommentCount="11" CreationDate="2012-10-09T04:34:52.277" FavoriteCount="3" Id="38979" LastActivityDate="2014-04-30T09:12:24.117" LastEditDate="2012-10-10T17:12:47.963" LastEditorUserId="919" OwnerUserId="12737" PostTypeId="1" Score="10" Tags="&lt;hypothesis-testing&gt;&lt;self-study&gt;" Title="Statistical test suggestion" ViewCount="335" />
  
  
  
  
  <row Body="&lt;p&gt;This is a fairly large topic in social psychology and questionnaire design.  Here are some ideas:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;The person could be faking it, either good or bad. People do this in order to appear &quot;good&quot; to the person doing the study. There are scales to detect this sort of faking, such as the Crowne=Marlowe scale.  These essentially ask questions to which virtually no one could answer &quot;yes&quot; (e.g. &quot;I have never told a lie in my life&quot;).&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Often, people designing questionnaires will ask the same question in different ways. One well-known issue is that people will give different age answers if you ask &quot;How old are you?&quot; and &quot;What is your birth date?&quot;  The latter have been found to be more accurate.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Another type of pattern is to answer all the questions with one answer on multiple choice questionnaires.  One way to detect this is to have some questions that are reverse coded. Then someone who answers (say) &quot;nearly all the time&quot; to both &quot;I am happy&quot; and &quot;I am sad&quot; may be suspect.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;You can also look at correlations among the questions and then identify people who have very different patterns.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Of course, none of these are fool-proof. But they are ways to investigate the issue.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-10-09T14:01:07.890" Id="39009" LastActivityDate="2012-10-09T14:01:07.890" OwnerUserId="686" ParentId="39006" PostTypeId="2" Score="9" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I have data on the status of a treatment in approx 800 hospitals. For example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Hosp    N   Given   C.I Refuse  Age.Group&#10;1       5      3     0    0      A&#10;1       11     5     1    1      B&#10;1       21     15    2    1      C&#10;1       32     24    1    1      D&#10;1       111    70    3    1      E&#10;2       10     7     1    0      A  &#10;2       25     12    2    0      B&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;etc. Please note these data are made up.&lt;/p&gt;&#10;&#10;&lt;p&gt;Each row represents one of 5 age group within a hospital. &lt;code&gt;N&lt;/code&gt; is the number of patients with a particular condition. &lt;code&gt;Given&lt;/code&gt; is the number of patients, out of &lt;code&gt;N&lt;/code&gt;, receiving the treatment, &lt;code&gt;C.I.&lt;/code&gt; is the number of patients, out of &lt;code&gt;N&lt;/code&gt;, who were contraindicated for the treatment, and &lt;code&gt;Refuse&lt;/code&gt; is the number of patients, out of &lt;code&gt;N&lt;/code&gt;, who refused the treatment&lt;/p&gt;&#10;&#10;&lt;p&gt;I need to test whether these proportions are different between the age groups&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;Given / N&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;C.I / N&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;Refuse / N&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Is it appropriate to do a binomial regression for each ratio, with &lt;code&gt;Age.Group&lt;/code&gt; as the explanatory variable ? &lt;/p&gt;&#10;&#10;&lt;p&gt;What alternative ways are there to do this ?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-09T15:26:51.243" Id="39017" LastActivityDate="2013-01-09T20:57:04.010" LastEditDate="2012-10-11T10:28:05.023" LastEditorUserId="7486" OwnerUserId="7486" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;hypothesis-testing&gt;" Title="Testing if a proportion is different between groups" ViewCount="560" />
  <row AnswerCount="1" Body="&lt;p&gt;In supervised learning, there are some ensemble methods that overcome others significantly (adaboost or random forests to mention some).&lt;/p&gt;&#10;&#10;&lt;p&gt;Few years later, also &lt;a href=&quot;http://en.wikipedia.org/wiki/Consensus_clustering&quot; rel=&quot;nofollow&quot;&gt;ensembles&lt;/a&gt; in unsupervised learning were proposed.&#10;Are there any techniques that outperform the others?&#10;What are your experiences with them w.r.t. their accuracy, easy manipulation and time complexity?&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2012-10-09T15:35:06.513" FavoriteCount="1" Id="39018" LastActivityDate="2012-10-09T17:09:20.077" OwnerUserId="14730" PostTypeId="1" Score="2" Tags="&lt;clustering&gt;&lt;unsupervised-learning&gt;&lt;ensemble&gt;&lt;method-comparison&gt;" Title="Which are the most effective clustering ensembles?" ViewCount="427" />
  <row Body="&lt;p&gt;For binary data, k-means is &lt;em&gt;really&lt;/em&gt; bad.&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem is that it computes &lt;em&gt;means&lt;/em&gt;, which no longer are binary, but more central to the data set than any of your observations. But how good is a cluster assignment when the cluster means are more similar to each other than the actual observations to each other or even to the &quot;centers&quot;? In your cluster centers, all species will then be &quot;a little bit present&quot;...&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that k-means is only well defined for Euclidean distance. Don't use it with other distance functions. If they are not compatible, the algorithm may stop converging in the worst case. K-means is only proven to converge when the mean minimizes the variance, too.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are quite a lot of distance-based clustering algorithms around (hierarchical clustering, DBSCAN, OPTICS, ...) that &lt;em&gt;can&lt;/em&gt; work with arbitrary distance functions. So you should be able to use these with whatever distance function you have found to be useful. And there are quite a lot of set based distance functions. I believe Jaccard similarity was even created when analyzing the presence and absence of species. So why not find a distance based clustering algorithm to use with Jaccard similarity?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-10-09T17:00:26.920" Id="39026" LastActivityDate="2012-10-09T17:00:26.920" OwnerUserId="7828" ParentId="39023" PostTypeId="2" Score="1" />
  <row AnswerCount="2" Body="&lt;p&gt;I want to include individual symptoms of a disorder to predict remission of the sum score of the disorder some weeks later (sum score = metric variable).&lt;/p&gt;&#10;&#10;&lt;p&gt;The individual symptoms are coded 0, 1, 2 and 3. They are not at all normally distributed, some of them are extremely skewed (e.g. 80% have 0 or 70% have 3). I want to treat them as ordinal, therefor. &lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a way to do this in SPSS? Dummy coding would imply nominal, not ordered, and just entering them as predictors like they are makes me wonder whether SPSS treats them as metric.&lt;/p&gt;&#10;&#10;&lt;p&gt;If not, is there a way to do this in R? I'm new to R so ... don't really know much about it.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-10-09T17:43:32.183" FavoriteCount="1" Id="39031" LastActivityDate="2013-04-03T11:05:01.580" OwnerUserId="14731" PostTypeId="1" Score="3" Tags="&lt;multiple-regression&gt;&lt;ordinal&gt;&lt;ordered-variables&gt;" Title="Ordinal predictors in linear multiple regression in SPSS or R" ViewCount="2500" />
  
  <row AcceptedAnswerId="39056" AnswerCount="1" Body="&lt;p&gt;When using SVM to build classifier for a collection of documents, we can use term occurrence, term frequency or even TF/IDF. I would like to know what are the main disadvantages of using term occurrence as the vector generation method?    I once heard that this vector generation method can cause support vector machine to fail.  How to explain this?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-09T19:45:06.027" Id="39049" LastActivityDate="2012-10-09T20:53:36.467" OwnerUserId="3026" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;data-mining&gt;&lt;svm&gt;&lt;text-mining&gt;&lt;natural-language&gt;" Title="Regarding the feature generation method with SVM-based classification method" ViewCount="238" />
  <row Body="&lt;p&gt;Correlation clustering algorithms such as 4C, ERiC or LMCLUS usually consider clusters to be linear manifolds. I.e. k-dimensional hyperplanes in a d-dimensional space. Well, for 4C and ERiC only locally linear, so they can in fact be non-convex. But they still try to detect clusters of a reduced local dimensionality.&lt;/p&gt;&#10;&#10;&lt;p&gt;Finding arbitrary shaped clusters in high dimensional data is a quite tough problem. In particular, because of the curse of dimensionality which lets the search space explode and at the same time also requires that you have a much larger input data if you still want &lt;em&gt;significant&lt;/em&gt; results. Way too many algorithms don't pay attention to whether what they find is still significant or could as well be random.&lt;/p&gt;&#10;&#10;&lt;p&gt;So in fact I believe there are other problems to solve before thinking about the convexity of non-convexity of complex clusters in high-dimensional space.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also have a look at the complexity of computing the convex hull in higher dimensions...&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, do you have a true use case for that beyond curiosity?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-09T19:59:55.000" Id="39050" LastActivityDate="2012-10-09T19:59:55.000" OwnerUserId="7828" ParentId="39027" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;Harvard's Stat 110 Probability taught by Joseph Blitzstein is available on &lt;a href=&quot;http://delicious.com/redirect?url=http://itunes.apple.com/us/course/statistics-110-probability/id502492375&quot; rel=&quot;nofollow&quot;&gt;iTunesU&lt;/a&gt; or &lt;a href=&quot;http://www.academicearth.org/courses/statistics-110-probability&quot; rel=&quot;nofollow&quot;&gt;Academic Earth&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2012-10-09T20:46:38.710" CreationDate="2012-10-09T20:46:38.710" Id="39055" LastActivityDate="2012-10-09T20:46:38.710" OwnerUserId="2513" ParentId="1761" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;My answer is somewhat different from the one posted by @mpiktas.&lt;/p&gt;&#10;&#10;&lt;p&gt;Consider a discrete-time random process $X_0, X_1, X_2, \ldots$ with the&#10;property that $X_{n+2} = -X_n$. This process actually has only two random&#10;variables $X_0$ and $X_1$: everything else is predetermined since the&#10;process must necessarily be&#10;$$X_0, X_1, -X_0, -X_1, X_0, X_1, -X_0, -X_1, \cdots $$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Can such a process be a stationary process?&lt;/strong&gt; &#10;Well, in order for the&#10;process to be stationary, it is necessary that &lt;em&gt;all&lt;/em&gt; the random&#10;variables have the same distribution.  Thus, $X_0$ and $X_1$&#10;must have the same distribution. So must $X_0$ and $X_2 = -X_0$ have&#10;the same distribution. If $f(x)$ denotes the common probability &#10;density function of $X_0$ and $X_1$ (including as a special case a&#10;discrete density function), then $f(x)$ must be an even function &#10;of $x$. If the density function admits a mean, the mean must be $0$.&lt;br&gt;&#10;(Note that &#10;we &lt;em&gt;could&lt;/em&gt; have $X_0$ and $X_1$ be standard Cauchy random&#10;variables whose density function is even but the mean does&#10;not exist, see e.g. &lt;a href=&quot;http://stats.stackexchange.com/q/36027/6633&quot;&gt;the answers to this question&lt;/a&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;Next, suppose that $(X_0,X_1)$ have joint density $f(x,y)$. &#10;If the process is stationary, then $(X_1,X_2) = (X_1,-X_0)$, &#10;$(X_2,X_3) = (-X_0,-X_1)$&#10;and $(X_3,X_4) = (-X_1,X_0)$ all must have the same joint density.&#10;It follows that &#10;$$f(x,y) = f(y,-x) = f(-x,-y) = f(-y,x).$$&#10;Note that this condition is always satisfied if&#10;$X_0$ and $X_1$ are &lt;em&gt;independent&lt;/em&gt; (in addition to&#10;being identically distributed random variables with even density&#10;functions) e.g. the independent uniform $\{+1,-1\}$ random&#10;variables in @cardinal's comment on mpiktas's answer or&#10;independent zero-mean Gaussian random variables with the same&#10;variance.  But I am not sure that independence is&#10;necessary for this relation to hold. &lt;strong&gt;(Note added in edit:&lt;/strong&gt;&#10;In fact, as cardinal's insightful comment on this answer shows, &#10;$(X_0,X_1)$ &#10;taking on values $(1,0), (0,1), (-1,0), (0,-1)$ with equal&#10;probability satisfy the above condition but are not independent &lt;strong&gt;)&lt;/strong&gt;. To tie this in with&#10;mpiktas's answer, note that if $X_0$ and $X_1$ have finite variance $\sigma^2$,&#10;then, for a stationary process, the &lt;em&gt;autocorrelation&#10;function&lt;/em&gt; $R_X(m,m+n) = E[X_mX_{m+n}]$ (which is the same as the &#10;autocovariance function $C_X(m,m+n) = \text{cov}(X_m, X_{m+n})$ since the&#10;mean is $0$) must not depend at all on the choice of $m$, but&#10;must be a function only of $n$, the separation between the&#10;variables.  From this we get that&#10;$$\text{cov}(X_0, X_1) = \text{cov}(X_1,X_2)&#10;= \text{cov}(X_1, - X_0) = -\text{cov}(X_0, X_1),$$&#10;that is, $X_0$ and $X_1$, the only two random variables in the process,&#10;must be &lt;em&gt;uncorrelated&lt;/em&gt; random variables.  Note also that the &#10;covariance matrix in mpiktas's answer must be of the form&#10;$$\begin{bmatrix}&#10;r(0) &amp;amp; 0 &amp;amp; -r(0) &amp;amp; 0 &amp;amp; r(0) &amp;amp; \ldots \\&#10;0 &amp;amp; r(0) &amp;amp; 0 &amp;amp; -r(0) &amp;amp; 0 &amp;amp; \ldots \\&#10;-r(0) &amp;amp; 0 &amp;amp; r(0) &amp;amp; 0 &amp;amp; -r(0) &amp;amp; \ldots \\&#10;0 &amp;amp; -r(0) &amp;amp; 0 &amp;amp; r(0) &amp;amp; 0 &amp;amp; \ldots \\&#10;r(0) &amp;amp; 0 &amp;amp; -r(0) &amp;amp; 0 &amp;amp; r(0) &amp;amp; \ldots \\&#10;\ldots &amp;amp; \ldots &amp;amp; \ldots &amp;amp; \ldots &amp;amp; \ldots&#10;\end{bmatrix}$$&#10;In particular, there is no need to consider values of $r(1)$ other than $0$&#10;and simulate anything to see if the eigenvalues are positive or not.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;In summary, there do exist discrete-time stationary normal &#10;random processes with the property that $X_{n+2} = -X_n$.&#10;Such random processes are necessarily of the form &#10;$$X_0, X_1, -X_0, -X_1, X_0, X_1, -X_0, -X_1, \cdots $$&#10;where $X_0$ and $X_1$ are zero-mean &lt;em&gt;uncorrelated&lt;/em&gt; Gaussian &#10;random variables with the same variance.  As a special&#10;case, $X_0$ and $X_1$ being &lt;em&gt;independent&lt;/em&gt; $N(0,\sigma^2)$&#10;random variables will work since independent Gaussian random&#10;variables are uncorrelated. But we could also have $X_1 = ZX_0$&#10;where $Z$, which takes on values $+1$ and $-1$ with equal probability&#10;$\frac{1}{2}$, is independent of $X_0$. (This is a standard example&#10;of uncorrelated marginally Gaussian random variables that are not &#10;independent; they are not &lt;em&gt;jointly&lt;/em&gt; Gaussian). It must also be&#10;said that these random processes are not particularly interesting&#10;as a model for random phenomena&#10;since &lt;em&gt;every&lt;/em&gt; realization (or sample path) of the process is&#10;necessarily of the form&#10;$$a, b, -a, -b, a, b, -a, -b, \cdots$$ &#10;for independent $X_0$ and $X_1$ and of the form&#10;$$a, a, -a, -a, a, a, -a, -a \cdots~~~~~ \text{or}~~~~~  a, -a, -a, a, a, -a, -a, a,\cdots$$&#10;for uncorrelated but not independent $X_0$ and $X_1$.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-10-10T02:38:30.700" Id="39067" LastActivityDate="2012-10-10T13:30:14.453" LastEditDate="2012-10-10T13:30:14.453" LastEditorUserId="6633" OwnerUserId="6633" ParentId="38852" PostTypeId="2" Score="5" />
  <row AnswerCount="2" Body="&lt;p&gt;I have problems in finding a proper similarity measure for clustering. I have around 3000 arrays of sets, where each set contains features of certain domain (e.g., number, color, days, alphabets, etc). I'll explain my problem with an example.&lt;/p&gt;&#10;&#10;&lt;p&gt;Lets assume i have only 2 arrays(a1 &amp;amp; a2) and I want to find the similarity between them. each array contains 4 sets (in my actual problem there are 250 sets (domains) per array) and a set can be empty.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;a1: {a,b}, {1,4,6}, {mon, tue, wed}, {red, blue,green}&#10;a2: {b,c}, {2,4,6}, {}, {blue, black}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I have come with a similarity measure using &lt;a href=&quot;http://en.wikipedia.org/wiki/Jaccard_index&quot; rel=&quot;nofollow&quot;&gt;Jaccard&lt;/a&gt; index (denoted as J):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;sim(a1,a2) = [J(a1[0], a2[0]) + J(a1[1], a2[1]) + ... + J(a1[3], a2[3])]/4&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;note:I divide by total number of sets (in the above example 4) to keep the similarity between 0 and 1. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;Is this a proper similarity measure and are there any flaws in this approach&lt;/code&gt;. I am applying Jaccard index for each set separately because I want compare the similarity between related domains(i.e. color with color, etc...)&lt;/p&gt;&#10;&#10;&lt;p&gt;I am not aware of any other proper similarity measure for my problem.&#10;Further, &lt;code&gt;can I use this similarity measure for clustering purpose?&lt;/code&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-10T04:56:23.780" Id="39071" LastActivityDate="2012-10-10T07:16:08.350" OwnerUserId="10141" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;mathematical-statistics&gt;&lt;distance-functions&gt;" Title="Proper similarity measure for clustering" ViewCount="122" />
  <row AnswerCount="1" Body="&lt;p&gt;In many cases, multivariate methods are used without normality tests.&#10;How are following methods robust if data are not normal?&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Principal components analysis&lt;/li&gt;&#10;&lt;li&gt;Canonical correlations analysis&lt;/li&gt;&#10;&lt;li&gt;Factor analysis&lt;/li&gt;&#10;&lt;li&gt;Multivariate regression analysis&lt;/li&gt;&#10;&lt;li&gt;Multivariate analysis of variance&lt;/li&gt;&#10;&lt;li&gt;Discriminant analysis&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" ClosedDate="2012-10-10T15:44:24.177" CommentCount="6" CreationDate="2012-10-10T05:49:21.313" Id="39076" LastActivityDate="2012-10-10T14:12:25.853" OwnerUserId="14730" PostTypeId="1" Score="1" Tags="&lt;multivariate-analysis&gt;&lt;normality&gt;&lt;robust&gt;" Title="How robust are multivariate methods to violations of normality?" ViewCount="322" />
  <row AcceptedAnswerId="39084" AnswerCount="1" Body="&lt;p&gt;I have collected a data from about 200 customers on their brand preference of 5 international fast food chain shops. Each customer was asked to rank all the 5 brands from highest to lowest (5 for the most preferred and 1 for the least preferred brand). The brand rankings will vary from one person to another. &lt;/p&gt;&#10;&#10;&lt;p&gt;I want to test if there are significant differences between the brand preferences. &lt;/p&gt;&#10;&#10;&lt;p&gt;At a fist glance ANOVA seems to be a good option. We could take the average of the ranks for each brand and then perform ANOVA. But the problem is that, the ranks can not be treated as scale, because the spaces among the ranks may not be equal. (e.g. Brand-2 and Brand-3 may differ a lot to a customer according to his preference than Brand-1 and Brand-2).&lt;/p&gt;&#10;&#10;&lt;p&gt;So, what can be the best alternative to test if the brand preferences are significantly different? I think I should adopt some non-parametric test like test of several medians instead of mean. What would be your suggestion regarding that? Will Kruskal-Wallis be a valid test here? &lt;/p&gt;&#10;&#10;&lt;p&gt;Would you please give me some indications as to how I should perform whatever you suggest in either R or in SPSS?   &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-10T08:21:16.137" FavoriteCount="1" Id="39083" LastActivityDate="2012-10-10T08:33:13.570" OwnerUserId="12603" PostTypeId="1" Score="3" Tags="&lt;anova&gt;&lt;nonparametric&gt;&lt;kruskal-wallis&gt;" Title="Nonparametric alternative to ANOVA for testing the difference between several brand preferences" ViewCount="1709" />
  
  <row Body="&lt;p&gt;Think about what significance means. A relationship of the form you suggest can be characterized as &#10;$Y = a_1X^2+a_2X+b$&#10;and empirically estimated as $\hat{Y}=\alpha_1\hat{X}^2+\alpha_2\hat{X}+\beta+\epsilon$.&lt;/p&gt;&#10;&#10;&lt;p&gt;What does the significance of an estimate - say, $\alpha_2$ - mean? The significance is Pr(data|H0), and given a probability that is &quot;not significant&quot;, what you really do not reject, is the possibility that the coefficient might really be zero.&lt;/p&gt;&#10;&#10;&lt;p&gt;Does this invalidate the assumption of a curvilinear relationship? Not in my opinion. Rather, it seems to suggest that $a_2$ is really zero.&lt;/p&gt;&#10;&#10;&lt;p&gt;Consider the following example (written in Stata). &lt;/p&gt;&#10;&#10;&lt;p&gt;First we generate some data:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set obs 20000&#10;gen x = uniform()&#10;gen control_one = uniform()&#10;gen control_two = uniform()&#10;drawnorm e, m(0) sd(0.5)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;We then specify a new variable X = x^2 and a relationship for an outcome variable Y&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;gen Y = control_one+control_two+X+e&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;(This corresponds to a multidimensional curvilinear model in x with coefficient of the linear and constant term equal to zero).&lt;/p&gt;&#10;&#10;&lt;p&gt;We then run some regressions:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;reg Y control_one control_two&#10;reg Y control_one control_two x&#10;reg Y control_one control_two X x&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The x term is significant in the second model, but not in the third. As far as I understand, this reflects your experience with real data. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-10T09:57:38.863" Id="39090" LastActivityDate="2012-10-10T09:57:38.863" OwnerUserId="11878" ParentId="39087" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;&lt;strong&gt;Ad Principal components analysis and Factor analysis:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;These methods are only exploratory and they never assume normality. They are non-stable (i.e. sensitive to small random variations of input data) when there are pairs of very similar eigenvalues, especially when the elements of the pair have relatively large value.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Ad Multivariate regression analysis, canonical correlation, MANOVA&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Distribution of covariance matrix is sensitive to kurtosis of the sample, so if you have data with adjusted kurtosis different from 0, this method will give wrong p-values (&lt;em&gt;see van der Vaart &quot;Asymptotic Statistics&quot;, p. 28, Cambridge Univ. Press, 2000&lt;/em&gt;)&lt;/p&gt;&#10;&#10;&lt;p&gt;I can even say, that the distribution of covariance matrix is sensitive &lt;em&gt;only&lt;/em&gt; to kurtosis of the sample and, in less degree, every other even moment of the distribution, but in descending power (i.e. the higher is the moment which is different from normal distribution, the less influence it has on distribution of the covariance matrix).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-10T10:18:16.803" Id="39092" LastActivityDate="2012-10-10T14:12:25.853" LastEditDate="2012-10-10T14:12:25.853" LastEditorUserId="10069" OwnerUserId="10069" ParentId="39076" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;The beta distribution follows from the sufficient statistics $(\log(x), \log(1-x))$.  Do those statistics make sense for your data?  If you have so many zeros and ones, then it seems doubtful that they do, and you might consider not using a beta distribution at all.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you were to choose the sufficient statistic $x$ instead (over your bounded support), then I believe you end up with a truncated exponential distribution, and with $(x,x^2)$ a truncated normal distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;I believe that both are easily estimated in a Bayesian way as they are both exponential families.  This is a modification of the model as you were hoping.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-10T10:26:53.913" Id="39094" LastActivityDate="2012-10-10T10:32:41.727" LastEditDate="2012-10-10T10:32:41.727" LastEditorUserId="858" OwnerUserId="858" ParentId="31300" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;The Jeffreys prior coincides with the Bernardo reference prior for one-dimensional parameter space (and &quot;regular&quot; models). Roughly speaking, this is the prior for which the Kullback-Leibler divergence between the prior and the posterior is maximal. This quantity represents the amount of information brought by the data. This is why the prior is considered to be uninformative: this is the one for which the data brings the maximal amount of information.&lt;/p&gt;&#10;&#10;&lt;p&gt;By the way I don't know whether Jeffreys was aware of this characterization of his prior ?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-10-10T11:30:49.947" Id="39098" LastActivityDate="2012-10-10T11:30:49.947" OwnerUserId="8402" ParentId="7519" PostTypeId="2" Score="13" />
  
  <row AcceptedAnswerId="39226" AnswerCount="1" Body="&lt;p&gt;I've got some time-series business data that I can fit relatively well with a &lt;code&gt;ARIMA(2,1,0)(1,1,0)[12]&lt;/code&gt; model (using R's excellent &lt;code&gt;forecast::Arima&lt;/code&gt; -- thanks Prof. Hyndman!). The series is dominated by seasonal effects, but has trends as well, thus the differencing. I'm not an expert in forecasting.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm exploring a future experiment (power analysis sorts of things) by simulating the effect of some sort of intervention that may increase (or decrease) the values in the series, probably multiplicatively. To do this, I'm scaling the numbers for the last N months by X%, and using the &lt;code&gt;xreg&lt;/code&gt; parameter to change the model from a differenced AR(1) to a regression with time-series errors. The vector I'm using as the regressor looks like &lt;code&gt;[0, 0, ..., 0, 1, 1, 1]&lt;/code&gt;, where 1s represent the months with the intervention in effect. &lt;/p&gt;&#10;&#10;&lt;p&gt;The coefficient I get from the model appears to be an additive effect, which makes sense, but is &lt;em&gt;much&lt;/em&gt; smaller than the actual effect (4000 vs 100,000). However, when I use &lt;code&gt;forecast&lt;/code&gt;, with and without 1s in the regressor, the difference is of the expected magnitude -- if anything, too high. &lt;/p&gt;&#10;&#10;&lt;p&gt;So, my questions:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;How do I interpret that coefficient. Is it additive? &lt;/li&gt;&#10;&lt;li&gt;Is it correct to use 1s in the regressor vector for all time periods that the treatment is in effect, or should I be thinking of this as an impulse that offsets the trend in the ARIMA model, and using a pattern like &lt;code&gt;0, 1, 0, 0, -1, 0&lt;/code&gt;?&lt;/li&gt;&#10;&lt;li&gt;Any other advice?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;This is related to these questions, which either don't answer my question or I don't fully understand:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/28284/variables-importance-on-regression-with-arima-errors-model&quot;&gt;Variables importance on regression with ARIMA errors model&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/18072/intervention-analysis-in-time-series-regression-with-seasonal-arima-errors?rq=1&quot;&gt;Intervention analysis in time-series regression with seasonal ARIMA errors&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/17623/how-to-detect-a-significant-change-in-time-series-data-due-to-a-policy-change&quot;&gt;How to detect a significant change in time series data due to a &amp;quot;policy&amp;quot; change?&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-10-10T13:41:35.317" FavoriteCount="1" Id="39110" LastActivityDate="2012-12-17T21:30:15.253" OwnerUserId="6" PostTypeId="1" Score="4" Tags="&lt;time-series&gt;&lt;change-point&gt;&lt;intervention-analysis&gt;" Title="How to interpret coefficients in a regression with ARIMA errors?" ViewCount="1196" />
  
  
  <row Body="&lt;p&gt;Unlike PCA- Laplacian eigenmaps uses the generalized eigen vectors corresponding to the smallest eigenvalues. It skips the eigen vector with the smallest eigen value (could be zero), and uses the eigen vectors corresponding to the next few smallest eigen values. PCA is a maximum variance preserving embedding using the kernel/gram matrix. Laplacian Eigenmaps is posed more as a minimization problem with respect to the combinatorial graph laplacian (refer papers by Trosset).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-10-10T17:28:36.800" Id="39124" LastActivityDate="2012-10-10T17:28:36.800" OwnerUserId="6897" ParentId="39115" PostTypeId="2" Score="0" />
  
  <row AnswerCount="2" Body="&lt;p&gt;Is there any convention regarding the minimum information that should be included in a table showing a linear regression output? &lt;/p&gt;&#10;&#10;&lt;p&gt;The reason for this question is that I want to make the output as easy to understand as possible. I have graphed the main findings but I want to include the other outputs in a detailed table.&lt;/p&gt;&#10;&#10;&lt;p&gt;This is in a general social science context (i.e. a presentation; not journal publication) and my audience is not trained in statistics, so I want to keep everything as simple as possible (while still retaining the critical information).&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-10-10T22:00:54.340" Id="39144" LastActivityDate="2012-10-11T02:39:33.233" LastEditDate="2012-10-11T02:39:33.233" LastEditorUserId="1036" OwnerUserId="9342" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;tables&gt;" Title="How should I present linear regression output in a table format?" ViewCount="1110" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I have this confusion related to Monte Carlo Markov Chain method. I know that Monte Carlo method is used to get the sample mean instead of calculating the high dimensional integration which is not tractable. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, I didn't get what's the role of Markov chain in this. I mean if we take samples from the posterior distribution of the parameters, then we can average the function of the parameters to get the expectation or mean. I didn't get what the role of Markov chain is here?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-10T23:33:38.413" Id="39153" LastActivityDate="2012-10-16T12:40:10.467" OwnerUserId="12329" PostTypeId="1" Score="3" Tags="&lt;monte-carlo&gt;&lt;markov-process&gt;" Title="Confusion related to MCMC technique" ViewCount="166" />
  <row Body="&lt;p&gt;You algebraically expand -(x-μ)$^2$/(2σ$^2$) and equate coefficients.  Notice a&amp;lt;0.  This works in yhr opposite direction as in Zen's solution.  But as he points out there are three equations in two unknowns.  But the constraint that the density integrates to 1 does reduce it to 2 equations in two unknowns.  His way fills in the details.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-11T02:55:57.490" Id="39165" LastActivityDate="2012-10-11T04:11:06.460" LastEditDate="2012-10-11T04:11:06.460" LastEditorUserId="11032" OwnerUserId="11032" ParentId="39163" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;I would recommend you &lt;a href=&quot;http://www.uvm.edu/~cmplxsys/newsevents/pdfs/2012/reshef-correlation-science-2011.pdf&quot; rel=&quot;nofollow&quot;&gt;this excellent article&lt;/a&gt; published in &lt;em&gt;Science&lt;/em&gt; in 2011 that I previously posted &lt;a href=&quot;http://stats.stackexchange.com/questions/6421/what-are-the-breakthroughs-in-statistics-of-the-past-15-years/38901#38901&quot;&gt;here.&lt;/a&gt; There is proposal of one new robust measure together with exhaustive and excellent comparison with other ones. Moreover, all measures are tested on robustness. Note that this new measure is also capable to identify more than one functional relation in data and also to identify nonfunctional relationships.&lt;/p&gt;&#10;" CommentCount="5" CommunityOwnedDate="2012-10-11T09:43:05.903" CreationDate="2012-10-11T09:43:05.903" Id="39183" LastActivityDate="2012-10-11T09:43:05.903" OwnerUserId="14730" ParentId="15900" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="39207" AnswerCount="1" Body="&lt;p&gt;What I'm trying to do is analyse results for an experiment I have conducted. An example set of results is&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Case A   B   C   D&#10; 1   2   3   1   8&#10; 2  11  11  14   5&#10; 3   2   3   4   3&#10; 4  12   7   8   7&#10; 5   6   6   9   4&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;A, B, C, and D are different participants.  I would like to identify which participant has the most difference in their results compared to everyone else.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2012-10-11T14:19:06.393" Id="39203" LastActivityDate="2012-10-11T14:58:26.953" LastEditDate="2012-10-11T14:24:57.807" LastEditorUserId="919" OwnerUserId="14862" PostTypeId="1" Score="0" Tags="&lt;group-differences&gt;" Title="Analysis of participant's results to see who is the most different" ViewCount="38" />
  <row Body="&lt;p&gt;pdist -&gt; linkage -&gt; dendrogram is also used in clustergram, just open the clustergram code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;dist = pdist(data, pdistArgs{:});&#10;Z = linkage(dist, linkageArgs);&#10;&#10;...&#10;&#10;[lineH, T, Perm] = dendrogram(Z,0,dendroArgs{:},'Orientation', dendroLoc);&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So the only difference can be in data preprocessing or parameters for pdist/linkage/dendrogram.&#10;I suggest you go over the clustergram parameters one by one. You can also look through the code although it is pretty long.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-10-11T15:16:49.877" Id="39210" LastActivityDate="2012-10-11T15:16:49.877" OwnerUserId="14434" ParentId="39194" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm trying to compare three groups data. But the data set is about a new drug trial. The data set has these characteristics:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Follow-up set. That is, after administration of the drug, a&#10;series of parameters were collected in the following date, day0,&#10;day1,day2,...,day28.&lt;/li&gt;&#10;&lt;li&gt;Unbalanced set. Because some patients died&#10;and some recovered, not all patients were followed to 28 days; for&#10;instance, some were followed for only 20 days. &lt;/li&gt;&#10;&lt;li&gt;&lt;strong&gt;Non normally distributed&#10;parameter. Especially, day25-day28, sometimes sd &gt;&gt; mean.&lt;/strong&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;So, I try to use the linear mixed-effect model with &lt;code&gt;group&lt;/code&gt;, &lt;code&gt;time&lt;/code&gt; and &lt;code&gt;group*time&lt;/code&gt; for the comparison. ANOVA is not suit for the unbalanced set.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, I wonder if it's correct model for &lt;strong&gt;non normal data set&lt;/strong&gt;? Or is there another method suited for this kind of set?&lt;/p&gt;&#10;&#10;&lt;p&gt;I use SPSS 19. Some of my friends use STATA or JMP. &lt;/p&gt;&#10;" CommentCount="8" CreationDate="2012-10-11T16:11:00.530" Id="39214" LastActivityDate="2012-10-11T16:24:21.643" LastEditDate="2012-10-11T16:24:21.643" LastEditorUserId="2970" OwnerUserId="14866" PostTypeId="1" Score="1" Tags="&lt;mixed-model&gt;&lt;unbalanced-classes&gt;" Title="How to compare multiple groups of unblanced repeated measures non normal data?" ViewCount="206" />
  <row Body="&lt;p&gt;There is no rule of thumb, as the values are &lt;strong&gt;not comparable across data sets&lt;/strong&gt;. Not even across different normalizations of the data set or across algorithms. You can mostly use them to compare different runs of the same algorithm, at most with slightly changed parameters (to some extend, not even across different &lt;code&gt;k&lt;/code&gt;!)&lt;/p&gt;&#10;&#10;&lt;p&gt;Say, you scale every axis by 10. The data set is virtually unchanged, yet the within cluster sum of squares probably goes up by a factor of 100.&lt;/p&gt;&#10;&#10;&lt;p&gt;Or you leave away dimensions. Obviously the distances will become smaller, and thus the sum will go down. Try selecting a single attribute, and k=100 - what is your result? Can you get below 290 this way? This doesn't mean it is better - it just shows that the scores are not comparable across different settings such as dimensionality and normalization.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-10-11T16:19:47.950" Id="39218" LastActivityDate="2012-10-18T16:38:57.267" LastEditDate="2012-10-18T16:38:57.267" LastEditorUserId="7828" OwnerUserId="7828" ParentId="39161" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;I'd like to share the answer to this question with you in very simple words. Because I also wondered in my early days on why &lt;code&gt;R&lt;/code&gt; gives a value when I input a single value in any density function of a continuous variable, say for example in &lt;code&gt;dnorm&lt;/code&gt;. As I know for a particular value the probability should be zero. &lt;/p&gt;&#10;&#10;&lt;p&gt;Here an important thing to be noticed is that if you manually put the value of the random variable, say, $X$, in the form of the probability density function (p.d.f.) of a continuous random variable, then you do not get the probability. What you get is called simply the 'ordinate'. If you consider a X-Y plane, then this value is the $Y$ coordinate value against your value for $X$ coordinate. &lt;/p&gt;&#10;&#10;&lt;p&gt;Following is a practical example of why a particular point gives probability zero in a continuous probability distribution: &lt;/p&gt;&#10;&#10;&lt;p&gt;While for a discrete distribution an event with probability zero is impossible (e.g. rolling 3½ on a standard die is impossible, and has probability zero), this is not so in the case of a continuous random variable. For example, if one measures the width of an oak leaf, the result of 3½ cm is possible, however it has probability zero because there are uncountably many other potential values even between 3 cm and 4 cm. Each of these individual outcomes has probability zero, yet the probability that the outcome will fall into the interval (3 cm, 4 cm) is nonzero. This apparent paradox is resolved by the fact that the probability that $X$ attains some value within an infinite set, such as an interval, cannot be found by naively adding the probabilities for individual values. Formally, each value has an infinitesimally small probability, which statistically is equivalent to zero.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-10-11T18:05:47.670" Id="39229" LastActivityDate="2012-10-11T18:33:49.580" LastEditDate="2012-10-11T18:33:49.580" LastEditorUserId="12603" OwnerUserId="12603" ParentId="39224" PostTypeId="2" Score="1" />
  
  
  <row AcceptedAnswerId="39239" AnswerCount="1" Body="&lt;p&gt;So, I struggle with Regression a lot. I just found out how to get 2 lines with the same slope, but I cannot manage to get 2 lines with the same intercept. I read about ANCOVA a lot (because I thought this was what I needed), but no one uses the same intercepts; just the same slope.&#10;Can someone help out with this?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-10-11T16:30:17.360" Id="39233" LastActivityDate="2012-10-11T20:31:21.393" OwnerDisplayName="cups" OwnerUserId="14305" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;regression&gt;" Title="Regression Lines With Same Intercept" ViewCount="140" />
  
  <row Body="&lt;p&gt;I believe (but this is based on anecdote) that R tends to be used more as a prototyping language by the companies you name above. R excels in the task of developing and testing multiple models quickly and effectively. However, it is not a good fit for personalisation tasks as these often need to take place as a user interacts with a particular website and I believe (again, this is mostly anecdote) that such models tend to be re-written in a compiled language (Java, C, C++).&lt;/p&gt;&#10;&#10;&lt;p&gt;That being said, good question and I would love to be proved wrong on this. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-11T19:47:02.513" Id="39235" LastActivityDate="2012-10-11T19:47:02.513" OwnerUserId="656" ParentId="39232" PostTypeId="2" Score="0" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Among the literature, some line charts use mean/SE but others use mean/SD. Which is correct or better?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-10-11T20:51:31.987" Id="39244" LastActivityDate="2012-10-12T00:49:56.150" LastEditDate="2012-10-11T21:46:53.400" LastEditorUserId="1036" OwnerUserId="14866" PostTypeId="1" Score="1" Tags="&lt;data-visualization&gt;&lt;standard-deviation&gt;&lt;standard-error&gt;" Title="Which is better, SD or SE in line charts?" ViewCount="112" />
  <row AnswerCount="1" Body="&lt;p&gt;I am trying to figure out the relationship between a dichotomous independent variable sex, an ordinal three level SES independent variable predicting a dichotomous variable for legalization of pot or against. I am confused about what test to run.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-11T21:10:59.590" Id="39246" LastActivityDate="2012-10-12T02:42:06.700" LastEditDate="2012-10-11T21:14:14.470" LastEditorUserId="7290" OwnerUserId="14872" PostTypeId="1" Score="0" Tags="&lt;categorical-data&gt;" Title="Can we predict whether or not persons will favor legalization of marijuana based on sex and three categories of SES?" ViewCount="37" />
  <row Body="&lt;p&gt;Permutation usually refers to something else, so it's probably better to call your problem &quot;random binary words&quot; or something similar. &lt;/p&gt;&#10;&#10;&lt;p&gt;The question of how long it takes to get at least one representative of each type is called the &lt;a href=&quot;http://en.wikipedia.org/wiki/Coupon_collector%27s_problem&quot;&gt;Coupon Collector Problem&lt;/a&gt;. If you assume that all binary words of length $N$ are equally likely, then there are $2^N$ types of coupons. You can write the time to collect all coupons as a sum of the times to collect the $i$th new coupon, a sum of independent geometric random variables. So, the expected number of coupons it takes to collect them all is $2^N \sum_{i=1}^{2^N} 1/i \sim 2^N \log 2^N$, or more precisely $2^N \log 2^N + 2^N \gamma + 1/2 + o(1)$. For $N=10$ this is about $7689$. The variance is $2^{2N} \sum_{i=1}^{2^N} 1/i^2 \approx 2^{2N} \frac {\pi^2}{6}$, so the standard deviation is about $2^N \frac{\pi}{\sqrt{6}}$. For $N=10$ this is about $1313$. Note that a normal approximation is NOT appropriate here.&lt;/p&gt;&#10;&#10;&lt;p&gt;One crude bound is Chebyshev's inequality, which says that the chance that a random variable is more than $k$ standard deviations away from the mean is at most $1/k^2$, and the similar Cantelli's inequality is that the chance that a random variable is at least $k$ standard deviations above the mean is at most $1/(k^2+1)$. This gives you an upper bound of about $7689 + 1313 \approx 9002$ for the median, and $7689 + 1313\sqrt{19} \sim 13412$ for the $95$th percentile. &lt;/p&gt;&#10;&#10;&lt;p&gt;If these bounds are not good enough, there are more precise but more complicated asymptotics known. Another approach, suitable perhaps up to $N = 25$, is to compute the exact distribution numerically using the representation as a sum of independent geometric distributions.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-10-11T21:15:09.593" Id="39247" LastActivityDate="2012-10-11T21:15:09.593" OwnerUserId="11981" ParentId="39230" PostTypeId="2" Score="5" />
  
  
  
  
  <row Body="&lt;p&gt;Yes, survival analysis may be used to model earthquake data, but perhaps not in the way you originally envisioned. &lt;/p&gt;&#10;&#10;&lt;p&gt;Specifically you can trade out time-to-death (survival) for another interval variable, and it need not be time-based.  It could be a set of financial thresholds, losses on an insurance policy, or perhaps the max magnitude of an earthquake experienced for a particular region (the studied unit in this example) between 2000-2010. &lt;/p&gt;&#10;&#10;&lt;p&gt;For n = 100 measurement regions, grouped by richter scale band: &lt;br&gt;&#10;9.0+:        1 &lt;br&gt;&#10;8.0-8.9:     2 &lt;br&gt;&#10;7.0-7.9:     6 &lt;br&gt;&#10;6.0-6.9:     9 &lt;br&gt;&#10;etc……           &lt;br&gt;&#10;&amp;lt;2.0:         40 &lt;br&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;From there all of the standard statistics apply:  Kaplan-Meier estimator, Nelson-Aelan hazard rate, their variance approximations, cumulative survival rates, conditional 'survival' analyses, etc.  And assuming you have appropriate covariates then Cox regression as well. &lt;/p&gt;&#10;&#10;&lt;p&gt;Not sure this is the best case to use or a great example.  But such survival analysis techniques are sometimes employed in insurance, particularly for cases where there's notable truncation, censoring, or grouping of the data you have to work with (though given the ease of capturing data these days this is less of an issue).   &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-12T00:51:51.150" Id="39258" LastActivityDate="2012-10-12T00:51:51.150" OwnerUserId="14845" ParentId="35750" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;I was interested in the same problem and &lt;a href=&quot;http://www.epscor.hawaii.edu/sites/web41.its.hawaii.edu.www.epscor.hawaii.edu/files/er_book/2012/05/Birdetal_Detecting_Measuring.pdf&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; is link for the best sourse I found then. There is also nice example, what is missing in many other sources about AMOVA.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-12T06:20:45.783" Id="39271" LastActivityDate="2012-10-12T06:20:45.783" OwnerUserId="14730" ParentId="22094" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;Another direct answer to the question is the rgl package, which can plot millions of points using OpenGL. Also, specify a point size (e.g. 3) and zoom out to see these centers of masses as monolithic blocks, or zoom in and see the structure of what used to be monolithic - the point sizes are constant but the distances among them on the screen depend on the zooming.  Alpha levels can also be used.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-12T09:40:34.383" Id="39276" LastActivityDate="2012-10-12T09:40:34.383" OwnerUserId="14885" ParentId="7348" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;The problem with your data is not that it is extremely detailed: you have no values at weekends, that's why it is plotted with gaps. There are two ways to deal with it:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Either try to guess approximate values in weekends with some smoothing methods (&lt;code&gt;smooth.spline&lt;/code&gt;, &lt;code&gt;loess&lt;/code&gt;, etc.). Code of simple interpolation is below. But in this case you will introduce something &quot;unnatural&quot; and artificial to the data. That's why I prefer second option.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;blockquote&gt;&#10;&lt;pre&gt;&lt;code&gt;currentDate &amp;lt;- min(as.Date(oracle$Date))&#10;dates &amp;lt;- c(currentDate)&#10;openValues &amp;lt;- c(oracle$Open[5045])&#10;i &amp;lt;- 5044&#10;while (i &amp;gt; 0) {&#10;  currentDate &amp;lt;- currentDate + 1;&#10;  dates &amp;lt;- c(dates, currentDate)&#10;  if (currentDate == as.Date(oracle$Date[i])) {&#10;        # just copy value and move&#10;        openValues &amp;lt;- c(openValues, oracle$Open[i])&#10;        i &amp;lt;- i-1&#10;      } else {&#10;        # interpolate value&#10;        openValues &amp;lt;- c(openValues, mean(oracle$Open[i:i-1]))&#10;  }&#10;}&#10;plot(dates, openValues, type=&quot;l&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;You can go from daily basis to a weekly basis, just averaging (for example) five sequential points that belog to one week (in this case you are &quot;killing&quot; some information). Just a quick example of how to do that would be&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;blockquote&gt;&#10;&lt;pre&gt;&lt;code&gt;openValues = c(mean(oracle$Open[1:5]));&#10;dates = c(as.Date(oracle$Date[1]));&#10;for (i in seq(6,5045,5)) {&#10;  openValues = c(openValues, mean(oracle$Open[i:i+5]));&#10;      dates = c(dates, as.Date(oracle$Date[i]));&#10;}&#10;plot(dates, openValues, type=&quot;l&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Hope it will help.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-10-12T12:51:27.340" Id="39287" LastActivityDate="2012-10-12T13:17:33.690" LastEditDate="2012-10-12T13:17:33.690" LastEditorUserId="11639" OwnerUserId="11639" ParentId="39279" PostTypeId="2" Score="7" />
  
  <row Body="&lt;p&gt;As @Bitwise mentions, clustering should be fine.&#10;Given your data matrix, you will need to transpose first:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;data &amp;lt;- t(data)&#10;hc &amp;lt;- hclust(dist(data))&#10;plot(hc)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2012-10-12T20:59:59.323" Id="39312" LastActivityDate="2012-10-12T21:03:59.047" LastEditDate="2012-10-12T21:03:59.047" LastEditorUserId="686" OwnerUserId="12565" ParentId="39303" PostTypeId="2" Score="1" />
  
  
  
  <row Body="&lt;p&gt;Ultimately we'd need to see the weighting algorithm itself to answer this question; otherwise we wouldn't know if a particular input transform invalidated the weighting algorithm producing the final score. That said: &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;b&gt;First thing to consider before changing the outcomes of that weighting scheme: &lt;/b&gt;&#10;Is that particular use of variable weighting and limited point allocation an established practice in your particular research domain?  If it is then you should be very careful changing the variable inputs directly.  You could invalidate the methodology altogether.  At minimum it will change the scaling of that variable and have implications for comparing your results to other results using the original methodology.&lt;br&gt;&#10;&lt;br&gt;If so and your intent is to remove people who didn't establish any preference across the 10 items then I'd create a new dichotomous/binary (dummy) variable identifying them and run your tests filtering on that or as a covariate.  Perhaps you can retrain your weights filtering those people in and out?  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;b&gt;Addressing the fixed sum issue &lt;/b&gt;&lt;br&gt;&#10;If it's not a common practice and instead a field experiment by you or the survey designers that you're now trying to work around you have a couple of options  beyond creation of new variable identifying people who just split points evenly, per the above point. &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Transform within items - one example would be to recenter the range on 0, running -50 to 50, then take their absolute value or square the values. Again, such methods probably invalidate the scoring algorithm.  &lt;/li&gt;&#10;&lt;li&gt;Transform across items - take the average of all 10 scores and then transform each individual variable by re-scoring it to the difference from the mean.  This becomes a measure of relative preference/sentiment/etc across the 10 items, and again could invalidate the methodology.  &lt;/li&gt;&#10;&lt;li&gt;Do #2 to create a new single measure in addition to the weighted output score, and then scale the output score by this new measure.  This may or may not be a good idea depending on what the scoring algorithm itself is doing - you have to evaluate that closely.  &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;A final option of course is to revisit the scoring algorithm itself if it's not part of an accepted methodology in your domain and alter so that it best suits your needs, using some of the techniques above. &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-10-13T17:38:14.740" Id="39338" LastActivityDate="2012-10-13T17:38:14.740" OwnerUserId="14845" ParentId="39316" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Yes, it makes sense if you restrict the support, in which case it is called a &lt;em&gt;truncated exponential distribution&lt;/em&gt;, which is a useful exponential family.  As mark999 points out, if you don't restrict the support, you won't be able to normalize the distribution.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-13T19:53:05.030" Id="40340" LastActivityDate="2012-10-13T19:53:05.030" OwnerUserId="858" ParentId="39309" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="40346" AnswerCount="2" Body="&lt;p&gt;If we draw two random variates from a discrete uniform distribution $[1, D]$, the probability that the samples are distinct is $(D-1)/D$. Explanations of the birthday problem state that if we sample a third time, the probability that this third sample is distinct from the first two is $(D-1)(D-2)/D^2$. Doesn't this assume that the first two samples were distinct? If they are not distinct, then the probability that the third sample is distinct is $(D-1)/D$. The solution to the birthday problem requires us to know the probability that the $n^\textrm{th}$ random variate is distinct from the previous samples. How can we calculate the probability that the $n^\textrm{th}$ sample is distinct from the previous samples without knowing the values of these previous samples?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-10-13T20:54:39.237" Id="40343" LastActivityDate="2012-10-13T21:19:24.573" LastEditDate="2012-10-13T21:10:17.923" LastEditorUserId="7290" OwnerUserId="15914" PostTypeId="1" Score="2" Tags="&lt;probability&gt;" Title="Calculating the probability of the Nth sample in the birthday problem w/o knowing the previous samples" ViewCount="86" />
  <row AnswerCount="2" Body="&lt;p&gt;I would appreciate some advise on an a problem I ran into.&#10;I use SPSS for statistical analysis of a study.&lt;/p&gt;&#10;&#10;&lt;p&gt;The study look into how a blood test predicts mortality with patients followup of 1 year. Patients were divided into quartiles, and using the first quartile as a reference group I used Cox regression to determine the unadjusted Hazard ratios (HR) and the adjusted HR for quartile 2-4.&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem is that the reference group (Quartile 1–the first 25%) did not have any events.&#10;Therefore I am getting Hazard ratios of 80,000+ for the 2nd, 3rd and 4th quartiles as well as error messages under SPSS.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can anyone advise me on what Im doing wrong?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-10-14T05:57:56.020" Id="40365" LastActivityDate="2012-10-14T10:44:03.903" LastEditDate="2012-10-14T10:44:03.903" LastEditorUserId="930" OwnerUserId="15922" PostTypeId="1" Score="5" Tags="&lt;regression&gt;&lt;spss&gt;&lt;survival&gt;" Title="Cox regression when reference group had zero events" ViewCount="1028" />
  <row Body="&lt;p&gt;A Survey of several of the variants can be found in the paper,&#10;&lt;a href=&quot;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=adaboost%20variants&amp;amp;source=web&amp;amp;cd=3&amp;amp;ved=0CC0QFjAC&amp;amp;url=http://www.deetc.isel.ipl.pt/sistemastele/docentes/AF/Textos/RT/SurveyBoosting.pdf&amp;amp;ei=iWR6UM28GMjziQK__4H4BA&amp;amp;usg=AFQjCNH9-YesRBvf3U0J5aTmRxSBKCWc_w&amp;amp;cad=rja&quot; rel=&quot;nofollow&quot;&gt;'Survey on Boosting Algorithms for Supervised and Semi-supervised Learning,' Artur Ferreira.&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Generally, the Original AdaBoost returns the binary valued class that is the ensemble sign result of several combined models.&lt;/p&gt;&#10;&#10;&lt;p&gt;Real AdaBoost returns a real valued probability of class membership.&lt;/p&gt;&#10;&#10;&lt;p&gt;The other variants are covered in the paper, but less frequently mentioned in common literature. As I understand it, Gentle Adaboost produces a more stable ensemble model.&#10;The AdaBoost.M1 and AdaBoost.M2 models are extensions to multi-class classifications (with M2 overcoming a restriction on the maximum error weights of classifiers from M1).&#10;&lt;a href=&quot;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=experiments%20with%20a%20new%20boosting%20algorithm%20freund%20&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;ved=0CCIQFjAA&amp;amp;url=http://www.cis.upenn.edu/~mkearns/teaching/COLT/boostingexperiments.pdf&amp;amp;ei=3Gp6UL6sA43LigK4i4DoCA&amp;amp;usg=AFQjCNFI7WAgro33s6k5fJegeJSp_K7pKQ&quot; rel=&quot;nofollow&quot;&gt;&quot;Experiments with a New Boosting Algorithm,&quot; Yoav Freund and Robert E. Schapire.&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-10-14T07:21:02.823" Id="40367" LastActivityDate="2012-10-14T08:01:09.100" LastEditDate="2012-10-14T08:01:09.100" LastEditorUserId="13519" OwnerUserId="13519" ParentId="40363" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;You only know the number of the females and males in site A? Do you want to test all the sites have the same ratio or site A has ratio 1:1?&lt;/p&gt;&#10;&#10;&lt;p&gt;I guess this is a problem wrt to Contingency table, for example, you may write it down in the form:&#10;\begin{align*}&#10;  &amp;amp;  A\ B\ C\ sum\\&#10;F\\&#10;M\\&#10;sum\\&#10;\end{align*}&#10;Then you may use some formulas to calculate p values,etc.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-10-14T00:05:26.177" Id="40369" LastActivityDate="2012-10-14T00:05:26.177" OwnerDisplayName="Julie" OwnerUserId="4599" ParentId="40368" PostTypeId="2" Score="0" />
  
  
  
  <row Body="&lt;p&gt;Three suggestions: &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Would you consider looking &lt;a href=&quot;http://code.google.com/p/ceres-solver/&quot; rel=&quot;nofollow&quot;&gt;CERES&lt;/a&gt; solver by Google?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Being somewhat naive on this, would you consider using a &lt;a href=&quot;http://leon.bottou.org/projects/sgd&quot; rel=&quot;nofollow&quot;&gt;Stochastic Gradient descent&lt;/a&gt;, possibly with some form of regularization (like the one done &lt;a href=&quot;http://www.harchaoui.eu/zaid/publications/dhm_2012_rod_atomdescent.pdf&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;)?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Check &lt;a href=&quot;http://ab-initio.mit.edu/wiki/index.php/NLopt&quot; rel=&quot;nofollow&quot;&gt;NLopt&lt;/a&gt; and in particular the implementations of COBYLA and BOBYQA (and possibly NEWUOA). BOBYQA practically forms quadratic models by interpolation and allows for box constraints (bounds) on the parameters. (COBYLA uses Linear Approximations)&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Also, just because you mentioned -Inf(s); I suspect you get those out of the &quot;determinant&quot; term. How exactly are you computing the determinants? For example, just using &lt;code&gt;log(det(A))&lt;/code&gt; in Matlab gave me -Inf(s) in some cases that &lt;code&gt;2*sum(log(diag(chol(A))))&lt;/code&gt; actually gave me a &quot;number&quot;. Maybe you would like to check the numerical stability of the matrix decomposition routines you employ.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-14T20:53:56.807" Id="40401" LastActivityDate="2012-10-14T21:26:55.903" LastEditDate="2012-10-14T21:26:55.903" LastEditorUserId="11852" OwnerUserId="11852" ParentId="26701" PostTypeId="2" Score="1" />
  
  
  <row AcceptedAnswerId="40497" AnswerCount="3" Body="&lt;p&gt;Is there a standard name for a situation where a random variable follows a distribution whose parameter is another random variable ? For example a binomial(15,p) variable where the the p is distributed as beta(1,2), or a Poisson(Y) where Y is distributed as exponential(2)&lt;/p&gt;&#10;&#10;&lt;p&gt;Is this called a compound distribution, or ?&lt;/p&gt;&#10;&#10;&lt;p&gt;Then my real question is, given Y is distributed according to some given pdf with parameter X (say &lt;code&gt;pdf1&lt;/code&gt;), but X is distributed according to another distribution (say &lt;code&gt;pdf2&lt;/code&gt;), how do I use Bayes rule:&#10;$$&#10;f_{X|Y}(x|y)=\frac{f_{Y|X}(y|x) \, f_X(x)}{f_Y(y)}&#10;$$&#10;?&lt;/p&gt;&#10;&#10;&lt;p&gt;$f_X(x)$ must just be &lt;code&gt;pdf2&lt;/code&gt;, right ?&lt;/p&gt;&#10;&#10;&lt;p&gt;Is $f_{Y|X}(y|x)$ just the pdf of Y (that is, &lt;code&gt;pdf1&lt;/code&gt;) with the pdf of X substituted in place of X ? &lt;/p&gt;&#10;&#10;&lt;p&gt;How do I work out $f_Y(y)$ ?&lt;/p&gt;&#10;&#10;&lt;p&gt;I hope it isn't asking too much for someone to tell me the general approach and also give an example of this, not necessarily one of those I mentioned above.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have looked in several statistics books but I didn't find the answer. &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-10-14T22:44:03.140" Id="40410" LastActivityDate="2013-08-13T02:14:28.983" OwnerUserId="11405" PostTypeId="1" Score="1" Tags="&lt;conditional-probability&gt;" Title="When a random variable has a distribution whose parameter is another random variable" ViewCount="1554" />
  <row Body="&lt;p&gt;I'm a novice at this myself, but I have gotten a lot of mileage out of Data Analysis: A Bayesian Tutorial by Devinderjit Sivia and John Skilling.&lt;/p&gt;&#10;&#10;&lt;p&gt;What I think you have described, however is Bayesian parameter estimation for a parameter $p$, say perhaps the probability associated with a coin coming up heads. The function $f_{X|Y}$ is the distribution of that parameter.&lt;/p&gt;&#10;&#10;&lt;p&gt;If this is the case you would call $f_{Y|X}$ you likelihood function, which we could take as a binomial, since our evidence would be a series of coin flips. Note that the parameter of $f_{X|Y}$ is not so much &quot;y&quot; as it is the number of heads and tails thrown (i.e. $f_{X|Y}(x;\#heads, \#tails)$ as this is what you need to properly parameterize your binomial.&lt;/p&gt;&#10;&#10;&lt;p&gt;As for $f_X$ it is our prior, which we could take to be uniform. $f_{X|Y}$ is our posterior, which ends up being a beta distribution for the case I just described.&lt;/p&gt;&#10;&#10;&lt;p&gt;As for $f_Y$ it doesn't really matter if you are just trying to find the best value for the parameter, because it's a normalization factor. However if you need the distribution then it's the integral over the range of the possible values of $x$, in this case $[0,1]$.&lt;/p&gt;&#10;&#10;&lt;p&gt;This article on Wikipedia may help as well. &lt;a href=&quot;http://en.wikipedia.org/wiki/Checking_whether_a_coin_is_fair&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Checking_whether_a_coin_is_fair&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-10-14T22:55:25.237" Id="40411" LastActivityDate="2012-10-15T12:44:08.700" LastEditDate="2012-10-15T12:44:08.700" LastEditorUserId="7038" OwnerUserId="7038" ParentId="40410" PostTypeId="2" Score="-2" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Say $X$ is a discrete random variable with cardinality $|X|$ and $Y$ is a discrete random variable with cardinality $|Y|$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Does it make sense to talk about the KL divergences $D_{KL}(X||Y)$ or $D_{KL}(Y||X)$ of these 2 probability distributions if $|X| \neq |Y|$ ? If so, how does one compute it ?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-10-15T09:18:38.903" Id="40426" LastActivityDate="2012-10-15T14:59:38.487" LastEditDate="2012-10-15T12:20:43.447" LastEditorUserId="15945" OwnerUserId="15945" PostTypeId="1" Score="2" Tags="&lt;probability&gt;&lt;information-theory&gt;&lt;mutual-information&gt;&lt;kullback-leibler&gt;" Title="KL divergence between 2 distributions with unequal cardinalities?" ViewCount="242" />
  <row AcceptedAnswerId="40445" AnswerCount="1" Body="&lt;p&gt;Is saying random numbers are independent equivalent to saying random numbers are auto-correlated in simulation. I am using Auto-correlation method in simulation.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-10-15T09:34:36.250" Id="40427" LastActivityDate="2012-10-15T20:57:42.590" LastEditDate="2012-10-15T10:13:52.863" LastEditorUserId="686" OwnerUserId="13957" PostTypeId="1" Score="-3" Tags="&lt;simulation&gt;&lt;random-variable&gt;&lt;autocorrelation&gt;" Title="Auto-correlation of Random numbers" ViewCount="787" />
  
  <row Body="&lt;p&gt;The &lt;a href=&quot;http://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test&quot; rel=&quot;nofollow&quot;&gt;Kolmogorov-Smirnov test&lt;/a&gt; tests the hypothesis that two independent random samples come from the same distribution.  It can also be used as a one-sample test of the hypothesis that the sample is drawn from a pre-specified theoretical distribution (&lt;em&gt;e.g.&lt;/em&gt;, a particular normal distribution).  The KS statistic is the supremum of the absolute difference of the &lt;a href=&quot;http://en.wikipedia.org/wiki/Empirical_distribution_function&quot; rel=&quot;nofollow&quot;&gt;empirical cumulative distribution functions&lt;/a&gt; of the two samples. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-15T10:31:32.203" Id="40434" LastActivityDate="2013-07-24T17:36:39.380" LastEditDate="2013-07-24T17:36:39.380" LastEditorUserId="919" OwnerUserId="7290" PostTypeId="5" Score="0" />
  <row Body="&lt;p&gt;You might also wanted to try using a marker variable (Williams, Hartman, &amp;amp; Cavazotte (2010). Method Variance and Marker Variables: A Review and Comprehensive Marker Technique. the Organizational Research Methods) which is simply explained in this youtube.&#10;&lt;a href=&quot;http://www.youtube.com/watch?v=w7zZCBlRXog&amp;amp;lc=5D_7EzRSjHAjwrJJdM0epb8dEiSNe_zaTs0AopwzZ1s&amp;amp;lch=email_reply&amp;amp;feature=em-comment_reply_received&quot; rel=&quot;nofollow&quot;&gt;http://www.youtube.com/watch?v=w7zZCBlRXog&amp;amp;lc=5D_7EzRSjHAjwrJJdM0epb8dEiSNe_zaTs0AopwzZ1s&amp;amp;lch=email_reply&amp;amp;feature=em-comment_reply_received&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-15T11:32:42.927" Id="40444" LastActivityDate="2012-10-15T11:32:42.927" OwnerUserId="14694" ParentId="39314" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;A very useful text is &lt;a href=&quot;http://www.springerlink.com/content/978-0-387-78191-4/&quot;&gt;The Statistical Analysis of Functional MRI Data&lt;/a&gt; by Nicole Lazar (free pdf via Springerlink with institutional access). Chapter 7 covers multivariate approaches to the analysis of fMRI data. You don't mention in your post but the analysis of resting state vs. task generally require different approaches. &lt;/p&gt;&#10;&#10;&lt;p&gt;Resting state typically relies on principal components analysis (PCA) or independent components analysis (ICA) with both considered as a correlation analysis.&lt;/p&gt;&#10;&#10;&lt;p&gt;For analyzing voxel activation in the presence of task, I recommend chapter 6 of the book I've linked, which covers spatiotemporal models. I have more experience in this area and a simple approach is to fit the time series to a linear model (i.e. ANOVA) and convolve the design matrix with the so-called &quot;canonical hemodynamic response function (HRF).&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;Additionally, I've found course materials from the University of New Mexico helpful when I was starting out in this field: &lt;a href=&quot;http://www.ece.unm.edu/~vcalhoun/courses/fMRI_Spring12/fmricourse.htm&quot;&gt;Analysis Methods in Functional Magnetic Resonance Imaging&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-10-15T12:53:54.883" Id="40449" LastActivityDate="2012-10-15T13:17:55.430" LastEditDate="2012-10-15T13:17:55.430" LastEditorUserId="15950" OwnerUserId="15950" ParentId="40400" PostTypeId="2" Score="8" />
  <row Body="&lt;p&gt;I think the main tradeoff is that RD has high internal validity, but low external, and the opposite is true for matching. Since we don't have a great deal of detail about what you are actually doing, you will have to decide which one is better in your case.&lt;/p&gt;&#10;&#10;&lt;p&gt;The regression discontinuity design comes closest to an ideal experiment, so it has high internal validity, but unfortunately low external validity. The parameter it estimates is a very localized sort of local average treatment effect since you're mainly considering units right around the discontinuity cutoff. If you have reasons to think there are heaps of heterogeneity in the effect, you're stuck up the proverbial creek with no paddle.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Propensity score matching/reweighting methods can only eliminate bias due to selection on observables, and will yield efficient estimates of many types of treatment effects (average treatment effect or the treatment effect on the treated), so they often have higher external validity compared to RD, but the internal validity that is often questionable since it is rare that selection depends only on observables. If there's selection on unobservables, matching can actually exacerbate bias.&lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, you might also want to consider an RD/Difference-in-Differences hybrid since you may have panel data on your counties:&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{equation}&#10;\hat\beta=\frac{(y_{1}^{+}-y_{1}^{-})-(y_{0}^{+}-y_{0}^{-})}{x_{1}^{+}-x_{1}^{-}}&#10;\end{equation}&#10;where $y$ is the mean outcome, the signs denote position relative to the cutoff and the numbers denote before and after, and $x$ is the mean treatment (may be 1 in the case of the deterministic assignment or sharp RD design). &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt;&#10;I just remembered &lt;a href=&quot;http://www-personal.umich.edu/~titiunik/papers/GeoRDD.pdf&quot; rel=&quot;nofollow&quot;&gt;a paper&lt;/a&gt; by Keele and Titiunik about geographic RD. &lt;/p&gt;&#10;" CommentCount="9" CreationDate="2012-10-15T16:35:39.547" Id="40463" LastActivityDate="2012-10-15T17:54:07.937" LastEditDate="2012-10-15T17:54:07.937" LastEditorUserId="7071" OwnerUserId="7071" ParentId="40421" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Suppose that you want to know how many likely voters plan to vote for the incumbant in your cities race for mayor this year so you take a simple random sample of likely voters and ask them if they plan to vote for the incumbant or the challenger.  The sampling distribution tells us the relationship between the proportion in our sample and the true proportion from the entire city.  Because of the sampling distribution we can make inference based only on the information about the proportion who said &quot;incumbant&quot; in our sample and the sample size, inference like hypothesis tests or confidence intervals.  If we used the joint distribution then we would have to use the information on how each individual answered the question instead of just the summary information (which is a lot simpler).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-15T17:54:00.773" Id="40468" LastActivityDate="2012-10-15T17:54:00.773" OwnerUserId="4505" ParentId="40465" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;Suppose we have two mouse strains S1 and S2 and we have already tested the effect of two different conditions (say treatment vs. control) on the level of gene expression of any strain, each experiment with several replicates.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now we are looking for the genes that are for example significantly higher expressed in treatment condition in both strains. I have already selected the genes that are higher expressed in the treatment condition for each of the strains, and calculated the related p-value. &lt;/p&gt;&#10;&#10;&lt;p&gt;A simple idea is to find the intersection - the genes that are higher expressed in both strains. Is this a good method? How to compute a p-value? The answers that can be easily computed in R are more than welcome!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-15T18:03:31.440" Id="40469" LastActivityDate="2012-10-16T18:30:49.997" LastEditDate="2012-10-16T10:01:26.820" LastEditorUserId="88" OwnerUserId="14699" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;multiple-comparisons&gt;&lt;p-value&gt;" Title="Merging the results of a statistical test in different conditions and calculating p-value" ViewCount="68" />
  
  <row Body="&lt;p&gt;What is the scale of the columns in your model matrix? If the scale differs widely, one often uses the correlation matrix, rather than the covariance - and I am assuming here that your columns were centered about the mean.&lt;/p&gt;&#10;&#10;&lt;p&gt;The first eigenvector often represents overall size. This may be more or less true in your case - all coefficients have the same sign - but they clearly favour variables 2, 3, and 4. (or perhaps that's due to the scaling of the variables.) The second eigenvector is forcible orthogonal to the first; it appears to be comparing the 2nd variable to the 4th - with a bit of 5 thrown in. You often find that sort of thing - each subsequent eigenvector &quot;throws in&quot; what was omitted from the others. The 3rd compares 1 and 2 to variable 5.&lt;/p&gt;&#10;&#10;&lt;p&gt;The last eigenvector is basically pulling out variable 1, which was downweighted by the first eigenvector.&lt;/p&gt;&#10;&#10;&lt;p&gt;96% of the variation goes to the first eigenvalue, which suggests that your data are fairly well summarized by a weighted mean - the rest being orthogonal noise.&lt;/p&gt;&#10;&#10;&lt;p&gt;Does this make sense from what you know of the problem?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-10-15T22:35:55.917" Id="40486" LastActivityDate="2012-10-15T22:35:55.917" OwnerUserId="14188" ParentId="40483" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;I think you should go with subject as a fixed effect since you only have 3 subjects.&lt;/p&gt;&#10;&#10;&lt;p&gt;What is the goal of the study? If you want to test whether all 8 treatments are the same - or test particular contrasts - you could treat subject like a fixed effect block. If you want to predict the response of an as-yet-unseen subject, you need random effects. You also need a bigger sample. &lt;/p&gt;&#10;&#10;&lt;p&gt;RE: &quot;dependence of observations.&quot; This doesn't matter. Whether subject is treated as random or fixed, you still have repeated measures - i.e. multiple observations nested within subject. &lt;/p&gt;&#10;&#10;&lt;p&gt;I would go with options 2 and 3, averaging response and treating subject as a fixed blocking factor. You don't lose power in the comparison between treatments because treatments are applied at the &quot;subject&quot; level, not the replicate level. By taking the mean of your repeated measures, you reduce the variance of your response, so you gain that way (i.e. N is smaller but so is $\sigma^2$)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-15T23:13:53.753" Id="40490" LastActivityDate="2012-10-15T23:13:53.753" OwnerUserId="14188" ParentId="37883" PostTypeId="2" Score="1" />
  <row AnswerCount="6" Body="&lt;p&gt;I have this little problem and I would appreciate some help.&lt;/p&gt;&#10;&#10;&lt;p&gt;As part of my master thesis, I have to identify a trend in a univariate (GDP) time series for different countries.  I have to separate the trend and the stochastic element in it for each country.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have managed to do so by doing:&lt;/p&gt;&#10;&#10;&lt;p&gt;variable c @trend  // for each country.&lt;/p&gt;&#10;&#10;&lt;p&gt;And then running a AR(1) on the residuals  // for each country.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, now I need to identify structural breaks in one of these countries.  I've been reading and searching all over the internet and books and I've found that the test most people use to identify these structural changes is the Chow Test.&lt;/p&gt;&#10;&#10;&lt;p&gt;I know how to run the test, but I have't been able to figure out how to interpret the results, and decide whether there is a structural break or not.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here there is an example of the results:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/RL9Lz.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;What puzzles me the most is the fact that, regardless the point I choose to break the series, I always get &lt;/p&gt;&#10;&#10;&lt;p&gt;Prob. F(2,47)  0.0016 //or any very significant value, with the same degrees of freedom.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can someone please help me understand how I should interpret these results in order to identify where the breaks lie?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-16T09:30:00.727" Id="40504" LastActivityDate="2014-11-05T04:49:45.200" LastEditDate="2013-01-29T08:33:00.967" LastEditorDisplayName="user10525" LastEditorUserId="88" OwnerUserId="15978" PostTypeId="1" Score="7" Tags="&lt;interpretation&gt;&lt;chow-test&gt;" Title="How to identify structural change using a Chow test on Eviews?" ViewCount="8216" />
  
  <row AnswerCount="0" Body="&lt;p&gt;my apologies for my poor knowledge on statistics. Is there a way to calculate the prediction interval at 95% confidence level of a perfect-fit model? I want to compare how my actual model prediction compares with the perfect-fit model. I want to see if the predicted points fall within the 95% prediction interval of the perfect-fit model. Any hint and idea is very much welcomed. Thank you very much in advance.    &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-10-16T13:11:36.077" Id="40517" LastActivityDate="2015-02-14T17:24:30.843" LastEditDate="2015-02-14T17:24:30.843" LastEditorUserId="53618" OwnerUserId="15985" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;predictive-models&gt;&lt;prediction-interval&gt;" Title="Computing 95% prediction interval of a perfect-fit model" ViewCount="295" />
  <row Body="PDF stands for Probability Density Function. The PDF of a variable gives the relative probability for each value of a continuous variable. Use this tag when asking about probability functions in general, whether PDFs or discrete probability mass functions." CommentCount="0" CreationDate="2012-10-16T14:00:21.240" Id="40526" LastActivityDate="2013-07-05T05:10:40.793" LastEditDate="2013-07-05T05:10:40.793" LastEditorUserId="805" OwnerUserId="686" PostTypeId="4" Score="0" />
  
  <row Body="&lt;p&gt;Using a squared term to capture a curvilinear relationship in your data is fine, but it's worth noting that this can only capture one exact shape of curvature.  This can be very limiting in practice.  What you want typically is to use &lt;a href=&quot;http://en.wikipedia.org/wiki/Smoothing_spline&quot; rel=&quot;nofollow&quot;&gt;smoothing splines&lt;/a&gt;.  I wrote about this here: &lt;a href=&quot;http://stats.stackexchange.com/questions/5015//25081#25081&quot;&gt;What are the advantages / disadvantages of using splines, smoothed splines, and Gaussian process emulators?&lt;/a&gt;, which may be helpful to read.  However, these techniques are somewhat advanced, and I doubt they can be done in Excel.  Moreover, extrapolation is very hard to do well (&lt;a href=&quot;http://stats.stackexchange.com/questions/423/what-is-your-favorite-data-analysis-cartoon/4968#4968&quot;&gt;here's a humorous example&lt;/a&gt;), and extrapolation is especially likely to go awry when using splines.  Most likely, you will need to work with a statistical consultant.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-16T16:59:52.657" Id="40538" LastActivityDate="2012-10-16T16:59:52.657" OwnerUserId="7290" ParentId="40518" PostTypeId="2" Score="1" />
  
  
  <row AcceptedAnswerId="40579" AnswerCount="5" Body="&lt;p&gt;We have a small dataset (about 250 samples * 100 features) on which we want to build a binary classifier after selecting the best feature subset. Lets say that we partition the data into:&lt;/p&gt;&#10;&#10;&lt;p&gt;Training, Validation and Testing&lt;/p&gt;&#10;&#10;&lt;p&gt;For feature selection, we apply a wrapper model based on selecting features optimizing performance of classifiers X, Y and Z, separately. In this pre-processing step, we use training data for training the classifiers and validation data for evaluating every candidate feature subset.&lt;/p&gt;&#10;&#10;&lt;p&gt;At the end, we want to compare the different classifiers (X, Y and Z). Of course, we can use the testing part of the data to have a fair comparison and evaluation. However in my case, the testing data would be really small (around 10 to 20 samples) and thus, I want to apply cross-validation for evaluating the models.&lt;/p&gt;&#10;&#10;&lt;p&gt;The distribution of the positive and negative examples is highly ill-balanced (about 8:2). So, a cross-validation could miss-lead us in evaluating the performance. To overcome this, we plan to have the testing portion (10-20 samples) as a second comparison method and to validate the cross-validation.&lt;/p&gt;&#10;&#10;&lt;p&gt;In summary, we are partitioning data into training, validation and testing. Training and validation parts are to be used for feature selection. Then, cross-validation over the same data is to be applied to estimate the models. Finally, testing is used to validate the cross-validation given the imbalance of the data.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;The question is:&lt;/strong&gt; If we use the same data (training+validation) used in selecting the features optimizing the performance of classifiers X, Y and Z, can we apply cross-validation over the same data (training+validation) used for feature selection to measure the final performance and compare the classifiers?&lt;/p&gt;&#10;&#10;&lt;p&gt;I do not know if this setting could lead to a biased cross-validation measure and result in un-justified comparison or not.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-10-17T08:46:40.110" FavoriteCount="2" Id="40576" LastActivityDate="2014-10-22T02:15:58.317" LastEditDate="2012-10-17T10:10:07.687" LastEditorUserId="88" OwnerUserId="14888" PostTypeId="1" Score="5" Tags="&lt;machine-learning&gt;&lt;cross-validation&gt;&lt;feature-selection&gt;&lt;train&gt;" Title="Is using the same data for feature selection and cross-validation biased or not?" ViewCount="1593" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I started learning ridge regression in R. I applied the linear ridge regression to my full data set and got the following results. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;gridge&amp;lt;-lm.ridge(divorce ~., data=divusa, lambda=seq(0,35,0.02)) &#10;select(gridge) &#10;modified HKB estimator is 0.07693804 &#10;modified L-W estimator is 0.3088377 &#10;smallest value of GCV at 0.02 which.min(gridge$GCV) &#10;0.02 &#10;2 &#10;&#10;round(coef(gridge)[2,-1],3) &#10;year   unemployed femlab marriage birth military&#10;-0.195  -0.053    0.790    0.148 -0.118   -0.042      &#10;&#10;round(coef(g)[-1],3)&#10; year unemployed femlab marriage birth  military&#10;-0.203  -0.049   0.808    0.150 -0.117 -0.043 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Questions:&lt;/strong&gt;  &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;How do I interpret the results?  &lt;/li&gt;&#10;&lt;li&gt;Do I have to do anything else for interpretation?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2012-10-17T14:10:41.860" Id="40588" LastActivityDate="2013-11-18T16:03:25.460" LastEditDate="2012-10-17T14:38:10.630" LastEditorUserId="7290" OwnerUserId="15966" PostTypeId="1" Score="6" Tags="&lt;r&gt;&lt;interpretation&gt;&lt;ridge-regression&gt;" Title="How do you interpret the results from ridge regression?" ViewCount="790" />
  <row Body="&lt;p&gt;That's because you use 1000 points and sum them up. You can use &lt;code&gt;mean&lt;/code&gt; instead of &lt;code&gt;sum&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Qnoise1 = mean((dataset-quants1).^2);&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2012-10-17T14:33:17.133" Id="40590" LastActivityDate="2012-10-17T14:33:17.133" OwnerUserId="5025" ParentId="40587" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="40657" AnswerCount="1" Body="&lt;p&gt;I am trying to model annual tree nut production using climate predictors. &lt;/p&gt;&#10;&#10;&lt;p&gt;The nut data (dependent) is a binary timeseries (0,1 -  representing unsuccessful and successful nut production), with one observation per year, and with 90 years of data and two missing years (88 onservations). &lt;/p&gt;&#10;&#10;&lt;p&gt;The independent variables are monthly climate variables, including months in previous years (for example, Temp.July.t, Temp.July.t-1)&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm using R, and have an basic-intermediate knowledge of statistics.&lt;/p&gt;&#10;&#10;&lt;p&gt;My problem is that the dependent data has strong temporal autocorrelation (nut production cannot be successful two years running).  I'm looking for a pointer towards a technique that will allow me to deal with the autocorrelation in the binary data and create a statistical model that allows me to investigate the relationship between nut production and climate.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-10-17T14:55:58.073" Id="40592" LastActivityDate="2012-10-18T13:11:14.740" LastEditDate="2012-10-17T15:17:24.153" LastEditorUserId="88" OwnerUserId="16017" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;time-series&gt;&lt;autocorrelation&gt;&lt;binary-data&gt;" Title="How to model binary dependent data with temporal autocorrelation?" ViewCount="406" />
  <row Body="&lt;p&gt;&lt;strong&gt;To answer my own question:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Generalized estimating equations (GEE) seem to be a suitable solution for my problem, i.e. multiple features measured longitudinally. &#10;There is a Matlab toolbox &lt;a href=&quot;https://dbe.med.upenn.edu/biostat-research/SJRatcliffe&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; for GEE.&lt;/p&gt;&#10;&#10;&lt;p&gt;In response to posters above, I did not find a Matlab repeated measures ANOVA package that satifactorily handles longitudinal data, I think repeated measures ANOVA is better suited for a relatively small number of categorical factors rather than longitudinal data. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-17T16:08:26.003" Id="40596" LastActivityDate="2012-10-19T21:30:28.050" LastEditDate="2012-10-19T21:30:28.050" LastEditorUserId="11030" OwnerUserId="11030" ParentId="39211" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="40601" AnswerCount="2" Body="&lt;p&gt;I've been wanting to experiment with a neural network for a classification problem that I'm facing. I ran into papers that talk of RBMs. But from what I can understand, they are no different from having a multilayer neural network. Is this accurate?&lt;/p&gt;&#10;&#10;&lt;p&gt;Moreover I work with R and am not seeing any canned packages for RBMs. I did run into literature that talks about deep learning networks which are basically stacked RBMs but not sure if it is worth the effort to implement them in R.&#10;Would anybody have any pointers? Thanks&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-10-17T17:09:14.977" FavoriteCount="12" Id="40598" LastActivityDate="2014-12-27T23:58:29.530" LastEditDate="2012-10-17T23:36:05.423" LastEditorUserId="88" OwnerUserId="13516" PostTypeId="1" Score="16" Tags="&lt;r&gt;&lt;machine-learning&gt;&lt;classification&gt;&lt;neural-networks&gt;" Title="Restricted Boltzmann machines vs multilayer neural networks" ViewCount="4430" />
  <row AnswerCount="2" Body="&lt;p&gt;I am running a frequency count on the instances of powerless of language in the language of prosecuting attorneys and the same frequency count of the defendants language. &lt;/p&gt;&#10;&#10;&lt;p&gt;I am asking if the frequency of the powerless language by women makes a favorable verdict and less powerless language by men creates a favorable verdict. I have 400 cases/transcripts. &lt;/p&gt;&#10;&#10;&lt;p&gt;Below is my hypothesis:&#10;Is the use of powerless language used by defense attorneys in arguments independent of the verdict? &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;a. Is the use of powerless language used by female defense &#10;    attorneys in closing    arguments independent of the verdict? &#10;Ha) The use of powerless language by a female defense attorney is &#10;    not independent of the verdict.&#10;Ho) The use of powerless language by a female defense attorney is               &#10;    independent of the verdict   &#10;b. Is the use of powerless language used by male defense attorneys &#10;    in closing  arguments independent of the verdict? &#10;Ha) The use of powerless language by a male defense attorney is &#10;    not independent of the verdict.&#10;Ho) The use of powerless language by a male defense attorney is                 &#10;    independent of the verdict. &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Would I run two separate chi squares: one on all of the prosecuting attorneys and one on all the defending attorneys with the verdicts?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-10-17T17:20:00.737" Id="40599" LastActivityDate="2012-11-02T15:40:14.290" LastEditDate="2012-10-18T04:30:09.383" LastEditorUserId="183" OwnerUserId="16020" PostTypeId="1" Score="2" Tags="&lt;chi-squared&gt;" Title="To chi square or not to chi square?" ViewCount="291" />
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;In my statistics lectures it is mentioned that for balanced data (same number of participants in each block), the results will be the same whether the model is fitted with factor A as a fixed effect or a random effect. For example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lme(effort ~ Type, random = ~1|Subject)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;will give me the same answer as&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lm(effort~ Type + Subject)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;By contrast, it is mentioned that with unbalanced data, the results will not be the same.&lt;/p&gt;&#10;&#10;&lt;p&gt;What is meant by 'same answer'. If I run both models I get two different outputs. What is the same?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-10-17T23:31:10.497" Id="40621" LastActivityDate="2012-11-17T03:01:28.033" LastEditDate="2012-10-17T23:34:03.303" LastEditorUserId="88" OwnerUserId="16032" PostTypeId="1" Score="2" Tags="&lt;random-effects-model&gt;" Title="Random effects vs fixed effects with balanced designs" ViewCount="140" />
  <row AnswerCount="2" Body="&lt;p&gt;I generate an AR(1) process as follows:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x=arima.sim(list(order = c(1,0,0),ar=0.67),n=1000,sd=sqrt(0.55))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;When I square it, and fit AR(1) to the squared process, it seems still to be fitted well using AR(1) model:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;y=x^2&#10;fit=ar(y,order.max=1, method=&quot;ols&quot;)&#10;acf(fit$resid[2:1000])&#10;Box.test(fit$resid,lag=round(log(length(y))),type=&quot;Ljung-Box&quot;,fitdf=1)$p.value&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/JGBf6.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The ACF plot of the model (for the squared process) residuals and the large residual test p value (larger than .05) both show that the residuals are independently distributed. What does this indicate? &lt;/p&gt;&#10;&#10;&lt;p&gt;One simulation of time series $x$ and $y$ are as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/LzIzh.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-10-17T23:57:37.473" Id="40624" LastActivityDate="2013-01-18T03:49:30.637" LastEditDate="2012-10-18T22:43:20.000" LastEditorUserId="14666" OwnerUserId="14666" PostTypeId="1" Score="5" Tags="&lt;time-series&gt;&lt;residuals&gt;&lt;hypothesis-testing&gt;&lt;arma&gt;" Title="How to understand the square of an AR(1) process?" ViewCount="206" />
  
  
  <row AcceptedAnswerId="40681" AnswerCount="1" Body="&lt;p&gt;It is easy to compare &lt;em&gt;paired&lt;/em&gt; data, using paired t-test. But suppose this pairing is hierarchical. Below is an example:&lt;/p&gt;&#10;&#10;&lt;p&gt;The stem cells of two mouse strains S1 and S2 are cultured in two different culture conditions C1 and C2, having 3 replicates for each Strain/Condition (2 x 2 x 3 = 12 samples total). We have examined the expression level of N genes in all of these samples. We want to identify the genes which are significantly (p &amp;lt; 0.02) higher expressed in culture condition C1 in any strain.&lt;/p&gt;&#10;&#10;&lt;p&gt;The case was easy if we had no replicates: just pairing the identical samples in two conditions C1 and C2 and running a paired t-test. If we had replicates but no strains the problem was easier: just running a normal t-test on two population of cells cultured in C1 and C2.&lt;/p&gt;&#10;&#10;&lt;p&gt;But how to deal the case while our data is hierarchical?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-18T15:35:58.583" FavoriteCount="0" Id="40674" LastActivityDate="2012-10-19T12:23:47.253" LastEditDate="2012-10-18T19:44:01.533" LastEditorUserId="88" OwnerUserId="14699" PostTypeId="1" Score="2" Tags="&lt;statistical-significance&gt;&lt;t-test&gt;&lt;multilevel-analysis&gt;&lt;hypothesis-testing&gt;" Title="Testing hierarchical data" ViewCount="291" />
  
  
  
  <row Body="&lt;p&gt;BLUE stands for &lt;em&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem&quot; rel=&quot;nofollow&quot;&gt;Best Linear Unbiased Estimator&lt;/a&gt;&lt;/em&gt;.  The phrase comes from the Gauss-markov theorem that the OLS estimator is BLUE when the standard linear regression assumptions (i.e., the errors are independent and identically distributed as a Gaussian with constant variance).  The acronym can be broken down as follows:  &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;An &lt;a href=&quot;http://en.wikipedia.org/wiki/Estimator&quot; rel=&quot;nofollow&quot;&gt;estimator&lt;/a&gt; is a specified calculation used on sample data to estimate a quantity (e.g., a population parameter--see also &lt;a href=&quot;/questions/tagged/estimators&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;estimators&amp;#39;&quot; rel=&quot;tag&quot;&gt;estimators&lt;/a&gt;).  &lt;/li&gt;&#10;&lt;li&gt;&quot;&lt;a href=&quot;http://en.wikipedia.org/wiki/Bias_of_an_estimator&quot; rel=&quot;nofollow&quot;&gt;Unbiased&lt;/a&gt;&quot; means that the expected value of the estimator equals the parameter being estimated (conceptually, the sampling distribution of the resulting estimates is centered on the true value--see also &lt;a href=&quot;/questions/tagged/unbiased-estimator&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;unbiased-estimator&amp;#39;&quot; rel=&quot;tag&quot;&gt;unbiased-estimator&lt;/a&gt;, and &lt;a href=&quot;/questions/tagged/bias&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;bias&amp;#39;&quot; rel=&quot;tag&quot;&gt;bias&lt;/a&gt;).  &lt;/li&gt;&#10;&lt;li&gt;&quot;Linear&quot; refers to linear in the parameters.  &lt;/li&gt;&#10;&lt;li&gt;&quot;Best&quot; implies that the sampling distribution of the resulting estimates having the lowest variance of all possible unbiased linear estimators.  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2012-10-18T18:13:59.507" Id="40689" LastActivityDate="2014-10-31T19:18:09.800" LastEditDate="2014-10-31T19:18:09.800" LastEditorUserId="7290" OwnerUserId="-1" PostTypeId="5" Score="0" />
  <row Body="&lt;p&gt;This is known as a &quot;hard-core&quot; poisson point process - so named by Brian Ripley in the 1970s; i.e. you want it to be random, but you don't want any points to be too close together.  The &quot;hard-core&quot; can be imagined as a buffer zone around which other points cannot intrude.&lt;/p&gt;&#10;&#10;&lt;p&gt;Imagine you're recording the position of some cars in a city - but you're only recording the point at the nominal centre of the car.  While they're on the streets no two point pairs can come close together because the points are protect by the &quot;hard-core&quot; of the bodywork - we'll ignore the potential super-position in multi-storey car parks :-)&lt;/p&gt;&#10;&#10;&lt;p&gt;There are procedures for generating such point processes - one way is just to generate points uniformly and then remove any that are too close together!&lt;/p&gt;&#10;&#10;&lt;p&gt;For some detail on such processes, refer for example to &lt;a href=&quot;http://www.mathematik.uni-ulm.de/stochastik/aktuelles/sh06/sh_schmidt.pdf&quot; rel=&quot;nofollow&quot;&gt;this&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-18T18:17:38.130" Id="40691" LastActivityDate="2012-10-18T18:17:38.130" OwnerUserId="6579" ParentId="40384" PostTypeId="2" Score="0" />
  
  
  
  
  <row AnswerCount="2" Body="&lt;p&gt;The &lt;code&gt;bild&lt;/code&gt; package appears to be an excellent package for serial binary responses.  But it is for discrete time.  I would like to specify a smooth function of time for the odds ratio connection of the current response Y with binary responses measured at earlier times, or at least a first-order Markov version of this.  I believe this is called alternating logistic regression.  Does anyone know of an R package that handles continuous time, i.e., measurement times can be at any follow-up time?  I don't need random effects in the model. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-10-19T12:25:43.613" FavoriteCount="1" Id="40739" LastActivityDate="2014-11-28T18:38:01.220" OwnerUserId="4253" PostTypeId="1" Score="11" Tags="&lt;r&gt;&lt;repeated-measures&gt;&lt;binary-data&gt;&lt;longitudinal&gt;" Title="Is there an R package for continuous time longitudinal binary responses?" ViewCount="656" />
  <row AcceptedAnswerId="40746" AnswerCount="1" Body="&lt;p&gt;I am using earth package for the following data.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x &amp;lt;- c(127, 128, 255, 256, 511, 512, 600, 700, 800, 900, 1000, 1023, 1100,&#10;       1200, 1300, 1400, 1500, 1600, 2047, 2048, 2100, 2200, 2300, 2400, 2500,&#10;       2600, 2700, 2800, 3000, 3100, 3200, 3300, 3500, 4063, 4064, 4100, 4200,&#10;       5200, 5400)&#10;&#10;y &amp;lt;- c(0.59, 0.61, 0.59, 1.55, 1.33, 3.50, 1.00, 1.22, 2.50, 3.00, 3.79,&#10;       3.98, 4.33, 4.45, 4.59, 4.72, 4.82, 4.90, 4.96, 7.92, 5.01, 5.01,&#10;       4.94, 5.05, 5.04, 5.03, 5.06, 5.10, 5.04, 5.06, 7.77, 5.07, 5.08,&#10;       5.08, 5.12, 5.12, 5.08, 5.17, 5.18) &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;After building the model, &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;model&amp;lt;-earth(y~x)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I get following regression model. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;summary(model)&#10;Call: earth(x=x, y=y)&#10;coefficients&#10;(Intercept)  5.225822553&#10;h(1400-x)   -0.003820087&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Is there any possibility that I can increase somehow the number of knots or regression splines?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-10-19T10:34:03.350" Id="40745" LastActivityDate="2012-10-19T14:03:21.703" OwnerDisplayName="Shahzad" OwnerUserId="17057" PostTypeId="1" Score="2" Tags="&lt;r&gt;" Title="Adaptive regression splines in earth package R" ViewCount="1233" />
  
  
  
  <row Body="&lt;p&gt;&lt;strong&gt;On color&lt;/strong&gt;&#10;Generally, use dark background in a dark room; light background in a well-lit room or room with plenty of natural light. Once you’ve picked dark background, then use light color for fonts and graphical components; vice versa for light background.&#10;I found it useful to actually project a &lt;a href=&quot;https://www.google.com/search?q=color+wheel&amp;amp;hl=en&amp;amp;safe=off&amp;amp;client=firefox-a&amp;amp;hs=C7u&amp;amp;tbo=d&amp;amp;rls=org.mozilla%3aen-US%3aofficial&amp;amp;source=lnms&amp;amp;tbm=isch&amp;amp;sa=X&amp;amp;ei=gXGBUPr9DebV0QGH-YHACg&amp;amp;ved=0CAcQ_AUoAA&amp;amp;biw=1050&amp;amp;bih=1569&quot;&gt;color wheel&lt;/a&gt; onto the screen and check if there is any segment that is not distinguishable. I personally found yellow-red-brown spectrum usually fails, and I tried to avoid using them to show gradient data.&lt;/p&gt;&#10;&#10;&lt;p&gt;If a graph can do the job with just black and white, then keep it black and white. Blue themes are deemed the most color-blind-friendly, while red-green combination is a usual culprit to avoid. Brewer has made a &lt;a href=&quot;http://colorbrewer2.org/&quot;&gt;website&lt;/a&gt; for choosing map colors. There you can also pick themes that are colorblind safe and, more importantly, photocopy-able. Color charts are doomed if they lose information after photocopying. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Line/shape and font&lt;/strong&gt;&#10;Whenever I have a chance, I’ll go to the room that I’ll be presenting and use the beamer there to project some reference slides (see the attached image). You can make one for fonts, one for lines with different thickness, etc. My general impression is that sans serif works better when the on-screen resolution is low.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ThSWm.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Format&lt;/strong&gt;&#10;I use .png for simpler graph and illustration and .tif for maps. I have not encountered any problem so far. I like that these two formats can tolerate a good degree of manual enlargement. In case if the audience really wants to see the picture up close, I can still do that without it turning pixelated.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Print&lt;/strong&gt;&#10;For very complicated graphs, I also brought print outs. A trend that I have been seeing in lectures is that more and more attendees are loading up my presentation on their computer while listening to me. The benefits are that they can see the graphs up close, and they can also take notes directly on their computer. So, I would also recommend at the beginning and the end, provide a link to the crowd in case if they'd like to download your work.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&#10;Since the question isn't about how to make good graphs, I'll save all those Clevenland and Tufte materials. Although, I have to say that Tufte's principles on &lt;a href=&quot;http://www.infovis-wiki.net/index.php/Data-Ink_Ratio&quot;&gt;data-ink ratio&lt;/a&gt; have been very helpful, as long as you know when to stop following his advices.&lt;/p&gt;&#10;&#10;&lt;p&gt;For online presentation, I'd recommend &lt;a href=&quot;http://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Dstripbooks&amp;amp;field-keywords=Garr+Reynolds&amp;amp;rh=n%3A283155%2Ck%3AGarr+Reynolds&quot;&gt;books&lt;/a&gt; and &lt;a href=&quot;http://www.garrreynolds.com&quot;&gt;website&lt;/a&gt; by Garr Reynolds. His book &lt;em&gt;Presentation Zen&lt;/em&gt; is pretty useful.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-19T15:54:13.507" Id="40757" LastActivityDate="2012-10-19T16:12:00.543" LastEditDate="2012-10-19T16:12:00.543" LastEditorUserId="13047" OwnerUserId="13047" ParentId="40584" PostTypeId="2" Score="5" />
  
  
  
  
  <row AcceptedAnswerId="40787" AnswerCount="1" Body="&lt;p&gt;I'm trying to find a full example of how to plot learning curves.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I watched Andrew Ng's ML class on Coursera and he mentions using learning curves to diagnose variance-bias issues.  &lt;/p&gt;&#10;&#10;&lt;p&gt;My notes show the top line as the test set or cv error slopping downward as the number of training examples increases.  This makes sense to me.&lt;/p&gt;&#10;&#10;&lt;p&gt;My problem is on the lower line of the plot which is training error.  I'm not sure what this means and why the error would increase with more training examples.  Should the training error be multiplied by -1? Or is there something else I need to do?&lt;/p&gt;&#10;&#10;&lt;p&gt;Any working examples would be appreciated as would an R package that helps with this.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-10-19T20:13:52.923" Id="40778" LastActivityDate="2012-10-19T22:56:28.357" OwnerUserId="7411" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;variance&gt;&lt;bias&gt;" Title="Learning Curves Example" ViewCount="301" />
  
  
  <row AcceptedAnswerId="40803" AnswerCount="1" Body="&lt;p&gt;Suppose I have  massive data set (think Terabytes) is available to train a learning algorithm. Which one of the following conditions must be true to obtain good performance (low error rate)&lt;/p&gt;&#10;&#10;&lt;p&gt;a. Using a complex model with many parameters, including high order polynomial features&#10;of the original space  (e.g x^10)&lt;/p&gt;&#10;&#10;&lt;p&gt;b. Training with a small number of parameters&lt;/p&gt;&#10;&#10;&lt;p&gt;I am thinking about regularization by adding peanlty&#10;and Bias -variance tradeoff. &#10;Please suggest what is good way to go...&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-10-20T03:56:10.013" Id="40801" LastActivityDate="2012-10-20T19:02:29.113" OwnerUserId="16096" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;machine-learning&gt;&lt;regularization&gt;" Title="How to obtain good performance (low error rate) on massive data set?" ViewCount="103" />
  
  
  <row AcceptedAnswerId="40811" AnswerCount="4" Body="&lt;p&gt;I'm starting to want to advance my own skillset and I've always been fascinated by machine learning. However, six years ago instead of pursuing this I decided to take a completely unrelated degree to computer science. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have been developing software and applications for about 8-10 years now, so I have a good handle but I just can't seem to penetrate the maths side of machine learning/machine learning/probabilities/statistics. &lt;/p&gt;&#10;&#10;&lt;p&gt;I start looking at learning material and on the first page it might include something which confuses me and immediately sets up a barrier in my learning &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Is a strong background in maths a total requisite for ML? Should I try and fill in the blanks of my maths before continuing with ML? Can self learning really work for just a developer without any hard computer science background? &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;h3&gt;Related Questions&lt;/h3&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/18973/can-you-recommend-a-book-to-read-before-elements-of-statistical-learning&quot;&gt;Can you recommend a book to read before Elements of Statistical Learning?&lt;/a&gt;  &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="5" CreationDate="2012-10-20T10:44:44.513" FavoriteCount="6" Id="40808" LastActivityDate="2012-10-26T12:13:53.970" LastEditDate="2012-10-25T08:00:25.153" LastEditorUserId="264" OwnerUserId="16101" PostTypeId="1" Score="15" Tags="&lt;machine-learning&gt;&lt;references&gt;&lt;learning&gt;" Title="Is a strong background in maths a total requisite for ML?" ViewCount="4276" />
  
  
  
  <row Body="ARMA is an acronym for auto regressive moving average; a stochastic process modelling time series. It adds moving average terms to the AR model." CommentCount="0" CreationDate="2012-10-20T22:56:36.543" Id="40832" LastActivityDate="2013-08-31T10:56:58.023" LastEditDate="2013-08-31T10:56:58.023" LastEditorUserId="27581" OwnerUserId="686" PostTypeId="4" Score="0" />
  
  <row Body="&lt;p&gt;I think you should use logistic regression with verdict as the dependent variable and sex of attorney and powerless language as independent variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;First, you need to consider how to operationalize 'powerless language'. You say you have a frequency count, but it might be better to scale this by the amount of language. Trials last very different times, so perhaps your measure should be &quot;powerless statements per hour&quot; or some such. Call this variable PL (you can call it whatever you'd like).&lt;/p&gt;&#10;&#10;&lt;p&gt;All your hypotheses are about language of the defense attorney, which simplifies things a bit. &lt;/p&gt;&#10;&#10;&lt;p&gt;The next independent variable is sex of the defense attorney. Code this, e.g., 1 for female and 0 for male.&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose your dependent variable is labeled &quot;V&quot; and can be G or NG (guilty or not guilty).&lt;/p&gt;&#10;&#10;&lt;p&gt;Then your model is &lt;/p&gt;&#10;&#10;&lt;p&gt;P(G) ~ PL + S + PL*S&lt;/p&gt;&#10;&#10;&lt;p&gt;where ~ means &quot;is related to&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;Then you use logistic regression. &lt;/p&gt;&#10;&#10;&lt;p&gt;In R&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;m1 &amp;lt;- glm(V~PL + Sex + PL*Sex, family = 'binomial')&#10;summary(m1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In SAS&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;proc logistic data = mydata;&#10; class sex;&#10; model V = PL|Sex;&#10;run;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="3" CreationDate="2012-10-20T23:18:55.847" Id="40837" LastActivityDate="2012-10-21T01:03:05.233" LastEditDate="2012-10-21T01:03:05.233" LastEditorUserId="686" OwnerUserId="686" ParentId="40599" PostTypeId="2" Score="1" />
  <row Body="" CommentCount="0" CreationDate="2012-10-21T01:17:45.010" Id="40842" LastActivityDate="2012-10-21T01:17:45.010" LastEditDate="2012-10-21T01:17:45.010" LastEditorUserId="-1" OwnerUserId="-1" PostTypeId="5" Score="0" />
  <row AnswerCount="1" Body="&lt;p&gt;I'm looking for some advice about a problem I've been assigned to solve. This is the problem. Suppose a car has to travel from P1 to P4, with intermediate points P2, P3. Say $X1$ is a r.v. that defines the travel time from P1 to P2 is normally distributed with $\mu$ = 25, $\sigma^2$ = 3, similarly $X2$ is a r.v. for the travel time from P2 to P3 and $X2 \sim N(15,3)$, finally X3 is a r,v, for the travel time from P3 to P4 and  $X3 \sim N(20,1)$ . Then define a random vector $(T2,T3,T4)$ with the times when the car passes by P2,P3 and P4.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have to define the distribution of the random vector ${\bf t}=[T2,T3,T4]$ through its covariance matrix and its $\mu$ vector of expectations. Here is what I´ve done so far:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;$T2$ would be distributed just as $X1$. $T3$ would be defined as $X1+X2$, and $T4$ would be defined as $X1+X2+X3$. Then $T3$ would also be gaussian distributed since the sum of gaussians is a gaussian, and the distribution would be $N(25+15,3+3)=N(40,6)$. Following a similar reasoning, $T4 \sim N(25+15+20,3+3+1)=N(60,7)$. Then, if I define a vector ${\bf u}=[U_1,U_2,U_3]$ where $U_i \sim N(0,1)$, then ${\bf Au+\mu}$ should define the vector ${\bf t}$. If we define ${\bf A}$ as:&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;$$&#10;\begin{bmatrix}&#10;\sqrt3 &amp;amp;  0  &amp;amp; 0\\&#10;0  &amp;amp;  \sqrt6 &amp;amp; 0\\&#10;0  &amp;amp;   0  &amp;amp; \sqrt7&#10;\end{bmatrix}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and $\mu$ as $[15,40,60]$, then ${\bf Au+\mu}$ would define the distribution of vector ${\bf t}$, and the covariance matrix would be ${\bf \Sigma = AA^t}$. Is it correct? I am not sure because I think that somehow $T2$ would affect $T3$ and $T4$ and therefore $Cov(T2,T3), Cov(T3,T4)$ would not be zero.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also I have to find the probability that the travel time from $P1$ to $P4$ is greater than 70. I think this would be solved as $P(T4&amp;gt;70)$, since $T4 \sim N(60,7)$,if this is true I just simply have to normalize and take a look and the normal distribution table or go to R and type pnorm(70,60,7). Do you think it is correct?&lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, I have to estimate the moment when the car passed by P3, given that it is known that it arrived at P4 in 65.234 time units. Also, if I know that it passed by P2 in 26.1 time units, I have to estimate the moment when the car passed by P3. In this point I'm lost and don't know exactly what to do.&lt;/p&gt;&#10;&#10;&lt;p&gt;So this is it, sorry for the looong post. Thanks for reading. Any comments and suggestions are deeply apreciated.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-10-21T05:56:16.717" FavoriteCount="1" Id="40854" LastActivityDate="2013-11-15T23:03:12.077" LastEditDate="2012-10-21T10:53:14.487" LastEditorUserId="88" OwnerUserId="16112" PostTypeId="1" Score="3" Tags="&lt;self-study&gt;&lt;normal-distribution&gt;&lt;covariance&gt;" Title="Obtaining the covariance matrix of a Gaussian random vector" ViewCount="374" />
  
  <row AnswerCount="2" Body="&lt;p&gt;Not sure if this is a valid question.&lt;/p&gt;&#10;&#10;&lt;p&gt;I was wondering if there is are good answers to this question:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Under what conditions is the power curve not smooth or continuous in Hypothesis testing?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Thanks.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-21T20:25:50.767" Id="40899" LastActivityDate="2012-10-22T04:56:19.297" OwnerUserId="14163" PostTypeId="1" Score="2" Tags="&lt;hypothesis-testing&gt;&lt;power&gt;&lt;type-i-errors&gt;&lt;type-ii-errors&gt;" Title="Conditions under which the power curve is not smooth or continuous in Hypothesis testing" ViewCount="70" />
  
  
  
  
  
  <row AcceptedAnswerId="41854" AnswerCount="5" Body="&lt;p&gt;I have two different measuring instruments, A and B, both measure the same physical quantity but with different unit of measures: $u_A$ and $u_B$.&lt;/p&gt;&#10;&#10;&lt;p&gt;A is a reference instrument.&lt;/p&gt;&#10;&#10;&lt;p&gt;I measured a reference part $L$ $n$ times  with A and I get the $n$ values $L_{Ai}$ ($i=1 \dots n$) expressed in term of the unit of measure $u_A$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then I measure the same reference part, $L$, $m$ times with B and I get the $m$ values $L_{Bj}$ ($j=1 \dots m$) expressed in term of the unit of measure $u_B$.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the future I will make my measures with B but I will be interested in the measure expressed in term of the unit of measure $u_A$.&lt;/p&gt;&#10;&#10;&lt;p&gt;I assume I can convert $u_B$ into $u_A$ by means of just one multiplicative conversion factor $k$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, I have three questions:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Is it possible to assess the validity of the above assumption starting from the values $L_{Ai}$ and $L_{Bj}$?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;If the assumption is valid, how can I compute the conversion factor $k$ to convert the measure from $u_B$ to $u_A$, i.e. $L_A=k L_B$?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;How to manage the case where I have more than one part, i.e. $L_1$, $L_2$, etc.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;My first attempt is to assume the assumption as valid and then compute $k$ as $k=\frac{m\sum_{i=1}^n LA_i}{n\sum_{j=1}^m LB_i}$ but it is based more on &quot;common sense&quot; rather than on some proper statistical basis.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can you give me some hints about the part of statistics that covers these kind of problem? Maybe linear regression?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-10-22T13:56:27.667" FavoriteCount="1" Id="40956" LastActivityDate="2012-11-04T19:25:45.987" LastEditDate="2012-10-28T12:42:03.597" LastEditorUserId="686" OwnerUserId="5221" PostTypeId="1" Score="3" Tags="&lt;regression&gt;&lt;repeated-measures&gt;&lt;linear-model&gt;&lt;measurement&gt;&lt;calibration&gt;" Title="Conversion between units of measurement" ViewCount="513" />
  <row AcceptedAnswerId="40966" AnswerCount="1" Body="&lt;p&gt;I'm using libSVM for a binary classification problem. After a test instance is assigned a label (1 or -1), I also want to know how likely it is assigned such a label. &lt;/p&gt;&#10;&#10;&lt;p&gt;I'm thinking about calculating the distance from the instance to the hyperplane in the feature space. The larger the distance is, the more likely the label assignment is correct. &lt;/p&gt;&#10;&#10;&lt;p&gt;Is my idea correct? Or is there already such an option in libSVM for my purpose? &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-10-22T15:53:56.520" Id="40965" LastActivityDate="2012-10-22T16:08:26.913" OwnerUserId="7802" PostTypeId="1" Score="1" Tags="&lt;svm&gt;&lt;libsvm&gt;" Title="probablistic output for binary SVM classification" ViewCount="820" />
  
  
  <row AcceptedAnswerId="40973" AnswerCount="1" Body="&lt;p&gt;Isn't this test for the determination of auto-correlation of residuals only necessary when time is some sort of a factor in the observed variables?&lt;/p&gt;&#10;&#10;&lt;p&gt;As it is I had a data-set that had one dependent variable (sales) and three independent variables, (unemployment rate, population size and advertizing expense). I find it difficult to imagine how the residuals in this situation can be serially-related without a sequence of observations being specified.&lt;/p&gt;&#10;&#10;&lt;p&gt;My group-mate was unable to explain this to me as I was very skeptical that the Durbin Watson test belonged in our report.&lt;/p&gt;&#10;&#10;&lt;p&gt;Please help me understand&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-22T17:33:19.127" FavoriteCount="1" Id="40972" LastActivityDate="2012-10-22T18:14:18.733" OwnerUserId="16150" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;correlation&gt;&lt;multiple-regression&gt;&lt;linear-model&gt;" Title="When do we use the Durbin-Watson test?" ViewCount="1154" />
  
  <row AnswerCount="3" Body="&lt;p&gt;I would like to create a composite scale using items that measure parent communication extracted from a UK panel survey (n=941). However although all 6 variables in question measure similar constructs - 2 of the items are dichotomous (yes/no) the other 4 are likert (1,2,3,4). My intention is to transform the 6 variables to z scores and then sum them to form the composite score which will be used in a path analysis.  I need to know if it is correct to do this, i.e., merge dichotomous variable (yes/No) and Likert scale (1,2,3,4) after they have been standardized to z scores.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Pending this being ok, would it be necessary to do an exploratory factor analysis to ensure the the composite scale measures one underlying construct or it is enough to do a reliability analysis?&lt;/p&gt;&#10;&#10;&lt;p&gt;This analysis is forming part of my PhD so I need to defend what I am doing and I would appreciate any guidance anyone can give me. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-22T18:16:25.670" Id="40976" LastActivityDate="2014-02-18T03:37:34.920" LastEditDate="2012-11-03T08:41:02.870" LastEditorUserId="930" OwnerUserId="16152" PostTypeId="1" Score="3" Tags="&lt;likert&gt;&lt;scales&gt;" Title="Can a dichotomous variable (yes/no) be merged with a Likert measure (1,2,3,4) using z scores?" ViewCount="1800" />
  <row AnswerCount="0" Body="&lt;p&gt;I am  constructing several degree day models using linear and non linear models. I need to compare my linear models to each other to see if they significantly differ. I also need to compare my non-linear models to see if they significantly differ. Further, I also need to see if my linear vs non-linear models differ. How do I test this?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-10-22T20:35:55.343" Id="40981" LastActivityDate="2012-10-22T21:08:10.397" OwnerUserId="16155" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;hypothesis-testing&gt;" Title="Are my lines significantly different?" ViewCount="171" />
  <row AnswerCount="1" Body="&lt;p&gt;Suppose that $(X_1, Y_1)$ and $(X_2, Y_2)$ are independent and have the same joint distribution $F_{X,Y}$, which is a known copula $C_{X,Y}(F(X), F(Y))$. Also, suppose that $V = X_1 + X_2$ and $W = Y_1 + Y_2$.&lt;/p&gt;&#10;&#10;&lt;p&gt;It would be great if someone could point me to a procedure for computing the copula $C_{V,W}(F(V), F(W))$. &lt;/p&gt;&#10;&#10;&lt;p&gt;If not, is there is some other way (beside direct simulation) to compute the joint distribution $F_{VW}$?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT: Added independence assumption&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-10-22T21:39:50.417" Id="40986" LastActivityDate="2013-08-27T00:15:08.883" LastEditDate="2013-06-27T20:43:57.133" LastEditorUserId="22468" OwnerUserId="16157" PostTypeId="1" Score="5" Tags="&lt;probability&gt;&lt;correlation&gt;&lt;estimation&gt;&lt;copula&gt;&lt;joint-distribution&gt;" Title="Joint distribution of two sums of correlated variables" ViewCount="246" />
  <row AcceptedAnswerId="44808" AnswerCount="1" Body="&lt;p&gt;I have an interesting question, with its original application in finance. Suppose I have a stock return $Y$, and a set of independent variables (other tradable assets) $X$. Typically, one &lt;em&gt;hedges&lt;/em&gt; Y with X by finding the least squares solution to regressing $Y$ on $X$. Thus $Y - X\beta$ is approximately zero.&lt;/p&gt;&#10;&#10;&lt;p&gt;But what if we instead decompose $Y = Y^{+} + Y^{-}$, where $Y^{+}$ is the vector of &lt;em&gt;positive&lt;/em&gt; components of $Y$ and zero otherwise, likewise for $Y^{-}$. If we then regress $Y^{-}$ onto $X$, then $Y - X\beta_{neg}$ only hedges the negative values, and exposes us to gains from positive values of $Y$ as &lt;/p&gt;&#10;&#10;&lt;p&gt;$$Y - X\beta_{neg}$$&#10;$$ = Y^{+} + ( Y^{-} - X\beta_{neg} )$$ &#10;$$ \approx Y^{+} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;This sounds great to hedge downside risk, but it doesn't really work in practice unfortunately. Any advice?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-10-23T13:20:04.813" FavoriteCount="1" Id="41023" LastActivityDate="2012-11-30T21:30:02.190" OwnerUserId="11867" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;finance&gt;" Title="Regressing on only the positive part of a vector" ViewCount="80" />
  <row Body="&lt;p&gt;Nick is right, a Beta distribution seems like the natural choice here. In principle, you can have any proposal distribution you like. As long as the proposal distribution covers the range of the parameter distribution that you wish to sample from, then MCMC is guaranteed to converge on the right distribution. This means, however, that the Dirichlet distribution is NOT appropriate, because it does not cover all possible sets of parameters; namely it does not cover those that do not sum to one. &lt;/p&gt;&#10;&#10;&lt;p&gt;In practice, the proposal distribution can have a big influence on how fast you reach convergence, so you should try to pick one that proposes 'good' parameter values (near the mode of the posterior distribution) more frequently. You could adapt the Beta distribution to reflect any knowledge you may have about where the mode of the true parameter distribution may lie. If you have no prior knowledge, then you might just as well propose from a uniform distribution over the interval [0,1].&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-23T15:03:32.797" Id="41035" LastActivityDate="2012-10-23T15:03:32.797" OwnerUserId="9782" ParentId="28984" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Mayhaps Wasserman's &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/1441923225&quot;&gt;All of Statistics&lt;/a&gt; would be of interest. You can sample the book from the link given - and just the first few paragraphs of the preface make a hard sale to your market - and you can likely download the book free through Springer if you are associated with a university.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; Oops, didn't notice how ancient this thread was.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-10-23T16:49:52.460" Id="41042" LastActivityDate="2012-10-23T16:49:52.460" OwnerUserId="5339" ParentId="18973" PostTypeId="2" Score="7" />
  <row Body="&lt;p&gt;There is a free introductory textbook at &lt;a href=&quot;http://www.openintro.org/stat/index.php&quot; rel=&quot;nofollow&quot;&gt;openintro&lt;/a&gt; that also has sample datasets and a library of homework/quiz questions.  That would be a place to start.  I don't know if it will get into as advanced topics as what you want.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-23T17:04:07.360" Id="41044" LastActivityDate="2012-10-23T17:04:07.360" OwnerUserId="4505" ParentId="41032" PostTypeId="2" Score="2" />
  
  
  
  <row AnswerCount="0" Body="&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;strong&gt;Possible Duplicate:&lt;/strong&gt;&lt;br&gt;&#10;  &lt;a href=&quot;http://stats.stackexchange.com/questions/41038/conditional-expectation-estimator-confusion&quot;&gt;Conditional Expectation / Estimator Confusion&lt;/a&gt;  &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&#10;&#10;&lt;p&gt;Let $X_1, X_2, X_3 \sim N(0, d^2)$ and $T = X_1^2 + X_2^2 + X_3^2.$ &lt;/p&gt;&#10;&#10;&lt;p&gt;I have an estimator for $d$, $$\hat{d} = \frac{\sqrt{T\ 2\pi}}{4},$$&#10;and another estimator for $d$, $$\tilde{d} = \frac{1}{3} \sqrt{\pi / 2}\ \left(|X_1| + |X_2| + |X_3|\right).$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I need to show that $\mathbb{E}(\tilde{d} | T) = \hat{d}$ and that $\mathbb{E}(|X_1| | T) = \frac{1}{2} \sqrt{T}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm mostly confused about how to proceed with the algebra to compute the expectations.  I have never had to compute E[estimator | something] before&lt;/p&gt;&#10;" ClosedDate="2012-10-23T20:56:02.867" CommentCount="0" CreationDate="2012-10-23T18:41:02.540" FavoriteCount="0" Id="41058" LastActivityDate="2012-10-23T20:28:27.790" OwnerUserId="16185" PostTypeId="1" Score="1" Tags="&lt;conditional-expectation&gt;" Title="Computing conditional expectations" ViewCount="50" />
  <row Body="&lt;p&gt;Just think about the differences between the different confidence intervals you could compute.  We are 1% confident that the true mean is in the area inside of a 96% confidence interval but outside of the 95% confidence interval (provided we used similar methods and the 95% interval is fully contained in the 96% interval), and so forth.  This of course assumes that all the conditions/assumptions for the interval hold (no black swans).&lt;/p&gt;&#10;&#10;&lt;p&gt;You can demonstrate this for yourself and the requestor by simulating a large number of samples from a known distribution, calculating the intervals, and seeing how far away they are when they do not contain the true value.  There are many tools that do theses simulations for teaching purposes (one such is in the TeachingDemos package for R).&lt;/p&gt;&#10;&#10;&lt;p&gt;Whether the interval is more likely to lie to the left or right of the true value depends on how the interval was constructed.  If it is an equal tail interval then either side is equally likely.  If the tails are not equal, such as for minimum length intervals, then the probabilities were part of the calculation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Be careful of the phrasing you use to explain intervals, your explanation above could be interpreted as the true value is constantly changing and 95% of the time it is in the interval that you calculated.  Generally we consider the true parameter value to be fixed and it is the interval that would change from sample to sample.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-10-23T19:01:29.090" Id="41059" LastActivityDate="2012-10-23T19:01:29.090" OwnerUserId="4505" ParentId="41056" PostTypeId="2" Score="3" />
  <row AnswerCount="0" Body="&lt;p&gt;I have been wading through the many discussions on outliers on this site but I am still unfortunately having difficulty determining what to do with my data set.&lt;/p&gt;&#10;&#10;&lt;p&gt;My study consists of a simple pre-post test setup whereby i conduct 6 tests prior to and 6 tests following my treatment. The purpose of the repeated tests is to decrease the effect of anomalous performance on the results and increase the sensitivity. We therefore expect that generally 1 or 2 of these 6 tests to be a bit different to the other 4. At present I've been removing abnormal scores by identifying those results that lie outside +/- 1 standard deviation. This practice has proven somewhat satisfactory (in that it detects most of those scores that upon visible inspection appear anomalous), however I read more and more that the use of standard deviation is not appropriate for determining outliers, and sometimes I've found that the scores deemed as outliers are not always those that appear the most anomalous. &lt;/p&gt;&#10;&#10;&lt;p&gt;Therefore I'm wondering whether you can offer any suggestions as to how i can improve/better standardise my selection process? Is my current method acceptable? If so, do you know of any published material that has validated this approach (or does this approach have a name that i could google?) Alternatively, would the use of an interquartile range approach using the median value prove more satisfactory?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-10-23T20:09:07.323" Id="41065" LastActivityDate="2012-10-23T20:09:07.323" OwnerUserId="16197" PostTypeId="1" Score="1" Tags="&lt;repeated-measures&gt;&lt;outliers&gt;" Title="Standardising the removal of outliers from a small data set" ViewCount="753" />
  
  <row Body="&lt;p&gt;It is an interesting twist to find a case where the demographic data will be seen as less reliable as the behavioral data. There isn't much good advice on how to select the calibration variables, other than they should correlate with both the (non-)response process and the variables of interest. The reasoning behind the widespread use of demographic variables for calibration is that age, education, race and gender affect pretty much everything in any social science. You can make a very simple case with your data, however, by modeling the probability of response as a function of all the variables that you think about including -- a propensity model, if you like. If you can demonstrate that donations are more significant in your model than age, nobody would have the grounds to object to your use of the former in calibration.&lt;/p&gt;&#10;&#10;&lt;p&gt;The question of how much calibration is enough has not been addressed much, either. I can think about this conceptually as a trade-off between improving the accuracy (which, for a given response variable $y$ and a set of calibration variables $\bf x$, is the variance of the residuals $e_i = y_i - {\bf x}_i' {\bf b}$) and the increase in the variability of the weights, and hence the design effect $1+{\rm CV}^2$. As you add predictors that decrease in their strength, the precision gains are diminishing; the CV though will continue increasing, so at some point, arguably, the two curves will meet, giving you the right number of calibration variables. That's just an idea, but may be I should write a paper about it :)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-23T23:16:50.167" Id="41075" LastActivityDate="2012-10-23T23:16:50.167" OwnerUserId="5739" ParentId="41057" PostTypeId="2" Score="2" />
  
  <row AcceptedAnswerId="41153" AnswerCount="2" Body="&lt;p&gt;I have two columns of data, and would like to show their differences through the visualization approach. The current issue is that these two columns are in-fact very close to each other. In other words, I would like to zoom in the minor differences that are existed using a graphical way. Are there any suggestions? Thanks.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is part of the data set.  The third column is the difference between the first column and second column.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;0.999998295 0.999998331 -3.56E-08&#10;0.99999381  0.999986678 7.13E-06&#10;0.999997682 0.999998494 -8.12E-07&#10;0.999989429 0.999989062 3.67E-07&#10;0.999998877 0.999998505 3.72E-07&#10;0.999995327 0.999996709 -1.38E-06&#10;0.999995779 0.999995146 6.33E-07&#10;0.999997484 0.999996701 7.83E-07&#10;0.999998829 0.999998076 7.53E-07&#10;0.999997523 0.999998836 -1.31E-06&#10;0.999998287 0.999996964 1.32E-06&#10;0.999983543 0.999989135 -5.59E-06&#10;0.999992705 0.999994249 -1.54E-06&#10;0.999995663 0.999998168 -2.50E-06&#10;0.999997263 0.999999195 -1.93E-06&#10;0.999994474 0.999996713 -2.24E-06&#10;0.999993058 0.999994063 -1.00E-06&#10;0.999998295 0.999996262 2.03E-06&#10;0.99999736  0.999998185 -8.25E-07&#10;0.999983507 0.996875105 0.003108402&#10;0.999996339 0.999996597 -2.58E-07&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2012-10-24T02:54:40.187" Id="41081" LastActivityDate="2012-10-25T12:33:27.583" LastEditDate="2012-10-24T10:23:59.590" LastEditorUserId="88" OwnerUserId="3269" PostTypeId="1" Score="1" Tags="&lt;data-visualization&gt;&lt;multivariate-analysis&gt;&lt;dataset&gt;&lt;data-mining&gt;&lt;descriptive-statistics&gt;" Title="Visualizing two variables which have a very similar values" ViewCount="180" />
  
  <row Body="&lt;p&gt;As @Bitwise mentions, feature selection or feature extraction is a huge area of research in itself and there are countless ways to do it.&lt;/p&gt;&#10;&#10;&lt;p&gt;The other answers are all valid in my opinion, but in the end, you will probably do like most, and pick the method that is the most intuitive for you and that you understand the best. I would still add two possible options.&lt;/p&gt;&#10;&#10;&lt;p&gt;Multiple regression is probably the oldest technique. The idea is to fit a model to describe the response from the predictors and keep only the predictors that have a large impact on the response (a large coefficient of proportionality). Here you would probably have to recode the absence of D, E, F and G as &lt;code&gt;D=0&lt;/code&gt;, &lt;code&gt;E=0&lt;/code&gt;, &lt;code&gt;F=0&lt;/code&gt;, G&lt;code&gt;=0&lt;/code&gt; or something like that.&lt;/p&gt;&#10;&#10;&lt;p&gt;Another technique that never gained the popularity that it deserves is co-inertia analysis (a variant of &lt;a href=&quot;http://en.wikipedia.org/wiki/Canonical_analysis&quot; rel=&quot;nofollow&quot;&gt;canonical analysis&lt;/a&gt;). There is no implementation of it, as far as I know, and you would have to start from scratch (&lt;a href=&quot;http://pbil.univ-lyon1.fr/R_old/articles/arti113.pdf&quot; rel=&quot;nofollow&quot;&gt;there&lt;/a&gt; for example). It is a linear method that finds the best linear combination of features that matches your outcome(s). This &lt;a href=&quot;http://blog.thegrandlocus.com/2012/07/The-geometry-of-style&quot; rel=&quot;nofollow&quot;&gt;blog post&lt;/a&gt; shows an example of how it can be used.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-24T16:28:59.800" Id="41131" LastActivityDate="2012-10-24T16:28:59.800" OwnerUserId="10849" ParentId="38831" PostTypeId="2" Score="1" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Suppose we have given a two $95 \%$ confidence intervals for $X_1$ and $X_2$. They are normally distributed. From this how would we get a $95 \%$ confidence interval for $X_{1}/X_{2}$?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-10-24T20:15:59.277" FavoriteCount="1" Id="41141" LastActivityDate="2012-10-25T04:39:59.617" OwnerUserId="16226" PostTypeId="1" Score="-2" Tags="&lt;confidence-interval&gt;" Title="Given Two 95% Confidence Intervals" ViewCount="108" />
  
  
  <row Body="&lt;p&gt;First of all, there's no sense to say that a distribution is independent of another distribution. You should say that the pair of random variables $(X,Y)$ is independent of the random variable $Z$. This is equivalent to say that the random variable $f(X,Y)$ is independent of the random variable $Z$ for every Borelian function $f$. In particular $X$ and $Z$ are independent.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-25T09:59:53.203" Id="41178" LastActivityDate="2012-10-25T09:59:53.203" OwnerUserId="8402" ParentId="41166" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;As @mbq mentions, using a log scale might work, and is possibly the commonest way of including items of very different scales on the same chart. (The charts below actually plot $log (1 - x)$ where the $x$s are the values in the first two columns, with the axis labels altered to match the original scale.) &lt;/p&gt;&#10;&#10;&lt;p&gt;But just bear in mind how misleading these might be for an audience not used to seeing them - they emphasise differences in values approaching 1 and de-emphasise differences in smaller values. (I imagine that's why @xan recommends two separate charts.)&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/gxqcV.png&quot; alt=&quot;line chart&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/Dthxz.png&quot; alt=&quot;scatter chart&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-25T12:33:27.583" Id="41185" LastActivityDate="2012-10-25T12:33:27.583" OwnerUserId="13348" ParentId="41081" PostTypeId="2" Score="1" />
  <row AnswerCount="2" Body="&lt;p&gt;My dependent variable is continuous, non-normal (skewed left according to Shapiro-Wilk test). I have two independent variables (treatment group by colour, food type). There are 3 levels within each independent variable. The number of observations for each independent variables are not equal.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have looked up non-parametric tests such as Friedman's Test and Scheirer-Ray-Hare Test, neither of which seem suitable (due to unequal number of observations).&lt;/p&gt;&#10;&#10;&lt;p&gt;Are there alternative tests that anyone could suggest? I am using SAS.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-25T17:36:18.097" Id="41199" LastActivityDate="2013-07-25T04:36:24.227" LastEditDate="2012-10-25T21:57:18.937" LastEditorUserId="930" OwnerUserId="13487" PostTypeId="1" Score="6" Tags="&lt;anova&gt;&lt;nonparametric&gt;&lt;sas&gt;" Title="Non-parametric for two-way ANOVA (3x3)" ViewCount="3042" />
  
  <row Body="&lt;p&gt;The most robust way to do backward selection using a given model, would be to remove one predictor, fit the model to the the remaining predictors and evaluate (in cross-validation of course). Repeat for all predictors and remove the one which has the smallest contribution. Then iterate the process until you achieve a combination of predictor number/performance that satisfies you. If you have 18 predictors, it would take 18+17+16+15+..+(k-1) model fittings/evaluations to find a model with k predictors.&lt;/p&gt;&#10;&#10;&lt;p&gt;If this your model fitting/evaluation is too expensive in term of resources, you can try any kind of predictor evaluation instead, you can use one of several possible measures (e.g. statistical tests/correlation/information gain) and use the same process of fitting and evaluating. You don't care about a threshold because at each iteration you remove the one with the lowest contribution regardless of the actual value. The true evaluation will be on what you are trying to predict (CV etc.).&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-10-26T00:18:37.607" Id="41219" LastActivityDate="2012-10-26T00:18:37.607" OwnerUserId="14434" ParentId="41217" PostTypeId="2" Score="-1" />
  
  <row Body="&lt;p&gt;The fact that this is not easy in R is a feature rather than a bug or deficiency representing advances in the science.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example after the first step of a stepwise regression the t or f statistics computed conditional on that first step are no longer distributed according to the t or f distributions and therefore any specified alpha levels will be wrong.&lt;/p&gt;&#10;&#10;&lt;p&gt;Further, stepwise regression is known to give coefficient estimates that are biased away from 0 and tests on those coefficients that are generally meaningless.  Sometimes stepwise regression becomes essentially an inneficient method for selecting a random subset of predictors (rather than a meaningful one).  These days penalized methods such as lasso or ridge regression among others are preferred to stepwise methods.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Even better is to not do model selection if not needed.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-10-26T03:36:54.587" Id="41229" LastActivityDate="2012-10-26T03:36:54.587" OwnerUserId="4505" ParentId="41221" PostTypeId="2" Score="4" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am choosing summary statistics for percentage data that is clustered near 100%. Each subject takes the same test twice. Each test consists of at least 40 questions. I want to be able to predict which subjects will improve more. I have a hypothesis that subjects with higher variability of performance in the first test will improve more. To test this, I computed the variance of each subject's per-question scores, and indeed those with higher variance improved more. But the problem is that the presence of the threshold introduces an artefact: subjects with near-100% performance to start with can't improve much and also may have lower variance simply because the upper tail of question scores is limited by the maximum score threshold. Inter-quartile range would suffer from the same problem. &#10;I will try &quot;one-sided variance&quot; (using only question scores lower than the mean), and &quot;lower quartile to median range&quot;. Any other ideas, and does anyone know of a use in the literature of something like this?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-10-26T16:20:57.797" Id="41252" LastActivityDate="2012-10-26T16:20:57.797" OwnerUserId="16283" PostTypeId="1" Score="1" Tags="&lt;variance&gt;&lt;threshold&gt;" Title="What's a good measure of spread near a threshold?" ViewCount="99" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have  two dimensional spatial (x,y - coordinates of meteo stations) data for small region (so I could neglect the shape of earth globe), for each (x,y) I have one observation of wind direction and speed I need to design metric for clustering problem of such data. It's simple to do when we consider only directions or only the speed of wind but how to combine these quantities ? I try to analyse this problem using R.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-10-26T18:31:58.797" Id="41268" LastActivityDate="2012-10-27T10:57:08.650" LastEditDate="2012-10-27T10:57:08.650" LastEditorUserId="88" OwnerUserId="4908" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;clustering&gt;&lt;spatial&gt;&lt;metric&gt;" Title="Which metric use for problem of clustering spatial data of wind direction and speed?" ViewCount="106" />
  
  
  <row Body="" CommentCount="0" CreationDate="2012-10-27T16:24:55.520" Id="41310" LastActivityDate="2012-10-27T16:24:55.520" LastEditDate="2012-10-27T16:24:55.520" LastEditorUserId="-1" OwnerUserId="-1" PostTypeId="5" Score="0" />
  
  
  <row Body="&lt;p&gt;Brief answer:&#10;Found this website which is looking at from simple statistical point of view, with poisson distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://users.stat.umn.edu/~geyer/lottery/&quot; rel=&quot;nofollow&quot;&gt;http://users.stat.umn.edu/~geyer/lottery/&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;As a matter of fact, the above answer actually gives the basic calculation of expected payout of profit for 1$ ticket.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-28T01:39:31.323" Id="41343" LastActivityDate="2012-10-28T01:39:31.323" OwnerUserId="16318" ParentId="26425" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="41377" AnswerCount="3" Body="&lt;p&gt;I have data for which I would like to take the log transformation before doing OLS.  The data include zeros.  Thus, I want to do a log(x + c).  I know a traditional c to choose is 1.  I am wondering whether there is a way to have the data choose c such that there would be no skew anymore using features like the sample mean or variance?  Is there a formaula?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-10-28T14:41:58.037" FavoriteCount="4" Id="41361" LastActivityDate="2012-11-05T16:24:33.130" OwnerUserId="9146" PostTypeId="1" Score="7" Tags="&lt;data-transformation&gt;&lt;skewness&gt;" Title="Choosing c such that log(x + c) would remove skew from the population" ViewCount="888" />
  <row AcceptedAnswerId="41365" AnswerCount="1" Body="&lt;p&gt;I have no formal training in statistics so please correct me if I use the wrong terms as I try to explain my problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a set of data (less than 80 points) with essentially 1 single outcome (a float we will call &lt;code&gt;dcl&lt;/code&gt;) that can potentially depends on 10 of other variables, most of them floats maybe one or two boolean.&lt;/p&gt;&#10;&#10;&lt;p&gt;While I might ask some multi-variate regression question later, let's start with something simple. &lt;/p&gt;&#10;&#10;&lt;p&gt;Historically, people in my field have focused on the strongest correlation between &lt;code&gt;dcl&lt;/code&gt; and variable &lt;code&gt;J&lt;/code&gt; and some of my data suggests some other dependence on a float &lt;code&gt;Re&lt;/code&gt; which is I'm sure at least weakly correlated with 'J' as they share some variables in their respective expressions. So my first question is how do I test the correlation and/or the independence of 'Re' and 'J' on the outcome 'dlc'? Intuitively and physically, I expect 'dlc' to depend strongly on 'J' and weakly on 'Re'. How do I prove this with a statistical analysis?&lt;/p&gt;&#10;&#10;&lt;p&gt;Here are a few graphs to illustrate the data:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/dVRMa.png&quot; alt=&quot;Re vs J&quot;&gt;   &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/hNPvb.png&quot; alt=&quot;dcl vs J&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/D3YJe.png&quot; alt=&quot;dcl vs Re&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Final point, in terms of software, I have python and R installed, I'm fairly proficient in python but I just installed R and know pretty much nothing about it.&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT 1: &lt;/p&gt;&#10;&#10;&lt;p&gt;Following gung's suggestion, I ran my dataset through R:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Call:&#10;lm(formula = dcl ~ J + I(J^2) + Re + I(Re^2), data = df)&#10;&#10;Residuals:&#10;    Min      1Q  Median      3Q     Max &#10;-9.0078 -3.7930 -0.4458  2.0869 21.2538 &#10;&#10;Coefficients:&#10;              Estimate Std. Error t value Pr(&amp;gt;|t|)    &#10;(Intercept)  1.648e+01  1.232e+00  13.380  &amp;lt; 2e-16 ***&#10;J           -2.662e+00  3.773e-01  -7.054 6.58e-10 ***&#10;I(J^2)       1.096e-01  2.071e-02   5.293 1.10e-06 ***&#10;Re           1.966e-06  1.621e-05   0.121    0.904    &#10;I(Re^2)      2.191e-11  3.441e-11   0.637    0.526    &#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 &#10;&#10;Residual standard error: 5.369 on 77 degrees of freedom&#10;Multiple R-squared: 0.4831, Adjusted R-squared: 0.4562 &#10;F-statistic: 17.99 on 4 and 77 DF,  p-value: 1.818e-10&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So now I need some help to decipher this (but I will look into R documentation too). I don't know if it's relevant but on physical grounds only, it's probable the dependency in J is $dcl \sim \frac{1}{\sqrt{J}}$. Can I put this directly into the model? Does that already tell me something about the dependency on $J$ vs $Re$?&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Call:&#10;lm(formula = dcl ~ J + I(J^(-0.5)) + Re + I(Re^(-0.5)), data = df)&#10;&#10;Residuals:&#10;    Min      1Q  Median      3Q     Max &#10;-7.8119 -3.0097 -0.8504  1.8506 12.1439 &#10;&#10;Coefficients:&#10;               Estimate Std. Error t value Pr(&amp;gt;|t|)    &#10;(Intercept)   8.175e-01  1.634e+00   0.500   0.6184    &#10;J            -2.946e-01  1.258e-01  -2.343   0.0217 *  &#10;I(J^(-0.5))   4.516e+00  7.053e-01   6.403 1.09e-08 ***&#10;Re            3.332e-05  6.684e-06   4.985 3.72e-06 ***&#10;I(Re^(-0.5))  6.009e+02  1.354e+02   4.438 2.98e-05 ***&#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 &#10;&#10;Residual standard error: 4.426 on 77 degrees of freedom&#10;Multiple R-squared: 0.6487, Adjusted R-squared: 0.6305 &#10;F-statistic: 35.55 on 4 and 77 DF,  p-value: &amp;lt; 2.2e-16&#10;&#10;&amp;gt; model = lm(dcl ~ J+I(J^(-0.5)) + Re+I(Re^(-0.5)), data=df)&#10;&amp;gt; summary(model)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT 2&lt;/strong&gt;: OK I'm starting to understand things better. Also, again based on physical grounds, I would think that the dependency is more something like $dcl ~ \frac{1}{\sqrt{J}} Re^{n}$ with possibly other variables in that product that I ignore. So when I enter such model in R (can I still use &lt;code&gt;lm&lt;/code&gt; for something non-linear?), here is what I get:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Call:&#10;lm(formula = dcl ~ I(J^(-0.5)) * I(Re^(-0.1)), data = df)&#10;&#10;Residuals:&#10;     Min       1Q   Median       3Q      Max &#10;-10.5363  -3.0192  -0.2556   1.4373  17.1494 &#10;&#10;Coefficients:&#10;                         Estimate Std. Error t value Pr(&amp;gt;|t|)    &#10;(Intercept)               -43.220      9.164  -4.716 1.03e-05 ***&#10;I(J^(-0.5))                63.813     11.088   5.755 1.62e-07 ***&#10;I(Re^(-0.1))              124.245     24.038   5.169 1.77e-06 ***&#10;I(J^(-0.5)):I(Re^(-0.1)) -142.744     27.269  -5.235 1.36e-06 ***&#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 &#10;&#10;Residual standard error: 4.668 on 78 degrees of freedom&#10;Multiple R-squared: 0.6042, Adjusted R-squared: 0.5889 &#10;F-statistic: 39.68 on 3 and 78 DF,  p-value: 1.122e-15&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Does that 4th line tell me something about the dependence between $J$ and $Re$? What kind of tools could I use to get an estimate on the exponent on Re? Because right now I'm just trying a few different numbers empirically to see how the errors evolve. Next for me is to plot the dcl vs the new model and see how well the data collapses visually...&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT 3:&lt;/p&gt;&#10;&#10;&lt;p&gt;In the end, I used &lt;code&gt;nls&lt;/code&gt; to explore the possible exponents of my fit. I also removed some outliers in my data that used a different experimental method. I settled on a fit that gave me decent Pr(&gt;|t|) and residuals and which visually produce a decent fit. The last outlier is actually another experiment with a different setup, but one that I trust. So in a sense it's good that it shows up as an outlier as it hints at other parameters that need to be explored. Thank you gung, I accept your answer as I believe it guided me in the right direction.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; model = nls(L.D ~ C*I(J^(c1))*I(Re_s^(c2)), start=list(C=10,c1=-0.25,c2=-0.25),data=df)&#10;&amp;gt; summary(model)&#10;Formula: L.D ~ C * I(J^(c1)) * I(Re_s^(c2))&#10;Parameters:&#10;   Estimate Std. Error t value Pr(&amp;gt;|t|)    &#10;C  57.20389   26.40011   2.167   0.0337 *  &#10;c1 -0.27721    0.05901  -4.698 1.27e-05 ***&#10;c2 -0.16424    0.04936  -3.327   0.0014 ** &#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 &#10;Residual standard error: 4 on 70 degrees of freedom&#10;Number of iterations to convergence: 8 &#10;Achieved convergence tolerance: 2.91e-06 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/dfgWX.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-10-28T14:58:38.357" FavoriteCount="1" Id="41362" LastActivityDate="2012-11-03T20:12:17.260" LastEditDate="2012-11-03T20:12:17.260" LastEditorUserId="16329" OwnerUserId="16329" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;regression&gt;&lt;multiple-regression&gt;&lt;independence&gt;" Title="I want to know how independent two variables are" ViewCount="346" />
  
  <row Body="&lt;p&gt;&lt;strong&gt;Because the intention is to do OLS, the choice of $c$ should be made in this context.&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In general, we ought to fit $c$ &lt;em&gt;simultaneously&lt;/em&gt; with the rest of the regression.  A quick and dirty way to do this recognizes that the regression $R^2$ is proportional to the log likelihood, so we could seek a value of $c$ that maximizes $R^2$.&lt;/p&gt;&#10;&#10;&lt;p&gt;This is a special example of the problem of choosing among a parameterized family of transformations $y \to f(y; \theta)$ to achieve the best possible fit of $y$ to explanatory values $x$.  This can be solved in &lt;code&gt;R&lt;/code&gt; rather simply and directly:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;xform &amp;lt;- function(f, theta, x, y, ...) {&#10;  g &amp;lt;- function(theta) -summary(lm(f(y, theta) ~ x))$r.squared&#10;  nlm(g, theta, ...)&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;(I am glossing over a somewhat delicate matter of choosing good starting values for the parameter: it is possible to obtain bad solutions with &lt;code&gt;nlm&lt;/code&gt; otherwise.  Standard methods of exploratory data analysis will produce decent starting values, but that's a subject for another day.)&lt;/p&gt;&#10;&#10;&lt;p&gt;As an example of the use of &lt;code&gt;xform&lt;/code&gt;, let's generate some highly skewed data for $y$ for which the &quot;started logarithm&quot; $\log(y+c)$ will produce an unskewed distribution:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(17)&#10;y &amp;lt;- sort(exp(rnorm(32, 4, 1))) + 100&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Evidently $\log(y-100)$ is drawn iid from a Normal distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;I will apply &lt;code&gt;xform&lt;/code&gt; to three choices of $x$:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Values from which $y$ differs by additive, homoscedastic residuals. In this case it would be a mistake to take the logarithm of $y$: it is a grossly incorrect model of the relationship between $x$ and $y$.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Values from which $y$ differs by multiplicative lognormal residuals (more or less).  In this case, taking the logarithm of $y$ is a good idea because it leads to a model for which OLS regression is appropriate.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Constant values of $x$, so that in effect we are looking at $y$ outside of the regression context altogether.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;In cases (1) and (2) I will plot the histogram of $y$ (to show it is highly skewed), the scatterplot of $y$ against $x$ (to exhibit the data), and the scatterplot of the transformed $y$ against $x$ with the OLS line superimposed, to see the result of the transformation.  In the third case those scatterplots are meaningless, so I only report the value of $c$ found by &lt;code&gt;xform&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(17)&#10;par(mfrow=c(2,3))&#10;&#10;y &amp;lt;- sort(exp(rnorm(32, 4, 1))) + 100&#10;x &amp;lt;- y - rnorm(length(y), 0, 50)&#10;hist(y)&#10;plot(x,y)&#10;const &amp;lt;- xform(function(y,c) log(y+c), 1-min(y), x, y)$estimate&#10;plot(x, log(y + const), ylab=sprintf(&quot;log(y + %0.1f)&quot;, const))&#10;abline(coef(fit1&amp;lt;-lm(log(y+const)~x)), col=&quot;Gray&quot;)&#10;&#10;x &amp;lt;- log((1:length(y) + rnorm(length(y), 10, 3)))&#10;hist(y)&#10;plot(x,y)&#10;const &amp;lt;- xform(function(y,c) log(y+c), 1-min(y), x, y)$estimate&#10;plot(x, log(y + const), ylab=sprintf(&quot;log(y + %0.1f)&quot;, const))&#10;abline(coef(fit2&amp;lt;-lm(log(y+const)~x)), col=&quot;Gray&quot;)&#10;&#10;x &amp;lt;- rep(1, length(y))&#10;const &amp;lt;- xform(function(y,c) log(y+c), 1-min(y), x, y)$estimate&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/WhovV.png&quot; alt=&quot;Plots&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The top row is the first case and the second row of plots are for the second case.&lt;/p&gt;&#10;&#10;&lt;p&gt;Please observe:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;The $y$ values are &lt;em&gt;identical&lt;/em&gt; in all three instances.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The $y$ values are constructed from a model in which $c=-100$.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The fitted value of $c$ in the first case, $1408392.5$, is essentially infinite.  This indicates it's bad to be taking the logarithm at all &lt;em&gt;for these $x$ values&lt;/em&gt;.  (Adding this huge value of $c$ to $y$ before taking the logarithm basically does not change the shape of the data: that's why the two scatterplots in the top row look the same.)&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The fitted value of $c$ in the second case, $-104.0$, is close to the value of $-100$ used to generate the data.  (Repeated simulations indicate that the fitted value in the second case will be biased slightly low, averaging around $-105$.)&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The fitted value of $c$ in the third case is $-115.8$, still close to the value used to generate the data.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;If we were to use the &quot;universal&quot; value of $c$ found in the third case (essentially by ignoring the $x$ values), here is what the scatterplot would look like in conjunction with the $x$ values from case 1:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/BkuU2.png&quot; alt=&quot;Scatterplot&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;For these particular $x$ values, the OLS line fit to the transformed $y$ values is a terrible description of the relationship between $y$ and $x$.  Notice how it underestimates most values of $y$ but grossly overestimates a few of them for $x$ between $140$ and $200$.&lt;/p&gt;&#10;&#10;&lt;p&gt;In summary, &lt;strong&gt;if you want to transform the response variable for a regression (to achieve symmetry or linearity), you &lt;em&gt;must&lt;/em&gt; account for the regression itself.&lt;/strong&gt;  This is because the regression only &quot;cares&quot; about the residuals, not the raw values of $y$.  As the extraordinarily bad value of $c$ in the first case shows, ignoring this advice could produce awful results.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-28T19:56:20.437" Id="41377" LastActivityDate="2012-11-05T16:24:33.130" LastEditDate="2012-11-05T16:24:33.130" LastEditorUserId="919" OwnerUserId="919" ParentId="41361" PostTypeId="2" Score="10" />
  <row Body="&lt;p&gt;Probably the most common model for forecasting new product adoption is the &lt;a href=&quot;http://en.wikipedia.org/wiki/Bass_diffusion_model&quot; rel=&quot;nofollow&quot;&gt;Bass diffusion model&lt;/a&gt;, which - similar to @gui11aume's answer - models interactions between current and potential users. New product adoption is a pretty hot topic in forecasting, searching for this term should yield tons of info (which I unfortunately don't have the time to expand on here...).&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-10-29T09:30:18.143" Id="41412" LastActivityDate="2012-10-29T09:30:18.143" OwnerUserId="1352" ParentId="41286" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="41431" AnswerCount="2" Body="&lt;p&gt;I am interested in examples of sources (R code, R packages, books, book chapters, articles, links etc) for &lt;strong&gt;learning statistical and mathematical concepts through R&lt;/strong&gt; (it could also be through other languages, but R is my favorite flavor).&lt;/p&gt;&#10;&#10;&lt;p&gt;The challenge is that the learning of the material relies on the programming, not just on how to run a code that performs the algorithm.&lt;/p&gt;&#10;&#10;&lt;p&gt;So (for example) a book like &quot;linear models with R&quot; (which is a great book) is not what I am looking for.  This is since this books mainly shows how to implement linear models in R, but it does not revolve around teaching linear models by using R.&lt;/p&gt;&#10;&#10;&lt;p&gt;The help files for the (wonderful) &lt;a href=&quot;http://cran.r-project.org/web/packages/TeachingDemos/index.html&quot;&gt;TeachingDemos package&lt;/a&gt; is a good example of what I am looking for.  It is an R package which includes functions for learning statistical concepts through various R applets and simulations.  The accompany help files are nice.  Of course, neither are sufficient, and require an external text-book in order to master many of the exact details.o learn them (well, the help files do).&lt;/p&gt;&#10;&#10;&lt;p&gt;All leads will be much appreciated...&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2012-10-29T17:08:40.023" CreationDate="2012-10-29T14:14:40.040" FavoriteCount="2" Id="41419" LastActivityDate="2012-10-29T15:50:11.547" OwnerUserId="253" PostTypeId="1" Score="7" Tags="&lt;r&gt;&lt;teaching&gt;&lt;learning&gt;" Title="Sources for learning (not just implementing) statistics/math through R" ViewCount="297" />
  
  
  <row AcceptedAnswerId="41440" AnswerCount="1" Body="&lt;p&gt;&lt;strong&gt;How can I put the arrays of a matrix in double $$ in R?&lt;/strong&gt;&lt;/p&gt;&#10;" ClosedDate="2012-10-29T19:14:37.133" CommentCount="3" CreationDate="2012-10-29T18:39:08.807" Id="41439" LastActivityDate="2012-10-29T18:49:51.680" OwnerUserId="16365" PostTypeId="1" Score="-1" Tags="&lt;r&gt;" Title="How to put $$ around numbers in R?" ViewCount="53" />
  <row AnswerCount="0" Body="&lt;p&gt;Effects of growth hormone (GH) replacement with recombinant human GH on bone and mineral metabolism were studied in 36 GH-deficient children. Several outcomes, including serum ionized calcium levels, were assessed at pretherapy (zero months) and 1, 3, 6, 9 and 12 months after the beginning of therapy. Simple linear regression was used to determine if changes in ionized calcium levels could be predicted from length of therapy. Each patient’s ionized calcium level was employed in the analysis. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Is the use of simple linear regression appropriate?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;a.&lt;/strong&gt;  Yes&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;b.&lt;/strong&gt;    No&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;c.&lt;/strong&gt;    The investigators should have used logistic regression instead&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-10-29T19:41:32.033" Id="41442" LastActivityDate="2012-11-01T16:13:10.943" LastEditDate="2012-11-01T16:13:10.943" LastEditorUserId="1036" OwnerUserId="16366" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;logistic&gt;&lt;self-study&gt;&lt;nonlinear-regression&gt;" Title="Whether to use linear regression or not" ViewCount="287" />
  <row Body="&lt;p&gt;The log model fails the lack of fit test by looking at the ANOVA, Pr(&gt;F) &gt; 0.05. By comparing it to the categorical model, you are basically doing a one way ANOVA (comparing SS_lack.of.fit to SS_pure.error[which is the categorical model]). The R^2 of the categorical model is naturally better because it consumes more degrees of freedom.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-29T19:50:50.960" Id="41444" LastActivityDate="2012-10-29T19:50:50.960" OwnerUserId="14120" ParentId="41428" PostTypeId="2" Score="0" />
  <row Body="Variables that are counts (non-negative integers) often have an excess of zeroes. Zero-inflated regression models (e.g. zero inflated Poisson, zero inflated negative binomial) are designed to deal with this. Less commonly, continuous data can have this issue, and there is zero-inflated normal regression to deal with that situation. " CommentCount="0" CreationDate="2012-10-29T22:24:14.537" Id="41459" LastActivityDate="2012-10-29T22:45:08.047" LastEditDate="2012-10-29T22:45:08.047" LastEditorUserId="686" OwnerUserId="686" PostTypeId="4" Score="0" />
  <row Body="&lt;p&gt;Using this paper on &lt;a href=&quot;http://www.wjh.harvard.edu/~moulton/mauchly_test.pdf&quot; rel=&quot;nofollow&quot;&gt;Mauchly Test&lt;/a&gt; (switched the $W$ to $\log(W)$),&#10;I can get an approximate value for $\chi^2$:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(reshape2); library(ez)&#10;dat &amp;lt;- data.frame(id=as.factor(1:10), matrix(rpois(90, 10), ncol=9))&#10;dat2 &amp;lt;- melt(dat)&#10;mod &amp;lt;- ezANOVA(data=dat2, dv = .(value), wid=.(id), within=.(variable), &#10;    detailed=TRUE, type=3)&#10;mod&#10;&#10;&#10;#or repeated measures&#10;k &amp;lt;- 5&#10;#number of particpants  &#10;n &amp;lt;- 9&#10;d &amp;lt;- 1 - ((2 * ((k - 1)^2)+(k - 1)+2)/(6*(k - 1)*(n - 1)))&#10;&#10;W &amp;lt;- mod[[2]][2]     #Mauchly's W from ezANOVA&#10;-(n-1)*d*log(W)      #chi^2&#10;(k*(k - 1)/2) - 1    #df&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2012-10-29T22:54:32.490" Id="41461" LastActivityDate="2012-10-29T23:09:33.920" LastEditDate="2012-10-29T23:09:33.920" LastEditorUserId="930" OwnerUserId="7482" ParentId="41399" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="41487" AnswerCount="3" Body="&lt;p&gt;For a long time now, I have been thinking about working with neural networks and genetic algorithms.  I have never been able to decide whether it makes sense to start writing my own code, or to reuse the many, many options made by others.&lt;/p&gt;&#10;&#10;&lt;p&gt;In my experience, assuming the programmer knows what he or she is doing to a reasonable extent, there is very little that replaces C or C++ code crafted to the individual users preferences.  For one thing, it empowers you to do almost anything with the code due to having complete knowledge of its workings.  One can for instance incorporate whatever new advances that one happens to read about in the literature.&lt;/p&gt;&#10;&#10;&lt;p&gt;On the other hand, one person will often find it hard to do as much as a team of people.  As an individual, it's always possible that one will miss various bugs.  There are many other disadvantages to being a loner.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, for those who have some experience in machine learning, I would like to hear what actually works in the real world.  Is it more common to use a few well-known programming packages or do many people write their own custom implementations?  I am specifically interested in the areas of RBM, neural networks, genetic programming and clustering.&lt;/p&gt;&#10;&#10;&lt;p&gt;Assume that I am willing to put a lot of time into learning about machine learning and that I will put at least as much time and effort into it as a graduate student would.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-10-29T23:10:39.253" FavoriteCount="1" Id="41463" LastActivityDate="2012-10-30T07:03:51.697" OwnerUserId="847" PostTypeId="1" Score="3" Tags="&lt;machine-learning&gt;&lt;clustering&gt;&lt;neural-networks&gt;&lt;genetic-algorithms&gt;" Title="How to decide whether to reuse old code or reinvent the wheel?" ViewCount="505" />
  
  <row Body="&lt;p&gt;Your coin flipping example is not a sample of anything; the analogy is wrong.&lt;/p&gt;&#10;&#10;&lt;p&gt;Your coin flipping example is analogous to measuring the height of one single person several times in order to estimate the magnitude of measurement error.&lt;/p&gt;&#10;&#10;&lt;p&gt;Your height measuring example is analogous to flipping, one time each, 100 randomly selected coins from a particular population of coins in circulation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Flipping a coin once is analogous to measuring a person's height once. Flipping one coin 100 times is like measuring one person's height 100 times.&lt;/p&gt;&#10;&#10;&lt;p&gt;Flipping one coin multiple times or measuring one person's height multiple times would not be a sample of anything unless you are sampling, that is randomly choosing from, a population of different measuring devices or measuring personnel.&lt;/p&gt;&#10;&#10;&lt;p&gt;Just making measurements is not the same as taking a sample. A sample is always a sample &lt;em&gt;of&lt;/em&gt; some thing*&lt;em&gt;s&lt;/em&gt;*.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-10-30T00:19:17.423" Id="41468" LastActivityDate="2012-11-29T15:00:50.780" LastEditDate="2012-11-29T15:00:50.780" LastEditorUserId="11346" OwnerUserId="11346" ParentId="38258" PostTypeId="2" Score="-1" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I was reading some lecture notes on simple linear regression where one section said that when the slope is 0 (hence, $H_0: \beta = 0$ is actually true), $\frac{(SSY - SSE)}{(DFY - DFE)}$ estimates sigma squared. This doesn't make sense to me. If beta is actually 0, then shouldn't SSY and SSE be the same value? &lt;/p&gt;&#10;&#10;&lt;p&gt;$$SSY = ∑(Y-\bar{Y})^2$$&#10;$$SSE = ∑(Y-\hat{Y})^2$$&lt;/p&gt;&#10;&#10;&lt;p&gt;However, if $\beta = 0$, that means the best estimate ($\widehat{\text{Y}}$) is essentially $\overline{\text{Y}}$, which in turn means&lt;/p&gt;&#10;&#10;&lt;p&gt;$$SSY = SSE =  ∑(Y-\bar{Y})^2$$&lt;/p&gt;&#10;&#10;&lt;p&gt;In other words&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac{SSY - SSE}{DFY - DFE} = \frac{0}{DFY - DFE}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there something that I am missing? I hope someone here can elaborate on this.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-10-30T08:45:30.010" Id="41493" LastActivityDate="2014-05-11T04:48:38.847" LastEditDate="2014-05-11T04:48:38.847" LastEditorDisplayName="user11392" LastEditorUserId="22311" OwnerDisplayName="user11392" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;variance&gt;&lt;residuals&gt;" Title="Estimate of $\sigma^2$ in a simple linear regression when $H_0: \beta = 0$ is true" ViewCount="888" />
  
  <row Body="&lt;p&gt;If the values of the variable are $X_i$ and there are $n$ values then:&lt;/p&gt;&#10;&#10;&lt;p&gt;Arithmetic mean: $\bar{x} = \frac{\sum_{i = 1}^{n}X_i}{n}$&lt;/p&gt;&#10;&#10;&lt;p&gt;Geometric mean: $g = (\prod_{i = 1}^n x_i)^{\frac{1}{n}}$&lt;/p&gt;&#10;&#10;&lt;p&gt;Harmonic mean: $\frac{1}{H} = \frac{1}{n}\sum_{i = 1}^n \frac{1}{x_i}$ &lt;/p&gt;&#10;&#10;&lt;p&gt;Trimmed mean: $\sigma_{\alpha}=\frac{1}{n-2k}\sum_{i = k+1}^{n-k}x_{(i)}$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\alpha$ is the degree of trimming, $k$ is the smallest integer less than or equal to $\alpha n$ and $x_{(i)}$ are ordered values of x.&lt;/p&gt;&#10;&#10;&lt;p&gt;Source: Dictionary of Statistics by Brian S. Everitt&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-30T12:36:13.857" Id="41501" LastActivityDate="2012-10-30T13:04:08.460" LastEditDate="2012-10-30T13:04:08.460" LastEditorUserId="686" OwnerUserId="686" PostTypeId="5" Score="0" />
  
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm looking for a data set that is easily accessible for comparing Kernel Logistic Regression (KLR) and regular logistic regression.  All the paper that I find using KLR use synthetic data sets.  I'm most interested in data sets with continuous variables, so I can apply the squared exponential kernel.&lt;/p&gt;&#10;&#10;&lt;p&gt;Data sets with a binary target and spatial predictor variables would also be interesting. &lt;/p&gt;&#10;" ClosedDate="2012-10-30T23:52:23.667" CommentCount="2" CreationDate="2012-10-30T16:55:37.643" Id="41525" LastActivityDate="2012-10-30T18:14:33.747" LastEditDate="2012-10-30T17:08:03.840" LastEditorUserId="88" OwnerUserId="10185" PostTypeId="1" Score="0" Tags="&lt;dataset&gt;&lt;kernel&gt;&lt;kernel-trick&gt;" Title="Free data set for comparing kernel logistic regression and regular logistic regression" ViewCount="459" />
  <row Body="&lt;p&gt;In general, absolute goodness of fit measures are not available for models different than linear regression. If you obtain a significant p - value for some predictor this means that the model you are fitting is better that using the null model (no predictor, just the relative frequency of the outcome). This does not imply that you would be able to obtain a good prediction of the outcome.&lt;/p&gt;&#10;&#10;&lt;p&gt;In my personal opinion logistic regression models are good to rank subjects about their relative risk.  The Gini index could be used to see how well the risk ranking works.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;$normalizedGini &amp;lt;- function(aa, pp) {&#10;     .Gini &amp;lt;- function(a, p) {&#10;      if (length(a) !=  length(p)) stop(&quot;Actual and Predicted need to be equal lengths!&quot;)&#10;      temp.df &amp;lt;- data.frame(actual = a, pred = p, range=c(1:length(a)))&#10;      temp.df &amp;lt;- temp.df[order(-temp.df$pred, temp.df$range),]&#10;      population.delta &amp;lt;- 1 / length(a)&#10;      total.losses &amp;lt;- sum(a)&#10;      null.losses &amp;lt;- rep(population.delta, length(a)) # Hopefully is similar to accumulatedPopulationPercentageSum&#10;      accum.losses &amp;lt;- temp.df$actual / total.losses # Hopefully is similar to accumulatedLossPercentageSum&#10;      gini.sum &amp;lt;- cumsum(accum.losses - null.losses) # Not sure if this is having the same effect or not&#10;      sum(gini.sum) / length(a)&#10;     }&#10;     .Gini(aa,pp) / .Gini(aa,aa)&#10;    } $&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2012-10-30T18:30:29.553" Id="41533" LastActivityDate="2012-10-30T18:52:00.893" LastEditDate="2012-10-30T18:52:00.893" LastEditorUserId="1036" OwnerUserId="6547" ParentId="41528" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Given that you have a large sample size (df = 1741) it is not surprising that you get highly significant results with a small effect size.&lt;/p&gt;&#10;&#10;&lt;p&gt;The two are answering different questions: The p value answers:&#10;&quot;If, in the population from which this sample was drawn, there was really no effect at all, how likely is it that, in sample of this size, a test statistic as large or larger than the one we got would occur?&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;The effect size is what it says - how big is the effect? &lt;/p&gt;&#10;&#10;&lt;p&gt;In a logistic regression, the most intuitive (at least to me) effect size measures are the odds ratios, rather than the parameter estimates. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-30T19:00:33.107" Id="41534" LastActivityDate="2012-10-30T19:00:33.107" OwnerUserId="686" ParentId="41528" PostTypeId="2" Score="3" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Can anyone suggest the statistical tools to compare CART, conditional inference tree, and random forests? I use these three algorithm for regression analysis and want to choose the better one.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-10-30T20:58:40.693" FavoriteCount="1" Id="41545" LastActivityDate="2012-10-30T23:36:30.927" LastEditDate="2012-10-30T22:57:27.147" LastEditorUserId="930" OwnerUserId="16415" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;machine-learning&gt;&lt;model-selection&gt;" Title="Model comparison" ViewCount="129" />
  
  <row Body="&lt;p&gt;I haven't dug into the JAGS source code, but often people consider the initial values to be iteration 0, and for iteration 1 to be the result after a single pass through the Gibbs sampler.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, if there are any Metropolis steps, there is likely to be a short adaptation phase before iteration 1 irrespective of the burn-in setting.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-10-30T21:08:08.267" Id="41546" LastActivityDate="2012-10-30T21:08:08.267" OwnerUserId="643" ParentId="38613" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;If you don't have sufficient evidence to reject a null, you fail to reject it.&lt;/p&gt;&#10;&#10;&lt;p&gt;When your data doesn't speak to it at all, it remains unrejected.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-30T23:27:11.660" Id="41554" LastActivityDate="2012-10-30T23:27:11.660" OwnerUserId="805" ParentId="41518" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;I have never seen x/d in any formula. Can you give a link to such a page?&#10;The best way to specify a formula is using + and :, for e.g., if you want to model y on x1 and x2 and interaction of x1 and x2, you will need to give: y ~ x1 + x2 + x1:x2 or x1 * x2 (which is a shortcut).&lt;/p&gt;&#10;&#10;&lt;p&gt;Now comes the question of interpreting coeff when you have interaction terms. Imagine a simple linear model: y ~ x1 + x2. The coeff of x1 or x2 indicates the increase in y with a unit increase in x1 or x2 respectively. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, the moment you add an interaction term, interpretation is not so easy. If you increase x1 by 1 unit in a model: y = b0 + b1 x1 + b2 x2 + b3 x1:x2, the increase in y is : b1 + b3*x2. As you see the increase is not linear, it depends on the level of x2. What you can possibly do is plot response curves for various levels of x2, and plot y vs x1, to show change in response.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hope this helps. I will try and answer the rest of the questions in another post.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-10-30T11:46:24.707" Id="41562" LastActivityDate="2012-10-30T11:46:24.707" OwnerDisplayName="Indrajit" OwnerUserId="25479" ParentId="41561" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Usually there is a reasonable correspondence between the bootstrap scheme and the sandwich estimator.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Simple bootstrap with resampling $\Leftrightarrow$ White's heteroskedasticity robust estimator&lt;/li&gt;&#10;&lt;li&gt;Block bootstrap with blocks of length $l \Leftrightarrow$ Newey-West estimator with $l$ lags&lt;/li&gt;&#10;&lt;li&gt;Bootstrap of clusters $\Leftrightarrow$ cluster-corrected standard errors.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;To read on clustered bootstraps, start from &lt;a href=&quot;http://www.citeulike.org/user/ctacmo/article/582039&quot; rel=&quot;nofollow&quot;&gt;Rao and Wu (1988)&lt;/a&gt;. Methods of this kind would be implemented in &lt;code&gt;survey&lt;/code&gt; package (although I am not sure it does the Rao and Wu bootstrap, precisely; my understanding is that &lt;code&gt;survey&lt;/code&gt; just resamples clusters without the small sample corrections that Rao and Wu introduced). To read about resampling methods in regression analysis, take a look at &lt;a href=&quot;http://www.citeulike.org/user/ctacmo/article/785121&quot; rel=&quot;nofollow&quot;&gt;Wu (1986)&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-10-31T04:16:44.253" Id="41569" LastActivityDate="2012-10-31T04:16:44.253" OwnerUserId="5739" ParentId="41547" PostTypeId="2" Score="1" />
  
  
  
  
  
  
  
  <row AcceptedAnswerId="41632" AnswerCount="1" Body="&lt;p&gt;If I'm not wrong, likelihood functions are sensitive to the size of the sample, i.e. the larger the sample, the lower the likelihood value. Given a sample $x$ of a random variable $X \sim f(\theta)$, and a parameter estimate $\hat\theta$, suppose I want to test the hypothesis that the likelihoods of different subsamples of $x$, let's call them $x_a$ and $x_b$ are equal.&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem is $x_a$ and $x_b$ have a different number of elements, say $n_a$ and $n_b$ respectively, and $n_a \neq n_b$, so I assume the likelihoods must be normalized in some way. Is it enough to divide by the sample size? For instance, can I use a test statistic such as&lt;/p&gt;&#10;&#10;&lt;p&gt;$T = \frac{\ell(\hat\theta|x_a)}{n_a} - \frac{\ell(\hat\theta|x_b)}{n_b}$&lt;/p&gt;&#10;&#10;&lt;p&gt;and then test the hypothesis that $T = 0$. A related question would be how do I find the probability distribution of the above statistic, but that is another story.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-10-31T21:17:47.797" FavoriteCount="1" Id="41626" LastActivityDate="2012-11-01T10:27:45.063" LastEditDate="2012-11-01T10:27:45.063" LastEditorUserId="930" OwnerUserId="10028" PostTypeId="1" Score="2" Tags="&lt;normalization&gt;&lt;likelihood&gt;" Title="Normalization of likelihood" ViewCount="1260" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;Basic setup: &lt;/p&gt;&#10;&#10;&lt;p&gt;regression model: $y = \text{constant} +\beta_1x_1+\beta_2x_2+\beta_3x_3+\beta_4x_4+\alpha C+\epsilon$&#10;where C is the vector of control variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm interested in $\beta$ and expect $\beta_1$ and $\beta_2$ to be negative. However, there is multicollinearity problem in the model, the coefficient of correlation is given by, &#10;corr($x_1$,$x_2)=$ 0.9345, corr($x_1$,$x_3)=$ 0.1765, corr($x_2$,$x_3)=$ 0.3019.&lt;/p&gt;&#10;&#10;&lt;p&gt;So $x_1$ and $x_2$ are highly correlated, and they should virtually provide the same information.  I run three regressions:  &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;exclude $x_1$ variable; 2. exclude $x_2$ variable; 3. original model with both $x_1$ and $x_2$. &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Results:&lt;br&gt;&#10;For regression 1 and 2, it provide the expected sign for $\beta_2$ and $\beta_1$ respectively and with similar magnitude. And $\beta_2$ and $\beta_1$ are significant in 10% level in both model after I do the HAC correction in standard error. $\beta_3$ is positive but not significant in both model. &lt;/p&gt;&#10;&#10;&lt;p&gt;But for 3, $\beta_1$ has the expected sign, but the sign for $\beta_2$ is positive with the magnitude twice greater than $\beta_1$ in absolute value. And both $\beta_1$ and $\beta_2$ are insignificant. Moreover, the magnitude for $\beta_3$ reduces almost in half compared to regression 1 and 2. &lt;/p&gt;&#10;&#10;&lt;p&gt;My question is: &lt;/p&gt;&#10;&#10;&lt;p&gt;Why in 3, the sign of $\beta_2$ becomes positive and much greater than $\beta_1$ in absolute value? Is there any statistical reason that $\beta_2$ can flip sign and has large magnitude?  Or is it because model 1 and 2 suffer omitted variable problem which inflated $\beta_3$ provided $x_2$ has positive effect on y? But then in regression model 1 and 2, both $\beta_2$ and $\beta_1$ should be positive instead of negative, since the total effect of $x_1$ and $x_2$ in regression model 3 is positive.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-10-31T22:24:08.713" Id="41633" LastActivityDate="2014-03-04T20:15:22.647" LastEditDate="2014-03-04T20:15:22.647" LastEditorUserId="32036" OwnerUserId="16423" PostTypeId="1" Score="3" Tags="&lt;regression&gt;&lt;multicollinearity&gt;" Title="Sign flipping when adding one more variable in regression and with much greater magnitude" ViewCount="289" />
  
  
  
  
  <row Body="&lt;p&gt;You might find Hahn, &amp;amp; Doganaksoy (2011). &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0470404418&quot; rel=&quot;nofollow&quot;&gt;&lt;em&gt;A Career in Statistics: Beyond the Numbers&lt;/em&gt;&lt;/a&gt;, to be helpful for your purposes.  &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-11-01T02:39:14.027" Id="41646" LastActivityDate="2012-11-01T02:39:14.027" OwnerUserId="7290" ParentId="41642" PostTypeId="2" Score="4" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I am a refugee from SPSS in the process of re-learning how to do everything in R. Mostly it's been fun, as R is great for a lot of things but I've run into a snag that I can't seem to find a solution for; any help appreciated.&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to run repeated-measures ANOVA with contrasts. I know linear mixed models are generally the better way to go but I work in a community where that is only just starting to catch on and my students need to understand what they're reading when they encounter older techniques. SPSS makes contrasts on repeated measures easy, but R does weird things I don't understand. Here's a sample:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;df.wide = data.frame(Subj = 1:8, Day1 = c(6,5,5,6,7,4,4,5), &#10;                     Day2 = c(5,5,6,5,3,2,4,7), Day3 = c(2,4,3,4,3,1,1,2))&#10;attach(df.wide)&#10;&#10;## contrasts as t-tests&#10;Linear = -Day1 + 0*Day2 + Day3&#10;Day1vs23 = -2*Day1 + Day2 + Day3&#10;t.test(Linear)&#10;t.test(Day1vs23)&#10;&#10;## repeated-measures with long-format data&#10;library(reshape2)&#10;df.long = melt(df.wide, id.vars=&quot;Subj&quot;, variable.name=&quot;Trial&quot;, value.name=&quot;DV&quot;)&#10;contrasts(df.long$Trial) = contr.poly(3)&#10;df.long$Subj = factor(df.long$Subj)&#10;Anv = aov(DV ~ Trial + Error(Subj/Trial), data=df.long)&#10;summary(Anv, split=list(Trial=list(&quot;Linear&quot;=1, &quot;Quad&quot;=2)))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The t-tests show t-values of 7.51 for Linear and 1.27 for quadratic, but this does NOT match the &lt;code&gt;aov()&lt;/code&gt; output, which provides F-values of 25.28 and 2.51, and of course substantially different p-values.  SPSS contrasts on the repeated measures yield t-values and F-values that match up. &lt;/p&gt;&#10;&#10;&lt;p&gt;It looks like R isn't partitioning the error term like SPSS does for contrasts, and both contrasts are being tested against a 14 d.f. error term, which is why it doesn't match the t-test.  That seems wrong to me.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, what am I doing wrong in R and how do I fix it?  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-01T05:03:48.653" FavoriteCount="2" Id="41654" LastActivityDate="2012-11-03T21:25:07.600" LastEditDate="2012-11-03T21:25:07.600" LastEditorUserId="930" OwnerUserId="16432" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;anova&gt;&lt;repeated-measures&gt;&lt;contrasts&gt;" Title="Contrasts in repeated mesures ANOVA in R" ViewCount="794" />
  <row Body="&lt;p&gt;Looks good to me.  You are calculating a t-score though, not a z-score.  A t-score (or t statistic, T-test, etc.) reflects the probability of a given mean for a sample of population X.  A z-score reflects a given score's deviation from a mean, given the standard deviation of that sample.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-11-01T05:42:09.927" Id="41657" LastActivityDate="2012-11-01T05:42:09.927" OwnerUserId="12605" ParentId="41656" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;If we're sticking with typical-sized polls so we have the CLT kicking in; then we're just dealing with the variances of normally distributed quantities. It depends on the dependence (specifically, the covariance) between the quantities.&lt;/p&gt;&#10;&#10;&lt;p&gt;$\rm{Var}(X + Y) = \rm{Var}(X) + \rm{Var}(Y) + 2 \rm{Cov}(X,Y)$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\rm{Var}(X - Y) = \rm{Var}(X) + \rm{Var}(Y) - 2 \rm{Cov}(X,Y)$&lt;/p&gt;&#10;&#10;&lt;p&gt;(that doesn't rely on normality, it's general; the meaningfulness of the resulting confidence intervals depends on normality)&lt;/p&gt;&#10;&#10;&lt;p&gt;The width of the confidence intervals for the proportions $X$ and $Y$ and for their sum or difference are based off their respective standard errors (the square root of the variance).&lt;/p&gt;&#10;&#10;&lt;p&gt;If $X$ and $Y$ are independent (based on different polls for example) then the variances add because the covariances are $0$.&lt;/p&gt;&#10;&#10;&lt;p&gt;So square the width of the CI's for $X$ and $Y$, add them, take the square root. That's the width of the CI for the sum or difference.&lt;/p&gt;&#10;&#10;&lt;p&gt;If $X$ and $Y$ are two proportions from the same poll, that is wrong, since their covariance is negative. If they add to 100% or nearly so, directly add the widths of their CIs to get the width of the difference. (For the sum, the variance will be 0 - or nearly so if they don't quite add to 100% - and the width will be a multiple of the square root of that). Estimates for the covariances can actually be calculated in general, using results for the multinomial distribution.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-01T06:05:52.743" Id="41659" LastActivityDate="2012-11-02T23:33:32.933" LastEditDate="2012-11-02T23:33:32.933" LastEditorUserId="805" OwnerUserId="805" ParentId="41156" PostTypeId="2" Score="7" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm trying to run a &lt;code&gt;cforest&lt;/code&gt; model in R with continuous and categorical variables. When I tried this in &lt;code&gt;randomForest&lt;/code&gt;, the explained variation was ok, but there was a large bias towards continuous variables. Therefore I switched to &lt;code&gt;cforest&lt;/code&gt;, using the codes provided by Strobl et al. &lt;/p&gt;&#10;&#10;&lt;p&gt;This was my code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;my_cforestcontrol &amp;lt;- cforest_control(teststat=&quot;quad&quot;, testtype=&quot;Univ&quot;, mincriterion=0, &#10;                                     ntree=2000, replace=F)&#10;my_cforest &amp;lt;- cforest(CSBUZZ ~ ., data=all, controls=my_cforestcontrol)&#10;myvarimp &amp;lt;- varimp(my_cforest)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;However, if I put &lt;code&gt;print(my_cforest)&lt;/code&gt; or just &lt;code&gt;my_cforest&lt;/code&gt;, the resulting summary doesn't give the percentage of variance explained. Is there any way I can get that information? I've tried several formula and the R help guide, but I couldn't find it anywhere.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-01T09:31:56.527" FavoriteCount="1" Id="41662" LastActivityDate="2012-11-01T10:18:07.200" LastEditDate="2012-11-01T10:18:07.200" LastEditorUserId="930" OwnerUserId="16435" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;random-forest&gt;" Title="How do you show explained variation in cforest?" ViewCount="221" />
  <row AcceptedAnswerId="41667" AnswerCount="1" Body="&lt;p&gt;I want to compare two samples with a Kolmogorov-Smirnov test. Wikipedia states the null hypothesis is rejected at $\alpha$ if:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\sqrt( \frac{nn'}{n+n'}) D_{nn'} &amp;gt; K_\alpha$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $n$ and $n'$ are the sizes of samples, D the KS-statistic and $K_\alpha$ the critical value (probably everyone here already knows). I wonder about the sample sizes: according to this formula every null hypothesis is rejected, if the samples are just large enough.&lt;/p&gt;&#10;&#10;&lt;p&gt;Could anybody enlighten me, what I am misunderstanding? &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-11-01T09:43:55.633" FavoriteCount="1" Id="41663" LastActivityDate="2012-11-01T13:22:07.833" LastEditDate="2012-11-01T13:22:07.833" LastEditorUserId="615" OwnerUserId="16434" PostTypeId="1" Score="2" Tags="&lt;distributions&gt;&lt;hypothesis-testing&gt;&lt;kolmogorov-smirnov&gt;" Title="Null hypothesis rejection with Kolmogorov-Smirnov" ViewCount="642" />
  
  <row AcceptedAnswerId="43770" AnswerCount="3" Body="&lt;p&gt;This question might probably also fit into Academia.SX. I'm writing my first paper in life sciences and I'm very often seeing asterisks as indicators for statistical significance (&lt;em&gt;*&lt;/em&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm wondering if there's a convention on which symbol, and how many of them are to be used to indicate what the significance is, e.g. &lt;code&gt;*&lt;/code&gt; for p &amp;lt; 0.05 and &lt;code&gt;***&lt;/code&gt; for p &amp;lt; 0.001?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-01T11:12:31.177" Id="41669" LastActivityDate="2012-11-16T19:25:11.940" LastEditDate="2012-11-01T11:45:45.250" LastEditorUserId="930" OwnerUserId="11462" PostTypeId="1" Score="0" Tags="&lt;statistical-significance&gt;" Title="Convention for symbols indicating statistical significance?" ViewCount="3386" />
  <row AcceptedAnswerId="41675" AnswerCount="2" Body="&lt;p&gt;I just need a simple yes/no answer (hopefully yes) to confirm I haven't done something stupid here - I'm doing some data analysis and looking at the correlation of 2 variables X and Y over the past 6 years. My correlation over these 6 years comes out in MS Excel as 96% (that is, the usual definition of correlation as detailed here &lt;a href=&quot;http://office.microsoft.com/en-us/excel-help/correl-HP005209023.aspx&quot; rel=&quot;nofollow&quot;&gt;http://office.microsoft.com/en-us/excel-help/correl-HP005209023.aspx&lt;/a&gt;): however, my correlations for each of these 6 years are 73%, 84%, 95%, 42%, 84% and 82% supposedly. &lt;/p&gt;&#10;&#10;&lt;p&gt;Is this possible, that the 6yr correlation is so much higher than &lt;strong&gt;&lt;em&gt;any&lt;/em&gt;&lt;/strong&gt; of the individual ones? I was surprised at how much higher the 6-year correlation was than any of the yearly ones. From drawing a picture this seems plausible, but I couldn't find any simple mathematical justification for the fact without things in the formula getting extremely messy, and there are a few hundred data points per year so it's not really feasible to check my data by hand. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-11-01T11:22:32.187" Id="41672" LastActivityDate="2012-11-01T13:09:54.300" LastEditDate="2012-11-01T12:15:24.877" LastEditorUserId="930" OwnerUserId="16438" PostTypeId="1" Score="3" Tags="&lt;correlation&gt;" Title="Correlations over 6yrs higher than any individual 1yr correlation over the same period" ViewCount="83" />
  <row AnswerCount="1" Body="&lt;p&gt;I was conducting a 'recall of negative words' memory experiment with a two-way between-group ANOVA and got no significant main effects and no significant interaction! How can I go about explaining this in the Discussion section? Do I just state that it seems that my 2 between subjects factors have no influence over memory? Or do I try to fault the sample size [N=67].&lt;/p&gt;&#10;&#10;&lt;p&gt;All help will be greatly appreciated!!!&lt;/p&gt;&#10;&#10;&lt;p&gt;PS: Do I write: the lack of significant main effects allowed for rejection of all experimental hypotheses? it that correct?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-11-01T12:01:28.307" Id="41678" LastActivityDate="2012-11-01T12:28:38.647" LastEditDate="2012-11-01T12:10:10.527" LastEditorUserId="930" OwnerUserId="16441" PostTypeId="1" Score="2" Tags="&lt;anova&gt;&lt;interpretation&gt;&lt;reporting&gt;" Title="How to do ANOVA write-up when there is no significant effect at all?" ViewCount="3076" />
  
  <row Body="&lt;p&gt;Yes, this is possible.  Indeed, it's not just possible but a fairly common situation.  For example I was looking recently at time series data (1987-2011) for world oil and natural gas prices (adjusted for general price inflation), and found that the correlation over the whole time period was higher than that over either half of the period. The reason is that both prices are on an upward trend, but the correlation coefficients for the shorter periods are more influenced by differences in fluctuations about the trend.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-01T13:09:54.300" Id="41684" LastActivityDate="2012-11-01T13:09:54.300" OwnerUserId="11060" ParentId="41672" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I agree with @xan about the inefficiencies of rainbow color maps. Here is another paper that shows that rainbow/categorical color maps are substantially worse than diverging ones for quantitative tasks, from InfoVis '11:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Michelle Borkin, Krzysztof Gajos, Amanda Peters, Dimitrios Mitsouras, Simone Melchionna, Frank Rybicki, Charles Feldman, and Hanspeter Pfister. 2011. Evaluation of Artery Visualizations for Heart Disease Diagnosis. IEEE Transactions on Visualization and Computer Graphics 17, 12 (December 2011), 2479-2488. &lt;a href=&quot;http://dx.doi.org/10.1109/TVCG.2011.192&quot;&gt;DOI=10.1109/TVCG.2011.192&lt;/a&gt;   &lt;a href=&quot;http://gvi.seas.harvard.edu/paper/evaluation-artery-visualizations-heart-disease-diagnosis&quot;&gt;Link to PDF, Slides, and Images.&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/un54l.jpg&quot; width=&quot;500&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The only thing rainbow/categorical color maps are good for is to show separate values of categorical variables. However, the colors you choose matter. If you need a categorical scale, check out this excellent paper from CHI '12 that uses the XKCD survey dataset that talks about how we perceive differences in color. It allows you to rate a color scale by how well humans perceive the differences. Their web-based &lt;a href=&quot;http://vis.stanford.edu/color-names/analyzer/&quot;&gt;Color Palette Analyzer&lt;/a&gt; will let you evaluate your own color scale, too!&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Jeffrey Heer and Maureen Stone. 2012. Color naming models for color selection, image editing and palette design. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '12). ACM, New York, NY, USA, 1007-1016. &lt;a href=&quot;http://doi.acm.org/10.1145/2207676.2208547&quot;&gt;DOI=10.1145/2207676.2208547&lt;/a&gt; &lt;a href=&quot;http://vis.stanford.edu/color-names/&quot;&gt;Link to PDF, online demos, etc.&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/AqJTv.png&quot; alt=&quot;Color Palette analysis example&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-01T16:43:34.597" Id="41691" LastActivityDate="2012-11-01T16:43:34.597" OwnerUserId="16446" ParentId="41492" PostTypeId="2" Score="12" />
  
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;What sampling procedure should the company use? Company A wants to&#10;  sample as few students as possible because accessing old data is time&#10;  intensive.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;You'll need to do a power analysis to determine the number of students that need to be sampled to adequately assess statistical significance.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;What statistical tests should the company apply to the data to see&#10;  whether there is a statistically significant difference in difficulty&#10;  between tests?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;You can set up this whole analysis as a repeated measure ANOVA. Testing for an interaction effect between student test exposure (one of your factors will be test exposure number), should address this.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;How should Company A account for the fact that they expect students'&#10;  scores to improve naturally over the course of the 10 tests?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I'm unclear whether this concern is in relation to [2], or to another statistical comparison of interest. Taking a repeated measures approach will account for this in the model.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-01T21:14:48.440" Id="41710" LastActivityDate="2012-11-01T21:14:48.440" OwnerUserId="8580" ParentId="41055" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;The van Elteren test, a stratified Wilcoxon Rank Sum Test, is correct. It is available in PROC freq. There's an example on the SAS website support documents. &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-11-01T21:33:31.873" Id="41712" LastActivityDate="2012-11-01T21:33:31.873" OwnerUserId="16452" ParentId="41707" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="41722" AnswerCount="2" Body="&lt;p&gt;I'm looking for reference to get a better understanding and overview of the methods for data modeling (mostly related to business questions like finding categories in data and setting of scoring functions for forecasting). After some internet research I couldn't single out a canonical reference. I have some very basic knowledge of statistics and I believe the topics that could be most useful for me would be SVM, Decision Trees, Clustering, Covariance Matrix, Correlations, Multivariate analysis ... just to name some keywords for the direction. Also I'd like to have some foundation to eventually dive into other statistical and data mining topics.&lt;/p&gt;&#10;&#10;&lt;p&gt;While the reference should introduce the concepts, I'd like it to be fast-paced (not chatty, not too many examples) and mathematically thorough while concentrating on applications. Of course there are specialized books on each topic, but I'd rather learn the basics of all of these topic quickly. So ideally one or two books for all of them together.&lt;/p&gt;&#10;&#10;&lt;p&gt;I cannot think of a different way to explain :)&lt;/p&gt;&#10;&#10;&lt;p&gt;Can you suggest good books? Or maybe just help me get some orientation in the vast field of statistics so that I know what to look for.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-11-01T22:51:40.057" FavoriteCount="1" Id="41714" LastActivityDate="2012-11-02T09:21:19.173" LastEditDate="2012-11-01T22:58:15.123" LastEditorUserId="8694" OwnerUserId="8694" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;classification&gt;&lt;modeling&gt;&lt;data-mining&gt;&lt;references&gt;" Title="Book on data modeling" ViewCount="213" />
  <row Body="&lt;p&gt;If you have some knowledge of R (or want to), this new book sounds like it might be useful to you.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/1461443423&quot; rel=&quot;nofollow&quot;&gt;R for Business Analytics&lt;/a&gt;, A Ohri.&lt;/p&gt;&#10;&#10;&lt;p&gt;It is a very quick and broad tutorial based overview of using R for many business analytics topics, with one chapter dedicated to data mining (using the R Rattle GUI). Don't expect a lot of formal math though.&lt;/p&gt;&#10;&#10;&lt;p&gt;*&lt;a href=&quot;http://intelligenttradingtech.blogspot.com/&quot; rel=&quot;nofollow&quot;&gt;recent review&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;** It's often useful to learn a lot of the practical aspects of data mining and machine learning by learning alongside specific programming language examples. If you have a particular programming language you prefer, I could try to recommend alternatives.&lt;/p&gt;&#10;&#10;&lt;p&gt;***One other broad, but gentle, introduction to these areas (less practical examples, and not business based, but more mathematically inclined) is:&#10;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/026201243X&quot; rel=&quot;nofollow&quot;&gt;Introduction to Machine Learning&lt;/a&gt;, Ethem Alpaydin.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;edit: In response to your comment. If Python is your language of choice, look no further.&#10;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/1420067184&quot; rel=&quot;nofollow&quot;&gt;Machine Learning&lt;/a&gt;, by Stephen Marsland is hands down the best practical book to pick up many of your requested topics, with many good code based illustrations to actually build and follow along.&lt;/p&gt;&#10;&#10;&lt;p&gt;On the Pandas side, you might have some interest in a current Coursera course (just 2 weeks in) that is Python oriented, with Python based applications towards Portfolio analysis, financial markets, and simulation backtesting (although it's a bit introductory, the instructor participates in a AI based Hedge Fund consulting firm).  &lt;a href=&quot;https://www.coursera.org/course/compinvesting1&quot; rel=&quot;nofollow&quot;&gt;Computational Investing Part 1&lt;/a&gt;, Tucker Balch&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-11-02T01:36:45.813" Id="41721" LastActivityDate="2012-11-02T09:21:19.173" LastEditDate="2012-11-02T09:21:19.173" LastEditorUserId="13519" OwnerUserId="13519" ParentId="41714" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am thinking about how one should check for time consistency of in a GLM.&lt;/p&gt;&#10;&#10;&lt;p&gt;Model:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \eta_{ijk} = \beta_{0} + \beta_{i}Year+\beta_{j}Var1+\beta_{k}Var2+\beta_{jk}(Var1 \times Var2) $$&lt;/p&gt;&#10;&#10;&lt;p&gt;If I were to add a time-interaction for the interaction variable (i.e. without time interactions for main effects) $ \beta_{ijk}(Var1\times Var2\times Year)$, and plot the predicted values for each year we would see how the modeled interaction variable varies with time. But would it be more suitable to control for the main effect time interactions as well?&lt;/p&gt;&#10;&#10;&lt;p&gt;Given that the main effects are fairly consistent with time my opinion would be that it sould not matter.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any objections or suggestions?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-11-02T10:07:57.703" Id="41740" LastActivityDate="2012-11-02T11:12:11.630" LastEditDate="2012-11-02T11:12:11.630" LastEditorUserId="686" OwnerUserId="13939" PostTypeId="1" Score="1" Tags="&lt;generalized-linear-model&gt;&lt;interaction&gt;" Title="Time consistency of interaction effect in GLM" ViewCount="75" />
  <row Body="&lt;p&gt;You may use 1.7159*tanh(2/3x) on hidden layers. This sigmoid has the property that it has max of its second derivatives at -1 and +1 values while its asymptotic limits are [-1.5,+1.5]. In that way you network will be more accurate on the points near the decision boundary. &lt;/p&gt;&#10;&#10;&lt;p&gt;The general concept to choose sigmoid for your purpose is to choose the one according to the rule, your output values are in the the range of points, makes the second derivative of sigmoid function maximum.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-02T10:33:39.487" Id="41743" LastActivityDate="2012-11-02T10:33:39.487" OwnerUserId="14289" ParentId="35776" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;box and whisker plots are commonly used to visually compare and summarize &#10;cross validation results.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/zb6aZ.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is an example, taken from the &lt;a href=&quot;http://cran.r-project.org/web/packages/cvTools/index.html&quot; rel=&quot;nofollow&quot;&gt;cvTools&lt;/a&gt; package in R.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(cvTools)&#10;## set up folds for cross-validation&#10;folds &amp;lt;- cvFolds(nrow(coleman), K = 5, R = 50)&#10;## compare LS, MM and LTS regression&#10;# perform cross-validation for an LS regression model&#10;fitLm &amp;lt;- lm(Y ~ ., data = coleman)&#10;cvFitLm &amp;lt;- cvLm(fitLm, cost = rtmspe,&#10;folds = folds, trim = 0.1)&#10;&#10;...&#10;&#10;# plot results for the MM regression model&#10;bwplot(cvFitLmrob)&#10;# plot combined results&#10;bwplot(cvFits)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="4" CreationDate="2012-11-02T18:49:41.467" Id="41779" LastActivityDate="2014-07-23T15:59:59.413" LastEditDate="2014-07-23T15:59:59.413" LastEditorUserId="40501" OwnerUserId="13519" ParentId="41773" PostTypeId="2" Score="8" />
  
  <row AcceptedAnswerId="41910" AnswerCount="2" Body="&lt;p&gt;I recently read an &lt;a href=&quot;http://www.psychologytoday.com/blog/golden-slumbers/201210/increase-longevity-seven-hours-sleep&quot; rel=&quot;nofollow&quot;&gt;article&lt;/a&gt; about how you can increase longevity by sleeping less. This article, like many others I've read, references a statistical study and implies that causation was found between two events(sleeping less and living longer). Well I'm not a statistician, but I'm aware of the common fallacies about causation and I find it very hard to accept the information given in this article and other similar ones. Even further, I can almost never know from an article if the original study found causation or not, without taking the time and digging into it. &lt;/p&gt;&#10;&#10;&lt;p&gt;Although I'd really like to, I'm not going to go into any depth bringing examples of how the causation might be wrong in the given article, because I would most likely be preaching to the choir. Luckily though, to my relief, the original &lt;a href=&quot;http://www.nytimes.com/2002/02/15/us/study-ties-6-7-hours-of-sleep-to-longer-life.html&quot; rel=&quot;nofollow&quot;&gt;article&lt;/a&gt; in NY Times only talks about correlation and even has a disclaimer in the end that basically says that the study doesn't imply causation. &lt;/p&gt;&#10;&#10;&lt;p&gt;Also, I now read that, in accordance with my intuition, it is actually really hard to find and/or test for causation. Reading through the &lt;a href=&quot;http://en.wikipedia.org/wiki/Correlation_does_not_imply_causation&quot; rel=&quot;nofollow&quot;&gt;wiki&lt;/a&gt; page on the subject I saw two models which seemed to be the most common ones used to find actual links in causation: Granger causality test and convergent cross mapping. But I see, that both of these have some gaps in them.      &lt;/p&gt;&#10;&#10;&lt;p&gt;Now, I have a few questions regarding all this.&lt;/p&gt;&#10;&#10;&lt;p&gt;Q1: Are there any models that can, with high accuracy, find actual causation in such widespread studies? If so what are the most common ones?&lt;/p&gt;&#10;&#10;&lt;p&gt;Q2: How often do these widespread studies actually test for causation? E.g. if I read somewhere that a very broad study implies that A causes B, do I have any reason whatsoever to believe that there's something more than correlation behind those claims.&lt;/p&gt;&#10;&#10;&lt;p&gt;Q3: Is there a good technique that can be used by a non-statistician to filter out good statistics from the bad ones in everyday life. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-11-02T22:40:52.927" Id="41784" LastActivityDate="2012-11-05T13:48:54.857" LastEditDate="2012-11-05T06:16:02.343" LastEditorUserId="16483" OwnerUserId="16483" PostTypeId="1" Score="4" Tags="&lt;causal-inference&gt;" Title="Causation implication" ViewCount="229" />
  <row AcceptedAnswerId="41800" AnswerCount="1" Body="&lt;p&gt;I’m reading &lt;em&gt;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/081764055X&quot; rel=&quot;nofollow&quot;&gt;A Probability Path&lt;/a&gt;&lt;/em&gt; by S. Resnick, and I got stuck on one problem:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;strong&gt;Problem 1.16&lt;/strong&gt; (&lt;em&gt;pp. 22-23&lt;/em&gt;) Suppose $C$ is a class of subset of $\Omega$ such that  $\Omega \in C$, &#10;  $A \in C $ implies that $A^c \in C$, and $C$ is closed under&#10;  &lt;em&gt;disjoint&lt;/em&gt; unions. Using an example, show that $C$ does not have to be a field.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;A hint is provided in book: &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Try $\Omega = \{1,2,3,4\}$ and let $C$ be the field generated by two&#10;  point subsets of $\Omega$.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="1" CreationDate="2012-11-03T09:33:44.517" FavoriteCount="1" Id="41799" LastActivityDate="2012-11-05T13:14:04.510" LastEditDate="2012-11-05T13:14:04.510" LastEditorUserId="2970" OwnerUserId="9136" PostTypeId="1" Score="3" Tags="&lt;probability&gt;&lt;self-study&gt;" Title="Using an example, show that $C$ does not have to be a field" ViewCount="141" />
  <row AnswerCount="1" Body="&lt;p&gt;Consider the below ANCOVA example in which I am trying to predict a continuous varibale &lt;code&gt;y&lt;/code&gt; by two variables &lt;code&gt;x&lt;/code&gt; (continuous) and &lt;code&gt;a&lt;/code&gt; (nominal) and their interaction. As I am also interested in the intercept I am using &lt;code&gt;summary.lm()&lt;/code&gt; instead of &lt;code&gt;summary.aov()&lt;/code&gt;. My question is: does the significant intercept in the below example represent an overall intercept (across factor levels of &lt;code&gt;a&lt;/code&gt;) or does this indicate that there is only a significant intercept in the first factor level (this is suggested &lt;a href=&quot;http://www3.imperial.ac.uk/pls/portallive/docs/1/1171922.PDF&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;)?&#10;If yes, is there a way to test for an overall intercept within the ANCOVA model?&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(123)&#10;sd1 &amp;lt;- 1&#10;sd2 &amp;lt;- 3.5&#10;n &amp;lt;- 100&#10;x1 &amp;lt;- 2+rnorm(n,0,sd1)&#10;x2 &amp;lt;- 2+rnorm(n,0,sd1)&#10;y1 &amp;lt;- x1+2+rnorm(n,0,sd2)&#10;y2 &amp;lt;- -x2-2+rnorm(n,0,sd2)&#10;a &amp;lt;- as.factor(c(rep(&quot;a1&quot;, n), rep(&quot;a2&quot;, n)))&#10;x &amp;lt;- c(x1,x2)&#10;y &amp;lt;- c(y1,y2)&#10;&#10;m1 &amp;lt;- lm(y1~x1); m2 &amp;lt;- lm(y2~x2)&#10;summary(m1); summary(m2)&#10;model &amp;lt;- lm(y~x*a)&#10;summary.lm(model)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2012-11-03T11:28:40.850" Id="41801" LastActivityDate="2012-11-03T22:32:22.410" LastEditDate="2012-11-03T22:32:22.410" LastEditorUserId="10389" OwnerUserId="10389" PostTypeId="1" Score="1" Tags="&lt;multiple-regression&gt;&lt;ancova&gt;" Title="Interpretation of intercept in ANCOVA" ViewCount="542" />
  <row Body="&lt;p&gt;OK, I've got it. Thank you, StasK, for such a good answer. I'll keep it accepted for others to learn, but in my particular case I was missing a very simple fact:&lt;/p&gt;&#10;&#10;&lt;p&gt;The procedure of bootstrap in accordance to Hall&amp;amp;Wilson guidelines for simple one-sampled mean test is this (in R-inspired pseudo code):&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;em&gt;&lt;code&gt;1&lt;/code&gt;&lt;/em&gt;&lt;code&gt;function(data&lt;/code&gt;, $\theta_0$ &lt;code&gt;) {&lt;/code&gt;&lt;br&gt;&#10;  &lt;em&gt;&lt;code&gt;2&lt;/code&gt;&lt;/em&gt; $\hat{\theta} \leftarrow $ &lt;code&gt;t.test(data, mu =&lt;/code&gt; $\theta_0$ &lt;code&gt;)$statistic&lt;/code&gt; &lt;br&gt;&#10;  &lt;em&gt;&lt;code&gt;3&lt;/code&gt;&lt;/em&gt; &lt;code&gt;count&lt;/code&gt; $\leftarrow 0$ &lt;br&gt;&#10;  &lt;em&gt;&lt;code&gt;4&lt;/code&gt;&lt;/em&gt;&lt;code&gt;for(i in 1:1000){&lt;/code&gt;&lt;br&gt;&#10;  &lt;em&gt;&lt;code&gt;5&lt;/code&gt;&lt;/em&gt; &lt;code&gt;bdata&lt;/code&gt; $\leftarrow$ &lt;code&gt;sample(data)&lt;/code&gt; &lt;br&gt;&#10;  &lt;em&gt;&lt;code&gt;6&lt;/code&gt;&lt;/em&gt; $\hat{\theta^*} \leftarrow $ &lt;code&gt;t.test(bdata, mu =&lt;/code&gt; $\hat{\theta}$ &lt;code&gt;)$statistic&lt;/code&gt; &lt;br&gt;&#10;  &lt;em&gt;&lt;code&gt;7&lt;/code&gt;&lt;/em&gt; &lt;code&gt;if (&lt;/code&gt; $\hat{\theta^*} \le \hat{\theta} $ &lt;code&gt;) count++&lt;/code&gt;&lt;br&gt;&#10;  &lt;em&gt;&lt;code&gt;8&lt;/code&gt;&lt;/em&gt;  &lt;code&gt;}&lt;/code&gt;&lt;br&gt;&#10;  &lt;em&gt;&lt;code&gt;9&lt;/code&gt;&lt;/em&gt; &lt;code&gt;count/1000&lt;/code&gt; &lt;br&gt;&#10;  &lt;em&gt;&lt;code&gt;10&lt;/code&gt;&lt;/em&gt; &lt;code&gt;}&lt;/code&gt;&lt;br&gt;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;The part I missed was that the $\theta_0$ was &quot;used&quot; in line &lt;em&gt;&lt;code&gt;2&lt;/code&gt;&lt;/em&gt; (where we set the reference $\hat{\theta}$).&lt;/p&gt;&#10;&#10;&lt;p&gt;It is interesting to note, that in the line &lt;em&gt;&lt;code&gt;2&lt;/code&gt;&lt;/em&gt; and &lt;em&gt;&lt;code&gt;6&lt;/code&gt;&lt;/em&gt; we could equally easily use &lt;code&gt;p.value&lt;/code&gt; instead of &lt;code&gt;statistic&lt;/code&gt;. In that case we should also change the $\le$ into $\ge$ in line &lt;em&gt;&lt;code&gt;7&lt;/code&gt;&lt;/em&gt;. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-03T11:32:21.767" Id="41802" LastActivityDate="2012-11-04T07:45:54.233" LastEditDate="2012-11-04T07:45:54.233" LastEditorUserId="10069" OwnerUserId="10069" ParentId="41683" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;The default for (non-ordered) factors in R are treatment contrasts, where the means of the factor levels are tested against the reference category (the first level). In this case, the intercept of the model (and all other coefficients) only hold for the reference level of the categorial variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hence, in your model&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lm(y ~ x * a)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;the coefficients of both the intercept and the predictor &lt;code&gt;x&lt;/code&gt; are estimated for the first level of &lt;code&gt;a&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you want to test the overall intercept, you need to specify another contrast for &lt;code&gt;a&lt;/code&gt;, for example a sum contrast. (If &lt;code&gt;a&lt;/code&gt; had more than two levels, you had to contstruct a different type of contrast coding to achieve the same tests like treatment contrasts.)&lt;/p&gt;&#10;&#10;&lt;p&gt;The type of contrast used by &lt;code&gt;lm&lt;/code&gt; can be specified with the &lt;code&gt;contrasts&lt;/code&gt; parameter:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lm(y ~ x * a, contrasts = list(a = contr.sum))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The p-values of both &lt;code&gt;a&lt;/code&gt; and the interaction between &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;x&lt;/code&gt; will be the same as in your model, but you inherently will obtain different results for the intercept and &lt;code&gt;x&lt;/code&gt;.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-11-03T12:19:08.453" Id="41805" LastActivityDate="2012-11-03T12:31:46.030" LastEditDate="2012-11-03T12:31:46.030" LastEditorUserId="13680" OwnerUserId="13680" ParentId="41801" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;I would look into using the &lt;a href=&quot;http://en.wikipedia.org/wiki/Akaike_information_criterion&quot; rel=&quot;nofollow&quot;&gt;Aikake Information Criterion&lt;/a&gt;, or the &lt;a href=&quot;http://en.wikipedia.org/wiki/Bayesian_information_criterion&quot; rel=&quot;nofollow&quot;&gt;Bayesian Information Criterion&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;What software are you using? Most statistical software should take care of this automatically for you.&lt;/p&gt;&#10;&#10;&lt;p&gt;Essentially they are ways of determining if the number of regression parameters is justified in terms of how much explanatory value they add to the model.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-11-03T15:26:30.333" Id="41810" LastActivityDate="2012-11-03T15:26:30.333" OwnerUserId="7883" ParentId="41808" PostTypeId="2" Score="0" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I know that a basic one-sample $t$ statistic can be calculated by &#10;$$t = \frac{x - \mu}{s / \sqrt{n}}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I also know that the following 4 functions $x\sim\mathcal{U}(0,1)$, $x\sim\mathcal{Exp}(2)$ (the exponential function where  $\lambda=2$), $P(x=1/2) =  1$, and the set of $\{P(x=025)=0.8, P(x=1.5)=0.2\}$, all have an $\mathbb{E}(x)=0.5$.&lt;/p&gt;&#10;&#10;&lt;p&gt;I've written a program to simulate the $t$-statistic for all of them, and have the values from $0, 0.25, 0.5,\ldots ,1.5$ with the values in 0.25 increments on the $t$-statistics for $n=10000$.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm looking for a way to theoretically prove the following 2 equations $P(x=1/2) = 1$ and the set of $\{P(x=025)=0.8, P(x=1.5)=0.2\}$. The simulation values for the first all 1's from 0.5 to 1.5 (as would be intuitive here) and I want to express it theoretically.  The values for the second are $0,0,0,0.8045,0.8045,\ldots ,0.8045$ for the set. &lt;/p&gt;&#10;&#10;&lt;p&gt;I'm trying to theoretically reason out and mathematically express the proof and equations that express how these values are gotten (not simulated but calculated).  For example, I'm trying to calculate the value for the set function and reason out why the simulated value is slightly above 0.8 at 0.8045 while the calculated value should be straight 0.8 from 0.5 to 1.5.  I feel like I'm missing something that would be obvious to another set of eyes, but that I'm missing after even the 100th time I look at it over and over.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; What I'm mainly not understanding here is the set case. I'm trying to express why the simulated value is 0.8045 (just above 0.800) whereas the $P(x=0.25)=0.8$ which I would think would give a sim value of 0.800 but doesn't.  I edited to make this clearer, but I'll re-read it a few more times to see if I can make it clearer still.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-11-03T21:53:52.670" Id="41829" LastActivityDate="2012-11-04T11:25:46.017" LastEditDate="2012-11-04T11:25:46.017" LastEditorUserId="805" OwnerUserId="16121" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;simulation&gt;&lt;t-distribution&gt;" Title="Finding t distribution values for uniform and exponential functions" ViewCount="100" />
  
  <row Body="&lt;p&gt;There are some problems with the data:&lt;/p&gt;&#10;&#10;&lt;p&gt;First there are still many rows that have zero, indicating that they don't belong to any of the 8 categories. A quick tabulation shows that 32 cases have not been assigned to any group, 1 case was assigned to two groups, only 67 of them seem correct.&lt;/p&gt;&#10;&#10;&lt;p&gt;Second, the variables cat5 and cat7 are perfectly collinear. That's why it's still being kicked out after you have removed cat6. I am guessing your categorization script that made the dummies might have some typos due to copy and paste, go check that.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-04T14:06:06.203" Id="41853" LastActivityDate="2012-11-04T14:06:06.203" OwnerUserId="13047" ParentId="41670" PostTypeId="2" Score="6" />
  <row AnswerCount="0" Body="&lt;p&gt;I am newbie in Excel and I wanted to know is it possible to make Microsoft Excel to interpret following dataset:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;2    18(o)    60(o)    5572(r)    4612(r)    6481(r)    7930&#10;3    17(o)    59(o)    4422(r)    3435(r)    4792(r)    6716&#10;4    16(o)    58(o)    2820(r)    1904(r)    3679(r)    5562&#10;5    15(o)    57(o)    1706(r)    790(r)    2622(r)    3879&#10;6    14(o)    56(o)    634(r)    272(r)    1718(r)    2722&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;as following (all chars except digits must be eliminated):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;2    18    60    5572    4612    6481    7930&#10;3    17    59    4422    3435    4792    6716&#10;4    16    58    2820    1904    3679    5562&#10;5    15    57    1706    790    2622    3879&#10;6    14    56    634    272    1718    2722&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;for further diagram plotting from this data.&lt;/p&gt;&#10;" ClosedDate="2012-11-04T17:34:17.823" CommentCount="3" CreationDate="2012-11-04T15:54:55.817" Id="41856" LastActivityDate="2012-11-04T15:54:55.817" OwnerUserId="14368" PostTypeId="1" Score="1" Tags="&lt;data-visualization&gt;&lt;dataset&gt;&lt;excel&gt;" Title="Extracting numbers from excel table items for diagram visualization" ViewCount="82" />
  
  <row Body="&lt;p&gt;If I understand you correctly, there is no mystery; what could be happening is that the two highly correlated measures are very close to each other in terms of how could the split is. &lt;/p&gt;&#10;&#10;&lt;p&gt;In your case, though, it isn't very close, it's exact. In particular, total number of words and fourth root of words will split identically, since they are monotonic (given that number of words must be positive). By including both in a tree, you are asking the tree algorithm to pick one at random. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-11-04T18:08:11.760" Id="41864" LastActivityDate="2012-11-04T18:08:11.760" OwnerUserId="686" ParentId="41862" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;Not without more information. If you have enough overlapping data, you can follow Peter's advice, and use both variables in your regression. If not, you may be able to add in some information from previous studies on the relationship between age and the balance score. If the relationship is very robust, you may be able to adjust your data to account for the effect of age. However, be very careful when doing this, and make sure to clearly state your methodology and assumptions, so that you don't (accidentally) mislead your audience.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-11-04T23:53:08.560" Id="41883" LastActivityDate="2012-11-04T23:53:08.560" OwnerUserId="9007" ParentId="41876" PostTypeId="2" Score="0" />
  <row AnswerCount="1" Body="&lt;p&gt;I am trying to generate binomial probabilities (in R) as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;${N \choose{k}} p^{k} (1-p)^{(n-k)}$&lt;/p&gt;&#10;&#10;&lt;p&gt;My problem is given $p \approx 0.03$, and $N =400$, $k&amp;gt;270$, I get the probability equal to $0$. I am actually evauating all the $k$'s up to $n$, so a high $n$ returns a probability matrix that has $0$'s for $k &amp;gt;270$. &lt;/p&gt;&#10;&#10;&lt;p&gt;I was wondering if there is a function in R / or an approximation (binomial or normal) that you would recommend in this case? &lt;/p&gt;&#10;&#10;&lt;h2&gt;Edit&lt;/h2&gt;&#10;&#10;&lt;p&gt;I ve tried dbinom function in R as well - &lt;code&gt;dbinom(400,280,0.03) = 0&lt;/code&gt;. So that does not help.. What I am after is a non-zero approximation..&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-11-05T00:08:37.937" Id="41885" LastActivityDate="2012-11-05T04:46:34.873" LastEditDate="2012-11-05T04:46:34.873" LastEditorUserId="2116" OwnerUserId="13106" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;probability&gt;&lt;binomial&gt;&lt;approximation&gt;" Title="Generating non-zero binomial probabilities (n,k ) with small p and large n - k" ViewCount="96" />
  <row Body="&lt;p&gt;I would imagine the DCC suffers the same limitations as the regular correlation with non-normal data. That is, there isn't an &lt;strong&gt;assumption&lt;/strong&gt; of normality, but non-normal data can cause odd findings; see the Anscombe quartet, for example.&lt;/p&gt;&#10;&#10;&lt;p&gt;As for kurtosis, taking the log can certainly make it worse. Take this example of the uniform distribution:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(2810101)&#10;x &amp;lt;- runif(100)&#10;logx &amp;lt;- log(x)&#10;library(moments)&#10;kurtosis(x)&#10;kurtosis(logx)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;where a Normally distributed variable has kurtosis of 3. &lt;/p&gt;&#10;&#10;&lt;p&gt;on the other hand, in this example&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(2829101)&#10;z &amp;lt;- c(rnorm(1000, 10, 1), rnorm(1000, 10, .01))&#10;kurtosis(z)&#10;kurtosis(log(z))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;However, you mention skewed data with kurtosis. Was your data right skew or left skew? Since the former is more common, I'll guess that. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(1919110)&#10;x &amp;lt;- c(rnorm(1000, 10, 1), rnorm(300, 30, 2), runif(10, 500, 600))&#10;skewness(x)&#10;kurtosis(x)&#10;skewness(log(x))&#10;kurtosis(log(x))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Here, taking the log improves kurtosis and skewness.&lt;/p&gt;&#10;&#10;&lt;p&gt;Taking the log had almost no effect on kurtosis.&lt;/p&gt;&#10;&#10;&lt;p&gt;As always, try plotting the data to see what is going on in your correlation.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-05T00:13:36.130" Id="41887" LastActivityDate="2012-11-05T00:13:36.130" OwnerUserId="686" ParentId="41884" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;Suppose all the simple correlations between $x_i$ and $x_j$ are $r$ for all $i,j=1,2,\dots,p, i\neq j.p&amp;gt;8$. Find the partial correlation coefficient $r_{1p.2468}.$&lt;/p&gt;&#10;&#10;&lt;p&gt;By definition, $$r_{1p.2468}=\frac{cov(e_{1.2468},e_{p.2468})}{\sqrt{var(e_{1.2468})}\sqrt{var(e_{p.2468})}}$$&#10;how can I find $cov(e_{1.2468},e_{p.2468}),var(e_{1.2468}),var(e_{p.2468})$ in terms of $r$?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-05T06:30:02.300" Id="41891" LastActivityDate="2012-11-12T15:21:57.313" LastEditDate="2012-11-12T15:21:57.313" LastEditorUserId="7290" OwnerUserId="12710" PostTypeId="1" Score="1" Tags="&lt;self-study&gt;&lt;multivariate-analysis&gt;&lt;partial-correlation&gt;" Title="Find the partial correlation coefficient $r_{1p.2468}.$" ViewCount="92" />
  <row AcceptedAnswerId="41897" AnswerCount="2" Body="&lt;p&gt;If I have only $\mathrm{Var}(X)$, how can I calculate $\mathrm{Var}(\frac{1}{X})$?&lt;/p&gt;&#10;&#10;&lt;p&gt;I do not have any information about the distribution of $X$, so I cannot use transformation, or any other methods which use the probability distribution of $X$. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-11-05T09:15:36.337" FavoriteCount="5" Id="41896" LastActivityDate="2013-09-05T05:12:52.053" LastEditDate="2013-09-05T05:12:52.053" LastEditorUserId="805" OwnerUserId="16534" PostTypeId="1" Score="6" Tags="&lt;distributions&gt;&lt;variance&gt;&lt;data-transformation&gt;" Title="Var(X) is known, how to calculate Var(1/X)?" ViewCount="2635" />
  <row Body="&lt;p&gt;It is impossible.&lt;/p&gt;&#10;&#10;&lt;p&gt;Consider a sequence $X_n$ of random variables, where &lt;/p&gt;&#10;&#10;&lt;p&gt;$$P(X_n=n-1)=P(X_n=n+1)=0.5$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Then: &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\newcommand{\Var}{\mathrm{Var}}\Var(X_n)=1 \quad \text{for all $n$}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;But $\Var\left(\frac{1}{X_n}\right)$ approaches zero as $n$ goes to infinity:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\Var\left(\frac{1}{X_n}\right)=\left(0.5\left(\frac{1}{n+1}-\frac{1}{n-1}\right)\right)^2$$&lt;/p&gt;&#10;&#10;&lt;p&gt;This example uses the fact that $\Var(X)$ is invariant under translations of $X$, but $\Var\left(\frac{1}{X}\right)$ is not.&lt;/p&gt;&#10;&#10;&lt;p&gt;But even if we assume $\mathrm{E}(X)=0$, we can't compute $\Var\left(\frac{1}{X}\right)$:&#10;Let &lt;/p&gt;&#10;&#10;&lt;p&gt;$$P(X_n=-1)=P(X_n=1)=0.5\left(1-\frac{1}{n}\right)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and &lt;/p&gt;&#10;&#10;&lt;p&gt;$$P(X_n=0)=\frac{1}{n} \quad \text{for $n&amp;gt;0$} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;Then $\Var(X_n)$ approaches 1 as $n$ goes to infinity, but $\Var\left(\frac{1}{X_n}\right)=\infty$ for all $n$.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-05T09:57:37.970" Id="41897" LastActivityDate="2013-08-31T20:15:34.887" LastEditDate="2013-08-31T20:15:34.887" LastEditorUserId="27581" OwnerUserId="5296" ParentId="41896" PostTypeId="2" Score="12" />
  <row AcceptedAnswerId="43592" AnswerCount="3" Body="&lt;p&gt;I am trying to solve this problem on and off for the past couple of months but to no success. This was supposed to be a very small part of my PhD thesis in navigation but I guess I underestimated the problem. It sounded trivial at the beginning, but now I am not so sure.&lt;/p&gt;&#10;&#10;&lt;p&gt;Lets say we have two ships, each with its own nominal position in 2D coordinates (mean). Due to errors in positioning systems we can only be certain that the ships are within 1 mile of the mean with 95% probability (normal distribution). Given these 2 positions and this probability distribution, what is the probability that the ships are within 5 miles from each other? Also, same question if the ship's probable position is an ellipse, not a circle.&lt;/p&gt;&#10;&#10;&lt;p&gt;I asked some people and they told me that there are no analytic solutions. If that is really the case, please explain how to solve it numerically.&lt;/p&gt;&#10;&#10;&lt;p&gt;As you can already tell, I come from engineering background, therefore my math is more than a bit rusty.&lt;/p&gt;&#10;&#10;&lt;p&gt;I apologize in advance if the question is too vague or too trivial for this forum. I will be more than happy to explain in more detail if needed.&lt;/p&gt;&#10;&#10;&lt;p&gt;I found &lt;a href=&quot;http://stats.stackexchange.com/questions/12209/percentage-of-overlapping-regions-of-two-normal-distributions&quot;&gt;this&lt;/a&gt;, but it is only for univariate case, and besides I don't know how to implement it in my case where I need to find the probability that the distance between two ships is less than 5 miles. &lt;/p&gt;&#10;&#10;&lt;p&gt;I imagine this problem as a plane with two hills that intersect and the solution is the volume under the circle with diameter of 5 miles that is located somewhere between the two peaks of hills (means).&lt;/p&gt;&#10;&#10;&lt;p&gt;Am I on the right track?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="16" CreationDate="2012-11-05T10:20:49.957" FavoriteCount="2" Id="41900" LastActivityDate="2012-11-14T17:16:35.063" OwnerUserId="16541" PostTypeId="1" Score="5" Tags="&lt;probability&gt;&lt;normal-distribution&gt;&lt;pdf&gt;&lt;bivariate&gt;" Title="Probability of collision (two bivariate normal distributions)" ViewCount="614" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;Suppose I have 2 independent variables whereby normal distribution can be assumed.&#10;However, there is unequal variance and interaction between the two factors is unknown.&lt;/p&gt;&#10;&#10;&lt;p&gt;Could someone suggest an alternative to 2-way ANOVA with the above mentioned scenario.&lt;/p&gt;&#10;&#10;&lt;p&gt;I know one possible non-parametric test is the Friedman's test. &#10;Are there any other non-parametric test available?&lt;/p&gt;&#10;" ClosedDate="2013-09-19T22:15:27.793" CommentCount="4" CreationDate="2012-11-05T17:24:30.287" FavoriteCount="1" Id="41934" LastActivityDate="2013-09-19T21:11:03.173" LastEditDate="2012-11-05T17:32:18.327" LastEditorUserId="930" OwnerUserId="16554" PostTypeId="1" Score="2" Tags="&lt;anova&gt;&lt;nonparametric&gt;&lt;hypothesis-testing&gt;" Title="Non-parametric alternative for 2-way ANOVA" ViewCount="6107" />
  <row Body="&lt;p&gt;This is only a partial answer to the question I think you're asking.&lt;/p&gt;&#10;&#10;&lt;p&gt;I think with RD we assume that conditional on treatment, the other variables are smooth functions of the assignment variable $z$. This means that the outcome variable $y$ should jump at the cutoff only because of the discontinuity in the level of treatment. (Well, technically only continuity at the cutoff is required, but a global assumption is somewhat easier to test.)&lt;/p&gt;&#10;&#10;&lt;p&gt;This differs from IV, since the assignment variable $z$ can have a direct impact on the outcome $y$, not just on the probability of treatment $x$, but not a discontinuous impact.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-11-05T18:59:37.370" Id="41939" LastActivityDate="2012-11-05T18:59:37.370" OwnerUserId="7071" ParentId="41923" PostTypeId="2" Score="0" />
  
  
  <row Body="&lt;p&gt;It makes sense if you're taking the $\theta$ as known. If you want to incorporate the uncertainty in $\theta$ you would either simulate from the joint distribution (asymptotically Gaussian) of the other parameters and $\theta$, or you could simulate one conditionally on the other (since $\theta$ is an input to the GLM, I'd suggest simulating $\theta$ from the normal approximation to its profile likelihood and then $\mu$ from the conditional of $\mu$ on $\theta$, since you get that bit of information out of the GLM). Then proceed as above to simulate the negative binomial.&lt;/p&gt;&#10;&#10;&lt;p&gt;This is quite similar to an approach Ripley has discussed (for example on the R-help mailing list) -- e.g. &lt;a href=&quot;https://stat.ethz.ch/pipermail/r-help/2003-May/033165.html&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; (though he was particularly terse in that one). &lt;/p&gt;&#10;&#10;&lt;p&gt;[If anyone has a reference for that approach, it would round out the answer to this question nicely.]&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-06T03:41:41.680" Id="42958" LastActivityDate="2012-11-06T03:41:41.680" OwnerUserId="805" ParentId="29962" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;One way this could happen is if the data are evenly spaced on the first PC and have a break on the second PC. e.g.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(2881010)&#10;PC1 &amp;lt;- runif(100, 0, 100)&#10;PC2 &amp;lt;- c(rnorm(50,-2,.5), rnorm(50, 2, .5))&#10;plot(PC1, PC2)&#10;&#10;&#10;junk &amp;lt;- cbind(PC1, PC2)&#10;library(cluster)&#10;m1 &amp;lt;- diana(junk, diss = FALSE)&#10;m2 &amp;lt;- kmeans(junk, 2)&#10;plot(x = PC1, y = PC2, col = m2$cluster)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;which leads to beautiful clusters on the 2nd dimension, even though it varies much less than the first &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-11-06T11:26:55.910" Id="42971" LastActivityDate="2012-11-06T11:26:55.910" OwnerUserId="686" ParentId="42957" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I think I've got a reasonable partial solution for the hierarchical Bayesian model. &lt;code&gt;rjags&lt;/code&gt; Code below....  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;dflong$dv &amp;lt;- scale(dflong$dv)[,1]&#10;dataList = list(  &#10;    y = dflong$dv, &#10;    rmFac  = dflong$rmFac ,&#10;    dvFac  = dflong$dvFac ,&#10;    id     = dflong$id ,&#10;    Ntotal = length(dflong$dv) ,&#10;    NrmLvl = length(unique(dflong$rmFac)),&#10;    Ndep   = length(unique(dflong$dvFac)),&#10;    NsLvl  = length(unique(dflong$id))&#10;)&#10;&#10;modelstring = &quot;&#10;model {&#10;for( i in 1:Ntotal ) {&#10;    y[i] ~ dnorm( mu[i] , tau[rmFac[i], dvFac[i]])&#10;    mu[i] &amp;lt;- a0[ dvFac[i] ] + aS[id[i], dvFac[i]] + a1[rmFac[i] , dvFac[i]]&#10;}&#10;for (k in 1:Ndep){&#10;    for ( j in 1:NrmLvl ) { &#10;        tau[j, k] &amp;lt;- 1 / pow( sigma[j, k] , 2 )&#10;        sigma[j, k] ~  dgamma(1.01005,0.1005)&#10;    }&#10;}&#10;for (k in 1:Ndep) {&#10;    a0[k] ~ dnorm(0, 0.001)&#10;    for (s in 1:NsLvl){&#10;        aS[s, k] ~ dnorm(0.0, sTau[k])&#10;    }&#10;    for (j in 1:NrmLvl) {&#10;        a1[j, k] ~ dnorm(0, a1Tau[k])&#10;    }&#10;    a1Tau[k] &amp;lt;- 1/ pow( a1SD[k] , 2)&#10;    a1SD[k]  ~ dgamma(1.01005,0.1005)&#10;&#10;    sTau[k] &amp;lt;- 1/ pow( sSD[k] , 2)&#10;    sSD[k]  ~ dgamma(1.01005,0.1005)&#10;}&#10;}&#10;&quot; # close quote for modelstring&#10;writeLines(modelstring,con=&quot;model.txt&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Again, base Bayesian repeated measures script from &lt;a href=&quot;http://doingbayesiandataanalysis.blogspot.com.au/&quot; rel=&quot;nofollow&quot;&gt;Kruschke&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-06T14:19:32.230" Id="42978" LastActivityDate="2012-11-06T14:51:50.480" LastEditDate="2012-11-06T14:51:50.480" LastEditorUserId="966" OwnerUserId="966" ParentId="41596" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have some samples that have been treated with various chemical agents. Users score the treatment effect using categorical values/bins (i.e $&amp;lt; 1, 2.5, 8, ... &amp;gt; 100$). Within a range of say 1-100 they can get some fairly precise metric but outside of that range they input greater or less than some value. So the question is a user wants to know how similar one sample is to another... So I could do some sort of hierarchical clustering but I'm wondering with this mixed data type: &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;What could be an alternative approach? &lt;/li&gt;&#10;&lt;li&gt;What distance metric would be suitable?&lt;/li&gt;&#10;&lt;li&gt;Are there alternative methods to clustering?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2012-11-06T18:49:46.167" Id="42995" LastActivityDate="2014-05-10T14:50:02.170" LastEditDate="2014-05-10T14:50:02.170" LastEditorUserId="26338" OwnerUserId="10341" PostTypeId="1" Score="1" Tags="&lt;clustering&gt;&lt;hierarchical&gt;" Title="Comparing sample profiles" ViewCount="23" />
  
  <row AcceptedAnswerId="43002" AnswerCount="2" Body="&lt;p&gt;I have a highly-imbalanced test data set. The positive set consists of 100 cases while the negative set consists of 1500 cases. On the training side, I have a  larger candidate pool: the positive training set has 1200 cases and  the negative training set has 12000 cases. For this kind of scenario, I have several choices:&lt;/p&gt;&#10;&#10;&lt;p&gt;1)  Using weighted SVM for the whole training set (P: 1200, N: 12000)&lt;/p&gt;&#10;&#10;&lt;p&gt;2)  Using SVM based on the sampled training set (P:1200, N :1200), the 1200 negative cases are sampled from 12000 cases.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there any theoretical guidance on deciding which approach is better?  Since the test data set is highly imbalanced, should I use the imbalanced training set as well?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-11-06T21:28:43.453" FavoriteCount="4" Id="42999" LastActivityDate="2012-12-06T17:29:34.443" OwnerUserId="3026" PostTypeId="1" Score="7" Tags="&lt;machine-learning&gt;&lt;classification&gt;&lt;data-mining&gt;&lt;svm&gt;&lt;bioinformatics&gt;" Title="training approaches for highly-imbalanced data set" ViewCount="3036" />
  <row AcceptedAnswerId="43004" AnswerCount="1" Body="&lt;p&gt;I collected data for a discrete probability distribution and I have the pairs (value, probability).&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Value    Probability&#10; 90      0,0033&#10;125      0.0204&#10;180      0.0847&#10;250      0.2516&#10;355      0.4653&#10;500      0.175&#10;710      0.0015&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I have been asked to compute a gaussian fit over that data but I am having troubles.&#10;Is it possible (and does it make sense) to fit a gaussian distribution over it?&lt;/p&gt;&#10;&#10;&lt;p&gt;I tried to build the corresponding gaussian distribution by computing mean (&lt;code&gt;334&lt;/code&gt;) and standard deviation (&lt;code&gt;100&lt;/code&gt;) of my data but, of course, it does not work.&lt;/p&gt;&#10;&#10;&lt;p&gt;Gaussian density function values are related to continuous values while the probability I have are continuous.&lt;/p&gt;&#10;&#10;&lt;p&gt;I guess that I have too few discrete values to compute the gaussian distribution, but I am not really into statistics so I am not sure and I'd like to double check with you that I'm not missing something.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-06T21:35:10.753" Id="43000" LastActivityDate="2012-11-06T22:38:06.443" LastEditDate="2012-11-06T21:58:53.243" LastEditorUserId="10841" OwnerUserId="10841" PostTypeId="1" Score="2" Tags="&lt;fitting&gt;" Title="Compute gaussian fit corresponding to a discrete variable" ViewCount="449" />
  <row Body="&lt;p&gt;Why are you not satisfied with the Spearman's rank correlation?&lt;/p&gt;&#10;&#10;&lt;p&gt;The &quot;ordinary&quot; correlation is typcially the &lt;a href=&quot;http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient&quot; rel=&quot;nofollow&quot;&gt;Pearson product-moment correlation coefficient&lt;/a&gt;. It is only suitable for data that have a linear relationship and do not have any outliers.&lt;/p&gt;&#10;&#10;&lt;p&gt;For data that do not exhibit a linear relationship, or may contain outliers, you can use the &lt;a href=&quot;http://en.wikipedia.org/wiki/Rank_correlation&quot; rel=&quot;nofollow&quot;&gt;Rank Correlation&lt;/a&gt;, such as the &lt;a href=&quot;http://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient&quot; rel=&quot;nofollow&quot;&gt;Spearman's correlation&lt;/a&gt; or the &lt;a href=&quot;http://en.wikipedia.org/wiki/Kendall%27s_tau_rank_correlation_coefficient&quot; rel=&quot;nofollow&quot;&gt;Kendall's τ&lt;/a&gt;. This does not need a normal distribution. You can find out more about those in the different Wikipedia articles.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2012-11-06T23:05:59.040" Id="43007" LastActivityDate="2012-11-06T23:05:59.040" OwnerUserId="12615" ParentId="43005" PostTypeId="2" Score="2" />
  
  
  
  <row Body="&lt;p&gt;My impression (perhaps mistaken?) is that your goal is to put a prior on sigma instead of on tau (which equals 1/sigma^2), because it is more intuitive to deal with sigam. As Erik replied earlier, this is straight forward in JAGS/BUGS:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;tau &amp;lt;- pow(sigma,-2)&#10;sigma ~ thePriorOfYourChoice&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;There are various examples of this in Doing Bayesian Data Analysis, such as Figure 18.1, p. 494. For an example of putting a gamma prior on sigma, see this blog post: &lt;a href=&quot;http://doingbayesiandataanalysis.blogspot.com/2012/04/improved-programs-for-hierarchical.html&quot; rel=&quot;nofollow&quot;&gt;http://doingbayesiandataanalysis.blogspot.com/2012/04/improved-programs-for-hierarchical.html&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-11-07T02:32:10.363" Id="43016" LastActivityDate="2012-11-07T02:32:10.363" OwnerUserId="16592" ParentId="41187" PostTypeId="2" Score="4" />
  
  
  <row AcceptedAnswerId="43041" AnswerCount="1" Body="&lt;p&gt;I have some categorical data from a clinic taken over a year long time period. The plan is to perform a Pearson's Chi Squared test (of independence) on this data.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am extracting data from a clinic's database and my aim is to see if male patients are more likely to cancel their appointment than female patients.&lt;/p&gt;&#10;&#10;&lt;p&gt;My query is this: Chi squared requires independent samples but there is a chance that patients may have had multiple appointments within that time period. Whilst I could identify these multi-attenders, how would I then assign them to any one category (presuming some would appear in multiple different categories ).&lt;/p&gt;&#10;&#10;&lt;p&gt;For instance...&lt;/p&gt;&#10;&#10;&lt;p&gt;John Doe may have:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Attended a cardiology appointment in January&lt;/li&gt;&#10;&lt;li&gt;Attended a cardiology appointment in February&lt;/li&gt;&#10;&lt;li&gt;Cancelled a cardiology appointment in March&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;To push this to it's limit:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;John Doe then goes through a (very fast) gender reassignment procedure and becomes Jane Doe and then cancels their cardiology appointment in April.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;So from one person we have:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Male/Cancelled: 1&lt;/li&gt;&#10;&lt;li&gt;Male/Attended: 2&lt;/li&gt;&#10;&lt;li&gt;Female/Cancelled: 1&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Is Chi Squared the correct test to use in this instance? If so, is there some data processing I should be performing on my data to account for patients who appear multiple times in different categories?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-07T11:05:26.307" Id="43037" LastActivityDate="2012-11-07T12:44:32.603" OwnerUserId="13526" PostTypeId="1" Score="1" Tags="&lt;chi-squared&gt;&lt;independence&gt;" Title="Independence over time" ViewCount="66" />
  
  <row AcceptedAnswerId="43064" AnswerCount="1" Body="&lt;p&gt;Conjugate prior is useful and beautiful in theory, for instance, Dirichlet distribution can serves as the conjugate prior for multinomial distribution. But I don't find any actual applications based on that.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can you provide some?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-11-07T13:48:08.243" Id="43043" LastActivityDate="2012-11-07T18:01:16.847" OwnerUserId="9950" PostTypeId="1" Score="0" Tags="&lt;multinomial&gt;&lt;dirichlet-distribution&gt;&lt;application&gt;&lt;conjugate-prior&gt;" Title="Any actual applications based on &quot;dirichlet distribution servers as a conjugate prior for multinomial distribution&quot;" ViewCount="92" />
  
  <row Body="&lt;p&gt;From my perspective this is completely legitimate question, which is in fact asked by many of my customers.&lt;/p&gt;&#10;&#10;&lt;p&gt;The mismatch can be attributed to the following:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;strong&gt;Missing Data&lt;/strong&gt;. SPSS by default excludes missing data case-wise. So if you happen to have missing observations in one factor, the cases on which the means are computed for the model (and, say, 2nd factor) are different (smaller in number) compared to those on which descriptive statistics are based.&lt;/li&gt;&#10;&lt;li&gt;Marginal means are computed with assumption, that each cell (i.e. combination of factor levels) &lt;em&gt;has equal weight&lt;/em&gt;. This is in accordance with the more basic assumptions of ANOVA, and this is how basically ANOVA sees the data. So, if you have e.g. sex on two levels and education on two levels, the marginal mean for males is computed as $\frac{(\text{Mean of males with high education}) + (\text{Mean of males with college education})}{2}$ rather than &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;$\frac{(\text{Mean of males with high edu}) \cdot (\text{# males with high edu}) + (\text{Mean of males with college edu}) \cdot (\text{# males with college edu})}{\text{Number of males with any edu}}$, which equivalent to simple mean of males with any education.&lt;/p&gt;&#10;&#10;&lt;p&gt;This second point is equivalent to the answers given by &#10;ttnphns and Kavin Kane, but in (my opinion) easier language.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-11-07T14:42:38.377" Id="43049" LastActivityDate="2012-11-07T17:18:17.730" LastEditDate="2012-11-07T17:18:17.730" LastEditorUserId="10069" OwnerUserId="10069" ParentId="41789" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;There are specific tools out there for statistical analysis. For example, R, SPSS, and Minitab.&lt;/p&gt;&#10;&#10;&lt;p&gt;Nevertheless, both Mathematica and MATLAB are capable of doing the required computations; which is &quot;better&quot; is a matter of taste and application requirements, most of the time.&lt;/p&gt;&#10;&#10;&lt;p&gt;MATLAB doesn't ship default with many statistical subroutines. However, it does have a Statistics toolbox that contains much of the functionality found default in some other packages. It all depends on what exactly you're trying to do, statistics wise, and what your comfort level with programming is. Maybe you can elaborate more on what you're attempting.&lt;/p&gt;&#10;&#10;&lt;p&gt;(Of course, the man-hours spent re-programming an algorithm for, say, ANOVA is often costlier than just buying the license to the toolbox that has a stock routine outright).&lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, to answer your question, I personally prefer MATLAB. I find that it is easier to program in and allows for easier rapid development and testing of algorithms, etc. Syntactically, I prefer MATLAB. MATLAB likes to keep things close to pseudo-code. Mathematica tries to keep things closer to mathematical notation.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can find MATLAB tutorials all over the internet. Just google &quot;MATLAB tutorial&quot; and you'll have a slew of results.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-07T17:11:03.977" Id="43072" LastActivityDate="2012-11-07T17:11:03.977" OwnerDisplayName="gorcee" OwnerUserId="13355" ParentId="43071" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Wolfram &lt;em&gt;Mathematica&lt;/em&gt; is a very capable software for doing statistics, and unlike Matlab, its statistical functionality is included in the core &lt;em&gt;Mathematica&lt;/em&gt;. &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Unlike R or Matlab it provides &lt;a href=&quot;http://www.wolfram.com/mathematica/new-in-8/probability-and-statistics-solvers-and-properties/&quot;&gt;symbolic support&lt;/a&gt; for probability computations. You can peruse &lt;a href=&quot;http://reference.wolfram.com/mathematica/guide/ProbabilityAndStatistics.html&quot;&gt;Probability &amp;amp; Statistics guide page&lt;/a&gt; to get an idea about the functionality.&lt;/li&gt;&#10;&lt;li&gt;&lt;em&gt;Mathematica&lt;/em&gt; is often faster than R for statistical data crunching. &lt;/li&gt;&#10;&lt;li&gt;&lt;em&gt;Mathematica&lt;/em&gt; has superior &lt;a href=&quot;http://www.wolfram.com/mathematica/new-in-8/statistical-visualization/&quot;&gt;statistical visualization&lt;/a&gt; functionality. See &lt;a href=&quot;http://reference.wolfram.com/mathematica/guide/StatisticalVisualization.html&quot;&gt;Statistical Visualization&lt;/a&gt; guide page on the web.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="9" CreationDate="2012-11-07T17:25:28.947" Id="43074" LastActivityDate="2012-11-07T17:25:28.947" OwnerDisplayName="Sasha" OwnerUserId="6174" ParentId="43071" PostTypeId="2" Score="5" />
  
  
  
  
  <row Body="&lt;p&gt;In general, multiple imputation works by using &lt;em&gt;all available information&lt;/em&gt; in the model to simulate the missing values: I use the word &quot;simulate&quot; because you're technically doing more than just prediction, which involves more parametric assumptions.&lt;/p&gt;&#10;&#10;&lt;p&gt;I assume the outcomes are either jointly missing or jointly observed, there are no cases where outcome A is known when outcome B isn't, or vice versa. &lt;/p&gt;&#10;&#10;&lt;p&gt;Collinearity is not an issue. If you are treating these outcomes as independent (reporting odds ratios from two logistic regression models with separate outcomes), then you don't even need to worry about whether contradictory outcomes are simulated: (e.g. patients both discharged living and died within 30 days -- I assume you wouldn't have observed that in the hospital).&lt;/p&gt;&#10;&#10;&lt;p&gt;Two overall thoughts about the analysis:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Why aren't you reporting a Cox proportional hazards model? Treating death/discharge as a 1/0 event indicator and time until the observed discharge or death as the time-to-event is a very similar, common, and preferred analysis. The hazard ratios approximate relative risks, just like odds ratios (for rare outcomes) and patients who are discharged &lt;em&gt;should be censored&lt;/em&gt;. This way you use all available information about when patients were at risk for dying, it is a much more powerful analysis.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;I don't really agree with the need for missing data methods. Complete case analyses of data (where rows of missing data are dropped from the analysis) are unbiased and require little validation of assumptions, unlike Multiple Imputation which has caveats of estimating and validating parametric models. 13% is fairly negligible when $n$ is 150 or more. As a rule of thumb, having 20 events (these are deaths) per variable in the adjusted model is sufficient for power considerations.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="14" CreationDate="2012-11-07T20:15:14.307" Id="43082" LastActivityDate="2012-11-07T20:15:14.307" OwnerUserId="8013" ParentId="41628" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;It seems to me that a first step would be to try to create some models of how tcp header data might relate to your categories. That is, do you have any theories?&lt;/p&gt;&#10;&#10;&lt;p&gt;If you do, it might turn out that you need to preprocess your packet info: for example using the window size of the previous packet rather than the current one, or the using the day of the week instead of the day of the month.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then you need to look carefully at your inputs and outputs. Are they categorical (&quot;car&quot;, &quot;truck&quot;), ordered categorical (&quot;small&quot;, &quot;medium&quot;, &quot;large&quot;), etc? Your linear regression is probably treating your categories like they're continuous (1..N) and your plot shows there's no such linear relationship -- and there's probably no reason to expect there should be.&lt;/p&gt;&#10;&#10;&lt;p&gt;Once you have an idea of models that might make sense, have meaningful variables, and know the types of these variables, methods will naturally fall into place. (For example, continuous variables in and binary category out naturally suggests logistic regression.)&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT: In terms of logistic regression, it can be used with multiple outcomes. Look for multinomial logistic regression.&lt;/p&gt;&#10;&#10;&lt;p&gt;In terms of validation, you train your model with your training set then predict on the validation data and see how accurate you are. Obviously, if you look at your accuracy on your training data, it'll tend to overestimate your accuracy since it's what you tuned your model to. A better test of how you'll do in the real world is to use data that your tuning (training) process never used.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-11-07T20:37:07.950" Id="43084" LastActivityDate="2012-11-08T20:45:15.817" LastEditDate="2012-11-08T20:45:15.817" LastEditorUserId="1764" OwnerUserId="1764" ParentId="43040" PostTypeId="2" Score="2" />
  
  <row AcceptedAnswerId="43231" AnswerCount="1" Body="&lt;p&gt;I will be running a machine learning anomaly detection algorithm on some data.  It's a binary classification problem: each data point is either anomalous or normal.  I have some known anomalous data that I can use in testing the features I select.&lt;/p&gt;&#10;&#10;&lt;p&gt;I will be using the F1 score for testing the features I've selected for this algorithm.  What would be considered a good F1 score?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-08T00:16:50.580" FavoriteCount="2" Id="43102" LastActivityDate="2013-11-19T10:27:48.867" LastEditDate="2012-11-09T00:54:03.530" LastEditorUserId="88" OwnerUserId="1618" PostTypeId="1" Score="4" Tags="&lt;machine-learning&gt;" Title="Good F1 score for anomaly detection" ViewCount="1229" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I would like to compare before-and-after &quot;scores&quot; on five quality of life questions across two groups. What I would like to know are:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Are the baseline scores across the two groups similar?&lt;/li&gt;&#10;&lt;li&gt;Is the change in scores significant (before and after) within each group?&lt;/li&gt;&#10;&lt;li&gt;Is the change in scores significant across both groups?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Will an independent t-test suffice to answer question number 1?&lt;/p&gt;&#10;&#10;&lt;p&gt;Will it be appropriate to perform a Wilcoxon signed rank test to answer question number 2? If not, what will be the more appropriate test?&lt;/p&gt;&#10;&#10;&lt;p&gt;On question 3, is Mann-whitney appropriate?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you so much for the help.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-08T03:28:48.437" Id="43113" LastActivityDate="2013-05-07T12:59:48.893" OwnerUserId="16380" PostTypeId="1" Score="4" Tags="&lt;repeated-measures&gt;&lt;ordinal&gt;&lt;scales&gt;" Title="Comparison of before and after ordinal data across two groups" ViewCount="392" />
  
  <row AcceptedAnswerId="43212" AnswerCount="1" Body="&lt;p&gt;I need to calculate the mean(+/-SD) final value of a variable from the mean(+/-SD) baseline value and the mean(+/-SD) change from baseline. &#10;So, for example:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Mean baseline weight is 40(+/-2) kg.&lt;/li&gt;&#10;&lt;li&gt;Mean change from baseline is +5(+/-1) kg.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I'd assume the correct final mean is 45kg, but how would I go about calculating a new SD?&lt;/p&gt;&#10;&#10;&lt;p&gt;I've googled for methods of calculating 'Pooled SD', but those only seem to apply to pooling same variables (like weights from different groups) and I don't think those apply here.&#10;&lt;a href=&quot;http://stats.stackexchange.com/questions/21999/how-to-calculate-sd-from-pre-sd-and-post-sd&quot;&gt;This&lt;/a&gt; question seems the most similar to my question but also seems to try the exact opposite of what I'm trying to achieve. &lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-11-08T11:13:00.643" FavoriteCount="1" Id="43140" LastActivityDate="2012-11-10T02:11:32.017" LastEditDate="2012-11-10T02:11:32.017" LastEditorUserId="183" OwnerUserId="10622" PostTypeId="1" Score="4" Tags="&lt;standard-deviation&gt;" Title="How to calculate the time 2 standard deviation given time 1 and change standard deviations?" ViewCount="536" />
  
  
  <row Body="&lt;p&gt;The earlier answer doesn't address the question of the gender composition of the top $5$ or top $10$ players. The analytical answer is simple, and it doesn't depend on the underlying distribution (as long as it is the same for men and for women, and continuous, and each person's ability is assumed to be independent of anyone else's). Under these assumptions, the number of women among the top $k$ follows a hypergeometric distribution very close to a binomial distribution. If there are $10$ times as many men participating as women, then each spot has a $1/11$ chance to be occupied by a woman. For that question, the techniques used to produce the results in the paper you cite are not needed. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you want to find the expected value of the $i$th highest out of a sample of size $n$, this is the expected value of an order statistic, and this does depend (slightly) on the distribution. For a uniform distribution on $[0,1]$, the expected value of the $i$th highest value out of $n$ is $\frac{n+1-i}{n+1}$. I think the expected value for an order statistic of a normal distribution doesn't have a closed form in general, but there are &lt;a href=&quot;http://stats.stackexchange.com/questions/9001/approximate-order-statistics-for-normal-random-variables&quot;&gt;good approximations&lt;/a&gt; which tell you how to adjust the naive guess of $\Phi^{-1}(\frac{n+1-i}{n+1})$ standard deviations above the mean.&lt;/p&gt;&#10;&#10;&lt;p&gt;While it may be worth understanding these order statistics as a null hypothesis, I doubt that the distribution of ratings for chess players is so well approximated by the normal distribution that the top ratings out of many millions of players are properly predicted by the corresponding values for a normal distribution. Typically, when you use a normal approximation, you don't count on it working several standard deviations from the mean, and it certainly doesn't work in the other direction.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-08T14:54:29.453" Id="43153" LastActivityDate="2012-11-08T14:54:29.453" OwnerUserId="11981" ParentId="12647" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="43227" AnswerCount="2" Body="&lt;p&gt;Consider the following setup: two mouse strains (&quot;KO&quot; and &quot;WT&quot;) have been compared in three independent experiments (&quot;E1&quot;, &quot;E2&quot; and &quot;E3&quot;). In each experiment, there were two groups corresponding to the two mouse strains compared, and each group consisted of four different mice. There were all in all 8 mice per experiment, and three experiments (total of 4 x 2 x 3 = 24 mice).&lt;/p&gt;&#10;&#10;&lt;p&gt;The question is, of course, whether the strains differ.&lt;/p&gt;&#10;&#10;&lt;p&gt;In R, I would do this as follows, given the variables &lt;code&gt;strain&lt;/code&gt; and &lt;code&gt;experiment&lt;/code&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;a &amp;lt;- data.frame(&#10;  response= c(100, 110, 120, 130, 200, 210, 220, 230, 105, 115, 125, 135, 205, &#10;              215, 225, 235, 105, 115, 125, 135, 215, 225, 235, 245),&#10;  experiment= rep( c( &quot;E1&quot;, &quot;E2&quot;, &quot;E3&quot; ), each= 8 ),&#10;  strain= rep( rep( c( &quot;WT&quot;, &quot;KO&quot; ), each= 4 ), 3 ) )&#10;&#10;myaov &amp;lt;- aov( response ~ strain + Error( experiment ), data= a ) &#10;summary( myaov )&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;However, I have been asked this question by persons who work on a regular basis with GraphPad Prism. It does feature &quot;repeated measures ANOVA&quot; which is (in the help file) said to be equivalent to random block design, but I find the explanations somewhat confusing:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&quot;Repeated measures&quot; vs. &quot;randomized block&quot; experiments&lt;/p&gt;&#10;  &#10;  &lt;p&gt;The term repeated measures is appropriate when you made repeated measurements from each subject.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;Some experiments involve matching but not repeated measurements. The term randomized-block describes these kinds of experiments. For example, imagine that the three rows were three different cell lines. All the Y1 data came from one experiment, and all the Y2 data came from another experiment performed a month later. The value at row 1, column A, Y1 (23) and the value at row 1, column B, Y1 (28) came from the same experiment (same cell passage, same reagents). The matching is by row.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;Randomized block data are analyzed identically to repeated-measures data. Prism always uses the term repeated measures, so you should choose repeated measures analyses when your experiment follows a randomized block design.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;My question: how can I analyse these data using GraphPad Prism?&lt;/strong&gt; Or, alternatively: does the quoted description fit my case?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-11-08T15:33:45.847" Id="43157" LastActivityDate="2012-11-13T23:21:03.670" LastEditDate="2012-11-08T20:54:44.223" LastEditorUserId="14803" OwnerUserId="14803" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;anova&gt;" Title="Mixed (type III) model ANOVA in R and GraphPad Prism" ViewCount="1564" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm keeping some fish for toxicity test and will analyze specific parameter after exposing toxicants. I am keeping the fish in normal condition and check the parameter daily as &quot;baseline&quot;. I plan to run unit root test to confirm whether there is change after toxicant exposure. However, I'm not quite sure how long the &quot;baseline&quot; data I need to obtain will considered &quot;enough&quot; for the upcoming analysis. Anyone may provide me hints? Thank you.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-08T10:47:02.237" Id="43160" LastActivityDate="2012-11-08T17:12:03.647" OwnerDisplayName="user1788005" OwnerUserId="18211" PostTypeId="1" Score="1" Tags="&lt;r&gt;" Title="In a time series study, how many time points are enough for unit root test" ViewCount="70" />
  
  
  <row Body="&lt;p&gt;What you are trying to do is essentially (1) fitting a model such as a non-parametric smoother for the data mean and then (2) estimating the standard deviation of the data about the mean with that model. Your current model is a nearest-neighbors approach. You might be interested in other more sophisticated approaches to the same problem, such as smoothing splines, kernel regression, or loess (local regression). Once you fit the nonparametric smoother then you can estimate the standard deviation of your measurement error by looking at residuals. All of these methods are available in R, and at least one is probably available in a software package of your choice.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-08T18:41:00.023" Id="43167" LastActivityDate="2012-11-08T18:41:00.023" OwnerUserId="16297" ParentId="43154" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I think little or nothing can be said with no more assumptions than stated.  With more assumptions about the dependence among the $X_n$, some limit theorems are proved in &lt;a href=&quot;http://www.d.umn.edu/~yqi/mydownload/08letter.pdf&quot; rel=&quot;nofollow&quot;&gt;this paper&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-08T19:36:18.577" Id="43169" LastActivityDate="2012-11-08T19:36:18.577" OwnerUserId="89" ParentId="43078" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;Why do you have to compare them?  If this is a homework assignment then you should indicate that and you should give more detail on what the assignment means by compare.&lt;/p&gt;&#10;&#10;&lt;p&gt;One way to get standard errors/confidence intervals/significance tests for LTS coefficients would be to bootstrap the process.&lt;/p&gt;&#10;&#10;&lt;p&gt;A possibly more interesting comparison would be to compare in a scatterplot (or possibly a Bland-Altman plot) the predicted values from the 2 models with an $y=x$ line.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then there is always the simple eye-ball comparison that the coefficient for Air.Flow did not change much, but the one for Water.Temp is a quarter of what it was under LS, which point(s) drive that? and Acid.Conc is much closer to 0, but was not sigificantly different before.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-11-08T20:17:19.913" Id="43177" LastActivityDate="2012-11-08T20:17:19.913" OwnerUserId="4505" ParentId="43171" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;&lt;strong&gt;Pairs plots&lt;/strong&gt;: This is &lt;em&gt;not&lt;/em&gt; a method of dimensionality reduction, but it is a really good way to get a quick overview of where some meaningful relationships might lie. In R, the base package contains the &lt;code&gt;pairs()&lt;/code&gt; function, which is good for continuous data (it converts everything to continuous). A better function is &lt;code&gt;ggpairs()&lt;/code&gt;, from the &lt;code&gt;GGally&lt;/code&gt; package:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(GGally)&#10;ggpairs(iris, colour='Species')&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/nH1uS.png&quot; alt=&quot;Iris pairs plot&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2012-11-09T02:54:18.217" CreationDate="2012-11-08T23:48:01.183" Id="43191" LastActivityDate="2012-11-08T23:48:01.183" OwnerUserId="9007" ParentId="41326" PostTypeId="2" Score="3" />
  <row AnswerCount="1" Body="&lt;p&gt;I'm trying to explore (rather than formally test) relationships for various subgroups between a continuous X and a binary Y (0 or 1). A good shortcut would seem to be to graph a smoothed X-Y fit for each subgroup.  But even when, at various levels of X, the mean of Y ranges from .1 to .3, R's smoothed fitline often simply hugs the horizontal where Y=0.  This is not informative at all and in fact seems inaccurate.&lt;/p&gt;&#10;&#10;&lt;p&gt;I've tried several different ways to obtain a smoother as well as many different values of f.  E.g.:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x=runif(500,0,1)  #x is continuous&#10;y=1:500&#10;y=ifelse(y &amp;lt; 100,1,0)  #y is now binary with mean ~ 0.2&#10;&#10;z=1:500&#10;z=ifelse(z&amp;lt;50 | (z&amp;lt;300 &amp;amp; z&amp;gt;100),1,0)  #z is binary too&#10;&#10;&#10;plot(x [z==1], y [z==1])&#10;m=lowess(x[z==1], y[z==1])&#10;lines(m, col='blue')  #The line hugs the points at y=0&#10;windows()&#10;scatter.smooth(x [z==1], y[z==1])   &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In this case this last line did produce the expected sort of fitline, but it's not a reliable occurrence when I work with real data.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-09T00:50:55.347" Id="43194" LastActivityDate="2012-11-10T15:15:19.240" OwnerUserId="2669" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;loess&gt;" Title="With a binary Y, why are R's lowess fits so often flat?" ViewCount="507" />
  
  <row Body="&lt;p&gt;Impossible, as explained in Whuber's comment:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The SD is the root of the variance. Let $X$ be the baseline weight and $Y$ the change from baseline. You need the variance of $X+Y$. It equals the variance of $X$ plus the variance of $Y$ &lt;em&gt;plus&lt;/em&gt; twice the covariance of $X$ and $Y$. If there is nonzero correlation between $X$ and $Y$, that covariance will be nonzero and cannot be neglected.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="3" CreationDate="2012-11-09T10:11:33.850" Id="43212" LastActivityDate="2012-11-09T15:19:34.543" LastEditDate="2012-11-09T15:19:34.543" LastEditorUserId="919" OwnerUserId="10622" ParentId="43140" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;As some of you may now, various statistical tests are used in the task of finding collocations in given text(either a single document or corpus of texts). Chi-square, t-test, likelihood ratio(with binomial distribution assumed). These tests are used to check if two words occur together significantly more often that apart from each other, that's how the strenth of collocation is measured.&lt;/p&gt;&#10;&#10;&lt;p&gt;A-a-anyway, one good thing to do is put your hands onto a large corpus of texts and save contingency tables(or just frequencies, really) of all the bigrams(two words) and unigrams(one word, obviously) from this corpus. We call this information background.&lt;/p&gt;&#10;&#10;&lt;p&gt;Little introduction for you there, now to the main part:&#10;when you receive a text(call it foreground), you extract all bigrams from it and have to decide whether this is a collocation or just some pair of words, like &quot;of the&quot; or &quot;and yes&quot;, which do not bear any meaning and not at all important in this particular text.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, to the question: when you stumble upon a bigram that has in fact been seen in background, it means that both unigrams it consists of can be found in background too, all frequencies are in place, everything else is a piece of cake.&lt;/p&gt;&#10;&#10;&lt;p&gt;But what if this bigram has never occurred before? The only place we can find it in is our foreground, but there's not enough information to infer the distribution and collocation strength. I mean, you can apply any of these statistical tests to foreground contingency table, but it won't be the same, you know? Not that reliable, I suppose.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, at last, to the question itself, all two parts of it:&lt;/p&gt;&#10;&#10;&lt;p&gt;1) What to do with bigrams that have never occurred in background corpus(and both unigrams are not there either, just two words we haven't seen before)&lt;/p&gt;&#10;&#10;&lt;p&gt;2) What to do with bigrams only one part of which has occurred in background corpus?&lt;/p&gt;&#10;&#10;&lt;p&gt;3) What to do with bigrams that have never occured in background corpus, but both words from it are present in background corpus?&lt;/p&gt;&#10;&#10;&lt;p&gt;How to make it smooth, so some scores of collocation strength can be created for all these cases and were at least a little reliable.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you in advance, kick this question to whatever site is appropriate if I've misunderstood the purpose of this particular part of stackexchange.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;&lt;strong&gt;UPD&lt;/em&gt;&lt;/strong&gt;: my 9th attempt to post it ended up with trimmed question body, I'm sorry, now it's intact.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-11-09T10:41:13.590" Id="43213" LastActivityDate="2012-11-09T12:25:52.177" LastEditDate="2012-11-09T12:25:52.177" LastEditorUserId="11768" OwnerUserId="11768" PostTypeId="1" Score="1" Tags="&lt;statistical-significance&gt;&lt;chi-squared&gt;&lt;nlp&gt;" Title="Combining contingency tables derived from background and foreground dataset" ViewCount="117" />
  
  
  <row AcceptedAnswerId="43248" AnswerCount="1" Body="&lt;p&gt;$\DeclareMathOperator{\E}{\mathrm{E}}$&lt;/p&gt;&#10;&#10;&lt;p&gt;An investor has $\$100,000$. When the current interest rate is $i$% (compounded continously so that the growth per year is $e^{i/100}$) she invests her money in an $i$ year CD, takes the profits, and then immediately reinvests the $\$100,000$. Suppose that the $k$th investment has an investment rate of $X_k$% where the $X_k \overset{\mathrm{iid}}{\sim} \operatorname{Uniform}\{1,2,3,4,5\}$. In the long run, how much money does she make per year?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;My Thoughts&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Modeling this as a renewal process, if I set the reward in the $i$th period $\E r_i$ and the length of the $i$th period, $\E t_i$ equal to $$\E r_i = \sum_{i = 1}^5 100,000(e^{i/100} - 1) \cdot i \approx 56,141.5 \qquad \E t_i = {1 + 2 + 3 + 4 + 5 \over 5} = 3$$ Then by one of our theorem, I should get the long-run rate at which rewards are earned as $${\E r_i \over \E t_i} = 18,713.83 $$ This should be my answer.  Is this a valid approach, or have I overlooked something?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-11-09T15:53:39.753" Id="43236" LastActivityDate="2012-11-09T21:43:28.810" OwnerUserId="13022" PostTypeId="1" Score="2" Tags="&lt;stochastic-processes&gt;" Title="Random Investment" ViewCount="105" />
  
  <row Body="&lt;p&gt;Look into &lt;a href=&quot;http://en.wikipedia.org/wiki/Pearson_distribution&quot; rel=&quot;nofollow&quot;&gt;Pearson distribution&lt;/a&gt;. This family is reach enough so that every 4-tuple of moments $m_1$, $m_2$, $m_3$ and $m_4$ corresponds to a unique (up to equivalence) member of the family.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is the parametrization on the case at hand in terms of skewness $\mathcal{s}$ and kurtosis $\kappa$:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;  \operatorname{Pearson}\left(\underbrace{2 \left(5 \kappa -6&#10;   \mathcal{s}^2-9\right)}_{a_1}, \underbrace{(\kappa +3) \mathcal{s} \sigma^2}_{a_0}, \underbrace{2 \kappa -3&#10;   \left(\mathcal{s}^2+2\right)}_{b_2},\underbrace{(\kappa +3) \mathcal{s}&#10;   \sigma^2}_{b_1}, \underbrace{\sigma^4 \left(4 \kappa -3 \mathcal{s}^2\right)}_{b_0} \right)&#10;$$&#10;The pdf of this distribution $f(x)$ satisfies differential equation:&#10;$$&#10;    f^\prime(x) + \frac{a_1 x+a_0}{b_2 x^2+b_1 x+b_0} f(x) = 0&#10;$$&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-09T16:00:29.453" Id="43237" LastActivityDate="2012-11-09T16:00:29.453" OwnerUserId="6174" ParentId="43205" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="43269" AnswerCount="1" Body="&lt;p&gt;I am analyzing a data set using a one-way, within-subjects anova with 12 levels. Each of the levels represents a unique trial type in a reaction time experiment. However, the trials can be reasonably grouped together into 4 equal groups, let us call them A through D. My primary interest is in showing that A differs from the controls (B-D). Following a significant Anova, linear contrasts show that this is the case. However, it seems  that I should also demonstrate that B-D are not different from each other. Parsimony suggests I could run another Anova (rather than pairwise tests) to demonstrate this. However, I have never seen this done. Is this simply convention or is there a more rigorous reason that I am missing? Thanks!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-09T19:32:11.710" Id="43252" LastActivityDate="2012-11-10T00:57:41.923" OwnerUserId="7172" PostTypeId="1" Score="2" Tags="&lt;anova&gt;&lt;post-hoc&gt;" Title="Post hoc ANOVA following a significant ANOVA?" ViewCount="194" />
  <row Body="&lt;p&gt;If one of your predictions are missing, I'm guessing that the reason for that is that at least one of your predictor values (X) is missing for that observation.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you want a prediction for that observation, you will have to supply the predictor variables that are missing. Ideally, you would take another measurement of the missing X variables. If that is impossible, you can impute the missing predictor values with k Nearest Neighbor analysis, an EM algorithm, or most crudely just the mean of the other predictors.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-10T00:46:28.170" Id="43268" LastActivityDate="2012-11-10T00:46:28.170" OwnerUserId="16228" ParentId="43267" PostTypeId="2" Score="1" />
  
  
  
  
  
  
  
  <row AnswerCount="2" Body="&lt;p&gt;How to choose the best values of alpha and beta in Holt's exponential smoothing? Leaving it upon R gives me $\alpha$ =1. Is this appropriate? &lt;/p&gt;&#10;&#10;&lt;p&gt;Entering different values of alpha and then comparing with the real data shows best result for $\alpha$ = 0.45. But then R calculates $\beta$=0.99. Is this fine?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-11T13:54:04.160" Id="43333" LastActivityDate="2014-03-25T12:32:42.520" LastEditDate="2013-01-16T11:26:07.987" LastEditorUserId="1352" OwnerUserId="2959" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;time-series&gt;&lt;forecasting&gt;&lt;exponential-smoothing&gt;" Title="Value of alpha and beta in Holt's exponential smoothing method" ViewCount="3221" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Can someone give me references on the theory of histograms applied to multivariate data? Are there any rules like the one dimensional Freedman-Diaconis available for the high dimensional case? What are the most common drawbacks? I'm planning to use the histogram to do density estimation.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-11-11T15:59:02.563" Id="43340" LastActivityDate="2012-11-11T21:48:36.480" LastEditDate="2012-11-11T21:48:36.480" LastEditorUserId="930" OwnerUserId="16722" PostTypeId="1" Score="1" Tags="&lt;data-visualization&gt;&lt;references&gt;&lt;histogram&gt;&lt;density&gt;" Title="References regarding rules for multidimensional histograms" ViewCount="44" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;As a follow-up to &lt;a href=&quot;http://stats.stackexchange.com/questions/41390/test-for-effects-of-two-categorical-variables-on-a-binary-response-variable&quot;&gt;this question&lt;/a&gt;, I have the following data:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;   Site Treatment Survival&#10;1   BED        DN      1.0&#10;2   BED        DN      1.0&#10;3   BED        DN      1.0&#10;4   BED        MB      1.0&#10;5   BED        MB      1.0&#10;6   BED        MB      0.9&#10;7   BED    Forest      0.4&#10;8   BED    Forest      0.5&#10;9   BED    Forest      0.4&#10;10  BRO        DN      0.9&#10;11  BRO        DN      1.0&#10;12  BRO        DN      1.0&#10;13  BRO        MB      1.0&#10;14  BRO        MB      1.0&#10;15  BRO        MB      1.0&#10;16  BRO    Forest      1.0&#10;17  BRO    Forest      1.0&#10;18  BRO    Forest      1.0&#10;19  LAP        DN      0.8&#10;20  LAP        DN      0.4&#10;21  LAP        DN      0.6&#10;22  LAP        MB      0.5&#10;23  LAP        MB      1.0&#10;24  LAP        MB      0.7&#10;25  LAP    Forest      0.2&#10;26  LAP    Forest      0.2&#10;27  LAP    Forest      0.4&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;on which I ran a binomial glm :&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; glm.out &amp;lt;- glm(Survival~Site*Treatment, data=surv,&#10;    family=&quot;binomial&quot;, weights=rep(10, nrow(surv)))&#10;&amp;gt; anova(glm.out, test=&quot;Chisq&quot;)&#10;&#10;Analysis of Deviance Table&#10;Model: binomial, link: logit&#10;Response: Survival&#10;Terms added sequentially (first to last)&#10;&#10;               Df Deviance Resid. Df Resid. Dev P(&amp;gt;|Chi|)    &#10;NULL                              26    138.254              &#10;Site            2   63.098        24     75.155 1.988e-14 ***&#10;Treatment       2   42.991        22     32.164 4.620e-10 ***&#10;Site:Treatment  4   13.874        18     18.290  0.007707 ** &#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;All effects are significant, so now I want to do post-hoc comparisons.  I have discovered the &lt;code&gt;glht&lt;/code&gt; function in the multcomp package, which seems to do what I want.  I read somewhere that with 2 independent variables, you should set &lt;code&gt;interaction_average=TRUE&lt;/code&gt; to get a result equivalent to a TukeyHSD for a linear model.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; Treat.comp &amp;lt;- glht(glm.out, mcp(Treatment=&quot;Tukey&quot;, interaction_average=TRUE))&#10;&amp;gt; summary(Treat.comp)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;which gives me these strange results:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;     Simultaneous Tests for General Linear Hypotheses&#10;Multiple Comparisons of Means: Tukey Contrasts&#10;Fit: glm(formula = Survival ~ Site * Treatment, family = &quot;binomial&quot;, &#10;    data = surv.san, weights = rep(10, nrow(surv.san)))&#10;&#10;Linear Hypotheses:&#10;                 Estimate Std. Error z value Pr(&amp;gt;|z|)&#10;DN - MB == 0       -0.202   3316.127   0.000        1&#10;Forest - MB == 0   -1.886   3316.127  -0.001        1&#10;Forest - DN == 0   -1.684   3316.127  -0.001        1&#10;(Adjusted p values reported -- single-step method)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Why are my P-values = 1 for all comparisons even though the Treatment effect was highly significant?  &lt;/p&gt;&#10;&#10;&lt;p&gt;If I try using &lt;code&gt;glht&lt;/code&gt; with &lt;code&gt;interaction_average=FALSE&lt;/code&gt; I get more reasonable results but with a warning message:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Warning message:&#10;In mcp2matrix(model, linfct = linfct) :&#10;  covariate interactions found -- default contrast might be inappropriate&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Can someone help me to understand what I am doing wrong?  Thank-you!!!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-11T20:45:26.417" Id="43361" LastActivityDate="2015-01-27T16:09:32.880" LastEditDate="2012-11-11T20:52:28.957" LastEditorUserId="16337" OwnerUserId="16337" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;multiple-comparisons&gt;&lt;binomial&gt;&lt;post-hoc&gt;" Title="Multiple comparisons with the glht function in R gives strange results" ViewCount="2125" />
  <row Body="&lt;p&gt;If your residual is high enough I believe you have explained the variance of the dependent variable by the independent variables. However, if the variance isn't explained sufficient by the independent variables you might want to consider to do a little further analysis with the help of some of the time series analysis packages that are available in R. Decomposing or STL might give you an insight into the components of the time series (trend, seasonality and randomness). &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-11T21:10:24.797" Id="43362" LastActivityDate="2012-11-11T21:10:24.797" OwnerUserId="12853" ParentId="43176" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;1) Only assess normality for the cases where you assume it. (I don't think this is an issue in your case, but it's a common problem so it bears mentioning.)&lt;/p&gt;&#10;&#10;&lt;p&gt;2) When checking a normality assumption, a Q-Q plot &lt;a href=&quot;http://stats.stackexchange.com/questions/36212/what-tests-do-i-use-to-confirm-that-residuals-are-normally-distributed/36220#36220&quot;&gt;is a &lt;em&gt;better&lt;/em&gt; idea than a formal hypothesis test&lt;/a&gt; - hypothesis tests don't actually answer the relevant question.&lt;/p&gt;&#10;&#10;&lt;p&gt;3) The Kruskal-Wallis test is the nonparametric rank-based equivalent to a one-way ANOVA. The Kruskal-Wallis is to the Wilcoxon-Mann-Whitney two-sample test as one-way ANOVA is to a two-sample t-test. It is used when you want to test against the null that more than two groups have the same location.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hope this helps some.&lt;/p&gt;&#10;&#10;&lt;p&gt;--&lt;/p&gt;&#10;&#10;&lt;p&gt;If you want to test specifically for a difference in &lt;em&gt;means&lt;/em&gt;, neither the K-S test nor the W-M-W really does it (though with some additional assumptions the W-M-W is also a test for a difference in means). &lt;/p&gt;&#10;&#10;&lt;p&gt;The best way to test for a difference in means is probably to do a permutation test, as long as the distributions would be the same under the null (so if your alternative is a location-shift, you're basically assuming identical shapes apart from location). &lt;/p&gt;&#10;&#10;&lt;p&gt;The one-sample K-S test is a test for a fully specified distribution - it can test any continuous distribution you can give the pdf for -- you would NOT use that to test the normality of data because unless you know the population parameters, the distribution isn't fully specified... and if you knew the population parameters, you &lt;em&gt;would not need to test the means!&lt;/em&gt; You could use a Smirnov test (&lt;a href=&quot;http://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test#Two-sample_Kolmogorov.E2.80.93Smirnov_test&quot; rel=&quot;nofollow&quot;&gt;a two sample K-S test&lt;/a&gt;) to test for any kind of difference between the two groups, but if your interest is a difference in means it's not a very powerful test.&lt;/p&gt;&#10;&#10;&lt;p&gt;Your confusion about what is being tested with K-S may be because you're muddling the two (the one and two-sample tests) together - they're used for different things.&lt;/p&gt;&#10;&#10;&lt;p&gt;Yes, a Kruskal-Wallis applied to two samples would give the same result as a Wilcoxon-Mann-Whitney for a two-tailed test (i.e. it doesn't allow a one-sided alternative, in the same way that a one-way ANOVA doesn't give you the one-sided alternative you can get with a two-sample t-test and a chi-square doesn't give the one-sided alternative of a two-sample proportions test).&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-11-12T00:32:58.653" Id="43371" LastActivityDate="2012-11-12T22:53:27.360" LastEditDate="2012-11-12T22:53:27.360" LastEditorUserId="805" OwnerUserId="805" ParentId="43368" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;I would say experience -- basic ideas are:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;to fit how classifiers work; giving a geometry problem to a tree, oversized dimension to a kNN and interval data to an SVM are not a good ideas&lt;/li&gt;&#10;&lt;li&gt;remove as much nonlinearities as possible; expecting that some classifier will do Fourier analysis inside is rather naive (even if, it will waste a lot of complexity there)&lt;/li&gt;&#10;&lt;li&gt;make features generic to all objects so that some sampling in the chain won't knock them out&lt;/li&gt;&#10;&lt;li&gt;check previous works -- often transformation used for visualisation or testing similar types of data is already tuned to uncover interesting aspects&lt;/li&gt;&#10;&lt;li&gt;avoid unstable, optimizing transformations like PCA which may lead to overfitting&lt;/li&gt;&#10;&lt;li&gt;experiment a lot&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="5" CreationDate="2012-11-12T00:55:53.880" Id="43372" LastActivityDate="2012-11-12T00:55:53.880" OwnerUserId="88" ParentId="43365" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;When Y is bounded, beta-regression often makes sense; see the paper &lt;a href=&quot;http://psychology3.anu.edu.au/people/smithson/details/betareg/Smithson_Verkuilen06.pdf&quot;&gt;&quot;A Better Lemon Squeezer&quot;&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;This allows for floor and ceiling effects; it also allows for modelling the variance as well as the mean. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-12T01:51:24.677" Id="43373" LastActivityDate="2012-11-12T01:51:24.677" OwnerUserId="686" ParentId="43366" PostTypeId="2" Score="6" />
  
  <row Body="&lt;p&gt;Values of $\alpha$ and $\beta$ close to one suggest the model is mis-specified.&lt;/p&gt;&#10;&#10;&lt;p&gt;Try using the &lt;code&gt;ets()&lt;/code&gt; function in the &lt;code&gt;forecast&lt;/code&gt; package instead. It will choose the model for you, and select the best values of the smoothing parameters.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-11-12T03:05:09.080" Id="43377" LastActivityDate="2012-11-12T03:05:09.080" OwnerUserId="159" ParentId="43333" PostTypeId="2" Score="5" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;Suppose for each sample S in training set TS I already know the k nearest neighbors.&lt;/p&gt;&#10;&#10;&lt;p&gt;The standard algorithms usually give S the same label as the majority of S's neighbors has.&lt;/p&gt;&#10;&#10;&lt;p&gt;What other use of nearest neighbors could I make? For instance, if I have not 3, not 5, but more neighbors, would it make sense to create a temporary model based on them and only then classify S? If so, what model would you recommend?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-12T06:58:34.370" Id="43388" LastActivityDate="2013-01-12T09:31:26.877" OwnerUserId="16748" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;classification&gt;&lt;k-nearest-neighbour&gt;" Title="Different use of neighbors in kNN classification algorithm" ViewCount="289" />
  <row Body="&lt;p&gt;One possibility is a 2-step model.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Find a trend/Cycle/seasonality model for the data ignoring businessdays/weekedays&lt;/li&gt;&#10;&lt;li&gt;Then perform a time series regression with dummy variables indicating the issues above (business days, holidays, ...). &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Step 2 can be extended to include more predictors. This model is &quot;easy&quot; and I will test its performance.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-11-12T08:12:52.770" Id="43390" LastActivityDate="2012-11-12T08:12:52.770" OwnerUserId="12147" ParentId="43242" PostTypeId="2" Score="0" />
  
  
  <row AnswerCount="0" Body="&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;strong&gt;Possible Duplicate:&lt;/strong&gt;&lt;br&gt;&#10;  &lt;a href=&quot;http://stats.stackexchange.com/questions/1444/how-should-i-transform-non-negative-data-including-zeros&quot;&gt;How should I transform non-negative data including zeros?&lt;/a&gt;  &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&#10;&#10;&lt;p&gt;I have a continuous variable that has many zeros values and is NOT normal, so I can't use parametric statistics on it. &lt;/p&gt;&#10;&#10;&lt;p&gt;I tried using &lt;code&gt;ln&lt;/code&gt; and root-squared transformations, but they didn't work. &lt;/p&gt;&#10;" ClosedDate="2012-11-12T17:27:10.770" CommentCount="4" CreationDate="2012-11-12T13:23:00.780" Id="43406" LastActivityDate="2012-11-12T13:48:40.787" LastEditDate="2012-11-12T13:48:40.787" LastEditorUserId="3826" OwnerUserId="16757" PostTypeId="1" Score="1" Tags="&lt;data-transformation&gt;&lt;normalization&gt;" Title="What is the best mathematical transformation for a variable with many zero values?" ViewCount="52" />
  <row Body="&lt;p&gt;I think I can help with this. &lt;/p&gt;&#10;&#10;&lt;p&gt;So, when $Z = \max(\{ X_1, X_2, \ldots, X_n\})$ takes on a particular value $Z=z$, this tells you that:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;* You have one sample $X_i = z$  &#10;&lt;/ul&gt;&#10;&#10;&lt;ul&gt;&#10;* Your remaining $n-1$ samples are all $\leq z$&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;As such, we can say by inspection that&#10;$$&#10;p(Z = z) \propto p(X=z)P(X \leq z)^{n-1}&#10;$$&#10;where I'm using lowercase $p$ to indicate a probability density, and uppercase to indicate an actual probability. $P(X \leq z)$ you can get using the cumulative density function of your distribution. You'll need to find the normalisation term too to ensure the whole thing integrates to $1$;&lt;/p&gt;&#10;&#10;&lt;p&gt;As for quickly approximating it, I suspect that unless this has a nice closed form, you're going to be looking at numeric methods. But that should be pretty straight forward:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;* Generate a load of samples using a uniform distribution in the range $[0,1]$&#10;&lt;/ul&gt;&#10;&#10;&lt;ul&gt;&#10;* Use importance sampling to weight them by $P(Z=z)$ (remember that when using importance sampling, you do not need a normalised distribution).&#10;&lt;/ul&gt;&#10;&#10;&lt;ul&gt;&#10;* Fit your approximating distribution to the data using the importance sampling weights.&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;and you're done.&lt;/p&gt;&#10;&#10;&lt;p&gt;Bear in mind that as $n$ grows, most of the probability mass of your distribution will concentrate near $1$. It may be worth using a slightly more complicated sampling procedure than just uniform to ensure you have a good number of samples in this region. &lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT: &#10;It's been pointed out below that I assume identically distributed data in the above. Yeah, that's wrong. I think we're still ok though.&lt;/p&gt;&#10;&#10;&lt;p&gt;let's take the two variable situation, where we want to find the distribution over $Z = \max(X,Y)$. Just using some standard marginalisation, and assuming independence of $X$ and $Y$, &#10;$$&#10;p(Z = z) = \int_x \int_y p(Z=z|X=x,Y=y)p(X=x)p(Y=y) dy dx.&#10;$$&#10;(For brevity, I'm not going to keep writing $p(X=x)$, $p(Y=y)$, etc and just use $p(X)$ and $p(Y)$. Hopefully all still clear.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Given $Z$ is the max of $X$ and $Y$, I'm gonna say&#10;$$&#10;p(Z=z|X,Y) = \delta(\max(X,Y),z)&#10;$$&#10;where the delta function uses its standard definition. So now we have&#10;$$&#10;p(Z = z) = \int_x \int_y \delta(\max(X,Y),z)p(X)p(Y) dy dx.&#10;$$&#10;We can split up the integration ranges into $X&amp;gt;Y$, $Y&amp;gt;X$ and $Y=X$ as follows,&#10;$$&#10;p(Z = z) =  \int_y \int_{x&amp;lt;y}\delta(\max(X,Y),z)p(X)p(Y) dx dy +  \int_x \int_{y&amp;lt;x} \delta(\max(X,Y),z)p(X)p(Y) dy dx + \int_x \int_{y=x} \delta(\max(X,Y),z)p(X)p(Y)dydx.&#10;$$&#10;I think those ranges are correct. They look right on paper.&lt;/p&gt;&#10;&#10;&lt;p&gt;That makes our max functions nice and easy to evaluate. &#10;$$&#10;p(Z = z) =  \int_y\int_{x&amp;lt;y} \delta(Y,z)p(X)p(Y) dx dy + \int_x\int_{y&amp;lt;x}  \delta(X,z)p(X)p(Y) dy dx + \int_x \int_{y=x} \delta(X,z)p(X)p(Y)dxdy&#10;$$&#10;which in turn let's us use the sifting property of the delta function to say&#10;$$&#10;p(Z = z) = \int_{x&amp;lt;z} p(X)p(Y=z) dx + \int_{y&amp;lt;z} p(X=z)p(Y) dy + \int_{y=z}p(X=z)p(Y)dy.&#10;$$&#10;Tidying all this up, and using similar notation to before to show probabilites rather than density functions, we get&#10;$$&#10;p(Z = z) = p(Y=z)P(X&amp;lt;z) +  p(X=z)P(Y&amp;lt;z) + 0,&#10;$$&#10;where I'm pretty sure the last term just integrates away to $0$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Assuming the above is all correct, I'm guessing a similar form applies when dealing with more than 2 variables.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-11-12T15:30:48.933" Id="43415" LastActivityDate="2012-11-13T11:04:41.293" LastEditDate="2012-11-13T11:04:41.293" LastEditorUserId="16765" OwnerUserId="16765" ParentId="43384" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;You can do this in a GLM framework with a log() link function or with a poisson regression model instead of transforming $q$. Both of these will handle the log of zero issue very well. However, getting the time series aspect of the problem right might be a challenge in this framework. For example, you might have shocks to demand that persist from day to day. &lt;/p&gt;&#10;&#10;&lt;p&gt;I am not sure why you need to transform $p$ since it is presumably greater than zero at all times. &lt;/p&gt;&#10;&#10;&lt;p&gt;Unfortunately, the elephant in the room is probably &lt;a href=&quot;http://en.wikipedia.org/wiki/Endogeneity_%28economics%29&quot; rel=&quot;nofollow&quot;&gt;endogeneity&lt;/a&gt;. Unless you have a very unusual case, you are not identifying the price elasticity. I would bet dollars to donuts that your price coefficient will be positive. Take a look at &lt;a href=&quot;http://www.soderbom.net/demand_final_slides11.pdf&quot; rel=&quot;nofollow&quot;&gt;these lecture notes&lt;/a&gt; to get an idea of what's going wrong.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-12T17:29:11.050" Id="43425" LastActivityDate="2012-11-12T21:41:49.117" LastEditDate="2012-11-12T21:41:49.117" LastEditorUserId="7071" OwnerUserId="7071" ParentId="43414" PostTypeId="2" Score="1" />
  <row AnswerCount="2" Body="&lt;p&gt;I want to compare a set of frequencies and determine the p-value of their dependence. One of them is the null distribution of frequencies and the other is the distribution that I want to test. The data look like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;nucleotide   background    selected&#10;1   0.1489113   0.06074766&#10;2   0.1428619   0.04205607&#10;3   0.1189465   0.63084112&#10;4   0.1209048   0.05140187 &#10;5   0.1218093   0.07476636&#10;6   0.1282073   0.04205607&#10;7   0.2183589   0.09813084&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I cannot use the $\chi^2$ test because the numbers are less than 1. I've tried to take the $\log_2$ of the ratio, but I end up with some negative numbers so the $\chi^2$ is a no go again. How can I conduct a test of independence with these values and compute a p-value?&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT, additional informations:&lt;/p&gt;&#10;&#10;&lt;p&gt;basically i'm working on DNA sequences. i have a background pool of sequences that act like a null distribution and from this background i have selected a number of sequences for a particular biological function. now, i wanted to compare the distribution of mutations in a small sequence of 7 nucleotides in the pool of selected sequences versus the background. &lt;/p&gt;&#10;&#10;&lt;p&gt;To achieve this, i've taken all the instances where this sequence is present with one mutation in both pools and determined the frequency of mutation for each nucleotide (e.g. number of mutations at a given nucleotide / total number of mutations over all nucleotides), that is how i end up with those frequencies.&lt;/p&gt;&#10;&#10;&lt;p&gt;now my aim was to show that the pattern of mutation is indeed much different in the selected pool with respect to the background (63% of the mutations in the selected pool happen in the 3rd nucleotide, and the other nucleotides undergo visibly less mutations than in the background)&lt;/p&gt;&#10;&#10;&lt;p&gt;to provide further information, the selected column describes the frequency of 214 mutations over the 7 nucleotides while the background column describes the frequency of 120508 mutations over 7 nucleotides. &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-11-12T18:41:06.530" FavoriteCount="1" Id="43431" LastActivityDate="2012-11-13T12:16:40.457" LastEditDate="2012-11-13T09:06:49.603" LastEditorUserId="16774" OwnerUserId="16774" PostTypeId="1" Score="1" Tags="&lt;statistical-significance&gt;&lt;chi-squared&gt;&lt;independence&gt;&lt;hypothesis-testing&gt;" Title="Independence test for values between 0 and 1" ViewCount="134" />
  
  <row AcceptedAnswerId="43435" AnswerCount="1" Body="&lt;p&gt;I was given the following exercise:&lt;/p&gt;&#10;&#10;&lt;p&gt;The Results obtained by 10 students in a test are the following:&lt;/p&gt;&#10;&#10;&lt;p&gt;72 95 79 83 93 80 91 74 70 86&lt;/p&gt;&#10;&#10;&lt;p&gt;Test the hypothesis that the mean score is 75.Use two test:one parametric and another non-parametric.&lt;/p&gt;&#10;&#10;&lt;p&gt;My doubt is for the parametric test, should I make a t-student test? Because I don't have a big sample to use the normal distribution...&lt;/p&gt;&#10;" CommentCount="13" CreationDate="2012-11-12T19:24:47.030" Id="43434" LastActivityDate="2012-11-12T20:17:40.840" OwnerDisplayName="user16775" PostTypeId="1" Score="0" Tags="&lt;hypothesis-testing&gt;&lt;self-study&gt;" Title="Non-parametric test" ViewCount="107" />
  <row AnswerCount="1" Body="&lt;p&gt;For many regression/classification algorithms, we have the bayesian version of it. Like bayesian linear regression, bayesian logistic regression, bayesian neuron network. I do not fully understand the math in them, but what are its advantages compared with the original algorithm? Is is of great practical use?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-11-12T20:17:31.017" FavoriteCount="3" Id="43436" LastActivityDate="2012-11-12T21:04:09.520" OwnerUserId="16732" PostTypeId="1" Score="2" Tags="&lt;bayesian&gt;&lt;multiple-regression&gt;" Title="what's the advantages of bayesian version of linear regression, logistic regression etc" ViewCount="482" />
  <row Body="&lt;p&gt;Doing Bayesian regression is not &lt;em&gt;an algorithm&lt;/em&gt; but a different approach to statistical inference. The major advantage is that, by this Bayesian processing, you recover the whole range of inferential solutions, rather than a point estimate and a confidence interval as in classical regression. (I can only recommend you to read a statistics manual to understand the difference between an algorithm and statistical inference.)&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-11-12T21:04:09.520" Id="43441" LastActivityDate="2012-11-12T21:04:09.520" OwnerUserId="7224" ParentId="43436" PostTypeId="2" Score="4" />
  
  <row AcceptedAnswerId="43557" AnswerCount="1" Body="&lt;p&gt;I will use naive Bayes or decision tree that gives rule model both. Is is necessary to normalize data before working with such algorithms.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-12T23:56:32.423" FavoriteCount="1" Id="43450" LastActivityDate="2012-11-14T08:29:04.723" OwnerUserId="14289" PostTypeId="1" Score="1" Tags="&lt;classification&gt;&lt;model-selection&gt;&lt;normalization&gt;&lt;association-rules&gt;" Title="Is it needed to normalize data before rule model extraction algorithms like ID3?" ViewCount="488" />
  <row AcceptedAnswerId="43454" AnswerCount="1" Body="&lt;p&gt;I need to create a raw data table for my science experiment, but I'm unsure what the level of uncertainty would be when measuring days. How would I determine the level of uncertainty when measuring a period of time?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-11-13T00:05:37.273" FavoriteCount="0" Id="43453" LastActivityDate="2012-11-13T04:06:36.683" LastEditDate="2012-11-13T04:06:36.683" LastEditorUserId="686" OwnerUserId="16786" PostTypeId="1" Score="1" Tags="&lt;survival&gt;&lt;measurement&gt;&lt;uncertainty&gt;" Title="Level of uncertainty when measuring a period of time?" ViewCount="128" />
  <row Body="&lt;p&gt;Exactly as you do with any other uncertainty, I don't see how measuring days is different from measuring meters.&lt;/p&gt;&#10;&#10;&lt;p&gt;That is, you can find SEM if you have some statistics of measurements or decide on some measurement uncertainty for each individual one (if you check 'em every day, the uncertainty is probably 0.5 day or so, if you aren't sure if it happened or not within a couple of days, it can be more etc.). Exactly as for any other measurement.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-13T00:49:08.247" Id="43454" LastActivityDate="2012-11-13T00:49:08.247" OwnerUserId="14741" ParentId="43453" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to fit some variables which are high collinear using nnls(non negative least squared routine) in R. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have about 1 million observations and 200 highly collinear variables for the coefficients to be found.&lt;/p&gt;&#10;&#10;&lt;p&gt;My problem is that in the final result, the algorithm seems to select variables that have pretty low correlation with the independent variable, it seems to throw the variables that have a high correlation in favor for the ones with a low correlation. I dont seem to understand why. Has anyone run into something similar, any way I can modify to pick variables in order of correlation, I am using the standard NNLS package in R.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-13T02:27:01.990" Id="43458" LastActivityDate="2012-11-13T02:32:08.360" LastEditDate="2012-11-13T02:32:08.360" LastEditorUserId="15989" OwnerUserId="15989" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;correlation&gt;" Title="NNLS in R correlation of variables" ViewCount="81" />
  <row AnswerCount="1" Body="&lt;p&gt;First let me describe the situation I'm dealing with:&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm looking at performance data for a software system. I have data for many versions of the software, including ongoing. For each version I have a series of values for the run-times of tests run against it, usually about 20 (of the same test). These generally look normally distributed (for each version).&lt;/p&gt;&#10;&#10;&lt;p&gt;What I want to know is, given some data for runs against a new version, is the new distribution &lt;em&gt;different&lt;/em&gt; in a way that merits investigation. The comparison could be either against the previous version, or against a set of previous versions that have been selected as a having &quot;stable&quot; performance: a kind of baseline. Any kind of change could be relevant: a change in the mean, variance or shape could be significant.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, the different versions are, well, different, but for those in the baseline I think I can assume that they're effectively samples from the same distribution. So I've got $X_1, ... X_n, X'$, for $n \geq 1$, and I want to test whether $X'$ is relevantly similar to the $X_i$s in an automated fashion.&lt;/p&gt;&#10;&#10;&lt;p&gt;From what I've seen on the internet I've come up with a few options:&lt;/p&gt;&#10;&#10;&lt;p&gt;a) Kolmogorov-Smirnov test: either of $X'$ vs $X_n$, or of $X'$ vs $\bigcup X_i$.&lt;/p&gt;&#10;&#10;&lt;p&gt;b) T-test: similarly.&lt;/p&gt;&#10;&#10;&lt;p&gt;c) Mann-Whitney/Wilcoxon test?&lt;/p&gt;&#10;&#10;&lt;p&gt;Firstly, I'm not clear which would be better for my situation, as they both test for different kinds of &quot;similarity&quot;, or whether I should use both and report some combination of the results.&lt;/p&gt;&#10;&#10;&lt;p&gt;Secondly, looking at the data, it looks like the $X_i$, while normally distributed, tend to move around a bit: so their means vary, but e.g. the variance is similar. If I just lump them into one big sample, then this information is lost; for example, it will look like the typical variance is much bigger than it actually is. For that reason I wondered whether I might be able to instead look at the distribution of the means and variances of the $X_i$s, and compare the mean and variance of $X'$ against that, somehow.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, I'm unsure how to do that: I think the means should follow a t-distribution, so I ought to be able to estimate the probability of getting $\bar{X'}$ given that the sample came from the same distribution, but that's the wrong conditional probability! (although that's kind of just what a p-value is...) I can't do a proper t-test as that requires either more than one value to compare against, or the assumption that the variance is shared, which I'm not sure I have.&lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, my inner Bayesian feels like I ought to be able to do better than producing p-values for rejecting the null hypotheses: surely I ought to be able to calculate a posterior probability that $X'$ is, say, drawn from the same distribution as the $X_i$s?&lt;/p&gt;&#10;&#10;&lt;p&gt;Apologies for the huge question; I hope this gives a reasonable idea of where I'm coming from! I'm mathematically trained, but I'm pretty unfamiliar with statistics, so I can cope with some maths.&lt;/p&gt;&#10;&#10;&lt;p&gt;Edit: I'm also familiar with R; I'm going to be using it to do the calculations... once I figure out what to calculate!&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-11-13T02:52:39.977" FavoriteCount="1" Id="43461" LastActivityDate="2013-03-10T05:34:28.123" LastEditDate="2012-11-13T03:02:47.480" LastEditorUserId="16791" OwnerUserId="16791" PostTypeId="1" Score="5" Tags="&lt;distributions&gt;&lt;hypothesis-testing&gt;&lt;bayesian&gt;&lt;t-test&gt;&lt;kolmogorov-smirnov&gt;" Title="How to compare a sample against some baseline data?" ViewCount="348" />
  <row Body="&lt;p&gt;Both your suggested approaches (number of attempts in the 10th trial; and the difference between the number of attempts in the 10th and 1st trials) depend too much on the final trial.  The subject might indeed have learnt, and just has an unlucky time on the 10th trial.  You need a way to incorporate the information from the 9th, 8th, 7th... trials.&lt;/p&gt;&#10;&#10;&lt;p&gt;I don't think there is a way to do this well without using longitudinal data analysis techniques, specifically mixed effects modelling.&lt;/p&gt;&#10;&#10;&lt;p&gt;In general a mixed effects level model with &quot;number of attempts&quot; as the response variable and &quot;trial number&quot; as the explanatory variable (plus another explanatory dummy variable for treatment / control, if that is part of the experiment) will do it.  You can then have a random element for each trial-subject combination, as well as a random element for the subject.  You will have complications because &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;the response variable is a censored ordinal variable &lt;/li&gt;&#10;&lt;li&gt;the explanatory variable is an ordinal variable; and/or even if you were to treat it as a continuous variable, the chances are that the relationship between it and the response is non-linear&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;There are solutions to those challenges, ranging in sophistication.&lt;/p&gt;&#10;&#10;&lt;p&gt;If all that is too much, I would try as a variant on your proposed approach - average number of attempts in the last three trials minus the average number of attempts in the first three.  You still have the problem of the censoring of the data however...&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-11-13T03:29:56.727" Id="43463" LastActivityDate="2012-11-13T03:29:56.727" OwnerUserId="7972" ParentId="43459" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;I guess by saying &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;For instance, if I have not 3, not 5, but more neighbors&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;you define a neighbor as &lt;em&gt;close to the instance of interest&lt;/em&gt;. This is a common problem. Either you have to fix the number of neighbors (via k) or define a maximum radius so that all instances within the resulting circle count as &lt;em&gt;neighbor&lt;/em&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Fixing k&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;When k is fixed, you could add the distance to calculate a weighted score and select the class label whose score is the minimum instead of for the majority.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;x be the example which shall be classified&lt;/li&gt;&#10;&lt;li&gt;dist the distance metric used&lt;/li&gt;&#10;&lt;li&gt;$x_1,...,x_k$ the nearest neighbors&lt;/li&gt;&#10;&lt;li&gt;$c_1,...,c_k$ the classes of the nearest neighbors respectively.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;For all classes, calculate the average distance from x to examples with class c&#10;$classdistance(x,c)=\frac{1}{|\{c_i|c_i=c,i=1,...,k\}|}\sum_{i,c_i=c}^{k}dist(x,x_i)$&lt;/p&gt;&#10;&#10;&lt;p&gt;The score for c given x is simply the normalized similarity across all classes, i.e. $score(c|x)= 1-\frac{classdistance(x,c)}{\sum_{c'}classdistance(x,c')}$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Fixing distance&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's say the maximum distance is d, so that we can define an $\hat{x}$ to be a neighbor of $x$ if $dist(x,\hat{x})&amp;lt;=d$.&lt;/p&gt;&#10;&#10;&lt;p&gt;So the easiest approach would be to calculate a weighted score as&#10;$classdistance(x,c)= AVG_{\hat{x},dist(\hat{x},x)&amp;lt;=d,class(\hat{x}=c)}(dist(x,\hat{x}))$&lt;/p&gt;&#10;&#10;&lt;p&gt;In another approach, one could throw the distance information with the specified radius away (by setting the distance to 1 for all instances within the radius) and instead uses a kernel function, hence introducing one's own weights. For example, with a uniform kernel, the selection of the final class label would be equivalent to the majority. But others are available to weight the instances farther away less. See &lt;a href=&quot;http://en.wikipedia.org/wiki/Kernel_%28statistics%29&quot; rel=&quot;nofollow&quot;&gt;Wikipedia - Kernel (statistics)&lt;/a&gt; for more functions and a graphical illustration.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-13T08:22:44.873" Id="43469" LastActivityDate="2012-11-13T08:22:44.873" OwnerUserId="264" ParentId="43388" PostTypeId="2" Score="1" />
  
  <row AnswerCount="5" Body="&lt;h3&gt;Note: I &lt;em&gt;am&lt;/em&gt; aware of &lt;a href=&quot;http://stats.stackexchange.com/questions/31867/bayesian-vs-frequentist-interpretations-of-probability&quot;&gt;philosophical&lt;/a&gt; differences between Bayesian and frequentist statistics.&lt;/h3&gt;&#10;&#10;&lt;p&gt;For example &quot;what is the probability that the coin on the table is heads&quot; doesn't make sense in frequentist statistics, since it has either already landed heads or tails -- there is nothing probabilistic about it. So the question has no answer in frequentist terms.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;But such a difference is specifically &lt;strong&gt;not&lt;/strong&gt; the kind of difference I'm asking about.&lt;/em&gt;  &lt;/p&gt;&#10;&#10;&lt;p&gt;Rather, I would like to know how their predictions for &lt;em&gt;well-formed&lt;/em&gt; questions actually &lt;em&gt;differ&lt;/em&gt; in the real world, &lt;em&gt;excluding&lt;/em&gt; any theoretical/philosophical differences such as the example I mentioned above.&lt;/p&gt;&#10;&#10;&lt;p&gt;So in other words:&lt;/p&gt;&#10;&#10;&lt;h3&gt;What's an example of a question, answerable in &lt;em&gt;both&lt;/em&gt; frequentist &lt;em&gt;and&lt;/em&gt; Bayesian statistics, whose answer is different between the two?&lt;/h3&gt;&#10;&#10;&lt;p&gt;(e.g. Perhaps one of them answers &quot;1/2&quot; to a particular question, and the other answers &quot;2/3&quot;.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Are there any such differences?  &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;If so, what are some examples?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;If not, then when does it actually ever make a &lt;em&gt;difference&lt;/em&gt; whether I use Bayesian or frequentist statistics when solving a particular problem?&lt;br&gt;&#10;Why would I avoid one in favor of the other?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="5" CommunityOwnedDate="2012-11-13T16:23:49.397" CreationDate="2012-11-13T09:13:38.200" FavoriteCount="16" Id="43471" LastActivityDate="2014-01-14T22:37:29.100" LastEditDate="2012-11-13T16:23:42.813" LastEditorUserId="88" OwnerUserId="10636" PostTypeId="1" Score="31" Tags="&lt;bayesian&gt;&lt;frequentist&gt;" Title="Examples of Bayesian and frequentist approach giving different answers" ViewCount="3624" />
  <row Body="&lt;p&gt;See my question &lt;a href=&quot;http://stats.stackexchange.com/questions/2356/are-there-any-examples-where-bayesian-credible-intervals-are-obviously-inferior&quot;&gt;here&lt;/a&gt;, which mentions a paper by Edwin Jaynes that gives an example of a correctly constructed frequentist confidence interval, where there is sufficient information in the sample to know for certain that the true value of the statistic lies nowhere in the confidence interval (and thus the confidence interval is different from the Bayesian credible interval).&lt;/p&gt;&#10;&#10;&lt;p&gt;However, the reason for this is the difference in the definition of a confidence interval and a credible interval, which in turn is a direct consequence of the difference in frequentist and Bayesian definitions of probability. If you ask a Bayesian to produce a Bayesian confidence (rather than credible) interval, then I suspect that there will always be a prior for which the intervals will be the same, so the differences are down to choice of prior.&lt;/p&gt;&#10;&#10;&lt;p&gt;Whether frequentist or Bayesian methods are appropriate depends on the question you want to pose, and at the end of the day it is the difference in philosophies that decides the answer (provided that the computational and analytic effort required is not a consideration).&lt;/p&gt;&#10;&#10;&lt;p&gt;Being somewhat tongue in cheek, it could be argued that a long run frequency is a perfectly reasonable way of determining the relative plausibility of a proposition, in which case frequentist statistics is a slightly odd subset of subjective Bayesianism - so any question a frequentist can answer a subjectivist Bayesian can also answer in the same way, or in some other way should they choose different priors.  ;o)&lt;/p&gt;&#10;" CommentCount="3" CommunityOwnedDate="2012-11-13T16:23:49.397" CreationDate="2012-11-13T10:01:50.367" Id="43475" LastActivityDate="2012-11-13T10:18:17.427" LastEditDate="2012-11-13T10:18:17.427" LastEditorUserId="887" OwnerUserId="887" ParentId="43471" PostTypeId="2" Score="7" />
  <row Body="&lt;p&gt;In Bayesian statistics the half normal,with a sufficiently large variation parameter, can be used as a noninformative prior distribution on the SD of a standard distribution. This is suggested in, for example:&lt;/p&gt;&#10;&#10;&lt;p&gt;Gelman, A. (2006). Prior distributions for variance parameters in hierarchical models. Bayesian analysis, 1, 515-534.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-11-13T10:28:10.617" Id="43477" LastActivityDate="2012-11-13T10:28:10.617" OwnerUserId="6920" ParentId="20291" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="43487" AnswerCount="3" Body="&lt;p&gt;I'm working on an algorithm that relies on the fact that observations $Y$s are normally distributed, and I would like to test the robustness of the algorithm to this assumption empirically.&lt;/p&gt;&#10;&#10;&lt;p&gt;To do this, I was looking for a sequence of transformations $T_1(), \dots, T_n()$ that would progressively disrupt the normality of $Y$. For example if the $Y$s are normal they have skewness $= 0$ and kurtosis $= 3$, and it would be nice to find a sequence of transformation that progressively increase both.&lt;/p&gt;&#10;&#10;&lt;p&gt;My idea was to simulate some normally approximately distributed data $Y$ and test the algorithm on that. Than test algorithm on each transformed dataset $T_1(Y), \dots, T_n(y)$, to see how much the output is changing.&lt;/p&gt;&#10;&#10;&lt;p&gt;Notice that I don't control the distribution of the simulated $Y$s, so I cannot simulate them using a distribution that generalizes the Normal (such as the Skewed Generalized Error Distribution).&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-11-13T10:51:17.607" FavoriteCount="3" Id="43482" LastActivityDate="2015-02-23T00:58:50.790" OwnerUserId="26105" PostTypeId="1" Score="11" Tags="&lt;data-transformation&gt;&lt;normality&gt;&lt;skewness&gt;&lt;kurtosis&gt;" Title="Transformation to increase kurtosis and skewness of normal r.v" ViewCount="1975" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;There are two methods which produce different ranking of a group of hypotheses.&#10;I want to compare the two methods showing that one method generally produces&#10;lower ranking than the other method for a specific subset of the considered group of &#10;hypotheses. I was thinking of comparing the quantiles of the two rankings on this subset,&#10;but not sure whether that was the right thing to do. Can anyone familiar with this give &#10;some hint? Thanks. Hanna&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-13T13:57:45.603" Id="43492" LastActivityDate="2012-11-13T17:49:45.440" LastEditDate="2012-11-13T16:25:10.940" LastEditorUserId="88" OwnerUserId="13154" PostTypeId="1" Score="1" Tags="&lt;mathematical-statistics&gt;&lt;nonparametric&gt;&lt;ordinal&gt;&lt;ranking&gt;" Title="Comparisons of two rankings" ViewCount="104" />
  
  
  <row Body="&lt;p&gt;This looks like a straightforward application of the &lt;a href=&quot;http://en.wikipedia.org/wiki/Binomial_test&quot; rel=&quot;nofollow&quot;&gt;binomial test&lt;/a&gt;, with the two categories being &quot;method A ranks this hypothesis higher than method B&quot; and &quot;method B ranks this hypothesis higher than method A&quot; (discarding ties), applied to all the hypotheses in your subset.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-13T17:49:45.440" Id="43510" LastActivityDate="2012-11-13T17:49:45.440" OwnerUserId="1352" ParentId="43492" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;If you look up the definitions of Pearson, one of them is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \rho := \frac{\text{Cov}(X,Y)}{\sigma_X\sigma_Y} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;which obviously is a standardization of the covariance. Data with unit variance, they are identical.&lt;/p&gt;&#10;&#10;&lt;p&gt;But what actually was your question?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-11-13T19:03:25.717" Id="43514" LastActivityDate="2012-11-13T19:03:25.717" OwnerUserId="7828" ParentId="43497" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;ANOVA is equivalent to regression using categorical (aka &quot;dummy&quot; or &quot;indicator&quot;) variables. So any techniques from regression can also be applied to ANOVA. Here are some suggestions:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;If the error terms appear to be normally distributed, you could try using weighted least squares.&lt;/li&gt;&#10;&lt;li&gt;If the error terms are not normally distributed, you would probably need to transform the variables.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2012-11-13T19:43:17.550" Id="43518" LastActivityDate="2012-11-13T19:43:17.550" OwnerUserId="10054" ParentId="42981" PostTypeId="2" Score="2" />
  
  
  
  
  
  <row Body="&lt;p&gt;Yes.  Many GLMs can be tested using a likelihood ratio test comparing the model with a restricted model that has only no covariates.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-14T03:34:13.497" Id="43546" LastActivityDate="2012-11-14T03:34:13.497" OwnerUserId="12833" ParentId="38578" PostTypeId="2" Score="0" />
  
  
  
  <row AcceptedAnswerId="43572" AnswerCount="1" Body="&lt;p&gt;I'm trying to wrap my head around the Exponential distribution and the meaning of its parameter. The parameter is the rate, right? So take, e.g.,&#10;$$X\sim \exp(0.05)\,.$$&#10;Now the probability of failure during the first time period is:&#10;$$P(X\le1)=1-P(X&amp;gt;1)=1-e^{-0.05}=0.04877\,.$$&#10;Now I can do the math and get the correct result and so on, but cannot wrap my head around why the result should be slightly less than 5%, rather than exactly 5%. I don't get the intuition behind it. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-11-14T09:18:36.103" Id="43560" LastActivityDate="2012-11-14T13:06:14.253" LastEditDate="2012-11-14T11:22:24.090" LastEditorUserId="7224" OwnerUserId="9728" PostTypeId="1" Score="2" Tags="&lt;probability&gt;&lt;exponential&gt;" Title="Understanding the exponential distribution" ViewCount="740" />
  <row Body="&lt;p&gt;As you say, LDA is supervised.&#10;How does your supervisor define &quot;learning&quot;?&lt;/p&gt;&#10;&#10;&lt;p&gt;But yes, usually it is counted as supervised learning.&#10;Reference, e.g. first 2 pages of The Elements of Statistical Learning&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;You can use LDA models for prediction of new cases. (I'd say that implies that something has been learned&lt;/li&gt;&#10;&lt;li&gt;However, you can also put emphasis on the projection aspect, which may be used in a descriptive rather than a predictive way.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I think &lt;a href=&quot;http://onlinelibrary.wiley.com/doi/10.1002/jbio.200810024/abstract;jsessionid=7361BB2D736B5ACA35FFB4D7DECDD378.d04t03&quot; rel=&quot;nofollow&quot;&gt;we wrote something here&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-14T09:25:30.363" Id="43561" LastActivityDate="2012-11-14T09:25:30.363" OwnerUserId="4598" ParentId="43559" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="43565" AnswerCount="3" Body="&lt;p&gt;I have the following data for web traffic in Excel&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;June 88,147&lt;/li&gt;&#10;&lt;li&gt;July 111,839&lt;/li&gt;&#10;&lt;li&gt;August   93,148&lt;/li&gt;&#10;&lt;li&gt;September    93,069&lt;/li&gt;&#10;&lt;li&gt;October  101,881&lt;/li&gt;&#10;&lt;li&gt;November 97,345&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I can do a chart with a trend line and a linear trend shows a slight upward trend, I would like the simplest way (stats is not my field!) to project / guestimate what it will be like in the future &lt;/p&gt;&#10;&#10;&lt;p&gt;The trend line in the chart gives me &lt;code&gt;y=14.321x - 491537&lt;/code&gt;, but I don't know to work that into a formula I tried &lt;code&gt;=14.321*E8-491537&lt;/code&gt; (&lt;code&gt;e8&lt;/code&gt; is November's value) and it gave me 902,545 which does not fit the trend at all!&lt;/p&gt;&#10;&#10;&lt;p&gt;The goal is a rough estimation of the month by which the figures will be up to 130k+ (presuming my slight upwards trend is correct!)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-14T10:53:39.580" Id="43563" LastActivityDate="2014-01-13T21:00:22.567" LastEditDate="2014-01-13T21:00:22.567" LastEditorUserId="7290" OwnerUserId="16841" PostTypeId="1" Score="3" Tags="&lt;forecasting&gt;&lt;excel&gt;&lt;web&gt;" Title="Simple extrapolation of web traffic" ViewCount="323" />
  <row AnswerCount="1" Body="&lt;p&gt;I'm trying to find the most probable path (i.e. sequence of states) on an HMM using the Viterbi algorithm. However, I don't know the transition and emission matrices, which I need to estimate from the observations (data).&lt;/p&gt;&#10;&#10;&lt;p&gt;To estimate these matrices, which algorithm should I use: the Baum-Welch algorithm or the Viterbi Training algorithm? Why?&lt;/p&gt;&#10;&#10;&lt;p&gt;In case I should use the Viterbi training algorithm, can anyone provide me a good pseudocode (it's not easy to find)?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-11-14T12:45:41.700" Id="43570" LastActivityDate="2013-12-14T23:43:22.107" LastEditDate="2012-11-15T09:21:40.817" LastEditorUserId="16842" OwnerUserId="16842" PostTypeId="1" Score="2" Tags="&lt;algorithms&gt;&lt;hidden-markov-model&gt;" Title="Viterbi training vs Baum-Welch algorithm" ViewCount="333" />
  
  <row Body="&lt;p&gt;For Venturini's (2011) outlier detection method &lt;em&gt;Washer&lt;/em&gt;, its inventor published an R implementation &lt;a href=&quot;https://sites.google.com/site/andreaventurini65/home/outlier-detection&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Venturini, A. (2011). &lt;a href=&quot;http://ideas.repec.org/a/bot/rivsta/v71y2011i3p329-344.html&quot; rel=&quot;nofollow&quot;&gt;Time Series Outlier Detection: A New Non Parametric Methodology (Washer)&lt;/a&gt;. &lt;em&gt;Statistica&lt;/em&gt; 71: 329-344.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="1" CreationDate="2012-11-14T14:57:07.533" Id="43580" LastActivityDate="2012-11-14T15:58:13.850" LastEditDate="2012-11-14T15:58:13.850" LastEditorDisplayName="user10525" OwnerUserId="16847" ParentId="43577" PostTypeId="2" Score="3" />
  
  
  
  
  
  
  
  <row AnswerCount="4" Body="&lt;p&gt;My work involves building statistical / econometric models using R, SPSS modeler. I am also doing my PhD (part time) in econometrics. In order to do more advanced data / model visualisation I am thinking about to pick up another programming language. Any suggestion will be much appreciated.  &lt;/p&gt;&#10;" ClosedDate="2012-11-15T16:40:19.870" CommentCount="1" CreationDate="2012-11-15T05:19:27.987" FavoriteCount="1" Id="43627" LastActivityDate="2012-11-15T17:48:57.053" OwnerUserId="1887" PostTypeId="1" Score="-1" Tags="&lt;data-visualization&gt;" Title="Learning an extra programming language (other than R)?" ViewCount="477" />
  <row Body="&lt;p&gt;Let's partition our N elements into two sets (s1, s2) with sizes N1, N2 and averages A1, A2 respectively.&lt;/p&gt;&#10;&#10;&lt;p&gt;To to deduce the average A of N elements from this information ( N1, N2, A1, A2 ), we can use this formula:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;   Average = Sum / NumElements &#10;=&amp;gt; Sum = Average * NumElements&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Also:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;   Average of all numbers = Sum of numbers / Count of numbers&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Sum of all numbers = N1*A1 + N2*A2&#10;Total Elements = N1 + N2&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Therefore &lt;strong&gt;correct answer is&lt;/strong&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;A = ( N1*A1 + N2*A2 ) / ( N1 + N2 )&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Only when sizes of both partitions is same:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;N1 = N2&#10;A = (N1*A1 + N2*A2 )/ (N1+N2) = (N1*A1 + N1*A2)/(N1+N2) = N1*(A1+A2)/(2*N1)&#10;=&amp;gt; A = (A1 + A2)/2&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Hence the first answer only holds when size of both partitions is same.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-15T05:39:17.063" Id="43628" LastActivityDate="2012-11-15T05:39:17.063" OwnerUserId="16870" ParentId="43031" PostTypeId="2" Score="1" />
  <row AnswerCount="2" Body="&lt;p&gt;Bit of a stats newbie question here. I've got about 10,000 rows of data about pupil performance, annotated with school and teacher details, that look as follows: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Pupil      School     Grade(1-5)    Teacher&#10;123        Morley     4             Adams&#10;124        Westlake   3             Morris&#10;125        Morley     5             Adams&#10;126        Westlake   2             Philips&#10;127        St Xavier  4             Smith&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I've calculated that the overall mean grade is 3.4 with standard deviation 1.0. &lt;/p&gt;&#10;&#10;&lt;p&gt;Now, say I want to work out whether one school (or one teacher) has overall a significantly lower or higher performance than average. &lt;/p&gt;&#10;&#10;&lt;p&gt;If I calculate that the overall mean grade at Morley is 3.9, with s.d. 0.8, the overall mean grade at Westlake is 3.2, with s.d. 0.7, and the overall mean at St Xavier is 3.8, with s.d. 0.5, what test can I use to say whether any of these schools have a significantly different performance from the average? &lt;/p&gt;&#10;&#10;&lt;p&gt;From my knowledge so far, I'm guessing I want to use some form of t-test, probably Student's t-test - although I'm not sure how to adapt it for multiple groups. &lt;/p&gt;&#10;&#10;&lt;p&gt;(Of course, there are lots of factors influencing why grades might be lower at one school than another and yet that school might still be performing well - I'm not getting that deep here. I just want to know how to compare means across sub-groups.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Many thanks for your help. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-15T10:59:41.850" Id="43640" LastActivityDate="2012-11-15T14:25:49.010" OwnerUserId="16875" PostTypeId="1" Score="2" Tags="&lt;hypothesis-testing&gt;&lt;t-test&gt;&lt;mean&gt;" Title="Comparing means across sub-groups?" ViewCount="260" />
  
  <row Body="&lt;p&gt;OpenCV has an implementation of random forests, based on the algorithm of Leo Breiman. &lt;a href=&quot;http://docs.opencv.org/modules/ml/doc/random_trees.html&quot; rel=&quot;nofollow&quot;&gt;Check out the documentation here&lt;/a&gt;. It is open source, and fairly readable, active.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-15T15:15:49.433" Id="43661" LastActivityDate="2012-11-15T15:15:49.433" OwnerUserId="14595" ParentId="26510" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;This sounds like a contrast-type problem! Let's suppose you're assuming a linear increase between level 1 and level 5:&lt;/p&gt;&#10;&#10;&lt;p&gt;$L=c_{1}\bar{X}_{1} + \cdots + c_{5}\bar{X}_{5}$,&lt;/p&gt;&#10;&#10;&lt;p&gt;where $c_{j}$ are the coefficients, and $\bar{X}_{i}$ are the group means (the mean at level $i$)--you're computing the weighted group means. For a linear contrast, you might use the coefficients $-2, -1, 0, 1, 2$, as the coefficients must sum to zero. People usually evaluate these while conducting an ANOVA or related regression analysis. There's a general overview of the concept &lt;a href=&quot;http://en.wikipedia.org/wiki/Contrast_%28statistics%29&quot; rel=&quot;nofollow&quot;&gt;on wikipedia&lt;/a&gt;. I don't know what statistical package you're using, but, if you're conducting your analysis in R, you'll want to look at the contrast section in Julian Faraway's &lt;em&gt;&lt;a href=&quot;http://cran.r-project.org/doc/contrib/Faraway-PRA.pdf&quot; rel=&quot;nofollow&quot;&gt;Practical Regression and ANOVA using R&lt;/a&gt;&lt;/em&gt;, which is both a great reference on the mathematics of what is happening, as well as how to actually use them in R.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-11-15T16:30:43.357" Id="43670" LastActivityDate="2013-01-14T18:40:37.840" LastEditDate="2013-01-14T18:40:37.840" LastEditorUserId="930" OwnerUserId="8580" ParentId="43083" PostTypeId="2" Score="1" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have developed a simple &lt;em&gt;Kernel Density Estimator&lt;/em&gt; in Java, based on a few dozen points (maybe up to one hundred or so) and a Gaussian kernel function. The implementation gives me the PDF and CDF of my probability distribution at any point.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would now like to implement a simple &lt;em&gt;sampling method&lt;/em&gt; for this KDE.  An obvious choice would of course be to draw from the very set of points making up the KDE, but I would like to be able to retrieve points that are slightly different from the ones in the KDE.&lt;/p&gt;&#10;&#10;&lt;p&gt;I haven't found so far a sampling technique that I could easily implement to solve this problem (without depending on external libraries for numerical integration or complex computations).  Any advices? I don't have specially strong requirements when it comes to precision or efficiency, my main concern is to have a sampling function that works and can be easily implemented. Thanks!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-11-15T17:58:22.647" Id="43674" LastActivityDate="2012-12-22T09:09:24.507" OwnerUserId="12793" PostTypeId="1" Score="2" Tags="&lt;sampling&gt;&lt;pdf&gt;&lt;kde&gt;" Title="simple sampling method for a Kernel Density Estimator" ViewCount="797" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I have a set of true positive (TP) values which are used to train a model. &lt;/p&gt;&#10;&#10;&lt;p&gt;I am using 5-fold cross validation to train my model (i.e. split my true positives into 5, use 4/5ths for training and 1/5th for testing)&lt;/p&gt;&#10;&#10;&lt;p&gt;I repeat this using different 1/5ths as the test set.&#10;For each run, I have a large set of mixed true positives / true negatives which I use my trained model to attempt to classify. I then obtain an ROC curve. &#10;This is done for each run of the cross validation (i.e. I end up with 5 ROC curves)&lt;/p&gt;&#10;&#10;&lt;p&gt;I then average the AUC and return it. &lt;/p&gt;&#10;&#10;&lt;h3&gt;My problem:&lt;/h3&gt;&#10;&#10;&lt;p&gt;I have two methods of classification: call them method A and method B.&#10;for each method, I get 5 ROC curves. &#10;How can I determine which method gives me a better ROC if I have more than one ROC for each?&lt;/p&gt;&#10;&#10;&lt;p&gt;I know computing the AUC and averaging for each method, then comparing averaged AUCs is NOT a good approach.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note: I have more than 1 model (roughly 120). I just explained in terms of one model for simplicity. So I have 120 models, each one having classified the data using method A and method B, and for each method A &amp;amp; B there are 5 ROCs from cross validation.&lt;/p&gt;&#10;&#10;&lt;h2&gt;Edit&lt;/h2&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;My problem more specifically is that I have &gt;100 sets of sequences, and for each set I construct a position weight matrix, which I then use to score against all sets merged together. I have several scoring schemes so I'd like to determine which ones give me the best classification. For this, I use cross validation: split my data into 5 for each set, train my pwm with 4/5ths of the data and test it on 1/5th. Pool the results from 5 runs, and plot an AUC.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-11-15T20:08:19.763" FavoriteCount="2" Id="43688" LastActivityDate="2012-11-22T08:58:41.223" LastEditDate="2012-11-21T16:54:16.840" LastEditorUserId="16887" OwnerUserId="16887" PostTypeId="1" Score="3" Tags="&lt;classification&gt;&lt;cross-validation&gt;&lt;model-selection&gt;&lt;roc&gt;&lt;auc&gt;" Title="Fastest way to compare ROC curves" ViewCount="799" />
  <row Body="&lt;p&gt;Look up the delta method.  Essentially you have a function $g(\boldsymbol{\beta}) = w_1\beta_1 + w_2\beta_2$.  The variance of $g$ is asymptotically:&#10;\begin{equation}&#10;Var(g(\boldsymbol{\beta})) \approx [\nabla g(\boldsymbol{\beta})]^T Var(\boldsymbol{\beta})[\nabla g(\boldsymbol{\beta})]&#10;\end{equation}&#10;Where $Var(\boldsymbol{\beta})$ is your covariance matrix for $\boldsymbol{\beta}$ (given by the inverse of the Fisher information, see: &lt;a href=&quot;http://en.wikipedia.org/wiki/Fisher_information&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Fisher_information&lt;/a&gt; if you're unsure of this).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-15T22:19:31.013" Id="43703" LastActivityDate="2012-11-15T22:19:31.013" OwnerUserId="13804" ParentId="43697" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;&lt;strong&gt;Crisp vs Fuzzy Logic&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;As far as I remember, crisp logic is the same as boolean logic. Either a statement is true or it is not, meanwhile fuzzy logic captures the degree to which something is true.&lt;/p&gt;&#10;&#10;&lt;p&gt;Consider the statement: &quot;The agreed to met at 12 o'clock but Ben was not punctual.&quot; &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Crisp logic: If Ben showed up precisley at 12, he is punctual, otherwise he is too early or too late.&lt;/li&gt;&#10;&lt;li&gt;Fuzzy logic: The degree, to which Ben was punctual, depends on how much earlier or later he showed up (e.g. 0, if he showed up 11:45 or 12:15, 1 at 12:00 and a linear increase / decrease in between).&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I do not exactly know who first used the term &quot;crisp&quot;, but I have seen it multiple times in the closely related &lt;a href=&quot;http://en.wikipedia.org/wiki/Fuzzy_set&quot;&gt;Fuzzy Set Theory&lt;/a&gt;, where it has been used to distinguish &lt;a href=&quot;http://en.wikipedia.org/wiki/Georg_Cantor&quot;&gt;Cantor's&lt;/a&gt; set theory from Zadeh's. So if you are looking for a reference, the &lt;a href=&quot;http://www-bisc.cs.berkeley.edu/Zadeh-1965.pdf&quot;&gt;original work of Zadeh&lt;/a&gt; or one the textbooks in the area might be a way to go.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;... in Machine Learning&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In Machine Learning most of the classifiers produce so called &lt;em&gt;scores&lt;/em&gt;, which are in general more or less rough estimates of the probability that the scored instance belongs to a particular class. &lt;/p&gt;&#10;&#10;&lt;p&gt;To the best of my knowledge, these scores have no explicit link to Fuzzy Logic. Both Fuzzy Logic and Probability Theory are close to each other, but technically they are not the same (&lt;a href=&quot;http://en.wikipedia.org/wiki/Fuzzy_logic#Comparison_to_probability&quot;&gt;Fuzzy_logic#Comparison_to_probability (english wikipedia)&lt;/a&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;So it is not correct to label the output of Logistic Regression as fuzzy. Aside, the mentioned decision tree also calculates scores (the subjective probability that an instance in the leaf belongs to the particular class), which often results in a majority-decision for the leaf.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Summary&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;But if you are willing to drop the difference between Fuzzy Logic and Probability for the sake of simplicity, you may say that the &lt;em&gt;scores&lt;/em&gt; produced by a suitable classifier are fuzzy, meanwhile the &lt;em&gt;decision&lt;/em&gt; for a class based on the score is crisp. For example in a direct mail campaign, you can calculate a score how likely it is that a customer will respond, but in the end you have to perform a crisp decision which customers you will send an actual letter.&lt;/p&gt;&#10;&#10;&lt;p&gt;This paper might be interesting (&lt;a href=&quot;http://www.mathematik.uni-marburg.de/~eyke/publications/asoc.pdf&quot;&gt;Eyke Hüllermeier- Fuzzy Sets in Machine Learning and Data Mining&lt;/a&gt;). From the abstract:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Over the past years, methods for the automated induction of models and&#10;  the extraction of interesting patterns from empirical data have&#10;  attracted considerable attention in the fuzzy set community. This&#10;  paper briefly reviews some typical applications and highlights&#10;  potential contributions that fuzzy set theory can make to machine&#10;  learning, data mining, and related fields. The paper concludes with a&#10;  critical consideration of recent developments and some suggestions for&#10;  future research directions.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="2" CreationDate="2012-11-16T10:57:03.260" Id="43732" LastActivityDate="2012-11-16T11:13:42.237" LastEditDate="2012-11-16T11:13:42.237" LastEditorUserId="264" OwnerUserId="264" ParentId="43729" PostTypeId="2" Score="7" />
  
  <row Body="&lt;p&gt;If the difference lies only in the relative class frequencies in the training and test sets, then I would recommend the EM procedure introduced in this paper:&lt;/p&gt;&#10;&#10;&lt;p&gt;Marco Saerens, Patrice Latinne, Christine Decaestecker: Adjusting the Outputs of a Classifier to New a Priori Probabilities: A Simple Procedure. Neural Computation 14(1): 21-41 (2002) (&lt;a href=&quot;http://www.mitpressjournals.org/doi/abs/10.1162/089976602753284446&quot;&gt;www&lt;/a&gt;)&lt;/p&gt;&#10;&#10;&lt;p&gt;I've used it myself and found it worked very well (you need a classifier that outputs a probability of class membership though).&lt;/p&gt;&#10;&#10;&lt;p&gt;If the distribution of patterns within each class changes, then the problem is known as &quot;covariate shift&quot; and there is an excellent book by &lt;a href=&quot;http://books.google.co.uk/books/about/Machine_Learning_in_Non_Stationary_Envir.html?id=Akd-8xFKac4C&amp;amp;redir_esc=y&quot;&gt;Sugiyama and Kawanabe&lt;/a&gt;.  Many of the papers by this group are available on-line, but I would strongly recommend reading the book as well if you can get hold of a copy.  The basic idea is to weight the training data according to the difference in density between the training set and the test set (for which labels are not required).  A simple way to get the weighting is by using logistic regression to predict whether a pattern is drawn from the training set or the test set.  The difficult part is in choosing how much weighting to apply.&lt;/p&gt;&#10;&#10;&lt;p&gt;See also the nice blog post by Alex Smola &lt;a href=&quot;http://blog.smola.org/post/4110255196/real-simple-covariate-shift-correction&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-11-16T12:40:23.247" Id="43737" LastActivityDate="2012-11-16T12:48:36.393" LastEditDate="2012-11-16T12:48:36.393" LastEditorUserId="887" OwnerUserId="887" ParentId="43716" PostTypeId="2" Score="7" />
  <row Body="&lt;p&gt;The proper Bayesian solution would be to treat the regularisation parameter as a nuisance parameter and marginalise it out of the analysis, using an appropriate hyper-prior.  In may be possible to do this analytically, which I have found to be reasonably effective, see section 2 of my paper&lt;/p&gt;&#10;&#10;&lt;p&gt;G. C. Cawley and N. L. C. Talbot, Preventing over-fitting during model selection via Bayesian regularisation of the hyper-parameters, Journal of Machine Learning Research, volume 8, pages 841-861, April 2007 (&lt;a href=&quot;http://www.jmlr.org/papers/volume8/cawley07a/cawley07a.pdf&quot; rel=&quot;nofollow&quot;&gt;www&lt;/a&gt;) &lt;/p&gt;&#10;&#10;&lt;p&gt;Alternatively you could choose the regularisation parameter by maximising the Bayesian evidence for the model, which is a common approach in neural networks, see e.g. &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.33.1345&quot; rel=&quot;nofollow&quot;&gt;this paper&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Cross-validation is also a reasonable non-Bayesian approach as Dmitri suggests (+1).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-16T13:11:41.423" Id="43739" LastActivityDate="2012-11-16T13:11:41.423" OwnerUserId="887" ParentId="34569" PostTypeId="2" Score="2" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;Let's say I have a $m \times n$ matrix where $m$ is the number of points and $n$ is the number of dimensions. I would like to give a target dimension parameter which is let's say d. d can be a set of values like $\{2,4,6,\ldots,n\}$. I would then approach the problem using Fisher's linear discriminant analysis to give me output matrix in $m \times d$. Is this possible to do? I still don't understand how LDA reduces dimensions from let's say 10,000 to 1,000.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-11-16T16:16:06.147" Id="43753" LastActivityDate="2013-10-03T10:24:57.977" LastEditDate="2012-12-01T08:43:05.110" LastEditorUserId="17230" OwnerUserId="3899" PostTypeId="1" Score="2" Tags="&lt;dimensionality-reduction&gt;&lt;discriminant-analysis&gt;" Title="How can you make linear discriminant analysis reduce dimensions to the number of dimensions you are looking for?" ViewCount="655" />
  <row Body="&lt;p&gt;The chain rule for conditional probabilities is&#10;$$P(A_1A_2\cdots A_n) = P(A_1)P(A_2\mid A_1)P(A_3\mid A_1A_2) \cdots&#10;P(A_n\mid A_1A_2\cdots A_{n-1}).$$ In your notation (which might be&#10;for joint probability mass functions or densities of random variables, this&#10;becomes&#10;$$p(a,b,c,\cdots, h) = p(a)p(b\mid a)p(c\mid a,b) \cdots p(h\mid a, b, c \cdots)$$&#10;or, for those who march to the beat of a different drummer,&#10;$$p(a,b,c,\cdots, h) = p(a\mid b,c,\cdots h)p(b\mid c, \cdots h)\cdots p(h)$$&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-11-16T17:58:55.597" Id="43763" LastActivityDate="2012-11-16T17:58:55.597" OwnerUserId="6633" ParentId="43758" PostTypeId="2" Score="1" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Is there a measure that shows how well GEE using a ordinal logistic regression model explains the amount of variance in the data? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-16T20:38:12.073" Id="43782" LastActivityDate="2012-11-16T21:11:28.310" LastEditDate="2012-11-16T21:11:28.310" LastEditorUserId="7290" OwnerUserId="16927" PostTypeId="1" Score="3" Tags="&lt;ordinal&gt;&lt;effect-size&gt;&lt;r-squared&gt;&lt;gee&gt;" Title="R-squared equivalent for Generalized Estimating Equations (GEE) using a ordinal logistic regression model" ViewCount="437" />
  <row Body="&lt;p&gt;If you have &quot;continuous&quot; (seemingly, as they could still be discrete) values in between 0 and 1 there are at least two cases:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;They came from a number of independent binary trials and the &quot;continuous&quot; value is the number of successes divided by trials. Then a binomial GLM might be appropriate. In this case you need to fit it in R as &lt;code&gt;glm(cbind(numberSuccesses,numberFailures)~x,family=binomial)&lt;/code&gt; &lt;/li&gt;&#10;&lt;li&gt;If that is not the case, then you might have something for which a &lt;a href=&quot;http://epub.wu.ac.at/726/&quot; rel=&quot;nofollow&quot;&gt;Beta Model&lt;/a&gt; might be more appropriate. The link I provided shows how to do that in R.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Note that in R &lt;code&gt;glm(y~x,family=binomial)&lt;/code&gt; with a &quot;continuous&quot; $y$ will throw a warning and in general the result will not be the same as in the case with number of successes and trials:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(1)&#10;successes&amp;lt;-sample(1:10,100,replace=TRUE)&#10;x&amp;lt;-1:100&#10;n&amp;lt;-12&#10;failures&amp;lt;-n-successes&#10;&#10;summary(glm(cbind(successes,failures)~x,family=binomial))&#10;Call:&#10;glm(formula = cbind(successes, failures) ~ x, family = binomial)&#10;&#10;Deviance Residuals: &#10;    Min       1Q   Median       3Q      Max  &#10;-2.8197  -0.9434   0.0454   0.9358   2.4921  &#10;&#10;Coefficients:&#10;            Estimate Std. Error z value Pr(&amp;gt;|z|)  &#10;(Intercept) -0.24622    0.11349   -2.17     0.03 *&#10;x            0.00080    0.00195    0.41     0.68  &#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 &#10;&#10;(Dispersion parameter for binomial family taken to be 1)&#10;&#10;    Null deviance: 134.99  on 99  degrees of freedom&#10;Residual deviance: 134.82  on 98  degrees of freedom&#10;AIC: 422.2&#10;&#10;Number of Fisher Scoring iterations: 3&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;but &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;props&amp;lt;-successes/n&#10;summary(glm(props~x,family=binomial))&#10;&#10;Call:&#10;glm(formula = props ~ x, family = binomial)&#10;&#10;Deviance Residuals: &#10;   Min      1Q  Median      3Q     Max  &#10;-0.852  -0.282  -0.105   0.394   0.760  &#10;&#10;Coefficients:&#10;             Estimate Std. Error z value Pr(&amp;gt;|z|)&#10;(Intercept) -0.134339   0.403836   -0.33     0.74&#10;x            0.000281   0.006941    0.04     0.97&#10;&#10;(Dispersion parameter for binomial family taken to be 1)&#10;&#10;    Null deviance: 20.888  on 99  degrees of freedom&#10;Residual deviance: 20.887  on 98  degrees of freedom&#10;AIC: 141.3&#10;&#10;Number of Fisher Scoring iterations: 3&#10;&#10;Warning message:&#10;In eval(expr, envir, enclos) : non-integer #successes in a binomial glm!&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="5" CreationDate="2012-11-16T21:35:27.087" Id="43789" LastActivityDate="2012-11-17T14:01:04.047" LastEditDate="2012-11-17T14:01:04.047" LastEditorUserId="8413" OwnerUserId="8413" ParentId="43785" PostTypeId="2" Score="7" />
  <row Body="" CommentCount="0" CreationDate="2012-11-16T23:52:26.457" Id="43800" LastActivityDate="2012-11-16T23:52:26.457" LastEditDate="2012-11-16T23:52:26.457" LastEditorUserId="-1" OwnerUserId="-1" PostTypeId="5" Score="0" />
  
  <row Body="&lt;p&gt;Recall that in the linear regression model&#10;$$&#10;Y_i = \boldsymbol{X}_i'\boldsymbol{\beta} + \varepsilon_i&#10;$$&#10;together with $\mathbb{E}(\varepsilon_i \mid \boldsymbol{X}_i) = 0$, a Wald test of the null hypothesis $\mathbf{r}\boldsymbol{\beta} - \boldsymbol{r}=\boldsymbol{0}$, requires the specification of the non-stochastic matrix $\mathbf{r}$ and  the non-stochastic vector $\boldsymbol{r}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;In &lt;code&gt;R&lt;/code&gt;, you can get away without having to specify the matrix $\mathbf{r}$ (which is typically sparse), and just specify the vector $\boldsymbol{r}$, if, as you slightly paradoxically put it, you want to test restrictions &quot;separately and simultaneously&quot; by  which I presume you mean that your tests do not involve linear combinations of the parameters. &lt;/p&gt;&#10;&#10;&lt;p&gt;So, for example, in the following example, you are jointly testing the null hypothesis that the intercept is 0, and the &lt;code&gt;pop75&lt;/code&gt; variable is equal to 1.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    library(aod)&#10;    require(stats)&#10;    lmLCH &amp;lt;- lm(sr ~ pop15 + pop75 + dpi + ddpi, data = LifeCycleSavings)&#10;    vR &amp;lt;- c(0, 0, 1, 0, 0)&#10;    wald.test(b=coef(lmLCH)-vR, Sigma = vcov(lmLCH), Terms=c(1, 3))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The vector &lt;code&gt;vR&lt;/code&gt; corresponds to the $\boldsymbol{r}$ above. This gives you the output:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Wald test:&#10;----------&#10;&#10;Chi-squared test:&#10;X2 = 16.3, df = 2, P(&amp;gt; X2) = 0.00028&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Equivalently, you can take the longer road home, by specifying the matrix $\mathbf{r}$,&#10;$$&#10;\mathbf{r} = \begin{bmatrix} 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\ 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0\end{bmatrix}&#10;$$&#10;and $\boldsymbol{r} = [0, 1]'$.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;vR &amp;lt;- c(0, 1)&#10;mR &amp;lt;- as.matrix(rbind(c(1, 0, 0, 0, 0), c(0, 0, 1, 0, 0)))&#10;wald.test(b=coef(lmLCH), Sigma = vcov(lmLCH), L = mR, H0 = vR)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Note that the two give identical results.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-11-17T08:15:37.387" Id="43818" LastActivityDate="2012-11-18T06:05:10.403" LastEditDate="2012-11-18T06:05:10.403" LastEditorUserId="8141" OwnerUserId="8141" ParentId="43809" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;Outliers are usually thought of in relation to the model, as the comments already discuss. But that does not say anything about how they are generated: They can be rare events by the very process described by the model (roughly 1 in 10⁹ standard normally distributed numbers will be &amp;lt; -6) or they can be generated by a process that is not included in your model. &lt;/p&gt;&#10;&#10;&lt;p&gt;Usually, one doesn't care about the former, as the model is adequate for them. &lt;/p&gt;&#10;&#10;&lt;p&gt;But with respect to the latter, you can simulate only things where you have an idea of the generating process. If you want unexpected rare events, there is no way but collecting data and waiting for them to occur. And IMHO it doesn't make sense to discuss this without discussing the underlying process/problem/task (not only the model). It is the very nature of these things that you cannot give a typical outlier. And you need to discuss the relevance of your outlier generating process to the model and problem. IMHO, the &lt;a href=&quot;http://en.wikipedia.org/wiki/No_free_lunch_in_search_and_optimization&quot; rel=&quot;nofollow&quot;&gt;no free lunch theorem&lt;/a&gt; applies very much for outlier detection.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Recommended reading ;-) &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.79.2501&amp;amp;rep=rep1&amp;amp;type=pdf&quot; rel=&quot;nofollow&quot;&gt;&quot;Journal of Machine Learning Gossip&quot; Paper of which a few copies still float aroud in the net&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-17T10:09:17.673" Id="43820" LastActivityDate="2012-11-17T10:09:17.673" OwnerUserId="4598" ParentId="38770" PostTypeId="2" Score="1" />
  <row AnswerCount="3" Body="&lt;p&gt;I recently completed a study whereby I randomly assigned participants to one of two treatment groups.  I tested participants at baseline, immediately post-intervention, 1 months, and 4 months on a somewhat large number of outcome variables. I was planning on running several mixed ANOVAs to examine group x time interactions.  Some of the comparisons will be 2 (group) x 2 (time: baseline and post-intervention) comparisons and some will be 2 (group) x 3 (time: baseline, 1 month, 4 month) comparisons.&lt;/p&gt;&#10;&#10;&lt;p&gt;Before beginning my analyses, I compared the two treatment groups on all baseline variables.  I found that the groups differ on 4 baseline variables if I use an alpha level of .05  or 2 baseline variables if I use an alpha level of .01 to compare the groups.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I have two questions about this:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;What alpha level should I be using to compare the groups at&#10;baseline? I was thinking an alpha level of .01 because I am&#10;comparing the two groups on 24 baseline characteristics and I&#10;thought I should chose a more stringent alpha level than .05 to&#10;reduce family-wise error rate seeing as a large number of tests are&#10;being performed, but from my readings it seems most people use .05. &#10;What do you recommend?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;What do I do about these differences? I could include these&#10;variables as covariates, but my sample size is quite small and using&#10;4 covariates does not seem appropriate (which is also partly why I&#10;am favouring only accepting differences if they are significant at&#10;the .05 level)&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Any help on this would be very much appreciated!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-17T13:46:42.227" FavoriteCount="1" Id="43825" LastActivityDate="2014-11-14T03:30:47.943" LastEditDate="2012-11-17T14:10:03.067" LastEditorUserId="8413" OwnerUserId="16941" PostTypeId="1" Score="7" Tags="&lt;mixed-model&gt;&lt;ancova&gt;" Title="Baseline differences in RCT: Which variables (if any) should be included as covariates?" ViewCount="803" />
  <row Body="&lt;p&gt;+1 to @FrankHarrell.  I might add one small point.  If you randomly assigned your participants to the groups, any 'significant' differences in covariate values prior to intervention are necessarily type I errors.  &lt;/p&gt;&#10;" CommentCount="12" CreationDate="2012-11-17T15:08:33.257" Id="43828" LastActivityDate="2012-11-17T15:08:33.257" OwnerUserId="7290" ParentId="43825" PostTypeId="2" Score="2" />
  
  
  
  
  
  <row Body="&lt;p&gt;Check here how others are doing it or participate yourself:&#10;Angry Birds AI Challenge &lt;a href=&quot;http://ai2012.web.cse.unsw.edu.au/abc.html&quot; rel=&quot;nofollow&quot;&gt;http://ai2012.web.cse.unsw.edu.au/abc.html&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-11-17T22:20:21.303" Id="43848" LastActivityDate="2012-11-17T22:20:21.303" OwnerUserId="16952" ParentId="23435" PostTypeId="2" Score="3" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Specifically, I would like to know what ologit in STATA estimates because I tried a simulation  and I had really weird outputs. So I'd like to know how exactly adding a varying term to each threshold e.g. 2*x affects the outcome of the ordered logistic model.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-11-18T11:35:40.010" Id="43873" LastActivityDate="2012-11-18T11:35:40.010" OwnerUserId="16960" PostTypeId="1" Score="1" Tags="&lt;stata&gt;&lt;discrete-data&gt;" Title="What happens when you add a varying term to the thresholds in ordered logistic regression?" ViewCount="126" />
  <row AnswerCount="1" Body="&lt;p&gt;I counducted a mixed model ANOVA with time as the within subjects variable (3 levels) and group as the between subjects variable (2 levels).&#10;All is significant (time, group, and timeXgroup).&#10;To test if differences over the three times are significant in both groups, I conducted two separate repeated measures ANOVAs, one for each group, with time (3 levels) as the withing subjects variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a problem with the control group, because ANOVA for this group is slightly significant (p = 0.044) but the multiple comparisons with Bonferroni correction are all not significant... Indeed, in the graph I can see that the differences across time are not very big. However I don't know how to explain this result...&#10;Can you help me?&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt;&lt;br&gt;&#10;I try to explain myself better so maybe you can understand my situation. These are the descriptive statistics:&lt;/p&gt;&#10;&#10;&lt;p&gt;Control group (N=10)&lt;/p&gt;&#10;&#10;&lt;p&gt;Mean (SD)&lt;br&gt;&#10;1st measure 2.40 (1.36)&lt;br&gt;&#10;2nd measure 2.87 (1.57)&lt;br&gt;&#10;3rd measure 2.55 (1.48)&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Experimental group (N=10)&lt;/p&gt;&#10;&#10;&lt;p&gt;Mean (SD)&lt;br&gt;&#10;1st measure 3.04 (1.14)&lt;br&gt;&#10;2nd measure 6.71 (3.52)&lt;br&gt;&#10;3rd measure 4.64 (1.76)&lt;/p&gt;&#10;&#10;&lt;p&gt;As you can see the control group has a small change over time, the experimental group instead has a big change. I ran the mixed 2x3 ANOVA to see if the two groups were significantly different and if there was an interaction. Since all was significant, I wanted to see if both group differ significantly over time or only the experimental group.&#10;Thank you!&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-11-18T15:50:04.190" FavoriteCount="1" Id="43877" LastActivityDate="2014-06-17T01:09:08.350" LastEditDate="2012-11-19T11:11:22.073" LastEditorUserId="16955" OwnerUserId="16955" PostTypeId="1" Score="3" Tags="&lt;anova&gt;&lt;statistical-significance&gt;&lt;repeated-measures&gt;&lt;post-hoc&gt;&lt;bonferroni&gt;" Title="Anova repeated measures is significant, but all the multiple comparisons with Bonferroni correction are not?" ViewCount="2102" />
  <row AcceptedAnswerId="43976" AnswerCount="3" Body="&lt;p&gt;I want to simulate temperature data for some &quot;what-if&quot; calculations.  The problem is, I only have a time series of 10 actual temperature data values.  I want to use temperature as an input to the simulation, so I need a way to generate a large number of temperature values that are consistent with the original 10 values.   It's probably ok to assume that they came from a normal distribution, but I don't know the mean or the variance. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have no way to prove it, but I doubt the 10 values do a good job of representing the full temperature range.   If I use the &lt;code&gt;sample&lt;/code&gt; function for the simulation, as shown below, I only get the original values back.   That just doesn't look right.   If I use the &lt;code&gt;rnorm&lt;/code&gt; function, I know that I don't know the variance, so I don't think that is right either.    So, I'm left with the &lt;code&gt;rt&lt;/code&gt; function (t-distribution).&lt;/p&gt;&#10;&#10;&lt;p&gt;Below is a mock up of the problem.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ori &amp;lt;- rnorm(n=10, mean=65, sd=5) #original 10 data points&#10;&#10;num.sam &amp;lt;- sample(x=ori, size=100, replace = TRUE) #simulation using sample&#10;num.tdis &amp;lt;- mean(ori) + (rt(n=100, df=10) * sd(ori)) #simulation using a t distribution&#10;&#10;hist(ori, breaks=40:90) &#10;hist(num.sam, breaks=40:90) &#10;hist(num.tdis, breaks=40:90) &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;My questions are,&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;When I only have data (mean and variance unknown), and it is reasonable to assume that the data came from a normal distribution, is it ok to generate data for a simulation using a t-distribution?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;For this type of situation, the only time I would use &lt;code&gt;rnorm&lt;/code&gt; for the simulation is if I knew the variance (not a variance estimated from the data), right?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;If a t-distribution simulation is ok for these conditions, are there any conditions where it is better to just sample the data (for example 100 original data points, 200, etc)?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Edit:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Since I used the original data to estimate the mean and variance, should the degrees of freedom in the third line of the code (for &lt;code&gt;rt(...)&lt;/code&gt;) be reduced from 10 to 9?   Or 8?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="4" CreationDate="2012-11-18T17:42:28.837" FavoriteCount="1" Id="43881" LastActivityDate="2012-11-19T22:14:39.257" LastEditDate="2012-11-19T16:11:42.740" LastEditorUserId="16967" OwnerUserId="16967" PostTypeId="1" Score="6" Tags="&lt;sampling&gt;&lt;simulation&gt;&lt;t-distribution&gt;" Title="With a small sample from a normal distribution, do you simulate using a t distribution?" ViewCount="411" />
  <row AcceptedAnswerId="43888" AnswerCount="1" Body="&lt;p&gt;I'm conducting an ANOVA mixed model 2 X 3 (group X condition). Right now, I'm checking for the assumptions of ANOVA, such as normality. I should check indipendently for group and condition, right? I did it in two ways:&#10;1) By, at first, splitting the sample (selecting &quot;Data/Split file...&quot; and entering the group variable) and then running the &quot;1 sample Kolmogorov-Smirnov Test&quot; and choosing &quot;Normal distribution&quot;.&#10;2) Without splitting the sample, but selecting &quot;Analyze/Descriptive Statistics/Explore...&quot; and entering the group variable as a factor and clicking &quot;Plots...&quot; and checking &quot;Normality plots with test&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;The 1) returns all variables absolutely not significant (0.80 &amp;lt; p &amp;lt; 0.90). The 2) ran both the Kolmogorov-Smirnov and Shapiro-Wilk tests, with the former resulting not significant for all variables (but all p values lower than or equal to 0.20), instead the latter in some cases considerably significant (e.g. 0.012).&lt;/p&gt;&#10;&#10;&lt;p&gt;I don't know which one should I trust, can you help me? Thanks!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-18T17:57:12.373" Id="43882" LastActivityDate="2012-11-18T20:59:24.370" OwnerUserId="16955" PostTypeId="1" Score="0" Tags="&lt;anova&gt;&lt;mixed-model&gt;&lt;spss&gt;&lt;normality&gt;&lt;kolmogorov-smirnov&gt;" Title="Normality test for anova mixed model in SPSS: I did it in two ways but the results are contrasting" ViewCount="1818" />
  
  <row Body="&lt;p&gt;The K-S test under Nonparametrics menu (your point 1) assumes that you know the population parameters (mean and variance) of the distribution; by default they are set equal to your sample statistics but they are treated as true parameters. You shouldn't rely on such K-S test unless your sample is very large.&lt;/p&gt;&#10;&#10;&lt;p&gt;The K-S test under Explore menu (your point 2) applies Lilliefors correction to account for the uncertainty fact that your mean and variance are just sample statistics, not true parameters. You should generally prefer this test. It &quot;stands for&quot; non-normality: p-value is lower.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are known a number of good alternatives to K-S normality test. Shapiro-Wilk is one of them; others include Anderson–Darling, D'Agostino–Pearson, Jarque–Bera - they all test different aspects of a distribution.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-11-18T20:59:24.370" Id="43888" LastActivityDate="2012-11-18T20:59:24.370" OwnerUserId="3277" ParentId="43882" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;If you suspect the numbers have been cooked, i.e. manually selected, one possibility is Benford's law used in forensic accounting, see: &lt;a href=&quot;http://en.wikipedia.org/wiki/Benford%27s_law&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Benford%27s_law&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Aside from that, it is probably easier just to test the whole row for randomness. see: &lt;a href=&quot;http://en.wikipedia.org/wiki/Diehard_tests&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Diehard_tests&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;If you have an overall problem, then you could zero in on its cause. &#10;Of course if you were trying to fool a random number test, you could easily do it by swapping the most frequently occurring number with instances of the least frequently occurring.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-19T03:59:30.627" Id="43911" LastActivityDate="2012-11-19T03:59:30.627" OwnerUserId="16978" ParentId="43902" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;Well, in R you could try this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(MASS)&#10;boxcox(y~x)&#10;plot(1/y^2~x) # since the profile likelihood has a maximum near 2&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/lahlV.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;But it really depends on what you mean by 'better fit to the data'&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-19T09:03:47.367" Id="43921" LastActivityDate="2014-05-29T01:17:06.637" LastEditDate="2014-05-29T01:17:06.637" LastEditorUserId="805" OwnerUserId="805" ParentId="40779" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I went for Fisher's Exact Test and Chi-Square Contingency Test to answer my question. It looked more elegant and intuitive [0,1]. &lt;/p&gt;&#10;&#10;&lt;p&gt;[0] &lt;a href=&quot;http://en.wikipedia.org/wiki/Fisher&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Fisher&lt;/a&gt;'s_exact_test&lt;/p&gt;&#10;&#10;&lt;p&gt;[1] &lt;a href=&quot;http://math.hws.edu/javamath/ryan/ChiSquare.html&quot; rel=&quot;nofollow&quot;&gt;http://math.hws.edu/javamath/ryan/ChiSquare.html&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-19T11:27:26.293" Id="43927" LastActivityDate="2012-11-19T11:27:26.293" OwnerUserId="2706" ParentId="43643" PostTypeId="2" Score="0" />
  
  
  
  <row Body="&lt;p&gt;You should be using a paired T-test since these are paired data, not a 2 sample test.&lt;/p&gt;&#10;&#10;&lt;p&gt;All possible permutations of pre-post data would be obtained using&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;expand.grid(pre=x, post=y)&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;And we know it's only a 2 by 100 matrix which is far from impossible. I don't know why you're replicating &lt;code&gt;x&lt;/code&gt;, or what &lt;code&gt;k&lt;/code&gt; is.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-11-19T19:07:39.757" Id="43959" LastActivityDate="2012-12-12T17:10:41.853" LastEditDate="2012-12-12T17:10:41.853" LastEditorUserId="8013" OwnerUserId="8013" ParentId="43958" PostTypeId="2" Score="2" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I wonder whether any of you can help me. SPSS puts confidence intervals in graphs of frequencies and proportions,&#10;but it clearly is not using a normal approximation, i.e. it is not using the formula $\text{CI} = m \pm 1.96 \sqrt{(p(1-p)/n}$.&#10;apart from anything else the SPSS interval is not symmetrical around the value.&lt;/p&gt;&#10;&#10;&lt;p&gt;So what is it doing? I could not track down the formula anywhere, not even searching on the web (which usually&#10;solves 99% of problems in life, plus or minus 1%).  :)&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-11-19T20:20:13.103" Id="43969" LastActivityDate="2012-11-20T14:25:59.343" LastEditDate="2012-11-19T20:39:01.220" LastEditorUserId="930" OwnerUserId="17002" PostTypeId="1" Score="5" Tags="&lt;confidence-interval&gt;&lt;spss&gt;&lt;proportion&gt;" Title="How are confidence intervals for proportions computed under SPSS?" ViewCount="3183" />
  <row Body="&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;If indeed $u$ is the part of the data you chose not to use and hence treat as unobserved, the correct quantity is the predictive distribution on $u$ given $x_0$, which usually writes as&#10;$$&#10;\int p(u|t,M,Z) p(t,M,Z|x_0) \, \text{d}t\text{d}M\text{d}Z&#10;$$&#10;if $u$ and $x_0$ are independent.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;I am afraid I do not understand your second point. I do not see how you get a posterior conditional on the brightness ($x_0$?) and then compute the posterior of the brightness...&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="2" CreationDate="2012-11-19T20:29:28.040" Id="43970" LastActivityDate="2012-11-19T20:29:28.040" OwnerUserId="7224" ParentId="43938" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;I would elaborate Neil G and Greg Snow's answers as follows :&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;run a noninformative Bayesian inference for your original $10$ data values&lt;/li&gt;&#10;&lt;li&gt;use the posterior predictive distribution to generate new data&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;The posterior predictive distribution derived from a noninformative prior exactly aims to provide your desire: a distribution that generates data &quot;consistent with the original data&quot;, taking into account the uncertainty about the model parameters.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, what is the posterior predictive distribution derived from the noninformative prior ? That depends on the choice of the noninformative prior, but there is a good &quot;default&quot; noninformative prior for the normal sample model. You can also &quot;cheat&quot; a little and use the &quot;Bayesian-frequentist&quot; predictive distribution (also called sometimes &quot;the frequentist predictive distribution&quot;). The principle of the frequentist predictive distribution is the following one. The classical $100(1-\alpha)\%$-prediction interval for a new observation is $\bar{y} \pm \mathrm{t}^*_{n-1}(\alpha/2) \hat\sigma\sqrt{1+\frac{1}{n}}$. Then the Bayesian-frequentist predictive distribution is taken to be the distribution of $\bar{y} + T  \hat\sigma\sqrt{1+\frac{1}{n}}$ where $\bar{y}$ and $\hat\sigma$ are considered as fixed and $T$ has the Student $\mathrm{t}_{n-1}$ distribution. Thus, the $100(1-\alpha)\%$-quantile of the frequentist predictive distribution equals the usual $100(1-\alpha)\%$-upper prediction bound.&lt;/p&gt;&#10;&#10;&lt;p&gt;I do not exactly remember the Bayesian predictive distribution derived from the default noninformative prior but it is very close to the frequentist predictive distribution (there are some slight differences such as $\mathrm{t}^*_{n-\frac{1}{2}}$ instead of $\mathrm{t}^*_{n-1}$). I will update my answer when I will find the formulas.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/36128/frequentist-performance-of-sequential-testing-based-on-predictive-power&quot;&gt;Here&lt;/a&gt; I asked a question related to the performance of these predictive distributions.&lt;/p&gt;&#10;&#10;&lt;p&gt;I claimed that the frequentist predictive distribution is derived from &quot;little cheating&quot; because it does not really has a theoretical fundation. But I'm sure it is possible to show the performance of the use of this distribution in a frequentist sense.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-19T22:07:14.253" Id="43980" LastActivityDate="2012-11-19T22:14:39.257" LastEditDate="2012-11-19T22:14:39.257" LastEditorUserId="8402" OwnerUserId="8402" ParentId="43881" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Statistics graphics use Jeffreys binomial for the CIs.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-11-19T22:44:19.980" Id="43983" LastActivityDate="2012-11-19T22:44:19.980" OwnerUserId="6768" ParentId="43969" PostTypeId="2" Score="3" />
  
  
  <row AcceptedAnswerId="44152" AnswerCount="1" Body="&lt;p&gt;Given the variables $X$ and $Y$, which are correlated, $X\ge0$, $Y\ge0$ and each follow a gamma distribution with different shape parameters, i.e.,$X\sim Gamma(a_1,\alpha)$ and $Y\sim Gamma(a_2,\alpha)$. &lt;/p&gt;&#10;&#10;&lt;p&gt;I understood that the Joint PDF $f_{X,Y}(x,y)$ can be obtained by making use of the Farlie Gumbel Morgenstern (FGM) Bivariate Gamma Copula which is applicable for the case of different shape parameters. The joint PDF is given by&lt;/p&gt;&#10;&#10;&lt;p&gt;$$h_{X,Y}(x,y) = f_X(x)f_Y(y) \left[1+\lambda\left(2F_X(x)-1\right)\left(2F_Y(y)-1\right)\right],\quad |\lambda|\le 1 $$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $f(.)$ denotes the marginal PDF and $F(.)$ denotes the marginal CDF. My major questions are:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;How to determine the value of $\lambda$?&lt;/li&gt;&#10;&lt;li&gt;Is it true that the FGM copula is used only for weak correlation? &lt;/li&gt;&#10;&lt;li&gt;Is there another method of calculating Pearson's correlation coefficient rather than the equation given by&#10;$$ \rho = \lambda K(a_1)K(a_2)$$ &#10;where $K(a)=\displaystyle\frac{1}{2^{2a-1} \beta(a,a) \sqrt{a}}$ and $\beta(.,.)$ is the beta function.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="3" CreationDate="2012-11-20T14:53:05.873" Id="44027" LastActivityDate="2012-11-21T22:37:52.523" LastEditDate="2012-11-20T15:02:00.530" LastEditorUserId="9169" OwnerUserId="9169" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;correlation&gt;&lt;gamma-distribution&gt;&lt;bivariate&gt;&lt;copula&gt;" Title="Farlie-Gumbel-Morgenstern Bivariate Gamma Distirbution" ViewCount="362" />
  <row AnswerCount="0" Body="&lt;p&gt;I was wondering which p-value to use when i am testing the significance of an interaction between two continuous variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is my model with the interaction:&lt;/p&gt;&#10;&#10;&lt;p&gt;Coefficients:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;                  Estimate Std. Error t value Pr(&amp;gt;|t|)    &#10;(Intercept)       110.2584    23.9969   4.595 0.000417 ***&#10;lg_hag              36.0716    18.2778   1.974 0.068510 .      &#10;raceblack         -12.9419     1.8198  -7.112 5.24e-06 ***&#10;racemexican        -9.0254     2.3207  -3.889 0.001636 ** &#10;racemulti/other    -9.8012     2.0640  -4.749 0.000311 ***&#10;sexfemale           3.9612     1.3405   2.955 0.010442 *  &#10;age_yr             -1.1009     0.1691  -6.511 1.38e-05 ***&#10;as.factor(educa)2   6.7660     2.2988   2.943 0.010686 *  &#10;as.factor(educa)3  14.1302     2.2394   6.310 1.92e-05 ***&#10;as.factor(educa)4  15.4776     1.8657   8.296 8.95e-07 ***&#10;as.factor(educa)5  19.8650     2.0524   9.679 1.40e-07 ***&#10;lg_hag:age_yr       -0.5236     0.2555  -2.049 0.07862 . &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So in this model, the p value for the interaction is 0.07862, limit to the significance&lt;/p&gt;&#10;&#10;&lt;p&gt;However, when I run the model without interaction, and then i do a anova (with a Chi square test as the models are GLM) between the two models (with and without interaction), i got a p-value of 0.04 (which is supposed to be the real p value of inteaction, in my view)&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Working (Rao-Scott) LRT for lg_hag:age_yr&#10;Working 2logLR =  4.199194 p= 0.041893&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Please if someone could give me an advice on what is the difference between the two p value, and which one to use to attest for the significance of the interaction.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you in advance&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-11-20T15:15:30.700" Id="44028" LastActivityDate="2012-11-20T16:31:14.067" LastEditDate="2012-11-20T16:31:14.067" LastEditorUserId="14649" OwnerUserId="14649" PostTypeId="1" Score="1" Tags="&lt;anova&gt;&lt;interaction&gt;&lt;p-value&gt;" Title="Difference between p values of interaction" ViewCount="196" />
  
  <row AnswerCount="0" Body="&lt;p&gt;A main hypothesis in our study is that people’s decisions (our dichotomous outcome variable) are influenced by how people in their social context have decided in the same matter.&lt;/p&gt;&#10;&#10;&lt;p&gt;In a multilevel analysis perspective, is it feasible to use as a level 2, or contextual variable, the aggregated (mean) of the outcome variable?&lt;/p&gt;&#10;&#10;&lt;p&gt;KML&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-20T16:09:49.837" Id="44035" LastActivityDate="2012-11-20T16:09:49.837" OwnerUserId="15983" PostTypeId="1" Score="1" Tags="&lt;multilevel-analysis&gt;&lt;aggregation&gt;" Title="Using an aggregated outcome variable as contextual in multilevel analysis" ViewCount="75" />
  
  
  
  <row Body="&lt;p&gt;You can adjust the penalty for different sorts of wrong classification. Consider that you could get 94% correct just by putting everyone in the &quot;no event&quot; category. But that wouldn't be useful.  Clearly, whatever program you are using is, implicitly or explicitly, using a different weighting.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can look at graphs: For continuous independent variables you can look at parallel boxplots. For categorical ones you can look at mosaic plots.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can avoid binning, which can lower statistical power.&lt;/p&gt;&#10;&#10;&lt;p&gt;You could try other methods, such as classification trees.&lt;/p&gt;&#10;&#10;&lt;p&gt;But, in the end, you may just not have a good model.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-11-20T19:53:05.933" Id="44057" LastActivityDate="2012-11-20T19:53:05.933" OwnerUserId="686" ParentId="44053" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="44075" AnswerCount="1" Body="&lt;p&gt;We want to explore the cross sectional association of 5 symptoms of psychopathology with functional impairment. &lt;/p&gt;&#10;&#10;&lt;p&gt;If we just take people who score high on symptom 1, then high on symptom 2, etc., and look at the means of social impairment in these groups, one particular symptom (let's call it symptom 1) stands out that is associated with very high functional impairment (twice as high as the others).  &lt;/p&gt;&#10;&#10;&lt;p&gt;The problem is that this is confounded with the sum score of all symptoms, because people who score high on symptom 1 have high scores on all other symptoms as well. This is different from all other symptoms. People who score high on symptom 2 only have average symptom load on the other symptoms (the same for symptom 3, symptom 4, etc).&lt;/p&gt;&#10;&#10;&lt;p&gt;I wonder (1) what test would be appropriate to test this, and (2) how one would control for the fact that for one symptom the sum of other symptoms seems particularly high. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-20T20:05:14.443" Id="44058" LastActivityDate="2012-11-21T00:09:43.780" LastEditDate="2012-11-20T21:08:51.930" LastEditorUserId="14731" OwnerUserId="14731" PostTypeId="1" Score="0" Tags="&lt;regression&gt;" Title="Regression: what item of subscales is strongest IV controlling for sum score of scale" ViewCount="93" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;If I recall correctly, if the population is normal, then if we take a sample of size n from it, we can use the Chi-Square test for variance to see if our sample's variance has a predetermined value or not.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there an equivalent test we can run if the population is NOT normal?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-11-21T05:16:25.990" Id="44090" LastActivityDate="2012-11-22T02:21:29.523" OwnerUserId="17051" PostTypeId="1" Score="0" Tags="&lt;variance&gt;&lt;chi-squared&gt;" Title="Analog of the Chi Square Test for Variance for Non-normal Populations" ViewCount="136" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm trying to wrap my head around mixture modelling, and I've come across a small matlab script that seems relevant.  In order to familiarize myself with pymix, I've decided to try rewriting the matlab script in python.  I appear to have successfully fitted/decomposed the data with the expectation maximization algorithm, but I'm not sure I understand the nature of the method's output.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is as follows:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;How can I replicate the second figure from the matlab code (below) with the output of &lt;code&gt;pymix.MixtureModel.EM&lt;/code&gt;?&lt;/li&gt;&#10;&lt;li&gt;Can someone explain the output of the above method?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I should mention that I'm horribly under-educated in statistics and mathematics in general (I'm working on it!), so please assume I know nothing =)&lt;/p&gt;&#10;&#10;&lt;p&gt;In any case, here's the matlab script:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;% Generate some data drawn from two Gaussians&#10;data = [0.4+randn(100,1).*0.15; 1+ randn(200,1).*0.25]';&#10;data(data &amp;lt; 0.05) = 0.05;&#10;[n,x] = hist(data);&#10;bar(x,n);&#10;&#10;% Make the mixture model pdf&#10;mixtureGauss = ...&#10;    @(x,m1,s1,m2,s2,theta) (theta*normpdf(x,m1,s1) + (1-theta)*normpdf(x,m2,s2));&#10;&#10;% Set up parameters for the MLE function&#10;options = statset('mlecustom');&#10;options.MaxIter     = 20000;&#10;options.MaxFunEvals = 20000;&#10;&#10;% Get max likilihood parameters for our mixture model (start with some&#10;%   reasonable guesses about the parameters)&#10;p = mle(data, 'pdf', mixtureGauss, 'start', [0.5 0.1 0.5 0.1 0.5], ...&#10;    'lowerbound', [-Inf 0 -Inf 0 0], 'upperbound', [Inf Inf Inf Inf 1], ...&#10;    'options', options);&#10;&#10;% Plot and print information&#10;hold on;&#10;x = linspace(min(data),max(data),100);&#10;plot(x, mixtureGauss(x,p(1),p(2),p(3),p(4),p(5))*max(n), 'r', 'LineWidth', 2);&#10;fprintf('Gauss 1: %0.2f (+/- %0.2f)\n', p(1), p(2));&#10;fprintf('Gauss 2: %0.2f (+/- %0.2f)\n', p(3), p(4));&#10;fprintf('Mix: %0.2f proportion first gaussian\n', p(5));&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And here's what I've done in python so far.  Note that I'm running this code in iPython with the &lt;code&gt;--pylab=inline&lt;/code&gt; option, thereby importing pyplot into my main workspace:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;import numpy as np&#10;import mixture&#10;&#10;# Generate some data drawn from two Gaussians&#10;data = np.concatenate((0.4 + np.random.randn(100) * 0.15, 1 + np.random.randn(200) * 0.25))&#10;&#10;data[np.nonzero(data &amp;lt; .05)] = .05&#10;print type(data), len(data)&#10;&#10;plt = Figure()&#10;hist(data, bins=50)&#10;show()&#10;&#10;#  Create DataSet object&#10;mixdat = mixture.DataSet()&#10;mixdat.fromArray(data)&#10;&#10;# reasonable-guess mixture, akin to random starting point in K-Means&#10;n1 = mixture.NormalDistribution(-2, 0.4)&#10;n2 = mixture.NormalDistribution(2, 0.6)&#10;&#10;# Mixture model and EM clustering&#10;mix = mixture.MixtureModel(2, [.5, .5], [n1, n2])&#10;postmat, _ = mix.EM(mixdat, 40, 0.1)&#10;&#10;fig = Figure()&#10;hist(data, bins=50)&#10;x = np.linspace(np.min(data), np.max(data), 100)&#10;# Now what?&#10;show()&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Any other comments, criticisms, or explanations are more than welcome.  Thanks for putting up with my ignorance ;-)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-21T10:06:38.707" FavoriteCount="1" Id="44105" LastActivityDate="2012-11-21T10:06:38.707" OwnerUserId="17054" PostTypeId="1" Score="1" Tags="&lt;matlab&gt;&lt;python&gt;&lt;mixture&gt;" Title="How can I plot the components of a mixture model obtained from pymix?" ViewCount="293" />
  <row AnswerCount="2" Body="&lt;p&gt;I'm looking to use a multivariate regression for prediction, but making use of (possibly) superior estimates of variance for both the independent and extraneous variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;My approach is to standardize the dependent and extraneous variables (by dividing their respective standard deviations derived from the full data history).  Once I have the standardized regression coefficients, I use these, together with my separately sourced variance estimates, to get back to an equation that will be used for generating the predictions. Do I simply multiply each coefficient by my separately sourced variance estimate?  But what about the dependent variable variance estimate?&lt;/p&gt;&#10;&#10;&lt;p&gt;As background info, my separate sourcing of variances estimates is because I believe I have better (more up-to-date) estimates than available from the full data history (let's say, because the variances aren't stable through time, I can estimate a more timely measure of variance using higher frequency data over a shorter, recent period).&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Is this approach a sensible standard practice?&lt;/li&gt;&#10;&lt;li&gt;How to go from the standardized betas, to un-standardized ones using the high frequency variance estimates?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I read here (http://stats.stackexchange.com/questions/29781/when-should-you-center-your-data-when-should-you-standardize) about WHEN to standardize, but not clear to me if this covers the case of standardizing both dependent and extraneous variables.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you!&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-11-21T10:46:37.837" Id="44109" LastActivityDate="2012-11-22T06:07:25.247" LastEditDate="2012-11-21T11:16:14.420" LastEditorUserId="686" OwnerUserId="17028" PostTypeId="1" Score="2" Tags="&lt;variance&gt;&lt;standardization&gt;&lt;multivariate-regression&gt;" Title="Multivariate regression estimation when the variables' variances are known a priori / sourced seperately" ViewCount="191" />
  
  
  <row Body="&lt;p&gt;&lt;strong&gt;For $n \gt 2$ this needs numeric integration&lt;/strong&gt;, as indicated in several of the links.&lt;/p&gt;&#10;&#10;&lt;p&gt;To be explicit, let $\phi_i$ be the PDF of $X_i$ and $\Phi_i$ be its CDF.  Conditional on $X_1 = t$, the chance that $X_1 \gt X_i$ for the remaining $i$ is the product of the individual chances (by independence):&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\Pr(t \ge X_i, i=2,\ldots,n) = \Phi_2(t)\Phi_3(t)\cdots\Phi_n(t).$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Integrating over all values of $t$, using the distribution function $\phi_1(t)dt$ for $X_1$, gives the answer&lt;/p&gt;&#10;&#10;&lt;p&gt;$$= \int_{-\infty}^{\infty} \phi_1(t) \Phi_2(t)\cdots\Phi_n(t)dt.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;For $n=20$, the integral takes 5 seconds with &lt;em&gt;Mathematica&lt;/em&gt;, given vectors $\mu$ and $\sigma$ of the means and SDs of the variables:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;\[CapitalPhi] = MapThread[CDF[NormalDistribution[#1, #2]] &amp;amp;, {\[Mu], \[Sigma]}];&#10;\[Phi] = PDF[NormalDistribution[First[\[Mu]], First[\[Sigma]]]];&#10;f[t] := \[Phi][t] Product[i[t], {i, Rest[\[CapitalPhi]]}]&#10;NIntegrate[f[t], {t, -Infinity, Infinity}]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;The value can be checked (or even estimated) with a simulation.&lt;/strong&gt; In the same five seconds it takes to do the integral, &lt;em&gt;Mathematica&lt;/em&gt; can do over 2.5 million iterations and summarize their results:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;m = 2500000;&#10;x = MapThread[RandomReal[NormalDistribution[#1, #2], m] &amp;amp;, {\[Mu],\[Sigma]}]\[Transpose];&#10;{1, 1./m} # &amp;amp; /@ SortBy[Tally[Flatten[Ordering[#, -1] &amp;amp; /@ x]], First[#] &amp;amp;]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;For instance, we can generate some variable specifications at random:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;{\[Mu], \[Sigma]} = RandomReal[{0, 1}, {2, n}];&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In one case the integral evaluated to $0.152078$; a simulation returned &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;{{1, 0.152387}, ... }&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;indicating that the first variable was greatest $0.152387$ of the time, closely agreeing with the integral.  (With this many iterations we expect agreement to within a few digits in the fourth decimal place.)&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-11-21T20:23:18.910" Id="44142" LastActivityDate="2012-11-21T20:23:18.910" OwnerUserId="919" ParentId="44139" PostTypeId="2" Score="6" />
  
  <row AnswerCount="0" Body="&lt;p&gt;My seminary paper deals with renewing tourist paths over a certain area. The workers doing the renewing must obviously go through all the paths at least once, but they are limited by a certain distance per day (e.g. 10 km). Where they begin or end is not relevant, they can use the car to get to the start just as well.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a table of paths, specified by starting location, end location and the length. What model should I use to get the optimal solution, where the number of &quot;mandays&quot; is minimized, while all paths get renewed.&lt;/p&gt;&#10;&#10;&lt;p&gt;I tried Chinesse Postman, but it returns one big cycle, which is not exactly what I want.&lt;/p&gt;&#10;&#10;&lt;p&gt;Example:&lt;/p&gt;&#10;&#10;&lt;p&gt;Imagine a graph with 4 nodes A-D, with following paths: A-B 10, A-B 10, A-C 10, B-D 10, C-D 10, C-D 10. Yes, there are two paths between nodes A-B and C-D. The optimal solution is to renew each edge in one day, resulting in 6 days. But the Chinese problem returns a path of 70, because it must go through one of the edges twice (the graph has no Euler cycle - kind of like the seven bridges of Königsberg). So the solution would span 7 days, renewing one path twice. &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-11-21T22:40:07.783" Id="44155" LastActivityDate="2012-11-26T21:07:39.673" LastEditDate="2012-11-26T21:07:39.673" LastEditorUserId="12022" OwnerUserId="12022" PostTypeId="1" Score="1" Tags="&lt;modeling&gt;&lt;optimization&gt;" Title="What model should I use?" ViewCount="67" />
  <row Body="&lt;p&gt;A lot of detail is given by the President of StataCorp, William Gould, in this Stata Journal article.&lt;a href=&quot;http://www.stata-journal.com/sjpdf.html?articlenum=pr0001&quot; rel=&quot;nofollow&quot;&gt;1&lt;/a&gt; It is a very interesting article about quality control of statistical software.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-21T23:16:47.400" Id="44160" LastActivityDate="2012-11-21T23:16:47.400" OwnerUserId="561" ParentId="7563" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;You already answered your own question: you shouldn't use a stacked area plot for any statistic for which the the statistic of the total is not the unweighted sum of the statistics over the component categories. In other words, the statisitic must be a &lt;a href=&quot;http://en.wikipedia.org/wiki/Linear_map&quot; rel=&quot;nofollow&quot;&gt;linear operator&lt;/a&gt;. The arithmetic average, count, and total are all linear operators, but the median, geometric mean, and most other statistics aren't.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, in your example case, I doubt that the difference would be massively noticable just from visual inspection from a plot. Have you tried plotting it to see what happens? You could plot the stacked area chart of the median, then plot a line with the true median over the top.&lt;/p&gt;&#10;&#10;&lt;p&gt;BTW, does this dataset exist? It would be quite a useful teaching tool, and would make a great addition to R's dataset package.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-22T04:19:15.847" Id="44168" LastActivityDate="2012-11-22T04:19:15.847" OwnerUserId="9007" ParentId="29591" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;&lt;strong&gt;Cover's Theorem:&lt;/strong&gt; Roughly stated, it says given any random set of finite points (with arbitrary labels), then with high probability these points can be made linearly separable [1] by mapping them to a higher dimension [2].&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Implication:&lt;/strong&gt; Great, what this theorem tells me is that if I take my dataset and map these points to a higher dimension, then I can easily find a linear classifier. However, most classifiers need to compute some kind of similarity like dot product and this means that the time complexity of a classification algorithm is proportional to the dimension of the data point. So, higher dimension means larger time complexity (not to mention space complexity to store those large dimensional points).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Kernel Trick:&lt;/strong&gt; Let $n$ be the original dimension of data points and $f$ be the map which maps these points to a space of dimension $N (&amp;gt;&amp;gt; n)$. Now, if there is a function $K$ which takes inputs $x$ and $y$ from the original space and computes $K(x, y) = \langle f(x), f(y) \rangle$, then I am able to compute the dot product in higher dimensional space but in complexity $O(n)$ instead of $O(N)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Implication:&lt;/strong&gt; So, if the classification algorithm is only dependent on the dot product and has no dependency on the actual map $f$, I can use the kernel trick to run the algorithm in high dimensional space with almost no additional cost.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Does Linear separability imply that points from the same class will get closer than the points from different classes?&lt;/strong&gt;&#10;No, there is no such guarantee as such. Linear separability doesn't really imply that the point from same class has gotten closer or that the points from two different classes have gotten any further.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;So why would kNN work?&lt;/strong&gt;&#10;It need not! However, if it does, then it is purely because of the kernel. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;What does that mean?&lt;/strong&gt;&#10;Consider the boolean feature vector $x = (x_1, x_2)$. When you use degree two polynomial kernel, the feature vector $x$ is mapped to the vector $(x_1^2, \sqrt{2} x_1x_2, x_2^2)$. From a vector of boolean features, just by using degree two polynomial, we have obtained a feature vector of &quot;conjunctions&quot;. Thus, the kernels themselves produce some brilliant feature maps. If your data has good original features and if your data could benefit from the feature maps created by these kernels. By benefit, I mean that the features produced by these feature maps can bring the points from the same class closer to each other and push points from different classes away, then kNN stands to benefit from using kernels. Otherwise, the results won't be any different than what you get from running kNN on the original data.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Then why use kernel kNN?&lt;/strong&gt;&#10;We showed that the computation complexity of using kernels is just slightly more than the usual kNN and if data benefits from using kernels then why not use them anyway?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Is there any paper that has studied which class of data that can benefit from kernels in kNN?&lt;/strong&gt;&#10;As far as I know, No.&lt;/p&gt;&#10;&#10;&lt;p&gt;[1] &lt;a href=&quot;http://en.wikipedia.org/wiki/Linear_separability&quot;&gt;http://en.wikipedia.org/wiki/Linear_separability&lt;/a&gt; &lt;br /&gt;&#10;[2] &lt;a href=&quot;http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4038449&amp;amp;tag=1&quot;&gt;http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4038449&amp;amp;tag=1&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-22T11:45:19.480" Id="44187" LastActivityDate="2012-11-22T11:45:19.480" OwnerUserId="10823" ParentId="44166" PostTypeId="2" Score="8" />
  
  <row Body="&lt;p&gt;Maybe &lt;a href=&quot;http://www-958.ibm.com/software/data/cognos/manyeyes/&quot; rel=&quot;nofollow&quot;&gt;http://www-958.ibm.com/software/data/cognos/manyeyes/&lt;/a&gt; is what you want. Beware that the data you upload is public though. Edit: Sorry, I see you asked for open source. My bad.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-22T16:58:15.590" Id="44207" LastActivityDate="2012-11-22T18:32:24.517" LastEditDate="2012-11-22T18:32:24.517" LastEditorUserId="17091" OwnerUserId="17091" ParentId="44204" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="44221" AnswerCount="1" Body="&lt;p&gt;Let us consider the following model:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$Y_j = X_j + \epsilon_j \hspace{15pt} j=1, ..., n$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Where $Y_j$ is a noisy signal, $\epsilon_j$ is the noise which is independend from the signal $X_j$. We have only i.i.d. samples of $Y_j$ and $\epsilon_j$ and are interested in the distribution of $X_j$. The density $f_{\epsilon}$ is assumed to be unknown.&#10;&lt;a href=&quot;http://www.math-info.univ-paris5.fr/~comte/BruitInconnu.pdf&quot; rel=&quot;nofollow&quot;&gt;Comte and Lacour&lt;/a&gt; suggest a method based on fourier transform to solve this problem (see section 2.2).&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's call $\varphi_X(t)$ the characteristic function for $X$ and $f^{*}_X$ the fourier transform of the density $f_X$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is my question:&#10;The main idea in deconvolution is to use the independence assumption for $X$ and $\epsilon$ and then use fourier transform to solve the equation $f^{*}_X = f^{*}_Y/f^{*}_{\epsilon}$. Applying the inverse fourier transform leads to an estimate for $f_X$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can I use the characteristic function instead of the fourier transform?&#10;Does this give me any advantages or disadvantages? I assume that both fourier transform or characteristic function could be used but would like to know what other people think.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-22T17:18:59.240" Id="44209" LastActivityDate="2012-11-22T20:36:59.883" OwnerUserId="13525" PostTypeId="1" Score="2" Tags="&lt;probability&gt;&lt;fourier-transform&gt;&lt;method-of-moments&gt;" Title="Deconvolution with fourier transform or characteristic function?" ViewCount="111" />
  <row AnswerCount="1" Body="&lt;p&gt;Suppose you have the logs of a web server. In these logs you have tuples of this kind:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;user1, timestamp1&#10;user1, timestamp2&#10;user1, timestamp3&#10;user2, timestamp4&#10;user1, timestamp5&#10;...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;These timestamps represent e.g. users' clicks. Now, &lt;code&gt;user1&lt;/code&gt; will visit the site multiple times (sessions) during the month, and you'll have bursts of clicks from each user during each session (supposing that when a user visits your site, he'll click on multiple pages).&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose you want to partition these burst of clicks in the sessions that generated them, but you don't have any additional source of information, only the list of timestamps. If you compute the distribution of intervals between two consequent clicks from the same user, you will obtain a long-tailed distribution. Intuitively, you'd look for a &quot;cut parameter&quot;, e.g. N seconds, where if &lt;code&gt;timestamp_{i+1} - timestamp{i} &amp;gt; N&lt;/code&gt;, then your &lt;code&gt;timestamp_{i+1}&lt;/code&gt; is the beginning of the new session. &lt;/p&gt;&#10;&#10;&lt;p&gt;The problem is that this distribution in reality is a mixture of two variables: X = &quot;interval between two consequent clicks in the same session&quot; and Y = &quot;interval between the last click of the previous session and the first of the new one&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;The question is, how to estimate this N, that divides the two distributions (with a bit of overlap, possibly) just by looking at the burst of clicks?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-11-22T18:32:35.217" FavoriteCount="1" Id="44212" LastActivityDate="2012-11-24T19:52:13.430" LastEditDate="2012-11-22T18:34:52.913" LastEditorUserId="930" OwnerUserId="17100" PostTypeId="1" Score="10" Tags="&lt;distributions&gt;&lt;estimation&gt;&lt;mixture&gt;" Title="Long-tailed distribution of time events" ViewCount="115" />
  
  <row Body="&lt;p&gt;Assume the outcome of each trial is a coin flip that only depends on some constant, unknown bias $\theta$ which you are trying to infer so you can predict the next outcome after seeing some data. Imagine that $\theta$ itself was drawn from some prior distribution, which we'll assume to be a &lt;a href=&quot;http://en.wikipedia.org/wiki/Beta_distribution#Bayesian_inference&quot; rel=&quot;nofollow&quot;&gt;Beta distribution&lt;/a&gt; with paramaters $a,b$, depicted graphically below &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/rkt5Y.png&quot; alt=&quot;Beta prior&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Then the generative model for your data can be written as&#10;$$&#10;\theta \sim Beta(a,b)&#10;$$&#10;$$&#10;X \sim Binomial(n,\theta)&#10;$$&#10;where $X$ is number of successes (or 1's in your example) out of $n$ trials.&lt;/p&gt;&#10;&#10;&lt;p&gt;The first distribution is known as your &lt;strong&gt;prior&lt;/strong&gt; and the second is your &lt;strong&gt;likelihood&lt;/strong&gt;. The Beta distribution is conjugate to the Binomial distribution, which means your posterior is still a Beta distribution. I assume you're familiar with Bayes' rule at least in theory so I'll just explain practically how to update your belief distribution in this model and make predictions about upcoming trials. &lt;/p&gt;&#10;&#10;&lt;p&gt;The &lt;strong&gt;predictive distribution&lt;/strong&gt; in the case of the Beta-Binomial model is simply the expectation (the mean) of your belief about $\theta$, which for $Beta(a,b)$ is $\frac{a}{a+b}$. So for example if you have no reason to assume a priori that any value b/t 0 and 1 is more likely than any other, you could set $a=b=1$ so so that your belief was totally uniform(see plot). Then your prediction is $\frac{1}{1+1} = 0.5$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Say that you observe 10 trials with 8 successes and 2 failures. The posterior distribution is then $Beta(a+8,b+2)$. Notice that the paramaters $a,b$ of your $Beta(a,b)$ prior can be interpreted as &quot;psuedo-observations&quot;, where $a$ is the number heads and $b$ the number of tails that you have in effect hallucinated, since they're treated the same as actual observations are in your posterior belief. &lt;/p&gt;&#10;&#10;&lt;p&gt;So you can easily calculate the predicted outcome for your examples above, but you have to assume some parameter values $a$ and $b$ for your prior. Then your prediction is simply &#10;$\frac{a+x}{a+b+N}$, where $x$ is the number of successes observed and $N$ is total number of trials. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-11-22T19:45:28.413" Id="44215" LastActivityDate="2015-02-24T02:07:18.167" LastEditDate="2015-02-24T02:07:18.167" LastEditorUserId="17096" OwnerUserId="17096" ParentId="44173" PostTypeId="2" Score="11" />
  
  
  
  <row AcceptedAnswerId="44233" AnswerCount="3" Body="&lt;p&gt;I am a newbie in data mining world. I have a general question.&#10;I have a data set which has 10 independent variables and one target variable named as category which has 9 values like: 1, 2, 3, 4, 5, 6, 7, 8, 9.&#10;the 10 independent variables have different kind of range of values. some of them have values between 0 - 5000, some have big range like 5,000,000 - 100,000,000 etc.&lt;/p&gt;&#10;&#10;&lt;p&gt;Moreover there is no specific relation  (linear etc.) existing between target and independent variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am basically trying to predict the target variable category by using all of these independent variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can someone suggest what should be my approach? I am very confused. Should I use regression models, decision trees or cluster analysis?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-11-22T22:32:57.333" Id="44227" LastActivityDate="2013-09-20T14:38:37.750" LastEditDate="2012-12-17T15:16:11.573" LastEditorUserId="7290" OwnerDisplayName="user16603" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;regression&gt;&lt;clustering&gt;&lt;data-mining&gt;&lt;cart&gt;" Title="Which type of regression fits better?" ViewCount="709" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm using SigmaPlot to run PCA on various measurements that should all correspond to size of an animal. Running PCA using a covariance matrix (instead of a correlation matrix, since all of the measurements are measured in the same units) yielded really, really small eigenvalues for the first three PCs, despite relatively high proportion of variance explained. I've read that proportion of variance explained should correspond to eigenvalue, but I'm not sure how (and I can't seem to find out).&lt;/p&gt;&#10;&#10;&lt;p&gt;Given the proportion of variance explained, how do I calculate the eigenvalues (i.e., confirm that SigmaPlot is not making some sort of error). Thanks!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-11-22T23:06:25.037" Id="44229" LastActivityDate="2012-11-22T23:06:25.037" OwnerUserId="7344" PostTypeId="1" Score="0" Tags="&lt;pca&gt;&lt;eigenvalues&gt;" Title="How do I convert the variances explained by a principal component into an eigenvalue?" ViewCount="484" />
  <row Body="&lt;p&gt;First of all. 9 categories sound like a lot. How big is you sample?&lt;/p&gt;&#10;&#10;&lt;p&gt;Start by assessing the independent variables. You might want to remove the ones which have a score on 3 standard deviations above them mean. This should reduce you range. Next you want to assess if the independent variables follows a normal distribution. If not, you want to apply some transformation to make them normal distributed. It is not a necessity but it is good common practice.&lt;/p&gt;&#10;&#10;&lt;p&gt;The way you describe it, logistic regression is the way to go with the methods you are mentioning. It all comes out on the characteristics of your dependent variable Cluster analysis is an explorative/undirected technique so that can not be used for prediction. K-means clustering can however be used for prediction...&lt;/p&gt;&#10;" CommentCount="11" CreationDate="2012-11-22T23:40:42.300" Id="44234" LastActivityDate="2012-11-23T11:06:17.313" LastEditDate="2012-11-23T11:06:17.313" LastEditorUserId="17107" OwnerUserId="17107" ParentId="44227" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;One way to analyze this is a Chi-squared test. Here's &lt;a href=&quot;http://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test#Goodness_of_fit&quot; rel=&quot;nofollow&quot;&gt;an example&lt;/a&gt;. But, as you point out, that discards information on repeated measures per individual.&lt;/p&gt;&#10;&#10;&lt;p&gt;An alternative is a &lt;a href=&quot;http://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval&quot; rel=&quot;nofollow&quot;&gt;Binomial proportion test&lt;/a&gt;. The idea is that you have observed some quantity $a_i$ of A answers from each subject $i$, out of n=24 questions.&lt;/p&gt;&#10;&#10;&lt;p&gt;$a_i \sim Bin(p,n)$&lt;/p&gt;&#10;&#10;&lt;p&gt;Here, $p$ is the latent true proportion of preference for A in your sample of subjects. By establishing a confidence interval over $p$, we can see if your expected proportion of 50% A answers falls within the CI; if it does not, you can say that people are significantly more likely to choose A or B.&lt;/p&gt;&#10;&#10;&lt;p&gt;The &lt;a href=&quot;http://cran.r-project.org/web/packages/binom/&quot; rel=&quot;nofollow&quot;&gt;R package binom&lt;/a&gt; sounds like it's relevant.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-11-23T01:41:23.047" Id="44238" LastActivityDate="2012-11-26T15:39:07.460" LastEditDate="2012-11-26T15:39:07.460" LastEditorUserId="8207" OwnerUserId="8207" ParentId="44200" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="44249" AnswerCount="1" Body="&lt;p&gt;I'm trying to fit raw data to curves, which works well on an individual basis.  However, I'd like to &quot;share&quot; parameters (sometimes referred as nested parameters) across more than one data series.  Is there a way to do this in R?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-11-23T06:59:43.017" FavoriteCount="1" Id="44246" LastActivityDate="2012-11-25T07:57:11.427" OwnerUserId="17113" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;nonlinear-regression&gt;&lt;curve-fitting&gt;&lt;nls&gt;" Title="nls curve fitting of nested/shared parameters" ViewCount="768" />
  
  
  <row Body="&lt;p&gt;Let $y_i$ denote the random variable $\hat{f}(x_i)$ and $Y_n$ denote $\{y_i\}_{i \leq n}$,&lt;/p&gt;&#10;&#10;&lt;p&gt;In the case where you have to fix all your points $\{x_i\}_{i \leq n}$ before you get any observation $y_i$, I suggest you to consider points which maximize some &lt;a href=&quot;http://en.wikipedia.org/wiki/Optimal_design&quot; rel=&quot;nofollow&quot;&gt;optimal design&lt;/a&gt; criterion. Like StasK said, &lt;strong&gt;D-optimality&lt;/strong&gt; is a common criterion. &lt;strong&gt;Latin Hypercube Sampling&lt;/strong&gt; is a simple way to approximate these designs, and is often implemented in software like Matlab.&lt;/p&gt;&#10;&#10;&lt;p&gt;If your procedure is sequential, i.e. you know $Y_i$ before to choose $x_{i+1}$, you may want to include your prior knowledge about $f$. A good criterion is then &lt;strong&gt;Information Gain&lt;/strong&gt; (a.k.a &lt;strong&gt;Mutual Information&lt;/strong&gt;).&#10;$$I(y_{i+1} \mid Y_i) = H(y_{i+1}) - H(y_{i+1} \mid Y_i)$$&#10;where $H(X)$ denote the &lt;a href=&quot;http://en.wikipedia.org/wiki/Differential_entropy&quot; rel=&quot;nofollow&quot;&gt;differential entropy&lt;/a&gt; of the variable $X$.&#10;You can then choose the $x$ maximising the information gain at each step,&#10;$$x_{i+1} = \underset{x \in S^n}{argmax}\ I(x \mid Y_i)$$&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-11-23T13:45:34.140" Id="44266" LastActivityDate="2012-11-23T13:56:31.373" LastEditDate="2012-11-23T13:56:31.373" LastEditorUserId="16137" OwnerUserId="16137" ParentId="44195" PostTypeId="2" Score="3" />
  <row AnswerCount="1" Body="&lt;p&gt;i am training a SVM (RBF kernel) with a dataset of ~1500 samples (balanced) using fminsearch on the CV error for parameter optimization (C and s). &lt;/p&gt;&#10;&#10;&lt;p&gt;After i found the &quot;best&quot; parameters (local optima possible) i am retraining my model on the whole dataset to derive a &quot;final&quot; model. &lt;/p&gt;&#10;&#10;&lt;p&gt;Is this a wise thing to do? Would the final model be proned to overfitting? &lt;/p&gt;&#10;&#10;&lt;p&gt;I experience worse performance on unseen data which might be OK as the CV during my optimization approach produces a somewhat optimistic estimate on the test error. &lt;/p&gt;&#10;&#10;&lt;p&gt;I think this adresses a pretty general problem but i could not find proper reasoning yet... Would it be a reasonable alternative to use just one of the models from the best performing crossvalidation?&lt;/p&gt;&#10;&#10;&lt;p&gt;I assumed that once the parameters are fixed the model will not suffer from overfitting no matter how many more samples i use for training?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks,&lt;/p&gt;&#10;&#10;&lt;p&gt;Pir&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-23T14:37:54.580" Id="44273" LastActivityDate="2012-11-23T15:28:39.013" OwnerUserId="17044" PostTypeId="1" Score="3" Tags="&lt;cross-validation&gt;&lt;svm&gt;&lt;train&gt;" Title="SVM retrain on whole dataset for final model --&gt; overfitting?" ViewCount="355" />
  
  <row Body="&lt;p&gt;The part of the overall random forest algorithm that uses &lt;code&gt;mtry&lt;/code&gt; is (adapted from &lt;a href=&quot;http://www-stat.stanford.edu/~tibs/ElemStatLearn/download.html&quot;&gt;The Elements of Statistical Learning&lt;/a&gt;):&lt;/p&gt;&#10;&#10;&lt;p&gt;At each terminal node that is larger than minimal size,&lt;/p&gt;&#10;&#10;&lt;p&gt;1) Select &lt;em&gt;mtry&lt;/em&gt; variables at random from the $p$ regressor variables,&lt;/p&gt;&#10;&#10;&lt;p&gt;2) From these &lt;em&gt;mtry&lt;/em&gt; variables, pick the best variable and split point,&lt;/p&gt;&#10;&#10;&lt;p&gt;3) Split the node into two daughter nodes using the chosen variable and split point.&lt;/p&gt;&#10;&#10;&lt;p&gt;As an aside - you can use the &lt;code&gt;tuneRF&lt;/code&gt; function in the &lt;code&gt;randomForest&lt;/code&gt; package to select the &quot;optimal&quot; &lt;code&gt;mtry&lt;/code&gt; for you, using the out-of-bag error estimate as the criterion.&lt;/p&gt;&#10;&#10;&lt;p&gt;The random selection of variables at each node splitting step is what makes it a random forest, as opposed to just a bagged estimator.  Quoting from The Elements of Statistical Learning, p 588 in the second edition:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The idea in random forests ... is to improve the variance reduction of bagging by reducing the correlation between the trees, without increasing the variance too much.  This is achieved in the tree-growing process through random selection of the input variables.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;There is no incremental increase in bias due to this.  Of course, if the model itself is fundamentally biased, e.g., by leaving out important predictor variables, using random forests won't make the situation any better, but it won't make it worse either.&lt;/p&gt;&#10;&#10;&lt;p&gt;The unbalanced use of predictor variables just reflects the fact that some are less important than others, where important is used in a heuristic rather than a formal sense, and as a consequence, for some trees, may not be used often or at all.  For example, think about what would happen if you had a variable that was barely significant on the full data set, but you then generated a lot of bootstrap datasets from the full data set and ran the regression again on each bootstrap dataset.  You can imagine that the variable would be insignificant on a lot of those bootstrap datasets.  Now compare to a variable that was extremely highly significant on the full dataset; it would likely be significant on almost all of the bootstrap datasets too.  So if you counted up the fraction of regressions for which each variable was &quot;selected&quot; by being significant, you'd get an unbalanced count across variables.  This is somewhat (but only somewhat) analogous to what happens inside the random forest, only the variable selection is based on &quot;best at each split&quot; rather than &quot;p-value &amp;lt; 0.05&quot; or some such.&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT in response to a question by the OP: Note, however, that variable importance measures are not based solely on counts of how many times a variable is used in a split.  Consequently, you can have &quot;important&quot; variables (as measured by &quot;importance&quot;) that are used less often in splits than less &quot;important&quot; variables (as measured by &quot;importance&quot;.)  For example, consider the model:&lt;/p&gt;&#10;&#10;&lt;p&gt;$ y_i = I(x_i &amp;gt; c) + 0.25*z_i^2 + e_i$&lt;/p&gt;&#10;&#10;&lt;p&gt;as implemented and estimated by the following R code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x &amp;lt;- runif(500)&#10;z &amp;lt;- rnorm(500)&#10;y &amp;lt;- (x&amp;gt;0.5) + z*z/4 + rnorm(500)&#10;df &amp;lt;- data.frame(list(y=y,x=x,z=z,junk1=rnorm(500),junk2=runif(500),junk3=rnorm(500)))&#10;foo &amp;lt;- randomForest(y~x+z+junk1+junk2+junk3,mtry=2,data=df)&#10;importance(foo)&#10;      IncNodePurity&#10;x         187.38456&#10;z         144.92088&#10;junk1     102.41875&#10;junk2      93.61086&#10;junk3      92.59587&#10;&#10;varUsed(foo)&#10;[1] 16916 17445 16883 16434 16453&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Here $x$ has higher importance, but $z$ is used more frequently in splits; $x$'s importance is high but in some sense very local, while $z$ is more important over the full range of $z$ values.  &lt;/p&gt;&#10;&#10;&lt;p&gt;For a fuller discussion of random forests, see Chap. 15 of The Elements..., which the link above allows you to download as a pdf for free.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-11-23T17:25:31.857" Id="44287" LastActivityDate="2012-12-05T15:57:29.197" LastEditDate="2012-12-05T15:57:29.197" LastEditorUserId="7555" OwnerUserId="7555" ParentId="44267" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;Based on the link provided below, the reasons why LS discriminant is not performing good in the upper left graph are as follow:&lt;br&gt;&#10;-Lack of robustness to outliers.&lt;br&gt;&#10;- Certain datasets unsuitable for least squares classification.&lt;br&gt;&#10;- Decision boundary corresponds to ML solution under Gaussian conditional distribution.  But binary target values have a distribution far from Gaussian.&lt;/p&gt;&#10;&#10;&lt;p&gt;Look at page 13 in &lt;a href=&quot;http://www.cedar.buffalo.edu/~srihari/CSE574/Chap4/Chap4-Part1.pdf&quot; rel=&quot;nofollow&quot;&gt;Disadvantages of Least Squares.&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-24T00:00:46.307" Id="44302" LastActivityDate="2012-11-24T00:07:37.287" LastEditDate="2012-11-24T00:07:37.287" LastEditorUserId="13138" OwnerUserId="13138" ParentId="43867" PostTypeId="2" Score="2" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I'm performing dimension reduction on some data sets and would like to evaluate how has a particular dimension reduction algorithm performed in terms of how much data is lost. If we are given 1000 dimensions, and we reduce it to 2, then how effective is it? I'm trying to figure till what should you do DR such that your results don't go bad? Is there a metric which does this? I'm using PCA.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Can I use some distance metric to do the evaulation?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-11-24T01:22:50.777" FavoriteCount="2" Id="44313" LastActivityDate="2012-11-24T18:45:25.473" LastEditDate="2012-11-24T02:44:31.907" LastEditorUserId="3899" OwnerUserId="3899" PostTypeId="1" Score="4" Tags="&lt;dimensionality-reduction&gt;" Title="How to evaluate dimension reduction from n-space to d-space?" ViewCount="133" />
  <row Body="&lt;p&gt;First, to address the issue of bell-shaped predictive values.  This will be a heuristic explanation, to say the least.  Let's take a linear regression as our &quot;example&quot; case.  If you think of the predictor variables as being drawn from some 20+ - dimensional probability distribution (more on the &quot;+&quot; later), a prediction from a linear regression can be thought of as a weighted average of 20+ random variables... the distribution of which, under a wide range of conditions, will tend towards Normality as the number of random variables goes up.  Hence the bell-shaped curve.&lt;/p&gt;&#10;&#10;&lt;p&gt;The &quot;+&quot; is present because, from a numerical standpoint, a categorical variable with 6 levels is equivalent to 6 binary variables (which, obviously, are not independent.)  Note I'm not claiming that these 6 binary variables contribute to the asymptotic Normal distribution to the same degree as a continuous variable would.&lt;/p&gt;&#10;&#10;&lt;p&gt;Insofar as other methodologies all have a &quot;weighted average&quot; flavor to them, something similar will occur, though mediated by the actual calculations involved.  Here's an example using random forests; I have 10 predictors, independently distributed $U(0,1)$, and the target variable $y$ is the normalized rank of the sum of the square roots of the 10 predictors (so $y$ is in (0,1]).&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;U &amp;lt;- as.data.frame(matrix(runif(10000),1000))&#10;y &amp;lt;- rowSums(sqrt(U))&#10;U$y &amp;lt;- rank(y)/length(y)&#10;&#10;foo &amp;lt;- randomForest(y~V1+V2+V3+V4+V5+V6+V7+V8+V9+V10, mtry=4, data=U)&#10;hist(predict(foo))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/s72yJ.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;As you can see, the bell-shaped histogram of the predicted values occurs in other contexts than yours.&lt;/p&gt;&#10;&#10;&lt;p&gt;Second, converting to a more uniformly-shaped prediction.  One way would be to rank the predicted values from 1-38000, then scale down to 0-1, just as with the original rank-valued target variable.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Here's what happens when we continue the above example in this vein:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; u.predict &amp;lt;- rank(predict(foo))/length(predict(foo))&#10;&amp;gt; mean((u.predict-U$y)^2)&#10;[1] 0.007450796&#10;&#10;&amp;gt; mean((predict(foo)-U$y)^2)&#10;[1] 0.01875836&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The mean squared error of the ranked predictions has decreased by a factor of roughly 2.5 relative to the &quot;raw&quot; predictions.  My guess as to why this works, which I admit surprised me, is that the original variable is a normalized rank, not a true interval-valued variable (i.e., 0.002 is not 2 times 0.001, really,) and the ranked predictions are in a way correcting for the fact that the estimation procedures are all assuming that they are.  &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-11-24T03:00:32.980" Id="44316" LastActivityDate="2012-11-24T03:09:59.667" LastEditDate="2012-11-24T03:09:59.667" LastEditorUserId="7555" OwnerUserId="7555" ParentId="44308" PostTypeId="2" Score="5" />
  <row AcceptedAnswerId="87659" AnswerCount="1" Body="&lt;p&gt;Is there any information out there about the distribution whose $n$th cumulant is given by $\frac 1 n$? The cumulant-generating function is of the form &#10;$$&#10;\kappa(t) = \int_0 ^ 1 \frac{e^{tx} - 1}{x} \ dx.&#10;$$&#10;I've run across it as the limiting distribution of some random variables but I haven't been able to find any information on it. &lt;/p&gt;&#10;" CommentCount="9" CreationDate="2012-11-24T03:19:39.243" FavoriteCount="1" Id="44319" LastActivityDate="2014-02-28T12:58:30.003" LastEditDate="2014-02-24T13:51:24.543" LastEditorUserId="22047" OwnerUserId="5339" PostTypeId="1" Score="16" Tags="&lt;distributions&gt;" Title="Distribution with $n$th cumulant given by $\frac 1 n$?" ViewCount="312" />
  <row AcceptedAnswerId="44337" AnswerCount="1" Body="&lt;p&gt;In &lt;code&gt;matlab&lt;/code&gt;, classregtree can be used to implement classification and regression trees (CART) you can find this in &lt;a href=&quot;http://www.mathworks.co.uk/help/stats/classregtree.html&quot; rel=&quot;nofollow&quot;&gt;the documentation&lt;/a&gt; however it's not clear what methods are used for either classification or regression, 3 methods exist:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Bagging&lt;/strong&gt; decision trees, an early ensemble method, builds multiple decision trees by repeatedly resampling training data with replacement, and voting the trees for a consensus prediction.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;A Random Forest&lt;/strong&gt; classifier uses a number of decision trees, in order to improve the classification rate.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Boosted Trees&lt;/strong&gt; can be used for regression-type and classification-type problems.&#10;Rotation forest - in which every decision tree is trained by first applying principal component analysis (PCA) on a random subset of the input features.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm tempted to say random forest but would like to clarify, does anyone know which method matlab implements for classification?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-24T16:35:57.173" Id="44336" LastActivityDate="2012-11-24T16:50:58.283" LastEditDate="2012-11-24T16:46:41.853" LastEditorUserId="686" OwnerUserId="6875" PostTypeId="1" Score="0" Tags="&lt;classification&gt;&lt;matlab&gt;&lt;cart&gt;" Title="Classification and regression trees (cart)" ViewCount="963" />
  
  <row Body="&lt;p&gt;Normally it means that you are taking the expectation with respect to that distribution (or that probability measure). Sometimes we change the probability measure and therefore the expectations are taken with respect to the new probability measure. So they want to specify exactly with respect to which probability measure, they are taking the expectations.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-11-24T21:24:07.323" Id="44350" LastActivityDate="2012-11-24T21:35:04.847" LastEditDate="2012-11-24T21:35:04.847" LastEditorUserId="13138" OwnerUserId="13138" ParentId="44349" PostTypeId="2" Score="4" />
  
  <row AcceptedAnswerId="44376" AnswerCount="1" Body="&lt;p&gt;So I have data from a randomized blind trial of 1mg of nicotine gum on dual n-back working memory scores; I analyzed them as usual with a t-test and found a small increase in means but a large increase in standard deviations on a f-test! Strange. I also have data for each day on mood/productivity that day on a 1-5 scale.&lt;/p&gt;&#10;&#10;&lt;p&gt;I wondered: is nicotine following an inverse U-curve, where it causes higher scores on the worser days (1-3) and lower scores on the better days (3-5)? I look around and it seems I want a multinomial logistic regression comparing the placebo &amp;amp; active days.&lt;/p&gt;&#10;&#10;&lt;p&gt;I enter the data &amp;amp; load &lt;code&gt;mlogit&lt;/code&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;nicotine &amp;lt;- read.table(stdin(),header=TRUE)&#10;day      active mp score&#10;20120824 1      3  35.2&#10;20120827 0      5  37.2&#10;20120828 0      3  37.6&#10;20120830 1      3  37.75&#10;20120831 1      2  37.75&#10;20120902 0      2  36.0&#10;20120905 0      5  36.0&#10;20120906 1      5  37.25&#10;20120910 0      5  49.2&#10;20120911 1      3  36.8&#10;20120912 0      3  44.6&#10;20120913 0      5  38.4&#10;20120915 0      5  43.8&#10;20120916 0      2  39.6&#10;20120918 0      3  49.6&#10;20120919 0      4  38.4&#10;20120923 0      5  36.2&#10;20120924 0      5  45.4&#10;20120925 1      3  43.8&#10;20120926 0      4  36.4&#10;20120929 1      3  43.8&#10;20120930 1      3  36.0&#10;20121001 1      3  46.0&#10;20121002 0      4  45.0&#10;20121008 0      2  34.6&#10;20121009 1      3  45.2&#10;20121012 0      5  37.8&#10;20121013 0      4  37.2&#10;20121016 0      4  40.2&#10;20121020 1      3  39.0&#10;20121021 0      3  41.2&#10;20121022 0      3  42.2&#10;20121024 0      5  40.4&#10;20121029 1      2  41.4&#10;20121031 1      3  38.4&#10;20121101 1      5  43.8&#10;20121102 0      3  48.2&#10;20121103 1      5  40.6&#10;&#10;library(mlogit)&#10;Nicotine &amp;lt;- mlogit.data(nicotine,shape=&quot;wide&quot;, choice=&quot;mp&quot;)&#10;mlogit(score ~ (active + mp)^2, Nicotine)&#10;Error in solve.default(H, g[!fixed]) : &#10;  Lapack routine dgesv: system is exactly singular&#10;Calls: mlogit ... mlogit.optim -&amp;gt; as.vector -&amp;gt; solve -&amp;gt; solve.default&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The error also happens even with the simplest call I can think of:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;mlogit(score ~ active, Nicotine)&#10;Error in solve.default(H, g[!fixed]) : &#10;  Lapack routine dgesv: system is exactly singular&#10;Calls: mlogit ... mlogit.optim -&amp;gt; as.vector -&amp;gt; solve -&amp;gt; solve.default&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Reading the documentation for &lt;code&gt;mlogit&lt;/code&gt; didn't much help, and look at the other questions having the same error, they're different enough I can't tell whether they apply or not.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you for your assistance.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-11-25T01:23:36.633" FavoriteCount="1" Id="44359" LastActivityDate="2012-11-25T13:49:23.580" OwnerUserId="16897" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;logistic&gt;&lt;multinomial&gt;" Title="R: mlogit error on data - &quot;system is exactly singular&quot;" ViewCount="3169" />
  <row Body="&lt;p&gt;Rather than using a stepwise procedure, I would fit an L1-regularized model, and discard predictors whose coefficients are effectively forced to be zero. See [&lt;a href=&quot;http://ai.stanford.edu/~ang/papers/icml04-l1l2.pdf&quot; rel=&quot;nofollow&quot;&gt;Ng 2004&lt;/a&gt;].&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-11-25T04:27:46.330" Id="44364" LastActivityDate="2012-11-25T04:27:46.330" OwnerUserId="8207" ParentId="41217" PostTypeId="2" Score="2" />
  
  
  
  
  <row Body="&lt;p&gt;If you have repeated measures data on two groups, and the distribution of your dependent variable is approximately normal, with homogeneous variance, and you are interested in knowing whether the pattern of change in scores differs between groups, then a repeated measures ANOVA would be appropriate. The effect of interest would be the two way interaction between the between-subject factor (group) and the within subject factor (time), which indicates that the pattern of change over time differs between the groups.  If you have repeated measures on your subjects, it is important to use a repeated measures design as this will increase the power and accuracy of your hypothesis tests.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-25T21:53:33.763" Id="44398" LastActivityDate="2012-12-26T08:55:07.843" LastEditDate="2012-12-26T08:55:07.843" LastEditorUserId="930" OwnerUserId="16989" ParentId="38053" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;I am not an expert on CARTs but you can try the book &lt;a href=&quot;http://www-stat.stanford.edu/~tibs/ElemStatLearn/&quot; rel=&quot;nofollow&quot;&gt;&quot;Elements of Statistical Learning&quot;&lt;/a&gt; which is freely available online (see chapter 9 for CARTs). I believe the book was written by one of the creators of the CART algorithm (Friedman).&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-11-25T23:36:20.507" Id="44401" LastActivityDate="2012-11-25T23:36:20.507" OwnerUserId="14434" ParentId="44382" PostTypeId="2" Score="2" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am working with Markov models with ordinal, non-normal distributions of probabilities.   Ultimately, I would like to create a metric for determining the probability of a specific path occurring.   At any given point along the path, there is a range of possible moves with associated probabilities.  However, these are not normal distributions which makes it difficult to discuss their statistical likelihood.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to create some type of metric that would measure the amount of &quot;deviation&quot; from more probable paths.   Because the data varies so drastically, it is difficult to get a good idea of how likely each event was.   A sampling distribution doesn't say much because variation from one path to the next path could be a difference of up to 60%.   Also, at each state there might be one possible move or 30.   This high degree of variation between states makes it difficult to measure.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-26T07:31:44.113" Id="44417" LastActivityDate="2012-11-26T07:31:44.113" OwnerUserId="17182" PostTypeId="1" Score="2" Tags="&lt;probability&gt;&lt;ordinal&gt;&lt;markov-process&gt;" Title="Metric for ordinal, non-normal distribution in Markov models" ViewCount="75" />
  <row Body="&lt;p&gt;I would try using EM.&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose that your exponential distribution is X, and your gamma distribution is Y.  We will learn their distributions in a Bayesian way since both are exponential families.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, for each training example $z$, we will find $x$ and $y$ such that $z=x + y$ where we assume that $x$ was drawn from $X$ and $y$ was drawn from $Y$.  Then, we will update the parameters in a Bayesian way.&lt;/p&gt;&#10;&#10;&lt;p&gt;To find $x$ and $y$, we run a simple bounded minimizer, and minimize the log-likelihood of $P(X=x, Y=y)$.  (Ideally, we would use the posterior predictive distribution, but it's probably best to get this working first.)&lt;/p&gt;&#10;&#10;&lt;p&gt;To update the parameters, we let the mean of $X$ be $E(x)$, the mean of $Y$ is $E(y)$, and $E(\log(Y)) = E(\log(y))$.  This is the standard Bayesian update.  You will probably want a different parametrization of the Gamma distribution, but that is a different question I think.&lt;/p&gt;&#10;&#10;&lt;p&gt;I've had pretty good results with this kind of approach.  Convergence can be made faster by throwing out some of the old data. (Above, the calculation of expectations relies on totals and counts — throwing out data is scaling these quantities.)&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-11-26T08:22:49.237" Id="44419" LastActivityDate="2012-11-26T08:28:15.390" LastEditDate="2012-11-26T08:28:15.390" LastEditorUserId="858" OwnerUserId="858" ParentId="44418" PostTypeId="2" Score="1" />
  
  <row AnswerCount="1" Body="&lt;p&gt;This question was on a HW in my Statistical Theory class and I find the professor's answer and explanation to be unsatisfactory. Please give me some guidance as to why &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;$\bar{x}$ is the MLE if this is the case, or &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Let me know if I am correct to think that both $\bar{x}$ and $1-\bar{x}$ maximize the likelihood function and therefore the MLE is not unique, or&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;If the problem is even well defined as it is posed.  I feel like this could be the case also.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I understand the definition of the MLE in more regular circumstances, as detailed in the Principles section of &lt;a href=&quot;http://en.wikipedia.org/wiki/Maximum_likelihood&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Maximum_likelihood&lt;/a&gt;, but the weird form of the PDF brings up issues with supremums that I am not used to dealing with.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Let $X_1,...,X_n$ be an i.i.d. sequence of 0-1 valued RV's with the probabilities &lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;P(X_1=1)=\begin{cases}&#10;\theta, &amp;amp; \theta\in\mathbb{Q}\\1-\theta, &amp;amp; \theta\notin\mathbb{Q}&#10;\end{cases}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\theta\in(0,1)$. Does the MLE of $\theta$ exist?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Outline of Professor's solution:&lt;/strong&gt; This is the main idea of my professors solution. The likelihood function is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;L(\theta|x_1,...,x_n)=\{\theta^{\sum{x_j}}(1-\theta)^{n-\sum{x_j}}\chi_{\theta\in\mathbb{Q}} +\theta^{n-\sum{x_j}}(1-\theta)^{\sum{x_j}}\chi_{\theta\notin\mathbb{Q}}\}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\chi_A$ is the indicator function of the set $A$. &#10;We have&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;\begin{eqnarray}&#10;\underset{\theta\in[0,1]}{\sup} L(\theta|x_1,...,x_n)&amp;amp;=&amp;amp;\underset{\theta\in[0,1]}{\sup} {\{\theta^{\sum{x_j}}(1-\theta)^{n-\sum{x_j}}\chi_{\theta\in\mathbb{Q}} +\theta^{n-\sum{x_j}}(1-\theta)^{\sum{x_j}}\chi_{\theta\notin\mathbb{Q}}\}}&#10;\\&amp;amp;=&amp;amp; \max\{\underset{\theta\in\mathbb{Q}}{\sup} \{\theta^{\sum{x_j}}(1-\theta)^{n-\sum{x_j}}\},\underset{\theta\notin\mathbb{Q}}{\sup} \{\theta^{n-\sum{x_j}}(1-\theta)^{\sum{x_j}}\}\}&#10;\\&amp;amp;=&amp;amp; \max\{\bar{x}^{\sum{x_j}}(1-\bar{x})^{n-\sum{x_j}},(1-\bar{x})^{n-\sum{x_j}}\bar{x}^{\sum{x_j}}\}&#10;\\&amp;amp;=&amp;amp; \bar{x}^{\sum{x_j}}(1-\bar{x})^{n-\sum{x_j}}&#10;\end{eqnarray}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Professor:&lt;/strong&gt; At this point, the professor argues that the supremum in the second term,$\underset{\theta\notin\mathbb{Q}}{\sup} \{\theta^{n-\sum{x_j}}(1-\theta)^{\sum{x_j}}\}\}$ is not attained since $\bar{x}$ is a rational number.  Since the data consists of rational numbers, the supremum of the first term , $\underset{\theta\in\mathbb{Q}}{\sup} \{\theta^{\sum{x_j}}(1-\theta)^{n-\sum{x_j}}\}$ is attained at $\hat{\theta}_1=\bar{x}$ and that this is the MLE of $\theta$.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Me:&lt;/strong&gt; It seems like we should consider the supremum over the closure of the sets $\mathbb{Q}$ and $\mathbb{R}\backslash\mathbb{Q}$, which would be $[0,1]$, in which case $\sup L(\theta|x_1,...,x_n)$ is achieved at both $\hat{\theta}_1=\bar{x}$ and $\hat{\theta}_2=1-\bar{x}$. Otherwise, we are essentially assuming that $\theta$ is rational and ignoring irrational $\theta$.  Is this the case?  If so, is this an undesirable property of the Likelihood Principle in weird cases like this? Is there any plausible situation where issues like this occur? Should I stop worrying about weird problems like this?&lt;/p&gt;&#10;&#10;&lt;p&gt;As an aside, considering $[0,1]\backslash\mathbb{Q}$ has Lebesgue measure 1 and $[0,1]\cap\mathbb{Q}$ has Lebesgue measure 0, it seems like $\bar{x}$ is a bad estimator, since it is an estimate of $\theta$ if it is in a very small set.  Also, if $\theta\in\mathbb{R}\backslash\mathbb{Q}$, $\hat{\theta}_2=1-\bar{x}$ is consistent, so I can't think of a good reason why $\bar{x}$ is better.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt; As @cardinal pointed out, the $x_i$ are obviously rational, so this is not an issue. This addressed my first (silly) misunderstanding, which involved assuming the estimator $\bar{x}$ could be irrational or rational.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-11-26T17:51:53.010" Id="44447" LastActivityDate="2014-07-06T14:47:17.807" LastEditDate="2012-11-26T20:35:16.177" LastEditorUserId="12518" OwnerUserId="12518" PostTypeId="1" Score="4" Tags="&lt;self-study&gt;&lt;mathematical-statistics&gt;&lt;maximum-likelihood&gt;" Title="Weird MLE Problem based on Dirichlet Function" ViewCount="203" />
  <row AcceptedAnswerId="44492" AnswerCount="1" Body="&lt;p&gt;Considering two Bayesian models:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Poisson Likelihood &amp;amp; Beta Prior:&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;$p(y|\lambda) \sim \text{Pois}(\lambda)$, $p(\lambda) \sim \text{Be}(a, b)$:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ p(\lambda|y) \propto \lambda^{a-1}e^{-b\lambda} \times \lambda^{y}e^{-\theta} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ = \lambda^{a+y-1}e^{-\lambda(1+b)} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \sim \text{Ga}(a+y, b+1) $$&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Normal likelihood &amp;amp; Normal Prior:&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;$p(x|\theta, \phi) \sim \text{N}(\theta, \phi)$, $p(\theta) \sim \text{N}(\theta_{0}, \phi_{0})$:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ p(\theta|x) \propto \text{exp}\left\{-\frac{1}{2}(\theta-\theta_{0})^{2}/\phi_{0}\right\} \times \text{exp}\left\{-\frac{1}{2}(x-\theta)^{2}/\phi\right\} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ = \text{exp}\left\{-\frac{1}{2}\left(\frac{\theta^{2}-2\theta\theta_{0}+\theta_{0}^{2}}{\phi_{0}}+\frac{x^{2}-2x\theta+\theta^{2}}{\phi}\right)\right\} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \propto \text{exp}\left\{-\frac{1}{2}\theta^{2}(\phi_{0}^{-1}+\phi^{-1})+\theta\left(\frac{\theta_{0}}{\phi_{0}}+\frac{x}{\phi}\right)\right\} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \sim \text{N}(\theta_{1}, \phi_{1}) $$&lt;/p&gt;&#10;&#10;&lt;p&gt;where:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\phi_{1} = \frac{1}{\phi_{0}^{-1}+\phi^{-1}}$, $\theta_{1} = \phi_{1}\left(\frac{\theta_{0}}{\phi_{0}}+\frac{x}{\phi}\right)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Why is it, in the first model, the terms ($a$, $b$, $y$, $n$) in the first model are retained, but in the second model, the terms $\left(\frac{\theta_{0}^{2}}{\phi_{0}}, \frac{x^{2}}{\phi}\right)$ are dropped?&lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, once again in the second model, why is it that the posterior mean is equal to $\theta_{1}$ and the posterior variance is equal to $\phi_{1}$?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm aware that if some term is a multiple of the parameter of interest (e.g. $c\theta$ or even $\frac{\theta}{c}$), then the scalar ($c$, in this instance) is absorbed by the proportionality sign, leaving just the parameter ($\theta$), but, if possible, could somebody explain the general rules of proportionality with regard to exponents?&lt;/p&gt;&#10;&#10;&lt;p&gt;A few examples:&lt;/p&gt;&#10;&#10;&lt;p&gt;$e^{c+\theta}$,&#10;$e^{c\theta}$,&#10;$\theta^{c+x}$,&#10;$\theta^{cx}$&lt;/p&gt;&#10;&#10;&lt;p&gt;Assuming $\theta$ is the parameter of interest, what terms would a proportionality sign absorb in the above expressions?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-11-26T18:20:10.400" Id="44450" LastActivityDate="2012-11-27T06:52:18.827" LastEditDate="2012-11-27T06:48:17.403" LastEditorUserId="7224" OwnerUserId="16527" PostTypeId="1" Score="1" Tags="&lt;bayesian&gt;&lt;parameterization&gt;" Title="Proportionality in Bayesian Models: What Is Absorbed?" ViewCount="88" />
  
  
  
  <row Body="&lt;p&gt;Your call to stats.chi2 is indeed incorrect. When you map your data using the mahalanobis distance, it is theoretically $\chi^2_2$ data, so you do not need to play with the loc, scale parameters in the stats.chi2 function (but do keep df=2, like you did). Here's my modified code, plus a pretty visualization of outlier detection.&lt;/p&gt;&#10;&#10;&lt;p&gt;I should mention this is somewhat cheating. We used the true mean and true covariance matrix to find outliers. In practice, we only have estimates (which depended on the outiers themselves!), hence we require more robust mean and covariance estimates, see this great &lt;a href=&quot;ftp://ftp.win.ua.ac.be/pub/preprints/99/Fasalg99.pdf&quot; rel=&quot;nofollow&quot;&gt;paper.&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The data that is above the 99.5% line is shown in red below&#10;&lt;img src=&quot;http://i.stack.imgur.com/tiw6U.png&quot; alt=&quot;The $\chi^2$ test for outliers&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Yf6hH.png&quot; alt=&quot;observed data. Red points are considered outliers&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#imports and definitions&#10;import numpy as np&#10;import scipy.stats as stats&#10;import scipy.spatial.distance as distance&#10;import matplotlib.pyplot as plt&#10;chi2 = stats.chi2&#10;np.random.seed(111)&#10;&#10;&#10;#covariance matrix: X and Y are normally distributed with std of 1&#10;#and are independent one of another&#10;covCircle = np.array([[1, 0.], [0., 1.]])&#10;circle = np.random.multivariate_normal([0, 0], covCircle, 1000) #1000 points around [0, 0]&#10;mahalanobis = lambda p: distance.mahalanobis(p, [0, 0], covCircle.T)&#10;d = np.array(map(mahalanobis, circle)) #Mahalanobis distance values for the 1000 points&#10;d2 = d ** 2 #MD squared&#10;&#10;degrees_of_freedom = 2&#10;&#10;x = range( len( d2 ))&#10;&#10;plt.subplot(111)&#10;&#10;plt.scatter( x, d2 )&#10;&#10;plt.hlines( chi2.ppf(0.95, degrees_of_freedom), 0, len(d2), label =&quot;95% $\chi^2$ quantile&quot;, linestyles = &quot;solid&quot; ) &#10;plt.hlines( chi2.ppf(0.975, degrees_of_freedom), 0, len(d2), label =&quot;97.5% $\chi^2$ quantile&quot;, linestyles=&quot;dashed&quot; ) &#10;plt.hlines( chi2.ppf(0.99, degrees_of_freedom), 0, len(d2), label =&quot;99.5% $\chi^2$ quantile&quot;, linestyles = &quot;dotted&quot; )&#10;&#10;plt.legend()&#10;plt.ylabel(&quot;recorded value&quot;)&#10;plt.xlabel(&quot;observation&quot;)&#10;plt.title( 'Detection of outliers at different $\chi^2$ quantiles' )&#10;&#10;plt.show()&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2012-11-26T23:34:02.557" Id="44468" LastActivityDate="2012-11-26T23:34:02.557" OwnerUserId="11867" ParentId="28593" PostTypeId="2" Score="1" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a list of &lt;a href=&quot;http://en.wikipedia.org/wiki/Unix_time&quot; rel=&quot;nofollow&quot;&gt;epoch/unix times&lt;/a&gt; at which an event happened. My hypothesis is that there are certain times during the week when this event might happen more frequently. How can I visualize/determine which times are most probable for this event to happen? I'd like to know both time and day of week. I'm working with excel. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;1353951487&#10;1353950051&#10;1353948443&#10;1353919982&#10;1353882762&#10;1353724050&#10;1353693717&#10;1353505109&#10;1353474394&#10;1353415614&#10;1353415429&#10;1353397809&#10;1353383231&#10;1353375862&#10;1353342807&#10;1353298391&#10;1353298644&#10;1353295194&#10;1353250737&#10;etc...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Since epoch time starts on a Thursday morning at 0:00 UTC, I subtracted three days from each time, to shift all times to seconds since Sunday Jan 4th 1970. (since the values listed are epoch seconds)&lt;/p&gt;&#10;&#10;&lt;p&gt;I then found the time modulo (seconds in a week) to find the remainder of seconds which took place after the most recent Sunday. &lt;/p&gt;&#10;&#10;&lt;p&gt;For the times listed above, here are the seconds since the most recent Sunday at 0:00 UTC.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Time        Seconds since most recent Sunday 0:00 UTC&#10;1353951487  149887&#10;1353950051  148451&#10;1353948443  406043&#10;1353919982  377582&#10;1353882762  340362&#10;1353724050  181650&#10;1353693717  151317&#10;1353505109  567509&#10;1353474394  536794&#10;1353415614  478014&#10;1353415429  477829&#10;1353397809  460209&#10;1353383231  445631&#10;1353375862  438262&#10;1353342807  405207&#10;1353298391  360791&#10;1353298644  361044&#10;1353295194  357594&#10;1353250737  313137&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="6" CreationDate="2012-11-27T04:57:44.537" Id="44485" LastActivityDate="2013-07-28T21:19:55.750" LastEditDate="2013-07-28T21:19:55.750" LastEditorUserId="919" OwnerUserId="17215" PostTypeId="1" Score="4" Tags="&lt;time-series&gt;&lt;probability&gt;&lt;excel&gt;" Title="Visualizing probability of event over time, based on epoch time" ViewCount="123" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I was arguing with my Stats teacher today in class because we were stuck on the topic of re-expression. She decided to give me extra credit if I could prove to her, using the internet, something related to unit conversions during re-expression. When you perform a power transformation with a power of $-1/2$ or $-1$, you're supposed to change the units and the value to negative (ie. for a power of $-1$ you might get $-32.8$ -gallon/mile). Are you supposed to change it back to $32.8$ gallon/mile?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-11-27T06:15:35.263" Id="44490" LastActivityDate="2013-01-06T06:31:03.767" LastEditDate="2012-12-07T05:32:23.807" LastEditorUserId="805" OwnerUserId="17211" PostTypeId="1" Score="1" Tags="&lt;self-study&gt;&lt;data-transformation&gt;&lt;power-law&gt;&lt;units&gt;" Title="Re-expression units conversion" ViewCount="52" />
  <row AnswerCount="0" Body="&lt;p&gt;I have been asked to analyze data from field observations of a fish community.&#10;Data are collected in two sampling-sites during 16 days (nonconsecutive), twice a day. Dependent variable is biomass. &lt;/p&gt;&#10;&#10;&lt;p&gt;The aim of the study is to find differences between Day/Night biomass and two tide level biomass, and interaction of those two factors.&lt;/p&gt;&#10;&#10;&lt;p&gt;If the observations were independent of each other then ANOVA would be OK. But in this case all observations come from only two sites (subjects). Due to fact that there are only two sites, than random effect model or repeated measures ANOVA also can't be used.&lt;/p&gt;&#10;&#10;&lt;p&gt;What could be solution to use in this situation?&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2012-11-27T10:09:14.953" Id="44503" LastActivityDate="2012-11-27T12:17:20.823" LastEditDate="2012-11-27T12:17:20.823" LastEditorUserId="686" OwnerUserId="17226" PostTypeId="1" Score="3" Tags="&lt;anova&gt;" Title="Violation of independence, ANOVA" ViewCount="254" />
  
  <row Body="&lt;p&gt;A completely different way would be to directly calculate the area of your polygon:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(geometry)&#10;polyarea(x=Data$Q, y=Data$DOC)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This yields 0.606.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-11-27T13:11:27.133" Id="44512" LastActivityDate="2012-11-27T13:11:27.133" OwnerUserId="1352" ParentId="44505" PostTypeId="2" Score="4" />
  
  
  <row AcceptedAnswerId="44612" AnswerCount="2" Body="&lt;p&gt;I'm running a lmer mixed effects model with a four-level factor (levels &quot;0&quot;,&quot;10&quot;,&quot;100&quot;,&quot;1000&quot;) as the fixed effect.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lmer(free ~ reward.f + (1|S), longdata)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I know that by default, R uses treatment contrasts and the levels 10, 100, and 1000 are compared to level &quot;0&quot;. I would instead like each level to be compared to the previous one, to test a monotonic decrease in &quot;free&quot; across levels of &quot;reward.f&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;I would do this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;contr.mat &amp;lt;- matrix(c(c(-1,1,0,0),c(0,-1,1,0),c(0,0,-1,1)),4)&#10;colnames(contr.mat) &amp;lt;- c(10,100,1000)&#10;contrasts(longdata$reward.f) &amp;lt;- contr.mat&#10;    contrasts(longdata$reward.f)&#10;     10 100 1000&#10;0    -1   0    0&#10;10    1  -1    0&#10;100   0   1   -1&#10;1000  0   0    1&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Is this the correct contrast matrix for these comparisons?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-27T17:10:48.733" FavoriteCount="1" Id="44527" LastActivityDate="2012-11-29T06:17:30.183" OwnerUserId="17237" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;hypothesis-testing&gt;&lt;mixed-model&gt;&lt;lmer&gt;&lt;contrasts&gt;" Title="Contrast for hypothesis test in R (lmer)" ViewCount="1705" />
  <row AnswerCount="0" Body="&lt;p&gt;The &lt;a href=&quot;http://en.wikipedia.org/wiki/Copula_%28probability_theory%29#Fr.C3.A9chet.E2.80.93Hoeffding_copula_bounds&quot; rel=&quot;nofollow&quot;&gt;Fréchet–Hoeffding upper bound&lt;/a&gt; applies to the copula distribution function and it is given by&lt;/p&gt;&#10;&#10;&lt;p&gt;$$C(u_1,...,u_d)\leq \min\{u_1,..,u_d\}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a similar (in the sense that it depends on the marginal densities) upper bound for the copula density $c(u_1,...,u_d)$ instead of the CDF?&lt;/p&gt;&#10;&#10;&lt;p&gt;Any reference would be greatly appreciated.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-11-27T17:11:45.807" Id="44528" LastActivityDate="2013-07-21T00:46:33.893" LastEditDate="2013-07-21T00:46:33.893" LastEditorUserId="22468" OwnerUserId="17238" PostTypeId="1" Score="6" Tags="&lt;references&gt;&lt;joint-distribution&gt;&lt;bounds&gt;&lt;copula&gt;" Title="Upper bounds for the copula density?" ViewCount="184" />
  <row AnswerCount="1" Body="&lt;p&gt;Suppose I am using a restaurant rating as an explanatory variable in a regression. The rating is  defined as $R=\frac{G}{G+B+N+S}$, where $G$ is good, $B$ is bad, $N$ is neutral and $S$ is silent. I have two conceptual issues. First, I want to adjust the rating when it's based on a small number of experiences, perhaps by shrinking it towards the overall mean. Second, people seem overly reluctant to rate highly-attended restaurants ($S$ is very high for places like the local joint that's been around for 30 years). If I plot ratings against $\ln(\text{experiences})$, I get an inverted U shape.&lt;/p&gt;&#10;&#10;&lt;p&gt;Are there any transformations that I can use to remedy these two issue?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-11-27T18:21:44.567" Id="44532" LastActivityDate="2012-11-29T14:18:54.670" LastEditDate="2012-11-29T13:57:58.847" LastEditorUserId="7290" OwnerUserId="7071" PostTypeId="1" Score="5" Tags="&lt;data-transformation&gt;&lt;rating&gt;&lt;shrinkage&gt;" Title="adjust rating based on number of experiences" ViewCount="81" />
  <row AnswerCount="1" Body="&lt;p&gt;&quot;discriminant function&quot; and &quot;classification function&quot; are two terms used in literature to denote a a function that maps a feature vector into a discrete class variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;I presume &quot;discriminant function&quot; has it's origin in statistics and &quot;classification function&quot; in machine learning. Is there any reason these terms cant be used as synonyms? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-27T20:32:18.360" Id="44544" LastActivityDate="2012-11-27T21:18:01.240" OwnerUserId="16837" PostTypeId="1" Score="0" Tags="&lt;classification&gt;&lt;terminology&gt;&lt;discriminant-analysis&gt;" Title="Is &quot;discriminant function&quot; a synonym for &quot;classification function&quot;" ViewCount="83" />
  <row Body="&lt;p&gt;Functional Data Analysis probably provides the framework you are looking for. Your wording &quot;&lt;em&gt;quite long series data&lt;/em&gt;&quot;, &quot;&lt;em&gt;continuous variables&lt;/em&gt;&quot; echoes the basic intuitions behind this approach. It actually seems you are describing a problem closely related to multivariate functional regression. &#10;Yao 's &lt;a href=&quot;http://anson.ucdavis.edu/~mueller/sparsereg.pdf&quot; rel=&quot;nofollow&quot;&gt;Functional Linear Regression Analysis for Longitudinal Data (preprint)&lt;/a&gt; describes in detail the theory behind this and Müller's et al. &lt;a href=&quot;http://www.biomedcentral.com/content/pdf/1471-2105-9-60.pdf&quot; rel=&quot;nofollow&quot;&gt;Inferring gene expression dynamics via functional regression analysis&lt;/a&gt; paper provides a nice test case (and presents some of the theory again). There is work on &lt;a href=&quot;http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9876.2009.00689.x/abstract&quot; rel=&quot;nofollow&quot;&gt;functional mixed effects regression&lt;/a&gt; also in case you are interested in &quot;clustering&quot; your data somehow (gender-wise, sibling associations etc.). &lt;/p&gt;&#10;&#10;&lt;p&gt;Assuming that your &quot;weapon of choice&quot; is MATLAB, the package &lt;a href=&quot;http://anson.ucdavis.edu/~ntyang/PACE/&quot; rel=&quot;nofollow&quot;&gt;PACE&lt;/a&gt; provides a lot of readily available routines for regression, smoothing, warping (time-registration) and component analysis (on a regular or irregular grid) for your data. (For the ME models you will probably need to use Functional PCA and then model the projections with R's &lt;a href=&quot;http://lme4.r-forge.r-project.org/&quot; rel=&quot;nofollow&quot;&gt;lmer()&lt;/a&gt;).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-11-28T05:40:50.983" Id="44566" LastActivityDate="2012-11-28T06:15:07.033" LastEditDate="2012-11-28T06:15:07.033" LastEditorUserId="11852" OwnerUserId="11852" ParentId="44548" PostTypeId="2" Score="4" />
  
  
  <row Body="&lt;p&gt;Take a look here: Ruscio, J., &amp;amp; Kaczetow, W. (2008). Simulating multivariate nonnormal data using an iterative technique. Multivariate Behavioral Research, 43, 355-381.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-11-28T11:01:08.213" Id="44586" LastActivityDate="2012-11-28T11:01:08.213" OwnerUserId="17269" ParentId="43520" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;Yes, for Single Exponential Smoothing, the lead time demand (which is the sum of the forecasts over the lead time) is necessarily a simple multiple of a one-step-forecast.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you use a more complex model (with seasonality and/or trend), this will of course not hold any more. In addition, you will need to think about safety stocks and quantile forecasts.&lt;/p&gt;&#10;&#10;&lt;p&gt;I recommend &lt;a href=&quot;http://otexts.com/fpp/&quot; rel=&quot;nofollow&quot;&gt;this free online textbook on forecasting&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-28T14:04:54.060" Id="44601" LastActivityDate="2012-11-28T14:04:54.060" OwnerUserId="1352" ParentId="44599" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="44636" AnswerCount="1" Body="&lt;p&gt;Suppose I have a python function using scipy that returns the expectation $E\left[ X \right]$ for some data assuming it is gamma distributed:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;def expectation(data):&#10;    shape,loc,scale=scipy.stats.gamma.fit(data)&#10;    expected_value = shape * scale&#10;    return expected_value&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;(My understanding is that scipy's parameterization of the gamma leaves us with $E\left[ X \right] = shape \cdot scale$.) However, I would like to generalize my code so I can drop in different distributions in place of the gamma -- for example, the log-normal distribution. Is there a way to write that code in a general way? In other words, how do I finish this function:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;def expectation(data, dist=scipy.stats.gamma):&#10;    ???&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I see a few possible approaches:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Use the &lt;code&gt;scipy.stats.*.expect&lt;/code&gt; method. Thus far I haven't been able to figure out how to use it. How would I parameterize the method given the &lt;code&gt;shape,loc,scale&lt;/code&gt; parameters above?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Use the &lt;code&gt;mean&lt;/code&gt; method of a &quot;frozen&quot; random variable object. In scipy-speak, is &quot;mean&quot; equivalent to $E\left[ X \right]$?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Give up on writing general code and just compute $E\left[ X \right]$ directly for each distribution. I don't want to do this if I can avoid it.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Additionally, please address whether under your suggested method I would pay any performance penalty, i.e. because it uses a numerical rather than analytical approach to the integral in evaluating the expectation.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-11-28T16:19:59.410" FavoriteCount="1" Id="44615" LastActivityDate="2012-11-28T21:00:07.143" OwnerUserId="17278" PostTypeId="1" Score="2" Tags="&lt;expected-value&gt;&lt;lognormal&gt;&lt;scipy&gt;" Title="How do you compute expected value of arbitrary distributions in scipy?" ViewCount="788" />
  <row Body="&lt;p&gt;Spearman's or Kendall's correlations are the standard way to do this.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-28T16:35:03.377" Id="44618" LastActivityDate="2012-11-28T16:35:03.377" OwnerUserId="1352" ParentId="44616" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;One solution would be to use a bootstrap test as an approximation to a permutation test.  Permutation tests are exact and most powerful; in this case there are too many permutations to calculate every one of them, so you'd approximate the test with the bootstrap.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Basically, you:&lt;/p&gt;&#10;&#10;&lt;p&gt;1) Calculate your test statistic, label it $T_0$, on the actual data, say for illustrative purposes the same chi-square statistic you've already calculated, &lt;/p&gt;&#10;&#10;&lt;p&gt;2) Construct 1,000 or 10,000 or so (&quot;many&quot;) random contingency tables under the assumption the null hypothesis is true, and for each one calculate the chi-square statistic, label them $T_1 \dots T_B$.  &lt;/p&gt;&#10;&#10;&lt;p&gt;3) Compare your test statistic's value $T_0$ with the the test statistic values $T_1 \dots T_B$ from the randomly-generated contingency tables, and see what fraction are more extreme than $T_0$; this gives you a bootstrap p-value.   &lt;/p&gt;&#10;&#10;&lt;p&gt;We are approximating the distribution of the test statistic under the null hypothesis by randomly generating a lot of values for the test statistic under the null hypothesis; this lets us estimate the p-value associated with the value of the statistic we actually observed. &lt;/p&gt;&#10;&#10;&lt;p&gt;I can't help you with the SPSS part of this, unfortunately.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's a reference which I've found helpful in the past: &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/038720279X&quot;&gt;Permutation, Parametric, and Bootstrap Tests of Hypotheses (Good)&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-11-28T18:33:22.473" Id="44627" LastActivityDate="2012-11-28T18:33:22.473" OwnerUserId="7555" ParentId="44622" PostTypeId="2" Score="8" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I am using R with the packages &lt;a href=&quot;http://cran.r-project.org/web/packages/kernlab/index.html&quot; rel=&quot;nofollow&quot;&gt;kernlab&lt;/a&gt; / &lt;a href=&quot;http://cran.r-project.org/web/packages/caret/index.html&quot; rel=&quot;nofollow&quot;&gt;caret&lt;/a&gt; and doing some analysis with SVM (&lt;code&gt;ksvm&lt;/code&gt;).&#10;I am using a Radial Based kernel for classification.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a few categorical variables which are set as factors in R, so they are internally represented as distinct integers. &lt;/p&gt;&#10;&#10;&lt;p&gt;Say in the case of a categorical variable with 3 levels, can I just leave it alone and SVM handles this automatically: levels 1, 2, 3.  Or do I have to dummy code them to two columns like so:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x0     x1&#10; 0      0          = level 1&#10; 0      1          = level 2&#10; 1      0          = level 3&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;etc?&lt;/p&gt;&#10;&#10;&lt;p&gt;I looked in the &lt;a href=&quot;http://rss.acs.unt.edu/Rdoc/library/kernlab/html/ksvm.html&quot; rel=&quot;nofollow&quot;&gt;documentation&lt;/a&gt; where it sounds like if you use the formula interface (which I do), then this is handled automatically:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&quot;If the predictor variables include factors, the formula interface&#10;  must be used to get a correct model matrix.&quot;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Does this mean so long as I use the formula interface &quot;dummy coding&quot; is happening for me behind the scenes?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-11-28T18:35:13.523" FavoriteCount="1" Id="44628" LastActivityDate="2014-03-01T04:10:45.813" LastEditDate="2012-11-28T21:03:37.133" LastEditorUserId="930" OwnerUserId="17142" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;categorical-data&gt;&lt;svm&gt;&lt;caret&gt;" Title="Do categorical variables have to be dummy coded in SVM?" ViewCount="1186" />
  
  <row Body="&lt;p&gt;The reason one would use an OP is to study a categorical variables that is ordered, but where the actual values reflect only a ranking. For example, take bond ratings. There's an underlying variable that is unobserved called creditworthiness that some agency has divided into bins, which range from AAA, AA, A, BBB, and so on to D. You can imagine coding these as 12, 11, 10,.... Now AAA is better than AA, and AA is better than A, but the two differences are not equivalent. In your case, AAA is like &quot;strong growth&quot; and D is &quot;no growth&quot;.  &lt;/p&gt;&#10;&#10;&lt;p&gt;This unobserved creditworthiness (or BF growth) is function of the explanatory variables (like sodium) and parameters $\beta$ and a normally* distributed error $\varepsilon$. Each bond rating corresponded to a specific range of the creditworthiness. These ranges are not necessarily the same length. Suppose a firm is now at AA and becomes more creditworthy. Eventually, it would pass over the boundary between AA and AAA and the firm would get a new ranking. The ordered probit would estimate the parameters $\beta$ using MLE, together with the values of the boundary (aka cut values) defining the bins of the creditworthiness.   &lt;/p&gt;&#10;&#10;&lt;p&gt;The interpretation of the parameters is a bit tricky since they are identified up to scale only. It's fairly easy to compare a ratio of two parameters to decide which one is more important. For a more involved exercise, you can also take the differences of adjacent cut values and divide by the sodium slope. This tells you the max change in sodium necessary to move out to the next bin. Alternatively, you can also look at the change in probability from being in a specific bin caused a change in sodium. &lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;*If the error has a logistic distribution, you would have the ordered logit instead of the probit. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-28T20:47:03.513" Id="44634" LastActivityDate="2012-11-28T21:41:31.447" LastEditDate="2012-11-28T21:41:31.447" LastEditorUserId="7071" OwnerUserId="7071" ParentId="44630" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;Consider using a graph based approach.&lt;/p&gt;&#10;&#10;&lt;p&gt;Try to find a threshold to define when users are &quot;somewhat similar&quot;. It can be quite low. Build a graph of these somewhat similar users.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then use a Clique detection approach to find groups in this graph.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-11-28T23:36:46.497" Id="44648" LastActivityDate="2012-11-28T23:36:46.497" OwnerUserId="7828" ParentId="44640" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;This is a problem that comes up so much I am kind of surprised that I haven't seen it explicitly tackled.&lt;/p&gt;&#10;&#10;&lt;p&gt;Well, if you knew the variances in the unpaired and in the paired (which would generally be a good deal smaller), the optimal weights for the two estimates of difference in groups means would be to have weights inversely proportional to the variance of the individual estimates of the difference in means.&lt;/p&gt;&#10;&#10;&lt;p&gt;The need to estimate variance causes some difficulty (the resulting ratio of variance estimates is F, and I think the resulting weights have a beta distribution, and a resulting statistic is kind of complicated), but since you're considering bootstrapping, this may be less of a concern. &lt;/p&gt;&#10;&#10;&lt;p&gt;An alternative possibility which &lt;em&gt;might&lt;/em&gt; be nicer in some sense (or at least a little more robust to non-normality, since we're playing with variance ratios) with very little loss in efficiency at the normal is to base a combined estimate of shift off paired and unpaired rank tests - in each case a kind of Hodges-Lehmann estimate, in the unpaired case based on medians of pairwise cross-sample differences and in the paired case off medians of pairwise-averages-of-pair-differences. Again, the minimum variance weighted linear combination of the two would be with weights proportional to inverses of variances. In that case I'd probably lean toward a permutation (/randomization) rather than a bootstrap - but depending on how you implement your bootstrap they can end up in the same place.&lt;/p&gt;&#10;&#10;&lt;p&gt;In either case you might want to robustify your variances/shrink your variance ratio. Getting in the right ballpark for the weight is good, but you'll lose very little efficiency at the normal by making it slightly robust.&#10;---&lt;/p&gt;&#10;&#10;&lt;p&gt;Some additional thoughts I didn't have clearly enough sorted out in my head before: &lt;/p&gt;&#10;&#10;&lt;p&gt;This problem has distinct similarities to the Behrens-Fisher problem, but is even harder.&lt;/p&gt;&#10;&#10;&lt;p&gt;If we fixed the weights, we &lt;em&gt;could&lt;/em&gt; just whack in a Welch-Satterthwaite type approximation; the structure of the problem is the same.&lt;/p&gt;&#10;&#10;&lt;p&gt;Our issue is that we want to optimize the weights, which effectively means weighting is not fixed - and indeed, tends to maximize the statistic (at least approximately and more nearly in large samples, since any set of weights is a random quantity estimating the same numerator, and we're trying to minimize the denominator; the two aren't independent).&lt;/p&gt;&#10;&#10;&lt;p&gt;This would, I expect, make the chi-square approximation worse, and would almost surely affect the d.f. of an approximation still further.&lt;/p&gt;&#10;&#10;&lt;p&gt;[If this problem is doable, there also just &lt;em&gt;might&lt;/em&gt; turn out be a good rule of thumb that would say 'you can do almost as well if you use only the paired data under these sets of circumstances, only the unpaired under these other sets of conditions and in the rest, this fixed weight-scheme is usually very close to optimal' -- but I won't hold my breath waiting on that chance. Such a decision rule would doubtless have some impact on true significance in each case, but if that effect wasn't so big, such a rule of thumb would give an easy way for people to use existing legacy software, so it could be desirable to try to identify a rule like that for users in such a situation.]&lt;/p&gt;&#10;&#10;&lt;p&gt;---&lt;/p&gt;&#10;&#10;&lt;p&gt;Edit: Note to self - Need to come back and fill in details of work on 'overlapping samples' tests, especially &lt;a href=&quot;http://en.wikipedia.org/wiki/Student%27s_t-test#Overlapping_samples&quot; rel=&quot;nofollow&quot;&gt;overlapping samples t-tests&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;--- &lt;/p&gt;&#10;&#10;&lt;p&gt;It occurs to me that a randomization test should work okay -&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;where the data are paired you randomly permute the group labels within pairs &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;where data are unpaired but assumed to have common distribution (under the null), you permute the group assignments&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;you can now base weights to the two shift estimates off the relative variance estimates ($w_1 = 1/(1+\frac{v_1}{v_2})$), compute each randomized sample's weighted estimate of shift and see where the sample fits into the randomization distribution.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2012-11-29T04:16:20.330" Id="44660" LastActivityDate="2013-08-05T04:35:53.537" LastEditDate="2013-08-05T04:35:53.537" LastEditorUserId="805" OwnerUserId="805" ParentId="25941" PostTypeId="2" Score="5" />
  
  <row Body="&lt;p&gt;I suggest a cluster analysis. Joachim Bacher discusses the different dissimilarity coefficients in depth in his script about cluster analysis, in particular the effects of treating the absence of a treat. For instance : are two items correlated when both show zero ? I remember that he also works with a multiple-response example from survey research which is close to your problem. &lt;/p&gt;&#10;&#10;&lt;p&gt;The script can be downloaded from :&#10;&lt;a href=&quot;http://www.clusteranalyse.net/sonstiges/zaspringseminar2002/&quot; rel=&quot;nofollow&quot;&gt;http://www.clusteranalyse.net/sonstiges/zaspringseminar2002/&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;HTH&#10;ftr&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-11-29T10:29:14.763" Id="44680" LastActivityDate="2012-11-29T10:29:14.763" OwnerUserId="17318" ParentId="44640" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;I have a number of plants and all of them undergo a 'treatment'. Then I measure the amount of plants that react to the treatment and the amount that do not. Is there a way to calculate a test statistic (to test for the significant effect of treatment)? If yes, is there a way to infer an effect size measure?&lt;/p&gt;&#10;&#10;&lt;p&gt;E.g.: R code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;n_total &amp;lt;- 120&#10;n_response &amp;lt;- 100&#10;n_no_response &amp;lt;- 20&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Or would it be appropriate to use McNemar test in this case in the following way and the use effect size measure as described &lt;a href=&quot;http://stats.stackexchange.com/questions/4219/effect-size-of-mcnemars-test&quot;&gt;here&lt;/a&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;tab &amp;lt;- cbind(c(n_total, 0), c(n_response, n_no_response))&#10;mcnemar.test(tab) &#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2012-11-29T10:33:07.663" Id="44682" LastActivityDate="2012-11-29T14:56:03.890" OwnerUserId="10389" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;effect-size&gt;" Title="Significance and effect size in treatment design when treatment-response in measured" ViewCount="53" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a question regarding my dat. It appears that i have extremely large values of the F statistics, ranging from 20 to 20,000. How is this possible? &#10;I should mention that it is panel data, controlled for fixed effects as well as clustering.&#10;thanx in advance, &#10;Luisa&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2012-11-29T15:37:56.157" Id="44709" LastActivityDate="2012-11-29T15:37:56.157" OwnerUserId="17168" PostTypeId="1" Score="1" Tags="&lt;statistical-significance&gt;&lt;panel-data&gt;" Title="Extreme values for f-statistic" ViewCount="456" />
  
  
  <row Body="&lt;p&gt;Answering just the first of your questions: &quot;What tests would you apply to determine if this &lt;em&gt;[sequence]&lt;/em&gt; is truly random?&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;How about treating it as a time-series, and checking for auto-correlations? Here is some R code. First some test data (first 1000 digits):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;digits_string=&quot;1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679821480865132823066470938446095505822317253594081284811174502841027019385211055596446229489549303819644288109756659334461284756482337867831652712019091456485669234603486104543266482133936072602491412737245870066063155881748815209209628292540917153643678925903600113305305488204665213841469519415116094330572703657595919530921861173819326117931051185480744623799627495673518857527248912279381830119491298336733624406566430860213949463952247371907021798609437027705392171762931767523846748184676694051320005681271452635608277857713427577896091736371787214684409012249534301465495853710507922796892589235420199561121290219608640344181598136297747713099605187072113499999983729780499510597317328160963185950244594553469083026425223082533446850352619311881710100031378387528865875332083814206171776691473035982534904287554687311595628638823537875937519577818577805321712268066130019278766111959092164201989&quot;&#10;digits=as.numeric(unlist(strsplit(digits_string,&quot;&quot;)))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Check the counts of each digit:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; table(digits)&#10;digits&#10;  0   1   2   3   4   5   6   7   8   9 &#10; 93 116 103 102  93  97  94  95 101 106 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Then turn it into a time-series, and run the Box-Pierce test:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;d=as.ts( digits )&#10;Box.test(d)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;which tells me:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;X-squared = 1.2449, df = 1, p-value = 0.2645&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Typically you'd want the p-value to be under 0.05 to say there are auto-correlations.&lt;/p&gt;&#10;&#10;&lt;p&gt;Run &lt;code&gt;acf(d)&lt;/code&gt; to see the auto-correlations. I've not included an image here as it is a dull chart, though it is curious that the biggest lags are at 11 and 22. Run &lt;code&gt;acf(d,lag.max=40)&lt;/code&gt; to show that there is no peak at lag=33, and that it was just coincidence!&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;P.S. We could compare how well those 1000 digits of pi did, by doing the same tests on real random numbers. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;probs=sapply(1:100,function(n){&#10;    digits=floor(runif(1000)*10)&#10;    bt=Box.test(ts(digits))&#10;    bt$p.value&#10;    })&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This generates 1000 random digits, does the test, and repeats this 100 times.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; summary(probs)&#10;    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. &#10;0.006725 0.226800 0.469300 0.467100 0.709900 0.969900 &#10;&amp;gt; sd(probs)&#10;[1] 0.2904346&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So our result was comfortably within the first standard deviation, and pi quacks like a random duck. (I used &lt;code&gt;set.seed(1)&lt;/code&gt; if you want to reproduce those exact numbers.)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-29T23:49:51.700" Id="44744" LastActivityDate="2012-11-30T00:05:34.870" LastEditDate="2012-11-30T00:05:34.870" LastEditorUserId="5503" OwnerUserId="5503" ParentId="44445" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;I would suggest included a minimal working example to show your data format &amp;amp; this seems more of a stackoverflow type question, that said make a new column: with the conditions you want&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;data.frame$group1&amp;lt;- BMI &amp;gt; 25 &amp;amp; CMV_risk_group == 1&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This can be converted with as.numeric if you need 0 and 1 instead of True/False.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can then subset as needed.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-11-30T00:03:54.860" Id="44746" LastActivityDate="2012-11-30T00:03:54.860" OwnerUserId="17315" ParentId="44742" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;The US National Institute of Standard has put together a battery of tests that a (pseudo-)random number generator must pass to be considered adequate, see &lt;a href=&quot;http://csrc.nist.gov/groups/ST/toolkit/rng/stats_tests.html&quot;&gt;http://csrc.nist.gov/groups/ST/toolkit/rng/stats_tests.html&lt;/a&gt;. There are also tests known as the &lt;a href=&quot;http://www.stat.fsu.edu/pub/diehard/&quot;&gt;Diehard suite of tests&lt;/a&gt;, which overlap somewhat with NIST tests. Developers of Stata statistical package &lt;a href=&quot;http://www.stata.com/support/cert/diehard/index.html&quot;&gt;report their Diehard results&lt;/a&gt; as a part of their certification process. I imagine you can take blocks of digits of $\pi$, say in groups of consecutive 15 digits, to be comparable to the &lt;a href=&quot;http://en.wikipedia.org/wiki/Floating_point&quot;&gt;double type accuracy&lt;/a&gt;, and run these batteries of tests on thus obtained numbers.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-30T00:35:59.147" Id="44747" LastActivityDate="2012-11-30T00:35:59.147" OwnerUserId="5739" ParentId="44445" PostTypeId="2" Score="8" />
  
  
  
  <row Body="&lt;p&gt;There seem to be two related issues here: 1) Overfitting and 2) Collinearity. As @fg said in a comment, with so many observations overfitting is not likely to be a real problem. However, collinearity may be. &lt;/p&gt;&#10;&#10;&lt;p&gt;High correlations among IVs is often a sign of problematic collinearity - that is, collinearity that can cause the model to be poorly estimated - but it is neither a necessary nor a sufficient condition for that. Since you are estimating a linear model I suggest getting the condition indices and proportion of variance explained matrix (you don't say if you are using R, SAS, SPSS or what, but this is available in all three of those and probably others).  High condition indices (30 or so is a recommended threshold) that have associated high proportion of variance explained on two or more variables may cause problems.  &lt;/p&gt;&#10;&#10;&lt;p&gt;An alternative to this (which also works well for nonlinear models), if you are using R, is the &lt;code&gt;perturb&lt;/code&gt; package.&lt;/p&gt;&#10;&#10;&lt;p&gt;The main problem caused is that small changes in the input can cause large changes in the model (and, thus, possibly  large changes in predictions).&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-11-30T11:35:35.980" Id="44764" LastActivityDate="2012-12-01T23:26:05.413" LastEditDate="2012-12-01T23:26:05.413" LastEditorUserId="686" OwnerUserId="686" ParentId="44763" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;I'm not sure I understand questions 1 and 4, but I think I can help you out with 2 and 3!&lt;/p&gt;&#10;&#10;&lt;p&gt;Regarding your second question, the type of analysis you're talking about is an assessment of the inter-rater reliability. You might also be interested in the reliability of your test as well, but, given the data you presented, I don't think you can really address that question here. If I were exploring test reliability, I would do one of two things:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Compare raters scores on the test of interest versus some gold-standard test that is well-established in the literature. This is the approach researchers will often take when they are interested in establishing the reliability of a new type of assessment.&lt;/li&gt;&#10;&lt;li&gt;Compare raters scores on the test of interest over time. One would take this approach in the case that the question of interest pertains to the temporal reliability of the measure in question (i.e., whether or not raters' answers change over time or repeated assessment). This is the sort of experiment that is often performed in the early stages of establishing the reliability of neuropsychological assessments over repeated administrations, such as the &lt;a href=&quot;http://en.wikipedia.org/wiki/Trail_Making_Test&quot; rel=&quot;nofollow&quot;&gt;Trail Making Test&lt;/a&gt;.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;With regard to your third question, I would use &lt;a href=&quot;http://en.wikipedia.org/wiki/Cohen%27s_kappa&quot; rel=&quot;nofollow&quot;&gt;Cohen's kappa&lt;/a&gt;. The linked wikipedia article is a pretty good reference on this, but, briefly, the kappa is a well-established measure of inter-rater agreement that quantifies the divergence from expected agreement that you observe in your data.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-11-30T15:39:16.620" Id="44777" LastActivityDate="2012-11-30T15:39:16.620" OwnerUserId="8580" ParentId="44775" PostTypeId="2" Score="2" />
  
  
  
  
  
  
  <row Body="An R package for performance evaluation by visualizing the performance of scoring classifiers." CommentCount="0" CreationDate="2012-11-30T23:55:50.660" Id="44824" LastActivityDate="2013-08-25T07:40:15.567" LastEditDate="2013-08-25T07:40:15.567" LastEditorUserId="21599" OwnerUserId="2817" PostTypeId="4" Score="0" />
  
  <row Body="&lt;p&gt;The weights $r_i$ are not a function of $w_i$. So when computing derivatives, you should treat them as a constant.&lt;/p&gt;&#10;&#10;&lt;p&gt;In particular, the partials w.r.t $w_i$ looks like:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;\frac{\partial{L}}{\partial{w_{k}}} = \sum_{i=1}^{n}r_i y_{i}x_{ik}g(-y_{i}z_{i})&#10;$$&#10;$$&#10;\frac{\partial^{2}{L}}{\partial{w_{j}}\partial{w_{k}}} = -\sum_{i=1}^{n}r_i x_{ij}x_{ik}g(y_{i}z_{i})g(-y_{i}z_{i})&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;If it helps you conceptually, you can think about problems with weighted samples as equivalent to unweighted problems, but where some particular observation $(x,y)$ appears $r$ times rather than only once.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-12-01T03:22:57.467" Id="44832" LastActivityDate="2012-12-01T03:22:57.467" OwnerUserId="17214" ParentId="44776" PostTypeId="2" Score="2" />
  
  
  
  
  <row Body="&lt;p&gt;(1) A statistic is a number you can calculate from a sample.  It's used to put into order all the samples you might have got (under an assumed model, where coins don't land on their edges &amp;amp; what have you).  If $t$ is what you calculate from the sample you actually got, &amp;amp; $T$ is the corresponding random variable, then the p-value is given by&#10;$\newcommand{\pr}{\mathrm{Pr}}&#10;\pr\left(T\geq t\right)$ under the null hypothesis, $H_0$.&#10;'Greater than' vs 'more extreme' is unimportant in principle. For a two-sided test on a Normal mean we could use&#10;$\pr(|Z|\geq |z|)$ but it's convenient to use&#10;$2\min [\pr(Z\geq z),\pr(Z\leq z)]$&#10;because we have the appropriate tables. (Note the doubling.)&lt;/p&gt;&#10;&#10;&lt;p&gt;There's no requirement for the test statistic to put the samples in order of their probability under the null hypothesis.  There are situations (like Zag's example) where any other way would seem perverse (without more information about what $r$ measures, what kinds of discrepancies with $H_0$ are of most interest, &amp;amp;c.), but often other criteria are used.  So you could have a bimodal PDF for the test statistic &amp;amp; still test $H_0$ using the formula above.&lt;/p&gt;&#10;&#10;&lt;p&gt;(2) Yes, they mean under $H_0$.&lt;/p&gt;&#10;&#10;&lt;p&gt;(3) A null hypothesis like &quot;The frequency of heads is not 0.5&quot; is no use because you would never be able to reject it.  It's a composite null including &quot;the frequency of heads is 0.49999999&quot;, or as close as you like.  Whether you think beforehand the coin's fair or not, you pick a useful null hypothesis that bears on the problem.  Perhaps more useful after the experiment is to calculate a confidence interval for the frequency of heads that shows you either it's clearly not a fair coin, or it's close enough to fair, or you need to do more trials to find out.&lt;/p&gt;&#10;&#10;&lt;p&gt;An illustration for (1):&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose you're testing the fairness of a coin with 10 tosses. There are $2^{10}$ possible results. Here are three of them:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\mathsf{HHHHHHHHHH}\\&#10;\mathsf{HTHTHTHTHT}\\&#10;\mathsf{HHTHHHTTTH}$&lt;/p&gt;&#10;&#10;&lt;p&gt;You'll probably agree with me that the first two look a bit suspicious.  Yet the probabilities under the null are equal:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\mathrm{Pr}(\mathsf{HHHHHHHHHH}) = \frac{1}{1024}\\ &#10;\mathrm{Pr}(\mathsf{HTHTHTHTHT}) = \frac{1}{1024}\\ &#10;\mathrm{Pr}(\mathsf{HHTHHHTTTH}) = \frac{1}{1024}$&lt;/p&gt;&#10;&#10;&lt;p&gt;To get anywhere you need to consider what types of alternative to the null you want to test.  If you're prepared to assume independence of each toss under both null &amp;amp; alternative (&amp;amp; in real situations this often means working very hard to ensure experimental trials are independent), you can use the total count of heads as a test statistic without losing information. (Partitioning the sample space in this way is another important job that statistics do.)&lt;/p&gt;&#10;&#10;&lt;p&gt;So you have a count between 0 and 10&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;t&amp;lt;-c(0:10)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Its distribution under the null is&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;p.null&amp;lt;-dbinom(t,10,0.5)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Under the version of the alternative that best fits the data, if you see (say) 3 out of 10 heads the probability of heads is $\frac{3}{10}$, so&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;p.alt&amp;lt;-dbinom(t,10,t/10)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Take the ratio of the probability under the null to the probability under the alternative (called the likelihood ratio):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lr&amp;lt;-p.alt/p.null&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Compare with&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;plot(log(lr),p.null)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So for this null, the two statistics order samples the same way. If you repeat with a null of 0.85 (i.e. testing that the long-run frequency of heads is 85%), they don't.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;p.null&amp;lt;-dbinom(t,10,0.85)&#10;plot(log(lr),p.null)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/J5QHP.png&quot; alt=&quot;lrt gof test&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;To see why&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;plot(t,p.alt)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Some values of $t$ are less probable under the alternative, &amp;amp; the likelihood ratio test statistic takes this into account.  NB this test statistic will not be extreme for&lt;/p&gt;&#10;&#10;&lt;p&gt;$\mathsf{HTHTHTHTHT}$&lt;/p&gt;&#10;&#10;&lt;p&gt;And that's fine - every sample can be considered extreme from some point of view.  You choose the test statistic according to what kind of discrepancy to the null you want to be able to detect.&lt;/p&gt;&#10;&#10;&lt;p&gt;... Continuing this train of thought, you can define a statistic that partitions the sample space differently to test the same null against the alternative that one coin toss influences the next one.  Call the number of runs $r$, so that&lt;/p&gt;&#10;&#10;&lt;p&gt;$\mathsf{HHTHHHTTTH}$&lt;/p&gt;&#10;&#10;&lt;p&gt;has $r=6$:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\mathsf{HH}\ \mathsf{T}\ \mathsf{HHH}\ \mathsf{TTT}\ \mathsf{H}$&lt;/p&gt;&#10;&#10;&lt;p&gt;The suspicious sequence&lt;/p&gt;&#10;&#10;&lt;p&gt;$\mathsf{HTHTHTHTHT}$&lt;/p&gt;&#10;&#10;&lt;p&gt;has $r=10$. So does&lt;/p&gt;&#10;&#10;&lt;p&gt;$\mathsf{THTHTHTHTH}$&lt;/p&gt;&#10;&#10;&lt;p&gt;while at the other extreme&lt;/p&gt;&#10;&#10;&lt;p&gt;$\mathsf{HHHHHHHHHH}\\&#10;\mathsf{TTTTTTTTTT}$&lt;/p&gt;&#10;&#10;&lt;p&gt;have $r=1$.  Using probability under the null as the test statistic (the way you like) you can say that the p-value of the sample&lt;/p&gt;&#10;&#10;&lt;p&gt;$\mathsf{HTHTHTHTHT}$&lt;/p&gt;&#10;&#10;&lt;p&gt;is therefore $\frac{4}{1024}=\frac{1}{256}$. What's worthy of note, comparing this test to the previous, is that even if you stick strictly to the ordering given by probability under the null, the way in which you define your test statistic to partition the sample space is dependent on consideration of alternatives.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-12-01T19:08:53.933" Id="44859" LastActivityDate="2014-04-23T13:01:17.677" LastEditDate="2014-04-23T13:01:17.677" LastEditorUserId="17230" OwnerUserId="17230" ParentId="44769" PostTypeId="2" Score="6" />
  <row AnswerCount="1" Body="&lt;p&gt;Say a basketball team wins a game. Obviously, at some point in the future, the basketball team will lose a game. So hence, after each game that the basketball team wins, the team is edging closer to their next lost. Are there any statistical tools that can help predict or explain this kind of situations?&lt;/p&gt;&#10;&#10;&lt;p&gt;Another example would be the stock market, which can either close higher or close lower after each day. If a stock market closes higher today, eventually one day it will close lower. That day would be approaching nearer. Are there any statistical tools or tests that can help model or predict these kind of events? For example, the stock market closes up for the past 4 days in a row, so what is the probability that it will close up tomorrow?&lt;/p&gt;&#10;&#10;&lt;p&gt;To get a better idea of what Im trying to explain. Lets say for example the past 5 game results (game 5 being the most recent) of a basketball team are:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Game 1: Win&lt;/p&gt;&#10;  &#10;  &lt;p&gt;Game 2: Win&lt;/p&gt;&#10;  &#10;  &lt;p&gt;Game 3: Win&lt;/p&gt;&#10;  &#10;  &lt;p&gt;Game 4: Win&lt;/p&gt;&#10;  &#10;  &lt;p&gt;Game 5: Lose&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;So from Game 1 to Game 4, after each win, the team's next lost is approaching closer. Are there any statistical tools for these kind of situations?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-12-01T19:20:03.607" Id="44862" LastActivityDate="2012-12-01T19:54:55.257" OwnerUserId="16119" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;descriptive-statistics&gt;" Title="Are there any formal definitions or statistical theories about this kind of events?" ViewCount="60" />
  
  
  
  
  <row AnswerCount="2" Body="&lt;pre&gt;&lt;code&gt;m &amp;lt;- matrix(nrow=5, ncol=5)  &#10;m &amp;lt;- ifelse(row(m)==col(m), 1, 0.4)  &#10;ch &amp;lt;- chol(m)    # Choleski decomposition  &#10;u &amp;lt;- matrix(rnorm(2000*5), ncol=5)  &#10;uc &amp;lt;- u %*% ch  &#10;cr &amp;lt;- pnorm(uc)  &#10;cr &amp;lt;- qbinom(cr,1,0.5)  &#10;cor(uc)  &#10;cor(cr)   &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The cor(uc) is 0.4. However, I also expected the cor(cr) to be 0.4, but it is around 0.2.&lt;br&gt;&#10;Why the correlation is shrunken?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-12-01T22:28:49.400" FavoriteCount="0" Id="44878" LastActivityDate="2012-12-03T20:32:59.937" LastEditDate="2012-12-01T22:53:46.293" LastEditorUserId="686" OwnerUserId="17266" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;probability&gt;" Title="How to generate binary data using Cholesky decomposition in R?" ViewCount="266" />
  <row Body="&lt;p&gt;I'm guessing the instruction means to define &lt;em&gt;Y&lt;/em&gt; in terms of &lt;em&gt;X&lt;/em&gt;, and then vice versa, using the regression constant and coefficient you've calculated.  In other words, you need to say that (filling in numerical values for &lt;em&gt;a&lt;/em&gt; and &lt;em&gt;b&lt;/em&gt;) &lt;em&gt;Y&lt;/em&gt; = &lt;em&gt;a&lt;/em&gt; + &lt;em&gt;bX&lt;/em&gt; + &lt;em&gt;e&lt;/em&gt;. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-01T22:31:11.670" Id="44879" LastActivityDate="2012-12-01T22:31:11.670" OwnerUserId="2669" ParentId="44848" PostTypeId="2" Score="0" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;Please help.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have 3 IVS and 1 DV in my model. The 3 IVs are rated by 150 Middle Managers while 50 Directors rate the DV. I need to conduct multiple regression. However for cases (rows in SPSS), where Middle Managers rate the 3 IVs, I have missing values for Directors' rating of the DV. How should I conduct regression? Do I need to restructure the data? Thanks. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-12-02T09:54:17.083" Id="44900" LastActivityDate="2012-12-02T17:30:56.797" OwnerUserId="17354" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;spss&gt;" Title="Regression with missing values" ViewCount="143" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Given that $Y$ follows multivariate normal distribution ,i.e, $N_n (0, \sigma^2 I_n)$, we want to find the distribution of $Y'Y$ given that $a'Y=0$ where $a$ is a non zero constant vector.&lt;/p&gt;&#10;&#10;&lt;p&gt;I know that the distribution of $Y'Y$ would be $\sigma^2 \chi_n^2$ if the condition is not given. How to approach for the given condition?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-12-02T16:24:02.590" Id="44924" LastActivityDate="2012-12-02T20:44:20.113" LastEditDate="2012-12-02T20:44:20.113" LastEditorUserId="930" OwnerUserId="17413" PostTypeId="1" Score="2" Tags="&lt;probability&gt;&lt;self-study&gt;&lt;mathematical-statistics&gt;" Title="Conditional distribution of quadratic forms" ViewCount="83" />
  
  <row AnswerCount="3" Body="&lt;p&gt;I am having some trouble understanding something that feels very basic in probability theory concerning the past history of repeated independent events.&lt;/p&gt;&#10;&#10;&lt;p&gt;Consider the tossing of a fair coin. $P(\text{head}) = P(\text{tail}) = 0.5$. Repeated tosses are independent events. Let's say that we already have four Heads in a row.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's my dilemma:  &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The probability of the next toss result being head = $0.5$, if I think of this as an independent event. But the probability of getting five heads in a row = $0.5^5$ given the history of the tossing.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Aren't these statements conflicting? Does the probability of the next toss being a head really differ? So, if were to bet on the outcome, would it be any better to bet on tails?&lt;/p&gt;&#10;&#10;&lt;p&gt;I can think of rationales for both cases but I can't wrap my mind around how both probabilities for a head can co-exist. An explanation would be deeply appreciated.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-12-02T18:00:10.560" Id="44929" LastActivityDate="2012-12-02T21:18:34.100" LastEditDate="2012-12-02T19:43:01.670" LastEditorUserId="930" OwnerUserId="17415" PostTypeId="1" Score="2" Tags="&lt;probability&gt;&lt;independence&gt;" Title="Probability of independent events given the history" ViewCount="187" />
  
  <row AnswerCount="1" Body="&lt;p&gt;1200 pregnant patients were asked by the doctor/nurse if they smoke or not. Once the patient admits to smoking she is referred to quit smoking clinic. In the quit smoking clinic they are offered nicotine replacement therapy. &lt;/p&gt;&#10;&#10;&lt;p&gt;The questions are:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;In the women who use nicotine replacement therapy do their babies do better than ones who refuse nicotine replacement therapy? &lt;/li&gt;&#10;&lt;li&gt;How do we test statistical significance?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="1" CreationDate="2012-12-02T18:21:01.070" Id="44933" LastActivityDate="2013-01-02T06:30:51.043" LastEditDate="2012-12-02T20:40:37.003" LastEditorUserId="930" OwnerUserId="17372" PostTypeId="1" Score="0" Tags="&lt;repeated-measures&gt;" Title="Subgroup analyses" ViewCount="49" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm trying to determine the influence (direction and relative strength) of certain attributes of incoming students to an academic program on their successful completion of the program. My sample size is in the range of several hundred students. There are about 20 variables all together, all of which we could reasonably expect to have some influence on successful completion of the program. (I'm not throwing in everything I could conceive of). My variables are as follows: &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Variables which are necessary to control for as determined by previous investigations of a similar nature. These are things like race, gender, age, incoming GPA, number of credits, etc. &lt;/li&gt;&#10;&lt;li&gt;Variables which are more specific to the particular program which haven't been investigated yet, such as pre-admission tests of subject matter specific to the program or grades in courses which are pre-requisites for the program. &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I've examined the data from a bunch of angles by now, but here are the three approaches which, when compared, are causing me grief: &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Performing a backward step-wise feature selection on a logistic regression model which at the beginning includes all the features. &lt;/li&gt;&#10;&lt;li&gt;Fitting a LASSO model to the data, cross validating to find the optimal lambda. &lt;/li&gt;&#10;&lt;li&gt;Including all the variables in a logistic regression model. No feature selection. &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;All the numeric variables going in to the analyses have been scaled, so that to answer my original question, I would just have to look at the absolute values and signs of the coefficients. The problem is that I don't know which model to go by. &lt;/p&gt;&#10;&#10;&lt;p&gt;Including all the variables leaves me with 16/20 variables having p-values above .1. The four variables which do have small p-values are those which were left in the step-wise routine. The LASSO model, when lambda=lambda.min, leaves me with approximately half the variables having non-zero coefficients. &lt;/p&gt;&#10;&#10;&lt;p&gt;My not particularly well informed feeling is that, if I were just trying to get an idea of the influence of these variables for my own use, I would use the relative sizes and directions of the lasso coefficients, but attribute more certainty to those variables which came up as significant in the full logistic regression model, ignoring the step-wise model altogether. &lt;/p&gt;&#10;&#10;&lt;p&gt;Is this intuition justified? &lt;/p&gt;&#10;&#10;&lt;p&gt;There's also the issue of presenting these findings to others. Standard regression techniques with p-values are pretty well known to anyone who has taken a course covering regression, while LASSO is not. Are LASSO coefficients ever presented analysis models such as the one I've described? &lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT based on comment: &lt;/p&gt;&#10;&#10;&lt;p&gt;If my main concern is coming up with an ordered list of variables in terms of their effect size, along with the direction, then which of the following options should I expect to get me better answers: 1) using the coefficients of the full model or 2) using the coefficients from the Lasso model, or perhaps the relaxed Lasso, and accepting that I'm not going to have any p-values or confidence intervals, etc.? &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-12-02T20:05:00.713" Id="44937" LastActivityDate="2012-12-03T02:23:58.887" LastEditDate="2012-12-03T02:23:58.887" LastEditorUserId="12275" OwnerUserId="12275" PostTypeId="1" Score="4" Tags="&lt;regression&gt;&lt;logistic&gt;&lt;multivariate-analysis&gt;&lt;model-selection&gt;&lt;lasso&gt;" Title="Logistic regression model for analysis of many IVs with a relatively small sample size" ViewCount="197" />
  <row Body="&lt;p&gt;The original suggestion for displaying an interaction via box-plot does not quite make sense in this instance, since both of your variables that define the interaction are continuous. You could dichotomize either &lt;code&gt;G&lt;/code&gt; or &lt;code&gt;P&lt;/code&gt;, but you do not have much data to work with. Because of this, I would suggest coplots (a description of what they are can be found in;&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Cleveland, William. 1994. &lt;a href=&quot;http://dx.doi.org/10.1214/lnms/1215463783&quot;&gt;Coplots, nonparametric regression, and conditionally parametric fits&lt;/a&gt;. IMS Lecture Notes Monograph Series 24: 21-36. PDF available in link from Project Euclid.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Below is a coplot of the &lt;code&gt;election2012&lt;/code&gt; data generated by the code &lt;code&gt;coplot(VP ~ P | G, data = election2012)&lt;/code&gt;. So this is assessing the effect of &lt;code&gt;P&lt;/code&gt; on &lt;code&gt;VP&lt;/code&gt; conditional on varying values of &lt;code&gt;G&lt;/code&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Ye7LY.png&quot; alt=&quot;coplot interaction&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Although your description makes it sound like this is a fishing expedition, we may entertain the possibility that an interaction between these two variables exist. The coplot seems to show that for lower values of &lt;code&gt;G&lt;/code&gt; the effect of &lt;code&gt;P&lt;/code&gt; is positive, and for higher values of &lt;code&gt;G&lt;/code&gt; the effect of &lt;code&gt;P&lt;/code&gt; is negative. After assessing marginal histograms and bivariate scatterplots of &lt;code&gt;VP, P, G&lt;/code&gt; and the interaction between &lt;code&gt;P&lt;/code&gt; and &lt;code&gt;G&lt;/code&gt;, it seemed to me that 1932 was likely a high leverage value for the interaction effect. &lt;/p&gt;&#10;&#10;&lt;p&gt;Below are four scatterplots, showing the marginal relationships between &lt;code&gt;VP&lt;/code&gt; and the mean centered &lt;code&gt;V&lt;/code&gt;, &lt;code&gt;G&lt;/code&gt; and the interaction of &lt;code&gt;V&lt;/code&gt; and &lt;code&gt;G&lt;/code&gt; (what I named &lt;code&gt;int_gpcent&lt;/code&gt;). I have highlighted 1932 as a red dot. The last plot on the lower right is the residuals of the linear model &lt;code&gt;lm(VP ~ g_cent + p_cent, data = election2012)&lt;/code&gt; against &lt;code&gt;int_gpcent&lt;/code&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/owQMV.png&quot; alt=&quot;high leverage regression&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Below I provide code that shows when removing 1932 from the linear model &lt;code&gt;lm(VP ~ g_cent + p_cent + int_gpcent, data = election2012)&lt;/code&gt; the interaction of &lt;code&gt;G&lt;/code&gt; and &lt;code&gt;P&lt;/code&gt; fail to reach statistical significance. Of course this is all just exploratory (one would also want to assess if any temporal correlation occurs in the series, but hopefully this is a good start. Save ggplot for when you have a better idea of what you exactly want to plot!&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#data and directory stuff&#10;mydir &amp;lt;- &quot;C:\\Documents and Settings\\andrew.wheeler\\Desktop\\R_interaction&quot;&#10;setwd(mydir)&#10;election2012 &amp;lt;- read.table(&quot;election2012.txt&quot;, header=T, quote=&quot;\&quot;&quot;)&#10;&#10;#making interaction variable&#10;election2012$g_cent &amp;lt;- election2012$G - mean(election2012$G)&#10;    election2012$p_cent &amp;lt;- election2012$P - mean(election2012$P)&#10;election2012$int_gpcent &amp;lt;- election2012$g_cent * election2012$p_cent&#10;&#10;summary(election2012)&#10;View(election2012)&#10;par(mfrow= c(2,2))&#10;hist(election2012$VP)&#10;    hist(election2012$G)&#10;hist(election2012$P)&#10;    hist(election2012$int_gpcent)&#10;&#10;#scatterplot &amp;amp; correlation matrix&#10;cor(election2012[c(&quot;VP&quot;,&quot;g_cent&quot;,&quot;p_cent&quot;,&quot;int_gpcent&quot;)])&#10;pairs(election2012[c(&quot;VP&quot;,&quot;g_cent&quot;,&quot;p_cent&quot;,&quot;int_gpcent&quot;)])&#10;&#10;#lets just check out a coplot for interactions&#10;#coplot(VP ~ G | P, data = election2012)&#10;coplot(VP ~ P | G, data = election2012)&#10;#example of coplot - http://stackoverflow.com/questions/5857726/how-to-delete-the-given-in-a-coplot-using-r&#10;&#10;#onto models&#10;&#10;model1 &amp;lt;- lm(VP ~ g_cent + p_cent, data = election2012)&#10;summary(model1)&#10;election2012$resid_m1 &amp;lt;- residuals(model1)&#10;&#10;election2012$color &amp;lt;- &quot;black&quot;&#10;    election2012$color[14] &amp;lt;- &quot;red&quot;&#10;&#10;attach(election2012)&#10;par(mfrow = c(2,2))&#10;plot(x = g_cent,y = VP, col = color, pch = 16)&#10;plot(x = p_cent,y = VP, col = color, pch = 16)&#10;plot(x = int_gpcent,y = VP, col = color, pch = 16)&#10;plot(x = int_gpcent,y = resid_m1, col = color, pch = 16)&#10;&#10;#what does the same model look like with 1932 removed&#10;&#10;model1_int &amp;lt;- lm(VP ~ g_cent + p_cent + int_gpcent, data = election2012)&#10;summary(model1_int)&#10;model2_int &amp;lt;- lm(VP ~ g_cent + p_cent + int_gpcent, data = election2012[-14,])&#10;summary(model2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2012-12-02T22:23:53.243" Id="44943" LastActivityDate="2012-12-02T22:23:53.243" OwnerUserId="1036" ParentId="44078" PostTypeId="2" Score="7" />
  
  <row Body="&lt;p&gt;&quot;Dynamic factor analysis&quot; might be your answer and is certainly worth reading up on.  It aims to identify a small number of underlying (unobserved) factors behind the co-movement of a large number of observed time series.  To implement as part of a broader modelling exercise you will probably be looking at R, Matlab, Stata or a specialist time series package like RATS.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are lots of potential pitfalls, however, including the possibility that in reducing the explanatory variables in your data to the underlying common &quot;structure&quot;, you may be throwing away precisely that part of it which is related to your response variable of interest.  However, the technique seems widely used, particularly in areas like econometrics that can try to combine it with a theoretical sense of what the structure underneath all those time series may be.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-12-03T03:49:50.120" Id="44956" LastActivityDate="2012-12-03T03:49:50.120" OwnerUserId="7972" ParentId="44954" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;How can I deal with categorial features inside linear ML algorithms? The obvious solution is to represent each value as a binary feature. For example, if the categorical feature color has three possible values: red, blue and green, then we replace it with three binary features [color == red], [color == green], [color == blue]. Are there better solutions?&lt;/p&gt;&#10;&#10;&lt;p&gt;Specifically, I am using Vowpal Wabbit's logistic regression. As far as I understood from the tutorial VW can operate with categorial features, but I did not find out how does in work.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-03T08:14:37.330" FavoriteCount="1" Id="44971" LastActivityDate="2012-12-03T13:45:24.347" LastEditDate="2012-12-03T11:47:01.203" LastEditorUserId="686" OwnerUserId="17427" PostTypeId="1" Score="6" Tags="&lt;machine-learning&gt;&lt;categorical-data&gt;&lt;linear-model&gt;" Title="Categorial features in linear machine learning algorithms" ViewCount="582" />
  <row AnswerCount="0" Body="&lt;p&gt;I am currently developing a software application that runs and generates results several statistical tests using .Net libraries. I have been comparing my results to R and another (non free) statistical package. For ANOVA test I can compare against NIST data sets, however I was wondering if there were any similar data sets accompanied with &quot;industrial standard&quot; certified p-values I could use for T-Tests and other statistical tests?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-03T10:51:57.917" Id="44979" LastActivityDate="2012-12-04T00:10:26.207" OwnerUserId="17430" PostTypeId="1" Score="1" Tags="&lt;dataset&gt;&lt;t-test&gt;&lt;p-value&gt;" Title="Reference/certified data set to compare custom software results" ViewCount="108" />
  <row Body="&lt;p&gt;I think that the following reference would be worth your immediate attention&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.jerrydallal.com/LHSP/LHSP.htm&quot; rel=&quot;nofollow&quot;&gt;http://www.jerrydallal.com/LHSP/LHSP.htm&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;while this would be useful once you have more experience&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www-users.york.ac.uk/~mb55/pubs/pbstnote.htm&quot; rel=&quot;nofollow&quot;&gt;http://www-users.york.ac.uk/~mb55/pubs/pbstnote.htm&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The SPSS manuals are worth reading, particularly the brief guide, the core system's user guide, statistics base and ProgDataMgmt book.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-12-03T12:03:12.140" Id="44983" LastActivityDate="2012-12-03T12:03:12.140" OwnerUserId="19815" ParentId="44981" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I think your question is sort of inside out.  The right path is not to get data and then ask &quot;what can I do with it?&quot; it is to ask &quot;What do I want to do?&quot; then get data then do the analysis.  The &quot;What do I want to do?&quot; should be in non-statistical language. &lt;/p&gt;&#10;&#10;&lt;p&gt;Even if you have your data before knowing what you want to do (an all too frequent occurrence) you should not be asking &quot;What statistical methods can I do with this data?&quot; but rather &quot;What interesting questions can this data help answer?&quot; Then you use the answer to these questions to choose an analytic method. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-12-03T12:14:08.673" Id="44988" LastActivityDate="2012-12-03T12:14:08.673" OwnerUserId="686" ParentId="44981" PostTypeId="2" Score="5" />
  <row AcceptedAnswerId="89264" AnswerCount="1" Body="&lt;p&gt;I have a heavy tailed logNormal distribution and i want to know if it's possible to make a linear regression in order to compute R² and slope for this data.&lt;/p&gt;&#10;&#10;&lt;p&gt;I make my linear regression on computed values log10(rank) and log10(population) (cf it's a rank size plot)&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/TQ01x.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I don't know if it's a good method, especially in the case of heavy tailed distribution. &lt;/p&gt;&#10;&#10;&lt;p&gt;Do you know better/robust method to compute slope of lognormal distribution?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-12-03T12:51:21.993" Id="44990" LastActivityDate="2014-03-09T01:46:55.527" OwnerUserId="4693" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;distributions&gt;&lt;hypothesis-testing&gt;&lt;robust&gt;&lt;lognormal&gt;" Title="computing slope on heavy tailed lognormal distribution" ViewCount="263" />
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;Two running teams take part in competitions on the same course, but their competitions differ because of the number of teams entered in each competition.&lt;/p&gt;&#10;&#10;&lt;p&gt;Results are based solely on position - times are not taken.&lt;/p&gt;&#10;&#10;&lt;p&gt;Competition 1 has 7 teams of 8 runners&#10;Each team enters 2 of its runners in each of 4 races&#10;Points are awarded on the basis of 1 for first place down to 14 for fourteenth place&#10;The points for all 4 races are added together.&#10;The winning team had a total of 40 points.&lt;/p&gt;&#10;&#10;&lt;p&gt;Competition 2 has 5 teams of 8 runners&#10;Each team enters 3 of its runners in each of two races (15 competitors in each) and a further 2 runners in a third race (10 competitors in this one).&#10;Points are awarded in each race on a similar basis to competition 1 (2 races award from 1 to 15 and the other, 1 to 10).&#10;The winning team has a total of 46 points&lt;/p&gt;&#10;&#10;&lt;p&gt;Statistically, which is the better team?&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm not a statistician, but my logic lead me initially to compare each team's performance with what I would (probably incorrectly) call the mean number of points in each competition (average of the most and least number of points the team could score). However this would seem disadvantage the team in competition 2, because their best possible points score is 15 versus 12 for team 1. And the &quot;mean&quot; numbers are in any case similar. I did think about comparing each team's performance against the best case and worst case scenarios in their respective competitions and averaging those scores? The other thought is that I should somehow compare each runner's performance against the best &amp;amp; worst case? &lt;/p&gt;&#10;&#10;&lt;p&gt;In reality, I'm a bit stuck. Help please!&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-12-03T19:05:08.153" Id="45027" LastActivityDate="2013-07-02T09:10:35.273" OwnerUserId="17444" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;estimation&gt;" Title="Compare two teams who competed in slightly different competitions" ViewCount="151" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a fold enrichment table for different days as given below. I need to find the significance of every fold enrichment.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;fold_enrichment &amp;lt;- matrix(c(0.3,0.43,0.5,0, 0.23,1.3,0.5,1, 1,0,0,6, 0.3,0.5,0.2,2), 4, 4,&#10;              dimnames = list(Day = c(&quot;Day1&quot;, &quot;Day7&quot;, &quot;Day14&quot;, &quot;Day28&quot;),&#10;                              Group = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;)))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In R, I tried using &lt;code&gt;fisher.test(fold_enrichment)&lt;/code&gt; and it gives me a single p-value&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Warning in fisher.test(fold_enrichment) :   'x' has been rounded to&#10;  integer: Mean relative difference: 0.7652582&lt;/p&gt;&#10;  &#10;  &lt;p&gt;Fisher's Exact Test for Count Data&lt;/p&gt;&#10;  &#10;  &lt;p&gt;data:  fold_enrichment  p-value = 0.6182 alternative hypothesis:&#10;  two.sided&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I have also seen another option &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;pairwise.fisher.test(Day,Group, p.adjust.method=&quot;bonferroni&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Can anyone suggest me the difference between fisher's test vs pairwise fisher's test and what would be the right solution for my problem?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-12-03T19:25:49.533" Id="45029" LastActivityDate="2012-12-03T19:45:52.207" LastEditDate="2012-12-03T19:45:52.207" LastEditorUserId="2725" OwnerUserId="2725" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;statistical-significance&gt;&lt;t-test&gt;" Title="fisher's test vs pairwise fisher's test" ViewCount="369" />
  
  <row Body="&lt;p&gt;This will do it:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ses(d[1:40], h=30, alpha=0.1, initial=&quot;simple&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2012-12-03T21:37:48.483" Id="45040" LastActivityDate="2012-12-03T21:37:48.483" OwnerUserId="159" ParentId="44984" PostTypeId="2" Score="3" />
  
  
  <row AcceptedAnswerId="45099" AnswerCount="2" Body="&lt;p&gt;For linear regression, we can check the diagnostic plots (residuals plots, Normal QQ plots, etc) to check if the assumptions of linear regression are violated.&lt;/p&gt;&#10;&#10;&lt;p&gt;For logistic regression, I am having trouble finding resources that explain how to diagnose the logistic regression model fit. Digging up some course notes for GLM, it simply states that checking the residuals is not helpful for performing diagnosis for a logistic regression fit.&lt;/p&gt;&#10;&#10;&lt;p&gt;Looking around the internet, there also seems to be various &quot;diagnosis&quot; procedures, such as checking the model deviance and performing chi-squared tests, but other sources state that this is inappropriate, and that you should perform a Hosmer-Lemeshow goodness of fit test. Then I find other sources that state that this test may be highly dependent on the actual groupings and cut-off values (may not be reliable).&lt;/p&gt;&#10;&#10;&lt;p&gt;So how should one diagnose the logistic regression fit?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-12-03T23:15:51.790" FavoriteCount="18" Id="45050" LastActivityDate="2013-09-20T11:36:16.487" LastEditDate="2013-09-20T08:44:07.163" LastEditorUserId="6029" OwnerUserId="2252" PostTypeId="1" Score="29" Tags="&lt;regression&gt;&lt;logistic&gt;" Title="Diagnostics for logistic regression?" ViewCount="4912" />
  
  
  <row AcceptedAnswerId="45067" AnswerCount="1" Body="&lt;p&gt;From &lt;a href=&quot;http://en.wikipedia.org/wiki/Factorial_experiment&quot; rel=&quot;nofollow&quot;&gt;Wikipedia&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;a full factorial experiment is an experiment whose design consists of two or more factors, each with discrete possible values or &quot;levels&quot;, and whose experimental units take on all possible combinations of these levels across all such factors. A full factorial design may also be called a fully crossed design. Such an experiment allows studying the effect of each &lt;strong&gt;factor&lt;/strong&gt; on &lt;strong&gt;the response variable&lt;/strong&gt;, as well as the effects of interactions between factors on the response variable.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;A &quot;response variable&quot; is the output in a prediction function (i.e. regression or classification), and &quot;factors&quot; are the components of the input of the prediction function.&lt;/p&gt;&#10;&#10;&lt;p&gt;So is it correct that factorial experiment is only used for collecting pairs of input and output of a prediction function in regression or classification.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-04T02:16:30.437" Id="45061" LastActivityDate="2012-12-04T03:05:27.363" LastEditDate="2012-12-04T02:30:00.317" LastEditorUserId="1005" OwnerUserId="1005" PostTypeId="1" Score="0" Tags="&lt;experiment-design&gt;" Title="Is factorial experiment used only for prediction (regression or classification)?" ViewCount="135" />
  <row AcceptedAnswerId="45115" AnswerCount="2" Body="&lt;p&gt;I have a list of the scores from various locations on different exams listed below. &#10;How can I compare the different test and scores to show which location is the best over all? &lt;/p&gt;&#10;&#10;&lt;p&gt;It would really help if someone could inform which type of test to do and why.  I first thought to use the Wilcoxon signed-rank test, but the data is not in pairs. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;             English  AP CalcBC    AP PhyscsB   AP Chem     AP Econ &#10;City      Rank  Score Rank Score  Rank  Score Rank  Score  Rank Score&#10;&#10;(Beijing)   6   36.33   4   39.14   1   35.54   1   47.94   3   46.55&#10;(Hefei)     5   38.95   11  26.31   9   18.61   8   27.83   8   30.52&#10;(Hexi)     na      na   8   33.04   na     na   6   37.7    7   32.46&#10;(Huzhou)    4   39.06   1   45.47   na     na   na     na   na     na&#10;(Jiangyin)  9   29.9    9   30      5   24.4    7   31.75   6   36.7&#10;(Jinan)     1   47.72   6   37.2    3   28.91   3   41.55   2   47.94&#10;(Nantong)   3   39.67   5   38.45   2   32.59   5   38.9    4   45.05&#10;(Shanghai)  na     na   3   39.44   6   23.18   2   44.4    5   43.86&#10;(Suzhou)    8   30.19   10  27.5    7   21.75   9   26      9   19.87&#10;(Yangzhou)  2   43.59   2   40.45   4   26.7    4   41.45   1   50.73&#10;(Yixing)    7   33.26   7   35.91   8   21.48   10  24.7    na     na&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="6" CreationDate="2012-12-04T04:09:47.210" Id="45073" LastActivityDate="2012-12-04T16:51:17.947" LastEditDate="2012-12-04T16:51:17.947" LastEditorUserId="7290" OwnerUserId="17112" PostTypeId="1" Score="1" Tags="&lt;sampling&gt;&lt;multivariate-analysis&gt;" Title="How can I tell which location is the best?" ViewCount="66" />
  
  <row AcceptedAnswerId="45092" AnswerCount="2" Body="&lt;p&gt;Why doesn't backpropagation work when you initialize all the weight the same value (say 0.5), but works fine when given random numbers? &lt;/p&gt;&#10;&#10;&lt;p&gt;Shouldn't the algorithm calculate the error and work from there, despite the fact that the weights are initially the same?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-04T12:25:02.853" FavoriteCount="1" Id="45087" LastActivityDate="2013-01-13T10:33:05.460" LastEditDate="2013-01-13T10:31:19.720" LastEditorUserId="88" OwnerUserId="17349" PostTypeId="1" Score="6" Tags="&lt;machine-learning&gt;&lt;neural-networks&gt;" Title="Why doesn't backpropagation work when you initialize the weights the same value?" ViewCount="517" />
  
  <row Body="&lt;p&gt;For your first question: What is a logistic regression classifier, I don't really know what kind of answer do you want.&lt;/p&gt;&#10;&#10;&lt;p&gt;But if you have some data that can be classified by category, you can use Logistic regression.&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;J(\Theta ) = -[\frac{1}{m}\sum_{i=1}^{m}y^{(i)} log( h_{\Theta }(x^{(i)})) + (1-y^{(i)}) log (1-h_{\Theta }(x^{(i)}))]&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;with $h_{\Theta }(x) = \frac{1}{1+exp(-\Theta ^{T}x))}$&#10;and with your data being {($x^{(1)}$, $y^{(1)}$), ($x^{(2)}$, $y^{(2)}$), ..., ($x^{(n)}$, $y^{(n)}$)}&lt;/p&gt;&#10;&#10;&lt;p&gt;You need to minimize $J(\Theta )$ using gradient descent to find good enough parameters ${\Theta }$&lt;/p&gt;&#10;&#10;&lt;p&gt;If you want to penalize large weight you can use the same formula and use regularization.&lt;/p&gt;&#10;&#10;&lt;p&gt;Try to find the course from Andrew Bg on Machine Learning and more specifically the Logistic Regression part.&lt;/p&gt;&#10;&#10;&lt;p&gt;(I wrote the all thing quickly, I hope I didn't forget anything)&#10;$$&#10;J(\Theta ) = -[\frac{1}{m}\sum_{i=1}^{m}y^{(i)} log( h_{\Theta }(x^{(i)})) + (1-y^{(i)}) log (1-h_{\Theta }(x^{(i)}))] + \frac{\lambda}{2m} \sum_{j=1}^{n}\Theta_{j}^{2}&#10;$$&#10;${\lambda}$ being how much penalty you want to apply to your model&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-12-04T13:56:28.740" Id="45095" LastActivityDate="2012-12-04T13:56:28.740" OwnerUserId="17463" ParentId="44645" PostTypeId="2" Score="1" />
  
  <row Body="&lt;ol&gt;&#10;&lt;li&gt;Yes it is possible! please see this: &lt;a href=&quot;http://www.youtube.com/watch?v=wTLsw-Ckfvw&quot; rel=&quot;nofollow&quot;&gt;http://www.youtube.com/watch?v=wTLsw-Ckfvw&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;Yes from the menu or from the syntax you can save these. Depending on you programming skills R might be more versatile for this...(maybe I'm biased)&lt;/li&gt;&#10;&lt;li&gt;Yes, please look in survival analysis from SPSS documentation.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2012-12-04T15:56:19.117" Id="45106" LastActivityDate="2012-12-04T15:56:19.117" OwnerUserId="17464" ParentId="41354" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="45110" AnswerCount="1" Body="&lt;p&gt;I am applying diferent methods to interpolate continuous spatial surfaces (kriging, splines, glm,etc). Most of the studies that have enough detail for me to follow usually focus on one specific method.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there any study that I might have missed where deterministic and statistical models are compared? &lt;/p&gt;&#10;&#10;&lt;p&gt;I understand that the selection of the best model is made by the quantification of error and statistical techniques, what I am looking for are studies/examples that for a certain type of dataset(static, time-series, climate, topographic) show that one model is more adapted than other and why.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-04T16:10:24.633" Id="45109" LastActivityDate="2012-12-04T16:17:17.490" OwnerUserId="7968" PostTypeId="1" Score="5" Tags="&lt;r&gt;&lt;time-series&gt;&lt;interpolation&gt;" Title="Spatial interpolation models: deterministic vs statistical" ViewCount="183" />
  <row Body="&lt;p&gt;An early paper to this effect is Evan Englund's &lt;a href=&quot;http://link.springer.com/article/10.1007%2FBF00890328?LI=true&quot;&gt;A Variance of Geostatisticians&lt;/a&gt; (Math. Geo. 1990) in which irregularly sampled elevation data from the Walker Lake area of Nevada were transformed and given to a dozen statisticians for interpolation using whatever methods they preferred. The results varied widely and the best ones were &lt;em&gt;not&lt;/em&gt; obtained by kriging, leading Englund to suggest (if I recall correctly) that various methods may be more suitable for particular kinds of datasets.&lt;/p&gt;&#10;&#10;&lt;p&gt;A version of this paper is &lt;a href=&quot;http://epa.gov/esd/cmb/research/papers/ee105.pdf&quot;&gt;available online&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;More of this literature can be found by chasing the citation trail.  There are a &lt;em&gt;huge&lt;/em&gt; number of papers in which two or more spatial interpolators are compared for a particular dataset.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-04T16:17:17.490" Id="45110" LastActivityDate="2012-12-04T16:17:17.490" OwnerUserId="919" ParentId="45109" PostTypeId="2" Score="5" />
  
  
  
  
  <row AcceptedAnswerId="45181" AnswerCount="1" Body="&lt;p&gt;I'm new to Stata and didn't find the answer in the help, navigating the menus, or online so far. I would just like to know how to access the quantile function of a distribution. For example in R, if I want to know the 0.95 point of the $\chi^2(1)$ distribution, I do:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; qchisq(0.95,1)&#10;[1] 3.841459&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Is there an equivalent command in Stata ?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-12-05T11:39:18.147" Id="45177" LastActivityDate="2012-12-05T12:36:22.510" OwnerUserId="7486" PostTypeId="1" Score="0" Tags="&lt;stata&gt;" Title="Quantile function in Stata" ViewCount="438" />
  <row AnswerCount="1" Body="&lt;p&gt;When analyzing data, using MLE or Bayesian methods, one needs to assume  a distribution for the data. For continuous data the are a number of distributions that are often considered, for example, the normal distribution, the t-distribution, the log-normal, etc.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;When analyzing the distribution of the SDs of a number of groups (or participants) what distributions could be appropriate?&lt;/strong&gt; Of course this is dependent on the data, and varies from case to case, but what could be some reasonable distributions to try?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-05T12:15:35.487" Id="45178" LastActivityDate="2012-12-12T15:54:09.207" OwnerUserId="6920" PostTypeId="1" Score="1" Tags="&lt;distributions&gt;&lt;standard-deviation&gt;" Title="Suitable distributions for the SDs of a number of groups (or participants)?" ViewCount="29" />
  <row AnswerCount="1" Body="&lt;p&gt;Hello and thanks in advance for your help! &lt;/p&gt;&#10;&#10;&lt;p&gt;I have conducted and EFA for a scale I made that includes 13 items.  Of these items, 2 are recoded.  Looking at the rotated component matrix I see that this scale has 2 factors, one with 11 items and one with the 2 recoded items.  What does this mean and how do I decribe this?  Does anyone know of a journal article or citation that deals with this sort of issue.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-12-05T12:51:51.840" Id="45182" LastActivityDate="2013-01-06T00:24:02.027" LastEditDate="2013-01-06T00:24:02.027" LastEditorUserId="88" OwnerUserId="17506" PostTypeId="1" Score="1" Tags="&lt;factor-analysis&gt;&lt;scales&gt;" Title="Recoded items as separate factors in factor analysis" ViewCount="367" />
  <row AnswerCount="1" Body="&lt;p&gt;I would like to construct a confidence interval around prediction from a neural network, without resorting to bootstrapping - given the computational cost. Can I use the Hessian returned in this way to produce a 95% CI?&lt;/p&gt;&#10;&#10;&lt;p&gt;1) Can you take the negative inverse of the Hessian as the var/covar matrix? I read &lt;a href=&quot;https://stat.ethz.ch/pipermail/r-help/2004-February/046272.html&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; that this depends on what is being maximized or minimized. Is this true and how can you know for certain?&lt;/p&gt;&#10;&#10;&lt;p&gt;2) Is this an accepted routine for producing a confidence interval around a prediction and if so, how would you do it? &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-12-05T13:51:34.553" Id="45188" LastActivityDate="2012-12-21T16:20:46.873" OwnerUserId="2040" PostTypeId="1" Score="3" Tags="&lt;regression&gt;&lt;maximum-likelihood&gt;&lt;neural-networks&gt;" Title="Prediction Interval for Neural Net With Hessian :: nnet in R" ViewCount="349" />
  <row Body="&lt;p&gt;Asymptotically the deviance should be chi-square distributed with mean equal to the degrees of freedom.  So divide it by its degrees of freedom &amp;amp; you should get about 1 if the data is not over-dispersed. To get a proper test just look up the deviance in chi-square tables - but note (a) that the chi square distribution is an approximation &amp;amp; (b) that a high value can indicate other kinds of lack of fit (which is perhaps why 'around 1' is considered good enough for government work).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-05T14:01:33.417" Id="45190" LastActivityDate="2012-12-05T14:01:33.417" OwnerUserId="17230" ParentId="37732" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;This is a &lt;em&gt;classification problem&lt;/em&gt;. Machine learning has a lot of tools to address problems such as these, e.g., Neural Networks, Support Vector Machines (SVM), Classification and Regression Trees (CART) etc.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-05T16:53:14.213" Id="45205" LastActivityDate="2012-12-05T16:53:14.213" OwnerUserId="1352" ParentId="45122" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;If you have multiple P-values (or equivalently, Z-stats), in order to combine them, you can either use Fisher's or Stouffer's formula.&lt;/p&gt;&#10;&#10;&lt;p&gt;Fisher's test results in a statistic $S = \sum_{i=1}^{N}-\log_{10}(P_i)$, which, under H0 is to $~\chi^2_{N-1}$. Stouffer's test results in a Z-statistic that is $\sum_{i=1}^NZ_i/\sqrt N$.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-12-05T19:48:35.053" Id="45215" LastActivityDate="2012-12-05T19:48:35.053" OwnerUserId="16645" ParentId="43542" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;The book Bayesian Econometric Methods is quite perfect for your need:&#10;&lt;a href=&quot;http://www.amazon.co.uk/Bayesian-Econometric-Methods-Exercises/dp/0521671736&quot; rel=&quot;nofollow&quot;&gt;http://www.amazon.co.uk/Bayesian-Econometric-Methods-Exercises/dp/0521671736&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Take a look at the table of contents and you will see that it covers all subjects you're interested on.&lt;/p&gt;&#10;&#10;&lt;p&gt;It presents several exercises with answers. In fact, the book is alll about exercises with answers. I have it and it's really good. They even have a website with matlab code to implement gibbs sampling described in the book. I was able to adapt the code into R, even knowing nothing of matlab.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-12-05T19:55:54.890" Id="45217" LastActivityDate="2012-12-05T20:01:11.620" LastEditDate="2012-12-05T20:01:11.620" LastEditorUserId="3058" OwnerUserId="3058" ParentId="44016" PostTypeId="2" Score="3" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I've been dealing with the problem of how to construct confidence intervals on penalized spline estimators in the presence of cluster-wise auto-correlation and heteroskedasticity.  My previous thread on this is here: &lt;a href=&quot;http://stats.stackexchange.com/questions/44798/penalized-spline-confidence-intervals-based-on-cluster-sandwich-vcv&quot;&gt;Penalized spline confidence intervals based on cluster-sandwich VCV&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;One of the commenters suggested using a wild bootstrap.  This is a bit tricky in the presence of clustered data, but this paper by Cameron et al. at UC Davis proposes what seems to be a relatively simple approach that accommodates the clustered context:&#10;&lt;a href=&quot;http://ideas.repec.org/a/tpr/restat/v90y2008i3p414-427.html&quot; rel=&quot;nofollow&quot;&gt;http://ideas.repec.org/a/tpr/restat/v90y2008i3p414-427.html&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Basically, the residual vector for each cluster has an equal chance of being left alone, or being multiplied by -1.  Thats it.  No resampling.  Over many iterations, you get many different combinations of cluster-wise residual vectors, but your combinations are restricted to the set of residual vectors that you already have, potentially multiplied by -1.  (I'm aware that there are alternative things that you might multiply your residual vector by, summarized on wikipedia: &lt;a href=&quot;http://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29#Wild_bootstrap&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29#Wild_bootstrap&lt;/a&gt;)&lt;/p&gt;&#10;&#10;&lt;p&gt;Basically this all seems too simple.  Am I missing some complicating wrinkle?  &lt;/p&gt;&#10;&#10;&lt;p&gt;Given that I'm programming my own estimators in R, I need to program my own bootstraps as well.  I want to make sure I get the procedure right before I go and code.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance.  &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-12-06T00:58:59.883" Id="45241" LastActivityDate="2012-12-06T00:58:59.883" OwnerUserId="17359" PostTypeId="1" Score="2" Tags="&lt;bootstrap&gt;&lt;heteroscedasticity&gt;&lt;splines&gt;&lt;clustered-standard-errors&gt;" Title="Wild cluster bootstrap seems really simple. Too simple. Am I missing someting?" ViewCount="426" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;Both the logistic function and standard deviation are usually denoted $\sigma$. I'll use $\sigma(x) = 1/(1+\exp(-x))$ and $s$ for standard deviation.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a logistic neuron with a random input whose mean $\mu$ and standard deviation $s$ I know. I hope the difference from the mean can be approximated well by some Gaussian noise. So, with a slight abuse of notation, assume it produces $\sigma(\mu + N(0,s^2))=\sigma(N(\mu,s^2))$. &lt;strong&gt;What is the expected value of $\sigma(N(\mu,s^2))$?&lt;/strong&gt; The standard deviation $s$ might be large or small compared with $\mu$ or $1$. A good closed form approximation for the expected value would be almost as good as a closed form solution. &lt;/p&gt;&#10;&#10;&lt;p&gt;I don't think a closed form solution exists. This can be viewed as a convolution, and the characteristic function for the logistic density is known ($\pi t ~\text{csch}  ~\pi t$), but I'm not sure how much that helps. The &lt;a href=&quot;http://oldweb.cecm.sfu.ca/cgi-bin/isc/lookup?number=0.2066209641419070372623508287180988405762817123258444554661413367648714222592830845419708162568291858&amp;amp;lookup_type=simple&quot;&gt;inverse symbolic calculator&lt;/a&gt; was unable to recognize the density at $0$ of the convolution of the logistic distribution's density and a standard normal distribution, which suggests but does not prove that there is no simple elementary integral. More circumstantial evidence: In some papers on adding Gaussian input noise to neural networks with logistic neurons, the papers didn't give closed form expressions either.&lt;/p&gt;&#10;&#10;&lt;p&gt;This question arose in trying to understand the error in the mean field approximation in Boltzman machines.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-06T13:09:26.000" Id="45267" LastActivityDate="2013-02-25T18:26:55.973" OwnerUserId="11981" PostTypeId="1" Score="7" Tags="&lt;machine-learning&gt;&lt;logistic&gt;&lt;normal-distribution&gt;" Title="Logistic input with Gaussian noise" ViewCount="187" />
  
  <row Body="&lt;p&gt;Jittering your data before testing is problematic (jittering is more used for plots so the points are not on top of each other, but the viewer can still understand that there were ties).  With jittering you are comparing datasets that are mixtures of the original and the jittering distribution and adding the jittering could artificially increase or decrease the p-value.&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem with ties in the KS test is that the p-values are computed based on assumptions that no ties are possible, so the work around is to not use the computed p-values but find your own.  One possibility is to do a permutation test:  run the KS test but ignore the p-value but record the KS test statistic; now combine the 2 sets of data and randomly choose out of the combined data a new set that is the same size as one of the original (and the rest will represent the other group); run the KS test on these new sets and again just record the test statistic.  Repeat this process a bunch of times (2,000 or so) and the recorded test statistics will represent the distribution under the null hypothesis, the proportion of the test statistics that are more extreeme than your original one is the p-value.&lt;/p&gt;&#10;&#10;&lt;p&gt;You mentioned that you thought the null should be rejected based on a plot of the data, perhaps the methods in this paper:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;&lt;pre&gt;&lt;code&gt; Buja, A., Cook, D. Hofmann, H., Lawrence, M. Lee, E.-K., Swayne,&#10; D.F and Wickham, H. (2009) Statistical Inference for exploratory&#10; data analysis and model diagnostics Phil. Trans. R. Soc. A 2009&#10; 367, 4361-4383 doi: 10.1098/rsta.2009.0120&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;would appeal to you instead.  The simplified version is that they generate several plots of the data under the assumption of the null being true (permuting similar to above) and show those plots along with the same plot of the original data.  If you cannot figure out which is the plot of the original data then that supports the null hypothesis, if the plot of the original is very obvious then that argues against the null.  The &lt;code&gt;vis.test&lt;/code&gt; function in the TeachingDemos package for R helps implement this test.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you have already decided that there is a difference and just want a p-value less than 0.05 then you can use &lt;code&gt;SnowsPenultimateNormalityTest&lt;/code&gt; that is also in the TeachingDemos package (but be warned, the documentation for that function is considered more useful than the function itself).&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-12-06T20:26:18.767" Id="45305" LastActivityDate="2012-12-06T20:26:18.767" OwnerUserId="4505" ParentId="42967" PostTypeId="2" Score="2" />
  
  <row AnswerCount="0" Body="&lt;p&gt;If $x_1, x_2, \ldots, x_n$ are i.i.d. negative binomial, then what is the distribution of $(x_1, x_2, \ldots, x_n)$ given&lt;/p&gt;&#10;&#10;&lt;p&gt;$x_1 + x_2 + \ldots + x_n = N\quad$?&lt;/p&gt;&#10;&#10;&lt;p&gt;$N$ is fixed. &lt;/p&gt;&#10;&#10;&lt;p&gt;If $x_1, x_2, \ldots, x_n$ are Poisson then, conditional on the total, $(x_1, x_2, \ldots, x_n)$ is multinomial. I am not sure if it is true for negative binomial, since it is a mixture Poisson.&lt;/p&gt;&#10;&#10;&lt;p&gt;In case you want to know, this is not a homework problem. Thanks.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-12-07T01:59:02.260" Id="45318" LastActivityDate="2012-12-07T03:34:51.933" LastEditDate="2012-12-07T03:34:51.933" LastEditorUserId="17230" OwnerUserId="7897" PostTypeId="1" Score="2" Tags="&lt;poisson&gt;&lt;multinomial&gt;&lt;negative-binomial&gt;&lt;conditioning&gt;" Title="conditional on the total, what is the distribution of negative binomials" ViewCount="74" />
  <row AcceptedAnswerId="45335" AnswerCount="2" Body="&lt;p&gt;There have been studies done that report that heterosexual individuals are somewhat more likely to be right-handed than homosexual individuals.  Does it necessarily follow that left-handed individuals are more likely to be homosexual than right-handed individuals?  I feel like this is a mathematical concept I can't place my finger on right now, and could be solved with a formal proof.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Handedness_and_sexual_orientation&quot; rel=&quot;nofollow&quot;&gt;Wikipedia article on this topic&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Edit: I think this is might be the method.&lt;/p&gt;&#10;&#10;&lt;p&gt;Assume individuals can only be either heterosexual (group $A$) or homosexual (group $B$), and can be either right-handed (group $R$) or left-handed (group $L$).  Given, &lt;/p&gt;&#10;&#10;&lt;p&gt;$P(A,R) &amp;gt; P(B,R)$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\because P(A,R)+P(A,L)=1, P(B,R)+P(B,L)=1$&lt;/p&gt;&#10;&#10;&lt;p&gt;Combine the two equations,&lt;/p&gt;&#10;&#10;&lt;p&gt;$P(A,R)+P(A,L)=P(B,R)+P(B,L)$&lt;/p&gt;&#10;&#10;&lt;p&gt;move terms,&lt;/p&gt;&#10;&#10;&lt;p&gt;$P(A,R)-P(B,R)=P(B,L)-P(A,L)&amp;gt;0$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\therefore P(B,L)&amp;gt;P(A,L)$&lt;/p&gt;&#10;&#10;&lt;p&gt;Therefore, a person who is left-handed is more likely to be homosexual than (fixed) a right-handed person is.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Is this correct, based on the (obviously) simplified assumptions?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-12-07T08:32:41.610" FavoriteCount="1" Id="45331" LastActivityDate="2012-12-07T11:50:53.670" LastEditDate="2012-12-07T11:50:53.670" LastEditorUserId="17574" OwnerUserId="17574" PostTypeId="1" Score="2" Tags="&lt;correlation&gt;&lt;mathematical-statistics&gt;&lt;proof&gt;" Title="Correlation of handedness to sexual orientation?" ViewCount="124" />
  <row Body="&lt;p&gt;David Blei has a great talk introducing LDA to students of a summer class: &lt;a href=&quot;http://videolectures.net/mlss09uk_blei_tm/&quot;&gt;http://videolectures.net/mlss09uk_blei_tm/&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;&lt;strong&gt;In the first video&lt;/em&gt;&lt;/strong&gt; he covers extensively the basic idea of topic modelling and how Dirichlet distribution come into play. The plate notation is explained as if all hidden variables are observed to show the dependencies. Basically topics are distributions over words and document distributions over topics.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;&lt;strong&gt;In the second video&lt;/em&gt;&lt;/strong&gt; he shows the effect of alpha with some sample graphs. The smaller alpha the more sparse the distribution. Also, he introduces some inference approaches.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-12-07T08:59:50.280" Id="45332" LastActivityDate="2012-12-07T17:57:22.557" LastEditDate="2012-12-07T17:57:22.557" LastEditorUserId="17084" OwnerUserId="17084" ParentId="37405" PostTypeId="2" Score="7" />
  
  <row Body="&lt;p&gt;Since we are testing for a uniform distribution, the values of each season should be equal. The most likely value for each season assuming a uniform distribution is the average of all the seasons.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;(13+24+18+25)/4 = 80/4 = 20&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The formula for the Chi-Squared Statistic is given here.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Pearson's_chi-squared_test&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;((13-20)^2)/20 = 2.45&#10;((24-20)^2)/20 = 0.80&#10;((18-20)^2)/20 = 0.20&#10;((25-20)^2)/20 = 1.25&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Chi-Squared Statistic &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;2.45 + 0.80 + 0.20 + 1.25 = 4.7&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The degrees of freedom are&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;df=4-1=3&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;You should be able to figure out the rest.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-12-07T10:00:13.320" Id="45336" LastActivityDate="2012-12-07T16:21:19.230" LastEditDate="2012-12-07T16:21:19.230" LastEditorUserId="17442" OwnerUserId="17442" ParentId="45333" PostTypeId="2" Score="-1" />
  <row Body="&lt;p&gt;Given the enormity of the mistake, to understand better where it lies, I think it will be easier to use figures instead of formulae. &lt;/p&gt;&#10;&#10;&lt;p&gt;Imagine a population of 100 individuals, with 2 left-handed homosexual, 8 right-handed homosexual, 9 left-handed heterosexuals, 81 right-handed heterosexuals.&#10;There are 20% of left-handed individuals among homosexuals, and 10% only among heterosexual, so &quot;heterosexual individuals are somewhat more likely to be right-handed than homosexual individuals&quot;. However pick a random left-handed individual : the probability of this individual to be homosexual is 2/11, that is 18%. He is &lt;strong&gt;more&lt;/strong&gt; likely to be heterosexual than homosexual.&lt;/p&gt;&#10;&#10;&lt;p&gt;Additionally, there are 10% of homosexual in this population, and 9% in the right-handed subpopulation. This is all you can get: more homosexuals among left-handed persons that among right-handed persons. Cf Douglas’ answer for a formalization of this – or try it with other figure to get a better understanding of what is going on.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-12-07T10:01:49.937" Id="45337" LastActivityDate="2012-12-07T10:07:03.060" LastEditDate="2012-12-07T10:07:03.060" LastEditorUserId="8076" OwnerUserId="8076" ParentId="45331" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;Your matrix may be square, but you may have a linear dependence in one of your columns. Multicollinearity can occur when one of your columns is equal to another column multiplied by some scalar value. If you don't have a problem with the pseudo-inverse, you can continue to use it. If you do, please follow the remedies in this article.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Multicollinearity&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Multicollinearity&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-12-07T10:19:47.817" Id="45338" LastActivityDate="2012-12-07T10:19:47.817" OwnerUserId="17442" ParentId="45329" PostTypeId="2" Score="1" />
  <row AnswerCount="2" Body="&lt;p&gt;I'd like to have a probability measure for the &lt;strong&gt;co-occurrence&lt;/strong&gt; of intervals in two datasets of intervals.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;dataset1: [125,500],[900,1300],[2220,2500], ...&#10;dataset2: [600,800],[1200,1400],[3020,3500], ...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;(Two exemplary datasets with an overlap in the second intervals)&lt;/p&gt;&#10;&#10;&lt;p&gt;The intervals within a dataset are represented by the coordinates of their start and end (just two integer values). Within a dataset intervals are non-overlapping.&#10;The number, average lengths and distribution of intervals may be very different between two datasets, but the length of the dataset that is the maximal integer a coordinate can have is identical and known for both.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am looking for a kind of p-value for the probability that there is a &quot;correlation&quot; between two datasets, that is that intervals overlap more frequently (or on the whole more widely in terms of intersection length) than can be expected by chance.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would be grateful for suggestions on how to address the problem in practical as well as statistical terms (perhaps using R).&#10;Perhaps there is even a solution for the &quot;correlation&quot; between multiple datasets ... &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks a lot :-)&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-12-07T12:17:22.207" Id="45341" LastActivityDate="2012-12-07T20:17:52.213" LastEditDate="2012-12-07T15:40:18.263" LastEditorUserId="17575" OwnerUserId="17575" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;correlation&gt;&lt;statistical-significance&gt;&lt;p-value&gt;" Title="p-value for correlation/co-occurrence in sets of intervals" ViewCount="251" />
  <row AnswerCount="0" Body="&lt;p&gt;I am curious if tests such as Rayleigh's test, Kuiper's test, and Watson's test are valid for bidirectional data (i.e. 180 degrees) as well as unidirectional data.&lt;br&gt;&#10;If not, what are the appropriate corresponding tests?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-12-07T12:56:49.833" Id="45345" LastActivityDate="2013-08-08T23:52:10.743" LastEditDate="2013-08-08T23:52:10.743" LastEditorUserId="22047" OwnerUserId="13449" PostTypeId="1" Score="1" Tags="&lt;directional-statistics&gt;" Title="Circular statistics and bidirectional data" ViewCount="159" />
  <row AcceptedAnswerId="46196" AnswerCount="2" Body="&lt;p&gt;I have a dataset which consists of 190,455 nodes and 1,241,638 edges. These can be broken down into a set of 2300 subgraphs which are not connected to each other. I am having trouble creating visualisations for this using tools such as Gephi (Yifan-Yu seems to work best) as the layout takes a very long time and doesn't seem to separate the subgraphs well.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a tool which performs a layout for such a dataset well? It would seem best to plot each subgraph individually, and then perform some merging at the end to avoid overlap while the algorithms seem to be trying to plot and arrange everything at once.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-12-07T13:42:48.610" FavoriteCount="2" Id="45349" LastActivityDate="2012-12-18T21:02:06.133" OwnerUserId="10670" PostTypeId="1" Score="3" Tags="&lt;data-visualization&gt;&lt;graph-theory&gt;" Title="Network Visualisation for huge dataset composed of many unconnected clusters" ViewCount="457" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I am working my way through LDA and I think I got they main idea of it. Please correct me if I am wrong. Given the Plate notation:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/8nzTh.png&quot; alt=&quot;LDA model&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The variables $\alpha$ and $\beta$ are Dirichlet distribution parameters. The variable $Z_{d,n}$ assigns observed word $W_{d,n}$ to topic $\phi_k$, which is a distribution over words. Variable $\theta_d$ is the document-specific topic distribution. Both distributions $\theta_d$ and $\phi_k$ are drawn from Dirichlet distributions.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, only $W_{d,n}$ is observed and can be &quot;directly&quot; calculated. My question:&#10;What exactly is inferred/calculated with e.g. Gibbs sampling, variational Inference and so on?&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;For instance: For a Gaussian Naive Bayes classifier one assumes that the likelihood of each feature is Gaussian. In other words each feature has a Gaussian distribution:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;P(x) =  \frac{1}{{\sigma \sqrt {2\pi } }} e^{ \frac{ - ( {x - \mu } )^2} {2\sigma ^2 } }&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;To find this distribution $\sigma$ and $\mu$ have to be determined which is pretty straight forward.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, plainly said: What Numbers do I determine for LDA?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-12-07T15:32:51.613" Id="45357" LastActivityDate="2013-05-04T22:25:42.990" LastEditDate="2013-05-04T22:25:42.990" LastEditorUserId="25249" OwnerUserId="17084" PostTypeId="1" Score="1" Tags="&lt;inference&gt;&lt;dirichlet-distribution&gt;&lt;topic-models&gt;" Title="Latent Dirichlet Allocation (LDA): What exactly is inferred?" ViewCount="524" />
  <row AcceptedAnswerId="45369" AnswerCount="1" Body="&lt;p&gt;Let us assume the following data generating process: &lt;/p&gt;&#10;&#10;&lt;p&gt;(1) $ \ \ y = X\beta + u$ &lt;/p&gt;&#10;&#10;&lt;p&gt;where $y$ and $u$ are $n\times 1$ vectors, $X$ is a $n\times k$-matrix with $rk(X)=k$ and $\beta$ is a $k\times 1$-vector of coefficients. For the OLS-estimator to be unbiased we need to assume that the conditional expectation of $u$ given $X$ i.e. $E(u|X)=0$ holds for (1). If the data are actually generated with (1) and $\beta_0$ is the true value for $\beta$ then we can write &lt;/p&gt;&#10;&#10;&lt;p&gt;$E(\hat{\beta}|X) = \beta_0 + (X^TX)^{-1}X^TE(u|X) = \beta_0$. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;My question is: does the assumption $E(u|X)$ already imply by the law of large numbers (LLN) that $\text{plim}_{n\to\infty} \ \frac{1}{n}X^Tu =0$?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;My guess is yes because by LLN  $\text{plim}_{n\to\infty} \ \frac{1}{n}X^Tu = \text{lim}_{n\to\infty}\frac{1}{n}E(X^Tu) = 0$ ?&lt;/p&gt;&#10;&#10;&lt;p&gt;The reason for this question is that if the assumption $E(u|X)=0$ does imply that $\text{plim}_{n\to\infty} \ \frac{1}{n}X^Tu =0$ than the asymptotic consistency of $\hat{\beta}$ follows directly from this particular assumption.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-12-07T15:50:42.297" FavoriteCount="2" Id="45359" LastActivityDate="2012-12-15T22:28:46.883" LastEditDate="2012-12-15T22:28:46.883" LastEditorUserId="3826" OwnerUserId="13636" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;least-squares&gt;&lt;expected-value&gt;&lt;asymptotics&gt;" Title="Asymptotic assumptions in OLS" ViewCount="134" />
  <row Body="&lt;p&gt;This is a linear model, so the easiest (but not completely kosher) way to fit it is using ordinary least squares (OLS) regression. It's easier if you rewrite the model a bit,&#10;$$w_i = \beta_0 + \beta_1 w_{i-1} + \beta_2 c_i.$$&#10;You should be able to convince yourself that this is essentially the same model, and that you can solve backwards to find your $k_1, k_2, k_3$ in terms of $\beta_0, \beta_1, \beta_2$. We can fit it in R as follows (with some random data):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; w &amp;lt;- rnorm(101,180,2)&#10;&amp;gt; c &amp;lt;- rnorm(100,2000,200)&#10;&amp;gt; model &amp;lt;- lm(w[2:101] ~ w[1:100] + c)&#10;&amp;gt; summary(model)&#10;&#10;Call:&#10;lm(formula = w[2:101] ~ w[1:100] + c)&#10;&#10;Residuals:&#10;    Min      1Q  Median      3Q     Max &#10;-5.5455 -1.4231  0.1289  1.1868  5.3232 &#10;&#10;Coefficients:&#10;             Estimate Std. Error t value Pr(&amp;gt;|t|)    &#10;(Intercept) 1.429e+02  1.784e+01   8.013 2.51e-12 ***&#10;w[1:100]    1.925e-01  9.919e-02   1.941   0.0552 .  &#10;c           1.095e-03  1.180e-03   0.928   0.3559    &#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 &#10;&#10;Residual standard error: 2.126 on 97 degrees of freedom&#10;Multiple R-squared: 0.04784,    Adjusted R-squared: 0.02821 &#10;F-statistic: 2.437 on 2 and 97 DF,  p-value: 0.09277 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Since the data are completely random, we don't actually expect there to be any relationship (other than the intercept), and those p-values are actually surprisingly low. You can do a similar fit in any statistical software.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you want to actually do it properly (as opposed to an easy and not-very-accurate hack), this type of analysis is called time series. MIT's OCW has a &lt;a href=&quot;http://ocw.mit.edu/courses/economics/14-384-time-series-analysis-fall-2008/index.htm&quot; rel=&quot;nofollow&quot;&gt;course&lt;/a&gt; about it, though I can't vouch for it personally (and I'm usually a little skeptical of stats courses from economists).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-07T17:34:03.043" Id="45367" LastActivityDate="2012-12-07T17:34:03.043" OwnerUserId="16297" ParentId="45364" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;This Markov network can only model pairwise correlations, whereas the RBM can model higher order correlations thanks to the latent variables. The RBM is a universal approximator of distributions, (2) is not.&lt;/p&gt;&#10;&#10;&lt;p&gt;To answer you questions, given an RBM in general you can't come up with an equivalent (2). In the same way that in general you can't approximate a kernel SVM with a linear SVM.&lt;/p&gt;&#10;&#10;&lt;p&gt;Given (2), you can always come up with the equivalent RBM.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-12-07T20:01:42.853" Id="45380" LastActivityDate="2012-12-07T20:01:42.853" OwnerUserId="17592" ParentId="45316" PostTypeId="2" Score="2" />
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Consider this example.  Suppose that for any pair $(x, y)$ of bacterial strain $x$ and (candidate) anti-bacterial agent $y$, we can experimentally determine some measure $f(x, y)$ (say, the concentration of $y$ needed to kill off 50% of a culture of $x$ in some standardized assay).  Now we want to devise a statistical model $M$ to predict $f(x, y)$ on the basis of some features of $x$ and $y$.  We decide on sets of features $V$ and $W$ for bacterial strains and anti-bacterials, respectively.  Therefore, each pair $(x, y)$ of strains and agents gets assigned a composite feature vector $(\mathbf{v}(x), \mathbf{w}(y))$.  The goal is to optimize the estimates $M(\mathbf{v}(x), \mathbf{w}(y))$ of $f(x, y)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;To train the model, our &quot;gold standard&quot; dataset consists of the experimentally measured $f(x, y)$ values for all $(x, y)$ pairs in the Cartesian product $X \times Y$ of some panel $X$ of strains and panel $Y$ of agents.  (This, BTW, is the &quot;rectangular&quot; sample alluded to in the post's title.)&lt;/p&gt;&#10;&#10;&lt;p&gt;To prevent over-fitting, we want to use some form of cross-validation.  My question concerns the protocol for selecting the subsets $T$ and $V$ of $X \times Y$ to use for training and validation, respectively.&lt;/p&gt;&#10;&#10;&lt;p&gt;My intuition is that $T$ should be in the form of a Cartesian product $X_t \times Y_t$, where $X_t$ and $Y_t$ are randomly chosen &lt;em&gt;proper&lt;/em&gt; subsets of $X$ and $Y$, respectively; and that $V$ should be of the form $X_v \times Y_v$, where $X_v = X - X_t$ and $Y_v = Y - Y_t$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Of course, this scheme does not use all of the data.  An alternative scheme would be to make the training set $T$ as a random subset of $X \times Y$, and take $V = (X \times Y) - T$.  This second scheme, however, seems wrong to me, because such training set $T$ would contain too much information about all of the strains in $X$ and the agents in $Y$ to make $V$ a fair challenge to the model.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is this the right intuition?  If so, what's the rigorous way to show it?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-08T17:49:18.490" Id="45433" LastActivityDate="2012-12-08T17:49:18.490" OwnerUserId="4769" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;cross-validation&gt;" Title="On cross-validation schemes for &quot;rectangular&quot; samples" ViewCount="43" />
  <row Body="&lt;p&gt;Basically, two approaches are mentioned in your post:&lt;/p&gt;&#10;&#10;&lt;p&gt;1- Using single FS metric over the entire &lt;strong&gt;training&lt;/strong&gt; dataset&#10;2- Partitioning data and using the FS metric over every split&lt;/p&gt;&#10;&#10;&lt;p&gt;In the second case, frequency of the feature among the splits is used to select the final set of features.&lt;/p&gt;&#10;&#10;&lt;p&gt;Your proposed framework (or the 2nd approach) can be viewed as applying &lt;a href=&quot;http://stats.stackexchange.com/questions/27750/feature-selection-and-cross-validation&quot;&gt;feature selection in a cross-validation&lt;/a&gt; setting. I do not think that the question is about the correctness of the proposed idea. Rather, it is more about the proper design and optimized results.&lt;/p&gt;&#10;&#10;&lt;p&gt;The idea proposed can be used to select a subset of features. However, you may need to split the data into more parts instead of only two halfs which also, can be fine. You may experiment with different number of splits.&lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, if you prefer or if you are required to use cross-validation, you may go with the second approach. Otherwise, just apply feature selection over the &lt;strong&gt;training data set&lt;/strong&gt; only.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-12-08T18:07:35.457" Id="45435" LastActivityDate="2012-12-08T18:07:35.457" OwnerUserId="14888" ParentId="45348" PostTypeId="2" Score="0" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have a large set of predictors (more than 43,000) for predicting a dependent variable which can take 2 values (0 or 1). The number of observations is more than 45,000. Most of the predictors are unigrams, bigrams and trigrams of words, so there is high degree of collinearity among them. There is a lot of sparsity in my dataset as well. I am using the logistic regression from the glmnet package, which works for the kind of dataset I have. My problem is how can I report p-value significance of the predictors. I do get the beta coefficient, but is there a way to claim that the beta coefficients are statistically significant?&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is my code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library('glmnet')&#10;data &amp;lt;- read.csv('datafile.csv', header=T)&#10;mat = as.matrix(data)&#10;X = mat[,1:ncol(mat)-1] &#10;y = mat[,ncol(mat)]&#10;fit &amp;lt;- cv.glmnet(X,y, family=&quot;binomial&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Another question is:&#10;I am using the default alpha=1, lasso penalty which causes the additional problem that if two predictors are collinear the lasso will pick one of them at random and assign zero beta weight to the other. I also tried with ridge penalty (alpha=0) which assigns similar coefficients to highly correlated variables rather than selecting one of them. However, the model with lasso penalty gives me a much lower deviance than the one with ridge penalty. Is there any other way that I can report both predictors which are highly collinear?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-12-08T22:16:41.247" Id="45449" LastActivityDate="2014-06-12T04:02:47.007" OwnerUserId="12245" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;multiple-regression&gt;&lt;lasso&gt;&lt;glmnet&gt;" Title="When using glmnet how to report p-value significance to claim significance of predictors?" ViewCount="480" />
  
  
  <row Body="&lt;p&gt;Yes, you use a classical test for proportion. You can use the exact binomial distribution. There are only two values as extreme as $x = 10$, this is $x = 0$ or $10$. Thus&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\def\P{\mathbb P}&#10;\begin{aligned} p &amp;amp;= \P( X = 0 \text{ or } X = 10)\\&#10;&amp;amp;= \P(X = 0) + \P(X = 10)\\&#10;&amp;amp;= 2\times \P(X = 0)\\&#10;&amp;amp;= 2 \times {10 \choose 0} \times \left(1 \over 2\right)^{10}\\&#10;&amp;amp;= 2 \times 1 \times {1\over 2^{10}}\\&#10;&amp;amp;= {1\over 512}\\&#10;&amp;amp;= 0.0019&#10;\end{aligned}$$ &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-12-09T06:51:18.340" Id="45469" LastActivityDate="2012-12-09T06:51:18.340" OwnerUserId="8076" ParentId="45468" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I would consider plotting some kind of aggregate function for each decade, say the sum of the value. or the count of instances (or both), if you want to continue using the scatterplot approach, try introducing transparency in the points, so the viewer can easily appreciate the increased density closer to the origin....&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-12-09T07:01:34.333" Id="45470" LastActivityDate="2012-12-09T07:01:34.333" OwnerUserId="17602" ParentId="45466" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="45568" AnswerCount="2" Body="&lt;p&gt;I am trying to do a few tests in Minitab and I don't understand why they are not working out.  &lt;/p&gt;&#10;&#10;&lt;p&gt;First, my data in minitab has several columns: &lt;code&gt;Class Name Gender Height Age&lt;/code&gt;, where &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;code&gt;Class&lt;/code&gt; has two values {A,B};&lt;/li&gt;&#10;&lt;li&gt;&lt;code&gt;Name&lt;/code&gt; is the name of the students; &lt;/li&gt;&#10;&lt;li&gt;&lt;code&gt;Gender&lt;/code&gt; is Female or Not female;&lt;/li&gt;&#10;&lt;li&gt;&lt;code&gt;Age&lt;/code&gt; is the age of the student. &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;My questions I have to answer are as follows: &#10;Is the &lt;em&gt;height&lt;/em&gt; of &lt;em&gt;Class A&lt;/em&gt; larger than &lt;em&gt;Class B&lt;/em&gt;?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;My understanding of the question&lt;/strong&gt; &#10;I should be  a 2 sample t-test of the differences of the  &lt;em&gt;Height&lt;/em&gt; of &lt;em&gt;Class A&lt;/em&gt; - &lt;em&gt;Class B&lt;/em&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;However, every time I try to do a two sample t-test it just compares class with height, which is silly and doesn't workout.   &lt;/p&gt;&#10;&#10;&lt;p&gt;How Do I get Minitab to test &lt;em&gt;Class A&lt;/em&gt;'s &lt;em&gt;height&lt;/em&gt; against &lt;em&gt;Class B&lt;/em&gt;'s &lt;em&gt;height&lt;/em&gt;? &lt;/p&gt;&#10;&#10;&lt;p&gt;Ps.  If I am incorrect in the correct hypothesis testing method, please let me know.  &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-12-09T08:37:46.093" Id="45474" LastActivityDate="2015-03-02T05:12:02.767" LastEditDate="2012-12-09T11:43:23.637" LastEditorUserId="930" OwnerUserId="17112" PostTypeId="1" Score="0" Tags="&lt;hypothesis-testing&gt;&lt;minitab&gt;" Title="How to perform two-sample t-test with Minitab?" ViewCount="375" />
  <row AcceptedAnswerId="46332" AnswerCount="1" Body="&lt;p&gt;I would like to forecast the non-stationary time series, involving several crucial a-priori assumptions following from studying of instances of such series.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;I've constructed time-averaged one-point probability distribution function approximated by normal distribution. $$\hat p(x) = \frac{1}{\sqrt{2\pi \sigma^2_{\infty}}} \exp\left(-\frac{x^2}{2\sigma^2_{\infty}}\right)$$&#10;From this point of view, I want the forecast $z_t(l)$ not to exceed this when $l \to \infty$. To put it in other words, variance of $z_t(l)$ must be bounded.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;strike&gt;The average two-point probability distribution function $\hat p(x_i,i;x_j,j)$ also has been constructed, which led to identification of  autocorrelation function. $\rho(j) \approx A j^{-\alpha} $ provided $0&amp;lt;\alpha&amp;lt;0.5$.&lt;/strike&gt;&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;At first, Box-Jenkins identification process led me to $ARIMA(0,1,3)$ model, however&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;I can't have bounded variance until $d \ne 0$ (which follows from equations for BJ weights $\psi_j$). At the same time, I can't use $d=0$ since initial autocorrelation decreases slowly (which is probably evidence of non-stationarity according BJ). This is the main obstacle to me.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Visually, simulation of $ARIMA(0,1,3)$ does not coincide with behaviour of my samples. And correlations of first difference of the series are in the bad agreement with correlations following from the model.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The analysis of residuals shows significant correlations starting lag 3. This is why my initial statement about $ARIMA(0,1,3)$ is incorrect.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Trying to fit different $ARIMA(p,0,0)$ models, I see that there is significant residual correlations close to the lag $p$ for every $p$. It may assume that I need $ARIMA(\infty,0,q)$ model (as limiting choice), for instance fractional ARIMA.&lt;/p&gt;&#10;&#10;&lt;p&gt;From [1] I've learned about Fractional $ARIMA(p,d,q)$ models which are $ARIMA(\infty,0,q)$ in effect.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;I've not found any GNU R packages with support of missing values for this. Missing values seems to be a kind of challenge.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The publications on fractional ARIMA are quite rare. Are such fractional models really used? Maybe there is a good replacement of ARIMA models for my needs? The forecasting is not my major, I have only pragmatic interest.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;From different literature (for instance [2]), I learned that it is practically impossible to decide between fractional ARIMA and models with &quot;level shift&quot;. However, I have not found the package for GNU R to fit 'level shift' models.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;&lt;sub&gt;[1]: Granger, Joyeux.: J. of time series anal. vol. 1 no. 1 1980, p.15&lt;/p&gt;&#10;&#10;&lt;p&gt;[2]:  Grassi, de Magistris.: &quot;When long memory meets the Kalman filter: A comparative study&quot;, Computational Statistics and Data Analysis, 2012, in press.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;/sub&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;b&gt;Update:&lt;/b&gt; to render my own progress and to answer @IrishStat&lt;/p&gt;&#10;&#10;&lt;p&gt;My statement about two-point probability distribution is incorrect in general. Constructed in this way function will depend on full series length. So, there is a little to extract from this. At least, parameter named $\alpha$ will depend on full series length.&lt;/p&gt;&#10;&#10;&lt;p&gt;Lists 2 and 3 also have been updated.&lt;/p&gt;&#10;&#10;&lt;p&gt;My data is available as dat file &lt;a href=&quot;https://gist.github.com/4369930&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;At the current moment, I doubt between FARIMA and level shifts, and I still can't find appropriate software to check this options. This is also my first experience with model identification, so any help will be appreciated.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-12-09T11:28:41.167" Id="45485" LastActivityDate="2012-12-25T17:18:41.140" LastEditDate="2012-12-24T16:58:27.273" LastEditorUserId="17543" OwnerUserId="17543" PostTypeId="1" Score="4" Tags="&lt;time-series&gt;&lt;forecasting&gt;&lt;arima&gt;" Title="Forecasting nonstationary time series" ViewCount="532" />
  <row Body="&lt;p&gt;The solution is to use another package called &lt;code&gt;rugarch&lt;/code&gt;. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;install.packages(&quot;rugarch&quot;)&#10;require(rugarch)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Let's construct a (Tx2) matrix called &lt;code&gt;data&lt;/code&gt;, where column 1 is $Y_t$ and column 2 is $X_t$. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;data &amp;lt;- cbind(rnorm(1000),rnorm(1000))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;We can then compute the GARCH(1,1) model that I described in my question:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;spec &amp;lt;- ugarchspec(variance.model = list(model = &quot;sGARCH&quot;, garchOrder = c(1, 1), &#10;submodel = NULL, external.regressors = NULL, variance.targeting = FALSE), &#10;mean.model = list(armaOrder = c(0, 0), external.regressors = matrix(data[,2]), &#10;distribution.model = &quot;norm&quot;, start.pars = list(), fixed.pars = list()))&#10;&#10;garch &amp;lt;- ugarchfit(spec=spec,data=data[,1],solver.control=list(trace=0))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Comparing this to &lt;code&gt;EViews&lt;/code&gt;: The &lt;code&gt;R&lt;/code&gt; coefficient estimates that are greater than $0.01$ differ to the &lt;code&gt;EViews&lt;/code&gt; estimates by approximately $1-2\%$, which is acceptable in my mind. $t$-statistics differ by more but it doesn't effect inference. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-09T13:37:43.430" Id="45493" LastActivityDate="2012-12-09T13:37:43.430" OwnerUserId="16811" ParentId="45482" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="45517" AnswerCount="2" Body="&lt;p&gt;Suppose I have a regression model shown below&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Model 1:&lt;/strong&gt;&#10;$$&#10;Y = \beta_0^\ + \beta_1SEX\ + \beta_2ALCOHOL\ + \beta_3SEX*ALCOHOL\&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The predictors I am interested in are SEX (binary: 0 female, 1 male) and Alcohol consumption (binary: drinker, non-drinker). Suppose that I found a significant interaction between SEX and ALCOHOL and decided to stratify the data by sex. So I would have two new models:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Model 2a:&lt;/strong&gt;&#10;$$&#10;\text{Female: } &#10;Y_F = \beta_0^\ +  \beta_2ALCOHOL\&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;So for the female subset, the intercept is still $ \beta_0$ and the slope for ALCOHOL is $ \beta_2$ &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Model 2b:&lt;/strong&gt;&#10;$$&#10;\text{Male: }&#10;Y_M = (\beta_0^\ + \beta_1) + (\beta_2\ +\beta_3) ALCOHOL&#10;$$  &lt;/p&gt;&#10;&#10;&lt;p&gt;For the male subset, the intercept is now $\beta_0^\ + \beta_1$ and the slope for ALCOHOL is $\beta_2^\ + \beta_3$&lt;/p&gt;&#10;&#10;&lt;p&gt;This is pretty straightforward. If you fit a model like this in any statistical package, you would get this kind of result. However, if say, in the model, I actually included an additional variable AGE, which is a covariate (assuming that it does not interact with either SEX or ALCOHOL), the original model would be the one below:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Model 3:&lt;/strong&gt;&#10;$$&#10;Y = \beta_0^\ + \beta_1SEX\ + \beta_2ALCOHOL\ + \beta_3SEX*ALCOHOL\ + \beta_4AGE\&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Further suppose that we still have a significant interaction between SEX and ALCOHOL and I would like to stratify the data again. I would get two models below if I followed the logic above:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Model 4a:&lt;/strong&gt;&#10;$$&#10;\text{Female: }&#10;Y_F = \beta_0^\ + \beta_2ALCOHOL\ + \beta_4AGE\&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Model 4b:&lt;/strong&gt;&#10;$$&#10;\text{Male: }&#10;Y_M = (\beta_0^\ + \beta_1) + (\beta_2\ +\beta_3)ALCOHOL + \beta_4AGE\&#10;$$  &lt;/p&gt;&#10;&#10;&lt;p&gt;However, the actual beta coefficients obtained using a computer program can be very different from what you obtain using the updated equations above. The difference in $\beta_0$ makes sense, because the stratified models in 4a and 4b still assume the pooled mean for age; namely, it estimates $\bar{Y}$ at the mean of all subjects' age, whereas in the stratified analyses done by a computer program, the intercepts of the models estimate $\bar{Y}$ at the mean age of a sub-group. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;However, I wonder why the slopes are different. In other words, why are the slopes in Models 4a and 4b different from those produced by a statistical package.&lt;/strong&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-09T17:22:43.153" Id="45505" LastActivityDate="2012-12-16T19:48:32.273" LastEditDate="2012-12-16T18:33:36.817" LastEditorUserId="7290" OwnerDisplayName="user11392" PostTypeId="1" Score="3" Tags="&lt;regression&gt;&lt;multiple-regression&gt;&lt;interaction&gt;&lt;regression-coefficients&gt;&lt;stratification&gt;" Title="Beta coefficients from stratified analysis when there are covariates?" ViewCount="428" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm trying to plot forecast and real data on the same plot. But there is always gap between them on the image :&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ni7FR.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I use for the forecasting following code:&#10;    y=boardings[,1]&#10;    ## Simple Exponential smoothing&#10;    #predict existing values&#10;    results=HoltWinters(y,beta=FALSE, gamma=FALSE)&#10;    #print data&#10;    print(cbind(y,results$fitted))&#10;    #draw the plot with fitted values&#10;    plot(results)&#10;    #predict future values&#10;    results2=forecast.HoltWinters(results,h=12)&#10;    print(results2)&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#draw the plot with prediction&#10;plot.forecast(results2,lwd=1,xlab='time',ylab='log.boardings',main='12-month prediction by Exponential Smoothing')&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The data is taken from the &lt;a href=&quot;http://cran.r-project.org/web/packages/timeSeries/index.html&quot; rel=&quot;nofollow&quot;&gt;timeSeries&lt;/a&gt; package with name &lt;code&gt;boardings&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would really appreciate any help.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-09T21:36:23.077" Id="45527" LastActivityDate="2012-12-09T23:09:13.940" LastEditDate="2012-12-09T23:09:13.940" LastEditorUserId="17647" OwnerUserId="17647" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;data-visualization&gt;&lt;forecasting&gt;" Title="Gap in plot between forecasted and real data" ViewCount="145" />
  <row AcceptedAnswerId="45541" AnswerCount="3" Body="&lt;p&gt;I am using logit models to predict whether or not children are unhealthy (binary indictor).  Many of my models have statistically significant relationships between predictors of interest (ie, if the child’s household has improved water/sanitation) and the outcome.   However, none of my models outperform a model which just uses demographic information (age, region, urban/rural, sex).  Furthermore, the models are pretty terrible at predicting unhealthy children (only about ~15% true positives correctly predicted, which are about 23% of the data).  Do these two facts render the statistically significant relationships between the predictors and outcome of interest meaningless?  How do I explain this to my (hopefully future) readers?  Thanks,  &lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT: &lt;/p&gt;&#10;&#10;&lt;p&gt;Here are the set of results from my model -- I've masked the actual variables names because I cannot share them publicly. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;variable    controls    proximal variables&#10;(Intercept) 0.212(0.255)    0.172(0.296)&#10;variable 1                  1.038(0.09)&#10;variable 2                  0.721(0.098)**&#10;variable 3                  0.986(0.095)&#10;variable 4                  0.810(0.104)**&#10;variable 5                  0.744(0.109)**&#10;variable 6                  0.981(0.15)&#10;variable 7                  1.317(0.051)**&#10;controls 1  0.769(0.087)**  0.763(0.087)**&#10;controls 2  1.078(0.015)**  1.087(0.015)**&#10;controls 3  0.666(0.098)**  0.838(0.11)&#10;controls 4  0.958(0.122)    1.129(0.127)&#10;controls 5  0.706(0.144)**  0.750(0.146)**&#10;controls 6  1.012(0.086)    0.998(0.089)&#10;controls 7  0.957(0.141)    1.010(0.142)&#10;controls 8  0.694(0.173)**  0.726(0.18)*&#10;controls 9  1.895(0.065)**  1.575(0.072)**&#10;controls 10 1.353(0.085)**  1.279(0.095)**&#10;controls 11 0.907(0.083)    0.949(0.095)&#10;controls 12 0.988(0.135)    0.920(0.145)&#10;Log Likelihood  -3417.879   -3356.028&#10;AIC                   6863.758  6754.056&#10;BIC                   6958.156  6895.653&#10;PCP                     0.702   0.697&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2012-12-09T22:59:36.050" FavoriteCount="1" Id="45532" LastActivityDate="2012-12-15T22:31:04.400" LastEditDate="2012-12-10T00:36:39.760" LastEditorUserId="13378" OwnerUserId="13378" PostTypeId="1" Score="3" Tags="&lt;statistical-significance&gt;&lt;modeling&gt;&lt;predictive-models&gt;&lt;interpretation&gt;" Title="Interpret statistically significant betas with poorly performing models" ViewCount="160" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I am writing a manuscript using an experimental design which predicts interactions between 1 continuous variable and multiple dichotomous variables, all predicting a continuous variable. As is traditional in experimental design, I have used ANCOVAs to analyze the data, but am concerned about the inability to specifically test the interaction between the covariate and the dichotomous variables. &lt;/p&gt;&#10;&#10;&lt;p&gt;Also, while I am expecting only a small amount of variance to be explained, there is only one significant finding among the multiple predictors and interactions. I suspect the non-significant findings may be due to the small amount of explained variance in the DV, which when spread across multiple predictors in the model is insufficient to distinguish between them. &lt;/p&gt;&#10;&#10;&lt;p&gt;A colleague has suggested that the best option to deal with both concerns is to do four 2 or 3 stage hierarchical regressions with dummy variables, i.e. each regression would comprise: &#10;stage 1: one dummy variable&#10;stage 2: add the continuous variable and the interaction between the two.&#10;After running these analyses, any significant predictors and interactions could be combined into a single hierarchical regression.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am unable to find a precedent for this unusual procedure but interestingly, it reveals a number of significant results, albeit explaining a very small amount of variance. &lt;/p&gt;&#10;&#10;&lt;p&gt;Is this a reasonable procedure to follow, given the requirements of my variables? &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-12-10T07:54:37.947" FavoriteCount="1" Id="45559" LastActivityDate="2012-12-10T21:52:33.493" LastEditDate="2012-12-10T21:52:33.493" LastEditorUserId="88" OwnerUserId="17668" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;multiple-regression&gt;&lt;categorical-data&gt;&lt;ancova&gt;" Title="Should I use ANCOVA or multiple regression with dummy variables?" ViewCount="1089" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I have used a DCC Garch model to estimate the co-movement between 2 indices using the following command in Stata:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;mgarch dcc (X Y = , noconstant), arch(1) garch(1) constraints(1 2)&#10;predict H*, variance&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;After the variance prediction I get a column with the variances per time unit. My question is how to transform the variances to the correlations per time unit?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-12-10T11:05:20.763" FavoriteCount="1" Id="45572" LastActivityDate="2014-05-10T09:57:28.100" LastEditDate="2013-07-15T13:17:55.350" LastEditorUserId="13091" OwnerUserId="16298" PostTypeId="1" Score="2" Tags="&lt;correlation&gt;&lt;variance&gt;&lt;garch&gt;" Title="Estimating correlation with DCC GARCH" ViewCount="701" />
  <row AnswerCount="0" Body="&lt;p&gt;Two common methods of testing whether a time series is stationary are the KPSS and ADF tests. If my understanding is correct, these tests essentially work by measuring the residuals of fitting the time-series to an autoregressive model which is linear.&lt;/p&gt;&#10;&#10;&lt;p&gt;So my question is this, if the time series is possibly of a non-linear nature are the results of the above tests still valid?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-10T11:36:38.787" Id="45578" LastActivityDate="2012-12-10T21:52:08.157" LastEditDate="2012-12-10T21:52:08.157" LastEditorUserId="88" OwnerUserId="16574" PostTypeId="1" Score="4" Tags="&lt;time-series&gt;&lt;hypothesis-testing&gt;&lt;stationarity&gt;&lt;nonlinear&gt;" Title="Unit root tests and stationarity" ViewCount="221" />
  
  
  <row AcceptedAnswerId="45597" AnswerCount="1" Body="&lt;p&gt;I have defined an distance measure based on some properties of points. But I'm not even sure that it corresponds to a valid distance in some vector space. Is this a necessary condition for clustering ? If yes, how do I check that it's a valid distance in some vector space. Just like Mercer's theorem can verify that the kernel is an valid cross product in some valid vector space, are there any such tests for distance ? &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-12-10T15:33:34.157" Id="45594" LastActivityDate="2012-12-10T21:56:58.277" LastEditDate="2012-12-10T15:44:19.783" LastEditorUserId="12946" OwnerUserId="12946" PostTypeId="1" Score="2" Tags="&lt;machine-learning&gt;&lt;clustering&gt;&lt;unsupervised-learning&gt;" Title="Is it necessary for a distance measure used in clustering to correspond to some valid vector space?" ViewCount="140" />
  <row AnswerCount="1" Body="&lt;p&gt;I'm trying to figure out a way for calculating price optimization in a commerce environment. In other words, I'm trying to analyze how a company can increase revenue and profitability by analyzing costs, customer buying behaviors, competitive activity, demand signals and historic data. &lt;/p&gt;&#10;&#10;&lt;p&gt;How can I come up with a strong algorithm to do this? Performance is not an issue since I'm looking to do analysis in a big data environment. &lt;/p&gt;&#10;" ClosedDate="2012-12-11T17:42:44.290" CommentCount="4" CreationDate="2012-12-10T16:35:16.163" Id="45596" LastActivityDate="2012-12-11T16:10:06.737" LastEditDate="2012-12-11T16:09:54.413" LastEditorUserId="3826" OwnerUserId="17679" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;optimization&gt;&lt;big-data&gt;" Title="Algorithm for price optimization" ViewCount="628" />
  
  <row Body="&lt;p&gt;Look closely at the actual algorithms. There is no general rule.&lt;/p&gt;&#10;&#10;&lt;p&gt;Some will require metric properties, other just assume you have some dissimilarity, and can trivially be rewritten to use similarity measures instead.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example DBSCAN (see &quot;Generalized DBSCAN&quot;) doesn't actually use the distances, but is only interested in a binary threshold decision to discern &quot;near&quot; and &quot;far&quot; objects. Metric properties allow the algorithm to run faster, by performing this selection efficiently.&lt;/p&gt;&#10;&#10;&lt;p&gt;k-means on the other hand is actually not even using distance, but as it tries to minimize &lt;em&gt;variance&lt;/em&gt;, it assigns each objec to the mean with the smalles &quot;sum of squared deviation&quot;. And the sum of squared deviations is the squared Euclidean distances. As taking the root of this value, this means each object is assigned to the closest mean by Euclidean distance. It is not mathematically correct to use other distances with k-means (although it may work, at least if the &lt;em&gt;mean&lt;/em&gt; function minimizes the distances; otherwise k-means may no longer converge!) - the reason why in k-means we assign points to the nearest mean is &lt;em&gt;not&lt;/em&gt; to minimize distances, it is to minimize the total sum of variances. This ensures convergence: reassignment reduces variances, and recomputing the means also reduces variances. As there are only a finite number of assignments, we must at some point stop.&lt;/p&gt;&#10;&#10;&lt;p&gt;On a side note, try to approach clustering from the &quot;knowledge discovery&quot; point of view, not from &quot;learning&quot;. You want to discover something new with clustering, not reproduce something you already know; so it is pretty much the exact opposite of learning labels.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-12-10T16:38:35.677" Id="45597" LastActivityDate="2012-12-10T21:56:58.277" LastEditDate="2012-12-10T21:56:58.277" LastEditorUserId="7828" OwnerUserId="7828" ParentId="45594" PostTypeId="2" Score="0" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;Suppose I have a conditional discrete distribution $P_{M|A}(m,a)$. If I take sum over $m$, i.e. $\sum_{m}P_{M|A}(m,a)$, do I get $f_A(a)$, where $f_A(a)$ is the pmf of the random variable A?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-10T21:18:17.373" FavoriteCount="1" Id="45615" LastActivityDate="2013-05-10T02:03:08.703" LastEditDate="2012-12-10T21:54:58.887" LastEditorUserId="7290" OwnerUserId="17688" PostTypeId="1" Score="0" Tags="&lt;probability&gt;&lt;discrete-data&gt;&lt;marginal&gt;" Title="Marginalizing a conditional distribution" ViewCount="818" />
  <row Body="&lt;p&gt;Change your mean to something smaller than 400 like 0.01 to see the time series more clearly. Then try the following code with rho &amp;lt;- 0.1 and rho &amp;lt;- 0.9 to see the effect of rho.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;rho &amp;lt;- 0.1&#10;#rho &amp;lt;- 0.9&#10;mu &amp;lt;- c(0.01,0.01)&#10;theta &amp;lt;- c(0.1,0.1)&#10;d &amp;lt;- ts(matrix(0,ncol=2,nrow=1001))&#10;e &amp;lt;- ts(rmvnorm(1001,sigma=cbind(c(400,rho*400),c(rho*400,400))))&#10;for(i in 2:1001)&#10; d[i,] &amp;lt;- mu + d[i-1,] - theta*(e[i-1,]+e[i,])&#10;&#10;par(mfrow=c(1,2))&#10;plot(d[,1]);plot(d[,2])&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Here are some plots:&#10;&lt;img src=&quot;http://i.stack.imgur.com/FzYAD.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-10T21:25:28.617" Id="45617" LastActivityDate="2012-12-10T22:16:50.103" LastEditDate="2012-12-10T22:16:50.103" LastEditorUserId="13138" OwnerUserId="13138" ParentId="44974" PostTypeId="2" Score="0" />
  
  
  <row Body="&lt;p&gt;You could set it up as a state space model and use the Kalman filter to account for time-varying regression parameters.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-11T00:29:29.333" Id="45626" LastActivityDate="2012-12-11T00:29:29.333" OwnerUserId="11623" ParentId="45599" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;Doesn't this just mean to put your available covariates into your model and test for significant effects? Of course it depends on whether you have an accelerated failure time or proportional hazard model, as to the direction of the significant effects, but that's how I read it.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-12-11T02:47:08.630" Id="45633" LastActivityDate="2012-12-11T02:47:08.630" OwnerUserId="8762" ParentId="45630" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;&lt;em&gt;(I'll try what I thought would be the most typical kind of answer.)&lt;/em&gt;  &lt;/p&gt;&#10;&#10;&lt;p&gt;Let's say you have a situation where there are several variables and one response, and you know a good deal about how one of the variables ought to be related to the response, but not as much about the others.  &lt;/p&gt;&#10;&#10;&lt;p&gt;In a situation like this, if you were to run a standard multiple regression analysis, that prior knowledge would not be taken into account.  A meta-analysis might be conducted afterwards, which might be interesting in shedding light on whether the current result was consistent with the other findings and might allow a slightly more precise estimate (by including the prior knowledge at that point).  But that approach wouldn't allow what was known about that variable to influence the estimates of the other variables.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Another option is that it would be possible to code, and optimize over, your own function that fixes the relationship with the variable in question, and finds parameter values for the other variables that maximize the likelihood of the data given that restriction.  The problem here is that whereas the first option does not adequately constrain the beta estimate, this approach over-constrains it.  &lt;/p&gt;&#10;&#10;&lt;p&gt;It may be possible to jury-rig some algorithm that would address the situation more appropriately, situations like this seem like ideal candidates for Bayesian analysis.  Anyone not dogmatically opposed to the Bayesian approach ought to be willing to try it in cases like this.  &lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2012-12-11T03:48:22.383" CreationDate="2012-12-11T03:48:22.383" Id="45635" LastActivityDate="2012-12-11T03:48:22.383" OwnerUserId="7290" ParentId="41394" PostTypeId="2" Score="3" />
  <row AnswerCount="0" Body="&lt;p&gt;In my experience with dealing with multicollinearity, often removing one collinear variable from the model results in the other collinear variable(s) becoming significant (assuming that all the collinear variables are significantly correlated with the dependent variable bivariately. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, I've recently encountered a situation where I have a model shown below in which $X_2$ and $X_3$ are highly correlated ($r &amp;gt; 0.95$) and the tolerance scores for the two variables are below $0.1$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;Y = \beta_0\ + \beta_1X_1\ + \beta_2X_2\ + \beta_3X_3\ +\beta_4X_1X_2\ +\beta_5X_1X_3\&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;*&lt;em&gt;all variables are continuous.&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The results of the regression model show that all 5 slopes are significant (3 slopes for the main effects and 2 slopes for the interactions). One of the possible solutions is to remove one of the highly collinear predictors. If I remove one of them - say $X_2$, I get a new model as shown below.&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;Y = \beta_0^\prime + \beta_1^\prime X_1\ + \beta_3^\prime X_3\ +\beta_5^\prime X_1X_3\&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;After $X_2$ is removed, while $\beta_0^\prime$, $\beta_1^\prime$, and $\beta_3^\prime$ are significant, crucially, $\beta_5^\prime$, the slope for the interaction term, is no longer significant. The same thing happens if I try to remove $X_3$. So I wonder what may be the reason that causes this pattern to arise and how it can be potentially dealt with. Thank you in advance!&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-12-11T05:48:06.317" FavoriteCount="1" Id="45637" LastActivityDate="2012-12-11T05:48:06.317" OwnerUserId="17698" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;interaction&gt;&lt;multicollinearity&gt;" Title="Dealing with multicollinearity when removing a highly collinear predictor reduces significance" ViewCount="683" />
  <row AcceptedAnswerId="45644" AnswerCount="2" Body="&lt;p&gt;I am reading the books about linear regression. There are some sentences about the L1 and L2 norm. I know them, just don't understand why L1 norm for sparse models. Can someone use give a simple explanation?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-11T07:25:01.253" FavoriteCount="11" Id="45643" LastActivityDate="2012-12-11T08:58:13.977" OwnerUserId="1803" PostTypeId="1" Score="18" Tags="&lt;regression&gt;" Title="Why L1 norm for sparse models" ViewCount="1984" />
  
  
  <row AcceptedAnswerId="45667" AnswerCount="2" Body="&lt;p&gt;Conditional on the fact that during the first t trials coin landed once on heads.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-12-11T12:55:24.257" FavoriteCount="0" Id="45660" LastActivityDate="2012-12-11T16:20:13.043" LastEditDate="2012-12-11T16:20:13.043" LastEditorUserId="919" OwnerUserId="17706" PostTypeId="1" Score="1" Tags="&lt;binomial&gt;&lt;conditional-probability&gt;" Title="What is the probability of tossing k heads in n trials of a fair coin?" ViewCount="210" />
  
  
  
  
  <row Body="&lt;p&gt;I know this is a late answer to your question, but I've recently come across a similar situation in my own research. Just out of curiosity, I did a parameter optimization experiment for the c-parameter in a linear svm, in which I wanted to compare the performance of svmlight and the Weka svm implementation. For both approaches, I was classifying the same textual data in 5x2 cross-validation, using binary feature modeling, and bag-of-words unigrams. My findings surprised me: svmlight consistently performed at least 0.10 better (in terms of area under the curve), compared to Weka! I bring this up here, because I think your question raises an interesting point that people don't always think about: not all implementations of svm are the same, and perhaps they should not be treated as such! I think a lot of people will try out the Weka svm on their data (either because it Weka is commonly used, or because it's easier than using a specific implementation like svmlight or libsvm), and falsely conclude, &quot;svms are not good for my data.&quot; As to why this is, I am as yet unclear. Perhaps there are differences in the quality of the code, or the optimization routines used in each? I'm going to look into this further!&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-12-11T17:53:28.810" Id="45678" LastActivityDate="2012-12-11T17:53:28.810" OwnerUserId="8580" ParentId="38139" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;Yes, you can generate random samples from $\{x_i\}_{i=1}^n$ where the associated probabilities $f(x_i)$ are known up to a multiplicative constant (the constant of integration (COI) using MCMC.  However, you do have to be able to calculate $f(x_i)$ (without the COI) for all $i$ should $x_i$ be proposed.  If your sample is large enough relative to $n$, you won't be saving yourself anything by not just calculating the COI by summing $f$ over all $i$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's an example in R that does this for $n = 10$, using an independence sampler for the proposal (which simplifies the calculation of the acceptance probability $\alpha$).  The proposal is for an index $j \in \{1, \dots, n\}$, although what is stored as the sampled value is the value of $x$ associated with the current index $i$ (if the proposal is rejected) or the accepted proposal $j$. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# Example x, fx&#10;x &amp;lt;- c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;,&quot;e&quot;,&quot;f&quot;,&quot;g&quot;,&quot;h&quot;,&quot;i&quot;,&quot;j&quot;)&#10;fx &amp;lt;- 0.01*(2^(0:9))   # Note this is unnormalized!&#10;&#10;# i is the &quot;current&quot; index&#10;# j is the &quot;proposal&quot; index&#10;# y is the output stream of samples from x&#10;&#10;# Initialization / allocation&#10;y &amp;lt;- rep(0, 10000)&#10;i &amp;lt;- sample(1:10,1)&#10;&#10;# MCMC (Metropolis-Hastings)&#10;u01 &amp;lt;- runif(length(y))&#10;for (k in 1:length(y)) {&#10;   j &amp;lt;- sample(1:10,1)  # Independence sampler&#10;   alpha &amp;lt;- fx[j] / fx[i]&#10;   if (u01[k] &amp;lt;= alpha) {&#10;      y[k] &amp;lt;- x[j]&#10;      i &amp;lt;- j&#10;   } else {&#10;      y[k] &amp;lt;- x[i]&#10;   }&#10;}&#10;&#10;# Compare sample frequencies to true probabilities&#10;SampleFreq &amp;lt;- table(y)/length(y)&#10;df &amp;lt;- data.frame(Value=names(SampleFreq), &#10;                 Sample.Freq=as.numeric(SampleFreq), &#10;                 True.Prob=fx/sum(fx))&#10;&amp;gt; df&#10;   Value Sample.Freq True.Prob&#10;1      a      0.0009 0.0009775&#10;2      b      0.0022 0.0019550&#10;3      c      0.0040 0.0039101&#10;4      d      0.0093 0.0078201&#10;5      e      0.0166 0.0156403&#10;6      f      0.0345 0.0312805&#10;7      g      0.0678 0.0625611&#10;8      h      0.1282 0.1251222&#10;9      i      0.2443 0.2502444&#10;10     j      0.4922 0.5004888&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2012-12-11T19:19:28.277" Id="45680" LastActivityDate="2012-12-11T19:19:28.277" OwnerUserId="7555" ParentId="45194" PostTypeId="2" Score="1" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;The randomForest implementation does not allow sampling beyond the number of observations, even when sampling with replacement.  Why is this?&lt;/p&gt;&#10;&#10;&lt;p&gt;Works fine:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;rf &amp;lt;- randomForest(Species ~ ., iris, sampsize=c(1, 1, 1), replace=TRUE)&#10;rf &amp;lt;- randomForest(Species ~ ., iris, sampsize=3, replace=TRUE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;What I want to do:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;rf &amp;lt;- randomForest(Species ~ ., iris, sampsize=c(51, 1, 1), replace=TRUE)&#10;Error in randomForest.default(m, y, ...) : &#10;  sampsize can not be larger than class frequency&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Similar error without stratified sample:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;rf &amp;lt;- randomForest(Species ~ ., iris, sampsize=151, replace=TRUE)&#10;Error in randomForest.default(m, y, ...) : sampsize too large&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Since I was expecting the method to take bootstrap samples when given replace=TRUE in both cases, I was not expecting this limit.&lt;/p&gt;&#10;&#10;&lt;p&gt;My objective is to use this with the stratified sampling option, in order to draw a sufficiently large sample from a relatively rare class.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-12-11T12:40:45.660" Id="45705" LastActivityDate="2013-01-29T03:43:27.603" OwnerDisplayName="cohoz" OwnerUserId="14519" PostTypeId="1" Score="6" Tags="&lt;r&gt;&lt;random-forest&gt;" Title="Sampling with replacement in R randomForest" ViewCount="1535" />
  <row Body="&lt;p&gt;First of all, what kind of data are you analyzing?  That could give you an idea of whether you should expect a spike at h=12. You are mostly correct in your reasoning about the look of an ACF for different ARMA models.  However, if you had an MA(12) process like&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;Y_t=\theta_{12}\epsilon_{t-12}+\epsilon_t&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;then you could have an ACF plot that is approx. zero for h=1,...,11, nonzero for h=12, and then approx. zero afterwards.&lt;/p&gt;&#10;&#10;&lt;p&gt;In your case, this doesn't look like anything worth modelling. Remember that if you test a hypothesis of $H_0$: the autocorrelation at lag $h$ is significantly different from zero at significance level $\alpha=0.10$, on average you expect to make a type I error 10% of the time.  This is probably one of those times.  &lt;/p&gt;&#10;&#10;&lt;p&gt;If you are curious, try fitting some MA(12) models and look at some model selction criterion (AIC, AICc, BIC, etc.) and see if any model gives a better fit than $Y_t=\epsilon_t$, $\epsilon_t \overset{iid}{\sim} N(0, \sigma^2)$. I am guessing no.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-12T04:37:59.943" Id="45711" LastActivityDate="2012-12-12T04:37:59.943" OwnerUserId="12518" ParentId="45702" PostTypeId="2" Score="0" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a question about Scientific Data Mining.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Do you know successful case studies of applying Data Mining / Machine Learning techniques in hydrodynamics?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;In general, does it make actually sense to try to apply DM/ML techniques to such deterministic systems as gas/fluid flows which are described by Navier-Stokes equations?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I guess that answer is 'no' in the case of very simple flows. But maybe it makes sense if we have a deal with complex data from turbulent/multiphase/... flows? &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;What could be the problem formulation in this area?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I will appreciate to your opinion, links to the papers and web-pages.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-12T11:07:49.943" Id="45733" LastActivityDate="2012-12-12T11:07:49.943" OwnerUserId="13464" PostTypeId="1" Score="3" Tags="&lt;machine-learning&gt;&lt;data-mining&gt;&lt;science&gt;" Title="Data Mining / ML applications in hydrodynamics?" ViewCount="89" />
  <row AnswerCount="1" Body="&lt;p&gt;First: I'm not well versed in statistics terminology so please forgive me - I'll try to be as verbose as possible with my problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;This is a problem which I've previously solved very naively. I'm looking to apply more standard statistical theory to this problem in an attempt to get more accurate results. Any help with validating my approach or (rejecting it) and methods I could use would be appreciated (even the correct terms).&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem is a commerce related one: I have a collection of products (say 100) which have a variety of attributes. For this example I'll pick 3 of them:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Discount Percentage (Current Price/RRP) * 100&lt;/li&gt;&#10;&lt;li&gt;Views (Number of times viewed)&lt;/li&gt;&#10;&lt;li&gt;Stock Level&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Using these attributes I wish to be able to configure various sorting rules in order to place these products into an order. The sorting rules are where the 'configuration' happens - but the statistical part of my problem comes before that.&lt;/p&gt;&#10;&#10;&lt;p&gt;In order to compare these attributes of products properly I need to:&lt;/p&gt;&#10;&#10;&lt;h2&gt;Standardize &amp;amp; Normalize.&lt;/h2&gt;&#10;&#10;&lt;p&gt;So the values and distribution of these attributes:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Discount Percentage: 0 to 100 with a distribution where most products have values of 0(50% of products), 20(25%), 30(10%), 50 (15%)&lt;/li&gt;&#10;&lt;li&gt;Views: 0 to 30000 with a fairly even distribution&lt;/li&gt;&#10;&lt;li&gt;Stock Level: 0 to 40 with a fairly even distribution (but different from the distribution for views)&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I take the entire population for each attribute, and using the max and min values I 'reset' each value to place in in a scale between 0 and 10 - where the maximum value is 0 and the minimum value is 10. For each member I work out if it is in the 0-10% percentile, 10-20 percentile etc. and give it a score of 1 to 10.&lt;/p&gt;&#10;&#10;&lt;p&gt;However I feel this is a very naive solution as it doesn't take into account the distribution. For some attributes I get 99 members of the population with a score of 10 and 1 member with a score of 0. I'd like a more even distribution where outliers cannot change the end result so drastically (is this called smoothing?). I &lt;em&gt;could&lt;/em&gt; do this after the step above - but too much data is destroyed by this point (imo) to do a good job here. Is there a technique for doing both these steps at the same time (does this have a name?).&lt;/p&gt;&#10;&#10;&lt;p&gt;Am I right in thinking that the normalization method for each attribute might end up being different? Should I just use trial and error on my data until I get to something that I deem satisfactory or can I consistently achieve a more equatable distribution without having to manually tweak this process? I'd like to have a range of scores between 0 and 1 (rather than 0 and 10).&lt;/p&gt;&#10;&#10;&lt;p&gt;I with probably end up using R to help me with this as by all accounts it seems to be the best tool for the job. I'm not sure if it's overkill for what I see as being a fairly simple problem - but the complexity involved in the 'correct' Normalisation process may be beyond what I can easily do in my scripting language. Before I jump in with that though I'd like to validate my assumptions/theory.&lt;/p&gt;&#10;&#10;&lt;h2&gt;Weighting&lt;/h2&gt;&#10;&#10;&lt;p&gt;Once the above steps have taken place I allow domain knowledge experts to configure the weighting of these various attributes in order to determine the sort order (for example they may want to say that the number of views is 3 times more important than then stock level, in which case I'll multiple the normalised 'views' figure by 3, and then add it to the others before sorting the entire population). I've read that sometimes this knowledge could be incorporated into the standardisation process - so that the figures themselves take into account their importance relative to other attributes. I don't want to do this.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the example above I've talked about a population of 100. My real population is between 1000 and 50000, but I still think this is small enough to be able to work with the whole set of data rather than taking a sample.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any pointers would be appreciated as I've spent hours reading and I feel now like I'm going round in circles.&lt;/p&gt;&#10;&#10;&lt;p&gt;-Thanks&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-12T12:26:34.607" Id="45737" LastActivityDate="2012-12-12T14:27:43.013" LastEditDate="2012-12-12T13:29:51.447" LastEditorUserId="17762" OwnerUserId="17762" PostTypeId="1" Score="1" Tags="&lt;standard-deviation&gt;&lt;normalization&gt;&lt;smoothing&gt;&lt;standardization&gt;" Title="Process for Standardising and Normalising data" ViewCount="700" />
  
  
  <row AcceptedAnswerId="45746" AnswerCount="2" Body="&lt;p&gt;I want to do survival analysis with a big data set.&lt;/p&gt;&#10;&#10;&lt;p&gt;The data collection started on &lt;code&gt;1997-01-01&lt;/code&gt; and is still continuing yet.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, episodes that started before &lt;code&gt;1997-01-01&lt;/code&gt; (and had an end date after &lt;code&gt;1997-01-01&lt;/code&gt; or no end date yet) are also in the database.&lt;/p&gt;&#10;&#10;&lt;p&gt;Therefore, although the official start of the data collection is &lt;code&gt;1997-01-01&lt;/code&gt; there is some data with a start date &lt;code&gt;&amp;lt; 1997-01-01&lt;/code&gt; in the database.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;[EDIT]&lt;/em&gt;&lt;br&gt;&#10;Episodes that started after &lt;code&gt;1997-01-01&lt;/code&gt; are also recorded. So what I know is the following:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;If start date is &lt;code&gt;&amp;gt; 1997-01-01&lt;/code&gt; and end date is &lt;code&gt;&amp;lt; today&lt;/code&gt;, there is no censoring and no truncation&lt;/li&gt;&#10;&lt;li&gt;If end date is &lt;code&gt;= today&lt;/code&gt; most likely censoring is the reason for this end date&lt;/li&gt;&#10;&lt;li&gt;If start date is &lt;code&gt;&amp;lt; 1997-01-01&lt;/code&gt; the observation is only inside my data set because end date is &lt;code&gt;&amp;gt;= 1997-01-01&lt;/code&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;What would be the propper way to handle these observations?&lt;/p&gt;&#10;&#10;&lt;p&gt;Would these observations cause a bias (then it would probably be better to remove them)?&lt;/p&gt;&#10;&#10;&lt;p&gt;Or should I simple leave these observations in my data set?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;[EDIT 2]&lt;/em&gt;&lt;br&gt;&#10;To be more specific about the problem: As the data collection started on &lt;code&gt;1997-01-01&lt;/code&gt; one would expect the maximum survival time of observations in the database would be (approximately) 15 years (we have December 2012 at the time of writing).&lt;/p&gt;&#10;&#10;&lt;p&gt;However, if episodes started before &lt;code&gt;1997-01-01&lt;/code&gt; and the event didn't take place yet, they would be longer than 15 years. This somehow &quot;feels&quot; strange - if one observation is &lt;code&gt;1995-01-01 - today&lt;/code&gt; it is included, if another observation is &lt;code&gt;1995-01-01 - 1996-12-31&lt;/code&gt; it is excluded.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-12-12T14:09:44.357" Id="45742" LastActivityDate="2014-04-26T17:05:32.440" LastEditDate="2012-12-12T15:47:47.640" LastEditorUserId="8595" OwnerUserId="8595" PostTypeId="1" Score="2" Tags="&lt;dataset&gt;&lt;survival&gt;&lt;bias&gt;" Title="Remove data starting before defined start date for survival analysis" ViewCount="206" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm currently applying to graduate school for a career that will be greatly enhanced if I have a firm grasp of what constitutes an excellent, good, mediocre, bad, etc. experiment and results. The vast majority of the material I expect to analyze will be medical journal articles.&lt;/p&gt;&#10;&#10;&lt;p&gt;While I have taken the requisite courses to evaluate the articles (stats, ethics, probability, experimental design, etc.) I haven't had such a course in several years. I am curious if anyone knows of a thorough book; a sort of quick-reference guide that will help me sort out the chaff when a clear interpretation isn't possible (i.e. - studies with only a few dozen participants, studies that are not blind, studies where the results seem well interpreted but the design itself is questionable).&lt;/p&gt;&#10;&#10;&lt;p&gt;It doesn't need to be &quot;How to Interpret p-Values 101&quot;, but something that would hopefully prevent me running to my old notes every time I forget what chi-squared represents in the context of an experiment.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-12-12T02:43:34.457" Id="45749" LastActivityDate="2012-12-13T17:07:06.567" OwnerDisplayName="MCM" PostTypeId="1" Score="2" Tags="&lt;references&gt;" Title="Recommended Text for Essential Experimental Statistics" ViewCount="88" />
  <row Body="&lt;ol&gt;&#10;&lt;li&gt;You can always take a larger sample.&lt;/li&gt;&#10;&lt;li&gt;The &quot;general&quot; rule of thumb is that you want at least 30 samples to be statistically reasonable, so yes a sample size of 18 is likely to be too small with a population of 800.&lt;/li&gt;&#10;&lt;li&gt;Your calculation is missing something as you should get a sample size around 89 for a population of 800 with 10% precision.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Yamane (1967) has a simplified formula for calculating sample size. A 95% confidence level and P = .05 are assumed.&lt;/p&gt;&#10;&#10;&lt;p&gt;$n=N/(1+N(e^2))$&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;n=800/(1+800(.10^2)) = 88.88&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2012-12-12T15:36:14.163" Id="45750" LastActivityDate="2012-12-12T18:12:38.153" LastEditDate="2012-12-12T18:12:38.153" LastEditorUserId="17315" OwnerUserId="17315" ParentId="45747" PostTypeId="2" Score="3" />
  
  
  
  <row AcceptedAnswerId="45849" AnswerCount="2" Body="&lt;p&gt;I have a set of data I need to model using the following:  &lt;/p&gt;&#10;&#10;&lt;p&gt;$$Y_i \sim N(\mu_i,\theta\mu^{2}_{i}) \quad \text{and}\quad \log \mu_i = \beta^{T}X_{i}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;($\theta\mu_i^2$ is the &lt;em&gt;variance&lt;/em&gt; of $Y_i$). I need to estimate the model parameters $\beta$ and $\theta$.  I am not sure how to go about that.  It was suggested to try to integrate out $\theta$ using joint MLE but once again I am not sure how to go about that.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I am also trying to implement this model in R, but as I am new to R and linear models I wasn't quite sure how to go about this. I tried:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;glm(Days ~ ., family=gaussian(link=&quot;log&quot;), data=quine,&#10;              start=c(log(mean(quine$Days)),0,0,0,0,0,0))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;But I am not sure if this is the right model to use.  Should I be looking into &lt;code&gt;nls&lt;/code&gt; or &lt;code&gt;gls&lt;/code&gt;?&lt;/p&gt;&#10;&#10;&lt;p&gt;Any suggestions would be appreciated.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Edit:  So thinking about this some more, would this be equivalent to a weighted poisson with weight $\frac{1}{\theta\mu}$.  So in R I would implement maybe as:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;glm(Days ~ ., family=poisson, data=quine,weights = 1/(theta*predict(model1,type=&quot;pear&quot;)))&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Where model1 is an un-weighted poisson.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I still don't know how to estimate theta though.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-12-12T17:57:18.820" FavoriteCount="1" Id="45765" LastActivityDate="2012-12-17T16:22:41.873" LastEditDate="2012-12-13T16:20:30.393" LastEditorUserId="14705" OwnerUserId="14705" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;regression&gt;&lt;generalized-linear-model&gt;" Title="Parameter estimation for a model defined by $Y_i \sim N(\mu_i,\theta\mu^{2}_{i})$" ViewCount="301" />
  <row Body="&lt;p&gt;You may want to look at survival analysis, with which you can estimate the survival function (the probability that the time of failure is greater than a specific time) and the hazard function (the instantaneous probability that a unit will fail, given it has not experienced failure so far). With most survival analysis approaches you can enter time-invariant and time-varying predictors.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are a variety of different survival analysis approaches including the semi-parametric &lt;a href=&quot;http://en.wikipedia.org/wiki/Proportional_hazards_models&quot; rel=&quot;nofollow&quot;&gt;Cox proportional hazards model&lt;/a&gt; (a.k.a. Cox regression) and parametric models. Cox regression doesn't require you to specify the underlying base hazard function but you might find that you need a parametric model to properly capture the failure patterns in your data. Sometimes parametric accelerated failure time models are appropriate, where the rate of failure increases over time. &lt;/p&gt;&#10;&#10;&lt;p&gt;You might try starting with Cox regression since it is the simplest to use and check how well you can predict failure on a holdout test set. I suspect you may have better results with some sort of survival analysis that explicitly takes into account time and censoring (pumps that have not failed yet) than with trying to turn this into a non-time-based classification problem. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-12T18:01:36.483" Id="45766" LastActivityDate="2012-12-13T20:41:28.367" LastEditDate="2012-12-13T20:41:28.367" LastEditorUserId="1434" OwnerUserId="8676" ParentId="45670" PostTypeId="2" Score="3" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I need to fit a gamma distribution that is shifted to the left and truncated at zero (so that for example, my data may only come from the right tail of the full distribution, and I don't have any observations less than zero).  &lt;/p&gt;&#10;&#10;&lt;p&gt;I can find the alpha and beta parameters to fit a regular old two parameter gamma no problem by MLE, but I can't find a good reference on how to fit the parameters for the shifted (and truncated) gamma case.  In the shifted-truncated Gamma case I have now three parameters: alpha, beta, and lambda, where lambda is the shifting parameter.&lt;/p&gt;&#10;&#10;&lt;p&gt;Does anyone knows of a paper that shows how to fit a shifted gamma using MLE?  I'd also be open to hear about other methods for fitting such distribution...&lt;/p&gt;&#10;&#10;&lt;p&gt;Suggestions are welcome!&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2012-12-12T19:45:58.550" FavoriteCount="1" Id="45782" LastActivityDate="2012-12-12T23:33:46.170" LastEditDate="2012-12-12T23:33:46.170" LastEditorUserId="9529" OwnerUserId="9529" PostTypeId="1" Score="0" Tags="&lt;maximum-likelihood&gt;&lt;curve-fitting&gt;&lt;fitting&gt;&lt;gamma-distribution&gt;" Title="MLE for Gamma Shifted Distribution" ViewCount="415" />
  <row Body="&lt;p&gt;First, here is your intuition illustrated in a simplified time series where the weekend is readily apparent in the ACF:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Pdngj.png&quot; alt=&quot;enter image description here&quot;&gt;&#10;However, this expected ACF pattern can be masked when the data have some trend:&#10;&lt;img src=&quot;http://i.stack.imgur.com/WuX4W.png&quot; alt=&quot;enter image description here&quot;&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/T6RSt.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;A solution (if this is a problem) is to estimate and control for the trend when determining the seasonality.  &lt;/p&gt;&#10;&#10;&lt;p&gt;R code that produced these plots follows:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# fourteen repeating 'weeks' of five zeroes and two ones&#10;weekendeffect &amp;lt;- rep(c(rep(0,5),1,1),times=14)&#10;&#10;plot(weekendeffect,&#10;    main=&quot;Weekly pattern of five zeroes &amp;amp; two ones&quot;,&#10;    xlab=&quot;Time&quot;, ylab=&quot;Value&quot;)  &#10;acf(weekendeffect, main=&quot;ACF&quot;)&#10;&#10;# add steady trend &#10;dailydrift &amp;lt;- 0.05&#10;drift &amp;lt;- seq(from=dailydrift, to=length(weekendeffect)*dailydrift, &#10;   by=dailydrift)&#10;driftingtimeseries &amp;lt;- drift + weekendeffect &#10;&#10;plot(driftingtimeseries,&#10;    main=c(&quot;Weekly pattern with daily drift of&quot;,dailydrift),&#10;    xlab=&quot;Time&quot;, ylab=&quot;Value&quot;)  &#10;acf(driftingtimeseries, main=c(&quot;ACF with daily drift of&quot;,dailydrift))&#10;&#10;&#10;# add larger trend &#10;dailydrift &amp;lt;- 0.1&#10;drift &amp;lt;- seq(from=dailydrift, to=length(weekendeffect)*dailydrift, &#10;   by=dailydrift)&#10;driftingtimeseries &amp;lt;- drift + weekendeffect &#10;&#10;plot(driftingtimeseries,&#10;    main=c(&quot;Weekly pattern with daily drift of&quot;,dailydrift),&#10;    xlab=&quot;Time&quot;, ylab=&quot;value&quot;)  &#10;acf(driftingtimeseries, main=c(&quot;ACF with daily drift of&quot;,dailydrift))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2012-12-12T19:47:06.533" Id="45783" LastActivityDate="2012-12-13T02:23:07.070" LastEditDate="2012-12-13T02:23:07.070" LastEditorUserId="16640" OwnerUserId="16640" ParentId="45191" PostTypeId="2" Score="12" />
  
  <row Body="&lt;p&gt;&lt;strong&gt;The challenge of this question lies in the large amount of data.&lt;/strong&gt;  Let's approach a solution by making a succession of approximations, each faster than the last, until a solution becomes practicable.  That solution indeed is a set of regression problems, as indicated in the question.  A method to speed it up is proposed and implemented, resulting in a four order-of-magnitude speed increase.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;The results obtained with simulated (yet realistic) data should be sobering&lt;/strong&gt;: they point to inherent limitations in the accuracy that can be expected when estimating $2N$ parameters from approximately $2N\times 10\times k$ observations of extremely rare outcomes.&lt;/p&gt;&#10;&#10;&lt;h3&gt;Theoretically ideal solutions&lt;/h3&gt;&#10;&#10;&lt;p&gt;Use Maximum Likelihood or Bayesian procedures to estimate the $2N$ parameters $f_1, \ldots, f_N$ and $g_1, \ldots, g_N$.  But we don't want to have to optimize a likelihood that depends on $2N \approx$ two billion linked variables!&lt;/p&gt;&#10;&#10;&lt;h3&gt;An excellent approximation&lt;/h3&gt;&#10;&#10;&lt;p&gt;Sample $j$, $j=1, 2, \ldots, k$, is a realization $(x_1^{(j)}, \ldots, x_N^{(j)})$ of a random variable $(X_1^{(j)}, X_2^{(j)}, \ldots, X_N^{(j)})$ which is a mixture of two multinomial distributions $F$ (with probabilities $f_i$) and $G$ (with probabilities $g_i$) in &lt;em&gt;known&lt;/em&gt; proportions: let $\alpha_j$ be the proportion of $F$,whence $1-\alpha_j$ is the proportion of $G$.   With $N$ so large, the $X_i$ are for all practical purposes independent and they have Poisson distributions.  This splits the problem into a billion subproblems, one for each index $i$:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Given that $X_i^{(j)}$ has Poisson distribution with expectation $\alpha_j f_i + (1-\alpha_j) g_i$, $j=1, \ldots, k$, estimate the &lt;em&gt;two&lt;/em&gt; parameters $f_i$ and $g_i$.&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In more familiar notation this says&lt;/p&gt;&#10;&#10;&lt;p&gt;$$X_i^{(j)} \sim\text{Poisson}(\mu_i^{(j)}),\quad \mu_i^{(j)} = \beta_0 + \beta_1 \alpha_j$$&lt;/p&gt;&#10;&#10;&lt;p&gt;with $\beta_0 = g_i$ and $\beta_1 = f_i - g_i$.  This is a &lt;a href=&quot;http://en.wikipedia.org/wiki/Generalized_linear_model&quot; rel=&quot;nofollow&quot;&gt;Generalized Linear Model&lt;/a&gt; (GLM) for $X_i$ with a Poisson distribution and identity link.  To make the estimates into an actual probability distribution, we can clamp them to the interval $[0,1]$ and rescale them all to make the $\hat{f_i}$ and the $\hat{g_i}$ separately sum to unity.&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem with this is that running a billion GLM estimates would take a full week of computing (in &lt;code&gt;R&lt;/code&gt;).&lt;/p&gt;&#10;&#10;&lt;h3&gt;Speeding things up&lt;/h3&gt;&#10;&#10;&lt;p&gt;We can approximate a Poisson regression by means of a Weighted Least Squares regression: because the variance of a Poisson$(\mu)$ variate equals $\mu$, we can use the observed values $x_i^{(j)}$ as weights.&lt;/p&gt;&#10;&#10;&lt;p&gt;Unfortunately, because the weights vary with $i$, this still requires conducting a billion separate linear regressions. An &lt;code&gt;R&lt;/code&gt; implementation goes only twice as fast as the GLM solution.&lt;/p&gt;&#10;&#10;&lt;h3&gt;Another approximation&lt;/h3&gt;&#10;&#10;&lt;p&gt;If we drop the weighting, the solution is not quite as good as before, but it works pretty well nevertheless.  Without weights, notice that every one of the regressions uses the same explanatory variable $\alpha$. Because the Ordinary Least Squares estimator is linear, we can do this regression once and for all and then each estimate is just a simple, fast, linear combination of the data.  &lt;strong&gt;This is extremely fast&lt;/strong&gt;: whereas the original GLM solution requires 3 to 4 seconds for $N=1000$, the same machine can perform $2\times 10^7$ OLS regressions in the same time.  Extrapolating this performance suggests cases where $N$ is a billion and $k$ is small can be computed in less than ten minutes (using a single core on a PC: doing this on a GPU could easily drop the time to less than one second plus I/O overhead).&lt;/p&gt;&#10;&#10;&lt;h3&gt;Code&lt;/h3&gt;&#10;&#10;&lt;p&gt;This working &lt;code&gt;R&lt;/code&gt; code illustrates the ideas and compares the GLM to the OLS results.  First we need to generate some potentially large datasets with multinomial distributions, after stipulating values for $N$, $k$, the totals in the datasets, and $\alpha_j$:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(17)                        # Makes output reproducible&#10;n &amp;lt;- 10^3                           # Number of indexes to f and g&#10;k &amp;lt;- 3                              # Number of samples&#10;alpha = seq(0.3, 0.7, length.out=k) # Sample proportions (of f)&#10;totals &amp;lt;- rep(10 * n, k)            # Sample totals, per sample&#10;&#10;# Parameter and data generation&#10;f &amp;lt;- rgamma(n,1,1); f &amp;lt;- f / sum(f)&#10;g &amp;lt;- rgamma(n,1,1); g &amp;lt;- g / sum(g)&#10;x &amp;lt;- sapply(1:k, function(i) rmultinom(1, totals[i], alpha[i]*f + (1-alpha[i])*g))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Here is &lt;code&gt;R&lt;/code&gt;'s implementation of the GLM solution.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;system.time({&#10;  raw &amp;lt;- apply(x, 1, function(y) {&#10;    coef &amp;lt;- glm(y ~ alpha, family=poisson(link=&quot;identity&quot;), start=c(1/n, 1/n))$coeff; &#10;    cumsum(coef) # Assemble hat(beta_0) and hat(beta_1) into hat(f) and hat(g)&#10;  });&#10;  z &amp;lt;- apply(raw, 1, function(y) {x &amp;lt;- pmax(0, y); x / sum(x)}) # Normalize the fits&#10;})&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;blockquote&gt;&#10;&lt;pre&gt;&lt;code&gt;  user  system elapsed &#10;  3.44    0.03    3.47 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;  &#10;  &lt;p&gt;There were 50 or more warnings (use warnings() to see the first 50)&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;The warnings are expected when using a linear link for Poisson regression; as we will see, they do little harm.&lt;/p&gt;&#10;&#10;&lt;p&gt;The OLS solution requires precomputation of the regression solution, implemented by &lt;code&gt;lm.init&lt;/code&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lm.init &amp;lt;- function(alpha, ...) {&#10;  k &amp;lt;- length(alpha)&#10;  i = matrix(c(rep(c(1, rep(0,k)), k-1), 1), k)&#10;  apply(i, 2, function(y) lm(y ~ alpha, ...)$coeff)&#10;}&#10;system.time({&#10;  raw.ols &amp;lt;- x %*% (t(lm.init(alpha)) %*% matrix(c(1,0,1,1), 2))&#10;  z.ols &amp;lt;- apply(raw.ols, 2, function(y) {x &amp;lt;- pmax(0, y); x / sum(x)})&#10;})&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The timing for $N=1000$ is $0$ seconds; for $N=10^7$ it is $1.6$ seconds.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's compare the estimates to the true values for the GLM and OLS cases:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;par(mfrow=c(2,2))&#10;plot(g, z[,1], ylab=&quot;g.hat&quot;, main=&quot;GLM&quot;); lines(c(0,max(g)), c(0,max(g)), col=&quot;Red&quot;)&#10;plot(f, z[,2], ylab=&quot;f.hat&quot;, main=&quot;GLM&quot;); lines(c(0,max(f)), c(0,max(f)), col=&quot;Red&quot;)&#10;plot(g, z.ols[,1], ylab=&quot;g.hat&quot;, main=&quot;OLS&quot;); lines(c(0,max(g)), c(0,max(g)), col=&quot;Red&quot;)&#10;plot(f, z.ols[,2], ylab=&quot;f.hat&quot;, main=&quot;OLS&quot;); lines(c(0,max(f)), c(0,max(f)), col=&quot;Red&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/L2XKR.png&quot; alt=&quot;Scatterplots of comparisons&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Although ideally the estimates would track along the diagonal lines of equality (indicating the points where no error is made), this amount of scatter is unsurprising: in the simulation the counts averaged only $10$ with a variance around $10$, the proportions $\alpha_j$ did not change much (they ranged only from $0.3$ to $0.7$, and only $k=3$ samples were observed.  The symmetric scatter around the lines of equality demonstrates that these estimates are unbiased.&lt;/p&gt;&#10;&#10;&lt;p&gt;As another check, we may compare the GLM and OLS results.  These are shown on square-root scales to resolve what happens with the tiniest parameters.  The GLM estimates are plotted on the horizontal axes and the OLS estimates on the vertical axes:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;plot(sqrt(z[,1]), sqrt(z.ols[,1]), main=&quot;g&quot;)&#10;plot(sqrt(z[,2]), sqrt(z.ols[,2]), main=&quot;f&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/qbjvb.png&quot; alt=&quot;Scatterplots&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The two methods are actually pretty close: OLS is not a bad approximation to Poisson regression. The approximation is worst for the small probabilities, as expected.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-12-12T20:40:34.973" Id="45789" LastActivityDate="2012-12-12T20:40:34.973" OwnerUserId="919" ParentId="45546" PostTypeId="2" Score="4" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Suppose I have census (count) data for a migratory species passing through a set of locations, at irregular intervals -- $n(i,t)$ is how many individuals were present at location $i$ on day $t$ for an irregularly spaced, and different, set of dates $t_1$, ... , $t_n$, for each location $i$.  For the first part of each year, none are present ($n(t)=0$), then the numbers gradually increase as the peak of migration approaches, then decrease as it passes.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to describe the date and duration of the onset of migration (i.e. before the peak), in each location, in a way that is reasonably robust to the haphazard temporal sampling, and allows comparison of these statistics between locations.&lt;/p&gt;&#10;&#10;&lt;p&gt;A couple of robust statistics?  I can't seem to find others doing this in the literature -- even just some key phrases to search for would help.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-13T03:11:36.690" Id="45812" LastActivityDate="2012-12-13T03:11:36.690" OwnerUserId="13419" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;descriptive-statistics&gt;" Title="Estimation of temporal onset and duration of growth from irregular count data" ViewCount="24" />
  <row Body="&lt;p&gt;Keeping in mind the comment above... if $X$ is a random variable, then the maximal variance distribution has probability $1-p$ at 0 and probability $p$ at 20; given the mean is 0.005, we can solve to get $p=1/4000$; this has variance $400 p (1-p) = 0.099975$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The case where $X$ is a dataset doesn't have a closed-form solution, but is easy to solve for a particular case: begin with any configuration of the data that gives the right mean; then since $(a+1)^2+(b-1)^2 = x^2 + y^2 + 2(x-y+1)$ is positive if $x&amp;gt;y$, you can iteratively pick any pair of datapoints that are not at the endpoints, and move them each one unit apart.  Given a particular sample size, you can work out what the distribution is from this.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-13T03:31:36.010" Id="45813" LastActivityDate="2012-12-13T03:31:36.010" OwnerUserId="13419" ParentId="45798" PostTypeId="2" Score="2" />
  
  
  
  <row Body="&lt;p&gt;I have asked this question on the r-sig-mixed-models mailing list and below is the answer from Douglas Bates:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Stéphane,&lt;/p&gt;&#10;  &#10;  &lt;p&gt;You are correct that there should not be an attempt to fit such a&#10;  model.  In theory the deviance function is degenerate (a singular&#10;  Hessian matrix at every parameter value).  In practice it will not be&#10;  exactly degenerate and the optimizer will attempt to locate a minimum&#10;  determined by round-off error.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;Again, it is probably good to throw an error in this case and to warn&#10;  when the number of levels in a grouping factor for random effects&#10;  term(s) is small (say, &amp;lt; 5).  I'll add an issue on the github&#10;  repository, &lt;a href=&quot;https://github.com/lme4/lme4/issues&quot; rel=&quot;nofollow&quot;&gt;https://github.com/lme4/lme4/issues&lt;/a&gt;, regarding this.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;By the way, the reason for printing out the number of levels in the&#10;  factors as part of the random-effects summary is so that the user will&#10;  notice when these are small, which usually means a mistake in the&#10;  model formula.  Of course, it requires that the user look at that part&#10;  of the summary and anyone who has used SAS is accustomed to ignoring&#10;  large parts of the output.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;Anyway, the issue is now recorded as&#10;  &lt;a href=&quot;https://github.com/lme4/lme4/issues/24&quot; rel=&quot;nofollow&quot;&gt;https://github.com/lme4/lme4/issues/24&lt;/a&gt;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="1" CreationDate="2012-12-13T12:10:47.533" Id="45839" LastActivityDate="2012-12-13T12:10:47.533" OwnerUserId="8402" ParentId="41510" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;You can use numerical values quite easily. In the term P(Feature|scam=Yes) you could put a gaussian distribution or any other empirical distribution from training data (for e.g. sort the data, create a function that returns the percentile of the given input numerical value). &lt;a href=&quot;http://bayesianthink.blogspot.com/2012/12/the-naive-bayesian-approach-to-machine.html&quot; rel=&quot;nofollow&quot;&gt;Here&lt;/a&gt; is a write up describing that&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-13T16:50:37.650" Id="45853" LastActivityDate="2012-12-13T16:50:37.650" OwnerUserId="13516" ParentId="43347" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;OK another, slightly different take on this:&lt;/p&gt;&#10;&#10;&lt;p&gt;A first basic problem is the phrase &quot;due to [random] chance&quot;.  The idea of unspecified 'chance' comes naturally to students but it is hazardous for thinking clearly about uncertainty and catastrophic for doing sensible statistics.  With something like a sequence of coin flips it is easy to assume that 'chance' is described by the Binomial setup with a probability of 0.5.  There is a certain naturalness to it for sure, but from a statistical point of view it's not more natural than assuming 0.6 or something else. And for other less 'obvious' examples, e.g. involving real parameters it's utterly unhelpful to think about what 'chance' would look like.  &lt;/p&gt;&#10;&#10;&lt;p&gt;With respect to the question, the key idea is understanding what &lt;em&gt;sort&lt;/em&gt; of 'chance' is described by H0, i.e. what actual likelihood/DGP H0 names.  Once that concept is in place, students finally stop talking about things happening 'by chance', and start asking what H0 actually is.  (They also figure out that things can be consistent with a rather wide variety of Hs so they get a head start on confidence intervals, via inverted tests). &lt;/p&gt;&#10;&#10;&lt;p&gt;The second problem is that if you're on the way to Fisher's definition of p-values, you should (imho) always explain it first in terms of the data's consistency with H0 because the point of p is to see that, not to interpret the tail area as some sort of 'chance' activity, (or frankly to interpret it at all).  This is purely a matter of rhetorical emphasis, obviously, but it seems to help.&lt;/p&gt;&#10;&#10;&lt;p&gt;In short, the harm is that this way of describing things will not generalise to any non-trivial model they might subsequently try to think about.  At worst it may just add to sense of mystery that the study of statistics already generates in the sorts of people such bowdlerised descriptions are aimed at.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-13T21:20:19.617" Id="45872" LastActivityDate="2012-12-14T12:00:04.247" LastEditDate="2012-12-14T12:00:04.247" LastEditorUserId="1739" OwnerUserId="1739" ParentId="16939" PostTypeId="2" Score="3" />
  
  
  <row Body="&lt;p&gt;&lt;em&gt;I don't have the Fleiss book at hand, so all this is IIRC.&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Answering @JohnMoeller's question in the comments for the moment: the original question is IMHO unanswerable as it is.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;So suppose that I have 30 samples, and I test c1 and c2 on each sample, and  record the accuracy for each on each sample. &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;doing this, you end up with a 2 x 2 contingency table giving classifier 1 correct/wrong against classifier 2 correct/wrong. Which is the starting point for &lt;a href=&quot;http://en.wikipedia.org/wiki/McNemar%27s_test&quot; rel=&quot;nofollow&quot;&gt;McNemar's test&lt;/a&gt;.&#10;So this is for a paired comparison, which is more powerful than comparing &quot;independent&quot; proportions (which are not completely independent if they come from drawing randomly from the same finite sample).&lt;/p&gt;&#10;&#10;&lt;p&gt;I cannot look up McNemar's &quot;small print&quot; right now, but 30 samples is not much. So you may even have to switch from McNemar's to &lt;a href=&quot;http://en.wikipedia.org/wiki/Fisher%27s_exact_test&quot; rel=&quot;nofollow&quot;&gt;Fisher's exact test&lt;/a&gt; [or something else] which calculates the binomial probabilities. &lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Means of proportions:&lt;br&gt;&#10;It doesn't matter whether you test one and the same classifier 10x with 10 test cases or once with all those 100 cases (the 2 x 2 table just counts all test cases).&lt;/p&gt;&#10;&#10;&lt;p&gt;If the 10 estimates of accuracy for each classifier in the original question are obtained by random hold out or 10-fold cross validation or 10x out-of-bootstrap, the assumption is usually that the 10 surrogate models calculated for each classifier are equivalent (= have the same accuracy), so test results can be pooled*. For 10-fold cross validation you then assume that the test sample size equals the total number of test samples. For the other methods I'm not so sure: you may test the same case more than once. Depending on the data/problem/application, this doesn't amount to as much information as testing a new case.&lt;/p&gt;&#10;&#10;&lt;p&gt;*If the surrogate models are unstable, this assumption breaks down. But you can measure this: Do iterated $k$-fold cross validation. Each complete run gives  one prediction for each case. So if you compare the predictions for the same test case over a number of different surrogate models, you can measure the variance caused by exchanging some of the training data. This variance is in addition to the variance due to the finite total sample size. &lt;/p&gt;&#10;&#10;&lt;p&gt;Put your iterated CV results into a &quot;correct classification matrix&quot; with each row corresponding to one case and each column to one of the surrogate models. Now the variance along the rows (removing all empty elements) is solely due to instability in the surrogate models. The variance in the columns is due to the finite number of cases you used for testing of this surrogate model. Say, you have $k$ correct predicitions out of $n$ tested cases in a column. The point estimate for the accuracy is $\hat p = \frac{k}{n}$, it is subject to variance $\sigma^2 (\hat p) = \sigma^2 (\frac{k}{n}) = \frac{p (1 - p)}{n}$.&lt;br&gt;&#10;Check whether the variance due to instability is large or small compared to the variance due to finite test sample size.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2012-12-14T01:25:23.767" Id="45888" LastActivityDate="2012-12-14T19:09:21.113" LastEditDate="2012-12-14T19:09:21.113" LastEditorUserId="4598" OwnerUserId="4598" ParentId="45851" PostTypeId="2" Score="5" />
  
  <row Body="" CommentCount="0" CreationDate="2012-12-14T12:35:39.080" Id="45902" LastActivityDate="2012-12-14T12:35:39.080" LastEditDate="2012-12-14T12:35:39.080" LastEditorUserId="-1" OwnerUserId="-1" PostTypeId="4" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;I have both time series data and moving average data for the same data-set and the same time interval. I would like to extract the overlapping points. I would like extract those points where the extremum between the current and the last crossing point (intersected point between Moving average and time series) is taken depending on the direction of the trend reveral. I don't have idea whether perform it based on tables (1-Time series table includes temperature and timestamps 2-Moving average table includes only Moving average results of temperature) or perform it based on plot of them in which I have add moving average to time series plot. &lt;/p&gt;&#10;&#10;&lt;p&gt;Table of Time Series:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;     ambtemp                  dt&#10;1     -1.14 2007-09-29 00:01:57&#10;2     -1.12 2007-09-29 00:03:57&#10;3     -1.33 2007-09-29 00:05:57&#10;4     -1.44 2007-09-29 00:07:57&#10;5     -1.54 2007-09-29 00:09:57&#10;6     -1.29 2007-09-29 00:11:57&#10;7     -1.42 2007-09-29 00:13:57&#10;8     -1.37 2007-09-29 00:15:57&#10;9     -1.32 2007-09-29 00:17:57&#10;10    -1.37 2007-09-29 00:19:57&#10;11    -1.14 2007-09-29 00:21:57&#10;12    -1.16 2007-09-29 00:23:57&#10;13    -1.08 2007-09-29 00:25:57&#10;14    -1.21 2007-09-29 00:27:57&#10;15    -1.26 2007-09-29 00:29:57&#10;16    -1.50 2007-09-29 00:31:57&#10;17    -1.35 2007-09-29 00:33:57&#10;18    -1.56 2007-09-29 00:35:57&#10;19    -1.60 2007-09-29 00:37:57&#10;20    -1.30 2007-09-29 00:39:57&#10;21    -1.24 2007-09-29 00:41:57&#10;22    -1.24 2007-09-29 00:43:57&#10;23    -1.10 2007-09-29 00:45:57&#10;24    -0.99 2007-09-29 00:47:57&#10;25    -1.04 2007-09-29 00:49:57&#10;26    -0.97 2007-09-29 00:51:57&#10;27    -0.92 2007-09-29 00:53:57&#10;28    -0.70 2007-09-29 00:55:57&#10;29    -0.58 2007-09-29 00:57:57&#10;30    -0.49 2007-09-29 00:59:57&#10;31    -0.54 2007-09-29 01:01:57&#10;32    -0.37 2007-09-29 01:03:57&#10;33    -0.24 2007-09-29 01:05:57&#10;34    -0.34 2007-09-29 01:07:57&#10;35    -0.43 2007-09-29 01:09:57&#10;36    -0.52 2007-09-29 01:11:57&#10;37    -0.16 2007-09-29 01:13:57&#10;38    -0.50 2007-09-29 01:15:57&#10;39    -0.15 2007-09-29 01:17:57&#10;40    -0.04 2007-09-29 01:19:57&#10;41     0.00 2007-09-29 01:21:57&#10;42    -0.08 2007-09-29 01:23:57&#10;43    -0.30 2007-09-29 01:25:57&#10;44    -0.14 2007-09-29 01:27:57&#10;45    -0.06 2007-09-29 01:29:57&#10;46    -0.02 2007-09-29 01:31:57&#10;47    -0.25 2007-09-29 01:33:57&#10;48    -0.33 2007-09-29 01:35:57&#10;49     0.01 2007-09-29 01:37:57&#10;50    -0.28 2007-09-29 01:39:57&#10;51    -0.26 2007-09-29 01:41:57&#10;52    -0.34 2007-09-29 01:43:57&#10;53    -0.43 2007-09-29 01:45:57&#10;54    -0.20 2007-09-29 01:47:57&#10;55     0.19 2007-09-29 01:49:57&#10;56     0.28 2007-09-29 01:51:57&#10;57     0.07 2007-09-29 01:53:57&#10;58    -0.32 2007-09-29 01:55:57&#10;59    -0.20 2007-09-29 01:57:57&#10;60    -0.42 2007-09-29 01:59:57&#10;61    -0.40 2007-09-29 02:01:57&#10;62    -0.54 2007-09-29 02:03:57&#10;63    -0.67 2007-09-29 02:05:57&#10;64    -0.42 2007-09-29 02:07:57&#10;65    -0.57 2007-09-29 02:09:57&#10;66    -0.73 2007-09-29 02:11:57&#10;67    -0.54 2007-09-29 02:13:57&#10;68    -0.70 2007-09-29 02:15:57&#10;69    -0.70 2007-09-29 02:17:57&#10;70    -0.58 2007-09-29 02:19:57&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Table of Moving average:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;  [1]      NA      NA      NA      NA      NA      NA      NA      NA      NA&#10; [10]      NA      NA      NA      NA      NA      NA      NA      NA      NA&#10; [19]      NA -1.3250 -1.3300 -1.3360 -1.3245 -1.3020 -1.2770 -1.2610 -1.2360&#10; [28] -1.2025 -1.1655 -1.1215 -1.0915 -1.0520 -1.0100 -0.9665 -0.9250 -0.8760&#10; [37] -0.8165 -0.7635 -0.6910 -0.6280 -0.5660 -0.5080 -0.4680 -0.4255 -0.3765&#10; [46] -0.3290 -0.2955 -0.2770 -0.2475 -0.2370 -0.2230 -0.2215 -0.2310 -0.2240&#10; [55] -0.1930 -0.1530 -0.1415 -0.1325 -0.1350 -0.1540 -0.1740 -0.1970 -0.2155&#10; [64] -0.2295 -0.2550 -0.2905 -0.3050 -0.3235 -0.3590 -0.3740 -0.3860 -0.3880&#10; [73] -0.3695 -0.4045 -0.4450 -0.4900 -0.5125 -0.5215 -0.5515 -0.5825 -0.6140&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/zgboi.png&quot; alt=&quot;Overlap of time series and moving average for different window size&quot;&gt;&#10;Overlap of time series and moving average for different window size&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ClaCH.png&quot; alt=&quot;Expected Results for window size of 40&quot;&gt;&#10;Expected Results for window size of 40&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-12-14T13:38:54.713" Id="45905" LastActivityDate="2012-12-14T13:38:54.713" OwnerUserId="17833" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;pattern-recognition&gt;" Title="Extract the pattern of the time-series dataset through moving average" ViewCount="163" />
  <row Body="&lt;p&gt;A review and critique of some t-test approaches is given in &lt;a href=&quot;http://www.hpl.hp.com/conferences/icml2003/papers/119.pdf&quot; rel=&quot;nofollow&quot;&gt;Choosing between two learning algorithms based on calibrated tests&lt;/a&gt;, &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.37.3325&amp;amp;rep=rep1&amp;amp;type=pdf&quot; rel=&quot;nofollow&quot;&gt;Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms&lt;/a&gt;, and &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=21D16E681FF16B0BA91D741A63806A31?doi=10.1.1.29.5194&amp;amp;rep=rep1&amp;amp;type=pdf&quot; rel=&quot;nofollow&quot;&gt;On Comparing Classifiers: Pitfalls to Avoid and a Recommended Approach&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-12-14T15:19:54.237" Id="45918" LastActivityDate="2012-12-14T18:29:19.407" LastEditDate="2012-12-14T18:29:19.407" LastEditorUserId="16837" OwnerUserId="16837" ParentId="45851" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;Are you doing cross-validation? It's possible that those are just internal debugging ids for the software. I edited my code to not be as verbose, so it usually just prints out summary statistics for me! Based on the fact that it's on the same line as the &quot;optimization finished&quot; statement, it may have to do with iterations while the model is converging. Either way, I don't think you have to worry. Have you read the papers associated with the software? Learning about the underlying algorithm, for me, goes a long way to demystifying it! You should read &lt;a href=&quot;http://delivery.acm.org/10.1145/1970000/1961199/a27-chang.pdf?ip=134.134.139.72&amp;amp;acc=ACTIVE%20SERVICE&amp;amp;CFID=228593052&amp;amp;CFTOKEN=39663155&amp;amp;__acm__=1355505357_ff327bd7ac80fcb1c63e5b639bb1a18d&quot; rel=&quot;nofollow&quot;&gt;LIBSVM: A Library for Support Vector Machines&lt;/a&gt;. &lt;a href=&quot;http://delivery.acm.org/10.1145/1450000/1442794/p1871-fan.pdf?ip=134.134.139.72&amp;amp;acc=PUBLIC&amp;amp;CFID=228593052&amp;amp;CFTOKEN=39663155&amp;amp;__acm__=1355505461_48d742d80ba96392dca0066c27b5b0ec&quot; rel=&quot;nofollow&quot;&gt;LIBLINEAR: A Library for Large Linear Classification&lt;/a&gt; is probably also worth a read!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-14T17:12:55.723" Id="45929" LastActivityDate="2012-12-14T17:12:55.723" OwnerUserId="8580" ParentId="45923" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Here is a graphical representation of the chain, with the vertices $2$ and $5$ highlighted:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/1QaZy.png&quot; alt=&quot;figure 1&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The initial state $a$ can be indicated by labeling the vertices with their values, highlighting the nonzero values:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/RSoJF.png&quot; alt=&quot;figure 2&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Two transitions from $a$, as computed by the matrix product $a\mathbb{P}\mathbb{P} = a\mathbb{P}^2$, is this distribution:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/4i3hY.png&quot; alt=&quot;figure 3&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The weight on vertex $2$ is precisely the chance of reaching $2$ after two steps; that is, it is $\Pr(X_2=2)$.  To represent this event, we now zero out all other weights, leaving the distribution $b = (0, 2/25, 0, 0, 0)$.  The question asks us to take two more steps, beginning at $b$, computing $b\mathbb{P}\mathbb{P} = b\mathbb{P}^2$:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/5344j.png&quot; alt=&quot;figure 4&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The labels give the new distribution. All two-step paths from vertex $2$ to vertex $4$ are highlighted.  (There is just one, making it easy to compute the new distribution: $2/25$ is multiplied by $p_{2,4}=2/5$, giving $4/125$ for the transition from $2$ to $4$, then that is multiplied by $p_{4,5}=1/10$, yielding $4/1250=2/625$ for the double transition $2\to 4\to 5$.  In more complicated situations we would have to examine all possible paths from $2$ to $5$ and add their contributions.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Evidently, the chance of reaching vertex $2$ at step $2$ &lt;em&gt;and&lt;/em&gt; then arriving at vertex $5$ at step $4$ is the final value at vertex $5$, $2/625 = 0.0032$.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-12-14T19:06:31.160" Id="45936" LastActivityDate="2012-12-14T19:06:31.160" OwnerUserId="919" ParentId="45920" PostTypeId="2" Score="4" />
  <row AnswerCount="0" Body="&lt;p&gt;When I try to use the data and example for HLR from &lt;a href=&quot;http://dl.dropbox.com/u/10246536/Web/RTutorialSeries/dataset_hlr.csv&quot; rel=&quot;nofollow&quot;&gt;this example dataset&lt;/a&gt; taken from this post in the R Tutorial Series on &lt;a href=&quot;http://rtutorialseries.blogspot.com/2010/01/r-tutorial-series-basic-hierarchical.html&quot; rel=&quot;nofollow&quot;&gt;hierarchical linear regression&lt;/a&gt; the results don't match when I try to use the same method in SPSS.  Is it because SPSS is using a different type of sum of squares (III)?&lt;/p&gt;&#10;&#10;&lt;p&gt;The F values match for the final model, but not the 2nd one and some of the sum of squares seem off.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#R method&#10;url &amp;lt;- &quot;http://dl.dropbox.com/u/10246536/Web/RTutorialSeries/dataset_hlr.csv&quot;&#10;datavar &amp;lt;- read.csv(url, header=T)&#10;&#10;#create three linear models using lm(FORMULA, DATAVAR)&#10;#one predictor model&#10;onePredictorModel &amp;lt;- lm(ROLL ~ UNEM, datavar)&#10;#two predictor model&#10;twoPredictorModel &amp;lt;- lm(ROLL ~ UNEM + HGRAD, datavar)&#10;#three predictor model&#10;threePredictorModel &amp;lt;- lm(ROLL ~ UNEM + HGRAD + INC, datavar)&#10;&#10;#get summary data for each model using summary(OBJECT)&#10;summary(onePredictorModel)&#10;summary(twoPredictorModel)&#10;summary(threePredictorModel)&#10;&#10;#compare successive models using anova(MODEL1, MODEL2, MODELi)&#10;test&amp;lt;- anova(onePredictorModel, twoPredictorModel, threePredictorModel)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Below here is the code for SPSS.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;*SPSS method&#10;data list free /YEAR ROLL UNEM HGRAD INC.&#10;begin data&#10;1   5501    8.1 9552    1923&#10;2   5945    7   9680    1961&#10;3   6629    7.3 9731    1979&#10;4   7556    7.5 11666   2030&#10;5   8716    7   14675   2112&#10;6   9369    6.4 15265   2192&#10;7   9920    6.5 15484   2235&#10;8   10167   6.4 15723   2351&#10;9   11084   6.3 16501   2411&#10;10  12504   7.7 16890   2475&#10;11  13746   8.2 17203   2524&#10;12  13656   7.5 17707   2674&#10;13  13850   7.4 18108   2833&#10;14  14145   8.2 18266   2863&#10;15  14888   10.1    19308   2839&#10;16  14991   9.2 18224   2898&#10;17  14836   7.7 18997   3123&#10;18  14478   5.7 19505   3195&#10;19  14539   6.5 19800   3239&#10;20  14395   7.5 19546   3129&#10;21  14599   7.3 19117   3100&#10;22  14969   9.2 18774   3008&#10;23  15107   10.1    17813   2983&#10;24  14831   7.5 17304   3069&#10;25  15081   8.8 16756   3151&#10;26  15127   9.1 16749   3127&#10;27  15856   8.8 16925   3179&#10;28  15938   7.8 17231   3207&#10;29  16081   7   16816   3345&#10;end data.&#10;&#10;&#10;REGRESSION /MISSING LISTWISE &#10;/STATISTICS COEFF OUTS R ANOVA CHANGE &#10;/CRITERIA=PIN (.05) POUT(.10) &#10;/NOORIGIN /DEPENDENT ROLL &#10;/METHOD=ENTER UNEM &#10;/METHOD=ENTER HGRAD &#10;/METHOD=ENTER INC.&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Or did I mess something up in the procedure for SPSS?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-12-14T20:13:46.503" Id="45939" LastActivityDate="2012-12-14T20:30:58.630" LastEditDate="2012-12-14T20:30:58.630" LastEditorUserId="1036" OwnerUserId="17843" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;regression&gt;&lt;spss&gt;" Title="Why are these hierarchical linear regression results in R and SPSS different?" ViewCount="447" />
  
  
  <row Body="&lt;p&gt;For two covariates ($x_1$ and $x_2$), the Cox model is written as (with standard notations)&#10;$$&#10;h(t; x_1, x_2) = h_0(t) \exp(x_1 \beta_1 + x_2 \beta_2).&#10;$$&#10;To interpret $\beta_1$, note that&#10;$$&#10;\frac{h(t; x_1 + 1, x_2)}{h(t; x_1, x_2)} &#10; = \exp(\beta_1).&#10;$$&#10;In words, the previous equation tells us that whenever $x_1$ increases by 1 unit ($x_2$ being held fixed), the hazard rate is multiplied by $\exp(\beta_1)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;On the log-scale, we have&#10;$$&#10;\log(h(t; x_1 + 1, x_2)) - \log(h(t; x_1, x_2)) = \beta_1.&#10;$$&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-15T05:46:35.877" Id="45956" LastActivityDate="2012-12-15T05:46:35.877" OwnerUserId="3019" ParentId="45954" PostTypeId="2" Score="2" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a data set of code patches and the bugs they produce. I'm using ordinary least squares to find a line which predicts bugs based on some attributes about the patch, such as the department which produced the code, how many issues were found in design, etc.&lt;/p&gt;&#10;&#10;&lt;p&gt;One thing is that the patches vary in size, and larger patches will (obviously) create more bugs. I'm trying to think of how I should handle this, and I came up with several options:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Use patch size (lines of code changed) as an independent variable. The problem with this is that I think I really want to predict defect &quot;density&quot;. E.g. the Sales department might cause one bug every 15 lines of code, but the shipping people cause one every 30 lines of code. So I would need a variable like &quot;department * lines of code changed&quot;, which I worry is more sensitive to overfitting. &lt;/li&gt;&#10;&lt;li&gt;Break it into samples of equal size. E.g. if a change to 10 lines of code caused 2 bugs, I pretend that this was actually 10 changes, each of which caused .2 bugs.&lt;/li&gt;&#10;&lt;li&gt;Divide through by the size and then weight by the size. i.e. minimize $\sum w_i(\frac{y_i}{w_i} - f(\vec{x_i}))^2$. &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Are any of these preferable, or is there another option I should be using? I assume this must be a pretty standard problem to have, but I haven't been able to find anything online.&lt;/p&gt;&#10;" CommentCount="13" CreationDate="2012-12-15T15:46:23.470" Id="45974" LastActivityDate="2012-12-15T15:46:23.470" OwnerUserId="900" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;linear-model&gt;" Title="Linear regression for samples of varying sizes" ViewCount="93" />
  <row Body="&lt;p&gt;In your simple example, once you've computed $\mathbb P(x_1=0 | x_2=1,x_3=1)=p_{0,11}$, $\mathbb P(x_1=1 |x_2=1,x_3=1)=p_{1,11}$ and $\mathbb P(x_1=2 |x_2=1,x_3=1)=p_{2,11}$, you draw $x_1=0$ with probability $p_{0,11}$, $=1$ with probability $x_{1,11}$, and $=2$ with probability $=p_{2,11}$. You can operationalize this as&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Draw $U\sim U[0,1]$&lt;/li&gt;&#10;&lt;li&gt;If $U \le p_{0,11}$, assign $x_1=0$, and you are done.&lt;/li&gt;&#10;&lt;li&gt;If $p_{0,11} &amp;lt; U \le p_{0,11} + p_{1,11}$, assign $x_1=1$, and you are done.&lt;/li&gt;&#10;&lt;li&gt;If $ p_{0,11} + p_{1,11}&amp;lt;U$, assign $x_1=2$.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;It is easy to see that thus generated $x_1$ has the required distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you drew the modal value, you will quickly get stuck in the local mode of the joint distribution, which is not what you want.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-15T16:30:48.343" Id="45978" LastActivityDate="2012-12-15T16:30:48.343" OwnerUserId="5739" ParentId="45946" PostTypeId="2" Score="2" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have a dataset covering daily data for 3 years (3x365 rows) for multiple attributes TotalPhoneCall (main attribute that I want to predict), Christmas day, weekend, weekday, Easter, 4th_july, etc.(some are seasonal). &lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to predict TotalPhoneCall for the following month. I have to use ARIMA with regression. I may filter out unnecessary attributes if needed.  How can I do this in R? &lt;/p&gt;&#10;" CommentCount="7" CreationDate="2012-12-16T00:10:02.473" Id="45993" LastActivityDate="2013-06-21T11:48:50.700" LastEditDate="2013-06-21T11:48:50.700" LastEditorUserId="3826" OwnerUserId="17866" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;arima&gt;&lt;multivariate-regression&gt;" Title="Multivariate ARIMA with regression" ViewCount="707" />
  <row AcceptedAnswerId="45996" AnswerCount="1" Body="&lt;p&gt;If I have the following dataset:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;rankdata &amp;lt;- read.table( text = &quot; time  id  rank&#10;                             1     a    1&#10;                             1     b    2&#10;                             2     a    2&#10;                             2     b    3&#10;                             2     c    1&#10;                             3     a    1&#10;                             3     b    3&#10;                             3     c    2&#10;                             4     a    2                     &#10;                             4     c    1&#10;                             5     a    1                                 &#10;                             5     c    1 &quot;, header = TRUE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I am trying to think of the best way to test whether ranks are consistant over time or not.&#10;I obviously work in &lt;code&gt;R&lt;/code&gt; so pointers to doing this there would be best.&lt;/p&gt;&#10;&#10;&lt;p&gt;Absent &lt;code&gt;ids&lt;/code&gt; within &lt;code&gt;time&lt;/code&gt; points are not unmeasured data points but ids that were not present at the time of ranking. So number of ranks within a time point may be 1-6 then 1-12 in the next but generally within this range. Also note I expect that there may be ties.&lt;/p&gt;&#10;&#10;&lt;p&gt;What if I had several data sets by say &lt;code&gt;island&lt;/code&gt;, I would need to incorporate island as a random effect. Each island will consist of completely independent &lt;code&gt;ids&lt;/code&gt;.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-12-16T00:14:43.840" Id="45994" LastActivityDate="2013-01-15T03:31:27.273" LastEditDate="2012-12-16T00:47:30.250" LastEditorUserId="17865" OwnerUserId="17865" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;ranks&gt;" Title="best approach to compare ranks over time" ViewCount="82" />
  <row AnswerCount="1" Body="&lt;p&gt;I am trying to read the significance tests. However, I am a bit confused what exactly is the difference between chisquared test and z test. I read that chisquared is non parametric and it doesn't make any assumption about the distribution of the population. However, z test assumes that the statistic parameter forms a normal distribution. Using central limit theorem, if the number of samples are very high we can assume that the statistic is from a normal distribution&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-12-16T06:31:20.530" Id="46005" LastActivityDate="2013-03-16T08:40:08.180" OwnerUserId="12329" PostTypeId="1" Score="0" Tags="&lt;normal-distribution&gt;&lt;chi-squared&gt;&lt;z-test&gt;" Title="Confusion related to significance test" ViewCount="101" />
  <row AnswerCount="2" Body="&lt;p&gt;The problem I have is as follows.  I have enrolment data describing the number of male and female preferences for two broad fields of education: i) Information Technology and ii) Engineering.  The data reflects &quot;first preferences&quot;, that is, the number of people who wrote down on paper that their most preferred degree was either one in the field of Information Technology or the field of Engineering.  The data does not reflect actual enrolments.  The numbers are as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Information Technology:&lt;/strong&gt;&lt;br&gt;&#10;Female: 266  (~13%)&lt;br&gt;&#10;Male: 1783  (~87%)&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Engineering:&lt;/strong&gt;&lt;br&gt;&#10;Female: 684  (~12.5%)&lt;br&gt;&#10;Male: 4773  (~87.5%)&lt;/p&gt;&#10;&#10;&lt;p&gt;The question I am trying to answer is: &lt;em&gt;is there a statistically significant difference between the gender ratios of Engineering versus Information Technology, and at what level of significance?&lt;/em&gt;  &lt;/p&gt;&#10;&#10;&lt;p&gt;I apologize if this is a noob question.  I only have relatively basic background knowledge in statistics and tests of significance and this problem didn't seem to fit any of the methods I am familiar with.  I'm hoping someone might be able to point me in the right direction as to which statistical test(s) would be appropriate given the dataset and any caveats I should be aware of.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-12-16T11:58:18.113" Id="46015" LastActivityDate="2012-12-18T01:23:39.650" LastEditDate="2012-12-18T01:23:39.650" LastEditorUserId="870" OwnerUserId="870" PostTypeId="1" Score="0" Tags="&lt;statistical-significance&gt;&lt;categorical-data&gt;&lt;experiment-design&gt;" Title="Testing statistical significance of male vs female enrolment preference percentages between two different fields of education" ViewCount="1061" />
  
  <row AcceptedAnswerId="46047" AnswerCount="4" Body="&lt;p&gt;Why are we using the squared residuals instead of the absolute residuals in OLS estimation?&lt;/p&gt;&#10;&#10;&lt;p&gt;My idea was that we use the square of the error values, so that residuals below the fitted line (which are then negative), would still have to be able to be added up to the positive errors. Otherwise, we could have an error of 0 simply because a huge positive error could cancel with a huge negative error.&lt;/p&gt;&#10;&#10;&lt;p&gt;So why do we square it, instead of just taking the absolute value? Is that because of the extra penalty for higher errors (instead of 2 being 2 times the error of 1, it is 4 times the error of 1 when we square it).&lt;/p&gt;&#10;" ClosedDate="2014-04-20T13:17:01.837" CommentCount="7" CreationDate="2012-12-16T12:17:52.220" FavoriteCount="5" Id="46019" LastActivityDate="2013-12-28T15:26:22.990" LastEditDate="2013-12-28T15:26:22.990" LastEditorUserId="805" OwnerUserId="16175" PostTypeId="1" Score="15" Tags="&lt;regression&gt;&lt;estimation&gt;&lt;least-squares&gt;&lt;residuals&gt;" Title="Why squared residuals instead of absolute residuals in OLS estimation?" ViewCount="4141" />
  
  <row AcceptedAnswerId="46093" AnswerCount="2" Body="&lt;p&gt;I came across this distribution in a computer game and wanted to learn more about its behaviour.  It comes from the decision as to whether a certain event should occur after a given number of player actions.  The details beyond this aren't relevant.  It seems applicable to other situations, and I found it interesting because it's easy to calculate and creates a long tail.&lt;/p&gt;&#10;&#10;&lt;p&gt;Every step $n$, the game generates a uniform random number $0 \leq X &amp;lt; 1$. If $X &amp;lt; p(n)$, then the event is triggered.  After the event once occurs, the game resets $n = 0 $ and runs through the sequence again.  I'm only interested in one occurrence of the event for this problem, because that represents the distribution that the game is using.  (Also, any questions regarding multiple occurrences can be answered with a single occurrence model.)&lt;/p&gt;&#10;&#10;&lt;p&gt;The main &quot;abnormality&quot; here is that the probability parameter in this distribution increases over time, or put another way, the threshold rises over time.  In the example it changes linearly but I suppose other rules could apply.  After $n$ steps, or actions by the user, &lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;p(n) = kn&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;for some constant $0 &amp;lt; k &amp;lt; 1$.  At a certain point $n_{\max} $, we get $p(n_{\max}) \geq 1 $.  The event is simply guaranteed to occur at that step.&lt;/p&gt;&#10;&#10;&lt;p&gt;I was able to determine that&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;f(n) = p(n)\left[1 - F(n - 1)\right]&#10;$$&#10;and&#10;$$&#10;F(n) = p(n) + F(n-1)\left[1 - p(n)\right]&#10;$$&#10;for PMF $f(n)$ and CDF $F(n)$.  In brief, the probability that the event will on the $n$th step is equal to the probability $p(n)$, less the probability that it has already happened on any preceding step.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's a plot from our friend Monte Carlo, for fun, with $k \approx 0.003$.  The median works out to 21 and average to 22. &#10;&lt;img src=&quot;http://i.stack.imgur.com/R1s2A.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;This is broadly equivalent to a first-order difference equation from digital signal processing, which is my background, and so I found that quite novel.  I'm also intrigued by the notion that $p(n)$ could vary according to any arbitrary formula.&lt;/p&gt;&#10;&#10;&lt;p&gt;My questions:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;What's the name of this distribution, if it has one?&lt;/li&gt;&#10;&lt;li&gt;Is there any way to derive an expression for $f(n)$ without reference to $F(n)$?&lt;/li&gt;&#10;&lt;li&gt;Are there other examples of discrete recursive distributions like this?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edits&lt;/strong&gt;&#10;Clarified process about random number generation.&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2012-12-16T22:45:36.237" Id="46045" LastActivityDate="2013-01-01T16:22:02.593" LastEditDate="2013-01-01T16:22:02.593" LastEditorUserId="2970" OwnerUserId="17885" PostTypeId="1" Score="10" Tags="&lt;probability&gt;&lt;distributions&gt;&lt;survival&gt;&lt;discrete-data&gt;" Title="What's the name of this discrete distribution (recursive difference equation) I derived?" ViewCount="353" />
  <row Body="&lt;p&gt;You have to decide how you do the error accounting: Are genes falsely attributed to age and to sickness are the same error (&quot;a false discovery&quot;)? Otherwise- consider controlling for errors &lt;strong&gt;within&lt;/strong&gt; each phenotype. &#10;In my experience, one typically looks for &quot;sickness genes&quot; so you would use an ANOVA just to control for age/gender but without testing their effects.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-17T07:58:57.903" Id="46067" LastActivityDate="2012-12-17T07:58:57.903" OwnerUserId="6961" ParentId="46038" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;I am working on sample size of 70 . My observed and expected values are different&#10;(Observed 586,&#10;648,&#10;526,&#10;662,&#10;658,&#10;502,&#10;..... so on for 70 samples and expected 570&#10;634,&#10;513,&#10;647,&#10;644,&#10;490,&#10;.....so on for 70 samples). I used Chi-sq test to check the null that there is no difference in observed and expected value. And I got a p-value of 0.99 which means I can't reject null. Can I use any other test for this kind of problem since I can see that for all the 70 samples observed is higher than expected but I cant show this result statistically. &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-12-17T10:12:20.667" Id="46072" LastActivityDate="2013-09-21T19:16:52.250" LastEditDate="2013-09-21T19:16:52.250" LastEditorUserId="27433" OwnerUserId="17539" PostTypeId="1" Score="1" Tags="&lt;statistical-significance&gt;&lt;mathematical-statistics&gt;&lt;chi-squared&gt;&lt;sample-size&gt;" Title="Significance of Chi-square test for large sample" ViewCount="546" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have two datasets of time-series data, each with arbitrary, non-linear sampling intervals. I'm planning to analyze them using either Pearson or Spearman correlation (determined by the detection of outliers using the 3-sigma rule) but I believe that the correlation between times at which each sample was taken may be relevant.&lt;/p&gt;&#10;&#10;&lt;p&gt;Does anyone know of a good algorithm to correlate this type of time-series data, taking into account sample time if it's relevant? Pseudo-code is particularly welcome... &lt;/p&gt;&#10;&#10;&lt;p&gt;A million thanks, in advance!&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-12-17T18:03:28.490" Id="46100" LastActivityDate="2012-12-17T18:03:28.490" OwnerUserId="17917" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;correlation&gt;" Title="Correlation of non-uniform time-series data?" ViewCount="212" />
  <row Body="&lt;p&gt;If you are trying to do model selection, &lt;strong&gt;I would definetly go for AIC and/or BIC&lt;/strong&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;The main reason for not looking at the likelihood-ratio tests that you gave in your example is that the p-values aren't comparable with each other (well...in general, p-values aren't comparable!): each time you calculate a p-value, you are not considering that there exist other possible models (e.g., when you perform the 2-peak model v/s the 3, 4, etc. separately) and therefore the null distribution that serves for the calculation of the p-value does not make sense for the problem at hand...the p-values are, therefore, meaningless for your problem!&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, in general, to compare the AIC values in order to select a model, a difference of more than $2$ between the AIC values is usually denoted as &quot;distinguishable models&quot; by the AIC (see this &lt;a href=&quot;http://en.wikipedia.org/wiki/Akaike_information_criterion#How_to_apply_AIC_in_practice&quot; rel=&quot;nofollow&quot;&gt;wikipedia article on the AIC&lt;/a&gt;). &lt;strong&gt;Due to the low number of samples, however, I would recommend calculating the AICc instead of the &quot;plain AIC&quot;, which is the &quot;low-sample corrected&quot; version of the AIC&lt;/strong&gt; and is given by:&#10;$$AICc=AIC+\frac{2k(k+1)}{n-k-1}$$&#10;where $AIC$ is the AIC value that you have, $k$ is the number of parameters and $n$ is the number of datapoints. You can check out more about the AICc in the wikipedia article that I cited before. For further references, please check out the excellent textbook of &lt;a href=&quot;http://www.springer.com/statistics/statistical+theory+and+methods/book/978-0-387-95364-9&quot; rel=&quot;nofollow&quot;&gt;Burnham &amp;amp; Anderson (2002)&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-17T18:35:39.067" Id="46103" LastActivityDate="2012-12-17T18:57:28.230" LastEditDate="2012-12-17T18:57:28.230" LastEditorUserId="9174" OwnerUserId="9174" ParentId="46098" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;I would use the &lt;a href=&quot;http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence&quot; rel=&quot;nofollow&quot;&gt;KL-Divergence&lt;/a&gt; between the feature values conditioned on&#10;the class labels, i.e.&lt;/p&gt;&#10;&#10;&lt;p&gt;$KL[ p(v_i | in) || p(v_i | out)]$&lt;/p&gt;&#10;&#10;&lt;p&gt;By itself the KL divergence provides an ordering on the features; thus if there are external constraints on the number of features that we want to  include in the model, e.g. the entire data set has hundreds of features but we only want a model with 10, then it would be reasonable to keep the 10 features with the highest KL divergence.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-12-17T20:01:34.097" Id="46108" LastActivityDate="2013-02-07T13:09:56.207" LastEditDate="2013-02-07T13:09:56.207" LastEditorUserId="16859" OwnerUserId="16859" ParentId="46106" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;What does correlation between the &lt;em&gt;maps&lt;/em&gt; mean?  On the other hand, if you mean the correlation between the temperature and the humidity, we then have to ask what the population is that you are interested in and what the context of the question is.  For example, the correlation between humidity and temperature in one location over time may differ from that in another location.  The correlation between humidity and temperature over the entire geography of the US (assuming you can take a random sample from the US) during the summer may differ from that during the winter.&lt;/p&gt;&#10;&#10;&lt;p&gt;To even take a stab at answer those questions requires knowing why you are asking them.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-17T21:42:18.210" Id="46121" LastActivityDate="2012-12-17T21:42:18.210" OwnerUserId="8871" ParentId="45225" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;You can simply use the F test and anova to compare them. Here are some codes.    &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; x &amp;lt;- 1:10&#10;&amp;gt; y &amp;lt;- 2*x + 3                            &#10;&amp;gt; yeps &amp;lt;- y + rnorm(length(y), sd = 0.01)&#10;&amp;gt; &#10;&amp;gt; &#10;&amp;gt; m1=nls(yeps ~ a + b*x, start = list(a = 0.12345, b = 0.54321))&#10;&amp;gt; summary(m1)&#10;&#10;Formula: yeps ~ a + b * x&#10;&#10;Parameters:&#10;   Estimate Std. Error t value Pr(&amp;gt;|t|)    &#10;a 2.9965562  0.0052838   567.1   &amp;lt;2e-16 ***&#10;b 2.0016282  0.0008516  2350.6   &amp;lt;2e-16 ***&#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 &#10;&#10;Residual standard error: 0.007735 on 8 degrees of freedom&#10;&#10;Number of iterations to convergence: 2 &#10;Achieved convergence tolerance: 3.386e-09 &#10;&#10;&amp;gt; &#10;&amp;gt; &#10;&amp;gt; m2=nls(yeps ~ a + b*x+c*I(x^5), start = list(a = 0.12345, b = 0.54321,c=10))&#10;&amp;gt; summary(m2)&#10;&#10;Formula: yeps ~ a + b * x + c * I(x^5)&#10;&#10;Parameters:&#10;   Estimate Std. Error  t value Pr(&amp;gt;|t|)    &#10;a 3.003e+00  5.820e-03  516.010   &amp;lt;2e-16 ***&#10;b 1.999e+00  1.364e-03 1466.004   &amp;lt;2e-16 ***&#10;c 2.332e-07  1.236e-07    1.886    0.101    &#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 &#10;&#10;Residual standard error: 0.006733 on 7 degrees of freedom&#10;&#10;Number of iterations to convergence: 2 &#10;Achieved convergence tolerance: 1.300e-06 &#10;&#10;&amp;gt; &#10;&amp;gt; anova(m1,m2)&#10;Analysis of Variance Table&#10;&#10;Model 1: yeps ~ a + b * x&#10;Model 2: yeps ~ a + b * x + c * I(x^5)&#10;  Res.Df Res.Sum Sq Df     Sum Sq F value Pr(&amp;gt;F)&#10;1      8 0.00047860                             &#10;2      7 0.00031735  1 0.00016124  3.5567 0.1013&#10;&amp;gt;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2012-12-18T00:29:44.210" Id="46135" LastActivityDate="2012-12-18T00:29:44.210" OwnerUserId="13138" ParentId="35601" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Very nice question. A thing to realize upfront is that machine learning is both an art and science and involves meticulously cleaning out data, visualizing it and eventually build models that suite the business in question, while simultaneously keeping it scalable &amp;amp; tractable.&#10;Skills wise, more important than anything else is to focus on probability and to use simple &lt;a href=&quot;http://bayesianthink.blogspot.com/2012/12/the-naive-bayesian-approach-to-machine.html&quot; rel=&quot;nofollow&quot;&gt;methods&lt;/a&gt; first before jumping onto complex ones. I prefer the R &amp;amp; Perl combination, since you known python that should be good enough. When working on a real job, you will invariably have to pull your own data so knowledge of SQL (or whatever other no-sql your company supports) is a must.&lt;/p&gt;&#10;&#10;&lt;p&gt;Nothing beats experience in the ML area, so engaging in sites like stackexchange, kaggle is also a great way to get exposed to this field. Good luck.&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2012-12-18T01:39:58.650" CreationDate="2012-12-18T01:39:58.650" Id="46137" LastActivityDate="2012-12-18T01:39:58.650" OwnerUserId="13516" ParentId="26044" PostTypeId="2" Score="0" />
  
  
  <row Body="&lt;p&gt;You can use insurance data. For example, in third party insurance for every policyholder &quot;claim frequency&quot; of different &quot;claim types&quot;  is a multivariate count.&#10;In &lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0167668710001289&quot; rel=&quot;nofollow&quot;&gt;this article&lt;/a&gt; a multivariate Poisson regression model suggested by the authors.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-12-18T06:48:03.133" Id="46157" LastActivityDate="2012-12-18T06:48:03.133" OwnerUserId="16365" ParentId="46145" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Along the lines of DWin answer, but taking into account the grid size...&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; sum(UD1$estimate)&#10;[1] 0.6217264&#10;&amp;gt; sum(UD2$estimate)&#10;[1] 0.622008&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Is that ok? Let's look at the grid.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; lapply( UD1$eval.points, function(x) head(diff(x)))&#10;[[1]]&#10;[1] 2.42362 2.42362 2.42362 2.42362 2.42362 2.42362&#10;&#10;[[2]]&#10;[1] 2.365521 2.365521 2.365521 2.365521 2.365521 2.365521&#10;&#10;[[3]]&#10;[1] 0.28 0.28 0.28 0.28 0.28 0.28&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So the result should be&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; 1/(2.42362*2.365521*0.28)&#10;[1] 0.6229463&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This looks good to me. Your integral can be roughly evaluated by&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; sum(UD2$estimate*UD1$estimate)/0.622&#10;[1] 2.189203e-05&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This is quick an (very) dirty. You should not rely on this result but try this again with much more points in the grid (say, a 200 × 200 × 200 grid at least).&lt;/p&gt;&#10;&#10;&lt;p&gt;Or you should find a way to evaluate the density at a point of the space, and go for a Monte Carlo integration. I tried this, using the formula displayed in &lt;code&gt;kde&lt;/code&gt; vignette (with covariance matrix &lt;code&gt;UD1$H&lt;/code&gt; and &lt;code&gt;UD2$H&lt;/code&gt;), but I failed to obtain the same values as in the &lt;code&gt;estimate&lt;/code&gt; component. &lt;strong&gt;If someone knows how to do that, I’ll be interested.&lt;/strong&gt; The &lt;code&gt;kde&lt;/code&gt; package seems very nice, but oriented towards graphic visualization. There's nothing to evaluate the kernel densities at a given point, for example.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Post Scriptum&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Of course this quantity is very small: if the two distributions were the same, we would obtain&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; sum(UD1$estimate*UD1$estimate)/0.622&#10;[1] 9.573283e-05&#10;&amp;gt; sum(UD2$estimate*UD2$estimate)/0.622&#10;[1] 5.473942e-05&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;You have to use some reference term, dividing your &lt;code&gt;2.2e-5&lt;/code&gt; by either one of the above quantities, or by the bigger one, or by the mean of the two... I don’t know. The volume of space in which they overlap is not a well defined notion. Note that the “voxel size” here is &lt;code&gt;1/0.6229463&lt;/code&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, when you edit a question, please write a Post Scriptum rather than transforming it in an other question: that makes the answers given hard to understand, or even that makes them look stupid... In the present case you are basically asking if someone can confirm that my answer is correct, a single line would have been sufficient... I can’t confirm that...&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-12-18T09:48:26.787" Id="46162" LastActivityDate="2012-12-18T21:20:32.597" LastEditDate="2012-12-18T21:20:32.597" LastEditorUserId="8076" OwnerUserId="8076" ParentId="46120" PostTypeId="2" Score="2" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have two variable in my research. the first is managers commitment &amp;amp; the second is personnel satisfaction. so we have two population &amp;amp; two questionnaire, one for managers &amp;amp; the other with &quot;different&quot; questions for personnel. moreover the number of managers is different from the number of personnel. how can I calculate correlation &amp;amp; regression between these two population as one population with two variable?&#10;thanks&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-18T11:43:13.263" Id="46172" LastActivityDate="2012-12-18T18:18:22.443" LastEditDate="2012-12-18T12:40:24.543" LastEditorUserId="10065" OwnerUserId="17939" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;correlation&gt;&lt;sem&gt;" Title="two questionnaires, two variables, two populations but one correlation" ViewCount="303" />
  <row AnswerCount="1" Body="&lt;p&gt;I am using binary logistic regression; the dependent variable is 1 or 0; the independent variables are two groups: the first group includes continuous variables (&lt;code&gt;LNTA: logarthim of total assets&lt;/code&gt;, &lt;code&gt;ROA: return on assets&lt;/code&gt;, and &lt;code&gt;Leverage&lt;/code&gt;; the second group includes categorical variables (&lt;code&gt;Type of auditor: 1 or 0&lt;/code&gt;, &lt;code&gt;Industry sector: 1,2,3, and 4&lt;/code&gt;, &lt;code&gt;country: 1, 2, 3, 4, 5, 6, 7, 8, 9, and 10&lt;/code&gt;), and finally &lt;code&gt;region: 1 or 2&lt;/code&gt;. The problem is that when I put all these independent variables together, I get results only for 8 countries not for 9 countries. I know that I have 10 countries and in the results table only 9 countries will appear as the country number 10 is reference category; but this case is different because both countries 9 and 10 are not included in the table.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-12-18T13:54:49.797" Id="46175" LastActivityDate="2012-12-19T20:25:50.743" LastEditDate="2012-12-18T17:37:49.693" LastEditorUserId="7290" OwnerUserId="17940" PostTypeId="1" Score="0" Tags="&lt;logistic&gt;&lt;spss&gt;&lt;categorical-data&gt;&lt;interpretation&gt;&lt;binary&gt;" Title="How do I interpret logistic regression output for categorical variables when two categories are missing?" ViewCount="1325" />
  <row AcceptedAnswerId="46187" AnswerCount="1" Body="&lt;p&gt;I came across variance stabilizing transformation while reading &lt;a href=&quot;http://www.kaggle.com/c/asap-aes/details/evaluation&quot;&gt;Kaggle Essay Eval method&lt;/a&gt;. They use a variance stabilization transformation to transform kappa values before taking their mean and then transform them back. Even after reading the wiki on variance stabilizing transforms I can't understand, why do we actually stabilize variances? What benefit do we gain by this?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-12-18T14:44:59.177" Id="46177" LastActivityDate="2012-12-18T19:24:02.717" LastEditDate="2012-12-18T19:24:02.717" LastEditorUserId="17941" OwnerUserId="17941" PostTypeId="1" Score="11" Tags="&lt;variance&gt;&lt;mathematical-statistics&gt;" Title="Why do we stabilize variance?" ViewCount="526" />
  
  
  
  
  <row Body="&lt;p&gt;You didn't show a dataset to replicate. You can see from a basic &lt;a href=&quot;http://stat.ethz.ch/R-manual/R-patched/library/rpart/html/predict.rpart.html&quot; rel=&quot;nofollow&quot;&gt;illustration&lt;/a&gt; that class typically only returns one class per prediction. Maybe in your case, 1 is the observation index and 0 is the prediction with 2 corresponding factor levels possible {0,1}.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;   &amp;gt; predict(fit, type = &quot;class&quot;)  # factor&#10;          1       2       3       4       5       6       7       8       9      10 &#10;    present  absent present present  absent  absent  absent  absent  absent present &#10;         11      12      13      14      15      16      17      18      19      20 &#10;    present  absent present  absent  absent  absent  absent  absent  absent  absent &#10;     21      22      23      24      25      26      27      28      29      30 &#10;&#10;Levels: absent present&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;You can also store the result in a variable and look at the structure of that variable to confirm the specific output objects using str() function. Notice the prediction attribute values and Factor names attributes are clearly specified here.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; p&amp;lt;-predict(fit, type = &quot;class&quot;)&#10; str(p)&#10; Factor w/ 2 levels &quot;absent&quot;,&quot;present&quot;: 2 1 2 2 1 1 1 1 1 2 ...&#10; - attr(*, &quot;names&quot;)= chr [1:81] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2012-12-18T18:56:23.423" Id="46192" LastActivityDate="2012-12-18T19:02:07.230" LastEditDate="2012-12-18T19:02:07.230" LastEditorUserId="13519" OwnerUserId="13519" ParentId="46176" PostTypeId="2" Score="2" />
  <row AnswerCount="0" Body="&lt;p&gt;I've been using the &lt;code&gt;KernSmoothIRT&lt;/code&gt; module in R to do non-parametric analysis of some items. &lt;/p&gt;&#10;&#10;&lt;p&gt;I'm trying to find a way to quantify the difficulty and discrimination of a particular set of items.  What is the best way to do this with this module?  &lt;/p&gt;&#10;&#10;&lt;p&gt;Geometrically, I think I'm looking for the value of OCC at theta=0 and the slope at P(theta)=0.5, but I'm not sure how to figure this out from what's generated by the library.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-12-19T01:25:30.100" FavoriteCount="1" Id="46208" LastActivityDate="2012-12-19T02:19:42.647" LastEditDate="2012-12-19T02:19:42.647" LastEditorUserId="7290" OwnerUserId="17956" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;nonparametric&gt;&lt;irt&gt;" Title="How do I extract difficulty and discrimination from a non-parametric IRT model" ViewCount="44" />
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I don't know how many steps are necessary for &lt;code&gt;lars()&lt;/code&gt; to select the variables till the algorithm proceeds to the saturated fit (especially using the &lt;code&gt;lars&lt;/code&gt; package in &lt;code&gt;R&lt;/code&gt;)? Can anyone give me a hint? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-19T14:15:02.960" Id="46240" LastActivityDate="2013-06-19T02:11:08.137" LastEditDate="2012-12-20T12:31:43.927" LastEditorUserId="2116" OwnerUserId="17976" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;lars&gt;" Title="Max steps in lars" ViewCount="114" />
  <row Body="&lt;p&gt;This turns out to be a two-part answer&lt;/p&gt;&#10;&#10;&lt;h3&gt;1. Log-Determinant&lt;/h3&gt;&#10;&#10;&lt;p&gt;The &lt;code&gt;spdep&lt;/code&gt; package does contain tools to efficiently calculate the log-determinant of a sparse weights matrix. The following lines of code drop the calculation time from 11 seconds to about 2. The code is not very clean because the methods are not intended for use by typical statistical consumers.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# Calculate log-determinant of (I- rho*W) - using methods in the spdep package&#10;env &amp;lt;- new.env(parent=globalenv())&#10;assign(&quot;listw&quot;, Wlist, envir=env)&#10;assign(&quot;can.sim&quot;, can.sim, envir=env)&#10;assign(&quot;similar&quot;, FALSE, envir=env)&#10;assign(&quot;family&quot;, &quot;SAR&quot;, envir=env)&#10;assign(&quot;n&quot;, n, envir=env)&#10;Matrix_setup(env, Imult=2, super=FALSE)&#10;get(&quot;similar&quot;, envir=env)&#10;logdet &amp;lt;- do_ldet(rho, env)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;h3&gt;2. Maximum Likelihood Radius&lt;/h3&gt;&#10;&#10;&lt;p&gt;Roger Bivand, the package author, explained to me that such a method is not really valid, and I discovered this myself when I couldn't find any traction on the likelihood function. What he instead recommended was to simply estimate the same model specification over a grid of radii and select the radius that provided the maximum point. The figure below shows how the likelihood value changes for higher sample sizes and different radii on my dataset.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/xjPsp.png&quot; alt=&quot;Likelihood versus radius as sample size increases&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-19T16:00:24.797" Id="46248" LastActivityDate="2012-12-19T16:00:24.797" OwnerUserId="10026" ParentId="45870" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I didn't see the lecture, so I can't comment on what was said. &lt;/p&gt;&#10;&#10;&lt;p&gt;My $0.02: If you want to get good estimates of performance using resampling, you should really do all of the operations during resampling instead of prior. This is really true of feature selection [1] as well as non-trivial operations like PCA. If it adds uncertainty to the results, include it in resampling.&lt;/p&gt;&#10;&#10;&lt;p&gt;Think about principal component regression: PCA followed by linear regression on some of the components. PCA estimates parameters (with noise) and the number of components must also be chosen (different values will result in different results =&gt; more noise).&lt;/p&gt;&#10;&#10;&lt;p&gt;Say we used 10 fold CV with scheme 1:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;conduct PCA&#10;pick the number of components&#10;for each fold:&#10;   split data&#10;   fit linear regression on the 90% used for training&#10;   predict the 10% held out&#10;end:&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;or scheme 2:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;for each fold:&#10;   split data&#10;   conduct PCA on the 90% used for training&#10;   pick the number of components&#10;   fit linear regression&#10;   predict the 10% held out&#10;end:&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;It should be clear than the second approach should produce error estimates that reflect the uncertainty caused by PCA, selection of the number of components and the linear regression. In effect, the CV in the first scheme has no idea what preceded it.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm guilty of not always doing all the operations w/in resampling, but only when I don't really care about performance estimates (which is unusual). &lt;/p&gt;&#10;&#10;&lt;p&gt;Is there much difference between the two schemes? It depends on the data and the pre-processing. If you are only centering and scaling, probably not. If you have a ton of data, probably not. As the training set size goes down, the risk of getting poor estimates goes up, especially if n is close to p. &lt;/p&gt;&#10;&#10;&lt;p&gt;I can say with certainty from experience that not including supervised feature selection within resampling is a really bad idea (without large training sets). I don't see why pre-processing would be immune to this (to some degree). &lt;/p&gt;&#10;&#10;&lt;p&gt;@mchangun: I think that the number of components is a tuning parameter and you would probably want to pick it using performance estimates that are generalizable. You could automatically pick K such that at least X% of the variance is explained and include that process within resampling so we account for the noise in that process. &lt;/p&gt;&#10;&#10;&lt;p&gt;Max&lt;/p&gt;&#10;&#10;&lt;p&gt;[1] Ambroise, C., &amp;amp; McLachlan, G. (2002). Selection bias in gene extraction on the basis of microarray gene-expression data. Proceedings of the National Academy of Sciences, 99(10), 6562–6566.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-19T18:12:32.107" Id="46251" LastActivityDate="2012-12-19T18:12:32.107" OwnerUserId="3468" ParentId="46216" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;The second sentence in the second section of &lt;a href=&quot;http://www-stat.stanford.edu/~imj/WEBLIST/2004/LarsAnnStat04.pdf&quot; rel=&quot;nofollow&quot;&gt;The LARS paper&lt;/a&gt; says that the LARS algorithm requires $m$ steps, where $m$ is the number of covariates.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-12-19T18:34:35.970" Id="46255" LastActivityDate="2013-06-19T02:11:08.137" LastEditDate="2013-06-19T02:11:08.137" LastEditorUserId="805" OwnerUserId="4505" ParentId="46240" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I don't know if you have SAS experience, but I've used SAS PROCs MI and Mianalyze to perform (and then synthesize) multiple imputations in several different models. Building the &quot;imputation model&quot; (this yields non-biased estimates of missing data, incorporating the uncertainty one finds in non-missing data) is probably the most difficult task.  The imputation model will include all or most analysis variables (i.e., predictors in your analysis model), as well as auxiliary variables-other variables that correlate with the dependent variable, the state of being missing or both. (Note: you might want to use p &amp;lt; .15 as a first threshold.) &lt;/p&gt;&#10;&#10;&lt;p&gt;One then selects parameters such as the number of iterations (both before the first imputation and between iterations), the estimation method, the sampling method, etc. Of course, preceeding all of this, one should determine what led to the missing data, and whether missing data are MCAR (missing completely at random), MAR (missing at random), or MNAR (missing not at random). Explaining these is beyond the scope of this forum, but--if you're not familiar with these terms--there are a number of good introductory-level descriptions on the web.  &lt;/p&gt;&#10;&#10;&lt;p&gt;The above can be quite time-consuming depending on the number of candidate variables for your imputation model; however, this has the &quot;silver-lining&quot; advantage of clarifying what's driving the imputation. There are also a number of good diagnostic tools that allow you to evaluate and compare different imputation models.&lt;/p&gt;&#10;&#10;&lt;p&gt;Mplus enables one to do all of this more quickly; basically, it models the state of being missing using ML estimation. You can read more about this at statmodel.com.&lt;/p&gt;&#10;&#10;&lt;p&gt;I agree that single imputation or dropping all missing cases is probably not the best approach, depending, of course, are your research questions. If SAS is an available language and you'd like to talk about this in more detail, please post. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-12-19T18:45:55.687" Id="46257" LastActivityDate="2012-12-19T20:10:02.963" LastEditDate="2012-12-19T20:10:02.963" LastEditorUserId="7290" OwnerUserId="17980" ParentId="10350" PostTypeId="2" Score="3" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;Is there any easy-to-use software for Tukey median-polishing rows and columns with lots of missing values?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-12-19T20:04:15.560" Id="46263" LastActivityDate="2012-12-20T02:28:26.867" OwnerUserId="17984" PostTypeId="1" Score="5" Tags="&lt;median&gt;" Title="Software for median polishing" ViewCount="209" />
  <row AnswerCount="0" Body="&lt;p&gt;I have data that appears very well-suited to discrete-time survival analysis, in that each subject has a consecutive sequence of observations from its entry into the system until its exit, and I'm interested in determining the likelihood that a subject will leave the system in the next time period.  The catch is that the time periods are not uniform in length.  There are several possible ways I can imagine addressing this problem.  The top two that come to mind are:&#10;1.  increasing the granularity by further discretizing my time intervals to the minimum length&#10;2.  leaving the individual time periods as is, but weighting each period in the analysis by its actual length&#10;I prefer the second option to the first one.  My questions are:&#10;1.  Is the second option acceptable?&#10;2.  Which of the two options is preferable?&#10;3.  Is there a better third option?&#10;Thank you!&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2012-12-19T20:05:34.207" Id="46264" LastActivityDate="2012-12-19T20:05:34.207" OwnerUserId="17813" PostTypeId="1" Score="2" Tags="&lt;survival&gt;&lt;discrete-data&gt;" Title="Discrete time survival analysis with varying length event windows" ViewCount="130" />
  <row AnswerCount="0" Body="&lt;p&gt;Suppose I have fitted an standard linear regression mode $Y=\beta_0+\beta_1X_1+\beta_2X_2+\beta_3X_3+\epsilon$. Based on the ACF plot or PACF plot of the residuals for this regreesion model, I found that the $\epsilon$'s are serially auto-correlated that can be modeled as an AR(1) i.e. $\epsilon=\phi\epsilon_{t-1}+\nu_t$, where $\nu_t$ are white noise. I decided to fit the above model with &lt;a href=&quot;http://en.wikipedia.org/wiki/Generalized_least_squares&quot; rel=&quot;nofollow&quot;&gt;Generalized least squares&lt;/a&gt; approach with the following code&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(nlme)&#10;model.gls=gls(Y~X_1+X_2+X_3,data=dat, correlation=corARMA(p=1), method=&quot;ML&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;A part of output in R is as follows:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Correlation Structure: AR(1)&#10; Formula: ~1 &#10; Parameter estimate(s):&#10;      Phi &#10;0.8067566 &#10;&#10;Coefficients:&#10;                Value  Std.Error    t-value p-value&#10;(Intercept) -3.172158 0.01504180 -210.88950  0.0000&#10;x1          -3.890222 0.14141947  -27.50839  0.0000&#10;x2           0.089368 0.11667430    0.76596  0.4456&#10;x3          -0.155348 0.10190694   -1.52441  0.1308&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Clearly $X_2$ and $X_3$ are not significant based on the t-test.    &lt;/p&gt;&#10;&#10;&lt;p&gt;Q1: Is it a correct approach to drop $X_2$ and $X_3$ and refit a new Generalized least squares model with the &lt;strong&gt;same&lt;/strong&gt; correlation structure (i.e. AR(1))? In this case the code should be like:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;model.gls=gls(Y~X_1,data=dat, correlation=corARMA(p=1), method=&quot;ML&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I am asking this because the AR(1) has been selected while all $X_1$, $X_2$ and $X_3$ have been included in the linear regression model. So dropping two terms may change the correlation structure (i.e AR(1)) to another ARMA model.   &lt;/p&gt;&#10;&#10;&lt;p&gt;If the approach in Q1 is not correct, how can we proceed and handle the non-significant terms, $X_2$ and $X_3$ in the fitted Generalized least squares?    &lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2012-12-19T21:35:37.490" Id="46270" LastActivityDate="2012-12-20T04:45:19.107" LastEditDate="2012-12-20T04:45:19.107" LastEditorUserId="13138" OwnerUserId="13138" PostTypeId="1" Score="0" Tags="&lt;time-series&gt;&lt;multiple-regression&gt;&lt;t-test&gt;&lt;autocorrelation&gt;&lt;gls&gt;" Title="Generalized least squares with insignificant predictor variable" ViewCount="240" />
  
  <row Body="&lt;p&gt;I don't understand at all; I think maybe you have some fundamental misperceptions. &lt;/p&gt;&#10;&#10;&lt;p&gt;First, you don't group data for ANOVA, you do ANOVA because your data is grouped. ANOVA and regression are the same thing. The distinction between ANOVA and regression is that ANOVA is sort of designed for categorical independent variables such as group. But you can do this in regression with dummy variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, you probably shouldn't be grouping your data. You should tell us what your dependent and independent variables are.&lt;/p&gt;&#10;&#10;&lt;p&gt;Second, ANOVA does not require that the data be normally distributed, it requires that the residuals from the model be normally distributed. You can't tell that until you run your model.&lt;/p&gt;&#10;&#10;&lt;p&gt;Third, with only 4 cases in a group (or even with 8 cases, which is what you have) no test of normality is going to have much power to detect anything but the most extreme violations of normality.&lt;/p&gt;&#10;&#10;&lt;p&gt;You may find this post helpful: &lt;a href=&quot;http://www.statisticalanalysisconsulting.com/how-to-ask-a-statistics-question/&quot; rel=&quot;nofollow&quot;&gt;How to ask a Statistics Question&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-12-19T22:08:39.943" Id="46274" LastActivityDate="2012-12-19T22:08:39.943" OwnerUserId="686" ParentId="46272" PostTypeId="2" Score="0" />
  <row AnswerCount="1" Body="&lt;p&gt;I'm using Genetic Algorithms to do inputs selection in a time series problem. The issue is that the number of possible inputs is very large (100 possible inputs + inputs' lags) and I don't know a procedure to determine the minimum number of lags to consider. &lt;/p&gt;&#10;&#10;&lt;p&gt;My procedure was to run a correlation analysis ex ante and leave de top 70 inputs in terms of correlation with the desired output. The problem with this procedure is that, as I increase the number of possible lags in consideration, more &quot;ridiculous&quot; inputs (i.e lag 140) are selected...&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-12-19T23:32:28.443" Id="46276" LastActivityDate="2013-08-18T08:06:35.120" LastEditDate="2013-07-19T02:17:33.883" LastEditorUserId="7250" OwnerUserId="17992" PostTypeId="1" Score="0" Tags="&lt;time-series&gt;&lt;genetic-algorithms&gt;" Title="How to chose optimal number of lags and inputs?" ViewCount="255" />
  <row Body="&lt;p&gt;Tricky problem! Is location fixed or random? Is position fixed or random? I assume that sample is random.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Since treatment is assigned to location, location is the sampling unit. Basically, the comparison between treatments is done at that level. $n=8$.&lt;/li&gt;&#10;&lt;li&gt;The measurement unit is the observation you take on your &quot;samples&quot; at a given time.&lt;/li&gt;&#10;&lt;li&gt;Location is not nested in treatment. The treatment is applied to the location.&lt;/li&gt;&#10;&lt;li&gt;Position is nested inside location.&lt;/li&gt;&#10;&lt;li&gt;Sample is nested inside position.&lt;/li&gt;&#10;&lt;li&gt;Time is nested inside Sample.&lt;/li&gt;&#10;&lt;li&gt;Time is crossed with treatment.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;You have 3 levels of nesting (time within sample, sample within position, position within location).&lt;/p&gt;&#10;&#10;&lt;p&gt;If location, position and sample are random, I think the R formula will look like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; Y ~ Treatment * Time +(1|location|position|sample)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;You have 1 row in your data frame for each sample observation at each time - with appropriate codings for all of your design characteristics.&lt;/p&gt;&#10;&#10;&lt;p&gt;Would it work to combine the repeated measures into a score such as their average or their difference? That could make the model easier to interpret.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2012-12-20T01:25:52.530" Id="46278" LastActivityDate="2012-12-20T04:13:53.743" LastEditDate="2012-12-20T04:13:53.743" LastEditorUserId="14188" OwnerUserId="14188" ParentId="46259" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;WEKA is fully implemented in Java. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.cs.waikato.ac.nz/ml/weka/&quot; rel=&quot;nofollow&quot;&gt;http://www.cs.waikato.ac.nz/ml/weka/&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;It has its own manual and &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0123748569&quot; rel=&quot;nofollow&quot;&gt;book&lt;/a&gt;. You can find the manual in the WEKA directory if you install it or straight away in the zip file if you don't download the self-extracting.&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2012-12-20T04:33:25.157" CreationDate="2012-12-20T04:33:25.157" Id="46281" LastActivityDate="2012-12-20T04:33:25.157" OwnerUserId="2719" ParentId="46252" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; In the original post, I noted that (aside from a minor error which has since been corrected) this was actually a problem with plotting the density, not with the simulation. At the time I wasn't sure what was going on, but I found that truncating the data seemed to solve the problem, so I recommended that. I have since figured out the real problem, so I decided to edit this to suggest a better solution.&lt;/p&gt;&#10;&#10;&lt;p&gt;With $n=3$, the distribution we are simulating from has very heavy tails. Even though the mean is zero and the inter-quartile range is about 4, in 100,000 draws (or more) we get a few on the order of 1,000,000 in absolute value.&lt;/p&gt;&#10;&#10;&lt;p&gt;By default, the &lt;code&gt;density&lt;/code&gt; function estimates the density at 512 equally-spaced points that completely cover the range of the data (plus a little). In this case, we were only interested in plotting the density over a very small portion of that range--for $x \in [0,5]$. As a result, when we plotted that window, we got what looked like a flat horizontal line, which connected the estimates at points on the order of -2000 and 2000. Not very useful.&lt;/p&gt;&#10;&#10;&lt;p&gt;My original solution, truncating the data, gave decent looking plots, but wasn't really very satisfying. So I played around with it a bit and figured out the problem and how to solve it: the &lt;code&gt;density&lt;/code&gt; function has arguments &lt;code&gt;from&lt;/code&gt; and &lt;code&gt;to&lt;/code&gt;, which give lower and upper bounds between which is estimates the density. By setting &lt;code&gt;from=0, to=5&lt;/code&gt; we get the following plot, without any truncation:&#10;&lt;img src=&quot;http://i.stack.imgur.com/QejQu.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;This is both easier and more correct (since the density is distorted slightly by the truncation) than my original solution. The complete code I used (adapted slightly from the code provided in the question) is:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;v &amp;lt;- rt(100000, 1)/sqrt(3-2)&#10;w &amp;lt;- rchisq(100000,2)&#10;z &amp;lt;- rnorm(n=100000, m=0, sd=1) &#10;z_eff_3 &amp;lt;- v + z * sqrt((3*(1+v*v))/w)&#10;&#10;plot(density(z_eff_3,from=0,to=5),xlim=c(0,5))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Hope that helps.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2012-12-20T05:18:00.730" Id="46282" LastActivityDate="2012-12-21T00:42:49.503" LastEditDate="2012-12-21T00:42:49.503" LastEditorUserId="16297" OwnerUserId="16297" ParentId="46280" PostTypeId="2" Score="9" />
  
  <row Body="&lt;p&gt;You may want to give random forests a try or bagging/boosting methods (I'm assuming you haven't tried these yet at this stage). &lt;/p&gt;&#10;&#10;&lt;p&gt;Usually these techniques work well for these kinds of problems. It will depend, if the classes are imbalanced but if they differ in characteristics then a classification method should have no problem distinguishing between them. &lt;/p&gt;&#10;&#10;&lt;p&gt;In some cases however the classifier might be the cause of the imbalance. You can scale the predictions however based on the confusion matrix. The main idea is listed here: &lt;a href=&quot;http://www.creatapreneur.com/2012/07/&quot; rel=&quot;nofollow&quot;&gt;http://www.creatapreneur.com/2012/07/&lt;/a&gt;. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-20T08:50:34.053" Id="46287" LastActivityDate="2012-12-20T08:50:34.053" OwnerUserId="17935" ParentId="16050" PostTypeId="2" Score="0" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm working in R, using glm.nb (of the MASS package) to model count data with a negative binomial regression model.  I'd like to get the standardized (&lt;em&gt;beta&lt;/em&gt;) coefficients from the model, but am given the unstandardized (&lt;em&gt;b&lt;/em&gt; &quot;Estimate&quot;) coefficients.&lt;/p&gt;&#10;&#10;&lt;p&gt;The R documentation does not seem to show of a way to retrieve the standardized beta weights easily for a negative bionomial regression model.&lt;/p&gt;&#10;&#10;&lt;p&gt;The R script is something like:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(&quot;MASS&quot;)&#10;nb = glm.nb(responseCountVar ~ predictor1 + predictor2 + &#10;    predictor3 + predictor4 + predictor5 + predictor6 + &#10;    predictor7 + predictor8 + predictor9 + predictor10 + &#10;    predictor11 + predictor12 + predictor13 + predictor14 + &#10;    predictor15 + predictor16 + predictor17 + predictor18 + &#10;    predictor19 + predictor20 + predictor21,&#10;    data=myData, control=glm.control(maxit=125))&#10;summary(nb)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and the output of the above is:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Deviance Residuals: &#10;    Min       1Q   Median       3Q      Max  &#10;-5.1462  -1.0080  -0.4247   0.2277   3.4336  &#10;&#10;Coefficients:&#10;                Estimate Std. Error z value Pr(&amp;gt;|z|)    &#10;(Intercept)   -3.059e+00  3.782e-01  -8.088 6.05e-16 ***&#10;predictor1    -2.447e+00  4.930e-01  -4.965 6.88e-07 ***&#10;predictor2    -1.004e+00  1.313e-01  -7.650 2.00e-14 ***&#10;predictor3     1.158e+00  1.440e-01   8.047 8.46e-16 ***&#10;predictor4     1.334e+00  7.034e-02  18.970  &amp;lt; 2e-16 ***&#10;predictor5     9.862e-01  2.006e-01   4.915 8.87e-07 ***&#10;predictor6     1.166e+00  2.378e+00   0.490  0.62392    &#10;predictor7    -1.057e-01  1.494e-01  -0.707  0.47936    &#10;predictor8     4.051e-01  7.318e-02   5.536 3.10e-08 ***&#10;predictor9    -3.320e-01  1.132e-01  -2.933  0.00336 ** &#10;predictor10    3.761e-01  1.561e-01   2.409  0.01600 *  &#10;predictor11    8.660e-02  4.332e-02   1.999  0.04557 *  &#10;predictor12   -1.583e-01  2.044e-01  -0.774  0.43872    &#10;predictor13    6.404e-02  3.972e-03  16.122  &amp;lt; 2e-16 ***&#10;predictor14    4.264e-03  2.297e-04  18.563  &amp;lt; 2e-16 ***&#10;predictor15    3.279e-03  5.697e-04   5.755 8.68e-09 ***&#10;predictor16    3.487e-03  3.447e-03   1.012  0.31177    &#10;predictor17    1.534e-04  1.647e-04   0.931  0.35182    &#10;predictor18   -7.606e-05  9.021e-05  -0.843  0.39917    &#10;predictor19    2.536e-04  1.733e-05  14.633  &amp;lt; 2e-16 ***&#10;predictor20    2.997e-02  4.977e-03   6.021 1.73e-09 ***&#10;predictor21    2.756e+01  3.508e+00   7.856 3.98e-15 ***&#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 &#10;&#10;(Dispersion parameter for Negative Binomial(0.9232) family taken to be 1)&#10;&#10;    Null deviance: 5631.1  on 1835  degrees of freedom&#10;Residual deviance: 2120.7  on 1814  degrees of freedom&#10;&#10;                                AIC: 19268    &#10;Number of Fisher Scoring iterations: 1    &#10;                              Theta: 0.9232 &#10;                          Std. Err.: 0.0282 &#10;                 2 x log-likelihood: -19221.9910&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;My question is&lt;/strong&gt;:  Is there a way to get the beta weights, or do I need to try to convert my unstandardized b coefficients to standardized beta coefficients (if so, how would I do that)?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-20T15:51:37.130" FavoriteCount="1" Id="46312" LastActivityDate="2012-12-20T16:27:22.777" LastEditDate="2012-12-20T15:56:45.253" LastEditorUserId="17915" OwnerUserId="17915" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;regression-coefficients&gt;&lt;negative-binomial&gt;&lt;standardization&gt;" Title="How to get the standardized beta coefficients from glm.nb regression in R?" ViewCount="2387" />
  
  <row Body="&lt;p&gt;I have never seen a model like Box-Jenkins identification process led me to ARIMA(0,1,3)  model BUT i had never seen a black swan until I went to Australia. Please post your data as it may suggest the need for&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Intervention Detection leading to including level shifts, local time trends et al&lt;/li&gt;&#10;&lt;li&gt;Time varying parameters&lt;/li&gt;&#10;&lt;li&gt;Time varying error variance&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;You cam also send it directly to me at dave@autobox.com and I will post it to the web.&lt;/p&gt;&#10;&#10;&lt;p&gt;If your data is confidential, simply scale it.&lt;/p&gt;&#10;&#10;&lt;p&gt;OK having received your data (some 80000 readings), I selected 805 observations starting at point 6287 and obtained.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Mma71.jpg&quot; alt=&quot;enter image description here&quot;&gt; . A significant change point was detected at period 137 suggesting time-varying parameters. The remaining 668 observations suggest a pdq ARIMA Model (3,0,0) with a level.step shift supporting your preliminary conclusions about lag 3. &lt;img src=&quot;http://i.stack.imgur.com/k2LHc.jpg&quot; alt=&quot;enter image description here&quot;&gt; . The Actual/Fit/Forecast graph is &lt;img src=&quot;http://i.stack.imgur.com/J5Lei.jpg&quot; alt=&quot;enter image description here&quot;&gt; The Residual Plot &lt;img src=&quot;http://i.stack.imgur.com/GKzQ7.jpg&quot; alt=&quot;enter image description here&quot;&gt; and the acf of the residuals is &lt;img src=&quot;http://i.stack.imgur.com/i1pfU.jpg&quot; alt=&quot;enter image description here&quot;&gt;. Since the acf of the residuals shows strong structure at periods 5 and 10 ,  &lt;img src=&quot;http://i.stack.imgur.com/qo1nz.jpg&quot; alt=&quot;enter image description here&quot;&gt; you might further investigate seasonal structure at lag 5. I hope this helps.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-20T20:51:22.287" Id="46332" LastActivityDate="2012-12-25T17:18:41.140" LastEditDate="2012-12-25T17:18:41.140" LastEditorUserId="3382" OwnerUserId="3382" ParentId="45485" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Sure. First, sort by average number of actions. Then make (say) 4 graphs, each with 25 lines, one for each quartile. That means you can shrink the y-axes (but make the y axis label clear). And with 25 lines, you can vary them by line type and color and perhaps plotting symbol and get some clarity&lt;/p&gt;&#10;&#10;&lt;p&gt;Then stack the graphs vertically with a single time axis.&lt;/p&gt;&#10;&#10;&lt;p&gt;This would be pretty easy in R or SAS (at least if you have v. 9 of SAS). &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-12-20T21:36:22.160" Id="46335" LastActivityDate="2012-12-20T21:45:27.663" LastEditDate="2012-12-20T21:45:27.663" LastEditorUserId="686" OwnerUserId="686" ParentId="46334" PostTypeId="2" Score="7" />
  <row Body="&lt;p&gt;The &lt;code&gt;fit.mult.impute&lt;/code&gt; function in the &lt;code&gt;Hmisc&lt;/code&gt; package will draw imputations created from &lt;code&gt;mice&lt;/code&gt; just as it will from &lt;code&gt;aregImpute&lt;/code&gt;.  &lt;code&gt;cph&lt;/code&gt; will work with &lt;code&gt;fit.mult.impute&lt;/code&gt;.  The harder question is how to do validation through resampling when also doing multiple imputation.  I don't think anyone has really solved that.  I usually take the easy way out and use single imputation to validate the model, using the &lt;code&gt;Hmisc transcan&lt;/code&gt; function, but using multiple imputation to fit the final model and to get standard errors.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-12-20T22:57:14.080" Id="46344" LastActivityDate="2012-12-20T22:57:14.080" OwnerUserId="4253" ParentId="46333" PostTypeId="2" Score="8" />
  <row Body="&lt;p&gt;I am not sure I understand the question. The '+' function in R does this already.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; a&amp;lt;-matrix(1:12,nr=3)&#10;&amp;gt; b&amp;lt;-rnorm(12)  # b is a vector&#10;&amp;gt; a+b           # R adds b to a as a matrix&#10;         [,1]     [,2]     [,3]     [,4]&#10;[1,] 1.144552 4.946283 8.026290 11.40905&#10;[2,] 1.038299 5.752317 7.544441 10.78278&#10;[3,] 3.173348 5.574810 9.805634 13.65378&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;What do you actually need a function for, that this doesn't do?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-12-21T01:27:15.753" Id="46353" LastActivityDate="2012-12-21T01:27:15.753" OwnerUserId="805" ParentId="46302" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Cross-validation gives a measure of out-of-sample accuracy by averaging over several random partitions of the data into training and test samples. It is often used for parameter tuning by doing cross-validation for several (or many) possible values of a parameter and choosing the parameter value that gives the lowest cross-validation average error.&lt;/p&gt;&#10;&#10;&lt;p&gt;So the process itself doesn't give you a model or parameter estimates, but you can use it to help choose between alternatives.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-21T05:59:07.813" Id="46369" LastActivityDate="2012-12-21T05:59:07.813" OwnerUserId="16297" ParentId="46368" PostTypeId="2" Score="10" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have dataset with large number of labelled data (say that there are &lt;em&gt;k&lt;/em&gt; classes). I have also another, much smaller dataset with unlabelled data that I want also to be labelled. The problem is that in the second dataset, number of classes need not to be the same as number of classes in first dataset (more precisely, the there may be more classes). In other words, some objects from unlabelled dataset can be classified to one of &lt;em&gt;k&lt;/em&gt; possible classes since they are &quot;too close of them&quot; but some objects should be clasified to new classes.&lt;/p&gt;&#10;&#10;&lt;p&gt;Real world example: In some applications, number of classes can increase with time. For example, number of known species increases with time, so when I see any unknown plant in nature, biologist can either to classify it or to say that this is new discovery (new class).&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there any technique that deals with this type of classification?&#10;Any help is appreciated.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-21T13:19:56.010" Id="46383" LastActivityDate="2012-12-21T13:19:56.010" OwnerUserId="14730" PostTypeId="1" Score="2" Tags="&lt;classification&gt;" Title="Classification with increasing number of classes" ViewCount="62" />
  
  <row Body="&lt;p&gt;The original t-table was created using simulation, so there is a clear precident.  Most bootstrap and permutation tests use simulation to determine the critical values.  Fisher even justified the use of the t-test as a good approximation to the permutation test that did not require as much computation.  As long as you are comforatable with the assumptions that go into your simulations then I see no problems with your approach (often I am more comforatable with the assumptions in a simulation (which I know and control) than with the assumptions from a canned procedure (some of which might not be known or apparent)).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-12-21T17:07:43.917" Id="46398" LastActivityDate="2012-12-21T17:07:43.917" OwnerUserId="4505" ParentId="46295" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="46412" AnswerCount="2" Body="&lt;p&gt;What is $\rho_{XY}$ when $X=0$ and $Y=0$? For all $X=Y$, $\rho_{XY}=1$ and this should be no exception. But using the following: $$\rho_{XY}=\frac{E(XY)-E(X)E(Y)}{\sqrt{Var(X)Var(Y)}}$$ yields a $\frac{0}{0}$ form. How do I take the limits or apply L'Hospital's rule to prove this equals 1?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-12-21T22:58:53.067" FavoriteCount="1" Id="46410" LastActivityDate="2013-09-04T14:01:11.120" LastEditDate="2013-09-04T14:01:11.120" LastEditorUserId="27581" OwnerUserId="10907" PostTypeId="1" Score="4" Tags="&lt;probability&gt;&lt;correlation&gt;" Title="What is the correlation coefficient between two zero random variables?" ViewCount="558" />
  <row Body="&lt;p&gt;$\frac{0}{0}$ seems correct to me. That is, it's meaningless. The correlation measures the linear relationship between two variables. But if either variable is a constant this is a meaningless idea. It isn't 0, it isn't 1, it's just ... not. So, $\frac{0}{0}$ seems right.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-21T23:04:56.813" Id="46411" LastActivityDate="2012-12-21T23:04:56.813" OwnerUserId="686" ParentId="46410" PostTypeId="2" Score="3" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am learning the generalized estimating equations (GEE) and the &lt;code&gt;geepack&lt;/code&gt; R package. There are some questions that I am a little confused. &lt;/p&gt;&#10;&#10;&lt;p&gt;In a GEE-constructed model, we have $Var(Y_{it})=\phi_{it}\cdot V(\mu_{it})$, where $\phi$ is the scale parameter. We further decompose $Var(Y)$ into $V^{1/2}R(\alpha)V^{1/2}$ where $\alpha$ is the correlation parameter. &lt;strong&gt;Three link-models&lt;/strong&gt; are specified in &lt;code&gt;geepack&lt;/code&gt;, for $\mu,\phi,\alpha$, respectively. See &lt;a href=&quot;http://cran.r-project.org/doc/Rnews/Rnews_2002-3.pdf&quot;&gt;this&lt;/a&gt; PDF file for details. &lt;/p&gt;&#10;&#10;&lt;p&gt;(1) In GEE1, can I say we only need to make sure that the mean structure is correctly specified, i.e. the link model $g(\mu)=X\beta$ is correct, while it doesn't matter whether the link models for $\phi$ and $\alpha$ are correctly specified?&lt;/p&gt;&#10;&#10;&lt;p&gt;(2) By default, the &lt;code&gt;geese.fit&lt;/code&gt; function makes the scale value &lt;code&gt;scale.value = 1.0&lt;/code&gt; -- does it say $\phi=1.0$? It is understandable that the default &lt;code&gt;alpha=NULL&lt;/code&gt; as people can specify different correlation structures and the program will assign appropriate &lt;code&gt;alpha&lt;/code&gt; values accordingly. My question is: how often people try to explicitly model the scale parameter $\phi$?&lt;/p&gt;&#10;&#10;&lt;p&gt;(3) This question is closely related to (2) about the scale parameter $\phi$. Recall the variance function is $Var(Y_{it})=\phi_{it}\cdot V(\mu_{it})$. In the Gaussian case, we have $\phi=\sigma^2$ and $V(\mu_{it})=1$; in the binomial case, we have $\phi=1$ and $V(\mu_{it})=\mu_{it}(1-\mu_{it})$; in the Poisson case, we have $\phi=1$ and $V(\mu_{it})=\mu_{it}$. Can I say that, in the negative binomial case, $\phi=1$ and $V(\mu_{it})=\mu_{it}+\varphi\mu_{it}^2$? Here $\varphi$ is the NB2 dispersion parameter, NOT the scale parameter $\phi$ in GEE.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you very much!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-22T03:41:56.273" FavoriteCount="2" Id="46420" LastActivityDate="2012-12-24T04:32:35.363" LastEditDate="2012-12-22T05:03:42.320" LastEditorUserId="14156" OwnerUserId="14156" PostTypeId="1" Score="5" Tags="&lt;r&gt;&lt;gee&gt;" Title="The role of scale parameter in GEE" ViewCount="424" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;The &lt;code&gt;summary.rq&lt;/code&gt; function from the &lt;a href=&quot;http://cran.r-project.org/web/packages/quantreg/quantreg.pdf&quot;&gt;quantreg vignette&lt;/a&gt; provides a multitude of choices for standard error estimates of quantile regression coefficients. What are the special scenarios where each of these becomes optimal/desirable?&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;&quot;rank&quot; which produces confidence intervals for the estimated parameters by inverting a rank test as described in Koenker (1994). The default option assumes that the errors are iid, while the option iid = FALSE implements the proposal of Koenker Machado (1999). See the documentation for rq.fit.br for additional arguments.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&quot;iid&quot; which presumes that the errors are iid and computes an estimate of the asymptotic covariance matrix as in KB(1978).&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&quot;nid&quot; which presumes local (in tau) linearity (in x) of the the conditional quantile functions and computes a Huber sandwich estimate using a local estimate of the sparsity.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&quot;ker&quot; which uses a kernel estimate of the sandwich as proposed by Powell(1990).&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&quot;boot&quot; which implements one of several possible bootstrapping alternatives for estimating standard errors.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I have read at least 20 empirical papers where this is applied either in the time-series or the cross-sectional dimension and haven't seen a mention of standard error choice. &lt;/p&gt;&#10;" CommentCount="6" CreationDate="2012-12-22T11:19:54.007" FavoriteCount="7" Id="46434" LastActivityDate="2013-09-11T12:12:38.470" LastEditDate="2012-12-23T11:04:46.303" LastEditorUserId="16811" OwnerUserId="16811" PostTypeId="1" Score="27" Tags="&lt;r&gt;&lt;standard-error&gt;&lt;quantile-regression&gt;&lt;estimators&gt;" Title="Quantile regression: Which standard errors?" ViewCount="1319" />
  
  <row Body="&lt;p&gt;Last summer, Kaggle ran a competition to predict a users psychopathy,  Machiavellianism, Narcissism etc. using only Twitter data. You can see the competition and results here:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://www.kaggle.com/c/twitter-personality-prediction&quot; rel=&quot;nofollow&quot;&gt;personality competition&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://www.kaggle.com/c/twitter-psychopathy-prediction&quot; rel=&quot;nofollow&quot;&gt;psychopathy&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I recall there was a published paper on the psychopathy prediction, here: &#10;&lt;a href=&quot;http://www.onlineprivacyfoundation.org/research_/Sumner_Predicting_Dark_Triad_Traits_from_Twitter_Usage_V5.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.onlineprivacyfoundation.org/research_/Sumner_Predicting_Dark_Triad_Traits_from_Twitter_Usage_V5.pdf&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-22T17:32:43.583" Id="46443" LastActivityDate="2012-12-22T17:32:43.583" OwnerUserId="11867" ParentId="46304" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;The answer is actually already there in the question, staring us in the face.  All we need to do is interpret it appropriately.&lt;/p&gt;&#10;&#10;&lt;p&gt;Look at the constants in the definition of $f$: $k^r$ divided by $(r-1)!$. The first factor should really be absorbed by the $x$'s and the implicit $dx$.  That is,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$k^r x^{r-1} dx = (kx)^{r-1} d(kx).$$&lt;/p&gt;&#10;&#10;&lt;p&gt;This makes it clear that the distribution is really a function of $kx$, revealing $k$ as a &lt;em&gt;scale factor.&lt;/em&gt;  If we can find the mean $\mu$ and variance $\sigma^2$ for $k=1$, we would only need to divide them by $k$ and $k^2$, respectively, to get the fully general answer.&lt;/p&gt;&#10;&#10;&lt;p&gt;That observation reduces the question to one of finding various &lt;em&gt;moments&lt;/em&gt; of &lt;/p&gt;&#10;&#10;&lt;p&gt;$$f_r(x) = \frac{x^r \exp(-x)}{x (r-1)!}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;There's only one constant in there, &lt;em&gt;and it must be there to make sure this distribution integrates to unity.&lt;/em&gt;  (That is the import of @jbowman's hint in the comments.)  In other words, you already know that&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\int_0^\infty x^r \exp(-x) \frac{dx}{x} = (r-1)!.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;To find the $j$th moment ($j=1,2$ are all that are needed), you must obtain a formula for&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\int_0^\infty x^j f_r(x) dx = \int_0^\infty  x^j\left(x^r \exp(-x) \frac{dx}{x}\right) = \int_0^\infty x^{r+j} \exp(-x) \frac{dx}{x}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;It is now but a moment's work to apply the penultimate formula to this expression to obtain the answers.  I leave the details--which are simple algebra, since the integration has been done--to the interested reader.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2012-12-22T18:25:45.130" Id="46445" LastActivityDate="2012-12-23T22:27:44.850" LastEditDate="2012-12-23T22:27:44.850" LastEditorUserId="919" OwnerUserId="919" ParentId="46422" PostTypeId="2" Score="3" />
  
  <row AnswerCount="0" Body="&lt;p&gt;In short: I am currently reading Online Learning with Kernels (http://books.nips.cc/papers/files/nips14/AA33.pdf) for fun and I can't figure out how he got to equation 8 from equations 6 and 7.&lt;/p&gt;&#10;&#10;&lt;p&gt;The idea is: We want to minimize a risk function $R_{stoch}[f,t]:=c(x_t,y_t,f(x_t))+\lambda\Omega[f]$. If we want apply the representer theorem on f, writing it as $f(x)=\sum\alpha_i k(x,x_i)$, how can we get to the STOCHASTIC gradient descent update? Say we take the soft margin loss for SVMs. It would be easy to take the gradient w.r.t. to f and loss (well sub-gradient for loss) and do gradient descent. But for online learning with stochastic gradient descent, I'm kinda lost.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you! Please do not hesitate to ask further details. Any help would be greatly appreciated.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-12-23T00:38:09.247" Id="46457" LastActivityDate="2012-12-23T00:38:09.247" OwnerUserId="18059" PostTypeId="1" Score="2" Tags="&lt;machine-learning&gt;&lt;mathematical-statistics&gt;&lt;svm&gt;&lt;optimization&gt;&lt;gradient-descent&gt;" Title="Kernel SVM in primal training with Stochastic Gradient Descent" ViewCount="354" />
  <row AcceptedAnswerId="46464" AnswerCount="1" Body="&lt;p&gt;Suppose $n = 5$, $s = 0.0771$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The question is: what's the $P(\sigma &amp;lt; 7.63\%)$?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-12-23T01:42:49.667" Id="46460" LastActivityDate="2012-12-23T18:36:28.427" LastEditDate="2012-12-23T18:36:28.427" LastEditorUserId="8077" OwnerUserId="9136" PostTypeId="1" Score="-1" Tags="&lt;mathematical-statistics&gt;&lt;chi-squared&gt;&lt;fiducial&gt;" Title="Probability distribution of $\sigma$" ViewCount="211" />
  
  
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I found this book &quot;Statistical methods in atmospheric sciences&quot; by Daniel Wilks in the library. Do you think it is good for learning statistical methods in general? &#10;The examples are as you might know or expect, from atmospheric sciences but discussion about the statistical methods is quite good. &lt;/p&gt;&#10;" ClosedDate="2013-06-20T03:05:29.743" CommentCount="1" CreationDate="2012-12-24T07:06:07.630" Id="46494" LastActivityDate="2013-06-15T15:10:34.410" OwnerUserId="2959" PostTypeId="1" Score="2" Tags="&lt;references&gt;" Title="Statistical methods in atmospheric sciences" ViewCount="155" />
  
  <row Body="&lt;p&gt;One difference I noted was that PCA can only give you either the term-term or Document-Document similarity (depending on how you multiplied the coreference matrix AA* or A*A) but SVD/LSA can deliver both since you have eigenvectors of both AA* and A*A. Actually I don't see a reason to use PCA ever over SVD.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-24T11:31:08.377" Id="46498" LastActivityDate="2012-12-24T11:31:08.377" OwnerUserId="17941" ParentId="21311" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;Parrondo's Paradox:&lt;/p&gt;&#10;&#10;&lt;p&gt;From &lt;a href=&quot;http://en.wikipedia.org/wiki/Parrondo%27s_paradox&quot; rel=&quot;nofollow&quot;&gt;wikipdedia&lt;/a&gt;: &quot;Parrondo's paradox, a paradox in game theory, has been described as: A combination of losing strategies becomes a winning strategy. It is named after its creator, Juan Parrondo, who discovered the paradox in 1996. A more explanatory description is:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;There exist pairs of games, each with a higher probability of losing&#10;  than winning, for which it is possible to construct a winning strategy&#10;  by playing the games alternately.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Parrondo devised the paradox in connection with his analysis of the Brownian ratchet, a thought experiment about a machine that can purportedly extract energy from random heat motions popularized by physicist Richard Feynman. However, the paradox disappears when rigorously analyzed.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;As alluring as the paradox might sound to the financial crowd, it does have requirements that are not readily available in financial time series.  Even though a few of the component strategies can be losing, the offsetting strategies require unequal and stable probabilities of much greater or less than 50% in order for the ratcheting effect to kick in.&#10;It would be difficult to find financial strategies, whereby one has $P_B(W)=3/4+\epsilon$ and the other, $P_A(W)=1/10 + \epsilon$, over long periods.&lt;/p&gt;&#10;&#10;&lt;p&gt;There's also a more recent related paradox called the &quot;&lt;a href=&quot;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=allison%20mixture&amp;amp;source=web&amp;amp;cd=3&amp;amp;ved=0CD0QFjAC&amp;amp;url=http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.115.9270&amp;amp;rep=rep1&amp;amp;type=pdf&amp;amp;ei=k-_YULSUA6mGiQKZxoH4DA&amp;amp;usg=AFQjCNHkT5KO8YHdkUPOe6vqR95NDpSW6Q&amp;amp;bvm=bv.1355534169,d.cGE&amp;amp;cad=rja&quot; rel=&quot;nofollow&quot;&gt;allison mixture&lt;/a&gt;,&quot; that shows we can take two IID and non-correlated series, and randomly scramble them such that certain mixtures can create a resulting series with non-zero autocorrelation.&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2012-12-25T00:08:00.193" CreationDate="2012-12-25T00:08:00.193" Id="46513" LastActivityDate="2012-12-27T17:19:36.683" LastEditDate="2012-12-27T17:19:36.683" LastEditorUserId="1913" OwnerUserId="13519" ParentId="23779" PostTypeId="2" Score="4" />
  
  
  
  <row AcceptedAnswerId="46796" AnswerCount="3" Body="&lt;p&gt;Is there a more scientific way of determining the number of significant digits to report for a mean or a confidence interval in a situation which is fairly standard - e.g. first year class at college.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have seen &lt;a href=&quot;http://stats.stackexchange.com/questions/8734/number-of-significant-figures-to-put-in-a-table&quot;&gt;Number of significant figures to put in a table&lt;/a&gt;, &lt;a href=&quot;http://stats.stackexchange.com/questions/46209/why-dont-we-use-significant-digits&quot;&gt;Why don't we use significant digits&lt;/a&gt; and &lt;a href=&quot;http://stats.stackexchange.com/questions/45926/number-of-significant-figures-in-a-chi-square-fit&quot;&gt;Number of significant figures in a chi square fit&lt;/a&gt;, but these don't seem to put their finger on the problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;In my classes I try to explain to my students that it is a waste of ink to report 15 significant digits when they have such a wide standard error in their results - my gut feeling was that it should be rounded to about somewhere of the order of $0.25\sigma$.  This is not too different from what is said by &lt;a href=&quot;http://www.astm.org/SNEWS/SO_2008/datapoints_so08.html&quot;&gt;ASTM - Reporting Test Results&lt;/a&gt; referring to E29 where they say it should be between $0.05\sigma$ and $0.5\sigma$.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;When I have a set of numbers like &lt;code&gt;x&lt;/code&gt; below, how many digits should I use to print the mean and standard deviation?&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(123)&#10;x &amp;lt;- rnorm(30) # default mean=0, sd=1&#10;# R defaults to 7 digits of precision options(digits=7)&#10;mean(x) # -0.04710376 - not far off theoretical 0&#10;sd(x) # 0.9810307 - not far from theoretical 1&#10;sd(x)/sqrt(length(x)) # standard error of mean 0.1791109&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;QUESTION: Spell out in detail what the precision is (when there is a vector of double precision numbers) for mean and standard deviation in this and write a simple R pedagogical function which will print the mean and standard deviation to the significant number of digits that is reflected in the vector &lt;code&gt;x&lt;/code&gt;.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2012-12-25T08:52:35.340" FavoriteCount="5" Id="46519" LastActivityDate="2013-01-01T14:28:09.593" LastEditDate="2012-12-28T08:15:45.987" LastEditorUserId="6579" OwnerUserId="6579" PostTypeId="1" Score="9" Tags="&lt;standard-deviation&gt;&lt;error&gt;&lt;reporting&gt;&lt;communication&gt;" Title="Number of significant digits to report" ViewCount="2879" />
  <row Body="&lt;p&gt;These are &quot;phase type distributions&quot; and great fun!  David Lando goes into a lot of detail in &quot;Credit Risk Modeling: Theory and Applications&quot;&lt;a href=&quot;http://www.amazon.co.uk/Credit-Risk-Modeling-Applications-Princeton/dp/0691089299/ref=sr_1_3?s=books&amp;amp;ie=UTF8&amp;amp;qid=1356427230&amp;amp;sr=1-3&quot; rel=&quot;nofollow&quot;&gt;Credit Risk Modelling: Theory and Applications&lt;/a&gt;.  Lando details how to model these based on the individual transactions rather than the more crude counts at the end of a month. I'm afraid I don't have my copy here.&lt;/p&gt;&#10;&#10;&lt;p&gt;How you implement will depend a lot on your exact rules for transitions.  What happens for partial payments?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-25T09:27:04.543" Id="46520" LastActivityDate="2012-12-25T09:27:04.543" OwnerUserId="6579" ParentId="41210" PostTypeId="2" Score="0" />
  
  
  
  
  
  
  <row AcceptedAnswerId="46600" AnswerCount="2" Body="&lt;p&gt;I have a data set with approximately 500 observations on eight key variables. There are a lot of missing data; only about 1/12 of the observations are complete. I am using &lt;code&gt;PROC MI&lt;/code&gt; and &lt;code&gt;MIANALYZE&lt;/code&gt; in SAS to run various regressions on multiply imputed data, and this is working well.  (There are about 200 variables in total, and there are high correlations among them which helps multiple imputation.) &lt;/p&gt;&#10;&#10;&lt;p&gt;However, I would also like to do factor analysis on the imputed data. This does not appear to be easily done in SAS, and it poses some interesting challenges:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;The signs of the factors are arbitrary, so different imputations could yield opposite signs; &lt;/li&gt;&#10;&lt;li&gt;What was factor 1 in one imputation could be factor 2 in another imputation (although in this case, with so few key variables, it is likely that one factor is enough),&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;and probably other issues as well.&lt;/p&gt;&#10;&#10;&lt;p&gt;I could do exploratory factor analysis on each imputed data set, of course, and I could then average them on a sort of ad-hoc basis, but this seems very sloppy. &lt;/p&gt;&#10;&#10;&lt;p&gt;Some Googling did not reveal any solutions, but ... well, Google doesn't always find everything.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any help, ideas, references appreciated.  I have access to SAS and to R.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2012-12-26T21:25:32.333" Id="46569" LastActivityDate="2013-08-08T13:33:17.653" LastEditDate="2012-12-27T11:26:49.893" LastEditorUserId="930" OwnerUserId="686" PostTypeId="1" Score="7" Tags="&lt;factor-analysis&gt;&lt;missing-data&gt;&lt;multiple-imputation&gt;" Title="Factor analysis on multiply imputed data" ViewCount="791" />
  
  
  
  
  
  
  <row Body="&lt;p&gt;With less than a year of data, it'll be impossible to account for any kind of yearly seasonal effect. (For example, if your data was shopping-related, you would have things like annual holidays, perhaps two sales a year, etc.)&lt;/p&gt;&#10;&#10;&lt;p&gt;You might want to look at Statistical Process Control tools like &lt;a href=&quot;http://en.wikipedia.org/wiki/Control_chart&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Control_chart&lt;/a&gt; perhaps?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-27T15:48:35.693" Id="46613" LastActivityDate="2012-12-27T15:48:35.693" OwnerUserId="1764" ParentId="9886" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;The following is what I ended up using:&lt;/p&gt;&#10;&#10;&lt;p&gt;Write $\sigma(N(\mu,s^2)) = \sigma(\mu + X)$ where $X \sim N(0,s^2)$. We can use a Taylor series expansion.&lt;/p&gt;&#10;&#10;&lt;p&gt;$\sigma(\mu + X) = \sigma(\mu) + X \sigma'(\mu) + \frac{X^2}{2} \sigma''(\mu)+ ... + \frac{X^n}{n!}\sigma^{(n)}(\mu) + ...$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\begin{eqnarray} E[\sigma(\mu + X)] &amp;amp; =&amp;amp;  E[\sigma(\mu)] + E[X \sigma'(\mu)] + E[\frac{X^2}{2} \sigma''(\mu)] + ... \newline &amp;amp; = &amp;amp; \sigma(\mu) + 0 + \frac{s^2}{2}\sigma''(\mu) + 0 + \frac{3s^4}{24}\sigma^{(4)}(\mu)+ ... + \frac{s^{2k}}{2^k k!}\sigma^{(2k)}(\mu) ... \end{eqnarray}$&lt;/p&gt;&#10;&#10;&lt;p&gt;There are convergence issues. The logistic function has a pole where $\exp(-x) = -1$, so at $x = k \pi i$, $k$ odd. Divergence is not the same thing as the prefix being useless, but this series approximation may be unreliable when $P(|X| \gt \sqrt{\mu^2 + \pi^2})$ is significant. &lt;/p&gt;&#10;&#10;&lt;p&gt;Since $\sigma'(x) = \sigma(x) (1-\sigma(x))$, we can write derivatives of $\sigma(x)$ as polynomials in $\sigma(x)$. For example, $\sigma'' = \sigma-3\sigma^2+2\sigma^3$ and $\sigma''' = \sigma - 7\sigma^2 + 12 \sigma^3 - 6\sigma^4$. The coefficients are related to &lt;a href=&quot;http://oeis.org/A028246&quot; rel=&quot;nofollow&quot;&gt;OEIS A028246&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-27T16:14:19.040" Id="46615" LastActivityDate="2012-12-27T16:14:19.040" OwnerUserId="11981" ParentId="45267" PostTypeId="2" Score="3" />
  <row AnswerCount="0" Body="&lt;p&gt;I want to fit a Cox PH model with random effect (Gamma frailty). Here is an example with 'kidney catheter' data set:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    &amp;gt; library(survival) &#10;    &amp;gt; data(kidney) &#10;    &amp;gt; fit &amp;lt;- coxph(Surv(time, status)~ age + sex + disease + frailty(id), kidney) &#10;    &amp;gt; fit&#10;&#10;        Call:&#10;        coxph(formula = Surv(time, status) ~ age + sex + disease + frailty(id),&#10;            data = kidney)&#10;&#10;                    coef     se(coef) se2    Chisq DF p&#10;        age          0.00318 0.0111   0.0111  0.08 1  7.8e-01&#10;        sex         -1.48314 0.3582   0.3582 17.14 1  3.5e-05&#10;        diseaseGN    0.08796 0.4064   0.4064  0.05 1  8.3e-01&#10;        diseaseAN    0.35079 0.3997   0.3997  0.77 1  3.8e-01&#10;        diseasePKD  -1.43111 0.6311   0.6311  5.14 1  2.3e-02&#10;        frailty(id)                           0.00 0  9.3e-01&#10;&#10;        Iterations: 6 outer, 28 Newton-Raphson&#10;             Variance of random effect= 5e-07   I-likelihood = -179.1&#10;        Degrees of freedom for terms= 1 1 3 0&#10;        Likelihood ratio test=17.6  on 5 df, p=0.00342  n= 76&#10;&#10;    &amp;gt; fit$history[[1]]$theta&#10;      [1] 5e-09&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Does the &lt;code&gt;fit$history[[1]]$theta&lt;/code&gt; returns the variance of random&#10;effect? Why is the value different?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-12-27T18:39:01.027" Id="46619" LastActivityDate="2012-12-27T18:39:01.027" OwnerUserId="17994" PostTypeId="1" Score="0" Tags="&lt;variance&gt;&lt;frailty&gt;" Title="Variance of a random effect in Cox PH" ViewCount="233" />
  <row AcceptedAnswerId="46628" AnswerCount="2" Body="&lt;p&gt;Since I'm a software engineer trying to learn more stats you'll have to forgive me before I even start, this is serious newb territory...&lt;/p&gt;&#10;&#10;&lt;p&gt;I've been learning &lt;a href=&quot;http://pypi.python.org/pypi/pymc/&quot;&gt;PyMC&lt;/a&gt; and working through some really (really) simple examples. One problem I can't get to work (and can't find any related examples for) is fitting a model to data generated from two normal distributions.&lt;/p&gt;&#10;&#10;&lt;p&gt;Say I have 1000 values; 500 generated from a &lt;code&gt;Normal(mean=100, stddev=20)&lt;/code&gt; and another 500 generated from a &lt;code&gt;Normal(mean=200, stddev=20)&lt;/code&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;If I want to fit a model to them, ie determine the two means and the single standard deviation, using PyMC. I know it's something along the lines of ...&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;mean1 = Uniform('mean1', lower=0.0, upper=200.0)&#10;mean2 = Uniform('mean2', lower=0.0, upper=200.0)&#10;precision = Gamma('precision', alpha=0.1, beta=0.1)&#10;&#10;data = read_data_from_file_or_whatever()&#10;&#10;@deterministic(plot=False)&#10;def mean(m1=mean1, m2=mean2):&#10;    # but what goes here?&#10;&#10;process = Normal('process', mu=mean, tau=precision, value=data, observed=True)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;i.e., the generating process is Normal, but mu is one of two values. I just don't know how to represent the &quot;decision&quot; between whether a value comes from &lt;code&gt;m1&lt;/code&gt; or &lt;code&gt;m2&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Perhaps I'm just completely taking the wrong approach to modeling this? Can anyone point me at an example? I can read BUGS and JAGS so anything is ok really.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-27T22:29:53.397" FavoriteCount="2" Id="46626" LastActivityDate="2013-03-31T16:27:40.770" LastEditDate="2012-12-28T03:21:55.667" LastEditorUserId="11867" OwnerUserId="18141" PostTypeId="1" Score="8" Tags="&lt;modeling&gt;&lt;python&gt;&lt;pymc&gt;" Title="Fitting model for two normal distributions in PyMC" ViewCount="1904" />
  <row AnswerCount="1" Body="&lt;p&gt;I´m using Octave to build a Neural Network regression (3 layers nn, using tanh as activation function, fmincg as optimizer and continuous output). The purpose of the model is to forecast demand for some products, based on multiple variables (past demand of the products, past stock levels, pendent orders, etc.). I'm normalizing all inputs and output.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;The quality of the forecast is finally evalued with SFE (statistical forecast error), which I defined in Octave as:&lt;/strong&gt;&#10;    $$SFE=\frac{\sum_{i=1}^{m}abs(Y_m-Forecast_m)}{\sum_{i=1}^{m}Forecast_m}$$.&#10;&lt;strong&gt;Despite this, I'm training the model as a &quot;traditional&quot; regression, using a sum of square errors as cost function (J):&lt;/strong&gt;&#10;    $$J=\frac{\sum_{i=1}^{m}(Y_m-Forecast_m)^2}{2m}$$.&#10;I think this is not correct, as learning should be made towards the real objective (SFE) and J is over-estimating the impact of bigger deviations. But as abs() has no derivative, I don't realize how SFE function could work under a back-prop training for the nn.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;How should the cost function (J) be defined so as to train the model towards SFE?&lt;/strong&gt; &#10;I'd like to use SFE itself, but I don't find it possible as I have to compute the gradient in order for the optimizer to work (fmincg for Octave) and abs() is not differentiable at 0. The answer should cover this aspect for the optimizer to work.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-28T03:54:17.150" FavoriteCount="1" Id="46639" LastActivityDate="2013-06-27T08:10:18.070" LastEditDate="2012-12-28T21:23:12.290" LastEditorUserId="17327" OwnerUserId="17327" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;forecasting&gt;&lt;neural-networks&gt;" Title="Choosing cost function for a neural network continuous forecast" ViewCount="766" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a dataset and I want to compute the &lt;strong&gt;mutual information (MI)&lt;/strong&gt; for a selected set of variables. The dataset is large enough so that computation of the MI may take undesirably long time. &lt;strong&gt;Can I just take a 10% sample of the data and use it to compute the MI measure?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I am aware of different sampling techniques including &lt;em&gt;random sampling, stratified sampling and bootstrapping&lt;/em&gt;. Even though techniques that require computing MI more than once such as bootstrapping or clustering provide good clues about bias and variance, they lead to some extra computational time. I want to compute MI only once. So, preparing a single random sample and using it for estimating MI is a solution. &lt;strong&gt;My concern is how accurate is this solution based on a sample size of 10% of data? What about the sampling error and do I have to pay attention to it?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Regarding computing the MI, I do not assume normality for estimating any probability; rather, a non-parametric method (&lt;em&gt;i.e.&lt;/em&gt; histogram) is used.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2012-12-28T14:34:27.353" Id="46646" LastActivityDate="2012-12-28T14:58:59.033" LastEditDate="2012-12-28T14:58:59.033" LastEditorUserId="919" OwnerUserId="14888" PostTypeId="1" Score="2" Tags="&lt;machine-learning&gt;&lt;probability&gt;&lt;sampling&gt;&lt;sample-size&gt;&lt;mutual-information&gt;" Title="Random sampling for estimating mutual information - Time complexity and sampling error?" ViewCount="119" />
  <row Body="&lt;p&gt;You are correct to be suspicious and you are correct that problems arise from some of the low cell counts in this case.  However, there is nothing wrong with &lt;a href=&quot;http://en.wikipedia.org/wiki/Fisher%27s_exact_test&quot;&gt;Fisher's test&lt;/a&gt; itself.  We just need to be careful in interpreting its results.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's review the data:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;         0  1  Total &#10;Site 1   7  2 |    9&#10;Site 2  95  9 |  104&#10;Site 3   0  1 |    1&#10;--------------+-----&#10;Totals 102 12 |  114&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Fisher's test sums the probabilities of all configurations of the data that are (a) consistent with the row and column totals and (b) have lower probabilities than the observed table (under the null hypothesis of no column-row association).&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose the one result for Site 3 were not included.  Fisher's test, applied to the first two rows only, gives a p-value of $0.2123$--far from &quot;significant&quot; evidence of any association within the first two sites.  Consider now the effect of including that single value from Site 3.  There are only two ways to maintain the value of $1$ for that row total: either the $1$ appears in the left column or in the right and a $0$ appears in the other entry.  Because the column totals are 102 and 12, the null hypothesis suggests that the $1$ should appear in the left column with a frequency of $12/114$ and in the right column with a frequency of $102/114$.  The former case actually &lt;em&gt;weakens&lt;/em&gt; the evidence of a row-column association and so would tend to elevate the p-value, whereas the latter case--which is what actually is observed--strengthens the evidence of an association and decreases the p-value.&lt;/p&gt;&#10;&#10;&lt;p&gt;At this point I will make an incorrect but suggestive observation: if the p-value for the test of the first two rows actually were a &lt;em&gt;probability&lt;/em&gt; (of the null hypothesis being true), we could update this probability (in a Bayesian sense) by multiplying the odds.  The odds of the data for Site 3 are 12:102, whence&lt;/p&gt;&#10;&#10;&lt;p&gt;$$0.2123 / (1 - 0.2123) \times 12 / 102 = 0.0317.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;This corresponds to a new probability or &quot;p-value&quot; of $0.0307$--remarkably close to the two-sided p-value of $0.0287$ obtained for the full table.&lt;/p&gt;&#10;&#10;&lt;p&gt;Whether we believe this intuition or not, the discrepancy in p-values is telling us that &lt;strong&gt;the apparently significant result for the full table is due almost entirely to the &lt;em&gt;single&lt;/em&gt; observation obtained at Site 3.&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Do you really want to draw a conclusion about the first two sites based on a single result from a &lt;em&gt;third, different&lt;/em&gt; site?  It is difficult to imagine a setting in which this would be wise.  Instead, you might conclude something like this:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Almost all the data were obtained at Sites 1 and 2.  Most of the observations (102 out of 114) were &quot;zeros&quot; (the left column's attribute).  They do not show significant evidence of an association with the columns (Fisher's Exact Test, p = 0.212).  A single value obtained at a third Site was one of the relatively rare &quot;ones&quot; (the right column's attribute).  Including this observation creates the appearance of an association in the entire table (Fisher's Exact Test, p = 0.029).  This may be taken as a (very) weak initial suggestion that Site 3 might differ from Sites 1 and 2 in having a greater tendency to exhibit &quot;ones.&quot;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="0" CreationDate="2012-12-28T16:07:21.840" Id="46648" LastActivityDate="2012-12-28T16:07:21.840" OwnerUserId="919" ParentId="46641" PostTypeId="2" Score="5" />
  
  <row Body="&lt;p&gt;I haven't found an algorithm that guarantees the highest-total-value for pairings selections, but I have found one that guarantees the lowest-total-value, so we can modify the original matrix to accommodate that algorithm. Basically, we just have to subtract all of the match quality scores from 1.0 and assign a value of 2.0 along the diagonal (to ensure a player never gets paired with themself). When we do that, we get something like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;           Player1  Player2  Player3  Player4&#10;=====================================================&#10;Player1 |    2.000    0.179    0.766    0.445&#10;Player2 |    0.179    2.000    0.167    0.358&#10;Player3 |    0.766    0.167    2.000    0.257&#10;Player4 |    0.445    0.358    0.257    2.000&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;As mentioned, our goal now is to select pairings with the LOWEST overall score, not the highest and that problem can be solved using the &lt;a href=&quot;http://en.wikipedia.org/wiki/Hungarian_algorithm&quot; rel=&quot;nofollow&quot; title=&quot;Hungarian Algorithm&quot;&gt;Hungarian Algorithm&lt;/a&gt;. That algorithm guarantees finding an optimal set of lowest value pairings and because we subtracted the match quality score from 1.0, gives us the pairings with the highest summed match quality scores, which was the original question.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note: When considering if we're done at each step of the algorithm, we have to ignore the duplicate symmetrical values, so that complicates applying the algorithm a bit.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-28T16:51:18.807" Id="46653" LastActivityDate="2012-12-30T07:40:00.480" LastEditDate="2012-12-30T07:40:00.480" LastEditorUserId="18149" OwnerUserId="18149" ParentId="46635" PostTypeId="2" Score="0" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm using the R package &lt;a href=&quot;http://cran.r-project.org/web/packages/ltm/index.html&quot; rel=&quot;nofollow&quot;&gt;ltm&lt;/a&gt; to create a 2-parameter logistic regression.&lt;/p&gt;&#10;&#10;&lt;p&gt;The input matrix is sparse - many users have taken a small subset of the items in the item bank.&lt;/p&gt;&#10;&#10;&lt;p&gt;For some of my data sets i'm running into this error:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Error in if (any(ind &amp;lt;- pr == 0)) pr[ind] &amp;lt;- sqrt(.Machine$double.eps) : &#10;  missing value where TRUE/FALSE needed&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Not sure what the issue is.  Doesn't repro on most of my data sets.  Any thoughts?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2012-12-28T19:33:22.053" FavoriteCount="1" Id="46661" LastActivityDate="2012-12-31T16:05:28.520" LastEditDate="2012-12-28T22:54:53.683" LastEditorUserId="930" OwnerUserId="17956" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;irt&gt;" Title="Error using ltm R package" ViewCount="271" />
  <row AnswerCount="0" Body="&lt;p&gt;I am wondering about this. When is it best to use Dantzig Selector (the infinity normed error measure plus the L1 regularizer) , the LASSO (the mean square error measure plus the L1 regularizer), and the LAD LASSO (the error measure containing the least absolute deviation plus the L1 regularizer)? To make things clear I will quote the minimization problems solved in each:&lt;/p&gt;&#10;&#10;&lt;p&gt;LASSO &#10;$$ \min_{\beta} \left| \mathbb{y} - \mathbb{X} \boldsymbol{\beta}\right|_2 + \lambda  \left|\boldsymbol{\beta}\right|_1 $$&lt;/p&gt;&#10;&#10;&lt;p&gt;LAD - LASSO&#10;$$ \min_{\beta} \left| \mathbb{y} - \mathbb{X} \boldsymbol{\beta}\right|_1 + \lambda  \left|\boldsymbol{\beta}\right|_1 $$&lt;/p&gt;&#10;&#10;&lt;p&gt;Dantzig Selector&#10;$$ \min_{\beta} \left| \mathbb{X}^T( \mathbb{y} - \mathbb{X} \boldsymbol{\beta})\right|_{\infty} + \lambda  \left|\boldsymbol{\beta}\right|_1 $$&lt;/p&gt;&#10;&#10;&lt;p&gt;Here $\mathbb{y}$ represents the observations, $\mathbb{X}$ represents the basis functions for each observation, $\boldsymbol{\beta}$ are the regression coefficients. The subscripts indicate the choice of the norms used. $\lambda$ is a positive coefficient determined by $n$ fold cross-validation typically.&lt;/p&gt;&#10;&#10;&lt;p&gt;If there exists a criterion can this be determined from the dataset? &lt;/p&gt;&#10;&#10;&lt;p&gt;Edit: I think a more reasonable question is the following: What are the features of the fit obtained from each one of these methods. For example I am looking for something like, &lt;/p&gt;&#10;&#10;&lt;p&gt;the LAD-LASSO is less sensitive to outliers in contrast to the LASSO. &lt;a href=&quot;http://hansheng.gsm.pku.edu.cn/pdf/2007/LAD-Lasso.pdf&quot; rel=&quot;nofollow&quot;&gt;http://hansheng.gsm.pku.edu.cn/pdf/2007/LAD-Lasso.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Also please provide references if you can.&lt;/p&gt;&#10;" CommentCount="10" CreationDate="2012-12-28T23:46:50.060" FavoriteCount="4" Id="46666" LastActivityDate="2012-12-31T03:52:15.087" LastEditDate="2012-12-31T03:52:15.087" LastEditorUserId="16664" OwnerUserId="16664" PostTypeId="1" Score="4" Tags="&lt;regression&gt;&lt;lasso&gt;" Title="Dantzig Selector, LASSO, LAD LASSO" ViewCount="219" />
  
  <row Body="&lt;p&gt;Welcome to the site.&lt;/p&gt;&#10;&#10;&lt;p&gt;It isn't clear what you did once you got the variables into a table. However, if you are looking for differences between two groups, I don't see how correlation is going to help. In RQ 7 you seem to have an ordinal dependent variable (going from 1 to 7). The simplest thing would be a t-test, but that is not technically correct since it is really for continuous variables. You could use a test of medians - medians don't depend so much on the data being continuous. Or you could use a test of ordinal differences such as the Jonckheere Terpstra test. Another possibility is ordinal logistic regression with the scale as the dependent variable and &quot;group&quot; as the independent variable. This would allow you to add any  other variables you have to the model.&lt;/p&gt;&#10;&#10;&lt;p&gt;For RQ 8, you say you have a nominal variable. I am not sure I understand what that variable is, but, if you are right then two possibilities are chi-square test of independence and multinomial logistic regression.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-29T13:02:04.573" Id="46680" LastActivityDate="2012-12-29T13:02:04.573" OwnerUserId="686" ParentId="46679" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;This sort of thing usually happens when you have correlated predictors.  Apart from examining the correlation matrix of you predictors, one way to check for high correlations is too look at the Principal Components of your predictors, and the amount of common variation in the first few components.&lt;/p&gt;&#10;&#10;&lt;p&gt;Another thing to note is that the coefficients can only be interpretted as a rate of change by assuming that all other predictors in the model stay unchanged.  However, if all of you variables are positively related to the outcome variable (as shown by the single variable regressions), then it is likely that they are also going to be positively related to each other.  So this means that if you increase (decrease) one predictor, the other predictors are likely to also increase (decrease) as well.  This wrecks the usual interpretation of the betas, because it makes the values of all the other betas important.&lt;/p&gt;&#10;&#10;&lt;p&gt;I assume you want to use your univariate regressions to validate or understand your multivariate regression better.  One way you can do this is to plot the predicted probabilities against each predictor variable.  This should still show a positive relationship with each dependent variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;From a technical perspective, the multivariate regression satisfies additional constraints compared to the regression with just one variable.  Assuming that you're using the logistic link function, these constraints take the form&#10;$$\sum_{i=1}^{n}n_i(f_i-p_i)x_{ij}=0\;\;\;\;\; j=0,\dots,q$$&#10;Where $f_i$ is the observed proportion of successes for the ith data point; $n_i$ the number of units the proportion is based on for the ith data point; $x_{ij}$ is the value of the jth predictor for the ith data point; $q$ is the number of predictors; and $p_i$ is the predicted probability for the ith data point, which is a function of the predictors $x_{i0},x_{i1},\dots,x_{iq}$ and their coefficients $\beta_0,\beta_1,\dots,\beta_q$.  I have used $x_{i0}$ and $\beta_0$ to denote the intercept term of the model, so $x_{i0}=1$.&lt;/p&gt;&#10;&#10;&lt;p&gt;These are the set of equations that define the regression coefficients (i.e. betas).  There is one equation for each predictor variable.  If we define a residual as the difference between the observed and predicted counts $r_i=n_i(f_i-p_i)$, then what the equations are doing is forcing the correlation between the $r_i$ and $x_{ij}$ to be equal to zero.&lt;/p&gt;&#10;&#10;&lt;p&gt;The regression with only one predictor has two constraint equations: one for the intercept term, and one for the single predictor variable -  call it $x_{i1}$.  For your case you say all of the betas are positive, which means that the predicted probability must be an increasing function $x_{i1}$.  We can use this and the constraint equation to deduce that the observed count $n_{i}f_{i}$ must also tend to increase as $x_{i1}$ increases.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, note that adding an additional variable to the model, say $x_{i2}$, does not change the constraint which must be satisfyied from using $x_{i1}$.  Now we now from the above paragraph that $n_if_i$ tends to increase as $x_{i1}$ increases.  This means that the predicted probability $p_i$ will also need to increase as $x_{i1}$ increases as well.  The negative value for the coefficient makes it seem like maybe this isn't true.  In fact, what the negative coefficient actually means here is that $p_i$ is increasing with $x_{i1}$ too much when only $x_{i2}$ is included in the model.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-12-29T14:16:35.000" Id="46682" LastActivityDate="2012-12-29T14:16:35.000" OwnerUserId="2392" ParentId="46669" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;I think you're right about the counting. For a Markov model of order $1$, with $K$ states, you need $K\times K$ parameters, minus $K$ parameters, since you have to consider the normalization of the conditional probabilities for each final state $K$; hence, $K\times K - K = K(K-1)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;For order $M$, it generalizes to $K^{M+1} - K = K^M  (K-1).$&lt;/p&gt;&#10;&#10;&lt;p&gt;See this post &lt;a href=&quot;http://stats.stackexchange.com/questions/46670/number-of-parameters-in-a-markov-chain&quot;&gt;Number of parameters in a Markov chain&lt;/a&gt;, where a similar question has been posted.&#10;Is the formula you mention maybe a typo? Or perhaps a different nomenclature for &quot;order&quot;?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-12-30T15:55:43.697" Id="46713" LastActivityDate="2012-12-30T16:13:19.100" LastEditDate="2012-12-30T16:13:19.100" LastEditorDisplayName="user10525" OwnerUserId="18197" ParentId="46711" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;(This isn't the whole answer, I might update later)&lt;/p&gt;&#10;&#10;&lt;p&gt;NNT is a transform of RD.  It's distribution has a different shape.  You're calculating CI's based on an assumption of normal distribution (thus the z-score for calculation).  If one of those distributions is normal only one is, NNT, or RD, not both (my guess, if one, NNT).  In general, with transformed values, if one distribution is normal then you can calculate the CI as you have on that distribution and then transform afterwards.  Distributions other than normal need their own CI calc methods.  And therefore, at least one of the method you've used is the wrong one for calculating the CI.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-30T17:18:58.747" Id="46717" LastActivityDate="2012-12-30T18:36:50.413" LastEditDate="2012-12-30T18:36:50.413" LastEditorUserId="601" OwnerUserId="601" ParentId="46709" PostTypeId="2" Score="1" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I am working on research involving the collection of data from employees and customers. Specifically, my research question deals with the influence of employee satisfaction on the quality of customer service. I will collect data from both employees and customers, and plan on running a correlation analysis on the results. I know i will run &lt;strong&gt;canonical correlation&lt;/strong&gt;, but my question is, if i have collected 200 questionnaires from employees, and I have 400 from customers, in a single SPSS file, there is an issue of missing data, since employees data is 200 less than the customers data. Does anyone have a suggestion for how I should handle this?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2012-12-31T05:26:34.490" Id="46743" LastActivityDate="2013-01-30T11:53:55.193" LastEditDate="2012-12-31T08:17:05.577" LastEditorUserId="3277" OwnerUserId="18206" PostTypeId="1" Score="1" Tags="&lt;correlation&gt;" Title="Canonical Correlation" ViewCount="146" />
  <row AcceptedAnswerId="56943" AnswerCount="2" Body="&lt;p&gt;I was wondering if anybody knows of the existence of an R package which implements entropy-based methods (maximum empirical likelihood, maximum exponential empirical likelihood, minimum discrepancy estimators, etc.)? Thanks&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2012-12-31T10:46:44.213" FavoriteCount="2" Id="46759" LastActivityDate="2013-05-18T13:37:28.973" LastEditDate="2013-05-18T13:37:28.973" LastEditorUserId="17636" OwnerUserId="17636" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;modeling&gt;&lt;maximum-entropy&gt;&lt;package&gt;" Title="Entropy-based methods in R" ViewCount="268" />
  
  <row AnswerCount="2" Body="&lt;p&gt;An example of a good measure of class separability in linear discriminant learners is Fisher's linear discriminant ratio. Are there other useful metrics to determine if feature sets provide good class separation between target variables? In particular, I'm interested in finding good multivariate input attributes for maximizing target class separation and it would be nice to have a non-linear/non-parametric measure to quickly determine if they provide good separability.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-01-01T00:01:20.527" FavoriteCount="5" Id="46780" LastActivityDate="2014-08-31T12:53:57.207" LastEditDate="2013-01-01T00:05:05.350" LastEditorUserId="13519" OwnerUserId="13519" PostTypeId="1" Score="8" Tags="&lt;machine-learning&gt;&lt;classification&gt;&lt;discriminant&gt;" Title="Good measures of feature selection and class separability in classification machine learning problems" ViewCount="459" />
  <row Body="&lt;p&gt;To think that you can learn what is important in people's decisions simply by asking expresses unjustified optimism.  But there are some sound methods of &quot;deriving&quot; the importance of different factors.  Years and years of research in psychology and behavioural economics have borne this out.  A colleague and I summarized some findings from the literature on this topic and explored some ways to apply them (in a higher education context) &lt;a href=&quot;http://www.maguireassoc.com/wp-content/uploads/2011/07/White-Paper-What-Drives-Student-Choices-June-2011.pdf&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-01T01:04:51.263" Id="46781" LastActivityDate="2013-01-01T01:04:51.263" OwnerUserId="2669" ParentId="27655" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Please have a look at implementation , &lt;a href=&quot;http://code.google.com/p/matrbm/&quot; rel=&quot;nofollow&quot;&gt;http://code.google.com/p/matrbm/&lt;/a&gt; . &#10;It has a classification rmb fit function worth considering.&#10;Also let me know if you find better implementations of DBNs and RMBs for classification and regression.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-01T02:10:42.230" Id="46782" LastActivityDate="2013-01-01T02:10:42.230" OwnerUserId="16969" ParentId="41029" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;I haven't done this personally, but a very good reference for R packages is the &lt;a href=&quot;http://rgm3.lab.nig.ac.jp/RGM/r_image_list?page=942&amp;amp;init=true&quot; rel=&quot;nofollow&quot;&gt;R Graphical Manual&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;A search on that site of &quot;empirical likelihood&quot; gave several results, including EEF.profile from the boot package. There are also some packages that claim entropy methods (FNN), although I don't know precisely what you are looking for. I would check that site. Be as specific as you can with the search bar.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-01T02:14:41.797" Id="46783" LastActivityDate="2013-01-01T02:14:41.797" OwnerUserId="14188" ParentId="46759" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a data set that's ~ 150R X 2000C and was curious if an RBM is appropriate in situation with this type of imbalance.  It's a microarray and I'm looking at a 0/1 classification problem.  I'd be curious if there are any coded examples of using this especially in R/Python/Java.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-01-01T15:15:14.423" Id="46797" LastActivityDate="2013-01-01T15:15:14.423" OwnerUserId="7411" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;machine-learning&gt;&lt;classification&gt;&lt;rbm&gt;" Title="Can RBMs be used for feature selection / reduction?" ViewCount="97" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to understand the concept of auto correlation and I am looking for some help to clear some doubts regarding my data.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a time series data and it has repeated experiments. Each sample has 4 time points and corresponding values for the genes under study. I have used &lt;code&gt;ind&lt;/code&gt; to represent each sample in my data. So &lt;code&gt;ind&lt;/code&gt; 1 means rat which is studied over 4 time points and the samples are in rows. I have 400 genes and corresponding values for each sample and each time point. The genes are in columns.&lt;/p&gt;&#10;&#10;&lt;p&gt;my dataset: &lt;code&gt;M1&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;No      ind  tme      gene_1      gene_2      gene_3      gene_4      gene_5  gene_7&#10;&#10;A1T1:2  1   -64 0.0307  0.0022  0.0010  0.0001  0.0007  0.0035&#10;&#10;A1T2:2  1   8   0.0365  0.0031  0.0003  0.0002  0.0009  0.0043&#10;&#10;A1T3:1  1   48  0.0182  0.0014  0.0001  0.0001  0.0005  0.0018&#10;&#10;A1T4:1  1   96  0.0134  0.0010  0.0001  0.0001  0.0003  0.0015&#10;&#10;A2T1:1  2   -64 0.0387  0.0032  0.0003  0.0002  0.0010  0.0051&#10;&#10;A2T2:1  2   8   0.0264  0.0022  0.0010  0.0001  0.0007  0.0032&#10;&#10;A2T3:1  2   48  0.0205  0.0017  0.0002  0.0001  0.0005  0.0022&#10;&#10;A2T4:1  2   96  0.0161  0.0012  0.0001  0.0001  0.0004  0.0018&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;What I want to do is identify is simple: if there is autocorrelation in my data?&lt;/p&gt;&#10;&#10;&lt;p&gt;my code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;s1&amp;lt;-read.table(&quot;M1.txt&quot;, sep=&quot; &quot;,header=T)&#10;s2&amp;lt;-s1[1:4,1]&#10;s2.v&amp;lt;-as.vector(s2)&#10;acf(s2.v)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I get a plot with 4 sub plots. I do not understand how to interpret the result. Also Can I give all the genes as input (for sample 1) to identify autocorrelation? &lt;/p&gt;&#10;&#10;&lt;p&gt;I am new to these concepts hence my understanding is basic. Thanking you for all the help.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-01-02T07:13:10.197" Id="46818" LastActivityDate="2013-01-02T09:45:47.530" LastEditDate="2013-01-02T09:45:47.530" LastEditorUserId="13680" OwnerUserId="18239" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;autocorrelation&gt;" Title="Multivariate values to acf() in R" ViewCount="436" />
  <row AcceptedAnswerId="47818" AnswerCount="2" Body="&lt;p&gt;Given the same set of covariates and distribution family, how can I compare models having  different link functions?&lt;/p&gt;&#10;&#10;&lt;p&gt;I think the correct answer here is &quot;AIC/BIC&quot;, but I am not 100% sure.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is it possible to have nested models if they have a different link?&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2013-01-02T16:04:05.927" FavoriteCount="4" Id="46833" LastActivityDate="2013-02-02T12:29:49.983" LastEditDate="2013-02-02T12:29:49.983" LastEditorUserId="686" OwnerUserId="18256" PostTypeId="1" Score="10" Tags="&lt;generalized-linear-model&gt;&lt;aic&gt;&lt;link-function&gt;" Title="Problem with comparing GLM models having a different link function" ViewCount="1159" />
  
  <row Body="&lt;p&gt;It seems that you can use &lt;a href=&quot;http://en.wikipedia.org/wiki/Regression_analysis&quot; rel=&quot;nofollow&quot;&gt;regression analysis&lt;/a&gt;. Also, probably you need to assign scores (real numbers) to the elements in your training set, if you don't have them. Although you can just use rank as your target value, it will make you get a poor model if you just have a small set of training samples.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-02T03:24:42.200" Id="46837" LastActivityDate="2013-01-02T03:33:50.797" OwnerDisplayName="Peter H" OwnerUserId="18265" ParentId="46836" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;(Technically, the P-value is the probability of observing data &lt;em&gt;at least as extreme&lt;/em&gt; as that actually observed, given the null hypothesis.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Q1. A decision to reject the null hypothesis on the basis of a small P-value typically depends on 'Fisher's disjunction': Either a rare event has happened or the null hypothesis is false. In effect, it is rarity of the event is what the P-value tells you rather than the probability that the null is false. &lt;/p&gt;&#10;&#10;&lt;p&gt;The probability that the null is false can be obtained from the experimental data only by way of Bayes' theorem, which requires specification of the 'prior' probability of the null hypothesis (presumably what Gill is referring to as &quot;marginal distributions&quot;). &lt;/p&gt;&#10;&#10;&lt;p&gt;Q2. This part of your question is much harder than it might seem. There is a great deal of confusion regarding P-values and error rates which is, presumably, what Gill is referring to with &quot;but is typically treated as such.&quot; The combination of Fisherian P-values with Neyman-Pearsonian error rates has been called an incoherent mishmash, and it is unfortunately very widespread. No short answer is going to be completely adequate here, but I can point you to a couple of good papers (yes, one is mine). Both will help you make sense of the Gill paper.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hurlbert, S., &amp;amp; Lombardi, C. (2009). Final collapse of the Neyman-Pearson decision theoretic framework and rise of the neoFisherian. Annales Zoologici Fennici, 46(5), 311–349. &lt;a href=&quot;http://xa.yimg.com/kq/groups/1542294/508917937/name/HurlbertLombardi2009AZF.pdf&quot;&gt;(Link to paper)&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Lew, M. J. (2012). Bad statistical practice in pharmacology (and other basic biomedical disciplines): you probably don't know P. British Journal of Pharmacology, 166(5), 1559–1567. doi:10.1111/j.1476-5381.2012.01931.x &lt;a href=&quot;http://onlinelibrary.wiley.com/doi/10.1111/j.1476-5381.2012.01931.x/pdf&quot;&gt;(Link to paper)&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-01-02T23:42:03.060" Id="46858" LastActivityDate="2013-01-03T08:21:12.547" LastEditDate="2013-01-03T08:21:12.547" LastEditorUserId="5594" OwnerUserId="1679" ParentId="46856" PostTypeId="2" Score="19" />
  <row AcceptedAnswerId="46930" AnswerCount="2" Body="&lt;p&gt;I have two random variables $X &amp;gt; 0$ and $Y &amp;gt; 0$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Given that I can estimate $$\text{Cov}(X, Y),$$ how can I estimate $$\text{Cov}(\log(X), \log(Y))?$$&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-01-03T07:20:49.083" FavoriteCount="1" Id="46874" LastActivityDate="2013-09-09T06:28:12.270" LastEditDate="2013-09-09T06:28:12.270" LastEditorUserId="805" OwnerUserId="7064" PostTypeId="1" Score="5" Tags="&lt;data-transformation&gt;&lt;covariance&gt;&lt;random-variable&gt;" Title="Covariance of transformed random variables" ViewCount="997" />
  <row Body="&lt;p&gt;For many purposes, you can use a normal approximation with continuity correction. &lt;/p&gt;&#10;&#10;&lt;p&gt;You can also use recursion for the probability that the total equals $y$, and add these from $y=x+1$ through $y=mn$. &lt;/p&gt;&#10;&#10;&lt;p&gt;There is also an &lt;a href=&quot;http://math.stackexchange.com/questions/28802/how-to-express-1xx2-cdotsxmn-as-a-power-series/28861#28861&quot;&gt;exact formula&lt;/a&gt; for the probability that the sum equals $y$ involving a single summation, which gives you a double sum for the probability the sum is greater than $x$. Be careful that in the linked answer, the &quot;dice&quot; had values from $0$ through $m$ instead of from $1$ through $m$. You can convert these by subtracting $1$ from each die and $n$ from the total, and using $m-1$ in place of $m$. Also, as whuber pointed out, there can be numerical instability if you are not careful in the order in which you add the terms of an alternating sum.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-03T14:27:03.630" Id="46885" LastActivityDate="2013-01-03T14:27:03.630" OwnerUserId="11981" ParentId="46872" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;On the derivation of the kalman filter in general, many good references exist. Among them,&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0471708585&quot; rel=&quot;nofollow&quot;&gt;Simon, D. Optimal State Estimation: Kalman, H Infinity, and Nonlinear Approaches&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/019964117X&quot; rel=&quot;nofollow&quot;&gt;Durbin, J. and Koopman, S.J. Time Series Analysis by State Space Methods&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0470529709&quot; rel=&quot;nofollow&quot;&gt;Gibbs, B.P. Advanced Kalman Filtering, Least-Squares and Modeling: A Practical Handbook&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;For your precise question about the EM estimation with state-space models, perhaps you can turn to section 6.3 of  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/B008549LTO&quot; rel=&quot;nofollow&quot;&gt;Shumway, R.H. and Stoffer, D.S. Time Series Analysis and Its Applications: With R Examples&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;where it is nicely explained.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-01-03T15:28:17.630" Id="46888" LastActivityDate="2013-05-09T13:54:57.893" LastEditDate="2013-05-09T13:54:57.893" LastEditorUserId="892" OwnerUserId="892" ParentId="46866" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Not sure if it exactly qualifies as a hello world, but in R there are also demos built into many packages. e.g.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(graphics)&#10;demo(graphics)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;will step the user through some basic graphics available in the package.&#10;Just mouse click over each image to step through basic graphics illustrations.&#10;With just two lines, the user is introduced into some&#10;of the inspiring capabilities of R graphics for statistics.&lt;/p&gt;&#10;&#10;&lt;p&gt;Corresponding code to generate the graphics is displayed in the R console.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/8YeJq.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-03T20:17:55.553" Id="46911" LastActivityDate="2013-01-03T20:33:27.927" LastEditDate="2013-01-03T20:33:27.927" LastEditorUserId="13519" OwnerUserId="13519" ParentId="46897" PostTypeId="2" Score="4" />
  <row AnswerCount="1" Body="&lt;p&gt;I'm interested in exploring &lt;a href=&quot;http://deeplearning.net/tutorial/dA.html&quot; rel=&quot;nofollow&quot;&gt;autoencoders&lt;/a&gt; which can be used to develop a compressed representation of data useful for machine learning.&lt;/p&gt;&#10;&#10;&lt;p&gt;In my experience random forests are easier to work with and more flexible than linear models and so I'd like to try to use them to build an autoencoder.&lt;/p&gt;&#10;&#10;&lt;p&gt;One might do this by using random forests to predict &lt;a href=&quot;http://onlinelibrary.wiley.com/store/10.1002/widm.12/asset/12_ftp.pdf?v=1&amp;amp;t=hbicuh5h&amp;amp;s=c2d37b13fcc7804095cc5bfb375677a384983f50&quot; rel=&quot;nofollow&quot;&gt;multiple outcomes&lt;/a&gt; then re-represent each data point as a binary sequence corresponding to the braches that it took.  For example if a forest consisted of two trees with three branches then the code 011 101 would represent a datapoint that took the second and third branches of the first tree and took the first and third branch of the second tree.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is anyone familiar with work like this?  I am interested in papers, implementations of multi-outcome random forests, and techniques that convert random forests into binary representations of data points.&lt;/p&gt;&#10;&#10;&lt;p&gt;Edit: clarifying&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-01-03T20:52:09.673" FavoriteCount="6" Id="46915" LastActivityDate="2014-02-08T10:21:10.247" LastEditDate="2013-01-03T21:15:12.647" LastEditorUserId="7494" OwnerUserId="7494" PostTypeId="1" Score="4" Tags="&lt;random-forest&gt;&lt;gbm&gt;&lt;deep-learning&gt;&lt;autoencoders&gt;" Title="has anyone implemented an autoencoder with random forests" ViewCount="675" />
  
  
  
  
  
  <row Body="&lt;p&gt;I guess your answer can be found in wikipedia &lt;a href=&quot;http://en.wikipedia.org/wiki/Conjugate_prior&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Conjugate_prior&lt;/a&gt;. The conjugate prior is the Normal-inverse gamma with hyperparameters $\mu_0, \nu, \alpha$ and $\beta$. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-01-04T02:49:51.317" Id="46939" LastActivityDate="2013-01-04T02:49:51.317" OwnerUserId="18274" ParentId="46937" PostTypeId="2" Score="3" />
  <row AnswerCount="1" Body="&lt;p&gt;Consider the model $ y = \beta_0 + \beta_1 x_1 + \beta_2  x_2 + \epsilon$.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;What is the relationship between the correlation coefficients $r_{y,x_1}$, $r_{y,x_2}$ and the regression coefficients $\beta_1$ and $\beta_2$? &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;In particular, how to interpret a situation where a particular correlation coefficient is statistically significant but the corresponding regression coefficient is not statistically significant?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;I recall reading a concept called &quot;partial correlation coefficients&quot;. Is that in some way relevant in the above context?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2013-01-04T03:18:10.450" FavoriteCount="3" Id="46941" LastActivityDate="2013-01-04T03:47:04.727" OwnerUserId="18315" PostTypeId="1" Score="4" Tags="&lt;regression&gt;&lt;correlation&gt;" Title="What is the relationship between correlation coefficients and regression coefficients in multiple regression?" ViewCount="993" />
  <row Body="&lt;p&gt;You should consider the log-normal distribution &lt;a href=&quot;http://en.wikipedia.org/wiki/Log-normal_distribution&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Log-normal_distribution&lt;/a&gt; which boils down to model the logarithm of the substance instead of the substance itself.&lt;/p&gt;&#10;&#10;&lt;p&gt;There also exists more complicated distributions that are suitable but the log-normal is often the default-choice when dealing with a positive and continuous random variable.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-04T05:44:16.843" Id="46947" LastActivityDate="2013-01-04T05:44:16.843" OwnerUserId="18274" ParentId="46946" PostTypeId="2" Score="1" />
  
  
  
  
  
  <row Body="&lt;p&gt;This answer aims to make a demonstration that is as elementary as possible, because such things frequently get to the essential idea.  The &lt;em&gt;only&lt;/em&gt; facts needed (beyond the simplest kind of algebraic manipulations) are linearity of integration (or, equivalently, of expectation), the change of variables formula for integrals, and the axiomatic result that a PDF integrates to unity.&lt;/p&gt;&#10;&#10;&lt;p&gt;Motivating this demonstration is the intuition that when $f_X$ is symmetric about $a$, then the contribution of any quantity $G(x)$ to the expectation $\mathbb{E}_X(G(X))$ will have the same weight as the quantity $G(2a-x)$, because $x$ and $2a-x$ are on opposite sides of $a$ and equally far from it. Provided, then, that $G(x) = -G(2a-x)$ for all $x$, everything cancels and the expectation must be zero.  The relationship between $x$ and $2a-x$, then, is our point of departure.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Notice, by writing $y = x + a$, that the symmetry can just as well be expressed by the relationship&lt;/p&gt;&#10;&#10;&lt;p&gt;$$f_X(y) = f_X(2a-y)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;for all $y$.  For any measurable function $G$, the one-to-one change of variable from $x$ to $2a-x$ changes $dx$ to $-dx$, while reversing the direction of integration, implying&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\mathbb{E}_X(G(X)) = \int G(x) f_X(x)dx = \int G(x) f_X(2a - x)dx = \int G(2a-x)f_X(x)dx.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Assuming this expectation exists (that is, the integral converges), the linearity of the integral implies&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\int \left(G(x) - G(2a - x)\right)f_X(x)dx = 0.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Consider the odd moments about $a$, which are defined as the expectations of $G_{k,a}(X) = (X-a)^k$, $k = 1, 3, 5, \ldots$.  In these cases &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\eqalign{&#10;G_{k,a}(x) - G_{k,a}(2a-x) &amp;amp;= (x-a)^k - (2a-x-a)^k \\&amp;amp;= (x-a)^k - (a-x)^k \\ &amp;amp;= (1^k - (-1)^k)(x-a)^k \\&amp;amp;= 2(x-a)^k,}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;precisely because $k$ is odd.  Applying the preceding result gives&lt;/p&gt;&#10;&#10;&lt;p&gt;$$0 = \int \left(G_{k,a}(x) - G_{k,a}(2a - x)\right)f_X(x)dx = 2\int (x-a)^k f_X(x)dx.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Because the right hand side is twice the $k$th moment about $a$, dividing by $2$ shows that this moment is zero whenever it exists.&lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, the mean (assuming it exists) is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\mu_X = \mathbb{E}_X(X) = \int x f_X(x)dx = \int (2a-x)f_X(x)dx.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Once again exploiting linearity, and recalling that $\int f_X(x)dx=1$ because $f_X$ is a probability distribution, we can rearrange the last equality to read&lt;/p&gt;&#10;&#10;&lt;p&gt;$$2\mu_X = 2\int x f_X(x)dx = 2a\int f_X(x)dx = 2a\times 1 = 2a$$&lt;/p&gt;&#10;&#10;&lt;p&gt;with the unique solution $\mu_X = a$.  Therefore all our previous calculations of moments about $a$ are really the central moments, QED.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;h3&gt;Postword&lt;/h3&gt;&#10;&#10;&lt;p&gt;The need to divide by $2$ in several places is related to the fact that there is a &lt;a href=&quot;http://en.wikipedia.org/wiki/Symmetry_group&quot; rel=&quot;nofollow&quot;&gt;group&lt;/a&gt; of order $2$ acting on the measurable functions (namely, the group generated by the reflection in the line around $a$). More generally, the idea of a &lt;em&gt;symmetry&lt;/em&gt; can be generalized to the action of any group. The theory of group representations implies that when the &lt;a href=&quot;http://mathworld.wolfram.com/GroupCharacter.html&quot; rel=&quot;nofollow&quot;&gt;character&lt;/a&gt; of that action on a function is not trivial, it is orthogonal to the trivial character, and that means the expectation of the function must be zero. The orthogonality relations involve adding (or integrating) over the group, whence the size of the group constantly appears in denominators: its cardinality when it is finite or its volume when it is compact.&lt;/p&gt;&#10;&#10;&lt;p&gt;The beauty of this generalization becomes apparent in &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0486432475&quot; rel=&quot;nofollow&quot;&gt;applications with manifest symmetry&lt;/a&gt;, such as in mechanical (or quantum mechanical) equations of motion of symmetrical systems exemplified by a benzene molecule (which has a 12 element symmetry group).  (The QM application is most relevant here because it explicitly calculates expectations.)  Values of physical interest--which typically involve multidimensional integrals of tensors--can be computed with no more work than was involved here, simply by knowing the characters associated with the integrands.  For instance, the &quot;colors&quot; of various symmetric molecules--their spectra at various wavelengths--can be determined &lt;em&gt;ab initio&lt;/em&gt; with this approach.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-01-04T15:57:45.720" Id="46977" LastActivityDate="2013-01-04T18:56:10.173" LastEditDate="2013-01-04T18:56:10.173" LastEditorUserId="919" OwnerUserId="919" ParentId="46843" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;This is an interesting area, I am working on as well. Similar hospital data. But my difficulty has been on choosing between mixed effect modelling and GEE for a prediction model with multiple imputation. Even though naturally mixed effect model will the first choice, considering the number of &quot; clusters&quot; (patients-12000) with multiple admissions but with smaller cluster size(2-10), the mixed effect model seems to computationally take long (6-12 hours) and the effect sizes seem to be over etsimated.The GEE on the other hand has better estimates and performance (especially calibration).&lt;/p&gt;&#10;&#10;&lt;p&gt;Mathematically yet to prove this but seems the best solution at hand for a large database, with multiple imputation, 1000s of clusters and with small cluster size.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-01-04T17:49:48.277" Id="46984" LastActivityDate="2013-01-04T17:49:48.277" OwnerUserId="18340" ParentId="26961" PostTypeId="2" Score="0" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have this confusion related to the calculation of a conditional distribution&lt;/p&gt;&#10;&#10;&lt;p&gt;suppose $y_n = N(0,w)$&lt;/p&gt;&#10;&#10;&lt;p&gt;$p(o_n|y_n) = N(D.y_n,\phi)$&lt;/p&gt;&#10;&#10;&lt;p&gt;How do I calculate &lt;/p&gt;&#10;&#10;&lt;p&gt;$p(y_n|o_n)$&lt;/p&gt;&#10;&#10;&lt;p&gt;Actually I was reading this paper related to imputation of missing data exploiting the spatial correlation between data &lt;a href=&quot;http://astro.temple.edu/~tua86150/Lou_IJCAI_11.pdf&quot; rel=&quot;nofollow&quot;&gt;http://astro.temple.edu/~tua86150/Lou_IJCAI_11.pdf&lt;/a&gt;. However, while going through the paper, I didn't get this part where it calculate $E(y_n|o_n)$. Any suggestions will be appreciated a lot.&lt;/p&gt;&#10;&#10;&lt;p&gt;Actually, $y_n$ are treated as latent variables in the paper and {$y_n,o_n$} represents a linear-Gaussian latent variable model. The latent variables {$y_n$} are treated as independent&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-05T14:50:18.083" Id="47027" LastActivityDate="2015-02-18T07:01:32.617" LastEditDate="2013-01-05T17:52:05.400" LastEditorUserId="919" OwnerUserId="12329" PostTypeId="1" Score="1" Tags="&lt;bayesian&gt;&lt;missing-data&gt;&lt;data-imputation&gt;&lt;latent-variable&gt;" Title="Confusion related to calculation of conditional distribution" ViewCount="222" />
  <row AnswerCount="0" Body="&lt;p&gt;I was a bit confused about the modelling of temporal correlation in a certain paper. Lets say, I have vector $\bf{x}$ of dimension m, and a time series &#10;$\bf{x_1},\bf{x_2},...\bf{x_N}$. Now I want to model the temporal correlation of the data points.&lt;/p&gt;&#10;&#10;&lt;p&gt;So if I define&lt;/p&gt;&#10;&#10;&lt;p&gt;$\bf{x_n} = A\bf{x_{n-1}} + w_n$ where A is the parameter and $w= N(0,\tau)$, how does this model the temporal correlation of each dimension?&lt;/p&gt;&#10;&#10;&lt;p&gt;This might be a very basic question. But I really want to know how this models the correlation.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-05T14:58:39.210" Id="47028" LastActivityDate="2013-01-05T17:51:13.523" LastEditDate="2013-01-05T17:51:13.523" LastEditorUserId="919" OwnerUserId="12329" PostTypeId="1" Score="2" Tags="&lt;time-series&gt;&lt;autocorrelation&gt;" Title="Confusion related to modelling of temporal correlation" ViewCount="51" />
  
  
  <row AcceptedAnswerId="47057" AnswerCount="1" Body="&lt;p&gt;I have a formula that says to work out DF for residuals, I need to do  (n-1)IJ. Here I and J the number of levels in the two factors, but I haven't got a clue what n is. I thought it was total sample size but when I do that I get the wrong answer. Then I thought it might be number of people in each group, but again, this gives me the wrong answer.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can anyone help me?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-01-05T22:31:41.773" Id="47048" LastActivityDate="2013-01-06T00:47:32.747" OwnerUserId="14836" PostTypeId="1" Score="0" Tags="&lt;anova&gt;" Title="How to work out the degrees of freedom for residual in 2 way anova" ViewCount="608" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I was reading &lt;a href=&quot;http://cs.ru.nl/~perry/publications/2011/ICANN2011/groot-icann2011.pdf&quot; rel=&quot;nofollow&quot;&gt;this paper&lt;/a&gt; related to Learning from multiple annotator using Gaussian processes. The idea is if we don't have the actual ground truth of a certain data, but only the labels from some noisy experts, then how can we learn a model from this data from multiple noisy experts and predict on future data&lt;/p&gt;&#10;&#10;&lt;p&gt;Basically, what the guys have done is taken the weighted average of the labels from individual predictors, based upon their precision, to get a single label for each point. Then they have used GP to assume that the data points belong to a Gaussian distribution. However, I have this question/confusion when they have derived the likelihood of $p(Y)$ where $Y$ is the set of labels given by the noise experts.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have added the screenshot as well&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/6vbLU.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I know the one who can answer this question has to go through the paper and all, but I have already spend significant time trying to figure out how this $-\log p(Y)$ turned out like that but couldn't. I would really appreciate if someone could give me some pointers&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-06T08:33:02.260" Id="47071" LastActivityDate="2013-01-06T15:00:50.443" LastEditDate="2013-01-06T15:00:50.443" LastEditorUserId="14675" OwnerUserId="12329" PostTypeId="1" Score="2" Tags="&lt;gaussian-process&gt;&lt;likelihood&gt;" Title="Confusion related to calculation of likelihood" ViewCount="84" />
  <row Body="&lt;p&gt;If you use $n$ bins, you will use $n-1$ degrees of freedom. For the same cost in degrees of freedom, you can fit $n-1$ restricted cubic splines. Thus, there should definitely not be more overfitting if you use splines.&lt;/p&gt;&#10;&#10;&lt;p&gt;Conversely, discretizing continuous variables is almost always a bad idea, see, e.g., &lt;a href=&quot;http://demonstrations.wolfram.com/MedianSplit/&quot;&gt;here&lt;/a&gt; or &lt;a href=&quot;http://psych.colorado.edu/~mcclella/MedianSplit/&quot;&gt;here&lt;/a&gt; or &lt;a href=&quot;http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/CatContinuous?CGISESSID=%206e7423daad44d40ec4551da9f85ec12e&quot;&gt;here&lt;/a&gt;. In your case, defining the bins by &quot;eye-balling&quot; will tempt you to change them &quot;just a little bit&quot; to get a better fit... I am not saying you will give in to this temptation, but you will need to account for this leeway in model creation in some way.&lt;/p&gt;&#10;&#10;&lt;p&gt;Bottom line: don't discretize, use restricted splines.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-01-06T11:13:58.893" Id="47076" LastActivityDate="2013-01-06T11:13:58.893" OwnerUserId="1352" ParentId="47074" PostTypeId="2" Score="8" />
  
  <row Body="&lt;p&gt;This would have been better presented with a test data object but based on your comments (that change the question) you could try using &lt;code&gt;offset()&lt;/code&gt; on K and T and &lt;code&gt;I()&lt;/code&gt; on your proposed &quot;one&quot; variable:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; dat &amp;lt;- data.frame(Y=1:10, one=1, K=rnorm(10), T=rnorm(10))&#10;&amp;gt; lm(Y~ I( ( offset(T)+0.6*one )/offset(K)+one), data=dat)&#10;&#10;Call:&#10;lm(formula = Y ~ I((offset(T) + 0.6 * one)/offset(K) + one), &#10;    data = dat)&#10;&#10;Coefficients:&#10;                               (Intercept)  I((offset(T) + 0.6 * one)/offset(K) + one)  &#10;                                   5.42701                                     0.04631  &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;That's probably not a complete answer, since it doesn't separately estimate a &quot;b&quot; coefficient for &quot;one&quot;, but at least it estimates an &quot;a&quot;. Now you can take the methods and see if &lt;code&gt;nls&lt;/code&gt; is effective:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; nls(Y~  ( offset(T)+0.6*beta )/( offset(K)+beta), data=dat, start=list(beta=1)&#10;+ )&#10;Nonlinear regression model&#10;  model:  Y ~ (offset(T) + 0.6 * beta)/(offset(K) + beta) &#10;   data:  dat &#10; beta &#10;2.123 &#10; residual sum-of-squares: 317.8&#10;&#10;Number of iterations to convergence: 12 &#10;Achieved convergence tolerance: 8.603e-06 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I don't think you can add another parameter to that model without creating a singular predictor matrix.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-01-06T21:42:54.107" Id="47112" LastActivityDate="2013-01-06T21:42:54.107" OwnerUserId="2129" ParentId="47034" PostTypeId="2" Score="2" />
  
  
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I have to analyse data that look at the relevance of some posts. Basically each post is rated by 100 persons, and the scale looks something like this (like a Likert scale):&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Irrelevant&lt;/li&gt;&#10;&lt;li&gt;Slightly relevant&lt;/li&gt;&#10;&lt;li&gt;Relevant&lt;/li&gt;&#10;&lt;li&gt;Useful&lt;/li&gt;&#10;&lt;li&gt;Extremely Useful&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;But each person can choose a certain position with a 0.25 increment on this scale. So, more specifically, the scale looks like this [1, 1.25, 1.50, 1.75, 2, 2.25,...3,...4,...,4.75,5]&lt;/p&gt;&#10;&#10;&lt;p&gt;I was asked to compute some intervals for each post. My first reaction was to compute the mean and the 95% CI, which is evidently wrong since the data is not normally distributed.&#10;Which is the best approach for this problem, where should I start looking? I read something about Bayesian Approach on this site, will that do? &#10;Thanks&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-07T12:08:44.583" Id="47146" LastActivityDate="2013-01-08T23:26:23.823" LastEditDate="2013-01-08T23:26:23.823" LastEditorUserId="17230" OwnerUserId="18429" PostTypeId="1" Score="1" Tags="&lt;confidence-interval&gt;&lt;likert&gt;&lt;scales&gt;" Title="&quot;Intervals&quot; for a scale-based answer" ViewCount="370" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I am using &lt;code&gt;caret&lt;/code&gt; and &lt;code&gt;repeatedcv&lt;/code&gt; with repeats for feature selection. That is, &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;rfeControl(functions = svmFuncs, method = &quot;repeatedcv&quot;, number = 10, repeats = 5,  &#10;           rerank = TRUE, returnResamp = &quot;all&quot;, saveDetails = FALSE, verbose = TRUE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I am quite confused about the way &lt;code&gt;rfeControl&lt;/code&gt; splits the input data using repetition. In general, if I am not mistaken, the most unbiased way of assessing the performance of the model is to: &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;iteratively create 2 subsets (test and training set)&lt;/li&gt;&#10;&lt;li&gt;do the validation to the training set (i.e. cross validation) and select the most significant predictors &lt;/li&gt;&#10;&lt;li&gt;assess the performance with the unknown test set&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;In case of &lt;code&gt;rfeControl&lt;/code&gt; with repeated-cv and repetition, the repetition is applied from (1) or during the validation process (2)?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-07T14:48:15.220" Id="47153" LastActivityDate="2013-01-21T09:46:56.527" LastEditDate="2013-01-07T15:08:50.553" LastEditorUserId="7290" OwnerUserId="18435" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;cross-validation&gt;&lt;feature-selection&gt;&lt;caret&gt;" Title="Feature selection using caret + repeatedcv" ViewCount="1445" />
  <row AnswerCount="0" Body="&lt;p&gt;I have data for multiple people, where each person performs and is graded on a task an arbitrary number of times each year across multiple years.  E.g. -  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;          year 1          year 2         year 3&#10;Person1   1 1 3 2 4       2 3 3          5 2 3 4 1  &#10;Person2   2 3 1           7 9 1 2        3 3 1  &#10;Person3  ...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So in year 1, person 1 does the task 5 times and gets scores 1 1 3 2 4.  I take the mean for each person in each year:    &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;          year 1          year 2         year 3  &#10;Person1   2.2             2.7            3  &#10;Person2   2               4.75           2.3  &#10;Person3  ...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;How do I test for a significant difference between people in the means for each year?  I assume that I could do a repeated measures ANOVA on the means, but this ignores the number of measurements per person per year.  For example, for person 1 in year1, there are 5 measurements compared to 3 for person2 in year1, making the former measurement more certain.  In general, some people can have far more measurements than other people across all years.  Thanks.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-07T16:30:35.240" Id="47160" LastActivityDate="2013-01-07T16:41:52.923" OwnerUserId="18438" PostTypeId="1" Score="1" Tags="&lt;anova&gt;&lt;repeated-measures&gt;" Title="Comparing performance between individuals across time with variable number of measurements" ViewCount="76" />
  
  <row AcceptedAnswerId="47193" AnswerCount="1" Body="&lt;p&gt;This is a very basic question about Bayesian inference. I'm not grasping one or more fundamental concepts.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's say I have two observed outcomes, &lt;em&gt;X&lt;/em&gt; and &lt;em&gt;Y&lt;/em&gt;. I want to infer the probabilities (&lt;em&gt;px&lt;/em&gt; and &lt;em&gt;py&lt;/em&gt;, respectively) of each occurring given &lt;em&gt;X&lt;/em&gt; and &lt;em&gt;Y&lt;/em&gt;. I do not know &lt;em&gt;N&lt;/em&gt;, the total number of trials. I'm assuming that &lt;em&gt;X&lt;/em&gt; and &lt;em&gt;Y&lt;/em&gt; are binomially distributed. How do I calculate the likelihood without &lt;em&gt;N&lt;/em&gt;? &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;What I ultimately want is to show the bivariate posterior distribution of &lt;em&gt;px&lt;/em&gt; and &lt;em&gt;py&lt;/em&gt; via MCMC. I do not care about estimating &lt;em&gt;N&lt;/em&gt;--I just want to show the chain in the plane of &lt;em&gt;px&lt;/em&gt; and &lt;em&gt;py&lt;/em&gt;. No convergence necessary.&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Clarification:&lt;/em&gt;  &lt;em&gt;X&lt;/em&gt; and &lt;em&gt;Y&lt;/em&gt; are drawn from the same &lt;em&gt;N&lt;/em&gt;: &lt;em&gt;X&lt;/em&gt; ~Binom(&lt;em&gt;N&lt;/em&gt;, &lt;em&gt;px&lt;/em&gt;) and &lt;em&gt;Y&lt;/em&gt; ~Binom(&lt;em&gt;N&lt;/em&gt;, &lt;em&gt;py&lt;/em&gt;). We have no other information about &lt;em&gt;px&lt;/em&gt; or &lt;em&gt;py&lt;/em&gt;, although I'll use a beta prior to start. Also assume that &lt;em&gt;X&lt;/em&gt; and &lt;em&gt;Y&lt;/em&gt; are independent.&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2013-01-07T21:21:47.407" FavoriteCount="1" Id="47190" LastActivityDate="2013-01-08T13:18:29.777" LastEditDate="2013-01-08T07:43:33.420" LastEditorUserId="88" OwnerUserId="18448" PostTypeId="1" Score="3" Tags="&lt;bayesian&gt;&lt;binomial&gt;&lt;mcmc&gt;" Title="Basic Bayesian MCMC to estimate two parameters from binomial distributions given unknown number of trials" ViewCount="571" />
  <row Body="&lt;h2&gt;Model and Pseudocode&lt;/h2&gt;&#10;&#10;&lt;p&gt;So I did some analysis in Python, though I used the pyMC library which hides all the MCMC mathy stuff. I'll show you how I modeled it in semi-pseudocode, and the results. &lt;/p&gt;&#10;&#10;&lt;p&gt;I set my observed data as $X=5, Y=10$. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;X = 5&#10;Y = 10&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I assumed that $N$ has a Poisson prior, with the Poisson's rate a $EXP(1)$. This is a pretty fair prior. Though I could have chosen some uniform distribution on some interval:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;rate = Exponential( mu = 1 )&#10;N = Poisson( rate = rate)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;You mention beta priors on $pX$ and $pY$, so I coded:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;pX = Beta(1,1) #equivalent to a uniform&#10;pY = Beta(1,1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And I combine it all:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;observed = Binomial(n = N, p = [pX, pY], value = [X, Y] )&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Then I perform the MCMC over 50000 samples, burned-in about half of that. Below are the plots I generated after MCMC.&lt;/p&gt;&#10;&#10;&lt;h2&gt;Interpretation:&lt;/h2&gt;&#10;&#10;&lt;p&gt;Let's examine the first graph for $N$. The &lt;em&gt;N Trace&lt;/em&gt; graph are the samples, in order, I generated from the posterior distribution. The &lt;em&gt;N acorr&lt;/em&gt; graph is the auto-correlation between samples. Perhaps there is still too much auto-correlation, and I should burn-in more. Finally, &lt;em&gt;N-hist&lt;/em&gt; is the histogram of posterior samples. It looks like the mean is 13. Notice too that no samples were drawn from below 10. This is a good sign, as that would be impossible given the data observed was 5 and 10. &lt;/p&gt;&#10;&#10;&lt;p&gt;Similar observations can be made for the $pX$ and $pY$ graphs.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/FjhKY.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ToevE.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/HJUWn.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;h2&gt;Different Prior on $N$&lt;/h2&gt;&#10;&#10;&lt;p&gt;If we restrict $N$ to be a Poisson( 20 ) random variable (and remove the Exponential heirarchy), we get different results. This is an important consideration, and reveals that the prior can make a large difference. See the plots below. Note the time to convergence was much larger here too. &lt;/p&gt;&#10;&#10;&lt;p&gt;On the other hand, using a Poisson( 10 ) prior produced similar results to the Exp. rate prior.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/80I4V.png&quot; alt=&quot;enter image description here&quot;&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/fPMVU.png&quot; alt=&quot;enter image description here&quot;&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/Jc9UL.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2013-01-07T22:01:39.620" Id="47193" LastActivityDate="2013-01-08T13:18:29.777" LastEditDate="2013-01-08T13:18:29.777" LastEditorUserId="11867" OwnerUserId="11867" ParentId="47190" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;Unless I got something wrong, I think it might me possible to bring down you $n,m$ problem to a $2,n$ problem, which you have resolved for $n=10$ and of which the solution is more generally $p = \frac{n+1}{2n}$&lt;/p&gt;&#10;&#10;&lt;p&gt;$P[X_m \ge X_{m-1} \ge ... \ge X_1]=$&lt;/p&gt;&#10;&#10;&lt;p&gt;$P[X_m \ge \max (X_{m-1},X_{m-2}  ...  X_1);X_{m-1} \ge \max (X_{m-2},X_{m-3}  ...  X_1);...X_1 \ge \max(X_1)=X_1]$ &lt;/p&gt;&#10;&#10;&lt;p&gt;Because they are $i.i.d$ this makes it:&lt;/p&gt;&#10;&#10;&lt;p&gt;$=P[X_m \ge \max (X_{m-1},X_{m-2}  ...  X_1]P[X_{m-1} \ge \max (X_{m-2},X_{m-3}  ...  X_1]... P[X_1 \ge \max(X_1)=X_1]$ &lt;/p&gt;&#10;&#10;&lt;p&gt;$=\prod_{i=1}^{i=m-1} P[X_m \ge X_{i}] \prod_{i=1}^{i=m-2} P[X_{m-1} \ge X_{i}]\prod_{i=1}^{i=m-(m-1)} P[X_{2} \ge X_{i}]$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $P_{i\not=j}[X_i \ge X_j] = \frac{n+1}{2n}$&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-07T22:34:46.117" Id="47194" LastActivityDate="2013-01-07T22:34:46.117" OwnerUserId="18287" ParentId="46928" PostTypeId="2" Score="-1" />
  
  
  
  
  
  
  <row Body="&lt;p&gt;No, the answer is not unique. There are many ways to show this. One possibility is to notice that spectral decomposition of a square $p$ by $p$ matrix $X$ is the solution to the &lt;em&gt;maximization&lt;/em&gt; of a convex function of $w$. Consider the first eigen-vector/value:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\lambda_1=\underset{w\in\mathbb{R}^{p}:||w||=1}{\max} w'Xw$$&lt;/p&gt;&#10;&#10;&lt;p&gt;(where $\lambda_1$ is the first eigen-value and $w^*$ the first eigen-vector).&lt;/p&gt;&#10;&#10;&lt;p&gt;The solution to such problems (e.g. the values of $w$ attaining that maximum) are, in general, not unique. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;However&lt;/strong&gt; the algorithms for computing these solutions are deterministic, meaning that  save for numerical corner cases, the solutions you get should be the same.&lt;/p&gt;&#10;&#10;&lt;p&gt;Example of such numerical corner cases: cases where several eigen-values are (numerically) the same, cases where the $X$ is rank-deficient...&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-08T09:59:06.157" Id="47216" LastActivityDate="2013-01-08T20:17:57.600" LastEditDate="2013-01-08T20:17:57.600" LastEditorUserId="603" OwnerUserId="603" ParentId="39073" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;I am not an R expert so maybe there is a simpler way but I have come across this before.  What I did before is implement a function that measures the distance (in time units) between the actual dates and saves that in a new column in the existing time series.  So we have something like:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;index/date | value | distance  &#10;01.01.2011 |  15   |   1  &#10;02.01.2011 |  17   |   3  &#10;05.01.2011 |  22   |   ..   &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This way, if your time series is not yet associated with an actual series of points in time (or wrong format or whatever), then you can still work with it.&lt;/p&gt;&#10;&#10;&lt;p&gt;Next, you write a function that creates a new time series for you, like so:  &lt;/p&gt;&#10;&#10;&lt;p&gt;First, you calculate how many units of time the time series actually would have between the dates of your chosing and create that timeline in zoo or ts or whatever the choice is with empty values.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Second, you take your incomplete time series array and, using a loop, fill the values into the correct timeline, according to the limits of your choosing.  When you come upon a row where the unit distance is not one (days (units) are missing), you fill in interpolated values.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Now, since this is your function, you can actually chose how to interpolate. For example you decide that if the distance is less than two units, you use a standard linear interpolation. If a week is missing, you do something else and if a certain threshold of missing dates is reached, you give out a warning about the data - really whatever you want to imagine.  &lt;/p&gt;&#10;&#10;&lt;p&gt;If the loop reaches the end date you return your new ts.&lt;/p&gt;&#10;&#10;&lt;p&gt;Advantage of such a function is that you can use different interpolations or handling procedures depending on the lengths of the gap and return a cleanly creates series in the format of your choosing. Once written, it allows you to gain clean and nice ts out of any sort of tabular data. Hope this helps you somehow.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-01-08T12:54:24.110" Id="47226" LastActivityDate="2013-01-08T14:09:57.377" LastEditDate="2013-01-08T14:09:57.377" LastEditorUserId="7290" OwnerUserId="18459" ParentId="47185" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;If you want to include &lt;strong&gt;past&lt;/strong&gt; space-time lags in your VAR model this is perfectly reasonable (as past spatial lags are exogenous, the same logic for past time periods of $y$ are exogenous). I would guess the most usual way to accomplish this is to include a $Wy_{t-1}$ term in the model, which is the column vector obtained when you pre-multiply the time lag, $y_{t-1}$ by $W$ (which is your &lt;em&gt;a priori&lt;/em&gt; specified spatial weights matrix). Although certainly an over-simplification in most realistic circumstances, you still have all the usual time-series models availables if you go this route and wouldn't be too arduous to code up yourself.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you want an endogenous spatial lag in the model (i.e. $Wy_{t}$), this might involve building an appropriate spatial weights matrix, and then using &lt;a href=&quot;http://stats.stackexchange.com/a/38590/1036&quot;&gt;the usual means&lt;/a&gt; to estimate models with endogenous spatial lags. An &quot;appropriate&quot; spatial weights matrix would look like $I_t \otimes W$ where $I_t$ is an Identity matrix with the number of rows and columns equal to the number of time periods, and $\otimes$ is the Kronecker product.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is a brief example in R of what such a block spatial weights matrix would like with a binary $W$;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; t &amp;lt;- diag(3)&#10;&amp;gt; w &amp;lt;- matrix(c(0,1,0,&#10;+               1,0,1,&#10;+               0,1,0), nrow = 3)&#10;&amp;gt; &#10;&amp;gt; t %x% w&#10;      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]&#10; [1,]    0    1    0    0    0    0    0    0    0&#10; [2,]    1    0    1    0    0    0    0    0    0&#10; [3,]    0    1    0    0    0    0    0    0    0&#10; [4,]    0    0    0    0    1    0    0    0    0&#10; [5,]    0    0    0    1    0    1    0    0    0&#10; [6,]    0    0    0    0    1    0    0    0    0&#10; [7,]    0    0    0    0    0    0    0    1    0&#10; [8,]    0    0    0    0    0    0    1    0    1&#10; [9,]    0    0    0    0    0    0    0    1    0&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;(This also shows an easy way to generate the space time lags, if you pre-multiply $W$ by an identity matrix where all the 1's are shifted down 1 row that will produce the $Wy_{t-1}$ column vector).&lt;/p&gt;&#10;&#10;&lt;p&gt;This has the negative that the matrix is huge, so it might not even be feasible to estimate this model. Also, as far as I'm aware, there isn't much current code floating around for complicated space-time models, so I would off-the-cuff say the time series aspect is somewhat limited to including AR or simple trend terms unless you want to code up your own estimators as well (I would love for people to correct me and point to working code libraries/examples if I am wrong).&lt;/p&gt;&#10;&#10;&lt;p&gt;I would suggest you get modelling/coding motivation from Lesage and Pace's &lt;a href=&quot;http://www.spatial-econometrics.com/&quot; rel=&quot;nofollow&quot;&gt;Matlab toolbox&lt;/a&gt;. Also there book goes into great detail about coding up spatial models (and is language agnostic) and so if your serious about rolling your own it would be highly recommended.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also FYI, I would suggest you utilize the handy functions in the python library &lt;a href=&quot;http://pysal.geodacenter.org/1.4/index.html#&quot; rel=&quot;nofollow&quot;&gt;pysal&lt;/a&gt; to implement your own STAR library if you so desire, they have all the annoying stuff about generating spatial weights matrices already taken care of. Also I suspect they are a good group to ask who is developing space-time models and if any working code is already available.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;It probably should be mentioned as well that in some fields (e.g. epidemiology) it is popular to fit Bayesian models and estimate the spatial terms via MCMC. I am admittedly less familiar with this though, and so would just point to the &lt;a href=&quot;http://www.mrc-bsu.cam.ac.uk/bugs/winbugs/geobugs.shtml&quot; rel=&quot;nofollow&quot;&gt;GeoBugs&lt;/a&gt; project where one might find examples (I can scrounge up some examples from my library requested).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-08T13:25:54.927" Id="47231" LastActivityDate="2013-01-08T13:25:54.927" OwnerUserId="1036" ParentId="47202" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Your differentiation is almost right, but you've put $X$ where you should have put $t$:&#10;$$\newcommand{\diff}{\mathrm{d}}&#10;\frac{\diff M_X(t)}{\diff t}=-2\left(\frac{-k}{2}\right)(1-2t)^{-\frac{k}{2}-1}&#10;$$&#10;Now all you need to do is set $t = 0$ in that expression, recalling that $1$ to the power of anything is $1$.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-01-08T14:03:14.653" Id="47232" LastActivityDate="2013-01-08T14:03:14.653" OwnerUserId="17230" ParentId="47229" PostTypeId="2" Score="2" />
  
  
  
  <row AcceptedAnswerId="47260" AnswerCount="2" Body="&lt;p&gt;I am working on a few algorithms where I have a list of $N$ samples. Currently I have plotted these into a histogram and have a view of how uniform the values are distributed within an interval, which is quite good as a visualization, although I need a comparable value of how uniform the dataset is, in order to measure how robust it is compared to my other algorithms.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have been looking at chi-squared test, but could not figure out how it would become helpful in my usecase?&lt;/p&gt;&#10;&#10;&lt;p&gt;Sample from dataset:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;8725&#10;462&#10;1492&#10;972&#10;9941&#10;8235&#10;8220&#10;6949&#10;1252&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Code for importing data and applying chi-squared in &lt;code&gt;R&lt;/code&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;mydata = read.csv2(&quot;/opt/doc/stat/uniform_test_1.csv&quot;)&#10;x &amp;lt;- sapply(mydata, as.numeric)&#10;chisq.test(x)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Result: &lt;code&gt;X-squared = 1664769844, df = 999998, p-value &amp;lt; 2.2e-16&lt;/code&gt;&lt;/p&gt;&#10;" CommentCount="15" CreationDate="2013-01-08T17:04:30.817" FavoriteCount="1" Id="47246" LastActivityDate="2013-04-23T04:23:12.890" LastEditDate="2013-01-08T20:03:25.620" LastEditorUserId="11580" OwnerUserId="11580" PostTypeId="1" Score="2" Tags="&lt;chi-squared&gt;&lt;histogram&gt;&lt;uniform&gt;" Title="Defining the &quot;uniformity&quot; of a dataset" ViewCount="366" />
  <row Body="&lt;p&gt;The &quot;goodness&quot; or &quot;badness&quot; of a regression model cannot be judged by any set of statistics alone. A model is &quot;good&quot; if it enlightens you, helps you solve a problem.... etc. or, to the extent to which it meets the &quot;Magic&quot; criteria, as introduced by Robert Abelson in his book &lt;a href=&quot;http://www.statisticalanalysisconsulting.com/book-review-statistics-as-principled-argument-by-robert-abelson/&quot; rel=&quot;nofollow&quot;&gt;Statistics as Principled Argument&lt;/a&gt; (link goes to my review of the book).&lt;/p&gt;&#10;&#10;&lt;p&gt;A high standard error (relative to the coefficient) means either that 1) The coefficient is close to 0 or 2) The coefficient is not well estimated or some combination. &quot;High&quot; by itself doesn't really have a set meaning (you can change the SE by changing the unit - measure in miles instead of microns and the SE will be tiny). &lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-01-08T19:23:35.557" Id="47254" LastActivityDate="2013-01-08T19:23:35.557" OwnerUserId="686" ParentId="47245" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;It is not hard to show that for every $\omega \neq \frac12$, $\exists N$ $\colon$ $X_n(\omega)=0$ for all $n \geq N$. Is it clear for you ? Then the conclusion is straightforward.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-01-08T21:44:54.600" Id="47263" LastActivityDate="2013-01-08T21:44:54.600" OwnerUserId="8402" ParentId="47261" PostTypeId="2" Score="6" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm wondering if there exist methods similar to one used in random forest algorithm - I mean taking simultaneously bootstrap sample and random subset of features, then building statistisal model. Have anyone took this approach for building set of regression models ? Is this approach ( random subsample plus random subset of features ) somehow universal ?&lt;/p&gt;&#10;&#10;&lt;p&gt;Edition :&#10;the question is all about the possibility of putting some other model in the place of classification tree in random forest, what is left is some kind of meta-algorithm (as bagging can be view as meta-algorithm) = bagging + random subsets of variables&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-01-08T23:30:51.190" Id="47272" LastActivityDate="2013-01-10T13:09:57.653" LastEditDate="2013-01-10T13:09:57.653" LastEditorUserId="4908" OwnerUserId="4908" PostTypeId="1" Score="-1" Tags="&lt;regression&gt;&lt;feature-selection&gt;&lt;algorithms&gt;&lt;random-forest&gt;&lt;theory&gt;" Title="Random forest like procedure for regression or other statistical models" ViewCount="172" />
  <row Body="&lt;p&gt;Here is a simple tool in HTML5 (a Venn Pie Chart) that might help to visualize conditional probabilities and Bayes' theorem:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://dl.dropbox.com/u/133074120/venn_pie_solver.html&quot; rel=&quot;nofollow&quot;&gt;http://dl.dropbox.com/u/133074120/venn_pie_solver.html&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-09T00:12:05.470" Id="47278" LastActivityDate="2013-01-09T00:12:05.470" OwnerUserId="18481" ParentId="27406" PostTypeId="2" Score="0" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am currently working on time series modeling, especially on stationarity tests. For this purpose, I am extensively using Pfaff's book &quot;&lt;a href=&quot;http://www.springer.com/statistics/statistical+theory+and+methods/book/978-0-387-75966-1&quot; rel=&quot;nofollow&quot;&gt;Analysis of integrated and cointegrated time series with R&lt;/a&gt;&quot; and I have some questions :&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;On page 63, there is a nice ordinogram (Figure 3.3) explaining how all the ADF tests are related, and what should be the underlying decision tree. First of all, one needs to estimate the ADF equation with a linear trend and test for $\pi=0$ (this statistic is called &lt;code&gt;tau3&lt;/code&gt; in the associated package &lt;a href=&quot;http://cran.r-project.org/web/packages/urca/index.html&quot; rel=&quot;nofollow&quot;&gt;urca&lt;/a&gt;). If we reject the null hypothesis, then there is no unit root. If we cannot reject, we test for $\beta_2=0$ given $\pi=0$ (this statistic is called &lt;code&gt;phi3&lt;/code&gt; in &lt;code&gt;urca&lt;/code&gt;). If we reject, then Pfaff writes &quot;test again for a unit root using a standardized normal&quot; with no further explanation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Does anyone understand what he is talking about? Does this &quot;normal test&quot; appear somewhere in the urca implementation?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Suppose the test &lt;code&gt;tau3&lt;/code&gt; for $\pi=0$ is rejected. Then the conclusion should be that there is no unit root but a trend in the series (the series is trend stationary). I have at disposal  the underlying linear regression result given by &lt;code&gt;ur.df()&lt;/code&gt; from the package &lt;code&gt;urca&lt;/code&gt;. Is it correct to conclude that there is actually no trend when the p-value of the t-statistic for the trend coefficient is significant? &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Thanks in advance for your help.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-01-09T09:29:12.463" FavoriteCount="1" Id="47302" LastActivityDate="2013-01-09T11:33:57.470" LastEditDate="2013-01-09T11:33:57.470" LastEditorUserId="17230" OwnerUserId="17913" PostTypeId="1" Score="5" Tags="&lt;r&gt;&lt;time-series&gt;&lt;stationarity&gt;" Title="Stationarity tests for time series" ViewCount="512" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have created 5 imputations of a dataset and have fit a survival model to them all in R. I want to combine the estimates of the coefficients and the standard errors of the coefficients. To do this I have taken the mean of the coefficients and combined the standard errors using Rubins formula, i.e. &#10;$$(1+1/m) \times \text{between imputation variance} + \text{within imputation variance}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I was thinking about it and I'm having some doubts as to whether this makes sense? Wouldn't the variance be over estimated in this case? I performed a t-test on the coefficients and it results in a relatively high p-value when I know the data should show a good correlation. I use Rubins df formula for the t-test. Is there an alternative method of producing the combine variance, one that doesn't result in over estimation?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-08T15:26:51.393" Id="47308" LastActivityDate="2014-07-17T19:33:17.803" LastEditDate="2013-01-09T13:28:11.110" LastEditorUserId="930" OwnerDisplayName="Tom" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;variance&gt;&lt;survival&gt;&lt;multiple-imputation&gt;" Title="Combined variance following multiple imputation with survival model" ViewCount="152" />
  <row Body="&lt;p&gt;I wouldn't call observed correlations spurious, but rather false causal inferences drawn from those correlations. Problems with ratios are of a kind with other types of confounding.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you define random variables $U=\frac{X}{Q}$ &amp;amp; $V=\frac{Y}{Q}$, where $X$, $Y$, &amp;amp; $Q$ are independent, then  $X$ &amp;amp; $Y$ are correlated.  This could mislead you into thinking there's a causal relationship from $X$ to $Y$ or vice versa, or from something other than $Q$ to both of them. It's no use however simply deciding to eschew the use of ratios. Observations aren't &lt;em&gt;essentially&lt;/em&gt; ratios, &amp;amp; if $U$, $V$, &amp;amp; $Q$ are independent you will introduce &quot;spurious&quot; correlations by using the ratios $X=\frac{U}{1/Q}$ &amp;amp; $Y=\frac{V}{1/Q}$. Including $Q$ in your analysis&amp;mdash;&amp;amp; it's important to note that 'scaling' by $Q$ is not the same thing&lt;sup&gt;&amp;dagger;&lt;/sup&gt;&amp;mdash;protects you whichever you use; but not from other confounders $R,S,T,\ldots$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://projecteuclid.org/euclid.ss/1177009870&quot; rel=&quot;nofollow&quot;&gt;Aldrich (1995), &quot;&quot;Correlations Genuine and Spurious in Pearson and Yule&quot;, &lt;em&gt;Statistical Science&lt;/em&gt;, &lt;strong&gt;10&lt;/strong&gt;, 4&lt;/a&gt; provides an intesting historical perspective.&lt;/p&gt;&#10;&#10;&lt;p&gt;&amp;dagger; See &lt;a href=&quot;http://stats.stackexchange.com/questions/11009/including-the-interaction-but-not-the-main-effects-in-a-model&quot;&gt;Including the interaction but not the main effects in a model&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-09T13:07:59.603" Id="47309" LastActivityDate="2014-02-13T16:43:05.563" LastEditDate="2014-02-13T16:43:05.563" LastEditorUserId="17230" OwnerUserId="17230" ParentId="47222" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Two comments:&lt;/p&gt;&#10;&#10;&lt;p&gt;First, I would explore other methods of variable selection. Looking at a set of unadjusted regressions and choosing the &quot;significant&quot; variables to then include in a final model is not an ideal approach. Your options are plentiful - search around here for topics on model selection to get you started. There are experts on this topic floating around, so perhaps some of them will add more here. &lt;/p&gt;&#10;&#10;&lt;p&gt;Second, think about the interpretation of the regression coefficient in a multiple regression. An increase in your clinical score is associated with X change in Y, holding covariates constant. Theoretically, if your clinical score increases, and you hold some component of that score constant, than the estimated association must be due to the other aspects of the clinical scale. &lt;/p&gt;&#10;&#10;&lt;p&gt;What to do really depends on what your goals are. Are you more interested in predicting things, or making any causal inferences? Can you discard the clinical scale entirely and focus on its components,or are you stuck using it?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-01-09T14:38:23.173" Id="47315" LastActivityDate="2013-01-09T14:44:05.143" LastEditDate="2013-01-09T14:44:05.143" LastEditorUserId="16049" OwnerUserId="16049" ParentId="47314" PostTypeId="2" Score="3" />
  
  <row AnswerCount="3" Body="&lt;p&gt;I have data from questionnaire from school. 35 questions are various questions (influence of friends etc.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Possible answers for 35 questions are &quot;definitely yes&quot;, &quot;mostly yes&quot;, &quot;mostly no&quot; and &quot;definitely no&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;I did hierarchical clustering using &lt;code&gt;hclust&lt;/code&gt; in R. Then I used &lt;code&gt;cutree&lt;/code&gt; for cut the dendrogram.&lt;/p&gt;&#10;&#10;&lt;p&gt;How to visualize data about clusters from &lt;code&gt;cutree&lt;/code&gt;? I wrote function for export information about clusters to CSV, but I want to display graphical information.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-01-09T15:04:21.803" FavoriteCount="1" Id="47319" LastActivityDate="2014-03-19T07:47:14.977" LastEditDate="2013-03-05T20:12:24.623" LastEditorUserId="5739" OwnerUserId="14745" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;data-visualization&gt;&lt;multilevel-analysis&gt;" Title="Data visualization from hierarchical clustering" ViewCount="786" />
  <row Body="&lt;p&gt;I believe your data is measured on a &lt;a href=&quot;http://en.wikipedia.org/wiki/Likert_scale&quot; rel=&quot;nofollow&quot;&gt;Likert Scale&lt;/a&gt; which might give you some leads.&lt;/p&gt;&#10;&#10;&lt;p&gt;I think the simplest way to approach this is that you could have already made graphs to explore the data before you clustered it, and the output of &lt;code&gt;cutree&lt;/code&gt; (cluster number for each questionnairs) can be used to enhance these graphs.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, in the &lt;code&gt;lattice&lt;/code&gt; package, using &lt;code&gt;xyplot&lt;/code&gt;, you can specify things like (making up data, perhaps you have it, perhaps you don't):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;xyplot (study.hours ~ age | cutree (h), data=surv)&#10;xyplot (year ~ major | cutree (h), data=surv, jitter.x=TRUE, jitter.y=TRUE)&#10;xyplot (year ~ major, groups=cutree (h), data=surv, jitter.x=TRUE, jitter.y=TRUE)&#10;xyplot (happy ~ gym, groups=cutree (h), data=surv, jitter.x=TRUE, jitter.y=TRUE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Etc. (In lattice, the vertical bar splits the graph up into multiple sub-graphs, while the groups= color-codes the points within the same graph.) The &lt;code&gt;jitter&lt;/code&gt;is because I envision gym and happy as being answers to survey questions that are encoded on an integer scale of 1-4, and without jitter, you'd just get 16 places in the graph where points are all overplotted. Look at &lt;code&gt;?panel.xyplot&lt;/code&gt; to see variables like &lt;code&gt;amount&lt;/code&gt; that allow you to jitter more or less.&lt;/p&gt;&#10;&#10;&lt;p&gt;You might also want to investigate the &lt;code&gt;kohonen&lt;/code&gt; package, which implements SOM's, which are ways to get 2-D visualizations of multi-dimensional data. Might be better for your purposes than &lt;code&gt;hclust&lt;/code&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-09T15:43:26.920" Id="47328" LastActivityDate="2013-01-09T16:17:43.090" LastEditDate="2013-01-09T16:17:43.090" LastEditorUserId="1764" OwnerUserId="1764" ParentId="47319" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="47392" AnswerCount="1" Body="&lt;p&gt;This question is with regards to using a test data set to validate an imputed Cox model using R.  With a non-imputed data set I would use &lt;code&gt;val.surv()&lt;/code&gt; from &lt;code&gt;rms&lt;/code&gt;, but I'm not sure how/if I can use it with my multiply imputed data set.&lt;/p&gt;&#10;&#10;&lt;p&gt;Further explanation:&#10;I created a predictive Cox PH model for 5-year RFS, and also used the &lt;code&gt;mice&lt;/code&gt; package to multiply impute some missing data in the training data set.  I then used &lt;code&gt;fit.mult.impute()&lt;/code&gt; in Dr. Frank Harrell's excellent &lt;code&gt;Hmisc&lt;/code&gt; package to obtain the pooled model.  I have a data set that I would like to test the model on, but I am not sure how best to validate the pooled model.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Multiple imputation is a common procedure that many researchers utilize, so there must be a way that R users are validating their models with imputed data.  I would like to know what functions/avenues are available for me to test this pooled model with my validation data set?  Here is some sample code to work with:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(rms)&#10;library(survival)&#10;library(mice)&#10;&#10;remove(veteran)&#10;data(veteran)&#10;veteran$trt=factor(veteran$trt,levels=c(1,2))&#10;veteran$prior=factor(veteran$prior,levels=c(0,10))&#10;&#10;#Set random data to NA &#10;veteran[sample(137,4),1]=NA&#10;veteran[sample(137,4),2]=NA&#10;veteran[sample(137,4),7]=NA&#10;&#10;impvet=mice(veteran)&#10;survmod=with(veteran,Surv(time,status))&#10;&#10;#make a CPH for each imputation&#10;for(i in seq(5)){&#10;    assign(paste(&quot;mod_&quot;,i,sep=&quot;&quot;),cph(survmod~celltype+karno,&#10;        data=complete(impvet,i),x=T,y=T))&#10;}&#10;&#10;#Now there is a CPH model for mod_1, mod_2, mod_3, mod_4, and mod_5.&#10;&#10;pooled_mod=fit.mult.impute(survmod~celltype+karno,cph,impvet,data=veteran,surv=T)&#10;&#10;#Here is a test data set.&#10;remove(veteran)&#10;test_dat=data.frame(trt=replicate(500,NA), celltype=replicate(500,NA), time=replicate(500,NA), status=replicate(500,NA), karno=replicate(500,NA), diagtime=replicate(500,NA), age=replicate(500,NA), prior=replicate(500,NA))&#10;for(i in seq(8)){&#10;test_dat[,i]=sample(veteran[,i],500,replace=T)&#10;}&#10;&#10;#Now there is a pooled model, &quot;pooled_mod&quot;, and a test data set, &quot;test_dat&quot;.&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I'm looking forward to hearing about the R methods that can help in this situation.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-01-10T03:22:47.257" FavoriteCount="1" Id="47377" LastActivityDate="2013-01-10T07:59:05.300" LastEditDate="2013-01-10T03:38:36.967" LastEditorUserId="18008" OwnerUserId="18008" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;predictive-models&gt;&lt;survival&gt;&lt;validation&gt;&lt;rms&gt;" Title="Best method to validate a multiply imputed Cox model with R?" ViewCount="458" />
  <row Body="&lt;p&gt;I went through the math and ended up with variant C:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$Var(X) = \frac{(\sum_i \omega_i)^2}{(\sum_i \omega_i)^2 - \sum_i \omega_i^2}\overline V$$&#10;where $\overline V$ is the non corrected variance estimation. The formula agrees with the unweighted case when all $\omega_i$ are identical. I detail the proof below:&lt;/p&gt;&#10;&#10;&lt;p&gt;Setting $\lambda_i = \frac{\omega_i}{\sum_i \omega_i}$, we have&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\overline V = \sum_i \lambda_i (x_i -  \sum_j \lambda_j x_j)^2$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Expanding the inner term gives:&#10;$$(x_i -  \sum_j \lambda_j x_j)^2 = x_i^2 + \sum_{j, k} \lambda_j \lambda_k x_j x_k - 2 \sum_j \lambda_j x_i x_j $$&lt;/p&gt;&#10;&#10;&lt;p&gt;If we take the expectation, we have that $E[x_i x_j] = Var(X)1_{i = j} + E[X]^2$, the term $E[X]$ being present in each term, it cancels out and we get:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$E[\overline V] = Var(X) \sum_i \lambda_i (1 + \sum_j \lambda_j^2- 2 \lambda_i )$$ that is &#10;$$E[\overline V] = Var(X) (1 - \sum_j \lambda_j^2)$$&#10;It remains to plug in the expression of $\lambda_i$ with respect to $\omega_i$ to get variant C. &lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-01-10T03:36:18.430" Id="47379" LastActivityDate="2013-01-13T00:39:09.737" LastEditDate="2013-01-13T00:39:09.737" LastEditorUserId="18274" OwnerUserId="18274" ParentId="47325" PostTypeId="2" Score="6" />
  <row AnswerCount="0" Body="&lt;p&gt;I am wondering if this the right place to ask this question. Normally it should be :).&#10;I am recently reading some papers about &lt;a href=&quot;http://en.wikipedia.org/wiki/Matrix_completion&quot; rel=&quot;nofollow&quot;&gt;matrix completion&lt;/a&gt; such as in &lt;a href=&quot;http://www.stanford.edu/~montanar/RESEARCH/FILEPAP/spectral16.pdf&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;, and &lt;a href=&quot;http://arxiv.org/pdf/0901.3150.pdf&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;I didn't go through some testing yet, but I am wondering  about the following problem, suppose I have some elements $E_1,E_2,....,E_n$ and suppose that I have information about $m$ elements $E_1,...,E_m$ such as $m &amp;lt; n$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose that at this stage I applied one of the matrix completion algorithms and I got my $n \times n$ matrix, How to assign labels (names) to the completed columns? &#10;The problem is that the column order can be arbitrary, so it may be mathematically correct but may have no real meaning if the labels are not assigned correctly.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-01-10T08:01:26.253" FavoriteCount="1" Id="47393" LastActivityDate="2013-01-14T03:38:18.453" LastEditDate="2013-01-14T03:38:18.453" LastEditorUserId="2325" OwnerUserId="2325" PostTypeId="1" Score="2" Tags="&lt;self-study&gt;&lt;inference&gt;&lt;matrix&gt;" Title="Matrix completion: How to assign names to the completed columns?" ViewCount="101" />
  
  <row AnswerCount="2" Body="&lt;p&gt;Next weekend you will be participating in 12km cross country race on a mountain.The average time between  two successive wild animal sightings on the mountain is reported to be 5 minutes&lt;/p&gt;&#10;&#10;&lt;p&gt;(a) What is the probability that you see at least one wild animal in the 11th minute of the     race given that you will see 3 wild animals since the start of the race?&lt;/p&gt;&#10;&#10;&lt;p&gt;(b) What is the probability that it will take more than a quarter of a hour before you see a wild animal after ten minutes of running?&lt;/p&gt;&#10;&#10;&lt;p&gt;Now I have attempted (a) , but I don't know whether my thinking is correct. (b) on the other hand makes little sense to me.&lt;/p&gt;&#10;&#10;&lt;p&gt;My attempt:&lt;/p&gt;&#10;&#10;&lt;p&gt;(a)$ X~ Poisson(\frac{1}{5})$ and $Y~Exponential(\frac{1}{5})$&lt;/p&gt;&#10;&#10;&lt;p&gt;$Pr(X&amp;gt;4) = Pr(X&amp;gt;1)$  (I am thinking that this is some variation of the memoryless property)&lt;/p&gt;&#10;&#10;&lt;p&gt;$Pr(X&amp;gt;1) =Pr(Y&amp;lt;1)$&lt;/p&gt;&#10;&#10;&lt;p&gt;$Pr(Y&amp;lt;1)= 1 - (e^{(\frac{-1}{5})})$&lt;/p&gt;&#10;&#10;&lt;p&gt;Perhaps, I was a bit to vague in my attempt of (a). So here goes my attempt of(b).From my understanding of the question , it is asking what it the probability that the time between events(in this case animal sightings) is more than 25 minutes given you have been running for ten minutes&lt;/p&gt;&#10;&#10;&lt;p&gt;Now from what I know the fact that you have been running ten minutes is irrelevant this is due to memory-less property of the exponential  distribution &lt;/p&gt;&#10;&#10;&lt;p&gt;so without further ado  I present my attempt at (b) &lt;/p&gt;&#10;&#10;&lt;p&gt;let $X $ be exponentially distributed random variable with $\lambda = 1/5$&lt;/p&gt;&#10;&#10;&lt;p&gt;then &#10;$Pr(X &amp;gt;15) = 1 - Pr(X &amp;lt;= 15)$&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-01-10T10:35:15.100" FavoriteCount="1" Id="47405" LastActivityDate="2013-01-11T14:17:53.883" LastEditDate="2013-01-11T09:01:30.950" LastEditorUserId="18461" OwnerUserId="18461" PostTypeId="1" Score="2" Tags="&lt;self-study&gt;&lt;poisson&gt;&lt;exponential&gt;" Title="Poisson / exponential distribution" ViewCount="953" />
  
  
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a question. &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;In a two sample t test, a confidence interval for $\mu_2-\mu_1$ is constructed based on the hypothesis test $H_0: \mu = \mu_0$ versus $H_{a}: \mu \neq \mu_0$? &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="2" CreationDate="2013-01-10T20:36:51.040" Id="47463" LastActivityDate="2013-01-10T20:36:51.040" OwnerUserId="18554" PostTypeId="1" Score="0" Tags="&lt;hypothesis-testing&gt;" Title="confidence interval for 2 sample t test" ViewCount="88" />
  
  <row Body="&lt;p&gt;If your treatments are factors (and not ordered factors), you could add the intercept into the model (i.e. remove the &quot;-1&quot;) and just do &lt;code&gt;summary(mod1)&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The default contrasts, as set in options, is to use contr.treatment for factors.  This sounds like what you want.  contr.treatment means that each coefficient represents a comparison of that level with level 1 (omitting level 1 itself).  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# view default contrasts in options&#10;options(&quot;contrasts&quot;)&#10;#$contrasts&#10;#        unordered           ordered &#10;#&quot;contr.treatment&quot;      &quot;contr.poly&quot; &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;When you do &lt;code&gt;summary(mod1)&lt;/code&gt;, the first level will not be labelled, but all the other levels will be in comparison to it.  &lt;/p&gt;&#10;&#10;&lt;p&gt;If your control condition is not your first level, you need to use &lt;code&gt;factor()&lt;/code&gt; with a levels argument or &lt;code&gt;relevel()&lt;/code&gt; to make it first.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-01-10T21:39:49.600" Id="47466" LastActivityDate="2013-01-10T21:39:49.600" OwnerUserId="16640" ParentId="47464" PostTypeId="2" Score="1" />
  
  
  
  <row AcceptedAnswerId="47540" AnswerCount="1" Body="&lt;p&gt;I want to compare two distributions, to see if they are significantly different. They represent task time completions (so they range from 1 to around 1000 seconds) in two different months. They are not normally distributed. I want to see if their central tendencies are significantly different (at a first glance the mode, mean and median between the two months seem very close, just 3-4 seconds difference), but also to see if their shapes are similar (again, at a first glance, they look similar).&#10;I am currently carrying this analysis with SPSS 20.&#10;I have the Mann-Whitney test for testing central tendencies and the Kolmogorov-Smirnov test for the shape of the distribution, (although I have read that the K-S test is an overall comparison test for the distributions).&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, in the first month I have 300,000 observations and in the second month 122,000 observations. So, a lot of data ... but disproportionate. Is this an impediment to running these tests, the fact that the sample sizes are not equal? I ran both Mann-Whitney and K-S and they both seem to reject the null. How much should I trust the results given my sample sizes? Do you suggest any alternative tests?&#10;Thanks&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-01-11T12:36:58.377" FavoriteCount="1" Id="47498" LastActivityDate="2013-01-11T21:38:50.853" LastEditDate="2013-01-11T12:54:28.087" LastEditorUserId="17230" OwnerUserId="18429" PostTypeId="1" Score="1" Tags="&lt;sample-size&gt;&lt;mann-whitney-u-test&gt;&lt;kolmogorov-smirnov&gt;" Title="Mann-Whitney U test and K-S test with unequal sample sizes" ViewCount="2548" />
  <row Body="&lt;p&gt;I have never seen this done, and I doubt other people have either. One usually gets informed answers on this site within a couple of hours of posting something. It's been a day, and no joy.&lt;/p&gt;&#10;&#10;&lt;p&gt;My thinking is this: if you want to tell the model that some values are more trustworthy than others, use weights. If you downweight values where you doubt the accuracy of the data, the model will basically accept a worse fit at that point -- which is what you want. &lt;/p&gt;&#10;&#10;&lt;p&gt;Example: suppose you have a very &quot;married&quot; set of covariates for someone coded &quot;unmarried&quot; in the dodgy data set. Without weights, the fitting algorithm could distort the parameter estimates in order to get some kind of fit. With weights, the algorithm need not try so hard. In effect, it lets you have bigger residuals when you don't trust the data. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you want to go with your first idea of substituting data with probabilities, I would iterate: estimate probabilities that someone is married or not, then fit the model with my best guesses, then go back and adjust the estimates. This is an EM approach. So, I would not replace 0's and 1's with 0.8 and 0.2 in the fit. I would use 1 and 0 according as the probabilities were less than or greater than 0.5 - but then I would go back and adjust the probabilities on the basis of lack of fit at those points.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you look at what happens in a logistic regression model, the math involved really expects that the data are going to be 0's or 1's. I think you want to stick with that. My advice boils down to using weights or estimating marital status from the rest of the data.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-01-11T12:49:06.097" Id="47500" LastActivityDate="2013-01-11T12:49:06.097" OwnerUserId="14188" ParentId="47454" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="47623" AnswerCount="3" Body="&lt;p&gt;In order to get more fundamental in my understanding of probability I watched mathematicalmonk's lectures involving $\sigma$-algebras etc. - good.  One of my main concerns was to better understand the basis for conditional probability: $P[A|B] = P[AB]/P[B]$.  My question is simple&lt;/p&gt;&#10;&#10;&lt;p&gt;How do we know that this quotient is indeed a probability measure?&lt;br&gt;&#10;After all, things like the ``Odds'' $P[A] / P[\bar{A}]$ is not a probability measure.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-01-11T13:57:58.907" Id="47506" LastActivityDate="2013-01-13T17:22:41.523" OwnerUserId="18575" PostTypeId="1" Score="4" Tags="&lt;mathematical-statistics&gt;&lt;conditional-probability&gt;" Title="Mathematical basis for conditional probability" ViewCount="275" />
  
  <row AcceptedAnswerId="47558" AnswerCount="2" Body="&lt;p&gt;I would like to estimate a regression model of the type:&lt;/p&gt;&#10;&#10;&lt;p&gt;$&#10;load_t  = seasonality_t + trend_t + \beta * temperature_t,&#10;$&lt;/p&gt;&#10;&#10;&lt;p&gt;and I have load data and temperature on high frequency (hourly data). My impression is that&#10;the temperature does not influence load at the same frequency as I measure load (i.e. a change in temperature for 2 or 3 hours does not imply a change in load immediately). This is how I understand the application of the concept of coherency as in &lt;a href=&quot;http://eprints.nuim.ie/1968/1/JR_C81dfisf.pdf&quot; rel=&quot;nofollow&quot;&gt;http://eprints.nuim.ie/1968/1/JR_C81dfisf.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I tried to look at average temperature per day and use this in the regression but the results were not satisfying. How can we incorporate temperature into a practical model in the best way? Any hints? Good references?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-11T16:52:41.427" Id="47519" LastActivityDate="2013-01-31T19:44:14.060" LastEditDate="2013-01-31T03:28:30.490" LastEditorUserId="919" OwnerUserId="12147" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;forecast&gt;" Title="Load forecast model with temperature data" ViewCount="345" />
  <row Body="&lt;p&gt;What's posted in the only answer by Dapz does not do what it's supposed to do.&#10;If I choose a value &gt; 0 for any of the $\beta_0$, say the &quot;i-th&quot;, the corresponding $\beta^*$ of &quot;i&quot; will be lower than with standard ridge regression, instead of higher as it should be (because we penalize for moving away from something &gt; 0, instead of moving away from 0). &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-01-11T19:14:40.907" Id="47529" LastActivityDate="2013-01-11T19:14:40.907" OwnerUserId="18585" ParentId="24889" PostTypeId="2" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;My current model is:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;$$y_i = a_0 +a_1x_i + a_2x_i^2 + \epsilon_i $$&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;With $E[\epsilon_i] = 0, Var[\epsilon_i] = \sigma^2$.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have the information:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;$$\begin{matrix} i &amp;amp; \mathrm{Response} \, y_i &amp;amp; \mathrm{Covariate} \, x_i \\ 1 &amp;amp; -1 &amp;amp; -0.5 \\ 2 &amp;amp; -0.25 &amp;amp;-0.25 \\ 3 &amp;amp; 1 &amp;amp; 0 \\ 4 &amp;amp; 1.2 &amp;amp; 0.25 \\ 5 &amp;amp; 2.6 &amp;amp; 0.5 \end{matrix}$$&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I need to propose an orthogonal polynomial regression model of the same order as my current one.&lt;/p&gt;&#10;&#10;&lt;p&gt;So I thought that if I let $\zeta_1$ be the linear terms and $\zeta_2$ be the quadratic terms, then I want a model where $\sum \zeta_1 = \sum \zeta_2 = 0$. So I want two general forms of $\zeta$. So let us let:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\zeta_1 = a + bx_i$$&#10;$$\zeta_2 = a + bx_i + cx_i^2$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Now I want to solve for $a,b,c$. &lt;/p&gt;&#10;&#10;&lt;p&gt;But I'm stuck on how to solve for them. I can't just use normal simultaneous equations from here can I because everything cancels out leaving me with everything equal to $0$. What do I do?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-01-11T21:46:49.373" Id="47541" LastActivityDate="2013-01-11T21:46:49.373" OwnerUserId="14836" PostTypeId="1" Score="1" Tags="&lt;regression&gt;" Title="Constructing an orthogonal polynomial regression model" ViewCount="146" />
  
  <row AcceptedAnswerId="47548" AnswerCount="1" Body="&lt;p&gt;I am looking for an empirical paper that perform a difference-in-difference regression where the whole did regression (treated+post+post*treated) is interacted with a continuous variable (or a categorical one)... Do you have an idea of such a paper ? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-11T23:47:46.940" FavoriteCount="1" Id="47545" LastActivityDate="2013-01-12T11:22:19.233" LastEditDate="2013-01-12T11:22:19.233" LastEditorUserId="930" OwnerUserId="18591" PostTypeId="1" Score="2" Tags="&lt;interaction&gt;&lt;econometrics&gt;&lt;empirical&gt;" Title="Difference-in-difference with interaction terms" ViewCount="269" />
  
  
  <row Body="&lt;p&gt;Suppose your data is $\{(x_i,y_i)\}$ with non constant $x_i$, and you choose an arbitrary line  $y=\beta_0 + \beta_1 x $ with residuals $r_i = y_i - \beta_0 - \beta_1 x_i$ associated with it.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now suppose that the least squares regression line of $\{r_i\}$ on $\{x_i\}$ gives $r=\hat\beta_2 + \hat\beta_3 x $. Since this is a least squares regression line, if at least one of $\hat\beta_2$ or $\hat\beta_3$ is non-zero then you have &#10;$$\sum_i(r_i - \hat\beta_2 - \hat\beta_3 x_i)^2 \lt \sum_i r_i^2$$ &#10;or written another way &#10;$$\sum_i(y_i - (\beta_0 + \hat\beta_2) - (\beta_1 + \hat\beta_3)x_i)^2 \lt \sum_i(y_i - \beta_0 - \beta_1 x_i)^2.$$ &lt;/p&gt;&#10;&#10;&lt;p&gt;So your original arbitrary line can only minimise the sum of squares of the residuals if both $\hat\beta_2$ and $\hat\beta_3$ are zero. $\hat\beta_3=0$ means a horizontal regression line of the residuals.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-01-12T03:01:23.577" Id="47555" LastActivityDate="2013-01-12T03:01:23.577" OwnerUserId="2958" ParentId="47481" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="47578" AnswerCount="3" Body="&lt;p&gt;I came across this forum as I am trying to get a convincing answer. Apologies for speaking 'plain English' as I am not a statistical genius!&lt;/p&gt;&#10;&#10;&lt;p&gt;Basically I am implementing a pre- and post- questionnaire to be asked to young people (aged 12-17), and aimed at demonstrating the impact of a day intervention (educational programme lasting one day). The idea is to ask students to self- assess before and after the programme delivery across a set of 5 indicators (teamwork, confidence, employability ...). The rating scale will be 1-5 (strongly disagree to strongly agree) pr 1-4 (removing middle option).  &lt;/p&gt;&#10;&#10;&lt;p&gt;My question relates to the need in pairing each pre- and post- answer to the same student? Indeed, I thought of two ways in collecting information. ONE giving bunches of pre- and post- forms to the class with no identifiers for each student; in which case I thought of aggregating all the pre- and post- answers for each indicators, then work out the difference in aggregate. TWO pairing the pre- and post- answers for each student (probably using a barcode or unique serial number), thereby enabling me to work out individual score difference which can also be aggregated subsequently. &lt;/p&gt;&#10;&#10;&lt;p&gt;I wonder which method results in the stronger results statistically speaking or if they are no difference in the end as arithmetically I will get to the same answer? I will use optical marking forms and the reason for my question is to decide whether time savings in the first method (whereas teachers can distribute forms regardless of who fills them in) will results in dubious results? I am considering the tradeoff between practicality on the ground and data solidity as I would like to work out statistically significant percentages increases (t-statistic that the software can work out automatically). &lt;/p&gt;&#10;&#10;&lt;p&gt;Many thanks for your responses. &#10;M&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-12T13:49:51.877" Id="47567" LastActivityDate="2013-07-20T04:54:37.873" OwnerUserId="18604" PostTypeId="1" Score="1" Tags="&lt;sampling&gt;&lt;t-test&gt;&lt;survey&gt;" Title="Pre- post- questionnaire (t-test)" ViewCount="1516" />
  <row Body="&lt;p&gt;Yes unlabelled data &lt;em&gt;can&lt;/em&gt; improve performance, and the methods used are normally described as &quot;semi-supervised&quot; or &quot;transductive&quot; learning.  Without knowing more about the application is isn't really possible to suggest suitable algorithms, but there is a very good book edited by Chapelle, Scholkopf and Zien, called &quot;Semi-supervised learning&quot; (MIT Press) which would be a good place to start for semi-supervised learning.&lt;/p&gt;&#10;&#10;&lt;p&gt;Mathematical proof is somewhat difficult as it will help in some situations, but not others, so the best you can do is bounds on generalisation performance.  Chapter 8 of Vladimir Vapnik's book &quot;Statistical Learning Theory&quot; is probably what you want on transductive learning.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-01-12T15:15:43.117" Id="47573" LastActivityDate="2013-01-12T16:26:48.707" LastEditDate="2013-01-12T16:26:48.707" LastEditorUserId="887" OwnerUserId="887" ParentId="47116" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;In medical statistics, you need stochastic processes to calculate how to adjust significance levels when stopping a clinical trial early. In fact, the whole area of monitoring clinical trials as emerging evidence points to one hypothesis or another, is based on the theory of stochastic processes. So yes, this course is a win.&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2013-01-12T16:13:52.467" CreationDate="2013-01-12T16:13:52.467" Id="47575" LastActivityDate="2013-01-12T16:13:52.467" OwnerUserId="14188" ParentId="4165" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;Suppose in group 1 there are $n$ people and in group 2 there are $n$ people. If we conduct a 2 sample t test versus a paired t test, would we use $2n$ for power calculations in the two sample t test and $n$ as the sample size for the paired t test?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-01-12T17:31:15.603" Id="47580" LastActivityDate="2013-05-12T05:39:43.940" LastEditDate="2013-05-12T02:43:24.460" LastEditorUserId="183" OwnerUserId="18607" PostTypeId="1" Score="2" Tags="&lt;t-test&gt;&lt;power&gt;" Title="How does power analysis differ between paired sample and independent groups t-test?" ViewCount="465" />
  
  <row AcceptedAnswerId="47596" AnswerCount="3" Body="&lt;p&gt;I'm trying to do an OLS regression with several independent variables, and want to better understand how to interpret the p-values from doing the t-tests on the independent variables within my regression. For example, here is my result:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;                            OLS Regression Results                            &#10;==============================================================================&#10;Dep. Variable:                      y   R-squared:                       0.612&#10;Model:                            OLS   Adj. R-squared:                  0.497&#10;Method:                 Least Squares   F-statistic:                     5.353&#10;Date:                Fri, 11 Jan 2013   Prob (F-statistic):            0.00390&#10;Time:                        16:12:03   Log-Likelihood:                -239.61&#10;No. Observations:                  23   AIC:                             491.2&#10;Df Residuals:                      17   BIC:                             498.0&#10;Df Model:                           5                                         &#10;==============================================================================&#10;                 coef    std err          t      P&amp;gt;|t|      [95.0% Conf. Int.]&#10;------------------------------------------------------------------------------&#10;const       4.268e+05   1.85e+04     23.092      0.000      3.88e+05  4.66e+05&#10;x1           -70.4536   2230.755     -0.032      0.975     -4776.936  4636.028&#10;x2         -2.384e+04   1.25e+04     -1.905      0.074     -5.02e+04  2565.514&#10;x3         -3821.8439   3848.891     -0.993      0.335     -1.19e+04  4298.607&#10;x4          4030.8183   2295.228      1.756      0.097      -811.689  8873.325&#10;x5         -3.955e+04   1.73e+04     -2.282      0.036     -7.61e+04 -2977.451&#10;==============================================================================&#10;Omnibus:                        2.870   Durbin-Watson:                   1.674&#10;Prob(Omnibus):                  0.238   Jarque-Bera (JB):                1.326&#10;Skew:                          -0.227   Prob(JB):                        0.515&#10;Kurtosis:                       4.085   Cond. No.                         21.8&#10;==============================================================================&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;From what I understand, if the p-values are above a certain threshold for a given variable (e.g. p-value &gt; 0.05) as is the case with variable x1's pvalue=0.975, then one can say that this particular regression doesn't gain any additional information from having this variable in there.  If I'm misunderstanding or generalizing too much, let me know.&lt;/p&gt;&#10;&#10;&lt;p&gt;What else is confusing me is that same variable, x1, when I run a regression with just x1 and x5, x1's p-value=0.05.  I'm guessing that I interpret this as, x1 has some useful information, but when compared with the information carried by x2, x2 and x4 together, x1 isn't useful.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Regarding feature selection&lt;/strong&gt;, would it be correct to try all various subsets of x1 through x5, throw out those that contain an independent variable whose p-value &gt; 0.05, and &lt;em&gt;then&lt;/em&gt; use the remaining combinations with cross-validation to find the best model parameters?&lt;/p&gt;&#10;&#10;&lt;p&gt;My end goal is to do feature selection from a large set of variables, and maybe p-values are not the best thing to use for this. In either case, I would like to better understand these p-values, and if you have a favorite feature selection method, I'd love to hear as well. Thanks&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-01-12T21:36:27.380" FavoriteCount="1" Id="47594" LastActivityDate="2013-01-13T13:06:09.553" OwnerUserId="5646" PostTypeId="1" Score="2" Tags="&lt;t-test&gt;&lt;feature-selection&gt;&lt;p-value&gt;&lt;least-squares&gt;" Title="How should I interpret the p-values (i.e. t-tests) in regressions, and can I use them for feature selection?" ViewCount="5312" />
  <row Body="&lt;p&gt;It sounds like you don't have a dataset which matches your explanatory variables (close to coast, etc) to your dependant variable (whether or not the city will thrive).&lt;/p&gt;&#10;&#10;&lt;p&gt;This means you do not have a supervised learning problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;A simple solution to your problem is to have cities evolve by some basic rules.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example:&lt;/p&gt;&#10;&#10;&lt;p&gt;Lets say that you have the 3 variables health quality, distance to the coast and population.&lt;/p&gt;&#10;&#10;&lt;p&gt;The rules could be:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Distance to the coast increases health quality.&lt;/li&gt;&#10;&lt;li&gt;Health quality increases population growth.&lt;/li&gt;&#10;&lt;li&gt;Having a high population decreases health quality.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;With specific numbers, these rules provide a transition function.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then pick some initial values and iterate using a transition function. After a few thousand iterations, the values of population, money, trade, etc would let you determine whether or not a city is thriving.&lt;/p&gt;&#10;&#10;&lt;p&gt;This is basically setting up a bunch of differential equations and trying to find a steady state. Its kind of a complicated &lt;a href=&quot;http://en.wikipedia.org/wiki/Lotka%E2%80%93Volterra_equation&quot; rel=&quot;nofollow&quot;&gt;predator prey situation&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-01-12T22:55:57.833" Id="47601" LastActivityDate="2013-01-12T23:08:17.987" LastEditDate="2013-01-12T23:08:17.987" LastEditorUserId="18614" OwnerUserId="18614" ParentId="47583" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I assume you are using logistic neurons, and that you are training by gradient descent/back-propagation. &lt;/p&gt;&#10;&#10;&lt;p&gt;The logistic function is close to flat for large positive or negative inputs. The derivative at an input of $2$ is about $1/10$, but at $10$ the derivative is about $1/22000$ . This means that if the input of a logistic neuron is $10$ then, for a given training signal, the neuron will learn about $2200$ times slower that if the input was $2$. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you want the neuron to learn quickly, you either need to produce a huge training signal (such as with a cross-entropy loss function) or you want the derivative to be large. To make the derivative large, you set the initial weights so that you often get inputs in the range $[-4,4]$. &lt;/p&gt;&#10;&#10;&lt;p&gt;The initial weights you give might or might not work. It depends on how the inputs are normalized. If the inputs are normalized to have mean $0$ and standard deviation $1$, then a random sum of $d$ terms with weights uniform on $(\frac{-1}{\sqrt{d}},\frac{1}{\sqrt{d}})$ will have mean $0$ and variance $\frac{1}{3}$, independent of $d$. The probability that you get a sum outside of $[-4,4]$ is small. That means as you increase $d$, you are not causing the neurons to start out saturated so that they don't learn. &lt;/p&gt;&#10;&#10;&lt;p&gt;With inputs which are not normalized, those weights may not be effective at avoiding saturation. &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-01-12T23:35:49.287" Id="47604" LastActivityDate="2013-01-14T20:42:42.580" LastEditDate="2013-01-14T20:42:42.580" LastEditorUserId="16763" OwnerUserId="11981" ParentId="47590" PostTypeId="2" Score="7" />
  <row AcceptedAnswerId="47637" AnswerCount="1" Body="&lt;p&gt;I asked this question on &lt;a href=&quot;http://stackoverflow.com/&quot;&gt;http://stackoverflow.com/&lt;/a&gt;, but I couldn't get what I want. So, I am asking it here.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am running winbugs from R and I need to use some variables in R output. When I type &lt;code&gt;schools.sim\$mean\$theta[1]&lt;/code&gt; in R, I get 10.2. However, when I type &lt;code&gt;schools.sim\$2.5%\$theta[1]&lt;/code&gt;an error message come up. Any one what I am doing wrong or any other way to get the bayesian intervals?&lt;/p&gt;&#10;&#10;&lt;p&gt;here is an example:&lt;/p&gt;&#10;&#10;&lt;p&gt;This is the R code&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(R2WinBUGS)&#10; data(schools)&#10;J &amp;lt;- nrow(schools)&#10;y &amp;lt;- schools$estimate&#10;    sigma.y &amp;lt;- schools$sd&#10;data &amp;lt;- list (&quot;J&quot;, &quot;y&quot;, &quot;sigma.y&quot;)&#10;&#10;&#10;inits &amp;lt;- function(){&#10; list(theta = rnorm(J, 0, 100), mu.theta = rnorm(1, 0, 100),&#10; sigma.theta = runif(1, 0, 100))&#10; }&#10;&#10; schools.sim &amp;lt;- bugs(data, inits, model.file = &quot;D:/model.txt&quot;,&#10; parameters = c(&quot;theta&quot;, &quot;mu.theta&quot;, &quot;sigma.theta&quot;),&#10; n.chains = 3, n.iter = 1000,&#10; bugs.directory = &quot;D:/PROGRAMLAR/WinBUGS14/&quot;)&#10;&#10; schools.sim&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and this is the winbugs code which must be stored as model.txt in D.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; model {&#10; for (j in 1:J)&#10;  {&#10; y[j] ~ dnorm (theta[j], tau.y[j])&#10; theta[j] ~ dnorm (mu.theta, tau.theta)&#10; tau.y[j] &amp;lt;- pow(sigma.y[j], -2)&#10; }&#10; mu.theta ~ dnorm (0.0, 1.0E-6)&#10; tau.theta &amp;lt;- pow(sigma.theta, -2)&#10; sigma.theta ~ dunif (0, 1000)&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" ClosedDate="2013-01-13T16:58:24.317" CommentCount="2" CreationDate="2013-01-13T00:50:32.557" Id="47613" LastActivityDate="2013-01-13T14:35:15.907" LastEditDate="2013-01-13T11:24:09.123" LastEditorUserId="18549" OwnerUserId="18549" PostTypeId="1" Score="-1" Tags="&lt;r&gt;&lt;bugs&gt;" Title="Extracting values from R output of bugs command" ViewCount="157" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm reviewing for a test, and I am not sure if I am getting the right solution.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $X$ and $Y$ be iid $\mathcal{N}(0, \sigma^2)$ random variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;a. Find the distribution of $U = X^2 + Y^2$, $V = \frac{X}{\sqrt{X^2 + Y^2}}$, &lt;/p&gt;&#10;&#10;&lt;p&gt;b. are $U,V$ independent?&lt;/p&gt;&#10;&#10;&lt;p&gt;c. Suppose $\sin(\theta) = V$. Find distribution of $\theta$ when $0 \le \theta \le \pi/2$.&lt;/p&gt;&#10;&#10;&lt;p&gt;(tentative answers):&lt;/p&gt;&#10;&#10;&lt;p&gt;I get &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;$f_{U,V}(u,v) = \frac{1}{4\pi} \sigma^{-2} \exp \left[ -u/(2\sigma^2) \right]| (1-v^2)^{1/2} + (1-v^2)v^2|$, &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;yes (density factors and supports dont rely on each other)and &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;$g(\theta) = \left[\cos^2(\theta) + \cos^3(\theta)\sin^2(\theta)\right]\frac{1}{8 \pi \sigma^4}$. &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Anybody recognize any of these distributions?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-01-13T01:05:15.937" Id="47616" LastActivityDate="2013-01-13T01:05:15.937" OwnerUserId="8336" PostTypeId="1" Score="4" Tags="&lt;distributions&gt;&lt;normal-distribution&gt;&lt;data-transformation&gt;" Title="Transforming two normal random variables" ViewCount="93" />
  <row Body="&lt;p&gt;The probability of winning a 5 set match is the sum of the probabilities of the events&#10;$$\begin{align*}&#10; \text{win in}~ 3:&amp;amp;  WWW\\&#10;\text{win in}~4:&amp;amp; LWWW,WLWW,WWLW\\&#10;\text{win in}~5:&amp;amp; LLWWW, LWLWW,LWWLW, WLLWW, WLWLW, WWLLW&#10;\end{align*}$$&#10;If the probability of winning a set is $p$ and the probability of losing a set is&#10;$q = 1-p$, the probability of winning a 5 set match works out to be&#10;$$p^3 + 3p^3q + 6p^3q^2 = p^3(1+3q+6q^2)$$ which of course is $\frac{1}{2}$ when $p=\frac{1}{2}$ as Thierry Silbermann has already pointed out to you.  See also&#10;an article titled &quot;The Drunken Tennis Player&quot; in Ian Stewart's &lt;em&gt;Game, Set and Match&lt;/em&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-13T04:40:54.180" Id="47622" LastActivityDate="2013-01-13T04:40:54.180" OwnerUserId="6633" ParentId="47617" PostTypeId="2" Score="2" />
  <row AnswerCount="0" Body="&lt;p&gt;I analyzed my data by using  maximum likelihood estimation and a Bayesian approach. Now, I want to see which model has a better fit. How could I do this using either plots or numbers? Please be specific if possible.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2013-01-13T12:49:38.197" Id="47633" LastActivityDate="2013-01-13T13:07:21.503" LastEditDate="2013-01-13T13:07:21.503" LastEditorUserId="686" OwnerUserId="18549" PostTypeId="1" Score="0" Tags="&lt;bayesian&gt;&lt;maximum-likelihood&gt;&lt;model-comparison&gt;" Title="How to compare a frequentist model with a Bayesian one" ViewCount="170" />
  <row Body="&lt;p&gt;I get the following when I run your code on OpenBUGS. You can pick the interval out of the table.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; schools.sim$summary&#10;                     mean       sd       2.5%       25%       50%       75%    97.5%     Rhat n.eff&#10;    theta[1]    12.163174 7.894141 -1.2702000  7.489750 11.170000 16.422500 32.14200 1.039443    62&#10;    theta[2]     9.143817 6.461938 -4.0285250  5.101000  9.396500 13.250000 21.42525 1.015981   150&#10;    theta[3]     7.754204 7.665143 -9.3537750  3.583250  8.474500 12.585000 21.12100 1.015841   360&#10;    theta[4]     8.812925 6.602441 -4.5149500  4.506500  9.231000 13.270000 20.40100 1.027968   110&#10;    theta[5]     6.754827 6.859304 -8.1867000  2.302000  7.487500 11.380000 17.67000 1.017059   410&#10;    theta[6]     7.265947 7.232761 -8.6278000  2.745500  8.155000 11.787500 18.89025 1.022475   190&#10;    theta[7]    11.501118 6.360316 -0.2822100  7.494500 11.165000 15.692500 25.01250 1.054659    42&#10;    theta[8]     9.684910 7.605208 -4.7286250  5.092000  9.574000 14.362500 25.14250 1.019421   140&#10;    mu.theta     9.182670 5.203911 -1.1587500  5.822500  9.265000 12.512500 18.18775 1.029443    88&#10;    sigma.theta  5.929714 5.622338  0.2344007  1.685247  4.395495  8.472499 20.17768 1.065369    50&#10;    deviance    60.671273 2.242092 57.2300000 59.230000 60.075000 61.930000 65.59050 1.034880   120&#10;    &amp;gt; schools.sim$summary[23]&#10;[1] -1.2702&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Here is the code I ran.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(R2OpenBUGS)&#10;&#10;data(schools)&#10;J &amp;lt;- nrow(schools)&#10;y &amp;lt;- schools$estimate&#10;    sigma.y &amp;lt;- schools$sd&#10;data &amp;lt;- list (&quot;J&quot;, &quot;y&quot;, &quot;sigma.y&quot;)&#10;&#10;model &amp;lt;- function() {&#10;  for (j in 1:J)&#10;  {&#10;    y[j] ~ dnorm (theta[j], tau.y[j])&#10;    theta[j] ~ dnorm (mu.theta, tau.theta)&#10;    tau.y[j] &amp;lt;- pow(sigma.y[j], -2)&#10;  }&#10;  mu.theta ~ dnorm (0.0, 1.0E-6)&#10;  tau.theta &amp;lt;- pow(sigma.theta, -2)&#10;  sigma.theta ~ dunif (0, 1000)&#10;}&#10;write.model(model,&quot;model.txt&quot;)&#10;&#10;inits &amp;lt;- function(){&#10;  list(theta = rnorm(J, 0, 100), mu.theta = rnorm(1, 0, 100),&#10;       sigma.theta = runif(1, 0, 100))&#10;}&#10;&#10;schools.sim &amp;lt;- bugs(data, inits, model.file = &quot;model.txt&quot;,&#10;    parameters = c(&quot;theta&quot;, &quot;mu.theta&quot;, &quot;sigma.theta&quot;),&#10;    n.chains = 3, n.iter = 1000)&#10;&#10;schools.sim$summary&#10;&#10;schools.sim$summary[23]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2013-01-13T14:35:15.907" Id="47637" LastActivityDate="2013-01-13T14:35:15.907" OwnerUserId="17124" ParentId="47613" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;A summary from &lt;a href=&quot;http://metaoptimize.com/qa/questions/5205/when-to-use-l1-regularization-and-when-l2&quot; rel=&quot;nofollow&quot;&gt;this useful discussion at MetaOptimize&lt;/a&gt; regarding the general issue of L1 versus L2 regularization:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;L1 (e.g. Lasso):  choose for a sparse model / feature selection as Shea Parkes mentions above, especially when n &gt;&gt; m&lt;/li&gt;&#10;&lt;li&gt;L2 (e.g. SVM):  choose when seeking rotational invariance and there are plenty of samples&lt;/li&gt;&#10;&lt;li&gt;L1+L2 (e.g. elastic-net):  if you want to combine both  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2013-01-13T14:57:44.020" Id="47638" LastActivityDate="2013-01-13T14:57:44.020" OwnerUserId="1344" ParentId="27120" PostTypeId="2" Score="0" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Assume I want to generate a sample of n-dimensional vector $z$ with a given correlation matrix $Corr(z)=R$. Some of the margins of its distribution are not normal (&lt;a href=&quot;http://quant.stackexchange.com/questions/5964/generate-correlated-random-variables-from-normal-and-gamma-distributions&quot;&gt;if you're interested here's the motivation behind my question&lt;/a&gt;), i.e. $z$ doesn't follow the multivariate normal distribution.&lt;br&gt;&#10;Let's assume that I have some imperfect method and I want to check if the sample correlation matrix is close to the theoretical one, i.e. I'd like to test the null hypothesis $H_0:R_{sample}=R$ vs $H_1:R_{sample}\neq R$&lt;br&gt;&#10;How can I do it?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-01-13T20:17:30.137" Id="47654" LastActivityDate="2013-01-13T20:17:30.137" OwnerUserId="18637" PostTypeId="1" Score="2" Tags="&lt;hypothesis-testing&gt;&lt;correlation&gt;&lt;multivariate-analysis&gt;" Title="test for correlation of a multivariate non-normal distribution" ViewCount="242" />
  <row Body="&lt;p&gt;Short answer: yes.&lt;/p&gt;&#10;&#10;&lt;p&gt;If I understand your set-up, your observations $y_t$ are binary: $y_t \in \{0,1\}$ for all $t$. Your predictions $\hat{y}_t$ take values on the unit interval: $\hat{y}_t \in [0,1]$ for all $t$. Errors are given by the difference between the true value and the predicted value, as usual: $e_t = \hat{y}_t - y_t$.  &lt;/p&gt;&#10;&#10;&lt;p&gt;The specific prediction or estimation procedure you are using does not matter for this definition.&lt;/p&gt;&#10;&#10;&lt;p&gt;A related issue -- perhaps one that motivates your question? -- concerns how you should evaluate the quality of your prediction system. There are a number of different &lt;a href=&quot;http://en.wikipedia.org/wiki/Scoring_rule&quot; rel=&quot;nofollow&quot;&gt;scoring rules&lt;/a&gt; that one could use to score a forecasting system that generates probabilistic forecasts of a dichotomous variable. A standard one is the &lt;a href=&quot;http://en.wikipedia.org/wiki/Brier_score&quot; rel=&quot;nofollow&quot;&gt;Brier Score&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-01-14T01:21:27.427" Id="47663" LastActivityDate="2013-01-14T01:21:27.427" OwnerUserId="17286" ParentId="47659" PostTypeId="2" Score="2" />
  
  
  
  <row Body="&lt;p&gt;The &lt;strong&gt;correlation&lt;/strong&gt; is a bivariate statistic that would involve only x and y and would not have an intercept. None of what you've given is a correlation.&lt;/p&gt;&#10;&#10;&lt;p&gt;But, given that there is an interaction with other variables, the correlation, while mathematically correct, will be misleading. &lt;/p&gt;&#10;&#10;&lt;p&gt;In one of your replies you write:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;For example, is it correct to say &quot;among all categories c, the&#10;  average correlation of x with y is b_1&quot;, and if so, which b_1 should I&#10;  use here? The one in model 1, 2, or 3?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;If you want to make a statement of this type, you can get a weighted average of the correlations (not the regression parameters) in the different categories of c. But I don't think this is a good approach.&lt;/p&gt;&#10;&#10;&lt;p&gt;Given this goal&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;What I'm interested in is a (justified) general statement about the&#10;  correlation I can make to someone who knows very little statistics.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;What I would do is present a graph of the variables x and y, with the dots on the scatterplot colored differently for each level of c, and with regression lines (or possibly loess lines) superimposed on top.&lt;/p&gt;&#10;&#10;&lt;p&gt;This wouldn't be a single simple statement, but sometimes there is no good single simple statement. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-14T11:49:15.670" Id="47673" LastActivityDate="2013-01-14T11:49:15.670" OwnerUserId="686" ParentId="47646" PostTypeId="2" Score="0" />
  
  <row AcceptedAnswerId="47686" AnswerCount="1" Body="&lt;p&gt;I've read in the news that the last month five pedestrians died from a population of 500,000&#10;Remembering that it is a poisson problem (the famous prussian horse kicks by Ladislaus Bortewicz). I fired up R to understand the probability&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ppois(5*12, 3, lower.tail=F)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;since in a month died 5, I multiplied it by 12 months and compared it to the official statistics of 3 pedestrians a year in this area. The result:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;[1] 1.310782e-56&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So, yes this is an unusual case.&#10;Am I doing everything right?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-01-14T14:33:49.143" FavoriteCount="1" Id="47683" LastActivityDate="2013-01-14T14:51:51.437" LastEditDate="2013-01-14T14:51:51.437" LastEditorUserId="930" OwnerUserId="778" PostTypeId="1" Score="5" Tags="&lt;poisson&gt;" Title="Calculating the probability of a rare event" ViewCount="300" />
  <row Body="&lt;p&gt;You may also be interested in the Nyström Method. There are a vast number of papers demonstrating different uses: &lt;a href=&quot;http://www.cs.berkeley.edu/~fowlkes/tutorial/fbcm_nystrom_tpami_04.pdf&quot; rel=&quot;nofollow&quot;&gt;clustering&lt;/a&gt;, &lt;a href=&quot;http://www.quinonero.net/Publications/quinonero-candela05a.pdf&quot; rel=&quot;nofollow&quot;&gt;Kernel Methods&lt;/a&gt; like SVM and &lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume6/drineas05a/drineas05a.pdf&quot; rel=&quot;nofollow&quot;&gt;also&lt;/a&gt; Gaussian Processes. I personally am not familiar with any concrete implementation, but if you will find many of it for different purposes in the Internet&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-01-14T14:57:09.913" Id="47688" LastActivityDate="2013-01-14T19:14:20.610" LastEditDate="2013-01-14T19:14:20.610" LastEditorUserId="17908" OwnerUserId="17908" ParentId="45314" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;Let $ \{ X_i \}_{i=1}^{T}$ be a path of the markov chain and let $P_{\theta}(X_1, ..., X_T)$ be the probability of observing the path when $\theta$ is the true parameter value (a.k.a. the likelihood function for $\theta$). Using the definition of conditional probability, we know &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ P_{\theta}(X_1, ..., X_T)  = P_{\theta}(X_T | X_{T-1}, ..., X_1) \cdot P_{\theta}(X_1, ..., X_{T-1})$$ &lt;/p&gt;&#10;&#10;&lt;p&gt;Since this is a markov chain, we know that $P_{\theta}(X_T | X_{T-1}, ..., X_1) = P_{\theta}(X_T | X_{T-1} )$, so this simplifies this to &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ P_{\theta}(X_1, ..., X_T)  = P_{\theta}(X_T | X_{T-1}) \cdot P_{\theta}(X_1, ..., X_{T-1})$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Now if you repeat this same logic $T$ times, you get &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ P_{\theta}(X_1, ..., X_T)  = \prod_{i=1}^{T} P_{\theta}(X_i | X_{i-1} ) $$ &lt;/p&gt;&#10;&#10;&lt;p&gt;where $X_0$ is to be interpreted as the initial state of the process. The terms on the right hand side are just elements of the transition matrix. Since it was the log-likelihood you requested, the final answer is: &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ {\bf L}(\theta) = \sum_{i=1}^{T} \log \Big( P_{\theta}(X_i | X_{i-1} ) \Big) $$ &lt;/p&gt;&#10;&#10;&lt;p&gt;This is the likelihood of a single markov chain - if your data set includes several (independent) markov chains then the full likelihood will be a sum of terms of this form. &lt;/p&gt;&#10;" CommentCount="14" CreationDate="2013-01-14T15:22:06.947" Id="47691" LastActivityDate="2013-01-14T15:36:53.617" LastEditDate="2013-01-14T15:36:53.617" LastEditorUserId="4856" OwnerUserId="4856" ParentId="47685" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;&lt;em&gt;While searching through unanswered questions I noticed this one again and decided, in agreement with whuber, that keeping essentially answered questions off of the unanswered tab is higher priority than my own personal preferences about what is &quot;worthy&quot; of answer vs. comment status, so I pasted my comment as an answer.&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;They are different because ${\bf K}_{X} + {\bf K}_Y$ is the sum of two covariance matrices while ${\bf K}_{X+Y}$ is the covariance matrix of the random variable $X+Y$. To see why the two matrices are different, use &lt;a href=&quot;http://en.wikipedia.org/wiki/Covariance#Properties&quot; rel=&quot;nofollow&quot;&gt;the bilinearity of covariance&lt;/a&gt; to see that&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ [{\bf K}_{X+Y}]_{ij}=[{\bf K}_{X}]_{ij} +[{\bf K}_{Y}]_{ij}+ {\rm cov}(X_i,Y_j)+{\rm cov}(X_j,Y_i)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;i.e. the cross-covariances are missing from ${\bf K}_{X} + {\bf K}_Y$ (note I assume $X,Y$ are of equal dimension to ensure that question makes sense). So, ${\bf K}_{X+Y}$ is the covariance matrix of $X+Y$ and ${\bf K}_{X} + {\bf K}_Y$ represents the special case where ${\rm cov}(X_i,Y_j)=-{\rm cov}(X_j,Y_i)$ for each pair $(i,j)$, the most notable example being when every element of $X$ is uncorrelated with every element of $Y$. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-01-14T16:15:35.943" Id="47695" LastActivityDate="2013-01-14T18:52:14.697" LastEditDate="2013-01-14T18:52:14.697" LastEditorUserId="4856" OwnerUserId="4856" ParentId="46832" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;The Nelder-Mead simplex algorithm seems to work well.. It is implemented in Java by the Apache Commons Math library at &lt;a href=&quot;https://commons.apache.org/math/&quot; rel=&quot;nofollow&quot;&gt;https://commons.apache.org/math/&lt;/a&gt; . I've also written a paper about the Hawkes processes at &lt;a href=&quot;http://vixra.org/abs/1211.0094&quot; rel=&quot;nofollow&quot;&gt;Point Process Models for Multivariate High-Frequency Irregularly Spaced Data&lt;/a&gt; . On an unrelated note, do you know if the elegant recursions available for the exponential kernel are possible with other kernels, or is this a property exclusively pertaining to the exponential kernel?&lt;/p&gt;&#10;&#10;&lt;p&gt;felix, using exp/log transforms seems to ensure positivity of the parameters.  As for the small alpha  thing,  search the arxiv.org for a paper called &quot;limit theorems for nearly unstable hawkes processes&quot;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-01-14T17:51:50.753" Id="47701" LastActivityDate="2015-01-17T03:31:01.587" LastEditDate="2015-01-17T03:31:01.587" LastEditorUserId="19663" OwnerUserId="19663" ParentId="24685" PostTypeId="2" Score="4" />
  
  <row AcceptedAnswerId="47725" AnswerCount="1" Body="&lt;p&gt;I'm very new to this--I took a class last quarter and got hooked--now I'm in this competition and I don't know where to start (kind of). My problem is that I got this data that looks like a bunch of relational database tables and I don't know how to shape them into something I can try to feed into some kind of model. For example, I have users, users that are friends with other users, events, and users who are attending events. I know how to merge users and events together, convert it to floating point or integer values, and throw it into the model. Its the friends part that's has me confused. How can I bring the friends aspect into the equation?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-15T01:10:57.437" Id="47721" LastActivityDate="2013-01-15T01:58:50.680" OwnerUserId="19672" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;" Title="How to shape data into something useful for machine learning algorithms" ViewCount="80" />
  <row Body="&lt;p&gt;In your comment you make clear that you are comparing the weighted instantaneous hazard ratio ( i.e., exp(beta)) with some summary measure of the ratios of cumulative hazards ( -log(frac.surv pop2)*time ). I think you will find that they are roughly the same during the early early phases of follow-up but they diverge as time increases. The reason for this is that as survival approaches zero the ratio of cumulative survival necessarily goes to unity, where as the hazard ratio may not.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-15T02:10:40.500" Id="47726" LastActivityDate="2013-01-15T02:10:40.500" OwnerUserId="2129" ParentId="26408" PostTypeId="2" Score="0" />
  <row AnswerCount="8" Body="&lt;p&gt;I learned that a statistic is an attribute you can obtain from samples.Taking many samples of same size, calculating this attribute for all of them and plotting the pdf, we get the distribution of the corresponding attribute or the distribution of the corresponding statistics.&lt;/p&gt;&#10;&#10;&lt;p&gt;I also heard that statistics are made to be estimators, how do these two concepts differ?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-01-15T02:30:09.810" FavoriteCount="2" Id="47728" LastActivityDate="2015-01-24T22:30:32.523" LastEditDate="2013-01-15T02:54:28.863" LastEditorUserId="3826" OwnerUserId="19675" PostTypeId="1" Score="12" Tags="&lt;estimators&gt;" Title="What is the difference between an estimator and a statistic?" ViewCount="3988" />
  <row Body="&lt;p&gt;&quot;6&quot; is an example of an estimator.  Say your question was, &quot;what is the slope of the best linear function mapping x to y?&quot;  Your answer could be &quot;6&quot;.  Or it could be $(X'X)^{-1}X'Y$.  Both are estimators.  Which one is better is left to you to decide.  &lt;/p&gt;&#10;&#10;&lt;p&gt;A really good TA once explained the concept of an estimator to me that way.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Basically, an estimator is a thing that you apply to data to get a quantity that you don't know the value of.  You know the value of a statistic -- it is a function of the data with no &quot;best&quot; or &quot;optimal&quot; about it.  There is no &quot;best&quot; mean.  There is just a mean.&lt;/p&gt;&#10;&#10;&lt;p&gt;Say you have a dataset on number of goats owned per person, and each person's happiness.  You're interested in how people's happiness changes with the number of goats they own.  An estimator can help you to estimate that relationship from your data.  Statistics are just functions of the data that you have.  For example, the variance of goat ownership may equal 7.  Te forula for calculating variance would be identical between goats and toasters, or whether you're interested in happiness or propensity to get cancer.  In that sense, all sensible estimators are statistics.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-15T03:22:13.183" Id="47732" LastActivityDate="2013-01-15T03:22:13.183" OwnerUserId="17359" ParentId="47728" PostTypeId="2" Score="1" />
  
  <row AnswerCount="1" Body="&lt;p&gt;If I difference a time series and take out trend and seasonality ... does it mean we are left with only irregularity on which we plot the acf and pacf to arrive at the MA and AR order?&lt;/p&gt;&#10;&#10;&lt;p&gt;Do 1st difference, 2nd difference always detrend the series, or do we need to detrend separately?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-01-15T08:17:50.350" Id="47741" LastActivityDate="2013-06-23T18:21:05.957" LastEditDate="2013-06-23T18:21:05.957" LastEditorUserId="22047" OwnerUserId="19677" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;stationarity&gt;" Title="Time-series stationarity" ViewCount="231" />
  <row Body="&lt;p&gt;There are a number of ways to assess the correlation between two binomial variables.  The most common in my experience is the &lt;a href=&quot;http://en.wikipedia.org/wiki/Phi_coefficient&quot; rel=&quot;nofollow&quot;&gt;Phi coefficient&lt;/a&gt;.  Notably, this coefficient for a 2x2 table has the same value as a regular correlation coefficient (Pearson's product moment) and bears a direct relationship with the $\chi^2$ test mentioned by gung.&lt;/p&gt;&#10;&#10;&lt;p&gt;With a 2x2 table of counts, you can do a &lt;a href=&quot;http://en.wikipedia.org/wiki/Fisher%27s_exact_test&quot; rel=&quot;nofollow&quot;&gt;Fisher's exact test&lt;/a&gt; instead of a $\chi^2$ without the requirement that you have all cells with N &gt;= 5.  I haven't vetted it, but a quick Google search shows &lt;a href=&quot;http://graphpad.com/quickcalcs/contingency1.cfm&quot; rel=&quot;nofollow&quot;&gt;this calculator&lt;/a&gt; available online.&lt;/p&gt;&#10;&#10;&lt;p&gt;All of this being said... given that you have a cell in your design where N &amp;lt; 5, I'd recommend caution in interpreting your correlation coefficient.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-15T08:47:28.113" Id="47743" LastActivityDate="2013-01-15T08:47:28.113" OwnerUserId="196" ParentId="40514" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;It is true that the median is more robust (subject to outliers) than the mean.  &lt;/p&gt;&#10;&#10;&lt;p&gt;My understanding is that the reason statistics tends to use the mean (and squared errors for that matter) is that in the long run,  on average, assuming symmetrical distributions, they get closer to the true answer than medians and absolute deviations.  &lt;/p&gt;&#10;&#10;&lt;p&gt;However, if you are not interested in being correct on average over the long run and are more interested in being close to right on any given point estimate, the median is probably a reasonable statistic for you to select.  &lt;/p&gt;&#10;&#10;&lt;p&gt;It is unclear from your question what exactly you want to do with your &quot;error estimate&quot;.  Do you want to use it to do a statistical test?  If all you need is another summary statistic to describe the central tendency of dispersion around your observed measure of central tendency AND you want to continue to leverage whatever 'advantage' the median is giving you... then I would recommend calculating the median absolute deviation.  That is the median of |X-Mdn|.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-15T09:08:39.833" Id="47745" LastActivityDate="2013-01-15T09:08:39.833" OwnerUserId="196" ParentId="47738" PostTypeId="2" Score="0" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am performing a non-inferiority test on two binomial trials. I want to determine if my test proportion, pT, is not worse than my control, pC, by more than an amount d. Assuming nT trials of my test and nC trials of my control, the standard statistic for such a test is given by (&lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2701110/&quot; rel=&quot;nofollow&quot;&gt;da Silva 2009&lt;/a&gt;)&#10;$$&#10;z = \frac{pT - pC + d}{\sqrt{\frac{pT(1-pT)}{nT} + \frac{pC(1-pC)}{nC}}},&#10;$$&#10;which is distributed according to a standard normal distribution. However, this equation assumes that the binomial distributions generated by (pT, nT) and (pC, nC) are approximately Gaussian. My test and control proportions are both very close to one so this approximation doesn't hold. Is there a way of performing such a test exactly using the actual binomial distributions?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-15T10:46:15.093" FavoriteCount="0" Id="47748" LastActivityDate="2013-01-15T10:46:15.093" OwnerUserId="10769" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;&lt;binomial&gt;" Title="Non-inferiority testing without normal approximation" ViewCount="77" />
  <row Body="&lt;p&gt;An option to reduce the number of presented profiles is to use 2k &lt;a href=&quot;http://www.cs.gmu.edu/~setia/cs700-F09/slides/lecture11.pdf&quot; rel=&quot;nofollow&quot;&gt;Factorial Design&lt;/a&gt; to reduce the number of comparisons. In R you can use &lt;a href=&quot;http://cran.r-project.org/web/packages/conf.design/conf.design.pdf&quot; rel=&quot;nofollow&quot;&gt;conf.design&lt;/a&gt; library (there are many other 'agricolae', 'AlgDeign'). This is a basic approach, but if you want to go deeper in the topic of the adaptivity ( from ACA ) you can see &lt;a href=&quot;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=374460&quot; rel=&quot;nofollow&quot;&gt;Fast Polyhedral Conjoint&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;You said: &quot;However, in order to do ACA, as far as I know of at this time, one must have the survey hosted by a very expensive (e.g. $10,000) conjoint-oriented survey host.&quot; So&#10;you can design design your conjoint analysis (using R ) and apply versatile questionnaires with open source tools (e.g &lt;a href=&quot;http://www.limesurvey.org/&quot; rel=&quot;nofollow&quot;&gt;Limesurvey.com&lt;/a&gt;) &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-15T11:04:21.113" Id="47751" LastActivityDate="2013-01-15T11:04:21.113" OwnerUserId="17464" ParentId="34044" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Assuming you have both types of measurement on each individual then the sample of differences contains all the answers.  You can calculate the mean square error, decompose it into bias (the mean of the differences) and variance, &amp;amp; also look for outliers, differences between subgroups, &amp;amp; so on.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-15T12:22:37.707" Id="47757" LastActivityDate="2013-01-15T12:22:37.707" OwnerUserId="17230" ParentId="47755" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;A &lt;a href=&quot;http://en.wikipedia.org/wiki/Beta_distribution&quot;&gt;Beta distribution&lt;/a&gt; is used to model things that have a limited range, like 0 to 1.&lt;/p&gt;&#10;&#10;&lt;p&gt;Examples are the probability of success in an experiment having only two outcomes, like success and failure. If you do a limited number of experiments, and some are successful, you can represent what that tells you by a beta distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;Another example is &lt;a href=&quot;http://en.wikipedia.org/wiki/Order_statistic&quot;&gt;order statistics&lt;/a&gt;. For example, if you generate several (say 4) uniform 0,1 random numbers, and sort them, what is the distribution of the 3rd one?&lt;/p&gt;&#10;&#10;&lt;p&gt;I use them to understand software performance diagnosis by sampling. If you stop a program at random $n$ times, and $s$ of those times you see it doing something you could actually get rid of, and $s&amp;gt;1$, then the fraction of time to be saved by doing so is represented by $Beta(s+1, (n-s)+1)$, and the speedup factor has a &lt;a href=&quot;http://en.wikipedia.org/wiki/Beta_prime_distribution&quot;&gt;BetaPrime&lt;/a&gt; distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://scicomp.stackexchange.com/a/2719/1262&quot;&gt;&lt;em&gt;More about that...&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-01-15T16:03:27.240" Id="47776" LastActivityDate="2013-01-15T16:28:09.647" LastEditDate="2013-01-15T16:28:09.647" LastEditorUserId="1270" OwnerUserId="1270" ParentId="47771" PostTypeId="2" Score="18" />
  <row Body="&lt;p&gt;The short version is that the Beta distribution can be understood as representing a distribution &lt;em&gt;of probabilities&lt;/em&gt;- that is, it represents all the possible values of a probability when we don't know what that probability is. Here is my favorite intuitive explanation of this:&lt;/p&gt;&#10;&#10;&lt;p&gt;Anyone who follows baseball is familiar with &lt;a href=&quot;http://en.wikipedia.org/wiki/Batting_average#Major_League_Baseball&quot;&gt;batting averages&lt;/a&gt;- simply the number of times a player gets a base hit divided by the number of times he goes up at bat (so it's just a percentage between &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt;). &lt;code&gt;.266&lt;/code&gt; is in general considered an average batting average, while &lt;code&gt;.300&lt;/code&gt; is considered an excellent one.&lt;/p&gt;&#10;&#10;&lt;p&gt;Imagine we have a baseball player, and we want to predict what his season-long batting average will be. You might say we can just use his batting average so far- but this will be a very poor measure at the start of a season! If a player goes up to bat once and gets a single, his batting average is briefly &lt;code&gt;1.000&lt;/code&gt;, while if he strikes out or walks, his batting average is &lt;code&gt;0.000&lt;/code&gt;. It doesn't get much better if you go up to bat five or six times- you could get a lucky streak and get an average of &lt;code&gt;1.000&lt;/code&gt;, or an unlucky streak and get an average of &lt;code&gt;0&lt;/code&gt;, neither of which are a remotely good predictor of how you will bat that season.&lt;/p&gt;&#10;&#10;&lt;p&gt;Why is your batting average in the first few hits not a good predictor of your eventual batting average? When a player's first at-bat is a strikeout, why does no one predict that he'll never get a hit all season? Because we're going in with &lt;em&gt;prior expectations.&lt;/em&gt; We know that in history, most batting averages over a season have hovered between something like &lt;code&gt;.215&lt;/code&gt; and &lt;code&gt;.360&lt;/code&gt;, with some extremely rare exceptions on either side. We know that if a player gets a few strikeouts in a row at the start, that might indicate he'll end up a bit worse than average, but we know he probably won't deviate from that range.&lt;/p&gt;&#10;&#10;&lt;p&gt;Given our batting average problem, which can be represented with a &lt;a href=&quot;http://en.wikipedia.org/wiki/Binomial_distribution&quot;&gt;binomial distribution&lt;/a&gt; (a series of successes and failures), the best way to represent these prior expectations (what we in statistics just call a &lt;a href=&quot;http://en.wikipedia.org/wiki/Prior_probability&quot;&gt;prior&lt;/a&gt;) is with the Beta distribution- it's saying, before we've seen the player take his first swing, what we roughly expect his batting average to be. The domain of the Beta distribution is &lt;code&gt;(0, 1)&lt;/code&gt;, just like a probability, so we already know we're on the right track- but the appropriateness of the Beta for this task goes far beyond that.&lt;/p&gt;&#10;&#10;&lt;p&gt;We expect that the player's season-long batting average will be most likely around &lt;code&gt;.27&lt;/code&gt;, but that it could reasonably range from &lt;code&gt;.21&lt;/code&gt; to &lt;code&gt;.35&lt;/code&gt;. This can be represented with a Beta distribution with parameters $\alpha=81$ and $\beta=219$:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;curve(dbeta(x, 81, 219))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/RJDrz.png&quot; alt=&quot;Beta(81, 219)&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I came up with these parameters for two reasons:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;The mean is $\frac{\alpha}{\alpha+\beta}=\frac{81}{81+219}=.270$&lt;/li&gt;&#10;&lt;li&gt;As you can see in the plot, this distribution lies almost entirely within &lt;code&gt;(.2, .35)&lt;/code&gt;- the reasonable range for a batting average.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;You asked what the x axis represents in a beta distribution density plot- here it represents his batting average. Thus notice that in this case, not only is the y-axis a probability (or more precisely a probability density), but the x-axis is as well (batting average is just a probability of a hit, after all)! The Beta distribution is representing a probability distribution &lt;em&gt;of probabilities&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;But here's why the Beta distribution is so appropriate. Imagine the player gets a single hit. His record for the season is now &lt;code&gt;1 hit; 1 at bat&lt;/code&gt;. We have to then &lt;em&gt;update&lt;/em&gt; our probabilities- we want to shift this entire curve over just a bit to reflect our new information. While the math for proving this is a bit involved (&lt;a href=&quot;http://en.wikipedia.org/wiki/Conjugate_prior#Example&quot;&gt;it's shown here&lt;/a&gt;), the result is &lt;em&gt;very simple&lt;/em&gt;. The new Beta distribution will be:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\mbox{Beta}(\alpha_0+\mbox{hits}, \beta_0+\mbox{misses})$&lt;/p&gt;&#10;&#10;&lt;p&gt;Where $\alpha_0$ and $\beta_0$ are the parameters we started with- that is, 81 and 219. Thus, in this case, $\alpha$  has increased by 1 (his one hit), while $\beta$ has not increased at all (no misses yet). That means our new distribution is $\mbox{Beta}(81+1, 219)$, or:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;curve(dbeta(x, 82, 219))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/aULXN.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Notice that it has barely changed at all- the change is indeed invisible to the naked eye! (That's because one hit doesn't really mean anything).&lt;/p&gt;&#10;&#10;&lt;p&gt;However, the more the player hits over the course of the season, the more the curve will shift to accommodate the new evidence, and furthermore the more it will narrow based on the fact that we have more proof. Let's say halfway through the season he has been up to bat 300 times, hitting 100 out of those times. The new distribution would be $\mbox{Beta}(81+100, 219+200)$, or:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;curve(dbeta(x, 82+100, 219+200))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/oBgYH.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Notice the curve is now both thinner and shifted to the right (higher batting average) than it used to be- we have a better sense of what the player's batting average is.&lt;/p&gt;&#10;&#10;&lt;p&gt;One of the most interesting outputs of this formula is the expected value of the resulting Beta distribution, which is basically your new estimate. Recall that the expected value of the Beta distribution is $\frac{\alpha}{\alpha+\beta}$. Thus, after 100 hits of 300 &lt;em&gt;real&lt;/em&gt; at-bats, the expected value of the new Beta distribution is $\frac{82+100}{82+100+219+200}=.303$- notice that it is lower than the naive estimate of $\frac{100}{100+200}=.333$, but higher than the estimate you started the season with ($\frac{81}{81+219}=.270$). You might notice that this formula is equivalent to adding a &quot;head start&quot; to the number of hits and non-hits of a player- you're saying &quot;start him off in the season with 81 hits and 219 non hits on his record&quot;).&lt;/p&gt;&#10;&#10;&lt;p&gt;Thus, the Beta distribution is best for representing a probabilistic distribution &lt;em&gt;of probabilities&lt;/em&gt;- the case where we don't know what a probability is in advance, but we have some reasonable guesses.&lt;/p&gt;&#10;" CommentCount="15" CreationDate="2013-01-15T16:41:13.993" Id="47782" LastActivityDate="2014-11-15T04:53:47.990" LastEditDate="2014-11-15T04:53:47.990" LastEditorUserId="8373" OwnerUserId="8373" ParentId="47771" PostTypeId="2" Score="145" />
  <row AnswerCount="1" Body="&lt;p&gt;I am currently working with a data set that contains about 26 IVs of almost all sorts of scale of measurement (binary, nominal, ordinal and interval scale variables). There are strong reasons to suspect that some variables are probably highly correlated, while some may not be related to any other IVs to a great extent. &lt;/p&gt;&#10;&#10;&lt;p&gt;I came across with great suggestions to resolve this problem in this site (which was an useful advice to use optimally scaled variables in the FA procedure and to use derived factor score as the IVs). But due to my inexperience in this field I am in need of some expert advice on the following issues:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;How should I check if Multicollinearity really exists?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I am not sure how to check Multicollinearity with such a heterogeneous data. I may calculate the Heterogeneous Correlation Matrix (or Spearman's Rank Correlation) by somehow forcing me to consider the nominal variables as ordinal but even if I do it what should be the value of the correlation coefficient at which Multicollinearity can be ignored? I am also not sure if it is going to give any insight at all, as I am missing something like a VIF measure!&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Should I take only those variables for a FA which are highly correlated?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Say, if I can find two sets of variables (one set containing 8 IVs and another containing 4 IVs) quite highly correlated to each other within each set, then should I use only those 12 variables for FA and derive FA scores for those two factors to use them as IVs? Clearly my intention is to use the other 14 variables separately as IVs along with the two derived scores. I am confused if I should actually use not the 12, but all 26 variables in the FA in this scenario. Remember in that case my FA scores are weighted by the other 14 unrelated variables too!&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Is there any problem to categorize a proportion type DV for an ordinal logistic regression?&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;I've actually found people using logistic regression instead. But I want to mention here that unfortunately I don't know the number of cases (or trials) out of which each proportion was calculated. So I cannot use the number of trials as the weights in the logistic regression. In that case a logistic regression may not be accurate enough. So, as I only know the proportions, won't it be good to categorize the proportions by median split or by quartile split? So that I can use it as a DV in a logistic or in an ordinal logistic regression? &lt;/p&gt;&#10;&#10;&lt;p&gt;I am thankful for reading this thread patiently and hoping some expert advice. &lt;/p&gt;&#10;&#10;&lt;p&gt;Regards. &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-01-15T18:52:19.983" FavoriteCount="3" Id="47799" LastActivityDate="2013-07-17T03:15:09.977" LastEditDate="2013-01-15T22:26:51.810" LastEditorUserId="88" OwnerUserId="12603" PostTypeId="1" Score="3" Tags="&lt;regression&gt;&lt;logistic&gt;&lt;factor-analysis&gt;&lt;multicollinearity&gt;" Title="Confusion related to multicollinearity, FA and regression of heterogeneous data" ViewCount="405" />
  <row Body="&lt;p&gt;Let's draw pictures in which &lt;em&gt;regions&lt;/em&gt; depict &lt;em&gt;events&lt;/em&gt; (such as &quot;the first light is red&quot;) and their &lt;em&gt;areas&lt;/em&gt; are proportional to the probabilities of those events.  Taking care to show areas accurately extends the &lt;a href=&quot;http://en.wikipedia.org/wiki/Venn_diagram&quot; rel=&quot;nofollow&quot;&gt;Venn diagram&lt;/a&gt; metaphor in a useful quantitative way.&lt;/p&gt;&#10;&#10;&lt;p&gt;For the traffic light problem, I will divide a unit square (representing the total probability) into four parts.  The left-right division will reflect the possibilities for the first light (set to red at the left, non-red at the right) and the top-bottom division will reflect the possibilities for the second light (red at the bottom, non-red at the top).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/g9awH.png&quot; alt=&quot;Figures&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;In the left figure&lt;/strong&gt;, the divisions have been made in a 40-60 ratio and a 30-70 ratio, respectively.  Where the red rectangle (of width 40%) and blue rectangle (of height 30%) intersect they form a purple rectangle of area 30% * 40% = 0.3 * 0.4 = 12% of the total area.  &lt;em&gt;Independent events can always be drawn in this way as separate overlapping rectangles.&lt;/em&gt;  (When you think about what this means--overlapping rectangles are a geometric way to multiply quantities--it becomes clear that this is the very definition of independence.)&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;The right figure&lt;/strong&gt; shows the actual information in the problem, which tells us the purple rectangle has an area of only 10% and asks us to find the area &lt;em&gt;not&lt;/em&gt; covered by &lt;em&gt;either&lt;/em&gt; rectangle: the white portion to the upper right, depicting the event &quot;Light 1 is not red and light 2 is not red.&quot;  This indicates &lt;em&gt;lack&lt;/em&gt; of independence: now it takes more than just two rectangles to carve up the square correctly.  (There's more than one way to do this.  For instance, I could have left the blue rectangle alone and adjusted the two halves of the red rectangle, making the bottom skinnier and--to keep its total area at 40%--the top fatter.  Either way works.)&lt;/p&gt;&#10;&#10;&lt;h3&gt;Solution&lt;/h3&gt;&#10;&#10;&lt;p&gt;Starting with the 10% purple rectangle, notice that the rest of the blue rectangle (at the right) has to include the remaining 20% = 30% - 10% of the time the second light is red.  Similarly, the rest of the vertical red rectangle has to include the remaining 30% = 40% - 10% of the time the first light is red.  This gives three rectangles of known area: 10%, 20%, and 30%.  They sum to 60%.  Consequently, because the sum of all areas must be 100%, &lt;strong&gt;the white area is 100% - 60% = 40%&lt;/strong&gt;.  This represents the probability to be found.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;h3&gt;Comments&lt;/h3&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;The white region in the first (left) figure is that of a rectangle with base 60% = 100% - 40% and height 70% = 100% - 30%, whence its area is 0.6 * 0.7 = 42% (and not 40%).&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;This area = probability method extends to more than two criteria: at &lt;a href=&quot;http://stats.stackexchange.com/questions/30842/what-is-the-probability-that-this-person-is-female/30850#30850&quot;&gt;What is the probability that this person is female?&lt;/a&gt; it is used to analyze a problem with three separate criteria.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Any two-by-two &lt;a href=&quot;http://en.wikipedia.org/wiki/Contingency_table&quot; rel=&quot;nofollow&quot;&gt;contingency table&lt;/a&gt; can (and usually should) be visualized this way, after translating its counts into frequencies relative to the total.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="2" CreationDate="2013-01-15T19:04:30.200" Id="47800" LastActivityDate="2013-01-15T19:30:42.970" LastEditDate="2013-01-15T19:30:42.970" LastEditorUserId="919" OwnerUserId="919" ParentId="47671" PostTypeId="2" Score="4" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm trying to look for difference in timing (ie. earlier/later) in a variable measured at regular intervals between two groups.&lt;/p&gt;&#10;&#10;&lt;p&gt;This seems like a simple experimental design, and working in R, I'm able to visualize the data in a way that makes sense to me, but somehow I'm getting confused when it comes to testing for  significance.&lt;/p&gt;&#10;&#10;&lt;p&gt;The data consist of weekly measurements of number of flowers for each individual, within and outside of the greenhouse. To take a small example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;expand.grid(week=(1:6),treatment=c(&quot;greenhouse&quot;,&quot;outside&quot;),individual=1:2)-&amp;gt;df&#10;c(0,3,10,2,0,0,0,0,0,2,18,0,0,1,19,0,0,0,0,0,1,2,15,1)-&amp;gt;flowers&#10;data.frame(cbind(df,flowers))-&amp;gt;df&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Visually,&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;qplot(week,flowers,data=df,facets=treatment~.)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;If my interest is simply to determine whether there's a significant difference in the time of flowering between the treatments; should I be doing a repeated measures ANOVA and looking at the interaction?&lt;/p&gt;&#10;&#10;&lt;p&gt;Simplifying (?) the problem even further, what if I remove the quantity of flowers, and just consider how many individuals are flowering? So the summarized data would be&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ddply(df, .(treatment,week), function(d) length(d[d$flowers&amp;gt;0,&quot;flowers&quot;]))-&amp;gt;indiv&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Which looks like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; qplot(week,V1,data=indiv,facets=treatment~.)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Here, my first thought was that I can just think of these as two distributions, and compare with a t-test; however, only individuals and not individualsxweek are independent, so perhaps this should also be a repeated measures ANOVA? Or do I need to venture into the world of more complex time-series math?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for your help!&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;&#10;As an update, I'm now also considering failure-time / survival analysis as a possible appropriate method.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-15T23:27:27.013" Id="47816" LastActivityDate="2013-01-22T22:49:46.677" LastEditDate="2013-01-22T19:50:43.507" LastEditorUserId="19711" OwnerUserId="19711" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;time-series&gt;&lt;hypothesis-testing&gt;&lt;survival&gt;" Title="Comparing means of two simple time series" ViewCount="462" />
  
  
  <row Body="&lt;p&gt;How many genes are sampled in either data set you've obtained? When the number of genes is large, it shouldn't be surprising that a gene may be highly differentially expressed in one replicate but not in another. This is a consequence of multiple testing. When you filter genes according to a $p=0.05$ statistical significance level, you have a 5% chance of making a type I error for any given gene. When averaged over several dozens or hundreds of genes, the chance of including at least one erroneous gene is greatly multiplied.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you don't want to exclude more genes than you have to, why not just include them all?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-01-16T05:04:58.373" Id="47832" LastActivityDate="2013-01-16T05:04:58.373" OwnerUserId="8013" ParentId="47828" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;What you are asking about is called &quot;interactions&quot;, which are frequently visualized and hopefully understood with so-called &quot;&lt;a href=&quot;https://www.google.com/search?q=interaction%20plot&quot; rel=&quot;nofollow&quot;&gt;interaction plots&lt;/a&gt;&quot;: you plot the relationship between one independent variable and the fitted observations &lt;em&gt;separately for each level of a factor&lt;/em&gt;. If there is no significant interaction, the lines will be parallel - if there is, they will not. Look at &lt;code&gt;?interaction.plot&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;This can readily be extended to ANCOVA. You would plot the continuous covariate on the x axis and use multiple lines linking the covariate to the fit - one line per level of the factor.&lt;/p&gt;&#10;&#10;&lt;p&gt;We recently had a &lt;a href=&quot;http://stats.stackexchange.com/questions/47265/how-to-investigate-a-3-way-interaction/47501&quot;&gt;question&lt;/a&gt; about understanding a 3-way interaction between two continuous and one factor variable.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-16T09:57:32.690" Id="47839" LastActivityDate="2013-01-16T09:57:32.690" OwnerUserId="1352" ParentId="47838" PostTypeId="2" Score="3" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;In the seminal paper &lt;a href=&quot;http://www.cs.ubc.ca/~murphyk/Papers/rbpf_uai00.pdf&quot;&gt;&quot;Rao-Blackwellised Particle Filtering for Dynamic Bayesian Networks&quot;&lt;/a&gt; by A. Doucet et. al. a sequential monte carlo filter (particle filter) is proposed, which makes use of a linear substructure $x^L_k$ in a markov process $x_k = (x^L_k, x^N_k)$. By marginalization of this linear structure, the filter can be split into two parts: a non-linear part which uses a particle filter, and one linear part which can be handled by a Kalman filter (conditioned on the non-linear part $x^N_k$). &lt;/p&gt;&#10;&#10;&lt;p&gt;I understand the marginalization part (and sometimes the described filter is also called marginalized filter). My intuition why it is called a Rao-Blackwellized Particle Filter (RBPF) is that the Gaussian parameters are a sufficient statistics for the underlying linear process, and following from the theorem of Rao-Blackwell an estimator conditioned on these parameters performs at least as good as the sampling estimator.&lt;/p&gt;&#10;&#10;&lt;p&gt;The Rao-Blackwell estimator is defined as $E(\delta(X)|T(X)) = \delta_1(X)$. In this context I would guess that $\delta(X)$ is the monte carlo estimator, $\delta_1(X)$ the RBPF, and $T(X)$ the gaussian parametrization. My problem is that I don't see where this is actually applied in the paper. &lt;/p&gt;&#10;&#10;&lt;p&gt;So why is this called a Rao-Blackwellized Particle Filter, and where does the Rao-Blackwellization actually happen?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-16T12:00:18.747" FavoriteCount="2" Id="47847" LastActivityDate="2014-12-01T12:47:51.850" LastEditDate="2013-01-16T13:38:00.933" LastEditorUserId="88" OwnerUserId="19731" PostTypeId="1" Score="11" Tags="&lt;monte-carlo&gt;&lt;particle-filter&gt;" Title="Rao-Blackwellization of sequential Monte Carlo filters" ViewCount="305" />
  <row Body="A basic forecasting technique for time series data, optionally including trend and/or seasonality, but (usually) excluding causal influences." CommentCount="0" CreationDate="2013-01-16T12:09:17.087" Id="47851" LastActivityDate="2013-01-16T13:25:37.557" LastEditDate="2013-01-16T13:25:37.557" LastEditorUserId="1352" OwnerUserId="1352" PostTypeId="4" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;Ok I am guessing this is a trivial question however having pondered it for a few days the only thing I have become clear on is my lack of statistical prowess. Yesterday I asked a question on fitting linear regression to a dataset piecewise and &lt;a href=&quot;http://stackoverflow.com/questions/14337439/piece-wise-linear-and-non-linear-regression-in-r&quot;&gt;this was successfully answered&lt;/a&gt;. Now I would like to compare these fits to a lognormal fit of the whole dataset with the reason that this is a standard method for comparing such data (even if it really doesn't fit well); I basically want to highlight that it does not fit as well. So without further ado here is the data:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x&amp;lt;-c(1e-08, 1.1e-08, 1.2e-08, 1.3e-08, 1.4e-08, 1.6e-08, 1.7e-08, &#10;1.9e-08, 2.1e-08, 2.3e-08, 2.6e-08, 2.8e-08, 3.1e-08, 3.5e-08, &#10;4.2e-08, 4.7e-08, 5.2e-08, 5.8e-08, 6.4e-08, 7.1e-08, 7.9e-08, &#10;8.8e-08, 9.8e-08, 1.1e-07, 1.23e-07, 1.38e-07, 1.55e-07, 1.76e-07, &#10;1.98e-07, 2.26e-07, 2.58e-07, 2.95e-07, 3.25e-07, 3.75e-07, 4.25e-07, &#10;4.75e-07, 5.4e-07, 6.15e-07, 6.75e-07, 7.5e-07, 9e-07, 1.15e-06, &#10;1.45e-06, 1.8e-06, 2.25e-06, 2.75e-06, 3.25e-06, 3.75e-06, 4.5e-06, &#10;5.75e-06, 7e-06, 8e-06, 9.25e-06, 1.125e-05, 1.375e-05, 1.625e-05, &#10;1.875e-05, 2.25e-05, 2.75e-05, 3.1e-05)&#10;&#10;y2&amp;lt;-c(-0.169718017273307, 7.28508517630734, 71.6802510299446, 164.637259265704, &#10;322.02901173786, 522.719633360006, 631.977073772459, 792.321270345847, &#10;971.810607095548, 1132.27551798986, 1321.01923840546, 1445.33152600664, &#10;1568.14204073109, 1724.30089942149, 1866.79717333592, 1960.12465709003, &#10;2028.46548012508, 2103.16027631327, 2184.10965255236, 2297.53360080873, &#10;2406.98288043262, 2502.95194879366, 2565.31085776325, 2542.7485752473, &#10;2499.42610084412, 2257.31567571328, 2150.92120390084, 1998.13356362596, &#10;1990.25434682546, 2101.21333152526, 2211.08405955931, 1335.27559108724, &#10;381.326449703455, 430.9020598199, 291.370887491989, 219.580548355043, &#10;238.708972427248, 175.583544448326, 106.057481792519, 59.8876372379487, &#10;26.965143266819, 10.2965349811467, 5.07812046132922, 3.19125838983254, &#10;0.788251933518549, 1.67980552001939, 1.97695007279929, 0.770663673279958, &#10;0.209216903989619, 0.0117903221723813, 0.000974437796492681, &#10;0.000668823762763647, 0.000545308757270207, 0.000490042305650751, &#10;0.000468780182460397, 0.000322977916070751, 0.000195423690538495, &#10;0.000175847622407421, 0.000135771259866332, 9.15607623591363e-05)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;You can see a plot of the data at the link above for quick interpretation.&lt;/p&gt;&#10;&#10;&lt;p&gt;So to my question- how to regress a lognormal curve to it? My school stats says this is not univariate data so I cannot use something like fitdistr in R. Am I right? If this is the case how might a find an approximate lognormal curve for this data if at all? Any help or pointers in a relevant direction would be great.&lt;/p&gt;&#10;&#10;&lt;p&gt;[Edit]&#10;With reference to the negative value I am wondering whether there is an approach that can handle it? This is because I have a second set of why values as shown below that are all negative and I would like to fit a similar model to these as well.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;y3&amp;lt;-c(0.0530500094068018, 0.160928860126123, -0.955328071740233, &#10;-2.53940686389203, -7.3241148240459, -8.32055726147533, -10.1979192158835, &#10;-12.0304091519687, -16.2527095992605, -19.9106624262052, -24.5089014234888, &#10;-28.7705733263437, -31.4294017506492, -35.8411743936776, -37.9005809712801, &#10;-40.2384353560669, -42.5902603334382, -44.6732472915729, -47.6606530197197, &#10;-54.2197956720375, -58.6590075712884, -61.5736755669354, -65.7179971091261, &#10;-63.1056155765268, -65.5404821687269, -59.2950249004724, -57.7385677458644, &#10;-63.3729518981994, -56.9303570422243, -69.2878310104119, -60.3990492926747, &#10;-14.1184714024129, -8.29495412660418, -3.44061704811896, 0.473805205298244, &#10;-5.47720584050456, -4.02534147802113, -2.13820456379, -0.641737730083625, &#10;0.25648844079225, 0.697033621376916, 0.622208830019496, 0.276775608633299, &#10;0.219104625574544, -0.0411577088679307, -0.0191732850594168, &#10;-0.0210255752601919, -0.0156084146143567, -0.00423245275017332, &#10;-0.000297816450447215, -1.24063266749809e-05, -1.53585832955629e-05, &#10;-2.32966128710771e-05, -2.39386641905819e-05, -2.15491949944693e-05, &#10;-1.50167366691665e-05, -8.3066700184436e-06, -7.89314461608438e-06, &#10;-6.20937293357605e-06, -3.64313623751525e-06)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2013-01-16T14:31:08.393" Id="47862" LastActivityDate="2013-01-16T20:48:42.693" LastEditDate="2013-01-16T20:48:42.693" LastEditorUserId="19741" OwnerUserId="19741" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;regression&gt;&lt;curve-fitting&gt;&lt;lognormal&gt;" Title="Lognormal curve fitting" ViewCount="236" />
  
  
  
  <row Body="&lt;p&gt;Not when $d\neq 0$; then $y_{t} - y_{t-1} -d = \epsilon_{t} $ is white noise (provided the errors $\epsilon$ are uncorrelated &amp;amp; their mean is zero).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-16T14:59:20.207" Id="47867" LastActivityDate="2013-01-16T14:59:20.207" OwnerUserId="17230" ParentId="47842" PostTypeId="2" Score="0" />
  <row AnswerCount="1" Body="&lt;p&gt;I have a non-linear reglationship and I want to find the best way to determine the value for the exponent $\gamma$ in the following regression:&lt;/p&gt;&#10;&#10;&lt;p&gt;$y = \beta x ^ \gamma$&lt;/p&gt;&#10;&#10;&lt;p&gt;I would preferably like to do this in R.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-01-16T15:42:18.330" Id="47870" LastActivityDate="2014-09-18T00:08:01.800" LastEditDate="2014-09-18T00:08:01.800" LastEditorUserId="805" OwnerUserId="284" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;nonlinear-regression&gt;" Title="Exponent for non-linear regression (in R)?" ViewCount="173" />
  
  
  
  <row Body="&lt;p&gt;You can mix them. As in other forms of regression, changing the scale will change the parameter estimates, but only in the same way that changing height from meters to feet would change it.&lt;/p&gt;&#10;&#10;&lt;p&gt;The meaning of the resulting model will be the same.&lt;/p&gt;&#10;&#10;&lt;p&gt;But you have to keep track of what you've done to the variables. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-16T20:30:40.037" Id="47891" LastActivityDate="2013-01-16T20:30:40.037" OwnerUserId="686" ParentId="47890" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;I suggest you give &lt;a href=&quot;http://www.hindawi.com/journals/tswj/aa/909231/&quot; rel=&quot;nofollow&quot;&gt;heavy-tail Lambert W x F&lt;/a&gt; or &lt;a href=&quot;http://projecteuclid.org/euclid.aoas/1318514301&quot; rel=&quot;nofollow&quot;&gt;skewed Lambert W x F &lt;/a&gt; distributions a try (disclaimer: I am the author). In R they are implemented in the &lt;a href=&quot;http://cran.r-project.org/web/packages/LambertW/&quot; rel=&quot;nofollow&quot;&gt;&lt;strong&gt;LambertW&lt;/strong&gt;&lt;/a&gt; package.&lt;/p&gt;&#10;&#10;&lt;p&gt;They arise from a parametric, non-linear transformation of a random variable (RV) $X \sim F$, to a heavy-tailed (skewed) version $Y \sim \text{Lambert W} \times F$.  For $F$ being Gaussian, heavy-tail Lambert W x F reduces to Tukey's $h$ distribution. (I will here outline the heavy-tail version, the skewed one is analogous.)&lt;/p&gt;&#10;&#10;&lt;p&gt;They have one parameter $\delta \geq 0$ ($\gamma \in \mathbb{R}$ for skewed Lambert W x F) that regulates the degree of tail heaviness (skewness).  Optionally, you can also choose different left and right heavy tails to achieve heavy-tails and asymmetry. It transforms a standard Normal $U \sim \mathcal{N}(0,1)$ to a Lambert W $\times$ Gaussian $Z$ by&#10;$$&#10;Z = U \exp\left(\frac{\delta}{2} U^2\right)&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;If $\delta &amp;gt; 0$ $Z$ has heavier tails than $U$; for $\delta = 0$, $Z \equiv U$. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you don't want to use the Gaussian as your baseline, you can create other Lambert W versions of your favorite distribution, e.g., t, uniform, gamma, exponential, beta, ... However, for your dataset a double heavy-tail Lambert W x Gaussian (or a skew Lambert W x t) distribution seem to be a good starting point.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(LambertW)&#10;set.seed(10)&#10;&#10;### Set parameters ####&#10;# skew Lambert W x t distribution with &#10;# (location, scale, df) = (0,1,3) and positive skew parameter gamma = 0.1&#10;theta.st &amp;lt;- list(beta = c(0, 1, 3), gamma = 0.1)&#10;# double heavy-tail Lambert W x Gaussian&#10;# with (mu, sigma) = (0,1) and left delta=0.2; right delta = 0.4 (-&amp;gt; heavier on the right)&#10;theta.hh &amp;lt;- list(beta = c(0, 1), delta = c(0.2, 0.4))&#10;&#10;### Draw random sample ####&#10;# skewed Lambert W x t&#10;yy &amp;lt;- rLambertW(n=1000, distname=&quot;t&quot;, theta = theta.st)&#10;&#10;# double heavy-tail Lambert W x Gaussian (= Tukey's hh)&#10;zz =&amp;lt;- rLambertW(n=1000, distname = &quot;normal&quot;, theta = theta.hh)&#10;&#10;### Plot ecdf and qq-plot ####&#10;op &amp;lt;- par(no.readonly=TRUE)&#10;par(mfrow=c(2,2), mar=c(3,3,2,1))&#10;plot(ecdf(yy))&#10;qqnorm(yy); qqline(yy)&#10;&#10;plot(ecdf(zz))&#10;qqnorm(zz); qqline(zz)&#10;par(op)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/hGpZj.png&quot; alt=&quot;ecdf and qqplot of skewed/heavy-tailed Lambert W x F distributions&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In practice, of course, you have to estimate $\theta = (\beta, \delta)$, where $\beta$ is the parameter of your input distribution (e.g., $\beta = (\mu, \sigma)$ for a Gaussian, or $\beta = (c, s, \nu)$ for a $t$ distribution; see paper for details):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;### Parameter estimation ####&#10;mod.Lst &amp;lt;- MLE_LambertW(yy, distname=&quot;t&quot;, type=&quot;s&quot;)&#10;mod.Lhh &amp;lt;- MLE_LambertW(zz, distname=&quot;normal&quot;, type=&quot;hh&quot;)&#10;&#10;layout(matrix(1:2, ncol = 2))&#10;plot(mod.Lst)&#10;plot(mod.Lhh)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/sJ8ay.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Since this heavy-tail generation is based on a &lt;strong&gt;bijective&lt;/strong&gt; transformations of RVs/data, you &lt;strong&gt;can remove heavy-tails from data&lt;/strong&gt; and check if they are &lt;em&gt;nice&lt;/em&gt; now, i.e., if they are Gaussian (and test it using Normality tests). &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;### Test goodness of fit ####&#10;## test if 'symmetrized' data follows a Gaussian&#10;xx &amp;lt;- get_input(mod.Lhh)&#10;normfit(xx)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ENyuH.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;This worked pretty well for the simulated dataset.  I suggest you give it a try and see if you can also &lt;a href=&quot;http://www.rdocumentation.org/packages/LambertW/functions/Gaussianize&quot; rel=&quot;nofollow&quot;&gt;&lt;code&gt;Gaussianize()&lt;/code&gt; your data&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, as @whuber pointed out, bimodality can be an issue here.  So maybe you want to check in the transformed data (without the heavy-tails) what's going on with this bimodality and thus give you insights on how to model your (original) data.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-01-17T01:37:08.723" Id="47917" LastActivityDate="2015-03-07T22:08:45.870" LastEditDate="2015-03-07T22:08:45.870" LastEditorUserId="11476" OwnerUserId="11476" ParentId="33115" PostTypeId="2" Score="4" />
  
  
  
  <row Body="&lt;p&gt;You haven't overfit your model, what you've done is demonstrate (again) that stepwise, forward and backward methods don't work well for this type of task. (Although it was good that you used a training and test set, this let you see that these methods can find things that aren't there).&lt;/p&gt;&#10;&#10;&lt;p&gt;Model selection is a big topic and has often been discussed, both here and elsewhere. I would generally advise against &lt;em&gt;any&lt;/em&gt; automatic variable selection scheme, but if you must use one, I suggest LASSO or LAR. Since you are using SAS, you can find both of these in GLMSELECT. Although this is intended for models that can be fit with PROC GLM, I have have good results using it for logistic models and then testing the resulting models further in LOGISTIC. &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-01-17T11:56:57.720" Id="47943" LastActivityDate="2013-01-17T11:56:57.720" OwnerUserId="686" ParentId="47942" PostTypeId="2" Score="6" />
  
  
  <row Body="&lt;p&gt;I don't have references on hand, unfortunately, but generally the more dependence there is between $X$ and $Y$, the more efficient it will be to update them jointly. In your example $X$ and $Y$ are independent; if, for example, $X$ and $Y$ were still jointly Gaussian but had a correlation of .99, updating them separately would be much worse. Updating them jointly will also not be very efficient unless your proposal distribution has roughly the same shape as the distribution you're sampling from, which motivated the development of adaptive MCMC methods which change the proposal distribution over time. The best-known paper on this is &lt;a href=&quot;http://www.math.unm.edu/~michele/Teaching/SC/material/Haario2001_adaptiveMCMC.pdf&quot; rel=&quot;nofollow&quot;&gt;Haario et al. (2001)&lt;/a&gt;, but there has been significant work in the area since then. With a good proposal distribution (either by making it adaptive or just by having a lucky guess), joint updating in the highly correlated case is just as efficient as joint updating in the independent case, while individual updating is still horribly slow.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-01-17T13:38:48.473" Id="47948" LastActivityDate="2013-01-17T13:38:48.473" OwnerUserId="16297" ParentId="47933" PostTypeId="2" Score="3" />
  <row AnswerCount="0" Body="&lt;p&gt;Suppose I have $n=100$ observations of ordinal data and get threshold coefficients $b_1, \dots, b_3$ and probit slope $b_4$. I want to test the hypothesis $H_{0}: \frac{b_{3}}{b_{4}} = \frac{1}{2}$ vs. $H_a: \frac{b_{3}}{b_{4}} \neq \frac{1}{2}$. So I want to count the proportion of times we fail to reject the null hypothesis. To do this, would we just keep sampling 100 observations similar to the previous ones and return the proportion of p-values greater than $0.05$? &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-01-17T15:08:29.333" Id="47956" LastActivityDate="2013-01-17T22:44:56.170" LastEditDate="2013-01-17T22:44:56.170" LastEditorUserId="88" OwnerUserId="19773" PostTypeId="1" Score="2" Tags="&lt;power&gt;" Title="Power analysis ordinal probit regression" ViewCount="140" />
  
  <row AcceptedAnswerId="47984" AnswerCount="1" Body="&lt;p&gt;I have a vector of sequences with presence (&lt;code&gt;1&lt;/code&gt;) and absence (&lt;code&gt;0&lt;/code&gt;), from were I have calculated the first order Markov Process. &lt;/p&gt;&#10;&#10;&lt;p&gt;This is how the data looks like:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;dataset=c(NA,NA,0,0,0,0,1,0,1,1,0,1,1,NA,NA,NA,NA,NA,0,&#10;          1,1,1,1,1,1,0,NA,1,1,1,1,1,1,0,0,NA,NA,0,1,1,&#10;          NA,NA,0,NA,0,0,0,1,1,1,0,1,1,1,1,0,1,NA,0,1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;For thsi vector I have calculated:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Probability of Presence (&lt;code&gt;P1&lt;/code&gt;) and Absence (&lt;code&gt;P0&lt;/code&gt;)&lt;/li&gt;&#10;&lt;li&gt;Probability of having Presence followed by Presence (&lt;code&gt;P11&lt;/code&gt;), Presence followed by Absence (&lt;code&gt;P10&lt;/code&gt;), Absence followed by Presence (&lt;code&gt;P01&lt;/code&gt;) and Absence followed by Absence (&lt;code&gt;P00&lt;/code&gt;)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;The transition probabilities were obtained by using a loop the checks for sequence of 2 values: to calculate &lt;code&gt;P_00&lt;/code&gt; I am using &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;   P_00 : dataset[j]==0 &amp;amp; dataset[j+1]==0 , etc.&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;These are the results:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;P_0=0.3913043&#10;P_1=0.6086957&#10;P_0+P_1=1&#10;&#10;P_00=0.1538462&#10;P_01=0.2307692&#10;P_11=0.4615385&#10;P_10=0.1538462&#10;P_00+P_01+P_11+P_10=1&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The idea is to populate the NA with presence/absence sequences according to the Probability values obtained for the sequence. The problem is that for now I am not being able to find the best and more adequate process for this problem and I am a newbie with this type of problems. Even if there is already a package that can do this for me, I would prefer to understand how can I do it.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-01-16T09:44:46.893" FavoriteCount="1" Id="47965" LastActivityDate="2013-01-17T21:46:35.783" LastEditDate="2013-01-17T17:56:19.370" LastEditorUserId="919" OwnerDisplayName="A.R" OwnerUserId="7968" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;markov-process&gt;" Title="Markov Chain process - Missing values" ViewCount="232" />
  
  <row Body="&lt;p&gt;I just know of two methods to deal with unbalanced sets with SVMs:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Use bagging: you create bootstrap samples of your data, so that you a a big number of well-balanced problems. You train a SVM on each of them, and then use majority voting on the resulting ensemble of classifiers. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;If you are using C-SVM, then you can reweight the missclassification cost,&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;$$C\sum_{i}\psi_{i}$$&#10;into&#10;$$ C_{+}\sum_{i \epsilon I_{+}}\psi_{i} + C_{-}\sum_{i \epsilon I_{-}}\psi_{i}$$ &#10;where $I_{+}$, resp. $I_{-}$, is the set of indices for the positive examples, resp. for the negative examples. You choose the new soft-marging constants so that $\frac{C_{+}}{C_{-}} = \frac{n_{-}}{n_{+}}$, where $n_{+}$ and $n_{-}$ are the number of positive and negative samples resp.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-17T21:17:40.130" Id="47980" LastActivityDate="2013-01-17T21:17:40.130" OwnerUserId="17908" ParentId="46022" PostTypeId="2" Score="3" />
  
  
  <row AcceptedAnswerId="47998" AnswerCount="1" Body="&lt;p&gt;I was recently debugging an R script and I found something very weird, the author defined their own p-value function&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;pval &amp;lt;- function(x, y){&#10;    if (x+y&amp;lt;20) { # x + y is small, requires R.basic&#10;        p1&amp;lt;- nChooseK(x+y,x) * 2^-(x+y+1);&#10;        p2&amp;lt;- nChooseK(x+y,y) * 2^-(x+y+1);&#10;        pvalue = max(p1, p2)&#10;    }&#10;    else { # if x+y is large, use approximation&#10;        log_p1 &amp;lt;- (x+y)*log(x+y) - x*log(x) - y*log(y) - (x+y+1)*log(2);&#10;        pvalue&amp;lt;-exp(log_p1);&#10;    }&#10;    return(pvalue)&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Where X and Y are values positive values greater than 0. The &amp;lt;20 case seems to be a calculation for some kind of hypergeometric distribution (something similar to Fisher test?) and does anyone know what the other calculation is? As a sidenote, I am trying to optimize this code so trying to figure out the proper R function to call and replace this with.&lt;/p&gt;&#10;&#10;&lt;p&gt;Edit: Paper detailing formula for p-value calculation can be found &lt;a href=&quot;http://genome.cshlp.org/content/7/10/986.long&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; (need to click pdf &#10;to see the formulas) Methods start on page 8 of the pdf and the formula in question can be found on page 9 under (1). The distribution they assume is a Poisson.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-18T02:05:36.133" Id="47997" LastActivityDate="2014-03-09T03:38:38.090" LastEditDate="2014-03-09T03:38:38.090" LastEditorUserId="805" OwnerUserId="10640" PostTypeId="1" Score="9" Tags="&lt;r&gt;&lt;hypothesis-testing&gt;&lt;p-value&gt;" Title="Unknown p-value calculation" ViewCount="449" />
  <row Body="&lt;p&gt;Only having access to a small sample may cause certain problems, for example: statistical tests may be underpowered; assumptions may be more difficult to verify; if data are not sufficiently normal, the Central Limit Theorem may not be able to ensure the normality of relevant sampling distributions; if the sample is small enough relative to the number of parameters to be estimated, a model could be saturated, overfitted or underdetermined.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-18T04:50:23.880" Id="48002" LastActivityDate="2013-01-18T08:00:34.637" LastEditDate="2013-01-18T08:00:34.637" LastEditorUserId="7290" OwnerUserId="7290" PostTypeId="5" Score="0" />
  <row AcceptedAnswerId="48044" AnswerCount="1" Body="&lt;p&gt;I have been using Latent Dirichlet Analysis for a while but I am a bit confounded as to it's practical ability to compare two documents. It is of course ideal for classification when you want to see in what category a certain document or word belongs, but in content comparison I am at my wits end.&lt;/p&gt;&#10;&#10;&lt;p&gt;An inferred document yields a distribution over n topics, summing to one. &lt;/p&gt;&#10;&#10;&lt;p&gt;Using Kolmogorov-Smirnov was recommended somewhere, but it has the annoying property of giving two identical distributions a very high score, even if they do not make sense. Two words not in the dictionary will give a perfect match, which is of course absurd.&lt;/p&gt;&#10;&#10;&lt;p&gt;The normalized dot product actually works very well because it punishes flat distributions (trivial documents), but I thought there would be a better one.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any suggestion is appreciated.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-18T07:34:44.400" FavoriteCount="1" Id="48005" LastActivityDate="2013-01-18T21:33:53.707" LastEditDate="2013-01-18T21:33:53.707" LastEditorUserId="8413" OwnerUserId="19795" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;dirichlet-distribution&gt;&lt;topic-models&gt;" Title="Latent Dirichlet analysis document comparison" ViewCount="154" />
  <row AnswerCount="0" Body="&lt;p&gt;I got this assignment from Generalized Linear Model class. At first glace it looked like it is an easy task, but there are a lot of subtle (at least in my opinion) things, which I would like to clarify with somebody. So here is the problem:&lt;/p&gt;&#10;&#10;&lt;p&gt;Assume that we are interested in random variable $Y=e^Z$. Variable $Z$ is obtained from simple linear regression i.e. $Z=a+bX+\varepsilon$, where $\varepsilon \sim N(0,4)$. Given 100 observations of $(Z,X)$ we have following empirical characteristics $\bar x=2$, $\sigma^2_x=1$, $\bar z = -1$ and $\operatorname {cov} (z,y)=-1 $.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;How do we construct &lt;em&gt;approximate&lt;/em&gt; Wald's 95% confidence interval for parameter $\theta = E(Y|X=1/2)$, using the fact $E e^{\lambda U} = e^{a \lambda+\sigma^2\lambda^2/2}$, when $U \sim N(a, \sigma^2)$?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;How do we construct exact Wald's 95% confidence interval for individual prediction of $Y$ when $X=1/2$?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Now assume that we do not know variance of $\varepsilon$ and from those 100 observations we know that the empirical variance of $Z$, $\sigma^2_z=4$.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I started by plugging in the empirical statistics to specify simple regression part and I got $Z=1-X+\varepsilon$, so the&#10;$$\operatorname{\log} Y =1-X+\varepsilon. $$&#10;As the $\operatorname {log} Y$ is normaly distributed then the distribution of the mean of $Y$ should be lognormal? If $Y$ is log normal then I can't write write it in canonical exponential family form&#10;$$\operatorname{exp}\left\{ \frac{y \theta-b(\theta)}{a(\phi)}-c(y,\phi) \right\},$$&#10;because I have $\operatorname{log}y$ instead of $y$ multiplied by $\theta$. That means that I do not know what $b(\theta)$ and other parameters are, which I need for standard error estimation.&lt;/p&gt;&#10;&#10;&lt;p&gt;So any tips on getting at least first question answered would be great!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-18T11:57:32.317" FavoriteCount="0" Id="48014" LastActivityDate="2013-01-18T11:57:32.317" OwnerUserId="14729" PostTypeId="1" Score="2" Tags="&lt;self-study&gt;&lt;generalized-linear-model&gt;&lt;exponential-family&gt;" Title="Confidence intervals for log normal responses" ViewCount="111" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I've got a semi-parametric model that I'm fitting with GAM's (mgcv in R).  It is of the form &lt;/p&gt;&#10;&#10;&lt;p&gt;$$y = \theta + X'\beta + f(Z) + \text{Interactions!} + \epsilon$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I've got 289 observations.&lt;/p&gt;&#10;&#10;&lt;p&gt;Many of the interactions that I'd like to specify are tensor (or Kronecker) product interactions.  Unpenalized, each interaction takes up $k^n$ degrees of freedom, where k is the number of knots and n is the number of continuous variables in the interaction.  Of course, penalization reduces the effective degrees of freedom, but GAM won't fit a model with more to-be-penalized parameters than degrees of freedom.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I've already coerced the models to specify 4 knots per Z variable and specified their location based on subject knowledge -- this isn't optimal in the sense that Simon Wood's thin plate regression splines are optimal, but it seems like a not-bad compromise in order to keep k down.&lt;/p&gt;&#10;&#10;&lt;p&gt;Anyway, I have a long candidate list of interactions that might be important.  The problem is that a model that specifies all of them has &gt;1000 degrees of freedom.  I can't use a model selection algorithm that starts saturated and eliminates.  I need to use one that starts with only the main effects (all of which I knot that I want to include a priori), and then adds terms.&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem is that adding one interaction might change the relevance of others, or of a set of others.  There are probably hundreds of combinations of interactions that are accessible given my df constraints.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I'm looking for ideas for model selection besides trial and error.&lt;/p&gt;&#10;&#10;&lt;p&gt;(I'm comparing models by &quot;lowest AIC wins&quot;)&lt;/p&gt;&#10;&#10;&lt;p&gt;Anybody have any ideas or experience with this sort of problem?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-01-18T17:24:38.363" Id="48032" LastActivityDate="2013-01-18T17:24:38.363" OwnerUserId="17359" PostTypeId="1" Score="3" Tags="&lt;nonparametric&gt;&lt;model-selection&gt;&lt;interaction&gt;&lt;gam&gt;" Title="Algorithm for selecting interactions without very many degrees of freedom (mgcv, gam)" ViewCount="119" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I got the equation for logistic reg, and I am comfortable with the result. Let's say logit(p), ln(p/q), or the model is something like    &lt;/p&gt;&#10;&#10;&lt;p&gt;$\text{logit}(p) = b+a_1X_1 + a_2X_2 + a_3X_3$&lt;/p&gt;&#10;&#10;&lt;p&gt;For example --&gt;  &lt;code&gt;$b = 10 , a_1 = 0.5 , a_2 = 0.6 , a_3 =0.7&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;So my equation is  &lt;code&gt;$\text{logit(p)} = 10 + 0.5X_1 + 0.6X_2 + 0.7X_3&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's say if I want my probability to be 40% ( p = 0.4 ) to be my cut off rate in the system. &lt;/p&gt;&#10;&#10;&lt;p&gt;How can I use this in the live system ? &lt;/p&gt;&#10;&#10;&lt;p&gt;I'm thinking of giving the developer this logit equation but use  in the live system this equation and block account that have fall below success rate threshold ? &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-01-18T21:44:35.410" Id="48054" LastActivityDate="2013-02-18T14:21:17.217" LastEditDate="2013-01-19T13:10:13.717" LastEditorUserId="686" OwnerUserId="6854" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;logistic&gt;" Title="How can I implement logistic regression in live decision system?" ViewCount="61" />
  
  <row Body="&lt;p&gt;&quot;The overline is also used to indicate a sample mean&quot;&#10;&lt;a href=&quot;http://en.wikipedia.org/wiki/Overline&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Overline&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;While a macron is a Greek diacritic over a single symbol while &quot;a line over a collection of symbols is a vinculum.&quot;&#10;see &lt;a href=&quot;http://en.wikipedia.org/wiki/Macron&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Macron&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-01-18T22:17:22.653" Id="48058" LastActivityDate="2013-09-18T18:52:03.233" LastEditDate="2013-09-18T18:52:03.233" LastEditorUserId="16393" OwnerUserId="16393" ParentId="48057" PostTypeId="2" Score="-2" />
  
  <row AcceptedAnswerId="48487" AnswerCount="1" Body="&lt;p&gt;There are $m$ normally distributed, independent random variables $N_1, \ldots, N_m$ with distinct means $\mu_1, \ldots \mu_m$ and standard deviations $\sigma_1, \ldots, \sigma_m$. Then, we observe a permutation of the numbers $\pi = \{1, \ldots, m\}$. How can we efficiently compute the conditional expectation of the random variables in same ordering as this permutation? &lt;em&gt;Added bonus: how can we compute the conditional variance?&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;An example:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;we have four independent random variables $N_1, N_2, N_3, N_4$, all with different means and variances. &lt;/li&gt;&#10;&lt;li&gt;We are given the permutation $\pi = (3, 1, 2, 4)$.&lt;/li&gt;&#10;&lt;li&gt;What's $\mathbf{E}((N_1, N_2, N_3, N_4) \mid N_3 &amp;gt; N_1 &amp;gt; N_2 &amp;gt; N_4)$?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;One way to do this is by Gibbs sampling: we can sample from the conditional distribution $(N_1, N_2, N_3, N_4) \mid N_3 &amp;gt; N_1 &amp;gt; N_2 &amp;gt; N_4$ by doing the following:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Initialize some arbitrary values $x_i^0$ such that $x_3^0 &amp;gt; x_1^0 &amp;gt; x_2^0 &amp;gt; x_4^0$&lt;/li&gt;&#10;&lt;li&gt;For $t = 1 \ldots M$, choose a random index $i$.&lt;/li&gt;&#10;&lt;li&gt;$x_{\pi_i}^t \sim \text{TruncatedNormal}(\mu_{\pi_i}, \sigma_{\pi_i}, x_{\pi_{i-1}}^{t-1}, x_{\pi_{i+1}}^{t-1})$, with lower/upper bounding values $-\infty$ or $\infty$ if $i=1$ or $i=m$, respectively, and $x^t = x^{t-1}$ otherwise&lt;/li&gt;&#10;&lt;li&gt;Compute the mean and variance of the $x^t$ after some break-in period.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;The above is basically an MCMC method that walks over the conditional distribution to obtain an average computationally. The question is, is there a more efficient, faster, or better way to compute this expectation?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;This question is related to &lt;a href=&quot;http://math.stackexchange.com/questions/270745/compute-probability-of-a-particular-ordering-of-normal-random-variables&quot;&gt;this one on math.stackexchange&lt;/a&gt; about computing the probability of a particular ordering, which it turns out can be done using the multivariate normal CDF (for which there exist numerical methods.)&lt;/em&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-01-19T07:22:49.320" Id="48075" LastActivityDate="2013-01-25T07:30:20.980" OwnerUserId="13188" PostTypeId="1" Score="1" Tags="&lt;normal-distribution&gt;&lt;sampling&gt;&lt;mcmc&gt;&lt;gibbs&gt;&lt;conditional-expectation&gt;" Title="Computing conditional expectation of ordered normal random variables" ViewCount="176" />
  
  <row Body="&lt;p&gt;At each step that you are asked to make a decision (i.e. choose one option among several options), you must have an additional set/partition to gauge the accuracy of your choice so that you do not simply pick the most favorable result of randomness and mistake the tail-end of the distribution for the center &lt;sup&gt;1&lt;/sup&gt;. The left is the pessimist. The right is the optimist. The center is the pragmatist. Be the pragmatist.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/VlVco.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Step 1) Training:&lt;/strong&gt; Each type of algorithm has its own parameter options (the number of layers in a Neural Network, the number of trees in a Random Forest, etc). For each of your algorithms, you must pick one option. That’s why you have a validation set. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Step 2) Validating:&lt;/strong&gt; You now have a collection of algorithms. You must pick one algorithm. That’s why you have a test set. Most people pick the algorithm that performs best on the validation set (and that's ok). But, if you do not measure your top-performing algorithm’s error rate on the test set, and just go  with its error rate on the validation set, then you have blindly mistaken the “best possible scenario” for the “most likely scenario.” That's a recipe for disaster.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Step 3) Testing:&lt;/strong&gt; I suppose that if your algorithms did not have any parameters then you would not need a third step. In that case, your validation step would be your test step. Perhaps Matlab does not ask you for parameters or you have chosen not to use them and that is the source of your confusion.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; It is often helpful to go into each step with the assumption (null hypothesis) that all options are the same (e.g. all parameters are the same or all algorithms are the same), hence my reference to the distribution. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;sup&gt;2&lt;/sup&gt; This image is not my own. I have taken it from this site: &lt;a href=&quot;http://www.teamten.com/lawrence/writings/bell-curve.png&quot;&gt;http://www.teamten.com/lawrence/writings/bell-curve.png&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-19T18:17:45.933" Id="48090" LastActivityDate="2013-01-19T19:55:17.137" LastEditDate="2013-01-19T19:55:17.137" LastEditorUserId="8401" OwnerUserId="8401" ParentId="19048" PostTypeId="2" Score="10" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I was wondering if someone could please help me with the following. I am trying to find the pdf of a set of dependent $\chi^2$ random variables. Suppose $x_1,x_2,...,x_n$ are independent normals $x_i \sim N(\mu_i, \sigma_i^2)$, not necessarily identical. Assume $n$ and all $\mu_i$'s and $\sigma_i's$ are known parameters, so this is really a probability, not an inferential statistics question. Select all possible subsets of size $n-1, n-2,..., n-k$ with $(n-k \geq 1)$ from this set. Each subset will then have a mean and variance equal to $\sum_{j\mbox{th subset}} \mu_j$  and $\sum_{j\mbox{th subset}} \sigma_j^2$, respectively. For each subset then compute the standardized sums: &lt;/p&gt;&#10;&#10;&lt;p&gt;$(\sum_{j\mbox{th subset}} x_j - \sum_{j\mbox{th subset}} \mu_j)^2/\sum_{j\mbox{th subset}} \sigma_j^2$&lt;/p&gt;&#10;&#10;&lt;p&gt;Each sum is then a $\chi^2_1$ (one degree of freedom). Question is to find the pdf of the $l$th order statistic of such set of $\chi_1^2$'s. Clearly, the sums cannot be independent $\chi^2$'s, since the subsets share some of the original normals. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have seen some related work on 'subset selection' problems in regression, specifically, a paper by Arvesen and McCabe of 1974. They mention their problem involves exactly this distribution, but then I believe they do not provide it, only providing asymptotic results and Monte Carlo simulations.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any help in this regard will be greatly appreciated! (I hope this edited version of the question is clearer--thanks very much!)&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-01-20T02:33:30.590" Id="48111" LastActivityDate="2013-01-21T00:05:52.703" LastEditDate="2013-01-21T00:05:52.703" LastEditorUserId="-1" OwnerUserId="19845" PostTypeId="1" Score="1" Tags="&lt;chi-squared&gt;&lt;non-independent&gt;&lt;order-statistics&gt;" Title="Order statistic of dependent chi-squares" ViewCount="177" />
  
  
  
  
  <row Body="&lt;p&gt;&quot;Average variation points&quot; isn't really a statistical term. For &quot;how many points, on average, a person gains or loses per day&quot; I can think of two answers, depending on what you mean:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\frac{50 + 40 + 0 + 20}{4} = \frac{90}{4} = 22.5$&lt;/p&gt;&#10;&#10;&lt;p&gt;which treats &quot;gains&quot; and &quot;losses&quot; as absolute values and&lt;/p&gt;&#10;&#10;&lt;p&gt;$\frac{50 - 40 + 0 + 20}{4} = \frac{30}{4} = 9.75$&lt;/p&gt;&#10;&#10;&lt;p&gt;which treats them as positive and negative values. &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-01-21T00:32:43.420" Id="48156" LastActivityDate="2013-01-21T00:32:43.420" OwnerUserId="686" ParentId="48152" PostTypeId="2" Score="1" />
  
  
  
  
  
  
  <row Body="&lt;p&gt;Well I don't remember much about Markov process. But I see a first mistake.&lt;/p&gt;&#10;&#10;&lt;p&gt;$$P_{14} \times P_{44}^x \times P_{41} = 1 \times (0.5)^x \times 0.5 = (0.5)^{x+1}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I just changed to follow the idea that from 1, the probability of going to 4 is 1.&#10;From 4 you can stay $x$ steps and if you are &quot;lucky&quot;, you can go back to 1.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you use the notation with $k$, from 4, you can go back in 1 in $(0.5)^{k+1}$, $k$ = $0, 1, ...$&lt;/p&gt;&#10;&#10;&lt;p&gt;You can change to $(0.5)^{k}$, $k$ = $1, 2, ...$&lt;/p&gt;&#10;&#10;&lt;p&gt;And from here, you need the step between 1 and 4 at the beginning. &lt;/p&gt;&#10;&#10;&lt;p&gt;$$P(\tau = k) = (0.5)^{k-1}, k = 2, 3, ...$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Hope it helps.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-21T15:19:56.117" Id="48195" LastActivityDate="2013-01-21T15:19:56.117" OwnerUserId="17463" ParentId="48185" PostTypeId="2" Score="1" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a group of 6 animals. &#10;I took the baseline reading (Time 0), put them under Treatment A, then take readings again at Times 2, 4, 6, and 12 hours.&#10;I rest them for 2 weeks.&#10;Then I took a baseline (time 0), put them under Treatment B, then take readings again at Times 2, 4, 6, 12 hours.&#10;I believe this is a repeated measures ANOVA, with within-subject (as Times- with 5 levels: 0,2,4,6,12 hrs) and between-subject (Treatment A, Treatment B).&lt;/p&gt;&#10;&#10;&lt;p&gt;My problem is, the subjects in Treatment A and Treatment B are the same subjects.&#10;How do I account for that (as in they are not randomized)?&lt;/p&gt;&#10;&#10;&lt;p&gt;Right now, I ran the repeated measures MANOVA and ANOVA for time*treatment, time, treatment.&#10;Then I ran paired-t test with bonferroni correction (0.05/4) to test Times 2,4,6,12 with Time 0 (of the same treatment).&#10;And I ran paired-t test to test Treatments (A vs B) across time points (Time 0,2,4,6,12).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-22T00:30:50.927" Id="48223" LastActivityDate="2013-01-22T00:30:50.927" OwnerUserId="19897" PostTypeId="1" Score="1" Tags="&lt;repeated-measures&gt;" Title="Repeated measures ANOVA, with same subjects under 2 treatments" ViewCount="134" />
  <row Body="&lt;p&gt;The basic form of the model is location-only component. It predicts the level of response. If some predictors are categorical, variability of response (more precisely, in the underlying continuous variable which is linked by a link function with the observed response) in different groups might be different (nonhomogeneity of variance, heteroscedasticity), but location-only model ignores this circumstance. In such situation adding scale component to the model - the component that accounts for the above nonhomogeneity and also estimates it - can improve the location prediction. So, the combined location+scale model is more general and in a sense is preferable.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-22T08:58:29.977" Id="48239" LastActivityDate="2013-01-22T08:58:29.977" OwnerUserId="3277" ParentId="48237" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;You could use zero- and/or one inflated beta regression models which combine the beta distribution with a degenerate distribution to assign some probability to 0 and 1 respectively. For details see the following references: &lt;/p&gt;&#10;&#10;&lt;p&gt;Ospina, R., &amp;amp; Ferrari, S. L. P. (2010). Inflated beta distributions. Statistical Papers, 51(1), 111-126.&#10;Ospina, R., &amp;amp; Ferrari, S. L. P. (2012). A general class of zero-or-one inflated beta regression models. Computational Statistics and Data Analysis, 56(6), 1609 - 1623.&lt;/p&gt;&#10;&#10;&lt;p&gt;These models are easy to implement with the gamlss package for R.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-22T10:08:45.227" Id="48241" LastActivityDate="2013-01-22T10:08:45.227" OwnerUserId="19910" ParentId="48028" PostTypeId="2" Score="6" />
  <row AcceptedAnswerId="48245" AnswerCount="1" Body="&lt;p&gt;I have been asked to look at some spatial data, which I have assumed is multi-normally distributed in 3 dimensions (and to a reasonable approximation this appears to be true). Actually, all I've been asked to do is provide a table so that someone can specify a probability and I tell them how many standard deviations of their data they need to have that coverage so I wasn't expecting this to take long (i.e. 1.96sd = 95% in 1D).&lt;/p&gt;&#10;&#10;&lt;p&gt;On a closer inspection of the data it is rather heavy-tailed and based on what I think it's wanted for these are the interesting data so I think I possibly shouldn't be using the normal distribtuion, making this a more interesting task. The only two-tailed, tail heavy distribution I know of is the t-distribution and a quick google search seems to imply that there is not a natural multi-variate extension of this although I could just use the one on wiki. Is there a multi-variate t-distribution that's considered 'best' or is there a better distribution that I should be considering?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-01-22T10:47:18.127" Id="48242" LastActivityDate="2013-01-22T17:23:38.803" LastEditDate="2013-01-22T17:23:38.803" LastEditorUserId="7734" OwnerUserId="7734" PostTypeId="1" Score="3" Tags="&lt;distributions&gt;&lt;multivariate-analysis&gt;" Title="Heavy tailed multi-variate distribution" ViewCount="99" />
  <row Body="&lt;p&gt;The probability of &quot;correctly not rejecting the null hypothesis&quot;--i.e., if the null hypothesis is true, we do not reject it--is controlled by the significance level at which we are doing the test. If I choose a significance level of $\alpha = .05$, so that I reject if my $p$-value is less than .05, then my probability of correctly not rejecting the null hypothesis is $1-.05 = .95$. If rejecting the null hypothesis when it is in fact true would have very bad consequences then we might use a smaller $\alpha$, say .01 or even .001, which gives us a higher probability of correctly not rejecting the null hypothesis.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, in fact, we already control this probability--in fact, this is much easier to control than the power. Because it's much easier there's much less discussion about it, which is probably why you concluded that statisticians aren't interested in it.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-01-22T14:31:53.260" Id="48250" LastActivityDate="2013-01-22T14:31:53.260" OwnerUserId="16297" ParentId="48249" PostTypeId="2" Score="3" />
  <row AnswerCount="2" Body="&lt;p&gt;I have a time series with daily observations over the course of multiple years (interest in topic &quot;superbowl&quot; over time). The seasonality in the data is yearly as well and it is very spiky (almost nothing all year and big increase/spike in January/February). I have started using R for this task (&lt;code&gt;forecast&lt;/code&gt; package) and have little experience with statistics.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x &amp;lt;- ts(myts, frequency=365)&#10;fit &amp;lt;- HoltWinters(x)&#10;plot(forecast(fit))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This works great and captures the seasonality of the data.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, I have read more about exponential smoothing (at &lt;a href=&quot;http://otexts.com/fpp/7/&quot; rel=&quot;nofollow&quot;&gt;http://otexts.com/fpp/7/&lt;/a&gt;) and understood that the HoltWinters model is one instance of the state space models implemented in ets. Unfortunately, I could not use ets so far since it complains about the high data frequency. I definitely need daily forecast (on the order of 30-60 steps).&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;fit &amp;lt;- ets(x, 'AAA')&#10;Error in ets(x, &quot;AAA&quot;) : Frequency too high&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Why can HoltWinters deal with this but not ets? Is there a good workaround? I have the same problem for seasonal ARIMA models and considered splitting up the data in years and using past years as exogenous input.&lt;/p&gt;&#10;&#10;&lt;p&gt;On a side note: How do you usually deal with leap days that screw up your 365 day period? Simply delete them?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you very much!&lt;/p&gt;&#10;&#10;&lt;p&gt;PS: I am aware of this: &lt;a href=&quot;http://robjhyndman.com/researchtips/longseasonality/&quot; rel=&quot;nofollow&quot;&gt;http://robjhyndman.com/researchtips/longseasonality/&lt;/a&gt;&#10;However, I couldn't get it too work well on my data, yet. On the other hand, HoltWinters worked fairly well.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Thanks for all the helpful comments and discussion. &#10;I uploaded the data at &lt;a href=&quot;http://timalthoff.de/data/data.zip&quot; rel=&quot;nofollow&quot;&gt;http://timalthoff.de/data/data.zip&lt;/a&gt; &#10;The plot below shows Super_bowl.dat.&lt;/p&gt;&#10;&#10;&lt;p&gt;I took the liberty of including more time series if you'd like to check out more examples.&lt;/p&gt;&#10;&#10;&lt;p&gt;At certain points in time I want to forecast the time series on the order of 60 days. These points in time usually are on the left flank of a big spike that represents a sudden interest in a topic. See example.png for an example (the vertical red lines are these points in time to start an out-of-sample forecast). For more info check out the README.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/iKy6A.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-01-22T14:46:24.277" FavoriteCount="4" Id="48253" LastActivityDate="2013-01-25T10:12:35.513" LastEditDate="2013-01-23T16:48:52.053" LastEditorUserId="919" OwnerUserId="19918" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;time-series&gt;&lt;forecast&gt;" Title="Time series forecast in R with yearly frequency" ViewCount="2550" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Please consider this data:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;dt.m &amp;lt;- structure(list(id = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12), occasion = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), .Label = c(&quot;g1&quot;, &quot;g2&quot;), class = &quot;factor&quot;),     g = c(12, 8, 22, 10, 10, 6, 8, 4, 14, 6, 2, 22, 12, 7, 24, 14, 8, 4, 5, 6, 14, 5, 5, 16)), .Names = c(&quot;id&quot;, &quot;occasion&quot;, &quot;g&quot;), row.names = c(NA, -24L), class = &quot;data.frame&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;We fit a simple variance components model. In R we have:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;require(lme4)&#10;fit.vc &amp;lt;- lmer( g ~ (1|id), data=dt.m )&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Then we produce a caterpillar plot:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;rr1 &amp;lt;- ranef(fit.vc, postVar = TRUE)&#10;dotplot(rr1, scales = list(x = list(relation = 'free')))[[&quot;id&quot;]]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/fyV1H.jpg&quot; alt=&quot;Caterpillar plot from R&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Now we fit the same model in Stata. First write to Stata format from R:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;require(foreign)&#10;write.dta(dt.m, &quot;dt.m.dta&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In Stata&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;use &quot;dt.m.dta&quot;&#10;xtmixed g || id:, reml variance&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The output agrees with the R output (neither shown), and we attempt to produce the same caterpillar plot:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;predict u_plus_e, residuals&#10;predict u, reffects&#10;gen e = u_plus_e – u&#10;predict u_se, reses&#10;&#10;egen tag = tag(id)&#10;sort u&#10;gen u_rank = sum(tag)&#10;&#10;serrbar u u_se u_rank if tag==1, scale(1.96) yline(0)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/hQKEP.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Clearty Stata is using a different standard error to R. In fact Stata is using 2.13 whereas R is using 1.32.&lt;/p&gt;&#10;&#10;&lt;p&gt;From what I can tell, the 1.32 in R is coming from&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; sqrt(attr(ranef(fit.vc, postVar = TRUE)[[1]], &quot;postVar&quot;)[1, , ])&#10; [1] 1.319977 1.319977 1.319977 1.319977 1.319977 1.319977 1.319977 1.319977 1.319977 1.319977 1.319977 1.319977&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;though I can't say I really understand what this is doing. Can someone explain ?&lt;/p&gt;&#10;&#10;&lt;p&gt;And I have no idea where the 2.13 from Stata is coming from, except that, if I change the estimation method to maximum likelihood:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;xtmixed g || id:, ml variance&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;....then it seems to use 1.32 as the standard error and produce the same results as R....&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/apbDd.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;.... but then the estimate for the random effect variance no longer agrees with R (35.04 vs 31.97).&lt;/p&gt;&#10;&#10;&lt;p&gt;So it seems to have something to do with ML vs REML: If I run REML in both systems, the model output agrees but the standard errors used in the caterpillar plots don't agree, whereas if I run REML in R and ML in Stata, the caterpillar plots agree, but the model estimates do not.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can anyone explain what is going on ?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-01-22T14:49:09.557" FavoriteCount="2" Id="48254" LastActivityDate="2013-07-05T00:10:19.430" LastEditDate="2013-07-05T00:10:19.430" LastEditorUserId="22047" OwnerUserId="7486" PostTypeId="1" Score="14" Tags="&lt;r&gt;&lt;mixed-model&gt;&lt;stata&gt;&lt;lmer&gt;" Title="Standard error of random effects in R (lme4) vs Stata (xtmixed)" ViewCount="2603" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I am working on a study where I have a sample of 86 students. I have tests for 5th grade, 6th grade, 7th grade, and 8th grade. I think I can run a Repeated Measures ANOVA since I have observations for 4 different occasions. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, I am unsure what my independent and dependent variables are in this set up.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-01-23T00:49:47.840" Id="48310" LastActivityDate="2013-02-22T10:58:58.800" LastEditDate="2013-02-22T10:58:58.800" LastEditorUserId="930" OwnerUserId="19953" PostTypeId="1" Score="0" Tags="&lt;anova&gt;&lt;repeated-measures&gt;" Title="Independent and dependent variables in Repeated Measures ANOVA" ViewCount="284" />
  
  <row Body="&lt;p&gt;You should definitely consider the Stats courses on Udacity. &lt;/p&gt;&#10;&#10;&lt;p&gt;ST101: &lt;a href=&quot;http://www.udacity.com/overview/Course/st101/CourseRev/1&quot; rel=&quot;nofollow&quot;&gt;http://www.udacity.com/overview/Course/st101/CourseRev/1&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;There is also a ST095 course on Udacity, which I would have linked to but the link was blocked in this answer. Replace the 101 in the above link with 095.&lt;/p&gt;&#10;&#10;&lt;p&gt;EdX, which is a consortium between Harvard, MIT and Berkeley also has a Stats course from Berkeley&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.edx.org/courses/BerkeleyX/Stat2.1x/2013_Spring/about&quot; rel=&quot;nofollow&quot;&gt;http://www.edx.org/courses/BerkeleyX/Stat2.1x/2013_Spring/about&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Hope this helps.&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2013-01-23T10:05:12.513" CreationDate="2013-01-23T10:05:12.513" Id="48330" LastActivityDate="2013-01-23T10:05:12.513" OwnerUserId="17937" ParentId="1761" PostTypeId="2" Score="1" />
  
  <row AnswerCount="6" Body="&lt;p&gt;I am hoping you can give me some suggestions. I am teaching in a very diverse (made of minority groups) college and the students are mostly Psychology majors. Most students are fresh from high school but some of them are older returning students above 40. Most of the students have motivational problems and aversion to math. But I am still looking for a book that covers the basic curriculum: from descriptive to sampling and testing all the way to ANOVA, and all in the context of experimental methods. The department requires me to use SPSS in class, but I like the idea of building the analysis in a spreadsheet such as excel.&lt;/p&gt;&#10;&#10;&lt;p&gt;p.s. the other teachers use a book that I don't like because of the extensive reliance on computational formulae. I find using these computational formulas - rather than the more intuitive and computationally intensive formula that is consistent with the rational and basic algorithm- unintuitive, unnecessary and confusing. This is the book I refer to Essentials of Statistics for the Behavioral Sciences, 7th Edition Frederick J Gravetter State University of New York, Brockport Larry B. Wallnau State University of New York, Brockport ISBN-10: 049581220X&#10;Thank you for reading!&lt;/p&gt;&#10;" CommentCount="2" CommunityOwnedDate="2013-01-23T15:00:13.770" CreationDate="2013-01-23T14:29:18.140" FavoriteCount="4" Id="48347" LastActivityDate="2014-09-15T17:40:37.500" OwnerUserId="1084" PostTypeId="1" Score="3" Tags="&lt;references&gt;&lt;teaching&gt;" Title="Any suggestions for a good undergraduate introductory textbook to statistics?" ViewCount="3484" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I know there must be a way to figure out this ANOVA but I could use some help.  I have qualitative data which I coded into 6 categories.  I want to run a one way ANOVA with this data but my categories are listed in 2 columns.  Is there a way I can run an ANOVA and pull the data from the 2 columns.  I only know how to run the ANOVA if that data is one column.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-01-23T22:07:46.520" Id="48386" LastActivityDate="2013-01-23T22:44:43.097" LastEditDate="2013-01-23T22:44:43.097" LastEditorUserId="930" OwnerDisplayName="user20002" PostTypeId="1" Score="0" Tags="&lt;anova&gt;" Title="How to carry out one-way ANOVA with qualitative data?" ViewCount="378" />
