  
  
  
  
  
  <row Body="&lt;p&gt;An approach that almost works is as follows: Note that $\left(\vec{b}^{\top}\vec{b} - \vec{b}^{\top}\vec{a}\right) / \sqrt{\vec{b}^{\top}\vec{b}}$ 'looks like' $\vec{z}^{\top} \vec{c}$, where $\vec{c}$ is a unit-length vector (it is actually $\vec{b}$ scaled to unit length), and $\vec{z} = \vec{b} - \vec{a} \sim \mathcal{N}\left(0,I\right)$. If it were the case that $\vec{c}$ were independent of $\vec{z}$, then one could claim that $\vec{b}^{\top}\vec{b} + Z_{\alpha} \sqrt{\vec{b}^{\top}\vec{b}}$ was a $\alpha$ confidence bound, where $Z_{\alpha}$ is the $\alpha$ quantile of the normal. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, $\vec{c}$ is &lt;em&gt;not&lt;/em&gt; independent of $\vec{z}$. It tends to be 'aligned with' $\vec{z}$. Now, when $\vec{a}^{\top}\vec{a} \gg 1$, $\vec{c}$ is essentially independent, and the confidence bound above gives proper coverage. When $0 &amp;lt; \vec{a}^{\top}\vec{a} \ll 1$, however, $\vec{z}^{\top}\vec{c}$ is more like a shifted, scaled, non-central chi-square random variable. &lt;/p&gt;&#10;&#10;&lt;p&gt;A little R simulation shows the effects of $\vec{a}^{\top}\vec{a}$ on normality of the quantity $\left(\vec{b}^{\top}\vec{b} - \vec{b}^{\top}\vec{a}\right) / \sqrt{\vec{b}^{\top}\vec{b}}$:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;z.sim &amp;lt;- function(p,eff.size,nsim=1e5) {&#10;    a &amp;lt;- matrix(eff.size * rnorm(p),nrow=p)&#10;    b &amp;lt;- rep(a,nsim) + matrix(rnorm(p*nsim),nrow=p)&#10;    atb &amp;lt;- as.matrix(t(a) %*% b)&#10;    btb &amp;lt;- matrix(colSums(b * b),nrow=1)&#10;    isZ &amp;lt;- (btb - atb) / sqrt(btb)&#10;}&#10;&#10;set.seed(99) &#10;isZ &amp;lt;- z.sim(6,1e3)&#10;jpeg(&quot;isZ.jpg&quot;)&#10;qqnorm(isZ)&#10;qqline(isZ)&#10;dev.off()&#10;&#10;jpeg(&quot;isChi.jpg&quot;)&#10;isZ &amp;lt;- z.sim(6,1e-3)&#10;qqnorm(isZ)&#10;qqline(isZ)&#10;dev.off()&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/1noSx.jpg&quot; alt=&quot;a&amp;#39;a large case&quot;&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/y3yz0.jpg&quot; alt=&quot;a&amp;#39;a small case&quot;&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-09-08T23:56:50.813" Id="114765" LastActivityDate="2014-09-08T23:56:50.813" OwnerUserId="795" ParentId="114411" PostTypeId="2" Score="0" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I apologize for the basic question. If $\{p_\theta(x): \theta\in K\subseteq\mathbb{R}\}$ is a smooth family of distributions, then the MLE $\hat{\theta}_n,$ under suitable regularity conditions satisfies $\sqrt{n}(\hat{\theta}_n-\theta)\to\mathcal{N}(0,I(\theta)^{-1})$ where $I(\theta)$ is the Fisher information. We also know from the Cramer-Rao bound that no estimator can outperform this estimator in the sense of having a smaller mean squared error (except on a set of zero Lebesgue measure, the superefficiency phenomenon). &lt;/p&gt;&#10;&#10;&lt;p&gt;I am interested in knowing similar results for other families of distributions which do not satisfy the above conditions. The canonical examples are:&#10;$\{\mathrm{Uniform}[\theta,\theta+1]: \theta\in\mathbb{R}\}$ and $\{\mathrm{Uniform}[0,\theta]: \theta&amp;gt;0\}.$ In these cases, the MLE estimates the parameter to within $O(1/n),$ since in the former case, $\hat{\theta}_n = \min\{X_1,X_2,\ldots, X_n\}$ and $n(\theta-\hat{\theta}_n)\to -Z,$ where $Z\sim\mathrm{exp}(1)$ and in the latter case, $\hat{\theta}_n = \max\{X_1,X_2,\ldots, X_n\}$ and $n(\theta-\hat{\theta}_n)\to W,$ where $W\sim\mathrm{exp}(\frac{1}{\theta}).$&lt;/p&gt;&#10;&#10;&lt;p&gt;Do we know &lt;/p&gt;&#10;&#10;&lt;p&gt;(1) If MLE is always an order-optimal estimator in the sense that it estimates $\theta$ to $O(\frac{1}{n^\alpha})$ where $\alpha$ is the best possible over all estimators?&lt;/p&gt;&#10;&#10;&lt;p&gt;(2) If MLE is not just order-optimal but optimal in a suitable sense similar to that of the Cramer-Rao lower bound, in that, any other estimator can outperform MLE only on a set of Lebesgue measure 0?&lt;/p&gt;&#10;&#10;&lt;p&gt;(3) Any general results on the distribution of convergence of $n^\alpha(\theta-\hat{\theta}_n)$ where $O\left(\frac{1}{n^\alpha}\right)$ is the rate of the convergence of the estimator?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-09T02:54:37.337" Id="114778" LastActivityDate="2014-09-09T02:54:37.337" OwnerUserId="55169" PostTypeId="1" Score="0" Tags="&lt;maximum-likelihood&gt;" Title="Question on MLE" ViewCount="41" />
  <row AcceptedAnswerId="114820" AnswerCount="1" Body="&lt;p&gt;So I'm trying to simulate one winner for each of the 4 NFL divisions, &lt;strong&gt;assuming that all teams are of equal ability&lt;/strong&gt;. It's clear that the probability of each team being the division winner is $1/4$.&lt;/p&gt;&#10;&#10;&lt;p&gt;I wonder if the following simulation strategy works. Instead of simulating each division, I randomly rank all 16 teams from 1 to 16, then compare them in group of 4. I did run the code, and the probability does converge to .25, but it takes 100,000 simulations to even get kinda close.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thus, I'm unsure whether the proposed simulation strategy is valid or not?&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;one_simu &amp;lt;- function() {&#10;  rank &amp;lt;- sample(1:16, replace=F)&#10;  groups &amp;lt;- split(rank, ceiling(seq_along(rank)/4)) # Split into 4 groups of 4&#10;  result &amp;lt;- rep(0, 16)&#10;  for (i in seq_along(rank)) {&#10;    group_of_i &amp;lt;- groups[[ceiling(i/4)] ] # The group that team 1 belongs to&#10;    if (rank[i] == max(group_of_i)) {&#10;      result[i] &amp;lt;- result[i] + 1  &#10;    }&#10;  } &#10;  return(result)&#10;}&#10;&#10;apply(replicate(1000, one_simu()), 1, mean)&#10;apply(replicate(100000, one_simu()), 1, mean)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2014-09-09T03:08:07.400" Id="114779" LastActivityDate="2014-09-09T17:05:06.813" LastEditDate="2014-09-09T14:31:26.637" LastEditorUserId="20148" OwnerUserId="20148" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;simulation&gt;" Title="What's the correct way to simulate a division winner?" ViewCount="48" />
  <row Body="&lt;p&gt;Even though I tend to say 'normal' more often (since that's what I was taught when first learning), I think &quot;Gaussian&quot; is a better choice, as long as students/readers are quite familiar with both terms:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;The normal isn't particularly typical, so the name is itself misleading. It certainly plays an important role (not least because of the CLT), but observed data is much less often particularly near Gaussian than is sometimes suggested.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The word (and associated words like &quot;normalize&quot;) has several meanings that can be relevant in statistics (consider &quot;orthonormal basis&quot; for example). If someone says &quot;I normalized my sample&quot; I can't tell for sure if they transformed to normality, computed z-scores, scaled the vector to unit length, to length $\sqrt{n}$, or a number of other possibilities. If we tended to call the distribution &quot;Gaussian&quot; at least the first option is eliminated and something more descriptive replaces it.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Gauss at least has a reasonable degree of claim to the distribution. &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="10" CreationDate="2014-09-09T03:36:27.800" Id="114783" LastActivityDate="2014-09-09T03:43:09.867" LastEditDate="2014-09-09T03:43:09.867" LastEditorUserId="805" OwnerUserId="805" ParentId="114763" PostTypeId="2" Score="40" />
  
  
  
  <row Body="&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Mode_%28statistics%29&quot; rel=&quot;nofollow&quot;&gt;The mode&lt;/a&gt; is bounded within 3^(1/2) multiple of the standard deviation around the mean for unimodal continuous distributions.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-09T09:11:10.670" Id="114804" LastActivityDate="2014-09-09T09:11:10.670" OwnerUserId="36545" ParentId="82673" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="114866" AnswerCount="1" Body="&lt;p&gt;I'm looking for a technique that would help me rank individuals according to 3 scores A, B and C (each representing a separate score in 3 specific fields).&lt;/p&gt;&#10;&#10;&lt;p&gt;A, B and C are all normalized value between 0 and 1, where 0 is the worst possible score, and 1 is the best possible score. I could simply average the three variables, except the probability densities of A, B and C are different and unknown.&lt;/p&gt;&#10;&#10;&lt;p&gt;My level of understanding of statistics is just enough to feel that averaging is probably a terrible idea.&lt;/p&gt;&#10;&#10;&lt;p&gt;What technique, if any, could help me order my individuals from best to worst? Do I need to know the probability density functions of A, B and C to do so?&lt;/p&gt;&#10;&#10;&lt;p&gt;Any help and/or reading advice is appreciated.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-09-09T14:51:33.287" Id="114843" LastActivityDate="2014-09-09T18:00:05.230" LastEditDate="2014-09-09T14:53:33.063" LastEditorUserId="919" OwnerUserId="35467" PostTypeId="1" Score="1" Tags="&lt;ranking&gt;&lt;valuation&gt;" Title="Rank individuals according to 3 different scores" ViewCount="33" />
  
  <row Body="&lt;p&gt;Regarding &lt;strong&gt;&quot;I would expect that the number and depth of the local minima would decrease as the sample size increases&quot;&lt;/strong&gt;, this is not true in general.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, let $X_1,\dots,X_n$ be a random sample from the $k$-component mixture&#10;$$&#10;  w_1\cdot\mathrm{N}(\mu_1,\sigma_1^2) + \dots + w_k\cdot\mathrm{N}(\mu_k,\sigma_k^2) \, ,&#10;$$&#10;in which $w_i\geq 0$ and $\sum_{i=1}^k w_i=1$. Define $\theta_i=(w_i,\mu_i,\sigma_i^2)$, for $i=1\dots,k$, and $\theta=(\theta_1,\dots,\theta_k)$, and let $x=(x_1,\dots,x_n)$. The likelihood function is&#10;$$&#10; L_x(\theta) = \prod_{i=1}^n \sum_{j=1}^k w_j\cdot\frac{1}{\sqrt{2\pi}\sigma_j} e^{-(x_i-\mu_j)^2/2\sigma_j^2} \, .&#10;$$&#10;Since for any permutation $\tau:\{1,\dots,k\}\xrightarrow{\rm 1:1}\{1,\dots,k\}$ we have&#10;$$&#10;  L_x(\theta_1,\dots,\theta_k) = L_x(\theta_{\tau(1)},\dots,\theta_{\tau(k)}) \, ,&#10;$$&#10;for this model the likelihood has at least $k!$ symmetric modes, no matter how large the sample size $n$ is.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-09-09T17:42:05.920" Id="114864" LastActivityDate="2014-12-25T02:37:01.780" LastEditDate="2014-12-25T02:37:01.780" LastEditorUserId="9394" OwnerUserId="9394" ParentId="114856" PostTypeId="2" Score="3" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Let$ X_1, ...,X_n$ be a random sample feom a distribution $Geometric(\theta)$ for $0&amp;lt;\theta&amp;lt;1$. I.e,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$p_{\theta}(x)=\theta(1-\theta)^{x-1} I_{\{1,2,...\}}(x)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Find the unbiased estimator with minimum variance for $g(\theta)=\frac{1}{\theta}$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Myattempt:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Since the geometric distributionis from the exponential family, the statistics $$\sum X_i $$ is complete and sufficient for $ \theta$. Also, if $$T(X)=X_1$$ is an estimatpr for $g(\theta)$, it is unbiased. Therefore, by the Rao-Blackwell theorem and Lehmann-Scheffé Theorem,&#10;$$W(X) = E[X_1|\sum X_i]$$&#10;Is the estimator we are looking for.&lt;/p&gt;&#10;&#10;&lt;p&gt;We have the following:&lt;/p&gt;&#10;&#10;&lt;p&gt;$W(X) = i \sum_{i=1}^t P(X_1=i|\sum X_i =t) = \sum_{i=1}^t i \frac{P(\sum_{i \geq 2} X_i &#10;=t-i)P(X_1=i)}{P(\sum_{i \geq 1}X_i =t)}$&lt;/p&gt;&#10;&#10;&lt;p&gt;Since the variables are iid geometric, the sums distributions are both negative binomials. But i am having troubles tosimplify the binomial coefficients and give a final answer with a better form, if it is possible.I wpuld be glad if I could get some help.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; I dont think you guys understand my doubt: Ithink I made all the correct steps, maybe only forgot some indicator function. Here is what I did:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$...=\sum_{i=1}^ti\frac{\binom{t-i-1}{n-2}\theta^{n-i}(1-\theta)^{t-i-n+1} \theta(1-\theta)^{i-1}}{\binom{t-1}{n-1}\theta^n(1-\theta)^{t-n}}=\sum_{i=1}^t i \frac{\binom{t-i-1}{n-2}}{\binom{t-1}{n-1}}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;As i said, I am having troubles to simplify this and with the somatory index&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-09-09T18:32:03.427" Id="114874" LastActivityDate="2014-09-10T10:14:58.323" LastEditDate="2014-09-10T10:14:58.323" LastEditorUserId="35224" OwnerUserId="35224" PostTypeId="1" Score="3" Tags="&lt;self-study&gt;&lt;estimation&gt;&lt;inference&gt;&lt;unbiased-estimator&gt;" Title="Unbiased estimator with minimum variance for $1/\theta$" ViewCount="83" />
  <row AnswerCount="0" Body="&lt;p&gt;Since PCA is sensitive to scaling, I am thinking of standardizing each explanatory variable to have mean 0 and standard deviation 1. What are the drawbacks of such a standardization?&#10;Thank you.&lt;/p&gt;&#10;" ClosedDate="2014-09-19T14:18:34.903" CommentCount="2" CreationDate="2014-09-09T23:47:30.443" Id="114906" LastActivityDate="2014-09-09T23:47:30.443" OwnerUserId="55447" PostTypeId="1" Score="0" Tags="&lt;pca&gt;" Title="Standardizing data in Principal Component Analysis?" ViewCount="33" />
  
  <row Body="&lt;p&gt;&lt;em&gt;I will switch notation to something more familiar. I hope it is not confusing.&lt;/em&gt;  &lt;/p&gt;&#10;&#10;&lt;p&gt;I don't see how one could estimate the $c$-function with a completely unbiased estimator. But I will provide an unbiased estimator for &quot;part&quot; of the $c$-function, and provide a formula for the remaining bias, so that it can be assessed by simulation.&lt;/p&gt;&#10;&#10;&lt;p&gt;We assume that we have a jointly normal $p$-dimensional random (column) vector &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\mathbf x \sim N\left (\mathbf μ, \frac 1n \mathbf I_p\right),\;\;\;\mathbf μ = (\mu_1,...,\mu_p)'$$&lt;/p&gt;&#10;&#10;&lt;p&gt;By the specification of the covariance matrix, the elements of the random vector are independent.&lt;/p&gt;&#10;&#10;&lt;p&gt;We are interested in the univariate random variable $Y = \mathbf x'\mathbf μ$. Due to joint normality, this variable has also a normal distribution&lt;/p&gt;&#10;&#10;&lt;p&gt;$$Y\sim N\left(\mathbf μ'\mathbf μ, \frac 1n \mathbf μ'\mathbf μ\right)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Therefore&lt;/p&gt;&#10;&#10;&lt;p&gt;$$P\left(\sqrt n\frac {Y-\mathbf μ'\mathbf μ}{\sqrt {\mathbf μ'\mathbf μ}} \leq \sqrt n\frac {c-\mathbf μ'\mathbf μ}{\sqrt {\mathbf μ'\mathbf μ}}\right)=\Phi\left(\sqrt n\frac {c-\mathbf μ'\mathbf μ}{\sqrt {\mathbf μ'\mathbf μ}}\right)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\Phi()$ is the standard normal CDF, and &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\Phi\left(\sqrt n\frac {c-\mathbf μ'\mathbf μ}{\sqrt {\mathbf μ'\mathbf μ}}\right) = \alpha \Rightarrow \sqrt n\frac {c-\mathbf μ'\mathbf μ}{\sqrt {\mathbf μ'\mathbf μ}} = \Phi^{-1}(\alpha)=z_{\alpha} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\Rightarrow c = \frac {\sqrt {\mathbf μ'\mathbf μ}}{\sqrt n} z_a + \mathbf μ'\mathbf μ \tag{1}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;We need therefore to obtain estimates for $\mathbf μ'\mathbf μ$ and its square root. &#10;For each element of the vector $\mathbf x$, say $X_k$ we have $n$ available i.i.d. observations, $\{x_{k1},...,x_{kn}\}$. So for each element of $\mathbf μ'\mathbf μ = (\mu_1^2,...,\mu_p^2)'$ let's try the estimator&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \text{Est}(\mu_k^2) = \frac 1n\sum_{i=1}^nX^2_{ki}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;This estimator has expected value &lt;/p&gt;&#10;&#10;&lt;p&gt;$$E\left(\frac 1n\sum_{i=1}^nX^2_{ki}\right) = \frac 1n \sum_{i=1}^nE(X^2_{ki}) =\frac 1n \sum_{i=1}^n\left(\text{Var}(X_{ki})+[E(X_{ki})]^2\right)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\Rightarrow E\left(\hat {\mu_k^2}\right) = \frac 1n\sum_{i=1}^n\left(\frac 1n+\mu_k^2\right) = \frac 1{n} + \mu_k^2$$&lt;/p&gt;&#10;&#10;&lt;p&gt;So an &lt;em&gt;unbiased&lt;/em&gt; estimator for $\mu_{ki}^2 $ is &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\hat {\mu_k^2} = \frac 1n\sum_{i=1}^nX^2_{ki} -\frac 1{n}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;implying  that&lt;/p&gt;&#10;&#10;&lt;p&gt;$$E\left[\sum_{k=1}^p\left(\frac 1n\sum_{i=1}^nX^2_{ki} -\frac 1{n}\right)\right] =\frac 1n E\left(\sum_{k=1}^p\sum_{i=1}^nX^2_{ki}\right) -\frac p{n} =\mathbf μ'\mathbf μ$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and so that &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\hat \theta \equiv \frac 1n\sum_{k=1}^p\sum_{i=1}^nX^2_{ki} -\frac p{n} \tag{2}$$&#10;is an unbiased estimator of $\mathbf μ'\mathbf μ$.&lt;/p&gt;&#10;&#10;&lt;p&gt;But an unbiased estimator for $\sqrt {\mathbf μ'\mathbf μ}$ does not seem to exist (one that is solely based on the known quantities, that is).&lt;/p&gt;&#10;&#10;&lt;p&gt;So assume that we go on and estimate $c$ by&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \hat c = \frac {\sqrt {\hat \theta}}{\sqrt n} z_a + \hat \theta \tag{3}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The bias of this estimator is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$B(\hat c) = E(\hat c - c) = \frac {z_{\alpha}}{\sqrt n}\cdot \left[E\left(\sqrt {\hat \theta}\right) - \sqrt {\mathbf μ'\mathbf μ}\right] &amp;gt;0$$&lt;/p&gt;&#10;&#10;&lt;p&gt;the &quot;positive bias&quot; result due to Jensen's Inequality. &lt;/p&gt;&#10;&#10;&lt;p&gt;In this approach, the size $n$ of the sample is critical, since it reduces bias for any given value of $\mathbf μ$.  &lt;/p&gt;&#10;&#10;&lt;p&gt;What are the consequences of this overestimation bias? Assume that we are given $n$,$p$, and we are told to calculate the critical value for $Y$ for probability $\alpha$, $P(Y\leq c) = \alpha$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Given a sequence of samples, we will provide an estimate $\hat c$ for which, &quot;on average&quot; $\hat c &amp;gt; c$. &lt;/p&gt;&#10;&#10;&lt;p&gt;In other words&lt;/p&gt;&#10;&#10;&lt;p&gt;$$P(Y\leq E(\hat c)) = \alpha^* &amp;gt; \alpha = P(Y\leq c)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;One could assess by simulation the magnitude of the bias for various values of $\mathbf μ$, and how, and how much, it distorts results.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-09-10T00:41:57.180" Id="114910" LastActivityDate="2014-09-10T00:41:57.180" OwnerUserId="28746" ParentId="114411" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;Why there is difference between lowess smoother plot and regression plot in the following plot?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/z5g3A.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I run packages: &quot;datasets&quot; &quot;utils&quot; &quot;graphics&quot; &quot;stats&quot; &quot;methods&quot; in RStudio.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-09-10T01:00:15.410" Id="114911" LastActivityDate="2014-09-10T03:51:27.837" LastEditDate="2014-09-10T03:51:27.837" LastEditorUserId="24587" OwnerUserId="24587" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;scatterplot&gt;" Title="Difference between lowess and regression plot" ViewCount="48" />
  
  
  
  
  
  
  
  <row Body="&lt;p&gt;You said &quot;...I have developed a new predictor (binary) for a disease....&quot; I am not quite sure what you mean by that. We do not develop predictors like vaccines. May be you identified a hitherto-unrecognized predictor. In any case, you have stated that there are already established predictors for the disease of interest. And you want to determine the unique predictive influence of the 'new predictor' given the already established predictors for the disease. I feel here block entry logistic regression would be very appropriate. Enter the already known predictors in the first block and enter the 'new predictor' in the second block.   &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-10T10:59:55.937" Id="114957" LastActivityDate="2014-09-10T10:59:55.937" OwnerUserId="52898" ParentId="114868" PostTypeId="2" Score="0" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Quasi-likelihood seems like a great way to use Iteratively Weighted Least Squares to fit linear linear models with a very general class of likelihoods.&lt;/p&gt;&#10;&#10;&lt;p&gt;But what is that class? Obviously the distribution must have a finite mean and a variance that can be expressed as a function of that mean. Are there cases when such a &quot;variance function&quot; doesn't exist? What other restrictions must the likelihood satisfy for this method to be valid?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-09-10T14:29:24.797" Id="114982" LastActivityDate="2014-09-19T18:55:09.287" OwnerUserId="36229" PostTypeId="1" Score="2" Tags="&lt;distributions&gt;&lt;generalized-linear-model&gt;&lt;maximum-likelihood&gt;&lt;quasi-likelihood&gt;" Title="What properties of a likelihood function are required for quasi-likelihood estimation?" ViewCount="82" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a small dataset (n=74) with a +/- 50 variables, not the best data but I have to work with it. The variables are used to select a product. I want to determine which variable or variables is/are the most &quot;efficient&quot; to predict the outcome. The dependent (Y) has two values (not selected=0; selected=1), the data suffers from a multicollinearity (Ridge log. regression is no option, to many variables, small dataset).&lt;/p&gt;&#10;&#10;&lt;p&gt;To my knowledge there are &lt;em&gt;no&lt;/em&gt; statistic tools/techniques that can deal with this, so I thought predict the conditional probability for the different combinations of variables. For example:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;    P(Y=1|X_1=1 \cap X_2=1 \cap X_3=0)  &#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Q1) is there a better way given the problems of dataset to select the most efficient  variabels? &lt;/p&gt;&#10;&#10;&lt;p&gt;and &lt;/p&gt;&#10;&#10;&lt;p&gt;Q2) is there an R-package that can handle the example P(y=1|etc.))?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-10T14:58:13.300" FavoriteCount="1" Id="114986" LastActivityDate="2014-09-12T11:35:06.640" LastEditDate="2014-09-12T11:35:06.640" LastEditorUserId="31619" OwnerUserId="31619" PostTypeId="1" Score="0" Tags="&lt;data-mining&gt;&lt;model-selection&gt;&lt;feature-selection&gt;&lt;conditional-probability&gt;" Title="Determine which variable or variables is/are the most efficient to predict the outcome" ViewCount="22" />
  
  
  
  
  
  <row Body="&lt;p&gt;EDIT: I wrote this answer for the StackOverflow post, and was only attempting to answer the coding part of his question. Obviously a LOESS line isn't ideal for his needs.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can use ggplot2 to give you smoothing line. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;libary(ggplot2)&#10;d = data.frame(DPConc= c(0, 83, 166, 416),&#10;           DPActivity=c(100, 67.71, 6.3, 16.55))&#10;ggplot(d, aes(x=DPConc, y=DPActivity)) + geom_point(size=3) + geom_smooth() + theme_minimal()&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;By default that uses a LOESS function to smooth the data points, but &lt;a href=&quot;http://docs.ggplot2.org/0.9.3.1/stat_smooth.html&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; you'll find information about other functions.&lt;/p&gt;&#10;&#10;&lt;p&gt;As others have said, go to CrossValidated for info on smoothing functions (although they may not be willing to help you interpolate 4 data points.)&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/W23Sv.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2014-09-10T18:14:12.687" Id="115021" LastActivityDate="2014-09-10T19:26:58.530" LastEditDate="2014-09-10T19:26:58.530" LastEditorUserId="40527" OwnerDisplayName="jarfa" OwnerUserId="40527" ParentId="115020" PostTypeId="2" Score="2" />
  <row AnswerCount="0" Body="&lt;p&gt;If I have a data set with daily, weekly and annually seasonality, how often should I update my forecasting model? As I have heard, forecasting models out there can have a good prediction for up to a day ahead for these types of data, hence should I update my model daily to find new values for seasonality for the upcoming periods?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-10T20:04:07.850" Id="115028" LastActivityDate="2014-09-10T20:04:07.850" OwnerUserId="55541" PostTypeId="1" Score="0" Tags="&lt;forecasting&gt;" Title="How often to update a forecasting model" ViewCount="19" />
  <row AcceptedAnswerId="115130" AnswerCount="2" Body="&lt;p&gt;I came across an article where the authors did a Principal Component Analysis on gene expression data, and found out the genes that are most correlated to the 1st principal component, and they used that gene list for further analysis.&#10;Can somebody tell me how to find out entities (genes in this case) that are most correlated to the 1st principal component?&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's &lt;a href=&quot;http://www.biomedcentral.com/1471-2164/10/135&quot; rel=&quot;nofollow&quot;&gt;the link to the original free article&lt;/a&gt;, and this is how they've calculated it:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Results of the gene set enrichment analysis of the genes most correlated to the 1st principal component. The correlations of the genes with the 1st principal component were transformed to SDs from the mean, then genes with values &gt; 1.5 (positive correlation) or &amp;lt; -1.5 (negative correlation) were selected.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I can make a toy example of 6 samples and 20 gene matrix and do the PCA the following way but how to proceed next:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;rm(list=ls())&#10;set.seed(12345)&#10;my.mat &amp;lt;- matrix(rnorm(120,0,0.5),nrow=6,byrow=TRUE)&#10;rownames(my.mat) &amp;lt;- paste(&quot;s&quot;,1:6,sep=&quot;&quot;)&#10;colnames(my.mat) &amp;lt;- paste(&quot;g&quot;,1:20,sep=&quot;&quot;)&#10;head(my.mat)&#10;&#10;#Ensure that input data is Z-transformed &#10;pca.object &amp;lt;- prcomp(my.mat,center=TRUE,scale.=TRUE)&#10;summary(pca.object)&#10;par(mfrow=c(1,2))&#10;plot(pca.object)&#10;biplot(pca.object)&#10;&#10;#The Rotation&#10;pca.object$rotation&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-09-10T20:33:25.457" Id="115032" LastActivityDate="2015-01-06T00:32:29.130" LastEditDate="2014-09-11T22:19:14.913" LastEditorUserId="28666" OwnerUserId="14137" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;pca&gt;" Title="How to find which variables are most correlated with the first principal component?" ViewCount="254" />
  <row Body="&lt;p&gt;According to the OP, &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The 'scientific logic' of mass action (which is essentially what this&#10;  is) would point to a linear or exponential relationship, and it's&#10;  definitely non-linear&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;We can start by using ggplot to test out the best model. We'll do it purely visually, by plotting the two different options. &lt;/p&gt;&#10;&#10;&lt;p&gt;First up, we need to get the data in to a data frame. We'll use the usual method:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;DP = data.frame(Activity = c(100,67.7,6.29,16.55),&#10;                Conc = c(0, 83, 166, 416))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now, we'll plot a linear fit to the data using ggplot's &lt;code&gt;geom_smooth()&lt;/code&gt; function:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;require(ggplot2)&#10;&#10;p.dp.1 &amp;lt;- ggplot(data = DP,&#10;               aes(x = Conc,&#10;                   y = Activity)) + &#10;  ylab(&quot;Activity&quot;) + &#10;  xlab(&quot;Concentration&quot;) +&#10;  geom_point() + &#10;  theme_bw(base_size = 8) +&#10;  geom_smooth(method = &quot;lm&quot;, &#10;              aes(colour = &quot;Linear&quot;)) +&#10;  scale_color_manual(name = &quot;Fits&quot;,&#10;                     breaks = c(&quot;Linear&quot;),&#10;                     values = c(&quot;blue&quot;))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Which gives us this fit:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/zDsuw.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;We can see the standard error (the grey-shaded area) is pretty large. &lt;/p&gt;&#10;&#10;&lt;p&gt;Because the OP suggested that this might be an exponential relationship, we'll now try adding a fit using an exponential. An exponential curve can be linearized by taking logs of both sides, and then doing a linear fit to the data, which would be &lt;a href=&quot;http://www.cookbook-r.com/Graphs/Scatterplots_(ggplot2)/#basic-scatterplots-with-regression-lines&quot; rel=&quot;nofollow&quot;&gt;very simple with ggplot&lt;/a&gt;. However, we have a problem; log(0) is -Inf, so we can't simply take the logs of both sides and do a linear fit. Instead, we have to use &lt;code&gt;glm()&lt;/code&gt; to do the fit, and pass it through &lt;code&gt;geom_smooth()&lt;/code&gt;. We can also take advantage of ggplot's ability to build up plots bit-by-bit, and just tack it on to the previous plot:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;p.dp.2 &amp;lt;- p.dp.1 +&#10;  geom_smooth(method = &quot;glm&quot;,&#10;              family = gaussian(link=&quot;log&quot;), &#10;              aes(colour = &quot;Exponential&quot;)) +&#10;  scale_color_manual(name = &quot;Fits&quot;,&#10;                     breaks = c(&quot;Linear&quot;,&quot;Exponential&quot;),&#10;                     values = c(&quot;red&quot;,&quot;blue&quot;))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Which gives us this:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/yK4aT.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Where we can see that the standard error (the grey bound) is much narrower than the linear fit. So, we have some justification of using the exponential, although it would still be nice to have a chemical model of activity versus concentration to base this on.&lt;/p&gt;&#10;&#10;&lt;p&gt;We now need the concentration at activity = 50. We can plot this using &lt;code&gt;geom_abline()&lt;/code&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;p.dp.3 &amp;lt;- p.dp.2 +&#10;  geom_abline(intercept = 50, slope =0,&#10;              aes(colour = &quot;Activity = 50&quot;)) +&#10;  scale_color_manual(name = &quot;Fits&quot;,&#10;                     breaks = c(&quot;Linear&quot;,&quot;Exponential&quot;,&quot;Activity = 50&quot;),&#10;                     values = c(&quot;black&quot;,&quot;red&quot;,&quot;blue&quot;))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;which gives us this:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/P0k3u.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Our best estimate for 50% activity is the intersection of the black line of activity = 50 and the best-fit line using the exponential model. We can see that this point is slightly to the right of the data point at Conc = 83, and to the left of the line of concentration = 100. &lt;/p&gt;&#10;&#10;&lt;p&gt;To get the actual intersect, we create the model for activity as a function of concentration outside of ggplot, and approximate the fit using &lt;code&gt;approx()&lt;/code&gt;. The model is set up like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;dp.model &amp;lt;- glm(formula = Activity ~ Conc,&#10;                family=gaussian(link=&quot;log&quot;),&#10;                data = DP)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Following &lt;a href=&quot;http://stackoverflow.com/questions/23957486/calibration-inverse-prediction-from-loess-object-in-r?rq=1&quot;&gt;this example&lt;/a&gt; we can get the concentration when activity is 50:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;conc.est &amp;lt;- approx(x = dp.model$fitted.values,&#10;          y = dp.model$data$Conc, xout=50)$y&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and then the concentration is...&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; conc.est&#10;89.29575&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;So, using an exponential model for activity as a function of concentration, we can estimate that concentration = 89.3 when activity = 50. We can see from the figures that the standard error associated with this estimate is huge, though. Unfortunately I can't figure out how to get the error bands for this estimate.&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2014-09-10T20:52:30.093" Id="115034" LastActivityDate="2014-09-13T03:19:04.560" LastEditDate="2014-09-13T03:19:04.560" LastEditorUserId="31314" OwnerUserId="31314" ParentId="115020" PostTypeId="2" Score="4" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Hi all I am doing a supervised classification of a binary labelled data where I have a mixture of categorical and continuous attributes. I have in total 6 attributes and nearly 6500 instances. I was wondering if there is an expected performance of a classifier based on (bias/variance) that might influence my decision to pick one classifier over another. I have missing data, noisy labels and outliers in my data. The list of classifiers I am trying to choose from are Logistic Regression, kNN, decision trees and ensemble learners. I tried weka and so far k-nn has given the best AUC after 10-fold cross-validation and testing on a new dataset. I am trying to find given the strengths and pitfalls of these classifiers if one is better over the other and why? Highly appreciate any help.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-10T22:11:03.200" Id="115043" LastActivityDate="2014-11-14T22:38:36.207" LastEditDate="2014-11-14T22:38:36.207" LastEditorUserId="7290" OwnerUserId="48918" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;" Title="What classifier to choose?" ViewCount="17" />
  <row AcceptedAnswerId="115062" AnswerCount="2" Body="&lt;p&gt;I'm using scipy and I'd like to calculate the chi-squared value of a contingency table of percentages. &lt;/p&gt;&#10;&#10;&lt;p&gt;This is my table, it's of relapse rates. I'd like to know if there are values that are unexpected, i.e. groups where relapse rates are particularly high: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;        18-25    25-34    35-44    ...&#10;Men     37%      36%      64%      ...&#10;Women   24%      25%      32%      ...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The underlying data looks like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;        18-25           25-34           35-44          ...&#10;Men     667 of 1802     759 of 2108     1073 of 1677   ...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Should I just use the raw values, so have a contingency table like this, and run a chi-squared test on the raw values?&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;        18-25        25-34        35-44      ...&#10;Men     667          759          1073       ...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;That doesn't seem quite right, because it doesn't capture the relative underlying size of each group. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have been Googling, but haven't been able to find an explanation I understand of what I should do. How should I find unexpected values in data like this?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-10T23:25:10.173" Id="115047" LastActivityDate="2014-09-11T02:38:16.657" OwnerUserId="36825" PostTypeId="1" Score="0" Tags="&lt;statistical-significance&gt;&lt;chi-squared&gt;&lt;scipy&gt;" Title="Calculating chi-squared values for percentages?" ViewCount="28" />
  
  
  <row AcceptedAnswerId="115063" AnswerCount="2" Body="&lt;p&gt;it's embarrassing that I'm asking this but I always seem to forget the correct treatment of this subject:&lt;/p&gt;&#10;&#10;&lt;p&gt;So say the question is stated as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;What is the probability of drawing 4 aces from a standard deck of 52 cards.&lt;/p&gt;&#10;&#10;&lt;p&gt;is it:&#10;   1/52 * 1/51 * 1/50 * 1/49 * 4!&lt;/p&gt;&#10;&#10;&lt;p&gt;or do I simply say well it is:&#10;  4/52 = 1/13&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-11T01:19:12.757" Id="115061" LastActivityDate="2014-09-11T07:34:02.017" OwnerUserId="14163" PostTypeId="1" Score="0" Tags="&lt;combinatorics&gt;" Title="Probability of getting 4 Aces" ViewCount="50" />
  
  <row AcceptedAnswerId="115141" AnswerCount="2" Body="&lt;p&gt;My work implies a lot of econometrics, and I had a good formation about it. Nevertheless, I am regularly faced with some semi or non parametric techniques (for instance I had to use quantile regressions, partial estimation, or nonparametric estimation of whole distribution estimations), and I had no courses about it, neither in statistics or econometrics. &lt;/p&gt;&#10;&#10;&lt;p&gt;So my question is which book would you recommend for someone to have both a good overview and a logical presentation of this domain ? I would like to have a good background for these techniques. I precise that I have a graduate level in econometrics and applied statistics, so some maths are fine, but with some intuition is very good too !&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-11T08:34:49.937" Id="115083" LastActivityDate="2014-09-11T17:44:12.623" OwnerUserId="26008" PostTypeId="1" Score="3" Tags="&lt;nonparametric&gt;&lt;references&gt;&lt;semiparametric&gt;" Title="Book for introductory nonparametric econometrics/statistics" ViewCount="90" />
  
  <row AnswerCount="1" Body="&lt;p&gt;First off, a very similar &lt;a href=&quot;http://stats.stackexchange.com/questions/10419/what-is-theta-in-a-negative-binomial-regression-fitted-with-r&quot;&gt;question has been asked before&lt;/a&gt;. But the answers to this question did not explain what high/low values of theta mean. Here's my crack at trying to figure out what high/low values of theta mean. So please don't close this question!&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's assume you've made two models: a negative binomial regression (NB) and a zero-inflated negative binomial regression (ZINB). The NB regression has a theta of 0.5 and the ZINB regression has a theta of 2. As I understand it, the higher theta in the ZINB regression indicates that more variance in the residuals has been accounted for, and therefore the negative binomial distribution that the model assumes has a more slender shape. Is this correct? Can anybody provide a more precise definition of the theta value, but without using equations?&lt;/p&gt;&#10;&#10;&lt;p&gt;I also quickly sketched a visualisation of my understanding. The residuals in the NB are more spread out, meaning the theta is smaller and the shape of the negative binomial distributions are more fat. The residuals in the ZINB are less spread out, meaning the theta is larger and the shape of the negative binomial distributions are more slender. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Fc5ez.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-09-11T09:03:06.830" FavoriteCount="1" Id="115087" LastActivityDate="2015-02-05T07:02:49.217" LastEditDate="2014-09-11T14:09:59.017" LastEditorUserId="22311" OwnerUserId="12492" PostTypeId="1" Score="0" Tags="&lt;generalized-linear-model&gt;&lt;modeling&gt;&lt;negative-binomial&gt;&lt;zero-inflated&gt;" Title="Interpretation of $\theta$ in negative binomial regression" ViewCount="279" />
  <row AcceptedAnswerId="115180" AnswerCount="2" Body="&lt;p&gt;We’ve run a mixed effects logistic regression using the following syntax;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# fit model&#10;fm0 &amp;lt;- glmer(GoalEncoding ~ 1 + Group + (1|Subject) + (1|Item), exp0,&#10;             family = binomial(link=&quot;logit&quot;))&#10;# model output&#10;summary(fm0)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Subject and Item are the random effects. We’re getting an odd result which is the coefficient and standard deviation for the subject term are both zero;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Generalized linear mixed model fit by maximum likelihood (Laplace&#10;Approximation) [glmerMod]&#10;Family: binomial  ( logit )&#10;Formula: GoalEncoding ~ 1 + Group + (1 | Subject) + (1 | Item)&#10;Data: exp0&#10;&#10;AIC      BIC      logLik deviance df.resid &#10;449.8    465.3   -220.9    441.8      356 &#10;&#10;Scaled residuals: &#10;Min     1Q Median     3Q    Max &#10;-2.115 -0.785 -0.376  0.805  2.663 &#10;&#10;Random effects:&#10;Groups  Name        Variance Std.Dev.&#10;Subject (Intercept) 0.000    0.000   &#10;Item    (Intercept) 0.801    0.895   &#10;Number of obs: 360, groups:  Subject, 30; Item, 12&#10;&#10;Fixed effects:&#10;                Estimate Std. Error z value Pr(&amp;gt;|z|)    &#10; (Intercept)     -0.0275     0.2843    -0.1     0.92    &#10; GroupGeMo.EnMo   1.2060     0.2411     5.0  5.7e-07 ***&#10; ---&#10; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1&#10;&#10; Correlation of Fixed Effects:&#10;             (Intr)&#10; GroupGM.EnM -0.002&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This should not be happening because obviously there is variation across subjects. When we run the same analysis in stata&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;xtmelogit goal group_num || _all:R.subject || _all:R.item&#10;&#10;Note: factor variables specified; option laplace assumed&#10;&#10;Refining starting values: &#10;&#10;Iteration 0:   log likelihood = -260.60631  &#10;Iteration 1:   log likelihood = -252.13724  &#10;Iteration 2:   log likelihood = -249.87663  &#10;&#10;Performing gradient-based optimization: &#10;&#10;Iteration 0:   log likelihood = -249.87663  &#10;Iteration 1:   log likelihood = -246.38421  &#10;Iteration 2:   log likelihood =  -245.2231  &#10;Iteration 3:   log likelihood = -240.28537  &#10;Iteration 4:   log likelihood = -238.67047  &#10;Iteration 5:   log likelihood = -238.65943  &#10;Iteration 6:   log likelihood = -238.65942  &#10;&#10;Mixed-effects logistic regression               Number of obs      =       450&#10;Group variable: _all                            Number of groups   =         1&#10;&#10;                                                Obs per group: min =       450&#10;                                                               avg =     450.0&#10;                                                               max =       450&#10;&#10;Integration points =   1                        Wald chi2(1)       =     22.62&#10;Log likelihood = -238.65942                     Prob &amp;gt; chi2        =    0.0000&#10;&#10;------------------------------------------------------------------------------&#10;        goal |      Coef.   Std. Err.      z    P&amp;gt;|z|     [95% Conf. Interval]&#10;-------------+----------------------------------------------------------------&#10;   group_num |   1.186594    .249484     4.76   0.000     .6976147    1.675574&#10;       _cons |  -3.419815   .8008212    -4.27   0.000    -4.989396   -1.850234&#10;------------------------------------------------------------------------------&#10;&#10;------------------------------------------------------------------------------&#10;  Random-effects Parameters  |   Estimate   Std. Err.     [95% Conf. Interval]&#10;-----------------------------+------------------------------------------------&#10;_all: Identity               |&#10;               sd(R.subject) |   7.18e-07   .3783434             0           .&#10;-----------------------------+------------------------------------------------&#10;_all: Identity               |&#10;                 sd(R.trial) |   2.462568   .6226966      1.500201    4.042286&#10;------------------------------------------------------------------------------&#10;LR test vs. logistic regression:     chi2(2) =   126.75   Prob &amp;gt; chi2 = 0.0000&#10;&#10;Note: LR test is conservative and provided only for reference.&#10;Note: log-likelihood calculations are based on the Laplacian approximation.&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;the results are as expected with a non-zero coefficient / s.e. for the Subject term.&lt;/p&gt;&#10;&#10;&lt;p&gt;Originally we thought this might be something to do with the coding of the Subject term, but changing this from a string to an integer did not make any difference.&lt;/p&gt;&#10;&#10;&lt;p&gt;Obviously the analysis is not working properly, but we are unable to pin down the source of the difficulties. (NB someone else on this forum has been experiencing a similar issue, but this thread remains unanswered &lt;a href=&quot;http://stats.stackexchange.com/questions/113610/instability-in-glmer-model-at-zero-random-effect-variance&quot;&gt;link to question&lt;/a&gt;)&lt;/p&gt;&#10;&#10;&lt;p&gt;Any comments greatly appreciated!&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-09-11T10:07:40.273" Id="115090" LastActivityDate="2014-09-12T02:12:47.677" LastEditDate="2014-09-11T18:50:51.203" LastEditorUserId="88" OwnerUserId="24109" PostTypeId="1" Score="4" Tags="&lt;mixed-effect&gt;&lt;lme4&gt;" Title="Using glmer, why is my random effect zero?" ViewCount="154" />
  
  
  <row Body="&lt;p&gt;I can't see how you could find the optimal time without using utilities and incorporating uncertainty.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-11T12:02:03.540" Id="115102" LastActivityDate="2014-09-11T12:02:03.540" OwnerUserId="4253" ParentId="115084" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;The online &lt;a href=&quot;http://www.statsoft.com/Textbook/Nonparametric-Statistics&quot; rel=&quot;nofollow&quot;&gt;StatSoft textbook&lt;/a&gt; is a good place to start. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-09-11T13:07:32.797" Id="115110" LastActivityDate="2014-09-11T13:29:41.620" LastEditDate="2014-09-11T13:29:41.620" LastEditorUserId="7290" OwnerUserId="49054" ParentId="115083" PostTypeId="2" Score="-1" />
  <row Body="&lt;p&gt;You are trying to linearize the problem. It's not possible. You have to use non-linear regression in this case.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-09-11T13:47:42.147" Id="115117" LastActivityDate="2014-09-11T13:47:42.147" OwnerUserId="36041" ParentId="115114" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Control has to be African American colon cancer patients who get placebo treatment. That is control should be as similar to the other group as possible, except that it's given a fake treatment.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-09-11T14:27:08.643" Id="115124" LastActivityDate="2014-09-11T14:27:08.643" OwnerUserId="54099" ParentId="115122" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;What your control group should be is largely a &lt;em&gt;substantive / theoretical&lt;/em&gt; question, not a statistical question.  That is, your control group should be designed to help you answer the theoretical question that you are conducting your study to answer.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I can imagine several different control groups in your case, but which you should choose is something only you can decide.  Here are some examples:  &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;strong&gt;Caucasian colon cancer patients with metastasis who are given the treatment.&lt;/strong&gt;  This design would allow you to isolate the effect of being an &lt;em&gt;African American&lt;/em&gt; (vs. Caucasian) with metastasis who is getting the treatment.  &lt;/li&gt;&#10;&lt;li&gt;&lt;strong&gt;African American colon cancer patients with metastasis who get placebo treatment.&lt;/strong&gt;  This design would allow you to isolate the effect of getting the &lt;em&gt;treatment&lt;/em&gt; (vs. not getting the treatment) for African Americans with metastasis.  (Since wondering if treatments are effective is a very common question in biomedical science, this setup is probably more typical, but that would only be because that &lt;em&gt;question&lt;/em&gt; is more typical, not because this setup is inherently correct irregardless of your question.)  &lt;/li&gt;&#10;&lt;li&gt;You could also include &lt;strong&gt;both&lt;/strong&gt; control groups.  Which would allow you to answer both questions, although at the expense of requiring more data, and the &lt;em&gt;statistical&lt;/em&gt; analyses to answer the questions would not be independent (the theoretical questions can be conceived of as independent, though).  &lt;/li&gt;&#10;&lt;li&gt;A final possibility is to have four groups: &lt;strong&gt;race (African American vs. Caucasian) X treatment (active vs. placebo)&lt;/strong&gt;.  This would allow you to answer the above two questions independently (from a statistical perspective), and would also allow you to answer if the effect of the treatment depends on race (i.e., if an interaction exists).  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;In fact there can be many more possibilities available, depending on the question you want to ask.  For instance, you could compare African Americans with metastasis on the treatment to African Americans (also with colon cancer but) without metastasis on the treatment, etc.  But in all cases, your substantive question comes first, and you design your experiment so as to best answer that question.  &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-09-11T14:42:03.350" Id="115125" LastActivityDate="2014-09-11T15:33:16.237" LastEditDate="2014-09-11T15:33:16.237" LastEditorUserId="7290" OwnerUserId="7290" ParentId="115122" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="115139" AnswerCount="1" Body="&lt;p&gt;I am regressing different dependent variables using same set of predictors for all dependent variables such as &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;y1=beta0+beta1.X1+beta2.X2+.....&#10;y2=beta0+beta1.X1+beta2.X2+.....&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Would that be multiple testing scenario? If so, how can be adjusted for it?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-09-11T15:24:15.070" Id="115129" LastActivityDate="2014-09-11T18:32:29.173" OwnerUserId="28820" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;mathematical-statistics&gt;&lt;multiple-regression&gt;&lt;multivariate-analysis&gt;&lt;multiple-comparisons&gt;" Title="Multiple testing" ViewCount="40" />
  <row AnswerCount="1" Body="&lt;p&gt;I have a data-set with 32 effect size estimates- only 11 of which report a value for the continuous moderator of interest (the samples anxiety level). A complete case analysis (restricted to the 11 cases) shows that anxiety is a significant predictor of the effect sizes in meta-regression. &lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to use imputation techniques to &quot;fill in&quot; the missing values to see if the relationship between anxiety and effect size (d) is still significant. If I do this using the &quot;mice&quot; function in R, it automatically selects d (the effect size) as a predictor to impute the plausible values of anxiety, as shown in the predictor matrix.&lt;/p&gt;&#10;&#10;&lt;p&gt;My issue is that this seems &lt;em&gt;circular&lt;/em&gt;- I already know that anxiety predicts d, so using d to predict the missing values of anxiety seems to be &quot;playing tennis without the net&quot; and will surely artificially strengthen the relationship.&lt;/p&gt;&#10;&#10;&lt;p&gt;On the other hand- I &lt;em&gt;don't know for sure&lt;/em&gt; that increased anxiety predicts d or whether increased anxiety is an outcome of the effect size- they may have a mutual influence. (d reflects a bias for threat, which literature suggests could be a &lt;em&gt;cause or consequence&lt;/em&gt; of anxiety.....). This would seem to make using d in imputation more legitimate. Also, the recommendation seems to be to use all variables that will appear in the model applied after imputation (which will of course include d as the outcome) in the imputation process (van Buuren, 1999).&lt;/p&gt;&#10;&#10;&lt;p&gt;So given this issue, should I simply remove the effect size as a predictor of anxiety for imputation and instead rely on other demographic variables, or just random sampling of the observed data? Doing this also seems wrong, since this seems to generate a false &quot;uncertainty&quot; in the imputed data, given we have an idea of the relation between d and anxiety.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Any help or references to resolve this problem would be much appreciated. Please let me know if anything is unclear.         &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-09-11T15:47:32.343" FavoriteCount="1" Id="115133" LastActivityDate="2015-01-04T22:40:09.000" OwnerUserId="21879" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;data-imputation&gt;&lt;multiple-imputation&gt;&lt;meta-regression&gt;&lt;mice&gt;" Title="Data imputation for meta analysis using mice package in R" ViewCount="92" />
  
  
  <row Body="&lt;p&gt;Yes, if I'm understanding what you mean by sensitive. You'll get greater power whenever the effect is larger relative to the variation in your measurements. So if your instrument is less variable, that increases power for the same effect size (on the scale of measurement). Or if the instrument responds to a greater degree to a stimulus, without being more variable, that also increases power.&lt;/p&gt;&#10;&#10;&lt;p&gt;All this is said with the caveat that the sample-size problem is formulated in a way that reflects the needed scientific goals and uses realistic estimates of the error SD(s).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-12T01:53:02.300" Id="115196" LastActivityDate="2014-09-12T01:53:02.300" OwnerUserId="52554" ParentId="115191" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a corpus of 2500 opinions, is it posible to use scikit´s restricted boltzmann machine implementation to extract a feature vector as a previous step to a classification task?. What aproach do i need to follow in order to use a restricted boltzman machine for text classification?, do i need to label my data to classify?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-12T04:08:13.623" Id="115204" LastActivityDate="2014-09-12T04:08:13.623" OwnerUserId="55660" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;classification&gt;&lt;feature-selection&gt;&lt;unsupervised-learning&gt;&lt;natural-language&gt;" Title="Unsupervised feature learning from raw text as a previous step for clasification?" ViewCount="17" />
  
  
  <row AcceptedAnswerId="115250" AnswerCount="1" Body="&lt;p&gt;I'm using &lt;strong&gt;gradient boosting regression model&lt;/strong&gt; (GBRT). &lt;/p&gt;&#10;&#10;&lt;p&gt;To evaluate this model, I use &lt;strong&gt;10-fold cross validation&lt;/strong&gt;, in each of which I set same param and compute the coefficient of determination as a measure of fitting.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, I find that there exists a huge difference in coefficient of determination obtained from each fold, e.g., the coefficient of determination from &lt;code&gt;fold_1&lt;/code&gt; to &lt;code&gt;fold_10&lt;/code&gt; is:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;[ 0.95310245  0.89725342  0.886711    **0.97063794**  0.84182142  0.80870443&#10;  0.70535911  0.8888032   **0.42510782**  0.70421155]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Although the mean is 0.81 and std is 0.31, there is a fold in which the coefficient of determination is 0.4, while another fold is 0.97.&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The only difference btw each fold is just the training &amp;amp; test data set, why does there exist such huge difference? Is such difference indicating that the performance of my model is not good?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-09-12T08:58:41.957" Id="115220" LastActivityDate="2014-09-12T18:40:41.447" LastEditDate="2014-09-12T18:40:41.447" LastEditorUserId="37540" OwnerUserId="55657" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;cross-validation&gt;&lt;coefficient&gt;" Title="why is there a huge difference existed in coefficient of determination obtained from 10-fold cross validation?" ViewCount="42" />
  
  <row AnswerCount="1" Body="&lt;p&gt;How can I (1) compare two linear models between years and (2) Can I compare 2 models with different response variables?&lt;/p&gt;&#10;&#10;&lt;p&gt;My data have 4 variables: y_meas, x, year, y_calc.  &quot;y_meas&quot; is a lab measured response varibale, and &quot;y_calc&quot; is an estimate of the same variable, using a standard calculation.  &quot;x &quot; is a dosage, similar(ish) between two years:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#create dataset&#10;set.seed(100)&#10;dat &amp;lt;- within(data.frame(x = rep(1:10, times=2)),&#10;                 {&#10;                   year &amp;lt;- rep(1990:1991, each = 10)&#10;                   y_meas &amp;lt;- 0.5 * x* (1:20) + rnorm(20)&#10;         y_calc &amp;lt;- 0.3 * x* (1:20) + rnorm(20)&#10;                   year &amp;lt;- factor(year)                 # convert to a factor&#10;                 }&#10;                 )&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I have two related questions:&#10;(1) Is there any difference between slope/intercept of models for 1990 and 1991?&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;m.1990&amp;lt;-lm(y_meas~x, data=subset(dat, year==1990))&#10;m.1991&amp;lt;-lm(y_meas~x, data=subset(dat, year==1991))&#10;anova(m.1990)&#10;anova(m.1991) &#10;# both models are significant&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I can't run &lt;code&gt;anova(m.1990,m.1991)&lt;/code&gt; because the models are not nested?  Do I need to use year as a dummy variable and run ANCOVA? What does this look like (roughly)?&lt;/p&gt;&#10;&#10;&lt;p&gt;(2) Assuming I can combine 1990 and 1991, can I compare the slope/intercept of 'y_meas~x' and 'y_calc~x'?   Yes, two different response variables, but are on the same scale.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-12T12:15:24.057" Id="115239" LastActivityDate="2015-02-02T05:40:22.583" OwnerUserId="55515" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;regression&gt;&lt;linear-model&gt;" Title="Compare linear regression models (same and different response variable)" ViewCount="130" />
  <row AcceptedAnswerId="115249" AnswerCount="2" Body="&lt;p&gt;Suppose four numbers $\{a,b,c,d\}$ where $a$ and $c$ random variables from a continuous distribution with support on $\mathbb{R}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Does $b\neq d$  imply $|a−b|−|c−d|+a−c\neq 0$ almost surely?&lt;/p&gt;&#10;&#10;&lt;p&gt;I really want to believe this proposition is true but that is typically not enough to make it so.&lt;/p&gt;&#10;&#10;&lt;h1&gt;Edit:&lt;/h1&gt;&#10;&#10;&lt;p&gt;MickMack's counter example made me realize I need to refine the proposition to make it: &lt;/p&gt;&#10;&#10;&lt;p&gt;Does $|b|\neq|d|$  imply $|a−b|−|c−d|+a−c\neq 0$?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-12T12:27:03.397" Id="115243" LastActivityDate="2014-09-12T13:22:24.170" LastEditDate="2014-09-12T13:22:24.170" LastEditorUserId="603" OwnerUserId="603" PostTypeId="1" Score="0" Tags="&lt;probability&gt;&lt;proof&gt;" Title="Stuck on a proposition" ViewCount="39" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm currently assessing the best option for looking at some community composition data, and determining how they differ among locations. Ultimately, as I'm simply looking to see if there is a difference depending on site (4 level factor), I was looking at Permanovas. Does anybody have any good references which explains the assumptions, such as maximum number of response variables (e.g. how large can the species composition be)?&lt;/p&gt;&#10;&#10;&lt;p&gt;Many thanks.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-12T13:50:41.587" Id="115261" LastActivityDate="2014-09-14T21:00:55.183" OwnerUserId="20973" PostTypeId="1" Score="0" Tags="&lt;multivariate-analysis&gt;&lt;assumptions&gt;&lt;permutation&gt;" Title="Rules of thumb for PERMANOVA" ViewCount="47" />
  <row AnswerCount="0" Body="&lt;p&gt;I've fit a logistic regression mixed effects model with glmer in R and I'm doing predictions with it. Given a new data that needs a probability prediction, I am interested in extracting the fixed and random effect scores that are summed up and input to the link function to obtain a probability. These contributing sub-scores help me understand and explain to others why a new sample got the score it did.&lt;/p&gt;&#10;&#10;&lt;p&gt;Currently I'm using ranef and fixef to extract the regression coefficients, then forming a loop that picks out the coefficient for each feature in the new sample whose probability I want to predict. This is a bit inelegant, especially when my predictors are interaction terms between categorical variables; in this case I have to join strings with colons to make the predictor value, e.g. &quot;John Smith:Morgan Stanley&quot;. Is there a more &quot;R&quot; way of doing this?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-12T15:03:47.990" Id="115272" LastActivityDate="2014-09-12T15:03:47.990" OwnerUserId="11646" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;regression&gt;&lt;predictive-models&gt;" Title="glmer predictions: how to extract scores that contributed" ViewCount="15" />
  <row Body="&lt;p&gt;@James makes a great point on estimating prediction intervals, but I don't think you want to say that the only thing that matters for point prediction is validating to avoid overfitting.  Some of those regression diagnostics have clear implications for improving a model's predictive performance.  One obvious case is if your model assumes a linear relationship with a predictor when the relationship is actually curvilinear then you can improve predictive performance by, for example, adding polynomial terms.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-09-12T17:48:00.103" Id="115290" LastActivityDate="2014-09-12T17:48:00.103" OwnerUserId="3748" ParentId="115266" PostTypeId="2" Score="3" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have a web crawler and i want to be able to differentiate a specific class of website (social networks), from others.&lt;/p&gt;&#10;&#10;&lt;p&gt;My problem is that my starting classified data is really small. What I wan't to do is: &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;initially train my algorithm on this data;&lt;/li&gt;&#10;&lt;li&gt;receive feedback from user (correct, wrong) on classified websites and use this to increase my data set and retrain my algorithm.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Is this feasible, or is my new data biased by having been previously already selected as a social network website by my algorithm?&lt;/p&gt;&#10;&#10;&lt;p&gt;Which supervised learning algorithm would be ideal for being retrained after acquiring new data?&lt;/p&gt;&#10;&#10;&lt;p&gt;Consider that the features of the website I was considering to give the algorithm for training was something like, number of specific html tags (like a [anchor tag], or img) and number of occurrence of some specific words. (If you have suggestions of better features I would be glad to hear also)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-12T20:23:53.803" Id="115298" LastActivityDate="2014-09-30T00:56:19.010" OwnerUserId="54582" PostTypeId="1" Score="1" Tags="&lt;classification&gt;&lt;dataset&gt;&lt;supervised-learning&gt;" Title="Supervised learning algorithm that can be easily retrained with new data" ViewCount="25" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I just ran an extended SVC gridsearch in libsvm on about 9000 multi-dimensional vectors representing a time series. Here are the highest scoring results:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;[local] 3 -7 72.4729 (best c=0.5, g=0.5, rate=76.9618)&#10;..&#10;[local] -1 -5 71.79 (best c=8.0, g=0.5, rate=77.4432)&#10;..&#10;[local] 15 -11 73.0326 (best c=2048.0, g=0.03125, rate=77.5887)&#10;..&#10;[local] -5 -3 66.0249 (best c=32768.0, g=0.03125, rate=77.723)&#10;..&#10;[local] 7 -3 77.9245 (best c=128.0, g=0.125, rate=77.9245)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Here's the contour of the cross validation accuracy:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/5JiOg.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Given the above I'm very tempted to settle on either a C value of 0.5 or 8.0 - anything above 100 seems heavily over fitted to me. Especially given the rather minuscule success rate differences.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any thoughts, insights, or pointers would be appreciated. Thanks in advance.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-09-12T20:34:34.050" Id="115300" LastActivityDate="2014-09-13T11:49:24.730" LastEditDate="2014-09-13T10:11:58.007" LastEditorUserId="54692" OwnerUserId="54692" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;svm&gt;&lt;cross-validation&gt;&lt;libsvm&gt;" Title="Selecting most realistic C and g params after gridsearch" ViewCount="65" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm having trouble proving a N.N.D matrix has a N.N.D G-Inverse.&lt;/p&gt;&#10;&#10;&lt;p&gt;So far I have:&lt;/p&gt;&#10;&#10;&lt;p&gt;If we assume x = Az where x &gt;= 0 and A is a nnd matrix.  So if Y is a G-inverse than:&lt;/p&gt;&#10;&#10;&lt;p&gt;x = Az = YAz = Yx &gt;= 0 .&lt;/p&gt;&#10;&#10;&lt;p&gt;Thus&lt;/p&gt;&#10;&#10;&lt;p&gt;Lx = LAz = Az &gt;= 0 .  So L is any G-inverse of A is also nnd. &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-09-12T22:49:11.017" Id="115313" LastActivityDate="2014-09-12T23:43:58.230" LastEditDate="2014-09-12T23:43:58.230" LastEditorUserId="55710" OwnerUserId="55710" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;self-study&gt;&lt;generalized-linear-model&gt;&lt;statistical-learning&gt;&lt;matrix-inverse&gt;" Title="A non-negative definate matrix has a non-negative generalized inverse" ViewCount="40" />
  
  <row Body="&lt;p&gt;Hint: setting $\sigma^2=1$ for convenience, we have that&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\begin{align}&#10; \frac{\mathrm d}{\mathrm dy} \exp(-(y-\mu-w)^2/2) &amp;amp;= &#10;-(y-\mu-w)\exp(-(y-\mu-w)^2/2)\\&#10;&amp;amp;= -y\exp(-(y-\mu-w)^2/2 \\&#10;&amp;amp;\qquad + (\mu+w)\exp(-(y-\mu-w)^2/2&#10;\end{align}$$&#10;and so you should be able to write the integrand as  $(\mu+w)$ times &#10;a Gaussian density less a perfect differential,&#10;and thus get the integral to work out to something involving $\Phi(\cdot)$ and an &#10;$\exp(g(w))$ term where $g(w)$ is a quadratic function of $w$.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-09-13T02:56:22.083" Id="115323" LastActivityDate="2014-09-13T02:56:22.083" OwnerUserId="6633" ParentId="115319" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;All items are not solved fully correctly in the question. I would recommend the following.&lt;/p&gt;&#10;&#10;&lt;p&gt;(0) The observations &quot;y&quot; do not need to be corrected as they are between 0 and 1 already. Applying the correction shouldn't create problems but it's not necessary either.&lt;/p&gt;&#10;&#10;&lt;p&gt;(1) cannot be answered by the likelihood ratio (LR) test. Generally in mixture models, the selection of the number of components cannot be based on the LR test because its regularity assumptions are not fulfilled. Instead, information criteria are often used and &quot;flexmix&quot; upon which betamix() is based offers AIC, BIC, and ICL. So you could choose the best BIC solution among 1, 2, 3 clusters via&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(&quot;flexmix&quot;)&#10;set.seed(0)&#10;m &amp;lt;- betamix(y ~ 1 | 1, data = d, k = 1:3)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;(2) The parameters in betamix() are not mu and phi directly but additionally link functions are employed for both parameters. The defaults are logit and log, respectively. This ensure that the parameters are in their valid ranges (0, 1) and (0, inf), respectively. One could refit the models in both components to get easier access to the links and inverse links etc. However, here it is probably easiest to apply the inverse links by hand:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;mu &amp;lt;- plogis(coef(m)[,1])&#10;phi &amp;lt;- exp(coef(m)[,2])&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This shows that the means are very different (0.25 and 0.77) while the precisions are rather similar (49.4 and 47.8). Then we can transform back to alpha and beta which gives 12.4, 37.0 and 36.7, 11.1 which is reasonably close to the original parameters in the simulation:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;a &amp;lt;- mu * phi&#10;b &amp;lt;- (1 - mu) * phi&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;(3) The clusters can be extracted using the clusters() function. This simply selects the component with the highest posterior() probability. In this case, the posterior() is really clear-cut, i.e., either close to zero or close to 1.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;cl &amp;lt;- clusters(m)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;(4) When visualizing the data with histograms, one can either visualize both components separately, i.e., each with its own density function. Or one can draw one joint histogram with the corresponding joint density. The difference is that the latter needs to factor in the different cluster sizes: the prior weights are about 1/3 and 2/3 here. The separate histograms can be drawn like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;## separate histograms for both clusters&#10;hist(subset(d, cl == 1)$y, breaks = 0:25/25, freq = FALSE,&#10;  col = hcl(0, 50, 80), main = &quot;&quot;, xlab = &quot;y&quot;, ylim = c(0, 9))&#10;&#10;hist(subset(d, cl == 2)$y, breaks = 0:25/25, freq = FALSE,&#10;  col = hcl(240, 50, 80), main = &quot;&quot;, xlab = &quot;y&quot;, ylim = c(0, 9), add = TRUE)&#10;&#10;## lines for fitted densities&#10;ys &amp;lt;- seq(0, 1, by = 0.01)&#10;lines(ys, dbeta(ys, shape1 = a[1], shape2 = b[1]),&#10;  col = hcl(0, 80, 50), lwd = 2)&#10;lines(ys, dbeta(ys, shape1 = a[2], shape2 = b[2]),&#10;  col = hcl(240, 80, 50), lwd = 2)&#10;&#10;## lines for corresponding means&#10;abline(v = mu[1], col = hcl(0, 80, 50), lty = 2, lwd = 2)&#10;abline(v = mu[2], col = hcl(240, 80, 50), lty = 2, lwd = 2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And the joint histogram:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;p &amp;lt;- prior(m$flexmix)&#10;    hist(d$y, breaks = 0:25/25, freq = FALSE,&#10;  main = &quot;&quot;, xlab = &quot;y&quot;, ylim = c(0, 4.5))&#10;lines(ys, p[1] * dbeta(ys, shape1 = a[1], shape2 = b[1]) +&#10;  p[2] * dbeta(ys, shape1 = a[2], shape2 = b[2]), lwd = 2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The resulting figure is included below.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/7dcW4.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-13T11:04:54.020" Id="115338" LastActivityDate="2014-09-13T11:04:54.020" OwnerUserId="55729" ParentId="114959" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;The y-axis is where the x (size) is 0, not where R draws a vertical line. Eye-balling your graph it seems that the line is approximately 1 when size is null, so there is no conflict between your graph and your regression output.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-13T12:38:57.853" Id="115344" LastActivityDate="2014-09-13T12:38:57.853" OwnerUserId="23853" ParentId="115343" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;First, let's try to simulate some data with a similar structure of your problem's data. My understanding from your description is that we have something like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;Say we have 4 schools with 8 subjects in each school, thus a total of 32 subjects. &#10;Each subject is posed with 3 tasks (one from each group), thus a total of 96 observations. As we have 6 different tasks, we will ask 16 times each task (6*16=96), so 32 tasks from each group (32*3=96).&lt;/p&gt;&#10;&#10;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;n.schools &amp;lt;- 2&#10;n.subject &amp;lt;- 20&#10;n         &amp;lt;- n.subject*3&#10;subject   &amp;lt;- rep(1:n.subject, 3)&#10;school    &amp;lt;- rep(1:n.schools, (n/n.schools))&#10;&#10;age    &amp;lt;- rnorm(n.subject, 30, 5)&#10;age    &amp;lt;- rep(age,3)&#10;gender &amp;lt;- sample(c(0,1),n.subject, replace=TRUE)&#10;gender &amp;lt;- rep(gender,3)&#10;&#10;A     &amp;lt;- c(rep(1,n/3),rep(0,2*n/3))&#10;B     &amp;lt;- c(rep(0,n/3),rep(1,n/3), rep(0, n/3))&#10;C     &amp;lt;- c(rep(0,2*n/3),rep(1,n/3))&#10;group &amp;lt;- c(rep(&quot;A&quot;,n/3), rep(&quot;B&quot;,n/3),rep(&quot;C&quot;,n/3))&#10;task  &amp;lt;- c(rep(&quot;a&quot;,n/6),rep(&quot;b&quot;,n/6),rep(&quot;c&quot;,n/6),rep(&quot;d&quot;,n/6),rep(&quot;e&quot;,n/6),rep(&quot;f&quot;,n/6))&#10;&#10;u.subject &amp;lt;- rnorm((n/3), 0, 1)&#10;u.school  &amp;lt;- rnorm(n.schools, 0 ,1)&#10;&#10;lattent &amp;lt;- -20 + 0.5*age + 2*gender + 2*A -3*B -3.2*C + u.subject + u.school + rnorm(n,0,1)&#10;pr      &amp;lt;- 1/(1+exp(-lattent))&#10;success &amp;lt;- rbinom(n, 1, pr)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now let's try a GLMM model as the one you propose:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(lme4)&#10;&amp;gt; glmer(success ~ age + gender + group/task + (1 + group/task | school/subject), family=binomial)&#10;Error: number of observations (=60) &amp;lt; number of random effects (=360) &#10;for term (1 + group/task | subject:school); &#10;the random-effects parameters are probably unidentifiable&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So what does this mean? In general, you need your mixed effects model to be identifiable, and one of the conditions for this to happen is $\sum_{i=1}^N (n_i-k) &amp;gt;0$, where $k$ is the number of random effects. In the case of balanced data, this can be written equivalently $N n_1 &amp;gt; N k$ or $n_1 &amp;gt; k$. In other words you need to have less random parameters than the number of observations in each cluster/group, subject in your case. This cannot be the case if you add the &lt;code&gt;group/task&lt;/code&gt; term in the random part. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;What &lt;code&gt;group/task&lt;/code&gt; or &lt;code&gt;school/subject&lt;/code&gt; means in the formula?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;This is simply an equivalent way of writing &lt;code&gt;group + group:task&lt;/code&gt; so you actually have &lt;code&gt;group&lt;/code&gt; and its interaction with &lt;code&gt;task&lt;/code&gt;. So the last question doesn't actually makes sense. The same principle is used when you put this term as a grouping factor in the right part of &lt;code&gt;(1 | ...)&lt;/code&gt; which actually translates to &lt;code&gt;(1| school) + (1 | school:subject)&lt;/code&gt; hence the one factor nested within the other.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, I suggest you read &lt;a href=&quot;http://lme4.r-forge.r-project.org/lMMwR/lrgprt.pdf#page=43&quot; rel=&quot;nofollow&quot;&gt;Chapter 2 from Bates book&lt;/a&gt; and pay extra attention to &lt;a href=&quot;http://glmm.wikidot.com/faq#toc27&quot; rel=&quot;nofollow&quot;&gt;model specification&lt;/a&gt;. It is really important to define what actually makes sense to include as fixed or random effects and what the grouping factors will be. This depends on how you designed the study and what you want to extract from your models. &lt;/p&gt;&#10;" CommentCount="9" CreationDate="2014-09-13T13:15:18.697" Id="115346" LastActivityDate="2014-09-13T13:15:18.697" OwnerUserId="42434" ParentId="113980" PostTypeId="2" Score="0" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have data from a between-subject experiment, where every subject was assigned to one of the two conditions, and completed varying number of trials (as much as they wanted). Number of trials is between 3 and 15, roughly Gaussian distribution with mean=6.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm analyzing the effect of condition on effort spent on each trial (measured by the number of steps taken in each trial). Now, should I include  random effect of subjects (considering that each subject has only experienced one condition)?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-09-14T04:41:11.907" FavoriteCount="2" Id="115381" LastActivityDate="2014-09-14T21:36:28.280" LastEditDate="2014-09-14T21:36:28.280" LastEditorUserId="55750" OwnerUserId="55750" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;mixed-model&gt;&lt;random-effects-model&gt;&lt;lmer&gt;&lt;mixed-effect&gt;" Title="Using a mixed effects regression model for between-subject design?" ViewCount="61" />
  
  
  <row Body="&lt;p&gt;No, you can't. This will not work because of interactions between hyperparameters, which are typically nonlinear and can be very large. Optimizing each hyperparameter separately essentially assumes there is no such interaction.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-09-14T14:51:41.547" Id="115402" LastActivityDate="2014-09-14T14:51:41.547" OwnerUserId="25433" ParentId="110431" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;Your calculations look correct, and your approach is probably the one intended. &lt;/p&gt;&#10;&#10;&lt;p&gt;Another way of getting the same result is to note that $W_9-W_7$ is independent of $W_7$ so $$cov(5W_7+6W_9,W_7) = cov(11W_7+6(W_9-W_7),W_7)$$ $$= 11 var(W_7)+6cov(W_9-W_7,W_7) =11\times 7 +6\times 0.$$&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-09-14T20:58:15.633" Id="115429" LastActivityDate="2014-09-15T06:05:14.117" LastEditDate="2014-09-15T06:05:14.117" LastEditorUserId="2958" OwnerUserId="2958" ParentId="115409" PostTypeId="2" Score="4" />
  <row AcceptedAnswerId="115445" AnswerCount="1" Body="&lt;p&gt;I have an assignment to implement a Gaussian &lt;em&gt;radial basis function&lt;/em&gt;-kernel principal component analysis (RBF-kernel PCA) and have some challenges here. It would be great if someone could point me to the right direction because I am obviously doing something wrong here.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, when I understand correctly, the RBF kernel is implemented like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$K(\mathbf{x}_i, \mathbf{x}_j) = \mathrm{exp}\left(- \gamma \|\mathbf{x}_i - \mathbf{x}_j\|^{2}_{2} \right)=\mathrm{exp}\left(- \frac{\|\mathbf{x}_i - \mathbf{x}_j\|^{2}_{2}}{2\sigma^2} \right),$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\|\mathbf{x}_i - \mathbf{x}_j\|^{2}_{2} = \sum_j(x_{ik} - x_{jk})^2$ is the squared Euclidean distance between two data points, $\mathbf{x}_i$ and $\mathbf{x}_j$, and $\gamma$ is a free parameter $\gamma = \frac{1}{2\sigma^2}$. The $\sigma^2$ can be chosen as the variance of the Euclidean distances between all pairs of data points.&lt;/p&gt;&#10;&#10;&lt;p&gt;To compare my approach to scikit-learn's implementation, I created a simple nonlinear dataset:&lt;/p&gt;&#10;&#10;&lt;h3&gt;Example dataset&lt;/h3&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;import matplotlib.pyplot as plt&#10;&#10;from sklearn.datasets import make_moons&#10;X, y = make_moons(n_samples=100, random_state=123)&#10;&#10;plt.figure(figsize=(8,6))&#10;&#10;plt.scatter(X[y==0, 0], X[y==0, 1], color='red')&#10;plt.scatter(X[y==1, 0], X[y==1, 1], color='blue')&#10;&#10;plt.title('A nonlinear 2Ddataset')&#10;plt.ylabel('y coordinate')&#10;plt.xlabel('x coordinate')&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/meGwf.png&quot; alt=&quot;enter image description here&quot;&gt;    &lt;/p&gt;&#10;&#10;&lt;h3&gt;scikit-learn RBF Kernel PCA&lt;/h3&gt;&#10;&#10;&lt;p&gt;When I used the scikit-learn implementation for dimensionality reduction onto 1 component axis, the classes separate quite nicely.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;scikit_kpca = KernelPCA(n_components=1, kernel='rbf', gamma=15)&#10;X_skernpca = scikit_kpca.fit_transform(X)&#10;&#10;plt.figure(figsize=(8,6))&#10;plt.scatter(X_skernpca[y==0, 0], np.zeros((50,1)), color='red', alpha=0.5)&#10;plt.scatter(X_skernpca[y==1, 0], np.zeros((50,1)), color='blue', alpha=0.5)&#10;&#10;plt.title('First component after RBF Kernel PCA')&#10;plt.show()&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/HXfj9.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;h3&gt;My approach&lt;/h3&gt;&#10;&#10;&lt;p&gt;Somehow, I am not able to reproduce those results. From what I understand, I have to compute all pairwise distances in order to compute the kernel. Then center the Kernel and extract the eigenvector that corresponds to the largest eigenvalue. This is what I have done so far:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;from sklearn.preprocessing import KernelCenterer&#10;from scipy.spatial.distance import pdist, squareform&#10;from scipy import exp&#10;&#10;&#10;# pdist to calculate the squared Euclidean distances for every pair of points&#10;# in the 100x2 dimensional dataset.&#10;sq_dists = pdist(X, 'sqeuclidean')&#10;&#10;# Variance of the Euclidean distance between all pairs of data points.&#10;variance = np.var(sq_dists)&#10;&#10;# squareform to converts the pairwise distances into a symmetric 100x100 matrix&#10;mat_sq_dists = squareform(sq_dists)&#10;&#10;# set the gamma parameter equal to the one I used in scikit-learn KernelPCA&#10;gamma = 15&#10;&#10;# Compute the 100x100 kernel matrix&#10;K = exp(gamma * mat_sq_dists)&#10;&#10;# Center the kernel matrix&#10;kern_cent = KernelCenterer()&#10;K = kern_cent.fit_transform(K)&#10;&#10;# Get the eigenvector with largest eigenvalue&#10;eigvals, eigvecs = np.linalg.eig(K)&#10;eigvals, eigvecs = zip(*sorted(zip(eigvals, eigvecs), reverse=True))&#10;X_pc1 = eigvecs[0]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/B82Iz.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;h1&gt;Edit&lt;/h1&gt;&#10;&#10;&lt;p&gt;Thanks a lot to @Kirill ! He found my mistakes and the problem is solved now! Here is the correct version for future reference:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;from sklearn.preprocessing import KernelCenterer&#10;from scipy.spatial.distance import pdist, squareform&#10;from scipy import exp&#10;from scipy.linalg import eigh&#10;&#10;&#10;# pdist to calculate the squared Euclidean distances for every pair of points&#10;# in the 100x2 dimensional dataset.&#10;sq_dists = pdist(X, 'sqeuclidean')&#10;&#10;# Variance of the Euclidean distance between all pairs of data points.&#10;variance = np.var(sq_dists)&#10;&#10;# squareform to converts the pairwise distances into a symmetric 100x100 matrix&#10;mat_sq_dists = squareform(sq_dists)&#10;&#10;# set the gamma parameter equal to the one I used in scikit-learn KernelPCA&#10;gamma = 15&#10;&#10;# Compute the 100x100 kernel matrix&#10;K = exp(-gamma * mat_sq_dists)&#10;&#10;# Center the kernel matrix&#10;kern_cent = KernelCenterer()&#10;K = kern_cent.fit_transform(K)&#10;&#10;# Get eigenvalues in ascending order with corresponding &#10;# eigenvectors from the symmetric matrix&#10;eigvals, eigvecs = eigh(K)&#10;&#10;# Get the eigenvectors that corresponds to the highest eigenvalue&#10;X_pc1 = eigvecs[:,-1]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/lLTVv.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-09-14T23:17:56.860" FavoriteCount="3" Id="115439" LastActivityDate="2014-09-18T09:01:10.857" LastEditDate="2014-09-18T09:01:10.857" LastEditorUserId="28666" OwnerUserId="39663" PostTypeId="1" Score="5" Tags="&lt;pca&gt;&lt;python&gt;&lt;scikit-learn&gt;&lt;kernel-trick&gt;" Title="How to apply a Gaussian radial basis function kernel PCA to nonlinear data?" ViewCount="424" />
  <row Body="&lt;p&gt;The correct answer should be similar, so that's probably correct; by my reckoning the box plot's lower inner-fence is -12.45 so your quartiles are probably fine (sample quartile definitions vary, but are generally close to Tukey's hinges, and - for some quartile definitions at least - sometimes hinges and quartiles do coincide).&lt;/p&gt;&#10;&#10;&lt;p&gt;Since all your observations are positive, there are no &quot;outliers&quot; (by the present definition) on the low side.&lt;/p&gt;&#10;&#10;&lt;p&gt;If your data are necessarily positive but sufficiently right-skew and with some values close to zero, negative values for the inner fence are quite common. &lt;/p&gt;&#10;&#10;&lt;p&gt;The rule has no way to know that negative values are impossible for your data; the rule isn't necessarily entirely appropriate for this kind of data, but it looks to me like you have correctly carried the calculation out on the data you have.&lt;/p&gt;&#10;&#10;&lt;p&gt;Incidentally, for the boxplot, here's an illustration of essentially the same &quot;outlier&quot; bound calculation:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/KX5um.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-15T01:16:45.980" Id="115446" LastActivityDate="2014-09-15T09:33:34.180" LastEditDate="2014-09-15T09:33:34.180" LastEditorUserId="805" OwnerUserId="805" ParentId="115443" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Your answer appears correct.&lt;/p&gt;&#10;&#10;&lt;p&gt;In particular, the number of ways will be the number of ways of choosing groups of 2,3 and 2 from a set of 7.&lt;/p&gt;&#10;&#10;&lt;p&gt;This is the &lt;a href=&quot;http://en.wikipedia.org/wiki/Multinomial_theorem#Multinomial_coefficients&quot; rel=&quot;nofollow&quot;&gt;multinomial coefficient&lt;/a&gt; ${7 \choose 2, 3, 2} = \frac{7!}{2!\, 3! 2!}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;As you state in your question, its value is equal to ${7\choose 2}{5\choose 3}$, which you can arrive at via a number of different routes.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-15T02:31:12.897" Id="115451" LastActivityDate="2014-09-15T02:31:12.897" OwnerUserId="805" ParentId="115448" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="115474" AnswerCount="1" Body="&lt;p&gt;According to &lt;a href=&quot;http://stackoverflow.com/q/25763977/1100107&quot;&gt;this discussion&lt;/a&gt;, it seems that the &lt;code&gt;train&lt;/code&gt; function of the &lt;code&gt;caret&lt;/code&gt; package returns a &lt;code&gt;mtry&lt;/code&gt; parameter of &lt;code&gt;randomForest&lt;/code&gt; possibly higher than the number of predictor variables, because it expands the &lt;code&gt;X&lt;/code&gt; data into a matrix with dummy variables and considers the columns of this matrix. However a value of the &lt;code&gt;mtry&lt;/code&gt; parameter highest than &lt;code&gt;ncol(X)&lt;/code&gt; is not valid in the &lt;code&gt;randomForest&lt;/code&gt; function. Therefore I'm lost here : what should we do in such a case ? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-15T08:00:25.967" Id="115470" LastActivityDate="2014-09-15T08:55:34.130" LastEditDate="2014-09-15T08:06:01.633" LastEditorUserId="8402" OwnerUserId="8402" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;feature-selection&gt;&lt;random-forest&gt;&lt;caret&gt;" Title="mtry tuning given by caret higher than the number of predictors" ViewCount="67" />
  <row AnswerCount="1" Body="&lt;p&gt;Would you explain a practical situation where random effect model is more appropriate than the fixed effect model?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-09-15T11:40:17.547" Id="115488" LastActivityDate="2014-09-15T13:52:15.403" OwnerUserId="26130" PostTypeId="1" Score="0" Tags="&lt;anova&gt;&lt;references&gt;&lt;experiment-design&gt;&lt;random-effects-model&gt;&lt;fixed-effects-model&gt;" Title="Random and Fixed effect model" ViewCount="77" />
  <row AnswerCount="0" Body="&lt;p&gt;I am working on a meta-analysis of clear-cell renal cell carcinoma data. The data are gene expression values obtained out of Affymetrix Human Genome U133 Plus 2.0 arrays. Currently, I am trying to find out if there are any tumor subclasses, basing on gene expression values. I am going to use k-means and SOM (as implemented in the ConsensusCluster software) to discover tumor clusters. Since the data come from multiple studies and, within each study, arrays were scanned on different days, I presume that there are batch effects present in the data. I would like to identify them using the &lt;code&gt;aov()&lt;/code&gt; function in R. I have decided that all samples from the same study scanned on the same day make up a batch. I also have status information (control or tumor). I have two questions:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;There are some batches which contain only one or two samples. In my opinion, these batches should be discarded from further analysis. However, this way I am loosing valuable data (49 out of 397 samples). How do you resolve such issues in your analyses?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;There are some batches which contain only tumors (or controls), not both. What would you suggest me to do with them?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="1" CreationDate="2014-09-15T12:08:18.470" Id="115492" LastActivityDate="2014-09-15T13:58:24.850" LastEditDate="2014-09-15T13:58:24.850" LastEditorUserId="55812" OwnerUserId="55812" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;anova&gt;&lt;microarray&gt;" Title="Preparing microarray data for batch effect detection by anova" ViewCount="53" />
  
  
  <row Body="&lt;p&gt;You expect the HR due to age of the sire to be about 1.02 per year of age. Note that once you standardized the ages they were no longer expressed in years, but rather in multiples of the SD of the ages. So it's not surprising that the coefficient values from the standardized data don't agree with your expectation. Note that the coefficients after scaling by 1 SD are simply half of those after scaling by 2 SD, and all p values are identical within your two presented analyses.&lt;/p&gt;&#10;&#10;&lt;p&gt;For this type of Cox analysis there is no need to standardize the ages (and it just leads to confusion, as you found). Standardization of predictor variables is important for some purposes (such as when you need to consider relative importance of different predictors in ridge regression or lasso), but even then it is best to present final results for variables like age re-scaled to their usual units, like years.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-15T17:14:22.603" Id="115527" LastActivityDate="2014-09-15T17:14:22.603" OwnerUserId="28500" ParentId="115444" PostTypeId="2" Score="0" />
  
  
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I am sorry for stating such a question. Unfortunatelly, it's very-very hard to go through all statistics.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a crossover study design with 3 treatments, 3 periods, and a baseline (covariate).&#10;The data table looks like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;SUBJECT     BASELINE   PERIOD1    PERIOD2    PERIOD3&#10;&#10;subj_1      baseline   diet3      diet1      diet2   &#10;subj_2      baseline   diet2      diet3      diet1&#10;subj_3      baseline   diet1      diet2      diet3&#10;subj_4      baseline   diet1      diet3      diet2&#10;...         ...        ...        ...        ...&#10;subj_27     baseline   diet2      diet1      diet3&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;PERIOD1, PERIOD2, PERIOD3 correspond to the time (week5, week11, week17). In between there is wash-out period.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is how to perform repeated measures ANOVA to such a experimental design?&lt;/p&gt;&#10;&#10;&lt;p&gt;I was trying to search for some examples and code but what I found out at the end is that I can apply mixed effect linear model to such data. Unfortunatelly, I did not find enough examples describing similar studies and the theory behind is very hard (I need time to understand).&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm trying to find solution in R but SAS is also good. As I understood, in R there is a package &lt;strong&gt;nlme&lt;/strong&gt; and I can apply a function &lt;em&gt;lme()&lt;/em&gt; to fit an ANOVA model.&lt;/p&gt;&#10;&#10;&lt;p&gt;In order to do so, I found out that I have to divide my data according to treatment sequences (I will have 6 sequences: diet1-diet2-diet3, diet1-diet3-diet2, diet2-diet1-diet3, diet2-diet3-diet1, diet3-diet1-diet2, diet3-diet2-diet1).&lt;/p&gt;&#10;&#10;&lt;p&gt;Then I have to create a mixed effect linear model:&lt;/p&gt;&#10;&#10;&lt;p&gt;I assume (according to what I read and found) that probably it should look like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;output_value ~ period(fixed effect) + diet(fixed effect) + sequence(fixed_effect)&#10;                   + subject(random_effect) + baseline_value(random_effect)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In R it probably looks something like:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lme( output_value ~ period + diet + sequence, random ~ 1 | subject + baseline_value )&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Is it right?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-09-15T22:29:03.157" FavoriteCount="0" Id="115556" LastActivityDate="2014-10-19T12:49:23.323" LastEditDate="2014-09-15T22:56:51.430" LastEditorUserId="43820" OwnerUserId="43820" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;mixed-model&gt;&lt;repeated-measures&gt;&lt;crossover-study&gt;" Title="repeated measures ANOVA, crossover trial, R" ViewCount="276" />
  <row Body="&lt;p&gt;Constraints are probably easier to implement in predictive mean matching multiple imputation.  This assumes that there is a significant number of observations with non-missing constraining variables that meet the constraints.  I'm thinking about implementing this in the R &lt;code&gt;Hmisc&lt;/code&gt; package &lt;code&gt;aregImpute&lt;/code&gt; function.  You may want to check back in a month or so.  It will be important to specify the maximum distance from the target that a donor observation can be, because the constraints will push donors further from the ideal unconstrained donor.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-09-15T22:46:16.013" Id="115557" LastActivityDate="2014-09-15T22:46:16.013" OwnerUserId="4253" ParentId="78632" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;Google Scholar search (9/15/2014) for:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;     Program      Hits&#10;         SAS 2,610,000&#10;SPSS or PSPP 1,640,000&#10;       Stata 1,280,000&#10;  Statistica   459,000&#10;         JMP   249,000&#10;  R and cran    86,500&#10;     Minitab    85,800&#10;      Systat    73,800&#10;        BMDP    45,900&#10;      SUDAAN    17,100&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;That said: there's lots of reasons besides popularity to choose a platform:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;performance for specific data set sizes&lt;/li&gt;&#10;&lt;li&gt;cost&lt;/li&gt;&#10;&lt;li&gt;extensibility&lt;/li&gt;&#10;&lt;li&gt;how fast new techniques are released/old techniques are updated&lt;/li&gt;&#10;&lt;li&gt;documentation and support&lt;/li&gt;&#10;&lt;li&gt;perpetual versus rental license&lt;/li&gt;&#10;&lt;li&gt;what your team uses&lt;/li&gt;&#10;&lt;li&gt;portability&lt;/li&gt;&#10;&lt;li&gt;multi-core support&lt;/li&gt;&#10;&lt;li&gt;multi-user support&lt;/li&gt;&#10;&lt;li&gt;the nature of the errors in the software or documentation&lt;/li&gt;&#10;&lt;li&gt;does it do specifically what you need it to do&lt;/li&gt;&#10;&lt;li&gt;interface&lt;/li&gt;&#10;&lt;li&gt;&amp;amp;c.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="7" CreationDate="2014-09-15T23:07:00.607" Id="115559" LastActivityDate="2014-09-15T23:07:00.607" OwnerUserId="44269" ParentId="115520" PostTypeId="2" Score="4" />
  
  
  <row Body="&lt;p&gt;Because &lt;a href=&quot;http://eprints.pascal-network.org/archive/00000447/01/MACH1769-01.pdf&quot; rel=&quot;nofollow&quot;&gt;bagging equalizes influence&lt;/a&gt;. This essentially means that the influence of so-called &lt;em&gt;leverage points&lt;/em&gt; (points which have a large impact on the overall model) decreases compared to non-bagged models. &#10;This is good if the leverage points are bad for the model's performance, which is not always the case. For an example of a leverage point, consider an outlier in least-squares regression.&lt;/p&gt;&#10;&#10;&lt;p&gt;The &lt;a href=&quot;http://robotics.stanford.edu/~ronnyk/vote.pdf&quot; rel=&quot;nofollow&quot;&gt;variance-reduction argument&lt;/a&gt; which most people know about does not explain everything as nicely (e.g. in some cases bagging &lt;em&gt;does&lt;/em&gt; increase variance).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-16T05:40:55.857" Id="115575" LastActivityDate="2014-09-16T05:40:55.857" OwnerUserId="25433" ParentId="115562" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;A seasonal dummy is nonrandom: Whatever sample you draw, winter is winter, never summer. The covariance of a random variable and a constant is zero.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-16T10:39:38.903" Id="115602" LastActivityDate="2014-09-16T10:39:38.903" OwnerUserId="55879" ParentId="44655" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;It is not necessary to have the same scales if you use Spearman's Rank Correlation. &lt;/p&gt;&#10;&#10;&lt;p&gt;The benefit of rank techniques is that the results do not change if you do monotonous transformations on the original data. Such transformations would take care of different scales.&lt;/p&gt;&#10;&#10;&lt;p&gt;Rank techniques basically replace the original values by their ranks, i.e. the number if the values were ordered. So if you transform monotonically the original data, their ranks do not change: E.g. the largest value will still be the largest value after the transformation. Consequently, statistics defined on the ranks (like Spearman's Rank Correlation or the Wilcoxon-Mann-Whitney-test) will be the same irrespective of the monotonous transformation.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-09-16T11:19:36.793" Id="115607" LastActivityDate="2014-09-16T12:15:27.640" LastEditDate="2014-09-16T12:15:27.640" LastEditorUserId="28705" OwnerUserId="28705" ParentId="115599" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="115692" AnswerCount="1" Body="&lt;p&gt;I have some data about conversion rates on different browsers, as follows.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;            Opera   |    Firefox   |    Chrome   |    ...&#10;Site A |     55%           75%            76%         ...&#10;Site B |     45%           70%            71%         ...&#10;Site C |     55%           10%            57%&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Except that I have more sites and more browsers than this in reality!&lt;/p&gt;&#10;&#10;&lt;p&gt;What's the best test to use to compare these? &lt;/p&gt;&#10;&#10;&lt;p&gt;I'd like a test that can identify when a particular site has an unusually low conversion rate on a particular browser - for example, site C on Firefox, in the above table. &lt;/p&gt;&#10;&#10;&lt;p&gt;I'm not sure if a chi-squared test is the right test to use here. &lt;/p&gt;&#10;&#10;&lt;p&gt;I do also have the underlying raw data, if that helps. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-16T11:30:05.773" Id="115608" LastActivityDate="2014-09-17T00:03:33.083" OwnerUserId="36825" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;&lt;statistical-significance&gt;&lt;chi-squared&gt;" Title="What's the right test to use to find significance in a table of incidence rates?" ViewCount="26" />
  
  <row AnswerCount="0" Body="&lt;p&gt;For a sample of size $n$ of non-normal random variables $x_1,...,x_n$. Is it possible to know which distribution the sample variance estimator follows?&lt;/p&gt;&#10;&#10;&lt;p&gt;Details: The distribution of $x_i$'s is not known. The only thing I know is that this $n$-dimensional sample was taken from a population with mean=0 and std=1. I also don't know about the distribution of the population.&lt;/p&gt;&#10;&#10;&lt;p&gt;thanks!&lt;/p&gt;&#10;" ClosedDate="2014-09-17T07:18:34.890" CommentCount="3" CreationDate="2014-09-16T13:21:23.753" FavoriteCount="0" Id="115617" LastActivityDate="2014-09-16T13:37:44.167" LastEditDate="2014-09-16T13:37:44.167" LastEditorUserId="52440" OwnerUserId="52440" PostTypeId="1" Score="1" Tags="&lt;distributions&gt;&lt;estimation&gt;&lt;variance&gt;" Title="Distribution of sample variance for non-normal random variable" ViewCount="30" />
  
  <row Body="&lt;p&gt;I think you might find something of interest using the caret package:&lt;br&gt;&#10;The method findCorrelation does what you want.&#10;You can change the value to the desired amount.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(caret) &#10;tooHigh &amp;lt;- findCorrelation(cor(rbind(Xtrain,Xtest)), .95) &#10;Xtrainfiltered &amp;lt;- Xtrain[, -tooHigh]&#10;Xtestfiltered  &amp;lt;-  Xtest[, -tooHigh]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2014-09-16T15:24:07.740" Id="115638" LastActivityDate="2014-09-16T15:24:07.740" OwnerUserId="44458" ParentId="115637" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;The idea behind the sharp cutoffs for the p value in the null hypothesis significance testing framework is: If a researchers, for all his experiments in his lifetime, always accepts only results with p &amp;lt;= 0.05 as statistically significant, he will, averaged over his career, commit a Type I error (falsely rejecting a true null hypothesis) only in 5% of his experiments.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'd say you are justified to use a significance level of alpha=0.05 for each of two independent experiments with planned comparisons. &lt;/p&gt;&#10;&#10;&lt;p&gt;The consensus seems to be to keep the overall type I error rate of all comparisons &lt;em&gt;per experiment&lt;/em&gt; to a fixed level, e.g. 0.05. This is arbitrary, but it helps to avoid the worst consequences of post-hoc comparisons and exploratory data analysis.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-09-16T17:51:51.713" Id="115663" LastActivityDate="2014-09-16T18:33:20.340" LastEditDate="2014-09-16T18:33:20.340" LastEditorUserId="34859" OwnerUserId="34859" ParentId="115662" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;The following &lt;code&gt;lmer&lt;/code&gt; model would seem appropriate:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;fit = lmer(&#10;    formula = outcome ~ (1|group) + treatment&#10;    , data = myData&#10;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This presumes that you're measuring each individual once only. In this case, you treat observations from individuals as having a Gaussian random deviation from their group mean, and the group means are in turn influenced by an effect of treatment.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-09-16T18:46:54.050" Id="115673" LastActivityDate="2014-09-16T18:46:54.050" OwnerUserId="364" ParentId="113661" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I am looking to simulate 2 survival curves under different assumptions on the mean / median process underlying the 2 arms. Has anyone done this in R?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-09-17T02:20:40.313" Id="115702" LastActivityDate="2014-09-17T02:34:10.193" LastEditDate="2014-09-17T02:34:10.193" LastEditorUserId="7290" OwnerUserId="2078" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;survival&gt;&lt;simulation&gt;" Title="Simulate KM curves with assumptions in R" ViewCount="33" />
  
  
  
  
  
  
  
  
  
  
  
  
  
  <row Body="&lt;ul&gt;&#10;&lt;li&gt;The ratio of the means is not the mean of the ratios. &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;$$ \dfrac{\sum_j x_j}{\sum_j y_j} = \sum_j w_j \dfrac{x_j}{y_j},$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $w_j = \dfrac{y_j}{\sum_j y_j}$. The correct way in this case is to calculate the ratio of the means of each coefficient, which converges to the ratio of the coefficients by the law of large numbers.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Calculate the ratios $r_j=b_1^{(j)}/b_2^{(j)}$. Using the sample of ratios, approximate the probability $P(b_1/b_2&amp;gt;0)$. This is the posterior probability of your hypothesis. This can be done by approximating the distribution of the ratios using a kernel density estimator and the by integrating this over $(0,\infty)$. A second method consists of dividing the number of positive ratios by the total number of ratios, as you did. THIS IS NOT A P-VALUE, p-values have nothing to do with Bayesian statistics. You have to understand what posterior probabilities mean. Try to read a book on the fundamentals of this theory.&lt;/li&gt;&#10;&lt;li&gt;For your last problem, calculate the sample of ratios for each group, say $r_A$ and $r_B$. Then, approximate $P(r_A&amp;gt;r_B) = P(r_B&amp;lt;r_A)$ as indicated in the following answer:&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://stats.stackexchange.com/a/30148&quot;&gt;http://stats.stackexchange.com/a/30148&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-18T07:35:50.083" Id="115880" LastActivityDate="2014-09-18T08:31:27.913" LastEditDate="2014-09-18T08:31:27.913" LastEditorUserId="56022" OwnerUserId="56022" ParentId="115873" PostTypeId="2" Score="0" />
  
  
  <row Body="&lt;p&gt;Neither the 5% type one error rate nor the 80% figure for power are universal. For example, particle physicists tend to use a &quot;5 sigma&quot; criterion that corresponds to a notional type I error rate that is roughly on the order of one in a million.  Indeed, I doubt your average physicist has even heard of Cohen.&lt;/p&gt;&#10;&#10;&lt;p&gt;But one reason why the two error rates you quote should be different would be that the cost of the two error types would not be the same.&lt;/p&gt;&#10;&#10;&lt;p&gt;As to why the type I error rate is often taken at 5%, part of the reason (some of the historical background for the convention) is discussed &lt;a href=&quot;http://stats.stackexchange.com/questions/55691/regarding-p-values-why-1-and-5-why-not-6-or-10&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-09-18T10:06:09.670" Id="115902" LastActivityDate="2014-09-18T10:12:02.950" LastEditDate="2014-09-18T10:12:02.950" LastEditorUserId="805" OwnerUserId="805" ParentId="115891" PostTypeId="2" Score="8" />
  <row AnswerCount="1" Body="&lt;p&gt;I have a set of 12 subjects: 6 administred with drug A and 6 with drug B. For each subject two time-points have been collected (before and after drug treatment) for each analyte (over 6000).&#10;I had a read around and it seems that mixed ANOVA suits my needs. Therefore I performed it using R as recommended on &lt;a href=&quot;http://www.cookbook-r.com/Statistical_analysis/ANOVA/#mixed-design-anova&quot; rel=&quot;nofollow&quot;&gt;http://www.cookbook-r.com/Statistical_analysis/ANOVA/#mixed-design-anova&lt;/a&gt; &#10;Which is to say: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;funAV &amp;lt;- function(x) aov(x ~ Class*time+ Error(Subjects/time),data=X)&#10;&#10;aovOb &amp;lt;- apply(X[,-(1:3)],2,funAV)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Once I get the results I have for each analyte a significant (or not significant) difference concerning Class (A or B), time (before or after) or a combination of both. On &lt;a href=&quot;https://statistics.laerd.com/spss-tutorials/mixed-anova-using-spss-statistics.php&quot; rel=&quot;nofollow&quot;&gt;https://statistics.laerd.com/spss-tutorials/mixed-anova-using-spss-statistics.php&lt;/a&gt; it is reported that there is no need for a post hoc analysis with a 2 level factor between subjects. If this is true and I find a Class effect how can I know if the difference lies at time before or after? The same is true on the other way round i.e. if I find a time effect how can I know if the difference lies at drug A or B? &#10;I looked for post hoc analysis methods anywhere but all I could get was a bunch of methods for repeated measures ANOVA which is not exactly my case since I have an additional factor (the drug). Many thanks.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-18T11:59:14.950" Id="115914" LastActivityDate="2015-03-07T20:20:58.527" OwnerUserId="42686" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;anova&gt;&lt;post-hoc&gt;&lt;mixed&gt;" Title="Mixed ANOVA post hoc analysis (and multiple comparisons)" ViewCount="119" />
  
  
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I used tbats to find the best fit model to a 3 years of daily data. It couldnt find a model and showed the following error: &quot; Missing values encountered. Using longest contiguous portion of time series&quot;. I checked my data and there is no missing data. I only have a lot of &quot;0&quot; in my data. Is this error related to too many &quot;0&quot;s that the data have?Thanks&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-09-18T17:43:38.440" Id="115953" LastActivityDate="2014-09-18T17:43:38.440" OwnerUserId="55541" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;forecasting&gt;" Title="TBATS missing value error" ViewCount="37" />
  <row Body="&lt;p&gt;I interpret your question as follows: You want to do a multinomial logistic regression, and you don't want to choose a reference category for the dependent/explained/left-hand-side/$y$- variable. In that case the answer is: no, you need a reference category. The problem is that probabilities add to 1. So if you have 3 categories and estimated two of the probabilities, you have estimated all there is to know: the probability of the third category is just one minus the already estimated probabilities. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you don't want to choose a reference category for a independent/explanatory/right-hand-side/$x$-variable, then you can don't need a reference category, see: &lt;a href=&quot;http://maartenbuis.nl/publications/ref_cat.html&quot; rel=&quot;nofollow&quot;&gt;http://maartenbuis.nl/publications/ref_cat.html&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-09-18T18:50:23.493" Id="115963" LastActivityDate="2014-09-18T19:50:56.047" LastEditDate="2014-09-18T19:50:56.047" LastEditorUserId="23853" OwnerUserId="23853" ParentId="115934" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="117842" AnswerCount="1" Body="&lt;p&gt;&lt;strong&gt;Edited:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to work out the above relationship, more precisely:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Let $(Y_{1}, ..., Y_{m})$ be a zero-mean vector with covariance matrix&#10;  $\Sigma$, and let $S \subset \{1, ..., m\}.$ The best linear predictor&#10;  of $Y_{S}$ upon $Y_{\setminus{S}}$, defined as $\tilde{\beta}_{S} =&#10; \Sigma^{-1}_{\setminus{S}, \setminus{S}} \Sigma_{\setminus{S}, S}$, is&#10;  a matrix multiple of $\Sigma^{-1}_{\setminus{S}, S}$.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;It is assumed that$(Y_{1}, ..., Y_{m})$ is jointly normally distributed.&lt;/p&gt;&#10;&#10;&lt;p&gt;The above statement seems to imply that there is a number of steps from&lt;/p&gt;&#10;&#10;&lt;p&gt;$\tilde{\beta} = [X^{T}X]_{\setminus{S}, \setminus{S}}^{-1} [X^{T}Y]_{\setminus{S}, S}$&lt;/p&gt;&#10;&#10;&lt;p&gt;to &lt;/p&gt;&#10;&#10;&lt;p&gt;$[X^{T}X]_{\setminus{S}, S}^{-1}  c$, where c is some scalar.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would be interested in those steps.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-09-18T20:41:10.140" Id="115973" LastActivityDate="2014-10-04T10:53:34.193" LastEditDate="2014-09-19T06:34:54.150" LastEditorUserId="56050" OwnerUserId="56050" PostTypeId="1" Score="2" Tags="&lt;multiple-regression&gt;&lt;covariance&gt;&lt;regression-coefficients&gt;&lt;matrix-inverse&gt;" Title="How to proof relationship between inverse covariance matrix and linear regression coefficients?" ViewCount="129" />
  
  
  <row Body="&lt;p&gt;You actually don't have to show that it is a maximum in this case.&#10;The root of the first derivative of the log-likelihood, that is the MLE, can be shown to be unique if the iid observation are considered from a random variable of the exponential family. That is, a r.v. for which the density has the form:&lt;/p&gt;&#10;&#10;&lt;p&gt;$f(x;\theta) = h(x)\,\exp{(s\,\theta - K(\theta) )}$&lt;/p&gt;&#10;&#10;&lt;p&gt;Where $h$ is a function only of the observations $x_i$, $\theta$ is called the natural parameter, $s$ is called the natural statistics and $K$ is a function only of the natural parameter.&lt;/p&gt;&#10;&#10;&lt;p&gt;In this case, given $X_1,..X_n$ iid with the density you have, we have&lt;/p&gt;&#10;&#10;&lt;p&gt;$\prod_{i=1}^n \frac{(x_i+1)}{\theta\,(1+\theta)} \exp(-x_i/\theta) \longrightarrow \prod_i (x_i+1) exp\big(-\frac{n\overline{X}}{\theta} - \log(\theta(1+\theta)) \big)$&lt;/p&gt;&#10;&#10;&lt;p&gt;As this random variable belongs to the exponential family the MLE is unique.&lt;/p&gt;&#10;&#10;&lt;p&gt;In general, for exponential families the MLE always exists, is unique, is consistent and is asymptotically normal.&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT: &lt;a href=&quot;http://www.stat.purdue.edu/~dasgupta/ml.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.stat.purdue.edu/~dasgupta/ml.pdf&lt;/a&gt; is a good explanation of this, maybe a bit too mathematical, but it depends from your academic background. Otherwise, V.S.Huzurbazar, in his &quot;The likelihood equation, consistency and the maxima of the likelihood function&quot; (1947) explains this theory in an easier way.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-09-18T23:08:17.690" Id="115995" LastActivityDate="2014-09-18T23:08:17.690" OwnerUserId="56063" ParentId="115962" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;Why not model like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\Delta\text{Score} = \beta_{0} + \beta_{\text{time}}\text{time} + \beta_{\text{age}}\text{age} + \beta_{\text{sex}}\text{sex} + \varepsilon$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Since presumably you are interested in &lt;em&gt;change&lt;/em&gt; in score?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-09-19T01:06:47.253" Id="116005" LastActivityDate="2014-09-19T01:06:47.253" OwnerUserId="44269" ParentId="116003" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;I think the simplest possible non-closed-form expression is the following:&lt;/p&gt;&#10;&#10;&lt;p&gt;Denote $d$ the common degrees of freedom, $F_X(x;d,d)$ the CDF of the F-distribution with common degrees of freedom, and $I$ the regularized beta function.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then for given $\tilde x$ we have (exploiting some simplifications due to the common degrees of freedom)&lt;/p&gt;&#10;&#10;&lt;p&gt;$$F_X(\tilde x;d,d) = I_{\frac {\tilde x}{1+\tilde x}}\left(\frac d2,\frac d2\right)=\frac {B\left(\frac {\tilde x}{1+\tilde x};\frac d2,\frac d2\right)}{B\left(\frac d2,\frac d2\right)} = q_1$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $B(\cdot \;;\cdot,\cdot)$ is the incomplete beta function and $B(\cdot,\cdot)$ the Beta function.  &lt;/p&gt;&#10;&#10;&lt;p&gt;By the properties of the regularized Beta function we have&lt;/p&gt;&#10;&#10;&lt;p&gt;$$I_{\frac {\tilde x}{1+\tilde x}}\left(\frac d2,\frac d2\right) = 1- I_{\frac {1}{1+\tilde x}}\left(\frac d2,\frac d2\right) \Rightarrow I_{\frac {1}{1+\tilde x}}\left(\frac d2,\frac d2\right) = 1-q_1 = \frac {B\left(\frac {1}{1+\tilde x};\frac d2,\frac d2\right)}{B\left(\frac d2,\frac d2\right)}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Using these two results we have&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac {B\left(\frac {\tilde x}{1+\tilde x};\frac d2,\frac d2\right)}{q_1} = \frac {B\left(\frac {1}{1+\tilde x};\frac d2,\frac d2\right)}{1-q_1}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\Rightarrow (1-q_1)\int_0^{\frac {\tilde x}{1+\tilde x}}(t-t^2)^{d/2 -1}dt - q_1\int_0^{\frac {1}{1+\tilde x}}(t-t^2)^{d/2 -1}dt = 0$$&lt;/p&gt;&#10;&#10;&lt;p&gt;...which looks a bit less nightmarish than the picture @whuber's comment describes.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-09-19T05:13:04.453" Id="116022" LastActivityDate="2014-09-19T18:07:03.293" LastEditDate="2014-09-19T18:07:03.293" LastEditorUserId="28746" OwnerUserId="28746" ParentId="115936" PostTypeId="2" Score="4" />
  <row AnswerCount="1" Body="&lt;p&gt;Say I have the following regression model:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\text{Wage}_i = constant + α·\text{YearsOfEduc}_i + β·\text{Age}_i + γ·\text{CompletedHighSchool}_i + \mbox{δ·$\text{NumOfSiblings}_i$} + ε·\text{Gender}_i + u_i$$&lt;/p&gt;&#10;&#10;&lt;p&gt;How would I perform an LR test of the hypothesis that the &lt;strong&gt;marginal effect of age on the wage&lt;/strong&gt; for&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;a male, &lt;/li&gt;&#10;&lt;li&gt;has two siblings,&lt;/li&gt;&#10;&lt;li&gt;who completed high school, &lt;/li&gt;&#10;&lt;li&gt;has 3 years of education&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;... is the &lt;strong&gt;same as&lt;/strong&gt; for:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;a female,&lt;/li&gt;&#10;&lt;li&gt;has one sibling,&lt;/li&gt;&#10;&lt;li&gt;who completed high school,&lt;/li&gt;&#10;&lt;li&gt;has 3 years of education.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I can modify the model above if needed, but what would be my unrestricted and restricted models, and how would I use them in a regression?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-09-19T09:55:05.830" Id="116037" LastActivityDate="2014-09-19T14:45:21.173" LastEditDate="2014-09-19T14:45:21.173" LastEditorUserId="56014" OwnerUserId="56014" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;hypothesis-testing&gt;&lt;statistical-significance&gt;&lt;inference&gt;&lt;lrt&gt;" Title="LR test on marginal effect" ViewCount="79" />
  
  <row Body="&lt;p&gt;Since you are conditioning on a number of variables, what you seem to want is a mixture of a marginal and a conditional (adjusted) effect of sex.  I would not know how to interpret such an estimate, as it can be manipulated by oversampling males according to the number of siblings he has, when obtaining the data.  You might consider a sequence of conditional estimates.  Here is an example in R for a model with three predictors: sex, sibs, and age, with age and number of sibs not assumed to operate linearly.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;require(rms)&#10;n &amp;lt;- 100; set.seed(1)&#10;sex &amp;lt;- sample(c('female', 'male'), n, TRUE)&#10;sibs &amp;lt;- sample(0:7, n, TRUE)&#10;age &amp;lt;- rnorm(n, 40, 10)&#10;dd &amp;lt;- datadist(sex, sibs, age); options(datadist='dd')&#10;y &amp;lt;- 3*(sex == 'male') + sibs + .01*(age - 40)^2 + rnorm(n, 0, 4)&#10;&#10;f &amp;lt;- ols(y ~ sex + pol(sibs, 2) + rcs(age, 5))&#10;coef(f)['sex=male']  # 2.744&#10;contrast(f, list(sex='male', sibs=0:5),&#10;            list(sex='female', sibs=1))&#10;&#10;   sibs Contrast      S.E.      Lower    Upper    t Pr(&amp;gt;|t|)&#10;      0 1.843579 0.9748423 -0.0925419 3.779700 1.89   0.0617&#10;      1 2.743908 0.9072221  0.9420867 4.545730 3.02   0.0032&#10;      2 3.694806 1.0995190  1.5110664 5.878546 3.36   0.0011&#10;*     3 4.696273 1.2790347  2.1559993 7.236546 3.67   0.0004&#10;*     4 5.748308 1.3630204  3.0412318 8.455384 4.22   0.0001&#10;*     5 6.850912 1.3617277  4.1464032 9.555421 5.03   0.0000&#10;&#10;Redundant contrasts are denoted by *&#10;&#10;Error d.f.= 92 &#10;&#10;Confidence intervals are 0.95 individual intervals&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Note that when sibs=1 for a male the contrast 2.744 equals the regression coefficient for sex.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you really want, you can get a weighted average of the conditional estimates according to some reasonable external distribution of sibs to marginalize on sibs.  This can be done by specifying &lt;code&gt;type='average', weights=...&lt;/code&gt; to &lt;code&gt;contrast&lt;/code&gt;.  But I would not recommend marginalization.&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2014-09-19T13:15:12.967" Id="116055" LastActivityDate="2014-09-19T13:15:12.967" OwnerUserId="4253" ParentId="116037" PostTypeId="2" Score="1" />
  
  
  <row AcceptedAnswerId="116069" AnswerCount="1" Body="&lt;p&gt;For a square matrix, is it appropriate to use a chi-squared distribution when each level of the variables are assumed to have the same overall frequency? &lt;/p&gt;&#10;&#10;&lt;p&gt;Specifically, I'm analyzing a dataset of the number of genes that have increased expression in an experimental treatment in two related species. My data look like this, with species 1 on the columns and species 2 on the rows:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;                  Low       Intermediate      High&#10; Low              2594          163            405&#10;&#10; Intermediate     1350          558            155&#10;&#10; High              467           65            322&#10;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I &lt;em&gt;a priori&lt;/em&gt; expect each class to have been the same in the common ancestor of the species (off-diagonals = 0). That is:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;                  Low       Intermediate      High&#10; Low              3786          0              0&#10;&#10; Intermediate      0          1425             0&#10;&#10; High              0           0              868&#10;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;My question is which of the off-diagonal cells have diverged more than expected by chance. &lt;/p&gt;&#10;&#10;&lt;p&gt;As a simple first pass, I've modified the standard &lt;code&gt;chisq.test&lt;/code&gt; (in R) to use the &lt;em&gt;overall&lt;/em&gt; total for each class (Low, Intermediate, High) rather than the marginal total for each class (assuming species independent...which they aren't). &lt;/p&gt;&#10;&#10;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;# data&#10;d &amp;lt;- matrix(c(2594L, 1350L, 467L, 163L, 558L, 65L, 405L, 155L, 322L), nrow=3, ncol=3)&#10;&#10;# row and column sums&#10;rs &amp;lt;- rowSums(d)&#10;cs &amp;lt;- colSums(d)&#10;&#10;# grand mean for each class&#10;gm &amp;lt;- (rs + cs) / sum(d * 2)&#10;&#10;Ec &amp;lt;- outer(gm, gm, &quot;*&quot;) * sum(d)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;where &lt;code&gt;Ec&lt;/code&gt; is the Expected value for each cell using the grand mean for each class.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is it reasonable to use a chisq distribution to determine if the observed values deviate from the expected values by more than chance? &lt;/p&gt;&#10;&#10;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;Ec.chistat &amp;lt;- sum((data-Ec)^2 / Ec)&#10;pchisq(Ec.chistat, df=(nrow(data)-1) * (ncol(data)-1), lower.tail = FALSE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I realize I could probably use a GLM for this, but it's convenient to keep in table format to directly address which of the off-diagonals have increased more than expected by chance.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note: for comparison, the standard chisq assuming independence of variable would be:&lt;/p&gt;&#10;&#10;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;rs &amp;lt;- rowSums(d)&#10;cs &amp;lt;- colSums(d)&#10;n &amp;lt;- sum(d)&#10;(E &amp;lt;- outer(rs, cs, &quot;*&quot;)/n)&#10;chistat &amp;lt;- sum((d - E) ^ 2 / E)&#10;pchisq(chistat, df=(nrow(d)-1) * (ncol(d)-1), lower.tail = FALSE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-09-19T15:10:34.647" FavoriteCount="0" Id="116066" LastActivityDate="2014-09-19T15:46:06.513" OwnerUserId="41273" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;chi-squared&gt;&lt;non-independent&gt;" Title="chi-squared test when variables dependent" ViewCount="104" />
  
  <row Body="&lt;p&gt;The pre-process options apply to the X variables, NOT the Y variable. There's currently no way to preProcess the Y variable (that I know of).  Perhaps there needs to be? &lt;a href=&quot;https://github.com/topepo/caret/issues&quot; rel=&quot;nofollow&quot;&gt;Please post an issue to the tracker!&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-19T17:21:47.297" Id="116082" LastActivityDate="2014-09-19T17:21:47.297" OwnerUserId="2817" ParentId="116051" PostTypeId="2" Score="0" />
  
  
  <row Body="&lt;p&gt;Intriguing question, I had this question for a while,. Here is my findings&#10;&lt;strong&gt;Short Answer&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;You can create any number of classifier you want, but the point is, you can only prove a few of them to be Bayes/universally-consistent! ( Bayes consistency means that classifier is asymptotically optimal, i.e. with infinite data its risk limits Bayes risk, which is optimal risk) &lt;/p&gt;&#10;&#10;&lt;p&gt;The consistency of a classifier, depends on loss function and (inverse)-link function (i.e. mapping from [0 1] probability space to $\mathbb{R}$, and vice versa.)&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Long answer&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;First, according to &lt;a href=&quot;http://stat.rutgers.edu/home/tzhang/papers/aos04_consistency.pdf&quot; rel=&quot;nofollow&quot;&gt;Tong's great paper&lt;/a&gt; all the (consistent) classifiers are equivalent! except in that they are minimizing different loss functions, and almost every difference between classifiers is a consequence of their loss functions. In fact, he showed that minimizing every loss function leads to optimal decision function (technically, inverse-link function), which is completely function of probabilities (even for SVMs!). His result is summarized in this table (by &lt;a href=&quot;http://www.svcl.ucsd.edu/publications/conference/2008/nips08/NIPS08LossesWITHTITLE.pdf&quot; rel=&quot;nofollow&quot;&gt;Hamed&lt;/a&gt;):&#10;&lt;img src=&quot;http://i.stack.imgur.com/Obqhe.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Despite of this unified view over all the classifiers, they are different in their outputs:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Probability-Calibrated: for these class the classifiers (e.g. Logistic Regression), output is DIRECTLY within a probability measure, which this in turn not only answers yes/no question of the classifier, but also gives confidence of the of the decision.&lt;/li&gt;&#10;&lt;li&gt;Not-probability-Calibrated: Other classifiers (e.g. SVM) are real-valued-output classifiers, which you can use some link functions to calibrate the to enforce outputs to be probabilities.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;It really depend on loss-function, link-function, calibration. For example, first line of the table says that, least-squares regression and classification are the same,(if your classifier output is calibrated-probabilities $\eta$, and using the corresponding inverse link function)&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-09-19T20:38:20.577" Id="116103" LastActivityDate="2014-09-19T21:08:49.523" LastEditDate="2014-09-19T21:08:49.523" LastEditorUserId="31645" OwnerUserId="31645" ParentId="116033" PostTypeId="2" Score="4" />
  
  
  
  <row AnswerCount="2" Body="&lt;p&gt;So I have a pretty well testing SVC train series which puts me into the mid 80 percentile without outrageous C/g values. My current C value is 2.0 and gamma is 0.5. Good numbers across the range during refinement - looking solid. Here's the cross-validation plot from my grid search:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/wCwv8.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I have been working on the libsvm command line as well as am writing C# test code via libsvm.net. On both sides I am experiencing very strange behavior. On the command line it happens when I change the default labels of the test series. On the C# side I am not supplying any labels, which may be incorrect, I don't see good examples that separate the training from the test data. In any case those test series labels should be ignored, right? &lt;/p&gt;&#10;&#10;&lt;p&gt;So this is a subset of my test series - the first ten rows. Let's call that test0:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;1 1:-0.2 2:1 3:-1 4:-1 5:0.2 6:1 7:-0.6 8:-0.6 9:0.2 10:-0.6 11:-0.6 12:-0.6&#10;0 1:1 2:0.2 3:-0.6 4:-0.6 5:1 6:0.2 7:0.2 8:-1 9:1 10:-0.6 11:0.6 12:-0.6&#10;1 1:0.2 2:-1 3:0.2 4:0.6 5:0.2 6:-1 7:0.2 8:1 9:0.2 10:-1 11:-0.2 12:1&#10;1 1:-0.2 2:-1 3:0.2 4:1 5:-0.2 6:-1 7:0.6 8:1 9:-0.6 10:-1 11:0.6 12:0.6&#10;1 1:-0.6 2:-0.2 3:-0.6 4:0.6 5:-1 6:0.6 7:1 8:0.2 9:-1 10:0.6 11:0.6 12:-0.2&#10;1 1:1 2:-0.2 3:-1 4:-1 5:1 6:0.2 7:-1 8:-0.6 9:1 10:-0.6 11:-0.6 12:-0.2&#10;1 1:1 2:-0.2 3:-1 4:0.6 5:1 6:-0.2 7:0.6 8:0.2 9:0.6 10:1 11:-0.2 12:-1&#10;1 1:-0.2 2:-0.6 3:-0.6 4:1 5:-0.2 6:-0.6 7:0.6 8:1 9:-0.6 10:-1 11:0.6 12:1&#10;-1 1:0.6 2:1 3:-0.6 4:-1 5:0.6 6:0.6 7:-0.6 8:-1 9:0.6 10:-0.2 11:-0.6 12:-0.6&#10;0 1:1 2:-0.6 3:-0.6 4:1 5:0.2 6:-0.6 7:1 8:0.2 9:-0.6 10:0.6 11:1 12:-0.6&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I run that against my model and this is what I'm getting in my predict0 file:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;1&#10;0&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;-1&#10;1&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The command line shows: Accuracy = 90% (9/10) (classification)&lt;/p&gt;&#10;&#10;&lt;p&gt;Excellent - this is what we want to see. But obviously on my C# end I'm not supplying any labels. Which is why I'm seeing different results there. In order to double check this on the LIBSVM command line I changed all the labels to 0 - here now is my test1 file:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;0 1:-0.2 2:1 3:-1 4:-1 5:0.2 6:1 7:-0.6 8:-0.6 9:0.2 10:-0.6 11:-0.6 12:-0.6&#10;0 1:1 2:0.2 3:-0.6 4:-0.6 5:1 6:0.2 7:0.2 8:-1 9:1 10:-0.6 11:0.6 12:-0.6&#10;0 1:0.2 2:-1 3:0.2 4:0.6 5:0.2 6:-1 7:0.2 8:1 9:0.2 10:-1 11:-0.2 12:1&#10;0 1:-0.2 2:-1 3:0.2 4:1 5:-0.2 6:-1 7:0.6 8:1 9:-0.6 10:-1 11:0.6 12:0.6&#10;0 1:-0.6 2:-0.2 3:-0.6 4:0.6 5:-1 6:0.6 7:1 8:0.2 9:-1 10:0.6 11:0.6 12:-0.2&#10;0 1:1 2:-0.2 3:-1 4:-1 5:1 6:0.2 7:-1 8:-0.6 9:1 10:-0.6 11:-0.6 12:-0.2&#10;0 1:1 2:-0.2 3:-1 4:0.6 5:1 6:-0.2 7:0.6 8:0.2 9:0.6 10:1 11:-0.2 12:-1&#10;0 1:-0.2 2:-0.6 3:-0.6 4:1 5:-0.2 6:-0.6 7:0.6 8:1 9:-0.6 10:-1 11:0.6 12:1&#10;0 1:0.6 2:1 3:-0.6 4:-1 5:0.6 6:0.6 7:-0.6 8:-1 9:0.6 10:-0.2 11:-0.6 12:-0.6&#10;0 1:1 2:-0.6 3:-0.6 4:1 5:0.2 6:-0.6 7:1 8:0.2 9:-0.6 10:0.6 11:1 12:-0.6&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And here's the predict1 file:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;1&#10;0&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;-1&#10;1&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Same predictions - very nice. However the command line gives me this: &lt;/p&gt;&#10;&#10;&lt;p&gt;Accuracy = 10% (1/10) (classification)&lt;/p&gt;&#10;&#10;&lt;p&gt;Say again? The predict2 file is correct 9 out of 10 times. That would be a minor hick-up - however on the C# side I'm getting incorrect predictions 50% of the times. I double checked the vectors that go into svm.Predict() and they are identical. The XML model on that end produces the identical output that I see on the command line version of LIBSVM, so I'm sure it's loading the right train file and gets the same settings.&lt;/p&gt;&#10;&#10;&lt;p&gt;I also tried other faux labels - one with all 3s - per the above my categories only allow -1, 0, and 1. Same results and same screwy output from LIBSVM.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's the method I wrote in C# - it's extremely simple:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;/// &amp;lt;summary&amp;gt;&#10;/// Makes the prediction based on the supplied data vector.&#10;/// &amp;lt;/summary&amp;gt;&#10;/// &amp;lt;param name=&quot;x&quot;&amp;gt;the vector&amp;lt;/param&amp;gt;&#10;/// &amp;lt;returns&amp;gt;the category prediction as a double&amp;lt;/returns&amp;gt;&#10;public double Predict(double[] vector)&#10;{&#10;   svm_node[] x = new svm_node[vector.Length];&#10;   for(int j = 0 ; j &amp;lt; vector.Length ; j++) // Save values for each attributes&#10;    {&#10;        x[j] = new svm_node() { index = j, value = vector[j] };&#10;    }&#10;&#10;    double predict = cSvm.Predict(x);&#10;    return predict;&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;That's it - exceedingly simple. I'm producing a single vector, which I'm feeding into libsvm. Perhaps I'm doing something wrong here as I'm getting the following output on that end:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;1:-0.2 2:1 3:-1 4:-1 5:0.2 6:1 7:-0.6 8:-0.6 9:0.2 10:-0.6 11:-0.6 12:-0.6&#10;Expected: 1 - prediction: 1 - correct.&#10;1:1 2:0.2 3:-0.6 4:-0.6 5:1 6:0.2 7:0.2 8:-1 9:1 10:-0.6 11:0.6 12:-0.6&#10;Expected: 0 - prediction: 1 - incorrect.&#10;1:0.2 2:-1 3:0.2 4:0.6 5:0.2 6:-1 7:0.2 8:1 9:0.2 10:-1 11:-0.2 12:1&#10;Expected: 1 - prediction: 0 - incorrect.&#10;1:-0.2 2:-1 3:0.2 4:1 5:-0.2 6:-1 7:0.6 8:1 9:-0.6 10:-1 11:0.6 12:0.6&#10;Expected: 1 - prediction: -1 - incorrect.&#10;1:-0.6 2:-0.2 3:-0.6 4:0.6 5:-1 6:0.6 7:1 8:0.2 9:-1 10:0.6 11:0.6 12:-0.2&#10;Expected: 1 - prediction: 0 - incorrect.&#10;1:1 2:-0.2 3:-1 4:-1 5:1 6:0.2 7:-1 8:-0.6 9:1 10:-0.6 11:-0.6 12:-0.2&#10;Expected: 1 - prediction: 1 - correct.&#10;1:1 2:-0.2 3:-1 4:0.6 5:1 6:-0.2 7:0.6 8:0.2 9:0.6 10:1 11:-0.2 12:-1&#10;Expected: 1 - prediction: 1 - correct.&#10;1:-0.2 2:-0.6 3:-0.6 4:1 5:-0.2 6:-0.6 7:0.6 8:1 9:-0.6 10:-1 11:0.6 12:1&#10;Expected: 1 - prediction: -1 - incorrect.&#10;1:0.6 2:1 3:-0.6 4:-1 5:0.6 6:0.6 7:-0.6 8:-1 9:0.6 10:-0.2 11:-0.6 12:-0.6&#10;Expected: -1 - prediction: 1 - incorrect.&#10;1:1 2:-0.6 3:-0.6 4:1 5:0.2 6:-0.6 7:1 8:0.2 9:-0.6 10:0.6 11:1 12:-0.6&#10;Expected: 0 - prediction: -1 - incorrect.&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;As you can see seven out of ten are incorrect here. Same input data. I'm really scratching my head here. Here is the output I get during model creation from my C# test:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;......*&#10;optimization finished, #iter = 6318&#10;nu = 0.29823202442349317&#10;obj = -2600.72689660147, rho = 0.1313953415634528&#10;nSV = 2067, nBSV = 1324&#10;.....*&#10;optimization finished, #iter = 5661&#10;nu = 0.3084318050488557&#10;obj = -2492.6705073363432, rho = 0.10201329253787896&#10;nSV = 1938, nBSV = 1290&#10;.....*&#10;optimization finished, #iter = 5079&#10;nu = 0.0938971979545962&#10;obj = -805.0415458108344, rho = 0.004972540301229775&#10;nSV = 1095, nBSV = 400&#10;Total nSV = 3700&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And here's the output LIBSVM gives me on the command shell using the svm-train command:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;....*..*&#10;optimization finished, #iter = 6318&#10;nu = 0.298232&#10;obj = -2600.726897, rho = 0.131395&#10;nSV = 2067, nBSV = 1324&#10;...*..*&#10;optimization finished, #iter = 5661&#10;nu = 0.308432&#10;obj = -2492.670507, rho = 0.102013&#10;nSV = 1938, nBSV = 1290&#10;...*..*&#10;optimization finished, #iter = 5079&#10;nu = 0.093897&#10;obj = -805.041546, rho = 0.004973&#10;nSV = 1095, nBSV = 400&#10;Total nSV = 3700&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Both are identical so I am certainly producing the same model with my C# code. Am I perhaps misunderstanding the API? In any case the command line behavior (i.e. incorrect reporting of test results) is strange as well although I do appreciate getting solid output on that side.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any help/input/insights/suggestions would be very welcome. Thanks in advance.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-20T12:19:26.057" Id="116139" LastActivityDate="2014-09-20T14:23:02.637" LastEditDate="2014-09-20T12:42:03.967" LastEditorUserId="54692" OwnerUserId="54692" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;svm&gt;&lt;cross-validation&gt;&lt;libsvm&gt;" Title="Varying LIBSVM predictions based on test series labels" ViewCount="131" />
  
  
  <row Body="&lt;p&gt;&lt;em&gt;(The likelihood function in the question is written mistakenly, the derivative of the log-likelihood is correct).&lt;/em&gt;  &lt;/p&gt;&#10;&#10;&lt;p&gt;To address the specific issue raised by the OP, we need, as mentioned in the comments, to determine whether the second derivative is negative &lt;em&gt;evaluated at the MLE&lt;/em&gt;. The tip here is that checking this does not necessarily require to obtain a closed-form expression for the MLE -in fact, sometimes, even if we have a closed-form expression, it is advisable &lt;em&gt;not&lt;/em&gt; to use it, but rather, exploit the first-order conditions in a different way.&lt;/p&gt;&#10;&#10;&lt;p&gt;And this is exactly the case here. The first-order condition eventually leads to a second-degree polynomial in $\theta$ (including the sample mean of the data as a parameter), with a single admissible root, So we have a closed-form expression for the MLE. Denoting the sample mean by $\bar x$, this expression here is &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\hat \theta = \frac {\bar x-1 + \sqrt{\bar x^2 +6\bar x +1}}{4} &amp;gt;0 \tag{1}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's turn now to the issue of the sign of the 2nd derivative. The second derivative is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac{d^2logL(\theta;x)}{d\theta^2}=-n\left(\frac{2\theta+2\theta^2-(2\theta+1)^2}{\theta^2(1+\theta)^2}\right) - 2\frac{\sum X_i}{\theta^3}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\Rightarrow \frac 1n \frac{d^2logL(\theta;x)}{d\theta^2}=-\frac 1{\theta^2}\left[\frac{-2\theta^2-2\theta-1}{(1+\theta)^2} + 2\frac{\bar x}{\theta}\right] \tag{2}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;We want the term inside the brackets in $(2)$ to be positive. Now imagine inserting $(1)$, the closed-form expression for the MLE, into $(2)$, and try to determine the sign of the exrpession... &lt;/p&gt;&#10;&#10;&lt;p&gt;...Instead, it is more efficient to go back to the first-order condition and &lt;em&gt;eliminate the sample mean&lt;/em&gt;, since the MLE must satisfy:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\bar x = \frac{(2\hat \theta+1)\hat \theta}{(1+\hat \theta)} \tag{3}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Inserting $(3)$ into the bracketed expression in $(2)$ we obtain&lt;/p&gt;&#10;&#10;&lt;p&gt;$$[\;] = \frac{-2\hat \theta^2-2\hat \theta-1}{(1+\hat \theta)^2} + 2\frac{1}{\hat \theta}\frac{(2\hat \theta+1)\hat \theta}{(1+\hat \theta)} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;which for sign purposes is equivalent to&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac{-2\hat \theta^2-2\hat \theta-1}{(1+\hat \theta)} + 2(2\hat \theta+1) &amp;gt;0$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\Rightarrow (4\hat \theta+2)(1+\hat \theta) &amp;gt; 2\hat \theta^2+2\hat \theta+1$$&lt;/p&gt;&#10;&#10;&lt;p&gt;which holds, since $\hat \theta &amp;gt;0$.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-20T21:48:26.690" Id="116166" LastActivityDate="2014-09-20T21:48:26.690" OwnerUserId="28746" ParentId="115962" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="130628" AnswerCount="1" Body="&lt;p&gt;Suppose you have an explanatory variable ${\bf{X}} = \left(X(s_{1}),\ldots,X(s_{n})\right)$ where $s$ represents a given coordinate. You also have a response variable ${\bf{Y}} = \left(Y(s_{1}),\ldots,Y(s_{n})\right)$. Now, we can combine both variables as: &lt;/p&gt;&#10;&#10;&lt;p&gt;$${\bf{W}}({\bf{s}}) = \left( \begin{array}{ccc}X(s) \\ Y(s) \end{array} \right) \sim N(\boldsymbol{\mu}(s), T)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;In this case, we simply choose $\boldsymbol{\mu}(s) = \left( \mu_{1} \; \; \mu_{2}\right)^{T}$ and $T$ is a covariance matrix that describes the relation between $X$ and $Y$. This only describes the value of $X$ and $Y$ at $s$. Since we have more points from other locations for $X$ and $Y$, we can &#10;describe more values of ${\bf{W}}(s)$ in the following way:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\left( \begin{array}{ccc} {\bf{X}} \\ {\bf{Y}} \end{array}\right) = N\left(\left(\begin{array}{ccc}\mu_{1}\boldsymbol{1}\\ \mu_{2}\boldsymbol{1}\end{array}\right), T\otimes H(\phi)\right)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;You will notice that we rearranged the components of $\bf{X}$ and $\bf{Y}$ to get all $X(s_i)$ in a column and after that, concatenate all $Y(s_i)$ together. Each component $H(\phi)_{ij}$ is a correlation function $\rho(s_i, s_j)$ and $T$ is as above. The reason we have the covariance $T\otimes H(\phi)$ is because we assume it is possible to separate the covariance matrix as $C(s, s')=\rho(s, s') T$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Question 1: When I calculate the conditional ${\bf{Y}}\mid{\bf{X}}$, what I'm actually doing is generating a set of values of $\bf{Y}$  based on $\bf{X}$, correct? I already have $\bf{Y}$ so I would be more interested in predicting a new point $y(s_{0})$. In this case, I should have a matrix $H^{*}(\phi)$ defined as&lt;/p&gt;&#10;&#10;&lt;p&gt;$$H^{*}(\phi) = \left(\begin{array}{ccc}H(\phi) &amp;amp; \boldsymbol{h} \\ \boldsymbol{h}&amp;amp;  \rho(0,\phi) \end{array}\right)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;in which $\boldsymbol{h}(\phi)$ is a vector $\rho(s_{0} - s_{j};\phi)$. Therefore, we can construct a vector (without rearrangement):&lt;/p&gt;&#10;&#10;&lt;p&gt;$${\bf{W^{*}}} = \left({\bf{W}}(s_{1}), \ldots, {\bf{W}}(s_{n}), {\bf{W}}(s_{0})\right)^{T} \sim N\left(\begin{array}{ccc}\boldsymbol{1}_{n+1} \otimes \left( \begin{array}{ccc} \mu_{1} \\ \mu_{2} \end{array} \right)\end{array},  H(\phi)^{*}\otimes T\right)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;And now I just rearrange to get a joint distribution $\left(\begin{array}{ccc}  {\bf{X}} \\  x(s_0) \\{\bf{Y}} \\ y(s_0)\end{array} \right)$ and obtain the conditional $p(y(s_0)\mid x_0, {\bf{X}}, {\bf{Y}})$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Is this correct? &lt;/p&gt;&#10;&#10;&lt;p&gt;Question 2: For predicting, the paper I'm reading indicates that I must use this conditional distribution $p(y(s_0)\mid x_0, {\bf{X}}, {\bf{Y}})$ and obtain a posterior distribution $p(\mu, T, \phi\mid x(s_0), {\bf{Y}}, {\bf{X}})$, but I'm not sure how to obtain the posterior distribution for the parameters. Maybe I could use the distribution $\left(\begin{array}{ccc}{\bf{X}} \\ x(s_0)\\ {\bf{Y}}\end{array}\right)$ that I think is exactly the same as $p({\bf{X}}, x(s_0), {\bf{Y}}\mid\mu, T, \phi)$ and then simply use Bayes' theorem to obtain $p(\mu, T, \phi\mid {\bf{X}}, x(s_0), {\bf{Y}}) \propto p({\bf{X}}, x(s_0), {\bf{Y}}\mid\mu, T, \phi)p(\mu, T, \phi)$&lt;/p&gt;&#10;&#10;&lt;p&gt;Question 3: At the end of the subchapter, the author says this:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;For prediction, we do not have ${\bf{X}}(s_0)$. This does not create any new&#10;  problems as it may be treated as a latent variable and incorporated&#10;  into $\bf{x}'$ This only results in an additional draw within each&#10;  Gibbs iteration and is a trivial addition to the computational task.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;What does that paragraph mean? &lt;/p&gt;&#10;&#10;&lt;p&gt;By the way, this procedure can be found in &lt;a href=&quot;http://sankhya.isical.ac.in/search/64a2/64a2020.pdf?origin=publication_detail&quot; rel=&quot;nofollow&quot;&gt;this paper&lt;/a&gt; (page 8), but as you can see, I need a bit more of detail.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-09-18T21:14:29.980" FavoriteCount="3" Id="116179" LastActivityDate="2014-12-29T17:47:10.937" LastEditDate="2014-12-29T17:47:10.937" LastEditorUserId="7224" OwnerDisplayName="Robert Smith" OwnerUserId="2676" PostTypeId="1" Score="10" Tags="&lt;probability&gt;&lt;bayesian&gt;&lt;conditional-probability&gt;&lt;gibbs&gt;" Title="Bayesian modeling using multivariate normal with covariate" ViewCount="129" />
  <row AnswerCount="0" Body="&lt;p&gt;Can we convert blob.noun_phrases into Pandas's DataFrame? &lt;/p&gt;&#10;&#10;&lt;p&gt;The data type of blob.noun_phrases is class 'textblob.blob.WordList'&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;blockquote&gt;&#10;    &lt;blockquote&gt;&#10;      &lt;p&gt;type(blob.noun_phrases)&lt;/p&gt;&#10;    &lt;/blockquote&gt;&#10;  &lt;/blockquote&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;class 'textblob.blob.WordList'&lt;/p&gt;&#10;&#10;&lt;p&gt;I am asking this because it would be much more easier to count noun_phrases that appear in the text using pandas than creating a function to count it textblob. &lt;/p&gt;&#10;&#10;&lt;p&gt;This is what i have:&lt;/p&gt;&#10;&#10;&lt;p&gt;from textblob import TextBlob&lt;/p&gt;&#10;&#10;&lt;p&gt;text = '''&#10;players control a character avatar within a game world in third person or first&#10;person view, exploring the landscape, fighting various monsters, completing quests,&#10;and interacting with non-player characters (NPCs) or other players. Also similar&#10;to other MMORPGs, World of Warcraft requires the player to pay for a&#10;subscription, either by buying prepaid game cards for a selected amount of&#10;playing time, or by using a credit or debit card to pay on a regular basis&#10;'''&lt;/p&gt;&#10;&#10;&lt;p&gt;blob = TextBlob(text)&#10;print(blob.noun_phrases)  # ['players control', 'character avatar' ...]&#10;Swapping out for the other implementation (an NLTK-based chunker) is quite easy.&lt;/p&gt;&#10;&#10;&lt;p&gt;from text.np_extractors import ConllExtractor&lt;/p&gt;&#10;&#10;&lt;p&gt;blob = TextBlob(text, np_extractor=ConllExtractor())&lt;/p&gt;&#10;&#10;&lt;p&gt;print(blob.noun_phrases) &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-21T00:48:26.827" Id="116180" LastActivityDate="2014-09-21T00:48:26.827" OwnerUserId="35577" PostTypeId="1" Score="0" Tags="&lt;python&gt;&lt;nlp&gt;" Title="converting textblob object into DataFrame" ViewCount="46" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a group of 222 test results that were reviewed separately by two interpreters X and Y. Based on their assessment, they were to assign management into 4 categories A, B, C and D.&#10;Then another variable that they were blinded to was revealed to them and again they were asked to assign management into the 4 categories, now with the new information.&lt;/p&gt;&#10;&#10;&lt;p&gt;So X initially chose A (72%) B (27%), C (2%) and D (0%) and Y initially chose A (72%), B (22%), C (4%) and D (3%)&#10;After the blinded variable was revealed, X changed their answers and it became A (77%), B (18%), C (5%) and D (0%) and Y became A (74%), B (18%), C (8%) and D (0%)&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to show the difference the variable had on X's choices and if that is statistically significant (p&amp;lt;0.05) and the same for Y. Then I want to evaluate if there was a difference in the pre and post variable results between X and Y and what it was driven by. My hypothesis is that the variable will make definite changes to A and more so for the Y reviewer&lt;/p&gt;&#10;&#10;&lt;p&gt;I thought doing multiple contingency tables might be the best thing to do? i.e. I did 2x2 table of X showing numbers of A or not A pre and post the variable and repeated this for each of B,C and D. Is this an appropriate thing to do? Or would it be better to show an Odds Ratio i.e that with the variable, the odds of doing something increased by such and such an amount? How would one do this.&lt;/p&gt;&#10;&#10;&lt;p&gt;THank you   &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-21T03:07:57.527" Id="116184" LastActivityDate="2014-09-21T03:07:57.527" OwnerUserId="56152" PostTypeId="1" Score="1" Tags="&lt;statistical-significance&gt;&lt;contingency-tables&gt;&lt;odds-ratio&gt;" Title="test to show statsticial change due to an intervention" ViewCount="20" />
  
  
  
  
  
  <row Body="The **probability integral transform** transforms a random variable by its own CDF. It transforms a variable with a continuous distribution to a uniform distribution." CommentCount="0" CreationDate="2014-09-21T16:04:13.563" Id="116223" LastActivityDate="2014-09-21T22:41:50.690" LastEditDate="2014-09-21T22:41:50.690" LastEditorUserId="805" OwnerUserId="44269" PostTypeId="4" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;How do I detect the number of significant modes in a set of data?  I have a set of 1 dimensional data where the number of modes are unknown.  I'm familiar with python and scikit learn so I use the Kernel Density Estimator module to create a gaussian function of the data.  I detected the maxima and minima by looking at when the slope of the function changes.  The result is....&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/2Zl1I.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;As you can see, there are six modes but there are four additional modes that are insignificant.  What is the best way to filter these insignificant modes out?  Any suggestions would be welcome. Thanks.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-21T21:28:03.427" Id="116247" LastActivityDate="2014-09-21T21:28:03.427" OwnerUserId="45710" PostTypeId="1" Score="1" Tags="&lt;python&gt;&lt;scikit-learn&gt;&lt;kernel-density-estimate&gt;&lt;scipy&gt;&lt;numpy&gt;" Title="How do I filter insignificant modes from a kernel density estimation" ViewCount="40" />
  <row Body="&lt;p&gt;There as many &quot;rules of thumb&quot; as authors who wrote about this. I could tell you my personal preferences, but my answer would certainly be biased (as anyone's probably would!). There is no reason to despair tough! The good news is that, the size of the hidden layer is not so relevant. Once your choice is within a certain acceptable range, any difference in the result wouldn't be statistically significant. It's not hard to have a intuition of the reasons for this. &lt;/p&gt;&#10;&#10;&lt;p&gt;A neural network is &lt;a href=&quot;http://en.wikipedia.org/wiki/Universal_approximation_theorem&quot; rel=&quot;nofollow&quot;&gt;universal function approximator&lt;/a&gt;. It draws a curve which separates the data points in different regions of space. This curve is composed by as many straight lines (hyperplanes indeed) as neurons in the hidden layer (i. e. the parameter of each hidden unit is nothing more than the parameters of a straight line). If you have too few hidden units, the curve will be composed of a small number of lines, thus it can't represent complex shapes. If you use too many hidden units, your shape can be so complex that it will model perfectly the training data, even its noisy, so it won't be able to generalize to new test cases. This is one of the most standard examples of the (in)famous &lt;a href=&quot;http://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff&quot; rel=&quot;nofollow&quot;&gt;bias-variance trade-off&lt;/a&gt;. If you have a number of hidden units that is not too big, neither too small, the curve described by you neural network will look just fine and the number of hidden units used will hardly affect the network performance.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can find a useful set of these &quot;rules of thumb&quot;, and also a brief discussion on the topic, in &lt;a href=&quot;ftp://ftp.sas.com/pub/neural/FAQ3.html#A_hu&quot; rel=&quot;nofollow&quot;&gt;SAS FAQ&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-21T21:32:03.183" Id="116248" LastActivityDate="2014-09-21T21:32:03.183" OwnerUserId="46039" ParentId="116227" PostTypeId="2" Score="0" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I am evaluating whether the prevalence of a specific infection in the ICU bears a relationship to the nurse:patient ratio over time.&#10;The infections are whole numbers (nominal variable)and the nurse: patient ratio is also a number (ratio variable).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-09-22T05:18:26.930" Id="116265" LastActivityDate="2014-09-22T06:44:15.787" OwnerUserId="56201" PostTypeId="1" Score="0" Tags="&lt;hypothesis-testing&gt;&lt;statistical-significance&gt;" Title="how do I test the relationship between infection and nurse:patient ratio" ViewCount="6" />
  <row AnswerCount="0" Body="&lt;p&gt;At any given time about 5.5% of women (age 15-45) are pregnant. A home pregnancy test is accurate 99% of the time if the woman taking the test is actually pregnant and 99.5% accurate if the woman is not pregnant. If the test yields a positive result, what is the posterior probability of the hypothesis that the woman is pregnant?&lt;/p&gt;&#10;&#10;&lt;p&gt;how to calculate the posterior probability of the hypothesis ?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-22T06:09:16.537" Id="116268" LastActivityDate="2014-09-22T06:09:16.537" OwnerUserId="56013" PostTypeId="1" Score="0" Tags="&lt;probability&gt;&lt;mathematical-statistics&gt;" Title="getting posterior probability of the hypothesis of hypothesis" ViewCount="17" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a data.frame that looks like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; head(mydata)&#10;model scenario iso3c year     feelecpc    gdppc      popdens    t&#10;EDGE  history   AFG  2002   0.0001057354 1052.958 3.404137e-05 2002&#10;EDGE  history   AFG  2003   0.0001373833 1096.756 3.544170e-05 2003&#10;EDGE  history   AFG  2004   0.0001289229 1066.685 3.682548e-05 2004&#10;EDGE  history   AFG  2005   0.0001245555 1145.717 3.811670e-05 2005&#10;...&#10;&amp;gt;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;feelecpc&lt;/code&gt; is electricity demand, &lt;code&gt;gdppc&lt;/code&gt; is GDP per capita and &lt;code&gt;popdens&lt;/code&gt; is population density. The last column is a copy of the year column and is only used to include a time trend instead of year dummies.&lt;/p&gt;&#10;&#10;&lt;p&gt;The data.frame is converted into a pdata.frame:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;pdata &amp;lt;- pdata.frame(mydata, index=c(&quot;iso3c&quot;, &quot;year&quot;))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;If I now estimate the following model:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;gmm1 &amp;lt;- pgmm(log(feelecpc) ~ lag(log(feelecpc), 1) + log(gdppc) + I(log(gdppc)^2) + log(popdens) + I(log(popdens)^2), gmm.inst = ~lag(log(feelecpc), 2:99), data=pdata)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I get an error:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Error in solve.default(crossprod(WX, t(crossprod(WX, A1)))) : &#10;  system is computationally singular: reciprocal condition number = 0&#10;In addition: Warning message:&#10;In pgmm(log(feelecpc) ~ lag(log(feelecpc), 1) + log(gdppc) + I(log(gdppc)^2) +  :&#10;  the second-step matrix is singular, a general inverse is used&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;If I leave out the second order polynomial of &lt;code&gt;popdens&lt;/code&gt;, only the warning is reported (by the way: Do I have to worry about this one?):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;gmm2 &amp;lt;- pgmm(log(feelecpc) ~ lag(log(feelecpc), 1) + log(gdppc) + I(log(gdppc)^2) + log(popdens), gmm.inst = ~lag(log(feelecpc), 2:99), data=pdata)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Also, the error does not occur when I estimate a model similar to &lt;code&gt;gmm1&lt;/code&gt;, but include year dummies:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;gmm3 &amp;lt;- pgmm(log(feelecpc) ~ lag(log(feelecpc), 1) + log(gdppc) + I(log(gdppc)^2) + log(popdens) + I(log(popdens)^2), gmm.inst = ~lag(log(feelecpc), 2:99), data=pdata, effect=&quot;twoways&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I suppose this points to some problem with my data?! Can somehelp help?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-08-13T17:57:27.393" Id="116287" LastActivityDate="2014-09-22T09:52:00.027" OwnerDisplayName="user2547406" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;panel-data&gt;&lt;plm&gt;" Title="R plm strange error when using pgmm" ViewCount="103" />
  <row AcceptedAnswerId="116518" AnswerCount="3" Body="&lt;p&gt;The notification after the ANOVA table after K-means analysis indicates that significance levels should not be looked at as the test of equal means, as the cluster solution has been derived based on Euclidean distance to maximize the distance. &#10;What test should I use to show whether the means of the clustering variables differ among the clusters? I have seen this warning in k-means outputs' provided ANOVA table, but in some references I see that post-hoc ANOVA tests are run. Should I ignore k-mean ANOVA outputs and run one-way ANOVA with post-hoc tests and interpret them in a traditional way? Or can I only imply about magnitude of F value and which variables contributed more to difference?&#10;Another confusion is that clustering variables are not normally distributed violating assumption of ANOVA, then I could use Kruskal-Wallis non-parametric test, but it has assumption about the same distributions. The inter-cluster distributions for the specific variables do not seem the same, some are positively skewed, some are negatively...&#10;I have 1275 large sample, 5 clusters, 10 clustering variables measured in PCA scores. &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-09-22T11:34:19.323" Id="116294" LastActivityDate="2014-09-24T01:05:16.310" OwnerUserId="54711" PostTypeId="1" Score="5" Tags="&lt;anova&gt;&lt;k-means&gt;" Title="Appropriateness of ANOVA after k-means cluster analysis" ViewCount="518" />
  
  <row Body="&lt;p&gt;It would be better to work at this from the ground up.  First, study the statistical principles involved.  Then see how those principles are used as a foundation for the analysis while you insert heavy subject matter knowledge.  In the vast majority of cases it is best to proceed something like this:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Make sure the measurements are valid&lt;/li&gt;&#10;&lt;li&gt;Formulate a model based on subject matter knowledge, being cognizant of causal pathways and what is measured when&lt;/li&gt;&#10;&lt;li&gt;Check that the number of parameters in the model can be supported by an adequate number of observations (a common rule of thumb is to have at least $15p$ as many observations in the least frequent $Y$ category where $p$ is the number of parameters required to fit the predictors)&lt;/li&gt;&#10;&lt;li&gt;Fit the single model&lt;/li&gt;&#10;&lt;li&gt;Interpret the model&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;In other words, don't think of this as a &quot;let's try different models on the same data&quot; exercise.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-22T12:44:31.663" Id="116298" LastActivityDate="2014-09-22T20:30:18.823" LastEditDate="2014-09-22T20:30:18.823" LastEditorUserId="4253" OwnerUserId="4253" ParentId="116292" PostTypeId="2" Score="1" />
  
  
  
  <row Body="&lt;p&gt;Your real problem is data snooping. You can't apply ANOVA or KW if the observations were assigned to groups (clusters) based on the input data set itself. What you can do is to use something like &lt;a href=&quot;https://stat.ethz.ch/R-manual/R-devel/library/cluster/html/clusGap.html&quot; rel=&quot;nofollow&quot;&gt;Gap statistic&lt;/a&gt; to estimate the number of clusters.&lt;/p&gt;&#10;&#10;&lt;p&gt;On the other hand, the snooped p-values are biased downward, so if ANOVA or KW test result is insignificant, then the &quot;true&quot; p-value is even larger and you may decide to merge the clusters. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-22T15:23:33.100" Id="116326" LastActivityDate="2014-09-23T20:41:14.917" LastEditDate="2014-09-23T20:41:14.917" LastEditorUserId="54099" OwnerUserId="54099" ParentId="116294" PostTypeId="2" Score="2" />
  
  
  
  
  <row AcceptedAnswerId="116496" AnswerCount="2" Body="&lt;p&gt;Today I taught an introductory class of statistics and a student came up to me with a question, which I rephrase here as: &quot;Why is the standard deviation defined as sqrt of variance and not as the sqrt of sum of squares over N?&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;We define population variance: $\sigma^2=\frac{1}{N}\sum{(x_i-\mu)^2}$ &lt;/p&gt;&#10;&#10;&lt;p&gt;And standard deviation: $\sigma=\sqrt{\sigma^2}=\frac{1}{\sqrt{N}}\sqrt{\sum{(x_i-\mu)^2}}$. &lt;/p&gt;&#10;&#10;&lt;p&gt;The interpretation we may give to $\sigma$ is that it gives the average deviation of units in the population from the population mean of $X$. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, in the definition of the s.d. we divide the sqrt of the sum of squares through $\sqrt{N}$. The question the student raises is why we do not divide the sqrt of the sume of squares by $N$ instead. Thus we come to competing formula: $$\sigma_{new}=\frac{1}{N}\sqrt{\sum{(x_i-\mu)^2}}.$$ The student argued that this formula looks more like an &quot;average&quot; deviation from the mean than when dividing through $\sqrt{N}$ as in $\sigma$.&lt;/p&gt;&#10;&#10;&lt;p&gt;I thought this question is not stupid. I would like to give an answer to the student that goes further than saying that the s.d. is &lt;em&gt;defined&lt;/em&gt; as sqrt of the variance which is the average squared deviaton. Put differently, why should the student use the &lt;em&gt;correct&lt;/em&gt; formula and not follow her idea?&lt;/p&gt;&#10;&#10;&lt;p&gt;This question relates to an older thread and answers provided &lt;a href=&quot;http://stats.stackexchange.com/questions/64272/why-is-square-root-taken-for-sample-count-n-in-standard-deviation-formula&quot;&gt;here&lt;/a&gt;. Answers there go in three directions:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;$\sigma$ is the root-mean-squared (RMS) deviation, not the &quot;typical&quot;&#10;deviation from the mean (i.e., $\sigma_{new}$). Thus, it is defined differently.&lt;/li&gt;&#10;&lt;li&gt;It has nice mathematical properties.&lt;/li&gt;&#10;&lt;li&gt;Furthermore, the sqrt would bring back &quot;units&quot; to their original scale. However, this would also be the case for $\sigma_{new}$, which divides by $N$ instead.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Both of points 1 and 2 are arguments in favour of the s.d. as RMS, but I do not see an argument against the use of $\sigma_{new}$. What would be the good arguments to convince introductory level students of the use of the average RMS distance $\sigma$ from the mean?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-09-22T17:10:30.133" FavoriteCount="4" Id="116342" LastActivityDate="2014-09-23T19:31:41.250" LastEditDate="2014-09-23T17:28:07.537" LastEditorUserId="24515" OwnerUserId="24515" PostTypeId="1" Score="7" Tags="&lt;variance&gt;&lt;standard-deviation&gt;&lt;basic-concepts&gt;&lt;teaching&gt;" Title="Why is the standard deviation defined as sqrt of the variance and not as the sqrt of sum of squares over N?" ViewCount="348" />
  
  <row Body="&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;If you want 200 copies of each of 5 levels in a polytomous variable in random order, then do this instead:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x &amp;lt;- sample(rep(paste0('pict', 1:5), 200))&#10;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;If you want to control for overall prevalence of a specific outcome, then you must choose which &lt;code&gt;beta&lt;/code&gt; you will fudge. I usually do &lt;code&gt;beta0&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;MM         &amp;lt;- model.matrix(~x)&#10;betas      &amp;lt;- rnorm(4)&#10;prevTarget &amp;lt;- 0.3&#10;prevDiff   &amp;lt;- function(beta0)  prevTarget - &#10;                               mean(binomial()$linkinv(MM%*%c(beta0, betas)))&#10;beta0      &amp;lt;- uniroot(prevDiff, c(-100, 100))$root&#10;mean(binomial()$linkinv(MM%*%c(beta0, betas)))&#10;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="8" CreationDate="2014-09-22T18:12:19.400" Id="116350" LastActivityDate="2014-09-22T18:28:55.703" LastEditDate="2014-09-22T18:28:55.703" LastEditorUserId="7290" OwnerUserId="8013" ParentId="116347" PostTypeId="2" Score="3" />
  
  <row Body="&lt;pre&gt;&lt;code&gt;    &quot;A trend is a trend is a trend, &#10;But the question is, will it bend? &#10;Will it alter its course &#10;through some unforeseen force &#10;And come to a premature end ? - Cairncross (1969)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This is quote that I got from the book &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0792374010&quot; rel=&quot;nofollow&quot;&gt;principles of forecasting&lt;/a&gt; by J Scott Armstrong. &#10;Following the above  quote Armstrong writes:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    Will the trend bend ? Some statisticians believe that the data can reveal this. &#10;In my judgement, this question can be best answered by domain knowledge. &#10;Experts often have a good knowledge of the series &#10;and what causes it to vary. - J Scott Armstrong, Principles of forecasting (2002)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Forecasting trend is very hard, unless you have domain knowledge and expertise to know on what causes trend to increase/decrease/stay the same it is going to very difficult to forecast the trend irrespective of using any algorithms. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-09-22T19:17:35.350" Id="116358" LastActivityDate="2014-09-22T22:11:17.877" LastEditDate="2014-09-22T22:11:17.877" LastEditorUserId="29137" OwnerUserId="29137" ParentId="116341" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;Yes. It is just a regression equation, after all. You can look at the table of coefficients (or &quot;solution&quot;, in some software) to see the estimate and $t$ test of the coefficient of $B$. This will test the contribution of $B$, after adjusting for $A$.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-22T20:12:36.530" Id="116366" LastActivityDate="2014-09-22T20:12:36.530" OwnerUserId="52554" ParentId="116364" PostTypeId="2" Score="0" />
  
  
  <row AcceptedAnswerId="116384" AnswerCount="1" Body="&lt;p&gt;The Glivenko-Cantelli Theorem (&lt;a href=&quot;http://en.wikipedia.org/wiki/Glivenko%E2%80%93Cantelli_theorem&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Glivenko%E2%80%93Cantelli_theorem&lt;/a&gt;) states that if $F$ is a distribution function, $X_1,\dots,X_n \sim F$, and $\hat{F}_n$ is the empirical distribution function, then&#10;$$\sup_{x \in \mathbb{R}} \lvert \hat{F}_n(x) - F(x) \rvert \xrightarrow{a.s.} 0 . \tag{1}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;How does this differ from simply stating the following?&lt;/strong&gt;&#10;$$\hat{F}_n(x) \xrightarrow{a.s.} F(x) \tag{2}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Using the definition of convergence almost surely, from (1):&#10;$$\begin{align*}&#10;\mathbb{P} \left( \lim_{n\rightarrow\infty} \left\lvert \sup_{x \in \mathbb{R}} \lvert \hat{F}_n(x) - F(x) \rvert \right\rvert = 0 \right) &amp;amp;= 1 \\&#10;\mathbb{P} \left( \lim_{n\rightarrow\infty} \sup_{x \in \mathbb{R}} \lvert \hat{F}_n(x) - F(x) \rvert = 0 \right) &amp;amp;= 1 \tag{1a}&#10;\end{align*}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Using the definition of convergence almost surely, from (2):&#10;$$\begin{align*}&#10;\mathbb{P} \left( \lim_{n\rightarrow\infty} \left\lvert \hat{F}_n(x) - F(x) \right\rvert = 0 \right) = 1 \tag{2a}&#10;\end{align*}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;To me, it seems that (1a) and (2a) are equivalent statements because of the least upper bound, and thus (1) and (2) are equivalent statements. But I have a feeling that I'm missing a subtle difference since otherwise I would think the Theorem would just be stated the simpler way (Equation (2)).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-22T20:33:51.803" Id="116374" LastActivityDate="2014-09-23T05:29:46.520" LastEditDate="2014-09-22T21:03:16.533" LastEditorUserId="36630" OwnerUserId="36630" PostTypeId="1" Score="4" Tags="&lt;probability&gt;&lt;convergence&gt;" Title="Glivenko-Cantelli Theorem" ViewCount="61" />
  
  
  <row Body="&lt;p&gt;I'll first try to share some intuition behind CNN and then comment the particular topics you listed.&lt;/p&gt;&#10;&#10;&lt;p&gt;The convolution and sub-sampling layers in a CNN are not different from the hidden layers in a common MLP, i. e. their function is to extract features from their input. These features are then given to the next hidden layer to extract still more complex features, or are directly given to a standard classifier to output the final prediction (usually a Softmax, but also SVM or any other can be used). In the context of image recognition, these features are images treats, like stroke patterns in the lower layers and object parts in the upper layers.&lt;/p&gt;&#10;&#10;&lt;p&gt;In natural images these features tend to be the same at all locations. Recognizing a certain stroke pattern in the middle of the images will be as useful as recognizing it close to the borders. So why don't we replicate the hidden layers and connect multiple copies of it in all regions of the input image, so the same features can be detected anywhere? It's exactly what a CNN does, but in a efficient way. After the replication (the &quot;convolution&quot; step) we add a sub-sample step, which can be implemented in many ways, but is nothing more than a sub-sample. In theory this step could be even removed, but in practice it's essential in order to allow the problem remain tractable.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thus:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Correct.&lt;/li&gt;&#10;&lt;li&gt;As explained above, hidden layers of a CNN are feature extractors as in a regular MLP. The alternated convolution and sub-sampling steps are done during the training and classification, so they are not something done &quot;before&quot; the actual processing. I wouldn't call them &quot;pre-processing&quot;, the same way the hidden layers of a MLP is not called so.&lt;/li&gt;&#10;&lt;li&gt;Correct.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;A good image which helps to understand the convolution is CNN page in the ULFDL tutorial. Think of a hidden layer with a single neuron which is trained to extract features from $3 \times 3$ patches. If we convolve this single learned feature over a $5 \times 5$ image, this process can be represented by the following gif:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/I7DBr.gif&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In this example we were using a single neuron in our feature extraction layer, and we generated $9$ convolved features. If we had a larger number of units in the hidden layer, it would be clear why the sub-sampling step after this is required.&lt;/p&gt;&#10;&#10;&lt;p&gt;The subsequent convolution and sub-sampling steps are based in the same principle, but computed over features extracted in the previous layer, instead of the raw pixels of the original image.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-09-22T23:04:27.443" Id="116390" LastActivityDate="2014-09-22T23:04:27.443" OwnerUserId="46039" ParentId="116362" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;R is typical - like most statistics packages, it uses QR decomposition for regression. &lt;/p&gt;&#10;&#10;&lt;p&gt;For fixed $n$, where $p&amp;lt;&amp;lt;n$, just calculating the decomposition itself is effectively quadratic in $p$ - doubling the number of predictors will multiply calculation time by around 4.&lt;/p&gt;&#10;&#10;&lt;p&gt;So if you go from $p=2$ (linear regression) to say $p=50$, you'd expect it to take something in the region of 600 times longer (in actuality probably somewhat less, for a variety of reasons).&lt;/p&gt;&#10;&#10;&lt;p&gt;So adding a lot of predictors will mean a substantially longer wait.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-23T01:24:45.853" Id="116400" LastActivityDate="2014-09-23T01:24:45.853" OwnerUserId="805" ParentId="116394" PostTypeId="2" Score="10" />
  <row AnswerCount="1" Body="&lt;p&gt;I have been reading the &lt;a href=&quot;http://www.ats.ucla.edu/stat/mult_pkg/faq/general/odds_ratio.htm&quot; rel=&quot;nofollow&quot;&gt;odds tutorial on UCLA's stats page&lt;/a&gt;. And I am trying to figure out if my interpretation of the results below is correct.  Based upon looking at the data the results seem to hold true. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Variables to Predict Submit or Cancel (1,0)&#10;DummyServiceA: Binary&#10;DummyServiceB: Binary&#10;DummyServiceC: Binary&#10;AT_START: Continous&#10;ID_SEQ: Continous&#10;TOT: Continous&#10;&#10;Logistic Regression Results &#10;exp(cbind(OR = coef(mymodel), confint(mymodel)))&#10;                                OR      2.5     97.5      &#10;DummyServiceA               0.3994   0.3215   0.4982   &#10;DummyServiceB               6.5028   5.1442   8.2549   &#10;DummyServiceC               0.2928   0.239    0.3604   &#10;AT_START                    0.9986   0.9984   0.9987   &#10;ID_SEQ                      0.949    0.94     0.9579   &#10;TOT                         0.9992   0.9984   0.9998   &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Odds of Submit are 60% lower if &lt;code&gt;ServiceA&lt;/code&gt; is selected instead of &lt;code&gt;ServiceB&lt;/code&gt; or &lt;code&gt;ServiceC&lt;/code&gt;&lt;/li&gt;&#10;&lt;li&gt;Odds of Submit are 550% higher if &lt;code&gt;ServiceB&lt;/code&gt; is selected instead of &lt;code&gt;ServiceA&lt;/code&gt; or &lt;code&gt;ServiceC&lt;/code&gt;&lt;/li&gt;&#10;&lt;li&gt;Odds of Submit are 71% lower if &lt;code&gt;ServiceC&lt;/code&gt; is selected instead of &lt;code&gt;ServiceA&lt;/code&gt; or &lt;code&gt;ServiceB&lt;/code&gt;&lt;/li&gt;&#10;&lt;li&gt;For every unit increase in &lt;code&gt;AT_START&lt;/code&gt; there is 0% change in odds for Submit&lt;/li&gt;&#10;&lt;li&gt;5% decrease in odds of Submit for every unit increase in &lt;code&gt;ID_SEQ&lt;/code&gt;&lt;/li&gt;&#10;&lt;li&gt;For every unit increase in &lt;code&gt;TOT&lt;/code&gt; there is 0% change in odds for Submit&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="1" CreationDate="2014-09-23T01:32:46.980" FavoriteCount="3" Id="116401" LastActivityDate="2014-09-23T08:29:47.920" LastEditDate="2014-09-23T01:49:31.127" LastEditorUserId="7290" OwnerUserId="56252" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;logistic&gt;&lt;interpretation&gt;&lt;logit&gt;" Title="Interpretation of odds in logistic regression" ViewCount="76" />
  <row AnswerCount="2" Body="&lt;p&gt;I'm looking for some advice on directions to head in a project I'm working on.&#10;Basically what I want to do is identify general (of varying size) groups in a 2-D grid of points belonging to one of three categories: red, blue or empty.  I'm not completely new to machine learning, but not familiar enough with the variety of methods to know which is best.  My first thought was still to use a clustering algorithm, since it seems like I'm basically looking for clusters of the types, but I was concerned with the low-dimensionality and concerned since I already know that they have in common, I more need to find the groups efficiently.  I more need to be able to identify whether is a group that needs to be further analysed or if it is a points or just a couple of points grouped together.  The problem size is small, 81-361 coordinates per analysis, but will have to be run after each update to the grid, most likely on the order of hundreds of times per run, but could extend to thousands.  I'd really like some input on this and appreciate any advice someone could impart on the topic.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-23T02:20:11.990" Id="116409" LastActivityDate="2014-09-28T22:48:54.173" LastEditDate="2014-09-23T02:32:43.747" LastEditorUserId="56257" OwnerUserId="56257" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;algorithms&gt;" Title="What's the best algorithm type for low-dimensional grouping" ViewCount="27" />
  <row Body="&lt;ol&gt;&#10;&lt;li&gt;Yes, this is very possible&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Yes, this will add power&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;You'll need to format the data according to your second example to use SPSS repeated measures. Then you need to select Analyze -&gt; General Linear Model -&gt; Repeated Measures. In the Within-Subject Factor Name box, first type Mode and select 3 for number of levels, then click add. Then enter Time into the Factor Name box, select 2 for number of levels, and click add. Then click 'Define'. You should be able to select the six variables you've created and they'll slot into the right within-subjects boxes. Then just adjust any settings you want, click OK, and you'll have the analysis you want. Here's the syntax, though for slightly different variable names:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;DATASET ACTIVATE DataSet0.&#10;GLM Mode1.Trial1Time Mode1.Trial2Time Mode2.Trial1Time Mode2.Trial2Time Mode3.Trial1Time Mode3.Trial2Time&#10;/WSFACTOR=Mode 3 Polynomial Time 2 Polynomial &#10;/METHOD=SSTYPE(3)&#10;/CRITERIA=ALPHA(.05)&#10;/WSDESIGN=Mode Time Mode*Time.&#10;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2014-09-23T02:38:01.817" Id="116411" LastActivityDate="2014-09-23T02:38:01.817" OwnerUserId="46273" ParentId="115135" PostTypeId="2" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm doing an independent study in Bayesian Statistics following some chapters from &lt;a href=&quot;http://www.stat.columbia.edu/~gelman/book/&quot; rel=&quot;nofollow&quot;&gt;BDA3&lt;/a&gt;. When solving the first question from Ch 10 I got stuck. It says:  &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;[If] a scalar variable $\theta$ is approximately normally distributed in a posterior distribution that is summarized by &lt;em&gt;n&lt;/em&gt; independent simulation draws [,] how large does &lt;em&gt;n&lt;/em&gt; have to be so that the 2.5% and 97.5% quantiles of $\theta$ are specified to an accuracy of $0.1\ {\rm sd}(\theta|y)$?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Just by following the formula for a numerical example in the same chapter I get $n=100$, but I do not understand actually what they mean by &lt;strong&gt;accuracy&lt;/strong&gt; in this context. &lt;/p&gt;&#10;&#10;&lt;p&gt;Any pointers?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-23T03:34:38.493" FavoriteCount="1" Id="116415" LastActivityDate="2014-09-23T03:49:31.240" LastEditDate="2014-09-23T03:49:31.240" LastEditorUserId="7290" OwnerUserId="56261" PostTypeId="1" Score="4" Tags="&lt;self-study&gt;&lt;bayesian&gt;&lt;accuracy&gt;" Title="Question 10.9 from Bayesian Data Analysis, what does accuracy mean here?" ViewCount="52" />
  <row Body="&lt;p&gt;K-means seems to be the best choice, especially with the low dimensionality of the problem.  Selecting the number of clusters seems likely to be be the most difficult part, but for the purpose of the project it may suffice to evenly subdivide the grid and then attempt the k-means.  I think it will accomplish what needs to be done here the best.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-23T03:56:24.127" Id="116416" LastActivityDate="2014-09-23T03:56:24.127" OwnerUserId="56257" ParentId="116409" PostTypeId="2" Score="0" />
  
  <row AnswerCount="0" Body="&lt;p&gt;In implementations of the &lt;code&gt;Metropolis-Hastings&lt;/code&gt; algorithm, how is the target distribution $\pi(\mathbf{x}) = P(\mathbf{x}|\mathbf{e})$ computed or estimated while computing the acceptance probability $\alpha(\mathbf{x'}|\mathbf{x}) = \min \left(1,\frac{\pi(\mathbf{x'})q(\mathbf{x}|\mathbf{x'})}{\pi(\mathbf{x})q(\mathbf{x'}|\mathbf{x})}\right)$?&lt;/p&gt;&#10;&#10;&lt;p&gt;For special cases of &lt;code&gt;Metropolois-Hastings&lt;/code&gt; such as &lt;code&gt;Gibbs Sampling&lt;/code&gt;, I understand that detailed balance of $q$ with $\pi$ simplifies the acceptance probably to $1$.&lt;/p&gt;&#10;&#10;&lt;p&gt;What I'm not understanding is how compute the target distribution when using something like a symmetric random walk proposal.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-09-23T03:59:03.870" Id="116417" LastActivityDate="2014-09-23T03:59:03.870" OwnerUserId="56262" PostTypeId="1" Score="2" Tags="&lt;mcmc&gt;&lt;metropolis-hastings&gt;" Title="Computing a Metropolis-Hastings target distribution?" ViewCount="42" />
  <row AnswerCount="0" Body="&lt;p&gt;I wish to test whether a spline-based smooth could be replaced by a linear term. I am using the &lt;code&gt;mgcv&lt;/code&gt; package in R.&lt;/p&gt;&#10;&#10;&lt;p&gt;Using the example from the &lt;a href=&quot;http://stat.ethz.ch/R-manual/R-devel/library/mgcv/html/smooth.construct.tp.smooth.spec.html&quot; rel=&quot;nofollow&quot;&gt;helpfile&lt;/a&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;require(mgcv); n &amp;lt;- 100; set.seed(2)&#10;x &amp;lt;- runif(n); y &amp;lt;- x + x^2*.2 + rnorm(n) *.1&#10;## is smooth significantly different from straight line?&#10;summary(gam(y~s(x,m=c(2,0))+x,method=&quot;REML&quot;)) ## not quite&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;produces the following output:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Family: gaussian &#10;Link function: identity &#10;&#10;Formula:&#10;y ~ s(x, m = c(2, 0)) + x&#10;&#10;Parametric coefficients:&#10;            Estimate Std. Error t value Pr(&amp;gt;|t|)    &#10;(Intercept) -0.02133    0.05315  -0.401    0.689    &#10;x            1.18249    0.10564  11.193   &amp;lt;2e-16 ***&#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1&#10;&#10;Approximate significance of smooth terms:&#10;        edf Ref.df     F p-value  &#10;s(x) 0.9334      8 0.304  0.0781 .&#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1&#10;&#10;R-sq.(adj) =   0.91   Deviance explained = 91.1%&#10;REML score = -70.567  Scale est. = 0.012767  n = 100&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Can I use the &quot;approximate significance of smooth terms&quot; to test the null hypothesis that a spline does not fit the data significantly better than a linear equation?&lt;/p&gt;&#10;&#10;&lt;p&gt;If so, &lt;strong&gt;how should I report the results&lt;/strong&gt; of this test?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-23T06:35:26.137" Id="116423" LastActivityDate="2014-09-23T06:35:26.137" OwnerUserId="179" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;&lt;curve-fitting&gt;&lt;splines&gt;&lt;gam&gt;" Title="How should I report the significance of a spline smooth obtained using mgcv in R?" ViewCount="67" />
  <row Body="&lt;p&gt;Graph cuts can optimize discrete (energy) functions of the form:&lt;/p&gt;&#10;&#10;&lt;p&gt;$E(\ell) = \sum_iD_i(\ell_i) + \sum_{i,j}S_{i,j}(\ell_i, \ell_j)$&lt;/p&gt;&#10;&#10;&lt;p&gt;where the $\ell_i$ assign a label to the i-th pixel, e.g. in your case they tell you which cluster, that was previously identified by KMeans, the pixel should belong to.&lt;/p&gt;&#10;&#10;&lt;p&gt;It seems that your code uses Gaussian potentials as data terms. As energy functions can be related to the negative log of probability distributions, the Gaussian potential also appears in the log domain in your graph cuts method, i.e. &lt;/p&gt;&#10;&#10;&lt;p&gt;$D_i(c) = -\log(\mathcal{N}(I_i | \mu_c, \Sigma_c)) = (I_i - \mu_c)^T \Sigma_c^{-1}(I_i - \mu_c) + \text{const}$&lt;/p&gt;&#10;&#10;&lt;p&gt;So basically you want to penalize quadratic differences from the pixel color to the mean color of the assigned cluster. The constant term does not depend on the cluster assignment and hence can be ignored.&lt;/p&gt;&#10;&#10;&lt;p&gt;The first piece of code uses the identity matrix for each $\Sigma_c$ and the second piece of code estimates $\Sigma_c$ from all datapoints clustered together by KMeans. The latter is done to account for clusters where the pixel colors are not spread evenly around the mean color in all directions. This picture illustrates this &lt;a href=&quot;http://mathworks.com/matlabcentral/fileexchange/screenshots/6502/original.jpg&quot; rel=&quot;nofollow&quot;&gt;http://mathworks.com/matlabcentral/fileexchange/screenshots/6502/original.jpg&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-09-23T06:47:25.487" Id="116425" LastActivityDate="2014-09-23T11:42:49.693" LastEditDate="2014-09-23T11:42:49.693" LastEditorUserId="35427" OwnerUserId="35427" ParentId="116317" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;You've used the &lt;code&gt;data-visualization&lt;/code&gt; tag and that's certainly a place you can start.&lt;/p&gt;&#10;&#10;&lt;p&gt;You might do a pairs plot (scatterplot matrix) which can sometimes give an indication of nonlinearity. However, sometimes it is hard to see clearly because of the effects of the other variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;Another alternative is to fit linear models to both and check added-variable plots (or perhaps partial residual plots). Nonlinearity should be better visible there, but it may take adjusting for the nonlinearity in some of the other variables before some of the nonlinearity is clear. &lt;/p&gt;&#10;&#10;&lt;p&gt;By contrast, the one that's actually linear should not show nonlinearity in added-variable plots.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-23T07:42:03.033" Id="116432" LastActivityDate="2014-09-23T07:49:11.063" LastEditDate="2014-09-23T07:49:11.063" LastEditorUserId="805" OwnerUserId="805" ParentId="116428" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;There is a related problem of dividing a series or sequence into spells with ideally constant values. See &lt;a href=&quot;http://stats.stackexchange.com/questions/67571/how-can-i-group-numerical-data-into-naturally-forming-brackets-e-g-income&quot;&gt;How can I group numerical data into naturally forming &amp;quot;brackets&amp;quot;? (e.g. income)&lt;/a&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;It's not quite the same problem as the question doesn't exclude spells with slow drift in any or all directions, but without abrupt changes. &lt;/p&gt;&#10;&#10;&lt;p&gt;A more direct answer is to say that we are looking for big jumps, so the only real issue is to define jump. The first idea is then just to look at first differences between neighbouring values. It's not even clear that you need to refine that by removing noise first, as if jumps can't be distinguished from differences in noise, they surely can't be abrupt. On the other hand, the questioner evidently wants abrupt change to include ramped as well as stepped change, so some criterion such as variance or range within fixed-length windows seems called for. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-23T09:28:32.937" Id="116442" LastActivityDate="2014-09-23T09:28:32.937" OwnerUserId="22047" ParentId="116363" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;You could try &lt;a href=&quot;http://en.wikipedia.org/wiki/Additive_smoothing&quot; rel=&quot;nofollow&quot;&gt;additive smoothing&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-09-23T12:08:15.733" Id="116453" LastActivityDate="2014-09-23T12:08:15.733" OwnerUserId="4600" ParentId="116438" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="116503" AnswerCount="1" Body="&lt;p&gt;I have traditionally used this simple test of interaction (&lt;a href=&quot;http://www.bmj.com/content/326/7382/219&quot; rel=&quot;nofollow&quot;&gt;http://www.bmj.com/content/326/7382/219&lt;/a&gt;) for comparing effect sizes between studies, and I would like to start performing it in R. Although I have scripted a custom function for it, I have also been told about the &quot;metafor&quot; package and decided to give it a try. However, I am not able to reproduce the results of the paper using the &lt;i&gt;rma&lt;/i&gt; function, probably because I do not understand the statistical basis behind it. Using the log RRs (-0.4005 and -0.1278) and SEs (0.1929 and 0.1070) from the paper, if I try to fit a fixed-effects model, here is what I get:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Fixed-Effects Model (k = 2)&#10;&#10;estimate       se     zval     pval    ci.lb    ci.ub          &#10; -0.1920   0.0936  -2.0518   0.0402  -0.3754  -0.0086        * &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I thought it could be due to the inverse-variance weighting, and when I disabled it the estimate got closer to that of the paper, but the standard error is still off by a wide margin, and so are the rest of the values.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Fixed-Effects Model (k = 2)&#10;&#10;estimate       se     zval     pval    ci.lb    ci.ub          &#10; -0.2642   0.1103  -2.3950   0.0166  -0.4803  -0.0480        * &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So, I'm assuming that the z-values shown in the Altman &amp;amp; Bland paper and in the &lt;i&gt;rma&lt;/i&gt; output are different. Does someone know what their difference is? Can the fixed-effects model included in &quot;metafor&quot; be used for these kind of simple interaction tests?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-23T13:04:52.447" Id="116457" LastActivityDate="2014-09-23T19:17:50.813" OwnerUserId="56284" PostTypeId="1" Score="1" Tags="&lt;interaction&gt;&lt;meta-analysis&gt;" Title="Metafor package: RMA and testing for interaction in two studies (Altman &amp; Bland 2003)" ViewCount="47" />
  
  
  
  <row Body="&lt;p&gt;If the 4 categories of each scale are numbers where the distance between each value is assumed to be equal, then you have interval level data and you can try a pearson regression although truncating the scale to 4 levels might create problems. If the 4 categories are not numeric but were assigned verbal descriptions then you probably have an ordinal scale in which case spearman ranks will be appropriate. A final option is to treat the data as nominal and run a chi-square test for independence (called crosstabs in SPSS). &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-23T16:40:22.840" Id="116479" LastActivityDate="2014-09-23T16:40:22.840" OwnerUserId="56298" ParentId="116474" PostTypeId="2" Score="0" />
  
  
  
  
  
  <row Body="&lt;p&gt;The standard effect size for a one-sample t-test is the difference between the sample mean and the null value in units of the sample standard deviation:&lt;br&gt;&#10;$$&#10;d = \frac{\bar x - \mu_0}{s}&#10;$$&#10;The interpretation here is essentially the same as for the two-sample version of the standardized mean difference: it is the number of standard deviations that your distribution diverges on average.  As in most cases with effect sizes, you can think of it as taking the $N$ out of your test statistic.  Thus, with a test statistic / $p$-value you get a sense of the confidence you have in your result, but these conflate the size with $N$, so from a small $p$ you don't know if you have a big effect with a small $N$ or a small effect with a big $N$.  Here, you would get a point estimate of the magnitude of the shift, but you don't know from $d = .5$ whether or not you can be confident that the true effect isn't $0$.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-23T23:36:31.750" Id="116531" LastActivityDate="2014-09-24T14:21:59.753" LastEditDate="2014-09-24T14:21:59.753" LastEditorUserId="7290" OwnerUserId="7290" ParentId="116514" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;This tag is for questions that seek a conceptual / non-mathematical understanding of statistics.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-23T23:40:17.660" Id="116532" LastActivityDate="2014-09-23T23:40:17.660" LastEditDate="2014-09-23T23:40:17.660" LastEditorUserId="7290" OwnerUserId="7290" PostTypeId="5" Score="0" />
  <row Body="&lt;p&gt;Not only is it okay to code it as 0/1, it's &lt;a href=&quot;http://en.wikipedia.org/wiki/Dummy_variable_%28statistics%29&quot; rel=&quot;nofollow&quot;&gt;common to do so&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;It feels like an entry of zero would mean that term will always contribute zero to the outcomeVariable, no matter the beta coefficient.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Correct. This is not a problem. The intercept has the mean for the 0 category, and the coefficient for the 0/1 variable has the difference in means between the two categories.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;So will Beta only be meaningful when DayorNight=1? If you choose -1 vs 1, or 1 vs 2, then your fit parameters and pvalues change. That troubles me.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;The p-value for this variable shouldn't change from such recoding. The estimated coefficient will change, of course, and the coefficient (and p-value) for the intercept will change, because such recoding changes the meaning of the intercept.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;If you force that variable to be categorical in statsmodels or R, it's just like using 0 or 1. &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;It doesn't matter whether it's numeric or a factor. In R, if you use a two level factor (Say with levels &lt;code&gt;Day&lt;/code&gt; and &lt;code&gt;Night&lt;/code&gt;), it simply creates a 0/1 variable itself. You can see this by calling &lt;code&gt;model.matrix&lt;/code&gt; on the RHS of your model formula.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-09-24T00:18:28.867" Id="116537" LastActivityDate="2014-09-24T00:18:28.867" OwnerUserId="805" ParentId="116534" PostTypeId="2" Score="2" />
  
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;This is from a typical introduction to kernel density estimation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose we want to estimate the probability &lt;strong&gt;density&lt;/strong&gt; function $p(x)$ given a set of samples $x_1,x_2 \ldots x_N$. The simplest method that does this is from the histogram of the samples. &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Divide the sample space into a number of bins and approximate the&#10;  &lt;strong&gt;density&lt;/strong&gt; at the center of each bin by the fraction of points in the &#10;  training data that fall into the corresponding bin.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Suppose we have divided the sample space into $K$ bins of width $h$. The histogram of the given data is computed as :&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;H(k) = \# \hspace{1mm} \mbox{of} \hspace{1mm} x^{(i)} \hspace{1mm} \mbox{in bin k }&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;This can be converted into a probability &lt;strong&gt;density&lt;/strong&gt; as&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;P(k) = \frac{H(k)}{N}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;For a given sample $x$, let $p_k(x)$ be the probability that $x$ falls in bin $k$. From above, we have&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;p_k(x) = P(k)&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Why is it that the above is mentioned further normalized by bin width $h$ as follows ?&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;p_k(x) = \frac{\# \hspace{1mm} \mbox{of} \hspace{1mm} x^{(k)} \hspace{1mm} \mbox{in same bin as x} }{N h}&#10;$$&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-09-24T12:02:41.437" Id="116589" LastActivityDate="2014-09-24T15:20:21.473" OwnerUserId="25458" PostTypeId="1" Score="1" Tags="&lt;pdf&gt;&lt;kernel-density-estimate&gt;" Title="Probability distribution estimation -- why normalize by bin width?" ViewCount="108" />
  <row AnswerCount="0" Body="&lt;hr&gt;&#10;&#10;&lt;p&gt;Hi I have sales data for previous 3 years(6 half years). I need to predict / forecast the sales for next 1-2 years. Tell me which method / model I should use. &lt;/p&gt;&#10;&#10;&lt;p&gt;As always sales dependent on country specific macro variables and account specific micro variable like the cash in hand ( size of the firm ) etc. &lt;/p&gt;&#10;&#10;&lt;p&gt;Can I create different models one including these variables another excluding these variables ?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-24T12:05:47.720" Id="116591" LastActivityDate="2014-09-24T12:05:47.720" OwnerUserId="56347" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;modeling&gt;&lt;forecasting&gt;" Title="Need to forecast a small data set. Suggest best method to go about" ViewCount="34" />
  <row Body="&lt;p&gt;There are several subtle points I think you missed:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;In a mechanistic world there may be a true model (e.g., Newtonian mechanics) but it other situations this is very unlikely.  Instead one usually finds that when more and better data become available one can get a better model.  BIC assumes there is a single true model.&lt;/li&gt;&#10;&lt;li&gt;Selecting a model according to the best BIC is a biased process that will result in overfitting.&lt;/li&gt;&#10;&lt;li&gt;$R_{adj}^2$ was designed to de-bias an ordinary $R^2$ in a linear model when there is a &lt;em&gt;single pre-specified&lt;/em&gt; model.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I believe that the only thing you can assume is that if your sample is very large and is representative of the future to which you want to apply a forecast, the apparent $R^2$ is an upper bound of the real $R^2$.&lt;/p&gt;&#10;" CommentCount="12" CreationDate="2014-09-24T13:11:50.297" Id="116596" LastActivityDate="2014-09-24T13:11:50.297" OwnerUserId="4253" ParentId="116590" PostTypeId="2" Score="4" />
  
  
  <row Body="&lt;p&gt;A quick and dirty solution using &lt;a href=&quot;http://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29#Parametric_bootstrap&quot; rel=&quot;nofollow&quot;&gt;parametric bootstrap&lt;/a&gt; that will give you a confidence interval would look like the following in &lt;a href=&quot;http://www.r-project.org/&quot; rel=&quot;nofollow&quot;&gt;R&lt;/a&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# Made up numbers, substitute with your own&#10;n_wa &amp;lt;- 50&#10;n_wb &amp;lt;- 50&#10;n_wc &amp;lt;- 50&#10;n_a &amp;lt;- 100&#10;n_b &amp;lt;- 100&#10;n_c &amp;lt;- 100&#10;&#10;# Parametric bootstrap&#10;no_boot_samp &amp;lt;- 10000&#10;n_wa_boot &amp;lt;- rbinom(no_boot_samp, size = n_a, prob = n_wa / n_a)&#10;n_wb_boot &amp;lt;- rbinom(no_boot_samp, size = n_b, prob = n_wb / n_b)&#10;n_wc_boot &amp;lt;- rbinom(no_boot_samp, size = n_c, prob = n_wc / n_c)&#10;p_wa_boot &amp;lt;- n_wa_boot / n_a&#10;p_w_boot &amp;lt;- (n_wa_boot + n_wb_boot + n_wc_boot) / (n_a + n_b + n_c)&#10;gci_boot &amp;lt;- p_wa_boot / p_w_boot&#10;&#10;# Calculating the 95% quantile bootstrap confidence intervall&#10;quantile(gci_boot, c(0.025, 0.975))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Using these made up data the output is here:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;  2.5%  97.5% &#10;0.8382 1.1600 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-09-24T15:31:21.060" Id="116614" LastActivityDate="2014-09-24T15:31:21.060" OwnerUserId="6920" ParentId="115885" PostTypeId="2" Score="3" />
  <row AnswerCount="1" Body="&lt;p&gt;Explain why a correlation between two variables does not necessarily imply that one variable causes the other to vary as it does. Give an example.&lt;/p&gt;&#10;" ClosedDate="2014-09-24T17:27:14.060" CommentCount="2" CreationDate="2014-09-24T16:24:37.877" Id="116626" LastActivityDate="2014-09-24T17:27:01.023" OwnerUserId="56365" PostTypeId="1" Score="0" Tags="&lt;correlation&gt;" Title="Confused on correlation between variables and variation" ViewCount="30" />
  <row Body="&lt;p&gt;Example from Wikipedia: &lt;a href=&quot;http://en.wikipedia.org/wiki/Correlation_does_not_imply_causation&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Correlation_does_not_imply_causation&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose during a summer, ice cream sales increases. Also, the rate of drowning deaths increases sharply as well. Can we conclude that therefore ice cream consumption causes drowning? Obviously not, and it is easy to see a &quot;third variable&quot; involved here: the proportion of the population going for a swim or water-related activities.&lt;/p&gt;&#10;&#10;&lt;p&gt;This &quot;third variable&quot; is otherwise known as a confounder, and confounding is one of many mechanisms why correlation between two variables does not necessarily imply that one variable causes the other to vary as it does.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-24T16:34:10.597" Id="116627" LastActivityDate="2014-09-24T16:34:10.597" OwnerUserId="56362" ParentId="116626" PostTypeId="2" Score="0" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm new in the use of R&lt;/p&gt;&#10;&#10;&lt;p&gt;I should ask you for a question,&lt;/p&gt;&#10;&#10;&lt;p&gt;I use the drc package for estimate weibull fitting curve to my data of inhibition of bioluminescence of Vibrio fischeri to different level of contaminants.&lt;/p&gt;&#10;&#10;&lt;p&gt;I use the fct W2.2&lt;/p&gt;&#10;&#10;&lt;p&gt;1) I want to ask you if someone know how I can obtain the two parameters &quot;shape&quot; and &quot;scale&quot; of weibull function.&lt;/p&gt;&#10;&#10;&lt;p&gt;2) for the mixture response (two or more contaminants) there is some functions that allows me to obtain the &quot;concentration addition&quot; (CA) curve and the &quot;indipendent action&quot; (IA) curve?&lt;/p&gt;&#10;&#10;&lt;p&gt;thanks in advance!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-24T17:29:09.717" Id="116640" LastActivityDate="2014-09-24T17:29:09.717" OwnerUserId="56376" PostTypeId="1" Score="0" Tags="&lt;weibull&gt;" Title="Weibull Concentration addition Indipendent action" ViewCount="6" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a univariate model in which I am looking to predict the number of articles per week in a newspaper about a protest (count data) by how many arrests of protesters occurred per week. I have 148 weeks in total. My hunch is that I want to lag the independent (or dependent?) variable. Basically I would predict that the arrests of the previous week influence the amount of news coverage for the following week. In simple terms, what kind of model specification would that require? OLS regression? &lt;/p&gt;&#10;&#10;&lt;p&gt;I've tried the following in R:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    &amp;gt; library(dyn)&#10;    &amp;gt; yt &amp;lt;- (OWS$Stories)&#10;    &amp;gt; yt &amp;lt;- ts(OWS$Stories)&#10;    &amp;gt; xt &amp;lt;- ts(OWS$Arrests)&#10;    &amp;gt; dyn$lm(yt ~ xt + lag(yt, -1))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The R^2 is great (.82) and both the indy variable as well as the lagged dependent variable are significant are significant in this model. But I worry that this model ignores the count variable nature of the dependent variable. Not sure if this is related, but I also ran a Durbin Watson test and there appears to be significant autocorrelation w/ my dependent variable. &lt;/p&gt;&#10;&#10;&lt;p&gt;The more complicated part of this is that it's time series count data. I keep reading about poisson and negative binomial models. I guess I can't use these though if I want to capture the time series element? So now I'm reading about poisson autoregression, but I don't know if that's the appropriate model either. I'm thinking I might want to re-specify the model to include a lagged X (arrests from previous week) as well as a lagged Y (stories from previous week). Can anyone point me to the proper specification for a good-fitting model here? Thanks!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-24T18:57:38.027" Id="116648" LastActivityDate="2014-09-24T21:24:32.467" LastEditDate="2014-09-24T21:24:32.467" LastEditorUserId="56386" OwnerUserId="56386" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;lags&gt;&lt;univariate&gt;" Title="What type of model should I use? (Time series, univariate, dependent variable is a count)" ViewCount="34" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a set of data with a dependent variable which represents a proportion, and many of the samples contain a response of 1.  I would like to build a random forest or GBM regression model on the data, with my main concern being the out of sample performance of E[y|x].  Are there any pitfalls to simply minimizing a squared error cost function over proportion data?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-09-24T19:22:14.403" Id="116652" LastActivityDate="2014-09-24T19:22:14.403" OwnerUserId="56389" PostTypeId="1" Score="0" Tags="&lt;random-forest&gt;&lt;proportion&gt;&lt;gbm&gt;" Title="Pitfalls of using random forest/GBM on proportion data?" ViewCount="62" />
  <row Body="&lt;p&gt;Models with mixed effects have 2 stochastic ingredients:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Residual errors&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Random effects&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;If you use a random slope and notice that the model performs better, it means that there is variability that was not properly captured by the residual errors, the random intercept, and fixed effects. Another direction that you can take is to be flexible on the residual errors (e.g. by using a Student-t instead of a normal distribution for the residual errors). Now the question is: which model performs better? The one with normal errors or with Student-t errors? The one with random slope and normal errors? The one with random slope and Student-t errors? ... And so on. If you can implement all of them and compare them, you can gain understanding on the features of the data. For example, if your model selection favours the model with Student-t errors and no random slope, it tells you that a single slope fits the data better (no variability among subjects in terms of the slope) but there might be some outliers that require a distribution with heavier tails than normal.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-24T20:09:58.130" Id="116660" LastActivityDate="2014-09-24T20:09:58.130" OwnerUserId="56391" ParentId="116636" PostTypeId="2" Score="4" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a predictive model function &lt;code&gt;mPred&lt;/code&gt; and two different input variables matrices of the same size (nxm), &lt;code&gt;iM1&lt;/code&gt; and &lt;code&gt;iM2&lt;/code&gt;. If I apply &lt;code&gt;mPred&lt;/code&gt; to both matrices I get two different vectors that are not of same size, &lt;code&gt;v1 size p&lt;/code&gt; and &lt;code&gt;v2 size q&lt;/code&gt;, of output variable &lt;code&gt;Y&lt;/code&gt;. The difference in terms of vectors size is because I'm only interested in values of &lt;code&gt;Y&lt;/code&gt; between a given range so I have to filter &lt;code&gt;v1&lt;/code&gt; and &lt;code&gt;v2&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now I'd like to compare &lt;code&gt;v1&lt;/code&gt; and &lt;code&gt;v2&lt;/code&gt;, namely in terms of differences between them. In the end, I'd like to obtain the &lt;code&gt;empirical cdf of the differences between v1 and v2&lt;/code&gt; (is it possible?). But I have some questions:&lt;/p&gt;&#10;&#10;&lt;p&gt;1- How can I make vectors &lt;code&gt;v1&lt;/code&gt; and &lt;code&gt;v2&lt;/code&gt; of the same size (do I need it?), without changing their statistical properties, i.e. resize the larger of the vectors by removing values uniformly over the range of the vector? To assist on this task I could obtain the histograms of &lt;code&gt;v1&lt;/code&gt; and &lt;code&gt;v2&lt;/code&gt; with the same bin's size and then remove uniformly one element from each bin until the sizes of each vector are the same. Is this reasoning OK? Or are there better methods?&lt;/p&gt;&#10;&#10;&lt;p&gt;2- After I have the vector with the same size I can make calculations between them without incurring in significant errors and bias. I'm particularly interested in analysing the differences between &lt;code&gt;v1&lt;/code&gt; and &lt;code&gt;v2&lt;/code&gt;. Should I subtract the histograms of &lt;code&gt;v1&lt;/code&gt; and &lt;code&gt;v2&lt;/code&gt;? or should I subtract the empirical cdf of each vector?&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm working on R and python but at this time I'd like to establish a procedure rather then the methods to do it. &lt;/p&gt;&#10;&#10;&lt;p&gt;Any help is appreciated. Thanks&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-24T21:01:40.820" Id="116671" LastActivityDate="2014-09-24T23:22:34.780" LastEditDate="2014-09-24T23:22:34.780" LastEditorUserId="13564" OwnerUserId="13564" PostTypeId="1" Score="0" Tags="&lt;group-differences&gt;&lt;histogram&gt;&lt;cdf&gt;&lt;ecdf&gt;" Title="Difference between two vectors of unequal size" ViewCount="22" />
  
  
  <row Body="&lt;p&gt;The conditional expectation for a normal random variable with mean $\tau$ and standard deviation $\sigma$ can be found from this formula, which is available, for example, here: www.actuaries.org/LIBRARY/ASTIN/vol35no1/189.pdf ‎&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ E[X | X &amp;gt; q ]={\tau + {{\sigma} \phi \left({{q-\tau} \over \sigma} \right) \over {1-\Phi \left( {{q-\tau} \over \sigma}\right)}  } },$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\Phi$ is the CDF and $\phi$ is the PDF for the standard normal. The general formula that can be used to derive these type of expectations is given by &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ E[X | X &amp;gt; q ]={ {\int_q^\infty xf(x)dx} \over {P[X&amp;gt;q]}}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Putting these together with your parameters, we have&lt;/p&gt;&#10;&#10;&lt;p&gt;$$E[Y | Y &amp;gt; \mu+c  ]={  {{{\int_{\mu+c}^\infty {y \over {\sigma \sqrt{2\pi}}} &#10;e^{{-{\left  ( {y-\mu-w} \right)^2}} \over {2 \sigma^2}} dy} }\over {1-\Phi \left( {{c-w} \over \sigma}\right)}  }=\mu+w +\left[{\sigma {\phi \left( {c-w} \over \sigma \right) } \over {1-\Phi\left( {c-w} \over \sigma \right)}} \right] }$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, then, we can solve for the integral you want:&lt;/p&gt;&#10;&#10;&lt;p&gt;$${\int_{\mu+c}^\infty {y \over {\sigma \sqrt{2\pi}}} &#10;e^{{-{\left  ( {y-\mu-w} \right)^2}} \over {2 \sigma^2}} dy}=\left(\mu+w \right) \left[{1-\Phi\left( {c-w} \over \sigma \right)}  \right]+\sigma {\phi \left( {c-w} \over \sigma \right) }$$&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-24T22:47:40.257" Id="116683" LastActivityDate="2014-09-24T22:47:40.257" OwnerUserId="24073" ParentId="115319" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;They are identically the same.&lt;/p&gt;&#10;&#10;&lt;p&gt;If $X$ is a nominal feature with $k$ different values and $C$ is your target class with $m$ classes.&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{align}&#10;\mbox{MI} &amp;amp;= \sum_{i=1}^k\sum_{j=1}^m P(x_i,c_j)\log \frac{P(x_i,c_j)}{P(x_i)P(c_j)}\\&#10;&amp;amp;=-\sum_{i=1}^k\sum_{j=1}^m P(x_i,c_j)\log P(c_j)  + \sum_{i=1}^k\sum_{j=1}^m P(c_j|x_i)P(x_i)\log P(c_j|x_i)\\&#10;&amp;amp;= -\sum_{j=1}^m P(c_j)\log P(c_j)  + \sum_{i=1}^kP(x_i)\sum_{j=1}^m P(c_j|x_i)\log P(c_j|x_i)\\&#10;&amp;amp;= H(C) - H(C|X) = \mbox{IG}&#10;\end{align}&lt;/p&gt;&#10;&#10;&lt;p&gt;This is a case of inconsistent naming. You might want to have a look at this &lt;a href=&quot;http://stats.stackexchange.com/questions/13389/information-gain-mutual-information-and-related-measures&quot;&gt;question&lt;/a&gt; too. &lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-09-24T23:21:53.663" Id="116686" LastActivityDate="2014-09-24T23:21:53.663" OwnerUserId="2719" ParentId="116679" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;The links you refer to define the log of the relative profile likelihood, (which I'll here call $r(\theta_0)$).&lt;/p&gt;&#10;&#10;&lt;p&gt;$r(\theta_0) = \log\cal{L}(\hat{\theta},\hat{\delta})-\log\cal{L}(\theta_0,\hat{\delta})$&lt;/p&gt;&#10;&#10;&lt;p&gt;Since you didn't specify a problem with that I presume you're able to compute it in particular instances of interest to you.&lt;/p&gt;&#10;&#10;&lt;p&gt;The idea is that a an asymptotic test of $\theta=\theta_0$ (against the two-tailed alternative) is obtained by comparing $2r$ with a $\chi^2_1$; if it exceeds the upper $\alpha$ (say 5%) point of the chi-square, you'd reject at the 5% level.&lt;/p&gt;&#10;&#10;&lt;p&gt;To turn that into a CI, simply define the $1-\alpha$ CI to be the set of $\theta_0$ values not rejected by that test.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-09-25T05:46:01.723" Id="116704" LastActivityDate="2014-09-25T05:46:01.723" OwnerUserId="805" ParentId="116692" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;You could try using weighted finite state transducers. First create character to word transducer L. Then create a weighted edit transducer E to assign penalties to the error tyopes. Represent your input as automata and then compose each of the machines and find the lowest cost path through the resulting machine. The following website should be of help&#10;&lt;a href=&quot;http://blog.notdot.net/2010/07/Damn-Cool-Algorithms-Levenshtein-Automata&quot; rel=&quot;nofollow&quot;&gt;http://blog.notdot.net/2010/07/Damn-Cool-Algorithms-Levenshtein-Automata&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-25T06:27:57.307" Id="116705" LastActivityDate="2014-09-25T06:27:57.307" OwnerUserId="13306" ParentId="108286" PostTypeId="2" Score="0" />
  
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;1) Can someone explain me the idea of this sentence:&lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;It is noted that nonparametric tests were employed because of the non-continuous nature of the scale scores&quot; (this is related to scale validity that is tested after survey)&lt;/p&gt;&#10;&#10;&lt;p&gt;2) What kind of tests could be used?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-09-25T14:05:42.720" Id="116748" LastActivityDate="2014-09-25T14:38:22.800" LastEditDate="2014-09-25T14:38:22.800" LastEditorUserId="22047" OwnerUserId="24973" PostTypeId="1" Score="0" Tags="&lt;scales&gt;&lt;validation&gt;" Title="what &quot;non-continuous nature of scores&quot; means" ViewCount="15" />
  <row Body="&lt;p&gt;1) Non-continuous means discrete. The scale scores could e.g. have been 0 to 5 with &lt;strong&gt;discrete&lt;/strong&gt; steps of 1. If any value could have been filled in e.g. 1.4, 4.332 etc. than the scale is said to be continuous. &lt;/p&gt;&#10;&#10;&lt;p&gt;2) There is a wide variety of non-parametric tests, depending on what you want to investigate. Check for example &lt;a href=&quot;http://www.stats.gla.ac.uk/steps/glossary/nonparametric.html&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; for some possible tests.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-25T14:29:38.727" Id="116754" LastActivityDate="2014-09-25T14:29:38.727" OwnerUserId="29894" ParentId="116748" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;Except from the two extremes, i.e. the events $\{H,H,H,H\}$ and $\{T,T,T,T\}$, the intermediate events can happen in more ways than one, and this is because your random variable does not specify the exact tosses where head or tails occur, it is concerned only with their number of appearances. So for example, the event &quot;3 heads&quot; can happen in 4 different ways: $\{T,H,H,H\}, \{H,T,H,H\},\{H,H,T,H\},\{H,H,H,T\}$, and &lt;em&gt;each&lt;/em&gt; of these events has probability $(0.35)^3\cdot 0.65$. So $P(3 Heads) = 4\cdot(0.35)^3\cdot 0.65 $  &lt;/p&gt;&#10;&#10;&lt;p&gt;I guess you can take it from there.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-09-25T14:39:01.597" Id="116757" LastActivityDate="2014-09-25T14:39:01.597" OwnerUserId="28746" ParentId="116747" PostTypeId="2" Score="2" />
  <row AnswerCount="0" Body="&lt;p&gt;I am writing a program to compute estimated values.&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose I I have a prior (discrete) distribution T that I can sample from, but don't know the analytic PMF.  That is to say I have a program that samples from T and each t sampled from T appears with frequency equal to the true (but currently unknown!)  prior distribution.  BUT, given a t, I wouldn't know what the prior on it is.&lt;/p&gt;&#10;&#10;&lt;p&gt;For each t in T I want to compute f(t).  This is the desired value.  But there are many t in T so it's impossible to get all t so MCMC is used.&lt;/p&gt;&#10;&#10;&lt;p&gt;So I am writing an MCMC program that has t from T as its states.  Currently, it has a t.  There it computes f(t).  Then a new t*  is proposed given current t.  BUT the new t* is iid from t because  t* is acquired by sampling from T (see previous paragraph).  An alternative would be to take t, then generate t* by something like t*=t+delta (where delta is some small change but doesn't include just any t* in T).  In any case, the ratio [f(t*)&lt;em&gt;P(t&lt;/em&gt;)]/[f(t)&lt;em&gt;P(t)] is computed where P(t) is the prior of t and P(t&lt;/em&gt;) is the prior of t*.  Since, given a t (from T) I don't know how to compute the prior I assign it uniform prior so the P(t) and P(t*) cancel out in the ratio.  That way the ration becomes f(t*)/f(t).  The new t* (proposal) is accepted with probability equal to min(1,f(t*)/f(t)).  Each time, f(t) is computed as MCMC iterates and the final estimate of f(t) is computed as the average f(t) over the sampled t values sampled during the course of MCMC.&lt;/p&gt;&#10;&#10;&lt;p&gt;=============================================================&lt;/p&gt;&#10;&#10;&lt;p&gt;Is the way I am assign the priors to t and t* okay given that the t and t* are iid from T???  How would things be different if t* is acquired by modifying t a little bet (the &quot;delta&quot;) AND assigning to the t and t* the true priors rather than the uniform prior???&lt;/p&gt;&#10;&#10;&lt;p&gt;I am exploring mathematical statistical ideas on how to compute P(t) (the prior) for a given t.  but need to study/work more for that.&lt;/p&gt;&#10;&#10;&lt;p&gt;Besides all this understanding/using MCMC better and more effectively is a goal.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-09-25T14:39:21.970" Id="116758" LastActivityDate="2014-09-25T14:39:21.970" OwnerUserId="56436" PostTypeId="1" Score="0" Tags="&lt;distributions&gt;&lt;mcmc&gt;&lt;uniform&gt;&lt;uninformative-prior&gt;" Title="validity of MCM proposal distribution? uniform prior?" ViewCount="43" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Why does the curse of dimensionality mimic multicollinearity, in the following sense..&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Consider the random vector $Y = [y_{1}, \dots, y_{4}]$  where each element is ~ Uniform (0,1).&lt;/li&gt;&#10;&lt;li&gt;Take 10 samples of $Y$.  Call this your dependent variable observations.&lt;/li&gt;&#10;&lt;li&gt;Let the independent variable $X$ be 1 for the first 5 samples of $Y$, and 0 for the last 5 samples.&lt;/li&gt;&#10;&lt;li&gt;Fit a multivariate regression for  $Y | X$    (allow there to be a constant term in the regression).&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Of course, the above fit succeeds.  No problem.&lt;br&gt;&#10;Now let's increase the dimensionality.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Now let $W = [w_{1}, \dots, w_{300}]$  &lt;/li&gt;&#10;&lt;li&gt;Take 10 samples of $W$, as above.&lt;/li&gt;&#10;&lt;li&gt;Use the same $X$ as above.&lt;/li&gt;&#10;&lt;li&gt;Fit a multivariate regression for $Y | X$.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;The fit fails due to multicollinearity (says Matlab).&lt;br&gt;&#10;I can try scaling the observations by large positive constants - still fails.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Why does increasing the dimension mimic multicollinearity?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;If I try a MANOVA test, I encounter same problem (of course, because ANOVA &lt;em&gt;is&lt;/em&gt; regression).&lt;/p&gt;&#10;&#10;&lt;p&gt;Note:  if I impose the constraint that the covariance matrix is diagonal, then it works.  Why?&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Matlab code for the  above, for anybody interested:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Y = rand(300,10);&#10;X = [ repmat([1 0],  5,1)  ;  repmat([1 1],  5,1) ];&#10;mvregress(X,Y')&#10;% generates error:  &quot;Covariance is not positive-definite.&quot;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="3" CreationDate="2014-09-25T18:35:36.563" Id="116789" LastActivityDate="2014-09-25T21:51:12.560" LastEditDate="2014-09-25T19:43:40.520" LastEditorUserId="22060" OwnerUserId="22060" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;anova&gt;&lt;multicollinearity&gt;&lt;multivariate-regression&gt;&lt;high-dimensional&gt;" Title="Curse of dimensionality mimics multicollinearity?" ViewCount="55" />
  <row Body="&lt;p&gt;Zuur et al., and Faraway (from @janhove's comment above) are right; using likelihood-based methods (including AIC) to compare two models with different fixed effects that are fitted by REML will generally lead to nonsense.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-09-25T20:09:59.610" Id="116796" LastActivityDate="2014-09-25T20:09:59.610" OwnerUserId="2126" ParentId="116770" PostTypeId="2" Score="4" />
  
  <row AnswerCount="4" Body="&lt;p&gt;In my logistic regression the sign of coefficients of a variable (location distance of an amenity) changes based on other variables (with time -ve, with travel distance +ve) in the model. When the location distance is the only variable in the model, it has +ve sign. &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Should the variable need to maintain the +ve sign no matter what other variables are added in the model?&lt;/li&gt;&#10;&lt;li&gt;Does changing sign signify a multicollinearity issue? Some IVs are gaining significance while in a bivariate model, they didn't show significance and vice versa.&lt;/li&gt;&#10;&lt;li&gt;Is it okay to add variables that don't have much significance (ex: travel distance has a significance of 0.33 individually, but 0.05 when added with other variables) but becomes significant in the model?&#10;thanks.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="1" CreationDate="2014-09-25T21:49:50.467" Id="116804" LastActivityDate="2014-09-27T15:52:39.053" LastEditDate="2014-09-26T15:59:55.580" LastEditorUserId="22047" OwnerUserId="20710" PostTypeId="1" Score="2" Tags="&lt;logistic&gt;&lt;statistical-significance&gt;&lt;coefficient&gt;" Title="Coefficient changes sign when adding a variable in logistic regression" ViewCount="329" />
  <row AnswerCount="0" Body="&lt;p&gt;I did a linear regression of crop yield against year and took the residuals of this regression for my further analysis&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;mdl1&amp;lt;-lm(yld ~ year)&#10;res&amp;lt;-resid(mdl1)&#10;&#10;raw.mdl&amp;lt;-lm(yld ~ rain)&#10;summary(raw.mdl)&#10;&#10;Slope estimate = 2.3&#10;&#10; res.mdl&amp;lt;-lm(res ~ rain)&#10; summary(res.mdl)&#10; Slope estimate = 5.3&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In case of 'raw.mdl', I can interpret the slope estimate as a change in one unit in rain can cause yield to go up by 2.3 unit. But I am not sure how to interpret the slope estimate when residuals are used? Can anyone help me in understanding what does slope estimate for residuals against yield would actually mean? &lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-25T23:38:02.593" Id="116815" LastActivityDate="2014-09-26T02:00:25.220" OwnerUserId="46228" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;regression&gt;&lt;residuals&gt;" Title="interpret regression slope of residuals against an independent variable" ViewCount="24" />
  <row Body="&lt;p&gt;One way to get at this fairly simply is just through simulation - you won't get the exact percentage to the second decimal, but you can nail it down very closely. I've input some R code below that will simulate the rolls you're making and spit out the probability that your ally dies.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# Creating a hundred thousand sets of your three rolls to hit     &#10;roll.1 &amp;lt;- sample(1:20, replace = TRUE, 100000)&#10;roll.2 &amp;lt;- sample(1:20, replace = TRUE, 100000)&#10;roll.3 &amp;lt;- sample(1:20, replace = TRUE, 100000)&#10;&#10;# Creating a hundred thousand sets of three damage rolls&#10;damage.1 &amp;lt;- replicate(100000, (sample(1:6, 1) + sample(1:6, 1) + 2))&#10;damage.2 &amp;lt;- replicate(100000, (sample(1:6, 1) + sample(1:6, 1) + 2))&#10;damage.3 &amp;lt;- replicate(100000, (sample(1:6, 1) + sample(1:6, 1) + 2))&#10;&#10;# Here we calculate the damage of each roll. Essentially this line is saying&#10;# &quot;Apply the full damage if the hit roll was 13 or more (13 + 4 = 17), and &#10;# apply half the damage if the roll was 12.&quot; Applying zero damage when the roll&#10;# was less than 12 is implicit here.&#10;hurt.1 &amp;lt;- ((roll.1 &amp;gt;= 13) * damage.1 + floor((roll.1 == 12) * damage.1 * .5))&#10;hurt.2 &amp;lt;- ((roll.2 &amp;gt;= 13) * damage.2 + floor((roll.2 == 12) * damage.2 * .5))&#10;hurt.3 &amp;lt;- ((roll.3 &amp;gt;= 13) * damage.3 + floor((roll.3 == 12) * damage.3 * .5))&#10;&#10;# Now we just subtract the total damage from the health&#10;health &amp;lt;- 20 - (hurt.1 + hurt.2 + hurt.3)&#10;&#10;# And this gives the percentage of the time you'd kill your ally.&#10;sum(health &amp;lt;= 0)/1000000&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;When I run this, I consistently get between 16.8% and 17.2%. So you had about a 17% chance of killing your ally with this spell.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you're interested, the below code also computes the exact probability using the method outlined in Micah's answer. It turns out the exact probability is 16.99735%&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# Get a vector of the probability to hit 0, 1, 2, and 3 times. Since you can&#10;# only kill him if you get 2 hits or more, we only need the latter 2 probabilities&#10;hit.times &amp;lt;- (dbinom(0:3, 3, 9/20))&#10;&#10;# We'll be making extensive use of R's `outer` function, which gives us all&#10;# combinations of adding or multiplying various numbers - useful for dice &#10;# rolling&#10;damage.prob &amp;lt;- table(outer(1:6, 1:6, FUN = &quot;+&quot;) + 2)/36&#10;&#10;damage.prob &amp;lt;- data.frame(damage.prob)&#10;colnames(damage.prob) &amp;lt;- c(&quot;Damage&quot;, &quot;Prob&quot;)&#10;damage.prob$Damage &amp;lt;- as.numeric(as.character(damage.prob$Damage))&#10;&#10;# Since we'll be multiplying by the probability to hit each number of times &#10;# later, we just use 8/9 as the probability to get full damage, and 1/9 as &#10;# the probability of half damage.&#10;damage.prob.full &amp;lt;- data.frame(&quot;Damage&quot; = damage.prob$Damage, &quot;Prob&quot; = damage.prob$Prob * 8/9)&#10;damage.prob.half &amp;lt;- data.frame(&quot;Damage&quot; = damage.prob$Damage * .5, &quot;Prob&quot; = damage.prob$Prob * 1/9)&#10;&#10;# Rounding down the half damage&#10;damage.prob.half$Damage &amp;lt;- floor(damage.prob.half$Damage)&#10;damage.prob.half &amp;lt;- aggregate(damage.prob.half$Prob, by = list(damage.prob.half$Damage), FUN = sum)&#10;colnames(damage.prob.half) &amp;lt;- c(&quot;Damage&quot;, &quot;Prob&quot;)&#10;&#10;damage.prob.total &amp;lt;- merge(damage.prob.full, damage.prob.half, by = &quot;Damage&quot;, all.x = TRUE, all.y = TRUE)&#10;damage.prob.total$Prob &amp;lt;- rowSums(cbind(damage.prob.total$Prob.x, damage.prob.total$Prob.y), na.rm=TRUE)&#10;&#10;# Below I'm multiplying out all the damage probabilities for 2 and 3 hits, then&#10;# summing the probabilities of getting each damage total that equals 20 or more.&#10;&#10;damage.2 &amp;lt;- outer(damage.prob.total$Damage, damage.prob.total$Damage, FUN = '+')&#10;prob.kill.2 &amp;lt;- sum(outer(damage.prob.total$Prob, damage.prob.total$Prob)[damage.2 &amp;gt;= 20])&#10;&#10;damage.3 &amp;lt;- outer(outer(damage.prob.total$Damage, damage.prob.total$Damage, FUN = &quot;+&quot;), damage.prob.total$Damage, FUN = &quot;+&quot;)&#10;&#10;prob.kill.3 &amp;lt;- outer(outer(damage.prob.total$Prob, damage.prob.total$Prob), damage.prob.total$Prob)[damage.3 &amp;gt;= 20]&#10;&#10;# Now we just multiply the probability of killing with 2 hits by the probability&#10;# of hitting twice, and the same for 3 hits. Adding that together we get the &#10;# answer.&#10;&#10;sum(prob.kill.2)*hit.times[3] + sum(prob.kill.3)*hit.times[4]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="5" CreationDate="2014-09-26T00:43:35.653" Id="116817" LastActivityDate="2014-09-26T01:59:32.630" LastEditDate="2014-09-26T01:59:32.630" LastEditorUserId="46273" OwnerUserId="46273" ParentId="116792" PostTypeId="2" Score="5" />
  
  
  <row Body="&lt;p&gt;Transform the data&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;{(M,3),(F,13),(M,35),(F,35)}, 35&#10;{(M,10),(F,23)}, 23&#10;...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;into&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;(M, 3) 0&#10;(F, 13) 0   &#10;(M, 10) 0&#10;(F, 23) 1&#10;...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;(note that for the first record you don't know who is the HOH)&lt;/p&gt;&#10;&#10;&lt;p&gt;Then, for each gender and age, calculate the probability of being the HOH, and smooth the curve with respect to age with, say, smooth.spline.  For each household select one who has the highest probability to be the HOH.  Select the smooth.spline parameter which best fits the data.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-26T05:17:14.877" Id="116830" LastActivityDate="2014-09-26T05:17:14.877" OwnerUserId="31264" ParentId="116530" PostTypeId="2" Score="0" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a logarithmic calibration line:&lt;/p&gt;&#10;&#10;&lt;p&gt;MachineSignal = -3.21 log(concentration) + 21.9&lt;/p&gt;&#10;&#10;&lt;p&gt;It seems obvious to me, that if I have a triplicate measurement of concentrations (measured indirectly through MachineSignal), they should be &quot;averaged&quot; by the geometric mean, not the arithmetic mean. (Arithmetic mean would be O.K. only for raw MachineSignal values.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Can I have some authoritatively - looking citation for that ? I need it for the research paper I am writing.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-09-26T08:45:28.843" Id="116848" LastActivityDate="2014-09-26T08:45:28.843" OwnerUserId="30657" PostTypeId="1" Score="0" Tags="&lt;standard-error&gt;&lt;error&gt;&lt;measurement-error&gt;&lt;geometric-distribution&gt;" Title="logarithmic calibration and geometric mean - citation search" ViewCount="14" />
  <row AcceptedAnswerId="116859" AnswerCount="1" Body="&lt;p&gt;(This question is an edited version of a question I &lt;a href=&quot;http://stats.stackexchange.com/questions/116837/cluster-analysis-of-binary-data&quot;&gt;previously posted&lt;/a&gt; which one user recommended would benefit from more focus).&lt;/p&gt;&#10;&#10;&lt;p&gt;I have 2000 questionnaires from respondents which ask 33 different questions about which issues are present in their lives - i.e. alcohol abuse, domestic violence, mental health, child abuse, learning difficulties etc.&lt;/p&gt;&#10;&#10;&lt;p&gt;Each question can only be answered yes/no (which I've re-coded as 1/0).&lt;/p&gt;&#10;&#10;&lt;p&gt;I'd like to use this dataset to start creating n profiles of respondents to define which variables naturally cluster together e.g. (alcohol abuse and domestic violence), (mental health, child abuse, domestic violence), (alcohol abuse, learning difficulties) across some/all of the 33 differnt variables. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www-01.ibm.com/support/docview.wss?uid=swg21476716&quot; rel=&quot;nofollow&quot;&gt;A note I've read on-line &lt;/a&gt; indicates that &lt;strong&gt;hierarchical cluster&lt;/strong&gt; analysis is not appropriate for a dataset of this scale/type due to sensitivity of the position of how data is sorted in the dataset, and recommends &lt;strong&gt;two-step&lt;/strong&gt; cluster analysis instead. &lt;/p&gt;&#10;&#10;&lt;p&gt;Consequently, I'd be really interested in your input on whether hierarchical, two-step or other methods are most appropriate for exploring clusters of responses that natually associate together using a &lt;strong&gt;binary&lt;/strong&gt; dataset.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-26T10:09:23.240" Id="116856" LastActivityDate="2014-10-02T05:51:00.570" LastEditDate="2014-09-26T10:27:14.810" LastEditorUserId="3277" OwnerUserId="27768" PostTypeId="1" Score="1" Tags="&lt;clustering&gt;&lt;spss&gt;&lt;binary-data&gt;" Title="Hierarchical or Two-step cluster analysis for binary data?" ViewCount="943" />
  
  <row AnswerCount="0" Body="&lt;pre&gt;&lt;code&gt;Monthly expense     Cumulative Freq.&#10;up to 50            7&#10;up to 80            25&#10;up to 120           49&#10;up to 200           58&#10;over  200           60&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Calculate the arithmetic mean knowing that the 2 families with the higher expense, spend together 500.&lt;/p&gt;&#10;&#10;&lt;p&gt;So I started calculating Absolute Frequencies and Middle values of the classes (doing 500 / 2 as max value for the last class):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Absolute freq.      Middlevalues    Absolute freq. * Middlevalues&#10;    7               25              175&#10;    18              65              1170&#10;    24              100             2400&#10;    9               160             1440&#10;    2               225             450&#10;=   60                              = 5635&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Arithmetic mean = &lt;code&gt;5635 / 60 = 93,91666667&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;But it's wrong because the sum of residuals is not zero (Middlevalues-Mean):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;-68,91666667&#10;-28,91666667&#10;6,083333333&#10;66,08333333&#10;131,0833333&#10;= 105,4166667&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;What am I doing wrong?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-09-26T13:15:18.670" Id="116870" LastActivityDate="2014-09-26T13:25:02.097" LastEditDate="2014-09-26T13:25:02.097" LastEditorUserId="36479" OwnerUserId="36479" PostTypeId="1" Score="0" Tags="&lt;distributions&gt;&lt;mean&gt;&lt;residuals&gt;&lt;frequency&gt;" Title="Mean in Cumulative frequencies distribution" ViewCount="21" />
  <row AnswerCount="2" Body="&lt;p&gt;I have time-series count data $N_{i,j}$ (population sizes in site $i$ and year $j$) and I want to correlate year-to-year changes with the environmental conditions $x_{i,j}$. For this, I am fitting this model:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\begin{eqnarray}&#10;\mbox{log} ( \mu_{i,j+1} ) &amp;amp;=&amp;amp; \mbox{log} ( \mu_{i,j} ) + \alpha + \beta  x_{i,j} + \gamma_j + \epsilon_{i,j}  \\&#10;\\&#10;N_{i,j} &amp;amp;\sim&amp;amp; \mbox{Poiss} ( \mu_{i,j} ) \\&#10;\gamma_{j} &amp;amp;\sim&amp;amp; \mbox{Norm} (0, \sigma ) \\&#10;\epsilon_{i,j} &amp;amp;\sim&amp;amp; \mbox{Norm} (0, \theta )&#10;\end{eqnarray}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;So I'm interested in parameter $\beta$, the slope of the relationship. $\gamma_{j}$ is the random effect for year (as the residuals within single year were correlated) and $\epsilon_{i,j}$ is an overdispersion term.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;The problem is in the year specific random effect $\gamma_{j}$.&lt;/strong&gt; I think I have to use it, because the residuals are significantly explained by year. But, also the environmental conditions $x_{i,j}$ (climate conditions) are very much correlated within years! I.e. the climate in the same year is much more similar than accross years (as you would expect).&lt;/p&gt;&#10;&#10;&lt;p&gt;So the question is whether the introduced year-specific random effect $\gamma_{j}$ cannot &quot;eat out&quot; the variability that would be explained by the yearly variation of the climate (i.e. the term $\beta  x_{i,j}$)? In case this problem occurs, the yearly variation of the climate would go to $\gamma_{j}$ instead of $\beta  x_{i,j}$ and we could easily miss the significant relationship - i.e. the significant $\beta$ slope! Couldn't this happen? If yes, how to fit this model in such a way that the variability remains in $\beta x_{i,j}$ while at the same time the yearly autocorrelation in residuals is handled?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-26T14:09:14.200" FavoriteCount="1" Id="116876" LastActivityDate="2014-10-03T07:59:10.997" LastEditDate="2014-10-01T09:49:08.270" LastEditorUserId="5509" OwnerUserId="5509" PostTypeId="1" Score="5" Tags="&lt;mixed-model&gt;&lt;generalized-linear-model&gt;&lt;random-effects-model&gt;" Title="Can correlated random effects &quot;steal&quot; the variability (and the significance) from the regression coefficient?" ViewCount="211" />
  
  
  
  <row Body="&lt;p&gt;&lt;em&gt;This is more like an extended comment rather than a full answer. I am working on a problem that is, I think, somewhat related to the question so I share my thoughts.&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In what sense are you interested in the expectation? For example, for $\kappa = 0$ I don't think you can define a sensible expectation which is location &quot;on&quot; the sphere.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let&#10;\begin{equation}&#10;c_d(\kappa) = \frac{\kappa^{p/2-1}}{2\pi I_{p/2-1}(\kappa)} \text,&#10;\end{equation}&#10;so the normalization constant is easier to handle.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that the problem is symmetric for rotation, so you can take, say, $\mu = e_1 = (1, 0, 0, \ldots, 0)$. In this case the PDF is simply&#10;\begin{equation}&#10;P(x) = c_p(\kappa) \exp(\kappa x_1) \text.&#10;\end{equation}&lt;/p&gt;&#10;&#10;&lt;p&gt;If you then take the expectation of $x$ componentwise, you get for each $x_i$, $i = 2, 3, \ldots, p$&#10;\begin{equation}&#10;\mathbb{E}[x_i] = \int_{x_i \in S^{p - 1}} x_i c_p(\kappa) \exp{\kappa x_1} \,dx_i \text, \tag{*}&#10;\end{equation}&#10;where $S^{p - 1} = \{x \in \mathbb{R}^p : |x| = 1\}$ the $(p - 1)$-sphere on which the $p$-variate vMF distribution is defined. Now divide $S^{p - 1}$ into two &quot;hemispheres&quot; $H_1$ and $H_2$,&#10;\begin{align}&#10;H_1 &amp;amp;= \{x \in S^{p - 1} : x_i \ge 0\} \text, \\&#10;H_2 &amp;amp;= \{x \in S^{p - 1} : x_i &amp;lt; 0\} \text.&#10;\end{align}&#10;It doesn't really matter which inequality is strict, the integrand of (*) vanishes for $x_i = 0$ anyway.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now let's write (*) using $H_1$ and $H_2$ (remember that $i \ne 1$, so $x_i$ will not appear in the argument of $\exp$),&#10;\begin{align}&#10;\mathbb{E}[x_i] &amp;amp;= \int_{x_i \in H_1 \cup H_2} x_i c_p(\kappa) \exp{\kappa x_1} \,dx_i \\&#10;&amp;amp;= \int_{x_i \in H_1} x_i c_p(\kappa) \exp{\kappa x_1} + \int_{x_i \in H_2} x_i c_p(\kappa) \exp{\kappa x_1} \\&#10;&amp;amp;= \int_{x_i \in H_1} x_i c_p(\kappa) \exp{\kappa x_1} + \int_{x_i \in H_1} -x_i c_p(\kappa) \exp{\kappa x_1} \\&#10;&amp;amp;= 0 \text.&#10;\end{align}&lt;/p&gt;&#10;&#10;&lt;p&gt;Well, the situation for $x_1$ is considerably harder, as evaluating $\mathbb{E}[x_1]$ would result in and ugly mess of Gamma and modified Bessel functions... For $\kappa = 0$, we have the uniform distribution on $S^{p - 1}$, so $\mathbb{E}[x_1] = 0$. For $\kappa &amp;gt; 0$, intuitively, there is more probability mass on the $x_1 \ge 0$ &quot;hemisphere&quot; than on the $x_1 &amp;lt; 0$ &quot;hemisphere&quot;, so $\mathbb{E}[x_1]$ should be $&amp;gt; 0$.&lt;/p&gt;&#10;&#10;&lt;p&gt;If we accept the handwaving above (or do the integration for real), we get $\mathbb{E}[x] = \mathbb{E}[x_1] e_1 = \mathbb{E}[x_1] \mu$. Combined with the fact that $\mathbb{E}[x_1] &amp;gt; 0$, this is vector pointing to the same direction as $\mu$. To get a vector that lies in $S^{p - 1}$, we can normalize to get&#10;\begin{equation}&#10;\frac{\mathbb{E}[x]}{|\mathbb{E}[x]|} = \frac{\mathbb{E}[x_1]\mu}{|\mathbb{E}[x_1]|} = \mu \text.&#10;\end{equation}&#10;If you remove the middle part of the equality above, the result will hold for any $\mu$ by rotation.&lt;/p&gt;&#10;&#10;&lt;p&gt;In this sense, yes, the expectation of the vMF distribution on the unit sphre is $\mu$ for $\kappa &amp;gt; 0$. For $\kappa = 1$, $\mathbb{E}[x] = 0$ (in the Euclidean sense) and the normalization will fail.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;The distribution of $x_i$ (with $\mu = e_1$) is interesting for another reason, only slightly related to your question. It is the same thing for cosine similarity (or after a bit of scaling and shifting, cosine distance) and the vMF distribution that the $\chi$-distribution is for the Euclidean distance and the Gaussian distribution. That is,&#10;\begin{align}&#10;\chi &amp;amp;= |x - \mu| &amp;amp;&amp;amp; \text{for $x \sim \mathcal{N}(\mu, 1)$,} \\&#10;\text{like } x_1 &amp;amp;= \mu^T x = \frac{\mu^T x}{|\mu| \cdot |x|} &amp;amp;&amp;amp; \text{for $x \sim \mathrm{vMF}(\mu, \kappa)$.}&#10;\end{align}&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-09-26T16:24:29.963" Id="116891" LastActivityDate="2014-09-26T16:34:49.423" LastEditDate="2014-09-26T16:34:49.423" LastEditorUserId="31032" OwnerUserId="31032" ParentId="116818" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;I used tbats to fit a model for a 3 years of historic data and the values work fine but as I did not include holidays, holiday predictions are really off. I used arima with regressor (holidays at regressors) and the predictions for holidays are much better than the one by tbats but tbats got more accurate results for normal days. I know it sounds un reasonable but is this ok that I use the arima model for holidays and the tbats model for normal days?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-09-26T17:15:09.500" Id="116896" LastActivityDate="2014-09-26T17:23:58.290" OwnerUserId="55541" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;time-series&gt;&lt;forecasting&gt;" Title="Is it reasonable to use a combination of two forecasting models for a dataset?" ViewCount="40" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm a statistical consultant, one of my clients requested something I haven't heard of before: She wants to compare the correlation between X and Y (two ratio scaled variables) before an intervention, to the correlation between X and Y (the same variables) after the intervention. &lt;/p&gt;&#10;&#10;&lt;p&gt;Usually I would use Fisher's Z (link included &lt;a href=&quot;http://vassarstats.net/rdiff.html&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;) and have been done with it, BUT - those aren't two different groups, but one - the same subjects responded to the first X and  to Y (before the intervention), and second X and Y (after the intervention). &lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a special kind of Fisher Z test for this situation? &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks,&#10;Nimrod&lt;/p&gt;&#10;&#10;&lt;p&gt;P.S.&#10;Just to make it a bit more complicated there are four groups in the research and I would need to do it for every one it separately. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-26T18:00:50.160" Id="116903" LastActivityDate="2014-09-26T18:00:50.160" OwnerUserId="56507" PostTypeId="1" Score="2" Tags="&lt;correlation&gt;&lt;intervention-analysis&gt;&lt;fisher-transform&gt;" Title="Special case of Fisher Z for comparing two correlations" ViewCount="23" />
  <row Body="&lt;p&gt;&lt;code&gt;rms&lt;/code&gt; is a good package, however, I am not aware of how the imputation was developed in &lt;code&gt;rms&lt;/code&gt; and &lt;code&gt;mice&lt;/code&gt; has had several publications on the topic.&lt;/p&gt;&#10;&#10;&lt;p&gt;Nikita, to the best of my knowledge, you must have a character variable in these data. If you are trying to impute date... which is generally a VERY bad idea... then you should convert the date to numeric (as the function of a number of days since Jan 1st 1970, the default numeric date conversion), then rerun the imputation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Again I stress that imputing date is a poor idea.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-09-26T20:21:18.030" Id="116921" LastActivityDate="2014-09-26T20:21:18.030" OwnerUserId="8013" ParentId="116494" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;Lets say I have N balls in a bag.  On my first draw, I mark the ball and replace it in the bag.  On my second draw, if I pick up a marked ball I return it to the bag.  If, however I pick up a non-marked ball then I mark it and return it to the bag.  I continue this for any number of draws. What is the expected number of balls in the bag given a number of draws and the marked/unmarked history of draws?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-09-26T21:02:59.850" FavoriteCount="1" Id="116927" LastActivityDate="2014-11-06T18:41:07.730" LastEditDate="2014-09-26T21:06:11.397" LastEditorUserId="919" OwnerUserId="56513" PostTypeId="1" Score="8" Tags="&lt;probability&gt;&lt;estimation&gt;&lt;population&gt;&lt;combinatorics&gt;&lt;capture-mark-recapture&gt;" Title="Estimating number of balls by successively selecting a ball and marking it" ViewCount="123" />
  
  <row Body="&lt;p&gt;If you just want the average mileage expense and nothing else, just split your dataset into two sets (rural trips and urban trips), total the expenses, and divide by the number of trips. If you're looking to test a statistically significant relationship, read on:&lt;/p&gt;&#10;&#10;&lt;p&gt;If half of your rural trips are zero, then it most likely holds that half of your urban trips are also, unless there is an unaccounted-for category. If so, that's one very well distributed binary variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;Since you just have two mutually-exclusive trip categories, a dummy variable will suffice. A dummy variable is a variable where there are only two possible outcomes, basically construed as &quot;is it or isn't it?&quot; What I mean by that is that the variable takes the value of 1 if the observation is true, and 0 if false. So, if you're testing gender, you pick one of the two genders, and code all your observations according to that gender. If you pick male, every person who is male takes on the value of 1, and every not-male (female) takes on 0. Then when you look to see what the effect of that variable is on your dependent variable, you say that being male increases/decreases Y, relative to being female. The same will work for rural/urban. Pick one--it doesn't really matter which, especially since they're so evenly distributed. Say you pick urban. You code every trip to an urban locale as 1, and every trip to a rural locale as 0. Then you only use that variable. (If you made a dummy for rural and a dummy for urban, they would cancel each other out. There's a concept called multicollinearity, where if two variables vary too closely with each other, it skews the results. Many stats programs will actually just drop one of the variables if you try to include both.)&lt;/p&gt;&#10;&#10;&lt;p&gt;So, you use your urban dummy variable, with mileage expense as the dependent variable. Whatever your beta is for urban is the effect of an urban destination on expense, &lt;em&gt;relative to going to a rural destination&lt;/em&gt;. Don't forget to say that part. So, say your beta comes back as 25.8, and it's significant. If your expenses are measured in dollars, that means that heading to an urban location is predicted to increase your expense by $25.80, relative to driving to a rural location.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-09-26T22:19:12.587" Id="116937" LastActivityDate="2014-09-26T22:19:12.587" OwnerUserId="54736" ParentId="116925" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;Code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x = rnorm(50); y = rnorm(50)&#10;xCDF = ecdf(x); yCDF = ecdf(y) &#10;plot(xCDF)&#10;lines(yCDF)&#10;F = function(z) xCDF(z)-yCDF(z)&#10;plot(seq(-2,2,by=0.02), F(seq(-2,2,by=0.02)) ,type=&quot;l&quot;) &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;What does the last plot mean? How can I interpret it?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-27T00:32:29.560" Id="116950" LastActivityDate="2014-09-27T01:18:24.333" LastEditDate="2014-09-27T01:18:24.333" LastEditorUserId="13564" OwnerUserId="13564" PostTypeId="1" Score="0" Tags="&lt;cdf&gt;" Title="What do I obtain if I subtract two CDFs?" ViewCount="30" />
  
  <row AnswerCount="0" Body="&lt;p&gt;This has already been asked and answered, but one of the answers didn't explain why a certain technique worked.&lt;/p&gt;&#10;&#10;&lt;p&gt;So my question is &quot;Why does calling randomForest(predictors, decision) instead of the normal (decision~ predictors) reducing running time?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-27T01:20:59.313" Id="116952" LastActivityDate="2014-09-27T01:20:59.313" OwnerUserId="46522" PostTypeId="1" Score="0" Tags="&lt;random-forest&gt;" Title="improving randomForest running time" ViewCount="21" />
  <row AnswerCount="3" Body="&lt;p&gt;The question is from a Master-level Probability Course.&lt;/p&gt;&#10;&#10;&lt;p&gt;It is well known that the underlying assumption for the binomial distribution is that there are &lt;em&gt;n&lt;/em&gt; independent Bernoulli trials. More specifically, the assumptions are:&lt;/p&gt;&#10;&#10;&lt;p&gt;(1) The number of trials, $n$, is fixed.&lt;/p&gt;&#10;&#10;&lt;p&gt;(2) There are two and only two outcomes, labelled as &quot;success&quot; and &quot;failure&quot;. The probability of outcome &quot;success&quot; is the same across the &lt;em&gt;n&lt;/em&gt; trials.&lt;/p&gt;&#10;&#10;&lt;p&gt;(3) The trials are &lt;em&gt;independent&lt;/em&gt;. That is, the outcome of one trial doesn't affect that of the others.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is, are there any counterexamples which just violate one of those three assumptions? Particularly, are there cases where Assumption (2) holds but (3) doesn't, or vice versa?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-09-27T03:56:10.910" FavoriteCount="0" Id="116954" LastActivityDate="2014-09-28T17:25:52.867" LastEditDate="2014-09-27T22:43:23.127" LastEditorUserId="56525" OwnerUserId="56525" PostTypeId="1" Score="3" Tags="&lt;self-study&gt;&lt;binomial&gt;&lt;assumptions&gt;" Title="Counterexample against binomial assumptions" ViewCount="92" />
  <row AcceptedAnswerId="116993" AnswerCount="1" Body="&lt;p&gt;I have a newbie question about logistic regression fit plots.  I'm fitting a very simple binary output based on a simple continuous input&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;X   Y&#10;0.1 0&#10;0.1 0&#10;0.1 0&#10;0.1 1&#10;0.5 0&#10;0.5 0&#10;0.5 1&#10;0.5 1&#10;0.9 0&#10;0.9 1&#10;0.9 1&#10;0.9 1&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;when using JMP, the fitting model is logistic.  I understand the fitted line, but what are the points plotted on the chart?  I only have 0,1 in the output, but why does the logistic plot show values of Y that are not 0,1?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ELPai.jpg&quot; alt=&quot;logistic scatter plot&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for any help with this.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-27T05:16:53.987" Id="116957" LastActivityDate="2014-09-27T15:34:44.887" LastEditDate="2014-09-27T06:07:56.830" LastEditorUserId="24808" OwnerUserId="56528" PostTypeId="1" Score="0" Tags="&lt;logistic&gt;&lt;jmp&gt;" Title="how is the logistic regression scatter plot created" ViewCount="73" />
  <row Body="&lt;p&gt;Your final goal is classification, if not mistaken your point, so don't think in the way to classify it cluster by cluster. Cluster itself can possess many class or many nonlinear decision boundaries too.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;You can use &lt;code&gt;cluster-then-labeling method&lt;/code&gt;.&#10;It is easy and intuitive and flexible. As the following:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Clustering&lt;/li&gt;&#10;&lt;li&gt;Within each cluster, taking the labeled data, train a easy classifier and label the unlabeled within that cluster .&lt;/li&gt;&#10;&lt;li&gt;Finally, you have a full labeled data, happy taking it to train a powerful classifier you prefered.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;There may be problem arised at step 3 if there are clusters with all the unlabeled data.&#10;You can classify it to unknown or if you use hierarchical clustering, you can find along the hierarchy the higher level group which has labeled data.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;If you know what's those features means and two set of features are strongly independent , you can also consider combining &lt;code&gt;co-training&lt;/code&gt;. it is also an easy wrapper method. For more and friendly resources of co-training , Tom Mitchell is an option, his book is very friendly I think.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-09-27T06:50:12.877" Id="116965" LastActivityDate="2014-09-27T07:42:42.323" LastEditDate="2014-09-27T07:42:42.323" LastEditorUserId="30310" OwnerUserId="30310" ParentId="116906" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;Breaking each one, singly:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;(1) The number of trials, $n$, is fixed.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;The experiment continues until $k$ successes are observed. Or until $m$ successes in a row. Or until the number of successes exceeds the number of failures by 2.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;(2) There are two and only two outcomes, labelled as &quot;success&quot; and &quot;failure&quot;. The probability of outcome &quot;success&quot; is the same across the n trials.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;P(success) is drawn from a beta distribution with mean $p$.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;(3) The trials are independent. That is, the outcome of one trial doesn't affect that of the others.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;P(Success|Success at previous trial) = $p_1$ and&lt;br&gt;&#10;P(Success|Failure at previous trial) = $p_2$&lt;/p&gt;&#10;&#10;&lt;p&gt;You suggested something like an urn model as a concrete example, and it's quite easy to construct several forms of urn model of this third case (if you use sampling with replacement) - or you could use dice if there's more than one die you could use.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-09-27T06:56:08.753" Id="116966" LastActivityDate="2014-09-28T04:31:08.157" LastEditDate="2014-09-28T04:31:08.157" LastEditorUserId="805" OwnerUserId="805" ParentId="116954" PostTypeId="2" Score="2" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have a two groups of data - 1/ All data and 2/ Selected_season, which is only some subgroup of all data. I want to find out:&#10;a/ if proportion of BM, HB, etc. groups is the same in All data and Selected_season&#10;b. if not - which groups are different?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/q5wFj.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Which statistical test should I use? Is it implemented in R?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-27T10:06:32.383" Id="116974" LastActivityDate="2015-03-04T20:50:13.337" OwnerUserId="37627" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;nonparametric&gt;&lt;count-data&gt;&lt;group-differences&gt;" Title="How to compare count of two groups?" ViewCount="147" />
  <row AnswerCount="1" Body="&lt;p&gt;everyone! My question is very basic. &lt;/p&gt;&#10;&#10;&lt;p&gt;If we find a statistical correlation between grade-point averages and test scores after taking an online short course, what kind of recommendation for a policy can we give based on this? &lt;/p&gt;&#10;&#10;&lt;p&gt;Can we recommend that the school develops different courses to suit students with different grade-point averages so as to help them get better test scores? &lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you very much.&lt;/p&gt;&#10;&#10;&lt;p&gt;[Edited]&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-27T11:15:46.907" Id="116978" LastActivityDate="2014-09-28T19:33:25.383" LastEditDate="2014-09-28T15:21:43.900" LastEditorUserId="56535" OwnerUserId="56535" PostTypeId="1" Score="0" Tags="&lt;correlation&gt;" Title="Recommendation based on correlation" ViewCount="19" />
  <row Body="&lt;p&gt;You have mentioned a number of analytic approaches but its hard to tell whether they are appropriate since you don't specify the objectives of your study. If your purpose is to build a model that predicts the response category that a case with a given set of predictors will fall into, then using classification trees might be appropriate. The goals of MDS and Factor analysis is to detect meaningful underlying dimensions that allow the explanation of observed similarities or dissimilarities between the investigated objects/cases. They are useful for data reduction as you suggest but you have to consider whether it is really necessary depending on the number of variables in your data (you dont give a clue) set and what use/interpretation is intended for you work.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-27T13:56:40.593" Id="116987" LastActivityDate="2014-09-27T13:56:40.593" OwnerUserId="54779" ParentId="116274" PostTypeId="2" Score="0" />
  <row AnswerCount="1" Body="&lt;p&gt;I would like to apply any regression methods, such as the ones available using WEKA libraries (for example, SVMs, NNs, Random Trees,...) . However, I am getting very low results since I am missing the relevant columns of data.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am posting this question since doing the internet search gives the results regarding the situations when there are missing data-points and not the whole data-sets.&lt;/p&gt;&#10;&#10;&lt;p&gt;What would be the best way, if any, to accommodate the missing columns and get better regression results ? What I need is something that would mock the missing data in such a way, that would yield the better statistical results regarding the prediction results.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have tried clustering, and than doing the regression per cluster. That does improve the results, but the improvement is not a significant one.&lt;/p&gt;&#10;&#10;&lt;p&gt;Furthermore, I have tried applying the regression methods multiple number of times, but that does not do anything at all for a data-set I have.&lt;/p&gt;&#10;&#10;&lt;p&gt;Mocking the data by adding a random data column improves the training (sometimes), but not the actual prediction.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any help or a hint would be more than appreciated!&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-09-27T17:34:42.690" Id="117000" LastActivityDate="2014-10-23T13:39:29.103" LastEditDate="2014-09-27T17:57:31.920" LastEditorUserId="56546" OwnerUserId="56546" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;classification&gt;&lt;data-mining&gt;&lt;prediction&gt;&lt;weka&gt;" Title="Regression/classification, how to accommodate the missing columns of data?" ViewCount="40" />
  <row AnswerCount="1" Body="&lt;p&gt;2 line background: Applied/Pure Math (ODE, PDE, Functional, measure theory and a little of probability theory) doing research in bioinformatics. So my stats knowledge is limited.&lt;/p&gt;&#10;&#10;&lt;p&gt;That being said, for my research I am using a bioinformatics software (ChromHMM) on a given multivariate data set. As of now I have 20 models ranging from 20 states to 40 states in each model. Some of the papers I've read regarding this use BIC to pick out the most &quot;efficient&quot; model. In other words, out of the 20 models I have, which one do I pick?&lt;/p&gt;&#10;&#10;&lt;p&gt;I am looking for a Matlab or R tutorial (and am willing to do the work myself if someone can supply reading material).&lt;/p&gt;&#10;&#10;&lt;p&gt;Related questions on SE:&#10;&lt;a href=&quot;https://stats.stackexchange.com/questions/105982/determing-states-in-hmm-with-bic&quot;&gt;Question 1&lt;/a&gt;&#10;and &lt;a href=&quot;https://stats.stackexchange.com/questions/65285/criteria-for-selecting-the-best-model-in-a-hidden-markov-model&quot;&gt;Question 2&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-27T21:28:31.047" Id="117012" LastActivityDate="2014-12-02T23:01:49.530" OwnerUserId="28715" PostTypeId="1" Score="1" Tags="&lt;hidden-markov-model&gt;" Title="How do I calculate BIC score for HMM?" ViewCount="88" />
  <row AcceptedAnswerId="117057" AnswerCount="1" Body="&lt;p&gt;I have been learning about Machine Learning (via Udacity) and Statistics (via Coursera) the past few months and trying to figure out a good way to combine them for a general approach to explaining data trends. I am aware so much of ML and stats rely on domain knowledge, and that without any domain knowledge we easily encounter the results of the &quot;No Free Lunch&quot; theorem. As such, I have incorporated domain knowledge in my approach accordingly.I walk through a simple example of stock market volumes below. &lt;/p&gt;&#10;&#10;&lt;p&gt;I know the answer to this question is &quot;It depends on the data and a variety of factors&quot;, but it I wanted a general rule of thumb on how to approach a situation, does a general approach such as the below work?&lt;/p&gt;&#10;&#10;&lt;p&gt;Sample case study: The stock market is higher in year X compared to year Y, why? &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Approach to analyzing the trend:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;1) &lt;strong&gt;Exploratory Analysis:&lt;/strong&gt; Using my &lt;strong&gt;domain knowledge&lt;/strong&gt; of the stock market it is reasonable to look at features X_1, X_2,..X_n. I then perform exploratory data analysis on each of the features relative to the stock market value (ie, how has the volatility market place changed over time relative to the value of the Nasdaq, for example). I do this for each one of the features I thought was worth investigating to get a feel for the data.&lt;/p&gt;&#10;&#10;&lt;p&gt;2) &lt;strong&gt;Feature Selection :&lt;/strong&gt; Use either information gain, or regression (or both) to see determine which features have the &lt;strong&gt;highest predictive power&lt;/strong&gt;. Take the top n features and use them in the next step.&lt;/p&gt;&#10;&#10;&lt;p&gt;3) &lt;strong&gt;Hypothesis Tests&lt;/strong&gt;: Run hypothesis tests on the data to try to see if each of the individual features had an impact on the market volume change. &lt;/p&gt;&#10;&#10;&lt;p&gt;4) &lt;strong&gt;Supervised Learning&lt;/strong&gt;: With fewer features, we need less data to generalize our findings and are less likely to falling prey to the curse of dimensionality.So we can then can train our historical data on a decision tree, use cross validation, and generate a model. We can then use this model as a heuristic to see what feature had the largest impact on classifying an &quot;up&quot; stock market instead of a &quot;down&quot; stock market. &lt;/p&gt;&#10;&#10;&lt;p&gt;Is this a reasonable way to look to analyze events that have transpired (in this hypothetical example, the &quot;events&quot; being that the stock market went up)? I am trying to integrate all of the content from my introductory courses in a way that can be practical and aid in drawing insights from data. This is a general question having nothing to do with stock market volume, I just wanted to use an example for the sake of discussion if it made life easier. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-27T22:48:05.850" Id="117017" LastActivityDate="2014-09-28T13:38:11.943" OwnerUserId="55317" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;hypothesis-testing&gt;&lt;statistical-significance&gt;&lt;feature-selection&gt;" Title="Strategy for Analyzing Data" ViewCount="77" />
  
  <row Body="&lt;p&gt;Ok, so I realized that I pretty much misunderstood what the JMP information page was telling me.  Turns out, the solution was much simpler than dealing with the profile likelihood.&lt;/p&gt;&#10;&#10;&lt;p&gt;Although I found my solution earlier in a different text, the paper I'm supplying also shows the same procedure.&#10;&lt;a href=&quot;http://courses.education.illinois.edu/EdPsy589/lectures/Draft_glm_chapter_feb_2010.pdf&quot; rel=&quot;nofollow&quot;&gt;Applied Generalized Linear&#10;Mixed Models: Continuous and&#10;Discrete Data&lt;/a&gt;.  Specifically look at page 46 (pdf pg 50) under confidence bands for predicted means.&lt;/p&gt;&#10;&#10;&lt;p&gt;For my testing I was solving a GLM with a Normal distribution and a logit link function.&lt;/p&gt;&#10;&#10;&lt;p&gt;For my solution I took my untransformed predictor $\hat{y_i}$ and converted it using $logit(\hat{y_i})=\hat{\eta}_i$ (my link function)&lt;/p&gt;&#10;&#10;&lt;p&gt;While you can calculate the covariance matrix $\bf{V_{\hat{\beta}}}$ I just used the one provided by JMP that way I could make sure my answer was exact.&lt;/p&gt;&#10;&#10;&lt;p&gt;I then computed $\sigma^{2}_{\hat{\eta}_i}=var(\hat{\eta}_{i})=\bf{X_{i}V_{\hat{\beta}}X^{T}_{i}}$&lt;/p&gt;&#10;&#10;&lt;p&gt;I could then compute the confidence interval on the transformed predictor using&#10;$\hat{\eta}_{i}\pm z_{\alpha/2}\hat{\sigma}_{\hat{\eta}_i}$&lt;/p&gt;&#10;&#10;&lt;p&gt;And finally I could untransform back to the appropriate scale usin &#10;$g^{-1}(\hat{\eta}_{i}\pm z_{\alpha/2}\hat{\sigma}_{\hat{\eta}_i})$&lt;/p&gt;&#10;&#10;&lt;p&gt;or $1/(1+\exp{^{-(\hat{\eta}_{i}\pm z_{\alpha/2}\hat{\sigma}_{\hat{\eta}_i})}})$ &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-28T02:05:47.767" Id="117028" LastActivityDate="2014-09-28T02:05:47.767" OwnerUserId="56406" ParentId="116692" PostTypeId="2" Score="0" />
  <row AnswerCount="2" Body="&lt;p&gt;I have two sets of data for setup time of a machine, one is setup time when fixture is running alone, another is when another fixture is running alongside. &lt;/p&gt;&#10;&#10;&lt;p&gt;For fixture running alone the data I gathered is not normal although there are only 12 data points.&lt;/p&gt;&#10;&#10;&lt;p&gt;For parallel running fixture the setup is normally distributed with p0.089 and sample size 10&lt;/p&gt;&#10;&#10;&lt;p&gt;I wanted to test the null hypothesis that there is no difference in setup time for these two scenarios, would comparing the median of these two data the right way to do it or not.&lt;/p&gt;&#10;&#10;&lt;p&gt;I understand the sample size is too low, but this experiment is for a product where you do not get many data points. they wanted just an estimate to start working, so all I wanted to do is atleast follow the right way of doing it.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-09-28T02:29:45.943" Id="117030" LastActivityDate="2014-09-28T08:55:40.550" OwnerUserId="56563" PostTypeId="1" Score="1" Tags="&lt;mean&gt;&lt;median&gt;" Title="Comparing normal with non-normal" ViewCount="81" />
  
  <row Body="&lt;p&gt;Taking your example of picking balls from an urn. Assume you have M balls in the urn, with some black and some red. You pick N balls (with N a fixed number) and count up the number B which are black. B is &lt;em&gt;not&lt;/em&gt; binomially distributed, because the probability of getting a black ball at each pick depends on how many black balls you've already picked.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-09-28T05:14:23.807" Id="117035" LastActivityDate="2014-09-28T05:14:23.807" OwnerUserId="1569" ParentId="116954" PostTypeId="2" Score="1" />
  
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I have this strange condition. I have two predictors. One of the predictors has low correlation with the target but less rmse. On the other hand another predictor has high correlation but high rmse as well. But the number of samples for the two predictors are different. So the correlation and rmses are calculated on different samples sizes.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can anyone explain why this is so?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-09-28T16:11:32.950" Id="117063" LastActivityDate="2014-09-29T02:33:55.250" OwnerUserId="12331" PostTypeId="1" Score="2" Tags="&lt;correlation&gt;&lt;prediction&gt;&lt;rms&gt;" Title="Is it possible for a predictor to have low correlation but low rmse as well" ViewCount="42" />
  <row Body="&lt;p&gt;Correct me if I am wrong here.&lt;/p&gt;&#10;&#10;&lt;p&gt;a) A counterexample only violates Assumption (1) would be a negative binomial process, where the number of trial is the random variable; The situation where a variable from a geometric distribution also works.&lt;/p&gt;&#10;&#10;&lt;p&gt;b) A counterexample only violates Assumption (3) would be a hypergeometric process. I came up with idea based on @Hong_Ooi's comment. The assumption (2) holds in a hypergeometric process setting is not that obvious at first glance though. &lt;/p&gt;&#10;&#10;&lt;p&gt;For example, suppose we have 10 balls, 6 blacks and 4 reds, in the urn. Suppose we draw 3 balls (fixed trials, assumption (1) holds) without replacement from the urn, and we are interested in the probability of 2 black balls out of the 3. The reason why assumption (2) holds is as follows.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Pr(1st draw is a black) = 6/10&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Pr(2nd draw is a black) = Pr(BB) + Pr(RB) = 6/10 * 5/9 + 4/10 * 6/9 = 54/90 = 6/10&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Pr(3rd draw is a black) = Pr(BBB) + Pr(BRB) + Pr(RBB) + Pr(RRB) = 6/10 *5/9 *4/8 + 6/10 *4/9 *5/8 + 4/10 *6/9 *5/8 + 4/10 * 3/9 *6/8 = 6/10&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The same goes for the probability of &quot;red&quot; ball. That said, assumption (2) holds. But obviously assumption (3）doesn't. For example, Pr(2nd draw is a black, given 1st is a black) is not equal to Pr(1st draw is a black). Thus the &lt;em&gt;independence&lt;/em&gt; assumption violates.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-28T17:08:19.073" Id="117067" LastActivityDate="2014-09-28T17:25:52.867" LastEditDate="2014-09-28T17:25:52.867" LastEditorUserId="56525" OwnerUserId="56525" ParentId="116954" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;FYI, from 2011 to 2014, he hit 43, 27, 28, and 35.&lt;/p&gt;&#10;&#10;&lt;p&gt;That's pretty close to his 162-game average of 32 (which of course includes those values), and about 1 SD under the 54 in 2010.&lt;/p&gt;&#10;&#10;&lt;p&gt;Looks like regression to the mean in action: An extreme group built by capitalising on noisy subjects (1 in this case) deviating from their group mean by chance.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.baseball-reference.com/players/b/bautijo02.shtml&quot; rel=&quot;nofollow&quot;&gt;http://www.baseball-reference.com/players/b/bautijo02.shtml&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-28T17:20:22.787" Id="117068" LastActivityDate="2014-09-28T17:20:22.787" OwnerUserId="4884" ParentId="6404" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I am given a simulation task to come up with several analysis strategies and compare their relative performances. The horizon is wide open; I appreciate all recommendations of methods and references.&lt;/p&gt;&#10;&#10;&lt;p&gt;Background:&lt;/p&gt;&#10;&#10;&lt;p&gt;There are ~15-20 inflammatory biomarkers that are commonly measured to predict cardiovascular disease onset, each biomarker has its own measurement error rate, inter-assay and intra-assay variability, these biomarkers' measurements tend to be positively correlated (in general, in a individual, if 1 biomarker level is high, other biomarkers' levels tend to be high as well.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Aim:&lt;/p&gt;&#10;&#10;&lt;p&gt;Somehow use these biomarkers information to predict cardiovascular disease onset prevalence at age 60&lt;/p&gt;&#10;&#10;&lt;p&gt;What I have:&#10;&lt;p&gt;cross sectional data (simulated) of the biomarkers and the binary outcome (imagine a 17-column data set, column 1 is SubjectID, column 2 through 16 are biomarker 1 through 15 measures, column 17 is the binary outcome), simulated data has ~1000 iid subjects, no missing data.&lt;/p&gt;&#10;&#10;&lt;p&gt;Concerns:&#10;&lt;p&gt;1. Select a subgroup of biomarkers (criteria?) or transform, or both?&#10;&lt;p&gt;2. What are the criteria to compare model/strategy performances?&lt;/p&gt;&#10;&#10;&lt;p&gt;Methods I have came across so far:&#10;&lt;p&gt;1. (sparse) partial least squares&#10;&lt;p&gt;2. weighted glm (PCA loading as weights)&#10;&lt;p&gt;3. penalized regression (glmnet; ridge, lasso)&#10;&lt;p&gt;4. Bayesian variable selection&#10;&lt;p&gt;5. best subset&#10;&lt;p&gt;6. forward stepwise&lt;/p&gt;&#10;&#10;&lt;p&gt;Model comparison criteria for binary outcome&#10;&lt;p&gt;1. AUC, sensitivity, specificity&#10;&lt;p&gt;2. misclassification rate&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-28T17:48:11.883" Id="117072" LastActivityDate="2014-09-28T17:48:11.883" OwnerUserId="42498" PostTypeId="1" Score="0" Tags="&lt;predictive-models&gt;&lt;model-selection&gt;&lt;binary-data&gt;&lt;model-comparison&gt;&lt;correlated-predictors&gt;" Title="analysis strategy for selecting and/or transforming correlated continuous biomarkers to predict binary endpoint" ViewCount="28" />
  <row AnswerCount="1" Body="&lt;p&gt;I perform principal component analysis (PCA) on a dataset, and then plot the first and the second principal components. I get the following phenomenon: one principal component appears to be a linear function of the other.&lt;/p&gt;&#10;&#10;&lt;p&gt;I assume there is no guarantee this would not happen, but I am wondering if there are theorems or known results that may help me understand what causes this phenomenon in my data.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-09-28T17:49:07.897" Id="117073" LastActivityDate="2014-09-29T22:25:56.730" LastEditDate="2014-09-29T21:59:57.600" LastEditorUserId="28666" OwnerUserId="40946" PostTypeId="1" Score="0" Tags="&lt;pca&gt;&lt;dimensionality-reduction&gt;" Title="How can there be a linear correlation between two PCA components?" ViewCount="91" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have a system of two inputs and one output that I'd like to model using the following Box-Jenkins transfer function (&quot;dynamic regression&quot;) structure:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$y_t=\frac {\beta_1(B)}{\nu_1(B)}\omega_1Bx_{1,t}+\frac{\beta_2(B)}{\nu_2(B)}\omega_2Bx_{2,t}+\frac {\theta(B)}{\phi(B)}z_t+c$$&lt;/p&gt;&#10;&#10;&lt;p&gt;(using notation to conform with this Rob Hyndman post: &lt;a href=&quot;http://robjhyndman.com/hyndsight/arimax/&quot; rel=&quot;nofollow&quot;&gt;http://robjhyndman.com/hyndsight/arimax/&lt;/a&gt; where $B$ is the backshift operator, numerator polynomials like $\theta(B)$ stand for $(1+\theta_1 B+\theta_2 B^2+\cdots +\theta_nB^n)$, and denominator polynomials like $\phi(B)$ stand for $(1-\phi_1 B-\phi_2 B^2-\cdots -\phi_nB^n)$ )&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;My problem is this:&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;My $y$ is a stationary variable. However, both of my $x$s are not. (or to be precise, unlike my $y$, my $x$s are only very weakly stationary and have significant autocorrelation.) In discussions of pre-whitening that I've read, it's recommended to pre-whiten the $x$ variable (and then apply that same filter to $y$) before looking at the correlation structure. However, those examples only cover cases with only one $x$. On the other hand, this section of Hyndman's book: &lt;a href=&quot;https://www.otexts.org/fpp/9/1&quot; rel=&quot;nofollow&quot;&gt;https://www.otexts.org/fpp/9/1&lt;/a&gt; recommends repeatedly applying a difference to $y$ and all $x$s until all of the variables are stationary.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;My question is:&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Should I difference $y$ and all $x$s once before looking at the cross-correlations, even though this will make $y$ over-differenced? Or, should I pre-whiten $x_1$ and $x_2$, then apply each filter to the other variables, and then look at the cross-correlations? Or is another method recommended?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-09-28T21:49:54.650" Id="117101" LastActivityDate="2015-02-06T03:06:02.477" LastEditDate="2014-10-06T22:05:14.883" LastEditorUserId="56601" OwnerUserId="56601" PostTypeId="1" Score="2" Tags="&lt;time-series&gt;&lt;arima&gt;&lt;autocorrelation&gt;&lt;arma&gt;&lt;dynamic-regression&gt;" Title="My transfer function has non-stationary inputs, but a stationary output. Should I difference both the inputs and outputs during structure estimation?" ViewCount="64" />
  <row AcceptedAnswerId="117117" AnswerCount="1" Body="&lt;p&gt;Here are the codes I input into R:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;X = rnorm(10000)&#10;Y = rchisq(10000, 1)&#10;&#10;ks.test(X, rchisq(1,1))&#10;&#10;Two-sample Kolmogorov-Smirnov test&#10;&#10;data:  X and rchisq(1, 1)&#10;D = 0.6369, p-value = 0.812&#10;alternative hypothesis: two-sided&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;However, if we do &lt;code&gt;ks.test(X, Y)&lt;/code&gt;, the statistic is highly significant.&lt;/p&gt;&#10;&#10;&lt;p&gt;I must be using the KS test wrong somehow, but how?&lt;/p&gt;&#10;&#10;&lt;p&gt;PS I was meant to write pchisq rather than rchisq&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-29T00:05:34.533" Id="117114" LastActivityDate="2014-10-01T14:10:55.147" LastEditDate="2014-10-01T14:10:55.147" LastEditorUserId="17398" OwnerUserId="17398" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;hypothesis-testing&gt;&lt;kolmogorov-smirnov&gt;" Title="How am I using KS test incorrectly in R?" ViewCount="58" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have pre post data for 5 different groups of kids living in different settings (rural, low income, high income, etc). The variables include age, gender, type of setting, treatment (exposure to one hour of preschool programming), viewing (home or school), treatment condition for viewing (treatment or control group), pre and post test scores. Sample sizes for each setting are around 50 kids. Whats the best way to determine which group performed the best on the test? There is more that one test that essentially assesses things like body parts, number knowledge, letter knowledge etc. Any help is appreciated. Tx&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-29T08:56:38.093" Id="117152" LastActivityDate="2014-09-29T08:56:38.093" OwnerUserId="56626" PostTypeId="1" Score="0" Tags="&lt;methodology&gt;" Title="methodology for assessing experimental design" ViewCount="13" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm trying to use a 1D histogram as a feature for machine learning. A histogram instance can be very sparse and the range of its bins is theoretically unbounded. Moreover it is expected that non-zero values for one instance would be grouped together with some holes between. On the other hand machine learning algorithms generally expect dense features with constant number of parameters.&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem seems to be quite similar to transforming text in the bag-of-words representation to a dense low-dimensional feature space.&lt;/p&gt;&#10;&#10;&lt;p&gt;An obvious approach would be to transform the sparse vector into a dense one. I've came across several suggestions for solving this problem:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;dictionary learning&lt;/li&gt;&#10;&lt;li&gt;semantic hashing&lt;/li&gt;&#10;&lt;li&gt;SparsePCA&lt;/li&gt;&#10;&lt;li&gt;random projections&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;What would you recommend and why? Thanks.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-09-29T09:07:48.563" Id="117154" LastActivityDate="2014-09-29T09:07:48.563" OwnerUserId="38246" PostTypeId="1" Score="0" Tags="&lt;dimensionality-reduction&gt;&lt;feature-construction&gt;&lt;sparse&gt;" Title="Reduction of sparse features for machine learning" ViewCount="24" />
  <row AnswerCount="2" Body="&lt;p&gt;The basic propensity score matching procedure works with cross-section data (ie collected at a certain point in time). The popular &lt;a href=&quot;http://repec.org/bocode/p/psmatch2.html&quot;&gt;psmatch2&lt;/a&gt; command uses a dummy variable indicating that an observations belongs to either the treatment or control group.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, in my dataset this indicator-function is time-varying. The data looks as follows: I identify individuals and track them over a period of 12 years. At some point during this time period, individuals may get treated, so they &quot;transfer from control to treatment group&quot;. Note that this can occur at any moment in time (sic!) for each individual.&lt;/p&gt;&#10;&#10;&lt;p&gt;Above that, my subsequent analysis would be time-dependent: From the moment the individuals are being treated, I want to estimate the ATT but only for the following year (sic!). Thus, I do not care anymore about the treated individuals 1 year after their treatment started.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;psmatch2&lt;/code&gt; seems to be very restrictive when it comes to that. Do you see any possibility on how to do that? Maybe there are also Matching Methods that allow me to do that.&lt;/p&gt;&#10;&#10;&lt;p&gt;P.S.: There is one similar &lt;a href=&quot;http://stats.stackexchange.com/questions/61218/propensity-score-matching-with-panel-data&quot;&gt;question&lt;/a&gt; here in the forum, however it did not help me to solve this problem.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-09-29T09:41:36.653" FavoriteCount="1" Id="117156" LastActivityDate="2014-09-29T20:09:16.990" LastEditDate="2014-09-29T10:53:53.263" LastEditorUserId="930" OwnerUserId="56628" PostTypeId="1" Score="6" Tags="&lt;stata&gt;&lt;panel-data&gt;&lt;propensity-scores&gt;" Title="Propensity Score Matching with time-varying treatment" ViewCount="250" />
  
  <row Body="&lt;p&gt;One way to go about this would be to calculate the circular dispersion as in &lt;a href=&quot;http://stats.stackexchange.com/a/115123/55410&quot;&gt;this  answer&lt;/a&gt;. If you set the constant $c$ fairly high, ie. 2 or 3, you may view all observations outside the interval    &lt;/p&gt;&#10;&#10;&lt;p&gt;$ \left[\hat\mu - c \hat\delta,  \hat\mu + c \hat\delta \right]$&lt;/p&gt;&#10;&#10;&lt;p&gt;as outliers. In that answer, you may also find some code to visualize this. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-29T10:25:30.627" Id="117158" LastActivityDate="2014-09-29T10:25:30.627" OwnerUserId="55410" ParentId="109704" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="117216" AnswerCount="3" Body="&lt;p&gt;I have a large data set of about 1.8 million rows with 80 variables.  I would like to find a good technique (code or package) in R that can reduce the amount of training data without damaging the representation of the original data too much.  I'm going to use this data set for two purposes: &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Predicting a binary outcome ( rows that have &quot;true&quot; as a value are only 1.5% of the data).&lt;/li&gt;&#10;&lt;li&gt;Predicting a continuous variable.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Any Idea what technique in R can help with this issue?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-29T12:10:25.487" Id="117171" LastActivityDate="2014-09-29T16:42:12.030" LastEditDate="2014-09-29T14:09:55.903" LastEditorUserId="2339" OwnerUserId="49422" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;sampling&gt;&lt;sample-size&gt;" Title="smart sampling techniques in r" ViewCount="79" />
  <row AnswerCount="3" Body="&lt;p&gt;What are the restrictions of application fields in searching for association rules (finding frequent itemsets)? &lt;/p&gt;&#10;&#10;&lt;p&gt;All examples I came across cover topic of 'true' basket-analysis in the sense of using a list of products which a sample of customers purchased with a goal to find rules such 'when one buys bread, it is likely butter is bought, too'.&lt;/p&gt;&#10;&#10;&lt;p&gt;What about &lt;strong&gt;more abstract applications?&lt;/strong&gt; I mean finding &lt;strong&gt;any&lt;/strong&gt; rules in dataset. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;EXAMPLE&lt;/em&gt;. Let's assume I have a huge dataset with tourist-trip prices in 2013 year. The data includes trip-price and trip-features (such country of destination, days the travel lasts, accommodation condition elements, means of transport, extracurricular activities etc.). I want to find different associations between price and other trip features. My idea is to categorize price variable and find frequent itemsets among these trips (e.g. &lt;em&gt;air conditioning&lt;/em&gt;=true, &lt;em&gt;5* hotel&lt;/em&gt;=true and &lt;em&gt;Australia&lt;/em&gt;=true &lt;strong&gt;=&gt;&lt;/strong&gt;  &lt;em&gt;high price&lt;/em&gt;=true).&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Is this a good way to work with such problems? &lt;/li&gt;&#10;&lt;li&gt;Would you suggest any other general way of dealing with searching for any types of assocciations in different data sets? &lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2014-09-29T12:11:06.237" FavoriteCount="1" Id="117172" LastActivityDate="2014-10-07T13:33:57.657" LastEditDate="2014-09-30T10:48:12.923" LastEditorUserId="32094" OwnerUserId="32094" PostTypeId="1" Score="1" Tags="&lt;data-mining&gt;&lt;association-rules&gt;" Title="Finding association rules / frequent Itemsets - what are the application restrictions" ViewCount="104" />
  <row AnswerCount="1" Body="&lt;p&gt;I am working with GBM models using the caret package and looking to find a method to solve the prediction intervals for my predicted data.  I have searched extensively but only come up with a few ideas to find prediction intervals for Random Forest.  Any help/R code would be greatly appreciated!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-29T14:10:33.723" FavoriteCount="2" Id="117190" LastActivityDate="2015-02-12T12:27:40.037" OwnerUserId="35751" PostTypeId="1" Score="2" Tags="&lt;caret&gt;&lt;prediction-interval&gt;&lt;gbm&gt;" Title="How to find a GBM Prediction Interval" ViewCount="253" />
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to simplify $E[YE[Y|X]|X]$ can I use this property: &lt;/p&gt;&#10;&#10;&lt;p&gt;$$E[E[Y|X]|X]= E[Y|X]$$&lt;/p&gt;&#10;&#10;&lt;p&gt;If yes I have never seen a Proof of this property (that seems very reasonable), could I have a reference? If I am right the correct result would be $$E[YE[Y|X]|X] = E[(Y|X)]^2$$ &lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-09-29T15:01:54.170" FavoriteCount="1" Id="117198" LastActivityDate="2014-09-29T15:30:23.683" OwnerUserId="48259" PostTypeId="1" Score="2" Tags="&lt;self-study&gt;&lt;expected-value&gt;&lt;conditional-expectation&gt;" Title="On $E[E[Y|X]|X]= E[Y|X]$" ViewCount="63" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a set of data for 2 visits in patients and I would like to see whether there is a effect of a difference of one variable on another. &#10;So, lets say, my variables are A, B, age + gender. I want to see the effect of (amount of change in B) on (amount of change in A). &lt;/p&gt;&#10;&#10;&lt;p&gt;Is it correct if I model this as &lt;/p&gt;&#10;&#10;&lt;p&gt;A(BL-FU) = B(BL-FU) + A(BL) + B(BL) + age + gender ?&lt;/p&gt;&#10;&#10;&lt;p&gt;Also age and gender effects both by A and B. &lt;/p&gt;&#10;&#10;&lt;p&gt;Please advice on the model. Thank you. &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-09-29T15:11:05.533" Id="117203" LastActivityDate="2014-09-29T15:37:45.890" LastEditDate="2014-09-29T15:37:45.890" LastEditorUserId="38393" OwnerUserId="38393" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;generalized-linear-model&gt;&lt;linear-model&gt;&lt;panel-data&gt;" Title="Linear model with longitudinal data, predicting difference" ViewCount="22" />
  <row AcceptedAnswerId="117758" AnswerCount="1" Body="&lt;p&gt;in GLMM faq page &lt;a href=&quot;http://glmm.wikidot.com/faq&quot; rel=&quot;nofollow&quot;&gt;http://glmm.wikidot.com/faq&lt;/a&gt; there is a statement about overfitting:&lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;One alternative (suggested by Robert LaBudde) is to &quot;fit the model with the random factor as a fixed effect, get the level coefficients in the sum to zero form, and then compute the standard deviation of the coefficients.&quot; This is appropriate for users who are (a) primarily interested in measuring variation (i.e. the random effects are not just nuisance parameters, and the variability [rather than the estimated values for each level] is of scientific interest)&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;What exactly does this mean in practice, doe sit mean using &lt;code&gt;offset()&lt;/code&gt; for the random effects that are now fixed effects?&lt;/p&gt;&#10;&#10;&lt;p&gt;Also is there a way to diagnose overfitting other than very small random effect variance components? For instance how do you get the df for a GLMM that are reported in papers?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-29T15:59:24.990" Id="117209" LastActivityDate="2014-10-03T15:17:19.973" LastEditDate="2014-09-29T16:11:49.213" LastEditorUserId="17865" OwnerUserId="17865" PostTypeId="1" Score="4" Tags="&lt;glmm&gt;&lt;lme4&gt;" Title="GLMM overfitting solutions" ViewCount="116" />
  
  <row Body="&lt;p&gt;I would actually say do both. I think it's more important to describe the non-transformed data because, well, that's the data you have. However, describing the transformed data tells about what you ended up analyzing and could also give the reader insight into why you had to transform the data.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-29T19:41:08.250" Id="117231" LastActivityDate="2014-09-29T19:41:08.250" OwnerUserId="46522" ParentId="117174" PostTypeId="2" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;A local tire dealer wants to offer a refunds without taking too big of a risk. The dealer is willing to refund no more than 4% of customers for what mileage can he guarantee these tires to last? The mean is 32,000 and the standard deviation is 2,500. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have tried using $x - \frac{32,000}{2,500} = .04$ and answer was 32,100 which does not make sense to me. I tried using 4% to find the number in the standard normal probabilities chart which gave me $x - \frac{32,000}/{2,500} = .4840$ and $x=33,210$ which also does not make sense to me. I have it noted I need to find the z score of 4% and plug it into $x-\frac{32,000}{2500}$ but I am still lost! &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-09-29T21:22:16.420" Id="117248" LastActivityDate="2014-09-29T23:18:59.140" LastEditDate="2014-09-29T23:18:59.140" LastEditorUserId="54028" OwnerUserId="56668" PostTypeId="1" Score="0" Tags="&lt;mathematical-statistics&gt;&lt;standard-deviation&gt;" Title="Snow tire tread problem" ViewCount="36" />
  <row Body="&lt;p&gt;&lt;a href=&quot;http://www.cs.berkeley.edu/~jordan/courses/281B-spring04/readings/escobar-west.pdf&quot; rel=&quot;nofollow&quot;&gt;Escobar and West&lt;/a&gt; give the standard Gibbs update for a gamma prior on the concentration parameter $\alpha$. Also note that the likelihood of the $\alpha$ given a partition $\mathscr P = \{b_1, b_2, \ldots, b_P\}$ of the data is given by &#10;$$&#10;\frac{\Gamma(\alpha) \alpha^P}{\Gamma(\alpha + N)} \prod_{p = 1} ^ P \Gamma(|b_p|), &#10;$$&#10;so you can use this to construct a Metropolis-Hastings update for a non-conjugate prior if you desire (of course, make sure that you work on in log-probability space). &lt;/p&gt;&#10;&#10;&lt;p&gt;It is also possible to estimate $\alpha$ by maximum marginal likelihood, although this is a little trickier. It is possible for the MLE of $\alpha$, to be $\hat \alpha = \infty$ or $\hat \alpha = 0$. It also happens to be the case that the Fisher information about $\alpha$ accrues at roughly a logrithmic rate, so you shouldn't expect the MLE to behave like other MLE's you've seen (the log-likelihood is often nowhere near quadratic even in moderate/large sample sizes). It is also possible for the likleihood of $\alpha$ to be multi-modal, as shown by &lt;a href=&quot;http://arxiv.org/pdf/1002.4756.pdf&quot; rel=&quot;nofollow&quot;&gt;Kyung et al&lt;/a&gt;, although I've never seen this in practice. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you &lt;em&gt;really&lt;/em&gt; want to use maximum likelihood, the MLE is defined by the moment matching condition $E_\alpha[P] = E_\alpha [P \mid \mbox{Data}]$ where $P$ is the number of groups. To get the MLE, one can iteratively update estimates $\hat \alpha^{(t)}$ to satisfy this moment matching condition, running a seperate chain for each $t$, setting &#10;$$&#10;E_{\hat \alpha^{(t+1)}} [P] = E_{\hat \alpha^{(t)}} [P \mid \mbox{Data}].&#10;$$ $E_{\alpha}[P]$ is given in closed form by $\sum_{i = 1} ^ N \frac{\alpha}{\alpha + i - 1}$, and $E_{\hat \alpha^{(t)}}[P \mid \mbox{Data}]$ is available from the output of the Markov chain. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-09-30T01:59:33.320" Id="117268" LastActivityDate="2014-09-30T15:00:58.227" LastEditDate="2014-09-30T15:00:58.227" LastEditorUserId="5339" OwnerUserId="5339" ParentId="117262" PostTypeId="2" Score="0" />
  
  
  <row Body="&lt;p&gt;The answer to your question is context dependent. Just about anything can happen&lt;/p&gt;&#10;&#10;&lt;p&gt;1) The trivial case is the quality of the fit reducing. This happens, when you remove a variable which explains part of the variance not explained by any other variable (you have written &quot;makes $R^{2}$ larger which is not correct.. Recall $R^{2}$ = $ 1 - \frac {S_{Res}}{S_{tot}}$ so when $S_{Res}$ increases, $R^{2}$ will decrease). &lt;/p&gt;&#10;&#10;&lt;p&gt;2) The quality of the fit remains the same: Consider the case where you are fitting a regression using multiple variables and you decide to remove a variable X which is perfectly correlated with another variable Y (which you do not remove). In this case, you would expect the quality of the fit to remain the same.&lt;/p&gt;&#10;&#10;&lt;p&gt;3) Suppose we have 100 observations and 300 variables we are using to predict. You do not have enough predictive power in this case BUT if you know before hand (say) 295 are useless and remove them your $R^{2}$ may increase&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-09-30T04:40:18.533" Id="117277" LastActivityDate="2014-09-30T04:40:18.533" OwnerUserId="49398" ParentId="117274" PostTypeId="2" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;I have 4 different groups and i want to look if the means significant differ from each other but before that i want to know if the power in every group is high enough. How can i calculate the power for every single group in SPSS?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-09-30T07:58:58.607" Id="117286" LastActivityDate="2014-09-30T07:58:58.607" OwnerUserId="56695" PostTypeId="1" Score="0" Tags="&lt;spss&gt;" Title="How can I calculate power in SPSS" ViewCount="44" />
  <row AnswerCount="0" Body="&lt;p&gt;I have the following setup. There is a set $S = \{S_1, \ldots, S_N\}$ of $N$ sensors that are probed for readings (once). Each reading is an independent sample from one of the two distributions $r_i \sim D_0$ or $r_i \sim D_1$. There is a fixed set $\mathcal T \subset S$ of size $t$ that yields samples from $D_1$. Essentially we are trying to identify this set. So we output $\mathcal A \subset N$. There are no particular restrictions on the size of $t=|\mathcal T|$ or c = $|\mathcal A|$. We can assume $|\mathcal A| &amp;gt; |\mathcal T|$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now individually, testing each sensor is a simple hypothesis testing problem where $H_0 \sim D_0$ vs $H_1 \sim D_1$. Essentially we can perform the Likelihood ratio test $L(\mathbf y) \gtrless \tau$ based on the desired error probabilities.&lt;/p&gt;&#10;&#10;&lt;p&gt;Based on this, it would seem that the optimal detector for the problem above would obtain the Likelihood ratios for each sensor reading $L(\mathbf y_i)$ for $1 \leq i \leq N$ and pick the $c$ largest values from these. We would output the corresponding sensors as our estimate, i.e. $\mathcal A$ consistes of the sensors corresponding to $c$ largest Likelihood ratios.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now I am trying to obtain $\mathcal P_r\left(\mathcal A = S'~|~\mathcal T = T'\right)$, for a fixed $S'$ and $T'$. My logic for computing this is as follows. &lt;/p&gt;&#10;&#10;&lt;p&gt;Let $f_i(x), F_i(x) $ be the pdf and cdf of the random variable $X = L(\mathbf y)$, corresponding to $\mathbf y \sim D_i$. We will use $X_i$ to denote the random variable corresponding to the Likelihood ration from sensor $i$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Assume there is an overlap of $k$ nodes between $S'$ and $T'$, i.e. $|S' \cup T'| = k$. Let $\mathcal S' = \{S_1,\ldots,S_c\}$. We can obtain a joint distribution for the Likelihood ratios $(x_1,\ldots,x_c)$ corresponding to this set $S'$ and the set $S\setminus S'$, $(x_{c+1}, \ldots, x_N)$. The set $S'$ is selected as long as $X_i &amp;gt; X_j$, where $i$ is the index of sensors in $S'$ and $j$ the index of those in $S\setminus S'$. i.e. $\min(X_1, \ldots, X_c) \geq max(X_{c+1}, \ldots, X_N)$. The probability can equivalently be written as&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{align}&#10;\mathcal P_r &amp;amp; \left(\mathcal A  = S'~| ~\mathcal T = T'\right)\\&#10;= &amp;amp; ~ P_r \left(\min(X_1, \ldots, X_c) \geq \max(X_{c+1}, \ldots, X_N) \right) \\&#10;= &amp;amp; ~ \int_0^\infty P_r\left( \min(X_1, \ldots, X_c) \geq t\right ) dF_{\max(X_{c+1}, \ldots, X_N)}(t) \\&#10;&amp;amp; \textit{(Using independence of $X_i$'s)}\\&#10;= &amp;amp; ~ \int_0^\infty\left((1-F_1(t))^{k}(1-F_0(t))^{c-k}\right) dF_{\max(X_{c+1}, \ldots, X_N)}(t)\\&#10;&amp;amp; \textit{(As we had assumed $k$ nodes overlap)}\\&#10;= &amp;amp; ~ \int_0^\infty(1-F_1(t))^{k}(1-F_0(t))^{c-k} \left( (t-k) F_1(t)^{t-k-1}F_0(t)^{n-c-(t-k)}dF_1(t)\right.\\&#10; + &amp;amp; ~ \left. (n-c - (t-k)) F_1(t)^{t-k}F_0(t)^{n-c-(t-k) -1 }dF_0(t)\right)\\&#10;&amp;amp; \textit{(pdf of order statistic)}\\&#10;= &amp;amp; ~ (t-k) \int_0^\infty(1-F_1(t))^{k}(1-F_0(t))^{c-k} F_1(t)^{t-k-1} F_0(t)^{n-c-(t-k)} f_1(t) dt \\&#10; + &amp;amp; ~ (n-c-(t-k))\int_0^\infty(1-F_1(t))^{k}(1-F_0(t))^{c-k} F_1(t)^{t-k-1} F_0(t)^{n-c-(t-k)-1} f_0(t) dt \\&#10;\end{align}&#10;I am stuck here. I would like to know the following&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;Can this expression be simplified any further. I would like to compute some type of bound on this or difference of two such probabilities &#10;(say for different $T$). I tried a few tricks to further simplify this (recursive series), but can't make headway. It seems I may be missing some simple tricks. My Likelihood ratios are complicated, so I assumed actual computations will be difficult. The Likelihood ratios are of the form $L(\mathbf y) = \prod_{i=1}^L\cosh(y_i)$&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Is there another way I can see the test to obtain a simpler probability function. The test by sorting Likelihood ratios, is it the most optimal strategy, or can there be a better detection mechanism. It seems intuitively sound, but I have not been able to come up with any formal reasoning.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Thank you for your help.&lt;/p&gt;&#10;&#10;&lt;p&gt;(I have also posted this on the Math forum)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-30T09:02:28.490" Id="117290" LastActivityDate="2014-09-30T09:02:28.490" OwnerUserId="56693" PostTypeId="1" Score="0" Tags="&lt;hypothesis-testing&gt;&lt;conditional-probability&gt;&lt;order-statistics&gt;" Title="Hypothesis testing and order statistics" ViewCount="39" />
  
  <row Body="&lt;p&gt;I am not sure understanding your question, but I suggest you looking at the statistical model details of parfm in the companion paper&lt;/p&gt;&#10;&#10;&lt;p&gt;Munda M, Rotolo F, Legrand C. (2012) parfm: Parametric Frailty Models in R. J Stat Soft, 51(12).&#10;&lt;a href=&quot;http://www.jstatsoft.org/v51/i11&quot; rel=&quot;nofollow&quot;&gt;http://www.jstatsoft.org/v51/i11&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Hope this helps&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-30T12:23:21.133" Id="117318" LastActivityDate="2014-09-30T12:23:21.133" OwnerUserId="56709" ParentId="89801" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;If you use &lt;code&gt;R&lt;/code&gt; and the &lt;code&gt;nlme&lt;/code&gt; package, the syntax would be:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(nlme)&#10;m &amp;lt;- lme(dv ~ iv*emo_eating, random=~1|id, data=d)&#10;summary(m)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;where &lt;code&gt;emo_eating&lt;/code&gt; is the continuous predictor, &lt;code&gt;id&lt;/code&gt; is the subject-ID variable, and &lt;code&gt;d&lt;/code&gt; is your data stored as a data frame. I am assuming that &lt;code&gt;iv&lt;/code&gt; is a fixed effect. The &lt;code&gt;summary(m)&lt;/code&gt; will give you regression output. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-30T12:44:48.483" Id="117321" LastActivityDate="2014-09-30T12:44:48.483" OwnerUserId="53057" ParentId="117312" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;Firstly, use the modified &lt;a href=&quot;http://en.wikipedia.org/wiki/Holm%E2%80%93Bonferroni_method&quot;&gt;Holm-Bonferroni&lt;/a&gt; method instead of the plain Bonferroni method.&lt;/p&gt;&#10;&#10;&lt;p&gt;Regarding the 49 vs 1200 question. In my opinion, if you want to make statistical assumptions on all the variables together, then you have to use the 1200 number. The number 49 would come in order if you wanted to test whether a single variable is somehow different from the others.&lt;/p&gt;&#10;&#10;&lt;p&gt;But still, both numbers are very high and I think you will have problems with the corrected α values as they will be ridiculously small.&lt;/p&gt;&#10;&#10;&lt;p&gt;Maybe a change of method is in order, as pairwise comparison is definitely not the best option here. Maybe some other method, which could test on all the variables altogether - like ANOVA or Friedman test (but I doubt that those two will be relevant for your case).&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-09-30T13:10:34.227" Id="117325" LastActivityDate="2014-09-30T13:10:34.227" OwnerUserId="56637" ParentId="117316" PostTypeId="2" Score="5" />
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;My situation:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;small sample size: 116&lt;/li&gt;&#10;&lt;li&gt;binary outcome variable&lt;/li&gt;&#10;&lt;li&gt;long list of explanatory variables: 50&lt;/li&gt;&#10;&lt;li&gt;explanatory variables did not come from the top of my head; their choice was based on the literature.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Following a suggestion to a previous &lt;a href=&quot;http://stats.stackexchange.com/questions/102892/how-to-select-a-subset-of-variables-from-my-original-long-list-in-order-to-perfo&quot;&gt;question&lt;/a&gt; of mine, I have run LASSO (using R's glmnet package) in order to select the subset of exaplanatory variables that best explain variations in my binary outcome variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have noticed that I get very different values of lambda.min through k-folds cross-validation (cv.glmnet command) according the value I attribute to k. I have tried the default (10) and 5. Which would be the most appropriate value for k, considering my sample size?&lt;/p&gt;&#10;&#10;&lt;p&gt;In my specific case, is it necessary to repeat cross-validation, say 100 times, in order to reduce randomness and allow averaging the error curves, as is suggested in &lt;a href=&quot;http://stats.stackexchange.com/questions/97777/variablity-in-cv-glmnet-results&quot;&gt;this&lt;/a&gt; post? If so: I have tried the code suggested in that post, but got error messages, could anyone suggest a better code?&lt;/p&gt;&#10;&#10;&lt;p&gt;UPDATE1: I have managed to use the &lt;code&gt;foldid&lt;/code&gt; option in &lt;code&gt;cv.glmnet&lt;/code&gt;, as suggested in the comments below, by organizing my x-matrix in a way that all the 32 observations belonging to one of my outcome classes appears in lines 1-32 and by using the folowing code: &lt;code&gt;foldid=c(sample(rep(seq(10),length=32),sample(rep(seq(10),length=84))&lt;/code&gt;. However, when I ran &lt;code&gt;cv.glmnet&lt;/code&gt;, only one of the levels of a categorical variable with four levels was included in the model. So following a suggestion to a &lt;a href=&quot;http://stats.stackexchange.com/questions/108745/how-to-deal-with-not-applicable-values-in-categorical-variables&quot;&gt;previous&lt;/a&gt; question of mine, I tried to run group-lasso using R's gglasso package. And now I am facing &lt;a href=&quot;http://stats.stackexchange.com/questions/117669/which-r-packages-offer-the-foldid-or-simliar-parameter-for-cross-validation-of&quot;&gt;this&lt;/a&gt; issue. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-30T14:41:45.790" Id="117339" LastActivityDate="2014-10-05T19:28:44.500" LastEditDate="2014-10-05T19:28:44.500" LastEditorUserId="48426" OwnerUserId="48426" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;lasso&gt;" Title="How to ensure that the most appropriate value for lambda is chosen in lasso?" ViewCount="150" />
  <row AnswerCount="0" Body="&lt;p&gt;I have groups of locations categorized by their sales performance and weekly variability into four tiers. We are running on-site tests for 100 locations in Tier A against a set of 50 control locations also selected form Tier A. &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Tier A: 369 locations&lt;/li&gt;&#10;&lt;li&gt;Tier B: 109 locations&lt;/li&gt;&#10;&lt;li&gt;Tier C: 109 locations&lt;/li&gt;&#10;&lt;li&gt;Tier D: 369 locations&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;If we want to expand the program into Tier B locations, should I also select a new subset of control locations from Tier B? Or is it valid to continue benchmarking those against the original Tier A control locations?&lt;/p&gt;&#10;&#10;&lt;p&gt;The goal is to measure success/ROI for the marketing programs at these locations.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-30T15:10:20.993" Id="117343" LastActivityDate="2014-09-30T15:10:20.993" OwnerUserId="45224" PostTypeId="1" Score="0" Tags="&lt;experiment-design&gt;&lt;control&gt;" Title="Selecting control locations for tiered groups?" ViewCount="5" />
  <row AnswerCount="0" Body="&lt;p&gt;How does one determine the percentage of a sample less than or equal to some x value for a set of discrete data that appear to be right-skewed?  For example, I have a number of data points, and if I were to draw a curve through them it would look something like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/VMq7e.gif&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;What statistics are most relevant for a sample that appears to be drawn from a right-skewed distribution (suppose we assume that it is from such a distribution based on the result of the sample)?  For example, what would be the best measure of deviation for such a distribution?  As mentioned by Nick it seems like the negative binomial distribution with a shape parameter $r$ of around 5.  &lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-09-30T16:51:03.377" Id="117355" LastActivityDate="2014-09-30T17:34:55.067" LastEditDate="2014-09-30T17:34:55.067" LastEditorUserId="40120" OwnerUserId="40120" PostTypeId="1" Score="0" Tags="&lt;skewness&gt;&lt;cdf&gt;" Title="CDFs for Right-Skewed Distributions" ViewCount="63" />
  <row Body="&lt;p&gt;First of all, you are right this dataset maybe is not the best to understand the mixed model. But let's look first why&lt;/p&gt;&#10;&#10;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;require(foreign)&#10;dt &amp;lt;- read.dta(&quot;http://www.ats.ucla.edu/stat/stata/seminars/svy_stata_intro/srs.dta&quot;)&#10;&#10;length(dt$dnum)          # 310&#10;length(unique(dt$dnum))  # 187 &#10;sum(table(dt$dnum)==1)   # 132&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;You see that you have 310 observations and 187 groups, of which 132 have only one observation. This does not mean that we should not use multi-level modelling, but just that we won't get very much different results as you stated. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Multi-level modelling motivation&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The motivation to use multi-level modelling starts from the design itself, and not just from the results of the undertaken analysis. Of course the most common example is taking multiple observations from individuals, but to make things more extreme to give a more easily understanding situation, think asking individuals from different countries around the world about their income. So best examples are those that have a lot of heterogeneity, as taking clusters which are homogeneous in the examining outcome of course won't make much difference.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;So, let's simulate some data to make things clearer, simulation works better as with real life data is not that obvious. Imagine you take $10$ countries and you ask $100$ individuals from each country about their income &lt;code&gt;y&lt;/code&gt; and something else &lt;code&gt;x&lt;/code&gt; that has a positive effect in income with coefficient $0.5$.&lt;/p&gt;&#10;&#10;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;set.seed(1)&#10;I &amp;lt;- 100&#10;J &amp;lt;- 10&#10;n &amp;lt;- I*J&#10;i &amp;lt;- rep(1:I, each=J)&#10;j &amp;lt;- rep(1:J,I)&#10;x &amp;lt;- rnorm(n,mean=0, sd=1)&#10;beta0  &amp;lt;- 1000&#10;beta1  &amp;lt;- 0.5&#10;sigma2 &amp;lt;- 1&#10;tau2   &amp;lt;- 200&#10;u &amp;lt;- rep(rnorm(I,mean=0,sd=sqrt(tau2)),each=J)&#10;y &amp;lt;- beta0 + beta1*x + u + rnorm(n,mean=0, sd=sqrt(sigma2))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So, running a linear model you get &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; summary(lm(y~x))&#10;&#10;Coefficients:&#10;            Estimate Std. Error  t value Pr(&amp;gt;|t|)    &#10;(Intercept) 999.8255     0.4609 2169.230   &amp;lt;2e-16 ***&#10;x             0.5728     0.4456    1.286    0.199    &#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1&#10;&#10;Residual standard error: 14.57 on 998 degrees of freedom&#10;Multiple R-squared:  0.001653,  Adjusted R-squared:  0.0006528 &#10;F-statistic: 1.653 on 1 and 998 DF,  p-value: 0.1989&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and you conclude that &lt;code&gt;x&lt;/code&gt; has no statistical effect in &lt;code&gt;y&lt;/code&gt;. See how big is the standard error. But running a random-intercept model&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; summary(lmer(y~x + (1|i)))&#10;&#10;Random effects:&#10; Groups   Name        Variance Std.Dev.&#10; i        (Intercept) 213.062  14.597  &#10; Residual               1.066   1.032  &#10;Number of obs: 1000, groups:  i, 100&#10;&#10;Fixed effects:&#10;            Estimate Std. Error t value&#10;(Intercept) 999.8247     1.4600   684.8&#10;x             0.4997     0.0327    15.3&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;you see how much the standard error of the estimate has changed. Looking at the random effect part, we see how the variability has been decomposed - most of the variability in income is between countries, and within countries people have more similar incomes. In simple words, what happened here is that not accounting for the clustering the effect of &lt;code&gt;x&lt;/code&gt; is &quot;getting lost&quot; (if we can use this kind of term), but decomposing the variability you find what you should actually get. &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-09-30T16:58:00.673" Id="117357" LastActivityDate="2014-09-30T21:52:46.163" LastEditDate="2014-09-30T21:52:46.163" LastEditorUserId="42434" OwnerUserId="42434" ParentId="116994" PostTypeId="2" Score="11" />
  <row AnswerCount="0" Body="&lt;p&gt;My model steps:&lt;br&gt;&#10;1.I fitted a logistic regression model $Y\sim X$;&lt;br&gt;&#10;2.then get the probability $P(Y=1)$ for each record;&lt;br&gt;&#10;3.then I summed the probability $R = \sum_i(P_i(Y=1))$ to be the expected return, $i$ is an index of records (rows). &lt;/p&gt;&#10;&#10;&lt;p&gt;Now I want to calculate a confidence interval for $R$. I looked on line and read some text books, it seems that the 95% confidence interval is derived at records level by $x \pm 1.96\,\text{SE}$, while SE = $\sqrt{\text{var}(\alpha+\beta\ x)}$, where $\alpha$ is intercept, $\beta$ is coefficient, $x$ is the feature value of the record. This is not what I am looking for. &lt;/p&gt;&#10;&#10;&lt;p&gt;I am thinking to do a bootstrapping on the data set, so that I can get the distribution of $R$. Is there other straightforward ways to calculate the confidence interval? &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-09-30T19:25:43.323" Id="117374" LastActivityDate="2014-10-01T04:36:17.203" LastEditDate="2014-10-01T04:36:17.203" LastEditorUserId="805" OwnerUserId="18303" PostTypeId="1" Score="0" Tags="&lt;logistic&gt;&lt;confidence-interval&gt;&lt;predictive-models&gt;&lt;prediction-interval&gt;" Title="confidence interval for aggregated expectation from logistic regression" ViewCount="21" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Is there a closed form distribution for the transformation given by $Y = \sqrt{X_1^2 + X_2^2}$, when $X_1, X_2$ are jointly normal but dependent random&#10;variables with different variance?&lt;/p&gt;&#10;&#10;&lt;p&gt;OBS: I know, when $X_1$ and $X_2$ are independent and have the same variance, the resultant distribution is $Y \sim {\rm Rice}(\sqrt{\mu_1^2 + \mu_2^2},\sigma)$, but in this case $X_1$ and $X_2$ are linear transformations of the same original distributions, so, they will be dependent and have different variance.&lt;/p&gt;&#10;" ClosedDate="2014-09-30T21:50:21.297" CommentCount="1" CreationDate="2014-09-30T19:37:25.403" Id="117377" LastActivityDate="2014-09-30T23:43:52.060" LastEditDate="2014-09-30T23:43:52.060" LastEditorUserId="22311" OwnerUserId="45759" PostTypeId="1" Score="1" Tags="&lt;distributions&gt;&lt;data-transformation&gt;&lt;random-variable&gt;" Title="Distribution for $Y = \sqrt{X_1^2 + X_2^2}$, when $X_1, X_2$ are dependent and normally distributed with different variance?" ViewCount="42" />
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I want to replicate a fuzzy regression using a linear programming problem approach. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have the following information: &quot; A fuzzy regression analysis with only one independent variable X results in the following bivariate regression model:&#10;$$ \hat{Y}=\tilde{A_o}+\tilde{A_1}X,$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\tilde{A_o}$ is a is a fuzzy intercept, $\tilde{A_1}$is a fuzzy slope coefficient, the parameters are expressed as $\tilde{A_i}=(m_i,c_i)$ where $m_i$ is a centre  and $c_i$ is the fuzzy half-width.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;To determine the fuzzy coefficients the following linear programming problem is used:&lt;/p&gt;&#10;&#10;&lt;p&gt;minimize $$ S= nc_0 + c_1\sum_{i=1}^{n}|X_i|$$&lt;/p&gt;&#10;&#10;&lt;p&gt;subject to $$c_0\geqslant0,\geqslant0,$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\sum_{j=0}^{l}m_iX_{ij}+(1-h)\sum_{j=0}^{l}c_i|X_{ij}| \geqslant Y_i+(1-h), \mbox{for i=1 to n}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\sum_{j=0}^{l}m_iX_{ij}-(1-h)\sum_{j=0}^{l}c_i|X_{ij}| \geqslant Y_i-(1-h), \mbox{for i=1 to n}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $h=0$&lt;/p&gt;&#10;&#10;&lt;p&gt;I have the following data :&#10;$[X_i : Y_i]=[(2:14),(4:16),(6:14),(8:18),(10:18),(12:22),(14:18),&#10;(16:22)]$&lt;/p&gt;&#10;&#10;&lt;p&gt;How to  solve the LP using R?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-01T08:22:07.310" Id="117437" LastActivityDate="2014-10-01T12:05:11.853" LastEditDate="2014-10-01T12:05:11.853" LastEditorUserId="805" OwnerUserId="31619" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;regression&gt;&lt;linear-model&gt;&lt;fuzzy&gt;" Title="Fuzzy regression (using linear programming)" ViewCount="51" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have several dependent and independent variables. All my dependent variables are binary, therefore, I want to perform a multivariate multiple logistic regression, is that possible to do in R?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-01T12:07:29.167" Id="117455" LastActivityDate="2014-10-01T12:07:29.167" OwnerUserId="21840" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;logistic&gt;&lt;multivariate-analysis&gt;" Title="Multivariate Multiple Logistic Regression in R" ViewCount="52" />
  
  
  
  <row Body="&lt;p&gt;Your example works.  In the distribution, $X_1$ is marginally independent of $X_3$.  That is, knowing the value of $X_3$ (but not $X_2$) does not change the distribution of $X_1$, and vice versa.  But the diagram does not show this.  According to the diagram, knowing $X_3$ could change the distribution of $X_1$.  &lt;/p&gt;&#10;&#10;&lt;p&gt;To see this mathematically:&#10;$$&#10;p(X_1=1) = 0.5 \qquad&#10;p(X_3=1) = 0.5&#10;\\&#10;p(X_1=1, X_3=1) = p(X_1=1,X_2=0,X_3=1) = 0.5^2&#10;\\&#10;p(X_1=1 | X_3=1) = \frac{p(X_1=1, X_3=1)}{p(X_3=1)} = 0.5 = p(X_1=1)&#10;$$&#10;and similarly for all values of $X_1$ and $X_3$.  The key step is realizing that $X_2$ can only have one value once $X_1,X_3$ are specified, so that $p(X_1,X_3)$ is always $0.5^2$.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-10-01T15:12:38.230" Id="117482" LastActivityDate="2014-10-01T17:29:10.070" LastEditDate="2014-10-01T17:29:10.070" LastEditorUserId="2074" OwnerUserId="2074" ParentId="117214" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="117581" AnswerCount="1" Body="&lt;p&gt;When determining the necessary sample size for a &lt;code&gt;t test&lt;/code&gt; of a difference of mean of two populations a good rule of thumb is typically $$ n = 16 {\sigma^2 \over \delta^2}$$ where $\delta$ is the threshold above which you want to detect a difference in the population means. However, if you want to reverse engineer this to say &quot;Given a sample size $n$, how  big of a difference can I detect between the two means&quot;, then the calculation for $\delta$ is given &lt;a href=&quot;http://www.evanmiller.org/how-not-to-run-an-ab-test.html&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; as $$ \delta = (t_{\alpha/2} + t_{\beta})\sigma \sqrt{2/n}$$ &lt;/p&gt;&#10;&#10;&lt;p&gt;I'm wondering what distribution to draw the above t-statistics from. Here $\alpha$ is typically $0.05$ for a $95\%$ confidence interval, and $1 - \beta$ is the power of the test. In &lt;code&gt;R&lt;/code&gt;, would it be &#10;&lt;code&gt;t_alpha_2 = abs(qt(.025, df = df))&lt;/code&gt; for $\alpha = 0.05$? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-01T15:19:55.563" Id="117485" LastActivityDate="2014-10-02T06:34:54.430" OwnerUserId="13022" PostTypeId="1" Score="2" Tags="&lt;hypothesis-testing&gt;&lt;distributions&gt;" Title="How to draw these t-statistics in R" ViewCount="34" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;Is it possible to perform a paired samples t-test in a structural equation modeling (SEM) program? I am confused, as I have found on the web that you can run a Wald test, but I am not sure if it is the same as a t-test. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-01T18:42:18.637" Id="117507" LastActivityDate="2014-11-07T07:37:12.287" LastEditDate="2014-11-07T04:51:09.657" LastEditorUserId="17072" OwnerUserId="49709" PostTypeId="1" Score="4" Tags="&lt;t-test&gt;&lt;sem&gt;&lt;mplus&gt;" Title="Paired samples t-test using a structural equation modeling approach" ViewCount="119" />
  <row Body="&lt;p&gt;Your model with auto.arima() looks good and it does not contradict in any way the fact that the original data is seasonally adjusted. The sma and the period term [12] means that R knows that this data was seasonally adjusted and auto.arima tries to find the best model which fit the data. If there were no seasonal terms in the model, then we would have a problem. And the sma2 is not significant. :)   &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-01T18:43:21.763" Id="117508" LastActivityDate="2014-10-01T18:43:21.763" OwnerUserId="56792" ParentId="117474" PostTypeId="2" Score="0" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am comparing two counties to see which one has more grandchildren living with grandparents that left the home because of lack of program support. One county has a grandparent support program and one does not. Hypothesis is that county with more resources/support has fewer removals.  What is the best test?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-10-01T19:56:06.323" Id="117514" LastActivityDate="2014-10-01T19:56:06.323" OwnerUserId="56794" PostTypeId="1" Score="0" Tags="&lt;hypothesis-testing&gt;" Title="Help! Stat test to compare groups" ViewCount="16" />
  <row AnswerCount="1" Body="&lt;p&gt;I am working with a data set of ~1200 rows and 60 variables, and I'm trying to build a multiple linear regression model. I do this by separating 10% of the dataset to be used for validation and I use the rest of the data to train the linear model. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have found that I am able to build very strong regression models, with an adjusted coefficient of determination (R-square) around .998 and &amp;lt;3% of the predicted values trained on the validation set deviating from the true values by more than accepted threshold (also 3%). I also have all the variables selected  (~20 variables) as significant at the 5% level, including first-order interaction variables. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, I have found that by randomly selecting a different 10% of the data as validation, the regression model I trained previously does not consistently have its explanatory variables remain significant, and the proportion of predicted values within the accepted threshold varies, sometimes improving but usually decreasing. The coefficients of the variables also vary. However, the R-square remains extremely high.&lt;/p&gt;&#10;&#10;&lt;p&gt;How can I be sure to build a stable model? Or rather, what could I reasonably consider a stable model? I'm confused by the apparent disparity between how the R-square and the actual predicted values explain the strength of the model. I'm also concerned that variables previously considered to be significant at the 5% level often fail to remain so. Can anyone give any tips on how to approach model building so that the proportion of the data saved for validation doesn't drastically alter explanatory variables selected?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-10-01T20:47:02.307" Id="117526" LastActivityDate="2014-10-01T22:40:07.787" OwnerUserId="56798" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;statistical-significance&gt;&lt;linear-model&gt;" Title="Multiple Linear Regression: Obtaining a Stable Model" ViewCount="83" />
  <row AnswerCount="1" Body="&lt;p&gt;I am curious about the contents while I read a note on machine learning. It could be obvious. So, please let me know if I am missing some fundamental things.&lt;/p&gt;&#10;&#10;&lt;p&gt;$X_1,X_2,...,X_n$ are from an i.i.d. distribution with mean $\mu$ and variance $\sigma&amp;lt;\infty$. Then,&#10;by C.L.T, $s_n$ is approximately normal with mean $\mu$ and $\frac{\sigma}{\sqrt{n}}$, so that it has the density: $f_{s_n}(x)=\frac{1}{\sqrt{\frac{2\pi \sigma^2}{n}}}e^{-\frac{n}{2}\frac{(x-u)^2}{\sigma^2}}$. &lt;/p&gt;&#10;&#10;&lt;p&gt;What I cannot grasp is the following comment: &lt;strong&gt;&quot;This approximation is only valid for x within about $\frac{\sigma}{\sqrt{n}}$ of $\mu$&quot;&lt;/strong&gt;. (FYI, this note is about large deviation.)&lt;/p&gt;&#10;&#10;&lt;p&gt;I also wondering how this statement is closely related to the &lt;strong&gt;Berry-Esseen thm&lt;/strong&gt;. &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-10-01T21:04:44.973" Id="117527" LastActivityDate="2015-03-03T23:12:14.120" LastEditDate="2014-10-01T21:31:47.020" LastEditorUserId="56688" OwnerUserId="56688" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;central-limit-theorem&gt;" Title="A simple question on CLT in possible connection with Berry-Esseen thm" ViewCount="50" />
  
  <row Body="&lt;p&gt;Let $X_t$ be a zero-mean covariance-stationary time series such that&#10;$$X_t = \varphi_1 X_{t-1} + \varphi_2 X_{t-2} + \varepsilon_t$$&#10;where $\varepsilon_t$ is white noise.&lt;/p&gt;&#10;&#10;&lt;p&gt;Using $L$ to mean the lag (backshift) operator, the above can be expressed as&#10;$$(1-\varphi_1L - \varphi_2L^2)X_t=\varepsilon_t . \tag{1}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Since $X_t$ is a covariance-stationary AR(2) process, the roots of its characteristic polynomial $(1-\varphi_1 z - \varphi_2 z^2) = 0$ must lie outside the unit circle. Thus, Equation (1) can be written as&#10;$$(1-\lambda_1 L)(1-\lambda_2 L)X_t=\varepsilon_t $$&#10;where $\vert\lambda_1\rvert&amp;lt;0$ and $\vert\lambda_2\rvert&amp;lt;0$. The last two inequalities are true of a covariance-stationary AR(2) process since then the roots of the characteristic polynomial, $z^*_1=1/\lambda_1$ and $z^*_2=1/\lambda_2$, will lie outside the unit circle. Therefore,&#10;$$X_t= \frac{1}{(1-\lambda_1 L)} \frac{1}{(1-\lambda_2 L)} \varepsilon_t .$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Expand the two fractions on the right-hand side in the equation above using the geometric series. and you'll have the Wold decomposition of an AR(2).&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-10-02T00:58:06.247" Id="117552" LastActivityDate="2014-10-02T12:28:53.177" LastEditDate="2014-10-02T12:28:53.177" LastEditorUserId="36630" OwnerUserId="36630" ParentId="117519" PostTypeId="2" Score="4" />
  
  <row AnswerCount="0" Body="&lt;p&gt;How to plot an ellipse from eigenvalues and eigenvectors for [ 5 4 &#10;                                                               4 5]. I know eigenvalues are 9 and 1 and eigenvectors are (0.5,0.5) and (0.5,-0.5). How can I plot an ellipse (centre, main axis and points of intersection)? Please and thank you!&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-10-02T09:02:18.527" Id="117591" LastActivityDate="2014-10-02T09:02:18.527" OwnerUserId="39755" PostTypeId="1" Score="0" Tags="&lt;mathematical-statistics&gt;&lt;multivariate-analysis&gt;&lt;linear-algebra&gt;" Title="Multivariate Statistics" ViewCount="22" />
  
  <row AnswerCount="0" Body="&lt;blockquote&gt;&#10;  &lt;p&gt;There is a building with 7 floors and 8 apartments per floor. The building has 2 vacant apartments. A lady wants an apartment on the top floor. What's the probability that she finds one in this building?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Considering first the probability that there are two vacant apartments in the top floor:&#10;$$\frac{\binom{8}{2}}{\binom{56}{2}} = \frac{1}{55}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Considering now the probability that there is one vacant apartment and that it is on the top floor:&#10;$$\frac{8}{56}$$&#10;But we know that there are two vacant apartments so the probability that the top floor has one vacant apartment doubles.&lt;/p&gt;&#10;&#10;&lt;p&gt;So the total probability is:&#10;$$\frac{1}{55}+\frac{16}{56} = 0.304...$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The solution manual says this is incorrect. I need to know why.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-10-02T10:37:55.210" Id="117603" LastActivityDate="2014-10-07T03:13:58.650" LastEditDate="2014-10-07T03:13:58.650" LastEditorUserId="7290" OwnerUserId="56519" PostTypeId="1" Score="0" Tags="&lt;probability&gt;&lt;self-study&gt;" Title="Why is my probability incorrect?" ViewCount="44" />
  <row AnswerCount="0" Body="&lt;blockquote&gt;&#10;  &lt;p&gt;Let X take on p values with equal probability. If n trials are to be conducted to ensure that the probability of not observing any of these p values is less than or equal to q, what is the value of n?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Here's what I've been able to do so far -&#10;Let $ X $ take on values $ 1, 2, ... p $. Let $ X_i $ denote the event that the value $ i $ is observed in the $ n $ trials. Then, we want&lt;/p&gt;&#10;&#10;&lt;p&gt;$ Pr(X_1^C \cup X_2^C \cup ... X_p^C) \leq q $&lt;/p&gt;&#10;&#10;&lt;p&gt;$ X_1^C \cup X_2^C \cup ... X_p^C = (X_1 X_2 ... X_p)^ C $, so we need $ Pr(X_1 X_2 ... X_p) \geq 1 - q $. &lt;/p&gt;&#10;&#10;&lt;p&gt;And one lower bound for this probability is &lt;/p&gt;&#10;&#10;&lt;p&gt;$ Pr(X_1 X_2 ... X_p) &amp;gt; \frac{{n-1 \choose p-1}}{p^n} $. Using this, &lt;/p&gt;&#10;&#10;&lt;p&gt;$ \frac{{n-1 \choose p-1}}{p^n} &amp;gt; 1 - q $&lt;/p&gt;&#10;&#10;&lt;p&gt;Not sure how to obtain the value of n from here. Are there other bounds/methods which make the calculation for $ n $ more conducive? &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-10-02T13:22:44.463" Id="117616" LastActivityDate="2014-10-02T13:42:40.653" LastEditDate="2014-10-02T13:42:40.653" LastEditorUserId="7290" OwnerUserId="56833" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;self-study&gt;&lt;random-variable&gt;" Title="Number of trials to observe all values of a uniform discrete random variable X with a probability of at least 1-q?" ViewCount="23" />
  <row AnswerCount="1" Body="&lt;p&gt;I have a group ( only 20 of them, each one has 170 time pointers)  of time series that I can consider as &quot;GOOD&quot;, meaning, they have consistent statistical characteristics. I am not sure how they are generated, but they are biological data, so I assume the current point is not independent to its temporal neighbors.  I think ARMA model is the good direction. So I fit the group of time series with ARAM model and get the order (p,q). Note that unfortunately, I don't have any sample from the &quot;BAD&quot; distribution, all other time series that deviates largely from the &quot;GOOD&quot; ones can be considered as &quot;BAD&quot;. &lt;/p&gt;&#10;&#10;&lt;p&gt;Now given a new time series, how can I measure the likelihood that it is generated by ARMA(p,q) ? Or anything that's proportional to the likelihood. &#10;I don't think ARMA itself can infer this probability, but I wonder if there are some other useful techniques that I am not aware of.&#10;Or is there any other approach I can take to tackle the problem? &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-02T14:04:49.520" Id="117618" LastActivityDate="2014-10-02T18:22:47.663" OwnerUserId="56800" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;python&gt;&lt;arma&gt;&lt;consistency&gt;" Title="The likelihood that a time series is generated by certain ARMA(p,q) ?" ViewCount="47" />
  <row Body="&lt;p&gt;You can get the population standard deviation by computing the variance via integration and then taking the square root. (It's not the only possible way to compute a variance but it's fairly routine integration for this problem.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Assuming you have a t$_{(\mu,\sigma^2,\nu)}$ distribution, you can replace $\mu$ by 0 without changing the variance. You can reparameterize to get rid of $\sigma$ (the variance is $\sigma^2$ times that of a standard t). Note that $\nu$ is the degrees of freedom. The location $\mu$ will only be the mean when it exists and $\sigma$ is a scale parameter but $\sigma^2$ is not the variance.&lt;/p&gt;&#10;&#10;&lt;p&gt;So you're left with finding the variance of a standard t$_\nu$.&lt;/p&gt;&#10;&#10;&lt;p&gt;$$f(x) = \frac{\Gamma(\frac{\nu+1}{2})} {\sqrt{\nu\pi}\,\Gamma(\frac{\nu}{2})} \left(1+\frac{x^2}{\nu} \right)^{-\frac{\nu+1}{2}},$$&lt;/p&gt;&#10;&#10;&lt;p&gt;So &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\int_{-\infty}^\infty x^2f(x)dx = 2c\int_0^\infty x^2(1+\frac{x^2}{\nu})^{-\frac{\nu+1}{2}}dx$$ &lt;/p&gt;&#10;&#10;&lt;p&gt;where $c$ is the constant term out the front. There are a number of ways to approach that.&lt;/p&gt;&#10;&#10;&lt;p&gt;You could write that as $2c\nu$ times an integral with $x^2/\nu$ in it, then add 1 and subtract 1, pull off the -1 term into a separate integral (easily done), so you end up with a different power (and so a different $\nu$, $\nu^*$ say). You then rescale $x$ to that new  $\nu^*$ and pull out the appropriate constant to leave something times $\int_0^\infty (1+\frac{x^2}{\nu^*})^{-\frac{\nu^*+1}{2}}dx$, supply the required constant to make that a pdf (multiply and divide by by that constant), cancel the integral of a pdf out, and you're left with constant terms being a function of $\nu$.&lt;/p&gt;&#10;&#10;&lt;p&gt;An alternative would be to recognize something like a beta of the second kind in the integral $\int_0^\infty x^2(1+\frac{x^2}{\nu})^{-\frac{\nu+1}{2}}dx$. Or there are other approaches to doing the integral.&lt;/p&gt;&#10;&#10;&lt;p&gt;The required result is given on the &lt;a href=&quot;http://en.wikipedia.org/wiki/Student%27s_t-distribution&quot; rel=&quot;nofollow&quot;&gt;Wikipedia page for the distribution&lt;/a&gt;, though, so if you just need the result for the standard t, you can get it there -- $\text{Var}(X)=\frac{\nu}{\nu-2}$, so $\text{sd}(X)=\sqrt{\frac{\nu}{\nu-2}}$.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-10-02T16:35:52.923" Id="117639" LastActivityDate="2014-10-03T01:09:57.220" LastEditDate="2014-10-03T01:09:57.220" LastEditorUserId="805" OwnerUserId="805" ParentId="117626" PostTypeId="2" Score="7" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a questionare. Questionare has six variables, suppose x1,x2,....,x6 and each variable has six questions. each questions has a response , strongly agree, agree, indifferent, disagree, strongly disagree. Response is in the form of percentage of people. I want to check the hypothesis between x1 and x5, x2 and x5, x3 and x5, x4 and x5, x6 and x5. &#10;What should i do and which one is the appropriate tool?&#10;I want to do this analysis in R.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-02T19:00:37.070" FavoriteCount="1" Id="117658" LastActivityDate="2014-10-02T19:00:37.070" OwnerUserId="4299" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;categorical-data&gt;" Title="Categorical Data" ViewCount="33" />
  <row AnswerCount="0" Body="&lt;p&gt;Software packages seem to prefer to work with the upper triangular part of the Cholesky factorization, see for example &lt;code&gt;cholupdate&lt;/code&gt;. Why is this? It seems that it is more natural to represent a covariance matrix by it's lower triangular Cholesky factorization. For example, $L z$, where $L$ is the lower triangular Cholesky factorization and $z$ is a vector of standard normal normals, will give you a sample from a multivariate normal distribution. What are the uses of the upper triangular part?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-10-02T19:41:53.917" Id="117661" LastActivityDate="2014-10-02T19:50:31.140" LastEditDate="2014-10-02T19:50:31.140" LastEditorUserId="53332" OwnerUserId="53332" PostTypeId="1" Score="0" Tags="&lt;cholesky&gt;" Title="Why use upper triangular Cholesky?" ViewCount="41" />
  <row Body="&lt;p&gt;It's primarily because you omitted the case where $x=0$. Also (I'm not sure I follow everything you are doing) you seem to be confusing the &lt;em&gt;binomial&lt;/em&gt; and the &lt;em&gt;negative&lt;/em&gt; binomial distributions. The binomial is for the number of successes in a given number of trials, and negative binomial is for the number of trials encountered before a given number of successes is observed.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-10-02T19:50:24.517" Id="117662" LastActivityDate="2014-10-02T19:50:24.517" OwnerUserId="52554" ParentId="117659" PostTypeId="2" Score="1" />
  
  
  
  <row AcceptedAnswerId="117792" AnswerCount="1" Body="&lt;p&gt;I am currently working on a project where I need to simulate the prices of a set of $D$ substitutable commodities over time.&lt;/p&gt;&#10;&#10;&lt;p&gt;I was hoping to do this using the following $D$-dimensional lognormal random walk model:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\ln(p_{t}) = \ln(p_{t-1}) + \varepsilon_t, $$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $p_{t}$ is a $D\times 1$ vector containing the prices of all commodities at time $t$, and $\varepsilon_t$ is a $D$-dimensional Gaussian random variable with mean $\mu \in \mathbb{R}^D$ and covariance $\Sigma \in \mathbb{R}^{D \times D}$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Given $T$ data points on the prices, $(p_{t})_{t=1}^T$, I can fit this model by constructing &lt;a href=&quot;http://en.wikipedia.org/wiki/Log-normal_distribution&quot; rel=&quot;nofollow&quot;&gt;MLE estimates&lt;/a&gt; for the parameters, $\hat{\mu}$ and $\hat{\Sigma}$. &lt;/p&gt;&#10;&#10;&lt;p&gt;My question is: what is the right way to &lt;code&gt;validate&quot; my model in such a case? Is there an interpretable&lt;/code&gt;metric&quot; to assess the predictive capacity of the random walk? &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-10-02T23:42:51.587" FavoriteCount="1" Id="117693" LastActivityDate="2014-10-03T20:44:55.083" LastEditDate="2014-10-03T20:44:55.083" LastEditorUserId="3572" OwnerUserId="3572" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;maximum-likelihood&gt;&lt;validation&gt;" Title="How to validate a lognormal random walk for time series data" ViewCount="41" />
  <row AnswerCount="0" Body="&lt;p&gt;Using Lasso reduces the coefficients of features of a model, reducing some to zero, and thereby performing feature selection. The number of features depends on the value of $\alpha$ aka $\lambda$.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the extreme case where I have no features remaining and only the intercept (=average) remains, plotting my predicted values against the known values will give me a horizontal line since the intercept has no dependence on any features that give rise to the known values.&lt;/p&gt;&#10;&#10;&lt;p&gt;As I increase the number of features used, the gradient will slowly increase, until over-fitting occurs and I have a perfect 45-degree line between prediction and known values.&lt;/p&gt;&#10;&#10;&lt;p&gt;If I want to settle for the case where only two features remain, I can choose an appropriate value of $\alpha$, but the gradient may still be very shallow.&lt;/p&gt;&#10;&#10;&lt;p&gt;It seems to me that I can then perform normal least-squares using only these remaining two features to try and gain a better (ie, not too shallow) fit. Is this reasonable? Or am I thinking about this in the wrong way? Thanks for any advice!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-03T05:40:04.180" Id="117715" LastActivityDate="2014-10-03T05:40:04.180" OwnerUserId="52769" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;feature-selection&gt;&lt;least-squares&gt;&lt;lasso&gt;&lt;fitting&gt;" Title="Least-squares fitting with only optimum features, after Lasso - valid?" ViewCount="26" />
  <row AnswerCount="0" Body="&lt;p&gt;Suppose you have a factory that does statistical process control by randomly selecting workpieces and measuring them.&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose you've got 1000 workpieces in a month, and each piece starts as raw material, goes through 100 steps,  and finally gets accepted or rejected as satisfying some percentage of acceptability tests.  So each finished product gets a grade, e.g. 85% acceptable, or 90% acceptable, or some value like that.  &lt;/p&gt;&#10;&#10;&lt;p&gt;If we measure every single workpiece after every single step, we would have 100,000 data points, and we could do statistics to see which measurements correlated with which levels of final acceptability score.&lt;/p&gt;&#10;&#10;&lt;p&gt;Even if we only measured each workpiece after every two steps, we would still have 50,000 datapoints, and we could probably use some incomplete-data method to make predictions.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have some rough idea of some of the methods that might be used:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Kalman_filter&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Kalman_filter&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;However, I don't know that those methods would be useful for this problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;The challenge is, this factory only measures workpieces at random, and it measures very few workpieces.  The measurements don't always occur at the same steps.  For 1000 workpieces at 100 steps, the factory only collects some very small number of measurements - perhaps 2000, perhaps just 1000.  The data is very sparse.&lt;/p&gt;&#10;&#10;&lt;p&gt;At some point, the data must be so sparse that it's impossible to predict the quality of the final products from the process control measurements that are made during production.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Are there any particular methods which are good for drawing conclusions from very sparse quality control data?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-03T06:11:19.000" Id="117717" LastActivityDate="2014-10-03T06:11:19.000" OwnerUserId="35890" PostTypeId="1" Score="0" Tags="&lt;statistical-control&gt;" Title="For industrial statistical process with sparse data, what is the maximum allowable sparseness?" ViewCount="9" />
  <row AcceptedAnswerId="117731" AnswerCount="1" Body="&lt;blockquote&gt;&#10;  &lt;p&gt;It is planned to conduct a study on the percentage of homeowners who have at least two TVs.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;What should be the sample size if we want to ensure that $95\%$ of estimation error is less than $0.01$?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;The original French text version:&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/9tDvE.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;We want to calculate &#10;$$\operatorname{A confidence interval}=\overline{X}\pm E\quad  \textrm{with}\quad E={{Z}_{\tfrac{\alpha }{2}}}\sqrt{\dfrac{P(1-P)}{n}}$$&#10;or &#10;$$E=0.01, \quad {Z}_{\tfrac{\alpha }{2}}=1.98\quad P = 0.5 \quad n=?$$&#10;Then &#10;$$\operatorname{A confidence interval}=1.98\times \sqrt{\frac{0.5\left(1-0.5\right)}{n}}=0.01$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$0.01=1.98\sqrt{\dfrac{{{0.5}^{2}}}{n}}$$&#10;$$n=9801$&#10;Thus the sample size should be equal $n=9801$&lt;/p&gt;&#10;&#10;&lt;p&gt;Am I right ?&lt;/p&gt;&#10;&#10;&lt;p&gt;Any help would be much appreciated&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-10-03T07:58:29.567" Id="117723" LastActivityDate="2014-10-03T09:24:47.803" LastEditDate="2014-10-03T09:04:20.620" LastEditorUserId="27991" OwnerUserId="27991" PostTypeId="1" Score="1" Tags="&lt;confidence-interval&gt;&lt;inference&gt;&lt;inferential-statistics&gt;" Title="Sampling: the % of homeowners who own at least 2 TVs" ViewCount="26" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a study where clustering occurs in one condition but not the other. In the treatment group I have repeated measurements on individuals who are nested within families. And in the control condition I only have repeated measurements on individuals, but no clustering due to family. &lt;/p&gt;&#10;&#10;&lt;p&gt;Some dummy data would look like this&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;       time id family     group           y&#10;    1     0  1      1 treatment  0.58407458&#10;    2     1  1      1 treatment  0.57629394&#10;    3     2  1      1 treatment  1.16558208&#10;    4     0  2      1 treatment -1.03117769&#10;    5     1  2      1 treatment  0.87066744&#10;    6     2  2      1 treatment  0.42714038&#10;    7     0  3      1 treatment  0.62503878&#10;    8     1  3      1 treatment  0.11275242&#10;    9     2  3      1 treatment  0.66396118&#10;    10    0  4      2 treatment  0.07094150&#10;    11    1  4      2 treatment -0.44600018&#10;    ..    ....      .  .......    .......&#10;    28    0 10      NA   control -1.66283938&#10;    29    1 10      NA   control  1.17574655&#10;    30    2 10      NA   control -0.59692375&#10;    31    0 11      NA   control -1.94929165&#10;    32    1 11      NA   control  0.88162730&#10;    33    2 11      NA   control -2.38991654&#10;    ..    ....      .  .......    .......&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;If both conditions were nested within families I would run&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lmer(y ~ time*group + (1 | family/id), data=study_data)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;But I don't think this fits the right model in this scenario? What would be the correct way to fit a partially clustered design?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-10-03T12:16:20.720" Id="117739" LastActivityDate="2014-10-03T12:30:20.743" LastEditDate="2014-10-03T12:30:20.743" LastEditorUserId="42434" OwnerUserId="11450" PostTypeId="1" Score="1" Tags="&lt;multilevel-analysis&gt;&lt;lmer&gt;" Title="Fitting a partially clustered/nested design using lmer" ViewCount="59" />
  
  <row Body="&lt;p&gt;You don't need to use MCMC.  You simply apply the GP prediction formula (as given in a &lt;a href=&quot;http://stats.stackexchange.com/questions/66709/confusion-related-to-predictive-distribution-of-gaussian-processes&quot;&gt;previous question&lt;/a&gt;).  This only involves some linear algebra.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-03T18:42:49.193" Id="117777" LastActivityDate="2014-10-03T18:42:49.193" OwnerUserId="2074" ParentId="117002" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;Let $ X_i$ be i.i.d $Exp(\theta)$ for i=1,...,4.We want to test $H_0: \theta =6$ versus $ H_1: \theta = 2$. Consider the following test:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\text{Test: Rejects H_o} \iff \frac{X_1 + X_2}{2}&amp;gt;4.5$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Evaluate the probability of error type I&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;My attempt:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;For the iid exponential we have that&lt;/p&gt;&#10;&#10;&lt;p&gt;$$Y=X_1+X_2 \sim Gamma(2,\theta)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;So,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\alpha=P(\text{Error type I})=P(Y &amp;gt; 9|H_0) = P(Y&amp;gt;9|\theta=6)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Under $ H_0,$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$Y \sim Gamma(2,6)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Then,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$12Y \sim Gamma(\frac{4}{2}, \frac{1}{2}) = \chi^2(4)$$&#10;$$\Rightarrow \alpha=P(12Y&amp;gt;108|H_0)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Is that correct? I couldnt find a good table that gives this value, so I thought there could be somehing wrong. If one could indicate a reliable table for chi-squared, I would be glad!&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;@Edit&lt;/strong&gt;:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;https://www.google.com.br/url?sa=t&amp;amp;source=web&amp;amp;rct=j&amp;amp;ei=yAkvVJjjD4TgsATzhoL4CA&amp;amp;url=http://stat.psu.edu/~mga/401/tables/Chi-square-table.pdf&amp;amp;cd=6&amp;amp;ved=0CDIQFjAF&amp;amp;usg=AFQjCNFuwKleTzB8plcVC0eecgCxbBlXxQ&amp;amp;sig2=y7qGreQ3SCBMn5ElGhPvHg&quot; rel=&quot;nofollow&quot;&gt;This&lt;/a&gt; table is great, but I think that 108 is tpo high (with makes me to have doubts about my solution). If it is correct, it follows that $\alpha$ is very close to 0.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-10-03T20:38:32.343" Id="117793" LastActivityDate="2014-10-03T20:45:37.540" LastEditDate="2014-10-03T20:45:37.540" LastEditorUserId="35224" OwnerUserId="35224" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;&lt;self-study&gt;&lt;chi-squared&gt;&lt;gamma-distribution&gt;" Title="Error type I for $X_i \sim Exp(\theta)$" ViewCount="16" />
  <row AcceptedAnswerId="117811" AnswerCount="1" Body="&lt;p&gt;I am trying to classify data into four different labels. The training data looks something like:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;[&#10;  [[0.1 , 0.6, 0.0, 0.3], 1, 10, 0, 0, 0],&#10;  [[0.7 , 0.3, 0.0, 0.0], 0, 7, 22, 0, 0],&#10;  [[0.0 , 0.0, 0.6, 0.4], 0, 0, 6, 0, 20],&#10;  ...&#10;]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Each row is an instance. First column contains the labels. Columns &lt;code&gt;2..n&lt;/code&gt; contain the features.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am trying to understand how to train a multilabel classifier using &lt;a href=&quot;http://scikit-learn.org/stable/modules/multiclass.html#multilabel-learning&quot; rel=&quot;nofollow&quot;&gt;SciKit OneVsRestClassifier&lt;/a&gt; but I just don't get how I must proceed in the code.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have been able to do single-label classification for this dataset replacing the first column with a single value (eg. the first instance would map completely to the second label) but I would like the more nuanced multi-label output. This is what I use for the single label classifier (assuming above dataset):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;labels = [1,0,3, ...]&#10;data = [[1, 10, 0, 0, 0], [0, 7, 22, 0, 0], [0, 0, 6, 0, 20], ...]&#10;clf = svm.SVC(kernel='poly')&#10;clf.fit(data, labels)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Any idea how to convert this to multi-labels?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-10-03T21:10:34.830" Id="117796" LastActivityDate="2014-10-03T22:41:46.450" OwnerUserId="56913" PostTypeId="1" Score="1" Tags="&lt;classification&gt;&lt;scikit-learn&gt;&lt;multilabel&gt;" Title="scikit multi label classification" ViewCount="207" />
  <row Body="&lt;p&gt;Looking at that Gelman and Vehtari reference, they seem to define a cross-validation criterion which resembles a likelihood - so that isn't interpretable as an absolute measure, in the same way AIC isn't. In other words, the &quot;out-of-sample prediction error&quot; measure they use is in itself not an &quot;absolute&quot; measure of fit quality, in the sense that it can't be used to measure fit quality per se.&lt;/p&gt;&#10;&#10;&lt;p&gt;Does that sound right?&lt;/p&gt;&#10;&#10;&lt;p&gt;I'd be surprised if either of those author pairings got the technical details around AIC wrong. My problems with B&amp;amp;A's approach tend to be more on the implementation...and the rhetoric... &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-10-03T21:47:15.463" Id="117805" LastActivityDate="2014-10-03T22:29:16.647" LastEditDate="2014-10-03T22:29:16.647" LastEditorUserId="56921" OwnerUserId="56918" ParentId="117790" PostTypeId="2" Score="2" />
  
  <row AcceptedAnswerId="117823" AnswerCount="1" Body="&lt;p&gt;Let the three independent events $ A, B,$ and $ C$  be such that $P(A)=P(B)=P(C)= \frac14.$  find $P[(A^*\cap B^*) \cup C].$&lt;/p&gt;&#10;&#10;&lt;p&gt;My solution starts from using the probability of their complements which is $\frac34$, I do not know how to answer this question. Please help.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-10-04T04:01:24.070" Id="117821" LastActivityDate="2014-10-04T04:50:29.973" LastEditDate="2014-10-04T04:25:56.073" LastEditorUserId="9394" OwnerUserId="56929" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;self-study&gt;" Title="Probability of three independent events" ViewCount="31" />
  <row AnswerCount="0" Body="&lt;p&gt;As you know we can use &lt;code&gt;Mcnemar's test&lt;/code&gt; to compare performance of two models in &lt;code&gt;binary classification problem&lt;/code&gt;. But in my case i want compare performance of two models that one of them has &lt;code&gt;outlier detection&lt;/code&gt; in per-processing stage and other doesn't have this technique, so we have different sample size in every model that will input to my model (for example &lt;code&gt;MLP&lt;/code&gt; or &lt;code&gt;SVM&lt;/code&gt;). I think in this case (because different size of samples) we can't use &lt;code&gt;Mcnemar's test&lt;/code&gt;. (Is this true?)&lt;/p&gt;&#10;&#10;&lt;p&gt;What test should i use in this case for compare these models?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-04T08:21:18.953" FavoriteCount="1" Id="117833" LastActivityDate="2014-10-04T08:54:01.363" LastEditDate="2014-10-04T08:54:01.363" LastEditorUserId="43534" OwnerUserId="43534" PostTypeId="1" Score="0" Tags="&lt;classification&gt;&lt;svm&gt;&lt;neural-networks&gt;&lt;binary&gt;&lt;mcnemar-test&gt;" Title="How we can statistically compare performance of two models before and after outlier detection?" ViewCount="32" />
  
  <row Body="&lt;p&gt;If you just want to sample $k$, then this is easy from the distributions given (plus a distribution for $k1$).  The procedure is:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Draw $k1$.&lt;/li&gt;&#10;&lt;li&gt;Draw $b2 \sim {\rm beta}(6,1)$ and set $k2 = \frac{k1}{b2}$.&lt;/li&gt;&#10;&lt;li&gt;Draw $b3 \sim {\rm beta}(2,12)$ and set $k3 = \frac{k1}{b3}$.&lt;/li&gt;&#10;&lt;li&gt;Draw $b4 \sim {\rm beta}(50,1)$ and $b5 \sim {\rm beta}(1,1)$ then solve the system&#10;$$&#10;k4+k5 = \frac{k1+k2+k3}{b4}&#10;\\&#10;k5 = \frac{k4}{b5}&#10;$$&#10;whose solution is $k5 = \frac{k1+k2+k3}{b4(1+b5)}$, $k4 = b5 k5$.&lt;/li&gt;&#10;&lt;li&gt;Finally, if any of the $k$'s are larger than 1, reject the sample and start over.\&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;You could avoid the rejection step by noting that all of the later $k$'s scale linearly with $k1$.  So once you've got $(b2, ..., b5)$, find the range of $k1$ values that ensure everyone is less than 1 and draw $k1$ within this range.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-04T11:37:52.753" Id="117845" LastActivityDate="2014-10-06T09:27:48.720" LastEditDate="2014-10-06T09:27:48.720" LastEditorUserId="2074" OwnerUserId="2074" ParentId="117780" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;These are all excellent proper scoring rules.  I have found them often difficult to interpret but that doesn't take away their value because of their high precision.  I recommend you supplement them with a high-resolution histogram of predicted risks once you establish that the predictions are well calibrated using smooth nonparametric calibration curves (e.g., using &lt;em&gt;loess&lt;/em&gt; with outlier detection turned off as in the R &lt;code&gt;rms&lt;/code&gt; package &lt;code&gt;val.prob&lt;/code&gt; function).  Supplement the histograms with the proportion of predicted risk &amp;lt; 0.05, &amp;lt; 0.1, &amp;lt; 0.2, &gt; 0.8, &gt; 0.9, &gt; 0.95.  Better discrimination among well-calibrated predictions means more predictions in the tails.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-04T15:29:19.167" Id="117862" LastActivityDate="2014-10-04T15:29:19.167" OwnerUserId="4253" ParentId="117859" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;It seems to me that the following is the mathematically simplest way to partial-out variables from a correlated set of items.   &lt;/p&gt;&#10;&#10;&lt;p&gt;Consider a correlation matrix R for 5 items, where we want to &quot;partial-out&quot; the first two variables. This is the initial correlation-matrix:&#10;$$ \text{ R =} \small \begin{bmatrix} \begin{array} {r}&#10;    1.00&amp;amp;   -0.15&amp;amp;    0.27&amp;amp;    0.53&amp;amp;    0.24\\&#10;   -0.15&amp;amp;    1.00&amp;amp;   -0.09&amp;amp;   -0.50&amp;amp;   -0.34\\&#10;    0.27&amp;amp;   -0.09&amp;amp;    1.00&amp;amp;    0.22&amp;amp;    0.19\\&#10;    0.53&amp;amp;   -0.50&amp;amp;    0.22&amp;amp;    1.00&amp;amp;    0.47\\&#10;    0.24&amp;amp;   -0.34&amp;amp;    0.19&amp;amp;    0.47&amp;amp;    1.00 \end{array}&#10;     \end{bmatrix} $$&#10;&lt;hr&gt;&#10;Now we want to partial out the first item. We determine the vector of correlations of all variables with it, this gives the vector $f_1$ (which is just the first column of &lt;strong&gt;R&lt;/strong&gt; : &#10;$$  f_1 = \small \begin{bmatrix} \begin{array} {r} &#10;    1.00\\&#10;   -0.15\\&#10;    0.27\\&#10;    0.53\\&#10;    0.24&#10;     \end{array} \end{bmatrix}&#10;$$&#10;Then build the matrix $R_1 = f_1 \cdot f_1^\tau$&#10;$$  \text{ R}_1 =\small \begin{bmatrix} \begin{array} {rrrrr} &#10;    1.00&amp;amp;   -0.15&amp;amp;    0.27&amp;amp;    0.53&amp;amp;    0.24\\&#10;   -0.15&amp;amp;    0.02&amp;amp;   -0.04&amp;amp;   -0.08&amp;amp;   -0.04\\&#10;    0.27&amp;amp;   -0.04&amp;amp;    0.07&amp;amp;    0.14&amp;amp;    0.06\\&#10;    0.53&amp;amp;   -0.08&amp;amp;    0.14&amp;amp;    0.28&amp;amp;    0.12\\&#10;    0.24&amp;amp;   -0.04&amp;amp;    0.06&amp;amp;    0.12&amp;amp;    0.06&#10;     \end{array} \end{bmatrix}&#10;$$ and subtract this from the original matrix to get $R_{ \; \cdot 1}$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$  \text{ R}_{\ \cdot 1} =\small \begin{bmatrix}  \begin{array} {rrrrr} &#10;    0.00&amp;amp;    0.00&amp;amp;    0.00&amp;amp;    0.00&amp;amp;    0.00\\&#10;    0.00&amp;amp;    0.98&amp;amp;   -0.05&amp;amp;   -0.42&amp;amp;   -0.30\\&#10;    0.00&amp;amp;   -0.05&amp;amp;    0.93&amp;amp;    0.07&amp;amp;    0.13\\&#10;    0.00&amp;amp;   -0.42&amp;amp;    0.07&amp;amp;    0.72&amp;amp;    0.35\\&#10;    0.00&amp;amp;   -0.30&amp;amp;    0.13&amp;amp;    0.35&amp;amp;    0.94&#10;     \end{array}&#10; \end{bmatrix}&#10;$$&#10;&lt;hr&gt;&#10;Now we look at the partial vector $f_{2 \cdot 1}$. First, we get just from extraction of the second column of the remaining covariance matrix. In order to have the entry in its second row such that then $R_{2 \cdot 1} = f_{2 \cdot 1} \cdot f_{2 \cdot 1}^\tau$ has the correct value in row and column 2 we must define $f_{2 \cdot 1} = f_{2 \cdot 1} / \sqrt{ f_{2 \cdot 1}[2]}$, thus we get:&#10;$$ f_{2 \cdot 1}= \small \begin{bmatrix}  \begin{array} {r} &#10;    0.00\\&#10;    0.99\\&#10;   -0.05\\&#10;   -0.42\\&#10;   -0.31&#10;     \end{array} \end{bmatrix} $$&#10;Then $ \text{ R }_{2 \cdot 1} =  f_{2 \cdot 1} \cdot  f_{2 \cdot 1}^\tau $ and we find&#10;$$ \text{ R }_{2 \cdot 1} = \small \begin{bmatrix}   \begin{array} {rrrrr} &#10;    0.00&amp;amp;    0.00&amp;amp;    0.00&amp;amp;    0.00&amp;amp;    0.00\\&#10;    0.00&amp;amp;    0.98&amp;amp;   -0.05&amp;amp;   -0.42&amp;amp;   -0.30\\&#10;    0.00&amp;amp;   -0.05&amp;amp;    0.00&amp;amp;    0.02&amp;amp;    0.01\\&#10;    0.00&amp;amp;   -0.42&amp;amp;    0.02&amp;amp;    0.18&amp;amp;    0.13\\&#10;    0.00&amp;amp;   -0.30&amp;amp;    0.01&amp;amp;    0.13&amp;amp;    0.09&#10;     \end{array}&#10; \end{bmatrix} $$&#10;and after removing that covariance as well by $ \text{ R }_{ \cdot 12}= \text{ R }_{ \cdot 1}- \text{ R }_{ 2\cdot 1} $ we get &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \text{ R }_{ \cdot 12} =\small \begin{bmatrix}   \begin{array} {rrrrr} &#10;    0.00&amp;amp;    0.00&amp;amp;    0.00&amp;amp;    0.00&amp;amp;    0.00\\&#10;    0.00&amp;amp;    0.00&amp;amp;    0.00&amp;amp;    0.00&amp;amp;    0.00\\&#10;    0.00&amp;amp;    0.00&amp;amp;    0.93&amp;amp;    0.05&amp;amp;    0.11\\&#10;    0.00&amp;amp;    0.00&amp;amp;    0.05&amp;amp;    0.54&amp;amp;    0.22\\&#10;    0.00&amp;amp;    0.00&amp;amp;    0.11&amp;amp;    0.22&amp;amp;    0.85&#10;     \end{array}&#10; \end{bmatrix} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;hr&gt;&#10;This can be iterated for the next variable(s) to be partialled out analoguously. You can then analyze the remaining nonzero-part as covariances, which are the &quot;partial correlations&quot; when the &quot;partialled-out&quot; variables are, so-to-say, &quot;held constant&quot;.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-10-04T18:49:04.510" Id="117880" LastActivityDate="2014-10-04T20:14:31.443" LastEditDate="2014-10-04T20:14:31.443" LastEditorUserId="1818" OwnerUserId="1818" ParentId="117840" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="117900" AnswerCount="1" Body="&lt;p&gt;I ask, because it is very hard to find information regarding the beta distribution and the bayesian inference, where the beta distri is NOT the prior.&lt;/p&gt;&#10;&#10;&lt;p&gt;My goal is to identify or to improve the two parameters $\alpha$ and $\beta$ of a fitted beta distribution from binned data with the pdf&lt;/p&gt;&#10;&#10;&lt;p&gt;$ f(x|\alpha,\beta)=\frac{1}{\text{B}(\alpha,\beta)} x^{\alpha-1}(1-x)^{\beta-1} $.&lt;/p&gt;&#10;&#10;&lt;p&gt;My first attempts to estimate the parameters is the method of moments, see &lt;a href=&quot;http://en.wikipedia.org/wiki/Beta_distribution#Method_of_moments&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Beta_distribution#Method_of_moments&lt;/a&gt; .&lt;/p&gt;&#10;&#10;&lt;p&gt;This works fine. But if the parameters are varying by time, what can I do? I thought of a mean/variance update and another use of the moments method. But can I update the mean and the variance each time a new measurement is coming, only with the old values of the mean and variance? And how can I weight the new data, so that my &quot;offline&quot; estimated parameters are more important? I couldn't find a solution so I thought maybe bayesian updating be the trick.&lt;/p&gt;&#10;&#10;&lt;p&gt;Assume, $\alpha$ and $\beta$ are the parameters $\theta$. Is there a way to estimate them? The problem is, my new data is only a &quot;bin&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example: I've got a variable $X_k$, where k is the discrete time. $X_{k+1}=f(X_k|\alpha,\beta)$ is a function of the beta pdf. $\alpha$ and $\beta$ are estimated from following measurements:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/gVFJv.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The parameters are estimated with moment matching. Then later I get new data. But only a count more at e.g. bin number 2. And then another measurement at bin 5 etc. After 100 measurements or so (or maybe at each measurement), I'd like to update the parameters. But I don't have the initial data anymore, since it need a lot of memory. Can that be done somehow?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you very much!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-10-04T19:22:52.700" Id="117882" LastActivityDate="2014-10-05T10:50:57.560" OwnerUserId="48253" PostTypeId="1" Score="2" Tags="&lt;bayesian&gt;&lt;beta-distribution&gt;" Title="Recursively updating the parameter of a Beta function in a bayesian way?" ViewCount="92" />
  
  
  
  <row Body="&lt;p&gt;I will use your example to explain how it works. Using polynomial contrasts with four groups yields following.&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{align}&#10;E\,write_1 &amp;amp;= \mu -0.67L  + 0.5Q -0.22C\\&#10;E\,write_2 &amp;amp;= \mu -0.22L -0.5Q + 0.67C\\&#10;E\,write_3 &amp;amp;= \mu + 0.22L -0.5Q -0.67C\\&#10;E\,write_4 &amp;amp;= \mu + 0.67L + 0.5Q + 0.22C&#10;\end{align}&lt;/p&gt;&#10;&#10;&lt;p&gt;Where first equation works for the group of lowest reading scores and the fourth one for the group of best reading scores. we can compare these equations to the one given using normal linear regression (supposing $read_i$ is continous)&lt;/p&gt;&#10;&#10;&lt;p&gt;$$E\,write_i=\mu+read_iL + read_i^2Q+read_i^3C$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Usually instead of $L,Q,C$ you would have $\beta_1, \beta_2, \beta_3$ and written at first position. But this writing resembles the one with polynomial contrasts. So numbers in front of $L, Q, C$ are actually instead of $read_i, read_i^2, read_i^3$. You can see that coefficients before $L$ have linear trend, before $Q$ quadratic and before $C$ cubic.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then R estimates parameters $\mu, L,Q,C$ and gives you&#10;$$&#10;\widehat{\mu}=52.79, \widehat{L}=14.26, \widehat{Q}=−0.97, \widehat{C}=−0.16&#10;$$&#10;Where $\widehat{\mu}=\frac{1}{4}\sum_{i=1}^4E\,write_i$ and estimated coefficients $\widehat{\mu}, \widehat{L}, \widehat{Q}, \widehat{C}$ are something like estimates at normal linear regression. So from the output you can see if estimated coefficients are significantly different from zero, so you could anticipate some kind of linear, quadratic or cubic trend. &lt;/p&gt;&#10;&#10;&lt;p&gt;In that example is significantly non-zero only $\widehat{L}$. So your conclusion could be: We see that the better scoring in writing depends linearly on reading score, but there is no significant quadratic or cubic effect.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-04T22:37:25.727" Id="117897" LastActivityDate="2014-10-04T22:37:25.727" OwnerUserId="49093" ParentId="105115" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;I have a set of percent data on histological abnormalities in fish gills and I need to compare the results between two sites A and B. We analyzed abnormalities in lamellae of a particular gill arch. For example for fish 1 we counted 500 lamellae and noted the number that had abnormality Y. The majority of the data is 0%. That is, in most fish, the lamellae counted had no such abnormality present. Obviously when I try different transformations the distribution does not change because of all the 0s (I have attached a graph of the distribution of the data). I do not know how to analyze this data. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/k5oyb.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-10-05T00:36:30.060" Id="117902" LastActivityDate="2014-10-05T07:48:21.507" LastEditDate="2014-10-05T07:48:21.507" LastEditorUserId="22047" OwnerUserId="56977" PostTypeId="1" Score="2" Tags="&lt;percentage&gt;" Title="How to analyze percent data when most of the data is 0%?" ViewCount="37" />
  <row AnswerCount="0" Body="&lt;p&gt;My objective is to classify sentences into useful (denote in boolean as 1) and not useful (denote in boolean as 0) categories.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have about 525 features  where 300 features are the most frequent and important keywords after removing stopwords and the rest are domain names. &lt;/p&gt;&#10;&#10;&lt;p&gt;The total number of documents I have is 793.&lt;/p&gt;&#10;&#10;&lt;p&gt;I manually labelled the classes as useful and not useful and I have about 93 useful and the rest (700) as not useful.&lt;/p&gt;&#10;&#10;&lt;p&gt;Below is the result of my logistic regression with the parameter values:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;model = LogisticRegression(penalty='l1')&#10;X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)&#10;model = model.fit(X, y)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/gS868.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Based on the results i got, it seems that the regression is doing a very bad job at classifying sentences as useful with really bad precision, recall and 1-score.&lt;/p&gt;&#10;&#10;&lt;p&gt;How can i improve the accuracy? &lt;/p&gt;&#10;&#10;&lt;p&gt;Note, as shown above, i already am implementing L1 regularization.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-10-05T00:36:37.187" Id="117903" LastActivityDate="2014-10-05T00:36:37.187" OwnerUserId="32470" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;logistic&gt;&lt;python&gt;&lt;scikit-learn&gt;&lt;accuracy&gt;" Title="How can I improve my sklearn logistic regression model" ViewCount="44" />
  
  <row Body="&lt;p&gt;If you are analyzing meditation effect, as per Baron &amp;amp; Kenny...all the relationship between X-&gt;Z ( Direct), Y-&gt;Z and X-&gt;Y should be positively related. In your case..If -.85 is direct relation...I can't accept X-&gt;Z relation for further mediation analysis.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-10-05T05:26:41.793" Id="117915" LastActivityDate="2014-10-05T05:26:41.793" OwnerUserId="56982" ParentId="15384" PostTypeId="2" Score="-4" />
  <row AnswerCount="0" Body="&lt;p&gt;I have process which has different states. It looks something like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/fyphv.png&quot; alt=&quot;Example&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In some cases the required tools for the assembly need to be fetched (same goes for the supplies for packaging). Typical state sequences may look like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;CASE #1:               CASE#2:               [...]&#10;--------               -------&#10;start                  start&#10;prepare tools          prepare tools&#10;fetch tools            assemble&#10;prepare tools          package&#10;assemble               ship&#10;package                finish&#10;get supplies&#10;package&#10;ship&#10;finish&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Within these states there are certain activities, like &lt;code&gt;mount front wheel&lt;/code&gt; or &lt;code&gt;place company logo&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have annoted recordings of such processes, which include the different states and the different activities including timestamps.&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to create a model which will predict the remaining time of a current process. With each phase (or better each activity) I would like to give a new estimate on how long the phase will last and how much more time the process needs in total.&lt;/p&gt;&#10;&#10;&lt;p&gt;Later I want to monitor different such processes which might also have varying structures (some states may be added or removed) and different activities.&lt;/p&gt;&#10;&#10;&lt;p&gt;I recognize that this type of problem might be represented as a &lt;em&gt;markov process&lt;/em&gt;. Would it make sense to formulate this problem similar to a &lt;em&gt;reinforcement learning problem&lt;/em&gt; using &lt;em&gt;temporal difference learning&lt;/em&gt;?&lt;/p&gt;&#10;&#10;&lt;p&gt;How would you approach this problem? I have a hard time finding literature on this &lt;em&gt;Remaining Process Time&lt;/em&gt; problem, what keywords would you search for?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-05T07:06:06.910" Id="117917" LastActivityDate="2014-10-05T07:06:06.910" OwnerUserId="56966" PostTypeId="1" Score="0" Tags="&lt;modeling&gt;&lt;markov-process&gt;&lt;reinforcement-learning&gt;" Title="Modelling remaing time of a process" ViewCount="23" />
  <row AnswerCount="1" Body="&lt;p&gt;I am trying to understand what the residuals from a regression convey about the model's adequacy/ability to explain the variance in the data.&#10;I read that if we are able to take the residuals from a regression model (say Model 1) and build another model (Model 2), by regressing a new set of predictors on it , then unless the R-square of Model 2 is zero, we conclude that Model 1 does not explain the variation in the original data adequately.&lt;/p&gt;&#10;&#10;&lt;p&gt;Could someone explain the concept of building a new model M2 with the residuals from a previous model as the response variable?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-10-05T08:37:29.237" Id="117920" LastActivityDate="2014-10-05T09:35:08.773" OwnerUserId="56986" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;residuals&gt;&lt;goodness-of-fit&gt;&lt;model&gt;" Title="Assessing residuals from a regression model" ViewCount="43" />
  
  <row Body="&lt;p&gt;This seems trivial. If you have age and first name for the total population, you have age distribution for each first name $p(Age=A|Name=N)$ .  Then given the list of names for some subpopulation S, you have&lt;/p&gt;&#10;&#10;&lt;p&gt;$p_S(Age=A) = \sum_N p_S(Name=N) p(Age=A|Name=N)$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $p_S(Age=A)$ is the age distribution of subpopulation $S$, and $p_S(Name=N)$ is the name distribution of this subpopulation.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-05T11:48:46.340" Id="117931" LastActivityDate="2014-10-05T11:48:46.340" OwnerUserId="31264" ParentId="117929" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;I dont think this kind of graph has a name,  but what you are doing is reasonable, and your interpretation, I think, valid.  I think what you are doing is related to Hampel's Influence function, see &lt;a href=&quot;https://en.wikipedia.org/wiki/Robust_statistics#Empirical_influence_function&quot; rel=&quot;nofollow&quot;&gt;https://en.wikipedia.org/wiki/Robust_statistics#Empirical_influence_function&lt;/a&gt;    especially the section about the empirical influence function.  And your plot could certainly be related to some measure of skewness of the data, since, if your data were perfectly symmetrical, the plot would be flat.  You should investigate that!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-05T17:26:26.307" Id="117955" LastActivityDate="2014-10-05T17:26:26.307" OwnerUserId="11887" ParentId="117950" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Model selection is not a simple process unless you are only dealing with a small number of variables.  The significance of some variables may depend on others.  If you have the time you could try every possible model and pick the 'best' one.  There are many criteria for selecting the best model.  Refer to any experimental design text book.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-05T22:45:17.733" Id="117977" LastActivityDate="2014-10-05T22:45:17.733" OwnerUserId="27875" ParentId="117964" PostTypeId="2" Score="0" />
  
  
  
  <row Body="&lt;p&gt;Memorylessness is a property of the following form:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\Pr(X&amp;gt;m+n \mid X &amp;gt; m)=\Pr(X&amp;gt;n)\ .$$&lt;/p&gt;&#10;&#10;&lt;p&gt;This property holds for $X_1=\ \text{time to the next event in a Poisson process}\ $, but it doesn't hold for $X_k=\ \text{time to the}\, k^\text{th}\, \text{event in a Poisson process}\ $ when $k&amp;gt;1$.&lt;/p&gt;&#10;&#10;&lt;p&gt;As for how to show it, you could try to do it from first principles.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you can show that the essentially equivalent form $P(X&amp;gt;s+t)\neq P(X&amp;gt;s)P(X&amp;gt;t)$, (for $s, t&amp;gt;0\ $), that would be sufficient; you already know the distribution for $X_k$.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-10-06T06:45:30.100" Id="118001" LastActivityDate="2014-10-06T06:50:42.680" LastEditDate="2014-10-06T06:50:42.680" LastEditorUserId="805" OwnerUserId="805" ParentId="118000" PostTypeId="2" Score="2" />
  <row AnswerCount="0" Body="&lt;p&gt;I would like to know some general parameters that can be used to describe how &quot;dirty&quot; the data is.&lt;/p&gt;&#10;&#10;&lt;p&gt;Issues I am having are the following:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Lots of missing values;&lt;/li&gt;&#10;&lt;li&gt;The values are some predictors are filled in but often completely wrong;&lt;/li&gt;&#10;&lt;li&gt;I can try to extract some other variables out of a huge text string, but by doing so I create other doubts of the observations without this long text string.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Is there a general approach to formulate how dirty the data is? &lt;/p&gt;&#10;&#10;&lt;p&gt;I was thinking about a kind of Sankey diagram that visualises what fraction of the data can be used when taking all the predictors into account?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-10-06T07:55:39.557" Id="118005" LastActivityDate="2014-10-06T13:34:15.687" LastEditDate="2014-10-06T13:34:15.687" LastEditorUserId="22047" OwnerUserId="36462" PostTypeId="1" Score="2" Tags="&lt;data-cleaning&gt;" Title="Is there a way to express how &quot;dirty&quot; a data set is?" ViewCount="45" />
  <row AnswerCount="0" Body="&lt;p&gt;The lasso estimator is define as $argmin_{\beta}~ MSE +\lambda \parallel \beta\parallel_1$ I am wondering if there is an alternative penalty function that is $C^\infty$ and that preserves the sparsity of the lasso estimator. &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-10-06T09:17:43.690" Id="118012" LastActivityDate="2014-10-06T09:17:43.690" OwnerUserId="25392" PostTypeId="1" Score="0" Tags="&lt;lasso&gt;&lt;regularization&gt;&lt;penalized&gt;" Title="Lasso equivalent estimator with differenziable penalty function" ViewCount="30" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have 7 non nested models: I use the Akaike method to see which one best describes the data.&lt;/p&gt;&#10;&#10;&lt;p&gt;If the models were nested I would use the likelihood ratio test: this woluld also automatically give me information on the confidence level: how can I do the same in my case?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-06T11:06:17.023" Id="118018" LastActivityDate="2014-10-06T15:54:12.967" LastEditDate="2014-10-06T12:29:07.703" LastEditorUserId="56583" OwnerUserId="56583" PostTypeId="1" Score="0" Tags="&lt;maximum-likelihood&gt;&lt;confidence&gt;" Title="confidence level in non nested models" ViewCount="18" />
  <row AcceptedAnswerId="118065" AnswerCount="1" Body="&lt;p&gt;this might be obvious one but I have spent much time without gaining anything.&lt;/p&gt;&#10;&#10;&lt;p&gt;If $\underline{X}$~$N_p(\underline{\mu},\sigma^2 I)$, where $\mu$ is known to lie on the unit sphere ($\mu^T\mu$), show that the mle of $\mu$ is $\frac{x}{(x^Tx)^{\frac{1}{2}}}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;I tried to use &quot;trace trick&quot; of the multi normal mle but could not find it properly..It would be very appreciated if you could give me any hints on this problem. Thank you.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-06T12:56:33.477" Id="118025" LastActivityDate="2014-10-06T18:10:27.830" OwnerUserId="56688" PostTypeId="1" Score="1" Tags="&lt;self-study&gt;&lt;normal-distribution&gt;&lt;maximum-likelihood&gt;" Title="Constrained MLE of multivariate normal" ViewCount="71" />
  <row AnswerCount="1" Body="&lt;p&gt;Can we find the lowest attainable bound for the 100 m sprint times, i.e. the quickest it can be run ever, using the past data? &lt;/p&gt;&#10;&#10;&lt;p&gt;So every now and again the record gets broken and we can map the new record time. But surely there must be a particular time which can not be broken? &#10;For example, we know that no human will ever run 100 m in 5 seconds.&#10;So what would be the lower bound for these record times? &lt;/p&gt;&#10;&#10;&lt;p&gt;I feel that perhaps times taken to run the 100 m (by professional runners) follow a log normal distribution but I would think that the mean also decreases every now and again and perhaps the variance as well. &lt;/p&gt;&#10;&#10;&lt;p&gt;So perhaps a way to go about solving for this would be to find a value for the &quot;mean&quot; whereby the semi-variance (variance below this &quot;mean&quot;) is zero. &lt;/p&gt;&#10;&#10;&lt;p&gt;Just a random question out of curiosity: any input or help would be appreciated. &lt;/p&gt;&#10;" CommentCount="8" CreationDate="2014-10-06T13:10:29.117" FavoriteCount="0" Id="118026" LastActivityDate="2014-11-11T02:30:49.900" LastEditDate="2014-10-06T13:41:30.130" LastEditorUserId="22047" OwnerUserId="57037" PostTypeId="1" Score="4" Tags="&lt;stochastic-approximation&gt;" Title="100 m sprint world record times - lower bound" ViewCount="102" />
  
  <row Body="&lt;p&gt;The Bayesian approach proposes using our previous expereince or knoweldge, if available. For example, If I ask you what is the probability of having a head in a coin flip, you will probably say 50%. However, if you actually flip the coin 3 times, you might get 1/3, 2/3 or even 3/3 heads. &lt;/p&gt;&#10;&#10;&lt;p&gt;From a frequentists point of view (opposed to Bayesians), the probabilities would be (1/3 ~= 33%, 66%, 100%). The Baysians, on the other hand,  say that we already previously know that the probability of having a head in a coin flip is 50% (our prior knoweldge). Let's use it! Now, the probabilities can be corrected by this prior factor so that, the probability of having 1/3 heads = (1/3)*(1/2) = 16% (and not 33% as we got before).&lt;/p&gt;&#10;&#10;&lt;p&gt;From a first glance, the Baysian approach seems very attractive as it allows us to make use of our prior knowedlge. However, if we are not able to quatify our prior knoweldge with good estimates, then, we might deviate from the actual description of our surrounding environment.&lt;/p&gt;&#10;&#10;&lt;p&gt;A small comment for the Gaussain distribution choice, is that it simplifies learning the parameters of the model. In particualr, it can be shown that if the distribution of the prior knowedlge is Gaussian, then the posterior distribution is also a Gaussain (&lt;a href=&quot;http://en.wikipedia.org/wiki/Conjugate_prior&quot; rel=&quot;nofollow&quot;&gt;conjugate prior&lt;/a&gt;). Needless to say, this might not be the best justification to use a Gaussian model as the assumptions of the Gaussian distribution might not hold for a particular dataset, yet, it performs well in many cases.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-06T14:09:24.860" Id="118031" LastActivityDate="2014-10-06T14:09:24.860" OwnerUserId="14888" ParentId="117958" PostTypeId="2" Score="1" />
  <row AnswerCount="2" Body="&lt;p&gt;I would like to build a logistic regression model in which I will be looking for predictor variables having a significant effect on the breeding success of a raptor bird. &lt;/p&gt;&#10;&#10;&lt;p&gt;The predictors in the dataset are highly correlated, which led me to consider logistic ridge regression. Furthermore, I investigated different breeding grounds in which one or multiple birds have been breeding. Since this makes the data clustered, I would need to add the breeding ground as a random effect in the model.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thus, I would need a 'mixed logistic ridge regression' approach if I am getting things right here. This paper suggests this approach too for another problem:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/22049265&quot; rel=&quot;nofollow&quot;&gt;http://www.ncbi.nlm.nih.gov/pubmed/22049265&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Are there any people aware of the availability of an R package or something related having implemented a mixed logistic ridge regression approach as the paper and myself just described? I did not succeed in finding one.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-10-03T19:48:54.737" Id="118034" LastActivityDate="2014-10-30T18:33:52.593" OwnerDisplayName="KoenVdB" OwnerUserId="42808" PostTypeId="1" Score="1" Tags="&lt;r&gt;" Title="Mixed logistic ridge regression" ViewCount="134" />
  <row Body="&lt;p&gt;This might do what you want:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://cran.r-project.org/web/packages/ridge/index.html&quot; rel=&quot;nofollow&quot;&gt;http://cran.r-project.org/web/packages/ridge/index.html&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;From the description:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Linear and logistic ridge regression for small data sets and genome-wide SNP data&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="1" CreationDate="2014-10-03T20:48:04.567" Id="118035" LastActivityDate="2014-10-03T20:48:04.567" OwnerDisplayName="rseubert" OwnerUserId="48412" ParentId="118034" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;There are plenty of websites dedicated to choosing color palettes. I don't know that there is a particular set of colors that is objectively the best, you will have to choose based on your audience and the tone of your work.&lt;/p&gt;&#10;&#10;&lt;p&gt;Check out &lt;a href=&quot;http://www.colourlovers.com/palettes&quot; rel=&quot;nofollow&quot;&gt;http://www.colourlovers.com/palettes&lt;/a&gt; or &lt;a href=&quot;http://design-seeds.com/index.php/search&quot; rel=&quot;nofollow&quot;&gt;http://design-seeds.com/index.php/search&lt;/a&gt; to get started. Some of them have colors that are two close to show different groups, but others will give you complementary colors across a wider range.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can also check out the &lt;a href=&quot;http://matplotlib.org/examples/color/colormaps_reference.html&quot; rel=&quot;nofollow&quot;&gt;non-default predefined colorsets in Matplotlib&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-06T14:54:37.473" Id="118040" LastActivityDate="2014-10-08T14:22:18.107" LastEditDate="2014-10-08T14:22:18.107" LastEditorUserId="930" OwnerUserId="46603" ParentId="118033" PostTypeId="2" Score="11" />
  <row Body="&lt;p&gt;Exponential distributions involve raising numbers to a certain power whereas geometric distributions are more general in nature and involve performing various operations on numbers such as multiplying a certain number by two continuously. Exponential distributions are more specific types of geometric distributions. &lt;/p&gt;&#10;&#10;&lt;p&gt;Exponential distributions: 2, 4, 16, 256 or 3, 9, 81, 6561.&lt;/p&gt;&#10;&#10;&lt;p&gt;Geometric distribution: 2, 4, 8, 16, 32, 64.&lt;/p&gt;&#10;&#10;&lt;p&gt;Just my two cents anyway.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-10-06T15:03:41.867" Id="118041" LastActivityDate="2014-10-06T15:03:41.867" OwnerUserId="57040" ParentId="17345" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;There's actually been a good deal of research on this in recent years.&lt;/p&gt;&#10;&#10;&lt;p&gt;A big point is &quot;semantic resonance.&quot; This basically means &quot;colors that correspond to what they represent,&quot; e.g. a time series for money should be colored green, at least for an audience in the USA. This apparently improves comprehension. One very interesting paper on the subject is by Lin, et al (2013): &lt;a href=&quot;http://vis.stanford.edu/papers/semantically-resonant-colors&quot;&gt;http://vis.stanford.edu/papers/semantically-resonant-colors&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;There's also the very nice iWantHue color generator, at &lt;a href=&quot;http://tools.medialab.sciences-po.fr/iwanthue/&quot;&gt;http://tools.medialab.sciences-po.fr/iwanthue/&lt;/a&gt;, with lots of info in the other tabs.&lt;/p&gt;&#10;&#10;&lt;h3&gt;References&lt;/h3&gt;&#10;&#10;&lt;p&gt;Lin, Sharon, Julie Fortuna, Chinmay Kulkarni, Maureen Stone, and Jeffrey Heer. (2013). Selecting Semantically-Resonant Colors for Data Visualization. Computer Graphics Forum (Proc. EuroVis), 2013&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-10-06T15:34:30.810" Id="118043" LastActivityDate="2014-10-06T21:44:43.730" LastEditDate="2014-10-06T21:44:43.730" LastEditorUserId="36229" OwnerUserId="36229" ParentId="118033" PostTypeId="2" Score="20" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have approx 500 variable (each variable containing 80 data points). I need to find how each of the variable is related to other 499 variables - linear, log, exponential and so on. Is there a statistical test which could provide me with this information? I don't want to go through manual process of having trendline and finding the R-squared value. &lt;/p&gt;&#10;&#10;&lt;p&gt;Help much appreciated :) &lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you!&#10;Vinay&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-06T16:04:47.733" Id="118049" LastActivityDate="2014-10-06T16:28:06.640" LastEditDate="2014-10-06T16:26:18.203" LastEditorUserId="42175" OwnerUserId="42175" PostTypeId="1" Score="0" Tags="&lt;correlation&gt;" Title="Relation between two variables" ViewCount="33" />
  
  
  
  <row AcceptedAnswerId="118192" AnswerCount="2" Body="&lt;p&gt;I've found that performing scaling in SVM problems really improves the performance of SVM ... But I don't understand why! &lt;/p&gt;&#10;&#10;&lt;p&gt;I have read this explanation: &quot;The main advantage of scaling is to avoid attributes in greater numeric ranges dominating those in smaller numeric ranges.&quot; &lt;/p&gt;&#10;&#10;&lt;p&gt;Unfortunately this didn't help me... Can somebody provide me a better explanation? &lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you in advance!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-06T23:13:40.443" Id="118092" LastActivityDate="2014-10-07T23:00:18.047" LastEditDate="2014-10-07T16:49:22.337" LastEditorUserId="805" OwnerUserId="57067" PostTypeId="1" Score="2" Tags="&lt;machine-learning&gt;&lt;classification&gt;&lt;svm&gt;&lt;scales&gt;" Title="Why does the scaling of feature vectors improve performance of SVM classifier?" ViewCount="105" />
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm curious if there's a standard way to compute a sensitivity/specificity score if I collect over different strata. &lt;/p&gt;&#10;&#10;&lt;p&gt;Obviously, I can calculate it per stratum and then weight it using the proportion of observations in each stratum, but I'm wondering if there is a more formal approach -- similar to the idea of a Cochran–Mantel–Haenszel test for odds ratios. &lt;/p&gt;&#10;&#10;&lt;p&gt;I did some googling but wasn't able to find something on target.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-06T23:48:51.870" Id="118099" LastActivityDate="2014-10-06T23:54:08.477" LastEditDate="2014-10-06T23:54:08.477" LastEditorUserId="22047" OwnerUserId="14104" PostTypeId="1" Score="0" Tags="&lt;self-study&gt;&lt;sensitivity&gt;&lt;specificity&gt;" Title="sensitivity and specificity over different strata" ViewCount="17" />
  
  <row AcceptedAnswerId="118116" AnswerCount="1" Body="&lt;p&gt;I am applying to a PhD program in biostatistics, and my undergrad was in B.S. applied math and limited pure math background; only topology and some analysis.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question, though, is how much pure math is required to be good at statistical research and genetics?  For my PhD, I think it may be silly to pursue genetics AND statistics specialization AND pure math courses; I think that is too ambitious.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am seeking general advice. My interests are in statistical genetics, mathematical modeling of cancer informatics, and data analytics.&lt;/p&gt;&#10;" CommentCount="4" CommunityOwnedDate="2014-10-07T15:47:26.487" CreationDate="2014-10-07T02:18:37.897" Id="118112" LastActivityDate="2014-10-07T03:23:12.507" LastEditDate="2014-10-07T03:23:12.507" LastEditorUserId="7290" OwnerUserId="51890" PostTypeId="1" Score="2" Tags="&lt;mathematical-statistics&gt;&lt;careers&gt;" Title="PhD in statistics, but unsure of how much pure math is needed as requirements" ViewCount="149" />
  
  <row Body="&lt;p&gt;You don't assume that there is any joint probability distribution for your training data (that would be trivially true, as you said) but you assume that both the training data $x_i, y_i$ as well as unseen test data, that you later want to apply your algorithm on, are drawn from the same distribution $p(x,y)$. If you would drop this assumption you could still minimize the objective function $1/N \sum_i L(f(x_i, w), y_i)$ but you would have no guarantees that the estimated parameters $w$ perform any good on test data drawn from a different distribution than your training data.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you read into computational learning theory you will find that you can (loosely) bound the test error based on some properties of your learning algorithm and the amount of training data, but these bounds only hold as long as the distribution for test and training data is the same. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-07T06:19:34.850" Id="118126" LastActivityDate="2014-10-07T06:19:34.850" OwnerUserId="35427" ParentId="118079" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;From the estimated Quantile we can compute probabilities&#10;&lt;img src=&quot;http://i.stack.imgur.com/CjiLf.png&quot; alt=&quot;Check the figure &quot;&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-10-07T09:19:08.093" Id="118140" LastActivityDate="2014-10-07T09:19:08.093" OwnerUserId="57057" ParentId="116928" PostTypeId="2" Score="-1" />
  
  <row Body="&lt;p&gt;If two classes come from the same distribution, then yes, separate PCAs should yield similar results. But the opposite is not true! PCA analyzes covariance structure of the data, which means that &lt;strong&gt;it ignores the mean&lt;/strong&gt;. So the two classes can be 100% linearly separated (leading to 100% classification accuracy with a linear method), but still have identical within-class covariances, e.g.:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/lmnFM.png&quot; alt=&quot;two classes&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Therefore your approach does not seem to make a lot of sense.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-10-07T10:08:33.577" Id="118148" LastActivityDate="2014-10-07T10:08:33.577" OwnerUserId="28666" ParentId="118105" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;It's &lt;a href=&quot;http://en.wikipedia.org/wiki/Cumulative_distribution_function&quot;&gt;quite common&lt;/a&gt; to drop the &quot;cumulative&quot;. There should be no danger of confusion with the probability &lt;em&gt;density&lt;/em&gt; function.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-10-07T10:43:20.990" Id="118154" LastActivityDate="2014-10-07T10:43:20.990" OwnerUserId="17230" ParentId="118153" PostTypeId="2" Score="6" />
  <row AnswerCount="1" Body="&lt;p&gt;I'm trying to implement anomaly detection based on clustering. I'm hopping for confirmation of my approach, and I'm exposing my idea, being aware that I could have miss something in my analysis, so any suggestions would also be very appreciated. I'm a beginner in this area, so any help would be great. &lt;/p&gt;&#10;&#10;&lt;p&gt;I must also add that the data is in the time series form, and that there is a number of parameters considered.&lt;/p&gt;&#10;&#10;&lt;p&gt;My approach is next:&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's say that there are 3 parameters, a, b and c. Since parameters are in time series form, I'm using sliding window, lets assume that size of the window is 4.&lt;/p&gt;&#10;&#10;&lt;p&gt;My representation of point is&lt;/p&gt;&#10;&#10;&lt;p&gt;[a1, a2, a3, a4, b1, b2, b3, b4, c1, c2, c3, c4]&lt;/p&gt;&#10;&#10;&lt;p&gt;[a2, a3, a4, a5, b2, b3, b4, b5, c2, c3, c4, c5]&lt;/p&gt;&#10;&#10;&lt;p&gt;And so on.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then I'm using clustering (K means clustering, where I use some kind of automatized elbow method to determine the number of clusters), which gives me centroids that represent what normal signals (normal values of parameters in a window) look like.&lt;/p&gt;&#10;&#10;&lt;p&gt;Based on that centroids and the distance of points to its nearest centroid (concretely certain number of points farthest from centroid) when a new measurement arrives, I assign it to nearest centroid and its distance to centroid, I determine weather a signal is anomalous.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is my approach sound, if not, what should I consider to make it work.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have tried it on data that I have synthesized and got &quot;good&quot; results, by testing on injected anomalies. But I'm still not sure in this approach, because of my lack of experience.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any comment would be great.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-07T13:04:32.977" FavoriteCount="1" Id="118168" LastActivityDate="2014-10-08T19:03:23.003" OwnerUserId="54659" PostTypeId="1" Score="1" Tags="&lt;clustering&gt;&lt;outliers&gt;&lt;novelty-detection&gt;" Title="Clustering based anomaly detection" ViewCount="73" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I know what intuitively what's going on but...&lt;/p&gt;&#10;&#10;&lt;p&gt;We know that in general the standard error of the mean is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \sigma_\bar{X}^2 = \sigma_X^2/n $$&lt;/p&gt;&#10;&#10;&lt;p&gt;Right?&lt;/p&gt;&#10;&#10;&lt;p&gt;But when I apply this to the binomial distributed random variable I seem to get an (algebraic) contradiction.&lt;/p&gt;&#10;&#10;&lt;p&gt;$$E[\bar{X}] = n*p/n = p, \text{  and  } \sigma_\bar{X}^2 = np(1-p)/n = p(1-p)  $$&lt;/p&gt;&#10;&#10;&lt;p&gt;This is clearly a contradiction since we know variance of the mean estimates goes down as n gets bigger.&lt;/p&gt;&#10;" ClosedDate="2014-10-07T15:24:15.867" CommentCount="3" CreationDate="2014-10-07T15:18:24.057" Id="118186" LastActivityDate="2014-10-07T15:18:24.057" OwnerUserId="38379" PostTypeId="1" Score="0" Tags="&lt;binomial&gt;&lt;error&gt;&lt;standard&gt;" Title="Standard error of the mean for binomial dist" ViewCount="17" />
  <row AnswerCount="0" Body="&lt;p&gt;I have asked this question but did not get a satisfactory answer.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;clc&#10;clear&#10;&#10;frequency =1000;&#10;t= linspace(-1000,0.1,1000);&#10;&#10;&#10;sine_wave = sin(2*pi*frequency*t);&#10;cos_wave  = cos(2*pi*frequency*t);&#10;&#10;[xc,lags] = xcorr(cos_wave,sine_wave);&#10;[~,I] = max(abs(xc));&#10;&#10;figure&#10;stem(lags,xc,'filled')&#10;legend(sprintf('Maximum at lag %d',lags(I)))&#10;title('Sample Cross-Correlation Sequence')&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I am getting a maximum at lag 22 (if you copy and paste the code provided you will see it in the plot) and I understand that phase lag = 2*pi&lt;em&gt;f&lt;/em&gt;(time delay) so in my case:&lt;/p&gt;&#10;&#10;&lt;p&gt;phase lag = 2*pi*1000*22 which is by no means close to pi/2... What am I missing here? &lt;/p&gt;&#10;&#10;&lt;p&gt;PS:I have never used this before, so if could tell me what I am doing wrong and if my expectations are even possible?&lt;/p&gt;&#10;&#10;&lt;p&gt;Reference to earlier question here:&#10;&lt;a href=&quot;http://dsp.stackexchange.com/questions/18535/validating-cross-corrolation-between-sine-and-cos-shouldnt-pahse-lag-be-pi-2/18536#18536&quot;&gt;http://dsp.stackexchange.com/questions/18535/validating-cross-corrolation-between-sine-and-cos-shouldnt-pahse-lag-be-pi-2/18536#18536&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="15" CreationDate="2014-10-07T17:11:13.300" Id="118196" LastActivityDate="2014-10-07T17:20:46.613" LastEditDate="2014-10-07T17:20:46.613" LastEditorUserId="57118" OwnerUserId="57118" PostTypeId="1" Score="0" Tags="&lt;cross-correlation&gt;" Title="Validating cross correlation between sine and cos, shouldn't phase lag be pi/2?" ViewCount="54" />
  <row AnswerCount="1" Body="&lt;p&gt;If i only have mean, std dev. and number of observations is it then possible to generate af 95% confidence interval or a Chi squared test in JMP using only these summaries as input?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-10-07T19:53:59.260" Id="118209" LastActivityDate="2014-10-07T20:12:02.987" OwnerUserId="57128" PostTypeId="1" Score="0" Tags="&lt;confidence-interval&gt;&lt;chi-squared&gt;&lt;sas&gt;&lt;jmp&gt;" Title="Confidence interval from summaries in JMP" ViewCount="28" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I need to test if a vector of observed values are uniform distribution. &lt;/p&gt;&#10;&#10;&lt;p&gt;Lets assume:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;This values are not a sample, but my entire universe.&lt;/li&gt;&#10;&lt;li&gt;I have a dataset of 12000 observations, where most of the data (80%) have the same value.&lt;/li&gt;&#10;&lt;li&gt;The range is not delimited.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I'm using this R code to test the uniformity with a significance level of 0.05&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Pearson's Chi-squared Test for Count Data&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    chi2IsUniform=(chisq.test(dataset)$p.value &amp;gt; 0.05)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;These results are reliable?&lt;/p&gt;&#10;&#10;&lt;p&gt;Do I need to use Monte Carlo to compute p-values?&lt;/p&gt;&#10;&#10;&lt;p&gt;If yes, we have 2 extra options on this function:&lt;/p&gt;&#10;&#10;&lt;p&gt;*simulate.p.value: a logical indicating whether to compute p-values by Monte Carlo     simulation.&#10;B: an integer specifying the number of replicates used in the Monte Carlo test.&#10;*&lt;/p&gt;&#10;&#10;&lt;p&gt;How many replicates do I have to use? It depends on my significance level?&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    chi2IsUniform=(chisq.test(dataset, simulate.p.value = TRUE, B = 1000)$p.value &amp;gt; 0.05)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="11" CreationDate="2014-10-07T21:54:24.333" Id="118226" LastActivityDate="2014-10-08T00:42:14.360" LastEditDate="2014-10-08T00:42:14.360" LastEditorUserId="57135" OwnerUserId="57135" PostTypeId="1" Score="0" Tags="&lt;chi-squared&gt;&lt;p-value&gt;&lt;uniform&gt;&lt;pearson&gt;" Title="Testing uniformity of data" ViewCount="92" />
  <row Body="&lt;p&gt;None of those proposed methods have been shown by simulation studies to work.  Spend your efforts formulating a complete model and then fit it.  Univariate screening is a terrible approach to model formulation, and the other components of stepwise variable selection you hope to use should likewise be avoided.  This has been discussed at length on this site.  What gave you the idea in the first place that variables should sometimes be removed from models because they are not &quot;significant&quot;?  Don't use $P$-values or changes in $\beta$ to guide any of the model specification.&lt;/p&gt;&#10;" CommentCount="14" CreationDate="2014-10-07T21:54:37.747" Id="118227" LastActivityDate="2014-10-07T21:54:37.747" OwnerUserId="4253" ParentId="118215" PostTypeId="2" Score="12" />
  
  <row Body="&lt;ol&gt;&#10;&lt;li&gt;The parameter here is $\sigma^2$, not $\sigma$ (though you could actually do it in terms of $\sigma$ throughout instead, and get to the equivalent final estimator). To reduce your confusion, consider $\tau=\sigma^2$. &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;$$\frac{d}{d\tau} \tau^{-1} = -\tau^{-2}$$&lt;/p&gt;&#10;&#10;&lt;ol start=&quot;2&quot;&gt;&#10;&lt;li&gt;The method is &lt;em&gt;maximum-likelihood&lt;/em&gt;. The idea is to find the parameter values that maximize the likelihood function. Under particular circumstances, derivative calculus can be used to find local turning points in the likelihood or more often, the log-likelihood, and to confirm that those turning points are local maxima. &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;$\hspace 1.3cm$&lt;img src=&quot;http://i.stack.imgur.com/0dxxO.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Sometimes you can produce an argument as to why the local maximum will also be a global maximum (which is what you actually want).&lt;/p&gt;&#10;&#10;&lt;p&gt;But there are plenty of situations where calculus doesn't solve that problem; don't confuse the tool with the problem it's being applied to. Sometimes you need to find the maximum in other ways.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-10-08T00:04:29.707" Id="118239" LastActivityDate="2014-10-08T11:14:35.143" LastEditDate="2014-10-08T11:14:35.143" LastEditorUserId="805" OwnerUserId="805" ParentId="118235" PostTypeId="2" Score="2" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a posterior I'd like to sample:&lt;/p&gt;&#10;&#10;&lt;p&gt;$p(\theta\mid Y)\propto L(Y\mid \theta) p(\theta)$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $p(\theta)$ is proper, so the posterior is proper. I can write $L(Y\mid\theta) = \int f(Y, Z\mid \theta) dZ$. MCMC on the target&lt;/p&gt;&#10;&#10;&lt;p&gt;$p(\theta, Z\mid Y)\propto f(Y, Z\mid \theta) p(\theta)$&lt;/p&gt;&#10;&#10;&lt;p&gt;is much easier than working on the margin. This is all very standard, of course. I have two questions: Is the joint posterior guaranteed to be proper? Is it as simple as invoking the proper $p(\theta)$ to conclude&lt;/p&gt;&#10;&#10;&lt;p&gt;$p(Y) = \int \left(\int f(Y, Z\mid \theta) p(\theta) dZ \right) d\theta = \int L(Y\mid \theta) p(\theta) d\theta &amp;lt;\infty$?&lt;/p&gt;&#10;&#10;&lt;p&gt;If not, does the marginal subchain for $\theta$ have the right stationary distribution? (We get this sort of guarantee for parameter expanded Gibbs samplers, for example, but that's a very special case).&lt;/p&gt;&#10;&#10;&lt;p&gt;In my particular case I can also derive a series of proper &quot;working models&quot; with proper &quot;priors&quot; on $Z$:&lt;/p&gt;&#10;&#10;&lt;p&gt;$f_t(Y, Z\mid \theta) p(\theta) p_t(Z)$&lt;/p&gt;&#10;&#10;&lt;p&gt;Here $\int f_t(Y, Z\mid \theta) p_t(Z)dZ \neq L(Y\mid \theta)$ for any $t$, but $\int f_t(Y, Z\mid \theta) p_t(Z)dZ \rightarrow L(Y\mid \theta)$ as $t\rightarrow 0$. If I had $\int f_t(Y, Z\mid \theta) p_t(Z)dZ = L(Y\mid \theta)$ this would suffice to show the marginal subchain has the right target (theorems to this effect are in the PX-DA papers as reviewed in &lt;a href=&quot;http://www.jstor.org/stable/1391021&quot; rel=&quot;nofollow&quot;&gt;http://www.jstor.org/stable/1391021&lt;/a&gt;).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-08T02:31:48.430" Id="118252" LastActivityDate="2014-10-08T02:31:48.430" OwnerUserId="57149" PostTypeId="1" Score="1" Tags="&lt;mcmc&gt;&lt;posterior&gt;" Title="Verifying propriety of MCMC" ViewCount="25" />
  <row Body="&lt;p&gt;Paul Tol provides a colour scheme optimised for colour differences (i.e., categorical or qualitative data) and colour-blind vision on &lt;a href=&quot;http://www.sron.nl/~pault/&quot;&gt;his website&lt;/a&gt;, and in detail in a &quot;technote&quot; (PDF file) linked to there. He states:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;To make graphics with your scientific results as clear as possible, it&#10;  is handy to have a palette of colours that are:&lt;/p&gt;&#10;  &#10;  &lt;ul&gt;&#10;  &lt;li&gt;distinct for all people, including colour-blind readers;&lt;/li&gt;&#10;  &lt;li&gt;distinct from black and white;&lt;/li&gt;&#10;  &lt;li&gt;distinct on screen and paper; and&lt;/li&gt;&#10;  &lt;li&gt;still match well together.&lt;/li&gt;&#10;  &lt;/ul&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I took the colour scheme from his &quot;Palette 1&quot; of the 9 most distinct colours, and placed it in my &lt;code&gt;matplotlibrc&lt;/code&gt; file under &lt;code&gt;axes.color_cycle&lt;/code&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;axes.color_cycle    : 332288, 88CCEE, 44AA99, 117733, 999933, DDCC77, CC6677, 882255, AA4499&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Then, borrowing from Joe Kington's &lt;a href=&quot;http://stackoverflow.com/a/9398214/857416&quot; title=&quot;answer&quot;&gt;answer&lt;/a&gt; the default lines as plotted by:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;import matplotlib.pyplot as plt&#10;import matplotlib as mpl&#10;import numpy as np&#10;&#10;x = np.linspace(0, 20, 100)&#10;&#10;fig, axes = plt.subplots(nrows=2)&#10;&#10;for i in range(1,10):&#10;    axes[0].plot(x, i * (x - 10)**2)&#10;&#10;for i in range(1,10):&#10;    axes[1].plot(x, i * np.cos(x))&#10;&#10;plt.show()&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;results in:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/nVHna.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;For diverging colour maps (e.g., to represent scalar values), the best reference I have seen is the paper by Kenneth Moreland available here &quot;&lt;a href=&quot;http://www.sandia.gov/~kmorel/documents/ColorMaps/&quot;&gt;Diverging Color Maps for Scientific Visualization&lt;/a&gt;&quot;. He developed the cool-warm scheme to replace the rainbow scheme, and &quot;presents  an algorithm that allows users to easily generate their own customized color maps&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Another useful source for information on the use of colour in scientific visualisations comes from Robert Simmon, the man who created the &quot;Blue Marble&quot; image for NASA. See his &lt;a href=&quot;http://earthobservatory.nasa.gov/blogs/elegantfigures/2013/08/05/subtleties-of-color-part-1-of-6/&quot;&gt;series of posts&lt;/a&gt; at the Earth Observatory web site.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-10-08T04:54:58.790" Id="118259" LastActivityDate="2014-10-08T05:36:22.307" LastEditDate="2014-10-08T05:36:22.307" LastEditorUserId="9311" OwnerUserId="9311" ParentId="118033" PostTypeId="2" Score="6" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I need to analyse unbalanced data through linear regression:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;modJuin=lm(TleafMax~TairMax*orientation, na.action=&quot;na.exclude&quot;, data=aJuin)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&quot;TairMax&quot; is a continuous numerical variable and &quot;orientation&quot; is a factor with two levels. Hence, I used drop1() to deal with the unbalanced data and test the model:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;drop1(modJuin, .~., test=&quot;F&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Which returns me this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Single term deletions&#10;&#10;Model:&#10;TleafMax ~ TairMax * orientation&#10;                    Df Sum of Sq    RSS     AIC   F value  Pr(&amp;gt;F)    &#10;&amp;lt;none&amp;gt;                            797.6  295.76                      &#10;TairMax              1    5158.6 5956.3 1003.48 2257.1256 &amp;lt; 2e-16 ***&#10;orientation          1      14.1  811.7  299.94    6.1672 0.01348 *  &#10;TairMax:orientation  1      11.1  808.7  298.63    4.8493 0.02831 *  &#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now, I know which variables lead to significantly different models when removed, apparently all of them (&lt;code&gt;TairMax&lt;/code&gt;, &lt;code&gt;orientation&lt;/code&gt;) plus the interaction (&lt;code&gt;TairMax:orientation&lt;/code&gt;).&#10;After that, I would like to obtain estimates for each variable. Hence, I used &lt;code&gt;summary()&lt;/code&gt; like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; summary.lm(modJuin)&#10;&#10;Call:&#10;lm(formula = TleafMax ~ TairMax * orientation, data = aJuin, &#10;    na.action = &quot;na.exclude&quot;)&#10;&#10;Residuals:&#10;    Min      1Q  Median      3Q     Max &#10;-2.9928 -1.1355 -0.0953  0.9155  5.2725 &#10;&#10;Coefficients:&#10;                     Estimate Std. Error t value Pr(&amp;gt;|t|)    &#10;(Intercept)           4.77483    0.42897  11.131   &amp;lt;2e-16 ***&#10;TairMax               0.89850    0.01891  47.509   &amp;lt;2e-16 ***&#10;orientation1          1.06529    0.42897   2.483   0.0135 *  &#10;TairMax:orientation1 -0.04165    0.01891  -2.202   0.0283 *  &#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1&#10;&#10;Residual standard error: 1.512 on 349 degrees of freedom&#10;  (2 observations deleted due to missingness)&#10;Multiple R-squared:  0.8721,    Adjusted R-squared:  0.871 &#10;F-statistic:   793 on 3 and 349 DF,  p-value: &amp;lt; 2.2e-16&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Here is my question:&lt;/strong&gt;&#10;Is it right to obtain estimates of the parameters of my linear model with &lt;code&gt;summary()&lt;/code&gt; after identifying significant variables with &lt;code&gt;drop1()&lt;/code&gt;?&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, I am not using directly summary() to determine the significance of my variables cause a colleague criticized me the other day: &quot;as you can see in the summary() output, your factors are treated by levels. In fact, one your factor levels is not appearing in the summary() output and the significance is only given for the other levels.&quot;. Is he right? In my case, this can be illustrated by the variable &quot;orientation&quot; for which only the level &quot;1&quot; is appearing in the summary() output (see above). If my colleague is wrong, I would use directly summary() to determine the significance of my variables as its output is order-independent (i.e. the order of the variables in the linear model is not impacting the summary() output)&lt;/p&gt;&#10;&#10;&lt;p&gt;Please note that I am aware about the problem to dig in main effects when interactions occur.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-08T08:25:10.093" Id="118270" LastActivityDate="2014-10-08T14:24:24.877" LastEditDate="2014-10-08T14:24:24.877" LastEditorUserId="57170" OwnerUserId="57170" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;linear-model&gt;&lt;summary-statistics&gt;" Title="Drop1() and Summary() on lm object" ViewCount="42" />
  
  <row Body="&lt;p&gt;First, are you sure you want correlations among time series? These are fraught with problems, chiefly that there are many false correlations. E.g. the population of China is highly correlated with the Dow Jones Industrial Average.&lt;/p&gt;&#10;&#10;&lt;p&gt;Second, if you &lt;em&gt;do&lt;/em&gt; want correlations, then instead of deleting outliers you can run a correlation measure that copes better with them, such as rank correlation, provided that the outliers are not spurious data. &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-10-08T11:03:47.397" Id="118288" LastActivityDate="2014-10-08T11:03:47.397" OwnerUserId="686" ParentId="118285" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;Given infinite data, k-NN is guaranteed to approach the Bayes error rate under ideal conditions. You probably don't have infinite data, and your k is probably not large enough (it has to approach infinity).&lt;/p&gt;&#10;&#10;&lt;p&gt;In practice, there's no reason k-NN should be the best classifier given finite data!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-08T13:02:54.540" Id="118298" LastActivityDate="2014-10-08T13:02:54.540" OwnerUserId="8960" ParentId="118268" PostTypeId="2" Score="0" />
  
  
  
  <row Body="&lt;p&gt;Q1: If you omit the intercept in univariate regression, you not only assume that no other (included) independent variable has an impact on the dependent variable – you also assume that the dependent variable is zero if the independent variable is zero. If the intercept of the true model is non-zero, doing OLS without the intercept induces potentially huge bias. It is in general preferable to include the coefficient for the intercept and see if it is significant, unless you are sure the specification really does not include the intercept.&lt;/p&gt;&#10;&#10;&lt;p&gt;Q2: Dummy variable is pretty much similar to any other independent variable. So the intercept may be significant in the presence of dummies. The issue is if all the observations had the same value for the dummy as then you would have perfect multicollinearity with the intercept – but again, this is an issue for any independent variable. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-10-08T17:02:14.657" Id="118339" LastActivityDate="2014-10-08T17:02:14.657" OwnerUserId="49819" ParentId="118336" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Consider the simplest case. One IV (so 2 parameters, including the constant). One data point. &lt;/p&gt;&#10;&#10;&lt;p&gt;Plot your one data point&lt;/p&gt;&#10;&#10;&lt;p&gt;Draw a straight line through that one point. Draw a different straight line through the same point. Draw a third one. ... and so on.&lt;/p&gt;&#10;&#10;&lt;p&gt;$\hspace{3cm}$&lt;img src=&quot;http://i.stack.imgur.com/Vfg7U.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;They all fit the data perfectly. Which one are you going to pick?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-09T02:58:30.550" Id="118392" LastActivityDate="2014-10-09T05:32:02.033" LastEditDate="2014-10-09T05:32:02.033" LastEditorUserId="805" OwnerUserId="805" ParentId="118278" PostTypeId="2" Score="6" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Suppose I have two coins, $A, B$ that each come up heads with probability $p_A, p_B$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Starting with a uniform prior on the values of $p_A, p_B$, and seeing data $s_A$ heads out of $N_A$ attempts, and respectively for $B$, I get  $$p_A, p_B \sim \mathrm{Beta}(1 + s_A, 1 + N_A - s_A), \mathrm{Beta}(1 + s_B, 1 + N_B - s_B)$$ that describes a posterior belief about the rates at which two separate coins, $A, B$ come up heads. &lt;/p&gt;&#10;&#10;&lt;p&gt;Now I want to compute the &lt;em&gt;loss&lt;/em&gt; at any moment of picking either $A$ or $B$ as the coin that is more likely to come up heads. &lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose $A$ has a higher empirical mean i.e., $\mathrm{heads}_A / \mathrm{flips}_A &amp;gt; \mathrm{heads}_B / \mathrm{flips}_B$. Call $\alpha_A = 1 + s_A, \beta_A = 1 + N_A - s_A$ and same for $B$, and so I compute the loss for picking $A$ to be the better coin. &lt;/p&gt;&#10;&#10;&lt;h2&gt;The Question&lt;/h2&gt;&#10;&#10;&lt;p&gt;Would my expected loss be $$\delta =  \int_0^1 \int_{p_B} ^1 (p_A - p_B) {p_A^{\alpha_A}(1 - p_A)^{\beta_A} p_B^{\alpha_B}(1 - p_B)^{\beta_B} \over \mathrm{B}(\alpha_A, \beta_A)\mathrm{B}(\alpha_B, \beta_B)} $$&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-09T09:13:29.880" Id="118410" LastActivityDate="2014-10-09T09:13:29.880" OwnerUserId="13022" PostTypeId="1" Score="0" Tags="&lt;expected-value&gt;&lt;beta-distribution&gt;" Title="Estimating the loss between two Beta distributions" ViewCount="14" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I have problem with getting p value from my mixed model,&#10;library(lme4)&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;DWR&amp;lt;-lmer(DWRm2~Growth.stage+Se.application+Growth.stage:Se.application+(1|Block),data=Sub1)&#10;summary(DWR)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;before I used this model and I got p value with summary my model but now I can't get it, I have just t value&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-10-09T10:28:48.293" FavoriteCount="1" Id="118416" LastActivityDate="2014-10-09T13:44:21.960" LastEditDate="2014-10-09T10:59:42.687" LastEditorUserId="22047" OwnerUserId="57277" PostTypeId="1" Score="1" Tags="&lt;mixed-model&gt;" Title="Getting P value with mixed effect with lme4 package" ViewCount="815" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I tell my students every year that there’s some correlation between successive draws in a Lehmer RNG, so they should use a Mersenne Twister or Marsaglia’s MWC256... but I am unable to provide a natural example where Lehmer would fail. Of course there are specially designed tests that Lehmer generators fail, but can someone provide a natural situation where the autocorrelation results in aberrant results?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for your thoughts.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-10-09T12:14:37.417" FavoriteCount="2" Id="118427" LastActivityDate="2015-01-02T00:10:31.983" LastEditDate="2014-10-09T15:00:11.027" LastEditorUserId="8076" OwnerUserId="8076" PostTypeId="1" Score="6" Tags="&lt;random-generation&gt;" Title="Natural example of bad results with a Lehmer Random Number Generator" ViewCount="113" />
  <row AnswerCount="0" Body="&lt;p&gt;I posted the same question on MSE since I was not really sure whether to post it here first or not. Anyway since I still did not get any answers I will be posting this here hoping for some help. &lt;/p&gt;&#10;&#10;&lt;p&gt;Say I have a finite discrete Markov chain constructed maybe using some data set. Is there any good way of checking it's accuracy? Any model will be required to be tested or maybe back tested in order to validate it. So I need to know if there are any such methods for Markov Chains. Hope someone can help me out. Thanks.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-10-09T12:21:02.953" Id="118428" LastActivityDate="2014-10-09T12:21:02.953" OwnerUserId="42891" PostTypeId="1" Score="0" Tags="&lt;probability&gt;&lt;stochastic-processes&gt;&lt;validation&gt;&lt;markov-chain&gt;" Title="A way to check the accuracy of a Markov chain?" ViewCount="25" />
  
  
  <row AnswerCount="3" Body="&lt;p&gt;In statistical software like MINITAB and SAS, the default alpha value (critical p-value) is set as 0.15. I would like to know (1) if there is any statistic basis to set it as 0.15 and (2) if this is a common accepted value in statistics. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have found that in &lt;a href=&quot;http://stats.stackexchange.com/questions/97257/stepwise-regression-in-r-critical-p-value&quot;&gt;this question&lt;/a&gt;, the critical p-value of 0.15 in R is actually based on AIC. So is AIC the basis to set 0.15 as default value?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance for your help!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-09T14:42:23.777" Id="118445" LastActivityDate="2014-10-09T22:59:32.477" OwnerUserId="37704" PostTypeId="1" Score="1" Tags="&lt;stepwise-regression&gt;" Title="What is the basis of setting critical p-value value in stepwise regression?" ViewCount="94" />
  <row Body="&lt;p&gt;Just for the sake of resolving the question, I'm going to state that this is probably due to a confusion over the modeling framework.  It would rarely (if ever) be sensible to model an average egg mass with a Poisson distribution (which only applies to a unitless count variable).  If you have average &lt;em&gt;counts&lt;/em&gt;, and have a measurement of the total exposure (i.e. you have total counts and the area or time over which they were collected), you can do a Poisson model with an offset.&lt;/p&gt;&#10;&#10;&lt;p&gt;In this case it would make more sense to use a log-Normal (i.e., by transforming the response and then fitting a linear mixed model, &lt;code&gt;lmer(log(Avg_egg_mass) ~ ...&lt;/code&gt;) or a Gamma model (the former, log-Normal approach is easier; I would generally only recommend a Gamma model in cases where there is a strong mechanistic or cultural reason to use one).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-09T14:48:28.257" Id="118447" LastActivityDate="2014-10-09T14:48:28.257" OwnerUserId="2126" ParentId="118430" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;You tried to show detailed balance for the Markov chain that is obtained by considering one transition of the Markov chain to be the 'Gibbs sweep' where you sample each component in turn from its conditional distribution. For this chain, detailed balance is not satisfied. The point is rather that each sampling of a particular component from its conditional distribution is a transition that satisfies detailed balance. It would be more accurate to say that Gibbs sampling is a special case of a slightly generalized Metropolis-Hastings, where you alternate between multiple different proposals. More details follow.&lt;/p&gt;&#10;&#10;&lt;h3&gt;The sweeps do not satisfy detailed balance&lt;/h3&gt;&#10;&#10;&lt;p&gt;I construct a counterexample. Consider two Bernoulli variables ($X_1,X_2$), with probabilities as shown in the following table:&#10;\begin{equation}&#10;\begin{array}{ccc}&#10;        &amp;amp; X_2 = 0     &amp;amp; X_2 = 1 \\&#10;X_1 = 0 &amp;amp; \frac{1}{3} &amp;amp; \frac{1}{3} \\&#10;X_1 = 1 &amp;amp; 0           &amp;amp; \frac{1}{3}&#10;\end{array}&#10;\end{equation}&#10;Assume the Gibbs sweep is ordered so that $X_1$ is sampled first. Moving from state $(0,0)$ to state $(1,1)$ in one move is impossible, since it would require going from $(0,0)$ to $(1,0)$. However, moving from $(1,1)$ to $(0,0)$ has positive probability, namely $\frac{1}{4}$. Hence we conclude that detailed balance is not satisfied.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, this chain still has a stationary distribution that is the correct one. Detailed balance is a sufficient, but not necessary, condition for converging to the target distribution. &lt;/p&gt;&#10;&#10;&lt;h3&gt;The component-wise moves satisfy detailed balance&lt;/h3&gt;&#10;&#10;&lt;p&gt;Consider a two-variate state where we sample the first variable from its conditional distribution. A move between $(x_1,x_2)$ and $(y_1,y_2)$ has zero probability in both directions if $x_2 \neq y_2$ and thus for these cases detailed balance clearly holds. Next, consider $x_2 = y_2$:&#10;\begin{equation}&#10;\pi(x_1,x_2) \mathrm{Prob}((x_1,x_2) \rightarrow (y_1,x_2)) = \pi(x_1,x_2)\,p(y_1 \mid X_2 = x_2) = \pi(x_1,x_2) \, \frac{\pi(y_1,x_2)}{\sum_z \pi(z,x_2)} \\&#10;= \pi(y_1,x_2) \, \frac{\pi(x_1,x_2)}{\sum_z \pi(z,x_2)} = \pi(y_1,x_2) \,p(x_1 \mid X_2 = x_2) = \pi(y_1,x_2) \mathrm{Prob}((y_1,x_2) \rightarrow (x_1,x_2)).&#10;\end{equation}&lt;/p&gt;&#10;&#10;&lt;h3&gt;How the component-wise moves are Metropolis-Hastings moves?&lt;/h3&gt;&#10;&#10;&lt;p&gt;Sampling from the first component, our proposal distribution is the conditional distribution. (For all other components, we propose the current values with probability $1$). Considering a move from $(x_1, x_2)$ to $(y_1, y_2)$, the ratio of target probabilities is&#10;\begin{equation}&#10;\frac{\pi(y_1,x_2)}{\pi(x_1,x_2)}.&#10;\end{equation}&#10;But the ratio of proposal probabilities is &#10;\begin{equation}&#10;\frac{\mathrm{Prob}((y_1,x_2) \rightarrow (x_1,x_2))}{\mathrm{Prob}((x_1,x_2) \rightarrow (y_1,x_2))} = \frac{\frac{\pi(x_1,x_2)}{\sum_z \pi(z,x_2)}}{\frac{\pi(y_1,x_2)}{\sum_z \pi(z,x_2)}} = \frac{\pi(x_1,x_2)}{\pi(y_1,x_2)}.&#10;\end{equation}&#10;So, the ratio of target probabilities and the ratio of proposal probabilities are reciprocals, and thus the acceptance probability will be $1$. In this sense, each of the moves in the Gibbs sampler are special cases of Metropolis-Hastings moves. However, the overall algorithm viewed in this light is a slight generalization of the typically presented Metropolis-Hastings algorithm in that you have alternate between different proposal distributions (one for each component of the target variable). &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-10-09T15:17:48.457" Id="118453" LastActivityDate="2014-10-09T15:17:48.457" OwnerUserId="24669" ParentId="118442" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;The basis is &lt;a href=&quot;https://en.wikipedia.org/wiki/Cargo_cult_science&quot; rel=&quot;nofollow&quot;&gt;cargo cult science&lt;/a&gt;. &quot;$p$-values&quot; produced from stepwise regression model building do not have the typical meaning of &quot;probability of observing the estimate/test statistic assuming the null hypothesis is true,&quot; but rather &quot;probability of observing the estimate/test statistic based on a series of unstated conditionals that are almost certainly predicated on some number of variables not included in the presented model.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;Use stepwise regression if you want to appear to be performing meaningful statistical analysis, while providing results (estimates, &quot;noise&quot; variables, missing &quot;real&quot; variables, $p$-values, $R^{2}$, etc.) that are very likely to be biased.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-10-09T15:52:30.930" Id="118456" LastActivityDate="2014-10-09T16:32:31.437" LastEditDate="2014-10-09T16:32:31.437" LastEditorUserId="44269" OwnerUserId="44269" ParentId="118445" PostTypeId="2" Score="3" />
  <row AnswerCount="0" Body="&lt;p&gt;While looking at one &lt;a href=&quot;http://projecteuclid.org/download/pdf_1/euclid.aoap/1034625254&quot; rel=&quot;nofollow&quot;&gt;paper&lt;/a&gt; about Metropolic Hasting optimal convergence rates, I came accross a discrete time generator of Markov chain. It is defined as follows:&#10;$$G V(x)=nE\left [ \left( V(y)-V(x)\right )\left(1\wedge \frac{\pi(y)}{\pi(x)}\right ) \right ],$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $n$ is the dimension of $x$, $y$ is the proposed value by proposal distribution, expectation is taken w.r.t. the proposal distribution and $\pi$ is the target distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is where could I find more information about the construction of discrete time generators? I know about the continuous time generators for diffusion processes, but cannot find information about the discrete analogue. &#10;A paper or (better) a textbook reference would be highly appreciated.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-09T16:07:54.740" Id="118458" LastActivityDate="2014-10-09T16:07:54.740" OwnerUserId="9343" PostTypeId="1" Score="1" Tags="&lt;mcmc&gt;&lt;stochastic-processes&gt;&lt;discrete-time&gt;&lt;generator&gt;" Title="Discrete time generator of stochastic process" ViewCount="13" />
  
  
  
  <row Body="&lt;p&gt;PCA isn't the right tool here. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you want to show that the currencies have approximately the same pairwise distribution with respect to each other, just pick a few of the currency pairs and show that the graphs look similar by putting them next to each other. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you want to show that all the currencies have identical graphs, you might show that the distribution of all squared Pearson correlation coefficients between each pair of currencies is very narrow (low variance).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-09T16:51:26.453" Id="118468" LastActivityDate="2014-10-09T16:51:26.453" OwnerUserId="20775" ParentId="118439" PostTypeId="2" Score="0" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have several variables that were measured on patients, for most patients the variables of interest were measured by 2 independent &quot;raters&quot;. Most of the variables are binary. I need to compute the rates within each variable, i.e. the proportion of 1's. I will also need to correlate those proportion with some other &quot;patient-level&quot; variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;What would make more sense: &quot;merging&quot; the two ratings into one (e.g. at least one &quot;yes&quot; is a &quot;yes&quot;, and &quot;no&quot; otherwise), or to do a multilevel model a-la WinBUGS. The justification for the first option is the fact that inter-rater reliability is good.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-09T17:12:42.773" Id="118473" LastActivityDate="2014-10-30T08:40:53.687" OwnerUserId="32047" PostTypeId="1" Score="0" Tags="&lt;proportion&gt;&lt;mixed-effect&gt;&lt;winbugs&gt;" Title="Measurements from two raters. Should I use multilevel/&quot;random-effects&quot; model?" ViewCount="26" />
  
  <row Body="&lt;p&gt;Yes, this is a classic case where you need to correct for multiple testing. It is irrelevant whether the tests are independent or not. If you are testing at say $\alpha$=0.05 then you have a 5% chance of a false positive by chance alone. 26 tests at this level will give ~1 false positive by chance. Bonferroni will protect against such error but it is very cconservative. You are better off using a less conservative multiple correction approach.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-09T20:28:28.727" Id="118503" LastActivityDate="2014-10-09T20:28:28.727" OwnerUserId="9233" ParentId="118498" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;I assume $j=1,...,9$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Yes, you can treat $\beta_1t_{0i}$ as object-specific intercept. The difference from random intercept is that $\beta_1t_{0i}$ is fixed. It is kind of like &lt;a href=&quot;http://stats.stackexchange.com/questions/21760/what-is-a-difference-between-random-effects-fixed-effects-and-marginal-model/68753#68753&quot;&gt;the difference between fixed and random effects&lt;/a&gt; in econometrics. All objects share the same $\beta_1$, which is then weighted by the baseline value of each object. If the baseline measurement is important, it is fine to do so.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-10-09T21:35:40.090" Id="118514" LastActivityDate="2014-10-09T21:35:40.090" OwnerUserId="21599" ParentId="118207" PostTypeId="2" Score="1" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Assume that the goal of Machine learning is to find a function that is able to minimize the generalization/expected/true error (assuming that the underlying distribution is fixed but unknown):&lt;/p&gt;&#10;&#10;&lt;p&gt;$$E(f) =E_{x,y}[Loss(y, f(x)]$$&lt;/p&gt;&#10;&#10;&lt;p&gt;In the case of boosting, we search for this best predictor by adding stumps and their vote weight (and hope that the true error does indeed go down). In boosting we specifically use the exponential loss function:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$Loss(y, f(x)) = e^{-yf(x)}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;If the joint distribution for the data was not unknown, would the algorithm for searching for the optimal predictor f in boosting be different? Is the way boosting works the way it is, because the underlying distribution is unknown?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-10T02:24:23.383" Id="118534" LastActivityDate="2014-10-10T02:24:23.383" OwnerUserId="37632" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;boosting&gt;&lt;loss-functions&gt;" Title="Would knowing the underlying distribution for our data affect how boosting searches for its predictor or how it minimizes the exponential loss?" ViewCount="16" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am given a general equation of an AR(3) process :&lt;/p&gt;&#10;&#10;&lt;p&gt;$Y_t\:=\:e_t\:+\:\Phi _1\:Y_{t-1}+\:\Phi _2Y_{t-2}\:\:+\:\Phi _3Y_{t-3}$&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to find the $\gamma _0$ of the AR(3) process but I am not too sure how should i go about doing it.&lt;/p&gt;&#10;&#10;&lt;p&gt;Do I take the variance of both sides of the AR(3) process? Or should I use Yule-Walker equations and derive the $\gamma _0$ from there?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-10-10T07:41:15.533" Id="118548" LastActivityDate="2014-10-10T08:21:36.377" LastEditDate="2014-10-10T08:21:36.377" LastEditorUserId="57351" OwnerUserId="57351" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;" Title="Autocovariance of an AR(3) process" ViewCount="48" />
  
  <row AnswerCount="1" Body="&lt;p&gt;One of the basic figures you get when running multiple linear regression using almost any off-the-shelf software is the F statistics. However, I cannot recall one situation, where the F value was low enough that I could say, the ratio of model MSE and sample variance was not significant. I do understand that the figure makes sense if we compare two competing models, but the way it is usually reported by most software is by measuring the relative decrease in variance. Maybe I am overlooking something, but why report the F statistic at all?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-10-10T08:20:49.903" Id="118553" LastActivityDate="2014-10-10T08:42:08.593" OwnerUserId="57355" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;multiple-regression&gt;&lt;linear-model&gt;&lt;residual-analysis&gt;&lt;f-statistic&gt;" Title="What is the benefit of knowing the F statistic in multiple linear regression?" ViewCount="48" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I would like information (references) about the reason why time series should be filtered before being used in a VAR model. &lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you in advance, &lt;/p&gt;&#10;&#10;&lt;p&gt;Nikos.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-10-10T11:42:40.287" FavoriteCount="1" Id="118571" LastActivityDate="2014-10-10T11:42:40.287" OwnerUserId="37366" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;filter&gt;" Title="Filtering of time series" ViewCount="55" />
  
  <row Body="&lt;p&gt;$X\sim \text{Bin}(n,p)$&lt;/p&gt;&#10;&#10;&lt;p&gt;$p = X/n$ &lt;/p&gt;&#10;&#10;&lt;p&gt;Since $p$ has a non-zero chance to be both 0 and 1, $E(\log(\frac{p}{1-p}))$, and also $\text{Var}(\log(\frac{p}{1-p}))$ are undefined.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you want some other answer, you'll need to keep $p$ away from 0 and 1.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-10T14:46:53.813" Id="118591" LastActivityDate="2014-10-10T14:46:53.813" OwnerUserId="805" ParentId="118403" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;It sounds like your problem involves moderation when both the moderator and the predictor are continuous. You may want to take a look at &quot;Multiple Regression: Testing and Interpreting Interactions&quot; by Leona Aiken (1992). &#10;Let me explain using &lt;code&gt;mtcars&lt;/code&gt; dataset in R. Both &lt;code&gt;wt&lt;/code&gt; and &lt;code&gt;hp&lt;/code&gt; are mean centered:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(dplyr)&#10;library(ggplot2)&#10;&#10;mtcars &amp;lt;- mtcars %&amp;gt;%&#10;  mutate(wt_c = wt - mean(wt),&#10;         hp_c = hp - mean(hp),&#10;         wt_f = cut(wt, breaks=quantile(wt, c(0, 1/3, 2/3, 1)), include.lowest=TRUE))&#10;## wt_f is created for visual exploration.&#10;&#10;summary(lm(mpg ~ wt*hp, data=mtcars))  &#10;             Estimate Std. Error t value Pr(&amp;gt;|t|)    &#10;(Intercept) 18.898400   0.495703  38.124  &amp;lt; 2e-16 ***&#10;hp_c        -0.030508   0.007503  -4.066 0.000352 ***&#10;wt_c        -4.131649   0.529558  -7.802 1.69e-08 ***&#10;hp_c:wt_c    0.027848   0.007420   3.753 0.000811 ***&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Here, &lt;code&gt;hp&lt;/code&gt;is negatively associated with &lt;code&gt;mpg&lt;/code&gt; at the mean &lt;code&gt;wt&lt;/code&gt; and &lt;code&gt;hp&lt;/code&gt; while holding &lt;code&gt;wt&lt;/code&gt; constant. Yet,  each increase in &lt;code&gt;wt&lt;/code&gt; by 1 unit increases the &lt;code&gt;mpg~hp&lt;/code&gt; slope by 0.03. That is, the negative association between &lt;code&gt;mpg&lt;/code&gt; and &lt;code&gt;hp&lt;/code&gt; is weaker for heavier cars (i.e., less negative) than for lighter cars. A plot really helps interpret this result.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ggplot(mtcars, aes(hp, mpg, colour=wt_f)) + geom_point(size=5) + &#10;stat_smooth(method=&quot;lm&quot;, size=2, se=FALSE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/yC5hd.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;It appears that, for heavier cars (the blue line), their &lt;code&gt;mpg&lt;/code&gt; is already much lower than lighter cars, so adding more horsepower does not penalize the cars' &lt;code&gt;mpg&lt;/code&gt; as steeply as for the lighter cars.&lt;/p&gt;&#10;&#10;&lt;p&gt;I suppose something like this may happen for your dataset: older people tend to have more money at hand, so they may not gain as much happiness from having more money as do younger people, who tend to have less money especially nowadays. This pattern, if true, should result in weaker slope/correlations among older people than among the younger counterparts.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-10-10T15:06:27.357" Id="118594" LastActivityDate="2014-10-10T15:06:27.357" OwnerUserId="53057" ParentId="118566" PostTypeId="2" Score="1" />
  <row AnswerCount="2" Body="&lt;p&gt;I'm new to asking questions here, so I hope this is a reasonable place for this.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am trying to fit a piece-wise regression. I expect my response (y) to increase from x0 to somewhere along x, then to plateau after a breakpoint; thereafter, slope =0. The model I would like to fit has 3 parameters -- intercept, slope, and breakpoint. In a best-case scenario, I would compare this model to an intercept-only/null model using AIC or some similar method.&lt;/p&gt;&#10;&#10;&lt;p&gt;I started with package segmented, with this hint to fix the second slope to 0:&#10;&lt;a href=&quot;https://stat.ethz.ch/pipermail/r-help/2007-July/137625.html&quot; rel=&quot;nofollow&quot;&gt;https://stat.ethz.ch/pipermail/r-help/2007-July/137625.html&lt;/a&gt;&#10;But after updating a bunch of packages, the whole thing (which was already a bit fussy) stopped working altogether. &lt;/p&gt;&#10;&#10;&lt;p&gt;So, now, I'm using optim(), but I'm not sure how to appropriately get an estimate of error, particularly around the breakpoint parameter. See code below for examples of attempts using bblme and nls (they both work, but both seem to fail on my data sets). &lt;/p&gt;&#10;&#10;&lt;p&gt;According to a SAS-using colleague, proc nlin seems to be able to fit this model easily &amp;amp; generates standard error estimates for the parameters... but, I'm determined to Not use SAS.&lt;/p&gt;&#10;&#10;&lt;p&gt;This exchange (and others) seems to indicate that there are general concerns with error estimates for this type of model,particularly the breakpoint parameter - and it references a potentially useful post by Venables that is unfortunately no longer available. &#10;&lt;a href=&quot;http://stats.stackexchange.com/questions/7527/change-point-analysis-using-rs-nls&quot;&gt;Change point analysis using R&amp;#39;s nls()&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;(Note, my data are weighted by a variable that represents effort. When it is available, you can feed wt to a weights=() statement, otherwise, I use an expanded dataset or weight the sum of squares in the model.)&lt;/p&gt;&#10;&#10;&lt;h3&gt;Simulate segmented data with diminishing sample size&lt;/h3&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(15)&#10;a.sim&amp;lt;-0 #  intercept&#10;b.sim&amp;lt;-0.5 #  slope for segment 1&#10;n&amp;lt;-12&#10;brk&amp;lt;-4 # breakpoint&#10;x &amp;lt;- c(1:n)&#10;y &amp;lt;- numeric(n)&#10;y[1:brk]&amp;lt;-a.sim+b.sim*x[1:brk]+rnorm(brk, 0, .2)&#10;y[(brk+1):n] &amp;lt;- (a.sim+ b.sim* brk) + rnorm((n-brk), 0, .2) # second, flat segment&#10;y[n]&amp;lt;-y[n]-.30*y[n] #subtract from last point, as these are often outside of the normal pattern&#10;wt&amp;lt;-c(rep(50, n-4), c(40, 40, 35, 5)) #weight variable &#10;&#10;dat&amp;lt;-as.data.frame(cbind(x, y, wt)) # make dataframe &#10;dat.expand &amp;lt;- dat[ rep( seq(dim(dat)[1]), dat$wt),]# second dataset with rows repeated based on weight&#10;&#10;with(dat, plot(x,y, ylim=c(0, max(y)), pch=16, cex=wt/(10)))### plot, with symbols representing weight variable&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;h3&gt;Use optim to solve&lt;/h3&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;mod&amp;lt;-function(par, data){&#10;  a &amp;lt;- par[1]&#10;  b &amp;lt;-par[2]&#10;  x.star &amp;lt;-par[3]&#10;  yfit&amp;lt;-c(NA,length(data$y))&#10;      small.x&amp;lt;-I(data$x&amp;lt;=x.star)&#10;  yfit[small.x==T]&amp;lt;-(a+b*data$x[small.x==T]) &#10;      yfit[small.x!=T]&amp;lt;-(a+b*x.star) &#10;      sum((data$y-yfit)^2)&#10;}&#10;fit1&amp;lt;-optim(par=c(a=.5, b=.5, x.star=3), mod, data=dat, hessian=T)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;this almost always works well and provides a reasonable fit. For production run, I feed it a large table of potential start values, to find a reasonable start point before actually optimizing.&lt;/p&gt;&#10;&#10;&lt;h3&gt;bblme for cis&lt;/h3&gt;&#10;&#10;&lt;p&gt;I talked to a statistician, who recommended this as a possible method for generating confidence intervals around the breakpoint. It works for my data in some instances, but usually gives errors.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(bbmle)&#10;mod2&amp;lt;-function(a,b,x.star,sig){&#10;yfit&amp;lt;-c(NA,length(y))&#10;small.x&amp;lt;-I(x&amp;lt;=x.star)&#10;yfit[small.x==T]&amp;lt;-(a+b*x[small.x==T]) &#10;yfit[small.x!=T]&amp;lt;-(a+b*x.star) &#10;-sum(dnorm(y,yfit,1, log=TRUE))}&#10;&#10;fit3&amp;lt;-mle2(mod2, start=list(a=0,b=0.5,x.star=brk), data=dat)&#10;ci&amp;lt;-confint(fit3) &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;h3&gt;nls method&lt;/h3&gt;&#10;&#10;&lt;p&gt;This basically never works for my kind of data (works if I simulate much larger datasets) I get either the singular gradient matrix error or&#10;Error in ....  step factor 0.000488281 reduced below 'minFactor' of 0.000976562... &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;nls.mod&amp;lt;-nls(y ~ ifelse(x &amp;lt;x.star, a+b*x,a+b*x.star), &#10;data = dat, weights=wt,&#10;start = c(x.star=brk,b=0.4, a=0))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;h3&gt;JAGS?&lt;/h3&gt;&#10;&#10;&lt;p&gt;So, I also have this working in JAGS/r2jags, again with help from a statistician. I would rather not use this approach if there is a straight-forward, frequentist way to get a reasonable estimate of error around the breakpoint. I'm not looking for perfection here. Just something reasonable.. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-10T15:37:33.930" FavoriteCount="1" Id="118598" LastActivityDate="2015-01-20T16:28:30.943" LastEditDate="2014-10-10T18:20:15.153" LastEditorUserId="57327" OwnerUserId="57327" PostTypeId="1" Score="4" Tags="&lt;standard-error&gt;&lt;jags&gt;&lt;nls&gt;&lt;piecewise-linear&gt;&lt;segmented-regression&gt;" Title="R - how to get standard error for a breakpoint/parameter in a piecewise regression" ViewCount="109" />
  
  <row Body="&lt;p&gt;You could map $(1,2,3)$ to $(1,3,5)$. That is easy, as a one-by-one recode or $\text{new} = 2 \times \text{old} - 1$, depending on your software.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I can't see any reason for doing that unless you want to average the 3 point scale and present results comparable to those for a 5-point scale. If you wanted to do that, you could naturally work on the averages rather than the raw data. &lt;/p&gt;&#10;&#10;&lt;p&gt;It would be a bad idea if the goal was to persuade or fool yourself or anybody else that you really had a 5-point scale, whereas you don't. &lt;/p&gt;&#10;&#10;&lt;p&gt;@Peter Flom has a very good answer to your more general question. &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-10-10T16:49:12.317" Id="118610" LastActivityDate="2014-10-10T16:49:12.317" OwnerUserId="22047" ParentId="118567" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm working on a research project involving Bayesian networks. BNs are directed acyclic graphs (DAGs) used to compactly represent joint distributions of variables. In many cases, multiple DAGs can represent the same distribution, and so compose an equivalence class. Equivalent DAGs share the same skeleton (same edges if one ignores direction), and the orientation of edges in their v-structures (X-&gt;Y, Z-&gt;Y, but no edge between X and Z) must be the same. Such equivalence classes can be represented by partially directed acyclic graphs (PDAGs), with the &lt;em&gt;compelled&lt;/em&gt; (i.e. have the same orientation) edges directed and the &lt;em&gt;reversible&lt;/em&gt; edges undirected.&lt;/p&gt;&#10;&#10;&lt;p&gt;Given a DAG, I would like to be able to find all other DAGs in its equivalence class. I'm aware of an efficient algorithm (Chickering 1995) that, given a DAG, finds the complete PDAG representing its equivalence class. I'm also aware of an algorithm (Dor and Tarsi 1992) that, given a PDAG, generates a random member DAG of the equivalence class. I am not interested in a random member, but rather in enumerating all members.&lt;/p&gt;&#10;&#10;&lt;p&gt;It might seem trivial at first glance--why not just try all combinations of orientations of the reversible edges, and discard the ones that aren't acyclic? But the number of possibilities grows exponentially in the number of reversible edges, so I'm worried that this won't work for large graphs. I've seen some sources claim that the proportion of edges that are reversible is relatively small, but still, if the graph is large enough, it will still be problematic. I'm working with graphs of up to several hundred variables, with several thousand being a possibility later on.  &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-10-10T18:57:33.970" Id="118619" LastActivityDate="2014-10-10T18:57:33.970" OwnerUserId="57389" PostTypeId="1" Score="2" Tags="&lt;algorithms&gt;&lt;graphical-model&gt;&lt;graph-theory&gt;&lt;bayesian-network&gt;" Title="Efficient algorithm to enumerate all member DAGs of a Markov equivalence class" ViewCount="35" />
  
  <row Body="&lt;p&gt;CCA is sensitive to outliers and assumes species response is a symmetrical unimodal function of position along environmental gradients. Hypothesis testing is based on randomization, so does not have distributional assumptions. But, CCA or not, transformations should be applied only if they improve data distribution (demonstrated using normality tests or PPCC fit).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-10-10T20:25:11.530" Id="118625" LastActivityDate="2014-10-10T20:25:11.530" OwnerUserId="57390" ParentId="118276" PostTypeId="2" Score="1" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a bounded non-convex function in 10-dimensional space. The function is quasi-smooth, you can imagine a histogram, here is an illustration, it just shows the idea and not related to my particular function:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/noBFY.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The function value is obtained by time consuming simulation (it takes about 10 seconds). Obviously if I want compute gradients, I need to approximate them by difference quotients. If you need more details about the function, I might be able to say more.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have read the article of&#10;L.M. Rios and N.V. Sahinidis, &lt;a href=&quot;http://link.springer.com/content/pdf/10.1007%2Fs10898-012-9951-y.pdf&quot; rel=&quot;nofollow&quot;&gt;Derivative-free optimization: A review of algorithms and comparison of software implementations&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;So I've tried all of TOMLAB solvers, proposed in article and also &lt;a href=&quot;http://www.mat.univie.ac.at/~neum/software/mcs/&quot; rel=&quot;nofollow&quot;&gt;MCS&lt;/a&gt; method, also mentioned in the article as one of the best.&#10;But neither of them could not overperform simple Brent method accompanied by hand picked initial guess (well, I hardly believe I can produce such great guesses).&lt;/p&gt;&#10;&#10;&lt;p&gt;I've heard about surrogate modeling, is it worth trying?&lt;/p&gt;&#10;&#10;&lt;p&gt;So which other global optimization methods should I consider?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-10-10T23:29:24.543" Id="118653" LastActivityDate="2014-10-11T09:38:23.033" LastEditDate="2014-10-11T09:38:23.033" LastEditorUserId="56429" OwnerUserId="56429" PostTypeId="1" Score="1" Tags="&lt;optimization&gt;&lt;algorithms&gt;&lt;minimum&gt;" Title="Finding a global minimum of non-convex quasi-smooth function that is costly to evaluate" ViewCount="48" />
  <row AnswerCount="0" Body="&lt;p&gt;If I have the objective function of Logistic Regression to optimize by maximizing it, would it change to a minimization problem when I add regularization term to it?&lt;/p&gt;&#10;&#10;&lt;p&gt;Or can I still solve the optimization problem by maximizing it?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-10-11T01:22:43.667" Id="118662" LastActivityDate="2014-10-11T01:22:43.667" OwnerUserId="38052" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;machine-learning&gt;&lt;logistic&gt;&lt;pattern-recognition&gt;" Title="Change in objective function optimization - Regularization in Logistic Regression" ViewCount="58" />
  
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;Questions: 1) will the mean(i) still form a normal distribution? &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;i) The $i^\text{th}$ mean won't be normal (the CLT isn't about samples of size $n_i$), though in very large samples a standardized mean may be close to normally distributed. &lt;/p&gt;&#10;&#10;&lt;p&gt;ii) The distribution of means across $i$ won't be normal in general even as sample sizes go to infinity. (I expect this is what you were trying to ask about)&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;2) What should be the std deviation of such a distribution? &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;This is unclear. Are you asking about the distribution of the sample means?&lt;/p&gt;&#10;&#10;&lt;p&gt;It would be some (scale) &lt;a href=&quot;http://en.wikipedia.org/wiki/Mixture_distribution#Finite_and_countable_mixtures&quot; rel=&quot;nofollow&quot;&gt;mixture distribution&lt;/a&gt;, so basic results about the moments - and from that, the variance - of finite mixtures should hold.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;3) If i now take average of all these means, can i construct confidence interval around that measurement for original population mean?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Possibly. One example approach: as the number of means goes to infinity, if some conditions hold* you could perhaps make use of an argument relating to the Lyapunov or Lindeberg versions of CLT along with Slutsky's theorem**. There are some other things that might be done.&lt;/p&gt;&#10;&#10;&lt;p&gt;* see those versions of the CLT; for example, you'll essentially need that none of the variances &quot;dominate&quot; the sum of the variances.&lt;/p&gt;&#10;&#10;&lt;p&gt;** but again, note that they're actually about what happens in the limit, not at a finite sample size.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Of course, if you know the $n_i$'s and the $\bar{x}_i$'s you can form the unweighted average, and if you have the sample standard deviations, you should also be able to calculate the &lt;a href=&quot;http://en.wikipedia.org/wiki/Standard_deviation#Sample-based_statistics&quot; rel=&quot;nofollow&quot;&gt;overall standard deviation&lt;/a&gt; as if you had all the data... and you can apply a more &quot;common&quot; approach then.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-10-11T01:27:05.043" Id="118663" LastActivityDate="2014-10-11T05:49:00.840" LastEditDate="2014-10-11T05:49:00.840" LastEditorUserId="805" OwnerUserId="805" ParentId="118658" PostTypeId="2" Score="2" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I am trying to derived estimates of model-averaged parameter effects on a fairly complicated set of models using an information-theoretic approach. I have several models that investigate continuous and categorical variables, interactions of these variables, and I have a global model which includes all of these variables and interactions. My question is, when I am model averaging, I read that the beta estimates for the slope of the effect of a quadratic term are represented by x+x^2. But, I also have a linear model of this same variable (e.g., x=linear time and x+x^2 = quadratic time). &lt;/p&gt;&#10;&#10;&lt;p&gt;My first question is: Do I use both beta estimates for the quadratic model (e.g., x and x^) as part of calculating my model-averaged estimate?&lt;/p&gt;&#10;&#10;&lt;p&gt;Second question is: In my global model, the way I've set up my models (which I assume must be incorrect) I only have a beta estimate for x and x^2 I assume these are to be interpreted as the quadratic term x+x^2). Should I somehow be able to have an extra parameter in the global model, so that I have two estimates for x? An x to represent the linear term by itself, and a different estimate for x that is part of the x+x^2? Thanks for any help you can give to explain this.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-11T18:37:37.667" Id="118710" LastActivityDate="2014-10-12T01:56:26.987" OwnerUserId="57433" PostTypeId="1" Score="2" Tags="&lt;generalized-linear-model&gt;&lt;aic&gt;&lt;beta-distribution&gt;" Title="Model averaging when linear and quadratic effects are modeled in a global model" ViewCount="69" />
  <row Body="&lt;p&gt;As expected: it depends on what you want. In terms of generalization performance, typically the performance differences are minor.&lt;/p&gt;&#10;&#10;&lt;p&gt;That said, minimizing the $l_1$-norm has the extremely attractive feature of yielding sparse solutions (the support vectors are a subset of the training set). When doing ridge regression, just like in least-squares SVM, all training instances become support vectors and you end up with a model the size of your training set. A large model requires a lot of memory (obviously) and is slower in prediction.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-10-11T23:55:39.040" Id="119722" LastActivityDate="2014-10-11T23:55:39.040" OwnerUserId="25433" ParentId="119714" PostTypeId="2" Score="2" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Bickel&amp;amp;Doksum, Mathematical Statistics, Basic ideas...Vol1&lt;/p&gt;&#10;&#10;&lt;p&gt;page 122, just above Cor2.3.1, it says:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Define the &lt;em&gt;convex support&lt;/em&gt; of a probability P to be the smallest convex set C such that P(C)=1.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I don't think this definition is quite common since I had hard time searching it without any reasonable result. &lt;/p&gt;&#10;&#10;&lt;p&gt;To prove the existence of MLE in canonical exponential family or Cor2.3.1, I need to understand this one in a rigorous way.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any ideas or comments would be helpful.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-10-12T06:50:03.060" Id="119743" LastActivityDate="2014-10-12T06:50:03.060" OwnerUserId="48956" PostTypeId="1" Score="0" Tags="&lt;mathematical-statistics&gt;&lt;maximum-likelihood&gt;&lt;exponential-family&gt;" Title="What is a convex support? (Bickel&amp;Doksum, Mathematical Statistics, Basic ideas...Vol1)" ViewCount="21" />
  
  <row Body="&lt;p&gt;Thanks for the question!&lt;/p&gt;&#10;&#10;&lt;p&gt;First off, the two models you show are identical - &lt;code&gt;(SamplingMonth | UnitID)&lt;/code&gt; is implicitly &lt;code&gt;(1 + SamplingMonth | UnitID)&lt;/code&gt;. If you wanted to run a model without random intercepts by UnitID, and just random slopes, you'd need to run &lt;code&gt;(0 + SamplingMonth | UnitID)&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;You mention that different FuelTypes would be expected to respond to seasonal changes differently. Right now you're modelling each individual stand as responding to seasonal changes differently. If you just wanted to model by FuelType, I think I would run something like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lmer(log(FMC) ~ Treatment + FuelType + (0 + SamplingMonth | FuelType) + (1 | UnitID), data=moisture)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Where you're specifying that you want a fixed effect of Treatment and FuelType, you want the effect of SamplingMonth to have different slopes in each FuelType, and you're modelling random intercepts for each UnitID. We don't model random intercepts for FuelType (thus the zero) because I assume from youre description that these are fixed categories and shouldn't be modeled as random representatives of an unknown distribution of Fuel Types. Hopefully that helps.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Update&lt;/em&gt;&lt;/strong&gt; The model I posted above was a bit off. I ran the model below, which I believe is correct, on the actual data to no warnings - assuming MOISTURECONTENT is your outcome variable. Here we treat SamplingMonth as a random factor that has both a random intercept and random slopes of FuelType, which models the expected variance in seasonal changes across different FuelTypes.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;summary(lmer(log(MOISTURECONTENT) ~ Treatment + FUELTYPE + (1 + FUELTYPE| SamplingMonth) + (1 |UNITID), data=moisture))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The results of this model indicate that there is substantially more variance in the moisture of the Litter Fueltype across months than the other FuelTypes. Note that this does treat months as being random factors, which is possibly not appropriate for your data if you expect there to be fixed effects of seasonal changes. But if you can assume these months are drawn from a random distribution of possible months within the season you tested, it should be sound. If you're concerned that that's not the case, you could potentially add a season factor (e.g. Autumn, Summer, Winter) and treat months within seasons as random factors.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you run the code below, you'll be able to see the variation in the monthly effects by FuelType:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;forestmodel &amp;lt;- lmer(log(MOISTURECONTENT) ~ Treatment + FUELTYPE + (1 + FUELTYPE| SamplingMonth) + (1 |UNITID), data=moisture)&#10;require(lattice)&#10;dotplot(ranef(forestmodel))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;If it's of interest, adding an interaction term between the Treatment and FUELTYPE variables reveals that the Treatment seems to have most of its effect at the 1000hour and Litter Fueltypes. Not sure if that's relevant, but I found it interesting.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;summary(lmer(log(MOISTURECONTENT) ~ Treatment*FUELTYPE + (1  + FUELTYPE| SamplingMonth) + (1 |UNITID), data=moisture))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="7" CreationDate="2014-10-12T09:52:45.223" Id="119750" LastActivityDate="2014-10-14T01:36:46.220" LastEditDate="2014-10-14T01:36:46.220" LastEditorUserId="46273" OwnerUserId="46273" ParentId="119728" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I have the following probability distribution (&lt;code&gt;-b&lt;/code&gt; output of &lt;code&gt;libsvm&lt;/code&gt;):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; 1 1:0.6 2:0.2 3:0.1 4:0.05 5:0.05 ...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Can I trace the median or something, and not show a discrete class (as in the example)? &lt;/p&gt;&#10;&#10;&lt;p&gt;Looking at the example, the answer should be something between 1 and 2 (not a discrete 1), closer to 1, but what value?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-12T12:46:35.337" Id="119755" LastActivityDate="2014-10-12T16:46:17.540" LastEditDate="2014-10-12T16:46:17.540" LastEditorUserId="805" OwnerUserId="46306" PostTypeId="1" Score="0" Tags="&lt;svm&gt;" Title="SVM Multiclass probability distribution" ViewCount="16" />
  <row Body="&lt;p&gt;$Y$ is a random variable whose &lt;em&gt;density&lt;/em&gt; $f_i(y)$ when $H_i$ is the true hypothesis&#10;is given by&#10;$$\begin{align}&#10;f_0(y) &amp;amp;= \begin{cases}1-|y|, &amp;amp; -1 &amp;lt; y &amp;lt; 1,\\0,&amp;amp;\text{otherwise,}\end{cases}\\&#10;f_1(y) &amp;amp;= \begin{cases}y, &amp;amp; 0 &amp;lt; y &amp;lt; 1,\\&#10;2-y, &amp;amp; 1 \leq y &amp;lt; 2,\\0,&amp;amp;\text{otherwise,}\end{cases}&#10;\end{align}$$&#10;making the likelihood ratio &#10;$$\Lambda(y) = \frac{f_1(y)}{f_0(y)}&#10;= \begin{cases}0, &amp;amp; -1 &amp;lt; y &amp;lt; 0,\\&#10;\frac{1-y}{y}, &amp;amp; 0 &amp;lt; y &amp;lt; 1,\\&#10;\infty, &amp;amp; 1 &amp;lt; y &amp;lt; 2,\\&#10;\text{undefined}, &amp;amp; \text{otherwise.}\end{cases}&#10;$$&#10;More to the point, when $H_0$ is the true hypothesis, &lt;em&gt;all&lt;/em&gt; the observations $y_i$&#10;necessarily are in the interval $(-1,1)$, and if at least one of them is negative,&#10;the &lt;em&gt;decision&lt;/em&gt; is that $H_0$ is indeed the true hypothesis (with no possibility&#10;of a false alarm or making a Type I error and no need to think about $p$-values&#10;or similar things dear to the heart of the hypothesis-tester).&#10;Similarly, when $H_1$ is the true hypothesis, &lt;em&gt;all&lt;/em&gt; the observations $y_i$&#10;necessarily are in the interval $(0,2)$, and if at least one of them exceeds $1$,&#10;the &lt;em&gt;decision&lt;/em&gt; is that $H_1$ is indeed the true hypothesis (with no possibility&#10;of a false dismissal or making a Type II error and no need to think about $p$-values).  It is &lt;em&gt;only&lt;/em&gt; in the case when &lt;em&gt;all&lt;/em&gt; the observations $y_i$&#10;are in the interval $(0,1)$ that we need to consider the likelihood ratio&#10;or the log-likelihood ratio, and there exists the possibility that we might&#10;make a Type I or Type II error. In other cases, we have an instance of&#10;what some people call &lt;em&gt;singular&lt;/em&gt; detection: there is no possibility that&#10;the decision is incorrect.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-12T14:56:41.787" Id="119763" LastActivityDate="2014-10-12T14:56:41.787" OwnerUserId="6633" ParentId="119732" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="119774" AnswerCount="1" Body="&lt;p&gt;I'm trying to use KS test to determine whether one group of data is scholastically dominates another. So I'm studying dataset regarding performance of companies, which are divided into 2 groups. Instead of comparing mean values for this two groups, I follow [1] and want to compare distributions using KS test (Table 3). They do two tests: one sided (A less then B) and two sided (equality). For that I use STATA's ksmirnov command, the problem is how to interpret the output. It return D and p but what one can conclude from these values is not clear for me. For instance, for my groups it returns:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;. ksmirnov performance, by(myGroup)&#10;Two-sample Kolmogorov-Smirnov test for equality of distribution functions&#10;&#10; Smaller group       D       P-value  Corrected&#10; ----------------------------------------------&#10; 0:                  0.0047    0.972&#10; 1:                 -0.1635    0.000&#10; Combined K-S:       0.1635    0.000      0.000&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The 0 is checking hypothesis that group0 has smaller values then group1. The 1 for hypothesis that group0 has larger values then group1. But I do not understand how to interpret D and p. What is the unit of D and is it big enough to accept hypothesis (for instance, for the confidence 0.05)?&lt;/p&gt;&#10;&#10;&lt;p&gt;[1] &lt;a href=&quot;http://www.etsg.org/ETSG2012/Programme/Papers/329.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.etsg.org/ETSG2012/Programme/Papers/329.pdf&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-10-12T15:21:13.960" Id="119765" LastActivityDate="2014-10-12T18:11:35.617" LastEditDate="2014-10-12T15:35:35.750" LastEditorUserId="58467" OwnerUserId="58467" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;&lt;stata&gt;&lt;kolmogorov-smirnov&gt;" Title="Two sample Kolmogorov-Smirnov test for the stochastic dominance" ViewCount="466" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm doing a study on visual selective attention (VSA) and how this is influenced by gender and video gaming. So my dependent variable is VSA (continuous) and my two independent variables are gender and video gaming (both categorical). &lt;/p&gt;&#10;&#10;&lt;p&gt;I want to see if these two variables affect VSA but also test for interaction effects between gender and video gaming. For this I should use a two-way between-groups ANOVA. The problem is that my data is not normally distributed.&lt;/p&gt;&#10;&#10;&lt;p&gt;I've read on this forum that ANOVA's are &quot;robust&quot; tests with a large sample (mine is 120) so I can do this test anyway, while other authors argue against it.&lt;/p&gt;&#10;&#10;&lt;p&gt;What should I do? Is there a non-parametric test I can do that still tests interaction as well. Or is there perhaps a totally different test I could do that will give the same result?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-10-12T19:55:56.967" Id="119788" LastActivityDate="2014-10-12T20:02:08.733" LastEditDate="2014-10-12T20:02:08.733" LastEditorUserId="7290" OwnerUserId="58482" PostTypeId="1" Score="0" Tags="&lt;anova&gt;&lt;nonparametric&gt;&lt;robust&gt;" Title="Two-way between groups ANOVA for non-normal data" ViewCount="51" />
  <row AnswerCount="2" Body="&lt;p&gt;I have two OLS regression models (in Stata):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;(1) Y = a + b_1 X_1 + b_2 X_2 + fixed effects + e&#10;(2) Y = a + b_1 X_1 + b_2 X_2 + b_3 (X_1 * X_2) + fixed effects + e&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In model 1, only b_2 is significant.&#10;In model 2, b_1 and b_3 are weakly significant.&lt;/p&gt;&#10;&#10;&lt;p&gt;What test can I do to see if model 2 is a &quot;more proper&quot; model than model 1?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-12T22:53:07.947" FavoriteCount="1" Id="119804" LastActivityDate="2014-10-13T08:15:08.833" OwnerUserId="48529" PostTypeId="1" Score="1" Tags="&lt;stata&gt;&lt;model-comparison&gt;" Title="How can I compare two regression models?" ViewCount="681" />
  <row AcceptedAnswerId="119811" AnswerCount="1" Body="&lt;p&gt;Is there a symbol to indicate that variables have been standardized?&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, if I have 2 different scoring functions &lt;strong&gt;&lt;em&gt;Score1&lt;/em&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;em&gt;Score2&lt;/em&gt;&lt;/strong&gt;. Let's say I want to form a combo score and show that the scores have been standardized, e.g., &lt;/p&gt;&#10;&#10;&lt;p&gt;\[ \text{ComboScore1} = \frac{\text{std. Score1} + \text{std. Score2}}{2} \]&lt;/p&gt;&#10;&#10;&lt;p&gt;two differentiate from a flavor of a ComboScore where the individual scores have not been standardized&lt;/p&gt;&#10;&#10;&lt;p&gt;\[ \text{ComboScore2} = \frac{\text{Score1} + \text{Score2}}{2} \]&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-10-12T23:32:21.920" Id="119810" LastActivityDate="2014-10-13T02:51:46.367" LastEditDate="2014-10-13T02:47:53.977" LastEditorUserId="39663" OwnerUserId="39663" PostTypeId="1" Score="2" Tags="&lt;normalization&gt;&lt;standardization&gt;&lt;notation&gt;" Title="Symbol to indicate normalized or standardized variables" ViewCount="32" />
  
  <row AnswerCount="0" Body="&lt;pre&gt;&lt;code&gt;model=ugarchspec(variance.model=list(model=&quot;sGARCH&quot;,garchOrder=c(1,1)),mean.model=list(armaOrder=c(1,1),include.mean=TRUE),distribution.model='std')&#10;modelfit=ugarchfit(spec=model,r,solver=&quot;hybrid&quot;,out.sample=500)&#10;coef(modelfit)&#10;modelfor=ugarchforecast(modelfit,data=NULL,n.ahead=1,n.roll=200,external.forecasts=list(mreg    for=NULL, vregfor=NULL))&#10;var=quantile(modelfor,0.05)&#10;plot(modelfor,HFRI$Date,which=3)&#10;print(var)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This code is in R.&#10;The Monthly VaR I get using this model is far less then what I get through Historical and Parametric methods. I get -0.06% from GARCH while I get -1.5% and -1.3% form parametric and historical respectively. Can anyone throw some light on the reason why this can happen. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-10-12T23:45:35.033" Id="119812" LastActivityDate="2014-10-12T23:45:35.033" OwnerUserId="58474" PostTypeId="1" Score="0" Tags="&lt;forecasting&gt;&lt;garch&gt;&lt;var&gt;" Title="VaR using GARCH" ViewCount="27" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;Let $X_1,\dots,X_n$ are i.i.d normal $N(\mu,\sigma^2).$ Suppose that we only observe&#10;$$&#10;X_1+X_2,\dots,X_1+X_n,\dots,X_{n-1}+X_n,&#10;$$&#10;i.e, $X_i+X_j$ for all $i&amp;lt;j.$&lt;/p&gt;&#10;&#10;&lt;p&gt;I wish to find the best estimator $\mu$ and $\sigma^2$ based on the above abservation. How will I do that?&lt;/p&gt;&#10;&#10;&lt;p&gt;As we know that if we observe $X$'s then it is easy to estimate $\mu$ and $\sigma^2$. However, since we observe ${n\choose 2}-$identically variables $X_i+X_j$ that are not independent one to the each other, then it would become more difficult. Any suggestion?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-10-13T07:52:56.300" Id="119843" LastActivityDate="2014-10-13T09:16:17.063" OwnerUserId="40104" PostTypeId="1" Score="1" Tags="&lt;normal-distribution&gt;&lt;estimation&gt;&lt;non-independent&gt;" Title="Estimation based on observing sum of two variables" ViewCount="63" />
  
  <row AnswerCount="1" Body="&lt;p&gt;There are a lot of datasets freely available across the web (see CV questions &lt;a href=&quot;http://stats.stackexchange.com/questions/145/free-dataset-resources&quot;&gt;here&lt;/a&gt;,  &lt;a href=&quot;http://stats.stackexchange.com/questions/7/locating-freely-available-data-samples&quot;&gt;here&lt;/a&gt;, or &lt;a href=&quot;http://cogsci.stackexchange.com/questions/4354/which-r-packages-have-good-collections-of-psychology-datasets&quot;&gt;here&lt;/a&gt;). Yet, I want the reverse: I am looking for &lt;strong&gt;a list of publications&lt;/strong&gt; in the social sciences or psychology that are &lt;strong&gt;based on freely available datasets&lt;/strong&gt;. My goal is to use them in stats classes and have student redo the analysis themselves. I think its a particularly nice learning effect to see that the things we do in class are the same things that are done in the real world.&lt;/p&gt;&#10;&#10;&lt;p&gt;So my question: Do you know publications in this field using freely available data sources?&#10;The statistical method is secondary. All methods are welcome.&lt;/p&gt;&#10;" CommentCount="1" CommunityOwnedDate="2014-10-13T16:59:16.537" CreationDate="2014-10-13T09:16:09.293" FavoriteCount="1" Id="119858" LastActivityDate="2014-10-13T09:38:08.827" OwnerUserId="7952" PostTypeId="1" Score="2" Tags="&lt;dataset&gt;&lt;psychometrics&gt;&lt;psychology&gt;&lt;reproducible-research&gt;&lt;social-science&gt;" Title="Publications in the social sciences or psychology based on freely available datasets?" ViewCount="36" />
  
  <row AcceptedAnswerId="119884" AnswerCount="1" Body="&lt;p&gt;I'm running flat clustering algorithm on my dataset that contains numeric (not categorical) data. &lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a method that can give me interpretation of clusters, and emphasize what are the most important variables and their range of values in cluster?&lt;/p&gt;&#10;&#10;&lt;p&gt;Something like - This cluster contains mostly elements for which Property XY has small values, and Property YZ has great values?&lt;/p&gt;&#10;&#10;&lt;p&gt;Or do I need post processing, like count elements in cluster that have value above certain threshold and something similar?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-10-13T12:02:36.460" Id="119874" LastActivityDate="2014-10-13T19:13:29.350" LastEditDate="2014-10-13T18:58:39.530" LastEditorUserId="54659" OwnerUserId="54659" PostTypeId="1" Score="1" Tags="&lt;clustering&gt;" Title="Cluster interpretation" ViewCount="61" />
  <row Body="&lt;p&gt;Your question is a little bit general, so I'll give a general answer.&lt;/p&gt;&#10;&#10;&lt;p&gt;$P(A|B) = \frac{P(A, B)}{P(B)} =  \frac{P(B|A)P(A)}{P(B)} = \frac{P(B|A)P(A)}{\int{P(B|A)P(A)dA}}$&lt;/p&gt;&#10;&#10;&lt;p&gt;So $P(B)$ is an integral over all possible data values ($B$), given all possible priors ($A$).&lt;/p&gt;&#10;&#10;&lt;p&gt;Maybe you'll find the diagram below helpful. It shows in a graphical way Bayes theorem by parts. It is a simplified example, where we have only two variables, of two dimensions each. As you can see, $P(A)$ is &quot;all cases where $A$ is true&quot; ($w$ and $x$) and $P(B)$ is &quot;all cases where $B$ is true&quot; ($w$ and $y$), and $P(A|B)$ is &quot;both $A$ and $B$ are true&quot;. However, as you can also see from the diagram, in $P(A|B) = P(B|A)P(A)/P(B)$, the $P(B)$ part stands also for &quot;all the possible values&quot; (i.e. $w, x, y, z$ on the diagram). It is a little bit ambiguous with Bayes theorem, but should be remembered. I hope this clarifies things a little bit.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/cZkYP.png&quot; alt=&quot;Wikipedia graph on Bayes theorem&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;You could also find helpful introductory chapters of books by &lt;a href=&quot;http://books.google.pl/books?id=ZXL6AQAAQBAJ&amp;amp;lpg=PP1&amp;amp;dq=bayesian%20data%20analysis&amp;amp;hl=pl&amp;amp;pg=PA6#v=onepage&amp;amp;q=bayesian%20data%20analysis&amp;amp;f=false&quot; rel=&quot;nofollow&quot;&gt;Gelman&lt;/a&gt; and &lt;a href=&quot;http://books.google.pl/books?id=ZRMJ-CebFm4C&amp;amp;lpg=PP1&amp;amp;dq=kruschke%20bayesian&amp;amp;hl=pl&amp;amp;pg=PA51#v=onepage&amp;amp;q=bayes%20formula&amp;amp;f=false&quot; rel=&quot;nofollow&quot;&gt;Krushke&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-10-13T12:27:31.123" Id="119877" LastActivityDate="2014-10-14T06:47:38.380" LastEditDate="2014-10-14T06:47:38.380" LastEditorUserId="35989" OwnerUserId="35989" ParentId="119849" PostTypeId="2" Score="2" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm trying to do a text classification task. &lt;/p&gt;&#10;&#10;&lt;p&gt;Here are some specs:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Context file size = 1M+ documents already labeled&lt;/li&gt;&#10;&lt;li&gt;Number of top-labels = 17&lt;/li&gt;&#10;&lt;li&gt;Number of sub-labels = around 130&lt;/li&gt;&#10;&lt;li&gt;Each document is constituted of: a small text representing some&#10;retailer's client feedback (about 15-20 words in average) + a number&#10;of topics related to the text of the feedback + the sub-label and&#10;top-label it belongs to.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I'm using Scikit Learn. For the moment I've tried several things:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;different vectorizers: CountVectorizer (count occurences) or&#10;TfidfVectorizer (compute the tfidf)&lt;/li&gt;&#10;&lt;li&gt;different tokenizers: unigrams, bigrams, trigrams&lt;/li&gt;&#10;&lt;li&gt;various algorithms: SVM, Logistic Regression, Multinomial Naives Bayes, Random Forest.&lt;/li&gt;&#10;&lt;li&gt;cascade of classifiers using the hierarchy in the data.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I can't get past 0.57 in accuracy rate (SVM with L1 norm by training on 500k documents) with those parameters... I'm always around 0.5 with those different configurations. And it doesn't really improve after 100k documents.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm trying to do some error analysis. I've computed a confusion matrix only with top-labels (test file size = 2k docs, algorithm = SVM):&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Cc1mv.png&quot; alt=&quot;Confusion Matrix for the top-labels classifier&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;top-label : (precision    recall  f1-score   support)&lt;/li&gt;&#10;&lt;li&gt;TL1      0.57      0.25      0.35        16&lt;/li&gt;&#10;&lt;li&gt;TL2       0.00      0.00      0.00         1 &lt;/li&gt;&#10;&lt;li&gt;TL3     0.57      0.55      0.56       258 &lt;/li&gt;&#10;&lt;li&gt;TL4       0.61      0.47      0.53       277 &lt;/li&gt;&#10;&lt;li&gt;TL5       0.46      0.41      0.43        27 &lt;/li&gt;&#10;&lt;li&gt;TL6       0.61      0.37      0.46        38 &lt;/li&gt;&#10;&lt;li&gt;TL7       0.69      0.31      0.43        35 &lt;/li&gt;&#10;&lt;li&gt;TL8       0.84      0.84      0.84       130 &lt;/li&gt;&#10;&lt;li&gt;TL9       0.50      0.06      0.11        31 &lt;/li&gt;&#10;&lt;li&gt;TL10       0.71      0.63      0.67       111 &lt;/li&gt;&#10;&lt;li&gt;TL11       0.64      0.34      0.45       143&lt;/li&gt;&#10;&lt;li&gt;TL12      0.73      0.93      0.82       815 &lt;/li&gt;&#10;&lt;li&gt;TL13       1.00      0.17      0.29        12 &lt;/li&gt;&#10;&lt;li&gt;TL14       0.72      0.72      0.72        32 &lt;/li&gt;&#10;&lt;li&gt;TL15       0.47      0.14   0.22    56 &lt;/li&gt;&#10;&lt;li&gt;TL16       0.51      0.88      0.65        81 &lt;/li&gt;&#10;&lt;li&gt;TL17       0.80      0.86      0.83        14&lt;/li&gt;&#10;&lt;li&gt;avg / total       0.67      0.68      0.65      2077&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;As you can see, one top-label creates a lot of confusion. It is the top-label &quot;product&quot; (TL12) which as you can imagine is really frequent in the context file (about 50% of the corpus). Some other top-labels that are also semantically related confuse each other a lot.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, when I'm checking the documents that were not classified well, I realize that the classifier often gets it wrong even though a very particular word is appearing in the document such as 'satisfied client' and 'thank you' in a document that should have been classified as 'congratulations, thanks'. I don't really understand why. Too much noise?&lt;/p&gt;&#10;&#10;&lt;p&gt;Do you have any advice? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-13T14:32:10.047" Id="119890" LastActivityDate="2014-10-13T14:39:09.747" LastEditDate="2014-10-13T14:39:09.747" LastEditorUserId="7290" OwnerUserId="58535" PostTypeId="1" Score="1" Tags="&lt;classification&gt;&lt;python&gt;&lt;text-mining&gt;&lt;unbalanced-classes&gt;&lt;natural-language&gt;" Title="Any advice on how to improve my accuracy rate in text classification?" ViewCount="116" />
  
  <row Body="&lt;p&gt;I would say that the answer should not come from the data, but rather from the science producing the data, the question to be answered, and the audience that you are trying to answer the question for.&lt;/p&gt;&#10;&#10;&lt;p&gt;Think about the science that underlies the data/question.  Do the discontinuities in the Random forest model make sense?  are you allowing enough interaction and non-linearity in the regression model?&lt;/p&gt;&#10;&#10;&lt;p&gt;What will your audience best understand? (random forests can sometimes be converted into a simple lookup table without needing to do any math, regression models can be more straight forward for future computation)&lt;/p&gt;&#10;&#10;&lt;p&gt;These and other questions outside of the data should drive the general form of the model, then the data can help determine the specifics (coefficients, cut-points) of the general model.  If one of the general forms is clearly superior, then pretty much any measure should show which is superior (and that should match with the science as well).  If you need a specific measure to decide, and different measures are likely to give different results, then a small change in the data would likely make a difference as well.  Would you be happy with a data driven decision that would have been different had one single data point not been collected?  Knowledge outside of the data will be consistent and better for these choices.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-13T20:29:05.197" Id="119927" LastActivityDate="2014-10-13T20:29:05.197" OwnerUserId="4505" ParentId="100281" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;Wilcoxon signed-rank test would be appropriate to compare matched (by evaluator) ratings. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-13T22:46:26.120" Id="119939" LastActivityDate="2014-10-13T22:46:26.120" OwnerUserId="57390" ParentId="119930" PostTypeId="2" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;Long time Lurker, first time asking.&lt;/p&gt;&#10;&#10;&lt;p&gt;For a research paper, i'm required to optimize some parameters of a certain function using an asymmetrical loss function, specifically LINEX and compare it to the LS version of it.&lt;/p&gt;&#10;&#10;&lt;p&gt;Usually i would just fire eviews and do this, but this equation is not included in the package.Frankly the main problem is that even if i understand the math behind it, i'm relatively new to statistical software and packages, so i don't really know how to implement it.&lt;/p&gt;&#10;&#10;&lt;p&gt;So the question is: How would you do this? Is there a software (R, MATLAB, ETC) that has LINEX already in it like Eviews has LS? &lt;/p&gt;&#10;&#10;&lt;p&gt;I know this is probably more of a tech issue, so i don't know if this is the correct place. All help is really appreciated&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-13T23:22:33.667" Id="119942" LastActivityDate="2014-10-13T23:22:33.667" OwnerUserId="58569" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;matlab&gt;&lt;loss-functions&gt;&lt;eviews&gt;" Title="How to do optimization using asymmetrical loss functions (LINEX)" ViewCount="44" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;How to check the normality of data collected on 5 point Likert scale? &lt;/p&gt;&#10;&#10;&lt;p&gt;As it is ordinal numbers not continuous. Using SPSS the Shapiro Wilk or Kolmogorov-Smirnov test indicate my data is not normal.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am validating the existing frameworks so most of the values (mean) are between 3.5- 4.5 &lt;/p&gt;&#10;&#10;&lt;p&gt;Can I go ahead with factor analysis? If yes then are there any constraints for the same?&lt;/p&gt;&#10;" ClosedDate="2014-10-14T19:54:35.320" CommentCount="4" CreationDate="2014-10-14T11:27:12.263" Id="119980" LastActivityDate="2014-10-14T13:21:20.460" LastEditDate="2014-10-14T11:41:21.303" LastEditorUserId="805" OwnerUserId="58596" PostTypeId="1" Score="0" Tags="&lt;factor-analysis&gt;&lt;ordinal&gt;&lt;likert&gt;" Title="Factor analysis on non normal data ( Ordinal data of Likert Scale)" ViewCount="138" />
  <row AnswerCount="0" Body="&lt;p&gt;I have features $X_t$ and response $Y_t$ (all continuous variables) and my objective is to find the best estimate of $f(X_t)=Y_t$ where $f$ is linear, and 'best' is defined as lowest generalisation error on unseen data. This is the single objective; I don't care about standard error properties or the properties of individual slope coefficients, I only care about predictive accuracy.&lt;/p&gt;&#10;&#10;&lt;p&gt;My data has the two properties:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;$Y_t$ is heavily autocorrelated. First-order autocorrelation could be anywhere between 20% and 80%.&lt;/li&gt;&#10;&lt;li&gt;Some of the features are heavily autocorrelated, ranging from 5% to 98% first order autocorrelation.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Given the data properties and my objective, what is the best regularised model I could use?  Ridge/Lasso/Elastic Net, non-negative garotte, PCA regression,...?&lt;/p&gt;&#10;&#10;&lt;p&gt;I have searched for a study on this but have been unsuccessful in finding something good yet.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-10-14T12:25:39.867" Id="119984" LastActivityDate="2014-10-14T12:25:39.867" OwnerUserId="58598" PostTypeId="1" Score="0" Tags="&lt;time-series&gt;&lt;machine-learning&gt;&lt;predictive-models&gt;&lt;linear-model&gt;&lt;autocorrelation&gt;" Title="How to improve linear model generalization when autocorrelation is present?" ViewCount="36" />
  <row Body="&lt;p&gt;You can use a Chi-squared test of independence in this case. &lt;/p&gt;&#10;&#10;&lt;p&gt;I think it will be much easier to apply than the (slightly overkill) Jensen-Shannon divergence. &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-10-14T12:43:49.507" Id="119985" LastActivityDate="2014-10-14T12:43:49.507" OwnerUserId="55410" ParentId="119982" PostTypeId="2" Score="2" />
  
  <row AnswerCount="0" Body="&lt;p&gt;What is the cumulative distribution function of the non-standardized student's t distribution in terms of inverse scaling parameter? I have found a number of related equations online, but not this one specifically. According to Wikipedia, the PDF in this form is:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;p\left(x \middle| v, \mu, \lambda\right) = \frac{\Gamma\left(\frac{v+1}{2}\right)}{\Gamma\left(\frac{v}{2}\right)}\left(\frac{\lambda}{\pi{}v}\right)^{\frac{1}{2}}\left(1+\frac{\lambda\left(x-\mu\right)^2}{v}\right)^{-\frac{v+1}{2}}&#10;$$&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-10-14T13:13:39.853" Id="119990" LastActivityDate="2014-10-14T16:03:58.880" LastEditDate="2014-10-14T16:03:58.880" LastEditorUserId="52379" OwnerUserId="52379" PostTypeId="1" Score="0" Tags="&lt;cdf&gt;&lt;students-t&gt;" Title="CDF on non-standardized t distribution" ViewCount="41" />
  
  
  
  <row Body="&lt;p&gt;Hmm, so I think I figured out the issues I was having with segmented. It had to do with the weight statement (it doesn't work to weight the lm and the segmented model). &lt;/p&gt;&#10;&#10;&lt;p&gt;Segmented seems like the best bet for me. &#10;It does a good job estimating breakpoints, even with my short datasets. It isn't terribly difficult to constrain the second slope to 0, and it has a built-in test for the significance of the change in slope. If anyone feels like explaining the difficulty with estimating error around a breakpoint estimate, I'm all ears!&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(segmented)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;h3&gt;with second segment not constrained to 0&lt;/h3&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;out.lm &amp;lt;- lm(y~x, data=dat)&#10;o&amp;lt;-segmented(out.lm, seg.Z= ~x, weights=wt, psi=10)&#10;with(dat, plot(x,y, ylim=c(0, max(y)), pch=16, cex=wt/(13), main=&quot;segmented&quot;))&#10;lines(x=dat$x, y=fitted(o), col=&quot;blue&quot;)&#10;lines.segmented(o, col=&quot;blue&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;h3&gt;fix from vito to fix slope to 0&lt;/h3&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;o2&amp;lt;-lm(y~1)&#10;xx&amp;lt;- -x&#10;o3&amp;lt;-segmented(o2,seg.Z=~xx, weights=wt, psi=list(xx=-4))&#10;&#10;points(x,fitted(o3),col=&quot;green&quot;)&#10;ci&amp;lt;-confint(o3, rev.sgn=TRUE)&#10;lines.segmented(o3, col=&quot;green&quot;, rev.sgn=TRUE, lwd=2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;h3&gt;test for slope change&lt;/h3&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;davies.test(o3,~xx)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-10-14T16:00:33.527" Id="120010" LastActivityDate="2014-10-14T16:00:33.527" OwnerUserId="57327" ParentId="118598" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="120040" AnswerCount="1" Body="&lt;p&gt;Given two measurements of a variable $x$:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\tilde{y_1}=x+e_1$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\tilde{y_2}=x+e_2$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $e_1,e_2$ are zero-mean random variables following a bivariate normal distribution, with a &lt;em&gt;known&lt;/em&gt; joint probability density function $p(e_1,e_2)$. How can we find $p(\tilde{y}|x)$?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-10-14T19:14:03.287" FavoriteCount="1" Id="120034" LastActivityDate="2014-10-14T20:05:20.377" LastEditDate="2014-10-14T19:36:03.853" LastEditorUserId="49185" OwnerUserId="49185" PostTypeId="1" Score="0" Tags="&lt;normal-distribution&gt;&lt;estimation&gt;&lt;bivariate&gt;" Title="Finding $p(\tilde{y}|x)$ given measurement model and error distribution" ViewCount="33" />
  
  <row Body="&lt;p&gt;Given $x$, you know the joint density of $[e_1,e_2]$, which is also multivariate normal, with means $\mu_i+x$ for $i=1,2$ respectively. And that's about it, the density of $Y=[\tilde{y}_1,\tilde{y}_2]^T$ given $x$ will be the aforementioned Gaussian density: &#10;$$ f_{Y|x} = C\exp\left(-\frac{1}{2}(Y-\mu)^TM^{-1}(Y-\mu)\right),$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\mu$ is the mean vector (given $x$), and $M$ is the covariance matrix, $cov(e_1,e_2)$, which is unchanged by different means.&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2014-10-14T20:05:20.377" Id="120040" LastActivityDate="2014-10-14T20:05:20.377" OwnerUserId="40946" ParentId="120034" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Here are my two cents from an advanced data analysis course I took while studying biostatistics (although I don't have any references other than my professor's notes):&lt;/p&gt;&#10;&#10;&lt;p&gt;It boils down to whether or not you need to address linearity and heteroscedasticity (unequal variances) in your data, or just linearity.&lt;/p&gt;&#10;&#10;&lt;p&gt;She notes that transforming the data affects both the linearity and variance assumptions of a model. For example, if your residuals exhibit issues with both, you could consider transforming the data, which potentially could fix both. The transformation transforms the errors and thus their variance.&lt;/p&gt;&#10;&#10;&lt;p&gt;In contrast, using the link function only affects the linearity assumption, not the variance. The log is taken of the mean (expected value), and thus the variance of the residuals is not affected.&lt;/p&gt;&#10;&#10;&lt;p&gt;In summary, if you don't have an issue with non-constant variance, she suggests using the link function over transformation, because you don't &lt;em&gt;want&lt;/em&gt; to change your variance in that case (you're already meeting the assumption).&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-10-14T22:40:06.827" Id="120048" LastActivityDate="2014-10-14T23:23:26.093" LastEditDate="2014-10-14T23:23:26.093" LastEditorUserId="30488" OwnerUserId="30488" ParentId="47840" PostTypeId="2" Score="3" />
  
  
  <row AcceptedAnswerId="120095" AnswerCount="2" Body="&lt;p&gt;I have seen a term &quot;permutation invariant&quot; version of the MNIST digit recognition task. What does it mean?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-15T08:16:15.990" Id="120089" LastActivityDate="2014-10-15T08:52:54.080" OwnerUserId="41749" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;neural-networks&gt;&lt;permutation&gt;" Title="What does &quot;permutation invariant&quot; mean?" ViewCount="59" />
  <row Body="&lt;p&gt;You should use resid(fit.glmer,type=&quot;response&quot;) since default type is &quot;deviance&quot; while type of y.glmer is &quot;response&quot;. I bet this will change pretty much your binnedplot figure of residuals against the predicted values with random part.&#10;See also: &lt;a href=&quot;http://stats.stackexchange.com/questions/99274/interpreting-a-binned-residual-plot-in-logistic-regression&quot;&gt;Interpreting a binned residual plot in logistic regression&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-15T08:44:43.317" Id="120094" LastActivityDate="2014-10-15T08:44:43.317" OwnerUserId="58650" ParentId="89991" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="120105" AnswerCount="2" Body="&lt;p&gt;I have a set of 500 points in 5D. Each point belongs to one of five classes, and the class labels are known.&lt;/p&gt;&#10;&#10;&lt;p&gt;I’d like to visualise the dataset in 2D such that the classes would be separated as much as possible.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am currently using PCA and doing a scatterplot of the first two principal components. This works quite well for some datasets, but not as well for others. This makes intuitive sense, since PCA maximises explained variance rather than separability.&lt;/p&gt;&#10;&#10;&lt;p&gt;Are there any known methods for finding a 2D projection that would maximise separability? I don’t have any specific measure in mind and am open to suggestions.&lt;/p&gt;&#10;&#10;&lt;p&gt;(Tagging with &lt;code&gt;[r]&lt;/code&gt; as I'd love to see some R code or pointers.)&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-10-15T09:40:24.433" FavoriteCount="3" Id="120102" LastActivityDate="2014-10-15T14:53:08.583" OwnerUserId="439" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;pca&gt;&lt;separability&gt;&lt;projection&gt;" Title="2D projection to maximise separability" ViewCount="227" />
  <row AcceptedAnswerId="120349" AnswerCount="1" Body="&lt;p&gt;I'm doing biological gene expression data. For the genes I analysed, each gene can produce two isoforms, one dominant (Gene isoform 1) one minus can be treated as a by-product (Gene isoform 2).  &lt;/p&gt;&#10;&#10;&lt;p&gt;I'm actually interested in is the expression change of the by-product following time series (Gene isoform 2). However, the problem is, the expression of &quot;Gene isoform 1&quot; is a confounder here. i.e. I want the variance of &quot;isoform 2&quot; expression is driven by itself, not driven by the variance of the dominant &quot;isoform 1&quot;. &lt;/p&gt;&#10;&#10;&lt;p&gt;Two graph to explain my problem:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Situation one:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;Gene isoform 1&quot; and &quot;Gene isoform 2&quot; both increase their expression over time, however they vary synchronously, it is likely the variation of Gene isoform 2 is due to the caused by the variation of &quot;Gene isoform 1&quot;. &lt;strong&gt;This is not what I want!&lt;/strong&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/xmZb5.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Situation 2:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;Gene isoform 1&quot; and &quot;Gene isoform 2&quot; both increase their expression over time, the increase of &quot;Gene isoform 2&quot; is much faster than &quot;Gene isoform 1&quot;, this means at least partially &quot;Gene isoform 2&quot; vary independent of &quot;Gene isoform 1&quot;, and this independent part of variance is exactly what I'm interested in.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/abYv9.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;The question is, how do I test the change of &quot;Gene isoform 2&quot; without the influence of &quot;Gene isoform 1&quot;.&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Thanks a tons for any answer!&lt;/strong&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-15T10:34:35.227" Id="120106" LastActivityDate="2014-10-20T16:50:01.413" LastEditDate="2014-10-15T12:20:20.987" LastEditorUserId="56698" OwnerUserId="56698" PostTypeId="1" Score="0" Tags="&lt;hypothesis-testing&gt;&lt;variance&gt;" Title="Variance decomposition time series hypothesis testing" ViewCount="50" />
  
  
  <row Body="&lt;p&gt;In statistics, $$A\oplus B:=\left[\begin{array}{cc}A &amp;amp; 0 \\ 0 &amp;amp; B \end{array} \right]$$ and (e.g. for a $2\times 2$-matrix $A$)&#10;$$A\otimes B:=\left[\begin{array}{cc}a_{11}B &amp;amp; a_{12}B \\ a_{21}B &amp;amp; a_{22}B \end{array} \right].$$&lt;/p&gt;&#10;&#10;&lt;p&gt;This focuses on matrices for their use in statistics as design or hypothesis  matrices etc., where these notations simplify the frequent block structure of such matrices. One can find the name &lt;strong&gt;Kronecker sum&lt;/strong&gt; for $\oplus$ and &lt;strong&gt;Kronecker product&lt;/strong&gt; for $\otimes$, especially in the manuals of statistical software. (Also very handy is the component wise matrix multiplication $A\#B=[a_{ij}b_{ij}]_{i,j}$ for equally shaped matrices. It's sometimes called Hadamard product.)&lt;/p&gt;&#10;&#10;&lt;p&gt;In mathematics, $\oplus$ and $\otimes$ have their slightly different typical meaning as direct sum or tensor product of vector spaces or even more general algebraic structures.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-10-15T12:02:03.027" Id="120121" LastActivityDate="2014-10-16T08:26:58.980" LastEditDate="2014-10-16T08:26:58.980" LastEditorUserId="28705" OwnerUserId="28705" ParentId="120005" PostTypeId="2" Score="7" />
  <row AnswerCount="1" Body="&lt;p&gt;I have a question regarding standardized regression coefficients and the correlation matrix. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have a two part problem that I am working on, and I need to show that the correlation matrix R is equal to:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$R = \frac{1}{N-1}X*'X$$ &lt;/p&gt;&#10;&#10;&lt;p&gt;I'm not quite sure how to set this up. I will post the problem I am working on and how far along I have gotten.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; To obtain numerically stable computation of regression coefficients it is often a good idea to first standardize all variables as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{align}&#10;y_i* &amp;amp;= \frac{y_i-\bar{y}}{s_y}  \\&#10;&amp;amp;\text{ and }  \\&#10;x_{ij}* &amp;amp;= \frac{x_{ij}-\bar{x}_j}{s_{x_j}},  \\&#10;&amp;amp;(1 \leq i \leq N, 1 \leq j \leq p)&#10;\end{align} &lt;/p&gt;&#10;&#10;&lt;p&gt;where $\bar{x}$ and $\bar{y}$ are the sample means and $s_y$ and $s_{x_j}$ are the sample standard deviations of the corresponding variables. Denote the estimated regression coefficients (called the standardized regression coefficients) for these data by $\hat{\beta_j}*$ $(0\leq j \leq p)$. Note that the $\hat{\beta_j}*$ are unitless and can be compared with each other to assess the relative importance of the $x_j$'s in their strength of linear relationship of $y$ in the presence of other predictor variables. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;a)&lt;/strong&gt; Show that $\hat{\beta}^*_0=0$. Hence we may define the model matrix $X*$ for the standardized data to be an $N \times p$ matrix by excluding the first column of $1$'s; similarly, let $\hat{\beta}^* = (\hat{\beta}_1^*,,...,\hat{\beta}_p^*)'$ to be a $p \times 1$ vector.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;b)&lt;/strong&gt; Show that the correlation matrix $R = (r_{x_jx_k})$ among the $x_j$'s and the correlation vector $ r = (r_{yx_1},...,r_{yx_p})'$ among $y$ and $x_j$ for $j = 1,2,...,p$ are given by&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{align}&#10;R &amp;amp;= \frac{1}{N-1}X*'X*$  \\&#10;  &amp;amp;\text{ and }  \\&#10;r &amp;amp;= \frac{1}{N-1}X*'y*'&#10;\end{align}&lt;/p&gt;&#10;&#10;&lt;p&gt;For part &lt;strong&gt;a&lt;/strong&gt;, &lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{align}&#10;Y_i &amp;amp;= \hat{\beta}_0^* + \hat{\beta}_1^*X_{i1}+...+\hat{\beta}_p^*X_{1p}+\hat{e}_i  \\&#10;&amp;amp; \\&#10;\frac{y_i-\bar{y}}{s_y} &amp;amp;= (\hat{\beta}_1\frac{s_1}{s_y})\frac{x_i1-\bar{x}_1}{s_1}+...+(\hat{\beta}_p\frac{s_p}{s_y})\frac{x_ip-\bar{x}_p}{s_1}  \\&#10;&amp;amp; \\&#10;\frac{y_i-\bar{y}}{s_y} &amp;amp;= (\hat{\beta}_1\frac{s_1}{s_y})\frac{x_i1-\bar{x}_1}{s_1}+...+(\hat{\beta}_p\frac{s_p}{s_y})\frac{x_ip-\bar{x}_p}{s_p}  \\&#10;&amp;amp; \\&#10;\hat{\beta}_0 &amp;amp;= \frac{y_i-\bar{y}}{s_y} - (\hat{\beta}_1\frac{s_1}{s_y})\frac{x_i1-\bar{x}_1}{s_1}+...+(\hat{\beta}_p\frac{s_p}{s_y})\frac{x_ip-\bar{x}_p}{s_p}  \\&#10;&amp;amp; \\&#10;\hat{\beta}_0 &amp;amp;= (\hat{\beta}_1\frac{s_1}{s_y})\frac{x_i1-\bar{x}_1}{s_1}+...+(\hat{\beta}_p\frac{s_p}{s_y})\frac{x_ip-\bar{x}_p}{s_p}  \\&#10;              &amp;amp;\quad -\bigg[(\hat{\beta}_1\frac{s_1}{s_y})\frac{x_i1-\bar{x}_1}{s_1}+...+(\hat{\beta}_p\frac{s_p}{s_y})\frac{x_ip-\bar{x}_p}{s_p}\bigg]&#10;\end{align}&lt;/p&gt;&#10;&#10;&lt;p&gt;Therefore, $\hat{\beta}_0 = 0$.&lt;/p&gt;&#10;&#10;&lt;p&gt;For part &lt;strong&gt;b&lt;/strong&gt;, I'm able to multiply out the matrices $ X*' X*$, but I'm not entirely sure how to interpret the meaning. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-10-15T13:34:42.597" Id="120130" LastActivityDate="2014-10-15T22:55:46.813" LastEditDate="2014-10-15T22:26:45.547" LastEditorUserId="22311" OwnerUserId="31636" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;statistical-significance&gt;&lt;mathematical-statistics&gt;&lt;experiment-design&gt;&lt;matrix&gt;" Title="Standardized regression coefficients" ViewCount="51" />
  
  <row Body="&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Stationary_process&quot; rel=&quot;nofollow&quot;&gt;Stationarity&lt;/a&gt; is a concept defined for stochastic processes. Since we look at the process as random function then we can extend the definition of stationarity to functions. I.e a stationary function is a realisation of stationary process. This extension is quite informal though. &lt;/p&gt;&#10;&#10;&lt;p&gt;Stationarity for a process means that the for a number of points $t_1,...,t_n$, $(X_{t_1},...,X_{t_n})\sim (X_{t_1+h},...,X_{t_n+h})$, where $\sim$ means equality in distribution. The question would be, what does this entail? For example take two points of the process which are close to each other $(X_t,X_{s})$. The relationship between these two points is expressed by their distribution function. Let us say the points are closely related, i.e. the correlation $corr(X_t,X_s)=0.9$. Then if we shift these points by any distance, due to stationarity their relationship remains the same, i.e. $corr(X_t,X_s)=corr(X_{t+h},X_{s+h})=0.9$. Informaly in this case we can say that $X_{s+h}$ would follow $X_{t+h}$ similar to the way $X_{s}$ follows $X_t$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Now due the way stationarity is defined this holds for any number of points. So in some sense function should look similar at different locations. Note that this depends on the distributional properties of the process. White noise is a stationary process too and it does not look similar at different points in the usual common sense.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-10-15T14:04:51.653" Id="120138" LastActivityDate="2014-10-15T14:04:51.653" OwnerUserId="2116" ParentId="120129" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="120290" AnswerCount="1" Body="&lt;p&gt;I'm working on the first model described in &lt;a href=&quot;http://www.statistica.it/gianluca/Research/BaioBlangiardo.pdf&quot; rel=&quot;nofollow&quot;&gt;this paper&lt;/a&gt; (&quot;Bayesian hierarchical model for the prediction of football [soccer] results&quot;).  The gist of the model is:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/NQP44.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The model includes two sum-to-zero constraints, on the team-level attack and defense parameters. Below is the winBUGS code verbatim from the paper.  Note that they enforce the sum-to-zero constraint by subtracting the mean.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;model {&#10;# LIKELIHOOD AND RANDOM EFFECT MODEL FOR THE SCORING PROPENSITY&#10;for (g in 1:ngames) {&#10;# Observed number of goals scored by each team&#10;        y1[g] ~ dpois(theta[g,1])&#10;        y2[g] ~ dpois(theta[g,2])&#10;# Predictive distribution for the number of goals scored&#10;    ynew[g,1] ~ dpois(theta[g,1])&#10;    ynew[g,2] ~ dpois(theta[g,2])&#10;# Average Scoring intensities (accounting for mixing components)&#10;    log(theta[g,1]) &amp;lt;- home + att[hometeam[g]] + def[awayteam[g]] &#10;    log(theta[g,2]) &amp;lt;- att[awayteam[g]] + def[hometeam[g]]&#10;}&#10;&#10;# 1. BASIC MODEL FOR THE HYPERPARAMETERS # prior on the home effect&#10;    home ~ dnorm(0,0.0001)&#10;&#10;# Trick to code the ‘‘sum-to-zero’’ constraint for (t in 1:nteams) {&#10;    att.star[t] ~ dnorm(mu.att,tau.att) &#10;    def.star[t] ~ dnorm(mu.def,tau.def) &#10;    att[t] &amp;lt;- att.star[t] - mean(att.star[])&#10;    def[t] &amp;lt;- def.star[t] - mean(def.star[]) }&#10;&#10;# priors on the random effects &#10;mu.att ~ dnorm(0,0.0001) &#10;mu.def ~ dnorm(0,0.0001) &#10;tau.att ~ dgamma(.01,.01) &#10;tau.def ~ dgamma(.01,.01)&#10;&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;My questions are:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Does it make sense to have mu.att be a hyper parameter, when the we're enforcing the sum-to-zero constraint by subtracting the mean anyway?  When I run this model in python (code below), I'm able to reproduce their results, but sure enough, mu_att and mu_def don't converge.  &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Does the model need an intercept, representing the average goal scoring intensity?  I've seen some other examples of models with sum-to-zero constraints, and they included an intercept. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;If I were to switch sum-to-zero enforcement techniques, from subtract-the-mean to set-first-equal-to-negative-sum-of-remaining, what would that mean for questions 1. and 2.? &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;For python folks, here is my attempt at reproducing this in pymc.&#10;First, I extract arrays from the &lt;a href=&quot;https://github.com/DanielWeitzenfeld/passtheroc_source/blob/master/content/downloads/notebooks/data/seria_a.csv&quot; rel=&quot;nofollow&quot;&gt;data&lt;/a&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;observed_home_goals = [row['home_score'] for i, row in df.iterrows()]&#10;observed_away_goals = [row['away_score'] for i, row in df.iterrows()]&#10;who_played_whom = [(row['i_home'], row['i_away']) for i,row in df.iterrows()]&#10;num_teams = len(df.i_home.unique())&#10;num_games = len(who_played_whom)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and then here is my model:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;home = pymc.Normal('home', 0, .0001, value=0)&#10;mu_att = pymc.Normal('mu_att', 0, .0001, value=0)&#10;mu_def = pymc.Normal('mu_def', 0, .0001, value=0)&#10;tau_att = pymc.Gamma('tau_att', .1, .1)&#10;tau_def = pymc.Gamma('tau_def', .1, .1)&#10;&#10;#team-specific parameters&#10;atts_star = pymc.Normal(&quot;atts_star&quot;, mu=mu_att, tau=tau_att, size=num_teams)&#10;defs_star = pymc.Normal(&quot;defs_star&quot;, mu=mu_def, tau=tau_def, size=num_teams) &#10;&#10;# trick to code the sum to zero contraint&#10;@pymc.deterministic&#10;def atts(atts_star=atts_star):&#10;    atts = atts_star.copy()&#10;    atts = atts - np.mean(atts_star)&#10;    return atts&#10;&#10;@pymc.deterministic&#10;def defs(defs_star=defs_star):&#10;    defs = defs_star.copy()&#10;    defs = defs - np.mean(defs_star)&#10;    return defs&#10;&#10;@pymc.deterministic&#10;def home_theta(who_played_whom=who_played_whom, home=home, &#10;               atts=atts, defs=defs): &#10;    home_attack = [atts[i[0]] for i in who_played_whom]&#10;    away_defense = [defs[i[1]] for i in who_played_whom]&#10;    return [math.exp(home + home_attack[i] + away_defense[i]) for i in range(num_games)]&#10;&#10;@pymc.deterministic&#10;def away_theta(who_played_whom=who_played_whom,&#10;               atts=atts, defs=defs): &#10;    away_attack = [atts[i[1]] for i in who_played_whom]&#10;    home_defense = [defs[i[0]] for i in who_played_whom]&#10;    return [math.exp(away_attack[i] + home_defense[i]) for i in range(num_games)]&#10;&#10;home_goals = pymc.Poisson('home_goals', mu=home_theta, value=observed_home_goals, observed=True)&#10;away_goals = pymc.Poisson('away_goals', mu=away_theta, value=observed_away_goals, observed=True)&#10;&#10;mcmc = pymc.MCMC([home, mu_att, mu_def, tau_att, tau_def, &#10;                  home_theta, away_theta, atts_star, defs_star, atts, defs, home_goals, away_goals])&#10;map_ = pymc.MAP( mcmc )&#10;map_.fit()&#10;mcmc.sample(100000, 10000, 4)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-10-15T16:16:12.760" Id="120162" LastActivityDate="2014-10-16T12:50:39.413" OwnerUserId="58665" PostTypeId="1" Score="1" Tags="&lt;bayesian&gt;&lt;hierarchical-bayesian&gt;&lt;pymc&gt;" Title="How to specify a hierarchical bayesian model with sum-to-zero constraints?" ViewCount="97" />
  
  
  <row Body="&lt;p&gt;It appears to be OK to my eye to do what you did. So far you have answered two different questions -- one on the least-squares means and one about the trends. So it is appropriate that different calls were needed to produce the results. I will note though that&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;In the first &lt;code&gt;lsmeans&lt;/code&gt; analysis, you asked for a Bonferroni adjustment. However, there is just one mean for each orientation, so no adjustment was made. Do you wish to compare these two means? Easy enough to do - &lt;code&gt;pairs(org.lsm, by = NULL)&lt;/code&gt; -- again, just one test, so adjustments will have no effect.&lt;/li&gt;&#10;&lt;li&gt;Similarly, do you want to compare the slopes? &lt;code&gt;pairs(pente)&lt;/code&gt; will do the trick.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2014-10-15T16:21:52.297" Id="120165" LastActivityDate="2014-10-15T16:21:52.297" OwnerUserId="52554" ParentId="119965" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Bootstrap inference for the extremes of a distribution is generally dubious. When bootstrapping n-out-of-n the minimum or maximum in the sample of size $n$, you have $1 - (1-1/n)^n \sim 1 - {\rm exp}(-1) = 63.2\%$ chance that you will reproduce your sample extreme observation, and likewise approximately ${\rm exp}(-1) - {\rm exp}(-2)=23.3\%$ chance to reproduce your second extreme observation, and so on. You get a deterministic distribution that has little to do with the shape of the underlying distribution at the tail. Moreover, the bootstrap cannot give you anything below your sample minimum, even when the distribution has the support below this value (as would be the case with most continuous distributions like say normal).&lt;/p&gt;&#10;&#10;&lt;p&gt;The solutions are complicated and rely on the combinations of asymptotics from &lt;a href=&quot;http://en.wikipedia.org/wiki/Extreme_value_theory&quot;&gt;extreme value theory&lt;/a&gt; and &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0387988548&quot;&gt;subsampling&lt;/a&gt; fewer than n observations (actually, way fewer, the rate should converge to zero as $n\to\infty$).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-10-15T16:27:42.320" Id="120166" LastActivityDate="2014-10-15T20:43:04.850" LastEditDate="2014-10-15T20:43:04.850" LastEditorUserId="5739" OwnerUserId="5739" ParentId="119748" PostTypeId="2" Score="5" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;When &lt;a href=&quot;http://en.wikipedia.org/wiki/Chordal_graph&quot; rel=&quot;nofollow&quot;&gt;chordal graphs&lt;/a&gt; are used to model probability distributions, why is it that they do not lose conditional independences when its transformed from a undirected to a directed to a factor graph and around the cycle?&lt;/p&gt;&#10;&#10;&lt;p&gt;Why is it that this conversation of a chordal graph is lossless in the sense of sets of conditional independences?&lt;/p&gt;&#10;&#10;&lt;p&gt;I am happy with an &lt;strong&gt;intuitive&lt;/strong&gt; or a precise &lt;strong&gt;rigorous&lt;/strong&gt; proof of this. Ideally, I want to see both because I tend to forget proof but remember intuitions/concepts much better.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-15T17:29:58.873" Id="120178" LastActivityDate="2014-10-15T17:29:58.873" OwnerUserId="37632" PostTypeId="1" Score="0" Tags="&lt;graphical-model&gt;&lt;graph-theory&gt;" Title="Why do chordal graphs not lose conditional independences when its transformed from undirected to directed to factor graphs and around?" ViewCount="11" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I am attempting a time dependent covariate analysis using SPSS but end up running into some difficulties. This is the first time I am trying it using SPSS so would appreciate some advise or direction.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a study population where the outcome is death and time to death (defined as the time interval from visit 1 to death)&lt;/p&gt;&#10;&#10;&lt;p&gt;The variable I am studying is Infection and whether this decreases survival&lt;/p&gt;&#10;&#10;&lt;p&gt;An infection can happen at anytime from visit 1 till the end of the study. So I can't use kaplan Mier cause of bias.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have been trying to use SPSS for time dependent covariate as follows&lt;/p&gt;&#10;&#10;&lt;p&gt;Using time to infection (Time from visit 1 to infection) T_*TimeInf I then try the model using death as an outcome, Time to death as the time variable and T_TimeInf as the covariate&lt;/p&gt;&#10;&#10;&lt;p&gt;My questions- I left the TimeInf field blank for all patients who did not have an infection. This is being read by SPSS as missing cases. Is this the correct way to run the analysis? Should I be running it a different way or am I missing a step?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance for the help&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-15T17:51:04.853" Id="120184" LastActivityDate="2014-10-15T18:10:12.483" LastEditDate="2014-10-15T18:10:12.483" LastEditorUserId="5739" OwnerUserId="58674" PostTypeId="1" Score="0" Tags="&lt;spss&gt;&lt;survival&gt;&lt;covariate&gt;" Title="Time dependent covariate SPSS" ViewCount="32" />
  <row Body="&lt;p&gt;Yes, you can. A dummy variable is no different, mathematically, from any other fixed effect you might choose to include. Presumably, if some of the categories all have the same impact on the response, it would make sense to zero them out and push their effect into the intercept term. &lt;/p&gt;&#10;&#10;&lt;p&gt;That said, using Wald statistics to cull variables is risky. You may get the right set of variables, but this isn't necessarily the case. &lt;/p&gt;&#10;&#10;&lt;p&gt;When you say &quot;random slope&quot;, are you talking about the coefficient of the categorical variable? If so, I would do some model checking. Look at the estimated random effects and see if they are trying to cover small but real differences in your categories.&lt;/p&gt;&#10;&#10;&lt;p&gt;To clarify that last point: suppose I have 4 categories: A,B,C and D. I decide to omit the dummies associated with C and D. The intercept in the model now corresponds to the case where categories C or D occur. It's like I'm recoding to A, B and Other. But let's suppose that C and D really are real, but just fairly small. &lt;/p&gt;&#10;&#10;&lt;p&gt;Now fit the random effects model. You will get random intercepts for individuals coded &quot;Other&quot; ... but if you plot these against the true categories (C and D), you might find that the C effects are large and the D effects are small (say), or vice versa. When you add random effects, you are giving a bunch of extra parameters to your model, which it could use to cover up for defects of the model itself. &lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-10-15T18:20:27.333" Id="120188" LastActivityDate="2014-10-15T20:14:40.373" LastEditDate="2014-10-15T20:14:40.373" LastEditorUserId="14188" OwnerUserId="14188" ParentId="120183" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;&lt;strong&gt;The question is on the right track.&lt;/strong&gt;  Since such a problem evidently is intended to help one understand and work with characteristic functions, I will include many of the details that might otherwise be omitted from the solution.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;For any random variable $Z$, then, and a real number $t$, let us write&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\phi_Z(t) = \mathbb{E}(e^{i t Z})$$&lt;/p&gt;&#10;&#10;&lt;p&gt;for the &lt;em&gt;characteristic function&lt;/em&gt; (cf) of $Z$.  For future reference, note that&lt;/p&gt;&#10;&#10;&lt;p&gt;$$|\phi_Z(t)| \le \mathbb{E}(|e^{i t Z}|) = \mathbb{E}(1) = 1.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;It is well known--and easily shown by expanding the exponential into its Taylor series at $0$--that when $\phi_Z$ is differentiable at the origin, its derivatives (divided by suitable powers of $i$) are the moments of $Z$.  Regardless, $\phi_Z$ will be continuous at $0$ (essentially because its behavior there reflects the tails of the distribution of $Z$, which necessarily approach asymptotic values and therefore don't change $\phi_Z(t)$ much for very small $|t|$).&lt;/p&gt;&#10;&#10;&lt;p&gt;When two variables $(Z_1,Z_2)$ are independent, the characteristic function of a linear combination $\alpha_1 Z_1 + \alpha_2 Z_2$ is neatly related to the cfs of the variables themselves &lt;em&gt;via&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\eqalign{\phi_{\alpha_1Z_1+\alpha_2Z_2}(t)&amp;amp;=\mathbb{E}\left(e^{i t (\alpha_1Z_1+\alpha_2Z_2)}\right)=\mathbb{E}\left(e^{i t \alpha_1Z_1}e^{i t \alpha_2Z_2}\right)=\mathbb{E}\left(e^{i t \alpha_1Z_1}\right)\mathbb{E}\left(e^{i t \alpha_2Z_2}\right) \\&#10;&amp;amp;=\phi_{Z_1}(\alpha_1 t)\phi_{Z_2}(\alpha_2 t).&#10;}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The penultimate equality required independence of the random variables $e^{it\alpha_iZ_i}$, but that is assured by the independence of the $Z_i$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Assuming $X$ and $Y$ are independent and that $X$ and $X-Y$ are independent, apply this result to a linear combination of $X$ and $X-Y$, say $\alpha X + \beta (X-Y)$, computing it in two ways, using both independence assumptions:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\eqalign{\phi_X((\alpha+\beta)t)\phi_Y(-\beta t) &amp;amp;= \phi_{(\alpha+\beta)X-\beta Y}(t)=\phi_{\alpha X+\beta(X-Y)}(t)=\phi_X(\alpha t)\phi_{X-Y}(\beta t) \\&#10;&amp;amp;=\phi_X(\alpha t)\phi_X(\beta t)\phi_Y(-\beta t).&#10;}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;This was the key move.&lt;/strong&gt;  The desired conclusion follows directly, but some care is needed in the analysis.&lt;/p&gt;&#10;&#10;&lt;p&gt;The tricky part is that we cannot just divide both sides by $\phi_Y(-\beta t)$, because for many values of $t$ it could be zero.  However, since $\phi_Y(0)=\mathbb{E}(1)=1$ and $\phi_Y$ is continuous at $0$, then for any real $\beta$, $\phi_Y(-\beta t)$ must be nonzero in a neighborhood of zero.  &lt;em&gt;Within this neighborhood&lt;/em&gt; (which expands as  $|\beta|$ decreases) we may cancel the common $\phi_Y(-\beta t)$ terms appearing at both ends of the preceding series of equations, yielding&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\phi_X((\alpha+\beta)t) = \phi_X(\alpha t)\phi_X(\beta t).$$&lt;/p&gt;&#10;&#10;&lt;p&gt;(Setting $\alpha=-\beta=1$ gives $1=\phi_X(0)=\phi_X(t)\phi_X(-t)$, which is the partial result reported in the question.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Another way to express this is to take logarithms.  For any real number $\gamma$ define the &lt;em&gt;cumulant generating function&lt;/em&gt; (cgf) as&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\psi(\gamma) = \log(\phi_X(\gamma)).$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Since $\phi_X$ is continuous and positive near $0$, $\psi$ is continuous near $0$.  Consider the special case $t=1$ and assume both $|\alpha|$ and $|\beta|$ are small enough that they and $|\alpha+\beta|$ lie in the neighborhood of values $t$ where $\phi_Y(-\beta t)$ is nonzero.  It is immediate that&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\psi(\alpha+\beta) = \psi(\alpha) + \psi(\beta);\ \psi(0) = 0.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;It is easy to show that the only such functions are the linear ones.  Thus, there must exist a number $\gamma$ for which&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\psi(t) = \gamma t$$&lt;/p&gt;&#10;&#10;&lt;p&gt;for all sufficiently small $t$.  Equivalently, in this neighborhood&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\phi_X(t) = e^{\gamma t}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Since $|\phi_X(t)| \le 1$ for (sufficiently small) positive &lt;em&gt;and&lt;/em&gt; negative values of $t$, $\gamma$ must be a purely imaginary number.  Writing it as $\gamma = i c$ for a real number $c$ yields&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\phi_X(t) = e^{i c t}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Since this is differentiable at the origin, we can now obtain all the moments of $X$.  In particular,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\mu_1=\mathbb{E}(X) = \frac{1}{i}\frac{d}{d t} e^{i c t} \vert_{t=0} = c$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\mu_2=\mathbb{E}(X^2) = \frac{1}{i^2}\frac{d^2}{d t^2} e^{i c t} \vert_{t=0} = c^2.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Therefore the variance of $X$ is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\text{Var}(X) = \mu_2 - \mu_1^2 = 0$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and the desired result follows immediately: &lt;strong&gt;$X$ must be almost everywhere a constant&lt;/strong&gt; (equal to the value of $c$ that emerged in the analysis).&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-10-15T18:23:08.187" Id="120190" LastActivityDate="2014-10-16T16:28:22.910" LastEditDate="2014-10-16T16:28:22.910" LastEditorUserId="919" OwnerUserId="919" ParentId="119808" PostTypeId="2" Score="4" />
  <row AnswerCount="0" Body="&lt;p&gt;I provided two English sentences for the same group of students to judge as grammatical or ungrammatical. 70% of the students judged the first sentence as grammatical whereas only 37% of the same students judged the second sentence as grammatical. I would like to find out whether the difference between the two proportions is statistically significant or not. &lt;/p&gt;&#10;" ClosedDate="2014-10-15T19:21:57.973" CommentCount="1" CreationDate="2014-10-15T18:39:31.260" Id="120192" LastActivityDate="2014-10-15T18:39:31.260" OwnerUserId="58678" PostTypeId="1" Score="0" Tags="&lt;statistical-significance&gt;" Title="How can I calculate the statistical significance for two different proportions of the sample?" ViewCount="7" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I am implementing a linear regression model in pymc3 where the unknown vector of weights is constrained to be a probability mass function, hence modelled as a Dirichlet distribution, as in the following code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;with pm.Model() as model:&#10;    #prior on precision of normal likelihood&#10;    tau = pm.Gamma('tau', alpha=1, beta=1)&#10;&#10;    phi = np.empty(ncountries, dtype=object)&#10;    y = np.empty((nyears-1, ncountries), dtype=object)&#10;    for icountry, country in enumerate(countries):&#10;        #prior Dirichlet allocation for each country&#10;        phi[icountry] = pm.Dirichlet('mix_{c}'.format(c=country),&#10;                                     np.roll(mix, icountry),&#10;                                     shape=ncountries)&#10;&#10;        for iyear, year in enumerate(years[1:]):&#10;            suffix = '_{y}-{c}'.format(y=year, c=country)&#10;            previous_pop = Xs[iyear, :]&#10;            #likelihood&#10;            y[iyear, icountry] = pm.Normal('obs' + suffix,&#10;                        mu=pm.Deterministic(&#10;                            'mu' + suffix,&#10;                            dot(phi[icountry], previous_pop)),&#10;                        tau=tau,&#10;                        observed=Ys[iyear, icountry])&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;After sampling the posterior by running:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    start = pm.find_MAP()&#10;    step = pm.Metropolis()&#10;    nsteps = 1000&#10;    trace = pm.sample(nsteps, step, start=start)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I analysed the trace of the Dirichlet variables, and found that their values do not add to one (below is an example):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;array([[ 0.01029745,  0.00627394,  0.00996922, ...,  1.83955829,&#10;     0.00962185,  0.01020659],&#10;   [ 0.01029745,  0.00627394,  0.00996922, ...,  1.83955829,&#10;     0.00962185,  0.01020659],&#10;   [ 0.01029745,  0.00627394,  0.00996922, ...,  1.83955829,&#10;     0.00962185,  0.01020659],&#10;   ...,&#10;   [ 0.02050308,  0.01685555,  0.01976797, ...,  1.92278065,&#10;     0.03956622,  0.00473735],&#10;   [ 0.01993214,  0.01632033,  0.01994876, ...,  1.92487858,&#10;     0.04078728,  0.00481424],&#10;   [ 0.01900882,  0.01528191,  0.02100671, ...,  1.92485693,&#10;     0.0395159 ,  0.00524575]])&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I am not familiar with theano variables, and found it difficult to explore how a Dirichlet RV is expressed in pymc3... Am I doing anything wrong, or should I just normalise the values returned in the trace so that they sum to one?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Quick update&lt;/strong&gt;&#10;It looks like the function &lt;code&gt;pm.find_MAP()&lt;/code&gt; employes a sort of gradient descent optimisation. This does not take into account the constraint resulting from the fact that a vector representing a draw from a Dirichlet distribution is a probability mass function (its values should be positive and their sum should be one). This constraint is apparently not enforced at the sampling stage of the algorithm either, and causes convergence problems as the precision of the likelihood distribution drifts towards zero.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-15T20:38:10.440" FavoriteCount="1" Id="120209" LastActivityDate="2014-10-17T07:48:47.570" LastEditDate="2014-10-16T10:30:03.617" LastEditorUserId="58686" OwnerUserId="58686" PostTypeId="1" Score="0" Tags="&lt;dirichlet-distribution&gt;&lt;pymc&gt;" Title="PyMC3 Dirichlet Distribution" ViewCount="111" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a dataset with 92 observations and two groups that corresponde to two analytical fractions of soil samples (i.e., light fraction or LF, and mineral-associated fraction or MoM). Each group has 46 observations. I have 14 variables, some of which show collinearity. I applied the Box's tests for equality of covariance matrix accross groups and I rejected the null hypothesis of equality of covariance matrices.&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to test wether these two fractions are different, and considered applying linear discriminant analysis, but since the covariance matrices are not equal, I think I can't so it.&lt;/p&gt;&#10;&#10;&lt;p&gt;Similarly, I read that to apply canonical discriminant analysis the variables need to follow a normal distribution and variances need to be homogeneous.&lt;/p&gt;&#10;&#10;&lt;p&gt;What other discriminant analysis could I use?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" ClosedDate="2014-10-16T09:42:40.160" CommentCount="4" CreationDate="2014-10-15T20:42:14.273" Id="120210" LastActivityDate="2014-10-15T20:42:14.273" OwnerUserId="58687" PostTypeId="1" Score="1" Tags="&lt;discriminant-analysis&gt;&lt;canonical-correlation&gt;" Title="Canonical discriminant analysis - lack of equality of covariance matrices" ViewCount="17" />
  
  <row Body="&lt;p&gt;Being Markov is a property of the &lt;em&gt;distribution&lt;/em&gt;, not the graph (although it is only defined &lt;em&gt;relative&lt;/em&gt; to a given graph). A graph can't be Markov or fail to be Markov, but a distribution can fail to be Markov relative to a given graph. &lt;/p&gt;&#10;&#10;&lt;p&gt;Here is an example in terms of causal networks. Say we know that $X_1$ influences $X_2$ and $X_3$, but $X_2$ and $X_3$ don't influence each other. Then we can represent the true causal relationships with the graph $G$:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/N7LPB.png&quot; alt=&quot;G = X_2 &amp;lt;- X_1 -&amp;gt; X_3&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The distribution $P(X_1, X_2, X_3)$ should represent that $X_2$ and $X_3$ become independent conditional on $X_1$, so the $P(X_1, X_2, X_3)$ is Markov relative to $G$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now let's say &lt;em&gt;we did not observe&lt;/em&gt; $X_1$, but only $X_2$ and $X_3$. Remember that neither of $X_2$ or $X_3$ influences the other. So we could represent the direct causal relationships between $X_2$ and $X_3$ using $G'$, the subgraph over those nodes:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/7FUFg.png&quot; alt=&quot;G&amp;#39; = X_2, X_3&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In this case, the marginal distribution $P(X_2, X_3)$ is &lt;em&gt;not&lt;/em&gt; Markov with respect to $G'$. $X_2$ is dependent on its non-descendent $X_3$, conditional on its parents in $G'$ (i.e. the empty set). This is because $G'$ omits the common cause $X_1$. &lt;/p&gt;&#10;&#10;&lt;p&gt;This example shows that the Markov condition is only reasonable when your graph includes all common causes of any pair of nodes in the graph (an assumption called &quot;causal sufficiency&quot;). Note that you can include a variable in the graph even if you did not observe it - $P(X_1, X_2, X_3)$ is Markov relative to $G$ even if I am missing data for $X_1$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Markov chains are similar, and often used to represent time series. The Markov assumption represents that the next state of the time series depends only on the current state, and not on any previous states. Here's an example representation:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/VOp45.png&quot; alt=&quot;time series&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-15T21:34:47.597" Id="120222" LastActivityDate="2014-10-15T21:41:21.297" LastEditDate="2014-10-15T21:41:21.297" LastEditorUserId="57345" OwnerUserId="57345" ParentId="113370" PostTypeId="2" Score="2" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;My question follows this post: &lt;a href=&quot;http://stats.stackexchange.com/questions/118207/longitudinal-data-baseline-effect-versus-random-intercept?newsletter=1&amp;amp;nlcode=254556%7c00c9&quot;&gt;Longitudinal data: baseline effect versus random intercept&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The topic is very interesting and I have two further questions, one very practical and another about modelling.&lt;/p&gt;&#10;&#10;&lt;p&gt;Q1 : I assume user7064 is computing the model with R. How such a case can be specified with the lm function? I have the impression it looks like an addition of two linear models, one with t0 and another with tj, but how to do this simultaneously?&lt;/p&gt;&#10;&#10;&lt;p&gt;Q2 : Can we imagine the same procedure with a baseline constituted of several observations? (eg. not just t0, but t-1, ..., t-x)&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks to all!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-16T05:55:30.963" Id="120252" LastActivityDate="2014-10-16T05:55:30.963" OwnerUserId="41072" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;panel-data&gt;&lt;random-effects-model&gt;" Title="Longitudinal data: baseline effect versus random intercept 2" ViewCount="29" />
  
  
  <row Body="&lt;p&gt;You can use the information to calculate the base rate fo the disease:&lt;/p&gt;&#10;&#10;&lt;p&gt;accuracy: $A$&lt;/p&gt;&#10;&#10;&lt;p&gt;sensitivity: $s^+$&lt;/p&gt;&#10;&#10;&lt;p&gt;specificity: $s^-$&lt;/p&gt;&#10;&#10;&lt;p&gt;base rate: $p$&lt;/p&gt;&#10;&#10;&lt;p&gt;positive test result: $T^+$&lt;/p&gt;&#10;&#10;&lt;p&gt;disease: $D$&lt;/p&gt;&#10;&#10;&lt;p&gt;$A=p\cdot s^++(1-p)\cdot s^-$&lt;/p&gt;&#10;&#10;&lt;p&gt;$0.8181= p\cdot0.9+ (1-p)\cdot 0.8$&lt;/p&gt;&#10;&#10;&lt;p&gt;$0.0181= 0.1p$&lt;/p&gt;&#10;&#10;&lt;p&gt;$p=.181$&lt;/p&gt;&#10;&#10;&lt;p&gt;For one test positive out of one:&lt;/p&gt;&#10;&#10;&lt;p&gt;using Bayes' theorem:&#10;$P(D|T^+)=\frac{p \cdot s^+}{p \cdot s^+ + (1-p) \cdot (1-s^-)}$&lt;/p&gt;&#10;&#10;&lt;p&gt;$P(D|T^+)=\frac{.181 \cdot .9}{.181 \cdot .9 + (1-.181) \cdot (1-0.8)}$&lt;/p&gt;&#10;&#10;&lt;p&gt;$P(D|T^+)=\frac{.1629}{.1629 + .1638}=.4986$&lt;/p&gt;&#10;&#10;&lt;p&gt;now for k tests out of m:&#10;as in Peter's answer, the probabilities are now drawn from a binomial to generate likelihoods:&lt;/p&gt;&#10;&#10;&lt;p&gt;$L^+$~$B(n,s^+)$ and $L^-$~$B(n,s^-)$ with inverted number of successes for the second.&#10;You can probably calculate confidence intervals based on the knwon properties (variance) of bionomial distributions...&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-16T13:42:00.687" Id="120297" LastActivityDate="2014-10-16T13:42:00.687" OwnerUserId="30395" ParentId="120278" PostTypeId="2" Score="0" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;Let us say that we have 5 sets of data:&#10;$$&#10;a = (a_1, a_2, \dots, a_{10}) \\&#10;b = (b_1, b_2, \dots, b_{10}) \\&#10;c = (c_1, c_2, \dots, c_{10}) \\&#10;d = (d_1, d_2, \dots, d_{10}) \\&#10;e = (e_1, e_2, \dots, e_{10}) &#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Let us suppose that we want to find out if some data are bigger than others. We perform for example the Wilcoxon signed rank test &lt;strong&gt;among each pair&lt;/strong&gt; and end with a 5x5 matrix of p-values. &lt;/p&gt;&#10;&#10;&lt;p&gt;This is a lot of data and the relationships are not immediately clear. Moreover if I have many instances of these data, then I need a new matrix for each instance. Presenting data like this for 40 instances is impossible in a short conference paper. I am looking for some methods on how to use some algebraic properties of significance tests to make this data more readable and less bloated. &lt;/p&gt;&#10;&#10;&lt;p&gt;One way would be to:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Identify all those sets of data for whom there is &lt;strong&gt;no other data&lt;/strong&gt; which is significantly better&lt;/li&gt;&#10;&lt;li&gt;Mark these sets data as &lt;strong&gt;&quot;first grade&quot;&lt;/strong&gt;&lt;/li&gt;&#10;&lt;li&gt;Identify all those sets of data for whom there is no other data, &lt;strong&gt;except for &quot;first grade&quot; data,&lt;/strong&gt; which is significantly better&lt;/li&gt;&#10;&lt;li&gt;Mark these sets of data as &lt;strong&gt;&quot;second grade&quot;&lt;/strong&gt;&lt;/li&gt;&#10;&lt;li&gt;continue to put all data into grades&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Then I would just write to which grade does a data set belong. Like that I can fit all relevant information into a 5x40 table.&lt;/p&gt;&#10;&#10;&lt;p&gt;So far I found just one problem with this. That is the Wilcoxon test as well as all ranking tests &lt;a href=&quot;http://notstatschat.tumblr.com/post/63237480043/rock-paper-scissors-wilcoxon-test&quot; rel=&quot;nofollow&quot;&gt;are not transitive&lt;/a&gt;. That is I can get a situation where for each data set there is some other data set which is significantly better and in this case my procedure would fail.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there some standard, beautiful way to present data such as this?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-16T15:11:00.640" Id="120311" LastActivityDate="2014-10-16T17:16:24.763" OwnerUserId="28978" PostTypeId="1" Score="0" Tags="&lt;hypothesis-testing&gt;&lt;statistical-significance&gt;&lt;data-visualization&gt;" Title="How to present statistical significance results among more than two datasets?" ViewCount="29" />
  
  
  <row AcceptedAnswerId="120322" AnswerCount="1" Body="&lt;p&gt;I am trying to figure out how regression residuals are calculated using the specific example in the attached graphic. &lt;/p&gt;&#10;&#10;&lt;p&gt;Would I simply B-A (Red letters in graphic) to get C so:&#10;22-30 = - 8 in this case? Would I do this for all data points and add the + and - values&#10;to get a residual value?  &lt;/p&gt;&#10;&#10;&lt;p&gt;Additionally, for D, if I had another data set would I compute the residual for all data points for 2 predictors and the line of best fit? &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/1W5m5.jpg&quot; alt=&quot;Graphic&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Source:&#10;&lt;a href=&quot;http://www.bisolutions.us/A-Brief-Introduction-to-Spatial-Regression.php&quot; rel=&quot;nofollow&quot;&gt;http://www.bisolutions.us/A-Brief-Introduction-to-Spatial-Regression.php&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2014-10-16T15:58:11.697" FavoriteCount="1" Id="120319" LastActivityDate="2014-10-16T16:28:45.097" OwnerUserId="57295" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;residuals&gt;" Title="How Are Regression Residuals Calculated - Specific Example" ViewCount="45" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm using libsvm for the binary classification and using a precomputed Kernel. In my particular problem there is no bias term (it's zero). Is there anyway to adjust the bias term in libsvm (and not optimize over b). &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-16T16:40:45.247" Id="120327" LastActivityDate="2014-10-16T17:00:25.497" LastEditDate="2014-10-16T17:00:25.497" LastEditorUserId="49722" OwnerUserId="49722" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;svm&gt;&lt;kernel&gt;&lt;libsvm&gt;&lt;kernel-trick&gt;" Title="Usage of libsvm with RBF kernel and no Offset" ViewCount="27" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;We are studying machine learning via Machine Learning: A Probabilistic Perspective (Kevin Murphy). While the text explains the theoretical foundation of each algorithm, it rarely says in which case which algorithm is better, and when it does, it doesn't say how to tell which case I'm in.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, for the choice of kernel, I've been told to do exploratory data analysis to gauge how complex my data is. In simple 2 dimensional data, I can plot and see whether a linear or radial kernel is appropriate. But what to do in higher dimension?&lt;/p&gt;&#10;&#10;&lt;p&gt;More generally, what do people mean when they say &quot;get to know your data&quot; before choosing an algorithm? Right now I can only distinguish classification vs regression algorithm, and linear vs non-linear algorithm (which I can't check).&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT: Even though my original question is about universal rule of thumb, I've been asked to provide more info on my particular problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;Data: A panel with each row being a country-month (~30,000 rows total, covering ~165 countries over ~15 years). &lt;/p&gt;&#10;&#10;&lt;p&gt;Response: 5 binary variables of interest (i.e. whether protest / coup / crisis, etc. happen in that month).&lt;/p&gt;&#10;&#10;&lt;p&gt;Features: ~ 400 variables (a mix of continuous, categorical, binary) detailing a bunch of characteristic of the 2 previous country-months (longer lag can be created). We only use lagged variable since the goal is prediction. &lt;/p&gt;&#10;&#10;&lt;p&gt;Examples include, exchange rate, GDP growth (continuous), level of free press (categorical), democracy, whether neighbor having conflict (binary). Note that a lot of these 400 features are lagged variables.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-16T17:54:36.183" FavoriteCount="2" Id="120337" LastActivityDate="2014-10-17T15:20:03.770" LastEditDate="2014-10-17T00:26:56.913" LastEditorUserId="20148" OwnerUserId="20148" PostTypeId="1" Score="4" Tags="&lt;machine-learning&gt;&lt;eda&gt;" Title="How to do exploratory data analysis to choose appropriate machine learning algorithm" ViewCount="317" />
  <row Body="&lt;p&gt;Let me highlight two issues that may be relevant for this problem &lt;em&gt;in a time series context&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;(1)&lt;/strong&gt; The trustworthiness of the in-sample modelling results may suffer from over-fitting.&lt;/p&gt;&#10;&#10;&lt;p&gt;Splitting the sample into a training sample and a test sample will help detect this problem. If a model is over-fitted in a training sample, the predictions in the test sample will be poorer than expected based on the measure of model fit obtained in the training sample. &lt;/p&gt;&#10;&#10;&lt;p&gt;Examples: &lt;br&gt;&#10;(&lt;em&gt;i&lt;/em&gt;) a neglected structural change &lt;em&gt;in the sample period&lt;/em&gt;; &lt;br&gt;&#10;(&lt;em&gt;ii&lt;/em&gt;) over-fitting due to data snooping (i.e. trying to obtain a better in-sample fit by making the model overly rich and/or complex).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;(2)&lt;/strong&gt; There is a structural change &lt;em&gt;out of sample&lt;/em&gt;. &lt;br&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In such a case using a training sample and a test sample is as useless as only the in-sample modelling. Out-of-sample predictions of the actual future data will be poorer than expected regardless of whether a training sample and a test sample was used or only in-sample modelling was done.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thus the answer to the main question depends on what kind of situation we are in.&lt;/p&gt;&#10;&#10;&lt;p&gt;In &lt;strong&gt;(1)&lt;/strong&gt;, it is enough to preclude/eliminate over-fitting. E.g. select a model before the data sample even arrives, then estimate the model coefficients using the data sample and use an unbiased estimator of the population $R^2$ &lt;strong&gt;(which might be very tricky to get!)&lt;/strong&gt; to assess the out-of-sample predictive power (using only in-sample data). In such a case we &lt;em&gt;will be able&lt;/em&gt; to infer predictive power from only the in-sample modelling results.&lt;/p&gt;&#10;&#10;&lt;p&gt;In &lt;strong&gt;(2)&lt;/strong&gt;, there is no justification for inferring predictive power from only the in-sample modelling results. However, there is also no justification for inferring predictive power from using a training sample and a test sample. Thus the former approach (&quot;in-sample only&quot;) is &lt;em&gt;not inferior&lt;/em&gt; to the latter one (&quot;training sample and test sample&quot;) with respect to inferring predictive power in a setting like &lt;strong&gt;(2)&lt;/strong&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;In summary, we need &lt;strong&gt;two conditions&lt;/strong&gt; to be met to be able &lt;strong&gt;to infer predictive power from only in-sample modelling results&lt;/strong&gt;: &lt;br&gt;&#10;&lt;strong&gt;(A)&lt;/strong&gt; Model is not over-fitted (and an unbiased measure of model fit is used). &lt;br&gt;&#10;&lt;strong&gt;(B)&lt;/strong&gt; There is no structural change out-of-sample.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-16T18:01:13.663" Id="120339" LastActivityDate="2014-10-18T12:03:18.987" LastEditDate="2014-10-18T12:03:18.987" LastEditorUserId="53690" OwnerUserId="53690" ParentId="116590" PostTypeId="2" Score="0" />
  
  <row AnswerCount="2" Body="&lt;p&gt;Wikipedia says that an interpretation of the area under the ROC curve is: &quot;the area under the curve is equal to the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;But is it the same interpretation of the area under the PR curve? If not, can you please give me an intuitive interpretation for it like the above?&lt;/p&gt;&#10;&#10;&lt;p&gt;Edit: PR == Precision-Recall&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-10-16T18:38:08.623" Id="120343" LastActivityDate="2015-02-09T09:54:45.233" LastEditDate="2014-10-17T07:50:12.637" LastEditorUserId="27779" OwnerUserId="27779" PostTypeId="1" Score="2" Tags="&lt;machine-learning&gt;&lt;data-mining&gt;&lt;roc&gt;&lt;precision-recall&gt;" Title="An intuitive meaning of the area under the PR curve?" ViewCount="86" />
  
  <row Body="&lt;p&gt;You could detrend isoform 2 by isoform 1 or test for isoform 1 as a covariate.&#10;So, a) can regress 1 vs 2, take the residual and regress it against time. This will look for a linear relationship of detrended data; b) here is an example of ANCOVA with repeated measures &lt;a href=&quot;http://stats.stackexchange.com/questions/19060/ancova-with-repeated-measures-in-r&quot;&gt;ANCOVA with repeated measures in R&lt;/a&gt; and time-series analysis with covariates &lt;a href=&quot;https://www.researchgate.net/publication/239802706_Covariate-Adjusted_Regression_for_Tim%E2%80%8C%E2%80%8Be_Series&quot; rel=&quot;nofollow&quot;&gt;https://www.researchgate.net/publication/239802706_Covariate-Adjusted_Regression_for_Tim%E2%80%8C%E2%80%8Be_Series&lt;/a&gt; ; &lt;a href=&quot;http://stats.stackexchange.com/questions/68309/assessing-seasonal-covariates-in-a-seaso&quot;&gt;Assessing Seasonal Covariates in a Seasonal ARIMA Time Series Model&lt;/a&gt;‌​nal-arima-time-series-model&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-16T19:26:07.860" Id="120349" LastActivityDate="2014-10-20T16:50:01.413" LastEditDate="2014-10-20T16:50:01.413" LastEditorUserId="57390" OwnerUserId="57390" ParentId="120106" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;Yes, you can. Please see &lt;a href=&quot;http://www.tandfonline.com/doi/abs/10.1080/00031305.1992.10475872#.VEAiMBZn06U&quot; rel=&quot;nofollow&quot;&gt;this paper&lt;/a&gt; by Larson for reference.&lt;/p&gt;&#10;&#10;&lt;p&gt;Stata has a user-contributed &lt;code&gt;aovsum&lt;/code&gt; command that allows recreating ANOVA analysis using just summary data. SAS has a &lt;a href=&quot;http://support.sas.com/kb/25/020.html&quot; rel=&quot;nofollow&quot;&gt;macro&lt;/a&gt; and R has an &#10;&lt;code&gt;ind.oneway.second&lt;/code&gt; function in the &lt;a href=&quot;http://cran.r-project.org/web/packages/rpsychi/rpsychi.pdf&quot; rel=&quot;nofollow&quot;&gt;rpsychi&lt;/a&gt; package for similar purpose.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you don't know any of these software packages, online calculator like &lt;a href=&quot;http://statpages.org/anova1sm.html&quot; rel=&quot;nofollow&quot;&gt;this&lt;/a&gt; is also available.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-10-16T19:52:42.680" Id="120354" LastActivityDate="2014-10-16T20:00:37.057" LastEditDate="2014-10-16T20:00:37.057" LastEditorUserId="13047" OwnerUserId="13047" ParentId="120351" PostTypeId="2" Score="1" />
  
  
  
  <row Body="&lt;p&gt;the &lt;code&gt;prop.test&lt;/code&gt; function in R seems like it would be appropriate. It will take a proportion (e.g. 20 a values drawn from 30 total draws) and calculate the confidence interval around that proportion with the following code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;prop.test(20, 30, correct = FALSE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Which gives a 95% confidence interval of 48.7% to 80.7% (around a sample value of 66.6%).&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-10-17T00:13:29.117" Id="120381" LastActivityDate="2014-10-17T00:13:29.117" OwnerUserId="46273" ParentId="120378" PostTypeId="2" Score="1" />
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I have a training dataset which differs significantly on several parameters (t-test/f-test) with the test set (for which the labels are not known to me.).&lt;/p&gt;&#10;&#10;&lt;p&gt;I am thinking of re-sampling from the training set and  use only those sets from training which do not differ significantly from the test set.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is this a good idea? How can we do so in R? The goal is to increase AUC on the test set&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-10-17T05:39:43.117" Id="120409" LastActivityDate="2015-02-22T09:29:20.403" OwnerUserId="58772" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;statistical-significance&gt;&lt;sampling&gt;" Title="How to ReSample the Training DataSet" ViewCount="59" />
  
  <row Body="&lt;p&gt;This situation relates to the so called &lt;a href=&quot;http://en.wikipedia.org/wiki/Softmax_function&quot; rel=&quot;nofollow&quot;&gt;softmax&lt;/a&gt; function that assigns probabilities based on scores. Let the scores be $s_1,s_2,\dots,s_n$ any real numbers. The softmax calculates the probability of selection of $j$th item as&#10;$$&#10;p_j = \frac{\exp(\alpha s_j)}{\sum_{i=1}^n\exp(\alpha s_i)}&#10;$$&#10;where $\alpha$ is a positive real number. As $\alpha\to 0$, the selection is getting uniform (each item has the same probability). As $\alpha\to\infty$, the item with highest score gets probability $100\%$. Due to these facts, this is a soft maximization, paramterized by $\alpha$. For more details and the application of this concept, see also &lt;a href=&quot;http://webdocs.cs.ualberta.ca/~sutton/book/ebook/node17.html&quot; rel=&quot;nofollow&quot;&gt;Sutton &amp;amp; Barto 1998&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-10-17T07:37:50.423" Id="120417" LastActivityDate="2014-10-17T07:37:50.423" OwnerUserId="56418" ParentId="120414" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;We currently do not automatically transform the Dirichlet to be on the simplex. This is done explicitly using the simplex transform (from &lt;a href=&quot;https://github.com/pymc-devs/pymc/blob/3eb2237a8005286fee32776c304409ed9943cfb3/pymc/examples/dirichlet.py#L10&quot; rel=&quot;nofollow&quot;&gt;https://github.com/pymc-devs/pymc/blob/3eb2237a8005286fee32776c304409ed9943cfb3/pymc/examples/dirichlet.py#L10&lt;/a&gt;):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; p, p_m1 = pm.model.TransformedVar(&#10;     'p', pm.Dirichlet.dist(a, shape=k),&#10;     pm.simplextransform)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Note though, that we have discussed to make this automatic: &lt;a href=&quot;https://github.com/pymc-devs/pymc/issues/315&quot; rel=&quot;nofollow&quot;&gt;https://github.com/pymc-devs/pymc/issues/315&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-17T07:48:47.570" Id="120418" LastActivityDate="2014-10-17T07:48:47.570" OwnerUserId="26704" ParentId="120209" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;To help answering your questions let me quote a nice explanation of the training process (&lt;a href=&quot;http://citizennet.com/blog/2012/11/10/random-forests-ensembles-and-performance-metrics/&quot; rel=&quot;nofollow&quot;&gt;taken from here&lt;/a&gt;):&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Sample N cases at random with replacement to create a subset of the data. The subset should be about 66% of the total set.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;At each node:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;For some number m (see below), m predictor variables are selected at random from all the predictor variables&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The predictor variable that provides the best split, according to some objective function, is used to do a binary split on that node.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;At the next node, choose another m variables at random from all predictor variables and do the same.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Depending upon the value of m, there are three slightly different systems:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Random splitter selection: m =1&lt;/li&gt;&#10;&lt;li&gt;Breiman’s bagger: m = total number of predictor variables&lt;/li&gt;&#10;&lt;li&gt;Random forest: m &amp;lt;&amp;lt; number of predictor variables. Brieman suggests three possible values for m: ½√m, √m, and 2√m &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;So going back to your questions:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;1.&lt;/strong&gt; The correlation won't really matter because, depending on the chosen system, the algorithm will look at either one variable at a time or choose the best 'splitter' from the subset.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;2.&lt;/strong&gt; Not sure what you mean by performance here... If you mean the 'speed' of the algorithm then you can figure out the performance affect by looking at the process described above (how and how many variables you choose for each node).&lt;/p&gt;&#10;&#10;&lt;p&gt;Now if by performance you meant the accuracy of the model then, in general, the more the predictors and the more independent they are the better, also because correlating them 'artificially' may lead to better results if initially the results are not satisfactory.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-17T09:14:56.923" Id="120426" LastActivityDate="2014-10-17T09:14:56.923" OwnerUserId="58777" ParentId="120413" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="120435" AnswerCount="1" Body="&lt;p&gt;I have a student (for a high school project) who has conducted an experiment and collected data in the following (simplified) way:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;id|situation|scoreX|scoreY&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;where &lt;code&gt;id&lt;/code&gt; identifies the subject; &lt;code&gt;situation&lt;/code&gt; only takes the nominal values &lt;code&gt;A&lt;/code&gt; or &lt;code&gt;B&lt;/code&gt;; and &lt;code&gt;scoreX&lt;/code&gt; and &lt;code&gt;scoreY&lt;/code&gt; are numerical measurements. Each subject has two entries, one where &lt;code&gt;situation=A&lt;/code&gt; and the other when &lt;code&gt;situation=B&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Originally she planned two hypothesis tests, using a t-test, with the alternatives: &lt;code&gt;mu_scoreX_A &amp;gt; mu_scoreX_B&lt;/code&gt; and seperately &lt;code&gt;mu_scoreY_A &amp;lt; mu_scoreY_B&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;My thinking is that if the two original tests were to give p-values of 0.1, individually they are not significant, yet we are seeing two 'almost' effects, which together could be significant.&lt;/p&gt;&#10;&#10;&lt;p&gt;So my question: isn't it better to do one test against the alternative: &lt;code&gt;mu_scoreX_A &amp;gt; mu_scoreX_B AND mu_scoreY_A &amp;lt; mu_scoreY_B&lt;/code&gt;. If so, what is the test one should use and do we need to adapt the significance too?&lt;/p&gt;&#10;&#10;&lt;p&gt;Taking my thinking one step further, are there rules and tests for any boolean combination of alternatives?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-17T09:19:36.487" Id="120427" LastActivityDate="2014-10-17T11:42:35.663" OwnerUserId="20817" PostTypeId="1" Score="2" Tags="&lt;hypothesis-testing&gt;&lt;t-test&gt;" Title="hypothesis test for a boolean combination of alternatives" ViewCount="38" />
  <row AnswerCount="0" Body="&lt;p&gt;I have two datasets consisting of the US Federal debt held by Federal banks with a timespan covering the period 1953Q1 to 1988Q4 and 1970Q1 to 2014Q2. (series FDHBFRB and FDHBFRBN from the FRED website.) Following slightly different definitions, the series are almost identical for the overlapping period. Despite the fact that there is a trend in the series, the correlation coefficient is of 0.9989.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to know how I can have a consistent dataset from 1953Q1 until 2014Q2.&lt;/p&gt;&#10;&#10;&lt;p&gt;My hunch would be to calculate growth rates for each series and expand the series using the growth rate of the other. But how would I deal with the overlapping period ? &lt;/p&gt;&#10;&#10;&lt;p&gt;Hereunder I pasted the dataset in case anyone of you wants to have a go with the data (I used yearly data in order to avoid too many observations. These are in Tab Separated Values):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;observation_date    FDHBFRB FDHBFRBN&#10;1953-01-01  24.9    &#10;1954-01-01  24.7    &#10;1955-01-01  23.9    &#10;1956-01-01  24.0    &#10;1957-01-01  23.4    &#10;1958-01-01  25.1    &#10;1959-01-01  26.2    &#10;1960-01-01  26.6    &#10;1961-01-01  27.7    &#10;1962-01-01  29.9    &#10;1963-01-01  32.3    &#10;1964-01-01  35.3    &#10;1965-01-01  39.3    &#10;1966-01-01  42.5    &#10;1967-01-01  46.9    &#10;1968-01-01  52.0    &#10;1969-01-01  54.5    &#10;1970-01-01  58.9    58.9&#10;1971-01-01  66.9    66.9&#10;1972-01-01  70.2    70.2&#10;1973-01-01  76.0    76.0&#10;1974-01-01  80.4    80.4&#10;1975-01-01  85.3    85.3&#10;1976-01-01  94.4    94.4&#10;1977-01-01  101.3   101.4&#10;1978-01-01  108.9   109.2&#10;1979-01-01  113.1   113.4&#10;1980-01-01  120.7   120.8&#10;1981-01-01  123.5   123.6&#10;1982-01-01  131.5   131.8&#10;1983-01-01  146.4   146.4&#10;1984-01-01  154.9   154.9&#10;1985-01-01  170.3   170.3&#10;1986-01-01  192.6   192.7&#10;1987-01-01  210.7   210.8&#10;1988-01-01  228.0   228.2&#10;1989-01-01  227.3   227.1&#10;1990-01-01  235.6   229.7&#10;1991-01-01  262.2   253.4&#10;1992-01-01  285.9   279.4&#10;1993-01-01  325.9   315.8&#10;1994-01-01  357.5   350.6&#10;1995-01-01  380.9   370.6&#10;1996-01-01  393.4   384.0&#10;1997-01-01  430.0   412.1&#10;1998-01-01  446.8   442.8&#10;1999-01-01      479.4&#10;2000-01-01      507.4&#10;2001-01-01      536.2&#10;2002-01-01      599.9&#10;2003-01-01      654.1&#10;2004-01-01      694.9&#10;2005-01-01      730.7&#10;2006-01-01      768.2&#10;2007-01-01      772.9&#10;2008-01-01      505.6&#10;2009-01-01      673.7&#10;2010-01-01      846.7&#10;2011-01-01      1572.0&#10;2012-01-01      1658.1&#10;2013-01-01      2003.4&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Thank you very much.&lt;/p&gt;&#10;&#10;&lt;p&gt;Kind regards,&#10;Olivier.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-17T09:36:56.990" FavoriteCount="1" Id="120430" LastActivityDate="2014-10-17T09:58:42.917" LastEditDate="2014-10-17T09:58:42.917" LastEditorUserId="32279" OwnerUserId="32279" PostTypeId="1" Score="0" Tags="&lt;time-series&gt;&lt;dataset&gt;" Title="How to merge overlapping discontinued data time series?" ViewCount="25" />
  <row AnswerCount="1" Body="&lt;p&gt;I'm trying to automate outlier detection in time-series and I used a modification of the solution proposed by Rob Hyndman &lt;a href=&quot;http://stats.stackexchange.com/questions/1142/simple-algorithm-for-online-outlier-detection-of-a-generic-time-series&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Say, I measure daily visits to a website from various countries. For some countries where the daily visits are a few hundrends or thousands, my method seems to be working reasonably. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, in cases where a country leads to only 1 or 2 visits per day, the limits of the algorithm are very narrow (e.g. 1 ± 0.001) and therefore the 2 visits are considered an outlier. How could I automatically detect such cases and how could I treat them to identify outliers? I wouldn't like to set a manual threshold of, say, 100 visits per day.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you!&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-10-17T10:17:55.257" FavoriteCount="1" Id="120433" LastActivityDate="2015-01-27T08:07:09.267" OwnerUserId="23772" PostTypeId="1" Score="4" Tags="&lt;time-series&gt;&lt;outliers&gt;&lt;computational-statistics&gt;" Title="Outlier Detection in Time-Series: How to reduce false positives?" ViewCount="89" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm trying to run a model using a cengaussian family distribution with the function MCMCglmm. The model is:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;mod1=MCMCglmm(PC1~1,random=~ID, data=trait2,family=&quot;cengaussian&quot;,verbose=FALSE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I get this error message:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Error in matrix(unlist(value, recursive = FALSE, use.names = FALSE), nrow = nr,:'data' must be of a vector type, was 'NULL'&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Also I wonder if it's possible (or mandatory) to give more information about the censoring, i.e. if it is right or left censoring, and the maximum/minimum values that can be present in the dataset as a consequence of the censoring.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks a lot&lt;/p&gt;&#10;&#10;&lt;p&gt;David&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-17T13:10:14.300" Id="120449" LastActivityDate="2014-10-17T13:10:14.300" OwnerUserId="58775" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;mixed-model&gt;&lt;mcmc&gt;&lt;censoring&gt;" Title="Error using &quot;cengaussian&quot; family with MCMCglmm" ViewCount="34" />
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;is the following equation correct?&lt;/p&gt;&#10;  &#10;  &lt;p&gt;exp(Total) = 2.44750 + exp(1*0.03643) - 0.18171 + (11*0.10723)&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;No, but close. Assuming you used natural log:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\ln{Total}  = -1.82886+2.44750+\ln(1)\times0.03643-0.18171+11\times0.10723$&lt;/p&gt;&#10;&#10;&lt;p&gt;Then, exponent on both sides:&lt;/p&gt;&#10;&#10;&lt;p&gt;$Total  = exp^{-1.82886+2.44750+\ln(1)\times0.03643-0.18171+11\times0.10723}$&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;If your goal is just to predict, the above simple substitution will work fine.&lt;/p&gt;&#10;&#10;&lt;p&gt;The $exp^{\beta}$ think you read about online is pertinent when you wish to interpret the coefficient individually.&lt;/p&gt;&#10;&#10;&lt;p&gt;And &lt;a href=&quot;http://www.herc.research.va.gov/resources/faq_e02.asp&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; provides some resource on dealing with retransformation bias, which I didn't know until Dimitriy pointed that out.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-10-17T13:38:16.877" Id="120451" LastActivityDate="2014-10-17T16:45:56.463" LastEditDate="2014-10-17T16:45:56.463" LastEditorUserId="13047" OwnerUserId="13047" ParentId="120443" PostTypeId="2" Score="1" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I apply multiple logistic regression in R. I need to approximately be able to interpret the coefficients and quantify the contribution of each independent variable. I am aware that direct interpretation of multiple logit coefficients is practically impossible. Hence I employed the following proxy: after the model has yielded the coefficients, I go back to the design matrix and: &lt;/p&gt;&#10;&#10;&lt;p&gt;1) I multiply the values of the independent valuables with the logit coefficients&lt;/p&gt;&#10;&#10;&lt;p&gt;2) I sum across the rows (excluding the dependent var), hence I get a vector of sums which is as long as the number of rows of the design matrix.&lt;/p&gt;&#10;&#10;&lt;p&gt;3) I add to each element of the vector of sums the model interscept&lt;/p&gt;&#10;&#10;&lt;p&gt;4) I apply &lt;code&gt;exp(x_i)/(1+exp(x_i))&lt;/code&gt; where &lt;code&gt;x_i&lt;/code&gt; the elements of the vector of sums to get to probabilities&lt;/p&gt;&#10;&#10;&lt;p&gt;5) I sum the vector of probabilities which equals the scalar total number of positives.&lt;/p&gt;&#10;&#10;&lt;p&gt;6) Next I repeat steps 2-5, but leaving one independent variable out every time. By subtracting the resulting sum of probabilities from the total number of positives I get the marginal contribution of every independent channel. I call this step 'linearisation of probabilities'.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note the model includes no interaction terms. If the above makes sense, my question is: which one makes more sense: apply the linearisation process to the whole dataset or just to the subset of the design matrix that includes positive observations (y = 1)?&lt;/p&gt;&#10;&#10;&lt;p&gt;Please let me know if the R code would help and I will include it.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-17T15:46:21.120" FavoriteCount="1" Id="120467" LastActivityDate="2015-01-12T22:01:55.777" OwnerUserId="58787" PostTypeId="1" Score="0" Tags="&lt;logistic&gt;" Title="Logistic regression in R: Leave-one-out interpretation of coefficients technique" ViewCount="57" />
  
  <row AcceptedAnswerId="120490" AnswerCount="1" Body="&lt;p&gt;Say I have 3 independent variables (x1..x3), all highly correlated with a dependent (Y). I'm looking to determine if x1 changes by a certain amount, how much Y would expected to change. Also, looking to say if x1 AND x2 change by a certain amount, how much Y would expect to change.&lt;/p&gt;&#10;&#10;&lt;p&gt;Would covariance assist with doing this? Appreciate any insight or point in right direction!&lt;/p&gt;&#10;&#10;&lt;p&gt;Edit: I've done separate linear regression to determine correlation of x1 to Y, x2 to Y, x3 to Y (have not done a multivariate to determine combination of those 3 to Y)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-17T19:07:09.960" Id="120487" LastActivityDate="2014-10-17T19:21:27.263" LastEditDate="2014-10-17T19:09:59.130" LastEditorUserId="919" OwnerUserId="58802" PostTypeId="1" Score="0" Tags="&lt;correlation&gt;&lt;multiple-regression&gt;" Title="Determining amount of change in correlated variables" ViewCount="25" />
  <row Body="&lt;p&gt;The Likelihood ratio test you're using uses a chi-square distribution to approximate the null distribution of likelihoods.  This approximation works best with large sample sizes, so its inaccuracy with a small sample size makes some sense.&lt;/p&gt;&#10;&#10;&lt;p&gt;I see a few options for getting better Type-I error in your situation:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;There are corrected versions of the likelihood ratio test, such as Bartlett's correction.  I don't know much about these (beyond the fact that they exist), but I've heard that Ben Bolker knows more.&lt;/li&gt;&#10;&lt;li&gt;You could estimate the null distribution for the likelihood ratio by bootstrapping. If the observed likelihood ratio falls outside middle 95% of the bootstrap distribution, then it's statistically significant.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Finally, the Poisson distribution has one fewer free parameter than the negative binomial, and might be worth trying when the sample size is very small.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-10-17T19:29:16.583" Id="120492" LastActivityDate="2014-10-17T19:38:01.190" LastEditDate="2014-10-17T19:38:01.190" LastEditorUserId="4862" OwnerUserId="4862" ParentId="120309" PostTypeId="2" Score="3" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm currently working on some experimental data. The experimental design consists of two treatments. In each treatment, 20 subjects are randomly matched in pairs and participate to a simple game. The game is repeated for 20 periods. In each period, the pairs are randomly re-matched and a single decision is made.&lt;/p&gt;&#10;&#10;&lt;p&gt;I estimate the effect of the treatment with a model that includes individual random effects, session dummies and some lagged variables (to control for dynamic session effects). When I estimate the cluster-robust covariance matrix, with the xtreg_re option vce(cluster Session), the standard errors are smaller than the unclustered ones; when I exclude the session dummies, the cluster-robust standard errors become larger than the unclustered ones.&lt;/p&gt;&#10;&#10;&lt;p&gt;I read the articlehttp://www.stata.com/support/faqs/st...luster-option/ on the comparison of the standard errors for robust, cluster, and standard estimators. I understand that there must be a cancellation of variation when the residuals are summed over clusters, but it's not clear to me why this happen when I include fixed effects for the clusters?&lt;/p&gt;&#10;&#10;&lt;p&gt;LITTLE UPDATE: I think I tracked down the source of the problem. Indeed, what I observe with the standard errors is not specific to my data nor to the FGLS. I actually could replicate the problem with a fake panel and with standard OLS. I think that the source of the problem is my main independent variable, which is a dummy which takes a 1 if the observation is in the main treatment and 0 if it is in the control group. The session dummies that I want to plug into my model, to control for possible static session effects, are actually very correlated with the treatment dummy: each session belongs either to the main treatment or to the control treatment.&#10;Nevertheless, I still am not sure how exactly the inclusion of the session dummies reduce my standard errors from the cluster robust covariance matrix and why I don't observe anything odd in the estimates of the parameters.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks,&#10;Giancarlo&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-10-17T20:54:44.000" Id="120507" LastActivityDate="2014-10-20T08:19:17.410" LastEditDate="2014-10-20T08:19:17.410" LastEditorUserId="41314" OwnerUserId="41314" PostTypeId="1" Score="1" Tags="&lt;random-effects-model&gt;&lt;fixed-effects-model&gt;&lt;clustered-standard-errors&gt;&lt;robust-standard-error&gt;" Title="cluster-robust standard errors are smaller than unclustered ones in fgls with cluster fixed effects" ViewCount="100" />
  <row AnswerCount="0" Body="&lt;p&gt;The goal of problem is to predict the weight for missing data .&#10;I have a dataset of categorical type as shown below&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ideal_for,occasion,color,outer_material,inner_material,sole_material,closure,tip_shape,weight,other_details,PID,shoe_type&#10;Men,Casual,&quot;Black, Red&quot;,,,,Laced,Round,350 gm,&quot;Cushioned Ankle and Tongue, Textured Sole, Padded Footbed&quot;,SHODR5ZRHRD2RXV7,sneakers&#10;Men,Casual,Brown,,,,Laced,Round,294 gm,&quot;Padded Footbed, Textured Outsole&quot;,SHODQ467KMYXRB2F,sneakers&#10;Men,Casual,Brown,,,,Laced,Round,373 gm,&quot;Padded Footbed, Textured Sole&quot;,SHODSM57F6ZHCCMT,sneakers&#10;Men,Casual,&quot;Navy, Yellow&quot;,,,,Laced,Round,265 gm,&quot;Padded Footbed, Textured Sole, Cushioned Ankle and Tongue&quot;,SHODR3SQEFPF3FTE,sneakers&#10;Men,Casual,Black,,,,Laced,Round,375 gm,&quot;Textured Sole, Padded Footbed, Cushioned Ankle&quot;,SHODS8SP6VDWMJFK,sneakers&#10;Men,Casual,Deep Sea Blue,,,,Laced,Round,368 gm,&quot;Padded Footbed, Textured Sole&quot;,SHODUHJAHQTRE6YH,sneakers&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;As you can see we have empty field in data , I replaced this empty field by NA &lt;/p&gt;&#10;&#10;&lt;p&gt;then I converted this categorical to numerical like as below&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ideal_for   occasion    color   outer_material  inner_material  sole_material   closure tip_shape   weight  other_details   PID shoe_type&#10;0   0   0   Black, Red  0   2   0   8   2   350.0   Cushioned Ankle and Tongue, Textured Sole, Padded Footbed   SHODR5ZRHRD2RXV7    76&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;eg 0 for men and 1 for women etc&lt;/p&gt;&#10;&#10;&lt;p&gt;the I applied polynomial regression to predict the weight of missing data,&#10;Is this process correct to predict weight ,I am getting 0.0 correlation score while validating model. what is the best method to predict weight for such categorical dataset&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-10-17T14:40:04.800" Id="120518" LastActivityDate="2014-10-17T23:24:58.800" OwnerDisplayName="madan ram" OwnerUserId="44771" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;dataset&gt;&lt;machine-learning&gt;&lt;regression&gt;" Title="algorithm to predict cost function" ViewCount="24" />
  <row Body="&lt;p&gt;You can fit this model directly using a generalized linear model.&lt;/p&gt;&#10;&#10;&lt;p&gt;Since your data are 0-1, you would want the binomial family, and a $\log$ link. &#10;This is available in many packages.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, you must be very careful with the log-link in a binomial model, since it's not constrained to lie in (0,1), it's only constrained at the 0-end -- so if the values for $P(Y=1)$ can get up near 1, the algorithm may have difficulty. &lt;/p&gt;&#10;&#10;&lt;p&gt;Here I've done an example in R.&lt;/p&gt;&#10;&#10;&lt;p&gt;First some data:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/7lh7j.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;This is data generated from an exponential decay model in $P(Y=1|X=x)$, and (in purple) a lowess smooth performed on transformed $y$ (then backtransformed), so that we can see that locally it looks like typically it does progress as desired.&lt;/p&gt;&#10;&#10;&lt;p&gt;I jittered (added a tiny bit of noise to) the y-values for the plot so you could more easily see the relative density.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's the command I used to glm fit and the resulting output (with a few unnecessary lines removed), performed in R. You'll note that I supplied starting values; usually this isn't necessary for GLMs, but may be the case for your model (it was for this data - it had trouble finding good start values on its own). My strategy for getting start values was to first fit a model without intercept and then supply start values as 0 for the constant-term and the coefficient from the no-intercept model for the x-term, which was sufficient in this case. A more sophisticated strategy might involve taking information from the lowess fit to guess good start values, but that wasn't needed here. A third possibility (if the typical proportion is small) would be to use parameters from a logit link as starting values, since if the proportions are small, log and logit are similar.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; logfit=glm(y~x,family=binomial(link=log),start=c(0,-.227))&#10; summary(logfit)&#10;&#10;Coefficients:&#10;            Estimate Std. Error z value Pr(&amp;gt;|z|)    &#10;(Intercept)  0.85492    0.16230   5.268 1.38e-07 &#10;x           -0.38053    0.04049  -9.399  &amp;lt; 2e-16 &#10;&#10;(Dispersion parameter for binomial family taken to be 1)&#10;&#10;    Null deviance: 364.22  on 319  degrees of freedom&#10;Residual deviance: 264.65  on 318  degrees of freedom&#10;AIC: 268.65&#10;&#10;Number of Fisher Scoring iterations: 5&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;The true coefficients in this case were 0.6 and -0.32; both estimated coefficients where within about 1.5 standard errors of the population values.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's the fit, in red (and for comparison, the model I generated the data from, in dashed dark-blue):&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/VyhQx.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;It's no big surprise that the fit drops more quickly than the true curve, since the local smooth we did at the beginning dropped faster still (it looks like there was an excess of 0's in the middle of the left-half, say around x=4 to 5, compared to what you might expect, causing a more rapid drop in the fit).&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Advice wise, if you don't have a particularly good reason outside the appearance of the data itself to use an exponential decay, you might consider a logit-link model instead. It's less likely to stray into difficulty.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-10-17T23:42:44.413" Id="120519" LastActivityDate="2014-10-18T00:12:24.297" LastEditDate="2014-10-18T00:12:24.297" LastEditorUserId="805" OwnerUserId="805" ParentId="120464" PostTypeId="2" Score="3" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am planning to create a semantic graph by creating an automatic ontology. I want to know which is the best process to do it.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Doing social network analysis to create people, relationships, likes, objects etc&#10;or&lt;/li&gt;&#10;&lt;li&gt;Carrying out Named Entiity Recognition, Relation extraction etc to extract people, relationships etc?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Can you outline me which is better and why, and if both arent the correct way, which is the better way?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-18T06:28:53.217" Id="120538" LastActivityDate="2014-10-18T06:34:23.670" LastEditDate="2014-10-18T06:34:23.670" LastEditorUserId="57082" OwnerUserId="57082" PostTypeId="1" Score="1" Tags="&lt;text-mining&gt;&lt;graph-theory&gt;&lt;social-network&gt;" Title="Is Social Network Analysis or NER the best way to create a semantic graph?" ViewCount="19" />
  <row AnswerCount="0" Body="&lt;p&gt;I had data with repeated measurement and nested design.  Conventional ANOVA requires strict control on homogeneity of variance and repeated measurement ANOVA requires assumption of sphericity.  Multi-level modeling allows greater flexibility as the covariance matrix could be modified and thus,  I used Linear Mixed Model(LMM) option in SPSS and defined the repeated measurement matrix with AR(1). &lt;/p&gt;&#10;&#10;&lt;p&gt;I am confuse do I still have to keep the assumption of homogeneity of variance if I use LMM.&lt;/p&gt;&#10;&#10;&lt;p&gt;My dependent variable is a continuous scale data, can I use a Generalized Linear Mixed Model (GLMM) for analysis if homogeneity of variance is not met?  However, my data is not binomial distributed.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-18T07:07:28.480" Id="120543" LastActivityDate="2014-10-18T07:07:28.480" OwnerUserId="53975" PostTypeId="1" Score="1" Tags="&lt;mixed-model&gt;&lt;multilevel-analysis&gt;&lt;glmm&gt;" Title="Assumptions of Linear Mixed Model" ViewCount="67" />
  
  <row AnswerCount="1" Body="&lt;p&gt;If someone is investigating differences in means between 2 groups, does it matter which we consider to be the IV and the DV? For example, can I use a t test to examine the difference in the age of children who have attained a certain developmental milestone and those who have not? Imagine the IV is milestone (Yes/no) and the DV age (mean). But clearly, the milestone does not cause the age. Is it reasonable to use a t test in this manner? Does presumption of causality matter?&lt;/p&gt;&#10;&#10;&lt;p&gt;And secondly, is it reasonable to switch round IVs and DVs in order to look at the data in different ways? E.g. to run some t tests with group (e.g. milestone) as the IV, and then run a logistic regression with all the continuous predictor variables (with group as the DV)?&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a better way to approach such analyses?&lt;/p&gt;&#10;&#10;&lt;p&gt;Many thanks&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-10-18T14:26:17.340" FavoriteCount="1" Id="120555" LastActivityDate="2014-10-22T08:47:33.113" OwnerUserId="58845" PostTypeId="1" Score="3" Tags="&lt;logistic&gt;&lt;t-test&gt;" Title="Independent/Dependent variables &amp; type of statistical test" ViewCount="45" />
  <row AnswerCount="0" Body="&lt;p&gt;i've been trying to come up with a way to measure attack and defense of a team in terms of goals scored and received.&lt;/p&gt;&#10;&#10;&lt;p&gt;The only thing that has crossed my mind is to divide the number of goals scored/recieved by the total number of goals scored/received. So if for example a team scored 10 goals in the last 5 matches and the total number of goals scored is 100 that would mean its attack is equal to 10%. &lt;/p&gt;&#10;&#10;&lt;p&gt;However this seems to be too elementary and i am not sure if it will be of practical use... Can someone propose something better?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-10-18T15:01:47.860" Id="120560" LastActivityDate="2014-10-18T15:01:47.860" OwnerUserId="58805" PostTypeId="1" Score="0" Tags="&lt;mathematical-statistics&gt;&lt;summary-statistics&gt;" Title="How to measure defense and attack of a soccer team using goals" ViewCount="76" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am new to choice experiments and trying to learn the mandatory basics. One reference I am using is the page &lt;a href=&quot;http://blogs.sas.com/content/jmp/2009/01/15/optimal-design-of-the-choice-experiment/&quot; rel=&quot;nofollow&quot;&gt;http://blogs.sas.com/content/jmp/2009/01/15/optimal-design-of-the-choice-experiment/&lt;/a&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;p&gt;I am trying to understand how to apply prior mean information in choice designs. Can somebody help me to understand &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;What are fitted coefficients? &lt;/li&gt;&#10;&lt;li&gt;What means less then x in magnitude? &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&quot;Typically the fitted coefficients of choice experiments are less than&#10;  2 in magnitude. Effects that are much larger than this are so dominant&#10;  that they swamp the effects of other alternatives.&quot;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Many thanks in advance.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-18T17:18:20.927" Id="120575" LastActivityDate="2014-10-18T17:18:20.927" OwnerUserId="58638" PostTypeId="1" Score="0" Tags="&lt;probability&gt;&lt;coefficient&gt;&lt;jmp&gt;&lt;conjoint-analysis&gt;&lt;choice&gt;" Title="What are fitted coefficients in magnitude in choice experiments?" ViewCount="14" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm a computer science grad student, with not much knowledge in Bayesian statistics, so I'm seeking for guidance for the simplest start.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have 10 variables, like demand, price etc. and I want to construct a very simple graphical model, for example, to estimate the demand in 2 years.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have &lt;a href=&quot;http://bnt.googlecode.com&quot; rel=&quot;nofollow&quot;&gt;Bayes Net Toolbox&lt;/a&gt; with MATLAB, and I'm following the examples, but I'm still not sure how to go on with the learning and probability tests.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would really appreciate some sample code, to for example get a probability estimation using my observation matrix &lt;code&gt;data&lt;/code&gt;, (250 x 10) matrix. So it contains 250 equally-sampled observations for 10 variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;Or could somebody explain to me the matrices in Hidden Markov Models (HMMs) section of &lt;a href=&quot;http://bnt.googlecode.com/svn/trunk/docs/usage_dbn.html&quot; rel=&quot;nofollow&quot;&gt;this tutorial&lt;/a&gt;?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-18T20:02:40.763" Id="120587" LastActivityDate="2014-10-18T20:48:42.447" LastEditDate="2014-10-18T20:48:42.447" LastEditorUserId="44001" OwnerUserId="44001" PostTypeId="1" Score="0" Tags="&lt;time-series&gt;&lt;forecasting&gt;&lt;matlab&gt;&lt;bayes&gt;&lt;bayes-network&gt;" Title="How to implement a simple Bayesian Network for Time Series Data?" ViewCount="87" />
  <row Body="&lt;p&gt;As usual.&lt;/p&gt;&#10;&#10;&lt;p&gt;$$E(X_n) = \int_{-\infty}^{\infty}f_X(x) x dx = \int_{-1/n}^{1/n}\frac{n-1}2 x dx+\int_{n}^{n+1}\frac{1}n x dx$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$=...= 1+\frac 1{2n}$$&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-10-18T22:19:50.123" Id="120599" LastActivityDate="2014-10-18T22:19:50.123" OwnerUserId="28746" ParentId="120597" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;In a crossover design, the effects that usually need to take into account are fixed sequence effect, period effect, treatment effect, and random subject effect. Note that by design the subject factor is nested within sequence (meaning that different subjects go through different sequences).&lt;/p&gt;&#10;&#10;&lt;p&gt;Basically you might want to keep all the effects in the model, and test whether they are significant or not. Prior to build the model, you also might want to test the carryover effect.&lt;/p&gt;&#10;&#10;&lt;p&gt;I've not used R to perform this kind of data analysis. But I guess there are a lot of sample code illustrating how to write R for a mixed-effects model. As far as SAS code, here are two references I think that both explains the experiment design and the SAS code very well.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://analytics.ncsu.edu/sesug/2004/SD04-Yarandi.pdf&quot; rel=&quot;nofollow&quot;&gt;1. Crossover Designs and Proc Mixed In SAS&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;https://www.math.iupui.edu/~indyasa/crosover.pdf&quot; rel=&quot;nofollow&quot;&gt;2. The 2-by-2 crossover study and Grizzle Model&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Hope it helps.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-19T03:33:20.973" Id="120618" LastActivityDate="2014-10-19T03:33:20.973" OwnerUserId="56525" ParentId="115556" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;To me the best default approach is to always use confidence intervals, using methods that recognize that the intervals should usually be asymmetric.  Use of $\pm k\times$SE implies symmetry that doesn't hold when the underlying data distribution is very asymmetric.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you want to show data distributions rather than precision of estimates of central tendency, then plotting the 0.25 and 0.75 quantiles along with the median often work well.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-19T04:44:25.177" Id="120624" LastActivityDate="2014-10-19T13:37:30.347" LastEditDate="2014-10-19T13:37:30.347" LastEditorUserId="4253" OwnerUserId="4253" ParentId="120616" PostTypeId="2" Score="5" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I am currently working on my thesis and interested in estimating a multilevel differential item functioning model and I using at WinBUGS. Until I had done model check-up, there are no errors. However, when I tried to &quot;update&quot; the sample, suddenly the trap message said &quot;undefined real result&quot; popped up. What am I doing wrong? Also, I have tried different prior and initial values but I could not solve the problem!&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;   # Model&#10;   Model&#10;   {&#10;   for (l in 1:10){&#10;   y[l] ~ dbern(p[l])&#10;   logit(p[l])&amp;lt;- u2[stu[l]] - beta[x[l]] + gamma[tea[l], x[l]]*grp[l] + alpha1[x[l]] *geo[l] +&#10;   alpha2[x[l]]*conf[l] + alpha3[x[l]]*ses[l]&#10;  }&#10;  for (t in 1:5){&#10;  for (i in 1:5){&#10;  gamma[t,i] ~ dnorm(gamma.hat[t,i], tau.gamma[i])&#10;  gamma.hat[t,i]&amp;lt;-pi1[i] + pi2[i]*inq[t]&#10;  }&#10;  }&#10;  # fixed effect prior&#10;   for (i in 1:5){&#10;   beta[i] ~ dnorm(0, .0001)&#10;   alpha1[i] ~ dnorm(0, .0001)&#10;   alpha2[i] ~ dnorm(0, .0001)&#10;   alpha3[i] ~ dnorm(0, .0001)&#10;   pi1[i] ~ dnorm(0, .0001)&#10;   pi2[i] ~ dnorm(0, .0001)&#10;    }&#10;   # Random effect prior&#10;   for (s in 1:10){&#10;    u2[s] ~ dnorm(0,tau.u2)&#10;    }&#10;   tau.u2 &amp;lt;- pow(sigma.u2, -2)&#10;   sigma.u2 ~ dunif (0, 100)&#10;   for (i in 1:5){&#10;   tau.gamma[i] &amp;lt;- pow(sigma.gamma[i],-2)&#10;   sigma.gamma[i] ~ dunif(0, 100)&#10;   }&#10;   }&#10;&#10;   # Data&#10;   list(y=c(0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,  1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0), ses=c(2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1), conf=c(1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3), geo=c(3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3), grp=c(1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0), inq=c(1, 2, 2, 1, 2), stu=c(1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10), x=c(1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5), tea=c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5))&#10;&#10;   #Initital values&#10;   list(beta=c(0, 0, 0, 0, 0), alpha1=c(0, 0, 0, 0, 0),  alpha2=c(0, 0, 0, 0, 0),  alpha3=c(0, 0, 0, 0, 0), sigma.gamma=c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1), u2=c(0, 0, 0, 0, 0), pi1=c(0, 0, 0, 0, 0), pi2=c(0, 0, 0, 0, 0), sigma.u2=1, gamma=structure(&#10;.Data=c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), .Dim=c(5, 5)))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2014-10-19T17:48:08.630" Id="120669" LastActivityDate="2014-10-22T20:58:34.127" LastEditDate="2014-10-22T20:58:34.127" LastEditorUserId="58806" OwnerUserId="58806" PostTypeId="1" Score="0" Tags="&lt;distributions&gt;&lt;multilevel-analysis&gt;&lt;error&gt;&lt;prior&gt;&lt;winbugs&gt;" Title="Undefined real result error at WinBUGS" ViewCount="114" />
  
  <row AnswerCount="0" Body="&lt;p&gt;hi i have a question on verifying the central limit theorem using R.&lt;/p&gt;&#10;&#10;&lt;p&gt;the question was to use data generated from different statistical distribution n see if the sample means follow normal distribution .&lt;/p&gt;&#10;&#10;&lt;p&gt;iam starting with a sample of size rchisq(20, 20) &#10;and then increasing the number of my samples by 5 in each interval. ie. , 25, 30, 25, ... then i will using the KS test shapiro test available in R to test for their normality for each sample. &lt;/p&gt;&#10;&#10;&lt;p&gt;question is should i first define a population of 1000 first then take different samples of size 20 and then running my tests ? this is because i read the defnition that CLT takes different combination of sample size N from a population to create population of means.&lt;/p&gt;&#10;&#10;&lt;p&gt;any help is greatly appreciated! thank you !&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-10-19T20:12:51.787" Id="120678" LastActivityDate="2014-10-19T22:33:45.853" LastEditDate="2014-10-19T22:33:45.853" LastEditorUserId="58910" OwnerUserId="58910" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;normality&gt;" Title="verify the central limit theorem of chi sq distribution" ViewCount="41" />
  <row AnswerCount="0" Body="&lt;p&gt;I have my main data feed which only collects a new value if it is different from the last (it checks every 13 seconds) and from this I build a line chart.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Is there a name for this type of chart?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;It's not Renko because Renko has a predefined value the next bar must pass.&#10;It's not a ticker because a ticker does not have to change value in order to log a new tick.&lt;/p&gt;&#10;&#10;&lt;p&gt;Other interesting points to this chart type are:&lt;/p&gt;&#10;&#10;&lt;p&gt;If you are viewing a higher ____ frame (not time frame, I log time in a separate array) then you are looking at the average of x moves; for example a 10x frame would be ten moves averaged to one and the high or low would be the highest (or lowest respectively) within those moves.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-19T21:01:37.743" Id="120682" LastActivityDate="2014-10-19T22:23:16.737" LastEditDate="2014-10-19T22:23:16.737" LastEditorUserId="22311" OwnerUserId="56655" PostTypeId="1" Score="0" Tags="&lt;dataset&gt;" Title="What is my chart type called?" ViewCount="29" />
  <row AnswerCount="0" Body="&lt;p&gt;I want to compare correlations between males and females on a range of dependent variables for a dataset. How do I do this? The gender variable is ordinal. I have tried selecting cases based on gender but i'm not sure this is correct...&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-10-20T00:44:03.770" Id="120698" LastActivityDate="2014-10-20T00:44:03.770" OwnerUserId="58920" PostTypeId="1" Score="0" Tags="&lt;correlation&gt;" Title="Comparing groups based on ordinal variables (gender)" ViewCount="19" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a sample of $N$ instances of a problem (assumed to be independent samples of an infinite population of instances). To each of these instances I apply $M$ methods, and record how quickly they &quot;solve&quot; the instance. The questions I want to answer, in general, is which is the best method (shortest time)?&lt;/p&gt;&#10;&#10;&lt;p&gt;My initial approach was to determine, for each instance, the best (smallest) of the times across the methods. This was used to normalize the solve times, so I have a dimensionless ratio $\ge 1$ for each method-instance pair. I then log-tranform this ratio, and take the mean across instances for each method (so the geometric mean, essentially). This captures not only the ranking in mean, but ideally, the degree to which a method is best. The problem is, the differences might not be statistically significant.&lt;/p&gt;&#10;&#10;&lt;p&gt;I can do, for each pair of methods (i.e. $O(M^2/2)$), a paired t-test to determine if the their means are different (two-tailed). I interpreted this as enabling me to say things like &quot;at this confidence level, Method B is best, and there is no significant difference between Methods A,C, and D&quot;. But then I realized that as I was doing multiple tests, I should be doing something like the &lt;a href=&quot;http://en.wikipedia.org/wiki/Bonferroni_correction&quot; rel=&quot;nofollow&quot;&gt;Bonferroni correction&lt;/a&gt;. I believe that I'd have to then adjust my signifance level for the individual tests to $\alpha/(number\ of\  pairs)$ for this to be a correct statement.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Is the methodology I outlined sound? I only really need to be able to say that one of the methods is better than all the rest. The Bonferroni correction seems very conservative, and I'm almost certain it will find that all my methods are the same. Should I rather than doing all-pairs paired t-tests with two tails be doing $M^2$ one-tailed t-tests?&lt;/li&gt;&#10;&lt;li&gt;Would be changing the problem to one where I rank the methods, for each instance, be &quot;better&quot; at distinguishing between the methods? If so, what then?&#10;&lt;ul&gt;&#10;&lt;li&gt;I could imagine &quot;averaging&quot; the ranks somehow, then my null hypothesis being that &quot;the rank is this&quot; and the alternative being &quot;it is not this rank&quot;&lt;/li&gt;&#10;&lt;li&gt;I could imagine a different hypothesis being that &quot;Method X is the best&quot; and doing that $M$ times - but then again I'd surely need to correct for doing $M$ tests.&lt;/li&gt;&#10;&lt;/ul&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;If its any help, I'd rather falsely say the methods are the same (Type 2) than say the methods are different when they are not (Type 1).&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;It has just struck me, on further reflection, that my (normalized) data is not at all normal, thus using a t-test is probably a bad idea to begin with and I should probably be using a &lt;a href=&quot;http://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test&quot; rel=&quot;nofollow&quot;&gt;Wilcoxon signed-rank test&lt;/a&gt; instead. But this still begs the question of how to adjust for doing multiple of these.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2014-10-20T02:02:26.250" Id="120704" LastActivityDate="2014-10-20T02:16:26.723" LastEditDate="2014-10-20T02:16:26.723" LastEditorUserId="58921" OwnerUserId="58921" PostTypeId="1" Score="2" Tags="&lt;ranking&gt;&lt;paired-comparisons&gt;&lt;bonferroni&gt;" Title="Appropriate correction for tests for determining best of family" ViewCount="9" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I have a set of data that I am using regression analyses on. All of the columns are numeric (as far as I can see) a mix of integers and reals. However, two of the columns are being read from the CSV as factors, not numeric. I can't see any reason for this.&lt;/p&gt;&#10;&#10;&lt;p&gt;My first attempts at a regression were just to see how good each column was as a predictor (of time) on its own. The columns in question caused a bit of an issue. As factors they were excellent predictors alone (adjusted R${^2}$ of 0.45) however when converting them to numeric, they became poor predictors (adjusted R${^2}$ of 0.01).&lt;/p&gt;&#10;&#10;&lt;p&gt;I have three questions. &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Why would these two columns be interpreted as factors rather than numeric;&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Why would there be such an enormous change in the quality of the predictor&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Can I justify using them (and other columns) as a factor?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="3" CreationDate="2014-10-20T02:53:00.860" Id="120711" LastActivityDate="2014-10-30T15:13:06.833" LastEditDate="2014-10-28T17:44:53.193" LastEditorUserId="930" OwnerUserId="24169" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;regression&gt;&lt;categorical-data&gt;" Title="Treating predictors as numerical or categorical variable in regression" ViewCount="400" />
  <row AnswerCount="1" Body="&lt;p&gt;I am comparing the means of days between notification and diagnosis for 13 years, as I want to know if the mean days from 2013 is significantly different from the previous 12 years. &lt;/p&gt;&#10;&#10;&lt;p&gt;I was thinking of using pairwise comparison with Dunnett's adjustment, but this method assumes that 2001 is my control, and I was wondering if I can change the control to 2013?&lt;/p&gt;&#10;&#10;&lt;p&gt;I am currently using Stata 13.0 as my analytical software&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-10-20T04:23:44.080" Id="120719" LastActivityDate="2014-10-20T08:14:54.327" LastEditDate="2014-10-20T08:14:54.327" LastEditorUserId="22047" OwnerUserId="49491" PostTypeId="1" Score="0" Tags="&lt;stata&gt;&lt;adjustment&gt;" Title="pairwise comparison with Dunnett's adjustment in Stata" ViewCount="40" />
  <row AcceptedAnswerId="120723" AnswerCount="1" Body="&lt;p&gt;In a study, drug was given to tt (treatment) group and not to cc (control) group and blood levels of (say) cholesterol were tested before and after in each subject. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have following data: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;  id grp preLevel postLevel diff percentDiff prePostRatio&#10;1  1  tt      140       110   30        0.21         1.27&#10;2  2  tt      150       120   30        0.20         1.25&#10;3  3  cc      135       125   10        0.07         1.08&#10;4  4  cc      155       145   10        0.06         1.07&#10;...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I can perform t.test or wilcox.test and compare the two groups. But what is the best parameter to analyze: absolute difference, percent difference or pre/post ratio? Or it does not matter and they will all produce same result?&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Edit: &lt;/p&gt;&#10;&#10;&lt;p&gt;For above data should I use t.test/ wilcox.test or repeated measures anova as described on this page: &lt;a href=&quot;http://ww2.coastal.edu/kingw/statistics/R-tutorials/repeated.html&quot; rel=&quot;nofollow&quot;&gt;http://ww2.coastal.edu/kingw/statistics/R-tutorials/repeated.html&lt;/a&gt; using following code with rearranged data:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;id  grp  pre_or_post level&#10;1   tt   pre         140&#10;1   tt   post        110&#10;2   tt   pre         150&#10;2   tt   post        120&#10;3   cc   pre         135&#10;3   cc   post        125&#10;...&#10;&#10;aov(level ~ grp * pre_or_post + Error(id/pre_or_post), data=mydata)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Also, how will the method of analysis differ if it is a parallel group study with different subjects in treatment and controls groups or it is a cross-over study with same subjects being in treatment and control groups at different times?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-10-20T04:55:01.297" Id="120721" LastActivityDate="2014-10-26T17:34:09.487" LastEditDate="2014-10-26T17:34:09.487" LastEditorUserId="56211" OwnerUserId="56211" PostTypeId="1" Score="1" Tags="&lt;anova&gt;&lt;t-test&gt;&lt;mann-whitney-u-test&gt;&lt;method-comparison&gt;" Title="Best way to compare pre vs post levels in treatment vs control groups?" ViewCount="97" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a dataset of fresh matter (FM) weights of salad measured in field over time (6 Dates, 40 Plants each). Those are measured independently, so at every date different plants were measured. As in every field-experiment I have plotted the plants in blocks. &#10;I want to know: Is there an influence of the block on fresh matter production.&lt;/p&gt;&#10;&#10;&lt;p&gt;My approach : &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Scaling and centering the data on every date to account for changes (growth) over time.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Comparing the scaled FM over Blocks with Kruskal-Wallis test:  ScaledFM~Block&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Do I violate any requirements for Kruskal-Wallis or in general for hypothesis testing by doing so?&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-10-20T09:25:25.220" Id="120733" LastActivityDate="2014-10-20T10:10:05.930" LastEditDate="2014-10-20T10:10:05.930" LastEditorUserId="22047" OwnerUserId="58938" PostTypeId="1" Score="0" Tags="&lt;kruskal-wallis&gt;&lt;centering&gt;" Title="Is scaling data valid for Kruskal-Wallis test?" ViewCount="30" />
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm using &lt;code&gt;glmer()&lt;/code&gt; with a binomial response variable. My optimal model has two fixed effects (flow and DNA) which in summary() show a non-significant p value but when I remove each fixed effect in turn from the model the likelihood ratio test comparing the two models shows a significant p value. I'm struggling to understand (1) if this is normal, and (2) how to report the results if the explanatory variables &quot;flow&quot; and &quot;DNA&quot; are important but their p values in the model are well above 0.05?&lt;/p&gt;&#10;&#10;&lt;p&gt;Optimal model:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;a25 &amp;lt;- glmer(Status_qpcr~(1|Root)+Flow+DNA,&#10;             family=binomial, data=spore)&#10;summary(a25)&#10;&#10;Generalized linear mixed model fit by maximum likelihood (Laplace&#10;Approximation) ['glmerMod']  &#10;Family: binomial  ( logit ) &#10;Formula: Status_qpcr ~ (1 | Root) + Flow + DNA   &#10;Data: spore&#10;      AIC      BIC   logLik deviance df.resid &#10;     72.9     81.0    -32.4     64.9       52 &#10;&#10;Scaled residuals: &#10;    Min      1Q  Median      3Q     Max &#10;-2.9318 -0.8163  0.4435  0.6848  1.6133 &#10;&#10;Random effects:  &#10;  Groups Name        Variance Std.Dev.  &#10;  Root   (Intercept) 0.3842   0.6199   &#10;  Number of obs: 56, groups:  Root, 9&#10;&#10;Fixed effects:&#10;Estimate Std. Error z value Pr(&amp;gt;|z|)   &#10;(Intercept) -0.97752    0.79252  -1.233    0.217   &#10;Flow         3.82779    2.27165   1.685    0.092 . &#10;DNA          0.01616    0.01039   1.556    0.120  &#10;--- &#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1&#10;&#10;Correlation of Fixed Effects:&#10;     (Intr) Flow   Flow -0.775        &#10;     DNA    -0.576  0.227&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Likelihood ratio test:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;a26 &amp;lt;- update(a25,~.-DNA)&#10;anova(a25,a26)&#10;&#10;Data: spore &#10;Models: &#10;    a26: Status_qpcr ~ (1 | Root) + Flow &#10;    a25: Status_qpcr ~ (1 | Root) + Flow + DNA&#10;    Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&amp;gt;Chisq)   &#10;a26  3 74.802 80.878 -34.401   68.802                            &#10;a25  4 72.897 80.998 -32.448   64.897 3.9049      1    0.04815 *&#10;&#10;a27 &amp;lt;- update(a25,~.-Flow)&#10;anova(a25,a27)&#10;&#10;Data: spore &#10;Models: &#10;    a27: Status_qpcr ~ (1 | Root) + DNA &#10;    a25: Status_qpcr ~ (1 | Root) + Flow + DNA&#10;    Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&amp;gt;Chisq)&#10;a27  3 78.440 84.723 -36.220   72.440                             &#10;a25  4 72.897 80.998 -32.448   64.897 7.5427      1   0.006025 **&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Thank you.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-20T15:08:01.830" FavoriteCount="1" Id="120768" LastActivityDate="2014-11-22T17:13:20.760" LastEditDate="2014-10-21T13:10:17.367" LastEditorUserId="2126" OwnerUserId="54797" PostTypeId="1" Score="1" Tags="&lt;p-value&gt;&lt;lme4&gt;&lt;glmer&gt;" Title="R lme4 1.1-7 glmer: p values for fixed effects in summary() and likelihood ratio test comparison" ViewCount="335" />
  <row Body="&lt;p&gt;You are right to think of correlation as the mean of the product of the standardized variables: this has great conceptual advantages over other definitions.  It also leads to minimal loss of floating point precision. However, it is not necessarily the best approach for all computation, especially when speed is important.&lt;/p&gt;&#10;&#10;&lt;p&gt;Our goal is to make algebraic manipulations that lead to calculations involving, if possible, dot products of the &lt;em&gt;original&lt;/em&gt; variables, for then the inherent sparse matrix operations ought to make short work indeed of those computations.  Let, then, $\mathbf u$ and $\mathbf v$ be two of the columns of the original sparse binary matrix $\mathbb Y$ (so that the $n$ entries in each of $\mathbf u$ and $\mathbf v$ are only zeros and ones).  Let $\mathbf 1 = (1,1,\ldots, 1)$ be an $n$-vector and write&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\bar u = (u_1 + u_2 + \cdots + u_n)/n = \frac{1}{n}\mathbf u \cdot \mathbf 1$$&lt;/p&gt;&#10;&#10;&lt;p&gt;(and likewise $\bar v =  \frac{1}{n}\mathbf v \cdot \mathbf 1$) for their means and&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\mathbf u_0 = (u_1 - \bar u, u_2 - \bar u, \ldots, u_n - \bar u)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;(and similarly for $\mathbf v_0$) for the centered vectors of their residuals.  Then&lt;/p&gt;&#10;&#10;&lt;p&gt;$$||\mathbf u_0||^2 = \mathbf u_0 \cdot \mathbf u_0$$&lt;/p&gt;&#10;&#10;&lt;p&gt;shows how to find the lengths of the residual vectors which are used to standardize them to&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\mathbf \upsilon = \mathbf u_0 / \sqrt{||\mathbf u_0||/n};\quad\mathbf \phi= \mathbf v_0 / \sqrt{||\mathbf v_0||/n}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;whence, by definition,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\rho_{\mathbf u, \mathbf v} = \mathbf \upsilon \cdot \mathbf \phi.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;(Apart from choices of when to divide by $n$, this appears to be what the code in the question is doing.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Working backwards (by plugging the foregoing into this formula) easily yields&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\eqalign{&#10;\rho_{\mathbf u, \mathbf v} &amp;amp;=\mathbf u_0 / \sqrt{||\mathbf u_0||/n} \cdot \mathbf v_0 / \sqrt{||\mathbf v_0||/n} \\&#10;&amp;amp;=n\frac{\left(\mathbf u - \bar u \mathbf 1\right)\cdot\left(\mathbf v - \bar v \mathbf 1\right)}{\sqrt{\left(\mathbf u - \bar u \mathbf 1\right)\cdot\left(\mathbf u - \bar u \mathbf 1\right)\,\left(\mathbf v - \bar v \mathbf 1\right)\cdot\left(\mathbf v - \bar v \mathbf 1\right)}}.&#10;}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Using the distributive law to expand the dot products shows that&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\eqalign{&#10;\left(\mathbf u - \bar u \mathbf 1\right)\cdot\left(\mathbf v - \bar v \mathbf 1\right) &amp;amp;= \mathbf u \cdot \mathbf v - \frac{2}{n}(\mathbf u \cdot \mathbf 1) (\mathbf 1 \cdot \mathbf v) + \frac{1}{n^2}(\mathbf u \cdot \mathbf 1)(\mathbf v \cdot \mathbf 1)\mathbf 1 \cdot \mathbf 1 \\&#10;&amp;amp;= \mathbf u \cdot \mathbf v -\frac{1}{n}(\mathbf u \cdot \mathbf 1)(\mathbf v \cdot \mathbf 1)&#10;}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;because $\mathbf 1 \cdot \mathbf 1 = n$.  Similar formulas hold for the expressions in the denominator.  This shows how &lt;strong&gt;the correlation coefficient can be computed in terms of dot products of the original (raw, sparse) vectors&lt;/strong&gt;.  Note in particular that the terms in the denominator can be written&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\mathbf u \cdot \mathbf u -\frac{1}{n}(\mathbf u \cdot \mathbf 1)(\mathbf u \cdot \mathbf 1)=n \bar u - \frac{1}{n}(n \bar u)^2 = n(\bar u - \bar u^2)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;An efficient implementation will compute column sums (from which their means $\bar u$ are immediately derived) and obtain all the dot products of all pairs of columns at once by means of a single matrix operation $\mathbb Y^\prime \mathbb Y$.  Using these it is simple and fast to obtain the correlation coefficients with the preceding formula.  To obtain lagged correlations at lag $k$, remove the first $k$ rows of $\mathbb Y$ (call this $\mathbb Y_{(k)}$ and separately remove the last $k$ rows (call this $\mathbb Y_{(-k)}$).  The essential material for the computation can be found in the non-diagonal entries of $\mathbb Y_{(k)}^\prime \mathbb Y_{(-k)}$.  The new denominators will scarcely differ from the old ones and so, to a high degree of accuracy, need not be recomputed at all; but for perfect accuracy note that the column sums in $\mathbb Y_{(k)}$ are of the form&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\sum_{i=k+1}^n u_i = \left(\sum_{i=1}^n u_i\right) - u_k - u_{k-1} - \cdots - u_2 - u_1$$&lt;/p&gt;&#10;&#10;&lt;p&gt;which are easily obtained from the &lt;em&gt;original&lt;/em&gt; column sums by means of just $k$ subtractions (and similarly for the column sums of $\mathbb Y_{(-k)}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thus, computing the entire $21\times 60\times 60$ array comes down to performing $21$ multiplications of &lt;em&gt;sparse binary matrices&lt;/em&gt; and adjusting the results.  The total number of numeric operations involved (with each dot product requiring about $2n$ multiplications and $2n$ additions) will be approximately &lt;/p&gt;&#10;&#10;&lt;p&gt;$$2 \times 5271159 \times 60^2 \times 21 \approx 800 \times 10^9.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Unparallelized, but running as native code on a modern PC, this would take two minutes &lt;em&gt;without any sparse matrix speed improvements&lt;/em&gt;.  Tests in &lt;code&gt;R&lt;/code&gt; (without using sparse arithmetic) with a $5271159 \times 6$ matrix took $1.3$ seconds; the quadratic scaling indicates &lt;code&gt;R&lt;/code&gt; would thereby require $130$ seconds, confirming the two minute estimate.  Given a random binary array of dimensions $5271159\times 60$ and mean of $1/40$, &lt;em&gt;Mathematica 9&lt;/em&gt; took one second to compute $\mathbb Y^\prime \mathbb Y$, suggesting the total computation for all $21$ lags should be around $20$ seconds.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-10-20T15:26:43.823" Id="120771" LastActivityDate="2014-10-20T15:41:35.257" LastEditDate="2014-10-20T15:41:35.257" LastEditorUserId="919" OwnerUserId="919" ParentId="120513" PostTypeId="2" Score="3" />
  
  
  
  <row AcceptedAnswerId="123010" AnswerCount="1" Body="&lt;p&gt;I'm using R(3.1.1), and ARIMA models for forecasting. &#10;I would like to know &lt;strong&gt;what should be the &quot;frequency&quot; parameter, which is assigned in the &lt;code&gt;ts()&lt;/code&gt; function&lt;/strong&gt;, if im using time series data which is:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;separated by &lt;strong&gt;minutes&lt;/strong&gt; and is spread over 180 days (1440 minutes/day) &lt;/li&gt;&#10;&lt;li&gt;separated by &lt;strong&gt;seconds&lt;/strong&gt; and is spread over 180 days (86,400 seconds/day).&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;If I recall right the definition, a &quot;frequency&quot; in ts in R, is the number of observations per &quot;season&quot;. &lt;/p&gt;&#10;&#10;&lt;h2&gt;Question part 1:&lt;/h2&gt;&#10;&#10;&lt;p&gt;then, what is the &quot;season&quot; in my case?&lt;/p&gt;&#10;&#10;&lt;p&gt;If the season is &quot;day&quot;, then is the &quot;frequency&quot; for minutes = 1440 and 86,400 for seconds?&lt;/p&gt;&#10;&#10;&lt;h2&gt;Question part 2:&lt;/h2&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Could the &quot;frequency&quot;  also depend on what I am trying to achieve/forecast?&lt;/strong&gt;&#10;for example, in my case, I'd like to have a very short-term forecast. &#10;One-step ahead of 10minutes each time. &#10;&lt;strong&gt;Would it then be possible to consider the season as an hour instead of a day?&lt;/strong&gt;&#10;In that case frequency= 60 for minutes, frequency = 3600 for seconds?&lt;/p&gt;&#10;&#10;&lt;p&gt;I've tried for example to use frequency = 60 for the minute data and got better results compared to frequency = 1440 (used &lt;code&gt;fourier&lt;/code&gt; see link below by Hyndman)&#10;&lt;a href=&quot;http://robjhyndman.com/hyndsight/forecasting-weekly-data/&quot; rel=&quot;nofollow&quot;&gt;http://robjhyndman.com/hyndsight/forecasting-weekly-data/&lt;/a&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;(The comparison was made by using MAPE for the measure of forecast accuracy)&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;In case the results are complete arbitrary, and the frequency cannot be changed. &#10;What would be actually the interpretation of using freq = 60 on my data?&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;I also think it's worth mentioning that my data contains seasonality at every hour and every two hours (by observing the raw data and the Autocorrelation function)&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-20T19:18:57.983" FavoriteCount="1" Id="120806" LastActivityDate="2014-11-07T01:51:15.527" OwnerUserId="57007" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;time-series&gt;&lt;arima&gt;" Title="&quot;Frequency&quot; value for seconds/minutes intervals data in R" ViewCount="486" />
  <row AnswerCount="0" Body="&lt;p&gt;This is an exercise that I have to finish and I hope you to be patient maybe I violated the rules since that it is not always possible to start solution of an exercise. Maybe can be very useful only steps or detailed answer.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/CYRLN.png&quot; alt=&quot;Exercise from my course Linear Models&quot;&gt;&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-10-20T19:32:35.457" Id="120807" LastActivityDate="2014-10-20T19:56:58.073" LastEditDate="2014-10-20T19:56:58.073" LastEditorUserId="919" OwnerUserId="58967" PostTypeId="1" Score="0" Tags="&lt;pdf&gt;&lt;joint-distribution&gt;" Title="Show about an arbitrary measurable function that has density" ViewCount="28" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I have a data set that is the result of a survey. The survey asks the respondents to name 5 people in their community whom they turn to for advice. It then goes to these 5 people and asks the same. I have calculated in-degree for each person (the number of people who have chosen them) and I would like to understand how the person's various features affect their in-degree.&lt;/p&gt;&#10;&#10;&lt;p&gt;The survey asked each person their age range, gender, location, religion, how often they read/watch news/go online..etc. (daily, weekly, monthly), job and other similar questions considering the person. I have information missing about 44% of the people who were mentioned, since not everybody who was mentioned was surveyed.&lt;/p&gt;&#10;&#10;&lt;p&gt;They also asked questions about the relationship between the person and the person they turn to for advice - how long thay have known each other (5-10 years, less than 5 years, 10-20 years, over 20 years), how often they meet and other similar things. I have relationship information for every connection.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to understand which predictors best affect a person's chance to have a high in-degree as well as which relationship attributes influence a person's decision to choose somebody as the person they turn to for advice.&lt;/p&gt;&#10;&#10;&lt;p&gt;Which statistical methods should I be looking into? Do I need to transform all my categorical variables? What would be the best way to do that? Also, in-degree distribution, so the outcome variable, is not normally distributed, it is extremely left-skewed.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-20T20:56:34.567" Id="120818" LastActivityDate="2014-10-22T04:42:22.673" OwnerUserId="17518" PostTypeId="1" Score="0" Tags="&lt;multiple-regression&gt;&lt;survey&gt;&lt;networks&gt;" Title="Survey analysis: relationship between one discrete outcome variable and multiple categorical and ordered categorical variables" ViewCount="28" />
  
  
  <row AcceptedAnswerId="120845" AnswerCount="1" Body="&lt;p&gt;I attempted to build a deep network (e.g. deep autoencoder) for some object classification, my result showed that the deep networks is worst than shallow network. However, from what I have read from lecture, deep network perform well. This raise me a question: does deep always better than shallow? If not, in what situation?&lt;/p&gt;&#10;&#10;&lt;p&gt;Is that any existing problem (published) showing that a shallow network is better than a deep network?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-20T23:59:37.977" Id="120840" LastActivityDate="2014-10-21T00:31:57.200" OwnerUserId="41749" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;neural-networks&gt;&lt;deep-learning&gt;" Title="Does Deep network (e.g. # of hidden layer=2) always better than shallow network (i.e. # of hidden layer=1)?" ViewCount="47" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm looking at the effects of inbreeding on survival in a captive animal species. I'm trying to clearly distinguish the effects of inbreeding from other possible random genetic factors on survival. &#10;I've done so by including a random factor which denotes non-independence between individuals that share the same mother and father in a Cox mixed effects model in &lt;code&gt;coxme&lt;/code&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to calculate the r2 of my top model to evaluate it's applicability to the data before I make any inferences based on the model outputs. Unfortunately, I have found no information on doing this with Cox mixed effects models, and have found no evidence for it being done in similar studies. &lt;/p&gt;&#10;&#10;&lt;p&gt;My Question is thus in 3 parts:  &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Is calculating r2 values to evaluate the fit of cox mixed effects models possible, and appropriate? &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;If calculating r2 is not possible or inappropriate, how do we evaluate the fit of our models to the data? &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;If r2 is possible, then how do I go about doing this? &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I'm still new to this kind of analysis (that's probably quite obvious), and any help would be greatly appreciated.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-21T00:39:02.057" Id="120846" LastActivityDate="2014-10-21T00:39:02.057" OwnerUserId="55793" PostTypeId="1" Score="0" Tags="&lt;survival&gt;&lt;cox-model&gt;" Title="Is calculating r2 appropriate for cox mixed effect models?" ViewCount="13" />
  
  <row Body="&lt;p&gt;That depends on the process generating these outcome, but if you think of a &quot;memoryless&quot; process where each outcome is independent of the previous ones, such as flipping coins, then it doesn't matter how many heads you flipped in the past, the probability of flipping heads in the next flip is always 0.5. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-10-21T02:52:46.993" Id="120856" LastActivityDate="2014-10-21T02:52:46.993" OwnerUserId="10119" ParentId="120831" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;The likelihood function is $p(y_i | \beta) = N(y_i; \sum_k \beta_k \mu_k, \sum_k \beta_k^2 \sigma_k)$, so a conjugate prior for $\beta$ would be proportional to this.  Unfortunately, this is not a standard distribution unless $\mu=0$ and $K=\{k\}$, in which case it is a &lt;a href=&quot;http://en.wikipedia.org/wiki/Generalized_gamma_distribution&quot; rel=&quot;nofollow&quot;&gt;generalized Gamma distribution&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-21T07:57:12.980" Id="120875" LastActivityDate="2014-10-21T08:20:24.093" LastEditDate="2014-10-21T08:20:24.093" LastEditorUserId="2074" OwnerUserId="2074" ParentId="120843" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;So I have some understanding of how to optimize Lagrangian functions for single values.  But for a support vector machine (SVM) we have:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$L = \sum_{i=1}^N \alpha_i - \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i  \alpha_j y_i y_j K(x_i, x_j)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I understand this involves using quadratic programming to solve for the alpha values (since there's more than one?) but as someone new to this area of math I'm having trouble understanding the process to find them.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any tips?  Apologies for the notation problems, not sure how to markup the equation correctly.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-21T10:00:47.027" Id="120891" LastActivityDate="2014-10-21T13:12:31.940" LastEditDate="2014-10-21T13:12:31.940" LastEditorUserId="25433" OwnerUserId="59020" PostTypeId="1" Score="0" Tags="&lt;svm&gt;" Title="Optimizing Lagrangian for SVM Alpha Values" ViewCount="32" />
  <row AnswerCount="2" Body="&lt;p&gt;Let's say I have 6 random variables whose distribution is parametrized by $\theta_1,\ldots ,\theta_6$ and I constructed confidence intervals for all 6 parameters. Minimal values in the confidence interval ranges for first 3 parameters are higher than maximal values for next 3 parameters.&#10;Does it allow me to say that first 3 variables are significantly higher than latter 3 without performing any test for means?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-21T10:33:57.063" Id="120897" LastActivityDate="2014-10-21T14:30:03.043" LastEditDate="2014-10-21T12:00:02.473" LastEditorUserId="28705" OwnerUserId="22392" PostTypeId="1" Score="0" Tags="&lt;confidence-interval&gt;" Title="Significance of difference based on confidence intervals" ViewCount="28" />
  
  <row AnswerCount="0" Body="&lt;p&gt;EWMA is defined as follows(&lt;a href=&quot;http://en.wikipedia.org/wiki/EWMA_chart&quot; rel=&quot;nofollow&quot;&gt;1&lt;/a&gt;, &lt;a href=&quot;http://www.itl.nist.gov/div898/handbook/pmc/section3/pmc324.htm&quot; rel=&quot;nofollow&quot;&gt;2&lt;/a&gt;, &lt;a href=&quot;http://www.investopedia.com/articles/07/ewma.asp&quot; rel=&quot;nofollow&quot;&gt;3&lt;/a&gt;, &lt;a href=&quot;http://pandas.pydata.org/pandas-docs/stable/generated/pandas.stats.moments.ewma.html&quot; rel=&quot;nofollow&quot;&gt;4&lt;/a&gt;, &lt;a href=&quot;http://www.stat.purdue.edu/~kuczek/stat513/Weighted%20moving%20average%20charts.pptx&quot; rel=&quot;nofollow&quot;&gt;5&lt;/a&gt;):&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \mu_{N} = \frac {1}{N} \Sigma_{i=1}^{N}x_i $$&#10;$$ =\frac {1}{N} \Sigma_{i=1}^{N-1}x_i + \frac{1}{N}x_{N} $$&#10;$$ =\frac {N-1}{N} \frac{1}{N-1} \Sigma_{i=1}^{N-1}x_i + \frac{1}{N}x_{N} $$&#10;$$ =\frac {N-1}{N} \mu_{N-1}+ \frac{1}{N}x_{N} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;at which point the following assumption is made:&#10;$$ \mu_{N} = \mu_{N-1} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;the general form is simplified using $\lambda $ to a more canonical EWMA by performing the following substitution:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \frac {1}{N} = \lambda $$&#10;and&#10;$$ \frac {N-1}{N} = 1-\lambda $$&lt;/p&gt;&#10;&#10;&lt;p&gt;yielding:&#10;$$\mu_{n} = \left( 1- \lambda\right) \mu_{n-1} + \lambda \cdot x_{i} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;And now one can, for the cost of a location to store the historic value and a few multiplications, have a recursive estimate of an updated mean given the input of a single point.  This is definition - it is what is done (&lt;a href=&quot;http://www.itl.nist.gov/div898/handbook/mpc/section3/mpc3522.htm&quot; rel=&quot;nofollow&quot;&gt;6&lt;/a&gt;, &lt;a href=&quot;http://support.sas.com/documentation/cdl/en/qcug/63964/HTML/default/viewer.htm#qcug_macontrol_a0000000362.htm&quot; rel=&quot;nofollow&quot;&gt;7&lt;/a&gt;).  One set of formalists use it in process control, others in investing.  Those who use it in the stock market are more likely, in my personal opinion, to be using voodoo than those who use it for process control.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is - what if I need to construct a different, arbitrary window size mean at a moments notice?  Can I use two of these EWMA creatures to make any EWMA value?  I think that I can, but I was hoping that the amazing and bright folks on CrossValidated might tell me where I made any mistake, or at least my thinking can use improvement.&lt;/p&gt;&#10;&#10;&lt;p&gt;If I modify the previous derivation, given here that $k$ is less than $N$, as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \mu_{N} = \frac {1}{N} \Sigma_{i=1}^{N}x_i $$&#10;$$ =\frac {1}{N} \Sigma_{i=1}^{N-k}x_i + \frac{1}{N}\Sigma_{N-k+1}^{N}x_{i} $$&#10;$$ =\frac {N-k}{N} \frac{1}{N-k} \Sigma_{i=1}^{N-1}x_i + \frac {k}{N} \frac{1}{k} \Sigma_{i=N-k+1}^{N}x_i  $$&#10;$$ =\frac {N-k}{N} \mu_{N-k}+ \frac{k}{N} \mu_{k} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;Similar substitution applies here, but subscript helps assert that the two parameters are not necessarily the same number:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \frac {1}{N} = \lambda_{2} $$&#10;and&#10;$$ \frac {N-1}{N} = 1-\lambda_{2} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;yielding:&#10;$$\mu_{n} = \left( 1- \lambda _{2}\right) \mu_{n-k} + \lambda_{2} \cdot \mu_{k} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;From this expression I infer that I can construct a longer period EWMA average as a sum of two shorter ones, or that I can construct a shorter period EWMA as the difference of two longer ones.  Note that while each of the $\mu$ values is determined as a function of time, the updated and allegedly arbitrary window size is computed across $ \mu $ values.  &lt;/p&gt;&#10;&#10;&lt;p&gt;It seems to suggest that, with two of these, I can make many (or any) of these.  While neat, it feels wrong to think that with two window-sizes (aka two weight values) I can infer the EWMA for (m)any window-size (or weight value).&lt;/p&gt;&#10;&#10;&lt;p&gt;Question list:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;How does the assumption that $\mu_{N}=\mu_{N-1} $ cause errors in the sums?  &lt;/li&gt;&#10;&lt;li&gt;Where does this approach break down in the &quot;real&quot; world?   &lt;/li&gt;&#10;&lt;li&gt;Has anyone tried this?  Has this been done somewhere (outside of stock-chart analysis) with rigor?  If so, can you provide references?&lt;/li&gt;&#10;&lt;li&gt;Does some special form of noise limit the span of the sum?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;UPDATE:&lt;/p&gt;&#10;&#10;&lt;p&gt;Slopes are defined as:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\frac {dy}{dx} =  \lim_{a \to 0} \frac{f(x+a)-f(a)}{(x+a)-a}  $&lt;/p&gt;&#10;&#10;&lt;p&gt;Using discrete sampling, where the $a$ cannot get all the way to zero, this can be approximated as:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac {dy}{dx} =  \frac{\Delta y}{\Delta x}  $$&lt;/p&gt;&#10;&#10;&lt;p&gt;But the $\Delta y$ can be approximated as a difference of two averages as:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\Delta y = \bar{y_2} - \bar{y_1} = EWMA_2-EWMA_1 $$&lt;/p&gt;&#10;&#10;&lt;p&gt;If I know the window size, and I make sure to align the offset of center values (not the leading values) so that the $\bar{y_1}$ and $\bar{y_2}$ are the best estimate of the mean at $\bar{x_2}$ and $\bar{x_1}$ then can I express the slope as:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac {dy}{dx} \approx \frac {EWMA_2 - EWMA_1} {\bar{x_2}-\bar{x_1} }$$&lt;/p&gt;&#10;&#10;&lt;p&gt;As long as the window-size is appropriate to the characteristic scale of change of the underlying curves, should this have too much error?&lt;/p&gt;&#10;&#10;&lt;p&gt;Questions, comments, and suggestions are all solicited.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-21T12:34:59.933" Id="120909" LastActivityDate="2014-11-17T17:25:22.093" LastEditDate="2014-11-17T17:25:22.093" LastEditorUserId="22452" OwnerUserId="22452" PostTypeId="1" Score="2" Tags="&lt;mean&gt;&lt;weighted-mean&gt;&lt;moving-average&gt;&lt;ewma&gt;" Title="with two EWMA means of different window sizes, can we estimate any other means? slopes? jumps?" ViewCount="24" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I am using a Gaussian model with a conjugate Normal-Inverse-Wishart (NIW) prior, as described &lt;a href=&quot;https://www.seas.harvard.edu/courses/cs281/papers/murphy-2007.pdf&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;. The advantage of this approach is that the marginal likelihood $p(y)$, which is what I am interested in, is available in closed form. &lt;/p&gt;&#10;&#10;&lt;p&gt;My problem is that the results seem to be dependent of the NIW hyper-parameters (I have no prior information), with some of the dangers being described &lt;a href=&quot;http://dahtah.wordpress.com/2012/03/07/why-an-inverse-wishart-prior-may-not-be-such-a-good-idea/&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;As an alternative, I am considering bootstrapping my data in order to obtain $m$ estimate of the mean and covariance. Then I could calculate the marginal likelihood:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;p(y) \approx \frac{1}{m} \sum_{i=1}^m p(y|\hat{\mu}_i, \hat{\Sigma}_i).&#10;$$  &lt;/p&gt;&#10;&#10;&lt;p&gt;Would this prior be an approximation to an Empirical Bayes prior, something else or just nonsense? &lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-21T13:12:50.013" FavoriteCount="1" Id="120914" LastActivityDate="2015-02-11T09:12:14.503" OwnerUserId="26105" PostTypeId="1" Score="4" Tags="&lt;bootstrap&gt;&lt;prior&gt;&lt;resampling&gt;&lt;conjugate-prior&gt;&lt;empirical&gt;" Title="Bootstrapping the data to set up a prior" ViewCount="111" />
  <row Body="&lt;p&gt;You might also consider using function &lt;code&gt;mixed&lt;/code&gt; in package &lt;code&gt;afex&lt;/code&gt; to return p values with Kenward-Roger df approximation, which returns identical p values as a paired t test:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(afex)&#10;mixed(y ~ x + (1|subj), type=3,method=&quot;KR&quot;,data=myDat) &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Or &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(lmerTest)&#10;options(contrasts=c('contr.sum', 'contr.poly'))&#10;anova(lmer(y ~ x + (1|subj),data=myDat),ddf=&quot;Kenward-Roger&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-10-21T13:13:25.763" Id="120915" LastActivityDate="2014-10-21T14:27:47.900" LastEditDate="2014-10-21T14:27:47.900" LastEditorUserId="27340" OwnerUserId="27340" ParentId="23276" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I am supposing that this is not a challenging exercise in some course. The context for this would be of interest.&lt;/p&gt;&#10;&#10;&lt;p&gt;We suppose from your 1) that you have 3 points known for a function, the quantile function that yields quantiles (percentiles) for given cumulative probabilities. &lt;/p&gt;&#10;&#10;&lt;p&gt;To get other points, you can use any method you like from general interpolation and extrapolation methods to assuming that data come from particular named distributions (e.g. Gaussian). But all methods are highly problematic; even injecting assumptions to make the problem tractable has the risk that the given quantiles contradict the assumptions being made. A simple example is that distance between the 10th and 50th percentiles and that between the 50th and 90th percentiles is equal in any symmetric distribution, so the assumption of any symmetric distribution would contradict your numeric example. Naturally you would be at liberty to assume an asymmetric distribution instead, or work on the basis that we don't usually expect assumptions to hold exactly. &lt;/p&gt;&#10;&#10;&lt;p&gt;So, the only safe answer to 1) is &quot;all the data&quot;. &lt;/p&gt;&#10;&#10;&lt;p&gt;The second problem 2) is even more intractable, if possible. Without knowing anything else about the data, it is entirely possible that no values are &quot;at&quot; a particular reported percentile, which is true whenever it lies between two different observed values, but is not itself observed; and also entirely possible whenever ties are present in the data that several values are &quot;at&quot; a particular percentile, or indeed that several percentiles are reported to be identical. Concocted examples such as 1,1,1,1,1,1,3 illustrate. &lt;/p&gt;&#10;&#10;&lt;p&gt;Knowing the sample size doesn't really help. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-21T15:28:57.950" Id="120929" LastActivityDate="2014-10-21T16:36:26.043" LastEditDate="2014-10-21T16:36:26.043" LastEditorUserId="22047" OwnerUserId="22047" ParentId="120923" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;You create the parameter trace plots to make sure that your a priori distribution is well calibrated which is indicated by your parameters having sufficient state changes as the MCMC algorithm runs.  &lt;/p&gt;&#10;&#10;&lt;p&gt;An extreme example is that you set your a priori distribution variance at 0.  Then the posterior parameter estimate will never change.  Your algorithm would say that you have the best parameter estimate, but it didn't check a sufficient number of parameters to determine if this truly is the best fit.  If you set the a priori distribution variance too high, you get a similar problem.  This is because the new parameter is less likely to be related to your data - so the log likelihood calculated with your new parameter is not likely to be better than the log likelihood using the old parameter.  (An example is if your &quot;true&quot; parameter is 0.5 and your initial estimate is 2, but you are selecting from a normal distribution with a mean of 2 and a variance of 10,000 then you are unlikely to get a parameter that is closer to 1.5 than your initial estimate of 2.)&lt;/p&gt;&#10;&#10;&lt;p&gt;You need to select an a priori variance that allows your parameter states to change enough that you don't get stuck on local minimums and maximums in the loglikelihood distribution, but yet fine enough that you get reasonable parameter estimates.  Most literature suggests you get your parameters to change states 40-60% of the time.  &lt;/p&gt;&#10;&#10;&lt;p&gt;One other reason for the trace plots is burn in.  Usually the burn in period is obvious in the plot (for example, if the true parameter is 1.5 and your initial estimate is 4 then you should see the parameter estimates moving quickly from 4 to 1.5 and then &quot;bouncing&quot; around 1.5).  Typically, you just exclude the first n iterations where n is large enough that you are certain to have removed the burn in (say 1000), but if the calculations are time consuming or if your parameter estimates are taking much longer to converge than your n allows then you may want to omit more or less observations to account for burn in.  You can check your plots to see where the burn in period ends to make sure that burn in is not affecting your results.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that I have been talking in context of parameter point estimates.  If you are estimating parameter variance then ensuring that you have appropriate state changes is even more important.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-10-21T16:11:32.780" Id="120937" LastActivityDate="2014-10-28T15:53:48.497" LastEditDate="2014-10-28T15:53:48.497" LastEditorUserId="45603" OwnerUserId="45603" ParentId="120936" PostTypeId="2" Score="4" />
  <row AnswerCount="1" Body="&lt;p&gt;For calculating prevalence and incidence rates... I am not sure if I should be subtracting &quot;deaths at end of year&quot; from the population total/at risk?&lt;/p&gt;&#10;&#10;&lt;p&gt;I know that &lt;strong&gt;incidence rate&lt;/strong&gt; involves the the number of new cases/total AT RISK during x year (&lt;em&gt;see question 1 example&lt;/em&gt;). Do I need to subtract the &quot;disease deaths&quot; from the &quot;total at risk&quot; population denominator if they occur by the &lt;strong&gt;end of year&lt;/strong&gt;? &lt;/p&gt;&#10;&#10;&lt;p&gt;And for calculating &lt;strong&gt;prevalence&lt;/strong&gt; at end of year (see &lt;em&gt;example question #3&lt;/em&gt;), do I need to subtract &quot;deaths from disease&quot; from the total population denominator?&lt;/p&gt;&#10;&#10;&lt;p&gt;Example: &#10;During year 2010, 5000 newly diagnosed cases of a disease occurred in a city population of 100,000. At the beginning of the year, there were a total of 20,000 people with the disease in the city. By the end of the year &lt;strong&gt;3000 died from the disease.&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;1. What is the incidence rate of the disease during 2010?&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;ol start=&quot;2&quot;&gt;&#10;&lt;li&gt;What is the prevalence of disease on Jan 1 2010?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;3. What is the prevalence of disease on Dec 31 2010?&lt;/strong&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-21T16:20:22.050" Id="120940" LastActivityDate="2015-03-05T08:58:51.050" LastEditDate="2014-10-21T16:59:53.963" LastEditorUserId="22311" OwnerUserId="59039" PostTypeId="1" Score="0" Tags="&lt;epidemiology&gt;" Title="Do I subtract end-of-year deaths from total population? (prevalence and incidence)" ViewCount="63" />
  
  <row AcceptedAnswerId="121000" AnswerCount="1" Body="&lt;p&gt;Let $A$ and $B$ be random variables and $f(A,B)=\frac{A}{B}$. How should I approximate $E(f(A,B))$? I think a Taylor expansion may be in order, but I am not sure how to fire it off in this function.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question comes from a practical problem in survey statistics. It may be discussed in textbooks, but I would not know where. Let a sample of size $n$ be taken from an (infinite) population. Not every sample unit may reply to the survey. Let $S$ indicate response ($S=1$) or non-response ($S=0$). The mean estimator $\hat{\mu}=\frac{1}{\sum{S_i}}\sum{S_iy_i}$ thus may be biased due to non-response from the population mean $\mu$. We may be inclined to specify this bias as $$B(\hat{\mu})=E(\frac{1}{\sum{S_i}}\sum{S_iy_i})-\mu$$ for realizations $y_i$ on random variable $Y$. Let $n_r$ be the realized sample size (response sample). I believe $$E(\frac{1}{\sum{S_i}})=\frac{1}{n_r}$$ is only true approximately and relates to the more general problem introduced above, where $A=1$. My specific question is why this equation holds true (approximately).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-21T18:06:09.373" FavoriteCount="1" Id="120956" LastActivityDate="2014-10-22T12:34:28.603" LastEditDate="2014-10-22T12:34:28.603" LastEditorUserId="17230" OwnerUserId="24515" PostTypeId="1" Score="5" Tags="&lt;mathematical-statistics&gt;&lt;ratio&gt;" Title="How should I evaluate the expectation of the ratio of two random variables?" ViewCount="107" />
  
  <row Body="&lt;p&gt;For me, censoring means that we observe partial-information about an observation $Z_i$. What I mean by this is that, rather than observing $Z_i = z_i$ we observe $Z_i \in a_i$ where $a_i$ is the realization of $A_i$, which is some random coarsening of the sample space. We might imagine that we first select a partition $\mathcal A_i$ of the sample space $\mathcal Z$, then $Z_i$ is generated, and we report the $A_i \in \mathcal A_i$ such that $Z_i \in A_i$ (equivalently, we report $I(Z_i \in A)$ for all $A \in \mathcal A_i$). Uninformative censoring of $Z_i$, for example, then means that $\mathcal A_i$ is independent of $Z_i$. &lt;/p&gt;&#10;&#10;&lt;p&gt;This is a little heuristic and sloppy. We should probably also require that the distribution of $[Z_i \mid Z_i \in a_i]$ is non-degenerate to consider $Z_i$ censored. We also might note that, as defined, this is a generalization of &lt;em&gt;missing data&lt;/em&gt; where for $Z_i = (X_i, Y_i)$ one might say $Y_i$ is missing if $a_i = \{x\} \times \mathcal Y$ where $\mathcal Y$ is the sample space of $Y$ and say $Z_i$ is missing if $a_i = \mathcal Z$. When one says &quot;$Z_i$ is censored&quot;, if they are following my definition, what they usually mean is &quot;$Z_i$ is censored, but is not missing&quot;. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-21T18:51:41.577" Id="120960" LastActivityDate="2014-10-21T18:51:41.577" OwnerUserId="5339" ParentId="120579" PostTypeId="2" Score="0" />
  <row Body="&lt;ol&gt;&#10;&lt;li&gt;Censored: This is a term used to indicate that the period of observation was cut off before the event of interest occurred. So ''censored data'' indicate that the period of a particular event as not or never occurred&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="2" CreationDate="2014-10-21T19:25:39.693" Id="120965" LastActivityDate="2014-10-21T19:25:39.693" OwnerUserId="59048" ParentId="120579" PostTypeId="2" Score="0" />
  
  
  <row Body="&lt;p&gt;In general, it's best to not use degree as a dependent variable when both the sender and receiver are included in the data. It's a problem of interdependent observations--an actor's indegree depends both upon their propensity to receive (or not receive) ties as well as the propensity of all other actors to send (or not send) ties, in addition to triadic and other higher order effects.&lt;/p&gt;&#10;&#10;&lt;p&gt;Rather then posing the question as an actor ability to have high (or low) indegree, it's best to pose the question as a matter of tie formation for the entire network. Some of the variables will be measured at the individual level and these measurements can correspond to an actor's propensity to receive ties.&lt;/p&gt;&#10;&#10;&lt;p&gt;The method to look into is referred to as p* or &quot;exponential random graph models.&quot; The literature on this method is large and there are a number of resources and tutorials readily available.&lt;/p&gt;&#10;&#10;&lt;p&gt;Lastly, it's unusual that the degree distribution is left-skewed. Degree distributions are usually right-skewed.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-22T04:42:22.673" Id="121012" LastActivityDate="2014-10-22T04:42:22.673" OwnerUserId="36673" ParentId="120818" PostTypeId="2" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;$$H= X(X'X)^{-1}X'$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I understand that if we multiply $Y$ matrix by $H$ matrix, then we will have $\hat Y$. That's why we call it $H$ matrix. &lt;/p&gt;&#10;&#10;&lt;p&gt;Can someone please let me know what $h_{ii}$ ($i$-th row $i$-th column of $H$ matrix) mean?&lt;br&gt;&#10;My teacher said it means &quot;HOW FAR OUT IN THE X-SPACE IS i-th OBSERVATION&quot; but I don't understand what that means...&lt;/p&gt;&#10;" ClosedDate="2014-10-22T07:22:30.413" CommentCount="1" CreationDate="2014-10-22T06:32:11.263" Id="121023" LastActivityDate="2014-10-22T07:18:58.053" LastEditDate="2014-10-22T07:18:58.053" LastEditorUserId="2116" OwnerUserId="59072" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;matrix&gt;" Title="What does hat matrix mean?" ViewCount="31" />
  <row Body="&lt;p&gt;You're absolutely right, normally distributed does not imply homoskedastic, and your example is great.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-22T07:30:05.543" Id="121027" LastActivityDate="2014-10-22T07:30:05.543" OwnerUserId="52432" ParentId="121022" PostTypeId="2" Score="5" />
  
  
  
  <row Body="&lt;p&gt;You cannot sample the sum without making distributional assumptions, but you can compute its variance, using the formula&#10;$$&#10;{\rm var}(X+Y) = {\rm var}(X) + {\rm var}(Y) + 2 {\rm cov}(X,Y)&#10;$$&#10;since ${\rm cov}(X,Y) = \rho \sqrt{{\rm var}(X) {\rm var}(Y)}$.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-10-22T13:35:38.627" Id="121057" LastActivityDate="2014-10-22T13:35:38.627" OwnerUserId="2074" ParentId="121038" PostTypeId="2" Score="4" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I am working on research paper for diagnosis of cancer.&lt;/p&gt;&#10;&#10;&lt;p&gt;List of Known prognostic factors&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Age of patient &lt;/li&gt;&#10;&lt;li&gt;Size of tumor&lt;/li&gt;&#10;&lt;li&gt;Grade of tumor&lt;/li&gt;&#10;&lt;li&gt;Lymphnode involvement&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;and list of Unknown factors which are to be assessed with prognosis by correlating with known prognostic factors.&#10;u1,u2 and u3&lt;/p&gt;&#10;&#10;&lt;p&gt;I have  128 patient records for all three unknown factor.&#10;I have 231 patient records for u1 data is there and have missing u2 and u3 data.&lt;/p&gt;&#10;&#10;&lt;p&gt;I wanted to know what statistical test and data visualization I can use PCA or Multiple anova to show the relationship between the known and unknown factors.  &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-10-22T17:34:10.640" Id="121082" LastActivityDate="2014-10-28T17:28:59.237" OwnerUserId="59107" PostTypeId="1" Score="0" Tags="&lt;hypothesis-testing&gt;&lt;correlation&gt;&lt;multiple-regression&gt;" Title="Number of possible statistical test can be used on the observation data." ViewCount="44" />
  
  <row AnswerCount="0" Body="&lt;p&gt;If $Y_0$ and $Y_1$ both have Weibull distribution i.e. $Y_0 \sim Weibull(\lambda_0,\beta_0)$ and $Y_1 \sim Weibull(\lambda_1,\beta_1)$  then what will be cumulative density function of $Y_0+Y_1$, i.e. $$\Pr(Y_0+Y_1&amp;lt;y)=\int_0^\infty \Pr(Y_0&amp;lt;y-z|z=Y_1)f_{Y_1}(Y_1)\ dY_1$$&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-10-22T19:16:01.380" Id="121093" LastActivityDate="2014-10-22T19:16:01.380" OwnerUserId="59111" PostTypeId="1" Score="0" Tags="&lt;distributions&gt;&lt;weibull&gt;&lt;convolution&gt;" Title="Sum of two independent Weibull Random Variable" ViewCount="87" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I don't know if this is a really silly question as I'm in no way a statistician and I don't know if this is something that's actually quite rudimentary... Thanks for reading in advance too it got kind of wordy too since I don't have a sample data set.&lt;/p&gt;&#10;&#10;&lt;p&gt;Essentially I have data that is plotted as a linear series that represents random sampling (&lt;strong&gt;I'm essentially trying to determine true outliers from what would be an incorrect call based on ascertainment bias&lt;/strong&gt;). I'm plotting along an x-axis, where as x increases, my sample size increases (it's not actually a time-series, but is similar). Each of the data are sampled randomly (think percent heads vs # coin tosses plotted along a time series). I'm basically trying to pull out and record events that appear as an outlier to the trend. &lt;/p&gt;&#10;&#10;&lt;p&gt;I am aware of interquartile ranges and moving averages. I can't use calculations involving interquartile range as some data are more heavily affected by ascertainment bias than others (esp at low sample size). THIS IS KEY: &lt;strong&gt;the goal is not for the algorithm to tell me that outliers occur at low sample sizes [as this is obvious], but I want it to specifically call outliers taking ascertainment bias into consideration (ie. a coin tossed twice, both times producing heads is NOT an outlier, even though heads is at 100% and expected average is 50%, yet a coin tossed 10000x and heads is at 80% would potentially be an outlier&lt;/strong&gt;). Note I only have one entry per sample size (so I definitely can't do IQR within a single sample size). I can't change this, my coin is flipped 10x only once before I move on to 11x and then 12x etc.&lt;/p&gt;&#10;&#10;&lt;p&gt;I don't see the point in using moving average since, I already know what the presumed intended average is supposed to be (in a coin toss example, I know the average is to approach 50% as sample size increases).&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a simple algorithm one could use for this? I don't have an actual example dataset yet, but essentially I'm sampling data completely randomly. The easiest applicable example is the coin toss example I have been using. If you can cater an answer to that I can just work on integrating that into my dataset after.&lt;/p&gt;&#10;&#10;&lt;p&gt;Edit: To clarify, I'm tossing a different coin for each data point along the x axis (ie. all of the 10x tosses are with one coin, 11x a different coin, 12x a yet different coin, etc.). The point is to identify coins that are NOT at 50:50 heads vs tails. I know the immediate reaction is to wonder why I don't toss them all the same # of times, but this is something out of my control, that's the current situation I am working with. Also, I'm asking about the &quot;best&quot; method to pull out as many true outliers as statistically possible, I know I can't determine all of them (the coin tossed only once is clearly impossible to determine whether it is a 50:50 coin or not).&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-10-22T20:43:23.593" Id="121103" LastActivityDate="2014-10-22T20:54:51.727" LastEditDate="2014-10-22T20:54:51.727" LastEditorUserId="48613" OwnerUserId="48613" PostTypeId="1" Score="1" Tags="&lt;mathematical-statistics&gt;&lt;outliers&gt;&lt;algorithms&gt;" Title="how to determine outliers in sample affected by ascertainment bias" ViewCount="31" />
  <row AnswerCount="0" Body="&lt;p&gt;I have two correlation matrices(Pearson).For example, metrices of (3 gene X 3 gene) for cancer samples and normal samples,individually. I have got gene-gene correlation for both sample. Now I want to compare these two matrices. I am not sure about what can be the output of this comparison,may be scores or any statistics.&#10;Anyone can help me out?&#10;Thanks in advance!&lt;/p&gt;&#10;" ClosedDate="2014-10-22T23:29:53.850" CommentCount="1" CreationDate="2014-10-22T20:11:34.043" Id="121108" LastActivityDate="2014-10-22T21:59:27.267" OwnerDisplayName="jessy" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;matrix&gt;" Title="Comparison of two correlation matrix of same dimension in R" ViewCount="13" />
  <row AnswerCount="0" Body="&lt;p&gt;Bayesian Estimation Supersedes the t-Test for &lt;strong&gt;John K. Kruschke&lt;/strong&gt; is one of the most important papers that I had read explaining how to run the Bayesian analysis and how to make the plots. &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;But the most important point is how to report the results from the plots to write up a paper? &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;My question here is: Do you know any model paper with a clear guideline explaining how to report the results of Bayesian analysis or if someone can help me to report the results from the following figures (The figures are a comparison between the means for two groups):&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/axXwY.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-10-23T02:27:32.743" FavoriteCount="1" Id="121119" LastActivityDate="2014-10-23T04:14:25.017" LastEditDate="2014-10-23T04:14:25.017" LastEditorUserId="59123" OwnerUserId="59123" PostTypeId="1" Score="3" Tags="&lt;bayesian&gt;" Title="How to write up and report a Bayesian analysis?" ViewCount="138" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Can CNNs be used with input data which is not an image? The reason I'm asking is because the original image is often clipped in size because of border effects when doing the convolution.  &lt;/p&gt;&#10;&#10;&lt;p&gt;But if the input is not an image, and I really need to use all the input data, is it possible to overcome the problem?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-10-23T03:05:32.617" Id="121121" LastActivityDate="2014-10-23T09:13:18.443" OwnerUserId="4889" PostTypeId="1" Score="0" Tags="&lt;classification&gt;&lt;neural-networks&gt;&lt;convolution&gt;" Title="Convolutional neural network with non-image input data" ViewCount="57" />
  <row AnswerCount="2" Body="&lt;p&gt;I need to implement anomaly detection on several time-series datasets.  I've never done this before and was hoping for some advice.  I'm very comfortable with python, so I would prefer the solution be implemented in it (most of my code is python for other parts of my work).&lt;/p&gt;&#10;&#10;&lt;p&gt;Description of the data:&#10;It's monthly time-series data that has only just begun to be collected in the &lt;strong&gt;past 2 years or so&lt;/strong&gt; (i.e. only 24-36 time periods).  Essentially, there are several metrics being monitored on a monthly basis for several clients.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;time_period    client    metric    score&#10;01-2013        client1   metric1   100&#10;02-2013        client1   metric1   119&#10;01-2013        client2   metric1   50&#10;02-2013        client2   metric2   500&#10;...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Here's what I'm thinking: pull data into a dataframe (pandas), then calculate a rolling 6 month average for each client / metric pair.  If the current time period's value exceeds some threshold based on the 6-month avg., then raise flag.  The problem seems rather simple. I just want to make sure I'm taking a solid approach.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any advice to flesh this idea out a bit would be greatly appreciated.  I know the question is a bit abstract, and I apologize for that.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-10-23T05:57:44.383" FavoriteCount="1" Id="121134" LastActivityDate="2015-01-04T16:40:00.870" OwnerUserId="59143" PostTypeId="1" Score="2" Tags="&lt;time-series&gt;&lt;machine-learning&gt;&lt;python&gt;&lt;computational-statistics&gt;" Title="Time Series Anomaly Detection with Python" ViewCount="437" />
  <row AnswerCount="1" Body="&lt;p&gt;Income is one of my independent variables in a sample of 2000 households. I'd like split the sample along household income lines (poor, vulnerable and non-poor). Is there a reason not to include income as independent variable?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-23T07:37:05.887" Id="121141" LastActivityDate="2014-10-23T08:43:23.810" OwnerUserId="27427" PostTypeId="1" Score="0" Tags="&lt;sample&gt;&lt;truncation&gt;" Title="Running regressions on subsamples: Include the variable on which the subsample is based in the regressions?" ViewCount="24" />
  <row Body="&lt;p&gt;There is no one single algorithm that will yield what you want. &lt;/p&gt;&#10;&#10;&lt;p&gt;If your desire is to compare two signals then from the signal procesing point of view you can do the following. &lt;/p&gt;&#10;&#10;&lt;p&gt;If the signal is stationary use multitaper magnitude-squared coherence from multitaper package using Harmonic F Statistic against a null hypothesis of white noise for the result.This will tell you in what frequencies the two signals overlap.&lt;/p&gt;&#10;&#10;&lt;p&gt;If the signal is non stationary use phase differential analysis and wavelet cross correlation. This will tell you across time how strong the similarities between the two series are.  &lt;/p&gt;&#10;&#10;&lt;p&gt;In regards to you desired output &lt;/p&gt;&#10;&#10;&lt;p&gt;1.Mean amplitude of the area of interest, relative to the start and end amplitudes&lt;/p&gt;&#10;&#10;&lt;p&gt;I believe you refer here at what are the dominant frequencies of each series. You can calculate these by extracting the amplitudes of the signal across frequencies from multitaper. &lt;/p&gt;&#10;&#10;&lt;p&gt;2.Slope of fall/rise transitions and of individual peaks and troughs&lt;/p&gt;&#10;&#10;&lt;p&gt;Use gradients as to measure this. The gradient represents the slope of the tangent of the graph of the function representing a trend or down. the gradient points in the direction of the greatest rate of increase of the function and its magnitude is the slope of the graph in that direction.&lt;/p&gt;&#10;&#10;&lt;p&gt;3.Number of peaks and troughs&#10;Use function finpeaks from R package MassArray&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-23T08:39:09.693" Id="121144" LastActivityDate="2014-10-23T08:47:33.360" LastEditDate="2014-10-23T08:47:33.360" LastEditorUserId="36428" OwnerUserId="36428" ParentId="90131" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;I think your data are skewed right strongly but because you use log you can't see that, so mean being righter than peak is obviously unsurprising.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-10-23T09:15:42.643" Id="121150" LastActivityDate="2014-10-25T11:43:12.070" LastEditDate="2014-10-25T11:43:12.070" LastEditorUserId="49311" OwnerUserId="49311" ParentId="121132" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;The problem with Gaussian processes regression is that Maximum Likelihood estimate for hyperparameters of covariance function can be bad: with this estimate we get true values as prediction for training points, bot we get constant value (equals mean) as prediction for other points.&lt;/p&gt;&#10;&#10;&lt;p&gt;I solved this problem using Bayesian regularization for hyperparameters: we impose a prior distribution on hyperparameters, and use MAP (Max of posterior distribution) as estimate. If you want to solve a specific problem - this approach will work for sure; if you try to build a general-purpose package - you have to be careful.  &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-10-23T11:01:38.467" Id="121157" LastActivityDate="2014-10-23T11:01:38.467" OwnerUserId="11984" ParentId="120265" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="121169" AnswerCount="1" Body="&lt;p&gt;I know how to calculate PCA and SVD mathematically and I know that both can be applied to Linear Least Squares regression.&lt;/p&gt;&#10;&#10;&lt;p&gt;The main advantage of SVD mathematically seems to be that it can be applied to non-square matrices.&lt;/p&gt;&#10;&#10;&lt;p&gt;My query is that given that, both focus on the decomposition of the $X^\top X$ matrix. Is there any advantage or additional insights provided by using SVD over PCA?&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm really looking for the intuition rather than any mathematical differences.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-10-23T11:39:57.050" FavoriteCount="1" Id="121162" LastActivityDate="2014-10-23T20:03:15.580" LastEditDate="2014-10-23T20:03:15.580" LastEditorUserId="28666" OwnerUserId="25973" PostTypeId="1" Score="8" Tags="&lt;pca&gt;&lt;least-squares&gt;&lt;svd&gt;" Title="Is there any advantage of SVD over PCA?" ViewCount="420" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I want to solve a puzzle such as this one:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/yA67B.png&quot; alt=&quot;scrambled image&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;by using a genetic algorithm. When the number of pieces grow, and maybe some are rotated, the number of combinations become overwhelming. I am hoping that a genetic algorithm will be a lot faster than an exhaustive search. As a fitness function I can detect pieces that don't fit with each other using a gradient filter.&lt;/p&gt;&#10;&#10;&lt;p&gt;What crossover operator would you say is suitable for this type of problem? I have previously tried simulated annealing and have observed that pieces often correctly become each other's neighbors, but are in the wrong place in the image as a whole. Is there a crossover operator that can move such correct groups?&lt;/p&gt;&#10;&#10;&lt;p&gt;Are there any other considerations regarding the configuration of a genetic algorithm for this problem that comes to mind?&lt;/p&gt;&#10;" CommentCount="14" CreationDate="2014-10-23T12:15:02.297" Id="121166" LastActivityDate="2014-10-23T21:29:02.043" OwnerUserId="55561" PostTypeId="1" Score="1" Tags="&lt;optimization&gt;&lt;genetic-algorithms&gt;" Title="Solving a scrambled image puzzle with a genetic algorithm" ViewCount="81" />
  <row Body="&lt;p&gt;The only solution I could come up with is to generate the columns of some numeric patterns, and to use the symbolic regression with the programs such as Eureqa, to train to R and R^2 up to 0.95, and then to use those patterns and (sequence of Eureqa) formulas with another data-set to do another training/predicting.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you do not use any functions such as smart-moving averages, instead of generating the patterns, simply enable the option to use integers and consants with Eureqa and have your original data as input columns. &lt;/p&gt;&#10;&#10;&lt;p&gt;The problem is, symbolic regression takes a lot of time to complete, and the end-result might give solutions that take a lot of time to compute.&lt;/p&gt;&#10;&#10;&lt;p&gt;As a side-note, taking the neural-net approach and overtraining the sample-data did nothing!&lt;/p&gt;&#10;&#10;&lt;p&gt;I am not sure if I am explaining this well, so if you want more details, just contact me any-time!&lt;/p&gt;&#10;&#10;&lt;p&gt;PS I am still waiting for someone else's solution for this post.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-23T13:32:46.053" Id="121177" LastActivityDate="2014-10-23T13:39:29.103" LastEditDate="2014-10-23T13:39:29.103" LastEditorUserId="56546" OwnerUserId="56546" ParentId="117000" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="121206" AnswerCount="1" Body="&lt;p&gt;Can anyone help me in understanding how Expectation Propagation updates are computed when we have a function on several variables? &#10;Like this example: &lt;a href=&quot;http://research.microsoft.com/en-us/um/cambridge/projects/infernet/docs/How%20to%20add%20a%20new%20factor%20and%20message%20operators.aspx&quot; rel=&quot;nofollow&quot;&gt;http://research.microsoft.com/en-us/um/cambridge/projects/infernet/docs/How%20to%20add%20a%20new%20factor%20and%20message%20operators.aspx&lt;/a&gt;&#10;This is a sum function. But for EP, we should have a factorized term. So how these mean and variance for each element in this sum is computed? &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-10-23T13:51:21.030" Id="121180" LastActivityDate="2014-10-24T15:35:53.523" LastEditDate="2014-10-23T14:39:52.757" LastEditorUserId="48243" OwnerUserId="48243" PostTypeId="1" Score="0" Tags="&lt;bayesian&gt;" Title="compute Expectation Propagation messages for sum" ViewCount="40" />
  <row AnswerCount="1" Body="&lt;p&gt;I have a binary outcome which were measured with two methods say &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt; such a $2\times 2$ table like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; x&amp;lt;-matrix(c(349, 125, 474, 4, 7, 11, 353, 132, 485),3,3)&#10;&amp;gt; dimnames(x)&amp;lt;-list(B=c(&quot;No&quot;, &quot;Yes&quot;, &quot;Total&quot;),&#10;+                                 A=c(&quot;No&quot;, &quot;Yes&quot;, &quot;Total&quot;))&#10;&amp;gt; x&#10;       A&#10;B        No Yes Total&#10;  No    349   4   353&#10;  Yes   125   7   132&#10;  Total 474  11   485&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;from which, you can see the event rate for method A is 11/485=0.023 and for method B is 132/485=0.27. Intuitively, these two methods seem out of consistency. I did chisq.test with p.value 0.11 and a fisher exact test (since the rare events) with a p.value 0.01.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, my question is which method should I use here to test: $H_0:$ these two methods are at equality for measure the outcome vs $H_1:$ they are different. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-23T14:00:10.757" Id="121181" LastActivityDate="2014-10-23T16:08:03.523" LastEditDate="2014-10-23T15:28:22.650" LastEditorUserId="35736" OwnerUserId="35736" PostTypeId="1" Score="0" Tags="&lt;hypothesis-testing&gt;" Title="Test equality of two measurements for a outcome" ViewCount="16" />
  <row AnswerCount="0" Body="&lt;p&gt;One of the criticisms of using a bootstrap procedure is that the results are not reproducible in the sense that you may come to a different conclusions when you re-run the bootstrap analysis again. &lt;/p&gt;&#10;&#10;&lt;p&gt;This happened to me today. I was trying to bootstrap a difference in means and found that I would not reject then null hypothesis using 1000 bootstrap samples. The next time I ran this procedure, I rejected the null hypothesis. So, I got the idea to bootstrap my bootstrap estimates. I found that in 1000 samples, I would reject the null hypothesis 959/1000 times. My interpretation, is that I can be fairly confident that I can reject the null hypothesis. Does this seem like a reasonable approach? If so, is there any literature to support this?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks,&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-10-23T14:37:50.443" Id="121188" LastActivityDate="2014-10-23T15:14:00.897" LastEditDate="2014-10-23T14:56:47.930" LastEditorUserId="88" OwnerUserId="59172" PostTypeId="1" Score="1" Tags="&lt;bootstrap&gt;" Title="Bootstrapping a bootstrap" ViewCount="54" />
  <row Body="&lt;p&gt;I don't see anything about your problem that is non-standard for counts of categories. The only thing that is even a little unusual is that you have extremely marked differences between languages. &lt;/p&gt;&#10;&#10;&lt;p&gt;For your data I get Pearson chi-square of $687.8$ with $15$ d.f. for a test of no association between the variables and the P-value is minutely small. For what it's worth, my program (Stata) reports the P-value as about $7 \times 10^{-137}$. &lt;/p&gt;&#10;&#10;&lt;p&gt;A good program should indeed flag small &lt;strong&gt;expected&lt;/strong&gt; frequencies, which are the issue rather than small observed frequencies: I see a flag that 4 cells have less than 1 as expected frequency. So, there is a bit of a worry about the P-value, but it is really quite secondary. You could change the P-value by more than 100 orders of magnitude either way, but the message would be the same. &lt;/p&gt;&#10;&#10;&lt;p&gt;To put it directly, a simple test underlines what is evident just by looking at the frequencies, namely that the languages are very different, which you know any way. If you have some sceptic who doubts that, then a chi-square test provides back-up. &lt;/p&gt;&#10;&#10;&lt;p&gt;Doing this with Fisher's test is on one level more correct statistically, but it will not change the practical or scientific conclusion one iota. &lt;/p&gt;&#10;&#10;&lt;p&gt;You have quantitative data that are pertinent to a discussion, but you don't need statistical inference to add gloss. The numbers speak eloquently for themselves, and the details are the interesting part. &lt;/p&gt;&#10;&#10;&lt;p&gt;Naturally, I am responding to your example, and being firm about what it implies in no way rules out different conclusions for other data. &lt;/p&gt;&#10;&#10;&lt;p&gt;If there is a predictive model that predicts actual (relative) frequencies, then testing that is a much more interesting question, but you would need to tell us the details. &lt;/p&gt;&#10;&#10;&lt;p&gt;To respond a little more directly to your question: Fisher's exact test often is impractical once the frequencies stop being very small. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-23T14:51:24.630" Id="121191" LastActivityDate="2014-10-23T14:58:23.333" LastEditDate="2014-10-23T14:58:23.333" LastEditorUserId="22047" OwnerUserId="22047" ParentId="121184" PostTypeId="2" Score="2" />
  <row AnswerCount="0" Body="&lt;p&gt;Probably, someone who's into technical analysis of share prices eats stuff like this for breakfast. Me, I couldn't devise a theoretically acceptable approach.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have this thing (private working set, to be exact) that gains size across time – bad.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a little program that collects a time series and I am able to graph it in excel and identify trends.&#10;However, I would like to automate this (as I am keeping track of so many processes, I can’t visually see all graphs sometimes. Plus, I would like to know in real time if a problem is there, not when my test is done and I have the time series).&lt;/p&gt;&#10;&#10;&lt;p&gt;The statistics is what baffles me. Let me present a graph:&#10;&lt;img src=&quot;http://i.stack.imgur.com/c8uUT.png&quot; alt=&quot;example results to be analyzed&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Each line has a letter:&lt;/p&gt;&#10;&#10;&lt;p&gt;A.  Is OK, size does not increase.&lt;/p&gt;&#10;&#10;&lt;p&gt;B.  Is also OK. Size increases as a product of work but it goes back to baseline when work is done&lt;/p&gt;&#10;&#10;&lt;p&gt;C.  Obviously bad, size increases without giving back&lt;/p&gt;&#10;&#10;&lt;p&gt;D.  Tricky one, normally a flat line, except for some small increases. The overall trend is to increase – bad&lt;/p&gt;&#10;&#10;&lt;p&gt;E.  Even more tricky, it gives back a little but not enough to compensate the increase&lt;/p&gt;&#10;&#10;&lt;p&gt;F.  Gives and takes sporadically, with an overall trend to increase&lt;/p&gt;&#10;&#10;&lt;p&gt;Was wondering if you could advise on an algorithm to analyze the timeseries that will find those “gains” as&#10;they happen.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Edit - 24/10/2014/9:51AM (more info): I talked to a guy who's properly educated in sciences and he suggested that I treat every type of graph as a &quot;profile&quot;. Each profile will have its own detector because it's hard for one formula to fit all types of increase trends.&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;The profiles I am going to add so far:&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;A. A cap. Let's say no process should go over 100MB (that's my case, your's will vary). &#10;This is a simple way to ensure that no matter the profile, the big increases will be caught. Some smaller increases may fly under the radar.&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;B. A profiler for C-graph - where there is simply not give-back, just increase. I actually have quite a few of those so this simple check is quite valid.&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;C. For D and E, I might try something like establishing a baseline and over a longer period of time, make sure it is not breached. Also will have to allow for some tolerance from the baseline (we can't expect the process to go EXACTLY back to the baseline)&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;D. For F, I think I want to average out every so-and-so readings to create a line like C and see the slope's angle.&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Any ideas will be welcome&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-10-23T17:32:31.480" FavoriteCount="1" Id="121208" LastActivityDate="2014-10-24T08:59:35.433" LastEditDate="2014-10-24T08:59:35.433" LastEditorUserId="59183" OwnerUserId="59183" PostTypeId="1" Score="2" Tags="&lt;time-series&gt;&lt;trend&gt;&lt;quality-control&gt;&lt;real-time&gt;" Title="Detecting a trend to increase, in a time series, in real time" ViewCount="74" />
  
  
  <row Body="&lt;p&gt;If you're interested in those particular comparisons, the easiest thing to do would be to split the data into &lt;code&gt;On&lt;/code&gt; and &lt;code&gt;Off&lt;/code&gt; subsets, and run &lt;code&gt;Day&lt;/code&gt;/&lt;code&gt;Night&lt;/code&gt; comparisons separately in each one.  (This would involve a few extra comparisons, but if you aren't doing something like an all-pairwise-comparisons analysis you can probably get away without a formal correction for multiple comparisons.)&lt;/p&gt;&#10;&#10;&lt;p&gt;The negative value is more problematic.  It's hard for me to see circumstances where a model like this (a 2x2 interaction of categorical fixed predictors) would give you predicted results that were significantly different from the group means.  I would look at model diagnostics and plots of your data: are there observations or individuals that are outliers?&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm also a little surprised that you're treating &lt;code&gt;Tracking&lt;/code&gt; as a random effect -- you don't say how many levels it has, but I'm guessing that it's either &lt;code&gt;Active&lt;/code&gt; or &lt;code&gt;Inactive&lt;/code&gt; (i.e., only two levels).  If you want to control for it you would be better off treating it as a fixed effect ...&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-10-23T20:20:46.290" Id="121226" LastActivityDate="2014-10-23T20:20:46.290" OwnerUserId="2126" ParentId="121198" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="121236" AnswerCount="2" Body="&lt;p&gt;I have the following model:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\hat{d}_i=a_i+b_i+c$, where&lt;/p&gt;&#10;&#10;&lt;p&gt;$a_i$ is a zero-mean Gaussian r.v., $b_i$ is a r.v of unknown distribution, and $c$ is a constant. I want to estimate $E_a\left[E_b\left(\hat{d}^2\right)\right]$, directly from the recorded sample $\hat{d}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can I say the following? &#10;$E_a\left[E_b\left(\hat{d}^2\right)\right]=\frac1N\Sigma_{i=1}^{N}\hat{d}_i^2$&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks a bunch!&lt;/p&gt;&#10;&#10;&lt;p&gt;Zohair&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-10-23T21:10:38.520" Id="121231" LastActivityDate="2014-10-23T22:12:21.593" OwnerUserId="59140" PostTypeId="1" Score="1" Tags="&lt;sample-mean&gt;" Title="Expectation of squared non zero-mean data w.r.t. two distributions" ViewCount="23" />
  <row AnswerCount="1" Body="&lt;p&gt;Up until know I only used neural networks to classify a single output, I set one output neuron for each class and check which neuron has the highest/lowest activation.&lt;/p&gt;&#10;&#10;&lt;p&gt;What I am trying to do is to detect a pattern and instead of outputting a single value (either class or activation value) I would like to output multiple values. eg,&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;[0,5 0,5 0,5] -&amp;gt; [0,5 0,5 0,5]&#10;[1 1 1] -&amp;gt; [1 1 1]&#10;[2 2 2] -&amp;gt; [-1 -1 -1]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So what I am wondering is can I use a network with 3 outputs and instead of checking activation, use all outputs as my output pattern?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-23T21:33:25.947" Id="121234" LastActivityDate="2014-10-24T21:53:55.590" OwnerUserId="59197" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;classification&gt;&lt;neural-networks&gt;&lt;artificial-intelligence&gt;" Title="Multi Output Neural Networks" ViewCount="51" />
  
  <row AcceptedAnswerId="121256" AnswerCount="1" Body="&lt;p&gt;I am working on a dataset with a continuous response (which could be dichotomized), one continuous covariate, and multiple categorical variables. The continuous covariate (weight) is directly correlated to the response, and must be accounted for so that we can determine which of the categorical variables are most influential to the response. Here is &lt;a href=&quot;http://pastebin.com/891mheRf&quot; rel=&quot;nofollow&quot;&gt;example data&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Each row is an individual subject, with the continuous response, the covariate of underlying primary importance (weight), then 10 categorical variables that are to be tested (individuals can score yes = 1 to multiple categories). &lt;/p&gt;&#10;&#10;&lt;p&gt;My first thought in working with this data was a linear model, with stepwise elimination of categorical variables.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; lm(Response~Weight+var1+var2...+var11)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;However, I believe there is extensive collinearity, since some variables may be eliminated early, but then are significant if you add them back into the model at the end. I'm curious if there is a better way to approach this data in R, that may help sort through which of the variables are of most importance to influencing the response. My two thoughts are&lt;/p&gt;&#10;&#10;&lt;p&gt;1) Building a single model with the continuous covariate and 5 categorical variables that were selected to be of most interest before the study, and refrain from any stepwise reduction of this model&lt;/p&gt;&#10;&#10;&lt;p&gt;2) Some sort of princicpal component regression, which I know little about at this point and thus wanted to ask advice before proceeding down that path&lt;/p&gt;&#10;&#10;&lt;p&gt;To help visualize the data, and the effect of Weight on the Response, I've constructed the follow plots. In the second plot, I attempt to control for the natural Response~Weight relationship.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; #GRAPH&#10; library(ggplot2)&#10; library(reshape2)&#10;&#10; Data &amp;lt;- read.table(&quot;Fake Data.txt&quot;,header=TRUE)&#10; #Creating long format for ggplot2&#10; Data2&amp;lt;-melt(Data, id.vars = c(&quot;Subject&quot;,&quot;Response&quot;,&quot;Weight&quot;), measure.vars = c(&quot;var1&quot;,&quot;var2&quot;,&quot;var3&quot;,&quot;var4&quot;,&quot;var5&quot;,&quot;var6&quot;,&quot;var7&quot;,&quot;var8&quot;,&quot;var10&quot;,&quot;var11&quot;))&#10;&#10; #Adding in weight to the varibles to be plotted&#10; Data2&amp;lt;-rbind(Data2,Data2[1:31,])&#10; levels(Data2$variable)&amp;lt;-c(levels(Data2$variable),&quot;Weight&quot;)&#10; Data2[311:341,4]&amp;lt;-&quot;Weight&quot;&#10; Data2[311:341,5]&amp;lt;-1&#10;&#10; #Removing rows where the categorical variable is 0=No&#10; for(i in 1:length(Data2[,1])){&#10; if(Data2[i,5]==0)Data2[i,]&amp;lt;-NA&#10; }&#10; Data3&amp;lt;-na.omit(Data2)&#10;&#10; #Plotting Response vs Weight for each 'Yes' group for the categorical variables&#10;  scatter &amp;lt;- ggplot(Data3, aes(Weight, Response, colour = variable))&#10; scatter + geom_point(aes(color = variable), size = 3) + geom_smooth(method = &quot;lm&quot;,aes(fill = variable), alpha = 0.1) + facet_wrap(~variable)+ guides(fill=FALSE,color=FALSE) &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/AgQog.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; #Zeroing the Response~Weight relationship to remove its influence. Correction coefficients from linear model fit to Response~Weight&#10; Data4&amp;lt;-Data3&#10; Data4$Response&amp;lt;-Data4$Response-(0.01494*(Data4$Weight)+ 84.67715)&#10;&#10; #Plotting Response vs Weight for each 'Yes' group for the categorical variables for zeroed Response~Weight relationship (as seen in bottom right facet)&#10; scatter2 &amp;lt;- ggplot(Data4, aes(Weight, Response, colour = variable))&#10; scatter2 + geom_point(aes(color = variable), size = 3) + geom_smooth(method = &quot;lm&quot;,aes(fill = variable), alpha = 0.1) + facet_wrap(~variable)+ guides(fill=FALSE,color=FALSE) &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/JfCrn.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;This second plot helps to show how, when the Response~Weight relationship is controlled for, variables like 'var10' have no influence on the response, while variables like 'var11' have all individuals below that zero-centered mean. Thus, from a visual test, I could identify var11 as a categorical variable of interest that negatively influences our response.&lt;/p&gt;&#10;&#10;&lt;p&gt;Additionally, this plot shows some of the confounding in this dataset, as you can see certain categorical variables 'clump'/are only documented in certain weight ranges. This is due to the underlying biology.&lt;/p&gt;&#10;&#10;&lt;p&gt;As a final note, I wonder if it is appropriate to use the corrected response in the second plot as the 'Response' for a linear model, thus eliminating the need for a 'Weight' covariate, or if it is incorrect to use such a transformation&lt;/p&gt;&#10;&#10;&lt;p&gt;Any thoughts are much appreciated&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-10-23T14:04:39.820" Id="121255" LastActivityDate="2014-10-24T01:10:15.230" OwnerDisplayName="user3708129" OwnerUserId="48549" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;covariance&gt;" Title="Multivariate Regression on Multicollinear Categorical Variables" ViewCount="66" />
  
  <row Body="&lt;p&gt;The question with the &quot;exactly 4 of 6&quot; vs &quot;exactly 2 of 3&quot; isn't very interesting, because P(&quot;hit exactly 2 of 3 throws&quot;) is &lt;em&gt;always&lt;/em&gt; better, (unless p=0 or 1 - when the chance is 0, in which case they're equal at 0).&lt;/p&gt;&#10;&#10;&lt;p&gt;The ratio of probabilities P(4 of 6)/P(2 of 3) is &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac{{{6}\choose {4}} p^4 (1-p)^2}{{3\choose 2} p^2 (1-p)} = 5 p^2(1-p)\,.$$ &lt;/p&gt;&#10;&#10;&lt;p&gt;This is maximized at $p=2/3$, where the ratio reaches 20/27 (32.9% chance for 4 of 6, 44.4% chance for 2 of 3).&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;The question &quot;at least 4 of 6&quot; vs &quot;at least 2 of 3&quot; is a bit more interesting. &lt;/p&gt;&#10;&#10;&lt;p&gt;There, &quot;at least 4 of 6&quot; wins if p&gt;0.7831 (roughly)&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/KjZwo.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;[As whuber points out in comments, this point may be solved for algebraically; it's exactly $\frac{2+\sqrt{34}}{10}$.]&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-10-24T01:50:13.607" Id="121258" LastActivityDate="2014-10-24T14:40:38.113" LastEditDate="2014-10-24T14:40:38.113" LastEditorUserId="805" OwnerUserId="805" ParentId="121238" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;You can use the function &lt;code&gt;pchisq&lt;/code&gt; to calculate the probability on the interval you specified which is really the interval $[0,16]$. So you can do the following:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; pchisq(16,df=8)&#10;[1] 0.9576199&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The &lt;code&gt;pchisq&lt;/code&gt; function as written computes the probability for all values below 16, but since the support of the chi-square distribution is for numbers greater or equal to zero, the probability of before the value 0 is just zero. So what I have written should be the answer.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-24T03:00:13.237" Id="121265" LastActivityDate="2014-10-24T03:00:13.237" OwnerUserId="34622" ParentId="121260" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;It is an assumption of the model. Think of it this way, in your model you have one of your equations written as:&#10;\begin{align}&#10;y_1 - \mu_1 &amp;amp;= \lambda_{11}f_1 + \lambda_{12}f_2 + \dots + \lambda_{1m}f_m + \epsilon_1 &#10;\end{align}&#10;The intention of writing something like this is that you think $y_1-\mu_1$ can be modeled as a linear combinations of $m$ factors. If this is true, then in the best case (probably impossible) you will get that the error of your model is 0. However, since you know it is almost impossible to find statistical models that work 100% of the time (otherwise you have determinism), then you should get an error close to zero, and since the error is a random variable, you can assume that it has a first moment (i.e mean) equal to zero.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-24T03:12:37.703" Id="121267" LastActivityDate="2014-10-24T03:12:37.703" OwnerUserId="34622" ParentId="121261" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm starting with &lt;code&gt;lme4&lt;/code&gt; and GLMM. Maybe this question can be basic for experimented researchers, but I'm still learning.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a pooled data where every observation is independent for every year. Will it be correct consider time random effect in a GLMM? Or maybe, Should I use other kind of models?&lt;/p&gt;&#10;&#10;&lt;p&gt;The model is specified as follow:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;model &amp;lt;- glmer(response ~ V1 + V2 + (1|TIME), family = binomial, data =data))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Thanks for any help, references are welcome.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-10-24T10:17:59.723" FavoriteCount="1" Id="121285" LastActivityDate="2014-10-24T10:21:28.710" LastEditDate="2014-10-24T10:21:28.710" LastEditorUserId="603" OwnerUserId="46168" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;random-effects-model&gt;&lt;glmm&gt;&lt;lme4&gt;" Title="Some doubts about using time random effect" ViewCount="33" />
  <row AcceptedAnswerId="121320" AnswerCount="2" Body="&lt;p&gt;If we have a set $S= \{s_1,s_2,..,s_n\}$ and each element in the set $S$ has an assigned probability $P_n$.&#10;Then a selection process is applied to the set $S$ such that, each element $S_n \in$ $S$ is selected in a returned set $RS$ based on its probability ($P_n$). (For example, if all probabilities are $1$, then $RS$ =$S$)&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there any mathematical formula that represent the number of elements in $RS$ ?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-24T14:08:57.733" Id="121311" LastActivityDate="2014-10-24T15:14:27.913" LastEditDate="2014-10-24T15:14:27.913" LastEditorUserId="919" OwnerUserId="11917" PostTypeId="1" Score="3" Tags="&lt;probability&gt;&lt;sampling&gt;&lt;weighted-sampling&gt;" Title="Probability selection of elements from a set" ViewCount="43" />
  <row AnswerCount="0" Body="&lt;p&gt;I am very new to statistical analysis and R. Recently I worked on a simple linear regression model to predict values. For example: consider the below data set&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Col A    Col B&#10;1         10&#10;2         16&#10;3         67&#10;4         ?&#10;5         ?&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;For such data i was able to predict the values using &lt;code&gt;lm&lt;/code&gt; and &lt;code&gt;predict&lt;/code&gt; functions in R. &lt;/p&gt;&#10;&#10;&lt;p&gt;Now, suppose I am given the below data set:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;inventory  jan-sales   feb-sales   mar-sales   apr-sales  may-sales&#10;12         4           0           2           ?          ?&#10;190        54          67          89          ?          ? &#10;123        67          22          11          ?          ? &#10;654       167          100        300          ?          ?&#10;789       567          10         80           ?          ? &#10;543       223          221        0            ?          ?&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In this data set, each line has total available units of a particular item and how many of those items were sold on each month. Now based on this data if I am asked to predict how many will be sold in the months of April and May, how do I use R and Linear regression or multiple linear regression to predict sales for April and May?&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2014-10-24T07:30:19.473" Id="121315" LastActivityDate="2014-10-24T14:22:06.650" OwnerDisplayName="Nandz" OwnerUserId="59222" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;regression&gt;" Title="Predicting values using linear regression" ViewCount="58" />
  <row AnswerCount="0" Body="&lt;p&gt;I asked before what is the &lt;a href=&quot;http://stats.stackexchange.com/questions/76904/robust-regression-and-sandwich-estimators&quot;&gt;intuition&lt;/a&gt; behind sandwich estimators. I must still missing something because I don't understand why sandwich estimators are not always applied to OLS residuals.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can you explain what sandwich estimators do in English and why we don't see them always applied to OLS models?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-10-24T15:05:58.500" Id="121321" LastActivityDate="2014-10-24T15:05:58.500" OwnerUserId="7795" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;least-squares&gt;&lt;sandwich&gt;" Title="Why sandwich estimators aren't always used in OLS regression?" ViewCount="37" />
  <row AnswerCount="1" Body="&lt;p&gt;Suppose we have population P that is split into large number of close size sub-populations (e.g. country is split to zips).&#10;For each sub-population a small sample was drawn and average of a variable X was calculated. &lt;/p&gt;&#10;&#10;&lt;p&gt;The researcher has only access to the average values and he/she needs to study a correlation between X and a random variable Y that researcher has the full access to the population data.&lt;/p&gt;&#10;&#10;&lt;p&gt;What is the correct way to organize the correlation calculation?&lt;/p&gt;&#10;&#10;&lt;p&gt;Concern #1 is that the researcher doesn't have access to the individual instances used for the sampling of Y. So if he/she take a ratio between the mean of X and mean of Y it might not be the same as the mean of the ratio X/Y. &lt;/p&gt;&#10;&#10;&lt;p&gt;Concern #2 is the small size of the X samples. However, since the number of sub-populations is large the same forces that lead to the CLT will be applied to the correlation.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-10-24T15:58:02.423" Id="121328" LastActivityDate="2014-10-29T21:37:29.670" OwnerUserId="59250" PostTypeId="1" Score="2" Tags="&lt;correlation&gt;&lt;sampling&gt;" Title="Sampling For Correlation" ViewCount="37" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a question about the reconstitution formula for the correspondence analysis.&#10;Let X be the observation matrix with variables by columns and observations by rows.&#10;I look for a decomposition&#10;$$ X^TNXMU=\lambda U$$&#10;In the new basis $U$ the matrix becomes&#10;$$\Psi = NX(U^{-1})^T=NXMU$$&#10;If I manipulate it I obtain the reconstitution formula&#10;$$X=N^{-1}\Psi U^{-1}M^{-1}=N^{-1}\Psi U^{T}$$&#10;Writing the explicit forms for N and M and writing the above formula component wise we obtain&#10;$$X_{ij}=X_{i.}\sum_{m=1}^{q}\Psi_{im}U_{jm}$$&#10;where $q$ is the number of non-null eigenvectors and $X_{i.}=\sum_{i=1}^N X_{ij}$&lt;/p&gt;&#10;&#10;&lt;p&gt;In the literature, however, this formula is used:&#10;$$X_{ij}=X_{i.}\left[X_{.j}+\sum_{m=1}^{q}\Psi_{im}U_{jm}\right]$$&#10;Where does that $X_{.j}$ come from?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-24T16:09:59.693" Id="121329" LastActivityDate="2014-10-24T16:09:59.693" OwnerUserId="59246" PostTypeId="1" Score="0" Tags="&lt;correspondence-analysis&gt;" Title="Correspondence analysis reconstitution formula" ViewCount="11" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I want to solve a regression method whose parameters are under quadratic constraint a'*a=1. Is there any method in Bayesian statistics to handle this constraint? Thank you in advance. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-10-24T17:30:21.863" Id="121335" LastActivityDate="2014-10-25T12:06:44.773" OwnerUserId="46414" PostTypeId="1" Score="3" Tags="&lt;bayesian&gt;" Title="Can quadratic constraints be handled by Bayesian methods?" ViewCount="37" />
  
  <row AcceptedAnswerId="121359" AnswerCount="1" Body="&lt;p&gt;Given 50 random samples, each of size 25, from a normal distribution with mean 20 and standard deviation 5. From each of the 50 samples, you can find a 90% confidence interval for the mean. Let Y be a random variable indicating how many of the confidence intervals containing the mean. What is the distribution of Y?&lt;/p&gt;&#10;&#10;&lt;p&gt;My friend and I are having a heated discussion about this. I think it is binomially distributed with 50 observations and probability of success 90%. My friend claims it is normally distributed with mean 45 and standard deviation 1.&lt;/p&gt;&#10;&#10;&lt;p&gt;I think this because Y has a fixed number of observations (50), each observation has two outcomes, and their probabilities are fixed (probability of containing the mean is 90%).&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-10-24T17:32:34.123" Id="121336" LastActivityDate="2014-10-24T20:40:14.820" LastEditDate="2014-10-24T20:37:41.343" LastEditorUserId="919" OwnerUserId="59257" PostTypeId="1" Score="4" Tags="&lt;self-study&gt;&lt;normal-distribution&gt;&lt;confidence-interval&gt;&lt;binomial&gt;" Title="Distribution of random sample of confidence intervals containing the true mean" ViewCount="41" />
  <row AnswerCount="0" Body="&lt;p&gt;for mulitnomial (or mixed) logit models, when the choice set is too large, either strategic sampling or random sampling of the choice set can be used. My question would be: Is that also true in presence of panel data with fixed effects?&lt;/p&gt;&#10;&#10;&lt;p&gt;Your help is greatly apreciated and many thanks,&lt;/p&gt;&#10;&#10;&lt;p&gt;m&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-24T17:34:26.753" Id="121337" LastActivityDate="2014-10-24T17:34:26.753" OwnerUserId="59258" PostTypeId="1" Score="2" Tags="&lt;panel-data&gt;&lt;discrete-data&gt;" Title="Discrete Choice Models" ViewCount="28" />
  
  <row Body="&lt;p&gt;I don't know of a success story, but I can think of an approach based on reparametrisation and variational inference. It is a long story short and quite complicated.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's say your parameters are  $\theta$. You can then obtain a variational lower bound on the marginal log-likelihood &#10;$$&#10;\mathcal{L} = \log \sum_\theta p(x|\theta) p(\theta) \\&#10;= \log \sum {q(\theta) \over q(\theta)} p(x|\theta) p(\theta) \\&#10;\geq \sum q(\theta) \log  {p(x|\theta) \over q(\theta)} p(\theta)\\&#10;= \mathbb{E}[\log p(x|\hat\theta)]_{\hat\theta \sim q} - KL(q(\theta)||p(\theta))&#10;$$&#10;Now, if you can i) sample from $q$ and ii) calculate the KL between $q$ and the prior, you can train this objective with stochastic gradient-based techniques, i.e. stochastic gradient descent or, even better, rmsprop or adadelta.&lt;/p&gt;&#10;&#10;&lt;p&gt;What you now need is to make sure that $q$ is only a distribution over values satisfying the constraint. You can achieve that either by&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Reparametrising $a$ in a way that each $\theta$ represents a &quot;valid&quot; $a$,&lt;/li&gt;&#10;&lt;li&gt;Projecting each sample to the set of orthogonal matrices.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;The first can be achieved by performing an Eigendecomposition $A = UDU^T$ where you a) have to make sure that all elements of D are either $1$ or $-1$. You will also have to orthogonalize $U$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The latter can be achieved by SVD and setting the singular values to 1.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-10-24T18:46:59.437" Id="121346" LastActivityDate="2014-10-24T18:46:59.437" OwnerUserId="2860" ParentId="121335" PostTypeId="2" Score="0" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I was just wondering if there is a set way to write the results within a CPH model, I have obtained results from SPSS and I have values for Sig B and the 95% Confidence interval.&lt;/p&gt;&#10;&#10;&lt;p&gt;What I am after is essentially the quickest format of writing the results such as if it were for an independent t-test the results would be in the format t(df) = t-value, p = significance value.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-24T19:51:58.873" Id="121351" LastActivityDate="2014-10-24T19:56:53.047" LastEditDate="2014-10-24T19:56:53.047" LastEditorUserId="22047" OwnerUserId="59266" PostTypeId="1" Score="0" Tags="&lt;cox-model&gt;" Title="How do I report the data from a Cox proportional hazard regression model on SPSS" ViewCount="28" />
  <row Body="&lt;p&gt;I wouldn't say it is useless, but it really depends on the application. Note, you never really know the distribution the data is coming from, and all you have is a small set of the realizations. Your sample mean is always finite in sample, but the mean could be undefined or infinite for some types of probability density functions. Let us consider the three types of Levy stable distributions i.e Normal distribution, Levy distribution and Cauchy distribution. Most of your samples do not have a lot of observations at the tail (i.e away from the sample mean). So empirically it is very hard to distinguish between the three, so the Cauchy (has undefined mean) and the Levy (has infinite mean) could easily masquerade as a normal distribution. &lt;/p&gt;&#10;" CommentCount="6" CommunityOwnedDate="2014-10-24T20:00:54.570" CreationDate="2014-10-24T20:00:54.570" Id="121352" LastActivityDate="2014-10-24T20:00:54.570" OwnerUserId="34622" ParentId="2492" PostTypeId="2" Score="0" />
  
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have average life expectancy at birth data for an 8 year period and I would like to use that 8 year period to predict the trend for average life expectancy for the next 5 years. I would then like to ask whether this deviates significantly from the actual average life expectancy over the next 5 years.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;What's the best regression model to fit to the observation base data in order to get predictions for next 5 years?&lt;/li&gt;&#10;&lt;li&gt;How can I assess whether the difference between the predicted and observed trend is significant?&lt;/li&gt;&#10;&lt;li&gt;How can I implement #1 and #2 in R?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2014-10-25T14:06:57.647" Id="121408" LastActivityDate="2014-10-25T14:21:11.210" LastEditDate="2014-10-25T14:21:11.210" LastEditorUserId="7290" OwnerUserId="59301" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;regression&gt;&lt;time-series&gt;&lt;forecasting&gt;&lt;life-expectancy&gt;" Title="Regression model for predicting life expectancy" ViewCount="42" />
  
  
  <row AcceptedAnswerId="121445" AnswerCount="2" Body="&lt;p&gt;I am reading a textbook on machine learning (Data Mining by Witten, et al., 2011) and came across this passage:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;... Moreover, different distributions can be used. Although the normal&#10;  distribution is usually a good choice for numeric attributes, it is&#10;  not suitable for attributes that have a predetermined minimum but no&#10;  upper bound; in this case a &quot;log-normal&quot; distribution is more&#10;  appropriate. Numeric attributes that are bounded above and below&#10;  can be modeled by a &lt;strong&gt;&quot;log-odds&quot;&lt;/strong&gt; distribution.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I have never heard of this distribution. I googled for &quot;log-odds distribution&quot; but could not find any relevant exact match. Can someone help me out? What is this distribution, and why does it help with numbers bounded above and below?&lt;/p&gt;&#10;&#10;&lt;p&gt;P.S. I am a software engineer, not a statistician.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-25T23:14:19.010" Id="121438" LastActivityDate="2014-10-27T20:41:52.813" LastEditDate="2014-10-26T06:28:05.727" LastEditorUserId="29072" OwnerUserId="29072" PostTypeId="1" Score="8" Tags="&lt;machine-learning&gt;&lt;distributions&gt;" Title="What is a log-odds distribution?" ViewCount="262" />
  <row Body="&lt;p&gt;I'm a software engineer (not a statistician) and I recently read a book called An Introduction to Statistical Learning. With applications in R.&lt;/p&gt;&#10;&#10;&lt;p&gt;I think what you're reading about is log-odds or logit. page 132&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Fourth%20Printing.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Fourth%20Printing.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Brilliant book - I read it from cover to cover. Hope this helps&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-10-26T00:08:47.240" Id="121441" LastActivityDate="2014-10-26T00:08:47.240" OwnerUserId="37396" ParentId="121438" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;As I mentioned in my comment, one possible &lt;em&gt;parametric solution&lt;/em&gt; that the data might fit could be an &lt;em&gt;under-damped harmonic oscillation&lt;/em&gt; or similar function: &lt;a href=&quot;http://hyperphysics.phy-astr.gsu.edu/hbase/oscda.html&quot; rel=&quot;nofollow&quot;&gt;http://hyperphysics.phy-astr.gsu.edu/hbase/oscda.html&lt;/a&gt;. In terms of &lt;em&gt;non-parametric solutions&lt;/em&gt;, I agree with @joaoFaria's answer, but I'd like to extend it a little. You could use &lt;code&gt;nls&lt;/code&gt; or &lt;code&gt;optim&lt;/code&gt; &lt;code&gt;R&lt;/code&gt; functions from the standard &lt;code&gt;stats&lt;/code&gt; package: &lt;a href=&quot;http://cran.r-project.org/doc/manuals/r-release/R-intro.html#Nonlinear-least-squares-and-maximum-likelihood-models&quot; rel=&quot;nofollow&quot;&gt;http://cran.r-project.org/doc/manuals/r-release/R-intro.html#Nonlinear-least-squares-and-maximum-likelihood-models&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;In addition to the suggested use of &lt;code&gt;AIC&lt;/code&gt; and &lt;code&gt;BIC&lt;/code&gt; statistics as &lt;em&gt;analytic fit criteria&lt;/em&gt;, you also may be interested in &lt;em&gt;regression fit visualization&lt;/em&gt;, using, for example, &lt;code&gt;R&lt;/code&gt; package &lt;code&gt;visreg&lt;/code&gt; (&lt;a href=&quot;http://cran.r-project.org/web/packages/visreg&quot; rel=&quot;nofollow&quot;&gt;http://cran.r-project.org/web/packages/visreg&lt;/a&gt;), which supports both linear and some &lt;em&gt;nonlinear models&lt;/em&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-26T03:29:18.687" Id="121449" LastActivityDate="2014-10-26T03:29:18.687" OwnerUserId="31372" ParentId="101126" PostTypeId="2" Score="0" />
  
  <row AcceptedAnswerId="121463" AnswerCount="1" Body="&lt;p&gt;In &lt;em&gt;Statistical Methods&lt;/em&gt; by Snedecor and Cochran, it states something to the effect of &quot;the within-group variance is an estimate of overall variance given the null hypothesis&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;and this point is used to justify that the ratio of between-group and within-group variances follows an F distribution under the null.&lt;/p&gt;&#10;&#10;&lt;p&gt;But why is this true only under the null? At first I thought it was some kind of weighted average, but with the quadratic numerator I'm not so sure.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-26T06:19:55.593" Id="121458" LastActivityDate="2014-10-26T08:52:33.443" OwnerUserId="36229" PostTypeId="1" Score="2" Tags="&lt;hypothesis-testing&gt;&lt;anova&gt;&lt;variance&gt;&lt;f-distribution&gt;" Title="&quot;Under the ANOVA null, within group and between group sample variance are both estimates of the pop. variance&quot; -- why only under the null?" ViewCount="30" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am collecting data on extroversion using a personality scale at a clinic.  I have about 50 subjects where 15 are in experimental group receiving interventions.  I am also collecting data on clinicians attitudes towards intervention program with about 15 clinicians.  I began two weeks ago and will be collecting data on a monthly basis for control, experimental, and clinicians.  I have a spreadsheet with all data collected and calculated the mean, SD, skewness, and z scores.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like advice on the following since I am new to statistics:&lt;/p&gt;&#10;&#10;&lt;p&gt;1) Should I run t tests after each time I collect data from subjects and clinicians to compare with pre-intervention means?&lt;/p&gt;&#10;&#10;&lt;p&gt;2) If I created the personality scale myself using research, will this potentially lessen the power of my study?    &lt;/p&gt;&#10;&#10;&lt;p&gt;3) I am wondering if collecting data on a monthly basis is overkill or better every three months?  I plan on collecting data for one year.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance!  I am stressing out since I want to be on the right path.  &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-10-26T17:23:11.510" Id="121488" LastActivityDate="2014-10-26T17:23:11.510" OwnerUserId="38410" PostTypeId="1" Score="0" Tags="&lt;t-test&gt;&lt;experiment-design&gt;&lt;psychology&gt;" Title="Advice on research study" ViewCount="38" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a DNA sequence has 1682 observations, I got the initial HMM parameters and other&#10;parameters by EM algorithm (Baum-Welch).&#10;When I tried to evaluate the model by Log-odds scoring, I calculated the Forward algorithm but the result was Zero because p(X/ HMM)=0 where X is the new sequence.&#10;How can I evaluate my model?&#10;Should I use whole the sequence for that alignment?&#10;thanks&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-26T22:22:29.890" Id="121513" LastActivityDate="2014-10-26T22:22:29.890" OwnerUserId="59373" PostTypeId="1" Score="0" Tags="&lt;hidden-markov-model&gt;" Title="Hidden Markov Models with long sequence" ViewCount="18" />
  <row AnswerCount="0" Body="&lt;p&gt;Suppose I run a bunch of experiments, and get a bunch of datapoints, but several of the datapoints were derived from the same experiment.  (In the extreme case, I just repeat the same datapoint 10 times.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose I want to perform a statistical analysis on these datapoints, like a linear or logistic regression, or something more sophisticated.&lt;/p&gt;&#10;&#10;&lt;p&gt;How would the non-independence of data affect my analysis?  What steps would I have to take to correct for this effect?&lt;/p&gt;&#10;" CommentCount="11" CreationDate="2014-10-26T22:34:02.000" Id="121518" LastActivityDate="2014-10-26T22:44:44.760" LastEditDate="2014-10-26T22:44:44.760" LastEditorUserId="59374" OwnerUserId="59374" PostTypeId="1" Score="3" Tags="&lt;regression&gt;&lt;independence&gt;" Title="Will something terrible happen if my data points aren't independent?" ViewCount="56" />
  <row AnswerCount="1" Body="&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;It's been many years since I've done any statistics (or any serious math), but I do remember that the sampling error decreases more slowly for larger sample sizes (like n^-1/2, at least for some statistics).&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;I also remember (from numerical analysis) that for processes modeled as linear ODEs, errors in constant coefficients or initial conditions increase exponentially with time (and at least as bad for non-linear processes), i.e. further reduction of initial condition errors only buy us a logarithmic increase in predictive precision over time.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;While a lot has been said about Big Data and bias errors, one thing is certain: whatever data you can already collect (along with whatever biases it may contain), you can sample it randomly without introducing additional bias errors. In short: if you can store it -- you can sample it (without bias).&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;In light of these (and I could be wrong, of course), it seems that any additional sample we collect has diminishing returns on statistics as well as predictions (and the benefits diminish quite rapidly). It seems, then, that even if storing and analyzing Big Data is relatively cheap, it still doesn't pay. We get lots of data that buys us little if any additional knowledge: very little extra statistic accuracy, no less bias, and virtually no added predictive ability[1]. What, then, are the benefits of Big Data?&lt;/p&gt;&#10;&#10;&lt;p&gt;(This question is most not a duplicate of &lt;a href=&quot;http://stats.stackexchange.com/questions/35971/is-sampling-relevant-in-the-time-of-big-data&quot;&gt;Is sampling relevant in the time of 'big data'?&lt;/a&gt;, and in any event, the answers to that question don't answer mine)&lt;/p&gt;&#10;&#10;&lt;p&gt;[1] This last point -- predictive ability -- seems the most pertinent, as that is what many commercial uses for Big Data are for. But user behavior changes all the time -- probably with some complicated feedback, and possibly non-linearly -- so whatever extra accuracy we get, say n^-1/2, this meager gain then becomes logarithmic when predictions are concerned. In fact, it can be argued that to get better predictions, it's preferable to reduce the time it takes to calculate the statistics (by sampling), than to enhance precision by increasing the sample size, because time has an exponential effect on &quot;knowledge&quot;, while sample size has a mere polynomial effect.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-10-26T23:30:23.657" FavoriteCount="1" Id="121520" LastActivityDate="2015-01-04T12:20:17.737" LastEditDate="2014-10-27T02:47:32.603" LastEditorUserId="59375" OwnerUserId="59375" PostTypeId="1" Score="5" Tags="&lt;large-data&gt;" Title="Wherefore Big Data?" ViewCount="93" />
  <row Body="&lt;p&gt;As far as I know, &lt;strong&gt;it is easier to actually look for non-gaussianity/normality than gaussianity/normality&lt;/strong&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;As I explained &lt;a href=&quot;http://stats.stackexchange.com/questions/20793/what-is-gaussianity-and-how-do-you-perform-gaussianity-testing-in-macroeconom/26104#26104&quot;&gt;in this other post&lt;/a&gt;, there are several ways of doing this, where the most intuitive is the search for higher-order cummulants in your data, because the gaussian distribution is the only one that has a finite number of non-zero cummulants (this is a theorem known as Marcinkiewicz's theorem). However, this is not recommended because it is computationally expensive and, of course, calculating every cummulant is not possible in real life. &lt;/p&gt;&#10;&#10;&lt;p&gt;One other way of measuring (and therefore testing for) non-gaussianity that has been particularly useful in Independant Component Analysis (an application where you need to measure the degree of non-gaussianity of samples) is negentropy. For an introduction on these measures, see &lt;a href=&quot;http://cis.legacy.ics.tkk.fi/aapo/papers/IJCNN99_tutorialweb/node12.html&quot; rel=&quot;nofollow&quot;&gt;these notes by Hyvärinen on the subject&lt;/a&gt; which is an extract of the paper by &lt;a href=&quot;http://www.cs.helsinki.fi/u/ahyvarin/papers/NN00new.pdf&quot; rel=&quot;nofollow&quot;&gt;Hyvärinen &amp;amp; Oja (2000)&lt;/a&gt; on Independant Component Analysis. If you are interested, search for his papers on efficient ways of calculating negentropy. Also, &lt;a href=&quot;http://stats.stackexchange.com/questions/35248/deriving-negentropy-getting-stuck/35261#35261&quot;&gt;in this post&lt;/a&gt; there is an actual derivation of Negentropy if you are looking for it.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-27T02:07:43.800" Id="121530" LastActivityDate="2014-10-27T02:07:43.800" OwnerUserId="9174" ParentId="121528" PostTypeId="2" Score="0" />
  
  
  <row AnswerCount="7" Body="&lt;p&gt;I understand that &lt;strong&gt;correlation is not causation&lt;/strong&gt;. Suppose we get high correlation between two variables. How do you check if this correlation is actually because of causation? Or,under what conditions, exactly, can we use experimental data to deduce a causal relationship between two or more variables?&lt;/p&gt;&#10;" CommentCount="13" CreationDate="2014-10-27T11:10:29.940" FavoriteCount="14" Id="121569" LastActivityDate="2014-11-07T05:06:05.337" LastEditDate="2014-10-28T12:50:05.060" LastEditorUserId="858" OwnerUserId="55490" PostTypeId="1" Score="27" Tags="&lt;correlation&gt;&lt;mathematical-statistics&gt;&lt;causality&gt;" Title="If 'correlation doesn't imply causation', then if I find a statistically significant correlation, how can I prove the causality?" ViewCount="3688" />
  
  
  <row Body="&lt;p&gt;Please use below code as above code is giving a mismatch error:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;forecast.fit2 &amp;lt;- predict(tourist.fit1 ,n.ahead = length(outcli), newxreg = outcli)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-10-27T14:47:04.167" Id="121594" LastActivityDate="2014-10-27T15:08:55.247" LastEditDate="2014-10-27T15:08:55.247" LastEditorUserId="26338" OwnerUserId="59416" ParentId="87111" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Note that your model &lt;code&gt;rm1&lt;/code&gt; includes a factor that is used both a fixed and a random effect -- namely the intercept. So yes, it is OK to do what you propose. The only issue is that there are a whole lot more random effects to estimate. There is always a trade off between parsimony and fit, so look at the AIC and BIC statistics, among others, to make sure you're not getting too fancy. For example, you might consider at least not having the interactions in there as random effects.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-10-27T15:06:04.567" Id="121597" LastActivityDate="2014-10-27T15:06:04.567" OwnerUserId="52554" ParentId="121593" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;It is possible to use a one-tailed test since only deviations in one direction are expected in this case and because it is essentially an interpretation problem. More importantly, however, is that if you reject the second hypothesis, a different test needs to be chosen for the first, because t-test has an assumption of equality of variances. Finally, F-test for inequality of variances is extremely sensitive to data meeting distributional assumptions (normality), Levene's test is a more robust alternative.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-27T17:43:23.013" Id="121617" LastActivityDate="2014-10-27T17:43:23.013" OwnerUserId="57390" ParentId="121469" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;Unless I'm making an elementary mistake (entirely possible!), this does not hold, even for discrete random variables with finite support (contrary to another answer). Recall the second Borel-Cantelli Lemma&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;strong&gt;Second Borel Cantelli Lemma&lt;/strong&gt;: Let $A_1, A_2, \ldots$ be independent events. If $\sum_{i = 1} ^ \infty P(A_i) = \infty$ then $P(A_i \mbox{ occurs infinitely often}) = 1$. &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Let $X_k$ be independent, such that $P(X_k = 1) = 1/k$ and $P(X_k = 0) = 1 - 1/k$. $E|X_k| \to 0$ so $X_k$ converges to $0$ in mean, but $\sum_k P(X_k = 1) = \infty$, and by independence the events $[X_k = 1]$ are independent. Hence with probability $1$, $X_k = 1$ occurs infinitely often; obviously for any $\omega$ such that $X_k(\omega) = 1$ infinitely often we cannot have $X_k(\omega) \to 0$, so the sequence &lt;em&gt;almost surely does not&lt;/em&gt; converge to $0$. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-27T20:04:22.443" Id="121637" LastActivityDate="2014-10-27T20:04:22.443" OwnerUserId="5339" ParentId="121621" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;2 Seems fine. I would go with it.&lt;/p&gt;&#10;&#10;&lt;p&gt;As for 1. I would suggest you to train a model to predict GCC using all features available in the dataset (which are not NA during the period Sept 2011 onwards) (ommit the rows which have any NA value before sep2011 while training). The model should be very good ( use K-fold cross validation). Now predict the GCC for the period Sept 2011 and onwards.&lt;/p&gt;&#10;&#10;&lt;p&gt;Alternatively, you can train a model which predicts MSCI, use it to predict the missing MSCI values. Now train a model to predict GCC using MSCI and then predict GCC for the period Sept 2011 and onwards&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-10-27T22:38:51.343" Id="121656" LastActivityDate="2014-10-27T22:38:51.343" OwnerUserId="59397" ParentId="121414" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="121676" AnswerCount="1" Body="&lt;p&gt;&lt;strong&gt;The statement&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The sampling distribution of the sample variance is a chi-squared distribution with degree of freedom equals to $n-1$, where $n$ is the sample size (given that the random variable of interest is normally distributed).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.csus.edu/indiv/j/jgehrman/courses/stat50/samplingdists/7samplingdist.htm#samplevariance&quot; rel=&quot;nofollow&quot;&gt;Source&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;My intuition&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;It kinda makes intuitive sense to me 1) because a chi-square test looks like a sum of square and 2) because a Chi-squared distribution is just a sum of squared normal distribution. But still, I don't have a good understanding of it.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Is the statement true? Why?&lt;/strong&gt;&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-10-27T23:09:44.377" FavoriteCount="1" Id="121662" LastActivityDate="2014-10-28T18:12:21.240" LastEditDate="2014-10-28T18:12:21.240" LastEditorUserId="24097" OwnerUserId="24097" PostTypeId="1" Score="4" Tags="&lt;distributions&gt;&lt;normal-distribution&gt;&lt;sampling&gt;&lt;chi-squared&gt;&lt;sample-size&gt;" Title="Why is the sampling distribution of variance a chi-squared distribution?" ViewCount="245" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I hope this is not &quot;opinion&quot; question&lt;/p&gt;&#10;&#10;&lt;p&gt;but I am curious as to whether or not the study of statistics and probability theory in the realm of cancer genomics is considered &quot;less philosophical&quot; than the pure math studies.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am only asking because something inside me is very skeptical of pure math, i.e.  algebra and geometry.  I just am lacking a faith to study these abstract notions.&lt;/p&gt;&#10;&#10;&lt;p&gt;but I am interested in modeling cancer genes, and using probability to study biology.&lt;/p&gt;&#10;&#10;&lt;p&gt;however, I have some ingrained notion that &quot;applied&quot; is not as respected as &quot;pure&quot; math.  Can someone help me with this?  &lt;/p&gt;&#10;&#10;&lt;p&gt;Even my algebra teacher has scoffed at applied studies.  So I am having this dilemma.&lt;/p&gt;&#10;&#10;&lt;p&gt;For instance,  straying away from functional analysis, and moving more into probability theory seems to me like a &quot;weaker&quot; move.   I am sure this is a philosophical question, but I am looking for the guidance of those working in statistics, and biostatistics.   Are these field theoretically rewarding and dense?&lt;/p&gt;&#10;&#10;&lt;p&gt;Ultimately,&#10; my question is that ,  do you think that biostatistics is only a particular scope on a specific part of biotechnology revolution? Do you think biostats is a general field, or particular and why?   &quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;The reason I am concerned about this, is because I am not interested in studying just a narrow field interested in just a small set of problems, and wish to be as general as possible.  Can someone advise on this?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you very much.&lt;/p&gt;&#10;" ClosedDate="2014-10-28T06:58:19.073" CommentCount="9" CreationDate="2014-10-28T04:33:03.410" Id="121691" LastActivityDate="2014-10-28T06:52:23.413" LastEditDate="2014-10-28T04:48:25.893" LastEditorUserId="51890" OwnerUserId="51890" PostTypeId="1" Score="0" Tags="&lt;philosophical&gt;" Title="Biostatistics and Pure" ViewCount="78" />
  <row AcceptedAnswerId="121803" AnswerCount="1" Body="&lt;p&gt;I am confused with the definition of the weights in glm and lm.&lt;/p&gt;&#10;&#10;&lt;p&gt;Using the McCullagh and Nelder (1989)'s notation, If random variable $y_i$ is from the Generalized Linear Model (GLM), then its density is modelled in the form:&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{equation}&#10;f(y_i) = exp\Big(\frac{m_i}{\phi} [\theta_i y_i - b(\theta_i) ]  + c(y_i;\phi)\Big) &#10;\end{equation}&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\theta_i$ is the canonical parameter, $\kappa$ is the dispersion parameter and $m$ is the known prior &quot;weight&quot;. I would like to know that this prior &quot;weight&quot; is NOT the weight specified in glm. help(glm) says that:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Non-NULL weights can be used to indicate that different observations have different dispersions (with the values in weights being inversely proportional to the dispersions); or equivalently, when the elements of weights are positive integers w_i, that each response y_i is the mean of w_i unit-weight observations. For a binomial GLM prior weights are used to give the number of trials when the response is the proportion of successes: they would rarely be used for a Poisson GLM. &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Therefore, in my understanding, what &quot;weight&quot; $w_i$ does is to re-parameterize the dispersion parameter as &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\phi=\frac{\phi^*}{w_i},$$ &lt;/p&gt;&#10;&#10;&lt;p&gt;where $\phi^*$ is the redefined dispersion parameter. &#10;This means that for example, when $y_i$ is modelled only with an intercept term $\beta_0$,&#10;lm function with non NULL &quot;weight&quot; specification maximizes the sum of the weighted likelihood of $y_i$ with respect to $\phi^*$ and $\beta_0$ where:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;f(y_i)=\sqrt{ \frac{w_i}{2\pi \phi^*} } \exp\Big(-\frac{1}{2}\frac{w_i (y-\beta_0)^2}{\phi^*}\Big),&#10;$$ &#10;where the identity link is used $\beta_0=\theta_i$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Similarly, glm function with family = &quot;poisson&quot; with non NULL &quot;weight&quot; maximizes the sum of the weighted likelihood of $y_i$ with respect to $\beta_0$ where:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;f(y_i)=\frac{\beta_0^{w_i y_i}}{y_{i}!} exp(-w_i \beta_0),&#10;$$ &lt;/p&gt;&#10;&#10;&lt;p&gt;where the log link is used $\beta_0=exp(\theta_i)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Similarly, glm function with family = &quot;binomial&quot; with non NULL &quot;weight&quot; maximizes the sum of the weighted likelihood of $y_i$ with respect to $\phi^*$ and $\beta_0$ where:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;f(y_i)=&#10;\begin{pmatrix}&#10;m\\&#10;y_i&#10;\end{pmatrix}&#10;\beta_0^{w_iy_i}(1-\beta_0)^{w_i(m-y_i)}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where logit link is used $\beta_0 = logit^{-1}(\theta_i)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is my understanding correct?&lt;/p&gt;&#10;&#10;&lt;p&gt;Reference:&lt;/p&gt;&#10;&#10;&lt;p&gt;C.E. McCulloch and J.A. Nelder. Generalized Linear Models. Chapman and Hall, London,&#10;1989.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-28T05:55:33.753" FavoriteCount="3" Id="121694" LastActivityDate="2014-10-28T23:46:29.687" LastEditDate="2014-10-28T23:46:29.687" LastEditorUserId="22311" OwnerUserId="46837" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;generalized-linear-model&gt;&lt;likelihood&gt;&lt;lm&gt;&lt;weighted-regression&gt;" Title="&quot;weight&quot; input in glm and lm functions in R" ViewCount="265" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a parametric model that I think it is &quot;degenerate&quot; in the sense that I can obtain the same model with different parameters.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, if I have a convolution of two 1D Gaussians (say, G1 and G2), I'll obtain another Gaussian G3 with a standard deviation of $\sigma_3 = \sqrt{\sigma_1^2 + \sigma_2^2}$, where $\sigma_1$ and $\sigma_2$ are the deviations of the convolved Gaussians. The same G3 curve can be obtained with different $\sigma_1$ and $\sigma_2$ by &quot;balancing&quot; them, so if I want to fit G3 to data through its parameters $\sigma_1$ and $\sigma_2$, I can choose multiple values for them and still get a good fit.&lt;/p&gt;&#10;&#10;&lt;p&gt;So my questions are:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Is &quot;degenerate&quot; the correct term for calling this model?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;How can I assess if a model is degenerate?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Many thanks!&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-10-28T12:11:44.933" Id="121725" LastActivityDate="2014-10-28T12:11:44.933" OwnerUserId="59475" PostTypeId="1" Score="1" Tags="&lt;model&gt;&lt;curve-fitting&gt;&lt;parametric&gt;" Title="How can I assess if a parametric model can reproduce the same curve with different parameters?" ViewCount="24" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have entered main effect terms in Block 1 and interaction terms in Block 2.  Based on their beta values, the main effect terms were not significant in Block 1 and the interaction terms were not significant in Block 2.  However, the main effect terms were significant in Block 2.  Can I say that Block 2 is the better model because the main effect terms were significant even though the interaction terms were not significant?  Thanks!&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-10-28T13:35:15.043" Id="121729" LastActivityDate="2014-10-28T13:35:15.043" OwnerUserId="59481" PostTypeId="1" Score="0" Tags="&lt;model&gt;" Title="How to choose the best regression model?" ViewCount="34" />
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I have data (some behavioral features, measured on some scales) on people. I want to cluster people based on these features. This is an unsupervised scenario, as I have no prior knowledge on the clusters, nor any training data. I might have some good guesses about the number of clusters. All measures are continuous.&lt;/p&gt;&#10;&#10;&lt;p&gt;The caveat is this: I prefer &quot;clean&quot; clusters and would be happy to &quot;throw away&quot; noisy cases and mark them as &quot;unclassified&quot;. Noisy cases may be outliers, or cases that are close enough to 2 or more clusters so that putting them in either cluster will make that cluster less homogeneous, and will put the clusters to one another. &#10;However, cases that are unclassified (that is, thrown away) must be far enough from each other (otherwise they could form a new cluster by themselves).&lt;/p&gt;&#10;&#10;&lt;p&gt;I wonder if there is any work that deal with this, or do you have any insights about how to do this and how to decide which cases should become &quot;unclassified&quot;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-28T16:29:01.793" Id="121758" LastActivityDate="2014-10-28T18:25:30.687" LastEditDate="2014-10-28T18:25:30.687" LastEditorUserId="42397" OwnerUserId="25986" PostTypeId="1" Score="0" Tags="&lt;classification&gt;&lt;clustering&gt;&lt;outliers&gt;&lt;unsupervised-learning&gt;" Title="unsupervised clustering with &quot;unclassified&quot; items" ViewCount="23" />
  
  
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a set of lines (image below) which should meet in a number of points. As you can see, now the angular coefficient doesn't vary noticeably, making intercepts hard to find. What transformation do you suggest to use to make things easier, or where do you think I should look for inspiration? Thanks!&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/AcDCx.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-10-28T22:37:03.057" Id="121811" LastActivityDate="2014-10-28T23:11:30.040" OwnerUserId="59518" PostTypeId="1" Score="0" Tags="&lt;data-transformation&gt;" Title="Find intercept of almost flat lines" ViewCount="30" />
  
  <row Body="&lt;p&gt;The power spectrum is the frequency-domain counterpart of the time-domain autocovariance function.&lt;/p&gt;&#10;&#10;&lt;p&gt;According to the frequency-domain view, a white noise process can be viewed as the sum of an infinite number of cycles with different frequencies where each cycle has the same weight.&lt;/p&gt;&#10;&#10;&lt;p&gt;The power spectrum is not used to predict a time series. It is used to examine the main characteristics of the series and propose a time series model accordingly. For example, it can be used to detect if seasonality is present in the data, if so, the spectrum will show peaks at the seasonal frequencies.&lt;/p&gt;&#10;&#10;&lt;p&gt;In principle, the same information can be obtained from both the spectrum and the autocovariance function, but in some contexts it is more convenient to work &#10;with the frequency-domain representation. For example, if you are interested in looking at the seasonal frequencies that determine the seasonal pattern observed in the series, the spectrum will show you the cycles that lead that component. If you want to analyze the frequency of some phenomenon such as the business cycle, the spectrum is a straightforward way to get a &#10;graphical view to it.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-28T22:53:20.850" Id="121814" LastActivityDate="2014-10-28T22:53:20.850" OwnerUserId="48766" ParentId="121748" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;In Matlab (&lt;a href=&quot;http://www.mathworks.nl/help/stats/corr.html&quot; rel=&quot;nofollow&quot;&gt;http://www.mathworks.nl/help/stats/corr.html&lt;/a&gt;),&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;[RHO,PVAL] = corr(X,Y);&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-10-28T23:45:40.627" Id="121827" LastActivityDate="2014-10-28T23:45:40.627" OwnerUserId="59096" ParentId="121801" PostTypeId="2" Score="2" />
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am facing a strange issue with auto.arima. On a dataset named data, I run the following code&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;auto.arima(data,d=0,D=1,xreg=1:length(data),max.p=3,max.q=3,max.order=10,&#10;       seasonal=TRUE,stepwise=FALSE,approximation=FALSE,ic=(&quot;aic&quot;),parallel=TRUE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The outcome is &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;   Series: data &#10;   ARIMA(1,0,3)(2,1,2)[12]                    &#10;&#10; Coefficients:&#10;        ar1      ma1      ma2     ma3    sar1     sar2     sma1    sma2  1:length(data)&#10;        0.6939  -0.6417  -0.2391  0.4350  0.6197  -0.5055  -1.3211  0.6027   5e-04&#10;  s.e.  0.1349   0.1654   0.1169  0.1502  0.3040   0.1762   0.3907  0.4269   3e-04&#10;&#10; sigma^2 estimated as 0.001792:  log likelihood=143.69&#10; AIC=-267.38   AICc=-264.79   BIC=-241.73&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Then I try to fit this model using the Arima function:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; Arima(data,order=c(1,0,3),seasonal=list(order=c(2,1,2),period=12),&#10;       xreg=1:length(data),method=&quot;ML&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;but I get the following error message:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; Error in optim(init[mask], armafn, method = optim.method, hessian = TRUE,  : &#10;   non-finite finite-difference value [1]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Does someone understand why this appears ? Thanks.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-10-29T08:22:53.853" Id="121882" LastActivityDate="2014-10-29T08:22:53.853" OwnerUserId="17913" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;forecasting&gt;&lt;arima&gt;" Title="auto.arima and Arima (forecast package)" ViewCount="72" />
  
  <row Body="&lt;p&gt;Beta(0.25, 0.75) corresponds to natural parameters that are negative.  That is like making &quot;negative&quot; observations and so you're seeing the pointwise reciprocal of Beta(1.75, 1.25).  Beta(1,1) is flat (and corresponds to zero natural parameters) — moving towards that flattens things out.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you add 1,1 to each of the parameter pairs that you graphed, all of the modes will line up as you expect.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-29T09:37:50.953" Id="121893" LastActivityDate="2014-10-29T18:44:15.677" LastEditDate="2014-10-29T18:44:15.677" LastEditorUserId="858" OwnerUserId="858" ParentId="121880" PostTypeId="2" Score="3" />
  <row AnswerCount="0" Body="&lt;p&gt;I have sales by state and week. &lt;/p&gt;&#10;&#10;&lt;p&gt;Alabama|Alaska|Arizona|...&lt;/p&gt;&#10;&#10;&lt;p&gt;231|139|277|...&lt;br&gt;&#10;256|154|307|...&lt;br&gt;&#10;267|160|320|...&lt;br&gt;&#10;256|154|307|...&lt;br&gt;&#10;267|160|320|... &lt;/p&gt;&#10;&#10;&lt;p&gt;I need to create a treatment group that bunches up the states that represent 30% of sales (which will be impacted by an advertising campaign) and a 70% control group. &lt;/p&gt;&#10;&#10;&lt;p&gt;How do I go about splitting them in a way that will give me the maximum correlation? The idea is to use a time series analysis and measure uplift in sales in the treatment vs. control group.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am diving into R head first, so any way of doing so in that language would be icing on the cake.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&#10;J&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-29T09:46:26.550" Id="121895" LastActivityDate="2014-10-29T09:46:26.550" OwnerUserId="59563" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;treatment-effect&gt;" Title="Creating a treatment group to measure uplift in sales for an advertising campaign" ViewCount="33" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a silly question about EFA. Can EFA be used to identify latent variables in a research design where multiple raters used the same rubric for various essays? Say, using the same rubric, the 1st group of the two raters rated the essays from one class, the 2nd rated another, et al. Does it make sense to use EFA?&lt;/p&gt;&#10;&#10;&lt;p&gt;My previous experience about EFA is: the identical instrument with measured variables is distributed to participants and EFA helps to identify latent constructs through the calculation of commonalities, uniqueness, and errors of measurements.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-29T12:58:27.783" Id="121915" LastActivityDate="2014-10-29T12:58:27.783" OwnerUserId="59573" PostTypeId="1" Score="0" Tags="&lt;factor-analysis&gt;" Title="question about factor analysis" ViewCount="23" />
  <row Body="&lt;p&gt;TL;DR: First choose how many components you want to keep, and then pass only those components to &lt;code&gt;varimax()&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Long answer:&lt;/p&gt;&#10;&#10;&lt;p&gt;I did the same thing once :)&lt;/p&gt;&#10;&#10;&lt;p&gt;Principal component analysis will find the one component that explains most of the joint variability, then the one component that explain most of the remaining joint variability, and so on. Since the aim of PCA is variable reduction, you typically don't want to keep all the components, just the most important ones (however you may define &quot;important&quot;).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;varimax()&lt;/code&gt; does not enforce it, but &lt;a href=&quot;http://stat.ethz.ch/R-manual/R-patched/library/stats/html/varimax.html&quot; rel=&quot;nofollow&quot;&gt;its documentation&lt;/a&gt; states that the first argument must be a loading matrix with less columns (components) than rows (variables).&lt;/p&gt;&#10;&#10;&lt;p&gt;According to &lt;a href=&quot;http://en.wikipedia.org/wiki/Varimax_rotation&quot; rel=&quot;nofollow&quot;&gt;Wikipedia&lt;/a&gt;:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Varimax is so called because it maximizes the sum of the variances of the squared loadings. [...]  Intuitively, this is achieved if, (a) any given variable has a high loading on a single factor but near-zero loadings on the remaining factors and if (b) any given factor is constituted by only a few variables with very high loadings on this factor while the remaining variables have near-zero loadings on this factor.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Thus, if you rotate all of your components with varimax, you get your old variables back.&lt;/p&gt;&#10;&#10;&lt;p&gt;Forget about the negative values. They are said to be platform-dependent and you can multiply all loadings in a factor by -1 if you will.&lt;/p&gt;&#10;&#10;&lt;p&gt;What you've got to do is to choose how many components you want to keep, and pass only those components to &lt;code&gt;varimax()&lt;/code&gt;. In example, if you want to keep 2 components:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;d2.varimax &amp;lt;- varimax(d2.pca$rotation[, 1:2])&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="7" CreationDate="2014-10-29T14:14:44.993" Id="121923" LastActivityDate="2014-10-29T16:30:24.260" LastEditDate="2014-10-29T16:30:24.260" LastEditorUserId="59578" OwnerUserId="59578" ParentId="120415" PostTypeId="2" Score="4" />
  <row AcceptedAnswerId="121947" AnswerCount="1" Body="&lt;p&gt;I am currently experimenting with PYMC and I am trying out a simple example so that I start learning how things work (I am also a Python beginner, but an experienced machine learner).&lt;/p&gt;&#10;&#10;&lt;p&gt;I have set myself the following very simple model selection task:&lt;/p&gt;&#10;&#10;&lt;p&gt;Imagine we have a dataset that has been entirely generated either from model 0 or model 1 (not a mixture). &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;Model 0 is a normal distribution with mean m0 and standard deviation&#10;s0.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Likewise, model 1 is also a normal distribution with mean m1 and&#10;standard deviation s1.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;We assume that we know all parameters m0,s0,m1,s1. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;We postulate a hidden binary random variable V which indicates the model that really generated the dataset. &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;What we would like to do is infer the posterior of V.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have made an attempt in the code below, but unfortunately after trying for a couple of days, I capitulate and throw myself at your feet for assistance.&lt;/p&gt;&#10;&#10;&lt;p&gt;The example is of course very simple and may appear contrived. What I would like to do in the future, is use PYMC for model selection. Hence, how one correctly handles a hidden indicator variable like V is important to me. Unfortunately, in my code I keep getting weird results for the posterior of V.&lt;/p&gt;&#10;&#10;&lt;p&gt;I hope the code below makes sense.&#10;I thank you all for your time.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;from pymc  import *&#10;import matplotlib &#10;from pylab import hist, show, plot, title, figure&#10;import numpy as np&#10;from numpy import *&#10;&#10;&#10;#-----------------------------------------------&#10;# Generate some data&#10;#-----------------------------------------------&#10;&#10;# Define means for gaussians&#10;mu0 =  5.0&#10;mu1 = -5.0&#10;&#10;# Define standard deviations for gaussians &#10;s0 = 1.0&#10;s1 = 1.0&#10;&#10;&#10;# This variable chooses from which model we generate the observed data.&#10;# It can be either 0 or 1&#10;true_model = 0&#10;&#10;# number of data items&#10;numData = 100&#10;&#10;if true_model==0:&#10;    # Generate data from first model&#10;    data = np.random.randn(numData,1)*s0 + mu0&#10;elif true_model==1:&#10;    # Generate data from second model&#10;    data = np.random.randn(numData,1)*s1 + mu1&#10;&#10;&#10;#-----------------------------------------------&#10;# Define variables&#10;#-----------------------------------------------&#10;PR = Dirichlet(&quot;priorOnModels&quot;, theta=(0.5,0.5)) # both models are equally likely&#10;V  = Categorical('V', p=PR)&#10;&#10;&#10;#-----------------------------------------------&#10;# Define model log-likelihood&#10;#-----------------------------------------------&#10;@potential&#10;def mymodel_loglikel(sw=V):&#10;&#10;    if sw==0:        &#10;        loglikel = distributions.normal_like(data, mu0, 1.0/s0**2)&#10;&#10;    elif sw==1:        &#10;        loglikel = distributions.normal_like(data, mu1, 1.0/s1**2)&#10;&#10;    return loglikel&#10;&#10;&#10;#-----------------------------------------------&#10;# Run sampler&#10;#-----------------------------------------------&#10;simplemodel = Model([mymodel_loglikel, PR, V])&#10;mc = MCMC(simplemodel)&#10;mc.sample(iter=12000,burn=2000)&#10;&#10;figure()&#10;hist(data)&#10;Matplot.plot(mc)&#10;print &quot;expectation of V variable is %.3f&quot; % np.mean(mc.trace('V')[:])&#10;&#10;show()&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT 2: Updated after Tom's suggestions&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In response to a comment below, the weird behaviour I get concerning V, is that its mean posterior is always 0.0. Judging from the trace, it seems that V is not properly sampled. I suspect a minor programming error somewhere, but can't pinpoint it.&lt;/p&gt;&#10;&#10;&lt;p&gt;I attach  below a figure of the trace plots of V.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/FDuFB.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-10-29T16:04:38.653" FavoriteCount="1" Id="121936" LastActivityDate="2014-10-31T16:02:10.733" LastEditDate="2014-10-31T09:45:59.397" LastEditorUserId="25839" OwnerUserId="25839" PostTypeId="1" Score="1" Tags="&lt;bayesian&gt;&lt;model-selection&gt;&lt;python&gt;&lt;mcmc&gt;&lt;pymc&gt;" Title="Simple model selection example in PYMC" ViewCount="137" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am conducting a small numerical study by simulating random variables to show that a formula I derived does indeed work. I am trying to get this work peer reviewed so I would like to know what the best practice is for these type of studies.&lt;/p&gt;&#10;&#10;&lt;p&gt;I usually use codeblocks and excel for generating random numbers. I have heard that excel's RNG is biased so I'm avoiding it for this. Codeblocks in open source so it could be tampered with, would using this in a peer reviewed paper be unacceptable like citing a Wikipedia article? &lt;/p&gt;&#10;&#10;&lt;p&gt;The other question I had was do people always provide an appendix with the code they used so that everything can be double checked?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-10-29T23:09:07.807" Id="121981" LastActivityDate="2014-10-29T23:09:07.807" OwnerUserId="27271" PostTypeId="1" Score="0" Tags="&lt;code&gt;&lt;c++&gt;" Title="Common practice in numerical studies" ViewCount="15" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am working on a basic fraud detection model. I have about 10 independent features and I am trying to predict if a given transaction is genuine or fraud. Most of the features are categorical and each categorical variable has hundreds of factors. &lt;/p&gt;&#10;&#10;&lt;p&gt;As each feature has so many factors what I am currently trying to bag factor levels into larger groups. For example assume feature 1 has about 1000 unique factors. Each factor appears multiple times in the training data. I calculated the percentage of fraud for each factor. All factors with less than 10% fraud transactions were put in group A, factors with less than 20% fraud transactions were put in group B and so on. &lt;/p&gt;&#10;&#10;&lt;p&gt;As I am using the dependent variable for bagging the independent variables I am not sure if my strategy is legit? Can some one help me with this? Alternatively what are other approaches for reducing the number of factors &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-30T04:12:45.527" Id="122005" LastActivityDate="2014-10-30T04:12:45.527" OwnerUserId="48876" PostTypeId="1" Score="1" Tags="&lt;logistic&gt;&lt;categorical-data&gt;&lt;modeling&gt;" Title="Categorical Variables - Factor Reduction - Can I use the dependent variable?" ViewCount="24" />
  
  
  <row Body="&lt;p&gt;The central assumption in DID estimation is that the trends in the outcome variable would have been parallel in the treated and control groups if there had been no treatment. A common way of checking if this assumption seems plausible is to see if the trends were parallel before the intervention. It isn't necessary to have random assignment for this assumption to hold, but it is much more likely to fail if assignment was based on some characteristics of the groups.&lt;/p&gt;&#10;&#10;&lt;p&gt;To be clear, the group fixed effects take care of any time-invariant factors that differ between groups - this is why we need parallell, but not identical, trends. Time fixed effects remove parallel trends, but if trends differ, you get biased results. &lt;/p&gt;&#10;&#10;&lt;p&gt;In your setting, I would be a bit worried about systematic differences between treated and control groups. If you have many groups, you can try adding group-specific linear time trends. This allows for differing trends between groups, as long as the treatment does not shift the slope. But an even better approach would be the synthetic control groups method of Abadie et al. The basic idea is to create a synthetic control group that is a weighted average of a number of potential control groups, where the weights are chosen so that the synthetic control group mimics the treatment group in terms of pre-intervention trend and pre-determined covariates as closely as possible. See these references for details:&lt;/p&gt;&#10;&#10;&lt;p&gt;Abadie, Alberto, Alexis Diamond, and Jens Hainmueller. “Comparative Politics and the Synthetic Control Method.” American Journal of Political Science, no. Forthcoming (2014). doi:10.1111/ajps.12116.&lt;/p&gt;&#10;&#10;&lt;p&gt;———. “Synthetic Control Methods for Comparative Case Studies: Estimating the Effect of California’s Tobacco Control Program.” Journal of the American Statistical Association 105, no. 490 (June 2010): 493–505. doi:10.1198/jasa.2009.ap08746.&lt;/p&gt;&#10;&#10;&lt;p&gt;Abadie, Alberto, and Javier Gardeazabal. “The Economic Costs of Conflict: A Case Study of the Basque Country.” The American Economic Review 93, no. 1 (March 1, 2003): 113–32.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-10-30T08:26:49.573" Id="122020" LastActivityDate="2014-10-30T08:26:49.573" OwnerUserId="37448" ParentId="122003" PostTypeId="2" Score="2" />
  <row AnswerCount="4" Body="&lt;p&gt;I'm interested in a continuous variable, namely blood pressure.&lt;/p&gt;&#10;&#10;&lt;p&gt;The higher the blood pressure, the greater the risk of heart attack and stroke.&#10;However, studies frequently report that also low blood pressure is associated adverse outcomes.&lt;/p&gt;&#10;&#10;&lt;p&gt;The question is: what is the optimal blood pressure? At what value of blood pressure does risk start to increase?&lt;/p&gt;&#10;&#10;&lt;p&gt;In other words, how can I model, and visualize graphically, what hazard ratio various levels of blood pressure is associated with. I suspect that some will suggest restricted cubic splines. Do you have any suggestions on suitable R packages that will help me visualize the effect of blood pressure on hazard. I'm fairly familiar with Cox regression and plan using the RMS package. Time-dependent variables are included.&lt;/p&gt;&#10;&#10;&lt;p&gt;Sample data (no time-dependent variables):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;event &amp;lt;- c(1,0,1,0,0,0,0,1,0,0,0,1,1,0,1,0,0,1,0,1,1,1,1,0,1,1,1,0,0,1)&#10;survival &amp;lt;- c(4,29,24,29,29,29,29,19,29,29,29,3,9,29,15,29,29,11,29,5,13,20,22,29,16,21,9,29,29,15)&#10;statin &amp;lt;- c(0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0)&#10;bloodpressure &amp;lt;- c(160,120,150,140,135,110,139,140,153,129,149,163,179,129,144,119,100,115,145,150,130,120,122,129,116,171,129,126,159,150)&#10;data &amp;lt;- data.frame(event, survival, statin, bloodpressure)&#10;View(data)&#10;&#10;require(rms)&#10;fit &amp;lt;- coxph(Surv(survival, event) ~ statin + rcs(bloodpressure, 3), data=data)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I had something like this in mind:&#10;&lt;a href=&quot;http://www.bmj.com/content/325/7372/1073/F1&quot; rel=&quot;nofollow&quot;&gt;http://www.bmj.com/content/325/7372/1073/F1&lt;/a&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/17sLf.gif&quot; alt=&quot;Via Poisson regression&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.nejm.org/doi/full/10.1056/NEJMoa1215740&quot; rel=&quot;nofollow&quot;&gt;http://www.nejm.org/doi/full/10.1056/NEJMoa1215740&lt;/a&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/AzAzC.png&quot; alt=&quot;Via Cox regression&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-30T09:48:32.263" FavoriteCount="2" Id="122027" LastActivityDate="2014-11-05T05:50:34.433" LastEditDate="2014-11-02T23:31:49.370" LastEditorUserId="35413" OwnerUserId="35413" PostTypeId="1" Score="5" Tags="&lt;regression&gt;&lt;multiple-regression&gt;&lt;survival&gt;&lt;cox-model&gt;&lt;splines&gt;" Title="Determine where hazards starts to increase for a continuous variable" ViewCount="168" />
  <row AnswerCount="1" Body="&lt;p&gt;Assume a simple clinical study with N=200. Half of the participants are men and half of the participants are women. The hemoglobin of the participants ranges between 80 and 150. There's also several other variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to split the data into training and tests for a classification task, in a way that the gender and hemoglobin levels would be balanced in each set.&lt;/p&gt;&#10;&#10;&lt;p&gt;It is easy to pick 50 male/female to each set, but having simultaneously similar hemoglobin levels is difficult. I guess  ideally I should check that the mean and s.d. are on about the same range. How should I go about doing this?&lt;/p&gt;&#10;&#10;&lt;p&gt;If this has been considered in an article, refs. would be great!&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT: to clarify, I want to exclude the possibility of gender or hemoglobin inbalance from affecting the classifier result. It should only depend on the other data I have.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-30T10:44:08.770" Id="122032" LastActivityDate="2014-11-14T22:41:22.560" LastEditDate="2014-11-14T22:41:22.560" LastEditorUserId="7290" OwnerUserId="59642" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;feature-selection&gt;" Title="Stratified sampling for creating test/training sets when there are continous and categorical variables to consider?" ViewCount="36" />
  <row Body="&lt;p&gt;For non-negative random variables, a &lt;a href=&quot;http://en.wikipedia.org/wiki/Weibull_distribution&quot; rel=&quot;nofollow&quot;&gt;Weibull distribution&lt;/a&gt; can exhibit a &quot;steeper fall&quot; to the right, with appropriate values for its shape parameter (while the scale parameter controls the values it will concentrate). For example with shape parameter $k=8$ and scale $\lambda =20$ we get&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/7WVdQ.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;while with shape parameter $k=8$ and scale $\lambda =2$ we get&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/YjgPE.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Same shape -different &quot;main&quot; range.  &lt;/p&gt;&#10;&#10;&lt;p&gt;For random variables that take on positive and negative values, the &lt;a href=&quot;http://en.wikipedia.org/wiki/Skew_normal_distribution&quot; rel=&quot;nofollow&quot;&gt;Skew-normal distribution&lt;/a&gt; (and the numerous variations-extensions that have sprung out of it) could be a candidate, with appropriate values for its shape (or &quot;skew&quot;) parameter.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-10-30T11:09:56.747" Id="122034" LastActivityDate="2014-10-30T11:09:56.747" OwnerUserId="28746" ParentId="122029" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;Use Tau-x (where the &quot;x&quot; refers to &quot;eXtended&quot; Tau-b). Tau-x is the correlation equivalent of the Kemeny-Snell distance metric -- proven to be the unique distance metric between lists of ranked items that satisfies all the requirements of a distance metric. See chapter 2 of &quot;Mathematical Models in the Social Sciences&quot; by Kemeny and Snell, also &quot;A New Rank Correlation Coefficient with Application to the Consensus Ranking Problem, Edward Emond, David Mason, Journal of Multi-Criteria Decision Analysis, 11:17-28 (2002).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-30T16:28:12.047" Id="122076" LastActivityDate="2014-10-30T16:28:12.047" OwnerUserId="59656" ParentId="56852" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Is there any difference between machine learning and stastitical techniques. I have searched a lot some researchers say that there are some overlap some are saying there is no difference.Can you give some research paper which suggest that both are different from of classification. &#10;Thanx in advance,..&lt;/p&gt;&#10;" ClosedDate="2014-10-30T17:43:14.977" CommentCount="0" CreationDate="2014-10-30T17:14:52.437" Id="122082" LastActivityDate="2014-10-30T17:14:52.437" OwnerUserId="28681" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;statistical-significance&gt;&lt;mathematical-statistics&gt;&lt;data-mining&gt;&lt;summary-statistics&gt;" Title="difference between machine learning and stastitical technique" ViewCount="44" />
  <row Body="&lt;p&gt;There is an R package named &lt;strong&gt;glmmLasso&lt;/strong&gt; that performs variable selection and shrinkage for generalized linear mixed models using the L1 penalty (the Lasso):&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://cran.r-project.org/web/packages/glmmLasso/index.html&quot; rel=&quot;nofollow&quot;&gt;http://cran.r-project.org/web/packages/glmmLasso/index.html&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The Lasso isn't the same as ridge regression (which uses the L2 penalty) but will still perform shrinkage of coefficients for correlated variables.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-10-30T18:33:52.593" Id="122090" LastActivityDate="2014-10-30T18:33:52.593" OwnerUserId="13634" ParentId="118034" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to figure out when (during multiple testing) to use $p$-values and when to use $q$-values.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://pages.pomona.edu/~jsh04747/Student%20Theses/Kimes_Final.pdf&quot; rel=&quot;nofollow&quot;&gt;This student thesis&lt;/a&gt; explains how this particular theorem (Eq. 11 in the paper) allows one to interpret the $q$-value as a simple conditional probability. According to the student, &quot;the $q$-value is the $p$-value with the conditional statement and event of interest reversed.&lt;/p&gt;&#10;&#10;&lt;p&gt;The student later gives an example:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&quot;If a giant meteor were heading toward the earth a hypothesis interesting to test might be that the world was ending soon. A $p$-value calculation would produce the probability of an event as catastrophic or more than a meteor heading towards the earth happening give that the world was ending. The $q$-value would be the probability of the world ending given the observation of an event as catastrophic as a meteor heading towards Earth. In this case, the $q$-value, rather than the $p$-value answers a much more natural question. It makes substantially more intuitive sense to be interested in how likely it is that the world will end than how likely the event of the meteor was given the world is indeed ending.&quot;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;This scenario makes sense as to why one would use the $q$-value over a $p$-value. But in what scenarios would it make sense to use a $p$-value over a $q$-value?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-10-30T20:40:41.547" Id="122102" LastActivityDate="2014-10-30T20:40:41.547" OwnerUserId="59666" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;&lt;p-value&gt;&lt;fdr&gt;" Title="p-value over q-value?" ViewCount="35" />
  
  <row Body="&lt;p&gt;Transforming the response prior to doing a linear regression is doing this:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$E(g(Y)) \sim \beta_0 + \beta_1x_1 + \ldots + \beta_px_p$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $g$ is a given function, and we assume that $g(Y)$ has a given distribution (usually normal).&lt;/p&gt;&#10;&#10;&lt;p&gt;A generalised linear model is doing this:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$g(E(Y)) \sim \beta_0 + \beta_1x_1 + \ldots + \beta_px_p$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $g$ is the same as before, and we assume that $Y$ has a given distribution (usually not normal).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-31T00:14:55.747" Id="122126" LastActivityDate="2014-10-31T00:14:55.747" OwnerUserId="1569" ParentId="122103" PostTypeId="2" Score="6" />
  <row AnswerCount="0" Body="&lt;p&gt;Please note, stats is NOT my area (hence why I need help!) and I may not be using the correct terminology. I hope I can explain my question clearly enough. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;BACKGROUND:&lt;/strong&gt;&#10;I have collected behavioural data from individual mice at 5 different time points. One group of mice (n=25) has been infected with a parasite that potentially changes behaviour. One group was sham infected - no parasite (n=25). &lt;/p&gt;&#10;&#10;&lt;p&gt;I have collected multiple behavioural measurements from each mouse at each timepoint. Some of the behavioural measures are probably correlated, so I want to use Factor Analysis to find the common factors that describe the data. I then want to get the factor scores for individuals and compare the factors scores for the treatment groups (parasite vs. no parasite) over time. This is to see whether behaviour changes over time, and whether the parasite infection has an effect on behaviour).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;QUESTION:&lt;/strong&gt; Can I pool the data from all time points for the Factor Analysis prior to repeated measures ANOVA using factor scores? The repeated measures from each animal are not independent, but I have a balanced design whereby each mouse is equally represented at each time point. My supervisor has suggested this means the lack of independence isn't a problem. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;However&lt;/strong&gt;, for the hell of it I performed a factor analysis (principal axis method) for each time point individually (n=50 for each time point). For the first two time points I get the same factor structure, however the third time point is different (the measures loading on factor 1 at time point three are the measures that loaded on factor 2 in the first two time points). This makes me think something is different about the structure of the data at this time point, and simply pooling all time points together doesn't make sense.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Can anybody recommend a better way to analyse these data?&lt;/strong&gt; Basically I want to know whether or not behaviour changes over time, and whether infection with the parasite has an effect on behaviour (over and above the change over time).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-10-31T00:51:40.197" FavoriteCount="1" Id="122130" LastActivityDate="2014-10-31T00:51:40.197" OwnerUserId="59678" PostTypeId="1" Score="1" Tags="&lt;repeated-measures&gt;&lt;factor-analysis&gt;" Title="Using Factor Analysis prior to repeated measures ANOVA" ViewCount="42" />
  <row AcceptedAnswerId="122202" AnswerCount="1" Body="&lt;p&gt;How can I calculate the variance of the precision in a normal distribution, knowing I used a conjugate prior? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-31T02:00:51.440" Id="122135" LastActivityDate="2014-10-31T16:37:57.227" LastEditDate="2014-10-31T02:23:30.600" LastEditorUserId="7290" OwnerUserId="53201" PostTypeId="1" Score="0" Tags="&lt;normal-distribution&gt;&lt;inference&gt;&lt;bayes&gt;&lt;conjugate-prior&gt;" Title="Variance of precision in conjugate prior" ViewCount="20" />
  <row AcceptedAnswerId="122639" AnswerCount="1" Body="&lt;p&gt;In the process of designing a quiz application that can assess the student for understanding of a particular concept, I came across Item Response Theory. I have absolutely no clue whether applying this would make my life easy or will complicate further. Till now I''ve been able to find few papers, blogs and also noted a few books on this topic. But I am not able to figure out what should I start with. Any good material to understand what this beast is in layman's terms? Also, it would be great if anyone could help me know what other tools towards similar end can be used.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-31T05:11:51.240" Id="122147" LastActivityDate="2014-11-04T15:21:20.870" OwnerUserId="59687" PostTypeId="1" Score="1" Tags="&lt;irt&gt;" Title="Understanding and applying Item Response Theory" ViewCount="31" />
  <row AnswerCount="0" Body="&lt;p&gt;My study design delivers both, count data and continous outcomes (e.g., numbers of taxa vs. an diversity index). As these variables are used as response variables, I have to use negative binomial glm for the count data (R function glm.nb) and guassian glm for the index. The predictors are environmental variables. &lt;/p&gt;&#10;&#10;&lt;p&gt;Now I want to compare the individual effects of the predictors. Beta coefficients seem to be useful for this purpose. But the betas for the negative binomial models are systematically lower (factor 100) than the betas for the gaussian models. The reason might be the different link functions (log for glm.nb; identity for glm). &lt;/p&gt;&#10;&#10;&lt;p&gt;I would very like to know, how the coefficients of the glm.nb models have to be rescaled to gain betas which are comparable with the gaussian models? The results should be that consistent, that a glm.nb-beta of 0.8 really means that this predictor has a minor importance compared to a glm-beta of 1. &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for your help in advance!!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-31T09:49:45.397" Id="122161" LastActivityDate="2014-10-31T09:49:45.397" OwnerUserId="59645" PostTypeId="1" Score="0" Tags="&lt;generalized-linear-model&gt;&lt;regression-coefficients&gt;&lt;negative-binomial&gt;" Title="How to make beta coefficients comparable?" ViewCount="38" />
  <row Body="&lt;p&gt;From what I see, the terminology used in the paper is slightly incorrect. What MED means is Median Euclidean Distance. What AED means is Average Euclidean Distance. &lt;/p&gt;&#10;&#10;&lt;p&gt;Euclidean distance is calculated by the formula in the following link&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Euclidean_distance&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Euclidean_distance&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;MED is calculated by taking median of all the euclidean distances between the actual and predicted points &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-31T10:04:03.193" Id="122162" LastActivityDate="2014-10-31T10:04:03.193" OwnerUserId="59397" ParentId="122158" PostTypeId="2" Score="1" />
  <row AnswerCount="2" Body="&lt;p&gt;I am a complete newbie to regression trees so maybe I am not understanding it properly. I got the following tree from my analysis (function &lt;a href=&quot;http://www.inside-r.org/packages/cran/tree/docs/tree&quot; rel=&quot;nofollow&quot;&gt;tree()&lt;/a&gt; from R package &lt;code&gt;tree&lt;/code&gt;):&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/xyDyZ.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;This is nice, but how can I get more precise tree? For example, the clc_312 variable could definitely be split at more thresholds, giving more precise model. This is obvious when I plot the clc_312 variable against the response variable:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/xgJvX.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;This plot shows the clc_312 variable cut in 10 pools and boxploted (don't know how to analyze this better, scatterplott is just a mess, you don't actually see the distribution of the response variable for particular range of clc_312 values). The green thing are means with CI, red line corresponds to the threshold detected by &lt;code&gt;tree()&lt;/code&gt;. The range could definitely be split more.&lt;/p&gt;&#10;&#10;&lt;p&gt;First I thought that &lt;code&gt;tree()&lt;/code&gt; will only split each variable once, but I did the following experiment on generated data, which shows that one variable can actually be split more times:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;require(tree)&#10;&#10;set.seed(123)&#10;N &amp;lt;- 100&#10;x &amp;lt;- runif(N, 0, 10)&#10;alpha &amp;lt;- -2&#10;beta1 &amp;lt;- 0.5&#10;beta2 &amp;lt;- 0.1&#10;sigma &amp;lt;- 0.005&#10;y &amp;lt;- alpha + beta1 * x + rnorm(N, 0, sigma)&#10;&#10;tree1 &amp;lt;- tree(y ~ x)&#10;plot(tree1, type = &quot;u&quot;)&#10;text(tree1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/iSxiW.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;So it &lt;em&gt;is&lt;/em&gt; possible! How do I force &lt;code&gt;tree()&lt;/code&gt; to give me more precise model and continue to split the variables further?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-31T10:59:16.457" Id="122168" LastActivityDate="2014-10-31T14:29:43.727" OwnerUserId="5509" PostTypeId="1" Score="2" Tags="&lt;multiple-regression&gt;&lt;random-forest&gt;&lt;cart&gt;" Title="How can I get more precise regression tree?" ViewCount="128" />
  <row Body="&lt;p&gt;You simply can't get high precision from a single tree unless your sample size is enormous and the signal:noise ratio is high.  Suppose that a predictor X acts linearly against Y.  Many splits will be needed to represent this relationship, but the catch is that the estimate of $E(Y|X)$ for each interval of $X$ makes no use of the estimate of $E(Y|X)$ in the surrounding $X$ intervals.  In other words, single trees do not use interpolation or borrowing of information across $X$.  Because of that the variance of the estimate of $E(Y|X)$ is high and the tree does not compete with smooth methods such as ordinary regression or regression splines.  Plotting the tree's predicted values against $X$ starts to reveal the problem.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-10-31T12:51:47.033" Id="122180" LastActivityDate="2014-10-31T12:51:47.033" OwnerUserId="4253" ParentId="122168" PostTypeId="2" Score="3" />
  
  
  <row AnswerCount="0" Body="&lt;h1&gt;Short question&lt;/h1&gt;&#10;&#10;&lt;p&gt;Do the likelihood function evaluated at the ML estimate and the marginalised posterior converge in the limit of a large number of observations?&lt;/p&gt;&#10;&#10;&lt;h1&gt;Long question&lt;/h1&gt;&#10;&#10;&lt;p&gt;I expect the two densities to converge. They do--almost. Any thoughts would be appreciated.&lt;/p&gt;&#10;&#10;&lt;p&gt;Consider the multinomial likelihood function over $l$ categories&#10;$$&#10;P(n|\rho) =\alpha \prod_i \rho_i^{n_i},&#10;$$&#10;where $\alpha$ is a normalisation constant, $n_i$ are count data, and the density parameters $\rho_i$ are normalised such that $\sum_i\rho_i=1$. The maximum likelihood estimate of the density parameters is&#10;$$&#10;\hat{\rho}_i=\frac{n_i}{n},&#10;$$&#10;where $n=\sum_in_i$. The likelihood function evaluated at the ML estimate is&#10;$$&#10;P(n|\hat{\rho})=\alpha\prod_i\left(\frac{n_i}{n}\right)^{n_i}.&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The joint distribution is&#10;$$&#10;\begin{align}&#10;P(n\rho)&amp;amp;=P(n|\rho)P(\rho)\\&#10;&amp;amp;=P(n|\rho)\Gamma(l)\delta(1-\sum_i\rho_i),&#10;\end{align}&#10;$$&#10;where we have assumed a flat prior for $\rho$ in the second equality. Marginalising yields&#10;$$&#10;\begin{align}&#10;P(n)&amp;amp;=\int d\rho\,P(n|\rho)\\&#10;&amp;amp;=\alpha\frac{\Gamma(l)}{\Gamma(l+n)}\prod_i^l\Gamma(n_i+1).&#10;\end{align}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose $n_i&amp;gt;&amp;gt;1$ such that we can use Stirling's approximation after taking logarithms&#10;$$&#10;\begin{align}&#10;\log \frac{P(n)}{\alpha}&amp;amp;=\log\Gamma(l)-\log\Gamma(l+n)+\sum_i^l\log(n_i!)\pm\log(n!)\\&#10;&amp;amp;\approx \log\Gamma(l)-\log\Gamma(l+n) +\log(n!) - n\log n + n +\sum_i^l n_i\log n_i - n_i\\&#10;&amp;amp;=\log\Gamma(l)-\log\Gamma(l+n) +\log(n!) \underbrace{- n\log n +\sum_i^l n_i\log n_i}_{\log\frac{P(n|\hat{\rho})}{\alpha}}.&#10;\end{align}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;So we almost recover the maximum-likelihood value except for the extra terms&#10;$$&#10;\log\Gamma(l)-\log\Gamma(l+n) + \log\Gamma(n+1).&#10;$$&#10;The extra terms are irrelevant if $n$ is fixed but causes me trouble for varying $n$.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-10-31T14:56:58.340" Id="122193" LastActivityDate="2014-10-31T14:56:58.340" OwnerUserId="17643" PostTypeId="1" Score="0" Tags="&lt;maximum-likelihood&gt;&lt;posterior&gt;&lt;marginal&gt;" Title="Do the marginalised posterior and likelihood function converge in the limit of a large number of observations" ViewCount="19" />
  
  <row Body="&lt;p&gt;It is very hard to interpret your results when you have not pre-specified the model but have engaged in significant testing and resulting model modifications.  And I don't recommend the global goodness of fit test you used because it has a degenerate distribution and not a $\chi^2$ distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;Two recommended ways to assess model fit are:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Bootstrap overfitting-corrected smooth nonparametric (e.g., *loess) calibration curve to check absolute accuracy of predictions&lt;/li&gt;&#10;&lt;li&gt;Examine various generalizations of the model, testing whether more flexible model specification works better.  For example, do likelihood ratio or Wald $\chi^2$ tests of extra nonlinear or interaction terms.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;There are some advantages of regression splines over fractional polynomials, including:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;The predictor can have values $\leq 0$; you can't take logs of such values as required by FPs&lt;/li&gt;&#10;&lt;li&gt;You needn't worry about a predictor's origin.  FPs assume that zero is a &quot;magic&quot; origin for predictors, whereas regression splines are invariant to shifting a predictor by a constant.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;For more about regression splines and linearity and additivity assessment see my handouts at &lt;a href=&quot;http://biostat.mc.vanderbilt.edu/CourseBios330&quot; rel=&quot;nofollow&quot;&gt;http://biostat.mc.vanderbilt.edu/CourseBios330&lt;/a&gt; as well as the R &lt;code&gt;rms&lt;/code&gt; package &lt;code&gt;rcs&lt;/code&gt; function.  For bootstrap calibration curves penalized for overfitting see the &lt;code&gt;rms&lt;/code&gt; &lt;code&gt;calibrate&lt;/code&gt; function.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-10-31T18:24:54.863" Id="122217" LastActivityDate="2014-11-02T22:31:20.287" LastEditDate="2014-11-02T22:31:20.287" LastEditorUserId="4253" OwnerUserId="4253" ParentId="122212" PostTypeId="2" Score="5" />
  
  <row Body="Methods for estimating parameters in sub-populations. " CommentCount="0" CreationDate="2014-10-31T18:47:25.003" Id="122223" LastActivityDate="2014-10-31T19:33:59.693" LastEditDate="2014-10-31T19:33:59.693" LastEditorUserId="17072" OwnerUserId="17072" PostTypeId="4" Score="0" />
  
  
  
  <row Body="&lt;p&gt;$b_3$ is the difference between white females and the sum of $a+b_1+b_2$.  That is, the difference between white females and the sum of non-white males plus the difference between non-white females and non-white males plus the difference between white males and non-white males.&lt;br&gt;&#10;\begin{align}&#10;b_3 = \bar x_\text{white female} - \big[&amp;amp;\ \ \bar x_\text{non-white male}\quad\quad\quad\quad\quad\quad\quad\ \ +  \\&#10;     &amp;amp;(\bar x_\text{non-white female} - \bar x_\text{non-white male}) +  \\&#10;     &amp;amp;(\bar x_\text{white male}\quad\quad\! - \bar x_\text{non-white male})\quad\ \big]&#10;\end{align}&lt;br&gt;&#10;Honestly, it's a bit of a mess to interpret in this way.  More typically, we interpret the &lt;em&gt;test&lt;/em&gt; of $b_3$ as a test of the additivity of the effects of ${\rm white}$ and ${\rm female}$.  (The expression within the square brackets $[]$ is the additive effect of ${\rm white}$ and ${\rm female}$.)  Then we make more substantive interpretations only of simple effects (i.e., the effect of one factor within a pre-specified level of the other factor).  People rarely try to interpret the interaction effect / coefficient in isolation.  &lt;/p&gt;&#10;&#10;&lt;p&gt;It may also help you to read my answer here: &lt;a href=&quot;http://stats.stackexchange.com/a/120035/7290&quot;&gt;Interpretation of betas when there are multiple categorical variables&lt;/a&gt;, which covers an analogous, but simpler, situation &lt;em&gt;without&lt;/em&gt; the interaction.  &lt;/p&gt;&#10;" CommentCount="10" CreationDate="2014-11-01T03:40:51.360" Id="122251" LastActivityDate="2014-11-01T14:45:23.520" LastEditDate="2014-11-01T14:45:23.520" LastEditorUserId="7290" OwnerUserId="7290" ParentId="122246" PostTypeId="2" Score="2" />
  <row AnswerCount="1" Body="&lt;p&gt;Suppose that I have a sequence of size $n$: $x_1,\ldots,x_n$, $x_i\in\{0,1\}$.  My null hypothesis is that all $n$ members of the sequence are drawn  independently from an identical Bernoulli distribution with $P(x_i=1)=p$.  My alternate hypothesis is that the $j$-th subsequence of length $m$ is drawn independently from an identical Bernoulli distribution with $P(x_i=1)=p_j$ for $(j-1)m&amp;lt;i\leq jm$, $1\leq j \leq n/m$ such that there exists $p_{\mathcal{X}}$ which partitions the set of subsequences; that is, letting $\mathcal{X}=\{j: p_j&amp;lt;p_{\mathcal{X}}\}$, $|\mathcal{X}|=\gamma n$ with $\gamma\in(0,1)$ a test parameter. We assume that the draws are independent across subsequences.&lt;/p&gt;&#10;&#10;&lt;p&gt;The fact that the sums of the subsequences $s_j=\sum_{i=(j-1)m+1}^{jm}x_i$ are drawn from binomial distribution yields the following alternative statement of the hypotheses:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$H_0: s_j\sim \text{Binomial}(p,m), 1\leq j \leq n/m\\&#10;H_1:s_j\sim \text{Binomial}(p_j,m)~\text{and there exists}~p_{\mathcal{X}}~\text{such that}~\mathcal{X}=\{j: p_j&amp;lt;p_{\mathcal{X}}\}, |\mathcal{X}|=\gamma n$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a statistical test between these two hypotheses given a parameter $\gamma$?  We know $n$ to be very large, order of hundreds of millions, $m$ is also large, order of tens of thousands, $p$ and $p_j$'s are unknown, but small (order of $10^{-4}$ so zeros are prevalent).  Any suggestions?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-01T04:55:19.623" Id="122256" LastActivityDate="2014-11-03T11:27:08.453" LastEditDate="2014-11-01T05:34:45.460" LastEditorUserId="6946" OwnerUserId="6946" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;&lt;binomial&gt;&lt;bernoulli-distribution&gt;" Title="Testing for samples being drawn from identical Bernoulli r.v.'s" ViewCount="42" />
  <row AnswerCount="0" Body="&lt;p&gt;I am looking for some mixed-type dataset with free-form textual features along with some structural features (preferably continuous) - labeled, for supervised setting (classification or prediction). It should not be necessarily large, moderate size would work - just for illustration purposes.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-01T07:03:22.723" Id="122261" LastActivityDate="2014-11-01T07:03:22.723" OwnerUserId="30331" PostTypeId="1" Score="0" Tags="&lt;dataset&gt;&lt;text-mining&gt;&lt;supervised-learning&gt;" Title="Dataset with mixed structured/unstructured data" ViewCount="28" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a model:&#10;$$&#10;\ln({\rm earnings}) = a+b_1{\rm female}+b_2{\rm white}+b_3{\rm female}\times{\rm white}&#10;$$&#10;${\rm female}$ and ${\rm white}$ are dummy variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;t= female=-1.65 white=8.86 female*white=-4.61 cons=66.07&lt;/p&gt;&#10;&#10;&lt;p&gt;p&gt;|t|= female=(0.100) white=0.000 female*white=0.000 cons=0.000&lt;/p&gt;&#10;&#10;&lt;p&gt;std error= female=0.0546456 white=0.043098 female*white=0.059699  cons=0.0396351 &lt;/p&gt;&#10;&#10;&lt;p&gt;95% confidence interval.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now i know there is gender pay difference with b1 (9%), I also know there is race pay difference with b2(38%). Now i need to know is their a gender pay gap for whites only. How can I figure that out regression above.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note:( my last question had two parts, part 1 interpreting b3 which was answered, it is part 2 which has not been asnwered yet I have edited my question with more detail so now it can be looked at more appropriately Thanks)&lt;/p&gt;&#10;" ClosedDate="2014-11-01T10:33:16.707" CommentCount="1" CreationDate="2014-11-01T09:37:30.700" Id="122268" LastActivityDate="2014-11-01T18:31:08.173" LastEditDate="2014-11-01T18:31:08.173" LastEditorDisplayName="user59740" OwnerDisplayName="user59740" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;self-study&gt;&lt;categorical-data&gt;&lt;interaction&gt;&lt;stata&gt;" Title="Dummy variable interpretation" ViewCount="33" />
  <row Body="&lt;p&gt;When you are specifying random effects in an &lt;code&gt;lme4::lmer&lt;/code&gt; model, the random&#10;factors go on the left of the pipe and the non-independence grouping variables&#10;go on the right, so the fully specified model in your question would very&#10;likely be:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lmer(rt ~ A*B*C + (A*B|subj))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I took some time to explore the difference between a random effect on the left&#10;of the pipe to a random effect on the right side of the pipe but it made a&#10;better post on it's own than as an answer to your particular question.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://rpubs.com/pedmiston/visualizing-random-effects&quot; rel=&quot;nofollow&quot;&gt;RPubs doc&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;https://gist.github.com/pedmiston/acd8eac94e786ba7ea87&quot; rel=&quot;nofollow&quot;&gt;Gist code&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The most noticeable difference between the following two models...&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lmer(rt ~ A + (1|subj/A))&#10;lmer(rt ~ A + (A|subj))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;...is that the latter estimates a random correlation parameter between random&#10;intercepts and random slopes. If you remove that random correlation&#10;parameter...&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lmer(rt ~ A + (1|subj/A))&#10;lmer(rt ~ A + (1|subj) + (0+A|subj))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;...the two models return the exact same fixed effects (parameter estimates&#10;and associated errors), though I would guess that similarity depends on&#10;the particular design of the study.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-11-01T15:26:23.567" Id="122289" LastActivityDate="2014-11-02T03:31:02.780" LastEditDate="2014-11-02T03:31:02.780" LastEditorUserId="26849" OwnerUserId="26849" ParentId="121504" PostTypeId="2" Score="3" />
  
  
  
  <row Body="&lt;p&gt;My answer to your question &quot;Would this yield a unique VAR?&quot; is &lt;strong&gt;yes&lt;/strong&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;A vector error correction (VECM) model has an equivalent vector autoregression (VAR) representation. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;(VECM)&lt;/strong&gt; $\;\;\;\Delta y_t=\Pi y_{t-1}+\Gamma_1\Delta y_{t-1}+...+\Gamma_{p-1}\Delta y_{t-(p-1)}+\varepsilon_t$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;(VAR)&lt;/strong&gt; $\;\;\;\;\;\;\;\; y_t=A_1 y_{t-1}+...+A_p y_{t-p}+\varepsilon_t$&lt;/p&gt;&#10;&#10;&lt;p&gt;where on one hand &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;(A)&lt;/strong&gt; $\;\;\Pi=-(I-A_1-...-A_p) \;$ and $\;\;\Gamma_i=-(A_{i+1}+...+A_p)$&lt;/p&gt;&#10;&#10;&lt;p&gt;while on the other hand &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;(B)&lt;/strong&gt; $\;\; A_1=\Pi+I+\Gamma_1$, $\;A_i=\Gamma_i-\Gamma_{i-1}$ for $i=2,...,p-1$, and $A_p=-\Gamma_{p-1}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem the of lack of uniqueness is &quot;inside&quot; the matrix $\Pi$ (if you factor it into a loading matrix $\alpha$ and a matrix containing the cointegrating vectors $\beta$, then you could ask what happens if you multiply $\alpha$ by a constant and divide $\beta$ by the same constant). However, there does not seem to be a problem once you go to the VAR representation; the possible factoring of $\Pi$ does not matter there. The VECM-equivalent VAR representation is unique.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-01T19:21:21.810" Id="122308" LastActivityDate="2015-02-18T06:53:46.770" LastEditDate="2015-02-18T06:53:46.770" LastEditorUserId="53690" OwnerUserId="53690" ParentId="32456" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a set of datapoints $x_i$ which have known upper bounds for absolute errors $\delta x_i$. (To clarify, this means each $x_i$ is actually $x_{i_0} \pm \delta x_i$). For simplicity, assume that all the errors are equal, i.e., $\delta x_i = \delta x$ $\forall x_i$. &lt;/p&gt;&#10;&#10;&lt;p&gt;The statistics of the datapoints is expected to fit a Gaussian. In other words, if one plots a histogram of the datapoints $x_i$, the histogram would fit a Gaussian reasonably well. Assume the mean of this Gaussian is zero.&lt;/p&gt;&#10;&#10;&lt;p&gt;How does one quantify the error in the standard deviation and variance of this Gaussian given the individual errors of the datapoints $x_i$ as described above?&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that I'm interested in the computing the error associated with the SD and variance, and not the SD and variance themselves.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-11-01T23:50:19.047" Id="122338" LastActivityDate="2014-11-01T23:50:19.047" OwnerUserId="43009" PostTypeId="1" Score="1" Tags="&lt;measurement-error&gt;&lt;error-propagation&gt;" Title="Error in standard deviation and variance from error in data" ViewCount="17" />
  <row Body="&lt;p&gt;I would actually say the opposite, that more commonly occurring events are easier to predict, if by that you mean something like constructing a regression model. &lt;/p&gt;&#10;&#10;&lt;p&gt;Assuming that the events you're talking about can be broken down into binary 'yes/no' or 'event/no event', I would think of this like a logistic regression. Since tests of binary outcomes are based on number of events observed rather than number of participants, a higher number of events is desirable and permits detecting smaller effects.&lt;/p&gt;&#10;&#10;&lt;p&gt;The challenge faced by RCT drug trials in detecting adverse events is an example of the problems posed by modelling rare events. &lt;/p&gt;&#10;&#10;&lt;p&gt;For an example of calculating sample sizes for binary outcomes see:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://ndt.oxfordjournals.org/content/25/5/1388.long&quot; rel=&quot;nofollow&quot;&gt;http://ndt.oxfordjournals.org/content/25/5/1388.long&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://rds.epi-ucsf.org/ticr/syllabus/courses/26/2009/02/03/Other/readings/Wittes_02.pdf&quot; rel=&quot;nofollow&quot;&gt;http://rds.epi-ucsf.org/ticr/syllabus/courses/26/2009/02/03/Other/readings/Wittes_02.pdf&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-02T00:02:33.703" Id="122341" LastActivityDate="2014-11-02T00:02:33.703" OwnerUserId="59627" ParentId="122331" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Hello there my fellow Cross Validated members; I’m here today to brainstorm a little bit with all of you out there, to flesh out our collectively acquired data analytic skills, and to try and find new perspectives on an issue that has been haunting me since we started the module of regression models, namely: How can we effectively build a useful model under multicollinearity conditions?&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is the overall scenario I’m working on:&#10;I’m currently being tasked with building a database of how time is spent within a finance department of a local company, with the final goal of deploying a total quality program based on the exploratory analysis made on this data, which we believe will lead us to an average overtime reduction of at least 25% for all of the activities within our area in about a year. To do this, we are supposed to steadily deploy a culture of continuous improvement in all of the collaborators, however, it has been very hard for me to convince the people in the area to follow suit, since I’ve just practically started to &quot;culturize&quot; myself with this kind of mind frame and I’m not sure how to go about translating what we’ve learned in this course to real arguments that help me and my team move forward with this project.&lt;/p&gt;&#10;&#10;&lt;p&gt;The Action Plan:&#10;Since the main thing I’m responsible for in the team is following through with the data acquisition and carrying on with the exploratory data analytics, I intervened with how the data was being collected so that it was steadily captured with classifications about exactly where is the time going (I.E. we have a column for the activity, another one for its classification, another one for a subclasification, and a final one where a judgment is made over whether or not the activity adds value to our customers), with the intent of building a model that lets me argue that, for every hour we spend on certain activities, we end up staying a Beta Slope amount of extra time within the office, that does not add value to our customers, so that I can convince my team that we really need to have a full compromise with the project so we can actually improve (In short, I’m planning to do a basic MLR to back up my arguments).&lt;/p&gt;&#10;&#10;&lt;p&gt;The Issues:&#10;Since the explanatory variables I’m working with are basically subsets of the response variable, all of my estimators turn out to be statistically insignificant, and the model presents a very heavy load of multicollinearity; and if this wasn’t enough, I have no way of asking most of my associates to capture an additional variable that can sort this out (since we’ve ben taking times for about a month now, and my boss will never approve the extra funding for the additional data), however I’m pretty sure that something might be doable with the information I have collected thus far…I’m just not sure what can I do, considering that the main issue I have is multicollinearity.&lt;/p&gt;&#10;&#10;&lt;p&gt;I thank you beforehand for any and all inputs that you can give us in this case.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-02T00:31:09.087" FavoriteCount="1" Id="122346" LastActivityDate="2014-11-02T00:31:09.087" OwnerUserId="59780" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;multiple-regression&gt;&lt;experiment-design&gt;&lt;inference&gt;&lt;multicollinearity&gt;" Title="How Can I Build A Regression Model With Collinear Data?" ViewCount="26" />
  
  
  <row AcceptedAnswerId="122412" AnswerCount="1" Body="&lt;p&gt;I have a bunch of IID samples from a random variable with unknown mean and unknown variance.  I now need to know the &lt;em&gt;variance of the average of&lt;/em&gt; these samples.&lt;/p&gt;&#10;&#10;&lt;p&gt;I found some references.  In decreasing order of helpfulness: &lt;a href=&quot;http://scipp.ucsc.edu/~haber/ph116C/iid.pdf&quot; rel=&quot;nofollow&quot;&gt;1&lt;/a&gt;, &lt;a href=&quot;https://onlinecourses.science.psu.edu/stat414/node/167&quot; rel=&quot;nofollow&quot;&gt;2&lt;/a&gt;, &lt;a href=&quot;http://mxp.physics.umn.edu/f03/Lecture/lecture031205.pdf&quot; rel=&quot;nofollow&quot;&gt;3&lt;/a&gt;.  These give a formula for the variance in terms of the variance of the original distribution.  What I haven't been able to find is a direct solution for the case of the original distribution having unknown variance.  This &lt;a href=&quot;http://stats.stackexchange.com/questions/29902/what-is-the-distribution-of-the-variance-of-a-sample-from-an-unknown-distributio&quot;&gt;similar question&lt;/a&gt; may be of use, but I am unsure how to apply it.&lt;/p&gt;&#10;&#10;&lt;p&gt;My first thought is to try to use the sample variance as an estimate for the population variance.  My question is, is that the right way to do it?  Is so, how do I correct for any differences?  If not, what is the right way?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-02T02:34:34.240" Id="122360" LastActivityDate="2014-11-02T20:36:35.213" OwnerUserId="16237" PostTypeId="1" Score="1" Tags="&lt;variance&gt;&lt;unbiased-estimator&gt;" Title="Variance of Mean of Samples from Unknown Distribution" ViewCount="63" />
  
  
  
  <row AnswerCount="2" Body="&lt;p&gt;Can we say &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\text{Var}(\beta_1) = \text{Var}\left(\frac{\sum (x_i-\bar x)y_i}{\sum (x_i- \bar x)^2}\right) = \left(\frac{\sum (x_i-\bar x)}{\sum (x_i- \bar x)^2}\right)^2 \text{Var}(y_i) \;\;??$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I am not sure if I can separate the $x$'s i from $\sum (x_i-\bar x)y_i$. This expression seems like a linear combination of $y_i$'s. Is this legit because every $y_i$ follows the same distribution?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-11-02T19:11:35.260" Id="122406" LastActivityDate="2014-11-02T21:54:20.297" LastEditDate="2014-11-02T20:13:55.910" LastEditorUserId="28746" OwnerUserId="52145" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;regression-coefficients&gt;" Title="The variance of linear regression estimator $\beta_1$" ViewCount="164" />
  <row Body="&lt;p&gt;As always &lt;code&gt;@Marc Claesen&lt;/code&gt; as a great answer. &lt;/p&gt;&#10;&#10;&lt;p&gt;I'd just add that the key concept that seems to be missing is the concept of a &lt;strong&gt;cost function&lt;/strong&gt;. In any model you have an implicit or explicit cost of false negatives to false positives (FN/FP). For the unbalanced data described one is often willing to have a 5:1 or 10:1 ratio. There are many ways of introducing cost functions into models. A traditional method is to impose a probability cut-off on the probabilities produced by a model - this works well for logistic regression. &lt;/p&gt;&#10;&#10;&lt;p&gt;A method used for strict classifiers that do not naturally output probability estimates is to undersample the majority class at a ratio that will induce the cost function you are interested in. Note that if you sample at 50/50 you are inducing an arbitrary cost function. The cost function is different but just as arbitrary as if you sampled at the prevalence rate. You can often predict an appropriate sampling ratio that corresponds to your cost function (it is usually not 50/50), but most practitioners that I've talked to just try a couple of sampling ratios and choose the one closest to their cost function.  &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-11-02T20:49:52.683" Id="122416" LastActivityDate="2014-11-02T20:49:52.683" OwnerUserId="34658" ParentId="122409" PostTypeId="2" Score="7" />
  
  <row Body="&lt;p&gt;I think that examples involving marbles or coins or height-measuring can be fine for practicing the math, but they aren't good for building intuition. College students like to question society, right? How about using a political example?&lt;/p&gt;&#10;&#10;&lt;p&gt;Say a political candidate ran a campaign promising that some policy will help the economy. She was elected, she got the policy enacted, and 2 years later, the economy is booming. She's up for re-election, and claims that her policy is the reason for everyone's prosperity. Should you re-elect her?&lt;/p&gt;&#10;&#10;&lt;p&gt;The thoughtful citizen should say &quot;well, it's true that the economy is doing well, but can we really attribute that to your policy?&quot; To truly answer this, we must consider the question &quot;would the economy have done well in the last 2 years without it?&quot; If the answer is yes (e.g. the economy is booming because of some new unrelated technological development) then we reject the politician's explanation of the data.&lt;/p&gt;&#10;&#10;&lt;p&gt;That is, to examine one hypothesis (policy helped the economy), we must build a &lt;b&gt;model&lt;/b&gt; of the world where that hypothesis is null (the policy was never enacted). We then make a &lt;b&gt;prediction&lt;/b&gt; under that model. We call the probability of observing this data in that alternate world the &lt;b&gt;p-value&lt;/b&gt;. If the p-value is too high, then we aren't convinced by the hypothesis--the policy made no difference. If the p-value is low then we trust the hypothesis--the policy was essential.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-11-03T01:21:12.400" Id="122437" LastActivityDate="2014-11-03T01:21:12.400" OwnerUserId="59667" ParentId="31" PostTypeId="2" Score="-1" />
  
  <row Body="&lt;p&gt;I'm not sure how to sum up the terms efficiently, but we can stop whenever the total number of rolls $n$ and the total number of successes $t$ are such that $\binom{n}{t}$ is even since we can partition the different orderings that we could have achieved $n$ and $t$ into two groups of equal probability each corresponding to a different outputted label.  We need to be careful that we haven't already stopped for these elements, i.e., that no element has a prefix of length $n'$ with $t'$ successes such that $\binom{n'}{t'}$ is even.  I'm not sure how to turn this into an expected number of flips.&lt;/p&gt;&#10;&#10;&lt;p&gt;To illustrate:&lt;/p&gt;&#10;&#10;&lt;p&gt;We can stop at TH or HT since these have equal probability.  Moving down Pascal's triangle, the next even terms are in the fourth row: 4, 6, 4.  Meaning that we can stop after rolls if one heads has come up since we can create a bipartite matching: HHHT with HHTH, and technically HTHH with THHH although we would already have stopped for those.  Similarly, $\binom42$ yields the matching HHTT with TTHH (the rest, we would already have stopped before reaching them).&lt;/p&gt;&#10;&#10;&lt;p&gt;For $\binom52$, all of the sequences have stopped prefixes. It gets a bit more interesting at $\binom83$ where we match FFFFTTFT with FFFFTTTF.&lt;/p&gt;&#10;&#10;&lt;p&gt;For $p=\frac12$ after 8 rolls, the chance of not having stopped is $\frac1{128}$ with an expected number of rolls if we have stopped of $\frac{53}{16}$.  For the solution where we keep rolling pairs until they differ, the chance of not having stopped is $\frac{1}{16}$ with an expected number of rolls if we have stopped of 4.  By recursion, an upper bound on the expected flips for the algorithm presented is $\frac{128}{127} \cdot \frac{53}{16} = \frac{424}{127} &amp;lt; 4$.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I wrote a Python program to print out the stopping points:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;import scipy.misc&#10;from collections import defaultdict&#10;&#10;&#10;bins = defaultdict(list)&#10;&#10;&#10;def go(depth, seq=[], k=0):&#10;    n = len(seq)&#10;    if scipy.misc.comb(n, k, True) % 2 == 0:&#10;        bins[(n,k)].append(&quot;&quot;.join(&quot;T&quot; if x else &quot;F&quot;&#10;                                   for x in seq))&#10;        return&#10;    if n &amp;lt; depth:&#10;        for i in range(2):&#10;            seq.append(i)&#10;            go(depth, seq, k+i)&#10;            seq.pop()&#10;&#10;go(8)&#10;&#10;for key, value in sorted(bins.items()):&#10;    for i, v in enumerate(value):&#10;        print(v, &quot;-&amp;gt;&quot;, &quot;F&quot; if i &amp;lt; len(value) // 2 else &quot;T&quot;)&#10;    print()&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;prints:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;FT -&amp;gt; F&#10;TF -&amp;gt; T&#10;&#10;FFFT -&amp;gt; F&#10;FFTF -&amp;gt; T&#10;&#10;FFTT -&amp;gt; F&#10;TTFF -&amp;gt; T&#10;&#10;TTFT -&amp;gt; F&#10;TTTF -&amp;gt; T&#10;&#10;FFFFFT -&amp;gt; F&#10;FFFFTF -&amp;gt; T&#10;&#10;TTTTFT -&amp;gt; F&#10;TTTTTF -&amp;gt; T&#10;&#10;FFFFFFFT -&amp;gt; F&#10;FFFFFFTF -&amp;gt; T&#10;&#10;FFFFFFTT -&amp;gt; F&#10;FFFFTTFF -&amp;gt; T&#10;&#10;FFFFTTFT -&amp;gt; F&#10;FFFFTTTF -&amp;gt; T&#10;&#10;FFFFTTTT -&amp;gt; F&#10;TTTTFFFF -&amp;gt; T&#10;&#10;TTTTFFFT -&amp;gt; F&#10;TTTTFFTF -&amp;gt; T&#10;&#10;TTTTFFTT -&amp;gt; F&#10;TTTTTTFF -&amp;gt; T&#10;&#10;TTTTTTFT -&amp;gt; F&#10;TTTTTTTF -&amp;gt; T&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="12" CreationDate="2014-11-03T06:24:40.697" Id="122456" LastActivityDate="2014-11-04T08:32:08.173" LastEditDate="2014-11-04T08:32:08.173" LastEditorUserId="858" OwnerUserId="858" ParentId="50295" PostTypeId="2" Score="1" />
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;in a two factorial ANOVA, the MS of Factor A (or B) as well as the interaction is divided by the MS of the residuals: e.g. F = MS_FactorA/MS_Residuals If I have two within factors, only the interaction is divided by the MS of the Residuals. However, the main effects are divided by the MS of the Interaction between the main effect of the person and the factor:F = MS_FactorA/MS_PxFactorA. Why is it that way?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you for your answers in advance!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-03T15:38:07.670" Id="122510" LastActivityDate="2014-11-03T15:38:07.670" OwnerUserId="43902" PostTypeId="1" Score="0" Tags="&lt;anova&gt;&lt;repeated-measures&gt;&lt;f-test&gt;" Title="F-Test in ANOVA with two within-Factors: why are there different denominators in the F-test?" ViewCount="18" />
  <row AnswerCount="1" Body="&lt;p&gt;I have done a survey trying to understand the waste disposal habits of people.  &lt;/p&gt;&#10;&#10;&lt;p&gt;One question is &quot;What proportion of waste paper do you recycle&quot;. Another question is &quot;Given a new local policy is enacted, what proportion of waste paper do you recycle.&quot;  &lt;/p&gt;&#10;&#10;&lt;p&gt;I have provided the following options to each of the questions: 0 to &amp;lt;10%, 10 to &amp;lt;20%, 20 to &amp;lt;30%, ...  and I just realized that I may have made a bad decision offering &quot;data bins&quot; for people to choose from.&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to find out whether the policy is effective in encouraging people to recycle paper.  &lt;/p&gt;&#10;&#10;&lt;p&gt;What statistical testing tool can I use? I am thinking about the chi-square distribution, but I think this only shows whether there is a difference, not increase or decrease.  &lt;/p&gt;&#10;&#10;&lt;p&gt;If there isn't any test I can use, I plan to do a chi-square test showing whether there is a difference, plus some other visual aid to show an increase or decrease. How does that sound?  &lt;/p&gt;&#10;&#10;&lt;p&gt;I know this is an awkward question, but since I am a newbie in the world of statistics, I haven't thought carefully about it beforehand. Any answers appreciated.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-11-03T15:47:11.740" Id="122512" LastActivityDate="2014-11-03T17:13:08.217" LastEditDate="2014-11-03T15:53:23.040" LastEditorUserId="59877" OwnerUserId="59877" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;&lt;chi-squared&gt;" Title="What are the alternatives of a chi square test in testing for changes in categorial data?" ViewCount="23" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm going to do linear regression on a data set with 60k observations, each with 120 features. The way I see it, there is no why in the world that with more then 50 samples per dimension, a linear low variance model will overfit. That being said, the fact is their are regularizations methods for linear models.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, on a healthy-sized dataset (and this is a question by itself.. I see it as the number of samples is greater then the square of the dimensionality), do I need to regularize a linear model?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-11-03T17:58:06.903" Id="122525" LastActivityDate="2014-11-03T18:18:52.847" LastEditDate="2014-11-03T18:18:52.847" LastEditorUserId="22311" OwnerUserId="34962" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;modeling&gt;&lt;linear-model&gt;&lt;regularization&gt;" Title="Is regularization of a linear model really needed?" ViewCount="70" />
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;When comparing the association between two sets of time series data does introducing a time lag in one of them constitute a &quot;manipulation&quot; such that the time lagged set automatically becomes the 'independent variable&quot; and would the fact that the fact that the data has been manipulated in that way therefore preclude the use of the Pearson's Correlation?      &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-03T19:20:04.597" Id="122536" LastActivityDate="2014-11-03T19:20:04.597" OwnerUserId="59897" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;correlation&gt;" Title="manipulation of dependent variable" ViewCount="5" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Assume that &lt;/p&gt;&#10;&#10;&lt;p&gt;E[$y_{it}|c_i, x_{it}$] = $c_i$ + $x'_{it}\beta$ &lt;/p&gt;&#10;&#10;&lt;p&gt;Eliminate $c_i$ by taking the expectation with respect to $c_i$, leading to &lt;/p&gt;&#10;&#10;&lt;p&gt;E[$y_{it}|x_{it}$] = E[$c_i|x_{it}$] + $x'_{it}\beta$&lt;/p&gt;&#10;&#10;&lt;p&gt;Can anybody explain this step?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-11-03T22:40:14.310" Id="122563" LastActivityDate="2014-11-03T22:44:02.350" OwnerUserId="59911" PostTypeId="1" Score="1" Tags="&lt;regression&gt;" Title="conditional expectation; regression" ViewCount="36" />
  
  
  
  
  <row Body="&lt;p&gt;Why would you avoid specifying the seasonal frequency of your data if you know it? It will be easier and likely more reliable then making some automated procedure estimate it for you. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you are not sure whether there is a seasonal pattern at a given frequency, you can try estimating a model with the seasonal component and another model without it. Then you would compare the properties of the two models and see which one fares better. (E.g. compare the AIC or BIC of the two models.)&lt;/p&gt;&#10;&#10;&lt;p&gt;I do not know of algorithms that would estimate the seasonal frequency for you, but I guess there must exist some. These kind of algorithms could have been use in automatized procedures; one example where automatized procedures could have been used is forecasting competitions with a large number of series to be forecasted  (but that is just my guess). Perhaps knowing that could help you find one.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-11-04T10:16:03.210" Id="122614" LastActivityDate="2014-11-04T10:16:03.210" OwnerUserId="53690" ParentId="122538" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;Make the two distributions very different.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Create data set 1:  OUT = 1, var1 ~ N(-10, 5), N = 100&#10;Create data set 2:  OUT = 0, var1 ~N(10,5), N = 100&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Merge the two and run your logistic regression.&lt;/p&gt;&#10;&#10;&lt;p&gt;(Since you didn't specify a language, I just wrote the above rather than R code)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-04T10:50:49.720" Id="122618" LastActivityDate="2014-11-04T10:51:14.973" LastEditDate="2014-11-04T10:51:14.973" LastEditorUserId="1352" OwnerUserId="686" ParentId="122612" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;For a k-steps stochastic process, assuming that it takes tk mins from the first step to reach the kth step. If we statistic the mean of cost time (tij) from ith step to jth step and the cost time standard deviation, which kind of process has the property that the mean of cost time between any two steps (i,j) is proportionl(linear) to the standard deviation?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-11-04T11:03:39.763" Id="122622" LastActivityDate="2014-11-04T11:03:39.763" OwnerUserId="45526" PostTypeId="1" Score="0" Tags="&lt;standard-deviation&gt;&lt;mean&gt;" Title="What kind of process is this?" ViewCount="25" />
  <row Body="&lt;p&gt;Following may be helpful (taking 2 patients, 2 methods and 3 time measurements to get a curve): &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; ddf&#10;  pt method c1 c2 c3&#10;1  1     m1  1  2  4&#10;2  2     m1  2  4  6&#10;3  1     m2  1  4  3&#10;4  2     m2  2  4  3&#10;&amp;gt; &#10;&amp;gt; &#10;&amp;gt; dput(ddf)&#10;structure(list(pt = c(1L, 2L, 1L, 2L), method = structure(c(1L, &#10;1L, 2L, 2L), .Label = c(&quot;m1&quot;, &quot;m2&quot;), class = &quot;factor&quot;), c1 = c(1L, &#10;2L, 1L, 2L), c2 = c(2L, 4L, 4L, 4L), c3 = c(4L, 6L, 3L, 3L)), .Names = c(&quot;pt&quot;, &#10;&quot;method&quot;, &quot;c1&quot;, &quot;c2&quot;, &quot;c3&quot;), class = &quot;data.frame&quot;, row.names = c(NA, &#10;-4L))&#10;&#10;&amp;gt; mm = melt(ddf, id=c('pt','method'))&#10;&amp;gt; mm&#10;   pt method variable value&#10;1   1     m1       c1     1&#10;2   2     m1       c1     2&#10;3   1     m2       c1     1&#10;4   2     m2       c1     2&#10;5   1     m1       c2     2&#10;6   2     m1       c2     4&#10;7   1     m2       c2     4&#10;8   2     m2       c2     4&#10;9   1     m1       c3     4&#10;10  2     m1       c3     6&#10;11  1     m2       c3     3&#10;12  2     m2       c3     3&#10;&amp;gt; &#10;&amp;gt; aov.out = aov(value ~ method * variable + Error(pt/variable), data=mm)&#10;&amp;gt; summary(aov.out)&#10;&#10;Error: pt&#10;          Df Sum Sq Mean Sq F value Pr(&amp;gt;F)&#10;Residuals  1      3       3               &#10;&#10;Error: pt:variable&#10;         Df Sum Sq Mean Sq&#10;variable  2   12.6     6.3&#10;&#10;Error: Within&#10;                Df Sum Sq Mean Sq F value Pr(&amp;gt;F)&#10;method           1  0.333  0.3333    0.50  0.530&#10;variable         2  1.400  0.7000    1.05  0.451&#10;method:variable  2  4.667  2.3333    3.50  0.164&#10;Residuals        3  2.000  0.6667               &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;For graphical display:&#10;For different methods in different graphs:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ggplot(mm, aes(x=variable, y=value, color=factor(pt), group=factor(pt)))+geom_point()+geom_line()+facet_grid(.~method)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/6xr3C.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;For different patients in different graphs:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ggplot(mm, aes(x=variable, y=value, color=factor(method), group=factor(method)))+geom_point()+geom_line()+facet_grid(.~pt)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/c1Hx0.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Or combine different patients to have boxplots for different methods to compare: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ggplot(mm, aes(x=variable, y=value))+geom_boxplot()+facet_grid(~method)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/1SrWz.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-04T11:50:10.917" Id="122625" LastActivityDate="2014-11-04T11:58:19.087" LastEditDate="2014-11-04T11:58:19.087" LastEditorUserId="56211" OwnerUserId="56211" ParentId="69927" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;To help you get started with IRT, consider getting &lt;a href=&quot;http://echo.edres.org:8080/irt/baker/&quot; rel=&quot;nofollow&quot;&gt;Baker's book&lt;/a&gt;. You can also get a free download of his &lt;a href=&quot;http://echo.edres.org:8080/irt/baker/software.htm&quot; rel=&quot;nofollow&quot;&gt;software&lt;/a&gt;. It is a self-directed learning tool and you might find it quite useful to start your understanding of this field.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-04T15:21:20.870" Id="122639" LastActivityDate="2014-11-04T15:21:20.870" OwnerUserId="24341" ParentId="122147" PostTypeId="2" Score="1" />
  <row AnswerCount="2" Body="&lt;p&gt;I am trying to prove the result that &lt;em&gt;exactly&lt;/em&gt; k occurrences of a Poisson process before the first occurrence of another independent Poisson process is a geometric random variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;$$P(k\text{ events of type }\lambda_1 \text{before first event of type }\lambda_2)P(\text{the next event is of type }\lambda_2)$$ &lt;/p&gt;&#10;&#10;&lt;p&gt;$$=\left( \int_0^\infty e^{-\lambda_1t}\frac{(\lambda_1t)^k}{k!}e^{-\lambda_2t}dt\right) (\frac{\lambda_2}{\lambda_1+\lambda_2})$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$= \frac{\lambda_1^{k}\lambda_2}{k!(\lambda_1+\lambda_2)} \int_0^\infty t^ke^{-(\lambda_1+\lambda_2)t}dt$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$=\frac{\lambda_1^{k}\lambda_2}{k!(\lambda_1+\lambda_2)} . \frac{\Gamma(k+1)}{(\lambda_1+\lambda_2)^{k+1}}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$= \frac{\lambda_1^{k}\lambda_2}{(\lambda_1+\lambda_2)^{k+2}}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I cannot figure out why I am having an extra $(\lambda_1+\lambda_2)$ term in the denominator. Can someone please point out where I am going wrong?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-04T16:26:24.103" Id="122648" LastActivityDate="2014-11-05T13:55:52.283" LastEditDate="2014-11-04T17:14:38.287" LastEditorUserId="59960" OwnerUserId="59960" PostTypeId="1" Score="0" Tags="&lt;probability&gt;&lt;poisson&gt;" Title="Occurrences of two independent Poisson processes" ViewCount="68" />
  <row AcceptedAnswerId="122891" AnswerCount="1" Body="&lt;p&gt;I'm a PhD student in Information Retrieval with some limited experience in ML. We've been working on a binary classification task with weka (I'm using weka programmatically via Java), specifically with Random Forest. &lt;/p&gt;&#10;&#10;&lt;p&gt;Our results are coming out a little weird because we have an unbalanced dataset (85/15ish). We're getting very high % correct, but precision and recall are very low for our target class (the 15% one).&lt;/p&gt;&#10;&#10;&lt;p&gt;My new understanding is that % correct is really not the right metric to be looking at. The professor I work with said (and I quote): &quot;You are measuring accuracy with &quot;percent correct&quot;. This is so rarely done in machine learning papers these days that I just blew right past it.&quot; He also referenced a paper explaining why I shouldn't use % correct as Accuracy [1].&lt;/p&gt;&#10;&#10;&lt;p&gt;In our case, we are interested in precision and recall to some extent, but the professor I'm working with (he's an ML expert) explained that we can and should use AUC-ROC to compare the runs because that's not sensitive to data balance. After he explained this in depth, I got it and understood. And, I was able to get the AUC data out of the Weka results, which are decent though not spectacular (in the 0.75 neighborhood).&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm used to IR systems in which you can tune for various metrics, e.g. Precision, F values, MAP, etc. However, as far as I can tell, Weka &lt;em&gt;always&lt;/em&gt; trains its classifier models to optimize for % correct. So even though I am interested in another metric, e.g., Precision or F1, I can't for the life of me figure out how to encourage Weka to train its model to focus on optimizing for anything other than % correct (say, F1). &lt;/p&gt;&#10;&#10;&lt;p&gt;I've combed the weka docs and Googled the heck out of it (incl. site search here on CrossValidated) but couldn't find anything to.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is that possible? I would really appreciate any insight into whether that's even a possibility at all, is it just not implemented in Weka, or if there's some reason why it shouldn't be done. Or, if there's something I'm missing because I'm calling weka from Java rather than using the GUI.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;&#10;&lt;p&gt;--&lt;/p&gt;&#10;&#10;&lt;p&gt;[1] Provost, F. J., Fawcett, T., &amp;amp; Kohavi, R. (1998, July). The case against accuracy estimation for comparing induction algorithms. In ICML (Vol. 98, pp. 445-453).&#10;&lt;a href=&quot;http://eecs.wsu.edu/~holder/courses/cse6363/spr04/pubs/Provost98.pdf&quot; rel=&quot;nofollow&quot;&gt;http://eecs.wsu.edu/~holder/courses/cse6363/spr04/pubs/Provost98.pdf&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-04T17:24:28.327" Id="122656" LastActivityDate="2014-11-06T09:23:30.650" OwnerUserId="59967" PostTypeId="1" Score="1" Tags="&lt;classification&gt;&lt;optimization&gt;&lt;binary-data&gt;&lt;weka&gt;&lt;metric&gt;" Title="Optimizing for target metrics in Weka" ViewCount="69" />
  <row AnswerCount="1" Body="&lt;p&gt;I have two and half years of the weekly time series data.  The seasonally period is 52 weeks.  I differed the data with log transformation and feed the data into the MATLAB arima model.  &lt;/p&gt;&#10;&#10;&lt;p&gt;ARIMA('Constant', 0 ,'D', 0 ,'Seasonality', 52, 'MALags',1 ,'SMALags',52);&lt;/p&gt;&#10;&#10;&lt;p&gt;The first plot is loged origin data and second plot is residual data.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/8r2ak.png&quot; alt=&quot;Log Transformed auto and partial correction plots&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/bWwOx.png&quot; alt=&quot;residual auto and partial correlation plots &quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;My questions:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;How to interpret residual data partial and auto correlations plots especillay at lag 52?  &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The partial corr plot shows getting larger, does it indicate system going to be unstable? &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Does the model adequate? &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;What can be done? &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Any insight will be very helpful.&#10;Thanks in advance.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-04T18:02:50.117" Id="122660" LastActivityDate="2014-11-04T19:35:23.213" OwnerUserId="59968" PostTypeId="1" Score="0" Tags="&lt;autocorrelation&gt;" Title="Diagnose ARIMA seasonality model residual auto and partial correlation plots" ViewCount="47" />
  
  <row Body="&lt;p&gt;It looks like you can find it in:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.math.utep.edu/Faculty/moschopoulos/Publications/1985-On_the_Distribution_of_the_Trace_of_a_Non-central_Wishart_Matrix.pdf&quot; rel=&quot;nofollow&quot;&gt;S. Kourouklis and P.G. Moschopoulos (1985) On the distribution of the trace of a non-central Wishart.  &lt;em&gt;Metron&lt;/em&gt; &lt;strong&gt;XLIII&lt;/strong&gt;(1--2): 85--92.&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;It looks like they cover the case of general covariance matrix $\Sigma$ there.&lt;/p&gt;&#10;&#10;&lt;p&gt;They also give pointers to in the paper to:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.stat.purdue.edu/docs/research/tech-reports/1981/tr81-46.pdf&quot; rel=&quot;nofollow&quot;&gt;Mathai, A.M. and Pillai, K.C.S. (1982)  Further results on the trace of a non-central Wishart matrix, &lt;em&gt;Comm. Statist.-Theor. Meth.&lt;/em&gt;, A 11, 1077-1086.&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;A.M. Mathai (1980) Moments of the trace of a noncentral Wishart matrix.  &lt;em&gt;Comm. Statist.  - Theor. Meth.&lt;/em&gt;, A9(8), 795--801.&lt;/p&gt;&#10;&#10;&lt;p&gt;That latter may be useful for computational purposes.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-11-04T18:35:41.267" Id="122664" LastActivityDate="2014-11-04T18:35:41.267" OwnerUserId="11945" ParentId="118212" PostTypeId="2" Score="2" />
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to extract Gabor features from an input image. So, I have setup a series of Gabor filters with different parameters (frequency, angle and standard deviation) and I am convolving each of these filters with the input image and looking at the mean and variance of the output magnitude image. So, in python it would look something like:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;import numpy as np&#10;from scipy import ndimage as nd&#10;&#10;# Here kernel is a given Gabor filter&#10;def filter_image(self, image):&#10;    filtered = np.zeros((len(self.kernels)*2,) + image.shape)        &#10;    for k, kernel in enumerate(self.kernels):            &#10;        filtered[k*2, :] = nd.convolve(image, np.real(kernel), mode='wrap')&#10;        filtered[k*2+1, :] = nd.convolve(image, np.imag(kernel), mode='wrap')&#10;&#10;return filtered&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And now, I am looking at the mean and variance of the power image as:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;def compute_features(self, image):&#10;    features = np.zeros((len(self.kernels), 2))&#10;    filtered = filter_image(image)&#10;    for k in range(len(self.kernels)):&#10;        power_image = np.sqrt(filtered[k*2]**2 + filtered[k*2+1]**2)&#10;        features[k, 0] = filtered[k, :].mean()&#10;        features[k, 1] = filtered[k, :].var()&#10;&#10;return features&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So, when I look at the mean and variance of the each of the filter responses, I notice that the variance is really high with respect to the mean. For example, I get values like (mean = 0.83, variance = 900). I wonder if this is something one sees often. Does this tell me that I do not really have any texture in the image? I am not sure how to interpret this.&lt;/p&gt;&#10;&#10;&lt;p&gt;I do apologize if this does not exactly belong on this forum.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-11-04T18:46:26.403" Id="122666" LastActivityDate="2014-11-04T23:44:57.233" LastEditDate="2014-11-04T23:44:57.233" LastEditorUserId="36540" OwnerUserId="36540" PostTypeId="1" Score="0" Tags="&lt;image-processing&gt;&lt;filter&gt;" Title="Gabor filters: Large variance compared to the mean" ViewCount="22" />
  <row AnswerCount="0" Body="&lt;p&gt;In Cowles's book ([Applied Bayesian Statistics - With R and OpenBUGS Examples–(&lt;a href=&quot;http://www.springer.com/statistics/statistical+theory+and+methods/book/978-1-4614-5695-7&quot; rel=&quot;nofollow&quot;&gt;http://www.springer.com/statistics/statistical+theory+and+methods/book/978-1-4614-5695-7&lt;/a&gt;)), page 108, there is a parameter $k$, which represents the equivalent prior sample size with respect to the mean. &lt;/p&gt;&#10;&#10;&lt;p&gt;Can someone tell me how to calculate it?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-11-04T18:54:47.397" Id="122669" LastActivityDate="2014-11-04T20:43:50.610" LastEditDate="2014-11-04T20:43:50.610" LastEditorUserId="930" OwnerUserId="53201" PostTypeId="1" Score="0" Tags="&lt;bayesian&gt;&lt;inference&gt;&lt;bayes&gt;&lt;conjugate-prior&gt;" Title="Conjugate prior equivalent prior sample size with respect to the mean" ViewCount="35" />
  <row AcceptedAnswerId="122672" AnswerCount="1" Body="&lt;p&gt;In practice, is there generally a difference between having 100 factors and 1000 factors in a model?  Is there a well-researched 'upper-bound' to how many factors a given model should have?&lt;/p&gt;&#10;" ClosedDate="2014-11-04T20:37:03.757" CommentCount="1" CreationDate="2014-11-04T19:07:22.437" Id="122671" LastActivityDate="2014-11-04T19:14:11.197" OwnerUserId="40120" PostTypeId="1" Score="-1" Tags="&lt;model-selection&gt;&lt;factor-analysis&gt;" Title="Deciding the Optimal Number of Factors" ViewCount="41" />
  <row AcceptedAnswerId="122683" AnswerCount="1" Body="&lt;p&gt;I fitted a pdf with CumFreq to data, and the best fitting pdf is a discontinuous function composed as:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    The cumulative frequency function is broken into :&#10;     Gumbel standard (&amp;lt;P) and Gumbel mirrored (&amp;gt;P) : Breakpoint : P =    166&#10;     X &amp;lt; P :  Freq = exp[-exp{-(As*X+Bs)}]&#10;              As =   0.0118       Bs = -1.4769&#10;     X &amp;gt; P :  Freq = 1-exp[-exp{-(As*X+Bs)}]&#10;              Ag =  -0.0046       Bg =    1.22&#10;     Average X:    214     Median X:    144     St.Dev. X:  142.2&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/8OWTI.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;How can I sample thse two functions to produce random numbers? how can I make sure that the area under the curve equals 1?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-11-04T19:16:10.827" Id="122673" LastActivityDate="2014-11-04T20:24:16.273" OwnerUserId="55158" PostTypeId="1" Score="0" Tags="&lt;probability&gt;&lt;pdf&gt;&lt;discontinuity&gt;" Title="How to sample discontinuous probability functions?" ViewCount="55" />
  
  <row Body="&lt;p&gt;No, that's not a problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you have an intercept term in your model, one of the state dummies will be dropped, and the others then give the state means relative to the omitted state. In this case, statistically significant state dummies just mean that those states have means that are statistically significantly different from the omitted state. I can't see how this would be a problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you instead have dropped your intercept term, and therefore can include dummies for all states, the state dummies simply capture the mean for each state. In this case, statistically significant dummies just mean that those states have means that are statistically significantly different from zero. I can't see how this would be a problem, either.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-11-04T21:07:43.427" Id="122689" LastActivityDate="2014-11-04T21:07:43.427" OwnerUserId="37448" ParentId="122670" PostTypeId="2" Score="1" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm trying to approximate the MSE of an estimator through simulation, in particular estimators of the form&#10;$$&#10;\hat{\theta} = \sum_{i=1}^N w_i X_i&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Where $X = \{X_1,...,X_N\}$ are i.i.d. samples from some distribution and $\sum_{i} w_i = 1$ but the $w_i$ are not independent of $X$, and N is large. &lt;/p&gt;&#10;&#10;&lt;p&gt;I also happen to know the true $\theta = \mathbb{E}[X_i]$&lt;/p&gt;&#10;&#10;&lt;p&gt;Currently, I am running a large number of simulations $M$ and computing MSE as&#10;$$&#10;\frac{1}{M} \sum_{j=1}^M (\hat{\theta}_j - \theta)^2&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;But this is rather expensive, I'm wondering if there is a better way. &lt;/p&gt;&#10;&#10;&lt;p&gt;edit: In case some additional context may be useful. &lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose that  each $X_j$ is actually distributed from one of $K &amp;lt;&amp;lt; N$ distributions and it is known which one, and further, that the true variance of the distribution, $\sigma_j^2$, is known (for evaluation purposes only). &lt;/p&gt;&#10;&#10;&lt;p&gt;Let $s_j^2$ denote the &lt;em&gt;sample variance&lt;/em&gt; for the distribution of $X_j$, computed using &lt;em&gt;all samples sharing that distribution&lt;/em&gt;. Then, $w_i = (s_i^2 + c_i) / \sum_j (s_j^2+c_j)$.  In other words, the dependence between $X_j$ and $w_j$ is certainly not negligible.  &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-11-04T23:24:07.743" Id="122702" LastActivityDate="2014-11-07T19:22:16.470" LastEditDate="2014-11-07T19:22:16.470" LastEditorUserId="5321" OwnerUserId="5321" PostTypeId="1" Score="0" Tags="&lt;estimation&gt;&lt;monte-carlo&gt;&lt;mse&gt;" Title="On approximating the MSE of an estimator" ViewCount="40" />
  
  
  
  
  <row Body="&lt;p&gt;You asked about relevant litterature.&lt;/p&gt;&#10;&#10;&lt;p&gt;If your model is a power law, log-normal, poisson or binomial you should check the work of Clauset et al. &quot;PowerLaw Distribution in Empirical Data&quot; &lt;a href=&quot;http://arxiv.org/pdf/0706.1062.pdf&quot; rel=&quot;nofollow&quot;&gt;http://arxiv.org/pdf/0706.1062.pdf&lt;/a&gt; They describe how to do so by bootstraping, performing fits on the ecdf of the actual data and generated data. You must be cautious when using such approach and follow the procedure described in details in this paper.&lt;/p&gt;&#10;&#10;&lt;p&gt;An important thing to remember is that you never prove that your data follow a model, you can only disprove it follow a model. p-values give you the closeness of the data and the model, but you can only carefully make conclusions by performing the same test for other likely models that could explain the data.&lt;/p&gt;&#10;&#10;&lt;p&gt;The authors of the paper also provide a library to perfom the tests for these models (poweRlaw package in R), and partial implementation in other languages.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-05T10:18:17.123" Id="122758" LastActivityDate="2014-11-05T14:24:48.090" LastEditDate="2014-11-05T14:24:48.090" LastEditorUserId="49548" OwnerUserId="49548" ParentId="122708" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;On a dataset, I performed spatial panel regressions with fixed effects, and with both a spatial lag and a spatial error (both are significant), using package splm in R (Millo and Piras 2012 Journal of Statistical Software). &#10;I would like to assess the Goodness of Fit of these models, and this seems to be a tricky issue. &lt;/p&gt;&#10;&#10;&lt;p&gt;The most detailed treatment of the question that I found so far is in Elhorst (2010):&#10;&lt;a href=&quot;http://regroningen.nl/elhorst/doc/Spatial%20Panel%20Data%20Models.pdf&quot; rel=&quot;nofollow&quot;&gt;http://regroningen.nl/elhorst/doc/Spatial%20Panel%20Data%20Models.pdf&lt;/a&gt; &#10;But he discuss spatial lag and spatial error models separately. Therefore, I am not sure if any of the formulas he propose (Table 1 p.24) is appropriate for a model having both spatil lag and error components. And anyway I am not entirely sure of how to practically calculate these formulas with my model output. &lt;/p&gt;&#10;&#10;&lt;p&gt;So, I'm not sure to understand how I can calculate any measure of goodness of fit for these models. &lt;/p&gt;&#10;&#10;&lt;p&gt;Calculating directly the squared correlation between the observed values and the fitted values, calculated as the (observed - residuals) gives very high corr2, above 0.9, while calculating the squared correlation between the observed and predicted values, calculated simply using the explanatory variables X multiplied by the beta, gives very low corr2 (like 0.04), as one could expect given that the spatial lag and errors are very significant.&lt;/p&gt;&#10;&#10;&lt;p&gt;Does anyone have an idea, advice, on how to do that correctly?&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that I use R package splm rather than Elhorst MATLAB package, because I don't know MATLAB..., but also because splm is the only one which calculates models having both spatial lag and error components together.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you very much &lt;/p&gt;&#10;&#10;&lt;p&gt;Patrick Meyfroidt&#10;University of Louvain, Belgium&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-05T11:52:05.170" Id="122765" LastActivityDate="2014-11-05T11:52:05.170" OwnerUserId="60039" PostTypeId="1" Score="3" Tags="&lt;panel-data&gt;&lt;goodness-of-fit&gt;&lt;spatial&gt;&lt;fixed-effects-model&gt;&lt;lags&gt;" Title="Goodness of fit for a spatial panel with fixed effects and both spatial lag and spatial error" ViewCount="40" />
  <row AnswerCount="0" Body="&lt;p&gt;Up until now, I have always used the &lt;code&gt;rnorm&lt;/code&gt; command in R, to simulate from a standard normal distribution. I've always wondered though, how does this command work?&lt;/p&gt;&#10;&#10;&lt;p&gt;What algorithm can be used to simulate from a standard normal distribution.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-11-05T13:09:58.193" Id="122774" LastActivityDate="2014-11-05T13:09:58.193" OwnerUserId="30494" PostTypeId="1" Score="1" Tags="&lt;simulation&gt;" Title="How to simulate from a standard normal random variable" ViewCount="30" />
  
  
  
  <row Body="&lt;p&gt;I'd be inclined to treat it as a Bayesian problem. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you do that, you end up with the ratio of posterior probabilities of the two multinomials being the likelihood ratio times the ratio of prior probabilities&lt;/p&gt;&#10;&#10;&lt;p&gt;$P(M_1|x_1,x_2,...)/P(M_2|x_1,x_2,...) =  P(x_1,x_2,...|M_1)/P(x_1,x_2,...|M_2)\times P(M_1)/P(M_2)$&lt;/p&gt;&#10;&#10;&lt;p&gt;Assuming your prior probabilities are both 0.5, you just get the likelihood ratio. &lt;/p&gt;&#10;&#10;&lt;p&gt;$P(x_1,x_2,...|M_1)/P(x_1,x_2,...|M_2) = {\Lambda}_{12}$&lt;/p&gt;&#10;&#10;&lt;p&gt;From that and the fact that the posterior probabilities sum to 1, you can back out the two posterior probabilities:&lt;/p&gt;&#10;&#10;&lt;p&gt;$P(M_2|x_1,x_2,...) =1/[1+\Lambda_{12}]$&lt;/p&gt;&#10;&#10;&lt;p&gt;$P(M_1|x_1,x_2,...) =1-1/[1+\Lambda_{12}]$&lt;/p&gt;&#10;&#10;&lt;p&gt;(And if the prior probabilities aren't equal, it's only a simple additional term in the calculation).&lt;/p&gt;&#10;&#10;&lt;p&gt;From here you can figure out what the chances are for concluding correctly at some sample size. If you set some cutoff for what constitutes 'high probability', you can find a suitable $n$.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-11-05T14:48:19.210" Id="122787" LastActivityDate="2014-11-05T21:31:31.717" LastEditDate="2014-11-05T21:31:31.717" LastEditorUserId="805" OwnerUserId="805" ParentId="122783" PostTypeId="2" Score="3" />
  
  
  <row Body="&lt;p&gt;The Jarque-Bera test checks for kurtosis and skew that match the normal distribution. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Jarque%E2%80%93Bera_test&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Jarque%E2%80%93Bera_test&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-11-05T17:22:44.290" Id="122812" LastActivityDate="2014-11-05T17:22:44.290" OwnerUserId="45129" ParentId="122784" PostTypeId="2" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;Assumptions:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;The teams never change&lt;/li&gt;&#10;&lt;li&gt;The teams don't improve in skill&lt;/li&gt;&#10;&lt;li&gt;The entire history of each team's performance against some subset of other teams is known&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;For example:&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a long list of match outcomes that look like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Team A beats Team B&#10;Team B beats Team A&#10;Team A beats Team B&#10;Team C beats Team A&#10;Team A beats Team C&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Problem:&lt;/p&gt;&#10;&#10;&lt;p&gt;Predict the correct betting odds of any team beating any other team.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the example above, maybe we conclude that A should beat B 66% of the time. That is based off direct observation and is pretty straightforward. However, finding the probability that C beats B seems harder. They've never played together, yet it seems like most likely that C &gt; B, with some low confidence.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-11-05T19:18:09.427" Id="122821" LastActivityDate="2014-11-05T19:18:09.427" OwnerUserId="21266" PostTypeId="1" Score="0" Tags="&lt;statistical-significance&gt;&lt;prediction&gt;&lt;odds-ratio&gt;&lt;odds&gt;" Title="Odds of a team winning a sports match given full history" ViewCount="23" />
  
  
  
  
  
  
  <row Body="&lt;p&gt;Assuming $N$ is reasonably large (maybe 35 or more per group), then you can use a $t$-test, which assumes the variance is both groups is equally large.  &lt;/p&gt;&#10;&#10;&lt;p&gt;This makes sense because you randomized, so we would not expect the variance of one the observations to be systematically different across groups. &lt;/p&gt;&#10;&#10;&lt;p&gt;To run this in R, see:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.stat.columbia.edu/~martin/W2024/R2.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.stat.columbia.edu/~martin/W2024/R2.pdf&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-11-05T23:47:35.943" Id="122854" LastActivityDate="2014-11-05T23:47:35.943" OwnerUserId="45129" ParentId="122852" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Sorry to say, but this is a totally confused mix up. &lt;/p&gt;&#10;&#10;&lt;p&gt;First, it appears that $f(\epsilon)$ is a probability density function. If this is the case it must intagrate to unity, over the specified domain of the variable. So we must have&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\int_0^1 \alpha \epsilon d\epsilon = 1 \Rightarrow \frac{\alpha}{2}\epsilon^2|_0^1 = 1 \Rightarrow \alpha =2$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Second, we do &lt;em&gt;not&lt;/em&gt; insert the joint density of the four independent errors (they must be independent, not just non-autocorrelated) into the functional specification of the regression. What we do to arrive at the maximum likelihood estimator is to note that from the regression specification we obtain by solving for $\epsilon$,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$y_{i} - \beta _{1} - \beta_{2}x_{i} = \epsilon _{i}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and substitute this into the joint probability density&lt;/p&gt;&#10;&#10;&lt;p&gt;$$L = \prod_{i=1}^4I_{\{\epsilon_i \in [0,1]\}}\cdot f(\epsilon_i) =  \prod_{i=1}^4\alpha I_{\{y_{i} - \beta _{1} - \beta_{2}x_{i} \in [0,1]\}} \cdot \left(y_{i} - \beta _{1} - \beta_{2}x_{i}\right)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Note the inclusion of the indicator function that expresses the restriction on the support of $\epsilon_i$.  &lt;/p&gt;&#10;&#10;&lt;p&gt;The log-likelihood is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\ln L = 4\ln2 + \sum_{i=1}^4\ln\left(y_{i} - \beta _{1} - \beta_{2}x_{i}\right) + \sum_{i=1}^4\ln I_{\{y_{i} - \beta _{1} - \beta_{2}x_{i} \in [0,1]\}}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The last term is translated into $2n$ inequality restrictions when we move to solve the above maximization problem with respect to the parameters, of the form&lt;/p&gt;&#10;&#10;&lt;p&gt;$$y_{i} - \beta _{1} - \beta_{2}x_{i} \geq 0,\;\;\; y_{i} - \beta _{1} - \beta_{2}x_{i} \leq 1$$&#10;for each $i$.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-11-06T00:02:43.280" Id="122856" LastActivityDate="2014-11-06T02:54:56.997" LastEditDate="2014-11-06T02:54:56.997" LastEditorUserId="28746" OwnerUserId="28746" ParentId="122850" PostTypeId="2" Score="3" />
  
  
  <row Body="&lt;p&gt;My cursory search did not find this option either.&#10;As you describe the problem, you want to use:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;An imbalanced dataset (85:15). &lt;/li&gt;&#10;&lt;li&gt;Random Forest. &lt;/li&gt;&#10;&lt;li&gt;ROC and AUC-based loss definitions.&lt;/li&gt;&#10;&lt;li&gt;Weka.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Let's try to relax one condition at a time.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here are some possible alternatives:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Intentionally skew the data: take all the instances from the 15%&#10;label and sample a similar number from the other label. Say you have&#10;850 yellow instances and 150 blue, take all the blue instances and&#10;sample 150 yellow ones. Then train a random forest using Weka. You can use bootstrap resampling if you want to diversify the data.&lt;/li&gt;&#10;&lt;li&gt;Use a cost-sensitive classifier, and mark the cost of false negatives higher. &lt;a href=&quot;http://weka.wikispaces.com/space/content?tag=cost-sensitive&quot; rel=&quot;nofollow&quot;&gt;cost-sensitive classification in Weka&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;Use a different loss function. Like you, I could not find how to do this for the current framework/algorithm combination.&lt;/li&gt;&#10;&lt;li&gt;Use a different algorithm. &lt;a href=&quot;http://weka.sourceforge.net/doc.dev/weka/classifiers/functions/SGD.html&quot; rel=&quot;nofollow&quot;&gt;SGD&lt;/a&gt; in Weka can use different loss functions.&lt;/li&gt;&#10;&lt;li&gt;Use a different ML framework. &lt;a href=&quot;http://scikit-learn.org/stable/index.html&quot; rel=&quot;nofollow&quot;&gt;scikit-learn&lt;/a&gt; seems more flexible, but I am unsure whether its implementation of random forest allows for ROC curve-based loss.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="1" CreationDate="2014-11-06T07:50:27.500" Id="122891" LastActivityDate="2014-11-06T09:23:30.650" LastEditDate="2014-11-06T09:23:30.650" LastEditorUserId="2314" OwnerUserId="2314" ParentId="122656" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;Since nobody have tried to answer so far, I give it a try. As Glen_b's comment tells you, you cannot expect a perfect answer, an algorithm, to find this.  But I have found it useful in data analysis to try different things, like first doing univariate analysis, then using multivariable regression.  Then look at the results. If some variables give strikingly different conclusions in those two analysis, then that is a sign to start thinking about what is happening, try to understand tyhe WHY, with your data, the two analysis gives different conclusions.&lt;/p&gt;&#10;&#10;&lt;p&gt;But such strategies can only be a start!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-06T09:38:48.990" Id="122898" LastActivityDate="2014-11-06T09:38:48.990" OwnerUserId="11887" ParentId="122576" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Perhaps you are looking for analysis which can treat &lt;em&gt;imprecise input data&lt;/em&gt;? I would look for terms like &lt;em&gt;metaanalysis&lt;/em&gt; and:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;statistics working with imprecise input parameters&#10;statistics working with imprecise input data&#10;statistics fuzzy input data&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Here are some resources:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.lix.polytechnique.fr/~adje/uploads/vstte_13.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.lix.polytechnique.fr/~adje/uploads/vstte_13.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.tandfonline.com/doi/abs/10.1080/17415977.2012.675505#.VDepYmd_vcw&quot; rel=&quot;nofollow&quot;&gt;http://www.tandfonline.com/doi/abs/10.1080/17415977.2012.675505#.VDepYmd_vcw&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://dl.acm.org/citation.cfm?id=778922&quot; rel=&quot;nofollow&quot;&gt;http://dl.acm.org/citation.cfm?id=778922&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://sci2s.ugr.es/keel/pdf/keel/articulo/gfs-extendido-IEEE.pdf&quot; rel=&quot;nofollow&quot;&gt;http://sci2s.ugr.es/keel/pdf/keel/articulo/gfs-extendido-IEEE.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://gnedenko-forum.org/Journal/2008/022008/RATA_2_2008-09.pdf&quot; rel=&quot;nofollow&quot;&gt;http://gnedenko-forum.org/Journal/2008/022008/RATA_2_2008-09.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0167947302001160&quot; rel=&quot;nofollow&quot;&gt;http://www.sciencedirect.com/science/article/pii/S0167947302001160&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-06T11:33:00.640" Id="122905" LastActivityDate="2014-11-06T11:33:00.640" OwnerUserId="5509" ParentId="122904" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="122915" AnswerCount="1" Body="&lt;p&gt;I wonder if single multinomial distribution (I will use the notation from JAGS/WinBUGS, but in fact, this is principial thing rather than of particular language)&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x[] ~ dmulti(p[], N)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;would give the same result as a sequence of binomial draws:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x[1] ~ dbin(p[1], N)&#10;x[2] ~ dbin(p[2]/(1 - p[1]), N - x[1])&#10;x[3] ~ dbin(p[3]/(1 - p[1] - p[2]), N - x[1] - x[2])&#10;...&#10;x[K] &amp;lt;- N - x[1] - x[2] - ... - x[K - 1]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Is it totally equivalent? Or did I miss something?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-06T12:08:17.807" Id="122907" LastActivityDate="2014-11-06T15:19:01.850" OwnerUserId="5509" PostTypeId="1" Score="3" Tags="&lt;binomial&gt;&lt;multinomial&gt;" Title="Can multinomial distribution be simulated by a sequence of binomial draws?" ViewCount="99" />
  <row AcceptedAnswerId="122956" AnswerCount="1" Body="&lt;p&gt;I am looking for a suggestions on how to estimate the sample size in such a problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;There is a group with one usefull treatment (let's say 10 milion people) but one consider if there might be more Effective way of treatment. So a control group is needed (with  adequate size - that's the issue) to Have statistically powierful significance results (if any exist). &lt;/p&gt;&#10;&#10;&lt;p&gt;The problem is that I don't Have any idea on how to calculate adequate sample size for control group. I'd like to Have the least possible sample size Because of control treatment's High costs.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank's for any advice.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-11-06T13:15:57.767" FavoriteCount="1" Id="122916" LastActivityDate="2014-11-06T17:52:15.257" OwnerUserId="49793" PostTypeId="1" Score="0" Tags="&lt;sample-size&gt;&lt;sample&gt;" Title="Sample size problem" ViewCount="44" />
  
  <row AcceptedAnswerId="123003" AnswerCount="1" Body="&lt;p&gt;I have a data set with several ordinal variables. In order to perform a MCA&#10;I've learned that the data must be structured as an indicator matrix.&lt;/p&gt;&#10;&#10;&lt;p&gt;E.g. I transform the 3-level variable knowledge into there binary coded variables.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;knowlegde =&amp;gt; low knowledge | medium knowledge | high knowledge&#10;1         =&amp;gt;             1                  0                0&#10;2         =&amp;gt;             0                  1                0&#10;3         =&amp;gt;             0                  0                1&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now, when I try to run the analysis, SPSS tells me that it treats all 0 as missing data and consequently stops the analysis.&lt;/p&gt;&#10;&#10;&lt;p&gt;How can I tell SPSS that the 0s are not missing? I haven't found a way to achieve this.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-11-06T13:52:29.247" Id="122920" LastActivityDate="2014-11-07T00:35:00.433" OwnerUserId="60132" PostTypeId="1" Score="0" Tags="&lt;spss&gt;&lt;correspondence-analysis&gt;" Title="Multiple Correspondence Analysis: SPSS treats 0 as missing value in indicator matrix" ViewCount="55" />
  <row Body="&lt;p&gt;For a more detailed description of these models see Chapter 5 from the book:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;a href=&quot;http://www.amazon.co.uk/Bayesian-Analysis-Edition-Chapman-Statistical/dp/158488388X&quot; rel=&quot;nofollow&quot;&gt;Bayesian Data Analysis&lt;/a&gt;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;One of the problems with fixing the parameter $b=1$ is that the shape of the distribution of $\sigma^2$ is restricted which, in some cases, might be unrealistic or induce bias on the inference on the remaining parameters.&lt;/p&gt;&#10;&#10;&lt;p&gt;These kind of models can be easily implemented using BUGS or JAGS in a Bayesian framework and you do not need to restrict any of the parameter values. Also, the shape of the prior is updated with the information of the data, resulting in a posterior distribution of the variables. The book I mentioned presents some codes for doing so. See also:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;a href=&quot;http://www.stat.columbia.edu/~gelman/research/published/taumain.pdf&quot; rel=&quot;nofollow&quot;&gt;Prior distributions for variance parameters in hierarchical models&lt;/a&gt;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="0" CreationDate="2014-11-06T15:50:15.757" Id="122933" LastActivityDate="2014-11-06T16:06:31.983" LastEditDate="2014-11-06T16:06:31.983" LastEditorUserId="60137" OwnerUserId="60137" ParentId="122932" PostTypeId="2" Score="2" />
  
  
  
  
  
  <row Body="&lt;p&gt;You're missing the definition of conditional probability: $P(x|y) = P(x,y)/P(y)$&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-11-06T18:44:02.683" Id="122965" LastActivityDate="2014-11-06T18:44:02.683" OwnerUserId="8175" ParentId="122960" PostTypeId="2" Score="1" />
  
  
  
  
  
  <row Body="&lt;p&gt;From Barlow and Proschan (1981):&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $T_i$ be the duration of the $i$th functioning period and $D_i$ be the down time for the $i$th repair.  Let the sequence $\{T_i+D_i\}$ be mutually independent; $T_i$ and $D_i$ are not assumed independent.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $F$ be the common distribution of $T_i$, $H$ the common distribution of $T_i+D_i$, and &#10;$M_H(t) = \sum_{i=1}^\infty H^{(i)}(t)$ be the renewal function corresponding to $H$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then by the Key Renewal Theorem&#10;$$A = \lim_{t\rightarrow\infty} A(t) = \frac{E[T]} { E[T] + E[D]}$$&#10;when the distribution $H$ is non-lattice.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, the equation you gave holds in the limit, with no assumptions other than that the expectations exist and the mild assumptions given above.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, if you have a system with $n$ machines following the same assumptions, and assuming that the machines are independent of each other, then the availability in the limit as $t\rightarrow\infty$ of each machine is also $A$.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Let $X_i(t)$ be equal to 1 if the $i$th machine is operating at time $t$ and 0 if it is not.  Then the availability of $n$ machines at a given time $t$ is given by&#10;$$A_1(t)+\cdots + A_n(t) = E[X_1(t)]+\cdots+E[X_n(t)]= E[X_1(t) + \cdots X_n(t)].$$&lt;/p&gt;&#10;&#10;&lt;p&gt;So, as $t\rightarrow\infty$ we must have a limiting availability of $nA$ machines.  This holds as time increases, rather than as the number of machines increases.  &lt;/p&gt;&#10;&#10;&lt;p&gt;It seems intuitively reasonable that as the number of machines increases, that the average availability at a given time will go to something, but I'm not sure that it would have to be the same quantity.&lt;/p&gt;&#10;&#10;&lt;p&gt;The dedication in this book is very funny --- I had forgotten it.&lt;/p&gt;&#10;&#10;&lt;p&gt;R.E. Barlow and F. Proschan (1981) Statistical Theory of Reliability and Life Testing.  TO BEGIN WITH.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-07T07:44:57.017" Id="123039" LastActivityDate="2014-11-07T07:44:57.017" OwnerUserId="11945" ParentId="71666" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;It is unbiased, let's see:  Let the linear model be $Y=X\beta +e$, in matrix form, with $E e=0$ and the variance-covariance matrix of the errors $e$ be $\Omega$.  We use for weights the matrix $W$. Then the weighted linear least squares estimator is&#10;$$&#10;   \hat{\beta} = (X'WX))^{-1}  X'WX Y&#10;$$&#10;and we can calculate its expectation as&#10;$$&#10;    E \hat{\beta} = (X'WX)^{-1} X* W E Y = (X'WX)^{-1} X* W X\beta =\beta&#10;$$&#10;and you can observe that the variance-covariance matrix $\Omega$ do not play any role in the computations! &lt;/p&gt;&#10;&#10;&lt;p&gt;Of course, if the weight matrix $W$ is estimated from the data in some way, then the above analysis is inadequate! This website  &lt;a href=&quot;http://www.itl.nist.gov/div898/handbook/pmd/section1/pmd143.htm&quot; rel=&quot;nofollow&quot;&gt;http://www.itl.nist.gov/div898/handbook/pmd/section1/pmd143.htm&lt;/a&gt;&#10;discusses this problem, and gives references.  Scroll down to &quot;Disadvantages of weighted least squares&quot;.  Their advice is:  &quot; It is important to remain aware of this potential problem, and to only use weighted least squares when the weights can be estimated precisely relative to one another [Carroll and Ruppert (1988), Ryan (1997)]. &quot;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-07T09:15:59.647" Id="123051" LastActivityDate="2014-11-07T09:15:59.647" OwnerUserId="11887" ParentId="123037" PostTypeId="2" Score="2" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a time series of events (weekly aggregate date)&#10;Each event is information on contracts closed by a sales representatives during working weeks.&#10;The goal is to calculate what is the expected number of contracts that should be closed by a sales representative operating in a certain city, basing the analysis on historical data.&lt;/p&gt;&#10;&#10;&lt;p&gt;The issues faced:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;There's no way to know if a sales rep was working, on holiday, ill. It&#10;is hard to close contracts, so the possibility that a working sales&#10;rep does not close on a deal is high. If a salesmen worked but closed no contracts, it will not show up on the dataset.&lt;/li&gt;&#10;&lt;li&gt;Some salesmen have working contracts with very variable number of working hours. It may be possible that they are on 50% workloads. No info of that in the dataset.&lt;/li&gt;&#10;&lt;li&gt;In certain cities more than 100 reps can be active simultaneously, in other small cities office headcount might be only 10 or 20 reps, some very small cities have one or two salesmen working there. So in some cases it does not make sense to just &quot;average&quot;  spreading holidays and illness periods over the sample.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;What methodology might well be tried out in an attempt to tackle this problem?&#10;Thanks.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-07T09:56:09.310" Id="123056" LastActivityDate="2014-11-07T09:56:09.310" OwnerUserId="26474" PostTypeId="1" Score="1" Tags="&lt;sample-size&gt;&lt;small-sample&gt;&lt;method-comparison&gt;" Title="Problem Approach: uncertainty about sample size" ViewCount="18" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm trying to predict output per worker for given inputs of capital (physical capital), labor (human capital) &amp;amp; productivity. I have a data set  of several countries&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;[[ &quot;USA&quot;, {&quot;Capital&quot;:3.21,&quot;Labor&quot;:1.31,&quot;Productivity&quot;:3.17}, 2.96 ]&#10; [ &quot;Norway&quot;, {&quot;Capital&quot;:3.46,&quot;Labor&quot;:1.27,&quot;Productivity&quot;:2.92}, 2.72 ]&#10; [ &quot;UK&quot;, {&quot;Capital&quot;:2.21,&quot;Labor&quot;:1.27,&quot;Productivity&quot;:2.76}, 2.25 ]&#10; [ &quot;Canada&quot;, {&quot;Capital&quot;:2.76,&quot;Labor&quot;:1.32,&quot;Productivity&quot;:2.5}, 2.22 ]&#10; [ &quot;Japan&quot;, {&quot;Capital&quot;:3.53,&quot;Labor&quot;:1.3,&quot;Productivity&quot;:2.12}, 2.04 ]&#10; [ &quot;South Korea&quot;, {&quot;Capital&quot;:2.34,&quot;Labor&quot;:1.22,&quot;Productivity&quot;:2}, 1.6 ]&#10; [ &quot;Mexico&quot;, {&quot;Capital&quot;:0.87,&quot;Labor&quot;:1.04,&quot;Productivity&quot;:1.65}, 0.86 ]&#10; [ &quot;Peru&quot;, {&quot;Capital&quot;:0.38,&quot;Labor&quot;:1.07,&quot;Productivity&quot;:1.01}, 0.41 ]&#10; [ &quot;India&quot;, {&quot;Capital&quot;:0.32,&quot;Labor&quot;:0.97,&quot;Productivity&quot;:1.11}, 0.38 ]&#10; [ &quot;Cameroon&quot;, {&quot;Capital&quot;:0.12,&quot;Labor&quot;:0.76,&quot;Productivity&quot;:1.39}, 0.3 ]&#10; [ &quot;Zambia&quot;, {&quot;Capital&quot;:0.1,&quot;Labor&quot;:0.85,&quot;Productivity&quot;:0.44}, 0.1 ]&#10; [ &quot;United-States&quot;, {&quot;Capital&quot;:3.21,&quot;Labor&quot;:1.31,&quot;Productivity&quot;:3.17}, 2.96 ]&#10; [ &quot;Canada&quot;, {&quot;Capital&quot;:3.21,&quot;Labor&quot;:1.19,&quot;Productivity&quot;:3.28}, 2.78 ]&#10; [ &quot;Italy&quot;, {&quot;Capital&quot;:3.41,&quot;Labor&quot;:0.85,&quot;Productivity&quot;:3.83}, 2.47 ]&#10; [ &quot;West-Germany&quot;, {&quot;Capital&quot;:3.59,&quot;Labor&quot;:1.05,&quot;Productivity&quot;:2.89}, 2.42 ]&#10; [ &quot;France&quot;, {&quot;Capital&quot;:3.5,&quot;Labor&quot;:0.87,&quot;Productivity&quot;:3.57}, 2.42 ]&#10; [ &quot;United-Kingdom&quot;, {&quot;Capital&quot;:2.86,&quot;Labor&quot;:1.06,&quot;Productivity&quot;:3.2}, 2.15 ]&#10; [ &quot;Hong-Kong&quot;, {&quot;Capital&quot;:2.38,&quot;Labor&quot;:0.96,&quot;Productivity&quot;:3.53}, 1.8 ]&#10; [ &quot;Singapore&quot;, {&quot;Capital&quot;:3.31,&quot;Labor&quot;:0.71,&quot;Productivity&quot;:3.42}, 1.79 ]&#10; [ &quot;Japan&quot;, {&quot;Capital&quot;:3.59,&quot;Labor&quot;:1.04,&quot;Productivity&quot;:2.09}, 1.74 ]&#10; [ &quot;Mexico&quot;, {&quot;Capital&quot;:2.78,&quot;Labor&quot;:0.71,&quot;Productivity&quot;:2.94}, 1.28 ]&#10; [ &quot;Argentina&quot;, {&quot;Capital&quot;:3.06,&quot;Labor&quot;:0.89,&quot;Productivity&quot;:2.05}, 1.24 ]&#10; [ &quot;USSR.&quot;, {&quot;Capital&quot;:3.95,&quot;Labor&quot;:0.95,&quot;Productivity&quot;:1.48}, 1.23 ]&#10; [ &quot;India&quot;, {&quot;Capital&quot;:2.27,&quot;Labor&quot;:0.6,&quot;Productivity&quot;:0.85}, 0.25 ]&#10; [ &quot;China&quot;, {&quot;Capital&quot;:2.86,&quot;Labor&quot;:0.83,&quot;Productivity&quot;:0.34}, 0.18 ]&#10; [ &quot;Kenya&quot;, {&quot;Capital&quot;:2.4,&quot;Labor&quot;:0.6,&quot;Productivity&quot;:0.52}, 0.17 ]&#10; [ &quot;Zaire&quot;, {&quot;Capital&quot;:1.6,&quot;Labor&quot;:0.53,&quot;Productivity&quot;:0.51}, 0.1 ]]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Each row represent country name, inputs (capital, labor, productivity) &amp;amp; output.&#10;Now I want to be able from given inputs say &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;{&quot;Capital&quot;:1.1,&quot;Labor&quot;:0.4,&quot;Productivity&quot;:0.3}   &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;to predict how much output will the country produce.&lt;/p&gt;&#10;&#10;&lt;p&gt;So far I've looked at two neural network implementations &lt;a href=&quot;http://github.com/harthur/brain&quot; rel=&quot;nofollow&quot;&gt;http://github.com/harthur/brain&lt;/a&gt;   which gives me bad predictions (or I'm using it/training it badly), and &lt;a href=&quot;http://cs.stanford.edu/people/karpathy/convnetjs&quot; rel=&quot;nofollow&quot;&gt;http://cs.stanford.edu/people/karpathy/convnetjs&lt;/a&gt; which I don't know how to use for continuous data. &lt;/p&gt;&#10;&#10;&lt;p&gt;I don't know are the neural networks right approach,or maybe different method/ library are more fitting for my problem, maybe surface fitting or whatever. I just want to create some bastard Solow model that roughly fits the data points I have.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-11-07T12:16:38.187" Id="123071" LastActivityDate="2014-11-07T20:22:56.707" LastEditDate="2014-11-07T18:43:27.507" LastEditorUserId="60218" OwnerUserId="60218" PostTypeId="1" Score="2" Tags="&lt;machine-learning&gt;&lt;neural-networks&gt;&lt;curve-fitting&gt;" Title="Predicting continuous output" ViewCount="62" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;When it comes to parallelising a problem, it involves the division of routines and subroutines between a number of nodes, namely; the master node and the slave nodes. Once each of these nodes completes its respective task, all the results from all the nodes are concatenated together to give the final output, hopefully in a shorter time than when using sequential computing methods.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is, is there a standard strategy which migrates this method over to the realm of neural networks? For example, my initial reasoning was that what would be &quot;parallelised&quot; in this case would be the feature vector, and each portion of this vector is computed simultaneously, and then concatenated together into one again and the final result will be classified.&lt;/p&gt;&#10;&#10;&lt;p&gt;Would this strategy make sense or would it be better to parallelise the neurons themselves instead?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-07T13:03:56.857" Id="123080" LastActivityDate="2014-11-10T10:31:19.067" OwnerUserId="58785" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;neural-networks&gt;&lt;parallel-computing&gt;" Title="Strategies for parallelising neural networks" ViewCount="50" />
  <row Body="&lt;p&gt;No, you cannot assume a complete mediation.&lt;/p&gt;&#10;&#10;&lt;p&gt;For a mediation you would need a significant direct relationship between the IV and the Mediator (&lt;code&gt;a&lt;/code&gt;), as well as a significant direct relationship between the Mediator and the DV (&lt;code&gt;b&lt;/code&gt;). &lt;/p&gt;&#10;&#10;&lt;p&gt;In addition, the decrease of the direct relationship between IV and DV without the Mediator (&lt;code&gt;c&lt;/code&gt;) and with the Mediator (&lt;code&gt;c'&lt;/code&gt;) is rather small. In a complete mediation this difference should be larger and &lt;code&gt;c'&lt;/code&gt; should be reasonably close to $0$. See &lt;a href=&quot;https://stats.stackexchange.com/questions/19729&quot;&gt;this question&lt;/a&gt; for more information about testing for complete mediation.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-07T15:21:08.243" Id="123095" LastActivityDate="2014-11-07T15:21:08.243" OwnerUserId="58464" ParentId="122376" PostTypeId="2" Score="0" />
  
  
  <row Body="&lt;p&gt;The two-group t-test balances (by computing their ratios) two aspects of the distributions of the values, one: the difference in the means, which your &quot;disturbance&quot; did increase somewhat, and two: the variance which your disturbance increases much more. The variance varies with the square of the differences of the point values from the mean and your single extra value that was hundreds of units from the mean would have added something on the order of 180^2 to the variance estimate where it had been much smaller. Under the hypothesis of normally distributed values (which your alteration has massively violated), the variance is now much larger and the estimate of the difference in group means now plausibly includes zero.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you want to use a test statistic that does not depend on squared deviations, consider the ks.test:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; ks.test(1:10, y = c(7:20, 200))&#10;#-------------------------&#10;    Two-sample Kolmogorov-Smirnov test&#10;&#10;data:  1:10 and c(7:20, 200)&#10;D = 0.7333, p-value = 0.003151&#10;alternative hypothesis: two-sided&#10;&#10;Warning message:&#10;In ks.test(1:10, y = c(7:20, 200)) : cannot compute exact p-value with ties&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;As became clear in my  correspondence with Glen_b I was conflating the wilcox.test in R with the ks.test in R. I meant to use the the &lt;code&gt;wilcox.test&lt;/code&gt; (Wilcoxon Rank Sum test):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; wilcox.test(1:10, y = c(7:20, 200))&#10;&#10;    Wilcoxon rank sum test with continuity correction&#10;&#10;data:  1:10 and c(7:20, 200)&#10;W = 8, p-value = 0.0002229&#10;alternative hypothesis: true location shift is not equal to 0&#10;&#10;Warning message:&#10;In wilcox.test.default(1:10, y = c(7:20, 200)) :&#10;  cannot compute exact p-value with ties&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-11-07T16:48:51.677" Id="123108" LastActivityDate="2014-11-08T23:32:06.960" LastEditDate="2014-11-08T23:32:06.960" LastEditorUserId="2129" OwnerUserId="2129" ParentId="123097" PostTypeId="2" Score="6" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have the following problem: I would like to estimate the effect of price variation caused by uncertainty on an outcome variable. P is my price, X is the variable measuring uncertainty and Y is the outcome. Now, generally I would set it up as an instrumental variable approach. However, my instrument X might also be correlated with Y directly and I would like to measure this effect in the second stage by including X as well. In other words, there are two channels through which uncertainty might affect Y: 1) directly, 2) indirectly through P. And I want to measure both effects.&lt;/p&gt;&#10;&#10;&lt;p&gt;First stage:&#10;$$&#10;P = \beta_0 + \beta_1 X + e_1&#10;$$&#10;Second stage&#10;$$&#10;Y = \beta_3 + \beta_4 \hat P+ \beta_5 X + e_2&#10;$$&#10;Does this make sense at all? If not, I would appreciate any suggestions and advice. Alternatively, please feel free to point me towards a literature that might be helpful or specific papers that deal with similar problems.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-11-07T16:49:18.130" Id="123109" LastActivityDate="2014-11-07T17:48:21.503" LastEditDate="2014-11-07T17:00:08.023" LastEditorUserId="7290" OwnerUserId="60176" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;panel-data&gt;&lt;sem&gt;&lt;instrumental-variables&gt;" Title="How to estimate model where instrument is correlated with dependent variable" ViewCount="54" />
  
  <row AcceptedAnswerId="123937" AnswerCount="1" Body="&lt;p&gt;Let's say we have a set of items with no fixed price, and a set of transactions in which these items are sold alone or in combination.  How do I go about assigning a value to  each of these items?&lt;/p&gt;&#10;&#10;&lt;p&gt;Take the following example (made up on the fly):&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;Basket 1: x = $5&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Basket 2: x = $6&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Basket 3: x,y = $9&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Basket 4: y,z = $4&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Basket 5: z = $3&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Basket 6: z = $4&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;What is the value of x, y, and z?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-07T17:48:30.190" Id="123115" LastActivityDate="2014-11-13T21:21:54.043" OwnerUserId="6108" PostTypeId="1" Score="1" Tags="&lt;finance&gt;" Title="How to assign a value to an item with no fixed price that is sold alone or in combination?" ViewCount="14" />
  <row AcceptedAnswerId="123119" AnswerCount="1" Body="&lt;p&gt;I have these two definitions for &quot;Probability distribution&quot; and &quot;Probability function&quot;:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Probability Distribution: Assigns a probability to each measurable subset of the possible outcomes of a random experiment, survey, or procedure of statistical inference. (&lt;a href=&quot;https://en.wikipedia.org/wiki/Probability_distribution&quot; rel=&quot;nofollow&quot;&gt;Wikipedia&lt;/a&gt;)&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;‎&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Probability Function: Assigns an event A a probability P(A) that represents the likelihood of event A occurring. (&lt;a href=&quot;https://courses.edx.org/courses/MITx/6.00.2_2x/3T2014/courseware/d39541ec36564a88af34d319a2f16bd7/3458865420d34c4cb4fdef3dffb07052/&quot; rel=&quot;nofollow&quot;&gt;MITx 6.00.2x Introduction to Computational Thinking and Data Science&lt;/a&gt;)&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Those are very similar yet slightly different definitions. Specifically, Probability Function seems to refer to &lt;strong&gt;a single event&lt;/strong&gt; whereas Probability Distribution seems to refer to &lt;strong&gt;a collection of events&lt;/strong&gt;. I've &lt;a href=&quot;https://www.google.com/search?q=Probability+function&quot; rel=&quot;nofollow&quot;&gt;googled around&lt;/a&gt; but not found a satisfying answer as it seems that both terms are used in different contexts.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-11-07T18:04:04.963" Id="123117" LastActivityDate="2014-11-07T18:28:12.217" OwnerUserId="48409" PostTypeId="1" Score="0" Tags="&lt;terminology&gt;" Title="Are &quot;Probability Distribution&quot; and &quot;Probability Function&quot; the same thing?" ViewCount="47" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm trying to perform some kind of clustering based anomaly detection for time series and it gives me solid results.&lt;/p&gt;&#10;&#10;&lt;p&gt;What I am interested in is - are there any methods to determine cause of anomally? Something that could tell - something is strange and I think that this parameter values are the reason?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-07T18:34:13.943" Id="123120" LastActivityDate="2014-11-07T18:34:13.943" OwnerUserId="54659" PostTypeId="1" Score="0" Tags="&lt;outliers&gt;&lt;novelty-detection&gt;" Title="Determine cause of anomaly" ViewCount="15" />
  <row Body="&lt;p&gt;Because you will be doing this for $\binom{10}{2}=45$ pairs of distributions, you will want a reasonably efficient method.&lt;/p&gt;&#10;&#10;&lt;p&gt;The question asks to solve (at least approximately) an equation of the form $G_0(\alpha)-G_1(1-\alpha)=0$ where the $G_i$ are the inverse empirical CDFs. Equivalently, you could &lt;strong&gt;solve $F_0(z)+F_1(z)-1=0$ where the $F_i$ are the empirical CDFs.&lt;/strong&gt; That is best done with a root-finding method which does not assume the function is differentiable (or even continuous) because these functions are discontinuous: they jump at the data values.&lt;/p&gt;&#10;&#10;&lt;p&gt;In &lt;code&gt;R&lt;/code&gt;, &lt;code&gt;uniroot&lt;/code&gt; will do the job.  Although it assumes the functions are continuous (it uses Brent's Method, I believe), &lt;code&gt;R&lt;/code&gt;'s implementation of the empirical CDFs makes them look sufficiently continuous.  To make this method work you need to bracket the root between known bounds, but this is easy: it must lie within the range of the union of both datasets.&lt;/p&gt;&#10;&#10;&lt;p&gt;The code is remarkably simple: given two data arrays &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;, create their empirical CDF functions &lt;code&gt;F.x&lt;/code&gt; and &lt;code&gt;F.y&lt;/code&gt;, then invoke &lt;code&gt;uniroot&lt;/code&gt;.  That's all you need.  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;overlap &amp;lt;- function(x, y) {&#10;  F.x &amp;lt;- ecdf(x); F.y &amp;lt;- ecdf(y)&#10;  z &amp;lt;- uniroot(function(z) F.x(z) + F.y(z) - 1, interval&amp;lt;-c(min(c(x,y)), max(c(x,y))))&#10;  return(list(Root=z, F.x=F.x, F.y=F.y))&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;It is reasonably fast: applied to all $45$ pairs of ten datasets ranging in size from $1000$ to $8000$, it found the answers in a total of $0.12$ seconds.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Alternatively, notice that &lt;strong&gt;the desired point is the median of an equal mixture of the two distributions.&lt;/strong&gt;  When the two datasets are the same size, just obtain the median of the union of all the data!  You can generalize this to datasets of different sizes by computing &lt;em&gt;weighted medians&lt;/em&gt;.  This capability is available via quantile regression (in the &lt;code&gt;quantreg&lt;/code&gt; package), which accommodates weights: regress the data against a constant and weight them in inverse proportion to the sizes of the datasets.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;overlap.rq &amp;lt;- function(x, y) {&#10;  library(quantreg)&#10;  fit &amp;lt;- rq(c(x,y) ~ 1, data=d, &#10;            weights=c(rep(1/length(x), length(x)), rep(1/length(y), length(y))))&#10;  return(coef(fit))&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Timing tests show this is at least three times slower than the root-finding method and it does not scale as well for larger datasets: on the preceding test with $45$ pairs of datasets it took $1.67$ seconds, more than ten times slower.  The chief advantage is that this particular implementation of weighted medians will issue warnings when the answer does not appear unique, whereas Brent's method tends to find unique answers right in the middle of an interval of possible answers.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;As a demonstration,&lt;/strong&gt; here is a plot of two empirical CDFs along with vertical lines showing the two solutions (and horizontal lines marking the levels of $\alpha$ and $1-\alpha$).  In this particular case, the two methods produce the same answer so only one vertical line appears.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/KJzzf.png&quot; alt=&quot;Figure&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#&#10;# Generate some data.&#10;#&#10;set.seed(17)&#10;x &amp;lt;- rnorm(32, 5, 2)&#10;y &amp;lt;- rgamma(10, 2)&#10;#&#10;# Compute the solution two ways.&#10;#&#10;solution &amp;lt;- overlap(x, y)&#10;solution.rq &amp;lt;- overlap.rq(x, y)&#10;F.x &amp;lt;- solution$F.x; F.y &amp;lt;- solution$F.y; z &amp;lt;- solution$Root&#10;    alpha &amp;lt;- c(F.x(z$root), F.y(z$root))&#10;    #&#10;    # Plot the ECDFs and the results.&#10;    #&#10;    plot(interval, 0:1, type=&quot;n&quot;, xlab=&quot;z&quot;, ylab=&quot;Probability&quot;, main=&quot;CDFs&quot;)&#10;    curve(F.x(x), add=TRUE, lwd=2, col=&quot;Red&quot;)&#10;    curve(F.y(x), add=TRUE, lwd=2, col=&quot;Blue&quot;)&#10;    abline(v=z$root, lty=2)&#10;abline(v=solution.rq, lty=2, col=&quot;Green&quot;)&#10;abline(h=alpha, lty=3, col=&quot;Gray&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-11-07T18:41:46.703" Id="123122" LastActivityDate="2014-11-07T19:56:11.123" LastEditDate="2014-11-07T19:56:11.123" LastEditorUserId="919" OwnerUserId="919" ParentId="122857" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;I am trying to compute the distribution of the following &#10;$$Z=\bigl(X+Y\bigl)^2$$ BUT I have that both $X$ and $Y$ are &lt;a href=&quot;https://en.wikipedia.org/wiki/Nakagami_distribution&quot; rel=&quot;nofollow&quot;&gt;Nakagami&lt;/a&gt; with parameter $m$. (A Nakagami random variable is the square root of a Gamma random variable.) So the above is hard to derive in general ( I would have to take the convolution assuming the summands are independent..)&lt;/p&gt;&#10;&#10;&lt;p&gt;So my alternative is to solve $$W=X^2+Y^2$$ which is easier to compute as it will be the sum of two Gamma distributions with scale and shape parameter $m$ and is also Gamma distributed.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is, when can one argue that $W$ and $Z$ have the same distribution, is it when X and Y are uncorrelated then they are equivalent? or will they never be equivalent?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-07T20:18:20.663" Id="123133" LastActivityDate="2014-11-09T13:43:46.097" LastEditDate="2014-11-09T13:43:46.097" LastEditorUserId="7224" OwnerUserId="59984" PostTypeId="1" Score="2" Tags="&lt;probability&gt;&lt;distributions&gt;&lt;gamma-distribution&gt;&lt;equivalent-distributions&gt;&lt;nakagami-distribution&gt;" Title="When is distribution of $|X+Y|^2 $ equivalent to $|X|^2+|Y|^2$?" ViewCount="106" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;A colleague says that estimating the following time series model is statistically sound: &lt;/p&gt;&#10;&#10;&lt;p&gt;$$y_t = \beta_0 + \beta_1  x_{1t} + \beta_2 x_{2t} + e_t$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $y_t$ is nonstationary $I(1)$, $x_{1t}$ is nonstationary but cointegrated with $y_t$, $x_{2t}$ is stationary, $e_t$ is a white noise residual and $\beta{_*}$ are parameters. &lt;/p&gt;&#10;&#10;&lt;p&gt;I'm not so sure. The safe approach would just be to fit an Error Correction Model to these variables, but in this case there is resistance to doing that (long story).&lt;/p&gt;&#10;&#10;&lt;p&gt;My intuition is that because the dependent variable is nonstationary and because $x_{2t}$ is stationary, that the covariance$(y_t,x_{2t})$ will be undefined and $\beta_2$ subject to change as the data set grows. &lt;/p&gt;&#10;&#10;&lt;p&gt;So an ECM is the straightforward/classic way to model these series, but is the equation above legitimate? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-07T21:32:28.430" Id="123144" LastActivityDate="2014-11-11T16:22:34.077" LastEditDate="2014-11-07T21:57:49.647" LastEditorUserId="60243" OwnerUserId="60243" PostTypeId="1" Score="3" Tags="&lt;time-series&gt;&lt;cointegration&gt;&lt;non-stationary&gt;" Title="Modeling an I(1) process with a cointegrating I(1) and an I(0) variable" ViewCount="152" />
  <row Body="&lt;p&gt;Without further information about the Nakagami distribution or even the dimension of $X$ and $Y$, here is one case when $Z$ and $W$ have the same distribution:&lt;/p&gt;&#10;&#10;&lt;p&gt;If you expand $W=(X+Y)^2$, you get&#10;$$&#10;W=(X+Y)^2 = X^2+2YX+Y^2=Z+2XY&#10;$$&#10;therefore $W=Z$ when $XY=0$ with probability 1.&lt;/p&gt;&#10;&#10;&lt;p&gt;I suspect this is the only non-trivial case when $Z$ and $W$ share the same distribution.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-11-07T21:35:37.397" Id="123145" LastActivityDate="2014-11-08T08:26:16.113" LastEditDate="2014-11-08T08:26:16.113" LastEditorUserId="7224" OwnerUserId="7224" ParentId="123133" PostTypeId="2" Score="6" />
  <row AcceptedAnswerId="123151" AnswerCount="2" Body="&lt;p&gt;$X\sim \Gamma(p,a)$,find the two dimensional moment generating function of ($X, \ln X$)&lt;/p&gt;&#10;&#10;&lt;p&gt;What I have done is &#10;$$M_{(X,lnX)}(t_{1},t_{2})=E(e^{t_{1x}+t_{2}lnx})=E(e^{t_{1}x}x^{t_{2}})=\int\int e^{t_{1}x}x^{t_{2}}f_{X,lnX}(x_{1},x_{2})dx_{1}dx_{2}$$&lt;br&gt;&#10;I need help to find the joint distribution of $X$ and $\ln X$.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-07T22:24:21.067" Id="123150" LastActivityDate="2014-11-08T00:51:05.847" LastEditDate="2014-11-07T22:33:52.260" LastEditorUserId="7290" OwnerUserId="60244" PostTypeId="1" Score="2" Tags="&lt;probability&gt;&lt;self-study&gt;" Title="$X\sim \Gamma(p,a)$,find the two dimensional moment generating function of (X, ln X)" ViewCount="53" />
  <row AnswerCount="0" Body="&lt;p&gt;I am doing a logistic regression in R, where I am modeling how potholes and weather correlate to accidents. When I run a logistic regression, I get the message &quot;Algorithm does not converge&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem I think I have is that I have 24,000 accidents with only 350 potholes related to these accidents. Is this to small of a sample size?&lt;/p&gt;&#10;&#10;&lt;p&gt;The other possible issue I thought of, is that when I look at sample logistic regressions, the outcome is either zero or one, but the only outcome I have is the outcome of one, or accident in my case. I do not have any non accident data in my set, could this be what is causing the problem?&lt;/p&gt;&#10;&#10;&lt;p&gt;I will attach my current code and its output.&lt;/p&gt;&#10;&#10;&lt;p&gt;require(ggplot2) require(sandwich) require(msm) mydata &amp;lt;- read.csv(&quot;C:\Users\myname\downloads\logreg1.csv&quot;)&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;head(mydata)&#10;        Date Pothole Rain Snow Accident&#10;  1 1/1/2012       0    0    0        1&#10;  2 1/1/2012       0    0    0        1&#10;  3 1/1/2012       0    0    0        1&#10;  4 1/1/2012       0    0    0        1&#10;  5 1/1/2012       0    0    0        1&#10;  6 1/1/2012       0    0    0        1&#10;  summary(mydata)&#10;           Date          Pothole            Rain              Snow            Accident&#10;   1/8/2014  :   87   Min.   :0.0000   Min.   :0.00000   Min.   : 0.0000   Min.   :1&lt;br&gt;&#10;   1/30/2013 :   82   1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.: 0.0000   1st Qu.:1&lt;br&gt;&#10;   3/21/2013 :   77   Median :0.0000   Median :0.00000   Median : 0.0000   Median :1&lt;br&gt;&#10;   12/21/2012:   76   Mean   :0.0173   Mean   :0.08077   Mean   : 0.1129   Mean   :1&lt;br&gt;&#10;   3/10/2013 :   66   3rd Qu.:0.0000   3rd Qu.:0.00000   3rd Qu.: 0.0000   3rd Qu.:1&lt;br&gt;&#10;   12/13/2013:   59   Max.   :8.0000   Max.   :3.32000   Max.   :11.1000   Max.   :1&lt;br&gt;&#10;   (Other)   :23606&lt;br&gt;&#10;  mylogit &amp;lt;- glm(Accident ~  Rain + Snow, data = mydata, family = &quot;binomial&quot;)&#10;  Warning message:&#10;  glm.fit: algorithm did not converge&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="1" CreationDate="2014-11-08T00:52:07.483" Id="123169" LastActivityDate="2014-11-08T04:41:10.220" OwnerDisplayName="Analysisugh" PostTypeId="1" Score="0" Tags="&lt;algorithms&gt;&lt;regression&gt;" Title="Algorithm does not converge in R" ViewCount="24" />
  
  
  <row Body="&lt;p&gt;I would like to complement the above answers with a reference where you can read more about time dependence in panel data models, as you requested: Verbeek, Marno. &lt;em&gt;A guide to modern econometrics&lt;/em&gt;, Wiley. &#10;There is a chapter in this book on panel data models that can serve as a good introduction.&lt;/p&gt;&#10;&#10;&lt;p&gt;As an example of contemporary research regarding time-dependence in panel data, you could read:&lt;/p&gt;&#10;&#10;&lt;p&gt;Fredrik N. G. Andersson: Exchange rates dynamics revisited: a panel data test of the fractional integration order. Empir Econ (2014) 47:389–409. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-08T14:00:54.923" Id="123191" LastActivityDate="2014-11-08T14:00:54.923" OwnerUserId="45419" ParentId="122741" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Yes, put it on the figure.  It is useless without units.  There is plenty of space in the middle of the black bar between the -5.000 and 5.000 .&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-11-08T16:58:39.180" Id="123203" LastActivityDate="2014-11-08T16:58:39.180" OwnerUserId="60274" ParentId="123199" PostTypeId="2" Score="4" />
  
  <row AnswerCount="0" Body="&lt;p&gt;In my multiple linear regression model, all of my explanatory variables have a VIF score, lower then 3, but the highest condition index is 709.&#10;The constant and one of the explanatory variables have 1.00 variance proportion values for this condition index.&lt;/p&gt;&#10;&#10;&lt;p&gt;What does this mean? &lt;/p&gt;&#10;&#10;&lt;p&gt;According to the VIF values, my model doesn't have a multicollinearity problem, but I don't know what to do with this high condition index and 1.0 variance proportions.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for the help!&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-11-08T17:16:29.317" FavoriteCount="1" Id="123207" LastActivityDate="2014-11-08T17:16:29.317" OwnerUserId="42424" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;multiple-regression&gt;&lt;multicollinearity&gt;&lt;vif&gt;" Title="Very low VIF values, but extreme high condition index" ViewCount="71" />
  
  <row Body="&lt;p&gt;You were correct in your third paragraph.  You are attempting to fit a logistic regression model where every response is the same (accident).  &lt;/p&gt;&#10;&#10;&lt;p&gt;Think about what you are feeding into the model: &#10;You are giving it 24,000 records with varying variable values for &quot;Pothole&quot;, &quot;Rain&quot;, and &quot;Snow&quot; that ALL result in an accident.  Your model has no built-in knowledge that some weather conditions do result in NO accident, yet alone the frequency of accident to non-accident at varying levels of weather conditions.  Intuitively, your model knows of no conditions that can result in NO accident.&lt;/p&gt;&#10;&#10;&lt;p&gt;Since your data points are not linearly separable, you cannot get convergence of the algorithm.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Ideally you would want a random sample of all trips for which you are interested along with associated weather conditions and the result of the trip (accident/no accident).  Practically, this random sample is very difficult to acquire so you will likely need a workaround to get pseudo-absences, but this is a different question in and of itself.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-08T17:55:24.307" Id="123211" LastActivityDate="2014-11-08T17:55:24.307" OwnerUserId="36381" ParentId="123210" PostTypeId="2" Score="3" />
  
  <row AcceptedAnswerId="123224" AnswerCount="1" Body="&lt;p&gt;I would like to calculate a sample size for the 2 sample proportion test when the significance level is assumed (0.05), power is assumed (0.90) and I'd like 2 sample proportion test to detect that there is at least 5% difference in proportions in 2 groups (in favour in 1 specific group).&lt;/p&gt;&#10;&#10;&lt;p&gt;Any idea on how to calcuate this? Or any reference to read?&#10;I've found package in &lt;code&gt;R&lt;/code&gt; -&gt; &lt;code&gt;pwr&lt;/code&gt; and a function &lt;code&gt;pwr.2p.test&lt;/code&gt; that enable to calculate a sample with given sig.level, power and given effect but the effect size shoud be delivered as Cohen's d that I can't connect with my desired effect size = at least 5%.&lt;/p&gt;&#10;&#10;&lt;p&gt;Learn about this from here &lt;a href=&quot;http://www.statmethods.net/stats/power.html&quot; rel=&quot;nofollow&quot;&gt;http://www.statmethods.net/stats/power.html&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for any advce.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-08T18:15:55.630" Id="123214" LastActivityDate="2014-11-08T20:47:05.430" OwnerUserId="49793" PostTypeId="1" Score="1" Tags="&lt;sample-size&gt;&lt;proportion&gt;&lt;effect-size&gt;&lt;cohens-d&gt;" Title="problem: a sample size for the assumed effect in the 2 sample proportion test" ViewCount="17" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;Any and all help would be much appreciated! I've detailed a bit of information below, but if I left anything out please let me know&lt;/p&gt;&#10;&#10;&lt;p&gt;My hypotheses are (info: NA= novel asssembly; S= selections or assembly piece):&lt;/p&gt;&#10;&#10;&lt;p&gt;1) H0: NA, and S will not affect the completion rates of the assembly tasks.&#10;H1: NA, and S will affect the completion rates of the assembly tasks.&lt;/p&gt;&#10;&#10;&lt;p&gt;2) H0: Time to complete an assembly between, NA, and S will be the same&#10;H1: Time to complete an assembly between, NA and S will be different.&lt;/p&gt;&#10;&#10;&lt;p&gt;3) H0: Completion rates for color-coded and non color-coded novel assemblies will&#10;be the same.&#10;H1: Completion Rates for color-coded and non color-coded novel assemblies will&#10;be different.&lt;/p&gt;&#10;&#10;&lt;p&gt;4) H0: Time to complete an assembly between color-coded and non color-coded &#10;novel assemblies will be the same.&#10;H1: Time to complete an assembly between color-coded and non color-coded &#10;novel assemblies will be different. &lt;/p&gt;&#10;&#10;&lt;p&gt;In my experiment, I have 3 independent variables at 4x3x2:&#10;- Assembly difficulty (low, medium, high, very high) which I believe is an ordinal variable&#10;- % increase in assembly pieces (0%, 25%, 50%) which I believe is an ordinal variable&#10;- Color coded assemblies (color coded, not color coded) which I believe is a nominal variable&#10;- I also gave participants a pre-test to measure their spatial rotation ability, I think this may be the 4th IV with 1 level, but I'm not sure what to type of variable it is.&lt;/p&gt;&#10;&#10;&lt;p&gt;I ran the study as a within-subects 4x3x2. I measured two dependent variables, completion time (coded in seconds) and completion rate which was a binary variable (success/failure). I used a complete counterbalanced design, similar to the one found here&lt;/p&gt;&#10;&#10;&lt;p&gt;At this point it feels like there are a ton of different methods I could use to analyze the data. I have some general ideas, but I don't feel like I know enough that I feel confident running the analyses and writing up the results. I'll bullet out what I'm thinking because bullets are easier: &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;ANOVA for Assembly difficulty x completion time / completion rate&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;ANOVA for % increase in assembly pieces x completion time / completion rate&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;ANOVA for color coded/not color coded x completion time / completion rate&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;rank all of the scores from high -&gt; low and see if there is a correlation with pre-test scores and completion times/rates&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I have an ok understanding of R (thanks to the seemingly infinite resources on the internet) and plan on using that as my tool for analyses. &lt;/p&gt;&#10;&#10;&lt;p&gt;I know there are quite a few tests that measure whether or not your data satisfies the assumptions required for particular methods. We never got too deep in these at school thus that's an area I know even less about. Any help in that department would be awesome.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-08T20:24:11.367" Id="123226" LastActivityDate="2014-11-09T17:08:01.367" OwnerUserId="60283" PostTypeId="1" Score="0" Tags="&lt;correlation&gt;&lt;anova&gt;&lt;repeated-measures&gt;" Title="Help need identifying analyses methods" ViewCount="23" />
  <row AcceptedAnswerId="123247" AnswerCount="3" Body="&lt;p&gt;$x_{1}...x_{n}$ are independent continuous random variables with common distribution function $F(x)$,consider the order statistics $(x_{(1)},...,x_{(n)})$, compute $E(F(x_{(n)})-F(x_{(1)}))$&lt;/p&gt;&#10;&#10;&lt;p&gt;I have no idea to this problem, anyone could help me? Thanks!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-08T22:05:54.067" Id="123233" LastActivityDate="2014-11-08T23:40:17.797" OwnerUserId="60244" PostTypeId="1" Score="3" Tags="&lt;probability&gt;&lt;order-statistics&gt;" Title="$x_{1}...x_{n}$ are independent continuous random variables with common distribution function $F(x)$,compute $E(F(x_{(n)})-F(x_{(1)}))$" ViewCount="75" />
  <row AnswerCount="1" Body="&lt;p&gt;I am fitting a model using geeglm in geepack and ran into a problem. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have a dataset pertaining to oil consumption and fit the below model.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;geeglm(formula = Consumption~Income + Price + Observation, &#10;       id=Id, &#10;       corstr=&quot;ar1&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;but it's output looks like this &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Coefficients:&#10;  (Intercept)  Income 1,014.99   Income 1,047.31   Income 1,064.58  &#10;      1316992           -453526          -2734582           1626503 &#10;  Income 1,138.72   Income 1,153.81   Income 1,158.30   Income 1,160.69  &#10;     -2879197           -639736            212062           -409109 &#10;  Income 1,182.30   Income 1,249.24   Income 1,294.34   Income 1,300.16  &#10;      1314828          -3177208             -4425           1214973 &#10;  Income 1,316.89   Income 1,336.83   Income 1,339.14   Income 1,407.93  &#10;      -821846          -3295240           -641897           2029621 &#10;  Income 1,430.15   Income 1,432.48   Income 1,433.70   Income 1,441.98  &#10;      -273614           -858012          -3472286           -851193 &#10;  Income 9,187.94   Income 9,224.06   Income 9,313.42   Income 9,720.17  &#10;       621587          -2171161             15996          -4233896 &#10;  Income 964.01              Price       Observation &#10;     -2469417           -439571            203036 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Why is it giving me a list of coefficients on every income when I set id=Id?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-08T22:57:17.797" Id="123237" LastActivityDate="2014-11-08T23:53:19.463" OwnerUserId="58436" PostTypeId="1" Score="1" Tags="&lt;mixed-model&gt;&lt;generalized-linear-model&gt;&lt;modeling&gt;" Title="Problem fitting a geeglm regression" ViewCount="14" />
  
  
  <row Body="&lt;p&gt;I will treat the case $a &amp;gt; 0$.  The other cases can be handled with the same methods.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $\Phi$ be the CDF of the standard Normal distribution governing the random variable $X$.  Because the graph of $y = ax^2 + bx + c$ is a parabola, the chance that $Y = aX^2 + bX + c$ is less than or equal to $t$ is the chance that $x$ lies &lt;em&gt;between&lt;/em&gt; the two roots of $ax^2 + bx + c - t$ (which are easily computed with a quadratic formula).  Calling them $x_{+}(t)$ and $x_{-}(t)$, this is given by $\Phi(x_{+}(t)) - \Phi(x_{-}(t))$.  Differentiation with respect to $t$ gives the PDF:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\phi(t) = \frac{1}{\sqrt{2 \pi  d}} e^{-\frac{d}{2 a^2}} \left(e^{\frac{\left(b-\sqrt{d}\right)^2}{8 a^2}}+e^{\frac{\left(b+\sqrt{d}\right)^2}{8 a^2}}\right) e^{-\frac{c-t}{a}}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $d=b^2 - 4a(c-t)$ is the discriminant of the quadratic form and $t$ is not less than the minimum of $y$, equal to $c - b^2/4a.$  Note the factor of $\sqrt{d}$ in the denominator: as $d$ approaches $0$ from above (that is, $c-t$ approaches $b^2/4a$ from higher values of $t$), the exponentials stay finite but the entire PDF has to diverge.  &lt;em&gt;Necessarily&lt;/em&gt; this PDF will have a vertical asymptote at $t = c - b^2/4a$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Plots of a typical CDF and PDF (for $a=1, b=-2, c=0$} are:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/dEEoi.png&quot; alt=&quot;Figure&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Because these plots coincide with the (obviously correct) plot of a $\chi^2(1)$ distribution when $b=0$--the two plots just get translated one unit to the right--they likely are correct.  Let's look at their counterparts for the example $a=6, b=30, c=80$ in the question:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/JqlRS.png&quot; alt=&quot;Figure 2&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The PDF looks just like that shown in the question, suggesting the &lt;code&gt;R&lt;/code&gt; code actually is correct.&lt;/p&gt;&#10;&#10;&lt;p&gt;The reason the PDF shoots up at the left is made clear by the CDF: the quadratic behavior of the parabola near its vertex at $-30/12$, where it has a height of $85/2$, focuses a great deal of probability just to the right of $t=85/2$.  The infinite density reflects the verticality of the CDF at that location.  (The verticality is clear in the first plot; in the second it is apparent only when zoomed in closely near its left endpoint.)  Values of $Y$ smaller than $85/2$ cannot be achieved at all, whence there is no probability associated with $t\lt 85/2$.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-11-09T02:35:57.887" Id="123254" LastActivityDate="2014-11-09T03:06:33.687" LastEditDate="2014-11-09T03:06:33.687" LastEditorUserId="919" OwnerUserId="919" ParentId="123171" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="123357" AnswerCount="1" Body="&lt;p&gt;Currently I’m trying to understand and use the Stahel-Donoho outlyngness measure.&lt;/p&gt;&#10;&#10;&lt;p&gt;But unfortunately I’ve got a problem in the part where one is taking the maximum over the set of directions. I found papers and tutorials about the Stahel-Donoho estimator, but their explanation is not good enough for me. In &lt;a href=&quot;http://arxiv.org/pdf/1009.5818.pdf&quot; rel=&quot;nofollow&quot;&gt;one&lt;/a&gt; paper,  it has been suggested that one way to find this set of directions is by taking $p$ directions through 2 randomly chosen observations. In another paper [0], the authors use many directions through 2 randomly chosen observations. The original definition itself uses as direction $d$ the direction orthogonal to the hyperplane through $p$ randomly chosen data points. Then, the full set of directions is obtained by repeating  this procedure for many random $p$ subsets (see page 3, 2nd paragraph from the bottom of this &lt;a href=&quot;http://e-archivo.uc3m.es/bitstream/handle/10016/14896/random_prieto_JCGS_2007.pdf?sequence=1&quot; rel=&quot;nofollow&quot;&gt;paper&lt;/a&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;Unfortunately I cannot get any idea how to deal with this problem. Should I use random projection? The problem I'm having is that I don’t know how to obtain the directions $d$ and projections.  I really stuck at the moment. Could someone kindly help me to understand or provide me a good reference for better understanding?&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;[0] Hubert, M., Rousseeuw P. J. and  Vanden Branden, K. (2005). ROBPCA: A New Approach to Robust Principal Component Analysis. Technometrics&#10;Volume 47, Issue 1.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="9" CreationDate="2014-11-09T06:12:13.053" FavoriteCount="0" Id="123262" LastActivityDate="2015-01-16T19:57:08.603" LastEditDate="2014-11-10T13:51:38.257" LastEditorUserId="603" OwnerUserId="52054" PostTypeId="1" Score="3" Tags="&lt;self-study&gt;&lt;outliers&gt;&lt;robust&gt;&lt;high-dimensional&gt;" Title="How to find set of directions in Stahel-Donoho outlyingness measure?" ViewCount="168" />
  
  <row AcceptedAnswerId="124861" AnswerCount="1" Body="&lt;p&gt;I'm planning on running a hierarchical multiple regression. In the first step, I would like to enter demographic characteristics, second step continuous predictor variables of interest, and third step interactions between the continuous predictor variables. However, the issue I've run into is that some of the demographic variables are more than two levels and either nominal (e.g., education level: less than BA, BA, graduate degree) or ordinal (e.g., age: 20-30, 31-40, 41-50, 51+). My dependent variable is continuous. &lt;/p&gt;&#10;&#10;&lt;p&gt;My question is: can I dummy code the nominal/ordinal variables and then run the multiple regression as I normally would? Or, would you recommend handling the data differently? I'm using SPSS, so I appreciate any help/input. Thanks!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-09T08:05:50.730" Id="123269" LastActivityDate="2014-11-20T19:40:05.593" LastEditDate="2014-11-20T19:40:05.593" LastEditorDisplayName="user60305" OwnerDisplayName="user60305" PostTypeId="1" Score="1" Tags="&lt;multiple-regression&gt;&lt;categorical-data&gt;&lt;spss&gt;" Title="Multiple Regression with Categorical Predictor Variables of More than Two Levels" ViewCount="78" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Assume a simple Poisson model with 2 unknown parameters (the intercept and the slope) Show that the expectation of the Hessian matrix = the observed Hessian matrix or equivalently the observed Information matrix = expected Information matrix. The same is true for the logit model.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-11-08T22:39:44.803" Id="123278" LastActivityDate="2014-11-09T11:36:16.073" LastEditDate="2014-11-09T11:36:16.073" LastEditorUserId="26338" OwnerDisplayName="user4229982" PostTypeId="1" Score="0" Tags="&lt;self-study&gt;&lt;generalized-linear-model&gt;&lt;poisson&gt;" Title="Poisson GLM,Hessian matrix = the observed Hessian matrix" ViewCount="68" />
  <row Body="&lt;p&gt;Are you constrained to the cohen's d approach? I've followed this site (&lt;a href=&quot;http://ww2.coastal.edu/kingw/statistics/R-tutorials/proport.html&quot; rel=&quot;nofollow&quot;&gt;http://ww2.coastal.edu/kingw/statistics/R-tutorials/proport.html&lt;/a&gt;) to do power tests recently. In my case I have a sense of the rates to expect in the samples and can plug in values for effect sizes I might encounter, so it may not work in your situation.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-11-09T13:35:25.250" Id="123286" LastActivityDate="2014-11-09T13:35:25.250" OwnerUserId="60294" ParentId="123111" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm using SPSS GLiMM to run a repeated measures logistic regression model. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Research Qn&lt;/strong&gt;&#10;I'm interested to know if a MP is more likely to make a particular response (e.g., apology, challenge, denial) after the party leader made a public statement, and if this likelihood is higher if the MP and party leader belongs to the same political party. Thus, there are 2 repeated measures for each MP: before public statement and after public statement. If an apology was made any time before/after the public statement, it was coded as 1.&lt;/p&gt;&#10;&#10;&lt;p&gt;The following are my variables of interest:&#10;L1: time/repeated measures (1=after public statement, 0=before public statement)&#10;L2: party membership (1=X party, 0=not X party)&#10;Outcome: a particular response was used or not (1= used, 0=not used)&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Model specification&lt;/strong&gt;&#10;I did the following:&lt;/p&gt;&#10;&#10;&lt;p&gt;1) only fixed effects(time. no random intercept, no random slopes. If it runs smoothly i.e., no warning messages, output looks normal, I proceed with adding a random intercept.&lt;/p&gt;&#10;&#10;&lt;p&gt;2) run a model with random intercept only, to check for ICCs. I only have 1 L1 predictor which is time (0-before intervention, 1- after intervention) as a FE. If ICC is high, then I included the intercept as a random effect.&lt;/p&gt;&#10;&#10;&lt;p&gt;3) If ICC is high (more than 0.1), I add in&#10;- a L2 predictor (party membership) as fixed effects and &#10;- a cross-level interaction (time X political party) as fixed effects&#10;I followed Heck et al., (2012) textbook's method. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Problems!&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem usually occurs at Step 3. Although I have a high ICC which justifies adding a L2 predictor, sometimes my model does not run. I get the following warning messages, depending on which DV I use. There're a couple of scenarios I encounter, as  described below.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;glmm:&lt;/em&gt; The covariance matrix of residuals is not positive definite in the iterative process. No output will be displayed.&#10;Execution of this command stops.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;glmm:&lt;/em&gt; The estimated covariance matrix of the random effects (the G matrix) is not positive definite. The procedure continues despite this warning. Subsequent results produced are based on the last iteration. Validity of the model fit is uncertain.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;glmm:&lt;/em&gt; The final Hessian matrix is not positive definite although all convergence criteria are satisfied. The procedure continues despite this warning. Subsequent results produced are based on the last iteration. Validity of the model fit is uncertain.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Questions&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;1) I get output that looks normal, but with one of the warning messages above. &#10;  are these results still usable?&lt;/p&gt;&#10;  &#10;  &lt;p&gt;2) I get output with no values in the tables.&#10;  what does this even mean??&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/vxsSs.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;3) My understanding is that I get these warning signs because the model might have been overly complex, which confuses me, because ICC is high enough to explain with a L2 predictor. &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I'm not sure where I went wrong - adding in L2 predictor and cross-level interaction as fixed effects (Heck's methods), or data does not support the complexity of model, or that I'm just not understanding the statistics properly. &lt;/p&gt;&#10;&#10;&lt;p&gt;I could share with you the pages from Heck et al., textbook that I used as reference, if you need it. &lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you!&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-11-09T15:08:22.567" Id="123290" LastActivityDate="2014-11-11T20:49:57.400" LastEditDate="2014-11-10T08:19:42.307" LastEditorUserId="60321" OwnerUserId="60321" PostTypeId="1" Score="0" Tags="&lt;spss&gt;&lt;generalized-linear-model&gt;&lt;repeated-measures&gt;&lt;intraclass-correlation&gt;" Title="Adding in L2 predictor in GLiMM fails even though ICC is high" ViewCount="36" />
  <row Body="&lt;p&gt;The question here focuses on estimating a probability. The proposed estimator&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\hat P(X\leq k) = \Phi(k-\bar X_n)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;is a consistent estimator since, $\text{plim} \bar X_n = \mu$, and by the Continuous Mapping (Mann-Wald) Theorem&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\text{plim} \Phi(k-\bar X_n) = \Phi(k-\mu) = P(X-\mu \leq k-\mu) = P(X \leq k)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The standard normal CDF $\Phi$ is here treated as &quot;just another function&quot;, since the random variable $k-\bar X_n$ does not have a &lt;em&gt;standard&lt;/em&gt; normal distribution, but rather &lt;/p&gt;&#10;&#10;&lt;p&gt;$$k-\bar X_n \sim N(k-\mu, 1/n)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and so the transformation $\Phi(k-\bar X_n)$ does not have a uniform $U(0,1)$ distribution, although it does range in $[0,1]$.&lt;/p&gt;&#10;&#10;&lt;p&gt;To consider the asymptotic distribution we use the Delta Method: applying the Mean Value Theorem we have (as an exact relation)&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\Phi(k-\bar X_n) = \Phi(k-\mu)+ \Phi'(\tilde m)\cdot(k-\bar X_n-k+\mu)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\tilde m$ is some point between $k-\bar X_n$ and $k-\mu$. Rearranging and multiplying by $\sqrt n$ we obtain&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\sqrt n\left(\Phi(k-\bar X_n) - \Phi(k-\mu)\right) = \phi(\tilde m)\cdot\sqrt n(\mu-\bar X_n) \tag{1}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;with $\phi$ being the standard normal PDF. We know that &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\sqrt n(\mu-\bar X_n) \xrightarrow{d} N(0,1) $$&lt;/p&gt;&#10;&#10;&lt;p&gt;and also, that since $k-\bar X_n$ estimates consistently $k-\mu$, $\tilde m$ is asymptotically sandwiched towards $k-\mu$ and so, by using again the Continuous Mapping theorem, $\phi(\tilde m) \rightarrow \phi(k-\mu)$. Combining these results together with Slutsky's lemma we obtain&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\sqrt n\left(\Phi(k-\bar X_n) - \Phi(k-\mu)\right) \xrightarrow{d} N\left(0,[\phi(k-\mu)]^2 \right) \tag{2}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;which provides the following finite-sample approximation to the variance that is requested by the question,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\text{Var}\left[\Phi(k-\bar X_n)\right] \approx \frac {e^{-(k-\mu)^2}}{2\pi n}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Unavoidably, if we want an &lt;em&gt;estimate&lt;/em&gt; of the variance, we will have to plug-in the obtained value of $\bar X_n =\bar x_n$ in place of $\mu$, given our choice of $k$.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-11-09T15:10:55.213" Id="123291" LastActivityDate="2014-11-09T21:18:01.130" LastEditDate="2014-11-09T21:18:01.130" LastEditorUserId="28746" OwnerUserId="28746" ParentId="123264" PostTypeId="2" Score="2" />
  <row AnswerCount="1" Body="&lt;p&gt;A stochastic matrix with states $S_1$, $S_2$, $S_3$, $S_4$ is given, now we would like to build up another stochastic matrix with finer states, meaning that the states $S_1$ will be considered as $S_1=Z_1+Z_2$ ('$+$' sign stands for the union of two new states), and $S_2=Z_3+Z_4$, $S_3=Z_5+Z_6$, and $S_4=Z_7+Z_8$. Indeed, this kind of information could be available from Bayesian Statistics. I am just wondering if anybody studied or knows references for such kind of problem. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-09T15:19:06.033" Id="123292" LastActivityDate="2014-11-24T11:23:41.497" LastEditDate="2014-11-09T15:42:26.610" LastEditorUserId="7290" OwnerUserId="60320" PostTypeId="1" Score="0" Tags="&lt;references&gt;&lt;markov-process&gt;&lt;hidden-markov-model&gt;&lt;markov-chain&gt;" Title="Embedding Markov Matrix" ViewCount="18" />
  
  
  <row Body="&lt;p&gt;The rule for the proper formulation of a hypothesis test is that the &lt;strong&gt;alternative&lt;/strong&gt; or &lt;strong&gt;research&lt;/strong&gt; hypothesis is the statement that, if true, is strongly supported by the evidence furnished by the data.&lt;/p&gt;&#10;&#10;&lt;p&gt;The &lt;strong&gt;null&lt;/strong&gt; hypothesis is generally the complement of the alternative hypothesis.  Frequently, it is (or contains) the assumption that you are making about how the data are distributed in order to calculate the test statistic.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here are a few examples to help you understand how these are properly chosen.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Suppose I am an epidemiologist in public health, and I'm investigating whether the incidence of smoking among a certain ethnic group is &lt;em&gt;greater&lt;/em&gt; than the population as a whole, and therefore there is a need to target anti-smoking campaigns for this sub-population through greater community outreach and education.  From previous studies that have been published in the literature, I find that the incidence among the general population is $p_0$.  I can then go about collecting sample data (that's actually the hard part!) to test $$H_0 : p = p_0 \quad \mathrm{vs.} \quad H_a : p &amp;gt; p_0.$$  This is a one-sided binomial proportion test.  $H_a$ is the statement that, if it were true, would need to be strongly supported by the data we collected.  It is the statement that &lt;strong&gt;carries the burden of proof&lt;/strong&gt;.  This is because any conclusion we draw from the test is conditional upon assuming that the null is true:  either $H_a$ is accepted, or the test is inconclusive and there is insufficient evidence from the data to suggest $H_a$ is true.  The choice of $H_0$ reflects the underlying assumption that there is no difference in the smoking rates of the sub-population compared to the whole.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Now suppose I am a researcher investigating a new drug that I believe to be equally effective to an existing standard of treatment, but with fewer side effects and therefore a more desirable safety profile.  I would like to demonstrate the equal efficacy by conducting a &lt;em&gt;bioequivalence&lt;/em&gt; test.  If $\mu_0$ is the mean existing standard treatment effect, then my hypothesis might look like this:  $$H_0 : |\mu - \mu_0| \ge \Delta \quad \mathrm{vs.} \quad H_a : |\mu - \mu_0| &amp;lt; \Delta,$$ for some choice of margin $\Delta$ that I consider to be clinically significant.  For example, a clinician might say that two treatments are sufficiently bioequivalent if there is less than a $\Delta = 10\%$ difference in treatment effect.  Note again that $H_a$ is the statement that carries the burden of proof:  the data we collect must &lt;strong&gt;strongly&lt;/strong&gt; support it, in order for us to accept it; otherwise, &lt;em&gt;it could still be true but we don't have the evidence to support the claim&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Now suppose I am doing an analysis for a small business owner who sells three products $A$, $B$, $C$.  They suspect that there is a statistically significant preference for these three products.  Then my hypothesis is $$H_0 : \mu_A = \mu_B = \mu_C \quad \mathrm{vs.} \quad H_a : \exists i \ne j \text{ such that } \mu_i \ne \mu_j.$$  Really, all that $H_a$ is saying is that there are two means that are not equal to each other, which would then suggest that &lt;em&gt;some&lt;/em&gt; difference in preference exists.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="1" CreationDate="2014-11-09T17:38:40.500" Id="123304" LastActivityDate="2014-11-09T17:38:40.500" OwnerUserId="36771" ParentId="123287" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;The trick is that $\mathbb{E}(\hat{\theta}) - \theta$ is a constant.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-11-09T19:35:35.633" Id="123322" LastActivityDate="2014-11-09T19:35:35.633" OwnerUserId="8013" ParentId="123320" PostTypeId="2" Score="2" />
  
  
  
  <row AcceptedAnswerId="123354" AnswerCount="1" Body="&lt;p&gt;In logistic regression with an intercept term and with at least one dependent variable which is categorical, is there a closed form for the variance of the sum of the intercept and the coefficient of the categorical variable, or do you have to sample from a multivariate distribution with the means and variances of the intercept and the coefficient to get a reliable measure of the variance of this sum?&lt;/p&gt;&#10;&#10;&lt;p&gt;$P(y) = \frac{exp(y)}{1 + exp(y)}$&lt;/p&gt;&#10;&#10;&lt;p&gt;$y = \beta_{0} + \beta_{1}x + \epsilon$&lt;/p&gt;&#10;&#10;&lt;p&gt;where x is categorical.&lt;/p&gt;&#10;&#10;&lt;p&gt;Would the formula for the variance of a sum (of two random variables) be applicable here?&lt;/p&gt;&#10;&#10;&lt;p&gt;$Var(\beta_{0} + \beta_{1}) = Var(\beta_{0}) + Var(\beta_{1}) + 2Cov(\beta_{0}, \beta_{1})$&lt;/p&gt;&#10;&#10;&lt;p&gt;The reason I ask, is that in &lt;a href=&quot;http://stats.stackexchange.com/questions/99633/logistic-regression-strange-standard-errors-from-glm-in-r#comment193235_99662&quot;&gt;this comment&lt;/a&gt; I got the impression that no closed form existed for the variance in question, and the advice was to sample from a multivariate distribution with the means and variances of $\beta_{0}$ and $\beta_{1}$&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(1)&#10;dependent.var &amp;lt;- sample(c(TRUE, FALSE), 100, replace = TRUE, prob = c(0.3, 0.7))&#10;independent.var &amp;lt;- ifelse(dependent.var, sample(c(&quot;Red&quot;, &quot;Blue&quot;), replace = TRUE,&#10;  size = 10, prob = c(0.8, 0.2)), sample(c(&quot;Red&quot;, &quot;Blue&quot;), size = 10,&#10;  replace = TRUE, prob = c(0.4, 0.6)))&#10;table(dependent.var, independent.var)&#10;##               independent.var&#10;## dependent.var Blue Red&#10;##         FALSE   42  26&#10;##         TRUE     7  25&#10;my.fit &amp;lt;- glm(dependent.var ~ 1 + independent.var, family = binomial(logit))&#10;coef(summary(my.fit))&#10;                    Estimate Std. Error   z value     Pr(&amp;gt;|z|)&#10;(Intercept)        -1.791759  0.4082482 -4.388897 0.0000113927&#10;independent.varRed  1.752539  0.4951042  3.539737 0.0004005255&#10;&amp;gt; vcov(my.fit)&#10;                   (Intercept) independent.varRed&#10;(Intercept)          0.1666666         -0.1666666&#10;independent.varRed  -0.1666666          0.2451282&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The logit of TRUE for a &quot;Red&quot; case is $ \beta_{0}+\beta_{1} \approx -0.039$. Is the variance for this estimate exactly&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;vcov(my.fit)[1,1] + vcov(my.fit)[2,2] + 2 * vcov(my.fit)[1,2]&#10;[1] 0.07846154 ?&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Or is this only an approximation, and a more accurate measure is to be found by sampling, e.g.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(MASS)&#10;var(rowSums(mvrnorm(n = 1E7, mu = coef(my.fit), Sigma = vcov(my.fit))))&#10;[1] 0.07842985 ?&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In this simple example, the sampling method does not seem to provide more accurate estimates of the variance (using 1E7 samples).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://pages.iu.edu/~hgevans/p410-p609/material/06_fit/conf_reg.html&quot; rel=&quot;nofollow&quot;&gt;Here&lt;/a&gt; it is stated that &quot;There is a correspondance between the covariance matrix of the fit parameters and Δχ2 confidence regions &lt;em&gt;only&lt;/em&gt; for the case of Gaussian uncertainties on the input measurements.&quot;. Is that a reason against relying on the closed form above, or is there perhaps another reason for the advice to sample instead of deriving the variance analytically in cases like this?&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT: (In response to the answer given by StasK). The advice I originally got was to simulate from the full model, not from the &lt;code&gt;vcov()&lt;/code&gt;, so here is the code to simulate from the full model:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(arm)&#10;sim.i &amp;lt;- sim(my.fit, 100000)&#10;logit.for.TRUE.red &amp;lt;- sim.i@coef[,1] + sim.i@coef[,2]&#10;var(logit.for.TRUE.red)&#10;[1] 0.07781206&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2014-11-10T01:07:59.890" Id="123345" LastActivityDate="2014-11-10T15:19:36.290" LastEditDate="2014-11-10T09:17:42.917" LastEditorUserId="45965" OwnerUserId="45965" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;logistic&gt;&lt;variance&gt;&lt;interaction&gt;" Title="Closed form for the variance of a sum of two estimates in logistic regression?" ViewCount="72" />
  <row AnswerCount="0" Body="&lt;p&gt;So, I have a group of nations that exhibit global spatial dependence. I want to partition the nations into groups so that, at an intuitive level at least, the &quot;sum of the spatial dependence within each group&quot; or some similar measure is maximized. This problem seems similar to local statistics, but I'm not looking for &quot;hot spots&quot;; I want to partition all the nations. It also seems similar to spatial regimes, but its an issue of dependence, not heterogeneity.&lt;/p&gt;&#10;&#10;&lt;p&gt;Anyone have any suggestions?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-11-10T04:38:16.933" Id="123352" LastActivityDate="2014-11-10T04:38:16.933" OwnerUserId="59676" PostTypeId="1" Score="1" Tags="&lt;spatial&gt;&lt;partitioning&gt;&lt;local-statistics&gt;" Title="Partitioning Spatial Dependence?" ViewCount="14" />
  <row AnswerCount="0" Body="&lt;p&gt;Parameter expansion is used in various GLMMs to accelerate e.g. EM or Gibbs convergence. Is anybody aware of a paper/work which implements PX for CPH?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-10T04:41:12.640" Id="123353" LastActivityDate="2014-11-10T04:54:37.590" LastEditDate="2014-11-10T04:54:37.590" LastEditorUserId="7290" OwnerUserId="60355" PostTypeId="1" Score="1" Tags="&lt;references&gt;&lt;cox-model&gt;&lt;glmm&gt;&lt;convergence&gt;" Title="How can parameter expansion be applied to cox proportional hazard models with random effects?" ViewCount="10" />
  <row Body="&lt;p&gt;I am going to change the order of questions about.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;I've found textbooks and lecture notes frequently disagree, and would like a system to work through the choice that can safely be recommended as best practice, and especially a textbook or paper this can be cited to.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Unfortunately, some discussions of this issue in books and so on rely on received wisdom. Sometimes that received wisdom is reasonable, sometimes it is less so (at the least in the sense that it tends to focus on a smaller issue when a larger problem is ignored); we should examine the justifications offered for the advice (if any justification is offered at all) with care.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Most guides to choosing a t-test or non-parametric test focus on the normality issue. &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;That’s true, but it’s largely misguided for several reasons that I address in this answer.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;If performing an &quot;unrelated samples&quot; or &quot;unpaired&quot; t-test, whether to use a Welch correction? &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;This (to use it unless you have reason to think variances should be equal) is the advice of numerous references. I point to some in this answer.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Some people use a hypothesis test for equality of variances, but here it would have low power. Generally I just eyeball whether the sample SDs are &quot;reasonably&quot; close or not (which is somewhat subjective, so there must be a more principled way of doing it) but again, with low n it may well be that the population SDs are rather further apart than the sample ones.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;Is it safer simply to always use the Welch correction for small samples, unless there is some good reason to believe population variances are equal?&#10;  That’s what the advice is. The properties of the tests are affected by the choice based on the assumption test.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Some references on this can be seen &lt;a href=&quot;http://stats.stackexchange.com/a/97120/805&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;http://stats.stackexchange.com/a/796/805&quot;&gt;here&lt;/a&gt;,  though there are more that say similar things.&lt;/p&gt;&#10;&#10;&lt;p&gt;The equal-variances issue has many similar characteristics to the normality issue – people want to test it, advice suggests conditioning choice of tests on the results of tests can adversely affect the results of both kinds of subsequent test – it’s better simply not to assume what you can’t adequately justify (by reasoning about the data, using information from other studies relating to the same variables and so on).&lt;/p&gt;&#10;&#10;&lt;p&gt;However,  there are differences. One is that – at least in terms of distribution of the null-distribution of the test statistic (and hence, its level-robustness) - non-normality is less important in large samples, while the effect of unequal variances under the equal variance assumption doesn’t really go away with large sample size.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;strong&gt;What principled method can be recommended for choosing which is the most appropriate test when the sample size is &quot;small&quot;?&lt;/strong&gt;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;With hypothesis tests, what matters (under some set of conditions) is primarily two things:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;em&gt;What is the actual type I error rate?&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;em&gt;What is the power behaviour like?&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;We also need to keep in mind that if we're comparing two procedures, changing the first will change the second (that is, if they’re not conducted at the same actual significance level, you would expect that higher $\alpha$ is associated with higher power).&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;With these small-sample issues in mind, is there a good - hopefully citable - checklist to work through when deciding between t and non-parametric tests?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I will consider a number of situations in which I’ll make some recommendations, considering both the possibility of non-normality and unequal variances. In every case, take mention of the t-test to imply the Welch-test:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;strong&gt;n medium-large&lt;/strong&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Non-normal (or unknown), likely to have near-equal variance:&lt;/p&gt;&#10;&#10;&lt;p&gt;If the distribution is heavy-tailed, you will generally be better with a Mann-Whitney, though if it’s only slightly heavy, the t-test should do okay. With light-tails the t-test should (often) be preferred.  Permutation tests are a good option. Bootstrap tests are also suitable.&lt;/p&gt;&#10;&#10;&lt;p&gt;Non-normal (or unknown), unequal variance (or variance relationship unknown):&lt;/p&gt;&#10;&#10;&lt;p&gt;If the distribution is heavy-tailed, you will generally be better with a Mann-Whitney &#10;- if inequality of variance is only related to inequality of mean - i.e. if H0 is true the difference in spread should also be absent. GLMs are often a good option, especially if there’s skewness and spread is related to the mean. A permutation test is another option, with a similar caveat as for the rank-based tests. Bootstrap tests are a good possibility here. &lt;/p&gt;&#10;&#10;&lt;p&gt;Zimmerman and Zumbo (1993)$^{[1]}$ suggest a Welch-t-test on the ranks which they say performs better that the Wilcoxon-Mann-Whitney in cases where the variances are unequal.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;strong&gt;n moderately small&lt;/strong&gt;  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;rank tests are reasonable defaults here if you expect non-normality (again with the above caveat). If you have external information about shape or variance, you might consider GLMs . If you expect things not to be too far from normal, t-tests may be fine.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;strong&gt;n very small&lt;/strong&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Because of the problem with getting suitable significance levels, neither permutation tests nor rank tests may be suitable, and at the smallest sizes, a t-test may be the best option (there’s some possibility of slightly robustifying it). However, there’s a good argument for using higher type I error rates with small samples (otherwise you’re letting type II error rates inflate while holding type I error rates constant).&#10;Also see de Winter (2013)$^{[2]}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The advice must be modified somewhat when the distributions are both strongly skewed and very discrete, such as Likert scale items where most of the observations are in one of the end categories. Then the Wilcoxon-Mann-Whitney isn’t necessarily a better choice than the t-test.&lt;/p&gt;&#10;&#10;&lt;p&gt;Simulation can help guide choices further when you have some information about likely circumstances.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;I appreciate this is something of a perennial topic, but most questions concern the questioner's particular data set, sometimes a more general discussion of power, and occasionally what to do if two tests disagree, but I would like a procedure to pick the correct test in the first place!&lt;/p&gt;&#10;  &#10;  &lt;p&gt;The main problem is how hard it is to check the normality assumption in a small data set:&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I disagree; at least partly. I don’t think this is the main problem. It &lt;em&gt;is&lt;/em&gt; difficult to check normality in a small data set, and to some extent that's an important issue, but I don't think it's actually the main issue. That difficulty is secondary to the more basic problem that trying to assess normality as the basis of choosing between tests adversely impacts the properties of the tests you're choosing between.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Any formal test for normality would have low power so violations may well not be detected. (Personally I wouldn't test for this purpose, and I'm clearly not alone, but &#10;  I've found this little use when clients demand a normality test be performed because that's what their textbook or old lecture notes or some website they found once declare should be done. This is one point where a weightier looking citation would be welcome.)&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Here’s an example of a reference (there are others)  which is unequivocal (Fay and Proschan, 2010$^{[3]}$):&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;em&gt;The choice between t- and WMW DRs should not be based on a test of normality.&lt;/em&gt;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;They are similarly unequivocal about not testing for equality of variance.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;To make matters worse, it is unsafe to use the Central Limit Theorem as a safety net: for small n we can't rely on the convenient asymptotic normality of the test statistic and t distribution.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Nor even in large samples -- asymptotic normality of the numerator doesn’t imply that the t-statistic will have a t-distribution. However, that may not matter so much, since  you should still have asymptotic normality (e.g. CLT for the numerator, and Slutsky’s theorem suggest that eventually the t-statistic should begin to look normal, if the conditions for both hold.)&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;One principled response to this is &quot;safety first&quot;: as there's no way to reliably verify the normality assumption on a small sample, run an equivalent non-parametric test instead. &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;That’s actually the advice that the references I mention (or link to mentions of) give.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Another approach I've seen but feel less comfortable with, is to perform a visual check and proceed with a t-test if nothing untowards is observed (&quot;no reason to reject normality&quot;, ignoring the low power of this check). My personal inclination is to consider whether there are any grounds for assuming normality, theoretical (e.g. variable is sum of several random components and CLT applies) or empirical (e.g. previous studies with larger n suggest variable is normal).&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Both those are good arguments, especially when backed up with the fact that the t-test is reasonably robust against moderate deviations from normality&lt;/p&gt;&#10;&#10;&lt;p&gt;Beware, however, the phrasing &quot;suggest the variable is normal&quot;. Being reasonably consistent with normality is not the same thing as normality. We can often reject actual normality with no need even to see the data – for example, if the data cannot be negative, the distribution cannot be normal. Fortunately, what matters is closer to what we might actually have from previous studies or reasoning about how the data are composed, which is that the deviations from normality should be small. &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;If so, I would use a t-test if data passed visual inspection, and otherwise stick to non-parametrics. But any theoretical or empirical grounds usually only justify assuming approximate normality, and on low degrees of freedom it's hard to judge how near normal it needs to be to avoid invalidating a t-test.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Well, that’s something we can assess the impact of fairly readily (such as via simulations, as I mentioned earlier). From what I've seen, skewness seems to matter more than heavy tails (but on the other hand I have seen some claims of the opposite - though I don't know what that's based on).&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;For people who see the choice of methods as a trade-off between power and robustness, claims about the asymptotic efficiency of the non-parametric methods are unhelpful. For instance, the rule of thumb that &quot;Wilcoxon tests have about 95% of the power of a t-test if the data really are normal, and are often far more powerful if the data is not, so just use a Wilcoxon&quot; is sometimes heard, but if the 95% only applies to large n, this is flawed reasoning for smaller samples.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;But we can check small-sample power quite easily! It’s easy enough to simulate to obtain power curves &lt;a href=&quot;http://stats.stackexchange.com/questions/71953/relative-efficiency-of-wilcoxon-signed-rank-in-small-samples/72029#72029&quot;&gt;as here&lt;/a&gt;.&lt;br&gt;&#10;(Again, also see de Winter (2013)$^{[2]}$).&lt;/p&gt;&#10;&#10;&lt;p&gt;Having done such simulations under a variety of circumstances, both for the two-sample and one-sample/paired-difference cases, the small sample efficiency at the normal in both cases seems to be a little lower* than the asymptotic efficiency, but the efficiency of the signed rank and Wilcoxon-Mann-Whitney tests is still very high even at very small sample sizes.&lt;/p&gt;&#10;&#10;&lt;p&gt;* if the tests are done at the same actual significance level; you can't do a 5% test with very small samples (and least not without randomized tests for example), but if you're prepared to perhaps do (say) a 5.5% or a 3.2% test instead, then the rank tests hold up very well indeed compared with a t-test at that significance level.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Small samples may make it very difficult, or impossible, to assess whether a transformation is appropriate for the data since it's hard to tell whether the transformed data belong to a (sufficiently) normal distribution. So if a QQ plot reveals very positively skewed data, which look more reasonable after taking logs, is it safe to use a t-test on the logged data? On larger samples this would be very tempting, but with small n I'd probably hold off unless there had been grounds to expect a log-normal distribution in the first place.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;There’s another alternative: make a different parametric assumption. For example, if there’s skewed data, one might, for example, in some situations reasonably consider a gamma distribution, or some other skewed family as a better approximation - in moderately large samples, we might just use a GLM, but in very small samples it may be necessary to look to a small-sample test - in many cases simulation can be useful. &lt;/p&gt;&#10;&#10;&lt;p&gt;Alternative 2: robustify the t-test (but taking care about the choice of robust procedure so as not to discretize the resulting distribution of the test statistic)  -  this has some advantages over a very-small-sample nonparametric procedure such as the ability to consider tests with low type I error rate.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here I'm thinking along the lines of using say M-estimators of location (and related estimators of scale) in the t-statistic to smoothly robustify against deviations from normality. Something akin to the Welch, like:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac{\stackrel{\sim}{x}-\stackrel{\sim}{y}}{\stackrel{\sim}{S}_p}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\stackrel{\sim}{S}_p^2=\frac{\stackrel{\sim}{s}_x^2}{n_x}+\frac{\stackrel{\sim}{s}_y^2}{n_y}$ and $\stackrel{\sim}{x}$, $\stackrel{\sim}{s}_x$ etc being robust estimates of location and scale respectively.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'd aim to reduce any tendency of the statistic to discreteness - so I'd avoid things like trimming and Winsorizing, since if the original data were discrete, trimming etc will exacerbate this; by using M-estimation type approaches with a smooth $\psi$-function you achieve similar effects without contributing to the discreteness. Keep in mind we're trying to deal with the situation where $n$ is very small indeed (around 3-5, in each sample, say), so even M-estimation potentially has its issues. &lt;/p&gt;&#10;&#10;&lt;p&gt;You could, for example, use simulation at the normal to get p-values (if sample sizes are very small, I'd suggest that over bootstrapping - if sample sizes aren't so small, a carefully-implemented bootstrap may do quite well, but then we might as well go back to Wilcoxon-Mann-Whitney). There's be a scaling factor as well as a d.f. adjustment to get to what I'd imagine would then be a reasonable t-approximation. This means we should get the kind of properties we seek very close to the normal, and should have reasonable robustness in the broad vicinity of the normal. There are a number of issues that come up that would be outside the scope of the present question, but I think in very small samples the benefits can far outweigh the costs and the extra effort required.&lt;/p&gt;&#10;&#10;&lt;p&gt;Of course if you didn't expect the distribution to be somewhat normal-like, you could undertake a similar robustification of a different parametric test.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;What if you want to check assumptions for the non-parametrics? Some sources recommend verifying a symmetric distribution before applying a Wilcoxon test, which brings up similar problems to checking normality. &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Indeed. I assume you mean the signed rank test*. In the case of using it on paired data, if you are prepared to assume that the two distributions are the same shape apart from location shift you are safe, since the differences should then be symmetric.&lt;/p&gt;&#10;&#10;&lt;p&gt;*(Wilcoxon’s name is  associated with both the one and two sample rank tests – signed rank and rank sum; with their U test, Mann and Whitney generalized the situation studied by Wilcoxon, and introduced important new ideas for evaluating the null distribution, but the &lt;em&gt;priority&lt;/em&gt; on the Wilcoxon-Mann-Whitney is clearly Wilcoxon’s$^\dagger$)&lt;/p&gt;&#10;&#10;&lt;p&gt;$\dagger$ -- or at least if we only consider Wilcoxon vs Mann&amp;amp;Whitney. It seems &lt;a href=&quot;http://en.wikipedia.org/wiki/Stigler%27s_law_of_eponymy&quot; rel=&quot;nofollow&quot;&gt;Stigler's Law&lt;/a&gt; beats me yet again, and Wilcoxon should perhaps share some of that priority with a number of earlier contributors, and (besides Mann and Whitney) should share credit with several discoverers of an equivalent test around the same time as the two well-known ones.[4][5]&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;[1]: Zimmerman DW and Zumbo BN, (1993),&lt;br&gt;&#10;&lt;em&gt;Rank transformations and the power of the Student t-test and Welch t′-test for non-normal populations,&lt;/em&gt;&lt;br&gt;&#10;Canadian Journal Experimental Psychology, &lt;strong&gt;47&lt;/strong&gt;: 523–39.&lt;/p&gt;&#10;&#10;&lt;p&gt;[2]: J.C.F. de Winter (2013),&lt;br&gt;&#10;&quot;Using the Student’s t-test with extremely small sample sizes,&quot;&lt;br&gt;&#10;&lt;em&gt;Practical Assessment, Research and Evaluation&lt;/em&gt;,  &lt;strong&gt;18&lt;/strong&gt;:10, August, ISSN 1531-7714&lt;br&gt;&#10;&lt;a href=&quot;http://pareonline.net/getvn.asp?v=18&amp;amp;n=10&quot; rel=&quot;nofollow&quot;&gt;http://pareonline.net/getvn.asp?v=18&amp;amp;n=10&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;[3]: Michael P. Fay and Michael A. Proschan (2010),&lt;br&gt;&#10;&quot;Wilcoxon-Mann-Whitney or t-test? On assumptions for hypothesis tests and multiple interpretations of decision rules,&quot;&lt;br&gt;&#10;&lt;em&gt;Stat Surv&lt;/em&gt;; &lt;strong&gt;4&lt;/strong&gt;: 1–39.&lt;br&gt;&#10;&lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2857732/&quot; rel=&quot;nofollow&quot;&gt;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2857732/&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;[4]: Berry, K.J., Mielke, P.W. and Johnston, J.E. (2012),&lt;br&gt;&#10;&quot;The Two-sample Rank-sum Test: Early Development,&quot;&lt;br&gt;&#10;&lt;em&gt;Electronic Journal for History of Probability and Statistics&lt;/em&gt;, Vol.8, December&lt;br&gt;&#10;&lt;a href=&quot;http://www.jehps.net/decembre2012/BerryMielkeJohnston.pdf&quot; rel=&quot;nofollow&quot;&gt;pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;[5]: Kruskal, W. H. (1957),&lt;br&gt;&#10;&quot;Historical notes on the Wilcoxon unpaired two-sample test,&quot;&lt;br&gt;&#10;&lt;em&gt;Journal of the American Statistical Association&lt;/em&gt;, &lt;strong&gt;52&lt;/strong&gt;, 356–360.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-11-10T11:59:23.353" Id="123389" LastActivityDate="2015-01-28T05:26:40.377" LastEditDate="2015-01-28T05:26:40.377" LastEditorUserId="805" OwnerUserId="805" ParentId="121852" PostTypeId="2" Score="11" />
  
  <row Body="&lt;p&gt;The answer was using the pcls function in the mgcv-package to alter the coefficients of the first fit to match the side constraints. Ignore the linear equality constraints and only apply inequality constraints. (The following example closely follows the one in the help to the pcls function.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;require(mgcv)&#10;x&amp;lt;-seq(0,2*pi,l=100)&#10;y&amp;lt;-sin(x)+rnorm(length(x),sd=0.1)&#10;m31&amp;lt;-gam(y~s(x),fit=FALSE)&#10;m32&amp;lt;-gam(G=m31)&#10;m33&amp;lt;-m32&#10;M&amp;lt;-model.matrix(m32)&#10;m31$Ain&amp;lt;-M&#10;m31$bin&amp;lt;-c(rep(0,nrow(M)))&#10;m31$sp&amp;lt;-m32$sp&#10;m31$off&amp;lt;-m31$off-1&#10;m31$p&amp;lt;-c(1,rep(0,ncol(M)-1))&#10;m31$C&amp;lt;-matrix(NA,0,0)&#10;p&amp;lt;-pcls(m31)&#10;m32$coefficients&amp;lt;-p&#10;plot(x,y)&#10;lines(x,predict(m32,type=&quot;response&quot;,newdata=data.frame(x=x)),col=2)&#10;lines(x,predict(m33,type=&quot;response&quot;,newdata=data.frame(x=x)),col=4)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-11-10T12:32:47.220" Id="123394" LastActivityDate="2014-11-10T12:32:47.220" OwnerUserId="35764" ParentId="109822" PostTypeId="2" Score="0" />
  
  
  
  
  <row Body="&lt;p&gt;It sounds like you want a system that, given some training data about a person's likes and dislikes, is able to predict whether the person will like something.  That problem is known as &lt;a href=&quot;https://en.wikipedia.org/wiki/Statistical_classification&quot; rel=&quot;nofollow&quot;&gt;classification&lt;/a&gt;.  The basic idea is to collect a data table where each row is an article of clothing, the columns describe the article via numerical or discrete attributes (such as 'color'), and one column gives the label ('like' or 'dislike').  Then you can apply an algorithm such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Logistic_regression&quot; rel=&quot;nofollow&quot;&gt;logistic regression&lt;/a&gt; to learn a classifier from this data.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-10T18:14:46.930" Id="123447" LastActivityDate="2014-11-10T18:14:46.930" OwnerUserId="2074" ParentId="122316" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;I saw the question from a different angle, correct me if I am off: When distributional assumptions are met, parametric tests, including ANOVA, are more powerful (or sensitive) than their non-parametric counterparts. Furthermore, ANOVA models can incorporate multiple factors and their interactions, repeated measures designs, random effects in mixed models etc. These advantages hold for any sufficient sample size, but designs with small sample sizes are less likely to have the degrees of freedom necessary for the abovementioned more complex models.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-10T21:49:02.047" Id="123472" LastActivityDate="2014-11-10T21:49:02.047" OwnerUserId="57390" ParentId="123455" PostTypeId="2" Score="0" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Experts argue that at most 30% of schools pupils benefit from homework. A sample is taken. Write down null and alternative hypotheses.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would have thought that the null would be that the number of school pupils benefiting from homework is greater than 30%, but I've been told that the null must include equality.&lt;/p&gt;&#10;&#10;&lt;p&gt;So what is the truth?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-11-10T23:59:14.190" Id="123486" LastActivityDate="2014-11-10T23:59:14.190" OwnerUserId="41558" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;&lt;self-study&gt;" Title="null hypothesis confusion" ViewCount="22" />
  
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;EDIT: SOLVED &lt;em&gt;The problem seems to have been an explanatory variable that was a factor. If it is made binary numeric insted, the values of BIC and AIC is calculated alright. However, the analyses give the same parameter estimates such as z-values, variances, p-values. I leave the original post as a reference.&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Original post:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I have some problems calculating AIC or BIC values for a structural equation model (SEM) fitted with Lavaan. (I fail to find anything when searching the web for the error message or parts of it.)&lt;/p&gt;&#10;&#10;&lt;p&gt;My model:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;model&amp;lt;- '&#10;        dependant ~ var1 + var2 + var3&#10;        var1 ~ var2&#10;        var3 ~ var1 + var2&#10;        '&#10;semla&amp;lt;-sem(model, data=dat, missing=&quot;listwise&quot;&#10;summary(semla)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;With the result:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lavaan (0.5-16) converged normally after  38 iterations&#10;&#10;                                              Used       Total&#10;Number of observations                            47          59&#10;&#10;Estimator                                         ML&#10;Minimum Function Test Statistic                0.000&#10;Degrees of freedom                                 0&#10;P-value (Chi-square)                           0.000&#10;&#10;Parameter estimates:&#10;&#10;Information                                 Expected&#10;Standard Errors                             Standard&#10;&#10;               Estimate  Std.err  Z-value  P(&amp;gt;|z|)&#10;Regressions:&#10;dependant ~&#10; var1            0.121    0.071    1.687    0.092&#10; var2            0.015    0.006    2.750    0.006&#10; var3            1.721    2.134    0.807    0.420&#10;var1 ~&#10; var2           -0.018    0.011   -1.663    0.096&#10;var3 ~&#10; var1           -0.003    0.005   -0.685    0.493&#10; var2            0.000    0.000    0.450    0.653&#10;&#10;Variances:&#10; dependant       0.049    0.010&#10; var1            0.205    0.042&#10; var3            0.000    0.000&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Howvere, when i do &lt;code&gt;AIC(semla)&lt;/code&gt; or &lt;code&gt;BIC(semla)&lt;/code&gt;i get this error message:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Error in fitMeasures(object, c(&quot;logl&quot;, &quot;npar&quot;, &quot;ntotal&quot;)) : &#10;object 'logl.H0' not found` &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Unfortunately I am not able to figure out what is wrong. As I have tried to simplify my model in several steps and the error persists it seems to be something fundamental. As I mentioned before, I'm not able to get any information by googling for the error message or part of it.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any help to solve the matter would be much appreciated.&lt;/p&gt;&#10;&#10;&lt;p&gt;(I have also posted this question on &lt;em&gt;Stack overflow&lt;/em&gt;, but realised that &lt;em&gt;Cross validated&lt;/em&gt; might be a more suitable place to ask, please feel free to express your opinions on the matter. &lt;a href=&quot;http://stackoverflow.com/q/26844510/3489824&quot;&gt;http://stackoverflow.com/q/26844510/3489824&lt;/a&gt;)&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-11-11T07:26:13.550" Id="123511" LastActivityDate="2014-11-18T14:20:18.300" LastEditDate="2014-11-18T14:20:18.300" LastEditorUserId="60430" OwnerUserId="60430" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;aic&gt;&lt;sem&gt;" Title="“object 'logl.H0' not found” - Error in fitMeasures when calculating AIC for Lavaan model" ViewCount="28" />
  <row AnswerCount="0" Body="&lt;p&gt;I calculated the sample mean, and it came out to be 100. The standard deviation given in the problem is 70. And the 95% confidence interval of $\mu$ I obtained is (50, 140). So if I know that 70 is the actual standard deviation, why is my CI not reliable? Is it because if I add or subtract one SD from the mean, I will end up outside of my CI?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-11-11T08:24:21.507" Id="123513" LastActivityDate="2014-11-11T08:24:21.507" OwnerUserId="45971" PostTypeId="1" Score="0" Tags="&lt;confidence-interval&gt;" Title="Question about confidence interval and its relationship to standard deviation" ViewCount="21" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am wondering how to analyze this scenario using non-linear mixed effects. I am a working at the novice level, and will have access to the Pinheiro and Bates book through my university library soon.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a data set measuring continuous age vs. a continuous response variable for a collection of study participants with a particular disorder. A genetic trait divides the population of subjects into two groups. I would like to try to detect differences in the response over time between the two genetic groups. The model function is non-linear and resembles basic logistic progression.&lt;/p&gt;&#10;&#10;&lt;p&gt;Which approach seems more appropriate:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Model the two divisions separately (i.e. two calls to nlme), which provides different fixed effects for the genetic trait and random effects at the individual subject level.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Model the whole population and specify random effects as genetic_trait/subject which provides population-level fixed effects, and random effects for genetic trait and subjects with particular genetic trait.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Approach 1 seems to assume that we know the genetic-trait populations will behave differently, while approach 2 is asking the modeling to tell us if they behave differently. Does that seem accurate?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-11T13:30:06.987" Id="123547" LastActivityDate="2014-11-11T13:30:06.987" OwnerUserId="13764" PostTypeId="1" Score="0" Tags="&lt;modeling&gt;&lt;nlme&gt;" Title="Need advice on how to model this scenario in NLME" ViewCount="12" />
  <row Body="&lt;p&gt;How about:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;poisson.test(c(n1, n2), c(t1, t2), alternative = c(&quot;two.sided&quot;))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This is a test which compares the Poisson rates of 1 and 2 with each other, and gives both a p value and a 95% confidence interval.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-11T13:41:53.107" Id="123548" LastActivityDate="2014-11-11T13:41:53.107" OwnerUserId="59795" ParentId="9561" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;What you need consider is &quot;standard errors&quot; around your estimates. This is essentially the level of noise around your measurement (signal). Standard errors become smaller with larger sample size and smaller standard deviation in the data (less heterogeneity). &lt;/p&gt;&#10;&#10;&lt;p&gt;In the case of % (0/1), you can come up with a conservative standard error by calculating sqrt(0.5*0.5/sample size). Based on this, you can calculate margin of error as follows.  &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Margin of error at 99% confidence ~ 1.29/sqrt(n)&lt;/li&gt;&#10;&lt;li&gt;Margin of error at 95% confidence ~ 0.98/sqrt(n)&lt;/li&gt;&#10;&lt;li&gt;Margin of error at 90% confidence ~ 0.82/sqrt(n)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Minha Hwang&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-11T15:49:35.707" Id="123565" LastActivityDate="2014-11-11T15:49:35.707" OwnerUserId="60458" ParentId="99502" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="123577" AnswerCount="2" Body="&lt;p&gt;I need to create a Multiple Linear regression model on those data explaining max03 T9 T12 T15 Ne9 Ne12 Ne15 Vx9 Vx12 Vx15 maxO3v&lt;/p&gt;&#10;&#10;&lt;p&gt;!My data &lt;a href=&quot;http://i.stack.imgur.com/mGU6I.png&quot; rel=&quot;nofollow&quot;&gt;1&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;My first intuition was to make a backward selection : &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;attach(ozone)&#10;res &amp;lt;- lm(maxO3~T9+T12+T15+Ne9+Ne12+Ne15+Vx9+Vx12+Vx15+maxO3v)&#10;shapiro.test(res$residuals)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;data:  res$residuals&lt;/p&gt;&#10;&#10;&lt;p&gt;W = 0.9682, p-value = 0.008945&lt;/p&gt;&#10;&#10;&lt;p&gt;But the first full model return non-normal residuals.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is it okay to continue doing a backward selection (AIC criterion)?&lt;/p&gt;&#10;&#10;&lt;p&gt;I don't think it [non-normal residuals] has an impact on that sort of selection, but I can't find a definite answer to that question.&lt;/p&gt;&#10;&#10;&lt;p&gt;If I keep doing the backward selection process&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;[...]&#10;res &amp;lt;- lm(maxO3~T12+Ne9+Vx15+maxO3v)&#10;drop1(res)&#10;&#10;summary(res)&#10;shapiro.test(res$residuals)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Shapiro-Wilk normality test&lt;/p&gt;&#10;&#10;&lt;p&gt;data:  res$residuals&#10;W = 0.9622, p-value = 0.002946&lt;/p&gt;&#10;&#10;&lt;p&gt;My residuals aren't normal at the end ...&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-11T15:59:30.817" FavoriteCount="1" Id="123568" LastActivityDate="2014-11-11T20:37:30.970" LastEditDate="2014-11-11T16:11:53.930" LastEditorUserId="48360" OwnerUserId="48360" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;multiple-regression&gt;&lt;model-selection&gt;&lt;aic&gt;" Title="Multiple linear regression, backward selection : Normality of the residuals?" ViewCount="60" />
  <row AcceptedAnswerId="123596" AnswerCount="1" Body="&lt;p&gt;Assume that we have a very long sequence (&lt;em&gt;i.e.&lt;/em&gt; a list) of nominal-valued observations. For example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;A&#10;A&#10;C&#10;B&#10;...&#10;B&#10;A&#10;B&#10;C&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;We have also a corresponding sequence of predictions generated by a &quot;predictor_1&quot;. Each prediction is given by the probabilities associated with the possible nominal observations. For example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;{A:0.5, B:0.1, C:0.4}&#10;{A:0.6, B:0.0, C:0.4}&#10;{A:0.7, B:0.2, C:0.1}&#10;{A:0.2, B:0.2, C:0.6}&#10;....&#10;{A:0.2, B:0.2, C:0.6}&#10;{A:0.7, B:0.2, C:0.1}&#10;{A:0.5, B:0.1, C:0.4}&#10;{A:0.6, B:0.0, C:0.4}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;We also have another sequence of predictions (in the same format), generated by another predictor. For example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;{A:0.4, B:0.1, C:0.5}&#10;{A:0.4, B:0.0, C:0.6}&#10;{A:0.1, B:0.2, C:0.7}&#10;{A:0.1, B:0.3, C:0.6}&#10;....&#10;{A:0.5, B:0.1, C:0.4}&#10;{A:0.2, B:0.2, C:0.6}&#10;{A:0.7, B:0.2, C:0.1}&#10;{A:0.6, B:0.0, C:0.4}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now, I want to determine what predictor is better. Or, in other words what predictions are &quot;more informative&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;The question is much more simple if our &quot;predictors&quot; give just one nominal value as a prediction (for example a predictor says that on the first step we expect to observe &lt;code&gt;A&lt;/code&gt; then, on the second step, we expect &lt;code&gt;B&lt;/code&gt; and so on). In this case we can use the percentage coincidences of predictions with the observations as a measure of the quality. But it is not clear to me what we should use in case of the above described &quot;probabilistic&quot; predictions. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-11T16:05:08.330" Id="123571" LastActivityDate="2014-11-11T19:49:06.877" LastEditDate="2014-11-11T19:49:06.877" LastEditorUserId="37428" OwnerUserId="2407" PostTypeId="1" Score="0" Tags="&lt;probability&gt;&lt;classification&gt;&lt;predictive-models&gt;&lt;accuracy&gt;" Title="How to compare probabilistic classifiers?" ViewCount="48" />
  
  
  <row Body="&lt;p&gt;Additional sources in favor of MOM:&lt;/p&gt;&#10;&#10;&lt;p&gt;Hong, H. P., and W. Ye. 2014. &lt;a href=&quot;http://link.springer.com/article/10.1007%2Fs11069-014-1073-z&quot; rel=&quot;nofollow&quot;&gt;Analysis of extreme ground snow loads for Canada using snow depth records&lt;/a&gt;. Natural Hazards 73 (2):355-371.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The use of&#10;  MML could give unrealistic predictions if the sample size is small (Hosking et al. 1985;&#10;  Martin and Stedinger 2000).&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Martins, E. S., and J. R. Stedinger. 2000. &lt;a href=&quot;http://onlinelibrary.wiley.com/doi/10.1029/1999WR900330/abstract&quot; rel=&quot;nofollow&quot;&gt;Generalized maximum-likelihood generalized extreme-value quantile estimators for hydrologic data&lt;/a&gt;. Water Resources Research 36 (3):737-744.&lt;/p&gt;&#10;&#10;&lt;p&gt;Abstract:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The three-parameter generalized extreme-value (GEV) distribution has found wide application for describing annual floods, rainfall, wind speeds, wave heights, snow depths, and other maxima. Previous studies show that small-sample maximum-likelihood estimators (MLE) of parameters are unstable and recommend L moment estimators. More recent research shows that method of moments quantile estimators have for −0.25 &amp;lt; κ &amp;lt; 0.30 smaller root-mean-square error than L moments and MLEs. Examination of the behavior of MLEs in small samples demonstrates that absurd values of the GEV-shape parameter κ can be generated. Use of a Bayesian prior distribution to restrict κ values to a statistically/physically reasonable range in a generalized maximum likelihood (GML) analysis eliminates this problem. In our examples the GML estimator did substantially better than moment and L moment quantile estimators for − 0.4 ≤ κ ≤ 0.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;In the Introduction and Literature Review sections they cite additional papers which concluded that MOM in some cases outperform MLE (again extreme value modelling), e.g.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Hosking et al. [1985a] show that small-sample MLE parameter&#10;  estimators are very unstable and recommend probability-weighted moment (PWM) estimators&#10;  which are equivalent to L moment estimators [Hosking, 1990].&#10;  [...]&lt;/p&gt;&#10;  &#10;  &lt;p&gt;Hosking et al. [1985a] showed that the probability-weighted moments (PM) or equivalent L moments(LM) estimators for the GEV distribution are better&#10;  than maximum-likelihood estimators (MLE) in terms of bias&#10;  and variance for sample sizes varying from 15 to 100. More&#10;  recently, Madsen et al. [1997a] showed that the method of&#10;  moments (MOM) quantile estimators have smaller RMSE&#10;  (root-mean-squareer ror) for -0.25 &amp;lt; K &amp;lt; 0.30 than LM and&#10;  MLE when estimating the 100-year event for sample sizes of&#10;  10-50. MLEs are preferable only when K &gt; 0.3 and the sample&#10;  sizes are modest (n &gt;= 50).&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;K (kappa) is the shape parameter of GEV.&lt;/p&gt;&#10;&#10;&lt;p&gt;papers which appear in the quotes:&lt;/p&gt;&#10;&#10;&lt;p&gt;Hosking J, Wallis J, Wood E (1985) &lt;a href=&quot;http://www.jstor.org/discover/10.2307/1269706?uid=3737856&amp;amp;uid=2&amp;amp;uid=4&amp;amp;sid=21104509571771&quot; rel=&quot;nofollow&quot;&gt;Estimation of the generalized extreme-value distribution by the method of probability-weighted moments&lt;/a&gt;. Technometrics 27:251–261.&lt;/p&gt;&#10;&#10;&lt;p&gt;Madsen, H., P. F. Rasmussen and D. Rosbjerg (1997) &lt;a href=&quot;http://onlinelibrary.wiley.com/doi/10.1029/96WR03848/abstract&quot; rel=&quot;nofollow&quot;&gt;Comparison of annual&#10;maximum series and partial duration series methods for modeling&#10;extreme hydrologic events&lt;/a&gt;, 1, At-site modeling, Water Resour. Res.,&#10;33(4), 747-758.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hosking, J. R. M., &lt;a href=&quot;http://www.jstor.org/discover/10.2307/2345653?uid=3737856&amp;amp;uid=2&amp;amp;uid=4&amp;amp;sid=21104509571771&quot; rel=&quot;nofollow&quot;&gt;L-moments: Analysis and estimation of distributions&#10;using linear combinations of order statistics&lt;/a&gt;, J. R. Stat. Soc., Ser. B,&#10;52, 105-124, 1990.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Additionally, I have the same experience as concluded in the above papers, in case of modeling extreme events with small and moderate sample size (&amp;lt;50-100 which is typical) MLE can give unrealistic results, simulation shows that MOM is more robust and has smaller RMSE. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-11-11T17:28:24.047" Id="123583" LastActivityDate="2014-11-11T17:28:24.047" OwnerUserId="58887" ParentId="80380" PostTypeId="2" Score="3" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;&lt;strong&gt;EDITED FOR CLARITY&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Hello,&lt;/p&gt;&#10;&#10;&lt;p&gt;Considering the picture below&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/nFELQ.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;each values X could be identified by the indeces &lt;code&gt;X_g_s_d_h&lt;/code&gt; &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;g = group g=[1:5]&#10;s = subject number (variable for each g)&#10;d = day number (variable for each s)&#10;h = hour h=[1:24]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;so X_1_3_4_12 means that the value X is referred to the &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;12th hour &#10;of 4th day&#10;of 3rd subject&#10;of group 1&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;First I calculate the mean (hour by hour) over all the days of each subject. Doing that the index d disappear and each subject is represented by a vector containing 24 values.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;X_g_s_h&lt;/code&gt; will be the mean over the days of a subject.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then I calculate the mean (subject by subject) of all the subjects belonging to the same group resulting in &lt;code&gt;X_g_h&lt;/code&gt;. Each group is represented by 1 vector of 24 values&lt;/p&gt;&#10;&#10;&lt;p&gt;Then I calculate the mean over the hours for each group resulting in  &lt;code&gt;X_g&lt;/code&gt;. Each group now is represented by 1 single value&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to see if the means &lt;code&gt;X_g&lt;/code&gt; are significantly different between the groups.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can you tell me what is the proper way?&lt;/p&gt;&#10;&#10;&lt;p&gt;ps&lt;/p&gt;&#10;&#10;&lt;p&gt;The number of subjects per group is different and it is also different the number of days for each subject. I have more than 2 groups&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-11-11T18:15:16.443" Id="123591" LastActivityDate="2014-11-12T10:23:25.527" LastEditDate="2014-11-12T10:23:25.527" LastEditorUserId="35660" OwnerUserId="35660" PostTypeId="1" Score="1" Tags="&lt;statistical-significance&gt;&lt;mean&gt;&lt;confidence&gt;" Title="calculate statistical value of the mean of a mean of a mean" ViewCount="40" />
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;I find this contradictory because they said that a sample mean averages together all values in the sample, but a population mean also averages together all the values.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;The excerpt never says anything about the population mean.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;since the population has the widest range doesn't the same thing apply even more?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Absolutely.  If you took the mean of the entire population then it would have even less variability.  But that has nothing to do with what the excerpt is talking about.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;So, why is the sample mean's standard deviation less than the population?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;This is explained in the exerpt.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-11-11T18:31:08.000" Id="123592" LastActivityDate="2014-11-12T08:44:44.860" LastEditDate="2014-11-12T08:44:44.860" LastEditorUserId="2074" OwnerUserId="2074" ParentId="123590" PostTypeId="2" Score="2" />
  <row AnswerCount="0" Body="&lt;p&gt;I Just need your valuable suggestions.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;                                         A        B         C         D&#10;hsa-miR-199a-3p, hsa-miR-199b-3p         NA   13.13892  5.533703  25.67405&#10;hsa-miR-365a-3p, hsa-miR-365b-3p   15.70536   52.86558 18.467540 223.51424&#10;hsa-miR-3689a-5p, hsa-miR-3689b-5p       NA   21.41597  5.964772        NA&#10;hsa-miR-3689b-3p, hsa-miR-3689c     9.58696   44.56490 10.102051  13.26785&#10;hsa-miR-4520a-5p, hsa-miR-4520b-5p 18.06865   28.06991        NA        NA&#10;hsa-miR-516b-3p, hsa-miR-516a-3p         NA   10.77471  8.039662        NA&#10;                                          E      &#10;hsa-miR-199a-3p, hsa-miR-199b-3p         NA&#10;hsa-miR-365a-3p, hsa-miR-365b-3p   31.93503&#10;hsa-miR-3689a-5p, hsa-miR-3689b-5p 24.26073&#10;hsa-miR-3689b-3p, hsa-miR-3689c          NA&#10;hsa-miR-4520a-5p, hsa-miR-4520b-5p       NA&#10;hsa-miR-516b-3p, hsa-miR-516a-3p         NA&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This is how my data frame look like. This is Back ground Subtraction values from 5 samples of micro array data.  &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;A is parent sample.&lt;/li&gt;&#10;&lt;li&gt;B is Resistant to specific drugs which will be applied with different combinations to C D E &lt;/li&gt;&#10;&lt;li&gt;C D E they are treatments &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I have no  duplicates of miRNAs in 5 samples  so instead of writing miRNAs names for every sample I just them once. So 5 samples have 2019 rows and and each row represents miRNAs but the values of samples in front of that miRNAs different for each sample. They are expression values.&lt;/p&gt;&#10;&#10;&lt;p&gt;For Anova I reshaped my data frame into:  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;reshape package&#10;melt function &#10; head(m)&#10;                                 MiRNAs                Group    value&#10;1                  hsa-miR-199a-3p, hsa-miR-199b-3p     A       NA&#10;2                  hsa-miR-365a-3p, hsa-miR-365b-3p     A 15.70536&#10;3 hsa-miR-3689a-5p, hsa-miR-3689b-5p, hsa-miR-3689e     A       NA&#10;4                   hsa-miR-3689b-3p, hsa-miR-3689c     A  9.58696&#10;5                hsa-miR-4520a-5p, hsa-miR-4520b-5p     A 18.06865&#10;6                  hsa-miR-516b-3p, hsa-miR-516a-3p     A       NA&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;2019 miRNAs for A 2019 miRNAs B and so on. By using:  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ANOVA1&amp;lt;-aov(m$value~m$Group) and they TukeyHSD &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I got following results:  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;TukeyHSD(ANOVA1)&#10;  Tukey multiple comparisons of means&#10;    95% family-wise confidence level&#10;&#10;Fit: aov(formula = m$value ~ m$Group)&#10;&#10;$`m$Group`&#10;          diff        lwr       upr     p adj&#10;B-A   73.87304  -88.20262 235.94869 0.7256734&#10;C-A  -25.55832 -196.36413 145.24749 0.9941714&#10;D-A  203.80312   20.26110 387.34514 0.0207431&#10;E-A   41.04993 -159.09661 241.19648 0.9807637&#10;C-B  -99.43136 -258.28853  59.42581 0.4290920&#10;D-B  129.93008  -42.54789 302.40805 0.2398572&#10;E-B  -32.82310 -222.87472 157.22851 0.9899165&#10;D-C  229.36144   48.65517 410.06771 0.0048776&#10;E-C   66.60826 -130.94103 264.15755 0.8892989&#10;E-D -162.75319 -371.41264  45.90627 0.2081150&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;My Question is, do I need to perform ANOVA with Control Vs treatment? or the way I performed is correct? Is there any way I can extract relevant miRNAs which played important role in differential expression in Treatment samples (B C D E)? How I can perform  Principal Component Regression for this data?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-11T20:04:36.443" Id="123604" LastActivityDate="2014-11-11T20:13:38.020" LastEditDate="2014-11-11T20:13:38.020" LastEditorUserId="7290" OwnerUserId="60474" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;regression&gt;&lt;machine-learning&gt;&lt;anova&gt;" Title="ANOVA and Principal Component Regression" ViewCount="21" />
  <row Body="&lt;p&gt;If you are looking for an 'exact' test for two binomial proportions, I believe you are looking for &lt;a href=&quot;http://en.wikipedia.org/wiki/Fisher%27s_exact_test&quot; rel=&quot;nofollow&quot;&gt;Fisher's Exact Test&lt;/a&gt;.  In R it is applied like so:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; fisher.test(matrix(c(17, 25-17, 8, 20-8), ncol=2))&#10;    Fisher's Exact Test for Count Data&#10;data:  matrix(c(17, 25 - 17, 8, 20 - 8), ncol = 2)&#10;p-value = 0.07671&#10;alternative hypothesis: true odds ratio is not equal to 1&#10;95 percent confidence interval:&#10;  0.7990888 13.0020065&#10;sample estimates:&#10;odds ratio &#10;  3.101466 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The &lt;code&gt;fisher.test&lt;/code&gt; function accepts a matrix object of the 'successes' and 'failures' the two binomial proportions.  As you can see, however, the two-sided hypothesis is still not significant, sorry to say.  However, Fisher's Exact test is typically only applied when a cell count is low (typically this means 5 or less but some say 10), therefore your initial use of &lt;code&gt;prop.test&lt;/code&gt; is more appropriate.&lt;/p&gt;&#10;&#10;&lt;p&gt;Regarding your &lt;code&gt;binom.test&lt;/code&gt; calls, you are misunderstanding the call.  When you run &lt;code&gt;binom.test(x=17,n=25,p=8/20)&lt;/code&gt; you are testing whether proportion is significantly different from a population where the &lt;strong&gt;probability of success is 8/20&lt;/strong&gt;.  Likewise with &lt;code&gt;binom.test(x=8,n=20,p=17/25)&lt;/code&gt; says the &lt;strong&gt;probability of success is 17/25&lt;/strong&gt; which is why these p-values differ.  Therefore you are not comparing the two proportions at all.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-11-11T20:49:51.660" Id="123616" LastActivityDate="2014-11-11T20:55:37.313" LastEditDate="2014-11-11T20:55:37.313" LastEditorUserId="37428" OwnerUserId="37428" ParentId="123609" PostTypeId="2" Score="5" />
  <row AnswerCount="0" Body="&lt;p&gt;I have subjects that fit into one of three, mutually exclusive groups, &quot;favorable,&quot; &quot;intermediate,&quot; and &quot;unfavorable&quot; based on their genetics. They can then be classified as either a &quot;responder&quot; or a &quot;non-responder&quot; to treatment. I am trying to determine whether being classified as &quot;favorable&quot;, &quot;intermediate,&quot; or &quot;unfavorable&quot; is a predictor of response. Can someone point me in the right direction on how to determine this?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-11-11T21:49:14.100" Id="123625" LastActivityDate="2014-11-11T21:49:14.100" OwnerUserId="60484" PostTypeId="1" Score="0" Tags="&lt;statistical-significance&gt;&lt;categorical-data&gt;&lt;binary-data&gt;" Title="How to determine significance across categories of binary data?" ViewCount="20" />
  
  <row AcceptedAnswerId="123657" AnswerCount="4" Body="&lt;p&gt;I'm currently finishing up a B.S. in mathematics and would like to attend graduate school (a master's degree for starters, with the possibility of a subsequent Ph.D.) with an eye toward entering the field of data science. I'm also particularly interested in machine learning.&lt;/p&gt;&#10;&#10;&lt;p&gt;What are the graduate degree choices that would get me to where I want to go?&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a consensus as to whether a graduate degree in applied mathematics, statistics, or computer science would put me in a better position to enter the field of data science?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you all for the help, this is a big choice for me and any input is very much appreciated. Typically I ask my questions on Mathematics Stack Exchange, but I thought asking here would give me a broader and better rounded perspective.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-11-11T22:18:31.987" FavoriteCount="2" Id="123628" LastActivityDate="2014-11-24T23:58:10.187" LastEditDate="2014-11-24T23:58:10.187" LastEditorDisplayName="user60305" OwnerDisplayName="user60305" PostTypeId="1" Score="5" Tags="&lt;careers&gt;" Title="Graduate Degree Choices for Data Science?" ViewCount="351" />
  
  <row Body="&lt;p&gt;In general, if you want to draw a $N \times 1$ random vector $x$ with a multivariate normal distribution with mean zero and $N \times N$ variance matrix $\Sigma$, then you do the following:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Draw $N$ independent and identically distributed $N(0,1)$ random variables, and stack them up in a vector $y$.&lt;/li&gt;&#10;&lt;li&gt;Calculate the Cholesky decomposition of $\Sigma$ (the CC' form).  I think all statistical programming languages have a function to do this.  &lt;code&gt;R&lt;/code&gt; has it in the function &lt;code&gt;chol&lt;/code&gt;.&lt;/li&gt;&#10;&lt;li&gt;Multiply $x=Cy$&lt;/li&gt;&#10;&lt;li&gt;Now, $x$ is distributed normal with mean zero and variance $\Sigma$.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;The other part of your question is how to construct $\Sigma$.  You did not say anything about the variances of the $\alpha,\beta$, so I am going to assume that each $\alpha$ needs to have variance $\sigma^2_\alpha$ and each $\beta$ needs to have variance $\sigma^2_\beta$.  &lt;/p&gt;&#10;&#10;&lt;p&gt;First, let's be clear about how we are stacking up the $\alpha$s and $\beta$s to make our vector $x$:&#10;\begin{align}&#10;x = \left( \begin{array}{l}&#10;           \alpha_1 \\&#10;           \beta_1 \\&#10;           \alpha_2\\&#10;           \beta_2\\&#10;           \vdots \\&#10;           \alpha_N\\&#10;           \beta_N&#10;           \end{array}&#10;    \right)&#10;\end{align}&lt;/p&gt;&#10;&#10;&lt;p&gt;Since you want the correlation between each $\alpha,\beta$ pair to be $\rho$, that means that the correlation matrix will start out (before we deal with the adjacency&#10;correlations) looking like:&#10;\begin{align}&#10;\left[ \begin{array}{l l l l l l l l}&#10;          1 &amp;amp; \rho &amp;amp;    0 &amp;amp;    0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0\\&#10;       \rho &amp;amp;    1 &amp;amp;    0 &amp;amp;    0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0\\&#10;          0 &amp;amp;    0 &amp;amp;    1 &amp;amp; \rho &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0\\&#10;          0 &amp;amp;    0 &amp;amp; \rho &amp;amp;    1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0\\&#10;          \vdots &amp;amp;&amp;amp;&amp;amp;\vdots&amp;amp;&amp;amp;&amp;amp;&amp;amp; \vdots\\&#10;          0 &amp;amp;    0 &amp;amp;    0 &amp;amp;    0 &amp;amp; 0 &amp;amp; \cdots &amp;amp;    1 &amp;amp; \rho\\&#10;          0 &amp;amp;    0 &amp;amp;    0 &amp;amp;    0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; \rho &amp;amp;    1\\        &#10;       \end{array}&#10;\right]&#10;\end{align} &lt;/p&gt;&#10;&#10;&lt;p&gt;You don't say anything about how you want the adjacent nodes' variables to be correlated.  So, I'll assume you want the correlation between two adjacent nodes' $\alpha$s to be $\rho_\alpha$, the correlation between two adjacent nodes' $\beta$s to be $\rho_\beta$ and the &quot;cross-correlation&quot; between the $\alpha$ and $\beta$ of two adjacent nodes to be $\rho_{\alpha\beta}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's imagine that nodes 1 and 2 are adjacent to one another and that nodes 1 and N are adjacent to one another.  Then, we want the correlation matrix to look like:&#10;\begin{align}&#10;\left[ \begin{array}{l l l l l l l l}&#10;          1 &amp;amp; \rho &amp;amp; \rho_\alpha &amp;amp;    \rho_{\alpha\beta} &amp;amp; 0 &amp;amp; \cdots &amp;amp; \rho_\alpha &amp;amp; \rho_{\alpha\beta}\\&#10;       \rho &amp;amp;    1 &amp;amp;  \rho_{\alpha\beta} &amp;amp;    \rho_\beta &amp;amp; 0 &amp;amp; \cdots &amp;amp; \rho_{\alpha\beta} &amp;amp; \rho_\beta\\&#10;        \rho_\alpha &amp;amp;    \rho_{\alpha\beta} &amp;amp;    1 &amp;amp; \rho &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0\\&#10;          \rho_{\alpha\beta} &amp;amp;    \rho_\beta &amp;amp; \rho &amp;amp;    1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0\\&#10;          0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0\\&#10;          \vdots &amp;amp;&amp;amp;&amp;amp;\vdots&amp;amp;&amp;amp;&amp;amp;&amp;amp; \vdots\\&#10;          0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0\\&#10;          \rho_\alpha &amp;amp; \rho_{\alpha\beta} &amp;amp;    0 &amp;amp;    0 &amp;amp; 0 &amp;amp; \cdots &amp;amp;    1 &amp;amp; \rho\\&#10;          \rho_{\alpha\beta} &amp;amp; \rho_\beta &amp;amp;    0 &amp;amp;    0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; \rho &amp;amp;    1\\        &#10;       \end{array}&#10;\right]&#10;\end{align} &lt;/p&gt;&#10;&#10;&lt;p&gt;To get this, we add, to the starting correlation matrix, the matrix:&#10;\begin{align}&#10;A \otimes \left[\begin{array}{l l}&#10;                 \rho_\alpha &amp;amp; \rho_{\alpha\beta}\\&#10;                  \rho_{\alpha\beta} &amp;amp; \rho_\beta&#10;                 \end{array}&#10;           \right]  &#10;\end{align}&lt;/p&gt;&#10;&#10;&lt;p&gt;In that expression, $\otimes$ means kroenecker product, and $A$ is the adjacency matrix (I'm assuming symmetric).&lt;/p&gt;&#10;&#10;&lt;p&gt;That's almost it.  One problem may arise.  We have not done anything to assure that the correlation matrix we have constructed is positive definite.  If there are a lot of adjacencies and if the correlations we have assigned have large absolute values, then there is a chance that the matrix will not be positive definite.  You must check for this before calculating the Choleski decomposition, and if the matrix is not positive definite, you have to make the correlations smaller in absolute value.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, the steps in doing the draw are:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Choose the correlations, $\rho$, $\rho_\alpha$, $\rho_\beta$, $\rho_{\alpha\beta}$&lt;/li&gt;&#10;&lt;li&gt;Choose the variances of $\alpha$ and $\beta$: $\sigma^2_\alpha$, $\sigma^2_\beta$&lt;/li&gt;&#10;&lt;li&gt;If you don't want $\alpha$ and $\beta$ to have zero means, then choose their means: $\mu_\alpha,\mu_\beta$.&lt;/li&gt;&#10;&lt;li&gt;Construct the &quot;starting&quot; correlation matrix above&lt;/li&gt;&#10;&lt;li&gt;Add the kroenecker product matrix to the starting correlation matrix to get the final correlation matrix&lt;/li&gt;&#10;&lt;li&gt;Check that the final correlation matrix is positive definite.  If not, lower the correlations in step 1 and try again.&lt;/li&gt;&#10;&lt;li&gt;Take the choleski decompostion of the final correlation matrix&lt;/li&gt;&#10;&lt;li&gt;Draw $2N$ normal (0,1) random variables and stack them up in a vector $y$&lt;/li&gt;&#10;&lt;li&gt;Multiply the vector $y$ by the choleski decomposition&lt;/li&gt;&#10;&lt;li&gt;Multiply the $1^{\text{st}}$, $3^\text{rd}$, etc elements of $y$ by $\sqrt{\sigma^2_\alpha}$&lt;/li&gt;&#10;&lt;li&gt;Multiply the $2^\text{nd}$, $4^\text{th}$, etc elements of $y$ by $\sqrt{\sigma^2}_\beta$&lt;/li&gt;&#10;&lt;li&gt;Add $\mu_\alpha$ to the  $1^{\text{st}}$, $3^\text{rd}$, etc elements of $y$&lt;/li&gt;&#10;&lt;li&gt;Add $\mu_\beta$ to the $2^\text{nd}$, $4^\text{th}$, etc elements of $y$ &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I attach an &lt;code&gt;R&lt;/code&gt; program which does all this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# This program written in response to a Cross Validated question&#10;# http://stats.stackexchange.com/questions/123018/correlated-random-draws-with-graph-structured-correlation&#10;&#10;# This program draws four pairs of random variables.  Each is a pair with members named alpha, beta.  &#10;# Each pair of random variable is located at a node in a network.  There is a 4 by 4 adjacency &#10;# matrix, A, which shows which nodes are adjacent to one another.  All random variables are &#10;# normal &#10;# &#10;# Parameters:&#10;# &#10;# rho - correlation within nodes between alpha and beta&#10;# rho_a - correlation between alphas of adjacent nodes&#10;# rho_b - correlation between betas of adjacent nodes&#10;# rho_ab - correlation between the alpha at one and the beta at the other adjacent nodes&#10;# &#10;# mu_a  - mean of alpha&#10;# mu_b  - mean of beta&#10;# sig_a - standard deviation of alpha&#10;# sig_b - standard deviation of beta&#10;# &#10;# William B Vogt&#10;&#10;library(matrixcalc)&#10;&#10;set.seed(12344321)&#10;&#10;&#10;# Set the parameters&#10;&#10;rho    &amp;lt;- 0.2&#10;rho_a  &amp;lt;- 0.3&#10;rho_b  &amp;lt;- 0.1&#10;rho_ab &amp;lt;- 0.1&#10;&#10;mu_a   &amp;lt;- 0&#10;mu_b   &amp;lt;- 0&#10;sig_a  &amp;lt;- 2&#10;sig_b  &amp;lt;- 3&#10;&#10;A &amp;lt;- matrix(c(0,1,1,1,1,0,0,0,1,0,0,0,1,0,0,0),nrow=4,ncol=4)&#10;print(A)&#10;&#10;### Construct Sigma, the desired correlation matrix&#10;# first, get the within-node correlations right&#10;Sigma &amp;lt;- diag(4)%x%matrix(c(1,rho,rho,1),nrow=2,ncol=2)&#10;# now, fill in the between-node correlations&#10;Sigma &amp;lt;- Sigma + A%x%matrix(c(rho_a,rho_ab,rho_ab,rho_b),nrow=2,ncol=2)&#10;&#10;# check that Sigma is positive definite&#10;is.positive.definite(Sigma)&#10;&#10;# factor it&#10;C &amp;lt;- chol(Sigma)  # This function works so that Sigma = C'C&#10;&#10;# Draw and transform N(0,1) variates&#10;&#10;# We need four nodes times 2 variables, or 8 variables:&#10;x &amp;lt;- rnorm(8,mean=0,sd=1)&#10;y &amp;lt;- t(C)%*%x&#10;&#10;# Finally, extract alpha and beta and fix up their means and vars&#10;alpha &amp;lt;- y[seq(from=1,to=7,by=2)]&#10;beta  &amp;lt;- y[seq(from=2,to=8,by=2)]&#10;alpha &amp;lt;- alpha*sig_a + mu_a&#10;beta  &amp;lt;- beta*sig_b + mu_b&#10;&#10;# Here is one draw on our network of four nodes:&#10;cbind(alpha,beta)&#10;&#10;&#10;&#10;&#10;&#10;&#10;&#10;# Just to check, let's do this draw 10000 times to make sure we are getting the correlations we want&#10;x &amp;lt;- matrix(rnorm(80000,mean=0,sd=1),nrow=8,ncol=10000)&#10;y &amp;lt;- t(C)%*%x&#10;y &amp;lt;- t(y)&#10;&#10;# If all is well, these two matrixes should be basically identical&#10;cor(y)&#10;Sigma&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="3" CreationDate="2014-11-11T23:06:26.607" Id="123636" LastActivityDate="2014-11-12T19:33:45.143" LastEditDate="2014-11-12T19:33:45.143" LastEditorUserId="25212" OwnerUserId="25212" ParentId="123018" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I currently have data that looks like:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;   Process Batch Replicate     Yield&#10;1        1     1         1  9.937642&#10;2        1     1         2 10.257642&#10;3        1     1         3 10.287642&#10;4        1     2         1 10.147642&#10;5        1     2         2 10.127642&#10;6        1     2         3 10.167642&#10;7        1     3         1  8.487642&#10;8        1     3         2  8.247642&#10;9        1     3         3  9.217642&#10;10       1     4         1 10.057642&#10;11       1     4         2 10.367642&#10;12       1     4         3 10.027642&#10;13       2     1         1 13.337642&#10;14       2     1         2 12.977642&#10;15       2     1         3 12.937642&#10;16       2     2         1 12.837642&#10;17       2     2         2 13.027642&#10;18       2     2         3 13.057642&#10;19       2     3         1 12.947642&#10;20       2     3         2 12.877642&#10;21       2     3         3 12.687642&#10;22       2     4         1 13.327642&#10;23       2     4         2 13.477642&#10;24       2     4         3 13.347642&#10;25       3     1         1  9.357642&#10;26       3     1         2  9.037642&#10;27       3     1         3  9.187642&#10;28       3     2         1  9.187642&#10;29       3     2         2  9.217642&#10;30       3     2         3  9.297642&#10;31       3     3         1 10.097642&#10;32       3     3         2 10.227642&#10;33       3     3         3 10.277642&#10;34       3     4         1 10.227642&#10;35       3     4         2 10.237642&#10;36       3     4         3 10.087642&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This appears to be to be a two-stage nested design. However, I am not sure whether to treat Process and Batch as fixed or random or both. From what I read, it appears that we must know this prior to the experiment. However, is there another more pedagogical way of determining what sort of design this is? Thank you!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-12T02:35:05.320" Id="123656" LastActivityDate="2014-11-12T02:35:05.320" OwnerUserId="38348" PostTypeId="1" Score="0" Tags="&lt;experiment-design&gt;" Title="How can I tell my data is a nested two-stage design?" ViewCount="6" />
  
  
  <row AcceptedAnswerId="123673" AnswerCount="1" Body="&lt;p&gt;We were given an example the other day for Wilcoxon test with two samples with different populations ($No. untreated=n_1=4$ and $ No. treated=n_2=7$).&lt;/p&gt;&#10;&#10;&lt;p&gt;It was stated that:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\mu_w=\frac{n_2(n_2+n_1+1)}{2} $&lt;/p&gt;&#10;&#10;&lt;p&gt;$\sigma_w^2=\frac{n_1n_2(n_2+n_1+1)}{12}$&lt;/p&gt;&#10;&#10;&lt;p&gt;I know that:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\frac{W-\mu_2}{\sigma_w}\sim N(0,1)$&lt;/p&gt;&#10;&#10;&lt;p&gt;So two parts to this question:&lt;/p&gt;&#10;&#10;&lt;p&gt;How is $W$ calculated? I believe the 4 ranks (unsigned) were summed for the untreated sample.&lt;/p&gt;&#10;&#10;&lt;p&gt;-What is the justification for the mean and standard deviation formula? Why is $n_1$ and $n_2$ not the other way around?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-11-12T03:49:16.913" FavoriteCount="1" Id="123662" LastActivityDate="2014-11-14T09:53:29.637" LastEditDate="2014-11-14T09:07:27.847" LastEditorUserId="40531" OwnerUserId="40531" PostTypeId="1" Score="3" Tags="&lt;wilcoxon&gt;" Title="Two Sample Wilcoxon Test with Different Sample Populations" ViewCount="37" />
  
  
  <row Body="&lt;p&gt;Why are you using &lt;code&gt;table&lt;/code&gt; to compare the output? Using &lt;code&gt;cbind&lt;/code&gt; to put the actual and predicted values side by side shows that the predictions are not the same as the actual, and you can compute standard accuracy metrics to quantify the degree to which they diverge.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(bnlearn)                       # Load the package in R&#10;library(forecast)&#10;&#10;data(gaussian.test)&#10;training.set = gaussian.test[1:4000, ] # This is training set to learn the parameters&#10;test.set = gaussian.test[4001:4010, ]  # This is test set to give as evidence&#10;res = hc(training.set)                 # learn BN structure on training set data &#10;fitted = bn.fit(res, training.set)     # learning of parameters&#10;pred = predict(fitted$C, test.set)     # predicts the value of node C given test set&#10;cbind(pred, test.set[, &quot;C&quot;])           # compare the actual and predicted&#10;accuracy(f = pred, x = test.set[, &quot;C&quot;])&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Comparing the actual and predicted:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; cbind(predicted = pred, actual = test.set[, &quot;C&quot;])           &#10;       predicted    actual&#10; [1,]  3.5749952  3.952410&#10; [2,]  0.7434548  1.443177&#10; [3,]  5.1731669  5.924198&#10; [4,] 10.0840800 10.296560&#10; [5,] 12.3966908 12.268170&#10; [6,]  9.1834888  9.725431&#10; [7,]  6.8067145  5.625797&#10; [8,]  9.9246630  9.597326&#10; [9,]  5.9426798  6.503896&#10;[10,] 16.0056136 16.037176&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Measuring accuracy of prediction:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; accuracy(f = pred, x = test.set[, &quot;C&quot;])&#10;                ME      RMSE       MAE      MPE     MAPE&#10;Test set 0.1538594 0.5804431 0.4812143 6.172352 11.26223&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2014-11-12T06:12:19.113" Id="123678" LastActivityDate="2014-11-12T06:12:19.113" OwnerUserId="8141" ParentId="96271" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I am investigating the time on book (t) before write off.The sample data consists of times taken until write off, however these times have been calculated from accounts that have been written off already, hence this distribution is conditional on write-off i.e. f(t| write off) ~ Gamma. For the probability of write off, I am assuming a binomial distribution since one can either get written off or not at any time. I am now interested in calculating the probability of write off given time on book i.e f(write off|t)&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-11-12T06:13:34.817" Id="123679" LastActivityDate="2014-11-12T06:13:34.817" OwnerUserId="59639" PostTypeId="1" Score="0" Tags="&lt;conditional-probability&gt;" Title="Conditional probabilty - case study" ViewCount="11" />
  <row Body="&lt;p&gt;Can you bring in dice? Then have them throw the dice (many times, they have the time ...), draw histograms, calculating mean n umber of eyes, drawing histograms of that for different $n$, so on ...&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2014-11-12T09:08:17.697" CreationDate="2014-11-12T09:08:17.697" Id="123688" LastActivityDate="2014-11-12T09:08:17.697" OwnerUserId="11887" ParentId="123633" PostTypeId="2" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a multivariate regression with more than two independent variables. However, I am only interested in two particular  variables. So for simplicity, I write this model in terms of the two variables of interest and omit the other ones:&lt;/p&gt;&#10;&#10;&lt;p&gt;$ y = \alpha + \beta_{1}x_{1} + \beta_{2}x_{2} +  \epsilon $&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, my theory predicts that $\beta_{1} $ should be larger than zero, whereas $\beta_{2} $ should be zero. Is it possible to test this in a multiple hypotheses test? I know that multiple hypotheses can be tested with an F-Test but what troubles me is that the test for $\beta_{1} $ is one-sided and the test for $\beta_{2}$ is two-sided.&lt;/p&gt;&#10;&#10;&lt;p&gt;Best&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-11-12T09:21:19.630" Id="123690" LastActivityDate="2014-11-12T09:29:02.917" LastEditDate="2014-11-12T09:29:02.917" LastEditorUserId="60520" OwnerUserId="60520" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;hypothesis-testing&gt;&lt;least-squares&gt;" Title="OLS and multiple hypothesis testing" ViewCount="20" />
  
  <row Body="&lt;p&gt;To make a prediction you will have to supply a value for all the fitted predictors. If you regard some predictors as negligible you could either refit omitting negligible predictors, or fix their values at the mean values from the calibration set. Either way, you will end up with an equation in the non-negligible predictors.&lt;/p&gt;&#10;&#10;&lt;p&gt;NOTES: &#10;a) Although F does not reach the &quot;magic&quot; p=0.05 personally I would not regard it as negligible. I'd think about including H as well,&lt;/p&gt;&#10;&#10;&lt;p&gt;b) Cavity has a substantial effect, so in Minitabs presentation you will have a separate equation for each cavity.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-11-12T11:04:38.853" Id="123701" LastActivityDate="2014-11-12T11:04:38.853" OwnerUserId="20637" ParentId="123685" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;Also, what is the difference between the interpretation of the coefficients of random and fixed effects?&#10;What I understand is this: in fixed effects, the coefficient of x shows that, what would be the increase in y, as compared to individual i's own average y, due to one unit change in x, as compared to i's average x. is this correct? Is the interpretation same for random effects?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-11-12T11:42:31.653" Id="123705" LastActivityDate="2014-11-12T17:54:37.753" LastEditDate="2014-11-12T11:54:29.547" LastEditorUserId="2116" OwnerUserId="59618" PostTypeId="1" Score="0" Tags="&lt;random-effects-model&gt;" Title="How do we interpret the coefficients of the random effects model?" ViewCount="45" />
  <row AcceptedAnswerId="123712" AnswerCount="1" Body="&lt;p&gt;I have a device which obtains velocity based values over a period of 1 minute (average). I also have another device with which I am comparing that device. Now the velocity values can be quite random, but both devices should be similar in terms of increase or decrease within each minute: essentially they should have the same flow.&lt;/p&gt;&#10;&#10;&lt;p&gt;I saw online that some research has utilized the Pearson correlation to compare these values, but what I'm wondering is why? I read that this correlation only works for linear based values, and from what I read up, the velocities themselves were quite random (although similar in terms of devices). Or at least I don't think they are linear.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-12T11:49:59.260" Id="123707" LastActivityDate="2014-11-12T12:16:07.667" LastEditDate="2014-11-12T12:16:07.667" LastEditorUserId="60528" OwnerUserId="60528" PostTypeId="1" Score="0" Tags="&lt;correlation&gt;&lt;pearson&gt;" Title="Pearson Correlation of Device Values" ViewCount="13" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a training set and a separate test set. In both sets, I have extracted two different parameters and I have compared the predicted values of these parameters to the actual values. So, I have calculated $R^{2}$ and the RMSE values of these parameters.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question now, even with the $R^{2}$ and RMSE calculated, do I still need to calculate other test statistic such as z-test?&lt;/p&gt;&#10;&#10;&lt;p&gt;thanks! &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-12T12:39:23.477" Id="123716" LastActivityDate="2014-11-12T12:39:23.477" OwnerUserId="60532" PostTypeId="1" Score="0" Tags="&lt;regression-coefficients&gt;&lt;z-test&gt;&lt;rms&gt;" Title="Accuracy Assessment: Do I still have to use any other test statistics?" ViewCount="9" />
  <row AcceptedAnswerId="123731" AnswerCount="1" Body="&lt;p&gt;I hail from Mathematica SE. A friend of mine asked me a statistics question (I'm an economist and am assumed to know such things) which kind of stumped me, since I usually deal with non-discrete statistics.&lt;/p&gt;&#10;&#10;&lt;p&gt;Say you have a DNA strand with 20 positions, each of which can have one of four nucleotides (A,C,G,T). There is a &quot;correct&quot; combination, say A,T,G,C,C,....&lt;/p&gt;&#10;&#10;&lt;p&gt;For each position the probability of having the correct nucleotid is 85%. The probability of having one of the others is 5% each.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now assume you have a sample of 100M strands. My question is : &lt;strong&gt;What is the most likely number of different combinations (20 nucleotides in a specific order) in our sample?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Bonus question: Could you point me to some further reading on the subject, as I am interested in refreshing my memory on discrete statistics.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-11-12T14:13:52.210" Id="123727" LastActivityDate="2014-11-12T17:12:46.490" LastEditDate="2014-11-12T16:03:20.603" LastEditorUserId="60539" OwnerUserId="60539" PostTypeId="1" Score="2" Tags="&lt;sample&gt;&lt;discrete-data&gt;" Title="Number of different combinations in a sample" ViewCount="70" />
  <row Body="&lt;p&gt;For the most part you are fine; however, the glaring issue is that you have no reason to convert &lt;code&gt;char&lt;/code&gt; to an integer as &lt;code&gt;nnet&lt;/code&gt; accepts factors.  That is why you only see 1's reported.  As an example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(123)&#10;vars &amp;lt;- as.matrix(replicate(18, rnorm(25)))&#10;&#10;# Wrong way&#10;char &amp;lt;- as.integer(factor(rep(letters[1:5], each=5)))&#10;df &amp;lt;- data.frame(char, vars)&#10;head(df)&#10;&#10;library(nnet)&#10;&#10;nn1 &amp;lt;- nnet(char ~ ., data=df, size=20, maxit=1000, range=0.1, trace=T)&#10;nn1$fitted.values&#10;    &amp;gt; nn1$fitted.values&#10;   [,1]&#10;1     1&#10;2     1&#10;3     1&#10;4     1&#10;5     1&#10;6     1&#10;...&#10;&#10;# Right way&#10;char &amp;lt;- rep(letters[1:5], each=5)&#10;df &amp;lt;- data.frame(char, vars)&#10;&#10;nn2 &amp;lt;- nnet(char ~ ., data=df, size=20, maxit=1000, range=0.1, trace=T)&#10;nn2$fitted.values&#10;    &amp;gt; nn2$fitted.values&#10;              a            b            c            d            e&#10;1  1.000000e+00 2.281148e-08 2.034399e-11 5.934214e-12 3.212223e-10&#10;2  1.000000e+00 1.568664e-09 3.117289e-14 7.895235e-14 5.656804e-23&#10;3  9.999958e-01 1.666613e-07 6.259551e-08 3.969482e-06 4.485918e-23&#10;4  9.999994e-01 5.522178e-07 3.361721e-10 1.284468e-08 1.236227e-20&#10;5  9.999909e-01 8.255399e-06 2.314208e-09 8.657084e-07 1.898005e-14&#10;6  1.718135e-17 1.000000e+00 6.838461e-14 1.594482e-12 3.872572e-21&#10;...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;When you submit actual classes, you then get output that you can actually use for predicting classes.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-12T14:28:32.757" Id="123729" LastActivityDate="2014-11-12T14:28:32.757" OwnerUserId="37428" ParentId="123655" PostTypeId="2" Score="0" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am running several subjects through a mixed model regression and have encountered problems in which several of the subjects have an undesirable distribution for the specific study I am performing.  I wish to eliminate any subject with multimodality, uniformity, or strong skewness present in the distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have several hundred subjects for each procedure and am running an automated process (SAS), so I can't individually examine the distributions.  While playing around with the statistics and several repeated examples, I've noticed that the statistic x=STDEV/RANGE may give me what I'm looking for.  It seems that, as I look at the distributions, anything with STDEV/RANGE greater than 0.27 gives me the type of distributions that I want.  &lt;/p&gt;&#10;&#10;&lt;p&gt;My question is, is there a better way to get only the subjects I want (roughly Normal, some outliers ok, but no multimode or uniform or strong skew)?  Is there any danger in the way I am currently thinking to do this?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks.&lt;/p&gt;&#10;&#10;&lt;p&gt;UPDATE:  It looks like 0.26 may be a better cutoff, as it eliminates the uniform and bimodal distributions 98% of the time (at least in my limited testing).  However, it eliminates a desirable distribution 16% of the time.  Though I would rather delete a desirable subject than allow an undesirable one into the study, ideally I could have both...&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-12T15:07:26.180" Id="123734" LastActivityDate="2014-11-12T16:05:55.473" LastEditDate="2014-11-12T16:05:55.473" LastEditorUserId="60542" OwnerUserId="60542" PostTypeId="1" Score="1" Tags="&lt;distributions&gt;&lt;standard-deviation&gt;&lt;range&gt;" Title="Using STDEV and RANGE to subset data to desirable subjects" ViewCount="36" />
  
  <row AcceptedAnswerId="123798" AnswerCount="1" Body="&lt;p&gt;We are working on time series data forecasting. Our input data set is large, so thought to average two consecutive data points and reduce it to half the size. But we have observed that average values increased slope of the line, which effecting our forecast values slop too.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is screenshot of the issue I am describing. (Please NOTE that this is dummy data I quickly put in to describe the problem).&lt;/p&gt;&#10;&#10;&lt;p&gt;I understand that increase in slope value for summarized data make sense because AVG values are higher comparing with originals, the issue I am struggling with (or) trying to answer is, is there any way we can summarize the data without increasing the slop to keep forecast within original range? &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/0iXPG.png&quot; alt=&quot;enter image description here&quot;&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you for your time!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-12T15:17:55.677" Id="123736" LastActivityDate="2014-11-12T22:23:42.030" OwnerUserId="24778" PostTypeId="1" Score="0" Tags="&lt;time-series&gt;&lt;forecasting&gt;&lt;summary-statistics&gt;" Title="Average time series data (summarize) increasing slope of line" ViewCount="36" />
  <row AnswerCount="1" Body="&lt;p&gt;Assuming perfect normality, independence, etc. and testing for equality of variance between two population (where we know that $\sigma_x^2 $ is not lower) and (thus) use a one-sided test.&lt;/p&gt;&#10;&#10;&lt;p&gt;$$H_0: \sigma_x^2 =  \sigma_y^2 $$&#10;$$H_1: \sigma_x^2 - \sigma_y^2 &amp;gt; 0 $$ &lt;/p&gt;&#10;&#10;&lt;p&gt;A sample of size $n=40$, gives  $s_x^2 =20 $ and $s_y^2=25$.&#10;Using $\alpha = 0.05 $ I will reject the hypothesis iff my test statistic exceeds the critical value $F&amp;gt;F_{0.05,39,39} \approx 1.69$&lt;/p&gt;&#10;&#10;&lt;p&gt;My test statistic is $F=S_x/S_y = 0.8 $ =&gt; I do not reject $H_0$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Have I made any mistake? A colleague suggests I switch the numerator and denominator but I can't see why.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-11-12T15:43:05.403" Id="123740" LastActivityDate="2014-11-12T16:12:59.807" OwnerUserId="38379" PostTypeId="1" Score="0" Tags="&lt;f-test&gt;" Title="F-test for equality of variance" ViewCount="30" />
  <row AnswerCount="0" Body="&lt;p&gt;I am very new to SARIMA and I am really facing a poblem which p,q,P,Q to use. Here is ACF and PACF of first-difference, two first plots (stationary) and ACF and PACF of first-seasonal difference (two last plots).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Y95H6.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I think that P=1, Q=1 because ACF and PACF of first-seasonal difference drop off at lag 1 and are outside the confidence band. What about p and q ? Can I san that p=2 and q=2 because it drop of at lag 2? Or should I take under consideration the fact that only at lag 4 ACF and PACF are outside confidence band? Is p=4 and q=4 or 2 for both?&lt;/p&gt;&#10;&#10;&lt;p&gt;Or should I do AIC test for (2,1,2)(1,1,1)12, (4,1,2)(1,1,1)12, (2,1,4)(1,1,1)12, (4,1,4)(1,1,1)12 and check which one has the smallest? Or assuming that p=2 and q=2 does not make any sense because its within the confidence band?&lt;/p&gt;&#10;&#10;&lt;p&gt;SARIMA (4,1,4)(1,1,1)12 gives me  lots of non-significant coefficients. :&#10;&lt;img src=&quot;http://i.stack.imgur.com/SKg85.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Or maybe should I try SARIMA(7,1,4)(1,1,1)12, SARIMA(8,1,4)(1,1,1)12 ? But it really does not seem reasonable.&lt;/p&gt;&#10;&#10;&lt;p&gt;Please tell me what to do now :(&#10;Thank you for any help!&lt;/p&gt;&#10;" ClosedDate="2014-11-12T21:38:50.080" CommentCount="2" CreationDate="2014-11-12T15:44:17.377" Id="123741" LastActivityDate="2014-11-12T15:44:17.377" OwnerUserId="60365" PostTypeId="1" Score="1" Tags="&lt;arima&gt;&lt;coefficient&gt;" Title="ARIMA, p and q identification. Please help" ViewCount="22" />
  <row Body="&lt;p&gt;&lt;strong&gt;When you chase the definitions, the issues become trivial&lt;/strong&gt; (although perhaps still unintuitive):&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;$\mathbb{E}(Y\mid X) = \mathbb{E}(Y\mid \mathcal{F}_X)$ by definition.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;For any subalgebra $\mathcal{G}\subset \mathcal{F}$, $\mathbb{E}(Y\mid \mathcal{G})$ is defined to be any $\mathcal{G}$-measurable function for which&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\int_G \mathbb{E}(Y\mid \mathcal{G})(\omega) \,\mathrm d \mathbb{P}(\omega) = \int_G Y(\omega) \,\mathrm d \mathbb{P}(\omega)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;for every $G\in \mathcal{G}$.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Therefore, whenever $\mathcal{F}_X = \mathcal{F}$, it must be the case that&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;$\mathbb{E}(Y\mid X)$ is $\mathcal{F}$-measurable and&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;$\int_F \mathbb{E}(Y\mid X)(\omega) \,\mathrm d \mathbb{P}(\omega) = \int_F Y(\omega) \,\mathrm d \mathbb{P}(\omega)$ for every $F\in \mathcal{F}$.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;The equality of the integrals for all measurable sets implies (as is well known and established early in any account of Lebesgue integration) that &lt;strong&gt;$\mathbb{E}(Y\mid X)$ must equal $Y$ &lt;em&gt;almost surely&lt;/em&gt;&lt;/strong&gt; (a.s.): they can differ only on a set of measure zero.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;The second part of the question requests an example.&lt;/strong&gt;  Let's construct a very simple but not entirely trivial one.  It concerns a finite binomial process used to model (among other things) &lt;a href=&quot;http://en.wikipedia.org/wiki/Binomial_options_pricing_model&quot; rel=&quot;nofollow&quot;&gt;changes in prices of a financial asset over time.&lt;/a&gt;  For simplicity, I have restricted it to a sequence of two times during which the price could go up ($+$) or down ($-$), whence&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;$\Omega$ can be identified with the set $\{++, +-, -+, --\}$.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;$\mathcal{F}$ consists of all subsets of $\Omega$ (the discrete algebra).&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;$\mathbb{P}$ is determined by its values on the atoms, written $p_{++}=\mathbb{P}(\{++\})$, &lt;em&gt;etc.&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/CgZ1b.png&quot; alt=&quot;Figure&quot;&gt;&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Let $Y$ be the price of the asset after the first time and $X$ be its price after the second time.  (These natural and meaningful descriptions show this is not some pathological construction we're about to review.)&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;The figure displays this model as a binary tree in which the individual (conditional!) probabilities label the branches, the elements of $\Omega$ are the four possible paths from left to right through the tree, and the values of $Y$ and $X$ are indicated at the points where they are determined.&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose all four prices assigned by $X$ are distinct.  Then, since any individual price is measurable in $\mathcal{B}(\mathbb{R})$, $\mathcal{F}_X$ contains all the atoms, whence it consists of $\mathcal{F}$ itself.  But $Y$ can assign at most two distinct prices, $Y_{+} = Y(++) = Y(+-)$ and $Y_{-} = Y(-+) = Y(--)$.  The inverse images of these two prices then are the sets $+_1=\{++,+-\}$ and $-_1=\{-+,--\}$.  They generate a strict subalgebra of $\mathcal{F}$: it has four measurable sets and does not include any of the atoms.  It describes what is &quot;known&quot; after the first time but before the second one.&lt;/p&gt;&#10;&#10;&lt;p&gt;The definition of conditional expectation needs to be checked only on a basis for $\mathcal{F}_X$.  The set of its atoms is most convenient.  Here is an example of a calculation for the atom $\{-+\}$:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\int_{\{-+\}} \mathbb{E}(Y\mid X)(\omega) \,\mathrm d \mathbb{P}(\omega) = \int_{\{-+\}} Y(\omega) \,\mathrm d \mathbb{P}(\omega) = Y(-+)p_{-+}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The parallel calculations for the other atoms make it clear that for all $\omega\in\Omega$, &lt;/p&gt;&#10;&#10;&lt;p&gt;$$Y(\omega) p_\omega = \int_{\omega} \mathbb{E}(Y\mid X)(\omega)  = \mathbb{E}(Y\mid X)(\omega) p_\omega,$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where the second equality computes the integral directly.  From this &lt;strong&gt;we can construct two interesting examples:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Suppose every outcome has nonzero probability.  Then we may always divide both sides by $p_\omega$, no matter what $\omega$ may be, and obtain&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\mathbb{E}(Y\mid X)(\omega) = Y(\omega).$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The conditional expectation of $Y$ is just $Y$ itself.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Suppose $p_{++}=p_{+-}=1/2$ and $p_{--}=p_{-+}=0$.  (This models a situation where an initial decrease is impossible.)  Then we may define $Y_{-}$ to be &lt;em&gt;any&lt;/em&gt; value, since it does not matter (due to the impossibility of this event): the defining equality for $\omega={--}$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$0 = Y_{-} p_{--} = Y(--) p_{--} = \mathbb{E}(Y\mid X)(--) p_{--} = 0$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and its counterpart for $\omega=-+$ automatically hold.  Thus, &lt;em&gt;it is not necessarily the case that $\mathbb{E}(Y|\mathcal{F}) = Y$, but the set of $\omega$ where the two sides differ must have zero probability&lt;/em&gt; (and, of course, be measurable with respect to $Y$).&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Looking back at the tree might supply some intuition:&lt;/strong&gt; in conditioning $Y$ on $X$, &lt;em&gt;whose values were determined later,&lt;/em&gt; we thereby have complete information about $Y$ &lt;em&gt;along any sets of paths having nonzero probability of occurring.&lt;/em&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-12T17:32:32.723" Id="123754" LastActivityDate="2014-11-12T17:48:54.543" LastEditDate="2014-11-12T17:48:54.543" LastEditorUserId="919" OwnerUserId="919" ParentId="123588" PostTypeId="2" Score="3" />
  <row AnswerCount="0" Body="&lt;p&gt;I am using logistic regression to fit a model with categorical/multinomial varaibles.&#10;data-description: There are over 300 variables as independent variables, sample size is 5000 which is divided into 80/20 for train/validation.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am getting the following error on 2 different procedures that I have tried upon,&#10;here is the code: proc logistic data=a outest=b covout;&#10;                           model dependent= list of independent;&#10;                                    /selection=stepwise&#10;                                     slenrty=0.70 slstay=0.75 details lackfit;&#10;                  run;&lt;/p&gt;&#10;&#10;&lt;p&gt;I know step-wise is not the best approach but my question pertains to this:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;error: &quot;variable x should either be numeric or in classes list&quot;&#10;So I formatted the variable into numeric, but it still is giving me the same error?&#10;I cant figure out what to do?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;I applied proc catmod on the same exact data/variable to fit the multinomial logistic model and it gives me this error: &quot;variable x in list does not match type prescribed for this list&quot;. The error comes for the same set of variables. Can anyone suggest a solution?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Also, HOW DO i tell sas to treat all of these variables as categorical? or is it just a programming error? I cant understand why am I getting this error?&#10;Any help would be much appreciated; I am stuck at this point and my experience level on statistics/modeling is of a beginner!&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-11-12T17:35:58.590" Id="123755" LastActivityDate="2014-11-12T17:35:58.590" OwnerUserId="55905" PostTypeId="1" Score="0" Tags="&lt;logistic&gt;&lt;categorical-data&gt;&lt;multivariate-analysis&gt;&lt;predictive-models&gt;&lt;sas&gt;" Title="variable error on logistic regression/ proc catmod- Building predictive model" ViewCount="137" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to analyse a dataset with 3 groups of subjects and one repeated measure with 6 levels (accuracy at identifying 6 different emotions). I am using the GLM command from SPSS to do this, which is fairly striaghtforward:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;GLM emotion1 emotion2 emotion3 emotion4 emotion5 emotion6 BY group&#10;  /WSFACTOR=level 6 Polynomial &#10;  /METHOD=SSTYPE(3)&#10;  /CRITERIA=ALPHA(.05)&#10;  /WSDESIGN=emotion &#10;  /DESIGN=group.&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The effect I am most interested in is group x emotion (i.e. do the groups differ in their relative accuracy at identifying the 6 emotions). I would like to set up contrasts which test the pairwise differences between the groups on this interaction term -- i.e. test whether there is a group x emotion effect when comparing group 1 vs 2, 1 vs 3 and 2 vs 3. &lt;/p&gt;&#10;&#10;&lt;p&gt;It is fairly simple to set up contrasts between the different levels of subject group using:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;/CONTRAST (group) = SPECIAL (1 -1 0)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;However, this produces a contrast of the main effect of group (across all emotions) rather than the group x emotion interaction. A slightly more complex method gets close to what I am after-- using the LMATRIX and MMATRIX subcommands I can indvidually specify &quot;between&quot; and &quot;within&quot; subject contrast terms. SPSS combines these automatically (to estimate the interaction between them) and also calculates a multivariate test across all the specified contrasts. If I specify a set of orthogonal, polynomial within subject contrasts I can use this to get a multivariate (MANOVA) estimate of what I want:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;/LMATRIX   'group comparisons'&#10;  group 1 -1 0;&#10;/MMATRIX 'within subject polynomials'&#10;    all -5 -3 -1 1 3 5;&#10;    all 5 -1 -4 -4 -1 5;&#10;    all -5 7 4 -4 -7 5;&#10;    all 1 -3 2 2 -3 1;&#10;    all -1 5 -10 10 -5 1;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The above code produces a MANOVA of the group x emotion interaction comparing group 1 to 2. &lt;/p&gt;&#10;&#10;&lt;p&gt;My question is: Can the GLM command be used to produce a group x emotion ANOVA contrast?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-12T19:13:58.490" Id="123776" LastActivityDate="2014-11-12T19:13:58.490" OwnerUserId="60555" PostTypeId="1" Score="0" Tags="&lt;spss&gt;&lt;repeated-measures&gt;&lt;contrasts&gt;" Title="Specifying interaction contrasts for within x between terms in SPSS" ViewCount="70" />
  <row Body="&lt;p&gt;I realize this is a somewhat older question now but perhaps this can shed some light.  There are several things wrong with your code.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;The code as it stands is NOT reproducible.  At no point is &lt;code&gt;h&lt;/code&gt; defined which is the exact parameter you wish to tune.&lt;/li&gt;&#10;&lt;li&gt;You aren't iterating over different bandwidths.  If you are using cross-validation to compare between different models, you need different models (i.e. different parameters).&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Modifying your code and utilizing the &lt;code&gt;cars&lt;/code&gt; dataset, here is a reproducible example of Leave-One-Out Cross-validation on bandwidth whereby you pass a 'grid' of different bandwidths to tune the model.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;cv &amp;lt;- function(data, used.function, bandwidth.grid)&#10;{&#10;  n &amp;lt;- nrow(data)&#10;  mse &amp;lt;- matrix(, nrow=length(bandwidth.grid), ncol=2)&#10;  for (b in 1:length(bandwidth.grid)){&#10;    cv.value &amp;lt;- rep(0, n-1)&#10;    for (i in 1:(n-1)){&#10;      new.data &amp;lt;- data[-i,]&#10;      funcargs &amp;lt;- list(reg.x=new.data[,1],reg.y=new.data[,2],x=data[i,1], h = bandwidth.grid[b])&#10;      cv.value[i] &amp;lt;- do.call(used.function, funcargs)&#10;    }&#10;    mse[b,] &amp;lt;- c(bandwidth.grid[b], 1/n*sum((new.data[,2]-cv.value)^2))&#10;  }&#10;&#10;  ## MSE&#10;  colnames(mse) &amp;lt;- c(&quot;bandwidth&quot;, &quot;mse&quot;)&#10;  return(mse)&#10;}&#10;&#10;### kernel estimator usind nadaraya-watson:&#10;fcn1 &amp;lt;- function(reg.x, reg.y, x, h){&#10;  return(ksmooth(reg.x, reg.y, x.point = x, kernel = &quot;normal&quot;, bandwidth = h)$y)&#10;}&#10;&#10;attach(cars)&#10;### CV-score for kernel estimator:&#10;cv(cbind(speed, dist), fcn1, seq(10))&#10;&amp;gt; cv(cbind(speed, dist), fcn1, seq(10))&#10;      bandwidth      mse&#10; [1,]         1 261.9555&#10; [2,]         2 223.3542&#10; [3,]         3 217.8303&#10; [4,]         4 214.0923&#10; [5,]         5 211.1874&#10; [6,]         6 211.4104&#10; [7,]         7 214.6941&#10; [8,]         8 220.1501&#10; [9,]         9 227.2262&#10;[10,]        10 235.5479&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Here we can see that a &lt;code&gt;bandwidth = 5&lt;/code&gt; would be best.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-12T19:17:46.923" Id="123777" LastActivityDate="2014-11-12T19:17:46.923" OwnerUserId="37428" ParentId="56471" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;The VectorGaussian in the example has an identity covariance matrix, which means the elements of the weight vector already have independent normal priors with mean zero and variance 1.  I suspect what you have actually done is replace the definition of w to be of type &lt;code&gt;VariableArray&amp;lt;double&amp;gt;&lt;/code&gt; instead of &lt;code&gt;Variable&amp;lt;Vector&amp;gt;&lt;/code&gt;.  This doesn't change the statistical model, but it does change how inference will be performed.  In this case, the inference is less accurate since the approximate posterior is being factorized over all elements of w.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-12T19:19:38.170" Id="123778" LastActivityDate="2014-11-12T19:19:38.170" OwnerUserId="2074" ParentId="123702" PostTypeId="2" Score="0" />
  
  <row AnswerCount="3" Body="&lt;p&gt;I'm trying to get the &quot;productivity&quot; of treatments like sending an email, calling or sending an SMS and their combinations in the paying debtor's probability.&lt;/p&gt;&#10;&#10;&lt;p&gt;I couldn't find one model that satisfies the most basic hypothesis I need to fulfill. For example, I can't use a simple decision tree, with, for example, quantity of SMS as a variable, because more SMS means that during the period we have that client, we sent him, let's suppose 3 SMS, but because he didn't pay (if he would had paid during the first days he wouldn't have received any SMS). In a logistic regression I would have the same issue.&lt;/p&gt;&#10;&#10;&lt;p&gt;So I thought that the time should be important in this type of analysis. Let's say someone with the same 'survival'  time getting more SMS has (I think) more chances to pay (die in the model). &lt;/p&gt;&#10;&#10;&lt;p&gt;So I was thinking that we should stratified clients regarding their survival time. But I was wondering how can I add this to those models or if there is a more suitable model to do this. I read about stratified Cox Models, but I wasn't sure if it was going to be able to capture this. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;My question is, what model would you recommend to do this and how would you insert the  fact that if we have more time since the client was loaded in the system, the treatments (which are supposed to have a positive impact) will increase but because is more difficult that this client pays.&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Maybe some model where the class is is someone heal or not depending on how much medicine you gave him...So, you will have both effects, more medicine more chances to heal, but more medicine is related to more time with no healing. Really don't know.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-12T20:03:45.463" FavoriteCount="1" Id="123784" LastActivityDate="2014-11-14T21:09:42.290" LastEditDate="2014-11-12T20:36:12.067" LastEditorUserId="46741" OwnerUserId="46741" PostTypeId="1" Score="4" Tags="&lt;logistic&gt;&lt;survival&gt;&lt;cox-model&gt;&lt;non-independent&gt;" Title="Cox Models treatments depending on time until event" ViewCount="75" />
  <row AnswerCount="1" Body="&lt;p&gt;K-medians is typically used with Manhattan distance rather than Euclidean distance. Why is this? &lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-11-12T22:33:47.140" Id="123799" LastActivityDate="2014-11-13T09:08:16.790" LastEditDate="2014-11-12T22:40:32.713" LastEditorUserId="29284" OwnerUserId="29284" PostTypeId="1" Score="1" Tags="&lt;clustering&gt;&lt;optimization&gt;&lt;k-means&gt;&lt;distance-functions&gt;&lt;segmentation&gt;" Title="Why is k-medians typically used with Manhattan rather than Euclidean distance?" ViewCount="70" />
  <row Body="&lt;p&gt;The correct alternative is the second one. What is missing is that the best (c,s) is the one that maximizes the &lt;strong&gt;average&lt;/strong&gt; accuracy over the &lt;code&gt;f&lt;/code&gt; folds. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-12T22:51:54.200" Id="123802" LastActivityDate="2014-11-12T22:51:54.200" OwnerUserId="8226" ParentId="123405" PostTypeId="2" Score="0" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm trying to smooth surface-fit experimental results on 2D domain (x,y), while maintaining common sensce physics-based constraints (monotonous, convex, etc.).&lt;/p&gt;&#10;&#10;&lt;p&gt;current procedure is hand-draw lines through resutls printouts grouped by x=const. &lt;/p&gt;&#10;&#10;&lt;p&gt;I tried matlab's curve fitting toolbox's cftool. Polynomial gets too waivy in gaps of (x,y) domain, or fits badly on lower orders. Lowess produced surfaces that didn't follow the requed constraints and didn't have good accuracy fitting the data. Custom equation where matlab optimize-search for its best-fit parameters didn't work out for me too. Fellow statistician could run loess in R and it produced more smooth results, but the constraints were still violated. &lt;/p&gt;&#10;&#10;&lt;p&gt;Are there ways for data fitting with constraints? &#10;is it possible to have the this also for 1-D domain, 3-D domain f(x,y,z)? &lt;/p&gt;&#10;&#10;&lt;p&gt;I searched hard and couldn't find any. Really, hoped to replace pensil with more accurate tools. &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-12T19:11:41.403" FavoriteCount="1" Id="123806" LastActivityDate="2014-11-12T23:29:02.420" OwnerDisplayName="Oleg" PostTypeId="1" Score="1" Tags="&lt;mathematical-statistics&gt;" Title="Spline surface data fitting with constraints" ViewCount="34" />
  
  <row AcceptedAnswerId="124480" AnswerCount="5" Body="&lt;p&gt;Around 600 students have a score on an extensive piece of assessment, which can be assumed to have good reliability/validity. The assessment is scored out of 100, and it's a multiple-choice test marked by computer.&lt;/p&gt;&#10;&#10;&lt;p&gt;Those 600 students also have scores on a second, minor, piece of assessment. In this second piece of assessment they are separated into 11 cohorts with 11 different graders, and there is an undesirably large degree of variation between graders in terms of their 'generosity' in marking, or lack thereof. This second assessment is also scored out of 100.&lt;/p&gt;&#10;&#10;&lt;p&gt;Students were not assigned to cohorts randomly, and there are good reasons to expect differences in skill levels between cohorts.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm presented with the task of ensuring that differences between cohort markers on the second assignment don't materially advantage/disadvantage individual students.&lt;/p&gt;&#10;&#10;&lt;p&gt;My idea is to get the cohort scores on the second assessment to cohere with cohort scores on the first, while maintaining individual differences within the cohorts. We should assume that I have good reasons to believe that performance on the two tasks will be highly correlated, but that the markers differ considerably in their generosity.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is this the best approach? If not, what is?&lt;/p&gt;&#10;&#10;&lt;p&gt;It'd be greatly appreciated if the answerer could give some practical tips about how to implement a good solution, say in R or SPSS or Excel.&lt;/p&gt;&#10;" CommentCount="30" CreationDate="2014-11-13T01:03:34.527" FavoriteCount="5" Id="123814" LastActivityDate="2014-11-19T18:30:54.100" LastEditDate="2014-11-15T02:11:47.547" LastEditorUserId="9162" OwnerUserId="9162" PostTypeId="1" Score="13" Tags="&lt;inter-rater&gt;" Title="How can I best deal with the effects of markers with differing levels of generosity in grading student papers?" ViewCount="454" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a dataset of size &gt;35K in size / &gt;50 dimensions. Used BIRCH algorithm for clustering. While testing, the data points with which cluster formed is not matching i.e., The data point shows closer to some other cluster than the original cluster. Which is practically incorrect. On analyzing found the issue is due to merging two cluster (one will less and another with very high data points). The center of the resultant cluster will be shifting towards the second cluster leaving points on the edges of first cluster nearer to some other cluster.&lt;/p&gt;&#10;&#10;&lt;p&gt;Would like to justify my understanding and see if there are any other proven methods to mitigate this issue. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-13T07:17:25.610" Id="123832" LastActivityDate="2014-11-13T07:30:35.283" LastEditDate="2014-11-13T07:30:35.283" LastEditorUserId="29327" OwnerUserId="29327" PostTypeId="1" Score="0" Tags="&lt;classification&gt;&lt;clustering&gt;&lt;pattern-recognition&gt;&lt;hierarchical&gt;" Title="Testing Cluster Assignment/Pattern Matching against BIRCH Clusters" ViewCount="18" />
  <row Body="&lt;p&gt;We would need more information on how you estimate/construct your copula $C$ to give any detailed advise. &lt;/p&gt;&#10;&#10;&lt;p&gt;In general, sampling from a copula can be achieved following a similar idea as the univariate case, but involving the conditional marginal distribution $F(v|u)$. To sample from the copula, one would take two independent uniform samples $\bf x$ and $\bf y$ of desired length $n$. For each $i \in \{1, \dots, n\}$, set $u_i=x_i$ and seek $v_i$ such that: $$y_i=F(v_i|u_i)=\frac{\partial C (u_i,v_i)}{\partial u_i}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The pairs $(u_i,v_i)$ are now a sample from the copula. Applying the inverse of the marginals $F_X$ and $F_Y$ gives a sample of the bivariate distribution $F_{XY}(x,y)=C(F_X(x), F_Y(y))$. See as well the thread on this &lt;a href=&quot;https://stats.stackexchange.com/questions/74588/how-to-generate-from-the-copula-by-inverse-conditional-cdf-function-of-the-copul/&quot;&gt;question&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-13T07:51:33.307" Id="123836" LastActivityDate="2014-11-13T07:51:33.307" OwnerUserId="30342" ParentId="123698" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I am wondering, when using partial least squares regression to investigate a research question, there is predictor component (T) and response component (U), if I want to adjust for confounders (C), do I just include the C variable in the T component ? I am using R.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you very much!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-13T08:16:10.337" Id="123837" LastActivityDate="2014-11-13T08:16:10.337" OwnerUserId="60599" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;confounding&gt;&lt;pls&gt;" Title="Include confounder into partial least squares regression" ViewCount="22" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am computing a GAM on a large set of data sets. Almost all of them work, just this one data set makes gam() throw an error. I paste a code that reproduces this error here:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(mgcv)&#10;&#10;dat &amp;lt;- read.csv(&quot;http://gagneurweb.genzentrum.lmu.de/exchange/data.csv&quot;)&#10;myknots &amp;lt;- 70&#10;lambda &amp;lt;- 0.0001&#10;&#10;form &amp;lt;- value ~ protein + offset(offset) + s(i, bs = &quot;ps&quot;, k = myknots, &#10;                  sp = lambda) + s(i, bs = &quot;ps&quot;, k = myknots, sp = lambda, &#10;                  by = PolII) + s(i, bs = &quot;ps&quot;, k = myknots, sp = lambda, by = TAF9)&#10;&#10;mod &amp;lt;- gam(form, dat, family=nb(link=&quot;log&quot;), control=gam.control(trace=TRUE))&#10;## Error in gam.fit4(x, y, sp, Eb, UrS = UrS, weights = weights, start = start,  : &#10;##   inner loop 3; can't correct step size&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The function part in gam.fit4() that throws this error is this one:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;if (pdev-old.pdev&amp;gt;div.thresh) { ## solution diverging&#10;   ii &amp;lt;- 1 ## step halving counter&#10;   if (iter==1) { ## immediate divergence, need to shrink towards zero &#10;         etaold &amp;lt;- null.eta; coefold &amp;lt;- null.coef&#10;   }&#10;   while (pdev-old.pdev &amp;gt; div.thresh)  { ## step halve until pdev &amp;lt;= old.pdev&#10;     if (ii &amp;gt; 100) &#10;        stop(&quot;inner loop 3; can't correct step size&quot;)&#10;     ii &amp;lt;- ii + 1&#10;     start &amp;lt;- (start + coefold)/2 &#10;     eta &amp;lt;- (eta + etaold)/2               &#10;     mu &amp;lt;- linkinv(eta)&#10;     dev &amp;lt;- sum(dev.resids(y, mu, weights,theta))&#10;&#10;     pdev &amp;lt;- dev + t(start)%*%St%*%start ## the penalized deviance&#10;     if (control$trace) &#10;            cat(&quot;Step halved: new penalized deviance =&quot;, pdev, &quot;\n&quot;)&#10;  }&#10;} ## end of pdev divergence&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Does anyone know what happens here? I tried diving into the math and/or the code of gam.fit4(), but quickly failed. What I understand is that the penalized deviance &quot;pdev&quot; improvement does not go below some div.thresh in 100 steps. The last ~80 lines of the trace output show the very same penalized deviance for me:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Step halved: new penalized deviance = 3601.788 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;But what is the reason for this? A bad data set?&lt;/p&gt;&#10;&#10;&lt;p&gt;I'd like to know what to do from here. Should I just try/catch this iteration and forget about this data set, or is there an elegant way out of this?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-11-13T09:41:03.167" Id="123849" LastActivityDate="2014-11-13T09:41:03.167" OwnerUserId="10767" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;generalized-linear-model&gt;&lt;negative-binomial&gt;&lt;gam&gt;&lt;mgcv&gt;" Title="R gam() throws error: &quot;Can't correct step size&quot;" ViewCount="83" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I recently checked several initialization strategies of neural network from my friends' implementation.&lt;/p&gt;&#10;&#10;&lt;p&gt;First one is classic one which initializes weights randomly from gaussian distribution with a specified variance.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;def sample_weights_classic(sizeX, sizeY, sparsity, scale, rng):&#10;    sizeX = int(sizeX)&#10;    sizeY = int(sizeY)&#10;    if sparsity &amp;lt; 0:&#10;        sparsity = sizeY&#10;    else:&#10;        sparsity = numpy.minimum(sizeY, sparsity)&#10;    sparsity = numpy.minimum(sizeY, sparsity)&#10;    values = numpy.zeros((sizeX, sizeY), dtype=theano.config.floatX)&#10;    for dx in xrange(sizeX):&#10;        perm = rng.permutation(sizeY)&#10;        new_vals = rng.normal(loc=0, scale=scale, size=(sparsity,))&#10;        values[dx, perm[:sparsity]] = new_vals&#10;    return values.astype(theano.config.floatX)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Second one is to initialize the orthogonalize weights according to this paper:&lt;a href=&quot;http://arxiv.org/abs/1312.6120&quot; rel=&quot;nofollow&quot;&gt;http://arxiv.org/abs/1312.6120&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Third one is to initialize weight by dividing the largest singular value.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;def sample_weights(sizeX, sizeY, sparsity, scale, rng):&#10;    &quot;&quot;&quot;&#10;    Initialization that fixes the largest singular value.&#10;    &quot;&quot;&quot;&#10;    sizeX = int(sizeX)&#10;    sizeY = int(sizeY)&#10;    sparsity = numpy.minimum(sizeY, sparsity)&#10;    values = numpy.zeros((sizeX, sizeY), dtype=theano.config.floatX)&#10;    for dx in xrange(sizeX):&#10;        perm = rng.permutation(sizeY)&#10;        new_vals = rng.uniform(low=-scale, high=scale, size=(sparsity,))&#10;        vals_norm = numpy.sqrt((new_vals**2).sum())&#10;        new_vals = scale*new_vals/vals_norm&#10;        values[dx, perm[:sparsity]] = new_vals&#10;    _,v,_ = numpy.linalg.svd(values)&#10;    values = scale * values/v[0]&#10;    return values.astype(theano.config.floatX)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Can someone tell me how is the third one help learning? is there any paper about this ?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-13T11:36:06.550" Id="123859" LastActivityDate="2014-11-13T11:36:06.550" OwnerUserId="45706" PostTypeId="1" Score="0" Tags="&lt;neural-networks&gt;&lt;deep-learning&gt;" Title="Weights Initialization of Neural Network" ViewCount="70" />
  <row AnswerCount="1" Body="&lt;p&gt;If independent variables are measured in 5-point Likert scale and dependent variable is dummy, then can I apply logistic regression model here? How independent variables are put in SPSS?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-13T12:34:38.947" Id="123863" LastActivityDate="2014-11-13T15:16:23.233" LastEditDate="2014-11-13T15:09:31.673" LastEditorUserId="13047" OwnerUserId="60611" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;logistic&gt;&lt;spss&gt;&lt;likert&gt;" Title="Likert scale and binary logistic model" ViewCount="43" />
  <row AcceptedAnswerId="123882" AnswerCount="1" Body="&lt;p&gt;I have a question about the conv neural net. Specially from the deeplearning tutorial at&#10;&lt;a href=&quot;http://deeplearning.net/tutorial/lenet.html&quot; rel=&quot;nofollow&quot;&gt;http://deeplearning.net/tutorial/lenet.html&lt;/a&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;In Fig 1 from that url, (and also similarly in between C3 and S4, in Fig 2 of the Gradient Based Learning paper by Lecun Yann), I cannot understand how all the feature maps from layer m-1 gets into a single pixel on layer m, using a single filter/kernel.&lt;/p&gt;&#10;&#10;&lt;p&gt;For this to happen the kernel needs to be 3D. But I cannot understand how exactly a 3D kernel convolution works on 3 different images. Is it a average of the 3 values after applying 3 2D convolutions ? The documentation says &quot;and pool over several input channels&quot; . What is the significance of pool here ?&lt;/p&gt;&#10;&#10;&lt;p&gt;Also the kernel (or weights) as created in the code below (under &quot;We use two convolutional filters with 9x9 receptive fields. ....&quot;), has all values different. I would have assumed that at least per filter/kernel the values will be replicated on the 3 planes. So that the same feature is extracted from all the 3 maps. If the values are all different, then conceptually, the &quot;one value&quot; that comes out of the convolution does not seem to have a &quot;purpose&quot; as it is getting all mixed messages.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-11-13T14:32:26.690" Id="123876" LastActivityDate="2014-11-13T15:28:46.907" LastEditDate="2014-11-13T14:39:17.093" LastEditorUserId="60597" OwnerUserId="60597" PostTypeId="1" Score="1" Tags="&lt;neural-networks&gt;&lt;deep-learning&gt;" Title="Unclear area in Convolutional Neural Net" ViewCount="59" />
  
  <row Body="&lt;p&gt;&lt;em&gt;(Notwithstanding the basic observation made by @DimitriyV.Masterov about the unsuitability of predicting a 6-values categorical variable with a binary one).&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Finding candidate instruments and choosing among them, is one of the fine arts of statistics and econometrics. A &lt;em&gt;very&lt;/em&gt; fine art. Although some hard quantitative procedures have been devised in order to assess the choices made, the fact remains that &lt;strong&gt;the non-quantitative, verbal and logical argument that supports the choice retains its central and critical importance in order for an instrument choice to have chances to be accepted as valid.&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;For your particular case, this means that prior to searching for instruments, you must first lay out what are the factors that affect a child's test score, by &lt;em&gt;name&lt;/em&gt; and not by the general tag &quot;unobserved factors&quot;.  &lt;/p&gt;&#10;&#10;&lt;p&gt;After you name these factors and develop an argument as to why they do affect a child's test score, and so as to why they are thought of as &quot;included in the error term&quot; of your regression specification, then you have to separate those for which you have data available, and those that you do not.  &lt;/p&gt;&#10;&#10;&lt;p&gt;For the factors on which data are available, probably you should include them in the regression specification as controls.  &lt;/p&gt;&#10;&#10;&lt;p&gt;For those remaining factors that are either unobservable, or for which data are not available to you, you have to point out which among them is correlated with &lt;strong&gt;SES&lt;/strong&gt; and &lt;em&gt;why&lt;/em&gt; (logical argument again). Denote this subset by $W$. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Then&lt;/em&gt;, you have to think what observable and available variable(s) is correlated with &lt;strong&gt;SES&lt;/strong&gt; but uncorrelated with $W$ -and you have to lay down (again) a convincing argument about this difficult to find &quot;correlation / non-correlation&quot; combination.  &lt;/p&gt;&#10;&#10;&lt;p&gt;So, to the specific question  &quot;&lt;strong&gt;Is this ok to do?&lt;/strong&gt;&quot;, with &quot;this&quot; representing the whole approach as described by you in the question, the answer is a &lt;strong&gt;-multidimensional- &quot;No&quot;&lt;/strong&gt;.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-11-13T16:37:04.940" Id="123885" LastActivityDate="2014-11-13T21:33:53.957" LastEditDate="2014-11-13T21:33:53.957" LastEditorUserId="28746" OwnerUserId="28746" ParentId="123878" PostTypeId="2" Score="5" />
  <row AnswerCount="0" Body="&lt;p&gt;I recently started a job in power trading. But due to a sudden change in employment I am required to work on econometric models to gauge the supply and demand side of national power markets. So something beween analyst and trader really. Since my job will also be to import power I have to gauge day ahead (spot) prices for a couple of markets. (I've got raw from Reuters for all my explanatory variables and I made pretty good experiences so far with their data)&lt;/p&gt;&#10;&#10;&lt;p&gt;I got so far: i have the impression that a dynamic regression would do the job quite well. The explanatory variables should be consumption, wind production, hydro production, solar production, nuclear production and gas prices (for the sake of simplicity I assume that consumption already takes care of the variable temperature). There is an additional variable I'd like to include, net import capacity, but I don't want to overdo it to start with. &lt;/p&gt;&#10;&#10;&lt;p&gt;Since there should be an auto regressive tendency, an ARIMA model should work? As software I chose R as MatLab was too pricey. &lt;/p&gt;&#10;&#10;&lt;p&gt;I appreciate that this is probably a rather basic question for this community. Nevertheless I would be very grateful if someone took the time to go through the process in an example (of course including R code would be wicked). I did find lots of papers online, nevertheless I would much prefer help from practitioners rather than academics. Also, I would be very interested in any form of relevant remote or f2f courses/seminars. If you know anything relevant please let me know. Equally, if someone with the right skill set is up for remote tutoring, please give me a buzz...&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance!&lt;/p&gt;&#10;&#10;&lt;p&gt;Cheers&#10;Markus&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-11-13T16:54:27.807" Id="123889" LastActivityDate="2014-11-13T16:54:27.807" OwnerUserId="60622" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;multiple-regression&gt;&lt;forecasting&gt;&lt;autoregressive&gt;" Title="Forecast of spot electricity prices" ViewCount="49" />
  <row AnswerCount="0" Body="&lt;p&gt;to estimate the VAR on which i then do the trace and max tests of cointegration, I use 4 different information criteria to decide how many lags. However, when testing the residuals, none of the lag choices have well-behaved white noise residuals. in fact, in increasing lags, i have to go up to 15 lags, and i am using monthly data, so i think it is too much. what do you think is wrong? what should i do?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-13T17:07:29.757" Id="123891" LastActivityDate="2014-11-13T17:07:29.757" OwnerUserId="60626" PostTypeId="1" Score="0" Tags="&lt;var&gt;&lt;vecm&gt;" Title="lag length selection for VAR" ViewCount="25" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a question about rank of a random binary matrix. Assume that I have to make a random binary matrix with its size are k rows and n colmuns (k&amp;lt;=n). Each columns only has 1 or 0 values. Now I want to caculate the probability that the binary matrix is full rank (k). Please help me formula it. Note that 0 and 1 is created with same probability.&#10;This maybe one solution. However, I don't know it is whether correct or not. Let us verify it&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/F7rob.png&quot; alt=&quot;enter image description here&quot;&gt;&#10;Besides, if the above formula is correct, then  what is probability of G has full rank if n=k? &lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-11-13T17:50:52.047" Id="123897" LastActivityDate="2014-11-13T18:58:49.297" LastEditDate="2014-11-13T18:58:49.297" LastEditorUserId="45939" OwnerUserId="45939" PostTypeId="1" Score="0" Tags="&lt;probability&gt;&lt;conditional-probability&gt;" Title="Interesting question about full rank of random matrix" ViewCount="40" />
  <row AcceptedAnswerId="123990" AnswerCount="2" Body="&lt;p&gt;I am new to quantile regression and most of the examples I see are in a multiple regression context with continuous predictors.  I am analyzing a designed experiment and was wondering if quantile regression is still valid with only categorical predictors (eg 2 x 2 factorial ANOVA context).  &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-11-13T19:11:07.243" Id="123911" LastActivityDate="2015-01-31T19:23:01.310" OwnerUserId="8196" PostTypeId="1" Score="3" Tags="&lt;anova&gt;&lt;multiple-regression&gt;&lt;quantile-regression&gt;" Title="Is it valid to use quantile regression with only categorical predictors?" ViewCount="103" />
  <row AnswerCount="1" Body="&lt;p&gt;I just ran into something interesting, and was hoping someone could shed some light on it.  I'm trying to calculate rolling at least N on multiple dice.  I wrote a simulation script in R that uses the sample(8,1,TRUE) to simulate a d8 (8 sided die) roll and iterates and sums.  I also calculated the probability in excel using a normal distribution with variance (for kdN) = k*(N^2-1)/12 and mean = k*(N+1)/2.  I used excel function of: 1-Norm.dist(number.to.roll, mean, sd, TRUE).  I consistently get a few percentage points higher on the simulation than on the calculation.  For example, to roll 8 on a 2d8, simulation succeeds 67.6% of the time (+/- 0.3%).  Excel predicts 62.1%.  I think it has something to do with dice being integers and excel calculating based on a continuous distribution.  I have tried using +1, or -1, or -.5 to get excel to match.  It gets closer, but I can't find a constant that work across different dice sizes.  Is there a discrete distribution that will model the rolls better?  I tried the R &quot;triangle&quot; package, but it gives values very close to the Normal approximation in Excel.  My question is less how to get the formula right, than can someone provide me insight as to why there is the difference?  This is for my own edification only.  Am fiddling with numbers while I wait for parts to show up at work.  :)  Thanks.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-11-13T20:07:44.497" Id="123924" LastActivityDate="2014-11-15T15:30:33.837" OwnerUserId="45948" PostTypeId="1" Score="2" Tags="&lt;simulation&gt;&lt;dice&gt;" Title="Dice rolls, simulation vs. theory" ViewCount="71" />
  
  <row AcceptedAnswerId="124063" AnswerCount="1" Body="&lt;p&gt;I was wondering whether I can use a measured variable as a moderator in a linear regression. Say I have a manipulated variable X, and two measured variables Y and Z. There is a main effect of X on Y, but not of either X or Y on Z. But I want to know if X*Y predicts Z. Can I do this? Is the “multicollinearity&quot; between X and Y a problem?&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, &#10;X = exercise v. control&#10;Y = heartrate at rest&#10;Z = risk of diabetes&lt;/p&gt;&#10;&#10;&lt;p&gt;Say exercise (relative to control) affects heartrate, but not risk of diabetes (i.e., there's no main effect of X on Z). Furthermore, heartrate does not affect risk of diabetes (i.e., no main effect of Y on Z). But I want to know if exercise is more effective at reducing risk of diabetes among people whose heartrates are lower (regardless of whether heartrate is lower because of exercise or not). Can I just run a linear regression with an interaction term? Or do I have a multicollinearity problem?&lt;/p&gt;&#10;&#10;&lt;p&gt;I don't know if I've expressed this clearly...&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-11-13T20:25:35.367" Id="123929" LastActivityDate="2014-11-14T19:26:15.437" OwnerUserId="60644" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;interaction&gt;" Title="Can moderators be affected by the manipulated/independent variable?" ViewCount="24" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I read a paper about the relationship between system load and running time of a task ,&#10;it said that:  &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;the relationship between the measured load execution and the running time is almost perfectly linear ($R^2 &amp;gt; 0.99$).      &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;(see page 4), and mentions (Figure 2) that between measured load and execution time has a coefficient of correlation = 0.998  &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;If load were presented as a continuous signal, we would summarize this relationship between running time and load as $$\frac{t_\text{exec} }{ 1 + \frac{1}{t_\text{exec}} \int^{t_\text{now}+t_\text{exec}}_{t_\text{now}} z(t)\, dt} = t_\text{nom}$$&#10;   where $t_\text{now}$ is when the task begins executing, $t_\text{nom}$ is the running time of the task on a completely unloaded machine, $z(t)$ is the continuous &quot;background&quot; load, and $t_\text{exec}$ is the task's running time.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;(also page 4)&lt;/p&gt;&#10;&#10;&lt;p&gt;All I want to know how did he get this equation?  &lt;/p&gt;&#10;&#10;&lt;p&gt;What is the equation between two variables where one of them is a continuous signal and there is linear relationship between them?  &lt;/p&gt;&#10;&#10;&lt;p&gt;Maybe that is the specific question, but you can help me if you just know the general equation between 2 variables their coefficients of correlation = 0.998?&lt;/p&gt;&#10;" CommentCount="10" CreationDate="2014-11-13T23:15:47.377" Id="123949" LastActivityDate="2014-11-15T00:15:39.657" LastEditDate="2014-11-15T00:15:39.657" LastEditorUserId="805" OwnerUserId="60660" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;correlation&gt;" Title="Relationship between variables in positive linear correlation" ViewCount="37" />
  <row Body="&lt;p&gt;If you believe the intervals (and that's questionable, given the strong level of evidence that people are overconfident, even statisticians. See &lt;a href=&quot;http://messymatters.com/calibration-results/&quot; rel=&quot;nofollow&quot;&gt;http://messymatters.com/calibration-results/&lt;/a&gt; ) then a standard weighting would be to weight by the inverse of the variance.&lt;/p&gt;&#10;&#10;&lt;p&gt;If your judgmental forecast is supposed to indicate 95% confidence that the value is between 115 and 125, then you know at 10 units (125-115) is 4 standard deviations, so the standard deviation is 2.5 and the variance is 2.5 squared, or 6.25.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, you could weight this forecast by 1/6.25 and do similar calculations for the other two, and then do a weighted average of the midpoints of the interval to get a combined forecast.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you were willing to assume that the three forecasts were independent you could treat these as three points in a meta-analysis and get a combined confidence interval. But independence of the forecasts is so doubtful I wouldn't have faith in that approach. I think I'd be tempted to regard each of the endpoints as an estimate of the 2.5th percentile and 97.5th percentile (if these are supposed to reflect 95% confidence) and do a weighted average similar to the one above using the inverse of the variance. &lt;/p&gt;&#10;&#10;&lt;p&gt;I look forward to seeing what others might suggest for this.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-11-14T02:18:09.910" Id="123965" LastActivityDate="2014-11-14T02:18:09.910" OwnerUserId="3919" ParentId="123793" PostTypeId="2" Score="2" />
  
  <row AcceptedAnswerId="124704" AnswerCount="1" Body="&lt;p&gt;I have a dataset with growth rate as a response variable (&lt;code&gt;resp&lt;/code&gt; in the example) and temperature, food availability, and salinity as predictor variables (&lt;code&gt;pred1&lt;/code&gt; through &lt;code&gt;pred3&lt;/code&gt; in the example). The predictor variables are &quot;continuous&quot; with weekly intervals and have different units. Measurements span weekly (with missing values for some samples) throughout a year (&lt;code&gt;week&lt;/code&gt; in the example; defined from the beginning of the experiment). I have several samples and I want to quantify (over all samples):&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;How much each predictor variable explains the variation in growth rate&lt;/li&gt;&#10;&lt;li&gt;The relative effect of each predictor variable on growth rate&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I understand that linear mixed models could be a solution for this problem as I have several samples and dependent measurements over time. My question is: &lt;strong&gt;What would be the optimal model formulations using &lt;code&gt;lme4&lt;/code&gt; package for R?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Example data is available &lt;a href=&quot;http://pastebin.com/SAHZeV8e&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;. And here is an overview of it:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(ggplot2)&#10;tmp &amp;lt;- melt(X, id = c(&quot;Sample&quot;, &quot;weeks&quot;))&#10;ggplot(tmp, aes(x = weeks, y = value)) + geom_line() + facet_wrap(Sample ~ variable, scales = &quot;free_y&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/E7Dsv.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I have tried following:&lt;/p&gt;&#10;&#10;&lt;p&gt;As a solution for point 1:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(&quot;lme4&quot;)&#10;library(&quot;MuMIn&quot;)&#10;&#10;p1 &amp;lt;- lmer(resp ~ pred1 + (1|Sample) + (1|weeks), data = X)&#10;p2 &amp;lt;- lmer(resp ~ pred2 + (1|Sample) + (1|weeks), data = X)&#10;p3 &amp;lt;- lmer(resp ~ pred3 + (1|Sample) + (1|weeks), data = X)&#10;&#10;margr2 &amp;lt;- data.frame(Pred = c(&quot;pred1&quot;, &quot;pred2&quot;, &quot;pred3&quot;), marginal.R2 = c(r.squaredGLMM(p1)[[1]], r.squaredGLMM(p2)[[1]], r.squaredGLMM(p3)[[1]]))&#10;&#10;ggplot(margr2, aes(x = Pred, y = marginal.R2)) + geom_bar(stat = &quot;identity&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Marginal $R^2$ calculated by the method published &lt;a href=&quot;http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12225/abstract;jsessionid=32160C53D46AD1F97A3CBF86E65B95D4.f02t03&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; should indicate the overall variance explained by each predictor variable as far as I have understood and assuming that my model formulations are correct.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/9JkwA.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;For the relative effect (point 2), I think that I first have to have the predictor variables on a same scale. Only then can I compare them by having them all in the model and removing the intercepts:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Xs &amp;lt;- X&#10;Xs[4:6] &amp;lt;- scale(Xs[4:6])&#10;&#10;mod &amp;lt;- lmer(resp ~ pred1 + pred2 + pred3 - 1 + (1|weeks) + (1|Sample), data = Xs)&#10;cis &amp;lt;- confint(mod)[4:6,]&#10;&#10;releff &amp;lt;- data.frame(par = rownames(cis), lower = cis[,1], est = fixef(mod), upper = cis[,2])&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In order to make the interpretation more intuitive, I scale the effects to maximum absolute value of across confidence intervals (I am only interested in relative effect):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;tmp &amp;lt;- c(releff$lower,releff$upper)&#10;&#10;add &amp;lt;- 100*releff[c(&quot;lower&quot;, &quot;est&quot;, &quot;upper&quot;)]/max(abs(tmp))&#10;colnames(add) &amp;lt;- paste0(&quot;rel.&quot;, colnames(add))&#10;&#10;releff &amp;lt;- cbind(releff, add)&#10;&#10;ggplot(releff, aes(x = par, y = rel.est, ymin = rel.lower, ymax = rel.upper)) + geom_pointrange() + geom_hline(yintercept = 0)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/pfeqd.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Predictor variables are &quot;significant&quot;, where the CIs do not cross the horizontal line (to my understanding). I am not sure whether these approaches make much sense and that's why I am asking for help.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-11-14T11:29:59.570" FavoriteCount="2" Id="123997" LastActivityDate="2014-11-19T16:37:46.510" OwnerUserId="10829" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;mixed-model&gt;&lt;lmer&gt;&lt;lme4&gt;" Title="How to formulate linear mixed model to find out effects of continuous variables?" ViewCount="170" />
  
  
  <row AcceptedAnswerId="124282" AnswerCount="1" Body="&lt;p&gt;I was wondering if associations rules can include the absence of an item, for example, in this simplified set of transactions:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;milk, butter&#10;milk, eggs&#10;beer, eggs&#10;beer, chips&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I would be interested in rules stating:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;if milk -&amp;gt; no chips&#10;if beer -&amp;gt; no butter&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;If this is statistically plausible, can it be implemented in &lt;code&gt;r&lt;/code&gt;, for example in &lt;code&gt;arules&lt;/code&gt;?&lt;/p&gt;&#10;&#10;&lt;p&gt;Maybe I just need to select those rules that have a lift inferior to 1?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-14T13:47:29.180" FavoriteCount="1" Id="124014" LastActivityDate="2014-11-16T20:36:53.140" LastEditDate="2014-11-14T14:25:55.440" LastEditorUserId="55524" OwnerUserId="55524" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;association-rules&gt;" Title="Association rules (market basket analysis) - rules involving the absence of items?" ViewCount="55" />
  
  
  <row Body="&lt;p&gt;My advice is to go to your local university math department and inquire about their tutoring services there. &lt;/p&gt;&#10;&#10;&lt;p&gt;Grad students can be helpful, as well as post-grads. Depending on the types of questions you're asking and the speed with which you need an answer, the on-line format may not be suited for in-depth questions, especially since people don't know your math background.&lt;/p&gt;&#10;&#10;&lt;p&gt;As you said in your comments, some schools frown upon its graduate students working, so checking with post-doc people might also be an option. &lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2014-11-14T17:24:50.757" CreationDate="2014-11-14T15:06:37.013" Id="124028" LastActivityDate="2014-11-14T15:06:37.013" OwnerUserId="36539" ParentId="122962" PostTypeId="2" Score="1" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;In a RCT setting where we have 1 treatment group vs 1 placebo, we want to investigate the effects of the active treatment to a particular lab result. &lt;/p&gt;&#10;&#10;&lt;p&gt;If we have the results for the baseline lab exam (x) and the lab exam 10 days later (y), what should I chose as the dependent variable, if I were to use ANCOVA or a mixed model?&lt;/p&gt;&#10;&#10;&lt;p&gt;Should I simply use y, or should I use the change (i.e. y-x) ? Another possibility is to use the baseline value (x) as a covariate in the model. In that case, I can get a parameter estimate for x. However, if I were to use (y-x) as my dependent variable, this essentially fixes the beta for my x to equal 1.&lt;/p&gt;&#10;&#10;&lt;p&gt;So the question is: Should I use the baseline value as a covariate? Assuming the randomization is perfect, I am thinking the baseline value is irrelevant to my assessment of the active treatment.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-11-14T17:07:21.747" Id="124051" LastActivityDate="2014-11-14T17:07:21.747" OwnerUserId="20738" PostTypeId="1" Score="0" Tags="&lt;mixed-model&gt;&lt;ancova&gt;" Title="Adjusting for Baseline values" ViewCount="17" />
  
  
  <row Body="&lt;p&gt;You have a misunderstanding.  The 'alternative' hypothesis ($H_1$) is simply the negation of the null hypothesis.  When conducting, say, a &lt;a href=&quot;http://www.statsoft.com/textbook/power-analysis&quot;&gt;power analysis&lt;/a&gt;, we will specify a specific sampling distribution around a point estimate (for example a mean treatment effect) that we believe in, but rejecting the null does not make that point estimate true.  Based on the logic of hypothesis testing, the alternative hypothesis is not that point estimate, it is just the negation of the null.  There is no particular sampling distribution of a test statistic that is associated with the negation of the null.  &lt;/p&gt;&#10;&#10;&lt;p&gt;In addition, the meaning of the $p$-value is predicated on what may well be a &lt;a href=&quot;http://en.wikipedia.org/wiki/Counterfactual_conditional&quot;&gt;counterfactual&lt;/a&gt; premise.  The $p$-value is the probability of getting a test statistic as far away from your null point value for your parameter (or further) if that point value &lt;em&gt;were&lt;/em&gt; true, whether it is actually true of not.  Even if the null isn't true, it can be true that the test statistic would have the specified distribution under the null.  &lt;/p&gt;&#10;&#10;&lt;p&gt;You are striking on an important insight, though.  Once you no longer believe the null obtains, it is no longer clear what meaning the $p$-value has to offer.  &lt;/p&gt;&#10;" CommentCount="9" CreationDate="2014-11-14T21:42:46.030" Id="124086" LastActivityDate="2014-11-14T21:42:46.030" OwnerUserId="7290" ParentId="124084" PostTypeId="2" Score="10" />
  
  <row AnswerCount="0" Body="&lt;p&gt;What books on machine learning are recommended for a CS graduate student without a huge background in statistics?&lt;/p&gt;&#10;&#10;&lt;p&gt;I do have some background in ML (and of course linear algebra, probability, etc.) but not really a great/firm background in statistics.&lt;/p&gt;&#10;&#10;&lt;p&gt;I saw a lot of recommendations for:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;PRML:  &lt;strong&gt;Pattern Recognition and Machine Learning&lt;/strong&gt; by Bishop. &lt;/li&gt;&#10;&lt;li&gt;ESL:  &lt;strong&gt;Elements of statistical Learning&lt;/strong&gt; by Hastie et al. &lt;/li&gt;&#10;&lt;li&gt;MLaPP: &lt;strong&gt;Machine Learning - a Probabilistic Perspective&lt;/strong&gt; by Murphy.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Which ML book is recommended for a grad student with my background?&#10;I see MLaPP is pretty new but seems to include a lot of topics including deep learning etc. - I'd love to know what did someone who read it though of it.&lt;/p&gt;&#10;" ClosedDate="2014-11-14T23:28:09.957" CommentCount="2" CreationDate="2014-11-14T21:58:50.503" Id="124088" LastActivityDate="2014-11-14T22:39:00.317" LastEditDate="2014-11-14T22:39:00.317" LastEditorUserId="7290" OwnerUserId="60731" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;references&gt;" Title="Machine Learning books for CS (non-statistician) grad student" ViewCount="24" />
  <row AnswerCount="0" Body="&lt;p&gt;I have the next model for investment:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10; i_{it} = \alpha_{1}i_{i,t-1} + \beta_{1}i_{i,t-2} + \beta_{2}s_{it} + \beta_{3}s*tax_{it} +  \beta_{4}tax_{it} + \gamma_i + \lambda_{t} +\epsilon_{it}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Given that is a dynamic model I use system GMM and my result is:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Fwkjm.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;So $$ \alpha_{1}&amp;gt;1 $$. What I should do? Should I use unit root methods?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-15T02:34:02.713" Id="124106" LastActivityDate="2014-11-15T02:34:02.713" OwnerUserId="60740" PostTypeId="1" Score="0" Tags="&lt;generalized-moments&gt;&lt;dynamic-regression&gt;" Title="Dynamic regression with ρ&gt;1" ViewCount="15" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am looking for a suggestion on how to answer a question, or what to read more about. I am sorry if I do not sound professional, I am not. First a contrived example.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a table of 50 states in the US. I am comparing the number of Walgreen stores per population (buis_density) with demographic data.&lt;/p&gt;&#10;&#10;&lt;p&gt;[&lt;strong&gt;name, buis_density&lt;/strong&gt;, income_median, age_median, pct_white, etc... ]&lt;/p&gt;&#10;&#10;&lt;p&gt;First I found the correlation coef between buis_density and each of the demographic columns to see strength of relationship and to say &quot;as the income_median goes up the number of Subways per person decreases&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;Next I found the Z-Values of each demographic column, chose 1 state (the highest buis_density), and found &lt;strong&gt;the SUM of the absolute values of the differences between Chosen State Z Values and Values for all other states.&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;z[0].val = ABS(chosenState.zAgeMedian - state[0].zAgeMedian) + ABS(chosenState.zIncomeMedian - state[0].zIncomeMedian)&lt;/p&gt;&#10;&#10;&lt;p&gt;My &quot;intention&quot; is the lower this number is the more similar the state is to the chosen state. But this doesn't really help much (assuming it makes sense).&lt;/p&gt;&#10;&#10;&lt;p&gt;What I really want to know is where more Walgreens should be built? So which state has similar demographic data to other states that have a LARGE buis_density, BUT has a small buis_density (kind of like an outlier).&lt;/p&gt;&#10;&#10;&lt;p&gt;I started the algorithm by sorting by buis_density, and using a ranking number to find a weighted average for each column. &lt;/p&gt;&#10;&#10;&lt;p&gt;weighted_z_income_avg = (50 * z_income_state_1 + 49 * z_income_state_2 .. ) / (50 + 49 ..)&lt;/p&gt;&#10;&#10;&lt;p&gt;Larger buis_densities have more weight, which gives me a &quot;new&quot; row of weighted averages that I am using as the ChosenState (as above). &lt;strong&gt;I don't think this works though because the  weighted averages of the z-values may end up close to zero.&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;My intention was if a state had a final number that is SMALL (similar to other states with a large buis_density) but also has low buis_denisty, it would be worth looking at.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-15T04:35:43.953" Id="124111" LastActivityDate="2014-11-15T04:35:43.953" OwnerUserId="60743" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;normal-distribution&gt;&lt;trend&gt;&lt;extreme-value&gt;" Title="Find Points that Break a mutli-valued Relationship" ViewCount="8" />
  
  <row AcceptedAnswerId="124115" AnswerCount="1" Body="&lt;p&gt;I'm working with a log-normal regression model. However, some of the dependent variable equal zero (not missing). Can I use an alternative specification like $log(y+1)$ ~ $X$ (most $y$s are really large)? Or should I just omit those observations? Do I have to do balance check every time I drop some observations and report all the result in appendix?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-11-15T05:15:27.890" Id="124114" LastActivityDate="2014-11-15T21:04:58.647" LastEditDate="2014-11-15T06:25:09.560" LastEditorUserId="52766" OwnerUserId="52766" PostTypeId="1" Score="0" Tags="&lt;econometrics&gt;" Title="Regarding log-normal specification" ViewCount="46" />
  <row Body="&lt;p&gt;If $L$ is a $p\times p$ lower triangular matrix and $C=LL^T$, then the Jacobian is given in Equation (55) of &lt;a href=&quot;https://dspace.library.cornell.edu/bitstream/1813/32744/1/BU-643-M.pdf&quot; rel=&quot;nofollow&quot;&gt;Henderson and Searle (1978)&lt;/a&gt; (see page 16) as&#10;$$J(C\rightarrow L) = 2^p \prod_{i=1}^p {l\,}_{i,i}^{p-i+1}.$$&#10;They note that this result was given as Theorem 4 of Deemer and Olkin (1951).&lt;/p&gt;&#10;&#10;&lt;p&gt;This result is also shown in other places, such as Theorem 4.2 of &lt;a href=&quot;http://digitool.library.mcgill.ca/webclient/StreamGate?folder_id=0&amp;amp;dvs=1416040621625~382&quot; rel=&quot;nofollow&quot;&gt;Njoroge (1988)&lt;/a&gt;, who explicitly stipulates that $C$ be positive definite symmetric and that $L$ have positive diagonal elements. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;&lt;em&gt;References&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;W.L. Deemer and I. Olkin (1951) The Jacobians of certain matrix transformations useful in multivariate analysis.  Based on lectures of P.L. Hsu at the University of North Carolina, 1947, &lt;em&gt;Biometrika&lt;/em&gt; &lt;strong&gt;38&lt;/strong&gt;:345--367.&lt;/p&gt;&#10;&#10;&lt;p&gt;H.V. Henderson and S.R. Searle (1978) Vec and vech operators for matrices, with some uses in Jacobians and multivariate statistics. Technical Report BU-643-M.  Biometrics Unit, Cornell University.  &lt;/p&gt;&#10;&#10;&lt;p&gt;The &lt;a href=&quot;http://www.jstor.org/stable/3315017&quot; rel=&quot;nofollow&quot;&gt;published version&lt;/a&gt; of the technical report seems to be:&lt;/p&gt;&#10;&#10;&lt;p&gt;H.V. Henderson and S.R. Searle (1979) Vec and vech operators for matrices, with some uses in Jacobians and multivariate statistics.  &lt;em&gt;The Canadian Journal of Statistics&lt;/em&gt; &lt;strong&gt;7&lt;/strong&gt;(1):65--81.&lt;/p&gt;&#10;&#10;&lt;p&gt;M.N. Njoroge (1988) &lt;em&gt;On Jacobians connected with matrix variate random variables.&lt;/em&gt; Thesis.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-15T09:19:58.977" Id="124125" LastActivityDate="2014-11-15T09:33:04.067" LastEditDate="2014-11-15T09:33:04.067" LastEditorUserId="11945" OwnerUserId="11945" ParentId="65188" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="124138" AnswerCount="1" Body="&lt;p&gt;I would like to fit the following model &lt;code&gt;Y (t) = m (t) + b * t + g * C (t) + N (t)&lt;/code&gt; with m (t) to be the long term mean monthly values (remove seasonal component), b the trend coefficient, C to be the matrix of explanatory variables, and N (t) the error term being AR (1).&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to ask you if this model is the same as the following:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;Y (t) - m (t) = b * t + g * C (t) + N (t)&lt;/code&gt;. Forced to have 0 intercept term, or I should also substract the mean from my regressors also.&lt;/p&gt;&#10;&#10;&lt;p&gt;Moreover, I would like to know if you can propose how this could be implemented, preferably in Matlab, or secondly in R.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am not familiar with this kind of models yet, so thanks in advance, all help is very much appreciated.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-15T09:56:12.537" Id="124129" LastActivityDate="2014-11-15T12:00:47.780" OwnerUserId="60754" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;regression&gt;&lt;time-series&gt;&lt;matlab&gt;" Title="Multilinear Model with fixed intercept" ViewCount="76" />
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to apply Gumbel Copula in R by using &quot;copula&quot; pkg. The parameter &quot;alpha&quot; of gumbel copula is 1.016. The copula structure is:&#10;gum.cop=archmCopula(family=&quot;gumbel&quot;,dim=2,param=alpha)&#10;But the problem is in the initial value/values. when I go for the fitting of copula the following function has been applied which give an error which is mentioned below.&lt;/p&gt;&#10;&#10;&lt;p&gt;start.vals = 1&#10;fit.gum.cop = fitCopula(gum.cop,data=cbind(u.hat,v.hat),start=1)&#10;Error in optim(start, loglikCopula, lower = lower, upper = upper, method = method,  : &#10;  initial value in 'vmmin' is not finite&lt;/p&gt;&#10;&#10;&lt;p&gt;The marginal of the bi-variate distribution are skewed GED and standerdized student's t dist. Please do guide what is the problem in my fitting procedure. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-15T10:58:36.807" Id="124133" LastActivityDate="2014-11-15T10:58:36.807" OwnerUserId="49456" PostTypeId="1" Score="0" Tags="&lt;bivariate&gt;&lt;copula&gt;&lt;gumbel&gt;" Title="How to fit Gumbel Copula" ViewCount="37" />
  <row AnswerCount="1" Body="&lt;p&gt;I look for an intuitive understanding of interaction effect in ANOVA or regression. Let's keep things simple as the following.&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose we have a standard 2 x 2 factorial design, where each factor variable has two levels, e.g. Factor &lt;code&gt;A&lt;/code&gt; has &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt;, so does factor &lt;code&gt;B&lt;/code&gt;. Let's denote the groups as:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;             B&#10;          0      1&#10;      ---------------&#10;    0 |  G1  |  G2  |&#10; A    ---------------&#10;    1 |  G3  |  G4  |&#10;      ---------------&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;If we include the interaction term in a two-way ANOVA or linear regression for some dependent variable &lt;code&gt;Y&lt;/code&gt;, and it turns out that the interaction between &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt; is significant . Does this mean that:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;The difference in &lt;code&gt;Y&lt;/code&gt; between G1 and G3 and that between G2 and G4 must be different?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;em&gt;At the same time&lt;/em&gt;, the difference in &lt;code&gt;Y&lt;/code&gt; between G1 and G2 and that between G3 and G4 must also be different?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;In other words, are the two conditions above necessary and sufficient for the interaction to be significant?&lt;/p&gt;&#10;&#10;&lt;p&gt;A related conceptual question: if I'm interested in showing that combining &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt; (i.e. G4) enhances &lt;code&gt;Y&lt;/code&gt; compared to having only either &lt;code&gt;A&lt;/code&gt; (i.e. G3) or &lt;code&gt;B&lt;/code&gt; (i.e. G2), do I have to show that the interaction between &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt; is significant? If not, what are my options here?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-15T12:59:00.923" Id="124140" LastActivityDate="2014-11-15T13:30:24.897" OwnerUserId="41394" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;anova&gt;&lt;interaction&gt;&lt;interpretation&gt;&lt;intuition&gt;" Title="Interpretation of significant interaction" ViewCount="35" />
  <row AnswerCount="1" Body="&lt;p&gt;If $ X_1, ..., X_n$ is a random sample from $ X \sim N(\mu_1, \sigma^2)$ and $Y_1,..., Y_n$ is a random sample from $Y \sim N(\mu_2, \sigma^2),$ if the samples are independent and $ \sigma^2$ is known, can we say that $\bar{X}-\bar{Y}$ is sufficient for $\mu_1 - \mu_2$ ?&lt;/p&gt;&#10;&#10;&lt;p&gt;My guess is that it is true. I thought we could define &lt;/p&gt;&#10;&#10;&lt;p&gt;$$W_i=X_i-Y_i \sim N(\mu_1 - \mu_2, 2 \sigma_2)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;n independent random variables and use it to show that $\bar{W} = \bar{X} - \bar{Y} $ is sufficient to the mean. Is that correct?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-11-15T13:15:45.290" Id="124141" LastActivityDate="2015-02-22T10:29:16.877" OwnerUserId="35224" PostTypeId="1" Score="2" Tags="&lt;self-study&gt;&lt;inference&gt;&lt;sufficient-statistics&gt;" Title="Sufficient statistics for $\mu_1 - \mu_2$" ViewCount="103" />
  
  
  <row AcceptedAnswerId="124152" AnswerCount="2" Body="&lt;p&gt;I have two groups, male and female teachers. I want to see if there is a statistically significant difference in mean pay.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are 48 male teachers, which is enough to use a t-test. However, there are only 23 female teachers, and further, the distribution of female pay is right skewed.&lt;/p&gt;&#10;&#10;&lt;p&gt;What are my options for a test?&lt;/p&gt;&#10;&#10;&lt;p&gt;Could I:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;resample female teacher pay to get 48 values, then do a t-test?&lt;/li&gt;&#10;&lt;li&gt;select only 23 &lt;em&gt;male&lt;/em&gt; teachers, then shuffle the pay values to create a null randomization distribution and then compare the mean difference to look for statistical signifigance?&lt;/li&gt;&#10;&lt;li&gt;some other method?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2014-11-15T14:14:31.140" Id="124149" LastActivityDate="2014-11-17T11:53:02.310" LastEditDate="2014-11-17T11:53:02.310" LastEditorUserId="2116" OwnerUserId="60760" PostTypeId="1" Score="1" Tags="&lt;t-test&gt;" Title="Which test to use when t-test is not apropriate?" ViewCount="73" />
  <row AnswerCount="1" Body="&lt;p&gt;&quot;The number of degrees of freedom is a measure of how certain we are that our sample population is representative of the entire population - the more degrees of freedom, usually the more certain we can be that we have accurately sampled the entire population.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;Can someone please explain this statement?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-11-15T14:17:18.210" Id="124150" LastActivityDate="2014-11-16T17:26:30.913" OwnerUserId="43204" PostTypeId="1" Score="2" Tags="&lt;degrees-of-freedom&gt;" Title="Degrees of freedom" ViewCount="85" />
  
  
  <row Body="&lt;p&gt;Expectation propagation is what you are looking for. BP is a special case of EP where variables are discrete.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-11-15T22:38:26.093" Id="124187" LastActivityDate="2014-11-15T22:38:26.093" OwnerUserId="59862" ParentId="124184" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;Does anyone use the generalized likelihood ratio test for detecting a sudden change in time series forecasting (ARIMA Model)? A &lt;a href=&quot;http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;amp;arnumber=6113293&amp;amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D6113293&quot; rel=&quot;nofollow&quot;&gt;paper&lt;/a&gt; by Bonne Zhu uses this technique for anomaly detection, but I don't know how I can implement it.&lt;/p&gt;&#10;&#10;&lt;p&gt;Could someone give me a good reference? Or could someone please send me a reference example of using it in R or anywhere?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-15T23:15:49.540" Id="124196" LastActivityDate="2014-11-19T03:19:18.707" LastEditDate="2014-11-19T03:19:18.707" LastEditorUserId="7290" OwnerUserId="59603" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;time-series&gt;&lt;forecasting&gt;&lt;references&gt;&lt;likelihood-ratio&gt;" Title="Generalized likelihood ratio test" ViewCount="40" />
  
  
  
  
  <row Body="&lt;p&gt;The principle is like a &quot;fuzzy&quot; version of the contraposition principle (or reductio ad absurdum principle, I'm not sure). &lt;/p&gt;&#10;&#10;&lt;p&gt;Consider that every dog has four legs. Then if you sample an animal with two legs you are sure it is not a dog.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now only consider that every dog has a high probability to have four legs (in other words a high majority of dogs have four legs). Then if you sample an animal with two legs you conclude it is unlikely a dog. This is the principle of hypothesis testing (but in practice it requires a sensible choice of the event having high probability under $H_0$).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-11-16T10:04:07.130" Id="124231" LastActivityDate="2014-11-16T10:04:07.130" OwnerUserId="8402" ParentId="124084" PostTypeId="2" Score="3" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have two teams, and both teams have multiple features related to that team. For example:&lt;/p&gt;&#10;&#10;&lt;p&gt;Players in the team.&#10;Players total points won.&#10;Team win percentage.&#10;Average player weight.&#10;Average player height.&lt;/p&gt;&#10;&#10;&lt;p&gt;How would I structure this training data to try and predict future match outcomes? If I placed all the features for both team1, team2, and outcome together (like 'example 1' below) I'm guessing this would produce poor and inaccurate results. At a guess I'd have to be able to analyse the two teams data separately to the outcome data like in 'example 2' below? If that is the case, how do I go about doing this?&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm still very new to machine learning so apologies if this is a silly question in any way!&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Example 1&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;[team1 - Players in the team,&#10;team1 - Players total points won,&#10;team1 - Team win percentage,&#10;team1 - Average player weight,&#10;team1 - Average player height,&#10;team2 - Players in the team,&#10;team2 - Players total points won,&#10;team2 - Team win percentage,&#10;team2 - Average player weight,&#10;team2 - Average player height,&#10;outcome]&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Example 2&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;[ [all,team,one,features], [all,team,two,features], result]&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-16T16:26:14.157" Id="124255" LastActivityDate="2014-11-16T16:26:14.157" OwnerUserId="27053" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;scikit-learn&gt;" Title="Grouping team features and comparing them to a single match outcome" ViewCount="8" />
  <row AcceptedAnswerId="124273" AnswerCount="1" Body="&lt;p&gt;I am trying to fit a beta distribution to election forecast data.&#10;The ultimate purpose is determining with what probability the election will be decided by one vote (more on this &lt;a href=&quot;http://stats.stackexchange.com/questions/124043/beta-binomial-distribution-with-a-priori-alpha-and-beta-to-account-for-pro&quot;&gt;here&lt;/a&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;My data is as follows:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;0.517 0.59 0.55 0.528 0.496 0.51 0.55 0.54 0.57&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I use the scipy &lt;code&gt;scipy.stats.beta.fit()&lt;/code&gt; function, as &lt;a href=&quot;http://stats.stackexchange.com/questions/68983/beta-distribution-fitting-in-scipy&quot;&gt;discussed here&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;For my data I get: &lt;/p&gt;&#10;&#10;&lt;p&gt;$\alpha = 0.7469674643946238 $&lt;/p&gt;&#10;&#10;&lt;p&gt;$\beta = 1.7165349362225657 $&lt;/p&gt;&#10;&#10;&lt;p&gt;which leads to a sloping distribution with its maximum close to zero.&lt;/p&gt;&#10;&#10;&lt;p&gt;For most other artificial data I have tried, I get distributions which actually hold water. Obviously, for my case this is wrong (it may not be for others), but I would like to make certain that the &lt;strong&gt;minima&lt;/strong&gt; of the distribution are close to 0 and 1 respectively, and the maximum is approximately in the range of my data points.&lt;/p&gt;&#10;&#10;&lt;p&gt;I had thought of fitting a normal distribution to my data, and a beta distribution to that - but that would be an approximation of an approximation. I'd rather do it all in one &quot;step&quot;.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-11-16T16:33:02.110" Id="124256" LastActivityDate="2014-11-17T20:00:17.250" OwnerUserId="32504" PostTypeId="1" Score="1" Tags="&lt;fitting&gt;&lt;curve-fitting&gt;&lt;beta-distribution&gt;" Title="Fitted Beta distribution always holds water. Can I force it not to?" ViewCount="101" />
  <row AnswerCount="0" Body="&lt;p&gt;It is needed how to estimate the upper confidence interval for variance.  I have the equations here from a document.  I attach a photo of one page.  I do have the raw data and hence the mean values, but I am having difficulty following the basic steps in order to make the cumulative distribution chi squared calculation. How do you estimate chisquared&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/rIuY8.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I have the entire document here so I can provide.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-11-16T16:53:19.340" Id="124259" LastActivityDate="2014-11-27T01:14:42.883" LastEditDate="2014-11-27T01:14:42.883" LastEditorUserId="57210" OwnerUserId="57210" PostTypeId="1" Score="0" Tags="&lt;confidence-interval&gt;&lt;confidence&gt;" Title="Upper confidence interval for the variance using chi squared distribution" ViewCount="24" />
  <row AnswerCount="1" Body="&lt;p&gt;I am looking for two random variables which fulfills the following two things:&lt;/p&gt;&#10;&#10;&lt;p&gt;a) $\mathbb E(X|Y)&amp;lt;\infty$ and $\mathbb E(Y|X)&amp;lt;\infty$&lt;/p&gt;&#10;&#10;&lt;p&gt;b) $E(X|Y)&amp;gt; Y$ and $\mathbb E(Y|X)&amp;gt;X$ a.s&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is the original task:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/rklwk.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2014-11-16T18:35:41.517" Id="124265" LastActivityDate="2014-11-17T01:14:35.527" LastEditDate="2014-11-16T20:43:36.543" LastEditorUserId="7224" OwnerUserId="59752" PostTypeId="1" Score="6" Tags="&lt;probability&gt;&lt;self-study&gt;&lt;random-variable&gt;&lt;conditional-expectation&gt;" Title="Random variables with some properties (conditional expectation)" ViewCount="102" />
  
  <row Body="&lt;p&gt;It looks like the &lt;code&gt;scipy.stats.beta.fit&lt;/code&gt; function is weird and undocumented. According to &lt;a href=&quot;http://stackoverflow.com/questions/23329331/how-to-properly-fit-a-beta-distribution-in-python&quot;&gt;this question&lt;/a&gt;, what it does is fit a generalized beta distribution (i.e. a beta distribuction which has been shifted and stretched to be nonzero on an interval other than $[0,1]$) with max and min included as parameters. If you don't specify the max and min, then it will choose weird values instead of the standard zero and 1 (see the top answer to the linked question.) So it looks like you should try&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;scipy.stats.beta.fit(data, floc=0, fscale=1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;on your data. Unfortunately, I don't have access to it right now so can't try it, but the correct values should be $\alpha = 168$, $\beta = 144$. You can get them from &lt;a href=&quot;http://www.wessa.net/rwasp_fitdistrbeta.wasp&quot; rel=&quot;nofollow&quot;&gt;an online app here&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-11-16T19:39:02.763" Id="124273" LastActivityDate="2014-11-17T20:00:17.250" LastEditDate="2014-11-17T20:00:17.250" LastEditorUserId="22047" OwnerUserId="13818" ParentId="124256" PostTypeId="2" Score="4" />
  <row AcceptedAnswerId="124371" AnswerCount="1" Body="&lt;p&gt;I have time-series data for N stocks.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;sample.data&amp;lt;-rep(10,rnorm(100))&lt;/code&gt;, where each column shows the returns of different stocks over time.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am trying to construct a portfolio weight vector to minimize the variance of the returns.&lt;/p&gt;&#10;&#10;&lt;p&gt;the objective function:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;min w^{T}\sum w&#10;s.t. e_{n}^{T}w=1&#10;\left \| w \right \|\leq C&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;where w is the vector of weights, &lt;code&gt;\sum&lt;/code&gt; is the covariance matrix, &lt;code&gt;e_{n}^{T}&lt;/code&gt; is a vector of ones, &lt;code&gt;C&lt;/code&gt; is a constant. Where the second constraint (&lt;code&gt;\left \| w \right \|&lt;/code&gt;) is an inequality constraint. &lt;/p&gt;&#10;&#10;&lt;p&gt;Is there any function in R that can do this?&#10;I tried using &lt;code&gt;solve.QP()&lt;/code&gt; from the &lt;code&gt;quadprog&lt;/code&gt; package, but it is not clear how to impose the inequality constraint for the norm of the weight vector.&lt;/p&gt;&#10;&#10;&lt;p&gt;The following code solves the problem if the second constraint was simply &lt;code&gt;w \leq C&lt;/code&gt;&#10;instead  &lt;code&gt;\left \| w \right \|\leq C&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;cov.Rt&amp;lt;-cov(sample.data)&#10;A.eq&amp;lt;-matrix(1,nrow(cov.Rt),ncol=1)&#10;B.eq&amp;lt;-1&#10;A.neq&amp;lt;-diag(10)&#10;B.neq&amp;lt;-matrix(0,nrow=10,ncol=1)&#10;&#10;A&amp;lt;-cbind(A.eq,A.neq)&#10;B&amp;lt;-c(B.eq,B.neq)&#10;mu&amp;lt;-colMeans(sample.data)&#10;&#10;solve.QP(cov.Rt,mu,A,B,meq=1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;How can this code be modified to solve the above problem for norms?&lt;/p&gt;&#10;&#10;&lt;p&gt;Any help would be appreciated.&lt;/p&gt;&#10;" ClosedDate="2014-11-17T12:07:58.567" CommentCount="2" CreationDate="2014-11-16T19:50:16.623" Id="124274" LastActivityDate="2014-11-17T12:07:33.203" LastEditDate="2014-11-16T20:12:38.353" LastEditorUserId="60827" OwnerUserId="60827" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;optimization&gt;" Title="Constraining norms with inequalities" ViewCount="32" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm running some GAMs using the mgcv R package on a dataset with ~8.5k observations, where productivity is the response and environmental conditions are the covariates. However I am unsure of which distribution to use and was seeing some advice.&lt;/p&gt;&#10;&#10;&lt;p&gt;The productivity response is definitely not normally distributed, which I think is the mgcv default if you don't specify anything? I've been using the Gamma on the logic that it reduces the residual variation:&lt;/p&gt;&#10;&#10;&lt;p&gt;family=Gamma(link=log)&lt;/p&gt;&#10;&#10;&lt;p&gt;Another option for productivity seems to be the Tweedie distribution, as biomass is a positive continuous quantity which can be very small or 0  eg:&lt;/p&gt;&#10;&#10;&lt;p&gt;family=Tweedie(1.25,power(.5))&lt;/p&gt;&#10;&#10;&lt;p&gt;The Tweedie produces higher a deviance explained (by about 5-6%) and lower AIC (by about 1500 units) than Gamma. The residuals show less structure (i.e. more constant variance), but the total deviance is much higher (residual plots 1 = gamma, 2 = tweedie). I'm not sure which of these criteria are most important for choosing a distribution? &lt;/p&gt;&#10;&#10;&lt;p&gt;My logic was that reducing the residual structure is a better outcome for the key assumption of constant variance. Also the key results I'm interested in (comparing the relative contributions of different covariates) don't fundamentally change with the distribution, but the magnitudes of their effects on productivity are different.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any advice is greatly appreciated,&lt;/p&gt;&#10;&#10;&lt;p&gt;Hugh&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/9LaRu.png&quot; alt=&quot;Gamma residuals&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/nni96.png&quot; alt=&quot;Tweedie residuals&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-16T22:46:27.620" Id="124290" LastActivityDate="2014-11-16T22:46:27.620" OwnerUserId="45178" PostTypeId="1" Score="1" Tags="&lt;residuals&gt;&lt;aic&gt;&lt;gamma-distribution&gt;&lt;gam&gt;&lt;tweedie-distribution&gt;" Title="Gamma vs tweedie distribution for large productivity dataset" ViewCount="51" />
  
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;When I fit a multilevel varying slope model, it is easy to summarize the variation in slope. However, I have not yet seen any materials that discusses how to &lt;em&gt;explain&lt;/em&gt; such variation (i.e. what about this group that allows it to have a greater effect than the rest?)&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, we want to test whether meal voucher helps student's performance. We fit a multilevel model of students within schools, allowing for the effect of meal voucher to vary across schools.&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;\begin{align}&#10;performance_i &amp;amp;\sim N(\alpha_{j[i]} + \beta_{j[i]} * voucher_i \\&#10;\alpha_j &amp;amp;\sim N(\text{school level covariates}, \sigma_\alpha) \\&#10;\beta_j &amp;amp;\sim N(\text{school level covariates}, \sigma_\beta)&#10;\end{align}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Now we need to explain why the variation in $\beta_j$ occurs. Is this causal inference goal possible with multi-level model?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-11-17T03:00:25.763" Id="124314" LastActivityDate="2014-11-17T23:41:04.310" LastEditDate="2014-11-17T23:41:04.310" LastEditorUserId="20148" OwnerUserId="20148" PostTypeId="1" Score="0" Tags="&lt;multilevel-analysis&gt;&lt;causal-inference&gt;" Title="Explaining why the slope varies in varying slope model?" ViewCount="33" />
  <row AnswerCount="0" Body="&lt;p&gt;Previous questions have asked about creating prediction intervals for estimates derived from &lt;a href=&quot;http://stats.stackexchange.com/questions/49750/how-do-i-calculate-prediction-intervals-for-random-forest-predictions&quot;&gt;random forests&lt;/a&gt; or &lt;a href=&quot;http://stats.stackexchange.com/questions/117190/how-to-find-a-gbm-prediction-interval&quot;&gt;boosted regression trees&lt;/a&gt;, in a similar way to is easily achieved with linear regression models.&lt;/p&gt;&#10;&#10;&lt;p&gt;A comment on &lt;a href=&quot;http://stats.stackexchange.com/questions/78079/confidence-interval-of-rmse&quot;&gt;this question&lt;/a&gt; described the RMSE as an estimate of the standard deviation of the residual error, supporting use of the RMSE in construction of an interval around a prediction (or estimate) from a BRT or RF.&lt;/p&gt;&#10;&#10;&lt;p&gt;Am I right in thinking that CART methods relax the requirement for homoscedasticity?  If so, it would seem that using the RMSE calculated across the full range of residuals would lead to inappropriately wide intervals in some regions, and too narrow intervals in others.  It would then seem the only way to estimate an interval would be through bootstrapping (BRT) or accessing trees' individual predictions (RF).&lt;/p&gt;&#10;&#10;&lt;p&gt;[That same question] (&lt;a href=&quot;http://stats.stackexchange.com/questions/78079/confidence-interval-of-rmse&quot;&gt;Confidence interval of RMSE&lt;/a&gt;) attracted advice on the construction of a confidence interval for the standard deviation of residuals, assuming mean residual is zero, with normal distribution, based on a chi squared statistic around the RMSE.  &lt;/p&gt;&#10;&#10;&lt;p&gt;How would such an interval on the SD be used?  Would using the high end of the CI for SD as the value in a CI such as $\hat{x} \pm z\hat{SD}_u$ be a valid, if conservative, interval?  Could you still attribute a specific 'confidence' value (e.g. 95%) to such an interval, given that it has 'nested' confidences?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-17T04:05:22.913" Id="124320" LastActivityDate="2014-11-18T20:45:11.370" LastEditDate="2014-11-18T20:45:11.370" LastEditorUserId="39945" OwnerUserId="39945" PostTypeId="1" Score="1" Tags="&lt;random-forest&gt;&lt;cart&gt;&lt;prediction-interval&gt;&lt;gbm&gt;&lt;mse&gt;" Title="Using an RMSE with derived confidence interval, to generate a prediction interval for an estimate" ViewCount="69" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to put a continuous target into predictive modelling method.&#10;The target is an amount that can range from 0 to unknown. I have roughly 1000 records (for modelling and validation altogether).&#10;In general I would use linear regression but I believe data does not meet requirements for this.&#10;A better approach will be a logistic regression. The issue here is that logistic regression works with binary outcome. &#10;I think I have 2 options: &#10;1) Split the target into groups (e.g. 0-500, 501-1000,1001-5001,5000-high) and use a specific type of logistic regression (unfortunately I am having hard time remembering the name of this)&#10;2)Find a way to model the continuous outcome different than the linear regression.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any suggestions?&lt;/p&gt;&#10;&#10;&lt;p&gt;Best Regards,&#10;Nikolay&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-17T06:44:50.690" Id="124328" LastActivityDate="2014-11-17T06:59:08.017" LastEditDate="2014-11-17T06:59:08.017" LastEditorUserId="58843" OwnerUserId="58843" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;regression&gt;&lt;modeling&gt;&lt;continuous-data&gt;" Title="Methods for predictive modeling on continous target" ViewCount="22" />
  
  <row Body="&lt;p&gt;Using standard datasets is good practice when you can.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's an example where I will test the hypothesis that the average predicted price of cars--given mileage, weight, and length--is less or equal to \$5,500 (or $H_0: \mathbb E[Y \vert X] \le 5,500$): &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;sysuse auto, clear&#10;reg price mpg weight length &#10;margins, post&#10;test _cons==5500&#10;display &quot;Ho: E[Y|X] &amp;lt;= 5500  p-value =&quot;ttail(r(df_r),sign(_b[_cons]-5500)*sqrt(r(F)))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Here's is the intuition. First, run the regression and calculate the average expected price with &lt;code&gt;margins&lt;/code&gt;. Do the the corresponding two-sided Wald test and use the results to calculate the test statistic and &lt;em&gt;p&lt;/em&gt;-value for the one-sided test.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's the nitty gritty. The Wald test is an &lt;em&gt;F&lt;/em&gt; test with 1 numerator degree of freedom and 70 denominator degrees of freedom (since we have 74 observations and 4 coefficients). The Student’s &lt;em&gt;t&lt;/em&gt; distribution with &lt;em&gt;d&lt;/em&gt; degrees of freedom &lt;em&gt;squared&lt;/em&gt; is equivalent to the &lt;em&gt;F&lt;/em&gt; distribution with 1 numerator degree of freedom and &lt;em&gt;d&lt;/em&gt; denominator degrees of freedom. Since the original &lt;em&gt;F&lt;/em&gt; test has 1 numerator degree of freedom, the square root of the &lt;em&gt;F&lt;/em&gt; statistic is the absolute value of the &lt;em&gt;t&lt;/em&gt; statistic for the one-sided test. We just need to determine whether this &lt;em&gt;t&lt;/em&gt; statistic is positive or negative, which you can do with the &lt;code&gt;sign()&lt;/code&gt; function applied to the differences. Then, using the &lt;code&gt;ttail()&lt;/code&gt; function on the &lt;em&gt;F&lt;/em&gt; statistic from the test command, you can calculate the &lt;em&gt;p&lt;/em&gt;-value for the one-sided test.&lt;/p&gt;&#10;&#10;&lt;p&gt;For completeness, if you want to test $H_0: \mathbb E[Y \vert X] \ge 5,500$, you can use&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;display &quot;Ho: E[Y|X] &amp;gt;= 5500  p-value =&quot;1-ttail(r(df_r),sign(_b[_cons]-5500)*sqrt(r(F)))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-11-17T10:31:57.037" Id="124355" LastActivityDate="2014-11-17T19:27:54.440" LastEditDate="2014-11-17T19:27:54.440" LastEditorUserId="7071" OwnerUserId="7071" ParentId="124343" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;In the &lt;a href=&quot;http://robjhyndman.com/hyndsight/tourism-forecasting-competition/&quot; rel=&quot;nofollow&quot;&gt;linked blog post&lt;/a&gt;, Rob Hyndman calls for entries to a tourism forecasting competition. Essentially, the blog post serves to draw attention to the relevant &lt;a href=&quot;http://robjhyndman.com/hyndsight/tourism-forecasting-competition/&quot; rel=&quot;nofollow&quot;&gt;IJF article&lt;/a&gt;, an &lt;a href=&quot;http://robjhyndman.com/papers/forecompijf.pdf&quot; rel=&quot;nofollow&quot;&gt;ungated version of which&lt;/a&gt; is linked to in the blog post.&lt;/p&gt;&#10;&#10;&lt;p&gt;The benchmarks you refer to - 1.38 for monthly, 1.43 for quarterly and 2.28 for yearly data - were apparently arrived at as follows. The authors (all of them are expert forecasters and very active in the &lt;a href=&quot;http://forecasters.org/&quot; rel=&quot;nofollow&quot;&gt;IIF&lt;/a&gt; - no snake oil salesmen here) are quite capable of applying standard forecasting algorithms or forecasting software, and they are probably not interested in simple ARIMA submission. So they went and applied some standard methods to their data. For the winning submission to be invited for a paper in the &lt;a href=&quot;http://forecasters.org/ijf/index.php&quot; rel=&quot;nofollow&quot;&gt;IJF&lt;/a&gt;, they ask that it improve on the best of these standard methods, as measured by the MASE.&lt;/p&gt;&#10;&#10;&lt;p&gt;So your question essentially boils down to:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Given that a MASE of 1 corresponds to a forecast that is out-of-sample as good (by MAD) as the naive random walk forecast in-sample, why can't standard forecasting methods like ARIMA improve on 1.38 for monthly data?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Here, the 1.38 MASE comes from Table 4 in the ungated version. It is the average ASE over 1-24 month ahead forecasts from ARIMA. The other standard methods, like ForecastPro, ETS etc. perform even worse.&lt;/p&gt;&#10;&#10;&lt;p&gt;And here, the answer gets &lt;em&gt;hard&lt;/em&gt;. &lt;a href=&quot;http://www.forecastpro.com/Trends/pdf/FS11%20SF%20Benchmarking.pdf&quot; rel=&quot;nofollow&quot;&gt;It is always &lt;em&gt;very&lt;/em&gt; problematic to judge forecast accuracy without considering the data.&lt;/a&gt; One possibility I could think of in this particular case could be accelerating trends. Suppose that you try to forecast $\exp(t)$ with standard methods. None of these will capture the accelerating trend (and this is usually a Good Thing - if your forecasting algorithm often models an accelerating trend, you will likely far overshoot your mark), and they will yield a MASE that is above 1. Other explanations could, as you say, be different structural breaks, e.g., level shifts or external influences like SARS or 9/11, which would not be captured by the non-causal benchmark models, but which could be modeled by dedicated tourism forecasting methods (although using &lt;em&gt;future&lt;/em&gt; causals in a holdout sample is a kind of cheating).&lt;/p&gt;&#10;&#10;&lt;p&gt;So I'd say that you likely can't say a lot about this withough looking at the data themselves. &lt;a href=&quot;http://www.kaggle.com/c/tourism1/data&quot; rel=&quot;nofollow&quot;&gt;They are available on Kaggle.&lt;/a&gt; Your best bet is likely to take these 518 series, hold out the last 24 months, fit ARIMA series, calculate MASEs, dig out the ten or twenty MASE-worst forecast series, get a big pot of coffee, look at these series and try to figure out what it is that makes ARIMA models so bad at forecasting them.&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT: another point that appears obvious after the fact but took me five days to see - remember that the denominator of the MASE is the &lt;em&gt;one-step ahead&lt;/em&gt; in-sample random walk forecast, whereas the numerator is the average of the &lt;em&gt;1-24-step ahead&lt;/em&gt; forecasts. It's not too surprising that forecasts deteriorate with increasing horizons, so this may be another reason for a MASE of 1.38. Note that the Seasonal Naive forecast was also included in the benchmark and had an even higher MASE.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-11-17T13:24:03.233" Id="124379" LastActivityDate="2014-11-22T04:09:08.940" LastEditDate="2014-11-22T04:09:08.940" LastEditorUserId="1352" OwnerUserId="1352" ParentId="124365" PostTypeId="2" Score="2" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a code which tests each possible order of ARIMA and selects the best model by choosing the one with the absolute minimum sum of lags from the PACF graph. The code then proceeds to add weight to recent errors and runs an optimization on the parameters to get the minimum mean absolute error.&lt;/p&gt;&#10;&#10;&lt;p&gt;The code runs fine and gives excellent results (e.g 0.2% MAPE etc) however once the parameters have been optimized the ACF and PACf graphs show lags outside the threshold.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to add into my code a loop which does the following:&lt;/p&gt;&#10;&#10;&lt;p&gt;if any of the first 4 lags of the ACF or PACF graphs of the residuals found from the optimized ARIMA model are outside the threshold (2/sqrt(n)) then the optimization is re-run but doesn't allow those parameters to be selected/those parameters are skipped in the optimization process.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is my code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;suppressMessages(library(lmtest))&#10;suppressMessages(library(car))&#10;suppressMessages(library(tseries))&#10;suppressMessages(library(forecast))&#10;suppressMessages(library(TTR))&#10;suppressMessages(library(geoR))&#10;suppressMessages(library(MASS))&#10;suppressMessages(gtools))&#10;#-------------------------------------------------------------------------------&#10;Data.col&amp;lt;-c(5403.676,6773.505, 7231.117, 7835.552, 5236.710, 5526.619, 6555.782,11464.727, 7210.069, 7501.610, 8670.903,10872.935, 8209.023, 8153.393,10196.448,13244.502, 8356.733,10188.442,10601.322,12617.821, 11786.526,10044.987,11006.005,15101.946,10992.273,11421.189,10731.312)&#10;#-------------------------------------------------------------------------------&#10;# Turns the Data.col into a time-series&#10;&#10;Data.col.ts &amp;lt;- ts(Data.col, deltat=(1/4), start = c(8,1))&#10;&#10;#-------------------------------------------------------------------------------&#10;# Starts the testing to see if the data should be logged&#10;&#10;trans&amp;lt;- BoxCox.lambda(Data.col, method = &quot;loglik&quot;)&#10;categ&amp;lt;-as.character( c(cut(trans,c(0,0.25,0.75,Inf),right=FALSE)) )&#10;Data.new&amp;lt;-switch(categ,&#10;                 &quot;1&quot;=log(Data.col.ts),&#10;                 &quot;2&quot;=sqrt(Data.col.ts),&#10;                 &quot;3&quot;=Data.col.ts&#10;)&#10;&#10;#----- Weighting ---------------------------------------------------------------&#10;fweight &amp;lt;- function(x){&#10;  PatX &amp;lt;- 0.5+x &#10;  return(PatX)&#10;}&#10;&#10;#Split the integral to several intervals, and pick the weights accordingly&#10;&#10;integvals &amp;lt;- rep(0, length.out = length(Data.new))&#10;for (i in 1:length(Data.new)){&#10;  integi &amp;lt;- integrate(fweight, lower = (i-1)/length(Data.new), upper= i/length(Data.new))&#10;  integvals[i] &amp;lt;- 2*integi$value&#10;}&#10;&#10;#----- Find best ARIMA model ---------------------------------------------------&#10;&#10;a &amp;lt;- permutations(n = 3, r = 6, v = c(0:2), repeats.allowed = TRUE)&#10;a &amp;lt;- a[ifelse((a[, 1] + a[, 4] &amp;gt; 2 | a[, 2] + a[, 5] &amp;gt; 2 | a[, 3] + a[, 6] &amp;gt; 2),&#10;              FALSE, TRUE), ]&#10;&#10;Arimafit &amp;lt;- matrix(0,&#10;                   ncol  = length(Data.new),&#10;                   nrow  = length(a[, 1]),&#10;                   byrow = TRUE)&#10;&#10;totb &amp;lt;- matrix(0, ncol = 1, nrow = length(a[, 1]))&#10;arimaerror &amp;lt;- matrix(0, ncol = length(Data.new), nrow = 1)&#10;&#10;for (i in 1:length(a[, 1])){&#10;  ArimaData.new &amp;lt;- try(Arima(Data.new,&#10;                             order    = a[i, c(1:3)],&#10;                             seasonal = list(order = a[i, c(4:6)]),&#10;                             method   = &quot;ML&quot;),&#10;                       silent = TRUE)&#10;&#10;  if (is(ArimaData.new, &quot;try-error&quot;)){&#10;    ArimaData.new &amp;lt;- arimaerror&#10;  } else {&#10;    ArimaData.new &amp;lt;- ArimaData.new&#10;  }&#10;&#10;  arimafitted &amp;lt;- try(fitted(ArimaData.new), silent = TRUE)&#10;&#10;  if (is(arimafitted, &quot;try-error&quot;)){&#10;    fitarima &amp;lt;- arimaerror&#10;  } else {&#10;    fitarima &amp;lt;- arimafitted&#10;  }&#10;&#10;  if (categ==&quot;1&quot;){&#10;    Arimafit[i, ] &amp;lt;- c(exp(fitarima))&#10;    Datanew &amp;lt;- c(exp(Data.new))&#10;  } else {&#10;    if (categ==&quot;2&quot;){&#10;      Arimafit[i, ] &amp;lt;- c((fitarima)^2)&#10;      Datanew &amp;lt;- c((Data.new)^2)&#10;    } else {&#10;      Arimafit[i, ] &amp;lt;- c(fitarima)&#10;      Datanew &amp;lt;- c(Data.new)&#10;    }&#10;  }&#10;&#10;  data &amp;lt;- c(Datanew)&#10;&#10;  arima.fits &amp;lt;- c(Arimafit[i, ])&#10;&#10;  fullres &amp;lt;- data - arima.fits&#10;&#10;  v &amp;lt;- acf(fullres, plot = FALSE)&#10;&#10;  w &amp;lt;- pacf(fullres, plot = FALSE)&#10;&#10;  if (v$acf[2]&amp;gt;(2/sqrt(length(Data.col)))|v$acf[2]&amp;lt;(-(2/sqrt(length(Data.col))))|v$acf[3]&amp;gt;(2/sqrt(length(Data.col)))|v$acf[3]&amp;lt;(-(2/sqrt(length(Data.col))))|v$acf[4]&amp;gt;(2/sqrt(length(Data.col)))|v$acf[4]&amp;lt;(-(2/sqrt(length(Data.col))))|v$acf[5]&amp;gt;(2/sqrt(length(Data.col)))|v$acf[5]&amp;lt;(-(2/sqrt(length(Data.col))))|v$acf[6]&amp;gt;(2/sqrt(length(Data.col)))|v$acf[6]&amp;lt;(-(2/sqrt(length(Data.col))))|v$acf[7]&amp;gt;(2/sqrt(length(Data.col)))|v$acf[7]&amp;lt;(-(2/sqrt(length(Data.col))))|w$acf[1]&amp;gt;(2/sqrt(length(Data.col)))|w$acf[1]&amp;lt;(-(2/sqrt(length(Data.col))))|w$acf[2]&amp;gt;(2/sqrt(length(Data.col)))|w$acf[2]&amp;lt;(-(2/sqrt(length(Data.col))))|w$acf[3]&amp;gt;(2/sqrt(length(Data.col)))|w$acf[3]&amp;lt;(-(2/sqrt(length(Data.col))))|w$acf[4]&amp;gt;(2/sqrt(length(Data.col)))|w$acf[4]&amp;lt;(-(2/sqrt(length(Data.col))))|w$acf[5]&amp;gt;(2/sqrt(length(Data.col)))|w$acf[5]&amp;lt;(-(2/sqrt(length(Data.col))))|w$acf[6]&amp;gt;(2/sqrt(length(Data.col)))|w$acf[6]&amp;lt;(-(2/sqrt(length(Data.col))))){&#10;    totb[i] &amp;lt;- &quot;n&quot;&#10;  } else {&#10;    totb[i] &amp;lt;- sum(abs(w$acf[1:4]))&#10;  }&#10;&#10;  j &amp;lt;- match(min(totb), totb)&#10;&#10;  order.arima &amp;lt;- a[j, c(1:3)]&#10;&#10;  order.seasonal.arima &amp;lt;- a[j, c(4:6)]&#10;}&#10;&#10;#----- ARIMA -------------------------------------------------------------------&#10;# Fits an ARIMA model with the orders set&#10;stAW &amp;lt;- Arima(Data.new, order= order.arima, seasonal=list(order=order.seasonal.arima), method=&quot;ML&quot;)&#10;parSW &amp;lt;- stAW$coef&#10;    WMAEOPT &amp;lt;- function(parSW)&#10;    {&#10;      ArimaW &amp;lt;- Arima(Data.new, order = order.arima, seasonal=list(order=order.seasonal.arima), &#10;                      include.drift=FALSE, method = &quot;ML&quot;, fixed = c(parSW))&#10;      errAR &amp;lt;- c(abs(resid(ArimaW)))&#10;      WMAE &amp;lt;- t(errAR) %*% integvals &#10;      return(WMAE)&#10;    }&#10;    OPTWMAE &amp;lt;- optim(parSW, WMAEOPT, method=&quot;SANN&quot;, set.seed(2), control = list(fnscale= 1, maxit = 5000))&#10;    # Alternatively, set  method=&quot;Nelder-Mead&quot; or method=&quot;L-BFGS-B&quot; &#10;    parS3 &amp;lt;- OPTWMAE$par&#10;Arima.Data.new &amp;lt;- Arima(Data.new, order = order.arima, seasonal=list(order=order.seasonal.arima), &#10;                        include.drift=FALSE, method = &quot;ML&quot;, fixed = c(parS3))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Before the parameters are optimized it gives a graph like this:&#10;&lt;img src=&quot;http://i.stack.imgur.com/cjY1x.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;After the parameters are optimized it gives a graph like this:&#10;&lt;img src=&quot;http://i.stack.imgur.com/6HKXl.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to stop this happening in the second picture. Is this possible to do using &lt;code&gt;optim&lt;/code&gt;?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-11-17T14:27:31.647" Id="124388" LastActivityDate="2014-11-18T10:22:22.550" LastEditDate="2014-11-18T10:22:22.550" LastEditorUserId="55943" OwnerUserId="55943" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;time-series&gt;&lt;optimization&gt;&lt;arima&gt;&lt;parameterization&gt;" Title="forcing certain parameters to be skipped during optim in R" ViewCount="29" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am confused on what to do with this problem:&lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;Suppose we identify fifty 50-to 54-year old women who have both a mother and sister with a history of breast cancer. Five of these women themselves have developed breast cancer at some time in their lives. If we assume that the expected prevalence rate of breast cancer in women whose mothers have had breast cancer is 4%, then does having a sister with the disease add to the risk?&quot;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-11-17T16:13:43.540" Id="124401" LastActivityDate="2014-11-17T16:13:43.540" OwnerUserId="60889" PostTypeId="1" Score="0" Tags="&lt;binomial&gt;" Title="What method to use in this cancer problem?" ViewCount="26" />
  
  <row Body="&lt;p&gt;&lt;strong&gt;The strength of maximum likelihood estimation is that it is a purely mechanical process.  Just set it up and turn the crank.&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;To avoid ladders of subscripts, let $m=n_1$ and $n=n_2$ and likewise let $\mu=\mu_1$ and $\nu=\mu_2$.  The data are given by the $m+n$-tuple&lt;/p&gt;&#10;&#10;&lt;p&gt;$$(v_1, v_2, \ldots, v_m, w_1, w_2, \ldots, w_n).$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The assumption that all $m+n$ values are independent implies the likelihood is the product of the probability densities evaluated at each of the $m+n$ values:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\eqalign{&#10;&amp;amp;L(\mu,\nu,\sigma; \mathrm{v}, \mathrm{w}) \\&amp;amp;= \prod_{i=1}^m \left(2\pi\sigma^2\right)^{-1/2}\exp\left(-\frac{(v_i-\mu)^2}{2\sigma^2}\right) \prod_{j=1}^n \left(2\pi\sigma^2\right)^{-1/2}\exp\left(-\frac{(w_j-\nu)^2}{2\sigma^2}\right).&#10;}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Taking logarithms gives&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\eqalign{&#10;&amp;amp;\Lambda(\mu_1,\mu_2,\sigma; \mathrm{v}, \mathrm{w}) = \log\left(L(\mu_1,\mu_2,\sigma; \mathrm{v}, \mathrm{w})\right)\\&#10;&amp;amp;=-\frac{n+m}{2}\log(2\pi\sigma^2) - \sum_{i=1}^m \frac{(v_i-\mu)^2}{2\sigma^2} -  \sum_{j=1}^n \frac{(w_j-\nu)^2}{2\sigma^2}.&#10;}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;For $\sigma^2 = 0$ this is undefined. Otherwise it is differentiable with partial derivatives&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\eqalign{&#10;-\frac{\partial\Lambda}{\partial \mu} &amp;amp;= \frac{1}{\sigma^2}\sum_{i=1}^m \left(v_i-\mu\right) \\&#10;-\frac{\partial\Lambda}{\partial \nu} &amp;amp;= \frac{1}{\sigma^2}\sum_{j=1}^n \left(w_j-\nu\right) \\&#10;-\frac{\partial\Lambda}{\partial \sigma} &amp;amp;=\frac{n+m}{\sigma} - \frac{1}{\sigma^3}\left(\sum_{i=1}^m(v_i-\mu)^2 + \sum_{j=1}^n(w_i-\nu)^2\right).&#10;}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The critical points are obtained as the simultaneous zeros of these equations, easily found in sequence as&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\eqalign{&#10;\hat\mu &amp;amp;= \frac{1}{m}\sum_{i=1}^m v_i \\&#10;\hat\nu &amp;amp;= \frac{1}{n}\sum_{j=1}^n w_i \\&#10;\hat\sigma^2 &amp;amp;= \frac{1}{n+m}\left(\sum_{i=1}^m (v_i-\hat\mu)^2 + \sum_{j=1}^n (w_j-\hat\nu)^2\right).&#10;}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;It is straightforward to check this is the unique maximum provided $\hat\sigma^2 \ne 0$.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-11-17T17:24:44.553" Id="124408" LastActivityDate="2014-11-17T18:25:24.877" LastEditDate="2014-11-17T18:25:24.877" LastEditorUserId="919" OwnerUserId="919" ParentId="124239" PostTypeId="2" Score="2" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I've been back-testing a trading strategy that has two parameters (both are # of days to look back), and I've tested the system for robustness by comparing my 10-year Sharpe ratio based on approx 7,000 different combinations of the parameters (see below)&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/UwAs2.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm now trying to figure out the best practice for which parameters I use when going live with the strategy.  Some of the options I'm considering:&lt;/p&gt;&#10;&#10;&lt;p&gt;1)  Actually use all 7,000 combinations and weigh these results into one trading strategy.  The thinking here is to avoid any look-back bias and would incorporate all parameters that I think are logical.&lt;/p&gt;&#10;&#10;&lt;p&gt;2)  Do a version of #1, but rather than incorporating all 7,000 combinations, take the top 1/3 of the combinations and use those.&lt;/p&gt;&#10;&#10;&lt;p&gt;3)  Do a version of #2, but make it more dynamic by recalculating the top 1/3 combinations each week and repeat that process moving forward.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-11-17T19:38:08.780" Id="124433" LastActivityDate="2014-11-17T19:38:08.780" OwnerUserId="25127" PostTypeId="1" Score="0" Tags="&lt;optimization&gt;&lt;parameterization&gt;" Title="Optimize parameters?" ViewCount="27" />
  
  
  <row Body="&lt;p&gt;The primary benefit of using monthly returns data instead of daily return data is that with monthly data, returns are at least &lt;em&gt;approximately&lt;/em&gt; normally distributed (or, at the very least, the simplifying assumption of normality is much less crazy for monthly returns than it is for daily returns).&lt;/p&gt;&#10;&#10;&lt;p&gt;As an aside, check out this quotation from Eugene Fama's book, &lt;em&gt;Foundations of Finance&lt;/em&gt; (available &lt;a href=&quot;http://faculty.chicagobooth.edu/eugene.fama/research/index.htm&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;):&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The usefulness of the portfolio model depends not on whether the&#10;  normality assumption which underlies it is an exact description of the&#10;  world (we know it is not), but on whether the model yields useful&#10;  insights into the essential ingredients of a rational portfolio&#10;  decision. Likewise, the usefulness of the model for securities prices&#10;  depends on how well it describes observed relationships between&#10;  average returns and risk. If the model does well on this score, we can&#10;  live with the small observed departures from normality in monthly&#10;  returns, at least until better models come along.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="1" CreationDate="2014-11-17T21:14:57.530" Id="124443" LastActivityDate="2014-11-17T21:21:35.547" LastEditDate="2014-11-17T21:21:35.547" LastEditorUserId="919" OwnerUserId="41810" ParentId="124404" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;An increasingly common way to show uncertainly is with spaghetti plots (&lt;a href=&quot;http://andrewgelman.com/2012/08/26/graphs-showing-regression-uncertainty-the-code/&quot; rel=&quot;nofollow&quot;&gt;Andrew Gelman&lt;/a&gt; &lt;a href=&quot;http://andrewgelman.com/2014/10/21/try-spaghetti-plot/&quot; rel=&quot;nofollow&quot;&gt;blog posts&lt;/a&gt;), which use overlaid translucent lines. The plot gives a raw view without assuming any model. That may be an advantage in this case since more uncertainty indicators assume all the uncertainty is in the Y direction, but you want to given a sense of the pulse location uncertainty instead.&lt;/p&gt;&#10;&#10;&lt;p&gt;20 overlaid lines:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/p1W45.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Another approach is to use uncertainly bands, which is equivalent to your error bars, but maybe neater. &lt;/p&gt;&#10;&#10;&lt;p&gt;Same data showing quartiles:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/YZpvF.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Either of these plots could be combined with a plot showing just the uncertainty of the pulse location:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/d7JLR.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-11-18T00:34:43.097" Id="124463" LastActivityDate="2014-11-18T00:34:43.097" OwnerUserId="1191" ParentId="124429" PostTypeId="2" Score="3" />
  
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;For a distribution of N values, how can I efficiently upper-bound the largest divergence between all non-negative distributions over the same random field? For example, for all distributions of a random variable that takes values in ([1,2,3,4]), i.e., N = 4, and the probability of a = 1 or a = 2 or a = 3 or a = 4 is always nonzero (but can be very small, e.g., 1e-1000).&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a known bound (other than infinity)? Say, given the number N, the divergence between the uniform distribution [1/4 1/4 1/4 1/4] and &quot;delta&quot; [1e-10 1e-10 1e-10 1/(1+3e-10)] over N is the largest?...&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks all in advance, A.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-18T11:15:18.083" Id="124508" LastActivityDate="2014-11-18T11:15:18.083" OwnerUserId="60973" PostTypeId="1" Score="1" Tags="&lt;mathematical-statistics&gt;&lt;kullback-leibler&gt;&lt;bounds&gt;" Title="Kullback Leibler divergence &quot;efficient&quot; upper bound" ViewCount="25" />
  
  <row Body="&lt;p&gt;Following may be helpful:&lt;/p&gt;&#10;&#10;&lt;p&gt;Your data can have following columns:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;subjectId physics_social pre_post eeg_level&#10;1  physics  pre  10&#10;1  physics  post 15&#10;1  social   pre  12&#10;1  social   post 11&#10;2...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Pre will be the control level. The medium is ignored here. Then ANOVA repeated measures can be applied (following is code in R): &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;aov(eeg_level ~ physics_social * pre_post + Error(subjectID/pre_post), data=yourdata)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Situation similar to yours in described on: &lt;a href=&quot;http://ww2.coastal.edu/kingw/statistics/R-tutorials/repeated.html&quot; rel=&quot;nofollow&quot;&gt;http://ww2.coastal.edu/kingw/statistics/R-tutorials/repeated.html&lt;/a&gt;  (see chicken-pasta example).&lt;/p&gt;&#10;&#10;&lt;p&gt;Another site with good explanation for pre-post data with repeated measures: &lt;a href=&quot;http://www.theanalysisfactor.com/pre-post-data-repeated-measures/&quot; rel=&quot;nofollow&quot;&gt;http://www.theanalysisfactor.com/pre-post-data-repeated-measures/&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;For coding ANOVA with different parameters in R: &lt;a href=&quot;http://www.statmethods.net/stATS/anova.html&quot; rel=&quot;nofollow&quot;&gt;http://www.statmethods.net/stATS/anova.html&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I myself struggled with similar questions and found that expert do not agree on the best approach. Other alternatives are: &lt;/p&gt;&#10;&#10;&lt;p&gt;t-test between post eeg_level values of physics vs social groups (ignoring the pre values)&lt;/p&gt;&#10;&#10;&lt;p&gt;t-test between pre-post difference in eeg_level values of physics vs social groups.&lt;/p&gt;&#10;&#10;&lt;p&gt;linear modelling: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lm(eeg_level~ physics_social+pre_post, data=mydata)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="4" CreationDate="2014-11-18T12:36:33.093" Id="124516" LastActivityDate="2014-11-19T01:55:44.613" LastEditDate="2014-11-19T01:55:44.613" LastEditorUserId="56211" OwnerUserId="56211" ParentId="124339" PostTypeId="2" Score="0" />
  
  
  <row AcceptedAnswerId="124527" AnswerCount="1" Body="&lt;p&gt;I am converting a high-dimensional model to a lower dimensional model by fitting a sliding window of it to a linear (parametric) model and looking at the evolution of parameter values over time.  I'm going from 6.3 million points to about 2500 values of 6 parameters.&lt;/p&gt;&#10;&#10;&lt;p&gt;Physics says the intercept should be a constant value, but when I use LM it moves around.  I think the motion is because of noise, and that it causes other parameter values not to indicate properly.  I would like to set it to a known constant value.&lt;/p&gt;&#10;&#10;&lt;p&gt;How do I make a linear model in R that has a prescribed intercept (not zero).&lt;/p&gt;&#10;&#10;&lt;p&gt;Current code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;for (i in 1:(n-k)){&#10;&#10;  fit &amp;lt;- lm(y ~ x1 + x2 + I(x3^2) + x4 + x5 + x6 , data=data[i:(i+k),])&#10;&#10;  #STORE PARAMETERS INTO VARIABLES&#10;...  #truncated for brevity&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Code that doesn't do the job:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;  fit &amp;lt;- lm(y ~ I(9.81) + x1 + x2 + I(x3^2) + x4 + x5 + x6 , data=data[i:(i+k),])&#10;  fit &amp;lt;- lm(y ~ 9.81 + x1 + x2 + I(x3^2) + x4 + x5 + x6 , data=data[i:(i+k),])&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Question:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;How do I prescribe the constant?&lt;/li&gt;&#10;&lt;li&gt;I tried searching for this both at google and CV - is there a vocabulary that I am missing?&lt;/li&gt;&#10;&lt;li&gt;Can you comment on how something like AIC or R2 are impacted by this model? I prefer to use AIC or BIC and I think that, as model selection criteria they should account for parameters, but the R2 changes (I think) in a fundamental way between the two.&lt;/li&gt;&#10;&lt;li&gt;I tried searching in CV for an answer to this question, but did not find it.  One alternate solution was suggested but its form is substantially different than what was requested.  It is about massaging the inputs, not about formatting the command without fundamentally altering the data. The answer that I liked (and found most useful) is about the form of the formula entered, not about creating new variables. &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;As usual, comments and suggestions are solicited.&lt;/p&gt;&#10;" ClosedDate="2014-11-18T15:15:10.160" CommentCount="4" CreationDate="2014-11-18T13:43:36.653" Id="124525" LastActivityDate="2014-11-18T16:02:09.997" LastEditDate="2014-11-18T16:02:09.997" LastEditorUserId="22452" OwnerUserId="22452" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;terminology&gt;&lt;lm&gt;&lt;intercept&gt;" Title="impose an intercept on lm in r" ViewCount="77" />
  <row AnswerCount="0" Body="&lt;p&gt;Wikipedia says this about the law of total covariance (&lt;a href=&quot;http://en.wikipedia.org/wiki/Law_of_total_covariance&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Law_of_total_covariance&lt;/a&gt;):&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;In probability theory, the law of total covariance,[1] covariance decomposition formula, or ECCE states that if $X$, $Y$, and $Z$ are random variables on the same probability space, and the covariance of $X$ and $Y$ is finite, then&lt;/p&gt;&#10;  &#10;  &lt;p&gt;$\text{cov}(X,Y)=E(\text{cov}(X,Y∣Z))+\text{cov}(E(X∣Z),E(Y∣Z))$.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;It says the following about probability spaces (&lt;a href=&quot;http://en.wikipedia.org/wiki/Probability_space&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Probability_space&lt;/a&gt;):&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;A probability space consists of three parts:[1][2]&lt;/p&gt;&#10;  &#10;  &lt;ol&gt;&#10;  &lt;li&gt;A sample space, $Ω$, which is the set of all possible outcomes.&lt;/li&gt;&#10;  &lt;li&gt;A set of events $\scriptstyle \mathcal{F}$, where each event is a set containing zero or more outcomes.&lt;/li&gt;&#10;  &lt;li&gt;The assignment of probabilities to the events; that is, a function $P$ from events to probabilities.&lt;/li&gt;&#10;  &lt;/ol&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I was not aware of the requirement that the random variables $X,Y$ and $Z$ must share the same probability space and, in particular, a sample space.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is this always true and, if so, why? A proof, or even some examples, showing that the law fails when the variables have different sample spaces would be much appreciated.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-11-18T14:21:10.020" Id="124530" LastActivityDate="2014-11-18T14:21:10.020" OwnerUserId="8503" PostTypeId="1" Score="2" Tags="&lt;probability&gt;&lt;covariance&gt;&lt;variance-covariance&gt;" Title="Can the law of total covariance apply to variables from different sample spaces?" ViewCount="64" />
  
  
  <row Body="&lt;p&gt;There's no need to step through model specifications because you only have one independent variable and one dependent variable.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;model &amp;lt;- glm(cbind(dead,number-dead)~dose,family=binomial(link=&quot;logit&quot;))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2014-11-18T16:34:17.850" Id="124547" LastActivityDate="2014-11-18T17:24:46.663" LastEditDate="2014-11-18T17:24:46.663" LastEditorUserId="22311" OwnerUserId="22311" ParentId="124524" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;After the process of data exploration process and discussion with client, we set up a set of variables as follows:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;T1, T2, T3, T6, T8, T2*T3, T1*t6    &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;During the process of variable selection, step-wise variable selection is based on regression model (lm). I assume that the above selected data set is based on linear regression modeling. At this step, I have the following questions:&lt;/p&gt;&#10;&#10;&lt;p&gt;Is that make sense to use other modeling techniques, such as Random forest and Lasso, based on the above selected set? Or these other techniques can perform poorly?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-18T16:59:24.610" Id="124551" LastActivityDate="2014-11-18T16:59:24.610" OwnerUserId="3026" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;feature-selection&gt;&lt;random-forest&gt;&lt;lasso&gt;&lt;glmnet&gt;" Title="regarding using Lasso and Random forest based on the variable selection result coming from other processes" ViewCount="49" />
  <row Body="&lt;p&gt;Multi-response Permutation Procedures (MRPP) is one possibility, with the two groups being apples and oranges. Note that both the degree of overlap and overall within-group variation matter for the final difference between groups, so it is possible to have two groups that overlap completely be different if they differ in within-group distance. So best to use an ordination technique such as NMDS to visualize the results. Significance value is obtained via permutation. &lt;a href=&quot;http://www.inside-r.org/packages/cran/vegan/docs/mrpp&quot; rel=&quot;nofollow&quot;&gt;http://www.inside-r.org/packages/cran/vegan/docs/mrpp&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-18T17:16:48.027" Id="124553" LastActivityDate="2014-11-18T17:16:48.027" OwnerUserId="57390" ParentId="124506" PostTypeId="2" Score="0" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Fix a Markov chain $\{ X_{t} \}_{t \in \mathbb{N}}$ with mixing time $\tau_{\mathrm{mix}}$. Assume that I know some finite bound on the mixing time $\tau_{\mathrm{mix}} &amp;lt; \tau &amp;lt; \infty$, and that I suspect that my bound isn't very tight (e.g. maybe it is off by a factor of 5).&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is: for fixed $\epsilon &amp;gt; 0$, can I efficiently leverage this upper bound $\tau$ on the mixing time (and possibly some reasonable amount of additional information, such as knowledge of the stationary distribution up to normalizing constant) to obtain a bound such as: &lt;/p&gt;&#10;&#10;&lt;p&gt;$\mathbb{P}[\tau_{\mathrm{mix}} &amp;gt; \tau']&amp;lt; \epsilon$,&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\tau'$ is a random number obtained via simulation that should (hopefully) be smaller than $\tau$ and (after a long enough simulation) quite close to $\tau_{\mathrm{mix}}$? For this to be interesting to me, I need some guarantees that a particular simulation gives me  such an estimate for a particular value of $\epsilon$ - it isn't enough to show that an estimate of the mixing time converges to the correct mixing time eventually (I am aware of this sort of result even without an upper bound on $\tau_{\mathrm{mix}}$, e.g. in recent work of Shalizi and McDonald).&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that without any upper bound on $\tau_{\mathrm{mix}}$, standard examples (such as the `witch's hat') imply that this is essentially impossible. However, any upper bound on $\tau_{\mathrm{mix}}$ effectively rules out these sorts of worst-case situations.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would also be interested in hearing about related problems (e.g. with spectral gap replacing mixing times), and about solutions at any level of generality (e.g. it is ok if the refinement only works for target distributions with rapidly-decaying tails, or even only for finite state spaces).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-18T22:41:17.417" Id="124594" LastActivityDate="2014-11-18T22:45:37.033" LastEditDate="2014-11-18T22:45:37.033" LastEditorUserId="22311" OwnerUserId="61008" PostTypeId="1" Score="2" Tags="&lt;mcmc&gt;&lt;convergence&gt;&lt;markov-chain&gt;&lt;diagnostic&gt;" Title="Refining &quot;good&quot; mixing time estimate" ViewCount="36" />
  <row Body="&lt;p&gt;If the first three are just spatial coordinates and the data are sparse you can simply do a 3D scatter plot with differently sized or colored points for the value. &lt;/p&gt;&#10;&#10;&lt;p&gt;Looks something like this:&#10;&lt;img src=&quot;http://www.rave.gatech.edu/galleryimages/3scatter3d.png&quot; alt=&quot;Scatter&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;If your data is intended to be &lt;em&gt;continuous&lt;/em&gt; in nature and exists on a lattice grid, you can plot several isocontours of the data using &lt;a href=&quot;http://en.wikipedia.org/wiki/Marching_cubes&quot; rel=&quot;nofollow&quot;&gt;Marching Cubes&lt;/a&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;Another approach when you have dense 4D data is to display several 2D &quot;slices&quot; of the data embedded in 3D. It will look something like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://people.rit.edu/pnveme/pigf/ThreeDGraphics/thrd_threev_slice_1.gif&quot; alt=&quot;Slices&quot;&gt;&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-11-18T22:44:52.377" Id="124596" LastActivityDate="2014-11-18T22:58:50.110" LastEditDate="2014-11-18T22:58:50.110" LastEditorUserId="61006" OwnerUserId="61006" ParentId="123767" PostTypeId="2" Score="2" />
  
  <row AnswerCount="1" Body="&lt;p&gt;If $X \sim N(0,1)$, then what is the joint probability distribution of $(X+1,X)$?&lt;/p&gt;&#10;&#10;&lt;p&gt;An attempt: $f(x,x+1)=f(x|x+1)f(x+1)=f(x+1)$, so $N((0,0),(0,0;0,1))$. Note sure though...&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-19T01:42:09.210" Id="124620" LastActivityDate="2014-11-19T09:20:48.243" OwnerDisplayName="user61014" PostTypeId="1" Score="0" Tags="&lt;probability&gt;&lt;distributions&gt;" Title="joint probability distribution with a constant" ViewCount="23" />
  <row AnswerCount="0" Body="&lt;p&gt;If we modify the gradient descent update for a convex objective function $f(\boldsymbol{\theta})$ from $\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \nabla f(\boldsymbol{\theta}_t)$ to $\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - g(\nabla f(\boldsymbol{\theta}_t))$, namely we transform the gradient by a function $g$, how should we choose a $g$ so that the convergence rate is faster than the original gradient descent?&lt;/p&gt;&#10;&#10;&lt;p&gt;More specifically, by running $\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - g(\nabla f(\boldsymbol{\theta}_t))$ we are indeed optimizing another surrogate function $\tilde{f}$ whose gradient is $g(\nabla f(\boldsymbol{\theta}_t))$, and we need $g(\boldsymbol{0})=\boldsymbol{0}$ so that $f$ and $\tilde{f}$ have the same minimum. We also need $g$ to be a non-decreasing function so that $\tilde{f}$ is still convex. But other than that, how should we choose a proper $g$ so that by optimizing $\tilde{f}$ we reach the optimum faster than optimizing $f$? Shall we choose a $g$ so that $\tilde{f}$ has a smaller Lipschitz constant $L$ and larger strongly-convex constant $\mu$ (since these two quantities determines the convergence rate of gradient descent)?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-19T02:56:02.117" Id="124626" LastActivityDate="2014-11-19T02:56:02.117" OwnerUserId="55661" PostTypeId="1" Score="0" Tags="&lt;optimization&gt;" Title="Faster gradient descent convergence by transforming the gradient?" ViewCount="13" />
  <row Body="&lt;p&gt;From basic conditional probability ideas:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\hspace{1cm}p(\theta, X,\alpha) = p(\theta\mid X,\alpha)\, p(X,\alpha)$&lt;/p&gt;&#10;&#10;&lt;p&gt;also&lt;/p&gt;&#10;&#10;&lt;p&gt;$\hspace{1cm}p(\theta, X,\alpha) = p(X \mid \theta,\alpha)\, p(\theta,\alpha)$&lt;/p&gt;&#10;&#10;&lt;p&gt;Hence&lt;/p&gt;&#10;&#10;&lt;p&gt;$\hspace{1cm}p(\theta\mid X,\alpha) = \frac{p(X \mid \theta,\alpha)\, p(\theta,\alpha)}{p(X,\alpha)}$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\hspace{3.3cm} = \frac{p(X \mid \theta,\alpha)\, p(\theta|\alpha)\, p(\alpha)}{p(X,\alpha)}$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\hspace{3.3cm}\propto p(X \mid \theta,\alpha)\, p(\theta|\alpha)\, p(\alpha)$&lt;/p&gt;&#10;&#10;&lt;p&gt;But here $\alpha$ appears to be fixed/known, and given $\theta$, $X$ doesn't depend on $\alpha$ (i.e. $X$ is only related to $\alpha$ via $\theta$ ...  $p(X|\theta,\alpha)=p(X|\theta)$), so the result then follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\hspace{3.3cm}\propto p(X \mid \theta)\, p(\theta|\alpha)$&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-11-19T05:18:19.643" Id="124631" LastActivityDate="2014-11-19T05:35:07.780" LastEditDate="2014-11-19T05:35:07.780" LastEditorUserId="805" OwnerUserId="805" ParentId="124630" PostTypeId="2" Score="5" />
  
  <row AnswerCount="0" Body="&lt;p&gt;If I want to pursue a social research about a service that some Universities provide, what is the minimum number of universities I should visit/contact in order to make the survey significant?&lt;/p&gt;&#10;" ClosedDate="2014-11-19T12:32:25.113" CommentCount="1" CreationDate="2014-11-19T08:11:11.220" Id="124643" LastActivityDate="2014-11-19T08:11:11.220" OwnerUserId="61026" PostTypeId="1" Score="0" Tags="&lt;survey&gt;&lt;social-science&gt;" Title="statistically valid sample" ViewCount="19" />
  
  <row AnswerCount="1" Body="&lt;p&gt;we have conducted an endline survey and now comparing the result with baseline survey. I am interested to find significant difference between baseline and endline. One of sample questions is given below. Baseline and endline sample for this question is different. This question had 5 possible answers. Multiple answers were possible. Now I want to analyse if the there is a significant difference between baseline and endline use of different sources to keep home warm. &lt;/p&gt;&#10;&#10;&lt;p&gt;What do you do to keep your home warm?&#10;&lt;img src=&quot;http://i.stack.imgur.com/b2Tmi.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;What I have done is use z-test to find two sample proportion difference using z-test using this (&lt;a href=&quot;http://epitools.ausvet.com.au/content.php?page=z-test-2&quot; rel=&quot;nofollow&quot;&gt;http://epitools.ausvet.com.au/content.php?page=z-test-2&lt;/a&gt;) resource. &lt;/p&gt;&#10;&#10;&lt;p&gt;I want to know is this a correct way? if not what else coudld be a correct alternative?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-11-19T09:29:08.790" Id="124649" LastActivityDate="2014-11-19T14:33:45.990" LastEditDate="2014-11-19T09:34:26.350" LastEditorUserId="57196" OwnerUserId="57196" PostTypeId="1" Score="2" Tags="&lt;statistical-significance&gt;&lt;survey&gt;&lt;proportion&gt;&lt;group-differences&gt;" Title="Performing significant difference tests in endline evaluation" ViewCount="44" />
  
  
  <row Body="&lt;p&gt;I do not have a text-book answer. However here are some thoughts.&lt;/p&gt;&#10;&#10;&lt;p&gt;Boosting can be seen in direct comparison with bagging. These are two different approaches of the bias variance tradeoff dilemma. While bagging have as weak learners, some learners with low bias and high variance, by averaging the bagging ensemble decrease the variance for a little bias. Boosting on the other hand works well with different weak learners. The boosting weak learners have high bias and low variance. By building up one learner on the top of another, the boosting ensemble tries to decrease the bias, for a little variance. &lt;/p&gt;&#10;&#10;&lt;p&gt;As a consequence, if you consider for example to use bagging and boosting with trees as weak learners, the best way to use is small/short trees with boosting and very detailed trees with bagging. This is why very often a boosting procedure uses a decision stump as weak learner, which is the shortest possible tree (a single if condition on a single dimension). This decision stump is very stable, so it has very low variance.&lt;/p&gt;&#10;&#10;&lt;p&gt;I do not see any reason to use trees with boosting procedures. However, short trees are simple, easy to implement and easy to understand. However, I think that in order to be succesfull with a boosting procedure, your weak learner has to have low variance, has to be rigid, with very few degrees of freedom. For example I see no point to have as a weak learner a neural network.&lt;/p&gt;&#10;&#10;&lt;p&gt;Additionally, you have to note that for some kind of boosting procedures, gradient boosting for example, Breiman found that if the weak learner is a tree, some optimization in the way how boosting works can be done. Thus we have gradient boosting trees. There is a nice exposure of boosting in the ESTL book.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-19T11:11:18.330" Id="124661" LastActivityDate="2014-11-19T11:11:18.330" OwnerUserId="16709" ParentId="124628" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;&lt;strong&gt;Theory&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Polynomial regression is a special case of linear regression. With the main idea of how do you select your features. Looking at the multivariate regression with 2 variables: &lt;code&gt;x1&lt;/code&gt; and &lt;code&gt;x2&lt;/code&gt;. Linear regression will look like this: &lt;code&gt;y = a1 * x1 + a2 * x2.&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Now you want to have a polynomial regression (let's make 2 degree polynomial). We will create a few additional features: &lt;code&gt;x1*x2&lt;/code&gt;, &lt;code&gt;x1^2&lt;/code&gt; and &lt;code&gt;x2^2&lt;/code&gt;. So we will get your 'linear regression':&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;y = a1 * x1 + a2 * x2 + a3 * x1*x2 + a4 * x1^2 + a5 * x2^2&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This nicely shows an important concept &lt;a href=&quot;http://en.wikipedia.org/wiki/Curse_of_dimensionality&quot;&gt;curse of dimensionality&lt;/a&gt;, because the number of new features grows much faster than linearly with the growth of degree of polynomial. You can take a look &lt;a href=&quot;https://www.youtube.com/watch?v=DoQa2Svvo2U&quot;&gt;about this concept here&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Practice with scikit-learn&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;You do not need to do all this in scikit. &lt;a href=&quot;http://scikit-learn.org/stable/modules/linear_model.html#polynomial-regression-extending-linear-models-with-basis-functions&quot;&gt;Polynomial regression&lt;/a&gt; is already available there (in &lt;strong&gt;0.15&lt;/strong&gt; version. Check &lt;a href=&quot;http://stackoverflow.com/q/22914458/1090562&quot;&gt;how to update it here&lt;/a&gt;).&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;from sklearn.preprocessing import PolynomialFeatures&#10;from sklearn import linear_model&#10;&#10;X = [[0.44, 0.68], [0.99, 0.23]]&#10;vector = [109.85, 155.72]&#10;predict= [0.49, 0.18]&#10;&#10;poly = PolynomialFeatures(degree=2)&#10;X_ = poly.fit_transform(X)&#10;predict_ = poly.fit_transform(predict)&#10;&#10;clf = linear_model.LinearRegression()&#10;clf.fit(X_, vector)&#10;print clf.predict(predict_)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2014-11-19T11:32:41.437" Id="124664" LastActivityDate="2014-11-19T11:32:41.437" OwnerUserId="14710" ParentId="58739" PostTypeId="2" Score="5" />
  <row AnswerCount="1" Body="&lt;p&gt;A numerical value was generated for each of 65 samples on machine A at time T, and then on machine B and time S. I tested for a difference between the two runs using R as follows:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;t.test(d$Run1, d$Run2, paired=T)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This showed a p-value of 0.26, which indicates that there is no difference between the runs, or that there is no difference between each pair of values? Is this the correct test to use?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-19T11:36:48.900" Id="124666" LastActivityDate="2014-11-19T12:43:15.280" OwnerUserId="2824" PostTypeId="1" Score="0" Tags="&lt;repeated-measures&gt;&lt;t-test&gt;" Title="Difference between two measures" ViewCount="36" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I used SPSS to do a repeated measures ANOVA including one within- and one between subject factor. Let's say the between-subject factor is treatment group with levels treatment and control, and the within-subject factor is time, with levels pre-treatment and post-treatment. I asked SPSS to also give me the Estimated Marginal Means and descriptive statistics.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the ouput, when I look at the Estimated Marginal Means for the interaction of group and time, I get four means, one for each cell of these two crossed factors. These means are the same as those I get in the descriptive statistics, which is what I expected. However, in the Estimated Marginal Means I also get standard errors of the means and 95% confidence intervals for the four means. I cannot figure out how these were computed, because they do not match the standard deviations for these 4 cells that are reported in the descriptive statistics. Sometimes the standard errors and CIs in the Marginal Means are larger than expected based on the standard deviations of the cells, and sometimes they are smaller.&lt;/p&gt;&#10;&#10;&lt;p&gt;Do the Marginal Means make some kind of adjustment, or compute the standard deviation of the cells in some different way than would be expected?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for your help.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-19T12:46:54.553" Id="124674" LastActivityDate="2015-02-21T05:21:53.997" OwnerUserId="61046" PostTypeId="1" Score="1" Tags="&lt;spss&gt;&lt;repeated-measures&gt;&lt;standard-error&gt;" Title="SPSS - Calculation of confidence intervals in estimated marginal means of repeated measures ANOVA" ViewCount="201" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am working with trees that were fertilized in a full factorial design (N x P x K) in plots that are replicated four times. I currently have a mixed effects model with this structure:  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; library(&quot;nlme&quot;)&#10;&amp;gt; model &amp;lt;- lme(growth ~ P*N*K, data=Lars_noNA, random=~1|rep/block/plot4040)&#10;&amp;gt; summary(model)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Where &lt;code&gt;rep&lt;/code&gt; = replicate, &lt;code&gt;block&lt;/code&gt; is part of the blocking layout, and &lt;code&gt;plot4040&lt;/code&gt; is a plot that received a nutrient treatment. Now I want to growth by individuals measured multiple times over 15 year and but I don't know if this is the way to handle the error associated with the repeated measures.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt;library (nlme)&#10;&amp;gt;model &amp;lt;- lme (AGBnew ~ P*N*K+year, data=mydata_sorted1, &#10;               random=~1|rep/block/plot4040/individual/year) &#10;&amp;gt;summary(model)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2014-11-19T16:02:49.423" Id="124701" LastActivityDate="2014-11-19T16:14:05.113" LastEditDate="2014-11-19T16:14:05.113" LastEditorUserId="7290" OwnerUserId="61058" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;repeated-measures&gt;&lt;lme&gt;" Title="Repeated measures linear mixed effects model" ViewCount="42" />
  
  <row Body="&lt;p&gt;Let's start saying you can use &lt;a href=&quot;https://www.mccallum-layton.co.uk/tools/statistic-calculators/confidence-interval-for-mean-calculator/#confidence-interval-for-mean-calculator&quot; rel=&quot;nofollow&quot;&gt;this tool&lt;/a&gt; to check whether you answer is correct or not. &lt;/p&gt;&#10;&#10;&lt;p&gt;You should get a confidence interval of ±1.13.&lt;/p&gt;&#10;&#10;&lt;p&gt;That said, the correct formula to use is:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/krpwO.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Where the bar x is your average, the z is a constant, s is the standard deviation and n is your sample size. The z varies depending on the confidence interval you are trying to calculate. In your case, because 99%, z is equal to 2.58.&lt;/p&gt;&#10;&#10;&lt;p&gt;Plug all the numbers in the equation and you should get &lt;code&gt;1.130633169350857&lt;/code&gt; which rounded gives you the ±1.13 I mentioned before. And that's it.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-11-19T17:31:56.623" Id="124708" LastActivityDate="2014-11-19T17:31:56.623" OwnerUserId="34675" ParentId="124706" PostTypeId="2" Score="0" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Both bayesglm() (in the arm R package) and various functions in the MCMCpack package are aimed at doing Bayesian estimation of generalized linear models, but I'm not sure they're actually computing the same thing. The MCMCpack functions use Markov chain Monte Carlo to obtain a (dependent) sample from the joint posterior for the model parameters. bayesglm(), on the other hand, produces... I'm not sure what.&lt;/p&gt;&#10;&#10;&lt;p&gt;It looks like bayesglm() produces a point estimate, which would make it MAP (maximum a posteriori) estimation rather than a full Bayesian estimation, but there's a sim() function that looks like it can be used to get posterior draws.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can someone explain the difference in intended use for the two? Can bayesglm() + sim() produce true posterior draws, or is it some sort of approximation?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-11-19T18:18:50.603" Id="124712" LastActivityDate="2014-11-19T19:46:39.447" OwnerUserId="61062" PostTypeId="1" Score="2" Tags="&lt;bayesian&gt;&lt;generalized-linear-model&gt;" Title="bayesglm (arm) versus MCMCpack" ViewCount="49" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm having some trouble wrapping my head around whether using Holt-Winters ETS or an ARIMA model for forecasting sales figures (which are highly seasonal).&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm been using R and the Forecast package -- which already has seen massive improvement from my hand-coded Excel-based ETS and ARIMA models (probably because many background AIC tests are conducted and parameters possibly optimized).&lt;/p&gt;&#10;&#10;&lt;p&gt;In any case, I'm starting with the accuracy() function before I start hand-calculating the accuracy measures myself (so I know more what's going on). &lt;/p&gt;&#10;&#10;&lt;p&gt;I know what most of the measures mean, but I'm afraid I'm not exactly sure about auto-correlation. I know that means the tendency of -- well, I assume the forecast rather than the sample, which is the same for both methods --- the tendency of the forecast to correlate with itself (given lag 1, or lag n, I'm not sure).&lt;/p&gt;&#10;&#10;&lt;p&gt;Is this a measure of accuracy? Or model fit? Or just a 'good to know'? I'm not sure what a large, small, or negative number might mean here. Well, I do -- negative means the forecast is likely to swing the opposite direction of the past --- but how does this judge the forecast accuracy?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for your help -- I have a stack of statistics and forecasting books (Armstrongs Principles of Forecasts and Makridakis, Wheelwright, and Hyndman's Forecasting Methods) --- both are great. Armstrong's is very high level, which is great -- the other explains how to properly execute methods and the reasoning -- but I still find the nitty gritty details of my statistics and forecast knowledge lacking in some fundamentals.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-11-19T18:26:35.740" Id="124714" LastActivityDate="2014-11-19T18:26:35.740" OwnerUserId="48454" PostTypeId="1" Score="0" Tags="&lt;time-series&gt;&lt;forecasting&gt;&lt;autocorrelation&gt;" Title="autocorrelation in evaluating time series forecasts" ViewCount="42" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am planning to build 5 successive random forests (RF) on a same data using r 'randomForest' package. I am leveraging work done as per the &lt;a href=&quot;https://sites.google.com/site/houtaodeng/publications&quot; rel=&quot;nofollow&quot;&gt;page&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;while building the first RF, each X variable should have equal chance of getting selected when that RF decides to split the data at any node.&lt;/p&gt;&#10;&#10;&lt;p&gt;But during the next RFs, chances of each variable getting selected for a split should be based upon variable importance given by the earlier RF. For example, if&lt;/p&gt;&#10;&#10;&lt;p&gt;RF1$importance is&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    MeanDecreaseGini&#10;X1         3.3182386&#10;X2         0.6457600&#10;X3         0.6196704&#10;X4         0.5687371&#10;X5         1.6559662&#10;X6         0.4337502&#10;X7         0.5900012&#10;X8         0.4284394&#10;X9         0.8748509&#10;X10        0.4065861&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;then during RF2, X1 variable should have higher probability of getting selected and so on. How can i make RF to choose variables for a split based upon some predefined variable importance array?&lt;/p&gt;&#10;&#10;&lt;p&gt;I know mtry parameter allows us to specify &quot;Number of variables randomly sampled as candidates at each split.&quot;, but i want to specify probability of selecting each variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;----------------update 1-------------------------------&lt;/p&gt;&#10;&#10;&lt;p&gt;adding sample code&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(randomForest)&#10;require(rpart)&#10;fit &amp;lt;- randomForest(Kyphosis ~ Age + Number + Start,   data=kyphosis)&#10;fit$mtry&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;above code will fit a random forest for a classification problem. But lets say i want to increase probability of selecting variable &quot;Age&quot; for any split. Right now each variable has an equal chance of getting selected. How can i achieve that?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-11-19T19:18:21.723" Id="124722" LastActivityDate="2014-11-20T22:40:50.917" LastEditDate="2014-11-20T22:40:50.917" LastEditorUserId="29065" OwnerUserId="29065" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;random-forest&gt;" Title="Package ‘randomForest’ R defining variable importance in advance" ViewCount="73" />
  <row AnswerCount="0" Body="&lt;p&gt;I've created a recurrent neural network, to which normalised values are passed as inputs. The normalization formula is:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\tilde{x_{i}} = \frac{1}{1+exp(-\frac{x_i-\bar{x}}{\sigma})},$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\bar{x}$ is the mean, and $\sigma$ is the standard deviation.&lt;/p&gt;&#10;&#10;&lt;p&gt;This normalizes values to the interval ${0,1}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, this formula works fine for lists greater than one element, but clearly will not work for elements with only one element, as the standard deviation will be zero. &lt;/p&gt;&#10;&#10;&lt;p&gt;I was wondering are they any alternatives I could use so that the normalisation of 10.5 will be the same, no matter if it is a single element or in a list?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-11-19T19:29:35.073" Id="124723" LastActivityDate="2014-11-19T19:29:35.073" OwnerUserId="61067" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;normal-distribution&gt;&lt;standard-deviation&gt;&lt;normalization&gt;" Title="Normalisation formula applicable to one or more data items" ViewCount="24" />
  
  
  
  
  <row AnswerCount="3" Body="&lt;p&gt;Suppose you are trying to estimate the pdf of a random variable $X$, for which there are tons of i.i.d. samples $\{X_i\}_{i=1}^{n}$ (i.e. $n$ is very large, think thousands - millions).&lt;/p&gt;&#10;&#10;&lt;p&gt;One option is to estimate the mean and variance, and to just assume it's Gaussian.&lt;/p&gt;&#10;&#10;&lt;p&gt;On the other end, one can take kernel density estimates, to get something more accurate (especially when there's so much data).&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem is, that I need to evaluate the resulting pdf very very fast. If I assume the pdf is Gaussian, then evaluating the pdf $f_X(x)$ is very fast, but the estimate might not be accurate. On the other hand, kernel density estimates will be way too slow to use. &lt;/p&gt;&#10;&#10;&lt;p&gt;So the question is: what are common ways to get pdf estimates that are more general than Gaussians, but in an incremental fashion? Ideally, I'd like to have a model with a number of parameters (say $k$), that can be used to trade-off estimation accuracy and evaluation speed.&lt;/p&gt;&#10;&#10;&lt;p&gt;Possible directions I thought about are:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Estimate the moments of the distribution, and find the pdf based on these moments alone. $k$ here is the number of moments. But then, what is the model for the pdf based on the model? &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Gaussian mixtures with $k'$ mixtures (here $k=3k'-1$ since for each element of the mixture we keep the mean, variance and weight, and the weights sum to one). Is this a good idea?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Any other ideas are welcome.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;&#10;&lt;p&gt;Related questions: &lt;a href=&quot;http://stats.stackexchange.com/questions/82663/fast-approximate-maximum-likelihood-parameter-estimation&quot;&gt;ML estimation&lt;/a&gt;; &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Update / clarification:&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for all the answers so far. &lt;/p&gt;&#10;&#10;&lt;p&gt;I really need the pdf (not the cdf, and not to sample from this distribution). Specifically, I am using the scalar pdf estimates for Naive Bayes (NB) classification and regression: given the label, each of the features has a pdf, and the NB assumption says that they are independent. So in order to calculate the posterior (the probability of the label given the feature values) I need the different pdf's evaluated at the observed feature values.  &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-11-19T22:15:10.613" FavoriteCount="1" Id="124746" LastActivityDate="2014-11-21T04:19:36.257" LastEditDate="2014-11-20T07:34:46.420" LastEditorUserId="46089" OwnerUserId="46089" PostTypeId="1" Score="5" Tags="&lt;estimation&gt;&lt;pdf&gt;" Title="Fast density estimation" ViewCount="125" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm an undergraduate student. I read about multivariate normal distribution in hogg and craig. And i wonder why the covariance is allowed to be positive SEMI-definite. I read this&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://mathoverflow.net/questions/77973/normal-distribution-with-positive-semi-definite-covariance-matrix&quot;&gt;Normal distribution with positive SEMI-definite covariance matrix&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;And I found this&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=6&amp;amp;cad=rja&amp;amp;uact=8&amp;amp;ved=0CD0QFjAF&amp;amp;url=http%3A%2F%2Firs.ub.rug.nl%2Fdbi%2F43789c5b13152&amp;amp;ei=tixtVLmOF4qXuASf5oL4BQ&amp;amp;usg=AFQjCNGSByhXz5dayhW5sDHg3iCRqdfmkQ&amp;amp;sig2=GDo_zl_O8Nh6eTQpYY1Jmg&amp;amp;bvm=bv.80120444,d.c2E&quot; rel=&quot;nofollow&quot;&gt;Multivariate statistic&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I don't understand it actually. it talks about affine subspace or something. We still need the probability distribution to integrate out to 1. 1 is a real number, so what is the relation of that &quot;affine subspace&quot; with $R^n$?? Can anybody explain it in simple way? I am totally curious about this. Any illustration will be appreciated, Thanks..&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-20T01:20:19.077" Id="124768" LastActivityDate="2014-11-20T02:11:03.600" LastEditDate="2014-11-20T02:00:45.480" LastEditorUserId="48417" OwnerUserId="48417" PostTypeId="1" Score="0" Tags="&lt;normal-distribution&gt;&lt;multivariate-analysis&gt;&lt;variance-covariance&gt;&lt;singular&gt;" Title="Multivariate normal with singular covariance" ViewCount="50" />
  <row AnswerCount="1" Body="&lt;p&gt;I've got a multivariate dataset (p=2) that I'm trying to calculate the W matrix for use in canonical variates analysis&lt;/p&gt;&#10;&#10;&lt;p&gt;If each $x_{kj}$ is the jth observational unit from the kth group, and $\bar{x}_k$ is the mean vector for the kth group, then:&#10;$W = \sum\limits_{k=1}^K{\sum\limits_{j=1}^{n_k}{(x_{kj} - \bar{x}_k)(x_{kj} - \bar{x}_k)^T}}$    &lt;/p&gt;&#10;&#10;&lt;p&gt;Doing this as a set of nested loops is easy enough, but I cannot for the life of me see how to vectorise this.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hopefully this code doesn't make you want to poke your eyes out, but here's my first attempt (the relevant stuff is at the bottom):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#dataframe setup K=6 (one for each sex/rug combination), n=4 for every group&#10;&#10;df = data.frame(rbind(&#10;      cbind(rep(&quot;M&quot;, 4), rep(&quot;A&quot;, 4), c(5,5,9,7), c(6,4,9,6)),&#10;      cbind(rep(&quot;M&quot;, 4), rep(&quot;B&quot;, 4), c(7,7,9,6), c(6,7,12,8)),&#10;      cbind(rep(&quot;M&quot;, 4), rep(&quot;C&quot;, 4), c(21,14,17,12), c(15,11,12,10)),&#10;      cbind(rep(&quot;F&quot;, 4), rep(&quot;A&quot;, 4), c(7,6,9,8), c(10,6,7,10)),&#10;      cbind(rep(&quot;F&quot;, 4), rep(&quot;B&quot;, 4), c(10,8,7,6), c(13,7,6,9)),&#10;      cbind(rep(&quot;F&quot;, 4), rep(&quot;C&quot;, 4), c(16,14,14,10), c(12,9,8,5))&#10;      ), stringsAsFactors = FALSE)&#10;&#10;colnames(df) = c(&quot;Sex&quot;, &quot;Drug&quot;, &quot;X1&quot;, &quot;X2&quot;)&#10;&#10;df$Sex = as.factor(df$Sex)&#10;df$Drug = as.factor(df$Drug)&#10;df$X1 = as.numeric(df$X1)&#10;df$X2 = as.numeric(df$X2)&#10;df$Group = as.factor(paste(df$Sex, df$Drug, sep=&quot;&quot;))&#10;&#10;xbar = lapply(df[,3:4], mean)&#10;&#10;groupMeans = aggregate(cbind(X1, X2)~Group,df,mean)&#10;&#10;#within groups SS calculation&#10;&#10;W = matrix(0, nrow = 2, ncol = 2)&#10;for(k in unique(df$Group)){&#10;  groupEntries = subset(df, select = c(X1,X2), subset=(Group == k))&#10;  xbar = subset(groupMeans, select = c(X1,X2), subset=(Group == k))&#10;  n = dim(groupEntries)[[1]]&#10;  for(j in 1:n){&#10;    centred = as.matrix(groupEntries[j,] - xbar)&#10;    W = W + t(centred) %*% centred&#10;  }&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-11-20T01:57:48.840" Id="124770" LastActivityDate="2014-11-20T08:08:33.587" OwnerUserId="25911" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;sums-of-squares&gt;&lt;canonical-correlation&gt;" Title="Vectorise Within-Groups Sum of Squares in R" ViewCount="25" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I hope the question to be clear.&lt;/p&gt;&#10;&#10;&lt;p&gt;The name &quot;semi-distribution&quot; certainly implies some meaning, yet, I'm unable to conclude what really means.&lt;/p&gt;&#10;&#10;&lt;p&gt;I found the term on this paper: &lt;a href=&quot;http://www.info.ucl.ac.be/~pdupont/pdupont/pdf/pr05.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.info.ucl.ac.be/~pdupont/pdupont/pdf/pr05.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;edit&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The definition given in the paper is:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Let $\Sigma$ be a finite alphabet, a &lt;em&gt;semi-distribution&lt;/em&gt; over $\Sigma^*$ is a function $\psi:\Sigma^*\to[0,1]$ satisfying $\sum_{u \in \Sigma^*} \psi(u)\le1$.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;My guess is that it is a semi-distribution and not a distribution because the sum of the probability of all events is possibly less than 1.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-11-20T04:15:56.133" Id="124781" LastActivityDate="2015-01-29T00:41:58.713" LastEditDate="2015-01-29T00:41:58.713" LastEditorUserId="22311" OwnerUserId="61106" PostTypeId="1" Score="1" Tags="&lt;distributions&gt;&lt;terminology&gt;" Title="What is the difference between a distribution and a semi-distribution?" ViewCount="78" />
  <row Body="&lt;p&gt;Most pressingly you need a repeated-measures design to account for the variable you measured twice (blood sugar). You could do this with a repeated-measures ANOVA (or more specifically a mixed-measures ANOVA since you would be including other non-repeated variables in your model).&lt;/p&gt;&#10;&#10;&lt;p&gt;This is a simple guide to choosing the right test:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.ats.ucla.edu/stat/mult_pkg/whatstat/&quot; rel=&quot;nofollow&quot;&gt;http://www.ats.ucla.edu/stat/mult_pkg/whatstat/&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;And you can use any of Andy Field's texts for a readable introductory reference to the theory and practical points of running these tests in SAS, R, or SPSS.&lt;/p&gt;&#10;&#10;&lt;p&gt;As an aside, depending on the experimental design, you may not need to control for the other variables in the model. If this is a well-executed RCT, the differences observed in various confounders between groups arise from chance and don't need to be modelled.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-11-20T05:02:15.457" Id="124785" LastActivityDate="2014-11-20T05:02:15.457" OwnerUserId="59627" ParentId="124148" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;In an &lt;a href=&quot;http://dx.doi.org/10.1037/0003-066X.60.5.410&quot; rel=&quot;nofollow&quot;&gt;article&lt;/a&gt; in the American Psychologist there is a series of bar graphs with a statistic next each bar. The statistic is described in the text and in the graph captions as being an effect-size measure. The statistics is $\lambda^2$. Now, I can find explanations of $\eta^2$, Wilks $\lambda$, and Cohen's $f^2$ as effect size measures but not of $\lambda^2$. What have I missed? Or is it more likely a typograpical error of some kind?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-11-20T05:19:49.363" Id="124787" LastActivityDate="2014-11-20T11:41:24.113" LastEditDate="2014-11-20T11:41:24.113" LastEditorUserId="61108" OwnerUserId="61108" PostTypeId="1" Score="3" Tags="&lt;effect-size&gt;" Title="Definition of $\lambda^2$ effect size measure" ViewCount="37" />
  
  <row AcceptedAnswerId="124850" AnswerCount="1" Body="&lt;p&gt;I suspect that this is entirely possible since the endogenous variable coefficient can biased in many possible way, thus leading to a near 0 estimate despite having a real causal relationship.&lt;/p&gt;&#10;&#10;&lt;p&gt;More formally, let $Y$ be the dependent variable, $X$ be the endogenous independent variable, and $Z$ be the instrument for $X$.&lt;/p&gt;&#10;&#10;&lt;p&gt;We have the naive regression:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;Y = \beta_0 + \beta_1X&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and the two-stage-least square regression:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;\hat X = \hat\delta_0 + \hat\delta_1Z \\&#10;Y = \mu_0 + \mu_1\hat X&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Is it possible for $\mu_1$ (2SLS estimate) to be significant whereas $\beta_1$ (naive regression estimate) is not?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-11-20T05:53:19.797" FavoriteCount="1" Id="124791" LastActivityDate="2014-11-20T16:13:44.130" LastEditDate="2014-11-20T15:29:25.087" LastEditorUserId="20148" OwnerUserId="20148" PostTypeId="1" Score="2" Tags="&lt;statistical-significance&gt;&lt;instrumental-variables&gt;" Title="Can the endogenous variable be insignificant while the instrument is significant?" ViewCount="64" />
  <row AnswerCount="0" Body="&lt;p&gt;I would appreciate some help and guidance on this issue. I have a panel data that looks like this.&#10;&lt;img src=&quot;http://i.stack.imgur.com/EXwJH.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;What I want to do is use the &lt;code&gt;FDIflow&lt;/code&gt; variable as a dependent variable and check how the other independent variables (gdp, literacy, riks, oil, ...etc) will impact it.&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem is that I have many missing values for the dependent variable. What I think about doing  is to replace the missing values with zeros and use a Tobit model with the censoring option ll(0) as shown below in the code. So my question do you think this is a good idea or there is a better way to handle this. Thank you in advance !&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;*Stata Code&#10;replace normfdiflow = 0 if missing(normfdiflow)&#10;xttobit normfdiflow logGDP logGDPcap CHNexp secondenroll dist cpi er gdpgr phone  openess voiceacc polstability goveff regqual rulelaw corrupcon, ll(0)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="3" CreationDate="2014-11-20T06:53:14.657" Id="124794" LastActivityDate="2014-11-20T07:28:52.373" LastEditDate="2014-11-20T07:28:52.373" LastEditorUserId="2116" OwnerUserId="61111" PostTypeId="1" Score="0" Tags="&lt;dataset&gt;&lt;missing-data&gt;&lt;panel-data&gt;" Title="Missing Values in Dependent Variable and the Question on Tobit model (Panel Data)" ViewCount="31" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Let's say that we have bag of marbles. We grab a random marble, and measure its weight. But because our sensor is noisy, we take $m$ different measurements. We do this $n$ times, getting $m*n$ measurements total.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm interested in inferring the following quantities:&lt;/p&gt;&#10;&#10;&lt;p&gt;1) The variance of the sensor.&lt;/p&gt;&#10;&#10;&lt;p&gt;2) The variance of the weights of marbles.&lt;/p&gt;&#10;&#10;&lt;p&gt;3) The variance of the estimate of population mean weight.&lt;/p&gt;&#10;&#10;&lt;p&gt;What algorithm should I use? Is there a name for this problem in the literature?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-20T09:47:42.493" Id="124807" LastActivityDate="2014-11-20T09:47:42.493" OwnerUserId="53332" PostTypeId="1" Score="0" Tags="&lt;variance&gt;" Title="Infer the variance of a sensor" ViewCount="10" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a Likert scale questionnaire of 5 scales (from not important at all to very important) and i want to analyse this questionnaire to find out the weight of each factor. and then get to a maturity model and in the end, an instrument.&lt;/p&gt;&#10;&#10;&lt;p&gt;The aim of this analyze is to getting to a maturity model as an instrument so that companies can find out how mature they are. &lt;/p&gt;&#10;&#10;&lt;p&gt;consider this equation in mind, Y = W1X1 + W2X2 + W3X3 + ...&lt;/p&gt;&#10;&#10;&lt;p&gt;(X)es are my factors that respondents should say how important they are in getting to (Y) in their own companies, from 1 to 5.&lt;/p&gt;&#10;&#10;&lt;p&gt;my issue is that first i have to find all of the weights (W1,W2,W3,..) of the factors, so next time i will have the mentioned equation something like this, for example: &#10;Y = (0.034)X1 + (0.564)X2 + (0.342)X3 + ... &lt;/p&gt;&#10;&#10;&lt;p&gt;when i get to this stage, respondents will just give value to the (X)es, I mean in next round of questionnaire, and through this we will get the Y value as an Index, then i can proceed in using this Index as an indicator of the position of the company in developed maturity model.&lt;/p&gt;&#10;&#10;&lt;p&gt;for example consider they enter 3,5,2 as their (X)es: Y = (0.034)x(3) + (0.564)x(5) + (0.342)x(2) =&gt; Y= 2.922&lt;/p&gt;&#10;&#10;&lt;p&gt;please tell me if i am using the right way, and then if yes, please show me the way, Thank you friends, please help me in this, i really need your answer urgently. thanks&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-20T09:52:54.843" FavoriteCount="1" Id="124808" LastActivityDate="2014-11-20T09:52:54.843" OwnerUserId="61102" PostTypeId="1" Score="0" Tags="&lt;spss&gt;&lt;regression-coefficients&gt;" Title="How use standardized regression coefficient in analyzing Likert scale questionnaire to calculate the weight of each factor?" ViewCount="50" />
  
  <row AcceptedAnswerId="125549" AnswerCount="1" Body="&lt;p&gt;I want to estimate a multilevel logit model. But I'm confused about the minimum number of groups and observations per group. What would be the minimum number of observations per group? &#10;My case: I have a small sample over 200 observations that I can classify in 40 groups, but there are some groups with only one observation. It's possible to estimate the model with this approach? Would be better classify in 5 groups with more observations per group? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-20T10:09:49.960" FavoriteCount="1" Id="124812" LastActivityDate="2014-11-26T09:07:48.870" LastEditDate="2014-11-21T10:07:50.753" LastEditorUserId="46168" OwnerUserId="46168" PostTypeId="1" Score="1" Tags="&lt;multilevel-analysis&gt;&lt;small-sample&gt;&lt;lme4&gt;" Title="Size multilevel logit model" ViewCount="31" />
  <row Body="&lt;p&gt;This has been covered before.  A model that is constrained to have predicted values in $[0,1]$ cannot possibly have an additive error term that would make the predictions go outside $[0,1]$.  Think of the simplest example of a binary logistic model -- a model containing only an intercept.  This is equivalent to the Bernoulli one-sample problem, often called (in this simple case) the binomial problem because (1) all the information is contained in the sample size and number of events or (2) the Bernoulli distribution is a special case of the binomial distribution with $n=1$.  The raw data in this situation are a series of binary values, and each has a Bernoulli distribution with unknown parameter $\theta$ representing the probability of the event.  There is no error term in the Bernoulli distribution, there's just an unknown probability.  The logistic model is a probability model.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-20T12:42:36.140" Id="124827" LastActivityDate="2014-11-20T12:42:36.140" OwnerUserId="4253" ParentId="124818" PostTypeId="2" Score="3" />
  
  
  
  <row AcceptedAnswerId="124844" AnswerCount="2" Body="&lt;p&gt;I often see forecasting methods described as long term or short term methods.&lt;/p&gt;&#10;&#10;&lt;p&gt;I assume the difference between short term and long term forecasting cannot just be the amount of time. I assume this because the sampling frequency and the rate at which the series changes is also significant. Also some series are timeless i.e. those derived form equations.&lt;/p&gt;&#10;&#10;&lt;p&gt;So what defines a short term and long term forecast?&lt;/p&gt;&#10;&#10;&lt;p&gt;Many thanks&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-11-20T14:40:02.500" Id="124843" LastActivityDate="2014-11-20T15:03:15.473" OwnerUserId="34953" PostTypeId="1" Score="2" Tags="&lt;forecasting&gt;" Title="What is the distinction between short term and long term forecasting?" ViewCount="189" />
  <row AnswerCount="0" Body="&lt;p&gt;I was wondering if someone knows good books or references that deal with this subject :&lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;The rate of convergence of the coverage probability of bootstrap confidence intervals&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;Many thanks for your help,&#10;Philippe&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-11-20T16:25:58.247" Id="124853" LastActivityDate="2014-11-21T07:34:45.797" OwnerUserId="61145" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;bootstrap&gt;&lt;convergence&gt;" Title="Rate of convergence of the coverage probability of bootstrap confidence intervals" ViewCount="52" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a question concerning the implementation of a bayes posterior&#10;predictive check. Let us assume i have this model (implementation is&#10;in R and jags):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;model {&#10;  for( i in 1 : N ) {&#10;     y[i] ~ dlnorm( mu , 1/sig^2 )&#10;   }&#10; sig ~ dunif( 0.01 , 1000 )&#10; mu ~ dnorm( 0,1E-6 )&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I want to estimate the parameters of a lognormal distribution given&#10;some data $y$. For the both parameters of the lognormal i use an&#10;uninformative prior distribution. I carry out the mcmc sampling using&#10;(code is form the book &quot;Doing bayesian data analysis&quot;):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;parameters = c(&quot;mu&quot; , &quot;sig&quot;)&#10;adaptSteps = 1000              &#10;burnInSteps = 1000            &#10;nChains = 3                   &#10;numSavedSteps=30000 &#10;thinSteps=1                  &#10;nPerChain = ceiling( ( numSavedSteps * thinSteps ) / nChains ) &#10;&#10;# Create, initialize, and adapt the model:&#10;jagsModel = jags.model( &quot;model.txt&quot; , data=datalist ,&#10;                    n.chains=nChains , n.adapt=adaptSteps )&#10;# Burn-in:&#10;cat( &quot;Burning in the MCMC chain...\n&quot; )&#10;update( jagsModel , n.iter=burnInSteps )&#10;# The saved MCMC chain:&#10;cat( &quot;Sampling final MCMC chain...\n&quot; )&#10;codaSamples = coda.samples( jagsModel , variable.names=parameters ,&#10;                        n.iter=nPerChain , thin=thinSteps )&#10;mcmcChain = as.matrix( codaSamples )&#10;chainLength = NROW(mcmcChain)&#10;&#10;mu &amp;lt;- mcmcChain[, &quot;mu&quot;]&#10;sig &amp;lt;-  mcmcChain[, &quot;sig&quot;]&#10;L &amp;lt;- length(mu)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now i want to check, how exact my model fits the dataset. According to&#10;Gleman (2003, P.162), i have $L$ simulations from the posterior density of the&#10;parameters mu and sig. For each of the $L$ parameters i simulated a new dataset&#10;and calculate the test statistic with this new dataset and the original dataset and compare the results.&#10;My algorithm looks like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;result &amp;lt;- numeric(L)&#10;for(i int 1:L) {&#10;  yrep &amp;lt;- rlnorm(length(y),mu[i],sig[i])&#10;  result[i] &amp;lt;- t(log(yrep),mu[i]) &amp;gt;= t(log(y),mu[i])&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The function $t$ is the test statistic and checks if the model is adequate except for the extreme tails. It is defined like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$T(y,m) = |y_{x_1} - m | - |y_{x_2} - m|$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $x_1$ and $x_2$ are chosen to represent approximately 90 and 10&#10;percent of the distribution (see Gelman 2003, P.165).&lt;/p&gt;&#10;&#10;&lt;p&gt;Is this the right approach? If yes, would it be enough to use the&#10;parameter values in the highest density interval only? I am fairly new to&#10;bayesian statistics so i am unsure if i understood the whole procedure.&#10;And is there a better test statistic? I know that my data will be approximately&#10;lognormal. But at least in the lower tails there will be a lot of noise (the data are diameter distributions of particles recorded continuously by an optical sensor with limited resolution). In future the algorithm should run from time to time during the recording and send a notification when there is a huge deviation from the lognormal model.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for the help!&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is the complete source&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;modelstring = &quot;&#10;model {&#10;  for( i in 1 : N ) {&#10;    y[i] ~ dlnorm( mu , 1/sig^2 )&#10;&#10;  }&#10;  sig ~ dunif( 0.01 , 1000 )&#10;  mu ~ dnorm( 0,1E-6 )&#10;}&#10;&quot; # close quote for modelstring&#10;writeLines(modelstring,con=&quot;model.txt&quot;)&#10;&#10;&#10;y &amp;lt;- rlnorm(1000,5,2)&#10;&#10;datalist &amp;lt;- list(&#10;    y = y,&#10;    N = length(y))&#10;&#10;parameters = c(&quot;mu&quot; , &quot;sig&quot;)&#10;adaptSteps = 1000              # Number of steps to &quot;tune&quot; the samplers.&#10;burnInSteps = 1000            # Number of steps to &quot;burn-in&quot; the samplers.&#10;nChains = 3                   # Number of chains to run.&#10;numSavedSteps=30000           # Total number of steps in chains to save.&#10;thinSteps=1                   # Number of steps to &quot;thin&quot; (1=keep every step).&#10;nPerChain = ceiling( ( numSavedSteps * thinSteps ) / nChains ) # Steps per chain.&#10;# Create, initialize, and adapt the model:&#10;jagsModel = jags.model( &quot;model.txt&quot; , data=datalist ,&#10;                        n.chains=nChains , n.adapt=adaptSteps )&#10;# Burn-in:&#10;cat( &quot;Burning in the MCMC chain...\n&quot; )&#10;update( jagsModel , n.iter=burnInSteps )&#10;# The saved MCMC chain:&#10;cat( &quot;Sampling final MCMC chain...\n&quot; )&#10;codaSamples = coda.samples( jagsModel , variable.names=parameters ,&#10;                            n.iter=nPerChain , thin=thinSteps )&#10;mcmcChain = as.matrix( codaSamples )&#10;chainLength = NROW(mcmcChain)&#10;&#10;mu &amp;lt;- mcmcChain[, &quot;mu&quot;]&#10;sig &amp;lt;-  mcmcChain[, &quot;sig&quot;]&#10;L &amp;lt;- length(mu)&#10;&#10;v &amp;lt;- numeric()&#10;for( i in 1:L){&#10;    yrep &amp;lt;- rlnorm(length(y),mu[i],sig[i])&#10;    v[i] &amp;lt;- t(log(yrep),mu[i]) &amp;gt;= t(log(y),mu[i])&#10;}&#10;&#10;t &amp;lt;- function(dist,theta){&#10;    abs(orderstat(dist,floor(length(dist)*0.90)) - theta) -&#10;        abs(orderstat(dist,floor(length(dist)*0.10)) - theta)&#10;}&#10;orderstat &amp;lt;- function(x,pos){&#10;    sort(x)[pos]&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Reference:&lt;/strong&gt;&#10;2003 &quot;Bayesian Data Analysis&quot;, Andrew Gelman et. al&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-20T17:11:37.843" Id="124857" LastActivityDate="2014-11-20T17:11:37.843" OwnerUserId="22167" PostTypeId="1" Score="2" Tags="&lt;p-value&gt;&lt;posterior&gt;&lt;bayes&gt;" Title="Implementation of Bayes posterior predictive check" ViewCount="71" />
  
  <row Body="&lt;p&gt;You can do this in Stata like this using &lt;a href=&quot;http://www.nber.org/cycles/cyclesmain.html&quot; rel=&quot;nofollow&quot;&gt;US recessions&lt;/a&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ssc install nbercycles&#10;ssc install freduse&#10;freduse GDP, clear&#10;gen ym = mofd(daten)&#10;tsset ym, monthly&#10;nbercycles GDP if tin(1947m1,2014m7), file(graph.do) replace&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The file graph.do will include detailed code about how to build such a graph by hand if you want to use a different country. &lt;/p&gt;&#10;&#10;&lt;p&gt;The graph will look like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/8slR0.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-11-20T21:48:59.367" Id="124883" LastActivityDate="2014-11-21T01:40:34.657" LastEditDate="2014-11-21T01:40:34.657" LastEditorUserId="7071" OwnerUserId="7071" ParentId="124725" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;If the values are numeric, i.e. integers between 1 and 100, you can consider modeling this variable as continuous with a parametric distribution (e.g. Gaussian). This has less parameters, and can hopefully generalize better. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you are looking for more complicated models, take a look at &lt;a href=&quot;http://stats.stackexchange.com/questions/124746/fast-density-estimation&quot;&gt;this question&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;HTH.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-20T22:45:30.567" Id="124891" LastActivityDate="2014-11-20T22:45:30.567" OwnerUserId="46089" ParentId="107475" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;The following three formulas are well known, they are found in many books on linear regression. It is not difficult to derive them.&lt;/p&gt;&#10;&#10;&lt;p&gt;$\beta_1= \frac {r_{YX_1}-r_{YX_2}r_{X_1X_2}} {\sqrt{1-r_{X_1X_2}^2}}$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\beta_2= \frac {r_{YX_2}-r_{YX_1}r_{X_1X_2}} {\sqrt{1-r_{X_1X_2}^2}}$&lt;/p&gt;&#10;&#10;&lt;p&gt;$R^2= \frac {r_{YX_1}^2+r_{YX_2}^2-2 r_{YX_1}r_{YX_2}r_{X_1X_2}} {\sqrt{1-r_{X_1X_2}^2}}$&lt;/p&gt;&#10;&#10;&lt;p&gt;If you substitute the two betas into your equation&#10;$R^2 = r_{YX_1} \beta_1 + r_{YX_2} \beta_2$, you will get the above formula for R-square.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Here is a geometric &quot;insight&quot;. Below are two pictures showing regression of $Y$ by $X_1$ and $X_2$. This kind of representation is known as &lt;em&gt;variables-as-vectors in subject space&lt;/em&gt; (please &lt;a href=&quot;http://stats.stackexchange.com/a/65817/3277&quot;&gt;read&lt;/a&gt; what it is about). The pictures are drawn after all the tree variables were centered, and so (1) every vector's length = st. deviation of the respective variable, and (2) angle (its cosine) between every two vectors = correlation between the respective variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/AI7fu.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;$\hat{Y}$ is the regression prediction (orthogonal projection of $Y$ onto &quot;plane X&quot;); $e$ is the error term; $cos \angle{Y \hat{Y}}={|\hat Y|}/|Y|$, multiple correlation coefficient.&lt;/p&gt;&#10;&#10;&lt;p&gt;The left picture depicts &lt;em&gt;skew coordinates&lt;/em&gt; of $\hat{Y}$ on variables $X_1$ and $X_2$. We know that such coordinates relate the regression coefficients. Namely, the coordinates are: $b_1|X_1|=b_1\sigma_{X_1}$ and $b_2|X_2|=b_2\sigma_{X_2}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;And the right picture shows corresponding &lt;em&gt;perpendicular coordinates&lt;/em&gt;. We know that such coordinates relate the zero order correlation coefficients (these are cosines of orthogonal projections). If $r_1$ is the correlation between $Y$ and $X_1$ and $r_1^*$ is the correlation between $\hat Y$ and $X_1$&#10;then the coordinate is $r_1|Y|=r_1\sigma_{Y} = r_1^*|\hat{Y}|=r_1^*\sigma_{\hat{Y}}$. Likewise for the other coordinate, $r_2|Y|=r_2\sigma_{Y} = r_2^*|\hat{Y}|=r_2^*\sigma_{\hat{Y}}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;So far it were general explanations of linear regression vector representation. Now we turn for the task to show how it may lead to $R^2 = r_1 \beta_1 + r_2 \beta_2$.&lt;/p&gt;&#10;&#10;&lt;p&gt;First of all, recall that in their question @Corone put forward the condition that the expression is true when all the three variables are &lt;em&gt;standardized&lt;/em&gt;, that is, not just centered but also scaled to variance 1. Then (i.e. implying $|X_1|=|X_2|=|Y|=1$ to be the &quot;working parts&quot; of the vectors) we have coordinates equal to: $b_1|X_1|=\beta_1$; $b_2|X_2|=\beta_2$; $r_1|Y|=r_1$; $r_2|Y|=r_2$; as well as $R=|\hat Y|/|Y|=|\hat Y|$. Redraw, under these conditions, just the &quot;plane X&quot; of the pictures above:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/btBHS.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;On the picture, we have a pair of perpendicular coordinates and a pair of skew coordinates, of the same vector $\hat Y$ of length $R$. There exist a general rule to obtain perpendicular coordinates from skew ones (or back): $\bf P = S C$, where $\bf P$ is &lt;code&gt;points X axes&lt;/code&gt; matrix of perpendicular ones; $\bf S$ is the same sized matrix of skew ones; and $\bf C$ are the &lt;code&gt;axes X axes&lt;/code&gt; symmetric matrix of angles (cosines) between the nonorthogonal axes.&lt;/p&gt;&#10;&#10;&lt;p&gt;$X_1$ and $X_2$ are the axes in our case, with $r_{12}$ being the cosine between them. So, $r_1 = \beta_1 + \beta_2 r_{12}$ and $r_2 = \beta_1 r_{12} + \beta_2$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Substitute these $r$s expressed via $\beta$s in the @Corone's statement $R^2 = r_1 \beta_1 + r_2 \beta_2$, and you'll get that $R^2 = \beta_1^2 + \beta_2^2 + 2\beta_1\beta_2r_{12}$, - which &lt;strong&gt;is true&lt;/strong&gt;, because it is exactly how a &lt;em&gt;diagonal of a&lt;/em&gt; &lt;a href=&quot;https://en.wikipedia.org/wiki/Parallelogram_law&quot; rel=&quot;nofollow&quot;&gt;parallelogram&lt;/a&gt; (tinted on the picture) &lt;em&gt;is expressed via its adjacent sides&lt;/em&gt; (quantity $\beta_1\beta_2r_{12}$ being the scalar product).&lt;/p&gt;&#10;&#10;&lt;p&gt;This same thing is true for any number of predictors X. Unfortunately, it is impossible to draw the alike pictures with many predictors.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-11-20T23:04:37.270" Id="124892" LastActivityDate="2014-11-22T15:18:11.667" LastEditDate="2014-11-22T15:18:11.667" LastEditorUserId="3277" OwnerUserId="3277" ParentId="124887" PostTypeId="2" Score="3" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a numeric attribute with values in the range of [0.0,1.0]. I want to apply a filter in WEKA that will convert the attribute to be binary where false &amp;lt;= 0.3 &amp;lt; true. However, I can't figure out an existing filter that does this. Is there such a filter or do I need to implement one myself? Thanks for your time!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-20T23:13:08.590" Id="124893" LastActivityDate="2014-11-20T23:13:08.590" OwnerUserId="54973" PostTypeId="1" Score="0" Tags="&lt;weka&gt;&lt;threshold&gt;&lt;filter&gt;" Title="WEKA filter for thresholding a numeric attribute to make it binary" ViewCount="38" />
  <row Body="&lt;p&gt;I talked about this in an &lt;a href=&quot;http://stackoverflow.com/a/20534713/2484687&quot;&gt;answer to a related SO question&lt;/a&gt;. Decision trees are just generally a very good fit for boosting, much more so than other algorithms. The bullet point/ summary version is this:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Decision trees are non-linear. Boosting with linear models simply doesn't work well. &lt;/li&gt;&#10;&lt;li&gt;The weak learner needs to be consistently better than random guessing. You don't normal need to do any parameter tuning to a decision tree to get that behavior. Training an SVM really does need a parameter search. Since the data is re-weighted on each iteration, you likely need to do another parameter search on each iteration. So you are increasing the amount of work you have to do by a large margin. &lt;/li&gt;&#10;&lt;li&gt;Decision trees are reasonably fast to train. Since we are going to be building 100s or 1000s of them, thats a good property. They are also fast to classify, which is again important when you need 100s or 1000s to run before you can output your decision. &lt;/li&gt;&#10;&lt;li&gt;By changing the depth you have a simple and easy control over the bias/variance trade off, knowing that boosting can reduce bias but also significantly reduces variance. Boosting is known to overfit, so the easy nob to tune is helpful in that regard. &lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2014-11-21T01:13:44.867" Id="124910" LastActivityDate="2014-11-21T01:13:44.867" OwnerUserId="34874" ParentId="124628" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="124932" AnswerCount="1" Body="&lt;p&gt;A large lecture theater has 270 seats, 24 of which can accomodate left-handed students.  Suppose it is known that 14% of people are left-handed.  One class held in the theater has 205 students.&lt;/p&gt;&#10;&#10;&lt;p&gt;(a) Let X be the number of left-handed students in the class.  The mean and standard deviation of X are, respectively,    and   &lt;/p&gt;&#10;&#10;&lt;p&gt;(b)  Let p hat be the proportion of left-handed students in the class.  The mean and standard deviation of p hat are, respectively,    and    .&lt;/p&gt;&#10;&#10;&lt;p&gt;(c)  What is the approximate probability that all left-handed students in the class get an appropriate desk?&lt;/p&gt;&#10;&#10;&lt;p&gt;I am assuming that this is supposed to be a hypergeometric probability that needs to be approximated to a normal distribution&lt;/p&gt;&#10;&#10;&lt;p&gt;mean is equation to 0.14*205 = 28.7&lt;/p&gt;&#10;&#10;&lt;p&gt;standard deviation is sqrt(205*.14*.86*((270-205)/(270-1)))=2.442140873&lt;/p&gt;&#10;&#10;&lt;p&gt;p hat = X/n&lt;/p&gt;&#10;&#10;&lt;p&gt;Not sure if that is even right. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-11-21T01:57:33.037" FavoriteCount="1" Id="124912" LastActivityDate="2014-11-23T07:22:31.547" OwnerUserId="59256" PostTypeId="1" Score="3" Tags="&lt;self-study&gt;&lt;normal-distribution&gt;&lt;hypergeometric&gt;" Title="Hypergeometric approximated Normal/Gaussian distribtuion" ViewCount="28" />
  
  <row Body="&lt;p&gt;Hmm, after I' done an example in my MatMate-language I see that there is already a python-answer, which might be preferable because python is widely used. But because you had still questions I show you my approach using the Matmate-matrix-language, perhaps it is more selfcommenting.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Method 1&lt;/em&gt;&lt;/strong&gt;&lt;br&gt;&#10;(Using MatMate):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;v=12         // 12 variables&#10;f=3          // subset-correlation based on 3 common factors&#10;vg = v / f   // variables per subsets&#10;&#10; // generate hidden factor-matrix&#10;             // randomu(rows,cols ,lowbound, ubound) gives uniform random matrix &#10;             //    without explicite bounds the default is: randomu(rows,cols,0,100)&#10;L = {   randomu(vg,f)     || randomu(vg,f)/100  || randomu(vg,f)/100 , _&#10;        randomu(vg,f)/100 || randomu(vg,f)      || randomu(vg,f)/100 , _&#10;        randomu(vg,f)/100 || randomu(vg,f)/100  || randomu(vg,f)     }&#10;&#10; // make sure there is itemspecific variance&#10; // by appending a diagonal-matrix with random positive entries&#10;L = L || mkdiag(randomu(v,1,10,20)) &#10;  // make covariance and correlation matrix&#10;cov = L *'   // L multiplied  with its transpose&#10;cor = covtocorr(cov)&#10;                   set ccdezweite=3 ccfeldweite=8&#10;                   list cor&#10;cor = &#10;   1.000,   0.321,   0.919,   0.489,   0.025,   0.019,   0.019,   0.030,   0.025,   0.017,   0.014,   0.014&#10;   0.321,   1.000,   0.540,   0.923,   0.016,   0.015,   0.012,   0.030,   0.033,   0.016,   0.012,   0.015&#10;   0.919,   0.540,   1.000,   0.679,   0.018,   0.014,   0.012,   0.029,   0.028,   0.014,   0.012,   0.012&#10;   0.489,   0.923,   0.679,   1.000,   0.025,   0.022,   0.020,   0.040,   0.031,   0.014,   0.011,   0.014&#10;   0.025,   0.016,   0.018,   0.025,   1.000,   0.815,   0.909,   0.758,   0.038,   0.012,   0.018,   0.014&#10;   0.019,   0.015,   0.014,   0.022,   0.815,   1.000,   0.943,   0.884,   0.035,   0.012,   0.014,   0.012&#10;   0.019,   0.012,   0.012,   0.020,   0.909,   0.943,   1.000,   0.831,   0.036,   0.013,   0.015,   0.010&#10;   0.030,   0.030,   0.029,   0.040,   0.758,   0.884,   0.831,   1.000,   0.041,   0.017,   0.022,   0.020&#10;   0.025,   0.033,   0.028,   0.031,   0.038,   0.035,   0.036,   0.041,   1.000,   0.831,   0.868,   0.780&#10;   0.017,   0.016,   0.014,   0.014,   0.012,   0.012,   0.013,   0.017,   0.831,   1.000,   0.876,   0.848&#10;   0.014,   0.012,   0.012,   0.011,   0.018,   0.014,   0.015,   0.022,   0.868,   0.876,   1.000,   0.904&#10;   0.014,   0.015,   0.012,   0.014,   0.014,   0.012,   0.010,   0.020,   0.780,   0.848,   0.904,   1.000&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The problem here might be, that we define blocks of submatrices which have high correlations within with little correlation between and this is not programmatically but by the constant concatenation-expressions . Maybe this &lt;strong&gt;&lt;em&gt;approach&lt;/em&gt;&lt;/strong&gt; could be modeled more elegantly in python.&#10;&lt;hr&gt; &#10;&lt;strong&gt;&lt;em&gt;Method 2(a)&lt;/em&gt;&lt;/strong&gt;&lt;br&gt;&#10;After that, there is a completely different approach, where we fill &lt;strong&gt;&lt;em&gt;the possible remaining covariance&lt;/em&gt;&lt;/strong&gt; by random amounts of 100 percent into a factor-loadings-matrix. This is done in Pari/GP:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;{L = matrix(8,8);  \\ generate an empty factor-loadings-matrix&#10;for(r=1,8, &#10;   rv=1.0;    \\ remaining variance for variable is 1.0&#10;   for(c=1,8,&#10;        pv=if(c&amp;lt;8,random(100)/100.0,1.0); \\ define randomly part of remaining variance&#10;        cv= pv * rv;  \\ compute current partial variance&#10;        rv = rv - cv;     \\ compute the now remaining variance&#10;        sg = (-1)^(random(100) % 2) ;  \\ also introduce randomly +- signs&#10;        L[r,c] = sg*sqrt(cv) ;  \\ compute factor loading as signed sqrt of cv&#10;       )&#10;     );}&#10;&#10;cor = L * L~&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and the produced correlation-matrix is&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;     1.000  -0.7111  -0.08648   -0.7806   0.8394  -0.7674   0.6812    0.2765&#10;   -0.7111    1.000   0.06073    0.7485  -0.7550   0.8052  -0.8273   0.05863&#10;  -0.08648  0.06073     1.000    0.5146  -0.1614   0.1459  -0.4760  -0.01800&#10;   -0.7806   0.7485    0.5146     1.000  -0.8274   0.7644  -0.9373  -0.06388&#10;    0.8394  -0.7550   -0.1614   -0.8274    1.000  -0.5823   0.8065   -0.1929&#10;   -0.7674   0.8052    0.1459    0.7644  -0.5823    1.000  -0.7261   -0.4822&#10;    0.6812  -0.8273   -0.4760   -0.9373   0.8065  -0.7261    1.000   -0.1526&#10;    0.2765  0.05863  -0.01800  -0.06388  -0.1929  -0.4822  -0.1526     1.000&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Possibly this generates a correlation-matrix with dominant principal components because of the cumulative generating-rule for the factor-loadings-matrix. Also it might be better to assure positive definiteness by making the last portion of variance a unique factor. I left it in the program to keep the focus on the general principle.         &lt;/p&gt;&#10;&#10;&lt;p&gt;A 100x100 correlation-matrix had the following frequencies of correlations (rounded to 1 dec place)&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    e    f            e: entry(rounded) f: frequency&#10;  -----------------------------------------------------&#10;  -1.000, 108.000&#10;  -0.900, 460.000&#10;  -0.800, 582.000&#10;  -0.700, 604.000&#10;  -0.600, 548.000&#10;  -0.500, 540.000&#10;  -0.400, 506.000&#10;  -0.300, 482.000&#10;  -0.200, 488.000&#10;  -0.100, 464.000&#10;   0.000, 434.000&#10;   0.100, 486.000&#10;   0.200, 454.000&#10;   0.300, 468.000&#10;   0.400, 462.000&#10;   0.500, 618.000&#10;   0.600, 556.000&#10;   0.700, 586.000&#10;   0.800, 536.000&#10;   0.900, 420.000&#10;   1.000, 198.000&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;[update]. Hmm, the 100x100 matrix is badly conditioned; Pari/GP cannot determine the eigenvalues correctly with the polroots(charpoly())-function even with 200 digits precision. I've done a Jacobi-rotation to pca-form on the loadingsmatrix L and find  mostly extremely small eigenvalues, printed them in logarithms to base 10 (which give roughly the position of the decimal point). Read from left to right and then row by row:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;log_10(eigenvalues):&#10;   1.684,   1.444,   1.029,   0.818,   0.455,   0.241,   0.117,  -0.423,  -0.664,  -1.040&#10;  -1.647,  -1.799,  -1.959,  -2.298,  -2.729,  -3.059,  -3.497,  -3.833,  -4.014,  -4.467&#10;  -4.992,  -5.396,  -5.511,  -6.366,  -6.615,  -6.834,  -7.535,  -8.138,  -8.263,  -8.766&#10;  -9.082,  -9.482,  -9.940, -10.167, -10.566, -11.110, -11.434, -11.788, -12.079, -12.722&#10; -13.122, -13.322, -13.444, -13.933, -14.390, -14.614, -15.070, -15.334, -15.904, -16.278&#10; -16.396, -16.708, -17.022, -17.746, -18.090, -18.358, -18.617, -18.903, -19.186, -19.476&#10; -19.661, -19.764, -20.342, -20.648, -20.805, -20.922, -21.394, -21.740, -21.991, -22.291&#10; -22.792, -23.184, -23.680, -24.100, -24.222, -24.631, -24.979, -25.161, -25.282, -26.211&#10; -27.181, -27.626, -27.861, -28.054, -28.266, -28.369, -29.074, -29.329, -29.539, -29.689&#10; -30.216, -30.784, -31.269, -31.760, -32.218, -32.446, -32.785, -33.003, -33.448, -34.318&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;[update 2]&lt;br&gt;&#10;&lt;strong&gt;&lt;em&gt;Method 2(b)&lt;/em&gt;&lt;/strong&gt;&lt;br&gt;&#10;An improvement might be to increase the itemspecific variance to some non-marginal level and reduce to a reasonably smaller number of common factors (for instance integer-squareroot of itemnumber):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;{  dimr = 100;&#10;   dimc = sqrtint(dimr);        \\ 10 common factors&#10;   L = matrix(dimr,dimr+dimc);  \\ loadings matrix &#10;                                \\     with dimr itemspecific and &#10;                                \\          dimc common factors&#10;   for(r=1,dim, &#10;         vr=1.0;                \\ complete variance per item &#10;         vu=0.05+random(100)/1000.0;   \\ random variance +0.05&#10;                                       \\ for itemspecific variance&#10;         L[r,r]=sqrt(vu);              \\ itemspecific factor loading  &#10;         vr=vr-vu;&#10;         for(c=1,dimc,&#10;                cv=if(c&amp;lt;dimc,random(100)/100,1.0)*vr;&#10;                vr=vr-cv;&#10;                L[r,dimr+c]=(-1)^(random(100) % 2)*sqrt(cv)&#10;             )&#10;        );}&#10;&#10;   cov=L*L~&#10;   cp=charpoly(cov)   \\ does not work even with 200 digits precision&#10;   pr=polroots(cp)    \\ spurious negative and complex eigenvalues...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The structure of the result       &lt;/p&gt;&#10;&#10;&lt;p&gt;in term of the distribution of correlations:&lt;img src=&quot;http://i.stack.imgur.com/tmEpN.png&quot; alt=&quot;image&quot;&gt;    &lt;/p&gt;&#10;&#10;&lt;p&gt;remains similar (also the nasty non decomposability by PariGP), but the eigenvalues, when found by jacobi-rotation of the loadingsmatrix, have now a better structure, for a newly computed example I got the eigenvalues as &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;log_10(eigenvalues):&#10;   1.677,   1.326,   1.063,   0.754,   0.415,   0.116,  -0.262,  -0.516,  -0.587,  -0.783&#10;  -0.835,  -0.844,  -0.851,  -0.854,  -0.858,  -0.862,  -0.862,  -0.868,  -0.872,  -0.873&#10;  -0.878,  -0.882,  -0.884,  -0.890,  -0.895,  -0.896,  -0.896,  -0.898,  -0.902,  -0.904&#10;  -0.904,  -0.909,  -0.911,  -0.914,  -0.920,  -0.923,  -0.925,  -0.927,  -0.931,  -0.935&#10;  -0.939,  -0.939,  -0.943,  -0.948,  -0.951,  -0.955,  -0.956,  -0.960,  -0.967,  -0.969&#10;  -0.973,  -0.981,  -0.986,  -0.989,  -0.997,  -1.003,  -1.005,  -1.011,  -1.014,  -1.019&#10;  -1.022,  -1.024,  -1.031,  -1.038,  -1.040,  -1.048,  -1.051,  -1.061,  -1.064,  -1.068&#10;  -1.070,  -1.074,  -1.092,  -1.092,  -1.108,  -1.113,  -1.120,  -1.134,  -1.139,  -1.147&#10;  -1.150,  -1.155,  -1.158,  -1.166,  -1.171,  -1.175,  -1.184,  -1.184,  -1.192,  -1.196&#10;  -1.200,  -1.220,  -1.237,  -1.245,  -1.252,  -1.262,  -1.269,  -1.282,  -1.287,  -1.290&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="3" CreationDate="2014-11-21T02:33:54.173" Id="124915" LastActivityDate="2014-11-21T05:19:50.937" LastEditDate="2014-11-21T05:19:50.937" LastEditorUserId="1818" OwnerUserId="1818" ParentId="124538" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;OP notes that the sample moments can be calculated fast enough for his needs, and suggests:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Estimate the moments of the distribution, and find the pdf based on these moments alone&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;This can be done with Pearson fitting which just requires the first 4 moments. But, it does assume that your data is unimodal and ... to be useful and robust ... that the kurtosis etc is not too large. See, for instance, chapter 5 of our book, Rose/Smith(2002 - free download):&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.mathstatica.com/book/bookcontents.html&quot; rel=&quot;nofollow&quot;&gt;http://www.mathstatica.com/book/bookcontents.html&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The 'input' is the first 4 moments ---- the pdf is then derived from those moments, where the functional forms are already worked out symbolically, so the resulting pdf is calculated effectively instantaneously. &lt;/p&gt;&#10;&#10;&lt;p&gt;I think the question would be better defined if the OP specified:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;How well does a Gaussian fit work?  &lt;/li&gt;&#10;&lt;li&gt;What does the kernel density estimate look like? Why not include a plot? &lt;/li&gt;&#10;&lt;li&gt;Does the distribution change shape? If so, please provide some examples.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2014-11-21T04:19:36.257" Id="124924" LastActivityDate="2014-11-21T04:19:36.257" OwnerUserId="24905" ParentId="124746" PostTypeId="2" Score="2" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am doing various analysis on a small sample. Basically, we have an experiment where 14 subjects (UID 1 ~ 14) used one of the 6 instruments (MID 1 ~ 6) on 3 occasions (Sequence 1 ~ 3). Each time an outcome score was registered (between 1 ~ 100). &lt;/p&gt;&#10;&#10;&lt;p&gt;The test was double blind. The subjects were told they are measuring 3 different conditions while in reality they were either measuring conditions A, B, A or B, A, B (randomly assigned to the machines and users). The objective was to see if &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt; are different or not.&lt;/p&gt;&#10;&#10;&lt;p&gt;To see if there is any significant difference between the ratings for the conditions A and B, I tried to fit a simple, random intercept model using the nlme package in R. I tried:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;f.1 &amp;lt;- lme(Score ~ Condition, random = ~1|UID, data)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;However, for some reason &lt;code&gt;lme&lt;/code&gt; fails to fit the model: it gives no error or warning but the variance of the fitted random effect is essentially zero:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; summary(f.1)&#10;Linear mixed-effects model fit by REML&#10; Data: data &#10;       AIC      BIC   logLik&#10;  349.3259 356.0815 -170.663&#10;&#10;Random effects:&#10; Formula: ~1 | UID&#10;         (Intercept) Residual&#10;StdDev: 0.0009303203 15.98295&#10;&#10;Fixed effects: Score ~ Condition &#10;               Value Std.Error DF   t-value p-value&#10;(Intercept) 77.47619  3.487766 27 22.213700  0.0000&#10;ConditionA  -0.85714  4.932446 27 -0.173776  0.8633&#10; Correlation: &#10;           (Intr)&#10;ConditionA -0.707&#10;&#10;Standardized Within-Group Residuals:&#10;       Min         Q1        Med         Q3        Max &#10;-2.9704269 -0.4677603  0.2472873  0.7835730  1.4628682 &#10;&#10;Number of Observations: 42&#10;Number of Groups: 14&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I tried doing the same thing using &lt;code&gt;lme4&lt;/code&gt; and got the same results. The estimates for the intercept and the &lt;code&gt;Condition&lt;/code&gt; factor is almost identical to a linear model if I use &lt;code&gt;lm&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am trying hard to understand what &lt;code&gt;lme&lt;/code&gt; or &lt;code&gt;lmer&lt;/code&gt; fail to estimate the random effect. I generated some data by simulation and both routines had no problem fitting the model so I doubt there is something wrong with the syntax of what I have used.&lt;/p&gt;&#10;&#10;&lt;p&gt;The data is here:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;   UID MID Seq Score Condition&#10;1    1   1   1    90  B&#10;2    1   1   2    85  A&#10;3    1   1   3    75  B&#10;4    2   4   1    75  A&#10;5    2   4   2    95  B&#10;6    2   4   3    85  A&#10;7    3   6   1    60  A&#10;8    3   6   2    82  B&#10;9    3   6   3    85  A&#10;10   4   3   1    60  A&#10;11   4   3   2    70  B&#10;12   4   3   3    75  A&#10;13   5   2   1    85  B&#10;14   5   2   2    85  A&#10;15   5   2   3    85  B&#10;16   6   5   1    90  B&#10;17   6   5   2    95  A&#10;18   6   5   3   100  B&#10;19   7   2   1    90  B&#10;20   7   2   2    70  A&#10;21   7   2   3    50  B&#10;22   8   1   1    70  B&#10;23   8   1   2    75  A&#10;24   8   1   3    80  B&#10;25   9   3   1    90  A&#10;26   9   3   2    30  B&#10;27   9   3   3    90  A&#10;28  10   6   1    50  A&#10;29  10   6   2    85  B&#10;30  10   6   3    92  A&#10;31  11   4   1    50  A&#10;32  11   4   2    85  B&#10;33  11   4   3    92  A&#10;34  12   5   1    65  B&#10;35  12   5   2    50  A&#10;36  12   5   3    90  B&#10;37  13   4   1    65  A&#10;38  13   4   2    70  B&#10;39  13   4   3    80  A&#10;40  14   2   1    60  B&#10;41  14   2   2   100  A&#10;42  14   2   3    80  B&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="8" CreationDate="2014-11-21T10:29:37.637" Id="124944" LastActivityDate="2014-11-21T10:29:37.637" OwnerUserId="9883" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;mixed-model&gt;&lt;nlme&gt;" Title="nlme estimates near zero variance for the random effects" ViewCount="66" />
  <row Body="&lt;p&gt;Receiver Operating Characteristic curves and associated Area Under Curve measures work with binary classifiers. There is more complexity when multiple values for the labels are possible (see &lt;a href=&quot;http://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_curve&quot; rel=&quot;nofollow&quot;&gt;Wikipedia&lt;/a&gt; link)&lt;/p&gt;&#10;&#10;&lt;p&gt;I believe the R package &lt;code&gt;PerfMeas&lt;/code&gt; supports this so you could take advantage of the RapidMiner R extension to get access to this.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-21T10:32:22.020" Id="124945" LastActivityDate="2014-11-21T10:32:22.020" OwnerUserId="28188" ParentId="124496" PostTypeId="2" Score="0" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;If I have some data that I believe is Normally distributed and I just want to test the hypotheses that the mean is equal to 1 of 3 values, my understanding is that the Bayes Factor is the ratio of marginal likelihoods.  So &lt;/p&gt;&#10;&#10;&lt;p&gt;BF = $\frac{p(x_i|H_0)}{p(x_i|H_1)}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Furthermore, from the texts I read, the marginal likelihood for the null is equal to &lt;/p&gt;&#10;&#10;&lt;p&gt;$\int(p(\theta_0|H_0)p(x_i|\theta_0,H_0)d\theta_0$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\theta_i$ is your parameter, in this case the mean.   &lt;/p&gt;&#10;&#10;&lt;p&gt;My question is, since the alternate hypothesis is that the mean is equal to one of the two values other than the null, is the marginal likelihood $p(x_i|H_1)$ equal to &lt;/p&gt;&#10;&#10;&lt;p&gt;$\int(p(\theta_{1,1}|H_1)p(x_i|\theta_{1,1},H_0)d\theta_{1,1}$$\int(p(\theta_{1,2}|H_1)p(x_i|\theta_{1,2},H_1)d\theta_{1,2}$.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-21T15:56:50.327" Id="124981" LastActivityDate="2014-11-21T18:18:52.483" OwnerUserId="37594" PostTypeId="1" Score="1" Tags="&lt;bayesian&gt;&lt;likelihood&gt;" Title="Marginal Likelihoods for Bayes Factors with Multiple Discrete Hypothesis" ViewCount="22" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I am working on an application where I have to extract or identify association / correlation between different sets of items. An example would be say if a person buys shoes at a store, would he/she buy socks also? So, I will have to find the association between shoes and socks based on legacy data.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, I know that &lt;a href=&quot;http://en.wikipedia.org/wiki/Apriori_algorithm&quot; rel=&quot;nofollow&quot;&gt;apriori&lt;/a&gt; is one famous algorithm for association rule mining. What I want to know that is there any other algorithm which is much more efficient than apriori for association rule mining? By efficiency I mean in terms of implementation easiness, correctness and speed.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-21T20:55:42.203" Id="125009" LastActivityDate="2014-11-22T13:10:21.903" LastEditDate="2014-11-21T21:11:55.220" LastEditorUserId="7290" OwnerUserId="61233" PostTypeId="1" Score="2" Tags="&lt;data-mining&gt;&lt;association-rules&gt;" Title="Best algorithm for association rule mining" ViewCount="76" />
  <row Body="&lt;p&gt;The value of x used to generate any point on that trend line is indeed the &quot;very big number&quot; that is the way Excel actually stores dates.  It is - as you partially surmised - the number of days since Jan. 1, 1900 - so for any remotely current dates, the number is pretty large.  If you are keeping not just dates, but dates and times - the time portion is kept as a fraction of a day (so 12 noon on a particular day is stored with a number 0.5 larger than the preceding midnight).&lt;/p&gt;&#10;&#10;&lt;p&gt;You will also want to be careful to stick just to the region of your data - with a 6th order polynomial - if you move very far outside the range between 2005 and 2012 - the values of the trend line are quite likely to become quite extreme (I think, looking at your coefficients, that if you tried to plot a value for a date in 1970 - or 2050 - it would be incredibly huge, but I might have things backwards, and those values could be extremely negative numbers).&lt;/p&gt;&#10;&#10;&lt;p&gt;Edited to add - User777 below is correct.  Excel stores dates so that Jan. 1, 1900 is 1 - so it stores a number that is either the number of days since &quot;Jan. 0, 1900&quot; - or &quot;Dec. 31, 1899&quot; - whichever way is easier for you to understand.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-11-21T23:45:33.063" Id="125025" LastActivityDate="2014-11-22T00:22:20.327" LastEditDate="2014-11-22T00:22:20.327" LastEditorUserId="59662" OwnerUserId="59662" ParentId="125021" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;The answer given by Zhubarb is great, but unfortunately it is in Python. Below is a Java implementation of the EM algorithm executed on the same problem (posed in the article by Do and Batzoglou, 2008). I've added some printf's to the standard output to see how the parameters converge.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;thetaA = 0.71301, thetaB = 0.58134&#10;thetaA = 0.74529, thetaB = 0.56926&#10;thetaA = 0.76810, thetaB = 0.54954&#10;thetaA = 0.78316, thetaB = 0.53462&#10;thetaA = 0.79106, thetaB = 0.52628&#10;thetaA = 0.79453, thetaB = 0.52239&#10;thetaA = 0.79593, thetaB = 0.52073&#10;thetaA = 0.79647, thetaB = 0.52005&#10;thetaA = 0.79667, thetaB = 0.51977&#10;thetaA = 0.79674, thetaB = 0.51966&#10;thetaA = 0.79677, thetaB = 0.51961&#10;thetaA = 0.79678, thetaB = 0.51960&#10;thetaA = 0.79679, thetaB = 0.51959&#10;Final result:&#10;thetaA = 0.79678, thetaB = 0.51960&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Java code follows below:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;import java.util.*;&#10;&#10;/*****************************************************************************&#10;This class encapsulates the parameters of the problem. For this problem posed&#10;in the article by (Do and Batzoglou, 2008), the parameters are thetaA and&#10;thetaB, the probability of a coin coming up heads for the two coins A and B.&#10;*****************************************************************************/&#10;class Parameters&#10;{&#10;    double _thetaA = 0.0; // Probability of heads for coin A.&#10;    double _thetaB = 0.0; // Probability of heads for coin B.&#10;&#10;    double _delta = 0.00001;&#10;&#10;    public Parameters(double thetaA, double thetaB)&#10;    {&#10;        _thetaA = thetaA;&#10;        _thetaB = thetaB;&#10;    }&#10;&#10;    /*************************************************************************&#10;    Returns true if this parameter is close enough to another parameter&#10;    (typically the estimated parameter coming from the maximization step).&#10;    *************************************************************************/&#10;    public boolean converged(Parameters other)&#10;    {&#10;        if (Math.abs(_thetaA - other._thetaA) &amp;lt; _delta &amp;amp;&amp;amp;&#10;            Math.abs(_thetaB - other._thetaB) &amp;lt; _delta)&#10;        {&#10;            return true;&#10;        }&#10;&#10;        return false;&#10;    }&#10;&#10;    public double getThetaA()&#10;    {&#10;        return _thetaA;&#10;    }&#10;&#10;    public double getThetaB()&#10;    {&#10;        return _thetaB;&#10;    }&#10;&#10;    public String toString()&#10;    {&#10;        return String.format(&quot;thetaA = %.5f, thetaB = %.5f&quot;, _thetaA, _thetaB);&#10;    }&#10;&#10;}&#10;&#10;&#10;/*****************************************************************************&#10;This class encapsulates an observation, that is the number of heads&#10;and tails in a trial. The observation can be either (1) one of the&#10;observed observations, or (2) an estimated observation resulting from&#10;the expectation step.&#10;*****************************************************************************/&#10;class Observation&#10;{&#10;    double _numHeads = 0;&#10;    double _numTails = 0;&#10;&#10;    public Observation(String s)&#10;    {&#10;        for (int i = 0; i &amp;lt; s.length(); i++)&#10;        {&#10;            char c = s.charAt(i);&#10;&#10;            if (c == 'H')&#10;            {&#10;                _numHeads++;&#10;            }&#10;            else if (c == 'T')&#10;            {&#10;                _numTails++;&#10;            }&#10;            else&#10;            {&#10;                throw new RuntimeException(&quot;Unknown character: &quot; + c);&#10;            }&#10;        }&#10;    }&#10;&#10;    public Observation(double numHeads, double numTails)&#10;    {&#10;        _numHeads = numHeads;&#10;        _numTails = numTails;&#10;    }&#10;&#10;    public double getNumHeads()&#10;    {&#10;        return _numHeads;&#10;    }&#10;&#10;    public double getNumTails()&#10;    {&#10;        return _numTails;&#10;    }&#10;&#10;    public String toString()&#10;    {&#10;        return String.format(&quot;heads: %.1f, tails: %.1f&quot;, _numHeads, _numTails);&#10;    }&#10;&#10;}&#10;&#10;/*****************************************************************************&#10;This class runs expectation-maximization for the problem posed by the article&#10;from (Do and Batzoglou, 2008).&#10;*****************************************************************************/&#10;public class EM&#10;{&#10;    // Current estimated parameters.&#10;    private Parameters _parameters;&#10;&#10;    // Observations from the trials. These observations are set once.&#10;    private final List&amp;lt;Observation&amp;gt; _observations;&#10;&#10;    // Estimated observations per coin. These observations are the output&#10;    // of the expectation step.&#10;    private List&amp;lt;Observation&amp;gt; _expectedObservationsForCoinA;&#10;    private List&amp;lt;Observation&amp;gt; _expectedObservationsForCoinB;&#10;&#10;    private static java.io.PrintStream o = System.out;&#10;&#10;    /*************************************************************************&#10;    Principal constructor.&#10;    @param observations The observations from the trial.&#10;    @param parameters The initial guessed parameters.&#10;    *************************************************************************/&#10;    public EM(List&amp;lt;Observation&amp;gt; observations, Parameters parameters)&#10;    {&#10;        _observations = observations;&#10;        _parameters = parameters;&#10;    }&#10;&#10;    /*************************************************************************&#10;    Run EM until parameters converge.&#10;    *************************************************************************/&#10;    public Parameters run()&#10;    {&#10;&#10;        while (true)&#10;        {&#10;            expectation();&#10;&#10;            Parameters estimatedParameters = maximization();&#10;&#10;            o.printf(&quot;%s\n&quot;, estimatedParameters);&#10;&#10;            if (_parameters.converged(estimatedParameters)) {&#10;                break;&#10;            }&#10;&#10;            _parameters = estimatedParameters;&#10;        }&#10;&#10;        return _parameters;&#10;&#10;    }&#10;&#10;    /*************************************************************************&#10;    Given the observations and current estimated parameters, compute new&#10;    estimated completions (distribution over the classes) and observations.&#10;    *************************************************************************/&#10;    private void expectation()&#10;    {&#10;&#10;        _expectedObservationsForCoinA = new ArrayList&amp;lt;Observation&amp;gt;();&#10;        _expectedObservationsForCoinB = new ArrayList&amp;lt;Observation&amp;gt;();&#10;&#10;        for (Observation observation : _observations)&#10;        {&#10;            int numHeads = (int)observation.getNumHeads();&#10;            int numTails = (int)observation.getNumTails();&#10;&#10;            double probabilityOfObservationForCoinA=&#10;                binomialProbability(10, numHeads, _parameters.getThetaA());&#10;&#10;            double probabilityOfObservationForCoinB=&#10;                binomialProbability(10, numHeads, _parameters.getThetaB());&#10;&#10;            double normalizer = probabilityOfObservationForCoinA +&#10;                                probabilityOfObservationForCoinB;&#10;&#10;            // Compute the completions for coin A and B (i.e. the probability&#10;            // distribution of the two classes, summed to 1.0).&#10;&#10;            double completionCoinA = probabilityOfObservationForCoinA /&#10;                                     normalizer;&#10;            double completionCoinB = probabilityOfObservationForCoinB /&#10;                                     normalizer;&#10;&#10;            // Compute new expected observations for the two coins.&#10;&#10;            Observation expectedObservationForCoinA =&#10;                new Observation(numHeads * completionCoinA,&#10;                                numTails * completionCoinA);&#10;&#10;            Observation expectedObservationForCoinB =&#10;                new Observation(numHeads * completionCoinB,&#10;                                numTails * completionCoinB);&#10;&#10;            _expectedObservationsForCoinA.add(expectedObservationForCoinA);&#10;            _expectedObservationsForCoinB.add(expectedObservationForCoinB);&#10;        }&#10;    }&#10;&#10;    /*************************************************************************&#10;    Given new estimated observations, compute new estimated parameters.&#10;    *************************************************************************/&#10;    private Parameters maximization()&#10;    {&#10;&#10;        double sumCoinAHeads = 0.0;&#10;        double sumCoinATails = 0.0;&#10;        double sumCoinBHeads = 0.0;&#10;        double sumCoinBTails = 0.0;&#10;&#10;        for (Observation observation : _expectedObservationsForCoinA)&#10;        {&#10;            sumCoinAHeads += observation.getNumHeads();&#10;            sumCoinATails += observation.getNumTails();&#10;        }&#10;&#10;        for (Observation observation : _expectedObservationsForCoinB)&#10;        {&#10;            sumCoinBHeads += observation.getNumHeads();&#10;            sumCoinBTails += observation.getNumTails();&#10;        }&#10;&#10;        return new Parameters(sumCoinAHeads / (sumCoinAHeads + sumCoinATails),&#10;                              sumCoinBHeads / (sumCoinBHeads + sumCoinBTails));&#10;&#10;        //o.printf(&quot;parameters: %s\n&quot;, _parameters);&#10;&#10;    }&#10;&#10;    /*************************************************************************&#10;    Since the coin-toss experiment posed in this article is a Bernoulli trial,&#10;    use a binomial probability Pr(X=k; n,p) = (n choose k) * p^k * (1-p)^(n-k).&#10;    *************************************************************************/&#10;    private static double binomialProbability(int n, int k, double p)&#10;    {&#10;        double q = 1.0 - p;&#10;        return nChooseK(n, k) * Math.pow(p, k) * Math.pow(q, n-k);&#10;    }&#10;&#10;    private static long nChooseK(int n, int k)&#10;    {&#10;        long numerator = 1;&#10;&#10;        for (int i = 0; i &amp;lt; k; i++)&#10;        {&#10;            numerator = numerator * n;&#10;            n--;&#10;        }&#10;&#10;        long denominator = factorial(k);&#10;&#10;        return (long)(numerator / denominator);&#10;    }&#10;&#10;    private static long factorial(int n)&#10;    {&#10;        long result = 1;&#10;        for (; n &amp;gt;0; n--)&#10;        {&#10;            result = result * n;&#10;        }&#10;&#10;        return result;&#10;    }&#10;&#10;    /*************************************************************************&#10;    Entry point into the program.&#10;    *************************************************************************/&#10;    public static void main(String argv[])&#10;    {&#10;        // Create the observations and initial parameter guess&#10;        // from the (Do and Batzoglou, 2008) article.&#10;&#10;        List&amp;lt;Observation&amp;gt; observations = new ArrayList&amp;lt;Observation&amp;gt;();&#10;        observations.add(new Observation(&quot;HTTTHHTHTH&quot;));&#10;        observations.add(new Observation(&quot;HHHHTHHHHH&quot;));&#10;        observations.add(new Observation(&quot;HTHHHHHTHH&quot;));&#10;        observations.add(new Observation(&quot;HTHTTTHHTT&quot;));&#10;        observations.add(new Observation(&quot;THHHTHHHTH&quot;));&#10;&#10;        Parameters initialParameters = new Parameters(0.6, 0.5);&#10;&#10;        EM em = new EM(observations, initialParameters);&#10;&#10;        Parameters finalParameters = em.run();&#10;&#10;        o.printf(&quot;Final result:\n%s\n&quot;, finalParameters);&#10;    }&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-11-21T23:56:26.753" Id="125026" LastActivityDate="2014-11-21T23:56:26.753" OwnerUserId="29072" ParentId="72774" PostTypeId="2" Score="2" />
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;Scipy's implementation of the test return a value from 0 to .5, &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;If that's actually true, then it's not giving you a p-value. &lt;/p&gt;&#10;&#10;&lt;p&gt;And indeed, I've just gone and looked. It &lt;em&gt;isn't&lt;/em&gt; giving a proper p-value -- though it says it is. It is instead doing something almost guaranteed to make some people do their tests wrong:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;em&gt;The reported p-value is for a one-sided hypothesis, to get the two-sided p-value multiply the returned p-value by 2.&lt;/em&gt;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Which one sided test is it doing? Well, if you get the two-sided p-value by doubling it, it's choosing the direction of the test &lt;em&gt;based on the data&lt;/em&gt;. Which means you can neither do one-sided &lt;em&gt;nor&lt;/em&gt; two sided tests using that number. It's &lt;em&gt;not&lt;/em&gt; actually a p-value for any specific test*, &lt;em&gt;but it claims it is&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;* not for a two-sided alternative, nor a one-sided greater than alternative, nor a one-sided less than alternative. So it's &lt;em&gt;not a p-value for any of the possible Mann-Whitney tests&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;[This is, frankly, &lt;em&gt;appalling&lt;/em&gt;. IMO that's &lt;em&gt;really bad&lt;/em&gt; software design; it looks almost guaranteed to lead novice users to make mistakes.]&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;and if I understand correctly, .5 means that there is a 100% chance that the two datasets came from the same source. Is this correct?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Well, let's double it so we are clear we have a two-tailed p-value of 1.0&lt;/p&gt;&#10;&#10;&lt;p&gt;No, that's not what it would mean at all. A p-value is the probability of a test statistic at least as unusual as the one observed if the null hypothesis were true.&lt;/p&gt;&#10;&#10;&lt;p&gt;If it's high, it doesn't mean the null is exactly true. It just means you won't see a statistic more consistent with the null -- but that can be produced by situations where the null is false.&lt;/p&gt;&#10;&#10;&lt;p&gt;You may like to search our site for questions on interpreting p-values.&lt;/p&gt;&#10;&#10;&lt;p&gt;e.g. these ones may help:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/46856/interpretation-of-p-value-in-hypothesis-testing&quot;&gt;Interpretation of p-value in hypothesis testing&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/60528/is-this-interpretation-of-the-p-value-legit&quot;&gt;is this interpretation of the p-value legit?&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/52255/interpreting-p-values&quot;&gt;Interpreting p-values&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-22T00:01:38.387" Id="125027" LastActivityDate="2014-11-22T15:45:12.053" LastEditDate="2014-11-22T15:45:12.053" LastEditorUserId="805" OwnerUserId="805" ParentId="124995" PostTypeId="2" Score="2" />
  
  
  <row AcceptedAnswerId="125524" AnswerCount="1" Body="&lt;p&gt;Given the fact the finite population correction effectively reduces the standard error of the sampling distribution, which (compared with the lack of a finite population correction) will increase the test statistic from a hypothesis test (and therefore increase the probability that the null will be rejected), it seems quite a powerful tool to use without a strong justification. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Here is one scenario I feel it is justified to use it:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;[Edit 2 - I have changed the scenario here slightly following a very valid point from Steve. I don't want non-response rates to distract from the focus of the finite population correction].&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a company which has new leadership in the past 12 months. During the old leadership, I performed a random sample on 50% of the employees (with a 100% response rate). Under new leadership, I conducted the same survey on a random sample of 45% of the employees (again, with a 100% response rate)&lt;/p&gt;&#10;&#10;&lt;p&gt;If I want to see how the results to some questions in the survey have changed in a statistically significant way, I should apply the finite population correction. I am comparing two sets of employees at two specific points in time. I don't care about anyone other than the population working within the company at those two points in time.&lt;/p&gt;&#10;&#10;&lt;p&gt;[Edit 2 - To clarify my concern]&lt;/p&gt;&#10;&#10;&lt;p&gt;In addition, those who were not working in the firm at those two points in time would not be able to answer the survey anyway. Taking a basic question like 'I have enjoyed working here in the last couple of months,' only those who have actually worked at the firm could reasonably answer. In which case, assuming an infinite population seems like it is applying an arbitrary restriction on the sampling distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Here is one scenario I feel it wouldn't be justified to use it:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I carry out a questionnaire to a sample of customers who bought products from me to assess their interests, in order for me to inform me how best to diversify my business for the next couple of years. While this sample is 20% of all those customers I have served since I opened, I could perceive the population here to be inclusive of all those customers who haven't bought products from me (because they aren't in the area or my prices were too high or they didn't know about my shop etc.) and therefore I have a population which leaves my sample to be &amp;lt;1% of it in size).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Questions:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Do people agree with these my interpretation of the two example scenarios? &lt;/p&gt;&#10;&#10;&lt;p&gt;When producing a piece of work, is it good practice to include the justification why you are using a finite population correction? &lt;/p&gt;&#10;&#10;&lt;p&gt;Is this largely a assessment of 'who' the population is and whether you want your results to apply just to the 'known' population or whether you are seeking to provide something that has a wider level of application than just the 'known' population?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-22T13:59:08.277" Id="125058" LastActivityDate="2014-11-27T00:44:08.873" LastEditDate="2014-11-26T14:07:33.610" LastEditorUserId="54254" OwnerUserId="54254" PostTypeId="1" Score="5" Tags="&lt;sampling&gt;&lt;finite-population&gt;" Title="Justifying the use of finite population correction" ViewCount="126" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a sample of graphs (more then 10000...).&#10;that look like in the image below:&#10;&lt;img src=&quot;http://i.stack.imgur.com/mBcIS.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I am searching an Unsupervised learning algorithems thet can help me to detect Anomaly observations.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here what i suggest for beggining: for every observation i have a Collection of points (x,y). With this collection, i find fourier series with regrresion (I compute coefficients with the base {1,sin(x),cos(x),sin(2x),cos(2x)...}).&#10;Now I have a set of coefficients instead of waves.&lt;/p&gt;&#10;&#10;&lt;p&gt;Somebody have an idea how to detect anomaly?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-22T15:33:56.223" Id="125066" LastActivityDate="2014-11-22T15:33:56.223" OwnerUserId="61269" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;data-visualization&gt;&lt;mathematical-statistics&gt;&lt;data-mining&gt;&lt;data-transformation&gt;" Title="Unsupervised learning algorithems to detect anomaly in waves" ViewCount="39" />
  
  
  
  <row Body="&lt;p&gt;Assuming you know, or can calculate, the standard deviations of the values of $x$ and $y$, the standardized estimate $b'$ is given by:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;b'=b \times \frac{s_x}{s_y} &#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;So to unstandardize: &lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;b=b' \times \frac{s_y}{s_x} &#10;$$&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-11-22T18:15:56.323" Id="125079" LastActivityDate="2014-11-22T18:15:56.323" OwnerUserId="17072" ParentId="125077" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;Here is a small counterexample that is not symmetric: -3, -2, 0, 0, 1, 4 is unimodal with mode = median = mean = 0.&lt;/p&gt;&#10;&#10;&lt;p&gt;Edit: An even smaller example is -2, -1, 0, 0, 3.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you want to imagine a random variable rather than a sample, take the support as {-2, -1, 0, 3} with probability mass function 0.2 on all of them except for 0 where it is 0.4.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-11-22T21:52:58.477" Id="125096" LastActivityDate="2014-11-22T22:25:50.893" LastEditDate="2014-11-22T22:25:50.893" LastEditorUserId="22228" OwnerUserId="22228" ParentId="125084" PostTypeId="2" Score="10" />
  <row AnswerCount="0" Body="&lt;p&gt;I have been searching online for a way to find the first and second derivative as shown In this youtube video Here is a screen shot&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/yZHQ7.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;(&lt;a href=&quot;https://www.youtube.com/watch?v=LcEqOzNov4E&quot; rel=&quot;nofollow&quot;&gt;https://www.youtube.com/watch?v=LcEqOzNov4E&lt;/a&gt; (Concavity, concave upwards and concave downwards intervals)) &lt;/p&gt;&#10;&#10;&lt;p&gt;I want to find the first and second &lt;strong&gt;derivative&lt;/strong&gt; of a linear regression like this article shows&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://tuckerreport.com/indicators/angle-measure/&quot; rel=&quot;nofollow&quot;&gt;http://tuckerreport.com/indicators/angle-measure/&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/vDK2M.gif&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The article calls it a 'arctangent line' though..&lt;/p&gt;&#10;&#10;&lt;p&gt;To quote the author&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The upper graph shows the prices with a fairly long term and smoothed linear regression curve. The lower subgraph has the result of the arctangent line, which I’ll call it the angle measure, that corresponds with the angle of the above regression curve. I am using the current value and measuring the angle with the average of the previous few bars. The dashed cyan line is the zero basis line, so the bright blue line is indicating an up angle in the regression curve if it is above the basis line. A down sloping curve on the regression line is indicated if the angle measure is below the zero basis line.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Another article (&lt;a href=&quot;https://www.mql5.com/en/articles/303&quot; rel=&quot;nofollow&quot;&gt;https://www.mql5.com/en/articles/303&lt;/a&gt;) shows an inverse fisher transform which is explained as tanh and looks like it's doing a very similar action&lt;/p&gt;&#10;&#10;&lt;p&gt;To quote the author&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Fisher transform is simply arctanh(x) and Inverse Fisher Transform is its inverse, tanh(x)&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;And here is an image of an inverse fisher on an RSI&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/5Hsxy.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Are these things related, I am not sure if I am on the right track I need more targeted keywords for Googling this or If anyone knows an example of code which I can study (any code for time series data, javascript, c++, python, mql, any...)&lt;/p&gt;&#10;&#10;&lt;p&gt;Simplified Question :How is this done?&lt;/p&gt;&#10;&#10;&lt;p&gt;I think this pic looks like what I am referring to&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/zFUJk.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2014-11-22T22:07:37.370" Id="125097" LastActivityDate="2014-11-23T10:22:59.190" LastEditDate="2014-11-23T10:22:59.190" LastEditorUserId="56655" OwnerUserId="56655" PostTypeId="1" Score="0" Tags="&lt;derivative&gt;" Title="Arctangent, Tanh, Concavity.. How can I find the derivatives?" ViewCount="54" />
  <row Body="&lt;p&gt;I think Item Factor Analysis and Subescale Factor Analysis could be seen as a two steps of a general procedure to evaluate measurements.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-11-22T23:26:08.857" Id="125101" LastActivityDate="2014-11-22T23:26:08.857" OwnerUserId="61293" ParentId="37389" PostTypeId="2" Score="0" />
  
  
  <row AcceptedAnswerId="125147" AnswerCount="1" Body="&lt;p&gt;Consider an i.i.d. sample $(X_1, Y_1), \dots, (X_N, Y_N)$, where each $X_i$ and $Y_i$ are $n$-dimensional column vectors, let $M \leq N$ and denote by $\hat{\beta}^{(M)}$ and $\hat{\beta}^{(N)}$ the coefficients resulting from applying linear regression to the sample comprising the first $M$/$N$ pairs, respectively. Is it true or false that&#10;$$&#10;\hat{\beta}^{(M)}_i \sim \hat{\beta}^{(N)}_i&#10;$$&#10;where $\hat{\beta}^{(K)}_i$ ($K\in\{M,N\}$) is the $i$th component of the vector $\hat{\beta}^{(K)}$.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-23T09:10:48.457" Id="125128" LastActivityDate="2014-11-23T13:10:47.777" LastEditDate="2014-11-23T09:24:47.170" LastEditorUserId="25386" OwnerUserId="25386" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;linear-model&gt;" Title="$\hat{\beta}^{(M)}_i\sim \hat{\beta}^{(N)}_i$ for linear regression?" ViewCount="34" />
  
  <row AcceptedAnswerId="125138" AnswerCount="1" Body="&lt;p&gt;My data looks as follows: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;X=cbind(var1=rbinom(100,1,0.26),var2=rbinom(100,1,0.01),&#10; var3=rbinom(100,1,0.07),var4=rbinom(100,1,0.09),var5=rbinom(100,1,0.004))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;If I use: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;X=melt(X)&#10;ggplot(Xaes(x=variable,y=value))+&#10;  geom_boxplot ()&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I get plot that isn't useful. How do to adjust the code, if I want to do it with &lt;code&gt;ggplot&lt;/code&gt;?&lt;/p&gt;&#10;" ClosedDate="2014-11-23T20:50:01.220" CommentCount="0" CreationDate="2014-11-23T11:02:18.730" Id="125136" LastActivityDate="2014-11-23T11:22:57.913" LastEditDate="2014-11-23T11:21:01.243" LastEditorUserId="26338" OwnerUserId="61312" PostTypeId="1" Score="-1" Tags="&lt;r&gt;&lt;ggplot2&gt;" Title="Rearrange boxplot with ggplot" ViewCount="41" />
  
  
  
  <row Body="&lt;p&gt;You can look at the up and downs as a random sequence, which is generated by some random process. For instance, let's assume that you're dealing with a stationary series $x_1,x_2,x_3,...,x_n \in f(x)$, where $f(x)$ is a probability distribution such as Gaussian, Poisson or anything else. This is stationary series. Now, you can create new variable $y_t$ such that $y_t=1:x_t&amp;lt;x_{t+1}$ and $y_t=0:x_t\ge x_{t+1}$, these are your ups and downs. This new sequence will form its own random sequence with interesting properties, see e.g. &lt;a href=&quot;http://scholar.google.com/scholar?q=Markov%20properties%20of%20gaps%20between%20local%20maxima%20in%20a%20sequence%20of%20independent%20random%20variables&amp;amp;btnG=&amp;amp;hl=en&amp;amp;as_sdt=0%2C47&quot; rel=&quot;nofollow&quot;&gt;V Khil, Elena. &quot;Markov properties of gaps between local maxima in a sequence of independent random variables.&quot; (2013).&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;For instance, look at &lt;a href=&quot;http://en.wikipedia.org/wiki/Autocorrelation&quot; rel=&quot;nofollow&quot;&gt;ACF&lt;/a&gt; and PACF of your series. There's nothing here. This doesn't seem like ARIMA model. It looks like uncorrelated sequence of $x_t$.&#10;&lt;img src=&quot;http://i.stack.imgur.com/rMzeg.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;This means that we could try applying known results for $y_t$, e.g. it's known that the average distance between two (up-down) pairs (or U-turns as some call them) is 3. In your data set the first peak (up-down) is 1957 and the last one is 2012, with 16 peaks in total. So, the average distance between peaks is 15/55=3.67. We know that the $\sigma=1.108$, and with 15 observations $\sigma_{15}=\sigma/\sqrt{15}=0.29$. So the mean distance between peaks is within $1.2\sigma_n$ from the theoretical mean. &lt;/p&gt;&#10;&#10;&lt;p&gt;UPDATE: on cycles&lt;/p&gt;&#10;&#10;&lt;p&gt;The graph in OP's question appears to suggest that there's some kind of long running cycle. There are several issues with this.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;If you generate a random sequence, sometimes something like a cycle may appear just randomly. So, with 58 data points which are purely observational data, it's impossible to declare a cycle without some kind of economic theory behind it. Nobody's going to take it seriously without the economic reasoning. For that you definitely need exogenous variables, I'm afraid.&lt;/li&gt;&#10;&lt;li&gt;Check out this wonderful paper: &lt;a href=&quot;http://people.virginia.edu/~slf9s/teaching/econ452/readings/Slutsky%201937.pdf&quot; rel=&quot;nofollow&quot;&gt;The Summation of Random Causes as the Source of Cyclic Processes&lt;/a&gt;, Eugen Slutzky, Econometrica, Vol. 5, No. 2 (Apr., 1937), pp. 105-146. Basically, sometimes cycles are caused by some sort of MA process. &lt;/li&gt;&#10;&lt;li&gt;This could be an illusion. I use this trick quite often in presentations. I show the actual data, then draw lines, circles or arrows to mess with my audience's brains :) The extra lines trick the brain into seeing trends which may not be there at all, or to make them look much stronger than they actually are.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="8" CreationDate="2014-11-23T19:32:51.660" Id="125170" LastActivityDate="2014-11-24T03:23:58.353" LastEditDate="2014-11-24T03:23:58.353" LastEditorUserId="36041" OwnerUserId="36041" ParentId="125154" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Here is one way to fit the model that you describe.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# sample series&#10;x &amp;lt;- AirPassengers&#10;# to illustrate a more general case, &#10;# take a subsample that does not start in the first season&#10;# and ends in the last season&#10;x &amp;lt;- window(x, start=c(1949,2), end=c(1959,4))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Indicator variables for the seasonal intercepts can be created in several ways, for example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# monthly intercepts&#10;S &amp;lt;- frequency(x)&#10;monthly.means &amp;lt;- do.call(&quot;rbind&quot;, &#10;  replicate(ceiling(length(x)/S), diag(S), simplify = FALSE))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Some arrangements for non-square time series (this has no effect if the series starts in the first season and ends in the last season):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;monthly.means &amp;lt;- ts(monthly.means, frequency = S, start = c(start(x)[1], 1))&#10;monthly.means &amp;lt;- window(monthly.means, start = start(x), end = end(x))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Regardless of the month the first observation belongs to, the first column is related to January, the second to February and so on.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# column names&#10;if (S == 12) {&#10;  colnames(monthly.means) &amp;lt;- month.abb&#10;} else&#10;  colnames(monthly.means) &amp;lt;- paste(&quot;season&quot;, 1L:S)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;First rows of the &lt;code&gt;monthly.means&lt;/code&gt; object:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;monthly.means[1:15,]&#10;#       Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec&#10;#  [1,]   0   1   0   0   0   0   0   0   0   0   0   0&#10;#  [2,]   0   0   1   0   0   0   0   0   0   0   0   0&#10;#  [3,]   0   0   0   1   0   0   0   0   0   0   0   0&#10;#  [4,]   0   0   0   0   1   0   0   0   0   0   0   0&#10;#  [5,]   0   0   0   0   0   1   0   0   0   0   0   0&#10;#  [6,]   0   0   0   0   0   0   1   0   0   0   0   0&#10;#  [7,]   0   0   0   0   0   0   0   1   0   0   0   0&#10;#  [8,]   0   0   0   0   0   0   0   0   1   0   0   0&#10;#  [9,]   0   0   0   0   0   0   0   0   0   1   0   0&#10;# [10,]   0   0   0   0   0   0   0   0   0   0   1   0&#10;# [11,]   0   0   0   0   0   0   0   0   0   0   0   1&#10;# [12,]   1   0   0   0   0   0   0   0   0   0   0   0&#10;# [13,]   0   1   0   0   0   0   0   0   0   0   0   0&#10;# [14,]   0   0   1   0   0   0   0   0   0   0   0   0&#10;# [15,]   0   0   0   1   0   0   0   0   0   0   0   0&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;For the monthly trends we can reuse `monthly.means':&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;monthly.trends &amp;lt;- monthly.means * seq_along(x) / S&#10;round(monthly.trends[1:15,], 2)&#10;#       Jan  Feb  Mar  Apr  May  Jun Jul  Aug  Sep  Oct  Nov  Dec&#10;#  [1,]   0 0.08 0.00 0.00 0.00 0.00 0.0 0.00 0.00 0.00 0.00 0.00&#10;#  [2,]   0 0.00 0.17 0.00 0.00 0.00 0.0 0.00 0.00 0.00 0.00 0.00&#10;#  [3,]   0 0.00 0.00 0.25 0.00 0.00 0.0 0.00 0.00 0.00 0.00 0.00&#10;#  [4,]   0 0.00 0.00 0.00 0.33 0.00 0.0 0.00 0.00 0.00 0.00 0.00&#10;#  [5,]   0 0.00 0.00 0.00 0.00 0.42 0.0 0.00 0.00 0.00 0.00 0.00&#10;#  [6,]   0 0.00 0.00 0.00 0.00 0.00 0.5 0.00 0.00 0.00 0.00 0.00&#10;#  [7,]   0 0.00 0.00 0.00 0.00 0.00 0.0 0.58 0.00 0.00 0.00 0.00&#10;#  [8,]   0 0.00 0.00 0.00 0.00 0.00 0.0 0.00 0.67 0.00 0.00 0.00&#10;#  [9,]   0 0.00 0.00 0.00 0.00 0.00 0.0 0.00 0.00 0.75 0.00 0.00&#10;# [10,]   0 0.00 0.00 0.00 0.00 0.00 0.0 0.00 0.00 0.00 0.83 0.00&#10;# [11,]   0 0.00 0.00 0.00 0.00 0.00 0.0 0.00 0.00 0.00 0.00 0.92&#10;# [12,]   1 0.00 0.00 0.00 0.00 0.00 0.0 0.00 0.00 0.00 0.00 0.00&#10;# [13,]   0 1.08 0.00 0.00 0.00 0.00 0.0 0.00 0.00 0.00 0.00 0.00&#10;# [14,]   0 0.00 1.17 0.00 0.00 0.00 0.0 0.00 0.00 0.00 0.00 0.00&#10;# [15,]   0 0.00 0.00 1.25 0.00 0.00 0.0 0.00 0.00 0.00 0.00 0.00&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Some arbitrary regressors:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(123)&#10;xreg1 &amp;lt;- runif(length(x), 100, 200)&#10;xreg2 &amp;lt;- rnorm(length(x), mean = mean(x))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edited&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;As I did in my first edit, the model could now be fitted using the function &lt;code&gt;lm&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lm(x ~ 0 + monthly.means + monthly.trends + xreg1 + xreg2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;But I overlooked the AR(1) structure for the error term that is mentioned in the question. As @RobHyndman says in the comment below, we can use the function &lt;code&gt;arima&lt;/code&gt; specifying an AR(1) process for the error term, &lt;code&gt;order = c(1,0,0)&lt;/code&gt;. An intercept should not be included in order to avoid exact multicollinearity with the dummies for the seasonal means. (The example is arbitrary, the output is printed just to show the coefficients that are estimated.)&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;xreg &amp;lt;- cbind(monthly.means, monthly.trends, xreg1, xreg2)&#10;fit &amp;lt;- arima(x, order = c(1,0,0), xreg = xreg, include.mean = FALSE)&#10;round(coef(fit), 2)&#10;#                ar1  monthly.means.Jan  monthly.means.Feb  monthly.means.Mar &#10;#               0.84              69.87              82.45              93.10 &#10;#  monthly.means.Apr  monthly.means.May  monthly.means.Jun  monthly.means.Jul &#10;#              85.17              76.39              77.48              83.31 &#10;#  monthly.means.Aug  monthly.means.Sep  monthly.means.Oct  monthly.means.Nov &#10;#              81.01              81.23              68.06              57.34 &#10;#  monthly.means.Dec monthly.trends.Jan monthly.trends.Feb monthly.trends.Mar &#10;#              73.20              27.07              23.36              27.84 &#10;# monthly.trends.Apr monthly.trends.May monthly.trends.Jun monthly.trends.Jul &#10;#              27.56              29.34              35.98              40.47 &#10;# monthly.trends.Aug monthly.trends.Sep monthly.trends.Oct monthly.trends.Nov &#10;#              40.30              32.07              27.85              24.02 &#10;# monthly.trends.Dec              xreg1              xreg2 &#10;#              25.60               0.00               0.08 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;We can do some simple check. In a model containing only the seasonal intercepts, the estimates of the coefficients match the values of the sample means.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;fit2 &amp;lt;- arima(x, order = c(0,0,0), xreg = xreg[,1:12], include.mean = FALSE)&#10;coef.seasonal.intercepts &amp;lt;- coef(fit2)&#10;sample.means &amp;lt;- lapply(split(x, cycle(x)), mean)&#10;all.equal(coef.seasonal.intercepts,&#10;  as.vector(unlist(sample.means)), check.attributes = FALSE)&#10;# TRUE&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edited&lt;/strong&gt; (Answer to a comment posted by the OP.)&lt;/p&gt;&#10;&#10;&lt;p&gt;If the disturbance term follows an AR(1) process, the Ordinary Least Squares estimator is unbiased but not efficient, i.e. on average it gives the true value but the standard errors of parameters estimates are higher than in the classical setting of independent errors (in other words, estimates are not efficient).&lt;/p&gt;&#10;&#10;&lt;p&gt;As you say, extending the model with the AR(1) error term via &lt;code&gt;arima&lt;/code&gt; will not change much the the estimates (standard OLS is still unbiased), but their standard errors will be smaller due to the gain in efficiency.&lt;/p&gt;&#10;&#10;&lt;p&gt;When the disturbance term is autocorrelated, omitting the AR term will lead to larger standard errors for the estimates, which implies a larger denominator in the t-statistics and therefore larger t-statistics. Hence, the tests for the null of non-significant regressors will be biased towards non-rejection. Using &lt;code&gt;arima&lt;/code&gt; to specify the AR error term will protect against this issue.&lt;/p&gt;&#10;&#10;&lt;p&gt;The code below is a small exercise to check these ideas: 10,000 series are generated from a model with intercept and AR(1) errors. If &lt;code&gt;xregcoef&lt;/code&gt; is set to a value other than zero, then a external variable is added to the data. A model that includes the intercept and the external regressor is fitted by means of &lt;code&gt;arima&lt;/code&gt; with &lt;code&gt;order=c(1,0,0)&lt;/code&gt; (stored in &lt;code&gt;fit1&lt;/code&gt;) and by means of &lt;code&gt;lm&lt;/code&gt; (stored in &lt;code&gt;fit2&lt;/code&gt;).&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(123)&#10;xreg &amp;lt;- runif(200, 2, 6)&#10;xregcoef &amp;lt;- 0&#10;res &amp;lt;- matrix(nrow = 10000, ncol = 6)&#10;colnames(res) &amp;lt;- c(&quot;coef 1&quot;, &quot;s.e. 1&quot;, &quot;t-stat. 1&quot;, &quot;coef 2&quot;, &quot;s.e. 2&quot;, &quot;t-stat. 2&quot;)&#10;for (i in seq.int(nrow(res)))&#10;{&#10;  x &amp;lt;- 2 + arima.sim(n=200, model=list(ar=0.7)) + xregcoef * xreg    &#10;  fit1 &amp;lt;- arima(x, order=c(1,0,0), xreg=xreg, include.mean=TRUE)&#10;  res[i,1] &amp;lt;- coef(fit1)[&quot;xreg&quot;]&#10;  res[i,2] &amp;lt;- sqrt(fit1$var.coef[&quot;xreg&quot;,&quot;xreg&quot;])&#10;  res[i,3] &amp;lt;- res[i,1]/res[i,2]    &#10;  fit2 &amp;lt;- summary(lm(x ~ 1 + xreg))&#10;  res[i,4:6] &amp;lt;- coef(fit2)[&quot;xreg&quot;, c(&quot;Estimate&quot;, &quot;Std. Error&quot;, &quot;t value&quot;)]&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Setting for example &lt;code&gt;xregcoef=3&lt;/code&gt;, we can see that estimates from both regressions are very close to the true value (unbiased estimates) but the standard errors are slightly higher&#10;for &lt;code&gt;lm&lt;/code&gt;, where when the AR structure is omitted.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# results for xregcoef=3&#10;t(apply(res, 2, summary))&#10;#               Min.  1st Qu.   Median     Mean  3rd Qu.     Max.&#10;# coef 1     2.81600  2.96300  3.00100  3.00000  3.03800  3.20100&#10;# s.e. 1     0.04313  0.05286  0.05491  0.05492  0.05693  0.06677&#10;# coef 2     2.65800  2.93300  3.00100  2.99900  3.06400  3.37000&#10;# s.e. 2     0.05928  0.08393  0.08879  0.08905  0.09400  0.12280&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Let's now consider &lt;code&gt;xregcoef=0&lt;/code&gt;, that is, the regressor is not part of the data generating process but we fit a model including this regressor. On average the coefficient is estimated as zero in both cases, however, as the standard errors are slightly higher in the second case, the null of the t-test is rejected slightly more often than it should, given a 5% significance level chosen below.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# results for xregcoef=0&#10;t(apply(res, 2, summary))&#10;#               Min.  1st Qu.    Median       Mean 3rd Qu.    Max.&#10;# coef 1    -0.18440 -0.03698 0.0007062  0.0003874 0.03763 0.20080&#10;# s.e. 1     0.04313  0.05286 0.0549100  0.0549200 0.05693 0.06677&#10;# t-stat. 1 -3.68100 -0.67410 0.0129200  0.0072410 0.68500 3.53400&#10;# coef 2    -0.34200 -0.06694 0.0012510 -0.0006191 0.06422 0.36970&#10;# s.e. 2     0.05928  0.08393 0.0887900  0.0890500 0.09400 0.12280&#10;# t-stat. 2 -3.96600 -0.75560 0.0138800 -0.0070140 0.72210 4.44500&#10;#&#10;# rejections of the null xregcoef=0 at the 5% significance level &#10;sum(abs(res[,3]) &amp;gt; 1.96) / nrow(res)&#10;# [1] 0.0516&#10;sum(abs(res[,6]) &amp;gt; 1.96) / nrow(res)&#10;# [1] 0.0749&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="7" CreationDate="2014-11-23T19:55:39.030" Id="125175" LastActivityDate="2014-11-25T17:19:18.923" LastEditDate="2014-11-25T17:19:18.923" LastEditorUserId="48766" OwnerUserId="48766" ParentId="125152" PostTypeId="2" Score="10" />
  <row Body="&lt;p&gt;You are trying to convince us that you can: &quot;the second sample has nothing to do with the first&quot;, therefore, you can. An experimental unit is a unit to which treatment is applied, as justified by the researcher/question of interest. As long as you convince the reviewers. But one might make a very compelling case that a person lucky enough to get 12 episodes has something going on compared to the one with 1 (which could be modeled as a covariate, for example), and if it is the same antibiotic family, a strong interaction with the number and time-to the prior episodes might be expected. Also, depending on the infection, there could be partial immunity (hence episode priority component). &lt;/p&gt;&#10;&#10;&lt;p&gt;Although not directly affecting your definition of an experimental unit, distributional assumptions are usually tested on a population basis, but you do not provide further information as to how you plan to address that, or what specific testing you are considering.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-23T21:29:23.687" Id="125180" LastActivityDate="2014-11-23T21:29:23.687" OwnerUserId="57390" ParentId="125062" PostTypeId="2" Score="0" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have the followig sequences:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;$Pr(X_n=n)=Pr(X_n=-n)=0.5$&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;$Pr(X_n=2^{n/2})=Pr(X_n=-2^{n/2})=0.5$&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I have to show whether they satisfy Lindeberg's condition or not, but this condition is a bit unclear for me.&lt;/p&gt;&#10;&#10;&lt;p&gt;$\lim_{n\to\infty} \frac{1}{s_n^2} \sum^{n}_{k=1} \mathbb{E} [(X_k -\mu_k)^2 \cdot \mathbb{1}_{|X_k-\mu_k|&amp;gt;\epsilon s_n}]=0$  &lt;/p&gt;&#10;&#10;&lt;p&gt;for all $\epsilon &amp;gt;0$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The expected values of the r.v.-s  are $0$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The variance in the first case: $n^2$, in the second case $2^n$.&lt;/p&gt;&#10;&#10;&lt;p&gt;In both cases the sums of the variances go to infinity. &#10;Isn't that enough for the condition to go to zero?&lt;/p&gt;&#10;&#10;&lt;p&gt;And I don't really understand that expression in the indicator function.&lt;/p&gt;&#10;&#10;&lt;p&gt;I appreciate any help! &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-11-23T22:10:50.500" Id="125183" LastActivityDate="2014-11-23T23:12:30.317" LastEditDate="2014-11-23T22:46:07.623" LastEditorUserId="28746" OwnerUserId="61337" PostTypeId="1" Score="5" Tags="&lt;random-variable&gt;&lt;convergence&gt;&lt;central-limit-theorem&gt;" Title="Do these random variables satisfy Lindeberg's condition?" ViewCount="76" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Is it okay to perform independent sample T-test to test the effect of an health awareness intervention on male and female population registered at 300 polling booths in a town.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-11-24T03:27:12.973" Id="125203" LastActivityDate="2014-11-24T03:27:12.973" OwnerUserId="61346" PostTypeId="1" Score="0" Tags="&lt;t-test&gt;" Title="T-test to check a significance" ViewCount="26" />
  <row AnswerCount="0" Body="&lt;p&gt;This is regarding theory of attribute type control charts.In a question it is given data for the number of non confirmities &lt;strong&gt;per 1000 meters&lt;/strong&gt; of wire. I have to calculate control limits for  a appropriate control chart.&lt;br&gt;&#10;My question is since this data is given per 1000 meters I thought of using a U chart,supposing that the inspection unit is of 1m and the sample size is then 1000 inspection units.  &lt;/p&gt;&#10;&#10;&lt;p&gt;But if I take the inspection unit to be 1000m,  I could have used a C chart as well, right?&lt;/p&gt;&#10;&#10;&lt;p&gt;So how to determine which chart is to be used?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-24T03:59:17.337" Id="125206" LastActivityDate="2014-11-24T03:59:17.337" OwnerUserId="45048" PostTypeId="1" Score="0" Tags="&lt;self-study&gt;&lt;theory&gt;&lt;quality-control&gt;&lt;control-chart&gt;" Title="How to determine if a U chart or a C chart to be used" ViewCount="19" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;As part of my class project i had to use decision tree classification on a training set which contains a set of matrices where each row is a vector recorded at a particular time stamp and each row vector describes the values observed by a fixed set of sensors. Decision tree, works with vector data but i have matrix data. How can i convert matrix data to vector data? Is there any standard practices for it? I tried to search but i could not locate any useful material.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-24T06:50:28.960" Id="125214" LastActivityDate="2014-11-24T06:50:28.960" OwnerUserId="59381" PostTypeId="1" Score="0" Tags="&lt;feature-selection&gt;&lt;information-retrieval&gt;" Title="Modeling 2-D data as vector" ViewCount="9" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a doubt regarding, which statistical significance test to apply on a confusion matrix.&#10;Say for example there is a two class classification problem. The confusion matrix is given as follows:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;  0   1&#10;0 60 20&#10;1 30 70&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Help will be highly appreciated.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-11-24T07:04:27.073" Id="125215" LastActivityDate="2014-11-24T07:04:27.073" OwnerUserId="61354" PostTypeId="1" Score="0" Tags="&lt;statistical-significance&gt;" Title="Statistical significance test for confusion matrix" ViewCount="35" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a LOT of time series observations and I would like to estimate a simple AR(1) model &#10;$$&#10;y_t =c+ \phi y_{t-1}+ \varepsilon_t \qquad \varepsilon_t \sim \text{N}(0, \sigma^{2}) &#10;$$&#10;with parameters $\theta =\left\{ c, \phi,\sigma^{2} \right\}$. Because of the amount of observations I would like to use stochastic gradient descent (or &quot;deterministic&quot; Robbins and Monro algorithm) to estimate the model &#10;$$&#10;\theta^{i}_{t+1} = \theta^{i}_t + \gamma_t s^{i}_t , \quad \sum \gamma_t =\infty \quad \text{and}\quad \sum \gamma^{2}_t &amp;lt;\infty&#10;$$&#10;where $\theta^{i}$ is an element of $\theta$ and $s^{i}_t$ is the derivative of log likelihood contribution at time t with respect to $\theta^{i}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have the following questions:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;First of all is this supposed to work? What alternative online estimation procedures are out there? I have the feeling that this is really not robust and needs a lot of case by case tuning.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;How to set $\gamma_t$? I know the standard choice of $\gamma_t$ is $t^{-2/3}$ which satisfy the requirements. My experience so far is that I should set $\gamma_t$ differently for different parameters as the data is more informative about the constant $c$ than about $\sigma^2$ and the gradients have totally different magnitude. If i use the same gamma some of the parameters would diverge (too large step size) while others would converge slowly. Is there a &quot;stochastic Newton-Raphson&quot; type algorithm which would scale the gradients properly? I guess trying different combinations would work but there are too many combinations if I would set different gammas for different parameters.  &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Any comment is appreciated! &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-24T09:14:01.780" Id="125222" LastActivityDate="2014-11-24T09:14:01.780" OwnerUserId="61356" PostTypeId="1" Score="0" Tags="&lt;time-series&gt;&lt;estimation&gt;&lt;algorithms&gt;&lt;gradient-descent&gt;&lt;online&gt;" Title="Basic questions about stochastic gradient descent / Robbins and Monro algorithm" ViewCount="33" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to solve a semi-supervised learning problem using &lt;a href=&quot;http://vikas.sindhwani.org/MR.pdf&quot; rel=&quot;nofollow&quot;&gt;LaplacianSVM&lt;/a&gt;. However, before applying LapSVM I would like either to perform feature selection or feature extraction. Furthermore, after training I need to perform model selection. As is a semi-supervised problem I am dealing with labelled and unlabeled data. I do not know anything about the structure of the data, only I suspect that some features are redundant. My ideas to tackle this are:&lt;/p&gt;&#10;&#10;&lt;p&gt;1) Apply semi-supervised feature selection (&lt;a href=&quot;http://ac.els-cdn.com/S092523120800115X/1-s2.0-S092523120800115X-main.pdf?_tid=54cba01e-73bd-11e4-a4fb-00000aacb362&amp;amp;acdnat=1416821953_7f789666cad723d062390070450e3802&quot; rel=&quot;nofollow&quot;&gt;LSDF&lt;/a&gt;) then select the most relevant $n$ features and train a LapSVM classifier. After this, use Leave-one-out cross validation on the labeled set to select the best free parameters for the feature selection phase and LapSVM phase.&lt;/p&gt;&#10;&#10;&lt;p&gt;2) Apply unsupervised feature extraction such as KPCA and then train the LapSVM classifier. After this, use Leave-one-out cross validation on the labeled set to select the best free parameters for KPCA phase and LapSVM phase.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to ask for advice, if is correct to apply FS or FE with LapSVM and if it is possible to do it more efficiently&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-24T09:47:50.123" Id="125226" LastActivityDate="2014-11-24T09:47:50.123" OwnerUserId="45299" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;cross-validation&gt;&lt;model-selection&gt;&lt;feature-selection&gt;&lt;semi-supervised&gt;" Title="Advice for feature selection or feature extraction with semi-supervised learning" ViewCount="50" />
  <row AnswerCount="0" Body="&lt;p&gt;Following post &#10;&lt;a href=&quot;http://stats.stackexchange.com/questions/67558/why-glmnet-can-be-calculated-parameters-for-all-category&quot;&gt;Why {glmnet} can be calculated parameters for all category?&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I have 4 categories or classes or responses for y (thus multinomial): &lt;em&gt;cat1, cat2, cat3&lt;/em&gt; and finally &lt;em&gt;no-cat&lt;/em&gt;. I want my &quot;&lt;strong&gt;no category (no-cat)&lt;/strong&gt;&quot; class to be the &lt;strong&gt;reference category&lt;/strong&gt;. Can I specify this in glmnet ? After performing a cv.glmnet I get coefficients for all categories.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note: I'm a new glmnet user.&#10;Thanks in advance,&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-11-24T10:30:56.457" Id="125235" LastActivityDate="2014-11-24T10:30:56.457" OwnerUserId="61260" PostTypeId="1" Score="0" Tags="&lt;classification&gt;&lt;multinomial&gt;&lt;logit&gt;&lt;glmnet&gt;" Title="glmnet: which is the reference category or class in multinomial regression?" ViewCount="55" />
  <row AnswerCount="0" Body="&lt;p&gt;this is a follow-up thread dealing with sampling from conditional copulas:&#10;Original question (with nice answer by whuber): &lt;a href=&quot;http://stats.stackexchange.com/questions/124865/sampling-from-conditional-copula/124985?noredirect=1#comment238224_124985&quot;&gt;Sampling from conditional copula&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Trying to sample from a conditional copula using R, right now. Would appreciate &#10;if someone experienced in this field could have a look at my code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;install.packages(&quot;copula&quot;)&#10;library(copula)&#10;# parameters&#10;theta  &amp;lt;- 2 #parameter for gumbel-copula&#10;#copula-object&#10;cgumbel &amp;lt;- gumbelCopula(param = theta, dim = 2,&#10;                        use.indepC = c(&quot;message&quot;, &quot;TRUE&quot;, &quot;FALSE&quot;))&#10;&#10;pquantile &amp;lt;- .5 # quantile I want to condition on (u = x^*)&#10;&#10;n &amp;lt;- 1e3&#10;valpha &amp;lt;- array(pquantile,n) #vector of identical quantile&#10;v &amp;lt;- seq(.01,.99,length.out=n) #arguemtns for conditional copula distribution&#10;vcCopula1 &amp;lt;- cCopula(cbind(valpha,v),cop=cgumbel) #conditional copula function&#10;&#10;plot(v,vcCopula1,xlab=&quot;v&quot;,ylab=&quot;c_u(v)&quot;, main=&quot;conditional copula distribution function&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In the end, I want to use the conditional Copula to have a look at the conditional distribution of the marginals (say x and y) (for example conditional expectation of x for a given quantile of y). As I understand it, I would just have to sample from the conditional copula for a given quantile of v, obtain u|v (where u and v are the on [0,1] from the cdf of x and y respectively) and apply $x = F_x^{-1} (u)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;best,&#10;noclue&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-24T10:35:23.943" FavoriteCount="1" Id="125237" LastActivityDate="2014-11-24T10:35:23.943" OwnerUserId="60208" PostTypeId="1" Score="0" Tags="&lt;sampling&gt;&lt;conditional-probability&gt;&lt;copula&gt;" Title="Sampling from conditional copula under R" ViewCount="62" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I am trying to understand the &lt;em&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation&quot; rel=&quot;nofollow&quot;&gt;Latent Dirichlet Allocation&lt;/a&gt;&lt;/em&gt; but therefore I need the basic understanding about what exactly a &lt;em&gt;latent variable&lt;/em&gt; is in that sense.&lt;/p&gt;&#10;&#10;&lt;p&gt;I know that the basic idea of a latent variable is something &lt;em&gt;unobserved&lt;/em&gt; (like an unknown parameter) that is assumed to explain an &lt;em&gt;observed&lt;/em&gt; event.&lt;/p&gt;&#10;&#10;&lt;p&gt;Could somebody explain to me (preferable in easy words) what exactly a latent variable is and perhaps give a simple example just to get me going?&lt;/p&gt;&#10;&#10;&lt;p&gt;Something I'd also would like to know is if the &lt;a href=&quot;http://en.wikipedia.org/wiki/Maximum_likelihood&quot; rel=&quot;nofollow&quot;&gt;Maximum Likelihood Estimate&lt;/a&gt; $\mathcal{L}(\theta;x_1,\ldots,x_n) = f(x_1,\ldots,x_n|\theta) = \prod_{i=1}^n f(x_i|\theta)$ e.g., where we want to find a parameter $\theta$ that models the observations $x_1,\ldots,x_n$ best, could be formalized in an environment depending on a latent variable or am I mixing things up here?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-24T15:04:00.310" Id="125281" LastActivityDate="2015-02-09T18:31:10.570" LastEditDate="2014-11-24T15:21:30.933" LastEditorUserId="61222" OwnerUserId="61222" PostTypeId="1" Score="1" Tags="&lt;latent-variable&gt;" Title="Explain: Latent Variable (e.g for Latent Dirichlet Allocation)" ViewCount="36" />
  <row AnswerCount="1" Body="&lt;p&gt;I am trying to compare the predictive ability of various models in predicting survival in patients.  I would like to examine the predictive performance of each model using 4 tests: squared Pearson correlation coefficient (R2), root mean squared prediction error (RMSPE), mean absolute prediction error (MAPE), and prediction bias. &lt;/p&gt;&#10;&#10;&lt;p&gt;I am trying to use SPSS to perform these tests but I cannot figure which would be the dependent (predicted or actual) and which would be the independent variable in the regression analysis. &lt;/p&gt;&#10;&#10;&lt;p&gt;Could anyone clarify this point?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks,&#10;JH&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-11-24T15:58:25.433" FavoriteCount="1" Id="125292" LastActivityDate="2015-02-02T14:40:43.263" OwnerUserId="60247" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;survival&gt;" Title="Comparison of predictive models" ViewCount="86" />
  
  <row AcceptedAnswerId="125315" AnswerCount="1" Body="&lt;p&gt;I have two &lt;em&gt;identical&lt;/em&gt; feature vectors. They have a distance score of &lt;strong&gt;0&lt;/strong&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;I perform DBSCAN Clustering (using sci-kit) and they get different labels.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is this expected behaviour?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-24T17:20:59.753" Id="125303" LastActivityDate="2014-11-24T18:36:48.563" LastEditDate="2014-11-24T18:36:48.563" LastEditorUserId="7828" OwnerUserId="34960" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;clustering&gt;&lt;distance-functions&gt;&lt;scikit-learn&gt;&lt;cosine-distance&gt;" Title="Why do two identical feature vectors (distance score 0) get different labels in DBSCAN?" ViewCount="18" />
  <row AnswerCount="0" Body="&lt;p&gt;Can't seem to figure this out very easily. I am looking for a similar output as that of statsmodels; however, statsmodels doesn't support some of the regressions that I want to do.&lt;/p&gt;&#10;&#10;&lt;p&gt;I basically want the R2, Adj R2, F-statistics, AIC, BIC, condition number, etc. I can implement these myself, but I think it would be easier if these were somehow already pre-built into scikit learn or someone's github... Thanks.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-24T17:32:40.103" Id="125305" LastActivityDate="2014-11-24T17:32:40.103" OwnerUserId="53990" PostTypeId="1" Score="0" Tags="&lt;python&gt;&lt;scikit-learn&gt;" Title="Is there a way to get the p-values/standard errors via a function in scikit learn?" ViewCount="9" />
  
  <row AnswerCount="0" Body="&lt;p&gt;It is likely that I am incorrect terminology here, but I am trying to compute a &quot;mean&quot; of an interval censored random variable. &lt;/p&gt;&#10;&#10;&lt;p&gt;Here is an example where:&lt;br&gt;&#10;1. a random sample from the standard normal distribution is discretized to create an interval censored random sample;&lt;br&gt;&#10;2. the marginal probability distribution of the sample is computed;&lt;br&gt;&#10;3. the midpoints of the intervals are computed;&lt;br&gt;&#10;4. the weighted mean of the interval midpoints is computed.  &lt;/p&gt;&#10;&#10;&lt;p&gt;A simple simulation shows that this leads to a biased estimate of the mean of the underlying variable, and obviously that bias depends on the:&lt;br&gt;&#10;1. number of intervals into which the variable is discretized, and;&lt;br&gt;&#10;2. the sample size.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;replIntMean = replicate(100, {&#10;  X = rnorm(1000)&#10;&#10;  # discretize the variable&#10;  XD = cut(X, quantile(X, probs = seq(0, 1, 0.1)), include.lowest = TRUE)&#10;&#10;  # compute the marginal probability table&#10;  probX = prop.table(table(XD))&#10;&#10;  # compute the upper and lower limit of the censoring intervals&#10;  liUL = regmatches(levels(XD), &#10;                    gregexpr(&quot;([\\+-]*[0-9]+\\.[0-9]+)&quot;, levels(XD)))&#10;&#10;  # computed the weighted mean of the variable&#10;  sum(probX * sapply(liUL, function(x) mean(as.numeric(x))))&#10;  },&#10;  simplify = &quot;array&quot;)&#10;&#10;plot(replIntMean, type = &quot;l&quot;)&#10;abline(h = 0, col = &quot;blue&quot;)&#10;abline(h = mean(replIntMean), col = &quot;red&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/PrcKZ.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I am wondering if there is any guidance in the literature on the right way to compute measures of central tendency of interval coded variables, and a discussion of their relative properties and interpreations.&lt;/p&gt;&#10;&#10;&lt;p&gt;Please note that the application is &lt;strong&gt;not&lt;/strong&gt; survival analysis as most of the references tend to be to that, and also I am aware that the simplest measure of central tendency here is the modal class.  &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-11-24T18:22:21.057" Id="125311" LastActivityDate="2014-11-24T18:22:21.057" OwnerUserId="8141" PostTypeId="1" Score="1" Tags="&lt;mean&gt;&lt;weighted-mean&gt;&lt;interval&gt;" Title="Mean of an interval censored random variable" ViewCount="26" />
  <row Body="&lt;p&gt;It turns out to be a first-hitting-time distribution:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/First-hitting-time_model&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/First-hitting-time_model&lt;/a&gt;&#10;&lt;a href=&quot;http://en.wikipedia.org/wiki/Inverse_Gaussian_distribution&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Inverse_Gaussian_distribution&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-24T18:34:13.243" Id="125314" LastActivityDate="2014-11-24T18:34:13.243" OwnerUserId="44526" ParentId="125086" PostTypeId="2" Score="0" />
  
  <row AcceptedAnswerId="125456" AnswerCount="1" Body="&lt;p&gt;Are cox model studies over too long a time-scale at risk of over estimating (or under estimating) a covariate's effect on hazard?&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm studying inbreeding in a captive animal population. Some individuals are 30+ years old. The oldest year of age, in this model's terms, is our longest (or latest) year of study. But there's the possibility that only a few animals live to be this long (perhaps &amp;lt;5%).&lt;/p&gt;&#10;&#10;&lt;p&gt;Could the hazard ratio outputs from my mixed effect Cox models be over-estimates due to the study period being too long and only a few individuals living to an old age (i.e., does having only a few of one group of interest - say inbred individuals - present for a long period in a study bias hazard ratio estimates)?&lt;/p&gt;&#10;&#10;&lt;p&gt;Any help, advice, or discussion would be greatly appreciated. Thank you. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-24T20:17:52.057" Id="125334" LastActivityDate="2014-11-25T16:21:23.563" OwnerUserId="55793" PostTypeId="1" Score="2" Tags="&lt;survival&gt;&lt;experiment-design&gt;&lt;cox-model&gt;&lt;hazard&gt;" Title="Study Length Over-Estimating Hazard Ratio?" ViewCount="29" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Dear all: I need to test which effects I should include in my model for genetic evaluation of cows. I was using the following code in R: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;model1 = lm(milk ~ factor(year) + factor(herd) + factor(season) + age + I(age^2), data=paula1)&#10;anova(model1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;However, all my effects were highly significant (&amp;lt; 2.2e-16 ***). I treid using &lt;code&gt;step(model1)&lt;/code&gt; to choose the best model, I tried to include other effects that I would never expect to be significant and they were. So I was thinking that I was doing something wrongly. Then I tried proc glm in SAS using the following code:&lt;/p&gt;&#10;&#10;&lt;p&gt;data paula1; set paula0;&#10;proc glm;&#10;class year herd season;&#10;model milk= year herd season age age*age;&#10;run;&lt;/p&gt;&#10;&#10;&lt;p&gt;And my results were very similar. I decided then to exclude some data and shuffle some variables and it is still signifficant. Now I have no doubts that my analysis are completely wrong, I just cant figure out what my mistake is. For some factors (e.g. herd I have more than 200 levels) and I have missing data as well (coded as NA in R and blank in SAS). The outputs look fine (sum of squares, degree of freedom, etc)&#10;Any help would be very much appreciated. Thanks. Paula&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-24T21:01:20.940" Id="125344" LastActivityDate="2014-11-24T21:49:24.210" OwnerUserId="60505" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;generalized-linear-model&gt;&lt;sas&gt;&lt;errors-in-variables&gt;" Title="Selecting variables using SAS and R - all effects are significant even when shuffling the data" ViewCount="43" />
  
  <row AnswerCount="0" Body="&lt;p&gt;For my research purpose I wrote a small simulation to compute an empirial p-value based on an input gene list and a reference gene list. &#10;The idea is to check for each gene in the input gene list, the associate score in the reference gene and to do the sum. After that to create N=100000 simulated list of the same size as the input gene list by picking randomly genes in the complete gene list, and to compute their associate score. At the end I can compute a p-value by counting the number of simulated list having a score superior or equal to the input gene list, then divide by N=100000.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now the problem, is that I have several input gene lists with different number of genes; and different reference gene list with different number of genes. How can I compare the results of these different simulations ( I know that p-values can not be compared .. ) &lt;/p&gt;&#10;&#10;&lt;p&gt;I saw that effect size can help me; and saw this figure in &quot;Effect size, confidence interval and statistical significance: a practical guide for biologists&quot;, Nakagawa et al. How can I plot such graphic with my different input gene list an reference gene lists&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/N6Q2m.jpg&quot; alt=&quot;effect size&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-11-24T22:00:34.047" FavoriteCount="0" Id="125349" LastActivityDate="2014-11-27T14:38:15.873" LastEditDate="2014-11-27T14:38:15.873" LastEditorUserId="7835" OwnerUserId="7835" PostTypeId="1" Score="0" Tags="&lt;p-value&gt;&lt;sample-size&gt;&lt;effect-size&gt;" Title="Compute effect size based on simulation using different lists of different length" ViewCount="19" />
  <row AnswerCount="0" Body="&lt;p&gt;What statistical analysis should I use if I have 2 groups of subjects (high and low comprehenders) which are divided into 3 subgroups according to text genre (poetry, short story, drama) and subjected into three reading tests (pre-, post-intervention and post-intervention with text familiarity)? I have six participants, one for each subgroup who each produced verbal reports of strategies for three texts (one genre). The dependent variable is the proportion of accurate verbal reports per text. My hypotheses are that a) genre b) condition (test) and c) reading proficiency will have a significant effect on the proportion of verbal reports.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-11-25T00:34:25.537" Id="125365" LastActivityDate="2014-11-25T00:34:25.537" OwnerUserId="61427" PostTypeId="1" Score="1" Tags="&lt;self-study&gt;&lt;computational-statistics&gt;" Title="What statistical analysis to use?" ViewCount="24" />
  
  <row Body="&lt;p&gt;I tried finding a proof without considering characteristic functions. &lt;a href=&quot;http://en.wikipedia.org/wiki/Kurtosis#Pearson_moments&quot; rel=&quot;nofollow&quot;&gt;Excess kurtosis&lt;/a&gt; does the trick. Here's the two-line answer: $\text{Kurt}(U) = \text{Kurt}(X + Y) = \text{Kurt}(X) / 2$ since $X$ and $Y$ are iid. Then  $\text{Kurt}(U) = -1.2$ implies $\text{Kurt}(X) = -2.4$ which is a contradiction as $\text{Kurt}(X) \geq -2$ for any random variable. &lt;/p&gt;&#10;&#10;&lt;p&gt;Rather more interesting is the line of reasoning that got me to that point. $X$ (and $Y$) must be bounded between 0 and 0.5 - that much is obvious, but helpfully means that its moments and central moments exist. Let's start by considering the mean and variance: $\mathbb{E}(U)=0.5$ and $\text{Var}(U)=\frac{1}{12}$. If $X$ and $Y$ are identically distributed then we have:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\mathbb{E}(X + Y) = \mathbb{E}(X) +  \mathbb{E}(Y) = 2  \mathbb{E}(X)= 0.5$$&lt;/p&gt;&#10;&#10;&lt;p&gt;So $\mathbb{E}(X) = 0.25$. For the variance we additionally need to use independence to apply: &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\text{Var}(X+Y) = \text{Var}(X) + \text{Var}(Y) = 2 \text{Var}(X) = \frac{1}{12}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Hence $\text{Var}(X) = \frac{1}{24}$ and $\sigma_X = \frac{1}{2\sqrt{6}} \approx 0.204$. Wow! That is a lot of variation for a random variable whose support ranges from 0 to 0.5. But we should have expected that, since the standard deviation isn't going to scale in the same way that the mean did.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, what's the &lt;em&gt;largest&lt;/em&gt; standard deviation that a random variable can have if the smallest value it can take is 0, the largest value it can take is 0.5, and the mean is 0.25? Collecting all the probability at two point masses on the extremes, 0.25 away from the mean, would clearly give a standard deviation of 0.25. So our $\sigma_X$ is large but not impossible. (I hoped to show that this implied too much probability lay in the tails for $X + Y$ to be uniform, but I couldn't get anywhere with that on the back of an envelope.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Second moment considerations &lt;em&gt;almost&lt;/em&gt; put an impossible constraint on $X$ so let's consider higher moments. What about &lt;a href=&quot;http://en.wikipedia.org/wiki/Skewness#Pearson.27s_moment_coefficient_of_skewness&quot; rel=&quot;nofollow&quot;&gt;Pearson's moment coefficient of skewness&lt;/a&gt;, $\gamma_1 = \frac{\mathbb{E}(X - \mu_X)^3}{\sigma_X^3} = \frac{\kappa_3}{\kappa_2^{3/2}}$? This exists since the central moments exist and $\sigma_X \neq 0$. It is helpful to know some properties of the cumulants, in particular applying independence and then identical distribution gives:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\kappa_i(U) = \kappa_i(X + Y) = \kappa_i(X) + \kappa_i(Y) = 2\kappa_i(X)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;This &lt;a href=&quot;http://en.wikipedia.org/wiki/Cumulant#Additivity&quot; rel=&quot;nofollow&quot;&gt;additivity property&lt;/a&gt; is precisely the generalisation of how we dealt with the mean and variance above - indeed, the first and second cumulants are just $\kappa_1 = \mu$ and $\kappa_2 = \sigma^2$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then $\kappa_3(U) = 2\kappa_3(X)$ and $\big(\kappa_2(U)\big)^{3/2} = \big(2\kappa_2(X)\big)^{3/2} = 2^{3/2} \big(\kappa_2(X)\big)^{3/2}$. The fraction for $\gamma_1$ cancels to yield $\text{Skew}(U) = \text{Skew}(X + Y) = \text{Skew}(X) / \sqrt{2}$. Since the uniform distribution has zero skewness, so does $X$, but I can't see how a contradiction arises from this restriction. &lt;/p&gt;&#10;&#10;&lt;p&gt;So instead, let's try the excess kurtosis, $\gamma_2 = \frac{\kappa_4}{\kappa_2^2} = \frac{\mathbb{E}(X - \mu_X)^4}{\sigma_X^4} - 3$. By a similar argument (this question is self-study, so try it!), we can show this exists and obeys:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\text{Kurt}(U) = \text{Kurt}(X + Y) = \text{Kurt}(X) / 2$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The uniform distribution has excess kurtosis $-1.2$ so we require $X$ to have excess kurtosis $-2.4$. But the smallest possible excess kurtosis is $-2$, which is achieved by the $\text{Binomial}(1, \frac{1}{2})$ Bernoulli distribution.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-11-25T01:17:57.463" Id="125368" LastActivityDate="2014-11-25T13:22:02.567" LastEditDate="2014-11-25T13:22:02.567" LastEditorUserId="22228" OwnerUserId="22228" ParentId="125360" PostTypeId="2" Score="6" />
  <row AnswerCount="2" Body="&lt;p&gt;I have two questions about the covariance structure in SAS (proc mixed).&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;I realize the compound symmetry structure in SAS allows the covariance term to be negative.  This is different from the literature where the covaraince is always positive so it is equivalent to a random intercept model. Why SAS defines it differently? I can hardly imagine a situation where the repeated measures are mutually negative correlated. Is this even possible?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;I don't understand the default variance components structure. see &lt;a href=&quot;http://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_mixed_sect019.htm&quot; rel=&quot;nofollow&quot;&gt;SAS documentation&lt;/a&gt;.  Why the first two variances equal to $\sigma^2_B$ and the second two equal to $\sigma^2_{AB}$. What do the subscript B and AB represent?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-25T01:23:04.227" Id="125369" LastActivityDate="2014-11-25T07:39:50.593" OwnerUserId="37051" PostTypeId="1" Score="0" Tags="&lt;mixed-model&gt;&lt;covariance&gt;&lt;sas&gt;" Title="different definition of compound symmetry in SAS" ViewCount="47" />
  <row AcceptedAnswerId="125390" AnswerCount="1" Body="&lt;p&gt;Let X = [11.17, 9.52, 10.69, 9.84, 10.84, 9.88, 10.28, 12.23, 8.49, 10.79] be a normally distributed data with mean = 10.37 and standard deviation = 1.02. How to determine the corresponding value to +1.02 and -1.02 standard deviations, which covers 69% area of the probability distribution. &lt;/p&gt;&#10;" ClosedDate="2014-11-25T09:49:01.203" CommentCount="8" CreationDate="2014-11-25T03:58:50.610" Id="125383" LastActivityDate="2014-11-25T06:42:39.963" LastEditDate="2014-11-25T06:42:39.963" LastEditorUserId="7224" OwnerUserId="36305" PostTypeId="1" Score="2" Tags="&lt;normal-distribution&gt;&lt;standard-deviation&gt;" Title="How to derive quantiles of a non-standard normal distribution" ViewCount="96" />
  <row Body="&lt;p&gt;My two cents (more familiar with R than SAS); compound symmetry with $\rho&amp;lt;0$ is useful in cases where there is some compensation within groups, e.g. individuals who do worse than average on some machines do better than average on others, or some resource is being divided among individuals in a group.  Technically, it's useful for cases where the among-group variance would otherwise be estimated as negative ... &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-25T04:38:09.820" Id="125386" LastActivityDate="2014-11-25T04:38:09.820" OwnerUserId="2126" ParentId="125369" PostTypeId="2" Score="0" />
  
  
  <row AcceptedAnswerId="125400" AnswerCount="1" Body="&lt;p&gt;I'm having trouble with this problem: There is 1 nickel, 2 dimes, and 3 quarters in a cup. You pick 3, one at a time, without replacement. What is the probability of getting a nickel on the first pick? What is the probability of getting a nickel on the second pick? What is the probability of getting a nickel on the third pick? &lt;/p&gt;&#10;&#10;&lt;p&gt;My attempt: &#10;1+2+3=6&#10;nickel on first pick: 1/6&#10;nickel on second pick: 1/5&#10;nickel on third pick: 1/4&lt;/p&gt;&#10;&#10;&lt;p&gt;However the back of the book says each pick has a 20/120=1/6 chance of being a nickel. How is this possible since the sample space decreases by 1 each pick because there is no replacement. I realize that 6*5*4=120 and that there are 20/120 outcomes with nickels. I also realize that if a nickel is chosen for for one of the three spots that it cannot be chosen for the other 2 spots but I still am not understanding. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-11-25T02:35:56.920" Id="125399" LastActivityDate="2014-11-25T08:07:30.670" OwnerDisplayName="Joey Platt" OwnerUserId="61435" PostTypeId="1" Score="0" Tags="&lt;probability&gt;" Title="Picking 3 coins from 6 coins without replacement. What is probability of getting a nickel on a certain pick?" ViewCount="56" />
  <row AnswerCount="0" Body="&lt;p&gt;I have estimated a VAR (vector autoregressive) model on credit growth in STATA. I want to test its predictive power by comparing its estimated credit growth to observed credit growth (correlation coefficient). I did the same with the estimated credit growth from an autoregressive model with only lagged dependent variables, and compare the correlation coefficients. The model with the highest correlation with observed credit growth has the highest predictive power. Does this make sense to do? This will be an &quot;in-sample&quot;-analysis.&lt;/p&gt;&#10;&#10;&lt;p&gt;And do I make the basic autoregressive model on only lagged dependent variables the same way as I made the VAR model? That means that I only include those lagged dependent variables that are jointly significant.&lt;/p&gt;&#10;&#10;&lt;p&gt;I also want to do an &quot;out-of-sample&quot; analysis. I've done it by using the forecasting function in STATA. The correlation between the forecast credit growth in the autoregressive model on only lagged dependent variables and observed credit growth is around 0,10. The VAR-model I have made with several explanatory variables has a much higher correlation between the forecast and observed credit growth (0,75). Does this seem unrealistic, meaning that I have done something wrong? Or does it only mean that the predictive power in my VAR-model is good?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-25T09:19:36.123" Id="125406" LastActivityDate="2014-11-25T09:51:39.810" LastEditDate="2014-11-25T09:51:39.810" LastEditorUserId="17230" OwnerUserId="61454" PostTypeId="1" Score="0" Tags="&lt;predictive-models&gt;&lt;prediction&gt;&lt;autoregressive&gt;" Title="Autoregressive model - predictive power" ViewCount="23" />
  
  
  
  <row Body="&lt;p&gt;This is simpler than the above answers make it.  The sample mean in a complete and sufficient statistic (when the variance is known, but our results do not depend on the variance, hence will be valid also in the situatioin that the variance is unknown.  Then the Rao-Blackwell together with the Lehmann-Scheffe theorem (see wikipedia ...) will imply that the conditional expectation of the median, given the arithmetic mean, is the unique minimum variance unbiased estimator of the expectation $\mu$.  But we know that is the arithmetic mean, hence the result follows.  \&lt;/p&gt;&#10;&#10;&lt;p&gt;We did also use that the median is an unbiased estimator, which follows from symmetry.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-11-25T12:32:00.423" Id="125430" LastActivityDate="2014-11-25T12:32:00.423" OwnerUserId="11887" ParentId="83840" PostTypeId="2" Score="2" />
  <row AnswerCount="0" Body="&lt;p&gt;Currently, I happen to analyze data where I do have a data matrix for two time points each.&lt;br/&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Each of these matrices is of dimension nxm, where n is the number of samples and m is the number of variables to fit. Now for each of these m rows/variables I want to fit a model with which I can predict from all m variables in the first time point the data for the variable of interest in the second time point.&lt;/p&gt;&#10;&#10;&lt;p&gt;Though this appears to work quite well, I learn two things: &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;First for variables where barely anything changes from time point 1 to time point 2, I get a good prediction when using 10-fold CV and Pearson's r for predicting the ABSOLUTE value (the model is mostly then just relying on the same variable at time point 1), but I do get out a poor Pearson's r when trying to predict the CHANGE of the particular variable from time point 1 to time point 2 (since almost all changes are in the measurement error). &lt;/li&gt;&#10;&lt;li&gt;On the other hand, I do have variables where there are heavy changes. Then I get out a comparatively week Pearson's r value for the ABSOLUTE value, but a quite good one for the CHANGES.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;In order to highlight the use of my method I somehow need to attribute that I do have this bimodal behaviour of the variables when passing from time point 1 to time point 2. Is there a commonly accepted method for this? Or would you proceed differently?&lt;/p&gt;&#10;&#10;&lt;p&gt;I am looking forward to hear your opinions!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-25T08:53:13.083" Id="125454" LastActivityDate="2014-11-25T16:16:35.503" OwnerDisplayName="tobias" PostTypeId="1" Score="0" Tags="&lt;r&gt;" Title="Determing goodness of fit at data with few and sometimes heavy change" ViewCount="10" />
  
  <row Body="&lt;p&gt;Suppose your two strings are first and last names.  Your data looks like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;label, firstname, lastname&#10;1, 'Tom', 'Jones'&#10;0, 'Bob', 'Dylan'&#10;1, 'Bob', 'Jones'&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Your bigram—or interaction term &lt;code&gt;firstname * lastname&lt;/code&gt;—would be&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;label, ..., fullname&#10;1, ..., 'Tom Jones'&#10;0, ..., 'Bob Dylan'&#10;1, ..., 'Bob Jones'&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now, when you run your classification algorithm, your data will need to take this form (usually the data format is automatically converted to this in your program, such as R):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;label, Tom, Jones, Bob, Dylan, TomJones, BobDylan, BobJones&#10;1, 1, 1, 0, 0, 1, 0, 0&#10;0, 0, 0, 1, 1, 0, 1, 0&#10;1, 0, 1, 1, 0, 0, 0, 1&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Since we have more than one &quot;Bob&quot; (i.e. both &quot;Bob Dylan&quot; and &quot;Bob Jones&quot;), then the variable &quot;Bob&quot; is not perfectly correlated with &quot;Bob Dylan&quot;.  Therefore, the classification algorithm is able to use the variation of &quot;Bob&quot;, and &quot;Bob Dylan&quot; to predict the label.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, &quot;Tom&quot; is perfectly correlated with &quot;Tom Jones&quot;, since there are other people who have the first name &quot;Tom&quot; in the data.  Therefore, the classification algorithm cannot use both &quot;Tom&quot;, and &quot;Tom Jones&quot;.  Instead, it can only use one or the other.&lt;/p&gt;&#10;&#10;&lt;p&gt;Since you'd like to use all the available information you have (i.e. firstname, lastname &lt;em&gt;and&lt;/em&gt; fullname), but you don't know which firstnames/lastnames are perfectly correlated with fullname, then you should use a ridge regression because this type of regression can deal with perfectly correlated independent variables.  Example code (in R) looks like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(glmnet)&#10;df &amp;lt;- read.csv(...)&#10;X &amp;lt;- sparse.model.matrix(~firstname + lastname + firstname*lastname, df)&#10;y &amp;lt;- df$label&#10;fit &amp;lt;- cv.glmnet(X, y, family='binomial', alpha=0)&#10;predictions &amp;lt;- predict(fit, X, type='response') &#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="4" CreationDate="2014-11-25T16:43:56.097" Id="125460" LastActivityDate="2014-11-25T16:43:56.097" OwnerUserId="41276" ParentId="125427" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;I think your derivation is messed up. Take a look at this &lt;a href=&quot;http://en.wikipedia.org/wiki/Product_distribution#Derivation_for_independent_random_variables&quot; rel=&quot;nofollow&quot;&gt;Wiki&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's MATLAB implementation:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; f1=@(x,z)normpdf(x).*normpdf(z./x)./abs(x)&#10; f=@(z)integral(@(x)f1(x,z),-10,10)&#10; &amp;gt;&amp;gt; f(1)&#10; ans =&#10; 0.1340&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Here's how you could simulate it:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; samp=prod(randn(2,100000));&#10; hist(samp,100);&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Use distribution fitting tool to fit nonparametric distribution to compare to theoretical distribution:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; dfittool(samp')&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/9HsGu.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-25T16:54:15.220" Id="125461" LastActivityDate="2014-11-25T16:54:15.220" OwnerUserId="36041" ParentId="109527" PostTypeId="2" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;I have data with continuous frequency and a continuous support which I'd like to analyze and plot using a kernel density estimate in R. I can create the basic plot in JMP fairly easily using the Distribution platform making using of the Freq option as shown below. It generates a histogram from which I can quickly overlap a density estimate. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/0uzmU.png&quot; alt=&quot;JMP&amp;#39;s handy distribution form&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;What's the equivalent of this &lt;code&gt;Freq&lt;/code&gt; option in R?&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Read on if you are interested in why I'm trying to do this. I am aiming to add vertical lines at the specific local maximums in the density, and that isn't so easy to do with JMP in an automated fashion. Plus, I'd like to improve my R knowledge. Below is a plot that &lt;strong&gt;very&lt;/strong&gt; roughly captures what I'm trying to get to with this data. The curve is a kernel density estimate. I don't care about the shadowgram, but I couldn't get JMP to make the plot without either a histogram or a shadowgram visible. I couldn't find any tutorials showing how to do anything like this in R. Could you point me in the right direction?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/vbNGL.png&quot; alt=&quot;density plot with a random shadowgram thrown in&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is the data set I used to make the plot above.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x   y&#10;0   0.5314379598&#10;0.1 1.3521410475&#10;0.2 0.0449186138&#10;0.3 0.4225592288&#10;0.4 0.0496982985&#10;0.5 1.1107962989&#10;0.6 0.2438757387&#10;0.7 0.7023629225&#10;0.8 1.7466573415&#10;0.9 0.7134595708&#10;1   1.1313968897&#10;1.1 0.6055951286&#10;1.2 0.2924553991&#10;1.3 0.2913429266&#10;1.4 0.3221193803&#10;1.5 0.4683161595&#10;1.6 0.682958405&#10;1.7 0.1082714352&#10;1.8 0.0489443405&#10;1.9 0.3309649262&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="6" CreationDate="2014-11-25T17:08:29.707" Id="125464" LastActivityDate="2014-12-02T04:42:03.383" LastEditDate="2014-12-02T04:42:03.383" LastEditorUserId="8097" OwnerUserId="8097" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;kernel-density-estimate&gt;&lt;jmp&gt;" Title="density estimate for continuous support with continuous frequency" ViewCount="34" />
  
  
  <row Body="&lt;p&gt;I would not use either chi-squared tests or McNemar's tests.  The chi-squared does not take the dependency in the data into account, and neither incorporates information about both items into a proper, unified analysis.  Instead, I would use a generalized linear mixed effects model (GLMM).  You will have a logistic regression model with fixed effects for group and item, and a group X item interaction.  In addition, you will have random effects for the subjects to take their dependence into account.  At a minimum, you should have a random intercept; you may want to have a random slope as well, I'm not sure.  (If the model will fit, it probably wouldn't hurt.)  The test of the interaction is the critical test of your hypothesis: if it is significant, then the increases in the rates significantly differ.  The directionality of that estimate (i.e., a positive vs. negative coefficient) will tell you which is a larger decrement, but it will depend on which levels are taken as the reference levels of your variables.  At any rate, which is larger is clear enough from your summary statistics, all you need is a test of its significance.  One last thing to bear in mind is that you phrase your question in terms of raw risk differences, whereas the model will test odds ratios, you will need to be clear on that.  &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-11-25T18:42:21.560" Id="125474" LastActivityDate="2014-11-25T18:42:21.560" OwnerUserId="7290" ParentId="125260" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="125539" AnswerCount="1" Body="&lt;p&gt;I have proven that: &#10;$X⊥Y|Z\ {\rm iff}\ p(x,y|z)=p(x|z)p(y|z)$ for all $x,y,z$ such that $p(z)&amp;gt;0$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The next question is to prove an alternative definition:&#10;$X⊥Y|Z$ iff there exist functions $g$ and $h$ such that&#10;$p(x,y|z)=g(x,z)h(y,z)$ for all $x,y,z$ such that $p(z)&amp;gt;0$.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm thinking that I need to integrate the function somehow...? &lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-11-25T19:27:09.160" Id="125479" LastActivityDate="2014-11-26T06:49:07.403" LastEditDate="2014-11-26T06:49:07.403" LastEditorUserId="7224" OwnerUserId="61489" PostTypeId="1" Score="5" Tags="&lt;machine-learning&gt;&lt;self-study&gt;&lt;conditional-probability&gt;&lt;independence&gt;&lt;integral&gt;" Title="Conditional independence iff joint factorizes" ViewCount="139" />
  <row AnswerCount="0" Body="&lt;p&gt;Assume I have a dataset of $N$ observations $ \mathbf{x} =\{x_1, x_2, ... x_N\}$ where $\{x_i \in \mathbb{R} | 0 \leq x_i \leq 1\}$ and I want to find out how they are distributed.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is it possible to do it e.g. with a combination of gaussian distributions and normalizing the result like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;$f(\mathbf{x})=\frac{1}{N}\sum_{i=1}^N \mathcal{N}(x_i,\mu)$ , where $\mu$ is considered to be &quot;&lt;em&gt;very&lt;/em&gt;&quot; small for each $x_i$.&lt;/p&gt;&#10;&#10;&lt;p&gt;or is this wrong and/or there are better methods I could/should consider?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-11-25T21:36:08.510" Id="125493" LastActivityDate="2014-11-25T21:36:08.510" OwnerUserId="61222" PostTypeId="1" Score="1" Tags="&lt;distributions&gt;&lt;modeling&gt;" Title="Find distribution for a dataset $\mathbf{x} = \{x_1, x_2, ... x_N\} \in \mathbb{R}$" ViewCount="15" />
  <row Body="&lt;p&gt;To answer my own question:&#10;We simulated above system with some chosen parameters m and s (snssde function).&#10;From simulated data we estimated m and s, first with fitsde function and then with above method. Above method works correctly, and its estimates are closer to true values than estimates from fitsde. Besides, in fitsde the Kessler method is the only one (out of four) that gives decent estimates.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-25T21:51:16.143" Id="125495" LastActivityDate="2014-11-25T21:51:16.143" OwnerUserId="59291" ParentId="125418" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;John Fox's &lt;a href=&quot;http://cran.r-project.org/doc/contrib/Fox-Companion/appendix-bootstrapping.pdf&quot; rel=&quot;nofollow&quot;&gt;Bootstrapping Regression Models&lt;/a&gt; is a great resource to get started as it provides lots of examples. &lt;/p&gt;&#10;&#10;&lt;p&gt;To answer your question directly, you'll simply need to access the &lt;code&gt;t0&lt;/code&gt; and &lt;code&gt;t&lt;/code&gt; values provided by the &lt;code&gt;boot&lt;/code&gt; object. To wit, &lt;code&gt;boot$t0&lt;/code&gt; is a numeric (of length $M+1$ where $M$ is the number of covariates) with the coefficients returned by running the regression with the original data and &lt;code&gt;boot$t&lt;/code&gt; is a a matrix of dimension $R \times (M+1)$ holding the coefficients returned from a regression run on a bootstrapped sample of the data in each row.&lt;/p&gt;&#10;&#10;&lt;p&gt;Therefore, in your example, the one-sided p-values can be calculated pretty simply as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;R &amp;lt;- 1e3 # number of bootstraps&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;extrema &amp;lt;- apply(X=results$t,&#10;  MARGIN=1,&#10;  FUN=&quot;&amp;gt;&quot;,&#10;  results$t0)&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;pvals &amp;lt;- rowSums(extrema)/(R+1)&lt;/code&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-25T21:55:20.633" Id="125496" LastActivityDate="2014-11-26T05:27:33.703" LastEditDate="2014-11-26T05:27:33.703" LastEditorUserId="3640" OwnerUserId="3640" ParentId="125432" PostTypeId="2" Score="1" />
  
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;I am running a regression where my dependent variable is a cross-section of variances. Therefore, I require my predicted values (fitted values) to be positive.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Then don't fit a model that doesn't obey such an obvious requirement...&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;However, when running a simple OLS regression,&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;... like, you know, OLS.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Please note that approximately, my dependent variable is distributed according to a Chi-square distribution.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Or rather, since population variances are usually not $1$, it should probably be approximately $\sigma^2$ times a chi-square -- so why not model it as, say a Gamma random variable (the distribution of a multiple of a chi-square)?&lt;/p&gt;&#10;&#10;&lt;p&gt;So why not use a GLM for this problem? All your fitted values are guaranteed to not go negative. See the example &lt;a href=&quot;http://stats.stackexchange.com/a/102753/805&quot;&gt;here&lt;/a&gt; (however, if you fit a straight line model, predicted values can - indeed, must - still go negative outside the data).&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Is there a way to impose a lower bound on the predicted values?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;If you fit a model for the mean such that the mean will remain positive (log-link, say, rather than identity-link) then out-of-sample predictions will obey the positivity restriction.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you're modelling variances, the identity link usually won't make sense anyway. Choose one of the others, and the model - fitted and predicted - will stay positive.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-11-26T00:49:12.493" Id="125510" LastActivityDate="2014-11-26T21:47:17.907" LastEditDate="2014-11-26T21:47:17.907" LastEditorUserId="805" OwnerUserId="805" ParentId="125487" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="125520" AnswerCount="1" Body="&lt;p&gt;Given a regression setting with covariates $X_{n \times m}$ and response $Y_{n \times p}$ where $p&amp;gt;1$, i.e the responses are vector-valued or multivariate, is there a Nadaraya-Watson estimator for kernel regression in this setting?&lt;/p&gt;&#10;&#10;&lt;p&gt;This boils down to how the following can be computed with this form of $Y$ :&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac{\sum_{i=1}^{n}K_h(x-x_i)y_i}{\sum_{i=1}^{n}K_h(x-x_i)}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;But since above, $y_i$ is now multivariate as well, what happens to this multiplication operation in the numerator, in this generalization to multivariate responses? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-26T01:42:44.263" Id="125517" LastActivityDate="2014-11-26T08:42:59.470" LastEditDate="2014-11-26T01:47:55.153" LastEditorUserId="6897" OwnerUserId="6897" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;machine-learning&gt;&lt;kernel-density-estimate&gt;&lt;nadaraya-watson&gt;&lt;kernel-regression&gt;" Title="What is Nadaraya-Watson Kernel Regression Estimator for Multivariate Response?" ViewCount="82" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I have two variables which may have a nonlinear relationship, according to the descriptive statistics.&lt;/p&gt;&#10;&#10;&lt;p&gt;The independent variable has a positive and significant coefficient in the OLS regression. When a square term is included in the model this becomes negative and insignificant while the square term becomes positive and  significant. &lt;/p&gt;&#10;&#10;&lt;p&gt;Can I argue that these variables have a nonlinear relationship? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-26T05:01:09.740" Id="125532" LastActivityDate="2014-11-28T12:56:39.487" LastEditDate="2014-11-28T12:56:39.487" LastEditorUserId="22047" OwnerUserId="61448" PostTypeId="1" Score="0" Tags="&lt;linear&gt;" Title="Nonlinear relationship interpretation" ViewCount="63" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a database with a binary response variable and 100 predictors (correlated and uncorrelated).&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to try the machine learning techniques in R I've been reading about in the last 3 weeks (ridge, lasso, decision tree, boosting, random forest, ANN, SVM, GP).&lt;/p&gt;&#10;&#10;&lt;p&gt;My problem is calibrating the 2 classes from the response variable. In &lt;code&gt;96%&lt;/code&gt; of the cases &lt;code&gt;Y=0&lt;/code&gt;. Because of this, I get really good predictions for this class and very poor for the other I'm more interested in. &lt;/p&gt;&#10;&#10;&lt;p&gt;I saw that each one of these methods has a  &lt;code&gt;&quot;weights&quot;&lt;/code&gt;option that I'm guessing might 'lift up' the importance of &lt;code&gt;Y=1&lt;/code&gt;, but I'm not sure. I have tried to include it in my analysis but I always get an error regarding the size of this vector.&lt;/p&gt;&#10;&#10;&lt;p&gt;Do you know if this option could help me achieve my goal? And if so, what size/form should it take? (defined as vector/as data frame /how many columns/ how many rows/...)&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-11-26T13:56:48.717" Id="125572" LastActivityDate="2014-11-26T19:06:51.553" LastEditDate="2014-11-26T19:06:51.553" LastEditorUserId="7290" OwnerUserId="61529" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;machine-learning&gt;&lt;classification&gt;&lt;weighted-regression&gt;&lt;rare-events&gt;" Title="R: &quot;weights&quot; option will help calibrating class inequality?" ViewCount="18" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Dummy_variable_%28statistics%29&quot;&gt;&quot;Dummy variable&quot; and &quot;indicator variable&quot;&lt;/a&gt; are labels frequently used terms to describe membership in a category with 0/1 coding; usually 0: Not a member of category, 1: Member of category.&lt;/p&gt;&#10;&#10;&lt;p&gt;On 11/26/2014 a quick search on scholar.google.com (with enclosing quotes) reveals &quot;dummy variable&quot; is used in about 318,000 articles, and &quot;indicator variable&quot; is used in about 112,000 articles. The term &quot;dummy variable&quot; also has a meaning in non-statistical mathematics of &quot;&lt;a href=&quot;https://en.wikipedia.org/wiki/Free_variables_and_bound_variables&quot;&gt;bound variable&lt;/a&gt;&quot; which is likely contributing to the greater use of &quot;dummy variable&quot; in indexed articles.&lt;/p&gt;&#10;&#10;&lt;p&gt;My topically-linked questions:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Are these terms always synonymous (within statistics)?&lt;/li&gt;&#10;&lt;li&gt;Are either of these terms ever acceptably applied to other forms of categorical coding (e.g. &lt;a href=&quot;http://www.ats.ucla.edu/stat/mult_pkg/faq/general/effect.htm&quot;&gt;effect coding&lt;/a&gt;, &lt;a href=&quot;http://www.ats.ucla.edu/stat/sas/webbooks/reg/chapter5/sasreg5.htm&quot;&gt;Helmert coding, etc.&lt;/a&gt;)?&lt;/li&gt;&#10;&lt;li&gt;What statistical or disciplinary reasons are there to prefer one term over the other?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="7" CreationDate="2014-11-26T18:09:39.437" Id="125608" LastActivityDate="2015-01-24T20:50:12.413" LastEditDate="2014-11-27T13:29:33.607" LastEditorUserId="17230" OwnerUserId="44269" PostTypeId="1" Score="8" Tags="&lt;categorical-data&gt;&lt;terminology&gt;&lt;indicator-variables&gt;" Title="&quot;Dummy variable&quot; versus &quot;indicator variable&quot; for nominal/categorical data" ViewCount="196" />
  <row AnswerCount="1" Body="&lt;p&gt;What does it mean X.Intercept equal to NA as a result of glm summary ? Thanks.&lt;/p&gt;&#10;&#10;&lt;p&gt;Coefficients: (1 not defined because of singularities)&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;            Estimate Std. Error z value Pr(&amp;gt;|z|)    &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;(Intercept)     1.98e+02   4.72e+01    4.18  2.9e-05 ***&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;X.Intercept.          NA         NA      NA       NA&lt;/strong&gt;    &lt;/p&gt;&#10;" ClosedDate="2014-11-26T19:37:39.350" CommentCount="0" CreationDate="2014-11-26T19:01:56.473" Id="125618" LastActivityDate="2014-11-26T19:31:21.117" OwnerUserId="49691" PostTypeId="1" Score="0" Tags="&lt;generalized-linear-model&gt;&lt;missing-data&gt;" Title="GLM - X.intercept equal to NA" ViewCount="9" />
  <row AnswerCount="0" Body="&lt;p&gt;Passage verbatim:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;According to a review by Silverman et al. (2010), the incidence of&#10;  autistic disorder is 0.6–1.0 percent in the population. &lt;strong&gt;The disorder&#10;  is four times more common in males than in females. However, if only&#10;  cases of autism with mental retardation are considered, the ratio&#10;  falls to 2:1, and if only cases of high-functioning autism are&#10;  considered (those with average or above-average intelligence and&#10;  reasonably good communicative ability), the ratio rises to&#10;  approximately 7:1 (Fombonne, 2005)&lt;/strong&gt;. &lt;em&gt;These data suggest that the&#10;  social impairments are much more common in males but the cognitive and&#10;  communicative impairments are more evenly shared by males and&#10;  females&lt;/em&gt;.  &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;What I extract from the bolded text:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Out of every 5 persons with autism 1 will be female and 4 will be male, or that 80% of autistic individuals are male.&lt;/p&gt;&#10;&#10;&lt;p&gt;Out of every 3 persons who have autism and are also mentally retarded 1 will be female and 2 will be male, or that 67% of autistic individuals with mental retardation are male.&lt;/p&gt;&#10;&#10;&lt;p&gt;Out of every 8 persons who have autism and are also high-funtioning 1 will be female and 7 will be male, or that 88% of of high-funtioning autistic individuals are male.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;My issue with the italicized text:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;It states that &quot;communicative impairments are more evenly shared by males and females,&quot; but the ratio of high-funtioning autism is 7:1. Wouldn't this suggest the opposite?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Are my numbers correct, and am I reading the italicized part correctly?&lt;/strong&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-11-26T19:14:56.383" Id="125622" LastActivityDate="2014-11-26T19:14:56.383" OwnerUserId="61556" PostTypeId="1" Score="0" Tags="&lt;proportion&gt;&lt;ratio&gt;" Title="Need clarification on a passage. (Autism Prevalence)" ViewCount="23" />
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;This looks like a classical problem, but I couldn't find an answer, maybe because I use wrong keywords in my search. &#10;Let's assume you are tracking company monthly revenue values over a couple of years. These values have some seasonality effect (inter-month variability) and year-to-year variability (intra-month variability). How would you quantify the variability of monthly revenue values? &lt;/p&gt;&#10;&#10;&lt;p&gt;An illustrative example:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/xBtiI.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-27T09:56:00.090" Id="125687" LastActivityDate="2014-11-27T09:56:00.090" OwnerUserId="3184" PostTypeId="1" Score="0" Tags="&lt;anova&gt;&lt;variance&gt;" Title="Assessing variability in a multiilevel model" ViewCount="15" />
  <row AnswerCount="2" Body="&lt;p&gt;I would like to compare some algorithms for performing sentiment classification (&lt;code&gt;Naive Bayes&lt;/code&gt;, &lt;code&gt;SVM&lt;/code&gt;, and &lt;code&gt;Random Forest&lt;/code&gt;).&#10;So far, I have collected about 100 000 unique opinions with the following distribution:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;10% negative&#10;90% positive&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;After some pre-processing (removing stop words, stemming etc) with &lt;code&gt;tm&lt;/code&gt; package I obtained a document-term-matrix with about 320,000 unique terms (100% sparsity). I have decided to narrow it down to 99,8% sparsity ending with about 1 400 terms.&lt;/p&gt;&#10;&#10;&lt;p&gt;So by now my data set has about 100 000 rows and 1 400 columns.&lt;/p&gt;&#10;&#10;&lt;p&gt;To apply Naive Bayes classificator (from &lt;code&gt;klaR&lt;/code&gt; package) I have changed each occurrence of term in document to a simple factor - YES or NO indicating if the term occurred in document or not.&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem is when I'm trying to perform 10-fold CV almost all of the created data sets have some terms with zero variance. As far I understand changing the Laplace correction factor should face this problem (but it didn't). How to approach this problem? What are recommended practices?&lt;/p&gt;&#10;&#10;&lt;p&gt;Solutions that comes to my mind:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Reduce data set size (for example 10 000 opinions in each class)&lt;/li&gt;&#10;&lt;li&gt;Stop-words can be represented as terms (lower probability of zero variance)&lt;/li&gt;&#10;&lt;li&gt;Use bootstraping instead of 10-fold CV (but then the algorithm complains about the duplicated rows)&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2014-11-27T10:11:51.547" Id="125688" LastActivityDate="2015-02-26T21:19:54.453" LastEditDate="2014-11-28T17:27:29.667" LastEditorUserId="55763" OwnerUserId="55763" PostTypeId="1" Score="2" Tags="&lt;classification&gt;&lt;text-mining&gt;&lt;naive-bayes&gt;&lt;caret&gt;&lt;sentiment-analysis&gt;" Title="How to prepare a dataset for text classification" ViewCount="83" />
  
  
  
  
  <row Body="&lt;p&gt;The error in age is $$1\times10^9\,\mathrm{yrs}-1\times10^8\,\mathrm{yrs}=9\times10^8\,\mathrm{yrs}$$&#10;The error in log age is $$ \log10^9\,\mathrm{yrs} - \log10^8\,\mathrm{yrs}= (9+\log\mathrm{yrs})-(8+\log\mathrm{yrs})=1$$, i.e. a multiplicative error in age of $10^1=10$. Both are correct: the estimated age of one thousand million years is ten times too high &lt;em&gt;and&lt;/em&gt; nine-hundred million years too high. For statistical modelling the question of interest is whether it's better to assume errors in age or errors in log age have the same distribution for stars of different ages.&lt;/p&gt;&#10;&#10;&lt;p&gt;What you're doing wrong is using a Taylor-series approximation for small errors when the error is large:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$e_{\log(\mathrm{age_r})} \approx \frac{e_\mathrm{age_r}}{\ln(10) \times \mathrm{age_r}} \quad\text{when $\mathrm{e_{age_r}}\ll \mathrm{age_r}$}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;(&amp;amp; you don't really need to use an approximation anyway).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-27T12:09:44.990" Id="125704" LastActivityDate="2014-11-27T13:17:47.240" LastEditDate="2014-11-27T13:17:47.240" LastEditorUserId="17230" OwnerUserId="17230" ParentId="125658" PostTypeId="2" Score="3" />
  
  
  
  
  
  <row Body="&lt;p&gt;&quot;Assume a spherical potato&quot; is probably a reasonable thing to do here.&lt;/p&gt;&#10;&#10;&lt;p&gt;As the blue part space is essentially random, you cannot represent it more efficiently than your list of 5000 spheres. What &lt;em&gt;would&lt;/em&gt; be possible is a spatial index that allows for an efficient search, so you don't need 5000*5000 comparisons.&lt;/p&gt;&#10;&#10;&lt;p&gt;My gut feeling would be to sort the red potato indices in all 3 dimensions. This gives us 3 lists. Now given a blue potato, search for its X coordinate in the red-X list, it's Y coordinate in the red-Y list and its Z coordinate in the red-Z list. As these three lists are essentially projections, all 3 lists of candidates will be mostly wrong. However, there's in all likelyhood going to be only a single common candidate on all 3 lists. &lt;/p&gt;&#10;&#10;&lt;p&gt;Finding numbers common to N lists is efficient if you first sort the lists. (This is basically just a matter of determining the union of N lists.) Start with &lt;code&gt;x&lt;/code&gt; = the smallest number of the first list. Do a binary search for &lt;code&gt;x&lt;/code&gt; on the second, third list etc until you don't find &lt;code&gt;x&lt;/code&gt;. Keep track of the points where you found &lt;code&gt;x&lt;/code&gt;. At that point, set x to the next number from the first list. As you have kept track of the positions in all lists where you found &lt;code&gt;x&lt;/code&gt; so far, you know you don't need to search before those points. This means the binary search ranges shrink fairly rapidly.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that there may be several candidates if one read potato can overlap more than one blue potato. &lt;/p&gt;&#10;&#10;&lt;p&gt;There's one slight challenge in the projection phase, and that's that the potatoes all have different sizes. You can solve this by using 3*2 lists, for red-min-X and red-max-X coordinates etc. This will obviously also produce 6 lists of candidates (again:indices), but the lists pairwise will have a lot of commonality. &lt;em&gt;Don't&lt;/em&gt; merge those pairs. The union of the min-X and min-Y candidate lists is a lot smaller than the union of the min-X and max-X candidate lists.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that you could also add non-orthogonal projections by sorting on X+Y, X+Y-Z etc, but for the simple sphere case with plenty of distance between them this probably does not add value. The list of candidate pairs will be small enough to calculate the overlap% for all of them, and accept that some false pairs turn out to have 0% overlap.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-11-27T15:57:06.260" Id="125726" LastActivityDate="2014-11-27T15:57:06.260" OwnerUserId="1287" ParentId="125722" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;It sounds like you're looking for the likelihood of the residual for the new observation, where the residuals are distributed $Normal(0,\sigma^2)$ and $\sigma$ is estimated by your model as $s^2$. I.e.:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ &#10;\hat{y} = X\beta \\&#10;r = y-\hat{y} \\&#10;p(r) = (2\pi s^2)^{-1/2} exp \left( -\frac{r^2}{2s} \right) $$&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-11-27T18:02:19.893" Id="125737" LastActivityDate="2014-11-27T18:08:20.503" LastEditDate="2014-11-27T18:08:20.503" LastEditorUserId="8451" OwnerUserId="8451" ParentId="125736" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="125747" AnswerCount="1" Body="&lt;p&gt;I have two models, one &lt;code&gt;lm(y ~ x1 + x2 + 0)&lt;/code&gt; which gives me a close to 0.90 something $R^2$ and another model &lt;code&gt;lm(y ~ x1 + x2)&lt;/code&gt; which gives me a close to 0.0003 ish $R^2$. Based on $R^2$ one might choose the first model. I have domain (prior) knowledge that a 0 value of $x_1$ and a 0 value of $x_2$ should produce a 0 value of $y$. So, is my explicit setting of 0 a fair thing? Based on this knowledge and the obtained value of $R^2$ can I go with the first model? &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Edit&lt;/em&gt;&lt;/strong&gt;: From the threads &lt;a href=&quot;http://stats.stackexchange.com/questions/26176/removal-of-statistically-significant-intercept-term-boosts-r2-in-linear-model&quot;&gt;Removal of statistically significant intercept term boosts $R^2$ in linear model&lt;/a&gt; and &lt;a href=&quot;http://stats.stackexchange.com/questions/7948/when-is-it-ok-to-remove-the-intercept-in-lm&quot;&gt;When is it ok to remove the intercept in lm()?&lt;/a&gt; I have learned that it is okay to do it as long as the domain knowledge above holds true. From &lt;a href=&quot;http://stats.stackexchange.com/questions/13314/is-r2-useful-or-dangerous&quot;&gt;Is $R^2$ useful or dangerous?&lt;/a&gt; I learned that $R^2$ alone is insufficient to make a decision about how good the model is. I need to look at AIC and BIC to make further decisions. However, because I my background is not in statistics, I am having trouble understanding the following in the first link:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Why does the first equality occur only when intercept is included?&lt;/li&gt;&#10;&lt;li&gt;why does $\bar{y}$ become 0 when intercept disappears?&lt;/li&gt;&#10;&lt;li&gt;Why in the second case, $R_0^2$ uses a reference model corresponding to &lt;strong&gt;noise&lt;/strong&gt; only?&lt;/li&gt;&#10;&lt;li&gt;What is the $\bar{y_1}$ in the notation?&lt;/li&gt;&#10;&lt;li&gt;Why is $\|\ y \|_2^2 = \| y - \bar{y_1}\|_2^2 + n \bar y^2$ true?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" ClosedDate="2014-11-27T18:44:06.717" CommentCount="2" CreationDate="2014-11-27T18:10:13.453" Id="125739" LastActivityDate="2014-11-28T22:15:46.793" LastEditDate="2014-11-28T22:15:46.793" LastEditorUserId="43500" OwnerUserId="43500" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;multiple-regression&gt;&lt;linear-model&gt;&lt;r-squared&gt;" Title="Which regression model to choose?" ViewCount="46" />
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have 20 individuals randomly distributed into two groups(treatment vs&#10;non-treatment) and test_score was measured before/after the treatment.&lt;/p&gt;&#10;&#10;&lt;p&gt;My central goal is to measure the effect of the treatment. &lt;/p&gt;&#10;&#10;&lt;p&gt;Now I realize I can model this in two ways:&lt;/p&gt;&#10;&#10;&lt;p&gt;1) post-test score as a response and pre-test and treatment as explanatory variables where treatment is an indicator like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;post-test-score ~ pre-test-score + treatment + treatment:pre-test-score&lt;/p&gt;&#10;&#10;&lt;p&gt;and measure the coefficient of treatment and test its significance.&lt;/p&gt;&#10;&#10;&lt;p&gt;OR&lt;/p&gt;&#10;&#10;&lt;p&gt;2) test score(both pre/post) as a response and pre/post &amp;amp; individuals&amp;amp; treatment as indicator variables&lt;/p&gt;&#10;&#10;&lt;p&gt;test_score ~ treatment + treatment:individual + treatment:pre_or_post&#10;(where individuals and pre_or_post are indicators where the first individual and pre are the baseline)&#10;and measure the coefficient of treatment:pre_or_post&lt;/p&gt;&#10;&#10;&lt;p&gt;How are these two methods different? and which one is better?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-27T21:00:06.940" Id="125772" LastActivityDate="2014-11-29T17:04:54.760" LastEditDate="2014-11-29T17:04:54.760" LastEditorUserId="11013" OwnerUserId="11013" PostTypeId="1" Score="0" Tags="&lt;generalized-linear-model&gt;&lt;repeated-measures&gt;&lt;modeling&gt;" Title="Two ways to model pre/post/treatment setting. Which one is preferred and why?" ViewCount="14" />
  <row AnswerCount="1" Body="&lt;p&gt;I am trying to understand what is word-level embedding. From what I found, it is a vector representation of a word which captures the context information in which the word is used. &lt;/p&gt;&#10;&#10;&lt;p&gt;I don't understand how this is represented in a vector and no simple example on-line at all.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can anyone explain or demonstrate how it works?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-27T21:08:27.020" Id="125773" LastActivityDate="2015-01-22T17:37:26.797" LastEditDate="2015-01-22T17:37:26.797" LastEditorUserId="919" OwnerUserId="61482" PostTypeId="1" Score="0" Tags="&lt;nlp&gt;" Title="What is a &quot;word-level embedding&quot;?" ViewCount="25" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a &lt;code&gt;coxme&lt;/code&gt; object:&#10;&lt;code&gt;(Age, status) ~ F + X1 + X2 + (1|R1) + (1|R2) + (1|R3)&lt;/code&gt;&#10;Where F is an individual's level of inbreeding, X1 &amp;amp; X2 are covariate factors, and R1, R2 &amp;amp; R3 are random factors denoting non-independence between individuals, and breeding facility. &lt;/p&gt;&#10;&#10;&lt;p&gt;I understand that the &lt;code&gt;coxme&lt;/code&gt; output gives me the % change in the rate of the event occurring due to inbreeding increase, but I'd like to get point estimates of survival to the 5th year of age for each inbreeding level from this model. With this, I'd like to calculate the effect of increasing inbreeding on survival to a specific point in time (giving us a rate which can be fed into population models for this species). &lt;/p&gt;&#10;&#10;&lt;p&gt;Seeing as there is no simple method to extract this data from a&lt;code&gt;coxme&lt;/code&gt; object (e.g., &lt;code&gt;survfit&lt;/code&gt;), is there a process by which I can extract it manually for specific points in time from my &lt;code&gt;coxme&lt;/code&gt; fit? &lt;/p&gt;&#10;&#10;&lt;p&gt;Alternatively, would it be more appropriate to extract this data from Kaplan Meier curves for each inbreeding strata of the model without the random factors (I don't feel like it would be accurate, but I'm not 100% sure)? &lt;/p&gt;&#10;&#10;&lt;p&gt;My cox models may not be appropriate to use in determining this rate, but it's something that I've worked with a lot so far and I'd like to know if it is at all possible. &lt;/p&gt;&#10;&#10;&lt;p&gt;Any discussion, comments, advice would be greatly appreciated, Thanks. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-27T21:20:51.090" Id="125775" LastActivityDate="2014-11-27T21:20:51.090" OwnerUserId="55793" PostTypeId="1" Score="0" Tags="&lt;survival&gt;&lt;cox-model&gt;&lt;kaplan-meier&gt;" Title="Is it possible to gain point survival estimates from a coxme object?" ViewCount="26" />
  
  
  <row Body="&lt;p&gt;Each flip of each coin is independent -- dime or penny, right or left pocket.  If you get heads on a flip of the penny, it's still equally probable that you'll get heads or tails flipping the dime or the penny again. So taken independently, p = 0.5 and d = 0.5.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, if you're trying to find the probability of a particular sequence of flip results, the probabilities are different, depending on how many flips and how many ways there are to get that particular sequence, but it still makes no difference which coin or pocket.  Due to their independence, you can't find s (unless one of the coins isn't fair).&lt;/p&gt;&#10;&#10;&lt;p&gt;That said, there are some studies showing interesting coin flip probabilities under certain conditions.  See &lt;a href=&quot;http://mathworld.wolfram.com/CoinTossing.html&quot; rel=&quot;nofollow&quot;&gt;Wolfram's Coin Tossing page&lt;/a&gt; for some of those.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-11-28T03:44:16.750" Id="125793" LastActivityDate="2014-11-28T03:44:16.750" OwnerUserId="30202" ParentId="125788" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;You typically plot a confusion matrix of your test set (recall and precision), and report an F1 score on them.&lt;br&gt;&#10;If you have your correct labels of your test set in &lt;code&gt;y_test&lt;/code&gt; and your predicted labels in &lt;code&gt;pred&lt;/code&gt;, then your F1 score is:&#10;&lt;code&gt;&#10;    from sklearn import metrics&#10;    # testing score&#10;    score = metrics.f1_score(y_test, pred, pos_label=list(set(y_test)))&#10;    # training score&#10;    score_train = metrics.f1_score(y_train, pred_train, pos_label=list(set(y_train)))&#10;&lt;/code&gt;&#10;These are the scores you likely want to plot.  &lt;/p&gt;&#10;&#10;&lt;p&gt;You can also use accuracy:&#10;&lt;code&gt;&#10;    pscore = metrics.accuracy_score(y_test, pred)&#10;    pscore_train = metrics.accuracy_score(y_train, pred_train)&#10;&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;However, you get more insight from a confusion matrix.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can plot a confusion matrix like so, assuming you have a full set of your labels in &lt;code&gt;categories&lt;/code&gt;:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;&#10;    import numpy as np, pylab as pl&#10;    # get overall accuracy and F1 score to print at top of plot&#10;    pscore = metrics.accuracy_score(y_test, pred)&#10;    score = metrics.f1_score(y_test, pred, pos_label=list(set(y_test)))&#10;    # get size of the full label set&#10;    dur = len(categories)&#10;    print &quot;Building testing confusion matrix...&quot;&#10;    # initialize score matrices&#10;    trueScores = np.zeros(shape=(dur,dur))&#10;    predScores = np.zeros(shape=(dur,dur))&#10;    # populate totals&#10;    for i in xrange(len(y_test)-1):&#10;      trueIdx = y_test[i]&#10;      predIdx = pred[i]&#10;      trueScores[trueIdx,trueIdx] += 1&#10;      predScores[trueIdx,predIdx] += 1&#10;    # create %-based results&#10;    trueSums = np.sum(trueScores,axis=0)&#10;    conf = np.zeros(shape=predScores.shape)&#10;    for i in xrange(len(predScores)):&#10;      for j in xrange(dur):&#10;        conf[i,j] = predScores[i,j] / trueSums[i]&#10;    # plot the confusion matrix&#10;    hq = pl.figure(figsize=(15,15));&#10;    aq = hq.add_subplot(1,1,1)&#10;    aq.set_aspect(1)&#10;    res = aq.imshow(conf,cmap=pl.get_cmap('Greens'),interpolation='nearest',vmin=-0.05,vmax=1.)&#10;    width = len(conf)&#10;    height = len(conf[0])&#10;    done = []&#10;    # label each grid cell with the misclassification rates&#10;    for w in xrange(width):&#10;      for h in xrange(height):&#10;          pval = conf[w][h]&#10;          c = 'k'&#10;          rais = w&#10;          if pval &amp;gt; 0.5: c = 'w'&#10;          if pval &amp;gt; 0.001:&#10;            if w == h:&#10;              aq.annotate(&quot;{0:1.1f}%\n{1:1.0f}/{2:1.0f}&quot;.format(pval*100.,predScores[w][h],trueSums[w]), xy=(h, w), &#10;                      horizontalalignment='center',&#10;                      verticalalignment='center',color=c,size=10)&#10;            else:&#10;              aq.annotate(&quot;{0:1.1f}%\n{1:1.0f}&quot;.format(pval*100.,predScores[w][h]), xy=(h, w), &#10;                      horizontalalignment='center',&#10;                      verticalalignment='center',color=c,size=10)&#10;    # label the axes&#10;    pl.xticks(range(width), categories[:width],rotation=90,size=10)&#10;    pl.yticks(range(height), categories[:height],size=10)&#10;    # add a title with the F1 score and accuracy&#10;    aq.set_title(lbl + &quot; Prediction, Test Set (f1: &quot;+&quot;{0:1.3f}&quot;.format(score)+', accuracy: '+'{0:2.1f}%'.format(100*pscore)+&quot;, &quot; + str(len(y_test)) + &quot; items)&quot;,fontname='Arial',size=10,color='k')&#10;    aq.set_ylabel(&quot;Actual&quot;,fontname='Arial',size=10,color='k')&#10;    aq.set_xlabel(&quot;Predicted&quot;,fontname='Arial',size=10,color='k')&#10;    pl.grid(b=True,axis='both')&#10;    # save it&#10;    pl.savefig(&quot;pred.conf.test.png&quot;)&#10;&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;and you end up with something like this (example from LiblinearSVC model), where you look for a darker green for better performance, and a solid diagonal for overall good performance.  Labels missing from the test set show as empty rows.  This also gives you a good visual of what labels are being misclassified as.  For example, take a look at the &quot;Music&quot; column.  You can see along the diagonal that 75.7% of the items that were predicted to be &quot;Music&quot; where actually &quot;Music&quot;.  Travel along the column and you can see what the other labels really were.  There was clearly some confusion with music-related labels, like &quot;Tuba&quot;, &quot;Viola&quot;, &quot;Violin&quot;, indicating that perhaps &quot;Music&quot; is too general to try and predict if we can be more specific.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/hDrrQ.jpg&quot; alt=&quot;Confusion Matrix example&quot;&gt;&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-11-28T04:26:33.283" Id="125797" LastActivityDate="2014-11-28T19:08:38.757" LastEditDate="2014-11-28T19:08:38.757" LastEditorUserId="30202" OwnerUserId="30202" ParentId="125756" PostTypeId="2" Score="2" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Can anyone please let me know how to prepare data to compute Superior predictive ability (SPA) test in R?&lt;/p&gt;&#10;&#10;&lt;p&gt;I am working on forecasting volatility in stock markets, the context is,&lt;/p&gt;&#10;&#10;&lt;p&gt;1) I used &quot;rugarch&quot; package to forecast the volatility using &quot;n&quot; autoregressive models.&lt;/p&gt;&#10;&#10;&lt;p&gt;3) Extracted &quot;sigma&quot; values from the forecasts of those &quot;n&quot; models and used it in a worksheet against realized volatility to calculate daily loss functions for each model, say, MSE, MAE, RMSE etc.&lt;/p&gt;&#10;&#10;&lt;p&gt;3) I want to compute the SPA test statistics using &quot;ttrTests&quot; package and find the SPA test p-values for a benchmark model against &quot;n&quot; alternatives. How to prepare data for this purpose?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks,&#10;Karthik&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-28T06:37:15.517" Id="125801" LastActivityDate="2014-11-28T06:37:15.517" OwnerUserId="61643" PostTypeId="1" Score="0" Tags="&lt;forecasting&gt;" Title="Preparing data for Superior predictive ability (SPA) test" ViewCount="16" />
  <row Body="&lt;p&gt;Consider an indirect evaluation of your features.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Train a (feature-selecting!) classificator like random forests on your &lt;em&gt;cluster labels&lt;/em&gt;&lt;/li&gt;&#10;&lt;li&gt;Validate that the classificator works well for this data set, otherwise try other classificators, parameters etc.&lt;/li&gt;&#10;&lt;li&gt;Inspect the classificator for the most important features.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;DBSCAN doesn't select features, and there is no obvious way to identify the features which have the most impact. However, random forests and decision trees can be used for this. So by training a classifier on the clusters, &lt;strong&gt;you can identify which features are most relevant for transferring the cluster structure to new objects&lt;/strong&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-28T09:41:56.707" Id="125814" LastActivityDate="2014-11-28T09:41:56.707" OwnerUserId="7828" ParentId="125347" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="125850" AnswerCount="2" Body="&lt;p&gt;Here is my question:&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to simulate a random variable using cauchy distribution with C language. Scale and position must be setted manually.&lt;/p&gt;&#10;&#10;&lt;p&gt;I fuond the GSL library wich contain the function: gsl_ran_cauchy (const gsl_rng * r, double a)&lt;/p&gt;&#10;&#10;&lt;p&gt;the problem that i can't unsterstand how to fix the position and the scale and there is not a lot of documentation about that.&lt;/p&gt;&#10;&#10;&lt;p&gt;Regards&lt;/p&gt;&#10;&#10;&lt;p&gt;cauchy-sequence simulation&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-28T10:36:56.040" FavoriteCount="0" Id="125817" LastActivityDate="2014-11-28T15:41:38.603" OwnerUserId="61654" PostTypeId="1" Score="2" Tags="&lt;clustering&gt;&lt;simulation&gt;&lt;programming&gt;" Title="Generate a random chain with cauchy distribution using C language" ViewCount="49" />
  
  <row AcceptedAnswerId="125855" AnswerCount="1" Body="&lt;p&gt;I have read that 2SLS estimator is still consistent even with binary endogenous variable (&lt;a href=&quot;http://www.stata.com/statalist/archive/2004-07/msg00699.html&quot; rel=&quot;nofollow&quot;&gt;http://www.stata.com/statalist/archive/2004-07/msg00699.html&lt;/a&gt;). In the first stage, a probit treatment model will be run instead of a linear model. &lt;/p&gt;&#10;&#10;&lt;p&gt;Is there any formal proof to show that 2SLS is still consistent even when 1st stage is a probit or logit model?&lt;/p&gt;&#10;&#10;&lt;p&gt;Also what if the outcome is also binary? I understand if we have a binary outcome and binary endogenous variable (1st and 2nd stages are both binary probit/logit models),  mimicking 2SLS method will produce a inconsistent estimate. Is there any formal proof for this?  Wooldridge's econometric book has some discussion but I think there is no rigorous proof to show the inconsistency.     &lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{verbatim}&lt;/p&gt;&#10;&#10;&lt;p&gt;data sim;&#10;     do i=1 to 500000;&#10;        iv=rand(&quot;normal&quot;,0,1);&#10;             x2=rand(&quot;normal&quot;,0,1);&#10;        x3=rand(&quot;normal&quot;,0,1);&#10;        lp=0.5+0.8*iv+0.5*x2-0.2*x3;&#10;        T=rand(&quot;bernoulli&quot;,exp(lp)/(1+exp(lp)));&#10;        Y=-0.8+1.2*T-1.3*x2-0.8*x3+rand(&quot;normal&quot;,0,1);&#10;        output;&#10;     end;&#10;     run;&lt;/p&gt;&#10;&#10;&lt;p&gt;****1st stage: logit model ****;&#10;****get predicted values   ****;&lt;br&gt;&#10;proc logistic data=sim descending;&#10;     model T=IV;&#10;     output out=pred1 pred=p;&#10;     run;&lt;/p&gt;&#10;&#10;&lt;p&gt;****2nd stage: ols model with predicted values****;&#10;proc reg data=pred1;&#10;     model y=p;&#10;     run;&lt;/p&gt;&#10;&#10;&lt;p&gt;\end{verbatim}&lt;/p&gt;&#10;&#10;&lt;p&gt;the coefficient of p =1.19984. I only run one simulation but with a large sample size. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-28T13:06:35.797" FavoriteCount="1" Id="125830" LastActivityDate="2014-12-02T04:40:50.853" LastEditDate="2014-12-02T04:40:50.853" LastEditorUserId="56456" OwnerUserId="56456" PostTypeId="1" Score="5" Tags="&lt;probit&gt;&lt;instrumental-variables&gt;&lt;endogeneity&gt;" Title="Consistency of 2SLS with Binary endogenous variable" ViewCount="354" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I was told that if it is reasonable that a linear regression had to go through the origin, one should force it. For example, we expect that mass is proportional to volume. No volume should mean no mass, so we have an extra data point.&lt;/p&gt;&#10;&#10;&lt;p&gt;But I've read that one should NEVER do that. What is the correct approach?&lt;/p&gt;&#10;" ClosedDate="2014-11-28T14:16:46.580" CommentCount="1" CreationDate="2014-11-28T14:08:49.297" FavoriteCount="0" Id="125839" LastActivityDate="2014-11-28T14:08:49.297" OwnerUserId="31711" PostTypeId="1" Score="0" Tags="&lt;regression&gt;" Title="Should one force a linear fit to go through the origin?" ViewCount="18" />
  <row Body="&lt;p&gt;So my problem was from misusing terms.  By saying &quot;block&quot; what I meant was &quot;nested&quot; and I found the solution &lt;a href=&quot;http://www.unt.edu/rss/class/Jon/R_SC/Module9/LMM_Examples.R&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, my code is now:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lmersp1loc=lmer(sp1~treatment + (1|location/treatment), data = mydataframe)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I had to load the package lme4, but everything seems to be working now.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-11-28T14:33:00.707" Id="125842" LastActivityDate="2014-11-28T14:39:44.017" LastEditDate="2014-11-28T14:39:44.017" LastEditorUserId="22468" OwnerUserId="61242" ParentId="125511" PostTypeId="2" Score="0" />
  
  <row AcceptedAnswerId="125853" AnswerCount="1" Body="&lt;p&gt;I am trying to use instrumental variables analysis to infer causality with observational data. I have come across a two-stage least squares (2SLS) regression which is likely to address the endogeneity issue in my research. However, I would like to first stage to be OLS and second stage to be probit within the 2SLS. Based on my reading and search, I have seen researchers use either 2SLS or first stage probit and second stage OLS, but not the other way round which is what I am trying to achieve. I am currently using Stata and &lt;em&gt;ivreg&lt;/em&gt; command in Stata is for a straight 2SLS. Any help on this would be much appreciated!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-28T15:17:58.520" Id="125848" LastActivityDate="2014-11-28T15:59:37.940" LastEditDate="2014-11-28T15:59:37.940" LastEditorUserId="26338" OwnerUserId="61674" PostTypeId="1" Score="5" Tags="&lt;stata&gt;&lt;probit&gt;&lt;instrumental-variables&gt;&lt;2sls&gt;" Title="2SLS but second stage Probit" ViewCount="111" />
  <row Body="&lt;p&gt;Generating from a Cauchy distribution is easy enough that you don't need a library function to do it.  Generate $U$ uniformly in $[0,1]$ using your favorite generator, then apply the formula:&#10;$$&#10;X = m + s \tan(\pi(U-1/2))&#10;$$&#10;where $m$ is the location and $s$ is the scale parameter.  This is comes from the formulas available on &lt;a href=&quot;http://en.wikipedia.org/wiki/Cauchy_distribution&quot; rel=&quot;nofollow&quot;&gt;Wikipedia&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-28T15:41:38.603" Id="125850" LastActivityDate="2014-11-28T15:41:38.603" OwnerUserId="2074" ParentId="125817" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;Your case is less problematic than the other way round. The expectations and linear projections operators go through a linear first stage (e.g. OLS) but not not through non-linear ones like probit or logit. Therefore it's not a problem if you first regress your continous endogenous variable $X$ on your instrument(s) $Z$,&#10;$$X_i = a + Z'_i\pi + \eta_i$$&#10;and then use the fitted values in a probit second stage to estimate&#10;$$\text{Pr}(Y_i=1|\widehat{X}_i) = \text{Pr}(\beta\widehat{X}_i + \epsilon_i &amp;gt; 0)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The standard errors won't be right because $\widehat{X}_i$ is not a random variable but an estimated quantity. You can correct this by bootstrapping both first and second stage together. In Stata this would be something like&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;// use a toy data set as example&#10;webuse nlswork&#10;&#10;// set up the program including 1st and 2nd stage&#10;program my2sls&#10;    reg grade age race tenure&#10;    predict grade_hat, xb&#10;&#10;    probit union grade_hat age race&#10;    drop grade_hat&#10;end&#10;&#10;// obtain bootstrapped standard errors&#10;bootstrap, reps(100): my2sls&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In this example we want to estimate the effect of years of education on the probability of being in a labor union. Given that years of education are likely to be endogenous, we instrument it with years of tenure in the first stage. Of course, this doesn't make any sense from the point of interpretation but it illustrates the code.&lt;/p&gt;&#10;&#10;&lt;p&gt;Just make sure that you use the same exogenous control variables in both first and second stage. In the above example those are &lt;code&gt;age, race&lt;/code&gt; whereas the (non-sensical) instrument &lt;code&gt;tenure&lt;/code&gt; is only there in the first stage.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-11-28T15:59:18.650" Id="125853" LastActivityDate="2014-11-28T15:59:18.650" OwnerUserId="26338" ParentId="125848" PostTypeId="2" Score="5" />
  
  <row Body="&lt;p&gt;So, after further research, I realized that there are indeed 2 definitions, depending on the field.&lt;/p&gt;&#10;&#10;&lt;p&gt;In statistics, the correlation is defined as: &#10; $\frac{E[ (x-\mu_x)(y-\mu_y)^T]}{\sigma_x\sigma_y}$&lt;/p&gt;&#10;&#10;&lt;p&gt;In signal processing &amp;amp; random processes, correlation ussually means:&#10;$E[x'y]$&lt;/p&gt;&#10;&#10;&lt;p&gt;As pointed out, the above quantity in statistics is refered to as the 'joint moment' or 'mixed moment'&lt;/p&gt;&#10;&#10;&lt;p&gt;Source: wikipedia:&#10;&lt;a href=&quot;http://en.wikipedia.org/wiki/Autocorrelation&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Autocorrelation&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-28T18:07:03.137" Id="125861" LastActivityDate="2014-11-28T18:07:03.137" OwnerUserId="35026" ParentId="125755" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;The likelihood expressed in terms of the prior, the posterior and the evidence is:&#10;$$p(D|\alpha,\beta)=\frac{p(\alpha,\beta|D)p(D)}{p(\alpha,\beta)}$$&#10;I assume that what you are looking at is:&#10;$$&#10;p(D) \int p(D|\alpha,\beta) \,\mathrm{d}\beta&#10;= p(D) \int \frac{p(\alpha,\beta|D)p(D)}{p(\alpha,\beta)} \,\mathrm{d}\beta&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;What you actually should look at is:&#10;$$&#10;\frac{\int p(\alpha,\beta|D)p(D) \,\mathrm{d}\beta}{\int p(\alpha,\beta) \,\mathrm{d}\beta}&#10;=\frac{ p(\alpha|D) p(D)}{p(\alpha)}&#10;= p(D|\alpha)&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;However, without seeing your complete code, we do not know what you are looking at. What we know for sure is that the Likelihood does not depend on the choice of the prior distribution.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-28T19:01:27.620" Id="125868" LastActivityDate="2014-11-28T19:01:27.620" OwnerUserId="61648" ParentId="91366" PostTypeId="2" Score="3" />
  
  
  <row Body="&lt;p&gt;The covariance matrix is essential to characterize the joint distribution of random variables. Hence, it is not sufficient to only use one variance parameter. In the case of a multivariate Normal distrubtion, you have some normally distributed random variables that are also jointly normally distributed. For example, take two normally distributed random variables $X1$ and $X2$ with $X1 \sim N(3,1) $ and $X2 \sim N(2,4)$. Assume, they are independent. This implies the covariance is zero. Now, one can show the joint distribution is also Normal with &#10;$N \left(    \left( \begin{array}{ccc} 3 \\ 2 \end{array} \right), \left( \begin{array}{ccc}&#10;1 &amp;amp; 0  \\&#10;0 &amp;amp; 4 \end{array} \right) \right) $. &lt;/p&gt;&#10;&#10;&lt;p&gt;In the picture below, you can see a sample and a contour plot taken from the stated joint distribution. There is now dependence but look at the scales of the axis and how the spread of the points is determined by the variance of each variable. If they had the same variance, there would be no difference.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/M061e.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, assume the $X1$ and $X2$ are still also jointly normally distributed but not independent and exhibit a covariance of $1.5$. This means, there is some linear dependence between them and the distribution is now $N \left(    \left( \begin{array}{ccc} 3 \\ 2 \end{array} \right), \left( \begin{array}{ccc}&#10;1 &amp;amp; 1.5  \\&#10;1.5 &amp;amp; 4 \end{array} \right) \right) $. The picture below shows a dramatically changed distribution. Now, high values of $X1$ correspond tho high values of $X2$ and vice versa.  The important point is, whenever multivariate analysis is done, one tries to find the dependence between random variables and in the real world one can find a many dependent variables. So, that makes it clear why a covariance matrix is needed to characterize a joint distribution and why it is important.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/rf4Tf.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Regarding your first question: Your given matrix is in the AR model multiplied by a scaler because for the purpose of simplicity homoscedasticity is often assumed. So this just means the variance does not depend on time in this model. &lt;/p&gt;&#10;&#10;&lt;p&gt;Note: For the illustration, I used the codes of the socalled &lt;a href=&quot;http://sfb649.wiwi.hu-berlin.de/quantnet/&quot; rel=&quot;nofollow&quot;&gt;Quantnet&lt;/a&gt; provided by the chair of statistics at the Humboldt University of Berlin. Click &lt;a href=&quot;http://sfb649.wiwi.hu-berlin.de/quantnet/index.php?p=show&amp;amp;id=1629&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; and you can download the code and play with the structure of the covariance matrix to see how it behaves in this two dimensional example.&lt;/p&gt;&#10;" CommentCount="10" CreationDate="2014-11-28T23:19:03.720" Id="125884" LastActivityDate="2014-11-28T23:40:55.890" LastEditDate="2014-11-28T23:40:55.890" LastEditorUserId="38160" OwnerUserId="38160" ParentId="125795" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;&lt;strong&gt;Thought:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I think &lt;a href=&quot;http://www.scholarpedia.org/article/Eigenfaces&quot; rel=&quot;nofollow&quot;&gt;eigenfaces&lt;/a&gt; is a decent way to convert what can be million-dimensional spaces to a few tens of dimensions.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Premise:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;So lets assume that you are using a decent eigenfaces tool, or one that:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;does preprocessing to align appropriate features&lt;/li&gt;&#10;&lt;li&gt;handles colors in an appropriate way&lt;/li&gt;&#10;&lt;li&gt;makes sure the pictures used are all the same size&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;This means you don't have &quot;pictures&quot; as much as you have vectors of length O(n=50) elements in size where the elements are weights for each eigen-face comprising the basis.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Analysis:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;First I would create 150-element vectors (concatenation of weight) as inputs and 1-element vectors (elements of closest match) as outputs.  If element 1 and 2 were closest then the output value would be &quot;12&quot;.  If elements 1 and 3 were closest then the output would be &quot;13&quot;.  If elements 2 and 3 were closest then output would be &quot;23&quot;.  Given that there are only 3 unique outputs, I could re-map them to case 1 for &quot;12&quot;, case 2 for &quot;13&quot; and case 3 for &quot;23.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Second I would want to throw away as much meaningless data as possible.  This means that I would try to use something like random forests to determine which of the ~150 columns were not informative.  There is also a &quot;random evil twin method&quot; but I don't have it at my fingertips the way R gives me with random forests.  (If you know a good R library for this, I invite you to put it in the comments).&lt;/p&gt;&#10;&#10;&lt;p&gt;Third, in my personal experience, if you have decent sample sizes, and decent basis a random forest can usually drop you down to the ~30 variables of interest, even from as much as 15k columns.  This is where you have to consider what is the general form of the answer.&lt;/p&gt;&#10;&#10;&lt;p&gt;You could try a dozen breeds of transforms of these variables to map the reduced inputs to the outputs:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;you could train an RF of the reduced inputs and call it good.&lt;/li&gt;&#10;&lt;li&gt;you could train a NN on the reduced inputs if you wanted better smooth interpolation and generalization than an RF&lt;/li&gt;&#10;&lt;li&gt;you could use some sort of linear transformation on the inputs&lt;/li&gt;&#10;&lt;li&gt;there are a few dozen other ML hammers to hit it with, but when you are a hammer every problem looks like a nail.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;More thoughts:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;I would be curious about which of the eigenfaces the reduced set references.  I would just like to see that data and let it talk to me.&lt;/li&gt;&#10;&lt;li&gt;I'm quite curious about your sample sizes and the nature of your variation.  If you are looking at 3 rows, then having 150 columns is not going to be too productive.  If you have a few thousand rows then you might be in great shape.  A few hundred rows and you might be average.  I would hope that you accounted for all sources of variation in terms of ethnicity, facial shape, and such.  &lt;/li&gt;&#10;&lt;li&gt;Dont be afraid of looking through simple models first.  They can be good.  Their interpretation and applicability are easily evaluated.  Their execution can be tested and confirmed with substantially less effort then complex and highly sensitive methods.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2014-11-29T01:39:14.727" Id="125894" LastActivityDate="2014-11-29T01:39:14.727" OwnerUserId="22452" ParentId="124681" PostTypeId="2" Score="0" />
  <row AnswerCount="2" Body="&lt;p&gt;Knowing that both $x \sim \mathcal{N}(0,1)$ and $y \sim \mathcal{N}(0,1)$ ($x,y$ independent from each other), I want to compute&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\mathbb{P}(3x &amp;gt; -y &amp;gt; x \land x &amp;gt; 0 \land y &amp;lt; 0)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm aware that if two events are independent, I can take the two probabilities and multiply. E.g., if I were only interested in &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \mathbb{P}(x &amp;gt; 0 \land y &amp;lt; 0) $$&lt;/p&gt;&#10;&#10;&lt;p&gt;I would simply compute $\mathbb{P}(x &amp;gt; 0) \mathbb{P}(y &amp;lt; 0)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, in the problem described above, the condition $3x &amp;gt; -y &amp;gt; x$ is clearly not independent from the two other conditions $x &amp;gt;0 \land y &amp;lt;0$. I think that I should re-formulate the condition to $3x &amp;gt; -y \land -y &amp;gt; x$ ... but that still leaves the issue that the conditions are somewhat dependent from each other.&lt;/p&gt;&#10;&#10;&lt;p&gt;How can I approach such a problem?&lt;/p&gt;&#10;" ClosedDate="2014-11-29T14:57:35.610" CommentCount="1" CreationDate="2014-11-29T10:49:15.510" FavoriteCount="1" Id="125912" LastActivityDate="2014-11-29T13:11:04.527" OwnerUserId="23694" PostTypeId="1" Score="2" Tags="&lt;probability&gt;&lt;random-variable&gt;" Title="How to compute $\mathbb{P}(3x &gt; -y &gt; x \land x &gt; 0 \land y &lt; 0)$?" ViewCount="70" />
  
  
  <row AcceptedAnswerId="125934" AnswerCount="1" Body="&lt;pre&gt;&lt;code&gt;Null Hypothesis: D(OIL_PRICES) has a unit root&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Exogenous: Constant&lt;br&gt;&#10;Lag Length: 0 (Automatic - based on SIC, maxlag=22)&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;        t-Statistic   Prob.*&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Augmented Dickey-Fuller test statistic  -37.22113    0.0000&#10;Test critical values:   1% level        -3.435299&lt;br&gt;&#10;    5% level        -2.863613&lt;br&gt;&#10;    10% level       -2.567923   &lt;/p&gt;&#10;&#10;&lt;p&gt;*MacKinnon (1996) one-sided p-values.   &lt;/p&gt;&#10;&#10;&lt;p&gt;Augmented Dickey-Fuller Test Equation&lt;br&gt;&#10;Dependent Variable: D(OIL_PRICES,2) &#10;Method: Least Squares&lt;br&gt;&#10;Date: 11/29/14   Time: 18:57&lt;br&gt;&#10;Sample (adjusted): 1/06/2009 2/28/2014&lt;br&gt;&#10;Included observations: 1267 after adjustments   &lt;/p&gt;&#10;&#10;&lt;p&gt;Variable    Coefficient Std. Error  t-Statistic Prob.  &lt;/p&gt;&#10;&#10;&lt;p&gt;D(OIL_PRICES(-1))   -1.042433   0.028006    -37.22113   0.0000&#10;C   0.050065    0.043335    1.155289    0.2482&lt;/p&gt;&#10;&#10;&lt;p&gt;R-squared   0.522716        Mean dependent var  -0.003875&#10;Adjusted R-squared  0.522339        S.D. dependent var  2.230622&#10;S.E. of regression  1.541651        Akaike info criterion   3.705162&#10;Sum squared resid   3006.510        Schwarz criterion   3.713283&#10;Log likelihood  -2345.220       Hannan-Quinn criter.    3.708213&#10;F-statistic 1385.413        Durbin-Watson stat  2.002267&#10;Prob(F-statistic)   0.000000            &lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-11-29T13:45:37.410" FavoriteCount="0" Id="125926" LastActivityDate="2014-12-03T17:24:50.390" LastEditDate="2014-11-29T14:48:12.917" LastEditorUserId="61707" OwnerUserId="61707" PostTypeId="1" Score="2" Tags="&lt;hypothesis-testing&gt;&lt;unit-root&gt;&lt;eviews&gt;" Title="How to read UNIT ROOT TEST results obtained from EVIEWS? I mean what values do we study to interpret our result?" ViewCount="239" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I've run a binary logistic regression analysis, and I’m unsure how to interpret some of the odds-ratios. I tested the predictive capacity of three independent categorical variables on a binary dependent variable, and in one of those categorical variables (one with five categories), all four categories test significant compared to the reference category, and all have odds ratios &amp;lt; 1. The largest of the four odds-ratios is 0.13. May I then conclude that all categories are at least (1-0.13=0.87=) 87% less likely than the reference category to achieve dependent outcome 1? Or doesn't that add up? Thanks in advance for the help!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-29T15:12:11.003" Id="125935" LastActivityDate="2014-11-29T15:12:11.003" OwnerUserId="61711" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;logistic&gt;&lt;categorical-data&gt;&lt;odds-ratio&gt;&lt;binary&gt;" Title="Interpretation of smaller-than-one odds-ratios in logistic regression with multi-category categorical independent variables" ViewCount="35" />
  <row Body="&lt;p&gt;It means that the analysis has a Bayesian interpretation, but that doesn't mean that it might not also have a frequentist interpretation as well.  The MAP estimate might be viewed as being a partialy Bayesian approach, with a more complete Bayesian approach being to consider the posterior distribution over the parameters.  It is still a Bayesian approach though, as the definition of probability would be a &quot;degree of plausibility&quot;, rather than a long run frequency.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-29T16:02:20.713" Id="125941" LastActivityDate="2014-11-29T16:02:20.713" OwnerUserId="887" ParentId="125938" PostTypeId="2" Score="7" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm working with a large set as a project for the business analytic course with $10^5$ observations and 170+ variables, some of which come with a missing value proportion of larger than 20%, even more than 95%. &lt;/p&gt;&#10;&#10;&lt;p&gt;I'm really thinking about under such situation, whether it's necessary to apply any imputation method to those variables rather than simply delete them. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you could give me any criteria on how could I delete them, thanks in advance!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-29T20:22:03.310" Id="125961" LastActivityDate="2014-11-29T20:47:43.010" LastEditDate="2014-11-29T20:42:20.803" LastEditorUserId="22468" OwnerUserId="61726" PostTypeId="1" Score="3" Tags="&lt;missing-data&gt;&lt;large-data&gt;&lt;eda&gt;" Title="Is there any rule of thumb to delete a variable in a large data set?" ViewCount="36" />
  <row Body="&lt;p&gt;If you use the L2 norm, i.e., quadratic penalty on the log likelihood function, penalization is very similar to a Bayesian procedure with a Gaussian prior with mean zero for the non-intercept regression coefficients.  But unlike the full Bayesian procedure that factors in the uncertainty about the the amount of penalization (analogous to treating the variance of random effects as if it were a known constant), the penalized maximum likelihood procedure pretends that the optimum penalty was pre-specified and is not an unknown parameter.  So it results in confidence limits that are a bit too narrow.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-11-29T20:57:05.067" Id="125964" LastActivityDate="2014-11-29T20:57:05.067" OwnerUserId="4253" ParentId="125938" PostTypeId="2" Score="4" />
  
  
  
  <row AcceptedAnswerId="125980" AnswerCount="1" Body="&lt;p&gt;Given a random scalar $ x \in \mathbb{R} $ and a random vector $ Y \in \mathbb{R}^n $ that are independent, can it be said that:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ {\rm cov}(xY) = {\rm var}(x){\rm cov}(Y) + {\rm var}(x)E[Y]E[Y]^T + E[x]^2{\rm cov}(Y) $$&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-11-29T23:33:33.050" Id="125975" LastActivityDate="2014-11-30T14:40:27.043" OwnerUserId="61732" PostTypeId="1" Score="4" Tags="&lt;variance&gt;&lt;covariance&gt;&lt;random-variable&gt;" Title="(Co)variance of product of a random scalar and a random vector" ViewCount="130" />
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;What type of distribution would you consider this data to be? &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;it's largely irrelevant what that looks like, since you have here the marginal distribution, but the GAM will be modelling the conditional distributions (conditional on the predictors, that is).&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;My first thought is an f-distribution, but I suppose it could also be a poisson distribution. &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;If it could be one, it could not possibly be the other, since one is continuous and the other is discrete. But again, it doesn't matter for this problem.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The f-distribution is not a family type. Does anyone know why not? &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Because the F is not exponential-family.&lt;/p&gt;&#10;&#10;&lt;p&gt;If there's a strong need for an F, there are other things you might try (but I see no reason to think that there is any need for an F at all, other than a very superficial similarity of the - irrelevant - shape of the marginal distribution).&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Should I go with the poisson distribution? &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I don't see why this would be a reasonable choice. Times aren't discrete. Even if that wasn't an issue, I see no reason to expect that the variance will be equal to the mean. Indeed, since you'd expect the spread-mean relationship not to depend on your time-unit, I'd plump for variance proportional to mean-squared (which suggests Gamma or lognormal as two easy options)&lt;/p&gt;&#10;&#10;&lt;p&gt;My first choice for &lt;em&gt;times&lt;/em&gt; would typically be Gamma, with either a log link or an inverse link (or rarely, perhaps identity), depending on my understanding of the circumstances. In some circumstances I might transform times (to speeds or log-times) and try to model those.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are other possibilities - inverse-Gaussian or Tweedie**, for example&lt;/p&gt;&#10;&#10;&lt;p&gt;**(not available by default, but there's a package that should make it work with GLMs and GAMs)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-30T02:04:31.163" Id="125982" LastActivityDate="2014-11-30T02:04:31.163" OwnerUserId="805" ParentId="125890" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;Thanks for posting the data. To a good first approximation, your data show exponential trends, as shown by a nearly linear plot with logarithmic scale for the response axis. Here you are. I would have shown units of measurement for the areas had I known what they should be. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ViLAS.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;For this kind of data, and for this sample size, &lt;strong&gt;calculating autocorrelation of the raw data is essentially useless&lt;/strong&gt;, as at best the results reflect the trend in the data that is evident any way. &lt;/p&gt;&#10;&#10;&lt;p&gt;Some people would want to fit a model and look at the autocorrelation of the residuals here. That would make more sense statistically. &lt;/p&gt;&#10;&#10;&lt;p&gt;Clearly your data are not quite equally spaced. Make sure that you are explicit about that with your software. Interpolating to get data every year would not be completely crazy, but would not be a good idea unless you took account of that in your modelling. I can't see how to do that, as most of your points would be interpolated, and the gain in degrees of freedom would be essentially spurious. &lt;/p&gt;&#10;&#10;&lt;p&gt;If your purpose is to understand these data, I doubt that you have enough data to make the proposed models and tests a very good idea. If your purpose is to learn about ARIMA models and Granger causality, you need a better dataset for the purpose. &lt;/p&gt;&#10;&#10;&lt;p&gt;Note: I doubt that any area is known to 10 significant figures! &lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-11-30T11:29:31.450" Id="126011" LastActivityDate="2014-11-30T11:35:22.140" LastEditDate="2014-11-30T11:35:22.140" LastEditorUserId="22047" OwnerUserId="22047" ParentId="126001" PostTypeId="2" Score="3" />
  
  <row AcceptedAnswerId="126359" AnswerCount="1" Body="&lt;p&gt;Let $X_{N \times d}$ be the data matrix, where $N$ is the number of samples and $d$ the size of the features space.&lt;/p&gt;&#10;&#10;&lt;p&gt;Using kernel PCA (kPCA), one first computes a kernel matrix $K_{N \times N}$, and then, after its eigenvectors $E_{N \times N}$ have been computed, it is possible to project the data onto the first $c \leq N$ components as: $$X_\mathrm{projected} = KE_c,$$ where $E_c$ denotes first $c$ columns of $E$. Equivalently, in Matlab notation:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Projected_data = K*E(:,1:c);&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The new projected data have now size ${N \times c} $.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to know if it is possible to project an unseen data vector $x_{1 \times d} $ onto the previously computed principal components $E$. If it is possible, what is the correct procedure?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-11-30T11:47:56.430" Id="126014" LastActivityDate="2014-12-03T11:22:52.173" LastEditDate="2014-12-02T16:32:45.810" LastEditorUserId="28666" OwnerUserId="36773" PostTypeId="1" Score="1" Tags="&lt;pca&gt;&lt;dimensionality-reduction&gt;&lt;kernel-trick&gt;" Title="Is it possible to project a new vector onto the PC space using kernel PCA?" ViewCount="58" />
  <row Body="&lt;p&gt;There are several issues:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;The sample size is far too low to reliably do what you are attempting&lt;/li&gt;&#10;&lt;li&gt;Classification error is an improper scoring rule that is optimized by an incorrect model with incorrect features and incorrect weights&lt;/li&gt;&#10;&lt;li&gt;You are using the bootstrap incorrectly.  The bootstrap, relying on samples with replacement, results in duplications of observations that increases the amount of overfitting.&lt;/li&gt;&#10;&lt;li&gt;With the more appropriate Efron-Gong optimism bootstrap, used to estimate the drop-off in predictive performance so as to get overfitting-corrected estimates of predictive accuracy, the philosophy is that one attempts to estimate the difference in predictive accuracy of the fitted model evaluated on the training data and the true unknown predictive accuracy.  The bootstrap estimates this because this difference (amount of overfitting) can be estimated by the difference between super-overfitting (evaluate accuracy on a bootstrap sample) and regular overfitting (evaluate accuracy of the model fitted on the bootstrap sample on the original sample).&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="2" CreationDate="2014-11-30T12:33:02.330" Id="126016" LastActivityDate="2014-11-30T12:33:02.330" OwnerUserId="4253" ParentId="125995" PostTypeId="2" Score="2" />
  
  <row AnswerCount="0" Body="&lt;p&gt;For a curve like the one below: &lt;/p&gt;&#10;&#10;&lt;p&gt;Given that we have a certain amount of vertices for approximation, say 5.&lt;br&gt;&#10;&lt;strong&gt;How to find appropriate &lt;code&gt;x&lt;/code&gt; values to place the vetices on the curve so that, when connecting the vertices with straight lines, the deviation between the two curves is minimal.&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;I mean the area that appears between the curves needs to be minimized.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/oli1s.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-11-30T15:23:10.963" Id="126028" LastActivityDate="2014-11-30T17:02:18.083" LastEditDate="2014-11-30T17:02:18.083" LastEditorUserId="919" OwnerUserId="61769" PostTypeId="1" Score="0" Tags="&lt;error&gt;&lt;fitting&gt;&lt;approximation&gt;&lt;minimum&gt;" Title="Fit vertices to minimize Error when linearly interpolating" ViewCount="7" />
  
  <row Body="&lt;p&gt;What do you mean by highly skewed data or imbalanced data? I have a population of 684 total (580 are good and 104 are bad), is this data highly skewed or imbalanced? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-30T17:45:38.403" Id="126041" LastActivityDate="2014-11-30T17:45:38.403" OwnerUserId="38376" ParentId="90779" PostTypeId="2" Score="0" />
  <row AnswerCount="1" Body="&lt;p&gt;On p.171 in Bickel and Doksum's Mathematical Statistics 2006,&#10;does &quot;The main practical &lt;strong&gt;import&lt;/strong&gt; of minimax theorems is, in fact, contained in a converse and its extension that we now give͝&quot; mean that Proposition 3.3.1 is implied by Theorem 3.3.1?&lt;/p&gt;&#10;&#10;&lt;p&gt;The proof of Proposition 3.3.1 doesn't rely on Theorem 3.3.1. The condition in Proposition 3.3.1 is also more general than Theorem 3.3.1. So I doubt &quot;import&quot; means &quot;implication&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/747Cx.png&quot; alt=&quot;enter image description here&quot;&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/WNkn0.png&quot; alt=&quot;enter image description here&quot;&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/4ASrX.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-11-30T18:25:08.490" Id="126046" LastActivityDate="2014-12-01T01:19:25.590" LastEditDate="2014-12-01T00:37:52.867" LastEditorUserId="22047" OwnerUserId="1005" PostTypeId="1" Score="-1" Tags="&lt;mathematical-statistics&gt;" Title="What does &quot;import&quot; mean for the relation between a proposition and a theorem?" ViewCount="86" />
  <row Body="&lt;p&gt;Why not choose $K=17$? It looks like the CV error goes down until then and flattens out afterwards. If all you care about is predictive accuracy then I would not choose $K=3$ because it looks pretty clear that you can do better.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-11-30T22:28:04.443" Id="126075" LastActivityDate="2014-11-30T22:28:04.443" OwnerUserId="13818" ParentId="126051" PostTypeId="2" Score="0" />
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I've watched an online lecture regarding CNN (&lt;a href=&quot;https://www.youtube.com/watch?v=wORlSgx0hZY&quot; rel=&quot;nofollow&quot;&gt;https://www.youtube.com/watch?v=wORlSgx0hZY&lt;/a&gt;) that confused me a bit. At roughly 8:35 in the lecture it was stated that it is important to use the absolute value of the tanh function on the output from the convolutional layer. I've never heard that before and I can't quite understand it just based on the lecture. Can anyone expand on that?&lt;/p&gt;&#10;&#10;&lt;p&gt;The lecture connects this to how &quot;the polarity of features is often irrelevant to recognize objects&quot; and &quot;the rectification eliminates cancellations between neighboring filter outputs when combined with average pooling&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;I've attached an excerpt of the slide used in the lecture below:&#10;&lt;img src=&quot;http://i.stack.imgur.com/f9daY.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-01T02:54:41.450" Id="126097" LastActivityDate="2014-12-01T04:21:53.893" OwnerUserId="29025" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;neural-networks&gt;&lt;feature-construction&gt;&lt;convolution&gt;" Title="Convolutional neural network - Using absolute of tanh on convolution output" ViewCount="52" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I'm currently running some data through the MCMCglmm package in R. I was wandering what actually happens in the chain that is created? I have a multiresponse model (as a quick overview of the data: I'm trying to estimate variance and covariance within and between 6 traits - e.g. body size, these traits are measured on multiple individuals of 80 families, and there are some random effects such as experimental block). &lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to know what is actually happening inside that chain making process - what is going on and how are the prior distributions and the data involved? It's obviously not just plucking numbers of thin air, it's using the prior and data in some way. What are the calculations or processes underlying this?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-12-01T07:44:23.180" Id="126122" LastActivityDate="2014-12-05T21:57:36.360" LastEditDate="2014-12-01T08:54:50.483" LastEditorUserId="16542" OwnerUserId="16542" PostTypeId="1" Score="0" Tags="&lt;bayesian&gt;&lt;mcmc&gt;" Title="What is going on in an MCMC chain?" ViewCount="111" />
  <row AnswerCount="1" Body="&lt;p&gt;I am using Gaussian Process Regression to interpolate my input points. I would like to measure the total uncertainty of my prediction thus I sum up the GPR prediction variances at all the testing points $x_i, i = \overline{1, N}$.&#10;$$&#10;V = \sum_{i = 0}^N \sigma^2(x_i)&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I was expecting that this total variance $V$ would reduce when I add more samples, and this is the case when I don't adapt the hyperparameters. However, when I adapt the hyperparameters using Maximum Likelihood, $V$ can increase when new points are added. &lt;/p&gt;&#10;&#10;&lt;p&gt;I found this quite counter-intuitive. Isn't hyperparameter adapting supposed to reduce the variance? Could some one please help me clarify this point. Thanks a lot!&lt;/p&gt;&#10;&#10;&lt;p&gt;I attach a simple Matlab script, based on GPML for your testing (&lt;a href=&quot;http://www.gaussianprocess.org/gpml/code/matlab/doc/index.html&quot; rel=&quot;nofollow&quot;&gt;http://www.gaussianprocess.org/gpml/code/matlab/doc/index.html&lt;/a&gt;)&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;covfunc = @covSEiso; hyp.cov = [log(0.1); log(1.0)]; &#10;likfunc = @likGauss; hyp.lik = log(0.01);&#10;&#10;z = linspace(-1.0, 1.0, 101); z = z';&#10;&#10;figure; grid on;&#10;axis([-1.1, 1.1, -2, 2]);&#10;x = [];&#10;y = [];&#10;&#10;ADAPT_PARAM = false;&#10;&#10;while 1&#10;    [xx, yy, button] = ginput(1);&#10;&#10;    if button == 3&#10;        break;&#10;    end&#10;&#10;    x = [x; xx];&#10;    y = [y; yy];&#10;&#10;    if ADAPT_PARAM&#10;    % Adapt the hyperparams&#10;        hyp.cov = [log(0.1); log(1.0)]; &#10;        hyp = minimize(hyp, @gp, -100, @infExact, [], covfunc, likfunc, x, y);&#10;    end&#10;&#10;    [m, s2] = gp(hyp, @infExact, [], covfunc, likfunc, x, y, z);&#10;    f = [m+2*sqrt(s2); flipdim(m-2*sqrt(s2),1)];&#10;&#10;    fill([z; flipdim(z,1)], f, [7 7 7]/8); &#10;    axis([-1.1, 1.1, -2, 2]); grid on; hold on;&#10;    plot(z, m, 'LineWidth', 2);&#10;    plot(x, y, 'r+', 'MarkerSize', 12)&#10;    hold off;&#10;&#10;    disp(['Total variance ', num2str(sum(s2))]);&#10;end&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2014-12-01T07:55:54.833" Id="126123" LastActivityDate="2014-12-20T01:09:14.810" LastEditDate="2014-12-04T15:52:13.417" LastEditorUserId="805" OwnerUserId="61192" PostTypeId="1" Score="2" Tags="&lt;matlab&gt;&lt;gaussian-process&gt;" Title="Gaussian Process Prediction Uncertainty" ViewCount="51" />
  <row AcceptedAnswerId="126255" AnswerCount="1" Body="&lt;p&gt;I am quite new in R and am on a stage of running a regression model there.&#10;The approach we have chosen is linear regression with dummy variables.&#10;As far as my knowledge and experience go when using dummies one should choose a null interval - class(group) within a variable that will receive 0 points. This should help tackle the multicollinearity.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is - how do we set (if we can) the null interval?&#10;I reviewed the &quot;dummies&quot; package but did not see an option there.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks,&#10;N&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-12-01T08:11:30.100" Id="126126" LastActivityDate="2014-12-02T07:59:11.607" LastEditDate="2014-12-02T07:59:11.607" LastEditorUserId="58843" OwnerUserId="58843" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;regression&gt;&lt;categorical-data&gt;" Title="Defining NULL group for dummy variables" ViewCount="47" />
  <row Body="&lt;p&gt;Your question is &lt;em&gt;very&lt;/em&gt; general. For information on what is MCMC I recommend you two great books by Robert and Casella, &lt;a href=&quot;http://link.springer.com/book/10.1007/978-1-4419-1576-4&quot; rel=&quot;nofollow&quot;&gt;one with examples in R&lt;/a&gt; and the second, &lt;a href=&quot;http://link.springer.com/book/10.1007%2F978-1-4757-4145-2&quot; rel=&quot;nofollow&quot;&gt;very detailed, handbook&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;More information on &lt;code&gt;MCMCglmm&lt;/code&gt; package you can find in its &lt;a href=&quot;http://cran.r-project.org/web/packages/MCMCglmm/&quot; rel=&quot;nofollow&quot;&gt;documentation and vignettes&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can find examples of Metropolis-Hastings - one of the &quot;classic&quot; MCMC algoruthms - on CV, e.g. &lt;a href=&quot;http://stats.stackexchange.com/questions/100725/metropolis-hastings-algorithm&quot;&gt;here&lt;/a&gt;, the same with &lt;a href=&quot;http://stats.stackexchange.com/questions/65693/confusion-related-to-gibbs-sampling&quot;&gt;Gibbs sampling&lt;/a&gt;. Those are the basic examples.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-01T08:38:00.987" Id="126128" LastActivityDate="2014-12-01T09:03:23.153" LastEditDate="2014-12-01T09:03:23.153" LastEditorUserId="35989" OwnerUserId="35989" ParentId="126122" PostTypeId="2" Score="1" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I would like to understand the output of the function CCA (so sparse CCA) in the package PMA. The following code is an example of the package itself, but I would like to interpret it.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;u &amp;lt;- matrix(c(rep(1,25),rep(0,75)),ncol=1)&#10;v1 &amp;lt;- matrix(c(rep(1,50),rep(0,450)),ncol=1)&#10;v2 &amp;lt;- matrix(c(rep(0,50),rep(1,50),rep(0,900)),ncol=1)&#10;x &amp;lt;- u%*%t(v1) + matrix(rnorm(100*500),ncol=500)&#10;z &amp;lt;- u%*%t(v2) + matrix(rnorm(100*1000),ncol=1000)&#10;# Can run CCA with default settings, and can get e.g. 3 components&#10;out &amp;lt;- CCA(x,z,typex=&quot;standard&quot;,typez=&quot;standard&quot;,K=3)&#10;print(out,verbose=TRUE) # To get less output, just print(out)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-12-01T07:04:25.507" Id="126134" LastActivityDate="2014-12-01T09:42:34.667" OwnerDisplayName="Anita" OwnerUserId="59643" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;correlation&gt;" Title="R: Package &quot;PMA&quot; the function CCA" ViewCount="58" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I wish to apply 10 fold cross validation to my model (with a sample size of 90). I have one question troubling me.&lt;/p&gt;&#10;&#10;&lt;p&gt;Once I am ready with a full model, I would break the full data into 9:1 ratio- 9parts for training and 1 part of testing, and in the same way I would be testing all the parts in 10 rounds.&lt;/p&gt;&#10;&#10;&lt;p&gt;The question that is troubling me is that how would I take the variables in each of the rounds. The variables would not be same.&lt;/p&gt;&#10;&#10;&lt;p&gt;If I am not correct please give me the steps to choose the variables in this stage?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-01T13:30:56.793" Id="126157" LastActivityDate="2014-12-01T14:15:53.163" OwnerUserId="48107" PostTypeId="1" Score="1" Tags="&lt;regression&gt;" Title="Regarding cross validation" ViewCount="68" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I have 4 groups.  I would like to compare the effect of 3 different treatments relating to drug treatment of a sample individually against a single control sample.&lt;/p&gt;&#10;&#10;&lt;p&gt;Control (no drug)&#10;Treatment A (low concentration of drug)&#10;Treatment B (medium concentration of drug)&#10;Treatment C (high concentration of drug)&lt;/p&gt;&#10;&#10;&lt;p&gt;What test should I use?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!!&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-12-01T15:46:25.190" Id="126174" LastActivityDate="2014-12-29T11:54:20.390" OwnerUserId="61842" PostTypeId="1" Score="1" Tags="&lt;anova&gt;&lt;t-test&gt;" Title="T-test or ANOVA" ViewCount="35" />
  
  <row AnswerCount="2" Body="&lt;p&gt;What does it mean that Moving Average Process is first-order, second order, third order, etc.&#10;MA(1), MA(2), MA(3)?&#10;How to simple understand it, without any complicated formulas, etc?&lt;/p&gt;&#10;&#10;&lt;p&gt;Kind regards, thank you for help!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-01T16:57:11.180" Id="126185" LastActivityDate="2014-12-01T19:00:36.047" OwnerUserId="60365" PostTypeId="1" Score="1" Tags="&lt;moving-average&gt;" Title="Moving Average (q) interpretation" ViewCount="34" />
  <row AnswerCount="1" Body="&lt;p&gt;I'm not sure whether there's an exact answer to this, but I'm wondering in the simplest case of doing Cox Regression with censoring on one variable, if we have N measurement values, what number (k) of them can be censored and still yield interpretable results?&lt;/p&gt;&#10;&#10;&lt;p&gt;In practice with MatLab, I've noted that increasing the number of censored values leads eventually to problems with the MLE converging.&lt;/p&gt;&#10;&#10;&lt;p&gt;Or, I suppose, another way to look at it is to ask - where is the trade-off between including censored values versus not including them?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-01T18:28:09.940" Id="126197" LastActivityDate="2014-12-01T18:44:57.347" OwnerUserId="36475" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;convergence&gt;&lt;censoring&gt;" Title="Max fraction of censored values for Cox regression?" ViewCount="16" />
  <row Body="&lt;p&gt;A moving average process is a weighted &quot;moving average&quot; of a stationary white noise process. Usually it is not an actual average but rather a linear combination of previous noise, or error terms, originating from an unknown process we cannot measure. These noise points are assumed to be independent and identically distributed (and usually Normal).&lt;/p&gt;&#10;&#10;&lt;p&gt;The order of this process is the number of previous points that are taken into account. A MA(1) process will use only the last time point, MA(2) uses two, etc.&lt;/p&gt;&#10;&#10;&lt;p&gt;Even if you ask to avoid formulas it is usually easier to read this in a formula instead of in words. I will try to explain what I write in detail.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;MA(1): $ x_{i} = \varepsilon_{i} + a_1 \cdot \varepsilon_{i-1}$&lt;/li&gt;&#10;&lt;li&gt;MA(2): $ x_{i} = \varepsilon_{i} + a_1 \cdot \varepsilon_{i-1} + a_2 \cdot \varepsilon_{i-2}$&lt;/li&gt;&#10;&lt;li&gt;...&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;This means that at a certain time point (here called $i$) the process will take the value $x_{i}$ which is modelled as the current point in the driving noise $\varepsilon_{i}$ and some factor $a_1$ times the previous noise value $\varepsilon_{i-1}$. In the MA(2) we also have a term for yet another point, two steps back in time. &lt;/p&gt;&#10;&#10;&lt;p&gt;When fitting a model of this type you generally try to infer the noise variance and regression parameters (i.e find the multipliers called $a$ in the formula) so that you can predict future values or in some other way understand your process better. For a higher order model you therefore need to find more multipliers than you would need to do for a lower order model. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-01T18:49:43.710" Id="126201" LastActivityDate="2014-12-01T18:49:43.710" OwnerUserId="17481" ParentId="126185" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Just take the ma polynomial (psi weights )and compute it's inverse. This will be the auto-projective polynomial ( pi weights) reflecting how previous values are weighted. In this way the model's memory in terms of the Y's will be better understood and accepted.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-01T19:00:36.047" Id="126204" LastActivityDate="2014-12-01T19:00:36.047" OwnerUserId="3382" ParentId="126185" PostTypeId="2" Score="0" />
  
  
  
  
  <row Body="&lt;p&gt;You can certainly have multivariate regression models where the response is not a single value but a vector of possibly dependent values.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can also have more than one input (multiple regression), so &quot;Team 1 performance&quot; and &quot;Team 2 performance&quot; (or some joint transformation of them, such as their sum and difference, perhaps) could both be predictors in such a model.&lt;/p&gt;&#10;&#10;&lt;p&gt;So at first glance, multivariate multiple regression would seem to be a good starting point.&lt;/p&gt;&#10;&#10;&lt;p&gt;The next thing to consider might be whether the response would be better modelled as something other than multivariate normal. You might look into GLMs, for example.&lt;/p&gt;&#10;&#10;&lt;p&gt;In addition, when looking at pairwise contests like this, it can be difficult to get any absolute measure of individual team performance since they depend heavily on what the other team does - this might lead toward something like Bradley-Terry type models.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-01T23:37:37.640" Id="126228" LastActivityDate="2014-12-01T23:37:37.640" OwnerUserId="805" ParentId="126191" PostTypeId="2" Score="4" />
  
  <row AcceptedAnswerId="126234" AnswerCount="1" Body="&lt;p&gt;Working on regression problem I started to think about representation of &quot;day of a week&quot; feature. I wonder which approach would perform better:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;one feature; value 1/7 for Monday; 2/7 for Tuesday...&lt;/li&gt;&#10;&lt;li&gt;7 features: (1, 0, 0, 0, 0, 0, 0) for Monday; (0, 1, 0, 0, 0, 0, 0) for Tuesday...&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;It's hard to measure it due to network configuration differences. (Additional six features should be reflected in number of hidden nodes I believe.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Number of all features is about 20. I use simple backprop to learn ordinary feed-forward neural network.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-12-01T23:52:40.783" Id="126230" LastActivityDate="2014-12-02T14:55:08.730" OwnerUserId="61877" PostTypeId="1" Score="3" Tags="&lt;machine-learning&gt;&lt;neural-networks&gt;&lt;feature-construction&gt;" Title="Optimal construction of day feature in neural networks" ViewCount="64" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I have a model which includes two exponential rate parameters. I would like to test whether a model with two individual rates describes some data better than a model for which both rates are the same. I think I could just estimate the model with two parameters and look at HDI around the difference. However, later I would like to compare more complex (an possibly not-nested) similar models. So I would like to use this as a toy example and calculate Bayes-Factors in JAGS.&lt;/p&gt;&#10;&#10;&lt;p&gt;Therefore, I would like to set up two models. One in which both rate parameters are drawn from the same normal distribution. And the other where they are drawn from independent normal distributions. &lt;/p&gt;&#10;&#10;&lt;p&gt;I'm unsure about the choice of priors, however.&#10;Do they need to be conjugate for the Bayes-Factors to be proper? (I suspect: Even if not in this toy example, when I use more complex models with different parameters they may have to be conjugate?).&lt;/p&gt;&#10;&#10;&lt;p&gt;How could I specify such conjugate priors (normal; unknown mean &amp;amp; unknown variance, I guess) for the rate parameters in JAGS? &lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Details and additional questions regarding the priors:&lt;/p&gt;&#10;&#10;&lt;p&gt;The model is as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;$y_i \sim B(N_i, p_i)$&lt;/p&gt;&#10;&#10;&lt;p&gt;$p_i $&amp;lt;- some_equation_with_two_exp_rate($v_1,v_2,i$)&lt;/p&gt;&#10;&#10;&lt;p&gt;Now I would like to specify different versions: One where both rates are equal ($v_1=v_2$) [and others, e.g., one that expresses $v_1&amp;gt;v_2$].&lt;/p&gt;&#10;&#10;&lt;p&gt;So first, for specifying the equality I was thinking of setting $v_1$ &amp;lt;- $x$ and $v2$ &amp;lt;- $x$ and $x$ ~ some_prior_dist. However, I think that wouldn't be correct, because the $v$'s represent two independent processes (with the same rate). So I tried adding hyperprior: $v_1$ ~ some_prior_dist($x,var$)  and $v_2$ ~ some_prior_dist($x,var$) and $x$ ~ some_hyper_prior_dist($m,var2$).&lt;/p&gt;&#10;&#10;&lt;p&gt;For the distributions I mainly tried dnorm. Estimations seems strange: while I still get reasonable posteriors for the $v_1$ and $v_2$, m yields strange estimates (very broad distributions, as if not sufficiently constrained by the data). Furthermore, I'm unsure about which variances ($var,var2$ in the dummies above) to fix and which should get individual priors. Any hints?&lt;/p&gt;&#10;&#10;&lt;p&gt;Many thanks,&#10;Jan&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-02T08:59:44.170" Id="126265" LastActivityDate="2015-02-22T15:30:28.877" LastEditDate="2014-12-04T23:40:42.387" LastEditorUserId="54847" OwnerUserId="54847" PostTypeId="1" Score="0" Tags="&lt;jags&gt;&lt;bugs&gt;&lt;conjugate-prior&gt;" Title="Comparing a model with two rate parameters to a model with one. Conjugate priors?" ViewCount="75" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am looking again at a popular statistical testing method used in finance, suspect it's a bit naughty, but would like to have a more experienced eye take a look also.&lt;/p&gt;&#10;&#10;&lt;p&gt;The method is the following,&lt;/p&gt;&#10;&#10;&lt;p&gt;1) estimate a percentile (i.e. the &quot;value at risk&quot;) of a history of n portfolio returns&lt;/p&gt;&#10;&#10;&lt;p&gt;2) record whether the following return is above or below the percentile&lt;/p&gt;&#10;&#10;&lt;p&gt;3) tot up the breaches and use a proportional chi squared test (binomial distribution tends towards normal after sufficient number of draws from the distribution)&lt;/p&gt;&#10;&#10;&lt;p&gt;The issue comes with the 'n' portfolio returns overlapping each other; i.e. rather than using mutually exclusive returns they overlap (often n-1 returns of adjacent value at risk figures).&lt;/p&gt;&#10;&#10;&lt;p&gt;However, a 'conditional' test is applied (no real consensus on a particular one) which tests for the independence of the sample of breaches - to ameliorate this possibility.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is this sort of thing frowned upon?  I.e. intentionally baking in overlapping data and dependence; then later trying to test the problem away (however flawed the tests may be).&lt;/p&gt;&#10;&#10;&lt;p&gt;(There is never enough data; I rather hold my hands up in that case, than push on with a flawed analysis)&lt;/p&gt;&#10;&#10;&lt;p&gt;I also posted the question here, without much luck:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://quant.stackexchange.com/questions/15551/overlapping-value-at-risk-backtest-data-an-issue&quot;&gt;http://quant.stackexchange.com/questions/15551/overlapping-value-at-risk-backtest-data-an-issue&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-12-02T09:30:43.210" FavoriteCount="1" Id="126267" LastActivityDate="2014-12-02T09:30:43.210" OwnerUserId="61908" PostTypeId="1" Score="2" Tags="&lt;methodology&gt;" Title="Violating Independence - Methodological Question" ViewCount="31" />
  
  
  <row Body="&lt;p&gt;Normally when estimating a confidence interval I think of an interval estimate for a population parameter (such as the mean). So when you say you want to compute a confidence interval for the difference, I assume you mean an interval estimate for the expected difference.  Is this correct?  &lt;/p&gt;&#10;&#10;&lt;p&gt;whether or not X-Y is Gaussian depends on the distribution of X and Y.  If both X and Y are Gaussian, than X-Y will be Gaussian.  if not, than X-Y will, in all likelihood, not be Gaussian. &lt;/p&gt;&#10;&#10;&lt;p&gt;When you're bootstrapping I am guessing that on each iteration you store the sample average difference.  So you're really developing an interval for the estimate of $E(X-Y)$.  Lets call this estimate $\bar{d}$.  &lt;/p&gt;&#10;&#10;&lt;p&gt;In the parametric approach $\bar{d}$  will be the sample average of $X-Y$.  $\bar{d}$ WILL be normally distributed by the central limit theorem, regardless of the distribution of $X$ and $Y$ assuming they are both iid.  So even though X-Y is not Gaussian, $\bar{d}$ will be (at least in the limit as the number of observations approaches infinity).&lt;/p&gt;&#10;&#10;&lt;p&gt;Assuming $X$ and $Y$ are iid, $X-Y$ will have mean $E(X-Y)=E(X) - E(Y)$ which can be approximated by the sample average $\bar{d}$.  The variance of this estimate can be derived in the following matter.&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;var(\bar{d}) = var(\frac{1}{n}\sum_{i=1}^{n}(x_i - y_i))=...=\frac{\sigma^2}{n}&#10;$$&#10;Where $\sigma^2=var(X-Y)=var(X)+var(Y)-2cov(X,Y)$.  So roughly speaking&#10;$$&#10;\bar{d} \sim n(E(X-Y) , \frac{\sigma^2}{n})&#10;$$ &#10;and more formally&#10;$$&#10;\sqrt(n)(\bar{d}-E(X-Y)) \rightarrow n(0,\frac{\sigma^2}{n})\;\;\;(in\;distribution)&#10;$$&#10;Of course we don't know $\sigma^2$ so we instead approximate it with the sample variance $s^2$.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can then form a confidence interval for $E(X-Y)$ with the above Gaussian distribution and it will hold asymptotically. I suppose you could also use the t-distribution in developing the confidence interval, I would not know off hand what the degrees of freedom would be (best guess n-1).&lt;/p&gt;&#10;&#10;&lt;p&gt;Hope that helps! &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-12-02T13:38:02.197" Id="126289" LastActivityDate="2014-12-02T15:59:31.227" LastEditDate="2014-12-02T15:59:31.227" LastEditorUserId="61846" OwnerUserId="61846" ParentId="126282" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="126315" AnswerCount="3" Body="&lt;p&gt;Knowing that a population sample (non-random) is biased in terms of its demographics, what are the best practices to correct for this issue?&lt;/p&gt;&#10;&#10;&lt;p&gt;That is, let's say that I can attach an array of demographics to the sample, and that I wish to transform this sample so that they resemble that of the population these results where picked. Later on, this adjusted sample will be used for mathematical modeling.&lt;/p&gt;&#10;&#10;&lt;p&gt;As I see it, it is quite straightforward to correct for one certain aspect. If males are under represented by &lt;code&gt;50 %&lt;/code&gt;, all males are assigned a weight of &lt;code&gt;2&lt;/code&gt;. But what if one wants to take into account several variables at the same time? Is building a &lt;code&gt;n&lt;/code&gt;-dimensional array the way to go? Are there better solutions?&lt;/p&gt;&#10;&#10;&lt;p&gt;Are there readily available methods for this? An &lt;code&gt;R&lt;/code&gt;-package?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-12-02T14:41:16.200" Id="126291" LastActivityDate="2014-12-05T08:28:51.800" LastEditDate="2014-12-05T07:32:28.937" LastEditorUserId="3401" OwnerUserId="3401" PostTypeId="1" Score="0" Tags="&lt;survey&gt;&lt;sample&gt;&lt;bias&gt;" Title="Correcting biased survey results" ViewCount="121" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm running a clustering task on unlabeled data, and assume we're validating our results by applying the Min-Cut measure as an internal validity index.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's refer the normalized version of the minimum cut:&#10;For each class A_i (k clusters) we compute&#10;NCut = 1/k sum_{i=1}^k W(A_i,A_i-complementary) / vol(A_i)&#10;where W is the cost of the cut.&lt;/p&gt;&#10;&#10;&lt;p&gt;In case that our data contains a lot of connected components, this will yield a low min-cut (since cost of the cuts is very small) which imply that our clustering is &quot;good&quot;. But, it is easy to think of not necessarily good partition for a big graph (with some nodes with high degree) which will output a low N-cut score. &lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a known &quot;fixed&quot; N-cut measure? any there any articles that talk about this drawback?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-02T15:07:37.340" Id="126294" LastActivityDate="2014-12-02T15:07:37.340" OwnerUserId="60522" PostTypeId="1" Score="0" Tags="&lt;clustering&gt;&lt;validation&gt;&lt;graph-theory&gt;" Title="The authenticity of the N-cut measure when the number of components in the data is high" ViewCount="6" />
  
  <row Body="&lt;p&gt;As Tim pointed out, you should use survey weighting. &lt;/p&gt;&#10;&#10;&lt;p&gt;In your case, more specifically, if all the auxiliary variables (your demographic variables) you want to use to make your sample match your population are &lt;strong&gt;qualitative&lt;/strong&gt; variables you will use:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Post-stratification: If you have the full joint distribution of these variables on the population&lt;/li&gt;&#10;&lt;li&gt;Raking: If you only have the marginal distributions of these variables on the population&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;More generally, if you have &lt;strong&gt;qualitative&lt;/strong&gt; and &lt;strong&gt;quantitative&lt;/strong&gt; auxiliary variables, you can use a &lt;a href=&quot;http://www.statcan.gc.ca/pub/12-001-x/2007002/article/10488-eng.pdf&quot; rel=&quot;nofollow&quot;&gt;Calibration approach&lt;/a&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;Tim also pointed out the &lt;code&gt;survey&lt;/code&gt; package in &lt;code&gt;R&lt;/code&gt;. There you can find three functions that implements these methods:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Post-stratification: &lt;code&gt;postStratify&lt;/code&gt;&lt;/li&gt;&#10;&lt;li&gt;Raking: &lt;code&gt;rake&lt;/code&gt;&lt;/li&gt;&#10;&lt;li&gt;Calibration: &lt;code&gt;calibrate&lt;/code&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;There is the &lt;code&gt;sampling&lt;/code&gt; package in &lt;code&gt;R&lt;/code&gt; containing the function for weighting.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Calibration: &lt;code&gt;calib&lt;/code&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;It is important to note though that these weighting methods were originally developed under a probability sampling framework, which does not appear to be your case (you referred to your sample as &quot;non-random&quot;). These methods might mitigate some potential bias in your estimates, as long as the auxiliary variables used in the weighting adjustments are related to your outcome variables and to the selection mechanism of your sample. See this &lt;a href=&quot;http://biostats.bepress.com/umichbiostat/paper35/&quot; rel=&quot;nofollow&quot;&gt;paper&lt;/a&gt; by Little and Vartivarian for a similar discussion in survey nonresponse. &lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-12-02T19:03:22.870" Id="126315" LastActivityDate="2014-12-05T07:53:35.540" LastEditDate="2014-12-05T07:53:35.540" LastEditorUserId="3330" OwnerUserId="61944" ParentId="126291" PostTypeId="2" Score="6" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm organizing a meta-analysis on performances with studies showing sometime positive and sometime negative estimates.&lt;/p&gt;&#10;&#10;&lt;p&gt;Sometime the result of the study is in a positive direction, with higher scores corresponding to better performance (e.g., number of tasks completed).&#10;Sometime the result of the study is in a negative direction, with lower scores corresponding to better performance (e.g., time to completion of the task).&lt;/p&gt;&#10;&#10;&lt;p&gt;How can I put together both types of studies?&#10;What kind of mathematical correction can I do?&lt;/p&gt;&#10;&#10;&lt;p&gt;I was thinking of estimating the effect size (in Hedges' g), then multiplying per -1 the positive-sided studies, so all studies will be in a negative direction.&#10;Is this a valid approach?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-02T20:00:32.863" Id="126327" LastActivityDate="2014-12-03T01:22:28.563" OwnerUserId="61953" PostTypeId="1" Score="1" Tags="&lt;meta-analysis&gt;" Title="Meta-analysis with studies reporting both positive and negative estimates" ViewCount="25" />
  
  <row AcceptedAnswerId="126679" AnswerCount="1" Body="&lt;p&gt;Let us work with the following structural model: $$y=\mathbf{x_{1i}β}+x_{2i}β_2+\varepsilon_i$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $x_{2i}$ is our single endogenous regressor. It turns out that given my instruments and my first stage regression, I can obtain $β_2$ in two different ways. I can use the two stage least squares regression $$y=\mathbf{x_{1i}β}+\hat{x}_{2i}β_2+\varepsilon_i \qquad (*)$$ where $\hat{x}_{2i}$ is our instrument predicted with the first stage regression.&lt;/p&gt;&#10;&#10;&lt;p&gt;And we can obtain the same estimate for $β_2$ with the following regression&#10;$$y=\mathbf{x_{1i}\beta}+x_{2i}β_2+\delta\hat{v}_i+\varepsilon_i \qquad (**)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\hat{v}_i$ is the estimated residual of the first stage regression, namely the portion of the original endogenous regressor that is correlated with the original $\varepsilon_i$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now I would like to prove algebraically that we can estimate $β_2$ using these two different approaches. I understand why the estimates must be the same. After all, in $(*)$ we are estimating the partial effect of $x_{2i}$ on $y$ controlling for the portion of $x_{2i}$ that was correlated with the error in the structural model above. Is this interpretation correct?&#10;Furthermore I tried to explicit this algebraically. I know I should be able to rewrite $(*)$ as $(**)$ or viceversa. Unfortunately I am not convinced of my result. I started from $(*)$, used the fact that $\hat{v_i}=x_{2i}-\hat{x}_{2i}$ and got the following result&#10;$$y=\mathbf{x_{1i}β}+x_{2i}β_2-β_2\hat{v_i}+\varepsilon_i$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, this result is similar to $(**)$ but I cannot understand it. In particular, how should I decompose $x_{2i}$? What am I missing? Could you give me a hint?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you for your help.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-12-02T20:49:51.267" FavoriteCount="2" Id="126335" LastActivityDate="2014-12-04T21:36:44.250" OwnerUserId="37226" PostTypeId="1" Score="4" Tags="&lt;econometrics&gt;&lt;instrumental-variables&gt;" Title="Instrumental variables equivalent representation" ViewCount="94" />
  
  <row AcceptedAnswerId="126355" AnswerCount="4" Body="&lt;p&gt;What are some theorems which might explain (i.e., generatively) why real-world data might be expected to be normally distributed?&lt;/p&gt;&#10;&#10;&lt;p&gt;There are two that I know of:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;The Central Limit Theorem (of course), which tells us that the sum of several independent random variables with mean and variance (even when they are not identically distributed) tends towards being normally distributed&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Let X and Y be independent continuous RV's with differentiable densities such that their joint density only depends on $x^2$ + $y^2$. Then X and Y are normal.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;(cross-post from &lt;a href=&quot;http://math.stackexchange.com/questions/1048871/reasons-for-data-to-be-normally-distributed&quot;&gt;mathexchange&lt;/a&gt;) &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Edit:&lt;/em&gt;&#10;To clarify, I am not making any claims about how much real world data is normally distributed. I am just asking about theorems that can give insight into what sort of processes might lead to normally distributed data.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-12-02T22:29:06.927" FavoriteCount="6" Id="126351" LastActivityDate="2014-12-09T19:35:41.463" LastEditDate="2014-12-03T16:30:46.900" LastEditorUserId="7290" OwnerUserId="61864" PostTypeId="1" Score="14" Tags="&lt;normal-distribution&gt;&lt;central-limit-theorem&gt;" Title="Reasons for data to be normally distributed" ViewCount="628" />
  <row AnswerCount="0" Body="&lt;p&gt;What is the &quot;standard reference&quot; that firstly describes or surveys in details the cascade forward neural network? This kind of net is available in matlab toolbox for long &lt;a href=&quot;http://www.mathworks.com/help/nnet/ref/cascadeforwardnet.html&quot; rel=&quot;nofollow&quot;&gt;cascadeforwardnet&lt;/a&gt; (as early as 1990's), but I can't find the original reference. Please note that it seems NOT the cascade-correlation network.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-02T23:37:53.583" Id="126358" LastActivityDate="2014-12-02T23:37:53.583" OwnerUserId="22147" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;neural-networks&gt;&lt;deep-learning&gt;" Title="What is the &quot;standard reference&quot; for cascade forward neural network?" ViewCount="22" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have three questions&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;1:&lt;/strong&gt; Can variances for different mean calculations be combined to obtain the combined variance? &lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{equation}&#10;S = \sqrt{s_1^2 +s_2^2 + s_3^2}&#10;\end{equation}&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;2:&lt;/strong&gt; When using weighted mean should the SEM/variance be calculated as described by &lt;a href=&quot;https://stats.stackexchange.com/questions/25895/computing-standard-error-in-weighted-mean-estimation&quot;&gt;Ming-Chih Kao&lt;/a&gt;, or would the straight forward method described in the &lt;a href=&quot;https://en.wikipedia.org/wiki/Weighted_arithmetic_mean#Weighted_sample_variance&quot; rel=&quot;nofollow&quot;&gt;wikipedia&lt;/a&gt; be sufficient?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;3:&lt;/strong&gt; The end result is $\phi_t$, obtaining the variances is a mean to construct confidence intervals each time period specified. Am I going about obtaining the average based on weights horribly wrong, should I combine $\mu$ and $\phi$ before obtaining $\beta$, and how should I create the CI if i do?&lt;/p&gt;&#10;&#10;&lt;p&gt;The basis for the questions are show bellow&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Equation 1a: A weighted mean&#10;\begin{equation}&#10;E(\mu_{tc}) = \mu_{tc} = \frac{\sum_{i=1}^{n} w_{tci} X_{tci}}{\sum_{i=1}^{n} w_{tci}}&#10;\end{equation}&lt;/p&gt;&#10;&#10;&lt;p&gt;Equation 1b: Variance&#10;\begin{equation}&#10;s_1 = Var(\mu_{tc}) =  \frac{\sum_{i = 1}^{n} w_{tci}(x_{tci} -  \mu_{tci})^2}{(\sum_{i=1}^{n}w_{tci})^2}&#10;\end{equation}&lt;/p&gt;&#10;&#10;&lt;p&gt;Equation 2a: A moving average&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{equation}&#10;E(\beta_{ct}) = \beta_{ct} = \frac{\mu_{ct-1} + \mu_{ct-2} + \dots + \mu_{ct-q}}{q}&#10;\end{equation}&lt;/p&gt;&#10;&#10;&lt;p&gt;Equation 2b: Variance:&#10;\begin{equation}&#10;s_2 = Var(\beta_{ct})  = \frac{\sum_{q=1}^{Q}(\mu_{ct-q}-\beta_{ct})^2}{Q}&#10;\end{equation}&lt;/p&gt;&#10;&#10;&lt;p&gt;Equation 3a: A weighted mean&#10;\begin{equation}&#10;E(\phi_t) = \phi_t = \frac{\sum_{c=1}^{C}Y_{tc}\beta_{ct}}{\sum_{c=1}^{C}Y_{tc}}&#10;\end{equation}&#10;Equation 3b: Variance:&#10;\begin{equation}&#10;s_3 = Var(\phi_t)= \frac{\sum_{c=1}^{C}Y_{tc}(\beta_{ct}-\phi_{tc})^2}{(\sum_{c=1}^{C}Y_{tc})^2}&#10;\end{equation}&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-12-03T02:17:55.270" FavoriteCount="1" Id="126374" LastActivityDate="2014-12-03T02:17:55.270" OwnerUserId="22159" PostTypeId="1" Score="0" Tags="&lt;variance&gt;&lt;mean&gt;&lt;standard-error&gt;&lt;weighted-mean&gt;&lt;moving-average&gt;" Title="Combining several variances, obtained from weighted mean and moving average" ViewCount="15" />
  
  <row Body="&lt;p&gt;It would help if you explained the data a little bit. It looks like what you have in the table and graph are aggregate numbers of raw data, and it looks like your estimates are regression estimates of some sort, which explains their linearity. &lt;/p&gt;&#10;&#10;&lt;p&gt;If my assumptions are correct, what you are trying to do is estimate whether the slope on Age is the same for males vs females. That can be done a number of different ways in a regression, for example:&#10;- Create an interaction term between gender and age&#10;- Run the model for males and females separately and test their coefficients (a joint F test or a likelihood ratio test, for example)&lt;/p&gt;&#10;&#10;&lt;p&gt;If you are using Stata, for example, you could do:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;reg wt c.age#female&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;or you could use suest:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;eststo est_fem: quietly reg wt age if female==1&#10;eststo est_male: quietly reg wt age if female==0&#10;suest est_fem est_male&#10;test [est_fem_mean]age==[est_male_mean]age&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now, this could get more complex if your children are nested within families, where you may need to take account of the multilevel structure. If that's the case, you need either to use cluster-robust standard errors, or decide on a random effects model...&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT based on your added questions regarding dummy coding:&#10;Different dummy coding doesn't matter. R can interpret factor variables logical variables or numerical variables as dummies so it doesn't matter if you call gender m/f, 0/1, 1/2. &lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-12-03T04:07:05.413" Id="126382" LastActivityDate="2014-12-04T02:09:27.690" LastEditDate="2014-12-04T02:09:27.690" LastEditorUserId="42898" OwnerUserId="42898" ParentId="126375" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="126435" AnswerCount="1" Body="&lt;p&gt;Are they the same thing? If not, could someone possibly explain the difference or point to the seminal papers describing the approaches?  &lt;/p&gt;&#10;&#10;&lt;p&gt;I am looking not for a detailed technical exposition, but a general taxonomy of the algorithms.  For example, are both of them for dimensionality reduction?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-12-03T04:32:50.237" FavoriteCount="1" Id="126384" LastActivityDate="2014-12-03T15:59:21.160" LastEditDate="2014-12-03T10:56:40.000" LastEditorUserId="28666" OwnerUserId="45129" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;pca&gt;&lt;neuroscience&gt;" Title="Sparse coding vs. sparse PCA, are they the same thing?" ViewCount="88" />
  
  <row AcceptedAnswerId="126404" AnswerCount="1" Body="&lt;p&gt;The beta coefficient for independent variables surprisingly become negative, and with significant p-value.&lt;/p&gt;&#10;&#10;&lt;p&gt;e.g. smoking (independent) and risk of lung cancer (dependent)&#10;Regression coefficient for smoking is surprisingly negative, where it suppose to be positive (since we expect that an increase in smoking will increase the chance of getting lung cancer)&lt;/p&gt;&#10;&#10;&lt;p&gt;This is a SIMPLE linear regression analysis, no other variables included.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, what are the reasons of getting unexpected or wrong sign of coefficient in simple linear regression, considering data cleaning and transformation have been done?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-12-03T05:54:27.963" FavoriteCount="1" Id="126390" LastActivityDate="2014-12-04T07:44:15.907" LastEditDate="2014-12-03T06:45:32.660" LastEditorUserId="61985" OwnerUserId="61985" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;regression-coefficients&gt;" Title="Undesirable direction of beta coefficient in simple linear regression" ViewCount="63" />
  
  <row Body="&lt;p&gt;$R^2$ will never decrease when you add more variables, and will in practice always increase, regardless of whether the added variable belongs in the model or not. So yes, adding an interaction will increase $R^2$.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-12-03T08:17:30.437" Id="126407" LastActivityDate="2014-12-03T08:17:30.437" OwnerUserId="37448" ParentId="126391" PostTypeId="2" Score="1" />
  
  
  <row AcceptedAnswerId="126460" AnswerCount="1" Body="&lt;p&gt;I have 2 groups (tinnitus sufferers and controls) who are significantly different in age - I would normally control for age as a covariate (as it is a cognitive task) but it violates the assumption of ANCOVA in that the independent variable and covariate are related (as expected, older people are more likely to suffer tinnitus so this difference is expected).&lt;/p&gt;&#10;&#10;&lt;p&gt;I am worried that putting in age as a covariate removes my group effect because age and group are too closely related for them both to be in the model - can I legitimately not include this as a covariate based on this? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-03T10:31:38.920" FavoriteCount="1" Id="126427" LastActivityDate="2014-12-03T14:23:05.447" OwnerUserId="61999" PostTypeId="1" Score="2" Tags="&lt;multicollinearity&gt;&lt;assumptions&gt;&lt;ancova&gt;&lt;covariate&gt;" Title="Covariate related to independent variable - best solutions" ViewCount="21" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Suppose users in a system like a social network are described by a number of tags. The number of tags can be assumed to be less than 10.&lt;/p&gt;&#10;&#10;&lt;p&gt;Example&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;John&lt;/strong&gt;: funny musician geek professor&lt;br&gt;&#10;&lt;strong&gt;Peter&lt;/strong&gt;: skinny tall soccer manager geek&lt;br&gt;&#10;&lt;strong&gt;Amelia&lt;/strong&gt;: musician feminist power soccer&lt;br&gt;&#10;&lt;strong&gt;Kate&lt;/strong&gt;: jazz drums night bar cinema art&lt;br&gt;&#10;&lt;strong&gt;Richard&lt;/strong&gt;: painting books art cinema geek  &lt;/p&gt;&#10;&#10;&lt;p&gt;Let → mean &quot;follows&quot;. &#10;We have that:&lt;/p&gt;&#10;&#10;&lt;p&gt;John → Richard&lt;br&gt;&#10;John → Kate&lt;br&gt;&#10;John → Amelia&lt;br&gt;&#10;Kate → Richard&lt;br&gt;&#10;Amelia → Peter&lt;br&gt;&#10;Peter → John    &lt;/p&gt;&#10;&#10;&lt;p&gt;or writing the tags:&lt;/p&gt;&#10;&#10;&lt;p&gt;funny musician geek professor → painting books art cinema geek&lt;br&gt;&#10;funny musician geek professor → jazz drums night bar cinema art&lt;br&gt;&#10;funny musician geek professor → musician feminist power soccer&lt;br&gt;&#10;jazz drums night bar cinema art → painting books art cinema geek&lt;br&gt;&#10;musician feminist power soccer → skinny tall soccer manager geek&lt;br&gt;&#10;skinny tall soccer manager geek  → funny musician geek professor  &lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a comprehensive logic/theory/model to predict for a new user who he is likely to follow based on his tags? If so, which one? And how? This smells like conditioned probabilities with some assumptions in the mix....&lt;/p&gt;&#10;&#10;&lt;p&gt;You can use as example James. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;James&lt;/strong&gt;:  musician geek police soccer&lt;/p&gt;&#10;&#10;&lt;p&gt;Who is the person James is more likely to follow?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-12-03T12:05:28.850" FavoriteCount="1" Id="126440" LastActivityDate="2014-12-03T23:18:16.503" LastEditDate="2014-12-03T23:18:16.503" LastEditorUserId="44723" OwnerUserId="44723" PostTypeId="1" Score="4" Tags="&lt;classification&gt;&lt;recommender-system&gt;" Title="Who will follow who based on tags?" ViewCount="36" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I need to sum some variables that are on different scales to make a new one.&#10;I'm wondering If it's correct to calculate the percentile scores first, and then to sum them up. &lt;/p&gt;&#10;&#10;&lt;p&gt;Imagine I have to measure general beauty and I have three variables: beauty of the hair, beauty of the face and beauty of the body. You could have a beautiful face and hair, but if you have an ugly body then you won't be considered beautiful (this is why I need to consider all the variables together and not just one by one separately).&#10;Also, you'll be perceived as beautiful if you are the most beautiful person among all the acquaintances of a specific group of people. Suppose they don't know anybody else except you and the other people in the group. Accordingly, no matter if someone more beautiful than you is in another group. This is why I'd like to use percentiles, instead of raw scores.&lt;/p&gt;&#10;&#10;&lt;p&gt;The three variables do not correlate. What if they do? Would things be different?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-12-03T13:39:45.450" FavoriteCount="0" Id="126450" LastActivityDate="2014-12-04T09:37:30.897" LastEditDate="2014-12-04T09:37:30.897" LastEditorUserId="36738" OwnerUserId="36738" PostTypeId="1" Score="-1" Tags="&lt;distributions&gt;&lt;quantiles&gt;&lt;psychometrics&gt;" Title="Sum of percentile rank scores of two or more variables" ViewCount="45" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a question concerning the coding of Effect Sizes for a large (educational) Meta Analysis with mostly latent outcome variables. &lt;/p&gt;&#10;&#10;&lt;p&gt;Some studies provide &quot;Intent to Treat&quot; Data from which I calculate Effect Sizes (Hedges g) and some studies only provide (local average treatment effect) LATE or TOT (treatment on treated) Data. Some studies report both. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Is it okay to mix ITT and LATE Treatment-Effects-Data or should I stick to one type of treatment operationalization?&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;My meta analysis is concerned with educational interventions and since we seek to explain the variability in treatment success of these interventions afterwards through Intervention characteristics I usually would prefer to include the estimates of the subjects who really completed the intervention (rather than who were assigned to the treatment).&lt;/p&gt;&#10;&#10;&lt;p&gt;Best Regards and thank`s in advance.&lt;/p&gt;&#10;&#10;&lt;p&gt;Tim&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-03T13:47:32.257" Id="126452" LastActivityDate="2014-12-03T13:47:32.257" OwnerUserId="62013" PostTypeId="1" Score="1" Tags="&lt;meta-analysis&gt;&lt;effect-size&gt;&lt;treatment-effect&gt;&lt;meta-regression&gt;&lt;systematic&gt;" Title="Mixing ITT and TOT/LATE Effect Sizes in Meta-Analysis" ViewCount="24" />
  <row AcceptedAnswerId="126582" AnswerCount="3" Body="&lt;p&gt;I have got a problem to devise a distance metric to get the similarity measurement of vectors. Someone suggested me to use dot product, which seems to me the same as the Cosine similarity metric; however in Wikipedia &lt;a href=&quot;http://en.wikipedia.org/wiki/Cosine_similarity&quot; rel=&quot;nofollow&quot;&gt;(Cosine Similarity)&lt;/a&gt;, it mentioned Cosine similarity is not a proper distance metric as it does not have the triangle inequality property and it violates the coincidence axiom (the proper distance metric should satisfy the four conditions &lt;a href=&quot;http://en.wikipedia.org/wiki/Metric_(mathematics)&quot; rel=&quot;nofollow&quot;&gt;(distance metric)&lt;/a&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;My questions are:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;What are the proper distance metric? Please name some examples.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Are Dice's coefficient and Jaccard index proper distance metric?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Are there any disadvantages of using dot product? (One of the reasons for the popularity of dot product is that it is very efficient to evaluate).&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Thanks a lot. A.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-03T13:51:32.853" Id="126453" LastActivityDate="2014-12-04T09:58:27.030" LastEditDate="2014-12-04T09:45:46.020" LastEditorUserId="14826" OwnerUserId="14826" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;clustering&gt;&lt;similarities&gt;&lt;metric&gt;" Title="How to get a valid distance metric?" ViewCount="75" />
  
  <row Body="&lt;p&gt;You might want to use a permutation importance (mean decrease in accuracy) rather than Gini (decrease in node purity), if your random forest implementation supports it. (R's &lt;code&gt;randomForest&lt;/code&gt; with the &lt;code&gt;importance=TRUE&lt;/code&gt; option, for example.) Both of these measures are biased towards continuous variables and categorical variables with lots of levels (i.e. variables with many splitting opportunities) if you're working with a mixture of variable types.&lt;/p&gt;&#10;&#10;&lt;p&gt;Given the choice between these two (possibly biased) methods, permutation importance seems more interpretable to me. Oh, in R's &lt;code&gt;randomForest&lt;/code&gt;, these values will be normalized unless you also add the option &lt;code&gt;scale=FALSE&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;It appears that &lt;code&gt;Boruta&lt;/code&gt; uses permutation importance, and automates the inclusion and exclusion of variables based on that.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-03T15:06:27.437" Id="126464" LastActivityDate="2014-12-03T15:06:27.437" OwnerUserId="1764" ParentId="60260" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;This is my new attempt to rewrite the previous question about combining a few linear regression models into single equation. The background is that I have a set of dependent variables Y which is identical (say 1,2,3,4,5) for every one of the observations. In every observation I’ve measured more than 100 different datasets of independent variables X.&#10;Due to “built-in” collinearity of X variables (and their high number) the multiple regression is failing because of &quot;ill-conditioned matrix&quot; error that I get in the software I use.  &lt;/p&gt;&#10;&#10;&lt;p&gt;The simplified example of what I am trying to do: &lt;/p&gt;&#10;&#10;&lt;p&gt;Set_1: y={1,2,3,4,5}; x1={10,20,30,40,50}; y=a1+b1X1&lt;br&gt;&#10;Set_2: y={1,2,3,4,5}; x2={100,200,300,400,500}; y=a2+b2X2&lt;br&gt;&#10;                     ….&lt;br&gt;&#10;Set_n: y={1,2,3,4,5};  xn={ ... }; y=an+bnXn&lt;/p&gt;&#10;&#10;&lt;p&gt;Is that possible (as I’m thinking) to combine all the n resulting linear models so that I can come up with a single equation. Since the X variables are highly correlated so probably the desired model could look something like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;Y = A + b1X1 + b2X2 + … + bnXn &lt;/p&gt;&#10;&#10;&lt;p&gt;So the actual question is how can I use the set linear models to obtain a single equation?&#10;All the help will be appreciated&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-03T16:37:38.800" Id="126470" LastActivityDate="2014-12-04T00:01:23.293" OwnerUserId="61989" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;generalized-linear-model&gt;&lt;modeling&gt;&lt;linear&gt;" Title="How can I use the set of linear models to obtain a single equation?" ViewCount="28" />
  <row AnswerCount="0" Body="&lt;p&gt;Say I have parameters $a(x$) and $b(x)$, with $N_a(x)$ and $N_b(x)$ experimental measurement counts, respectively. $x$ is an independent variable ($x_1, x_2, ...$). For each $a$ and $b$, I can calculate the statistical significance between $x_1$, $x_2$, etc.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then I calculate $c(x) = f(&amp;lt;a&amp;gt;,&amp;lt;b&amp;gt;)$. Note that in general $N_a$ is not the same as $N_b$ and both vary with $x$. The means and error estimates of $c$ at $x_1, x_2, ...$ can be easily computed using standard error propagation. But how do I compute the statistical significance ($p$-value) between $c(x_1)$, $c(x_2)$, etc?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-03T17:05:01.880" Id="126473" LastActivityDate="2014-12-04T07:46:23.213" LastEditDate="2014-12-04T07:46:23.213" LastEditorUserId="62022" OwnerUserId="62022" PostTypeId="1" Score="1" Tags="&lt;p-value&gt;&lt;error-propagation&gt;" Title="Error propagation and p value" ViewCount="27" />
  
  <row Body="&lt;p&gt;It depends on what kind of survival analysis you are doing.  &lt;/p&gt;&#10;&#10;&lt;p&gt;A censored observation doesn't have to have only one cause (the study ended), it can have several causes (study ended, dropout, death from a cause that isn't of interest to the study [run over by a truck], etc.).  If the event of interest is death due to disease and not having received a transplant, you can call having received a transplant a censored observation.  &lt;/p&gt;&#10;&#10;&lt;p&gt;On the other hand, you can do competing risks survival analysis instead, if that better addresses your study question.  If you think of SA as having outcomes analogous to logistic regression ({0,1} vs. {died, didn't}), then competing risks SA is analogous to multinomial logistic regression where there are &gt;2 categories.  In this case, your categories would be {died, got a transplant, were censored}.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-12-03T18:57:55.413" Id="126485" LastActivityDate="2014-12-03T18:57:55.413" OwnerUserId="7290" ParentId="126483" PostTypeId="2" Score="2" />
  
  
  
  <row AcceptedAnswerId="126535" AnswerCount="1" Body="&lt;p&gt;I understand that an Latent Dirichlet allocation models each document as a mixture of topics where a topic is a distribution over words. &lt;/p&gt;&#10;&#10;&lt;p&gt;What is not clear to me whether I need to manually specify the topics or whether this is something the LDA is doing for me, similarly to a clustering algorithm?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-03T23:12:16.887" FavoriteCount="1" Id="126530" LastActivityDate="2014-12-04T00:00:19.930" OwnerUserId="20563" PostTypeId="1" Score="0" Tags="&lt;topic-models&gt;" Title="Topics in Latent Dirichlet allocation" ViewCount="37" />
  <row Body="&lt;p&gt;My rule of thumb for a one tailed test is to ask &quot;If the result was in the opposite direction to that which I expect (or hypothesize) will my conclusions be the same as if there is no difference?&quot; If that isn't true, then you shouldn't be doing a one tailed test. For example, if you are seeing if some doctors have more patients die than would be expected, because you suspect the doctors are murdering their pateints, a one tailed test should be used. If a doctor has exactly the expected number of patients die, or fewer than the expected number of patients die, your conclusions are the same - there is no evidence they're murdering the patients. &lt;/p&gt;&#10;&#10;&lt;p&gt;The only time I've seen one tailed used is if the p-value is marginally significant. No one ever says they're using a one-tailed test if their p-value is 0.001 or 0.800, because there's no need. So the use of one-tailed tests immediately makes me believe that someone is trying to deceive me.&lt;/p&gt;&#10;&#10;&lt;p&gt;Yes, it's very unsound to look at the data and decide that a one-tailed test is appropriate. You could just make up a p-value if you want to, and this would also be unsound. What you're suggesting isn't that bad, but it's getting closer.&lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, there's no such thing as a one-tailed F-table. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-12-03T23:12:42.943" Id="126531" LastActivityDate="2014-12-03T23:12:42.943" OwnerUserId="17072" ParentId="126519" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;You can't derive the combined multiple regression model from the individual separate models, since the result depends on the relationships amongst the various $x$ variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you have the original data, you could fit a straight multiple regression to $y$ on $x_1, x_2, x_3, ... x_k$, (assuming you have enough data to estimate all the parameters), though if your example data is remotely typical you'll have multicollinearity problems.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-04T00:01:23.293" Id="126537" LastActivityDate="2014-12-04T00:01:23.293" OwnerUserId="805" ParentId="126470" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;This paper seems to discuss multivariate poisson distribution with covariance structures.  Check it out. Happy reading!&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://link.springer.com/article/10.1007%2Fs11222-005-4069-4&quot; rel=&quot;nofollow&quot;&gt;http://link.springer.com/article/10.1007%2Fs11222-005-4069-4&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-04T02:54:56.747" Id="126548" LastActivityDate="2014-12-04T02:54:56.747" OwnerUserId="45129" ParentId="126542" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;The position of a consultant is awkward because it's of the consultant's interest to not work as much as the client requests, so any push back of &quot;let's not&quot; is going to cause sour feelings.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Let them fall, then pick them up:&lt;/strong&gt; From my experience, I would suggest granting their wish first by doing what they asked. And then present the alternative method and explain to them the pros and cons of the presented methods. Yes, some time is wasted at the beginning but can often cut down a lot of dreadful tug-a-wars.&lt;/p&gt;&#10;&#10;&lt;p&gt;This approach has not caused any problem so far. Most of the times I won, yet occasionally some stubborn ones insisted they'd keep the old one, but at least I felt I had presented a chance for the client to make an informed choice. If things get too ridiculous, I would request my name to be removed from all the works related to the project.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Slowly build up their statistical &quot;taste&quot;:&lt;/strong&gt; For example, in your case I'd suggest using a scatter plot to break them in, and then perhaps inspire them to appreciate a cross-correlation function plot, which probably can tell even more than a scatter plot.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Show evidence in literature:&lt;/strong&gt; Arguments related to types of analysis, terminology, and data presentation can also be solved by presenting a few current publications that support your suggestions.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-12-04T03:52:42.433" Id="126554" LastActivityDate="2014-12-04T04:37:20.370" LastEditDate="2014-12-04T04:37:20.370" LastEditorUserId="13047" OwnerUserId="13047" ParentId="126553" PostTypeId="2" Score="4" />
  <row AcceptedAnswerId="126570" AnswerCount="2" Body="&lt;p&gt;I have a sparse dataset where a lot of the columns (features) contain mostly zero values. Class labels are multiple discrete categories (10 classes to be precise). I'm wondering if this should trouble classifying the dataset by learning an SVM with kernel (say RBF, polynomial, or linear)? And which kernel should cause trouble, which should not?&lt;/p&gt;&#10;&#10;&lt;p&gt;Empirically, I fit an SVM with RBF kernel using R (package e1071) and it throws a lot of warnings and the prediction accuracy is very poor. I'm not sure if this is the problem with SVM and RBF kernel or something else.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-04T05:09:30.193" Id="126558" LastActivityDate="2014-12-04T08:05:43.260" OwnerUserId="23783" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;classification&gt;&lt;svm&gt;&lt;kernel&gt;&lt;sparse&gt;" Title="Kernel SVM on sparse data" ViewCount="51" />
  <row AnswerCount="0" Body="&lt;p&gt;In pybrain LSTM layer there are these buffer that are used to store values.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;'bufferlist': [   ('ingate', 20),&#10;                      ('outgate', 20),&#10;                      ('forgetgate', 20),&#10;                      ('ingatex', 20),&#10;                      ('outgatex', 20),&#10;                      ('forgetgatex', 20),&#10;                      ('state', 20),&#10;                      ('ingateError', 20),&#10;                      ('outgateError', 20),&#10;                      ('forgetgateError', 20),&#10;                      ('stateError', 20),&#10;                      ('inputbuffer', 80),&#10;                      ('inputerror', 80),&#10;                      ('outputbuffer', 20),&#10;                      ('outputerror', 20)],&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Could anyone explain what these variables are for? I am trying to get the activation of an LSTM layer. Which variable should I take?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-04T06:06:03.657" Id="126563" LastActivityDate="2014-12-04T06:06:03.657" OwnerUserId="55663" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;neural-networks&gt;" Title="pybrain LSTM layer buffer variables" ViewCount="40" />
  
  
  <row Body="&lt;p&gt;This is an excellent question, but unfortunately (or maybe fortunately?) I have only recently written &lt;a href=&quot;http://stats.stackexchange.com/a/123136/28666&quot;&gt;a very long answer in a related thread&lt;/a&gt;, addressing your question almost exactly. I would kindly ask you to look there and see if that answers your question.&lt;/p&gt;&#10;&#10;&lt;p&gt;Very briefly, if we just focus on PCA and FA loadings $\mathbf W$, then the difference is that PCA finds $\mathbf W$ to reconstruct the sample covariance (or correlation) matrix $\mathbf C$ as close as possible: $$\mathbf C \approx \mathbf W \mathbf W^\top,$$ whereas FA finds $\mathbf W$ to reconstruct &lt;em&gt;the off-diagonal part&lt;/em&gt; of the covariance (or correlation) matrix only: $$\mathrm{offdiag}\{\mathbf C\} \approx \mathbf W \mathbf W^\top.$$ By this I mean that FA does not care what values $\mathbf W \mathbf W^\top$ has on the diagonal, it only cares about the off-diagonal part.&lt;/p&gt;&#10;&#10;&lt;p&gt;With this in mind, the answer to your question becomes easy to see. If the number $n$ of variables (size of $\mathbf C$) is large, then the off-diagonal part of $\mathbf C$ is almost the whole matrix (diagonal has size $n$ and the whole matrix size $n^2$, so the contribution of the diagonal is only $1/n \to 0$), and so we can expect that PCA approximates FA well. If the diagonal values are rather small, then again they don't play much role for PCA, and PCA ends up being close to FA, exactly as @ttnphns said above.&lt;/p&gt;&#10;&#10;&lt;p&gt;If, on the other hand, $\mathbf C$ is either small or strongly dominated by the diagonal (in particular if it has very different values on the diagonal), then PCA will have to bias $\mathbf W$ towards reproducing the diagonal as well, and so will end up being quite different from FA. One example is given in this thread:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/76543&quot;&gt;Why do PCA and Factor Analysis return different results in this example?&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="6" CreationDate="2014-12-04T09:59:35.340" Id="126584" LastActivityDate="2015-01-26T21:27:22.123" LastEditDate="2015-01-26T21:27:22.123" LastEditorUserId="28666" OwnerUserId="28666" ParentId="126556" PostTypeId="2" Score="4" />
  
  <row AcceptedAnswerId="126595" AnswerCount="1" Body="&lt;p&gt;I need to model an ARIMA with a time-series data. &#10;But my data is the statistics of land area, and it's annual data, so I have 64 points between 1950~2014. Because it increased by a stable rate, So I think maybe it can used to build ARIMA. &#10;Since I know the time series analysis may need as many observation as possible, so I'm little concerned about my data. But in other hand, the land area wouldn't change dramatically like interest rate, price. So I think may be I have enough observations for ARIMA modelling. So can anyone give some advice? &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-12-04T11:26:29.197" Id="126592" LastActivityDate="2014-12-04T12:58:18.030" LastEditDate="2014-12-04T12:58:18.030" LastEditorUserId="2116" OwnerUserId="61464" PostTypeId="1" Score="-1" Tags="&lt;time-series&gt;&lt;modeling&gt;&lt;arima&gt;" Title="How many observations do I need to implement ARIMA?" ViewCount="71" />
  
  <row AnswerCount="0" Body="&lt;p&gt;if an activation Function has an Non-Zero output for an input of zero f(0) != 0, does that, strictly speaking, mean that the corresponding neuron would have to output a value in every time step of the neural net Activation and not just when its layer is processed?&lt;/p&gt;&#10;&#10;&lt;p&gt;Please tell me if a different Q&amp;amp;A would be more suited for this question.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-04T12:09:59.190" Id="126597" LastActivityDate="2014-12-04T12:23:22.133" LastEditDate="2014-12-04T12:23:22.133" LastEditorUserId="62075" OwnerUserId="62075" PostTypeId="1" Score="0" Tags="&lt;neural-networks&gt;&lt;artificial-intelligence&gt;" Title="Artificial Neuron Activation" ViewCount="10" />
  <row Body="&lt;p&gt;Since you have time series data, your data is auto-correlated thus this means you have violated the independence assumption underlying the t test. I suggest that you build an ARMAX model at a higher level of aggregation say monthly. Use Intervention Detection schemes along with an ARIMA structure to identify to test for a possible mean shift or change in trend over the 48 months.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2014-12-04T13:03:39.647" Id="126599" LastActivityDate="2014-12-04T13:03:39.647" OwnerUserId="3382" ParentId="126596" PostTypeId="2" Score="0" />
  
  
  
  
  <row AcceptedAnswerId="126692" AnswerCount="1" Body="&lt;p&gt;So I ran in this problem: I have to test whether Empirical Rule is applicable. Proportions I got is 73%, 94,7% and 99.1% (within one, two and three standard deviations). I'm worried about 73%. This is quite far away from 68% (that the empirical rule states). I have more than 1500 random variables. Can I still state that Empirical Rule works here? What are the acceptable deviations?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-12-04T19:00:04.633" Id="126656" LastActivityDate="2014-12-05T20:34:33.013" LastEditDate="2014-12-04T19:01:37.040" LastEditorUserId="26338" OwnerUserId="62101" PostTypeId="1" Score="2" Tags="&lt;normal-distribution&gt;&lt;standard-deviation&gt;&lt;empirical&gt;" Title="Empirical Rule. Is it applicable/usable in this case?" ViewCount="87" />
  <row AcceptedAnswerId="126661" AnswerCount="1" Body="&lt;p&gt;I am trying to estimate the gender wage gap for male and female office workers in a large Swedish company to test whether there is gender discrimination. The Hausman test rejects the null that the individual fixed effects are random and therefore I cannot rely on pooled OLS or random effects. The problem is that I cannot keep my female dummy in a fixed effects regression because it is not varying over time.&lt;/p&gt;&#10;&#10;&lt;p&gt;I was suggested to use a Hausman test instead in order to test for discrimination but I really can’t see how this should be used to find a difference in earnings between male and female workers. I was hoping that maybe someone here would understand this advice a bit better. If so, could you please shed some light on this for me?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-04T19:06:58.450" FavoriteCount="1" Id="126659" LastActivityDate="2014-12-04T19:23:51.483" LastEditDate="2014-12-04T19:23:51.483" LastEditorUserId="22468" OwnerUserId="62103" PostTypeId="1" Score="7" Tags="&lt;econometrics&gt;&lt;panel-data&gt;&lt;random-effects-model&gt;&lt;fixed-effects-model&gt;&lt;hausman&gt;" Title="How to use the Hausman test for gender discrimination?" ViewCount="451" />
  
  <row Body="&lt;p&gt;To verify if you have trained your neural net correctly you can start by feeding the same input that you have used to train the network.&lt;/p&gt;&#10;&#10;&lt;p&gt;Eg. Training data&#10;Input 1:70 will have Output 1:70&#10;After you have trained your network use the same input instead of your training/test data and see if you get the same output. This is a good way to detect overfitting, if you feed in data that your network has never seen before and it performs badly. &lt;/p&gt;&#10;&#10;&lt;p&gt;Hope it helps!&#10;Good luck.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-12-04T19:46:12.000" Id="126668" LastActivityDate="2014-12-04T19:46:12.000" OwnerUserId="62106" ParentId="124960" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="134064" AnswerCount="1" Body="&lt;p&gt;Where in the literature is the inner product kernel $k(x,y) = (1+\epsilon)^{\langle x, y \rangle}$ mentioned? Does it have a name?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-04T22:29:49.893" Id="126683" LastActivityDate="2015-01-19T18:15:49.310" OwnerUserId="62114" PostTypeId="1" Score="1" Tags="&lt;kernel&gt;" Title="Inner Product Kernel $k(x,y) = (1+\epsilon)^{\langle x, y \rangle}$" ViewCount="32" />
  
  <row Body="&lt;p&gt;The answer lies in the $dt$ terms in Cox's equation 21, which your question omitted.&lt;/p&gt;&#10;&#10;&lt;p&gt;Cox says that in the continuous case equation 21, $$\frac{\lambda(t; z)\,dt}{1 - \lambda(t; z)\,dt} = e^{-z\beta} \frac{\lambda_0(t)\,dt}{1 - \lambda_0(t)\,dt},$$ reduces to equation 9, $$\lambda(t; z) = e^{z\beta} \lambda_0(t).$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Cox notes that &quot;in discrete time $\lambda_0(t; z) dt$ is a non-zero probability&quot;--in particular, it's the probability of an event occurring between $t$ and $t + dt$, given survival up to time $t$. The continuous-time case corresponds to the limit in which you consider infinitesimally small discrete time slices, that is, $dt \to 0$. In that case, the $\lambda(t; z)\,dt$ and $\lambda_0(t)\,dt$ in the denominators go to zero, so the denominators becomes 1, and then canceling the $dt$ in the numerator yields equation 9.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-05T03:39:40.500" Id="126700" LastActivityDate="2014-12-05T03:39:40.500" OwnerUserId="60642" ParentId="125783" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;So I have this table of statistics which I'll try to reproduce below.  It is meant to be a summary of a multivariate regression of &quot;PolityIVScore&quot; against log foreign aid and a dummy variable for whether a given country speaks English.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Foreign aid: 0.4317 &#10;(0.115)&#10;&#10;Speaks English: 4.650&#10;(0.292)&#10;&#10;Intercept: -4.562&#10;(2.296)&#10;&#10;n = 192&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Part of my uncertainty is how to read this summary--my guess is that the first number for each variable is the coefficient in the BLP, the &quot;Best Linear Predictor&quot;, an approximation of the conditional expectation function.  And my guess is that the number in parentheses is the standard error for the estimate of this coefficient.  If anyone knows whether that's the right or wrong interpretation, please let me know.&lt;/p&gt;&#10;&#10;&lt;p&gt;As for the problems, I am asked to first give a prediction for the polity score given a country has 10 units foreign aid and speaks English.  For that I computed $-4.562 + 0.4317(10) + 4.650(1)$ which I'm pretty sure is correct.  I'm then asked to give a 95% confidence interval for $\frac{\partial BLP}{\partial ForeignAid}$.  For this I think I compute it correctly by computing $0.4317 \pm z_{\alpha/2}(0.115)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The next part is where I'm a little uncertain--it asks me to find $\frac{\partial BLP}{\partial ForeignAid}$ when $ForeignAid=0$.  I would think that since this is a linear regression that the partial derivative would be constant and so the value of the variable wouldn't affect it.  &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-12-05T04:12:42.103" Id="126702" LastActivityDate="2014-12-05T04:21:07.900" LastEditDate="2014-12-05T04:21:07.900" LastEditorUserId="36041" OwnerUserId="59093" PostTypeId="1" Score="0" Tags="&lt;self-study&gt;&lt;confidence-interval&gt;&lt;multivariate-regression&gt;" Title="Getting a confidence interval for a predicted value in regression" ViewCount="19" />
  
  <row Body="&lt;p&gt;By the Law of Iterated Expectations (careful with parentheses)&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\Lambda_0 =  E\big[R_0 (x_i)' \Omega_0^{-1}(x_i) r(w_i, \theta_0)r(w_i, \theta_0)' \Omega_0^{-1}(x_i) R_0 (x_i) \big] $$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$=E\Big(E\big[R_0 (x_i)' \Omega_0^{-1}(x_i) r(w_i, \theta_0)r(w_i, \theta_0)' \Omega_0^{-1}(x_i) R_0 (x_i)\mid x_i \big] \Big)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Then &quot;take out what's known&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;$$=E\Big(R_0 (x_i)' \Omega_0^{-1}(x_i) \cdot E\big[r(w_i, \theta_0)r(w_i, \theta_0)' \mid x_i \big]\cdot \Omega_0^{-1}(x_i) R_0 (x_i)\Big)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and since as you too write,&#10;$$ \Omega_0 =  E\Big[r(w_i,\theta_0 )r(w_i,\theta_0 )'\mid x_i\Big]$$&lt;/p&gt;&#10;&#10;&lt;p&gt;substitute to get the result.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-12-05T10:55:12.350" Id="126728" LastActivityDate="2014-12-05T10:55:12.350" OwnerUserId="28746" ParentId="126707" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;Assume we have a cross-section of $N$ stocks. $Y_i$ is an sample variance estimate of stock returns for stock $i$. This sample variance is estimated using $T_i$ number of observations. All $T_i$ are not necessarily equal, i.e. the sample size for $Y$ estimation differ for i = 1,2,.., N.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now I want to run a cross-sectional weighted least squares regression:&lt;/p&gt;&#10;&#10;&lt;p&gt;$Y_i = \beta X_i + \epsilon_i$&lt;/p&gt;&#10;&#10;&lt;p&gt;What is the best choice of weights here, such that the weights are based on $T_i$ for each $Y_i$. In other words, I want to assign a smaller weight to stock $i$ if $T_i$ is small.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-05T14:56:46.743" Id="126761" LastActivityDate="2014-12-05T15:09:52.553" OwnerUserId="29183" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;least-squares&gt;&lt;weighted-regression&gt;&lt;generalized-least-squares&gt;" Title="Determine weights in weighted least squares regression" ViewCount="35" />
  
  <row Body="&lt;p&gt;&lt;strong&gt;Regression&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;First of all, for linear regression your variables doesn't have to be normally distributed. What you are interested in is if the &lt;em&gt;residuals&lt;/em&gt; are normally distributed. &lt;/p&gt;&#10;&#10;&lt;p&gt;Simple linear regression is defined as:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$y_i = \beta_0 + \beta_1x_i + \epsilon_i$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\beta_0$ is an intercept, $\beta_1$ is a slope and $\epsilon_i$ is a residual. The regression line is an expected value for $y_i$ observation being a linear function of the explanatory variable:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$E(y_i) = \beta_0 + \beta_1x_i$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and there is some variance ($\sigma$) of residuals around this value:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ y_i \sim Normal(\beta_0 + \beta_1x_i, \sigma)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The relation between your explanatory and the dependent variable has to be linear with residuals normally distributed and having a stable (i.e. homoscedastic) variance. Below you can see a diagram illustrating this (source &lt;a href=&quot;http://www.seaturtle.org/mtn/archives/mtn122/mtn122p1.shtml&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;). As you can see, it's not $X$ or $Y$ that are normally distributed, but the residuals centred around value estimated using the linear regression model.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/QeQQC.gif&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Normalization&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The second thing is normalization. We define it as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$z_i = \frac{x_i - \mu}{\sigma}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;so that the resulting variable $z$ has mean of 0 and standard deviation of 1. This &lt;em&gt;doesn't&lt;/em&gt; make this variable a normally distributed one. After transformation, it has a &lt;a href=&quot;http://en.wikipedia.org/wiki/Standard_score&quot; rel=&quot;nofollow&quot;&gt;standard scale&lt;/a&gt;, so if you use this kind of transformed variable you'll get a standardized regression estimates (estimates with mean 0 and standard deviation of 1, i.e. $z$-scores).&lt;/p&gt;&#10;&#10;&lt;p&gt;Below you can find an example of 1000 values drawn from uniform distribution (left side) and the same values &quot;normalized&quot; (right side). The distribution on the right side is not more &quot;normal&quot; than it was before, the only thing that changed is it's scale.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/z5LqD.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;So the only thing that changes in here is the scale of explanatory variable. The change is helpful because it enables you to get &quot;scale free&quot; estimates of regression parameters, what makes their interpretation easier because the regression parameters relate to the standard deviations. The same is with normalizing the dependent variable - the only thing that changes is scale, the data stays the same. This kind of data transformations are commonly used in multilevel modelling where &lt;a href=&quot;https://www.journalism.wisc.edu/~dshah/blog-club/Site/Hoffman.pdf&quot; rel=&quot;nofollow&quot;&gt;centering&lt;/a&gt; (i.e. subtracting the mean) is commonly used.&lt;/p&gt;&#10;&#10;&lt;p&gt;Check those &lt;a href=&quot;https://www.youtube.com/playlist?list=PL5-da3qGB5IDvuFPNoSqheihPOQNJpzyy&quot; rel=&quot;nofollow&quot;&gt;online lectures&lt;/a&gt; to find out more on linear regression.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-05T15:10:39.437" Id="126764" LastActivityDate="2014-12-05T17:12:54.283" LastEditDate="2014-12-05T17:12:54.283" LastEditorUserId="35989" OwnerUserId="35989" ParentId="126749" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;That would bias the model and invalidate statistical inference.  And I didn't see any polynomial terms in your model.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-12-05T16:32:56.677" Id="126775" LastActivityDate="2014-12-05T16:32:56.677" OwnerUserId="4253" ParentId="126772" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;A sequence of random variables (we need at least two!) can &lt;em&gt;fail&lt;/em&gt; to be iid (&lt;em&gt;independent and identically distributed&lt;/em&gt;) in three different ways: &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;The&#10;random variables are &lt;em&gt;not&lt;/em&gt; independent but they &lt;em&gt;are&lt;/em&gt; identically distributed,&lt;/li&gt;&#10;&lt;li&gt;The random variables &lt;em&gt;are&lt;/em&gt; independent but are &lt;em&gt;not&lt;/em&gt; identically distributed,&lt;/li&gt;&#10;&lt;li&gt;The random variables are neither independent nor identically distributed.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;For an example of 1., consider the &quot;sampling without replacement&quot; method&#10;described in &lt;a href=&quot;http://stats.stackexchange.com/a/17874/6633&quot;&gt;this answer&lt;/a&gt;. Or&#10;consider an experiment in which we have a  bag containing two coins with &#10;different probabilities $p_1$ and $p_2$ of turning up Heads. We choose one of the coins&#10;at random and toss the chosen coin $n$ times. Let $X_i$ be the indicator &#10;function of Heads on the $i$-th toss. Then, the law of total probability tells&#10;us that&#10;$$P\{X_i = 1\} = P\{X_i = 1\mid ~\text{coin #1}\}\times \frac 12&#10;+ P\{X_i = 1\mid ~\text{coin #2}\}\times \frac 12 = \frac{p_1+p_2}{2}.$$&#10;Thus, the $X_i$'s are identically distributed Bernoulli random variables&#10;with parameter $\frac{p_1+p_2}{2}$. &lt;em&gt;However&lt;/em&gt;, they are &lt;em&gt;not&lt;/em&gt; independent&#10;random variables since &#10;the law of total probability gives us that&#10;$$P\{X_i=1, X_j=1\}=\frac{P\{X_i=1, X_j=1 \mid~\text{#1}\}&#10;+ P\{X_i=1, X_j=1 \mid~\text{#2}\}}{2}&#10;= \frac{p_1^2+p_2^2}{2}$$&#10;which does not equal&#10;$P\{X_i=1\}P\{X_j=1\} = \left(\frac{p_1+p_2}{2}\right)^2$.&lt;/p&gt;&#10;&#10;&lt;p&gt;For an example of 2., consider a similar experiment in which the&#10;the $i$-th (independent toss) is of a coin that has probability $p_i$ &#10;different from $p_1, p_2, \ldots, p_{i-1}$. (Let $p_i = e^{-i}$ if&#10;you need an explicit example). Then the $X_i$ are independent&#10;Bernoulli random variables (by assumption)&#10;but they are &lt;em&gt;not&lt;/em&gt; identically distributed&#10;since they all have different parameters.&lt;/p&gt;&#10;&#10;&lt;p&gt;I will leave the exercise of coming up with an example of 3. to you. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-12-05T17:56:45.993" Id="126800" LastActivityDate="2014-12-05T17:56:45.993" OwnerUserId="6633" ParentId="126785" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;I believe that your calculation is figuring the probability not of being in the ith stage - but of being in one of the stages less than or equal to i.  To get the probabilities, subtract the value of the immediately preceding stage.  So - for example&lt;/p&gt;&#10;&#10;&lt;p&gt;P(1) is your calculated value minus your calculated P(0) value = .0866 - .0486 = .038 - which lines up with the R result to within rounding error.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-12-05T18:25:32.077" Id="126804" LastActivityDate="2014-12-05T18:25:32.077" OwnerUserId="59662" ParentId="126768" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;The difference between a stochastic process and a time series is somewhat like the difference between a cat on a keyboard and an answer on Stack Exchange: Cats on keyboards can produce answers, but cats on keyboards &lt;em&gt;are not&lt;/em&gt; answers. Furthermore, not every answer is produced by a cat on a keyboard.&lt;/p&gt;&#10;&#10;&lt;p&gt;A time series can be understood as a collection of time-value–data-point pairs. A stochastic process on the other hand is a mathematical model or a mathematical description of a distribution of time series¹. Some time series are a realisation of stochastic processes (of either kind). Or, from another point of view: I can use a stochastic process as a model to generate a time series.&lt;/p&gt;&#10;&#10;&lt;p&gt;Furthermore, time series can also be generated in other ways:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;They can be the result of observations and are thus generated by reality. While I can model reality as a stochastic process (I could also say that I regard reality as a stochastic process), reality &lt;em&gt;is not&lt;/em&gt; a stochastic process in the same way that the interior of a box is not a set of points (though we often regard the two equivalent in modelling contexts).&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;They can be generated by deterministic processes. Now, strictly speaking, we could (and arguably should) define stochastic processes and deterministic processes in a way that the latter are special cases of the former, but we very rarely make use of this and speaking of deterministic processes as special cases of stochastic processes may cause some confusion – you could compare it to calling $x=2$ a system of non-linear equations.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;sup&gt;¹ If it is a discrete-time stochastic process. Continuous-time stochastic process are distributions of functions rather than time series.&lt;/sup&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-12-05T21:41:31.270" Id="126823" LastActivityDate="2014-12-05T22:03:04.903" LastEditDate="2014-12-05T22:03:04.903" LastEditorUserId="36423" OwnerUserId="36423" ParentId="126791" PostTypeId="2" Score="0" />
  <row AnswerCount="2" Body="&lt;p&gt;In spatial data analysis, a simple way to model the covariance stucture between spatial observations is via a covariance function like $cov(y_i,y_j) = C e^{-rD_{ij}}$, based on some (euclidean) distance matrix $D$. Bayesian parameter estimation via MCMC would require repeatedly inverting the changing covariance matrix, which is computationally impossible for large N.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a kind of standard solution to this problem? Is it possible to directly parametrize the Cholesky decomposition in term of the parameters, to circumvent the matrix inversion?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-05T21:42:12.110" Id="126825" LastActivityDate="2014-12-07T07:39:18.953" OwnerUserId="44783" PostTypeId="1" Score="2" Tags="&lt;bayesian&gt;&lt;covariance&gt;&lt;spatial&gt;&lt;cholesky&gt;&lt;kriging&gt;" Title="Direct parametrization of Cholesky decomposition of spatial covariance matrix" ViewCount="35" />
  
  
  <row Body="&lt;p&gt;It changes the ordering as well, since the orientation of the hyperplane typically changes when you fiddle with the cost function. Translating the separating hyperplane in feature space does not induce changes to ROC curves, but rotating it does.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-12-05T22:33:36.243" Id="126834" LastActivityDate="2014-12-05T22:33:36.243" OwnerUserId="25433" ParentId="126826" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;In your model &lt;code&gt;Rotation&lt;/code&gt; is both a fixed effect slope and a random effect nested in &lt;code&gt;Block&lt;/code&gt; in here: &lt;code&gt;(1|Block/Rotation)&lt;/code&gt;. If you wanted &lt;code&gt;Rotation&lt;/code&gt; to be a random slope in &lt;code&gt;Block&lt;/code&gt; you would have to define it as &lt;code&gt;(1 + Rotation|Block)&lt;/code&gt;. What &lt;code&gt;(1|Block/Rotation)&lt;/code&gt; means is: &lt;code&gt;(1|Block) + (1|Block:Rotation)&lt;/code&gt;, i.e. a random intercept for &lt;code&gt;Block&lt;/code&gt; and a random intercept for &lt;code&gt;Rotation&lt;/code&gt; nested in &lt;code&gt;Block&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Check also &lt;a href=&quot;http://stackoverflow.com/questions/27306472/translating-proc-mixed-to-lmer-sas-to-r/27306613#27306613&quot;&gt;this post&lt;/a&gt; on SO or &lt;a href=&quot;http://stats.stackexchange.com/questions/126742/specification-of-mixed-model&quot;&gt;here&lt;/a&gt;. More information on defining models in lme4 you can find in an article by &lt;a href=&quot;http://arxiv.org/pdf/1406.5823.pdf&quot; rel=&quot;nofollow&quot;&gt;Bates et al. (in press)&lt;/a&gt; or in this &lt;a href=&quot;http://lme4.r-forge.r-project.org/lMMwR/lrgprt.pdf&quot; rel=&quot;nofollow&quot;&gt;online book&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Btw, why you don't want REML method for estimation?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-05T23:01:08.343" Id="126838" LastActivityDate="2014-12-05T23:01:08.343" OwnerUserId="35989" ParentId="126814" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;Normal is not a good prior for Gamma because shape parameters both have to be greater than zero. Maybe you could think of Log-Normal or truncated Normal? &lt;/p&gt;&#10;&#10;&lt;p&gt;I would imagine the model (in &lt;a href=&quot;http://mc-stan.org/&quot; rel=&quot;nofollow&quot;&gt;Stan&lt;/a&gt;) rather like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;data {...}&#10;parameters {...}&#10;model {&#10;   alpha ~ lognormal(...);&#10;   beta ~ lognormal(...);&#10;   lambda ~ gamma(alpha, beta);&#10;   p &amp;lt;- binomial(N, lambda * x);&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;so hyperpriors for $\lambda$'s are drawn from common distributions. If you want $\lambda$'s to have Gamma distributions, Normal (case &lt;em&gt;b&lt;/em&gt;) is not a choice for you. Gamma is also rather chosen as a prior for scale parameter in Normal distribution, then for location (case &lt;em&gt;b&lt;/em&gt;). So case &lt;em&gt;a&lt;/em&gt; looks better, but Normal distribution doesn't sound like a good prior for parameters for $\lambda$'s.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you provided more information or pasted your code maybe I could elaborate a little bit more.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-06T00:07:28.350" Id="126847" LastActivityDate="2014-12-06T00:07:28.350" OwnerUserId="35989" ParentId="126837" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;The order of a Markov chain is &lt;strong&gt;how far back in the history the transition probability distribution is allowed to depend on.&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;For a first-order Markov chain, the probability distribution of the next state can only depend on the current state. So your transition matrix will be 4x4, like so:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\left(\begin{array}{cccc}&#10;P(A \to A) &amp;amp; P(A \to C) &amp;amp; P(A \to T) &amp;amp; P(A \to G) \\&#10;P(C \to A) &amp;amp; P(C \to C) &amp;amp; P(C \to T) &amp;amp; P(C \to G) \\&#10;P(T \to A) &amp;amp; P(T \to C) &amp;amp; P(T \to T) &amp;amp; P(T \to G) \\&#10;P(G \to A) &amp;amp; P(G \to C) &amp;amp; P(G \to T) &amp;amp; P(G \to G) \\&#10;\end{array}\right)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;(where $P(A \to C)$ is the probability of seeing a $C$ if you just read an $A$, etc.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, if you go up to order 2, the probability distribution of the next state can depend on the previous &lt;em&gt;two&lt;/em&gt; states. So instead of just looking at $P(A \to C)$, we now need to look at $P(AA \to C), P(CA \to C), P(TA \to C), P(GA \to C)$. This will result in a $16 \times 16$ transition matrix that I don't really want to write out, but hopefully you get the idea.&lt;/p&gt;&#10;&#10;&lt;p&gt;As for drawing the state diagram, you can find some examples online--the basic idea is that you draw a circle for each state, an arrow between each two states with transitions, and label each arrow with the transition probability. Here's an example with four states (from &lt;a href=&quot;http://www.intechopen.com/books/matlab-a-fundamental-tool-for-scientific-computing-and-engineering-applications-volume-2/wireless-channel-model-with-markov-chains-using-matlab&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;):&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/VJInP.jpg&quot; alt=&quot;Transition diagram with four states&quot;&gt;&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-12-06T06:10:05.283" Id="126872" LastActivityDate="2014-12-06T06:10:05.283" OwnerUserId="60642" ParentId="126868" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="126960" AnswerCount="2" Body="&lt;p&gt;What are theoretical reasons to keep variables which coefficients are not significant?&lt;/p&gt;&#10;&#10;&lt;p&gt;I have several coefficients with p &gt; 0.05.   What's causing large p values?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-12-06T11:41:14.240" FavoriteCount="1" Id="126889" LastActivityDate="2014-12-07T15:54:40.117" OwnerUserId="43204" PostTypeId="1" Score="0" Tags="&lt;regression-coefficients&gt;" Title="Regression coefficients significance" ViewCount="86" />
  <row AnswerCount="2" Body="&lt;p&gt;In my data set I have two variables: month (March or April) and wind direction (N, NE, E, SE, S, SW, W, NW). I have to know if there is a difference in wind direction between these months. What kind of test statistic should I use? (I work in R.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's some more information: &#10;The data is from a study that examines the extent to which background noise in a certain neighborhood depends on the time and certain weather conditions.&#10;The question is: Determine the dominant wind direction in the month &#10;March and April? Is there a difference in wind direction between these months? &#10;For the first question, I thought I just needed to look at &lt;code&gt;table(month, wind direction)&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;The data consist of 2357 measurements of eight variables (month, day, hour, weekday, background noise, wind direction, windspeed and wind angles) that were registered every hour of 50 days in March and April 2010 in the backyard of a particular house. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://r.789695.n4.nabble.com/attachment/4700349/0/achtergrondlawaai.txt&quot; rel=&quot;nofollow&quot;&gt;http://r.789695.n4.nabble.com/attachment/4700349/0/achtergrondlawaai.txt&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="15" CreationDate="2014-12-06T15:47:17.413" Id="126907" LastActivityDate="2014-12-07T10:33:26.780" LastEditDate="2014-12-07T10:33:26.780" LastEditorUserId="62220" OwnerUserId="62220" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;self-study&gt;" Title="Difference between two variables" ViewCount="142" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I have two models, one restricted, the other full. Is there a test-statistic decision criteria I could use to make a decision as to which model might be better?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-12-06T17:34:27.520" Id="126919" LastActivityDate="2014-12-08T00:49:14.173" LastEditDate="2014-12-08T00:49:14.173" LastEditorUserId="62224" OwnerUserId="62224" PostTypeId="1" Score="0" Tags="&lt;self-study&gt;&lt;model-comparison&gt;" Title="Choosing the best multiple regression model" ViewCount="48" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am not a statistician; I'm an engineer, so much of this is a foreign language to me, though it seems like it wouldn't be that hard to understand if explained in a different way.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am trying to learn how to combine lots of paired comparisons of items into a unified ranking of the items.  I can understand things like &lt;a href=&quot;http://en.wikipedia.org/wiki/Elo_rating_system&quot; rel=&quot;nofollow&quot;&gt;Elo&lt;/a&gt; and &lt;a href=&quot;http://en.wikipedia.org/wiki/Glicko_rating_system&quot; rel=&quot;nofollow&quot;&gt;Glicko&lt;/a&gt;, but the &lt;a href=&quot;http://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model&quot; rel=&quot;nofollow&quot;&gt;Bradley-Terry model&lt;/a&gt; is supposed to be the more accurate, more general method that they are based on, so I am trying to learn that.  I've been reading many different references and I just get lost trying to get an intuitive understanding of it.&lt;/p&gt;&#10;&#10;&lt;p&gt;If it's possible to &lt;em&gt;Explain Like I'm 5&lt;/em&gt; the Bradley-Terry model in a single answer (preferably using more plots and graphs than equations), please do that.  If that's too much for a single answer then I will ask a more specific question:&lt;/p&gt;&#10;&#10;&lt;p&gt;This modeling is based around &quot;&lt;a href=&quot;http://en.wikipedia.org/wiki/Logistic_regression&quot; rel=&quot;nofollow&quot;&gt;logistic regression&lt;/a&gt;&quot;.  To me, that sounds like finding the best fit for a &lt;a href=&quot;http://en.wikipedia.org/wiki/Logistic_function&quot; rel=&quot;nofollow&quot;&gt;logistic function&lt;/a&gt; to some data points?  Is that correct?  If so, what are the data points in this context, and what do we do with the fit information after we have it?&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-12-06T20:22:40.517" Id="126932" LastActivityDate="2014-12-06T20:22:40.517" OwnerUserId="11633" PostTypeId="1" Score="0" Tags="&lt;logistic&gt;&lt;paired-comparisons&gt;&lt;intuition&gt;&lt;bradley-terry-model&gt;" Title="Logistic regression for paired comparisons" ViewCount="43" />
  
  <row Body="&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Random variables would be identically distributed if they have the same distribution function (if their distribution has the same shape). But that doesn't imply independence. &lt;/p&gt;&#10;&#10;&lt;p&gt;So a pair of correlated random variables could have the same marginal distribution function, but are not independent.&lt;/p&gt;&#10;&#10;&lt;p&gt;The two sets of random variables $X_i$ and $Y_i$ (which &lt;code&gt;x1&lt;/code&gt; and &lt;code&gt;y1&lt;/code&gt; below are samples from) have the same marginal distribution, but are not independent:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/vWKxJ.png&quot; alt=&quot;plot of identically distributed, not independent; note the symmetry about y=x&quot;&gt;&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Random variables would be independent if the conditional distribution (&quot;distribution of one when you know the other&quot;) would be the same as the marginal distribution (&quot;distribution of one without any information on the other&quot;). That doesn't imply identically distributed.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, if I pour some sugar into a cup and roll a die on the table, whether the number of grains of sugar is odd or even and the outcome on the die would presumably be independent.&lt;/p&gt;&#10;&#10;&lt;p&gt;These two samples below are drawn from different distributions, but are drawn independently. If you drew a new pair, and knew that say the observed value of $W$ was between 0.05 and 0.10, it would tell you nothing further about what the corresponding $Y$ was than you already knew.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/qf7pJ.png&quot; alt=&quot;independent, not identically distributed; the shape of the density of y looks the same at every value of w&quot;&gt;&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2014-12-06T22:22:31.180" Id="126946" LastActivityDate="2014-12-06T22:34:19.833" LastEditDate="2014-12-06T22:34:19.833" LastEditorUserId="805" OwnerUserId="805" ParentId="126892" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;But if you do need to do Box-Cox, it is available in the automatic data prep procedure (you can turn all the other parts off).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-06T22:49:56.110" Id="126949" LastActivityDate="2014-12-06T22:49:56.110" OwnerUserId="6768" ParentId="126781" PostTypeId="2" Score="0" />
  
  
  <row Body="&lt;p&gt;This can be shown by a classical method used everywhere related to least squares estimation and conditional expectation. Let $f(x) = E(Y|X = x)$, then write:&#10;$$E_{Y|X}[(Y - c)^2|X = x] = E_{Y|X}[(Y - f(x) + f(x) - c)^2|X = x]$$&#10;Expand the complete square and show that the cross product term is 0 as follows:&#10;$$E_{Y|X}[(Y - f(x))(f(x) - c)|X = x] = (f(x) - c)E_{Y|X}[Y - f(x)|X = x] = (f(x) - c)(f(x) - f(x)) = 0$$&#10;where the first equality follows from $f(x) - c$ is a function of $x$ (technically, $\sigma(X)$-measurable) thus can be taken out from the conditional expectation. The second equality holds due to the linearity of expectation and our definition of $f(x)$. Therefore,&#10;$$E_{Y|X}[(Y - c)^2|X = x] = E_{Y|X}[(Y - f(x) + f(x) - c)^2|X = x]&#10;= E_{Y|X}[(Y - f(x))^2|X = x] + (f(x) - c)^2 \geq E_{Y|X}[(Y - f(x))^2|X = x]$$&#10;And the equality can be attained by taking $c = f(x)$, which is the solution.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-12-07T00:09:43.863" Id="126957" LastActivityDate="2014-12-07T00:09:43.863" OwnerUserId="20519" ParentId="126956" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;Let $\hat{f}(x_i)=\hat{y}_i$, then&#10;$$&#10;\omega = E_\boldsymbol{y}[op]&#10;=E_\boldsymbol{y}[Err_{in}-\overline{err}]&#10;=E_\boldsymbol{y}[Err_{in}]-E_\boldsymbol{y}[\overline{err}]&#10;=E_\boldsymbol{y}[\frac{1}{N}\sum_{i=1}^{N}E_{Y^0}[L(Y_i^0,\hat{f}(x_i))]-E_\boldsymbol{y}[\frac{1}{N}\sum_{i=1}^{N}L(y_i,\hat{f}(x_i))]&#10;=\frac{1}{N}\sum_{i=1}^{N}E_\boldsymbol{y}E_{Y^0}[(Y_i^0-\hat{y}_i)^2]-E_\boldsymbol{y}[(y_i-\hat{y}_i)^2]&#10;=\frac{1}{N}\sum_{i=1}^{N}E_\boldsymbol{y}E_{Y^0}[({Y_i^0})^2]+E_\boldsymbol{y}E_{Y^0}[{\hat{y}_i}^2]-2E_\boldsymbol{y}E_{Y^0}[Y_i^0\hat{y}_i]-E_\boldsymbol{y}[y_i^2]-E_\boldsymbol{y}[\hat{y}_i^2]+2E_\boldsymbol{y}[y_i\hat{y}_i]&#10;=\frac{1}{N}\sum_{i=1}^{N}E_\boldsymbol{y}[y_i^2]+E_\boldsymbol{y}[\hat{y}_i^2]-2E_\boldsymbol{y}[y_i]E_\boldsymbol{y}[\hat{y}_i]-E_\boldsymbol{y}[y_i^2]-E_\boldsymbol{y}[\hat{y}_i^2]+2E_\boldsymbol{y}[y_i\hat{y}_i]&#10;=\frac{2}{N}\sum_{i=1}^{N}E_\boldsymbol{y}[y_i\hat{y}_i]-E_\boldsymbol{y}[y_i]E_\boldsymbol{y}[\hat{y}_i]&#10;=\frac{2}{N}\sum_{i=1}^{N}E_\boldsymbol{y}[y_i\hat{y}_i-y_iE_\boldsymbol{y}[\hat{y}_i]-E_\boldsymbol{y}[y_i]\hat{y}_i+E_\boldsymbol{y}[y_i]E_\boldsymbol{y}[\hat{y}_i]]&#10;=\frac{2}{N}\sum_{i=1}^{N}E_\boldsymbol{y}[(\hat{y}_i-E_\boldsymbol{y}[\hat{y}_i])([y_i-E_\boldsymbol{y}[y_i])]&#10;=\frac{2}{N}\sum_{i=1}^{N}cov(\hat{y}_i,y_i)&#10;$$ Q.E.D.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-07T02:31:02.247" Id="126971" LastActivityDate="2014-12-07T02:31:02.247" OwnerUserId="62238" ParentId="88912" PostTypeId="2" Score="0" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/74QB3.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;How to analyze above the data to predict the probability that people have disease with a model? Factors thought to influence infection include city, age, and diet.&#10;BUT, I don't know how to do because the response variable is not numeric.&#10;I tried to fit following model.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;fm1 &amp;lt;- lm(disease ~ city, data=Data1)&#10;fm2 &amp;lt;- lm(disease ~ age, data=Data1)&#10;fm3 &amp;lt;- lm(disease ~ city + age + diet, data=Data1)&#10;fm4 &amp;lt;- lm(disease ~ age + I(age^2) + city*diet, data=Data1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;but they couldn't be worked because the response variable is not numeric.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-12-07T08:58:07.370" Id="126996" LastActivityDate="2014-12-07T23:46:27.707" LastEditDate="2014-12-07T23:46:27.707" LastEditorUserId="22311" OwnerUserId="62249" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;generalized-linear-model&gt;&lt;modeling&gt;&lt;model-selection&gt;&lt;multicollinearity&gt;" Title="what if response variable is 'yes or no' in R?" ViewCount="189" />
  
  
  <row Body="&lt;p&gt;As a Bayesian, it does not make much of a difference to me to think of $\mathcal{P}(\lambda)$ as a given distribution, indexed by a parameter $\lambda$, or as a conditional distribution, conditional on the realisation of a random variable $\lambda$. Indeed, in either case, when I observe &#10;$$&#10;y_1,\ldots,y_n\stackrel{\text{iid}}{\sim}\mathcal{P}(\lambda)&#10;$$&#10;the data is indexed by a &lt;em&gt;given&lt;/em&gt; if unknown value of $\lambda$. Whether or not this $\lambda$ is the realisation of a random variable does not change the behaviour of the data. Remember, there is only &lt;em&gt;one&lt;/em&gt; realisation of $\lambda$ for a given dataset, no matter its size. So, even when making the assumption that $\lambda\sim\pi(\lambda)$ (a certain prior distribution chosen by me), I do not get observations from the marginal&#10;$$&#10;m(y_i) = \int_0^\infty f(y_i|\lambda)\pi(\lambda)\,\text{d}\lambda&#10;$$&#10;but observations from the conditional. Hence, for the likelihood function and related inference, conditioning or not does not make a difference as the parameter is assumed fixed for the data at hand.&lt;/p&gt;&#10;&#10;&lt;p&gt;Conditioning only makes a difference when running a Bayesian analysis, since the posterior&#10;$$&#10;\pi(\lambda|y_1,\ldots,y_n) \propto f(y_1|\lambda)\cdots f(y_n|\lambda)\pi(\lambda)&#10;$$&#10;only makes sense as a conditional distribution from the joint distribution&#10;$$&#10;\pi(\lambda,y_1,\ldots,y_n) = f(y_1|\lambda)\cdots f(y_n|\lambda)\pi(\lambda)&#10;$$&#10;(meaning that $\lambda$ &lt;em&gt;has to&lt;/em&gt; be a random variable for this derivation to make sense).&lt;/p&gt;&#10;&#10;&lt;p&gt;At a probabilistic level, assuming the $\sigma$-algebra on $\mathcal{Y}\times\Theta$ is induced by the products of the measurable sets on $\mathcal{Y}$ and on $\Theta$, writing $p_\lambda(y)$ or $p(y|\lambda)$ does not make a difference either.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-07T14:06:35.990" Id="127012" LastActivityDate="2014-12-07T17:52:11.727" LastEditDate="2014-12-07T17:52:11.727" LastEditorUserId="7224" OwnerUserId="7224" ParentId="126921" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Since &#10;$$\operatorname{cov}(U,V)=a^2\operatorname{cov}(Z,Y)+ab\operatorname{cov}(Z,X)+ab\operatorname{cov}(Y,Y)+b^2\operatorname{cov}(Y,X)=ab\operatorname{var}(Y)\geq 0,$$&#10;$U$ and $V$ are &lt;strong&gt;dependent&lt;/strong&gt; if &#10;$\operatorname{cov}(U,V) =ab\operatorname{var}(Y) &amp;gt; 0$.&lt;/p&gt;&#10;&#10;&lt;p&gt;If $\operatorname{cov}(U,V) = 0$, it must be that&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;$Y$ is a degenerate random variable (a.k.a. a constant a.s.) so that $U$ and $V$ are essentially scaled versions of independent random variables $Z$ and $X$ and so $U$ and $V$ are independent.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;or&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;at least one of $a$ and $b$ is zero and so once again $U$ and $V$ are scaled&#10;versions of independent random variables and therefore are independent.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;I don't know how you decided via moment-generating functions that&#10;$U$ and $V$ are independent, but the calculations must be incorrect.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-12-07T14:49:36.173" Id="127016" LastActivityDate="2014-12-07T14:49:36.173" OwnerUserId="6633" ParentId="127013" PostTypeId="2" Score="1" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have treated and control groups with a problem of selection in the treatment group. &lt;/p&gt;&#10;&#10;&lt;p&gt;I am interested in the identification of the following model: $y= exp(X^\prime\beta + \alpha\cdot T)$ where $T$ denotes the assignment to the treatment. &lt;/p&gt;&#10;&#10;&lt;p&gt;To address the problem of selection, I was planning to estimate the propensity score, i.e., $P(T=1|X)$, in a separate regression and use the predicted values as the only control in the regression (eventually as a set of dummies for its quantiles). &lt;/p&gt;&#10;&#10;&lt;p&gt;Would this strategy work? Or should I rather rely on more complicated matching estimators?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-07T15:42:38.653" FavoriteCount="0" Id="127022" LastActivityDate="2014-12-09T07:12:22.760" LastEditDate="2014-12-09T07:12:22.760" LastEditorUserId="26338" OwnerUserId="46789" PostTypeId="1" Score="3" Tags="&lt;regression&gt;&lt;causal-inference&gt;&lt;propensity-scores&gt;&lt;treatment-effect&gt;" Title="Propensity Score can be used as a covariate in regression?" ViewCount="91" />
  
  <row Body="&lt;p&gt;As @amoeba mentioned in the comments, PCA will only look at one set of data and it will show you the major (linear) patterns of variation in those variables, the correlations or covariances between those variables, and the relationships between samples (the rows) in your data set.&lt;/p&gt;&#10;&#10;&lt;p&gt;What one normally does with a species data set and a suite of potential explanatory variables is to fit a constrained ordination. In PCA, the principal components, the axes on the PCA biplot, are derived as optimal linear combinations of all variables. If you ran this on a data set of soil chemistry with variables pH, $\mathrm{Ca^{2+}}$, TotalCarbon, you might find that the first component was&lt;/p&gt;&#10;&#10;&lt;p&gt;$$0.5 \times \mathrm{pH} + 1.4 \times \mathrm{Ca^{2+}} + 0.1 \times \mathrm{TotalCarbon} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;and the second component&lt;/p&gt;&#10;&#10;&lt;p&gt;$$2.7 \times \mathrm{pH} + 0.3 \times \mathrm{Ca^{2+}} - 5.6 \times \mathrm{TotalCarbon} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;These components are freely selectable from the variables measured, and which get chosen are those that explain sequentially the largest amount of variation in the dataset, and that each linear combination is orthogonal (uncorrelated with) the others.&lt;/p&gt;&#10;&#10;&lt;p&gt;In a constrained ordination, we have two datasets, but we are not free to select whatever linear combinations of the first data set (the soil chem data above) we want. Instead we have to select linear combinations of the variables in the second data set that best explain variation in the first. Also, in the case of PCA, the one data set is the response matrix and there are no predictors (you could think of the response as predicting itself). In the constrained case, we have a response data set which we wish to explain with a set of explanatory variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;Although you haven't explained which variables are the response, normally one wishes to explain variation in the abundances or composition of those species (i.e. the responses) using the environmental explanatory variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;The constrained version of PCA is a thing called Redundancy Analysis (RDA) in ecological circles. This assumes an underlying linear response model for the species, which is either not appropriate or only appropriate if you have short gradients along which the species respond.&lt;/p&gt;&#10;&#10;&lt;p&gt;An alternative to PCA is a thing called correspondence analysis (CA). This is unconstrained but it does have an underlying unimodal response model, which is somewhat more realistic in terms of how species respond along longer gradients. Note also that CA models &lt;strong&gt;relative abundances or composition&lt;/strong&gt;, PCA models the raw abundances.&lt;/p&gt;&#10;&#10;&lt;p&gt;There is a constrained version of CA, known as &lt;em&gt;constrained&lt;/em&gt; or &lt;em&gt;canonical correspondence analysis&lt;/em&gt; (CCA) - not to be confused with a more formal statistical model known as canonical correlation analysis.&lt;/p&gt;&#10;&#10;&lt;p&gt;In both RDA and CCA the aim is to model the variation in species abundances or composition as a series of linear combinations of the explanatory variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;From the description it sounds like your wife wants to explain variation in the millipede species composition (or abundance) in terms of the other variables measured.&lt;/p&gt;&#10;&#10;&lt;p&gt;Some words of warning; RDA and CCA are just multivariate regressions; CCA is just a weighted multivariate regression. Anything you've learned about regression applies, and there are a couple of other gotchas too:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;as you increase the number of explanatory variable, the constraints actually become less and less and you aren't really extracting components/axes that explain the species composition optimally, and&lt;/li&gt;&#10;&lt;li&gt;with CCA, as you increase the number of explanatory factors, you risk inducing an artefact of a curve into the configuration of points in the CCA plot.&lt;/li&gt;&#10;&lt;li&gt;the theory underlying RDA and CCA are less well-developed than more formal statistical methods. We can only reasonably choose which explanatory variables to keep using step-wise selection (which is not ideal for all the reasons we don't like it as a selection method in regression) and we have to use permutation tests to do so. &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;so my advice is the same as with regression; think ahead of time what your hypotheses are and include variables that reflect those hypotheses. &lt;em&gt;Don't&lt;/em&gt; just throw all explanatory variables into the mix.&lt;/p&gt;&#10;&#10;&lt;h1&gt;Example&lt;/h1&gt;&#10;&#10;&lt;h2&gt;Unconstrained ordination&lt;/h2&gt;&#10;&#10;&lt;h3&gt;PCA&lt;/h3&gt;&#10;&#10;&lt;p&gt;I'll show an example comparing PCA, CA and CCA using the &lt;strong&gt;vegan&lt;/strong&gt; package for R which I help maintain and which is designed to fit these kinds of ordination methods:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(&quot;vegan&quot;)                        # load the package&#10;data(varespec)                          # load example data&#10;&#10;## PCA&#10;pcfit &amp;lt;- rda(varespec)&#10;## could add `scale = TRUE` if variables in different units&#10;pcfit&#10;&#10;&amp;gt; pcfit&#10;Call: rda(X = varespec)&#10;&#10;              Inertia Rank&#10;Total            1826     &#10;Unconstrained    1826   23&#10;Inertia is variance &#10;&#10;Eigenvalues for unconstrained axes:&#10;  PC1   PC2   PC3   PC4   PC5   PC6   PC7   PC8 &#10;983.0 464.3 132.3  73.9  48.4  37.0  25.7  19.7 &#10;(Showed only 8 of all 23 unconstrained eigenvalues)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;vegan&lt;/strong&gt; doesn't standardise the Inertia, unlike Canoco, so the total variance is 1826 and the Eigenvalues are in those same units and sum to 1826&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; cumsum(eigenvals(pcfit))&#10;      PC1       PC2       PC3       PC4       PC5       PC6       PC7       PC8 &#10; 982.9788 1447.2829 1579.5334 1653.4670 1701.8853 1738.8947 1764.6209 1784.3265 &#10;      PC9      PC10      PC11      PC12      PC13      PC14      PC15      PC16 &#10;1796.6007 1807.0361 1816.3869 1819.1853 1821.5128 1822.9045 1824.1103 1824.9250 &#10;     PC17      PC18      PC19      PC20      PC21      PC22      PC23 &#10;1825.2563 1825.4429 1825.5495 1825.6131 1825.6383 1825.6548 1825.6594&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;We also see that the first Eigenvalue is about half the variance and with the first two axes we have explained ~80% of the total variance&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; head(cumsum(eigenvals(pcfit)) / pcfit$tot.chi)&#10;      PC1       PC2       PC3       PC4       PC5       PC6 &#10;0.5384240 0.7927453 0.8651851 0.9056821 0.9322031 0.9524749&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;A biplot can be drawn from the scores of the samples and species on the first two principal components&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; plot(pcfit)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/5UfwM.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;There are two issues here&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;The ordination is essentially dominated by three species &amp;mdash; these species lie farthest from the origin &amp;mdash; as these are the most abundant taxa in the data set&lt;/li&gt;&#10;&lt;li&gt;There is a strong arch of curve in the ordination, suggestive of a long or dominant single gradient that has been broken down into the two main principal components to maintain the metric properties of the ordination.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;h3&gt;CA&lt;/h3&gt;&#10;&#10;&lt;p&gt;A CA might assist with both these points as it handles long gradient better due to the unimodal response model, and it models relative composition of species not raw abundances.&lt;/p&gt;&#10;&#10;&lt;p&gt;The &lt;strong&gt;vegan / R&lt;/strong&gt; code to do this is similar to the PCA code used above&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;cafit &amp;lt;- cca(varespec)&#10;cafit&#10;&#10;&amp;gt; cafit &amp;lt;- cca(varespec)&#10;&amp;gt; cafit&#10;Call: cca(X = varespec)&#10;&#10;              Inertia Rank&#10;Total           2.083     &#10;Unconstrained   2.083   23&#10;Inertia is mean squared contingency coefficient &#10;&#10;Eigenvalues for unconstrained axes:&#10;   CA1    CA2    CA3    CA4    CA5    CA6    CA7    CA8 &#10;0.5249 0.3568 0.2344 0.1955 0.1776 0.1216 0.1155 0.0889 &#10;(Showed only 8 of all 23 unconstrained eigenvalues) &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Here we explain about 40% of the variation among sites in their relative composition&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; head(cumsum(eigenvals(cafit)) / cafit$tot.chi)&#10;      CA1       CA2       CA3       CA4       CA5       CA6 &#10;0.2519837 0.4232578 0.5357951 0.6296236 0.7148866 0.7732393&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The joint plot of the species and site scores is now less dominated by a few species&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; plot(cafit)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/q2zH1.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Which of PCA or CA you choose should be determined by the questions you wish to ask of the data. Usually with species data we are more often interested in difference in the suite of species so CA is a popular choice. If we have a data set of environmental variables, say water or soil chemistry, we wouldn't expect those to respond in a unimodal manner along gradients so CA would be inappropriate and PCA (of a correlation matrix, using &lt;code&gt;scale = TRUE&lt;/code&gt; in the &lt;code&gt;rda()&lt;/code&gt; call) would be more appropriate.&lt;/p&gt;&#10;&#10;&lt;h2&gt;Constrained ordination; CCA&lt;/h2&gt;&#10;&#10;&lt;p&gt;Now if we have second set of data which we wish to use to explain patterns in the first species data set, we must use a constrained ordination. Often the choice here is CCA, but RDA is an alternative, as is RDA after transformation of the data to allow it to handle species data better.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;data(varechem)                          # load explanatory example data&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;We re-use the &lt;code&gt;cca()&lt;/code&gt; function but we either supply two data frames (&lt;code&gt;X&lt;/code&gt; for species, and &lt;code&gt;Y&lt;/code&gt; for explanatory/predictor variables) or a model formula listing the form of the model we wish to fit.&lt;/p&gt;&#10;&#10;&lt;p&gt;To include all variables we could use &lt;code&gt;varechem ~ ., data = varechem&lt;/code&gt; as the formula to include all variables &amp;mdash; but as I said above, this isn't a good idea in general&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ccafit &amp;lt;- cca(varespec ~ ., data = varechem)&#10;&#10;&amp;gt; ccafit&#10;Call: cca(formula = varespec ~ N + P + K + Ca + Mg + S + Al + Fe + Mn +&#10;Zn + Mo + Baresoil + Humdepth + pH, data = varechem)&#10;&#10;              Inertia Proportion Rank&#10;Total          2.0832     1.0000     &#10;Constrained    1.4415     0.6920   14&#10;Unconstrained  0.6417     0.3080    9&#10;Inertia is mean squared contingency coefficient &#10;&#10;Eigenvalues for constrained axes:&#10;  CCA1   CCA2   CCA3   CCA4   CCA5   CCA6   CCA7   CCA8   CCA9  CCA10  CCA11 &#10;0.4389 0.2918 0.1628 0.1421 0.1180 0.0890 0.0703 0.0584 0.0311 0.0133 0.0084 &#10; CCA12  CCA13  CCA14 &#10;0.0065 0.0062 0.0047 &#10;&#10;Eigenvalues for unconstrained axes:&#10;    CA1     CA2     CA3     CA4     CA5     CA6     CA7     CA8     CA9 &#10;0.19776 0.14193 0.10117 0.07079 0.05330 0.03330 0.01887 0.01510 0.00949&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The triplot of the above ordination is produced using the &lt;code&gt;plot()&lt;/code&gt; method&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; plot(ccafit)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/yYsmh.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Of course, now the task is to work out which of those variables is actually important. Also note that we have explained about 2/3 of the species variance using just 13 variables. one of the problems of using all variables in this ordination is that we've created an arched configuration in sample and species scores, which is purely an artefact of using too-many correlated variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you want to know more about this, check out the &lt;strong&gt;vegan&lt;/strong&gt; documentation or a good book on multivariate ecological data analysis.&lt;/p&gt;&#10;&#10;&lt;h2&gt;Relationship with regression&lt;/h2&gt;&#10;&#10;&lt;p&gt;It is simplest to illustrate the link with RDA, but CCA is just the same except everything involves row and column two-way-table marginal sums as weights.&lt;/p&gt;&#10;&#10;&lt;p&gt;At it's heart, RDA is equivalent to the application of PCA to a matrix of fitted values from a multiple linear regression fitted to each species (response) values (abundances, say) with predictors given by the matrix of explanatory variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;In R we can do this as&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;## centre the responses&#10;spp &amp;lt;- scale(data.matrix(varespec), center = TRUE, scale = FALSE)&#10;## ...and the predictors&#10;env &amp;lt;- as.data.frame(scale(varechem, center = TRUE, scale = FALSE))&#10;&#10;## fit a linear model to each column (species) in spp.&#10;## Suppress intercept as we've centred everything&#10;fit &amp;lt;- lm(spp ~ . - 1, data = env)&#10;&#10;## Collect fitted values for each species and do a PCA of that&#10;## matrix&#10;pclmfit &amp;lt;- prcomp(fitted(fit))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The Eigenvalues for these two approaches are equal:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; (eig1 &amp;lt;- unclass(unname(eigenvals(pclmfit)[1:14])))&#10; [1] 820.1042107 399.2847431 102.5616781  47.6316940  26.8382218  24.0480875&#10; [7]  19.0643756  10.1669954   4.4287860   2.2720357   1.5353257   0.9255277&#10;[13]   0.7155102   0.3118612&#10;&amp;gt; (eig2 &amp;lt;- unclass(unname(eigenvals(rdafit, constrained = TRUE))))&#10; [1] 820.1042107 399.2847431 102.5616781  47.6316940  26.8382218  24.0480875&#10; [7]  19.0643756  10.1669954   4.4287860   2.2720357   1.5353257   0.9255277&#10;[13]   0.7155102   0.3118612&#10;&amp;gt; all.equal(eig1, eig2)&#10;[1] TRUE&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;For some reason I can't get the axis scores (loadings) to match, but invariably these are scaled (or not) so I need to look into exactly how those are being done here.&lt;/p&gt;&#10;&#10;&lt;p&gt;We don't do the RDA via &lt;code&gt;rda()&lt;/code&gt; as I showed with &lt;code&gt;lm()&lt;/code&gt; etc, but we use a QR decomposition for the linear model part and then SVD for the PCA part. But the essential steps are the same.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-12-07T17:20:00.443" Id="127034" LastActivityDate="2014-12-10T02:39:11.253" LastEditDate="2014-12-10T02:39:11.253" LastEditorUserId="1390" OwnerUserId="1390" ParentId="126864" PostTypeId="2" Score="13" />
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to understand when it is appropriate to use the MAP and when MRR should be used. I found &lt;a href=&quot;https://dibt.unimol.it/TAinSM2012/slides/dawn.pdf&quot; rel=&quot;nofollow&quot;&gt;this&lt;/a&gt; presentation that states that MRR is best utilised when the number of relevant results is less than 5 and best when it is 1. In other cases MAP is appropriate. I have two questions:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;I don't really understand why this is so.&lt;/li&gt;&#10;&lt;li&gt;I can't find a citable reference for this claim.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Please note that I don't have a very strong statistical background so a layman's explanation would help a lot. Thank you.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-07T18:26:12.743" Id="127041" LastActivityDate="2014-12-07T18:26:12.743" OwnerUserId="62275" PostTypeId="1" Score="0" Tags="&lt;information-theory&gt;&lt;average-precision&gt;" Title="Mean Average Precision vs Mean Reciprocal Rank" ViewCount="25" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;If for both the treatment and control groups the correlation between the covariate x and the dependent variable y is substantial but negative, the error variance for ANCOVA as compared to ANOVA (less, more or about the same)? I wouldn't think that the direction of the correlation would matter but I am thrown by the wording of the &quot;about the same&quot;.  Maybe the error variance between ANOVA and ANCOVA is what I don't understand.  Any help would be much appreciated!&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-12-07T20:13:24.190" Id="127049" LastActivityDate="2014-12-07T20:44:33.290" LastEditDate="2014-12-07T20:44:33.290" LastEditorUserId="62278" OwnerUserId="62278" PostTypeId="1" Score="1" Tags="&lt;self-study&gt;&lt;ancova&gt;" Title="error variance in ANCOVA vs ANOVA" ViewCount="17" />
  <row Body="&lt;p&gt;A model distribution and a prior distribution are defined by either their pdf or their cdf. So your dataset is an exponential sample with parameter $\lambda$, which means that&#10;$$&#10;F_\lambda(x)=1-e^{-\lambda x}\ \text{ and }\ f_\lambda(x)=\lambda e^{-x\lambda}&#10;$$&#10;are the cdf and pdf for this model. Therefore the likelihood function is defined as&#10;$$&#10;L(\lambda|x_1,\ldots,x_n) = \prod_{i=1}^n f_\lambda(x_i) = \lambda^n \exp\left\{-\lambda\sum_{i=1}^n x_i\right\}\,.&#10;$$&#10;If your prior on $\lambda$ is a truncated normal $\text{N}(0,1)$, then its pdf is given by&#10;$$&#10;\pi(\lambda) = \sqrt{\dfrac{2}{\pi}}\ \exp\{-\lambda^2/2\}\mathbb{I}_{\lambda&amp;gt;0}\,.&#10;$$&#10;(The cdf is irrelevant for the computation of the posterior.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Now the posterior distribution is defined via Bayes' formula by&#10;$$&#10;\pi(\lambda|x_1,\ldots,x_n) \propto \pi(\lambda) L\lambda|x_1,\ldots,x_n)\,.&#10;$$&#10;Hence,&#10;\begin{align*}&#10;\pi(\lambda|x_1,\ldots,x_n) &amp;amp;\propto \lambda^n \exp\{-\lambda^2/2\} \exp\left\{-\lambda\sum_{i=1}^n x_i\right\}\mathbb{I}_{\lambda&amp;gt;0}\\&#10;&amp;amp;\propto \lambda^n \exp\left\{-\frac{1}{2}\left(\lambda+\sum_{i=1}^n x_i\right)^2\right\}\mathbb{I}_{\lambda&amp;gt;0}&#10;\end{align*}&#10;which is the density of a non-standard distribution.&lt;/p&gt;&#10;" CommentCount="12" CreationDate="2014-12-07T20:48:48.817" Id="127055" LastActivityDate="2014-12-08T12:40:48.803" LastEditDate="2014-12-08T12:40:48.803" LastEditorUserId="7224" OwnerUserId="7224" ParentId="127046" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;I'm not sure what the basis is for your colleague's claim -- but they should support the claims they make before you accept them as true -- there's an astonishing amount of misinformed folklore about. (How do they know that this is true? Do you have good reason to think it must be true in your case?)&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Both&lt;/em&gt; tests assume$^\dagger$ continuous distributions and both are impacted by ties (however, it's relatively easy to deal with ties in the Mann-Whitney and some software will do so automatically). &lt;/p&gt;&#10;&#10;&lt;p&gt;--&lt;/p&gt;&#10;&#10;&lt;p&gt;$\dagger$ Edit: To support my claim of the assumption of continuity in respect of the Mann-Whitney (since whuber says I am wrong on this point, I had better justify it), I refer to the beginning of &lt;a href=&quot;http://projecteuclid.org/download/pdf_1/euclid.aoms/1177730491&quot; rel=&quot;nofollow&quot;&gt;Mann and Whitney (1947)&lt;/a&gt;:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;strong&gt;1. Summary.&lt;/strong&gt; Let $x$ and $y$ be two random variables with continuous cumulative distribution functions $f$ and $g$. &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;So for Mann and Whitney's version of the test, they do explicitly assume continuity - and not idly, since they do rely on it in their derivation. However, it's possible (as I mention later) to deal with ties in the Mann-Whitney by working out the distribution of the test statistic at the null under the pattern of ties, or by correctly computing the effect of ties on the variance of the statistic under the normal approximation (what's usually referred to as the 'adjustment for ties').&lt;/p&gt;&#10;&#10;&lt;p&gt;--&lt;/p&gt;&#10;&#10;&lt;p&gt;For both tests, if the effect of the ties are not properly dealt with, both kinds of error rate are impacted - their type I error rates are lowered, and lowering the significance level necessarily lowers power ($=1-\beta$). &lt;/p&gt;&#10;&#10;&lt;p&gt;It's not 100% clear to me which test might be the most impacted, nor under what circumstances, but offhand I'd have expected the greater sensitivity generally went with the KS test* - and this is even before one 'adjusts' the Mann-Whitney for ties (i.e. if you used the normal approximation and used the variance for the no ties case).&lt;/p&gt;&#10;&#10;&lt;p&gt;*(personally, I'd use simulation suited to the specific instance to see what the properties would be under the sorts of conditions you see, at those sample sizes.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Below is an illustration of the impact on the distribution of p-values under identical population distributions with of a moderate level of ties$^\ddagger$ with sample sizes of 33 and 67 under the default settings in R (which for the Mann-Whitney uses the normal approximation with correct calculation of variance in the presence of ties for this sample size):&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/bvzrW.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;For the tests to work 'as advertized' under the null, these distributions should look close to uniform. As you see, the Mann-Whitney (at least when properly calculating the variance of the sum of the ranks under the presence of ties, as here) is indeed very close to uniform. Since (as we can see) for the Kolmogorov-Smirnov test the proportion of p-values below $\alpha$ will be much smaller than $\alpha$, the test is highly conservative, with corresponding effects on power. [If anything, the effect is somewhat stronger than I'd have anticipated.]&lt;/p&gt;&#10;&#10;&lt;p&gt;$\ddagger\,$(the impact on the variance of the test statistic is fairly small in percentage terms)&lt;/p&gt;&#10;&#10;&lt;p&gt;Further, if your interest lies in a location-shift alternative, the Mann-Whitney would have greater power against that alternative to start with, so even if it did lose more power as a result of the discreteness (which I doubt), it may still have more power afterward.&lt;/p&gt;&#10;&#10;&lt;p&gt;You don't say how heavily tied your data are, nor in what sort of pattern. If both tests are more impacted than you're prepared to accept, you can work with the permutation distribution of either test statistic for you data (or with the permutation distribution of some other statistic, including a difference in sample medians if you wish).&lt;/p&gt;&#10;&#10;&lt;p&gt;In spite of many books (especially in some particular areas of application) stating that it is, the Mann-Whitney is &lt;em&gt;not&lt;/em&gt; actually a test for a difference in medians. However, if you additionally assume that the populations distributions are the same under the null, and restrict the alternative to a location-shift, then it's a test for difference in any reasonable location measure - population medians, population lower quartiles, even population means (if they exist).&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;In short, which test do I trust?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Don't&lt;/em&gt; simply trust what anyone says (including me!) - unless they have solid evidence (I haven't brought any that's directly relevant to your situation,, and none relating to power because I haven't seen your pattern of ties and I am not 100% sure whether you're only interested in location shifts). &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;What kind of data do you have (what are you measuring, how are you measuring it, and how do ties arise)? What are you interested in finding out? Why do you mention medians?&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Use simulation to find out how any tests you contemplate behave in circumstances similar to yours, and decide for yourself whether there's a problem to worry about. For both tests,  see what the impact of ties is on the test, both under the null and under alternatives you care about, and then the case of the Mann-Whitney, see the effect of the adjustment for ties, and compare it with dealing with the exact permutation distribution (or in large samples like yours, with the randomization distribution). For the KS you can look at the exact permutation distribution as well.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-12-07T22:00:57.057" Id="127061" LastActivityDate="2014-12-08T07:56:53.220" LastEditDate="2014-12-08T07:56:53.220" LastEditorUserId="805" OwnerUserId="805" ParentId="127027" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;I think what you want is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Dominated_convergence_theorem&quot; rel=&quot;nofollow&quot;&gt;dominated convergence theorem&lt;/a&gt;. From Wikipedia:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Let $f_n$ be a sequence of real-valued measurable functions on a measure space $(S, Σ, \mu)$. Suppose that the sequence converges pointwise to a function $f$ and is dominated by some integrable function $g$ in the sense that&#10;  $$|f_n(x)| \le g(x)$$&#10;  for all numbers $n$ in the index set of the sequence and all points $x \in S$.&#10;  Then $f$ is integrable and&#10;  $$\lim_{n\to\infty} \int_S |f_n-f|\,d\mu = 0$$&#10;  which also implies &#10;  $$\lim_{n\to\infty} \int_S f_n\,d\mu = \int_S f\,d\mu.$$&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Here $f_n = p(x |\theta_n) p(\theta_n | \psi_n)$. Since it is a product of probabilities, it seems hopeful that you can bound the $f_n$ by some integrable $g$.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-12-07T22:48:38.673" Id="127066" LastActivityDate="2014-12-07T22:48:38.673" OwnerUserId="60642" ParentId="126259" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;We have $n$ random samples from a population which has normal distribution, and the mean of the population is known.&lt;/p&gt;&#10;&#10;&lt;p&gt;How do we change the procedure of finding a confidence interval for the population variance, given we know the exact mean of the population?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-07T23:56:38.750" Id="127071" LastActivityDate="2014-12-08T00:41:25.763" LastEditDate="2014-12-08T00:22:39.150" LastEditorUserId="28462" OwnerUserId="62291" PostTypeId="1" Score="1" Tags="&lt;mathematical-statistics&gt;&lt;confidence-interval&gt;&lt;variance&gt;" Title="Confidence interval for the variance when we know the mean of the population" ViewCount="27" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I know I need to use a within-subjects design for this, but I can't figure out what type of design to use. I am an amateur at statistics and haven't taken a statistics course in years.&#10;I will try to be brief. An experiment was done where we asked a small sample of people (8) to do the same task under three conditions. The task involved recording the person's X-Y pixel clicks on buttons on the screen. The conditions were things like intensity of screen changing, color changing, etc. From the XY pixel locations, we are able to determine if the click was a hit or miss and also how far off it was. Thus, for each subject, under each condition, we can determine the percent error rate and average distance from the center of the target. &lt;/p&gt;&#10;&#10;&lt;p&gt;It should be noted that we randomized the order of the conditions when administering the test to avoid any confounding factors due to learning.&lt;/p&gt;&#10;&#10;&lt;p&gt;The above is what I have already done. However, I know somehow I need to correct for error variances within subjects... but I don't know how all that works. I'm not sure what kind of statistical test I need to use to produce the following results:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;1. I would like a chart showing the average error rates between the three conditions (with error bars) and I would like to be able to say whether or not the conditions have a statistically significant effect on error rate.&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;2. I would like a chart showing the average distances produced between the three conditions (with error bars) and I would like to say whether or not the conditions have a statistically significant effect on error rate.&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is a sample I guess of what I am trying to do (with the error rates) (overall, for all subjects):&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/aF3kG.png&quot; alt=&quot;error rate sample&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm not good with statistics at all. I've done some research and I know I need to do a within-subjects design since all the subjects were exposed to the conditions, but I'm not sure what else I need to do.&lt;/p&gt;&#10;&#10;&lt;p&gt;Sorry that I need so much help. :(&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-08T05:31:05.117" Id="127103" LastActivityDate="2014-12-08T05:31:05.117" OwnerUserId="62316" PostTypeId="1" Score="0" Tags="&lt;statistical-significance&gt;&lt;repeated-measures&gt;&lt;group-differences&gt;" Title="What type of within-subjects design should I use? Error-rates, Distances" ViewCount="6" />
  <row Body="&lt;p&gt;Under the model, $E(y|x)=g(\mathbf{x} \beta)$.  Therefore $E(u|x)=0$, making $Var(u|x) = E(u^2|x) - E(u|x)^2 = E(u^2|x)$.  The last step is to note that $Var(y|x) = Var(u|x)$ since $y=u+g(\mathbf{x}\beta)$ and the last term is constant wrt $\mathbf{x}$.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-12-08T09:12:59.530" Id="127118" LastActivityDate="2014-12-08T09:12:59.530" OwnerUserId="2074" ParentId="127117" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;This is explained in section 2.2.4 of the paper &lt;a href=&quot;http://research.microsoft.com/apps/pubs/default.aspx?id=155552&quot; rel=&quot;nofollow&quot;&gt;Decision Forests for Classification, Regression, Density Estimation, Manifold Learning and Semi-Supervised Learning&lt;/a&gt;.  &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;In the case of classification each leaf may store the empirical distribution over the classes associated to the subset of training data that has reached that leaf.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;During testing, each tree leaf yields a distribution over classes and the forest output is the average of these leaf distributions.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-08T10:14:01.553" Id="127125" LastActivityDate="2014-12-08T10:14:01.553" OwnerUserId="2074" ParentId="126754" PostTypeId="2" Score="1" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a mixed model, fit using &lt;code&gt;lmer&lt;/code&gt; in R, that has three interaction terms (&lt;code&gt;X1:X1&lt;/code&gt;, &lt;code&gt;X1:X3&lt;/code&gt;, and &lt;code&gt;X1:X4&lt;/code&gt;), and the significance of these three interaction terms (not the main effects of &lt;code&gt;X1&lt;/code&gt;, &lt;code&gt;X2&lt;/code&gt;, &lt;code&gt;X3&lt;/code&gt;, and &lt;code&gt;X4&lt;/code&gt;, though those are included in the model also) is what is of interest. Variables &lt;code&gt;X1&lt;/code&gt;, &lt;code&gt;X2&lt;/code&gt;, and &lt;code&gt;X3&lt;/code&gt; are continuous, but &lt;code&gt;X4&lt;/code&gt; is a factor with 4 levels (&lt;code&gt;A&lt;/code&gt;, &lt;code&gt;B&lt;/code&gt;, &lt;code&gt;C&lt;/code&gt;, and &lt;code&gt;D&lt;/code&gt;). All three interactions are sufficiently replicated within individuals (which are plants), so &lt;code&gt;individual&lt;/code&gt; will be treated as a random factor. I'm trying to follow the advice of &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3672519/pdf/fpsyg-04-00328.pdf&quot; rel=&quot;nofollow&quot;&gt;Dale Barr&lt;/a&gt; and &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3881361/pdf/nihms533954.pdf&quot; rel=&quot;nofollow&quot;&gt;colleagues&lt;/a&gt; to keep the random effects structure of my model &quot;maximal,&quot; so I am trying to model random slopes and intercepts between individuals for each interaction term. I understand that this can be done with this syntax:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;code&gt;(1 + X1:X2 | INDIVIDUAL) + (1 + X1:X3 | INDIVIDUAL) + (1 + X1:X4 | INDIVIDUAL)&lt;/code&gt;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;The full model, including this random effect structure, &lt;em&gt;usually&lt;/em&gt; converges (N = 242), but it doesn't always. The reason for this, I &lt;em&gt;think&lt;/em&gt;, is that this model calls for the estimation of 34 parameters, 22 of which are associated with the error structure. Given that the rule of thumb I've heard is &quot;at least 10 data points per parameter,&quot; this seems high. 15 of these 22 parameters seem to be associated with the last of the three terms above, &lt;code&gt;(1+X1:X4|INDIVIDUAL)&lt;/code&gt;. Because &lt;code&gt;X4&lt;/code&gt; has 4 levels, the model seems to be estimating five variances (one for each level &lt;code&gt;A&lt;/code&gt; through &lt;code&gt;D&lt;/code&gt; plus the intercept) and then also ten covariances to model this one random effect. At least, this is what the output of &lt;code&gt;summary(model)&lt;/code&gt; is suggesting to me.&lt;/p&gt;&#10;&#10;&lt;p&gt;In short, I would like to suppress (i.e. force to 0) those ten covariances. At least for a non-interaction term, following Barr et al. (2013) above, this can be done with this syntax:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;(0 + X | INDIVIDUAL) + (1 | INDIVIDUAL). &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;This forces the intercept and slope offset terms to be uncorrelated. But trying something similar:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;(0 + X1:X4 | INDIVIDUAL) + (1 | INDIVIDUAL)&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;In my case, only seems to succeed in making the intercept and slope offset terms uncorrelated, suppressing the four covariances associated with the intercept. I am still getting covariances between the slope offsets for each &lt;code&gt;X1:X4A&lt;/code&gt;-&lt;code&gt;D&lt;/code&gt; combination. Shy of specifying my own variance-covariance matrix for this term (something I have no idea how to do), what syntax will get me the desired outcome? If more information is needed, let me know and I'd be happy to provide it!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-08T03:40:52.580" Id="127172" LastActivityDate="2014-12-08T16:52:46.283" LastEditDate="2014-12-08T16:52:46.283" LastEditorUserId="61175" OwnerDisplayName="Bajcz" OwnerUserId="61175" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;mixed-model&gt;&lt;interaction&gt;&lt;random-effects-model&gt;&lt;lmer&gt;" Title="In R package lme4, how do you force the random slopes and intercepts to be uncorrelated for an interaction term?" ViewCount="65" />
  <row Body="&lt;p&gt;Let's assume that $x_i, y_i &amp;gt; 0,$&lt;/p&gt;&#10;&#10;&lt;p&gt;The formula simplifies to&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10; n\cdot \log(\lambda)+n\cdot \log(\frac{1}{\lambda})-\lambda\sum x_i-\frac{1}{\lambda}\sum y_j  \\=n\cdot \log(\lambda)+n\cdot \log(1)-n\cdot \log(\lambda) &#10;-\lambda\sum x_i-\frac{1}{\lambda}\sum y_j &#10;\\=-\lambda\sum x_i-\frac{1}{\lambda}\sum y_j &#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and the derivative w.r.t. $\lambda$ is&#10;$$&#10;-\sum x_i+\lambda^{-2}\sum y_j &#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;so if we define $\bar{x} = \sum x_i$ and $\bar{y}=\sum y_j $, then&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;-\bar{x}+\lambda^{-2}\bar{y} = 0&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\lambda = \sqrt{\frac{\bar{y}}{\bar{x}}}$, since $\lambda$ needs to be positive&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-08T15:57:13.747" Id="127173" LastActivityDate="2014-12-08T23:26:22.947" LastEditDate="2014-12-08T23:26:22.947" LastEditorUserId="28746" OwnerUserId="62238" ParentId="127170" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;While I agree that there's not a unique solution to the problem, I use the recommendation of this blog:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://blogs.nature.com/methagora/2013/07/data-visualization-points-of-view.html&quot; rel=&quot;nofollow&quot;&gt;http://blogs.nature.com/methagora/2013/07/data-visualization-points-of-view.html&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The posts on colour tackle the issues of colour-blindness and Gray-scale printing and gives an example of colour scale that solves this both issues.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the same articles is analysed also the continuous colour scales, which many uses for heat plots and so on. It is recommended not to use the rainbow, because of some sharp transitions (like the yellow zone, much smaller than the red). Instead, it is possible to make transitions between other pairs of colours.&lt;/p&gt;&#10;&#10;&lt;p&gt;A good set of colours for this purpose is blue and orange (a classic!). You can make a test, by applying colour-blind and Gray filters and see if you can still notice the difference.&lt;/p&gt;&#10;&#10;&lt;p&gt;For the thickness of lines, some of the issues of the blog mentioned before deal with this point. Lines, if you have many, should have the same thickness, that is &quot;thin&quot;. Use thick lines only if you want to call attention to that object.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-08T16:13:08.230" Id="127175" LastActivityDate="2014-12-08T16:13:08.230" OwnerUserId="62352" ParentId="126480" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;I want to use regression output (b, se B and cov of several predictors) as input for a new analysis.&lt;/p&gt;&#10;&#10;&lt;p&gt;One example: I want to compute:&lt;/p&gt;&#10;&#10;&lt;p&gt;SE^2(b1) + SE^2(b2) + 2 COV (b1,b2).&lt;/p&gt;&#10;&#10;&lt;p&gt;If I do a basic regression in Mplus (which I used because I have a choice of estimators, that I don't have in spss), and I look at the covariance table, the values reported there are extremely different from the covariances I get from the bcov command in SPSS regression.&lt;/p&gt;&#10;&#10;&lt;p&gt;They are so different that I am convinced they do not reflect the same value&#10;(e.g., .484 in mplus, and -.064 in spss).&lt;/p&gt;&#10;&#10;&lt;p&gt;Can anyone tell me:&#10;- What &quot;estimated sample statistics&quot; -&gt; covariates in Mplus vs. &quot;coefficient correlations&quot; =&gt; &quot;covariances&quot;  in spss refer to?&#10;- which one you think might be the cov(b1), referred to in the output? (my instinct says the SPSS one is the one I need)&#10;- How I could obtain this in the other program (i.e., if I need the spss one, how can I get this in MPlus)?&lt;/p&gt;&#10;&#10;&lt;p&gt;Please note that I did use the same estimator in mplus for my mock data on which I tested this, all other estimates (b, seB etc) are the same.&lt;/p&gt;&#10;&#10;&lt;p&gt;Model in Mplus:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;MODEL:  !Main effects&#10;            leka on S;&#10;            leka on P;&#10;            leka on M;&#10;          !Correlations between predictors&#10;            S with P;&#10;            M with P;&#10;            S with M;                      &#10;OUTPUT: tech1; tech4; samp; stand; mod(4); Sampstat Mod(3.84);&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Model in SPSS:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;REGRESSION&#10;/MISSING LISTWISE&#10;/STATISTICS COEFF OUTS BCOV R ANOVA&#10;/CRITERIA=PIN(.05) POUT(.10)&#10;/NOORIGIN &#10;/DEPENDENT Leka_t3&#10;/METHOD=ENTER S P M.&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-12-08T17:22:10.357" Id="127186" LastActivityDate="2014-12-08T17:50:48.380" LastEditDate="2014-12-08T17:50:48.380" LastEditorUserId="17072" OwnerUserId="61053" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;spss&gt;&lt;covariance&gt;&lt;mplus&gt;" Title="Regression: Covariance table in spss vs. mplus" ViewCount="60" />
  <row AnswerCount="0" Body="&lt;p&gt;I have read many very useful posts on this site as regards randomization procedures.   I wonder if the following example may be useful for discussion purposes.  &lt;/p&gt;&#10;&#10;&lt;p&gt;My basic question is which of two randomization approaches is more appropriate. My intuition is option 1, but I'd be interested in other views.&lt;/p&gt;&#10;&#10;&lt;p&gt;These data are from a fictional tournament.  There are five teams and each play each other twice.  Each row/observation is a separate contest.  Teams are put into variables id1, id2 and then there is a result variable.  A &quot;W&quot; refers to id1 beating id2, a &quot;L&quot; refers to id1 losing to id2.  A &quot;T&quot; refers to a tie.&lt;/p&gt;&#10;&#10;&lt;p&gt;x1&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#   id1 id2 result&#10;#1   JY  CS      L&#10;#2   JY  VY      L&#10;#3   JY  OZ      L&#10;#4   JY  LQ      L&#10;#5   CS  VY      W&#10;#6   CS  OZ      W&#10;#7   CS  LQ      T&#10;#8   VY  OZ      L&#10;#9   VY  LQ      L&#10;#10  OZ  LQ      L&#10;#11  CS  JY      W&#10;#12  VY  JY      W&#10;#13  OZ  JY      W&#10;#14  LQ  JY      L&#10;#15  VY  CS      L&#10;#16  OZ  CS      L&#10;#17  LQ  CS      L&#10;#18  OZ  VY      W&#10;#19  LQ  VY      T&#10;#20  LQ  OZ      T&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;From these data, I am interested in calculating the standard deviation of win percentages.  This can be done with the following function:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;wpct_sd &amp;lt;- function(df){&#10;&#10;table1 &amp;lt;- table(df$id1, df$result) &#10;table2 &amp;lt;- table(df$id2, df$result)&#10;&#10;W &amp;lt;- table1[,&quot;W&quot;] + table2[,&quot;L&quot;]&#10;sd(W/8) #each team played in 8 contests&#10;&#10;}&#10;&#10;mysd &amp;lt;- wpct_sd(x1) &#10;mysd #0.2877716&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;My interest is now in whether this standard deviation of win percentages is higher than one would expect should contests be decided randomly.   If ties did not exist, I could see that we could just assign &quot;W&quot; and &quot;L&quot; randomly to each entry in the result column and then determine the distribution of standard deviations of win percentages for 'n' randomizations.   As ties  do exist, how to deal with those?&lt;/p&gt;&#10;&#10;&lt;h3&gt;Randomization 1.  - Simply take the information in the 'result' variable variable and randomly permute them.&lt;/h3&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;outcome &amp;lt;- NULL&#10;for (i in 1:5000){&#10;x1$result &amp;lt;- sample(x$result)&#10;outcome[[i]] &amp;lt;- wpct_sd(x)&#10;}&#10;&#10;#plot&#10;hist(outcome)&#10;abline(v = mysd, col=&quot;red&quot;, lty=2)&#10;sum(outcome &amp;gt;= mysd) / 5000  #just above 0.05ish&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/qzbP2.png&quot; alt=&quot;plot of randomization 1 - red line is observed value&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;h3&gt;Randomization 2.   From observed data's result variable, determine the probability of winning/losing/tying individual contests.    Do this by calculating the probability of ties, and then assume equal likelihood of teams winning/losing a tie.&lt;/h3&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;length(x1$result) #20 - number of contests in tournament&#10;    pTie &amp;lt;- sum(x1$result==&quot;T&quot;) / 20&#10;pTie #0.15 - this is the proportion of ties&#10;&#10;#generate random results based on the proportion of wins/losses/ties observed in results&#10;sample(c(&quot;W&quot;, &quot;L&quot;, &quot;T&quot;), prob=c(0.425, 0.425, 0.15), replace=T, 20)&#10;&#10;outcome &amp;lt;- NULL&#10;&#10;for (i in 1:5000){&#10;  x1$result &amp;lt;- sample(c(&quot;W&quot;, &quot;L&quot;, &quot;T&quot;), prob=c(0.425, 0.425, 0.15), replace=T, 20)  &#10;  outcome[[i]] &amp;lt;- wpct_sd(x1)&#10;}&#10;&#10;#plot&#10;hist(outcome)&#10;abline(v = mysd, col=&quot;red&quot;, lty=2)&#10;sum(outcome &amp;gt;= mysd) / 5000  #wider variability between 0.045 and 0.055&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/t9RvA.png&quot; alt=&quot;Plot from randomization 2 - red line is observed value&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Hopefully the question is clear.  Essentially, which is the more appropriate randomization?     I don't actually care at all about these data, they are purely fictional, but I am trying to gain more insight into which randomization procedure is more informative as to understanding whether our observed standard deviation of win percentages is greater than expected by chance for these type of data.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Data for reproducing:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x1 &amp;lt;- structure(list(id1 = structure(c(2L, 2L, 2L, 2L, 1L, 1L, 1L, &#10;5L, 5L, 4L, 1L, 5L, 4L, 3L, 5L, 4L, 3L, 4L, 3L, 3L), .Label = c(&quot;CS&quot;, &#10;&quot;JY&quot;, &quot;LQ&quot;, &quot;OZ&quot;, &quot;VY&quot;), class = &quot;factor&quot;), id2 = structure(c(1L, &#10;5L, 4L, 3L, 5L, 4L, 3L, 4L, 3L, 3L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, &#10;5L, 5L, 4L), .Label = c(&quot;CS&quot;, &quot;JY&quot;, &quot;LQ&quot;, &quot;OZ&quot;, &quot;VY&quot;), class = &quot;factor&quot;), &#10;    result = c(&quot;L&quot;, &quot;L&quot;, &quot;L&quot;, &quot;L&quot;, &quot;W&quot;, &quot;W&quot;, &quot;T&quot;, &quot;L&quot;, &quot;L&quot;, &quot;L&quot;, &#10;    &quot;W&quot;, &quot;W&quot;, &quot;W&quot;, &quot;L&quot;, &quot;L&quot;, &quot;L&quot;, &quot;L&quot;, &quot;W&quot;, &quot;T&quot;, &quot;T&quot;)), .Names = c(&quot;id1&quot;, &#10;&quot;id2&quot;, &quot;result&quot;), row.names = c(NA, -20L), class = &quot;data.frame&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-12-08T19:15:34.417" Id="127205" LastActivityDate="2014-12-08T19:32:06.427" LastEditDate="2014-12-08T19:32:06.427" LastEditorUserId="49482" OwnerUserId="49482" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;randomization&gt;" Title="Principles for choosing randomization procedure - example from tournament data" ViewCount="22" />
  <row AnswerCount="1" Body="&lt;p&gt;PROBLEM STATEMENT:&lt;/p&gt;&#10;&#10;&lt;p&gt;The original data $y_t$ is a noisy version of a time series obtained from an autoregressive process excited by a deterministic non-linear signal $x_t$. The error terms $u_t$ is :&lt;/p&gt;&#10;&#10;&lt;p&gt;$u_t = z_t - (\hat{a}z_{t-1} + \hat{b}z_{t-2} + \hat{c}z_{t-3} )$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $y_t = ay_{t-1} + by_{t-2} + cy_{t-3} + \mathbf{h_x^T}x_t $ OR&lt;/p&gt;&#10;&#10;&lt;p&gt;$y_t = \mathbf{h_y^Ty_{t-1}}+ \mathbf{h_x^T}x_t $&#10;and the noisy version is,&lt;/p&gt;&#10;&#10;&lt;p&gt;$z_t = y_t + v_t$&lt;/p&gt;&#10;&#10;&lt;p&gt;$v_t = N(0,\sigma^2_v)$&#10; $x_t$ is a deterministic signal of dimension $d$.&lt;/p&gt;&#10;&#10;&lt;p&gt;$x_t = f(x_{t-1},x_{t-2},\ldots,x_{t-d})$. This signal depends on the initial condition $x_1$. &lt;/p&gt;&#10;&#10;&lt;p&gt;$\mathbf{h_x} = {[h_{x1},\ldots,h_{xq}]}^T$;&lt;/p&gt;&#10;&#10;&lt;p&gt;$\mathbf{h_y} =  {[h_{y1},\ldots,h_{yp}]}^T$;&lt;/p&gt;&#10;&#10;&lt;p&gt;$\mathbf{y_{t-1}} = {[y_{t-1},\ldots, y_{t-p}]}^T$&lt;/p&gt;&#10;&#10;&lt;p&gt;Since, the measurement noise $v_t$ is Gaussian, the pdf is&#10;\begin{align}&#10;f(z_1|\theta) = &amp;amp;\prod_{t=1}^N \frac{1}{\sqrt{2 \pi \sigma^2_v}} \exp{\bigg(-\frac{{(z_t - y_t)}^2}{2 \sigma^2_v}\bigg)}&#10;\end{align}&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Assumptions :&lt;/strong&gt; Let $d$ = 2, known $\sigma^2_\epsilon = 1$; unknown $\sigma^2_v$. &#10;due to deterministic nature, $P(&#10;\mathbf{x_{t+1}}|\mathbf{x_t})$ =1; Pdf of $P(\mathbf{x_1}) = N(0,\sigma^2_x\mathbf{I})$ and $P(z_t|y_t) = N(\mathbf{h_y^Ty_t},\sigma^2_v)$;&lt;/p&gt;&#10;&#10;&lt;p&gt;Order $p$ = 2; $q$ =2&lt;/p&gt;&#10;&#10;&lt;p&gt;Q1: What will be the pdf and the likelihood function for $u_t$ where $\hat{a},\hat{b},\hat{c}, \sigma^2_v, x_1,$ are unknown. &lt;/p&gt;&#10;&#10;&lt;p&gt;Q2: The objective is to find estimators for the unknwown parameters $\theta = [\mathbf{h_x},\mathbf{h_y},x_1,\sigma^2_v,\sigma^2_x]$ from $u$. Is that possible?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-12-08T19:38:16.837" Id="127209" LastActivityDate="2014-12-09T20:02:31.287" OwnerUserId="28545" PostTypeId="1" Score="0" Tags="&lt;pdf&gt;&lt;unbiased-estimator&gt;&lt;likelihood-function&gt;" Title="Help in finding the pdf for Gaussian distribtuion of time series model" ViewCount="30" />
  
  
  
  <row Body="&lt;p&gt;If you think that each experiment adds a random constant to $y$ (where the distribution of the random constant depends on the experiment), then you can simply include a quadratic factor for $x$ and code each experiment as a &lt;a href=&quot;https://en.wikipedia.org/wiki/Dummy_variable_(statistics)&quot; rel=&quot;nofollow&quot;&gt;dummy variable&lt;/a&gt; (so your tuples go from $(y, x)$ to $(y, x, x^2, e_1, e_2, \dots)$, where $e_i = 1$ if the tuple is from experiment 1, and 0 otherwise). Then you can fit a standard linear model to find the parameters.&lt;/p&gt;&#10;&#10;&lt;p&gt;This corresponds to a model $y = ax^2 + bx + c_i + \epsilon$--so the constant that gets added to $y$ depends on which experiment you're in. This model assumes that the noise term $\epsilon$ is constant across all experiments (that is, the &quot;experiment component&quot; of the randomness can have a different mean across experiments, but has the same variance). If you want to check this assumption, you can plot the residuals grouped by experiment and check if the distributions look the same.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-12-08T22:11:20.550" Id="127230" LastActivityDate="2014-12-08T22:11:20.550" OwnerUserId="60642" ParentId="127227" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;Construct a design in enough of the factors to (say A, B, C,...) to create the correct number of runs. Then find an alias for each of the remaining factors in terms of the ones you have, and use the corresponding product to create the levels of that factor. &lt;/p&gt;&#10;&#10;&lt;p&gt;A different example: def rel: I = ABCD, so that D = ABC&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;A B C  D=ABC&#10;- - -   -&#10;+ - -   +&#10;- + -   +&#10;+ + -   -&#10;- - +   +&#10;+ - +   -&#10;- + +   -&#10;+ + +   +&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="3" CreationDate="2014-12-09T00:37:35.440" Id="127245" LastActivityDate="2014-12-09T00:37:35.440" OwnerUserId="52554" ParentId="127243" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;So I am given that $X$ is a binary r.v. and the following assignments, $E[X] = A, E[Y|X=1]=B, E[Y|X=0]=C, E[Y^{2}|X=1] = D, E[Y^{2}|X=0]=E$.  I must express my answers in terms of these expressions and done so for several exercises, like $Pr\{X=1\} = A$ and &lt;/p&gt;&#10;&#10;&lt;p&gt;$E[Y] = \sum Pr\{Y= y\}y = \sum Pr\{Y=y, X=0\}y + Pr\{Y=y, X=1\}y$&lt;/p&gt;&#10;&#10;&lt;p&gt;$=\sum Pr\{Y=y|X=0\}Pr\{X=0\}y + Pr\{Y=y|X=1\}Pr\{X=1\}y = (1-A)C + AB$&lt;/p&gt;&#10;&#10;&lt;p&gt;and so on.&lt;/p&gt;&#10;&#10;&lt;p&gt;I also found the slope of the BLP of $Y$ given $X$, using the formula $\frac{Cov[X,Y]}{V[X]}$ and formulas for each of these.  But now it asks to find &quot;the asymptotic (limiting value of) $\hat{\beta}_{1}$ from fitting the regression: $Y = \hat{\beta}_{0} + \hat{\beta}_{1}X + \epsilon$.&quot;  It was my understanding that these two things are the same thing--so I'm confused by the sequence of questions.  Do I misunderstand the coefficients of the regression here?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-09T05:25:44.277" Id="127260" LastActivityDate="2014-12-09T05:25:44.277" OwnerUserId="59093" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;mathematical-statistics&gt;&lt;linear-model&gt;" Title="The asymptotic slope of the BLP" ViewCount="11" />
  <row Body="&lt;p&gt;As Cagdas Ozgenc points out in his answer, there is a simple sufficient condition to render this question moot: that the likelihood be concave in the parameter space (almost surely with respect to the sampling distribution of the data).  That does cover many interesting cases (ie, the exponential family), but basically leaves everything else out.&lt;/p&gt;&#10;&#10;&lt;p&gt;I don't have an answer here, but think that there are several ways this question could be refined or restated:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;What properties does the MLE have in finite samples, and under what models?  Although everyone likes the MLE, it's usage (AFAIK) is predicated on asymptotic guarantees.  I can't think of any finite sample guarantees for it.&lt;/li&gt;&#10;&lt;li&gt;What properties, if any, does a local maxima have?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;h2&gt;Bibliography&lt;/h2&gt;&#10;&#10;&lt;p&gt;&quot;Evaluation of the Maximum-Likelihood Estimator where the Likelihood Equation has Multiple Roots&quot; VD Barnette 1966.&#10;&lt;strong&gt;The Cauchy distribution with location parameter offers a canonical example of a likelihood with multiple roots (even asymptotically).&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://math.univ-lille1.fr/~biernack/index_files/testML_full_version.pdf&quot; rel=&quot;nofollow&quot;&gt;&quot;Testing for a Global Maximum of the Likelihood&quot;&lt;/a&gt; Christophe Biernacki, 2005.&#10;&lt;strong&gt;A test for consistency for a root of the likelihood equation, based on comparing the observed maximized likelihood to its expected value under the putative argmax&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://projecteuclid.org/download/pdf_1/euclid.ss/1009213001&quot; rel=&quot;nofollow&quot;&gt;&quot;Eliminating Multiple Root Problems in Estimation&quot;&lt;/a&gt; Small, Wang, Yang 2000.&#10;&lt;strong&gt;If you are going to read one paper, this is probably it.  Discusses all of the above, also in the context of generalized estimating equations, plus suggests smoothing or penalizing the likelihood to help resolve multiple roots.&lt;/strong&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-09T07:32:21.413" Id="127263" LastActivityDate="2014-12-09T07:32:21.413" OwnerUserId="52092" ParentId="49987" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;OK fine, I'll wade in... :-)  &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;I have the following dataset which appears to be normally distributed but is truncated at 0.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;As we've seen, this is not obviously true.  But let's leave the identity of the generating distribution aside for a moment.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;If I ignore the values which are 0 I get an even better distribituion.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Well, you get a sample without a spike at 0.  But it seems to me that 0 is a case where the system you are measuring gets the right answer.  As you note: it 'should' be zero, so the rest is just the shape of some function of the noise. &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;I would like to conclude that some percentage of the values are within kσ of μ &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Right.  So, if I may creatively reconstruct a bit, the inference task is as follows: &lt;/p&gt;&#10;&#10;&lt;p&gt;Given a large sample of values, find $a$ and $b$ such that $x$ percent of values will fall between $a$ and $b$.  &lt;/p&gt;&#10;&#10;&lt;p&gt;For concreteness let's set $x=0.95$  &lt;/p&gt;&#10;&#10;&lt;p&gt;The task is made trickier by the fact you don't have an analytic form for the generating distribution.  However, If the underlying distribution is smooth, the sample is random, etc. then we have some guarantee that statistics you compute from the sample you have will converge on their true values as the sample size increases.  (Obviously there are regularity conditions here which I'm ignoring for exposition.)  &lt;/p&gt;&#10;&#10;&lt;p&gt;In particular, two such statistics are the 0.025, 0.975 quantiles.  You might then solve the task by setting using the sample to estimate them and then setting $a$ to the first and $b$ to the second.  We might call this the non-parametric approach.&lt;/p&gt;&#10;&#10;&lt;p&gt;You have approached the problem in a parametric manner by first searching for a density that your histogram looks like and then trying to use analytic results about the integrals of that density to determine which $a$ and $b$ would bound 95% of the probability.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I speculate that you thought about it as follows (apologies if I've missed your line of thought):  You're familiar with some results for the Normal distribution which depend only on knowing its mean and variance and wonder how to apply them here.  As you note, applying this distributional assumption is made problematic by the sample's apparent spike at zero and lack of negative values.&lt;/p&gt;&#10;&#10;&lt;p&gt;That's what you're responding to when you say that&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;the problem is that X⎯⎯⎯ (dashed line) does not match with the centre of my distribution which seems problematic.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;This isn't quite true - the mean is &lt;em&gt;a&lt;/em&gt; measure of central tendency - so I guess what you're thinking is problematic is that the mode and the mean don't seem to line up as you're expecting.  This is a natural consequence of the skew, and should suggest that a Normal assumption might actually not be the one to reach for.&lt;/p&gt;&#10;&#10;&lt;p&gt;Nevertheless, you wonder &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;how should I calculate X⎯⎯⎯ and S(X) and how/can I uses these estimate μ and σ.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;but I think the answer might be to stop wondering how to calculate these quantities.  They would be useful if your distribution were Normal, but if it isn't, they may not be. Consequently your next question &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;after I have estimated μ and σ is it valid to say Pr(X&amp;lt;μ+1.65σ)=0.95 like if I had a non-truncated normal distribution?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;will not necessarily help solve your actual inference task either, though it's an interesting question in itself.  &lt;/p&gt;&#10;&#10;&lt;p&gt;The answer in general is 'not quite', e.g. 95% of the probability in a unit Normal distribution is below 1.65 but only about 92% of it is below μ+1.65σ if it's truncated at zero (when μ becomes about 0.8 and σ about 0.6).  Also, as a minor point, your question asks about a range around the mean whereas the expression above is one-tailed.&lt;/p&gt;&#10;&#10;&lt;p&gt;Fortunately, it seems to me, you have enough data to get $a$ and $b$ from the data directly.  But if you do pursue the 'find a distribution and use its analytical results' route then, since these are maxima you are plotting, you probably want to follow @Aksakal's advice and fit an extreme value distribution.  &lt;/p&gt;&#10;&#10;&lt;p&gt;This leaves the question of the spike at zero.  But what you want to do about that depends on the use you want to put your desired interval.  Perhaps you want to explicitly model it, but perhaps not.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-09T11:39:28.033" Id="127289" LastActivityDate="2014-12-09T11:39:28.033" OwnerUserId="1739" ParentId="127080" PostTypeId="2" Score="1" />
  
  
  
  
  <row Body="&lt;p&gt;You will have to define what values you do consider spikes in the data. This can be defined either manually (by defining a manually fixed threshold) or using a proportional value to the standard deviation of data.&lt;/p&gt;&#10;&#10;&lt;p&gt;For this second case you could find the standard deviation of your vector x (&lt;code&gt;std(x)&lt;/code&gt; for matlab) and then filter the vector by using only the values that are lower than the mean plus &lt;strong&gt;n&lt;/strong&gt; standard deviations.&lt;/p&gt;&#10;&#10;&lt;p&gt;Something like:&#10;&lt;code&gt;x = x(x&amp;lt;mean(x)+n*std(x))&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, what do you actually want to achieve with the timestamp? I understand you want for a new input (time,value) to find other (time,values) that are similar to it. Given you do not have an specific classification for this different 'groups' you might want to follow an unsupervised learning approach to this problem. A simple solution (efficient but not always accurate) would be to use a clustering algorithm. A simple K-means algorithm will give you K-clusters, and you could calculate roughly the degree of belonging to this group as the distance to the centroid (for a more accurate belonging you will need to make a more sophisticated calculation or use fuzzy clustering). &lt;/p&gt;&#10;&#10;&lt;p&gt;A simpler solution would be to calculate the K-Nearest-Neighbors and there you will have a natural feature space grouping. This neighbors won't vote for an specific category to your data but just give you an idea of which other values are closer to to your input spatially.&lt;/p&gt;&#10;&#10;&lt;p&gt;Unless you want a more specific output, like constraining for time-ranges, this will give you a rather &quot;natural&quot; approach (spatially).&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-12-09T14:23:25.250" Id="127314" LastActivityDate="2014-12-09T14:23:25.250" OwnerUserId="45573" ParentId="127308" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;Most of the 31000 genes are unlikely to differ much in expression among the cell lines (at least when appropriately normalized), so they add no information to the problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;For a practical biological problem like this, it may help to concentrate on genes whose expression levels are relatively high on an absolute basis. That way it will be easier to validate your results on these 29 lines and then apply and test your predictions on cell lines beyond those you are now examining, for example with standard PCR instead of the expensive microarray or RNAseq methods used to examine 31000 genes at once.&lt;/p&gt;&#10;&#10;&lt;p&gt;Start by (&lt;em&gt;a&lt;/em&gt;) limiting your analysis to highly expressed genes whose normalized expression levels have the greatest variance among cell lines (typically on a log scale in gene expression work) and closest relation to IC50 values, so that your intractable $p \gg n$ problem becomes a less difficult $p &amp;gt; n$ problem. Then (&lt;em&gt;b&lt;/em&gt;) combine information from different genes whose expression levels co-vary among cell lines. &lt;/p&gt;&#10;&#10;&lt;p&gt;The &quot;supervised principal components&quot; method described in section 18.6 of &lt;a href=&quot;http://statweb.stanford.edu/~tibs/ElemStatLearn/&quot; rel=&quot;nofollow&quot;&gt;The Elements of Statistical Learning&lt;/a&gt;, second edition, provides a documented way to accomplish this. Genes are rank-ordered with respect to univariate relations to IC50 values (accomplishing goal &lt;em&gt;a&lt;/em&gt;, if you limit to highly expressed genes) and PCA is performed on a subset of genes with the highest relations to IC50s (accomplishing goal &lt;em&gt;b&lt;/em&gt;). The number of genes included in the PCA and the number of principal components retained are chosen by cross-validation. &lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-12-09T15:36:07.617" Id="127324" LastActivityDate="2014-12-09T17:13:56.237" LastEditDate="2014-12-09T17:13:56.237" LastEditorUserId="28500" OwnerUserId="28500" ParentId="87082" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Yes, interaction terms are specified in the same way for Cox regression as for linear regression. Interpretation of the interaction coefficients in Cox regression is probably best done in terms of the hazard ratios, shown in the &lt;code&gt;exp(coef)&lt;/code&gt; values.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-09T16:16:07.500" Id="127330" LastActivityDate="2014-12-09T16:16:07.500" OwnerUserId="28500" ParentId="127029" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;This question is on &quot;propagation of error&quot; or &quot;error propagation&quot; and is a very common problem.  If the variables are correlated, the problem is fairly complicated, if not, you may simply sum the squares of the errors multiplied by the partial derivatives of the function: &lt;a href=&quot;http://en.wikipedia.org/wiki/Propagation_of_uncertainty#Simplification&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Propagation_of_uncertainty#Simplification&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Wikipedia has a very nice page on it: &lt;a href=&quot;http://en.wikipedia.org/wiki/Propagation_of_uncertainty&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Propagation_of_uncertainty&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-09T16:56:33.637" Id="127334" LastActivityDate="2014-12-09T16:56:33.637" OwnerUserId="62438" ParentId="93316" PostTypeId="2" Score="0" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Is the requirement of monotonic sigmoidal relation between p and X'B in logistic regression equivalent as logit[p] having linear relation with X'B? X is the vector of independent variables and B is a vector of estimates.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-12-09T17:44:45.727" Id="127348" LastActivityDate="2014-12-09T17:44:45.727" OwnerUserId="60233" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;logistic&gt;" Title="Is monotonic sigmoidal relation between p and X'B in logistic regression equivalent as logit[p] having linear relation with X'B?" ViewCount="24" />
  <row Body="&lt;p&gt;The (potentially causal) interpretation of your model doesn't come from the model itself.  It comes from the design / setup of your study.  Causality can primarily be inferred from your model when you ran a true experiment.  There are various methods for attempting to infer causality with observational data (e.g., instrumental variables, difference-in-differences, propensity scores, etc.), but they all require additional assumptions and are not as strong as experiments in general.  If you don't have a true experiment, it is safest to always assume your model estimates a marginal association only.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Omitted / lurking variables affect this when they are correlated with $X$ variables in your model and with your response ($Y$) variable.  In that case, they bias your estimates such that a variable could appear causal when it actually isn't.  To understand this better, it may help to read my answer here: &lt;a href=&quot;http://stats.stackexchange.com/a/58712/7290&quot;&gt;Estimating $b_1x_1+b_2x_2$ instead of $b_1x_1+b_2x_2+b_3x_3$&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-12-09T18:22:36.637" Id="127354" LastActivityDate="2014-12-09T18:22:36.637" OwnerUserId="7290" ParentId="127351" PostTypeId="2" Score="2" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm following an example from the book &quot;R by Example&quot;, where they talk about two-way ANOVA.&lt;/p&gt;&#10;&#10;&lt;p&gt;The database used in &lt;code&gt;poison&lt;/code&gt;. The analysis is:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;L &amp;lt;- aov(Time ~ Poison * Treatment, data = poison)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Further on, the book says:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The residual plots suggest a reciprocal transformation of the response&#10;  (poison survival time) (The dependent variable&lt;code&gt;Time&lt;/code&gt;)...&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;That is, a more appropriate variable would be &lt;code&gt;1/Time&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is the residual plot, using &lt;code&gt;plot(L)&lt;/code&gt;:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/WWeUq.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I guess that the reciprocal relationship is evident by the gradually increasing residuals. Why is this correct?&lt;/p&gt;&#10;&#10;&lt;p&gt;When making the reciprocal model using &lt;code&gt;L &amp;lt;- aov(1/Time ~ Poison * Treatment, data = poison)&lt;/code&gt;, the residual plot no longer has this property:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/R46Zl.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;So my questions is &lt;strong&gt;how could I've known that the special pattern in the first residual plot suggests a reciprocal relationship?&lt;/strong&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-12-09T18:28:20.340" FavoriteCount="1" Id="127356" LastActivityDate="2014-12-09T22:01:06.933" LastEditDate="2014-12-09T22:01:06.933" LastEditorUserId="28666" OwnerUserId="53419" PostTypeId="1" Score="6" Tags="&lt;r&gt;&lt;self-study&gt;&lt;anova&gt;&lt;residuals&gt;" Title="How to tell that a reciprocal relationship exists by a residual plot?" ViewCount="48" />
  
  
  <row Body="&lt;p&gt;Logistic regression doesn't really have an error term.  Alternatively, you can think of the response distribution (the binomial) as having its random component intrinsically 'built-in' (for more, it may help to read my answer here: &lt;a href=&quot;http://stats.stackexchange.com/a/30909/7290&quot;&gt;Difference between logit and probit models&lt;/a&gt;).  As a result, I think it is conceptually clearer to generate data for simulations directly from a binomial parameterized as the logistic transformation of the structural component of the model, rather than use the logistic as a sort of error term.  &lt;/p&gt;&#10;&#10;&lt;p&gt;From there, if you want the long run probability that $Y = 1$ to be $.5$ or $.1$, you just need your structural component to be balanced around $0$ (for $.5$), or $-2.197225$ (for $.1$).  I got those values by converting the response probability to the log odds:&lt;br&gt;&#10;$$&#10;\log(\text{odds}(Y=1)) = \frac{\exp(Pr(Y = 1))}{(1+\exp(Pr(Y = 1))}&#10;$$&#10;The most convenient way to do this will be to use those values for your intercept ($\beta_0$) and have your slope be $0$.  (Alternatively, you can use any two parameter values, $\beta_0$ and $\beta_1$, that you like such that, given your $X$ values, the log mean odds equals, e.g., $-2.197225$.)  Here is an example with &lt;code&gt;R&lt;/code&gt; code:  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lo2p = function(lo){      # this function will perform the logistic transformation&#10;  odds = exp(lo)          #   of the structural component of the data generating&#10;  p    = odds / (1+odds)  #   process&#10;  return(p)&#10;}&#10;&#10;N     = 1000              # these are the true values of the DGP&#10;beta0 = -2.197225&#10;beta1 = 0&#10;&#10;set.seed(8361)            # this makes the simulation exactly reproducible&#10;x     = rnorm(N)&#10;lo    = beta0 + beta1*x&#10;p     = lo2p(lo)          # these will be the parameters of the binomial response&#10;&#10;b0h   = vector(length=N)  # these will store the results&#10;b1h   = vector(length=N)&#10;y1prp = vector(length=N)  # (for the proportion of y=1)&#10;&#10;for(i in 1:1000){         # here is the simulation&#10;  y        = rbinom(n=N, size=1, prob=p)&#10;  m        = glm(y~x, family=binomial)&#10;  b0h[i]   = coef(m)[1]&#10;  b1h[i]   = coef(m)[2]&#10;  y1prp[i] = mean(y)&#10;}&#10;&#10;mean(b0h)                 # these are the results&#10;# [1] -2.205844&#10;mean(b1h)&#10;# [1] -0.0003422177&#10;mean(y1prp)&#10;# [1] 0.100036&#10;hist(b0h)&#10;hist(b1h)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/SbSNN.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/nPjs8.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-12-09T19:07:47.923" Id="127362" LastActivityDate="2014-12-09T21:22:43.023" LastEditDate="2014-12-09T21:22:43.023" LastEditorUserId="7290" OwnerUserId="7290" ParentId="127226" PostTypeId="2" Score="2" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm trying to fit 2D ellipticals gaussians to an image (with correlated background noise) using prewritten fitting software. I'd like to know about the uncertainty in the fit parameters with this software. To do this I created many simulated images with known &quot;true&quot; source parameters and ran those through the fitting software. Theres 6 fit parameters - gaussian peak, x center position, y center position ,major axis size, minor axis size, and rotation angle - and one calculated parameter of the total integrated power (function of the peak and sizes).  &lt;/p&gt;&#10;&#10;&lt;p&gt;What I want to be able to do is to take my real data and fit it and then use the simulation data to say for a source with fitted parameters x_i the error on the peak is y, error on size is j, etc. But I get confused on how best to do this as the parameters are not independent. &lt;/p&gt;&#10;&#10;&lt;p&gt;My initial attempt went like: &#10;1. bin sim data by fitted peak (or Signal to noise ratio)&#10;2. compute ratio of true value to fit value for each parameter&#10;3. compute mean of ratios and standard deviation in each bin&#10;4. fit a function to the SD of the ratios as function of fitted peak&#10;5. take value of fitted peak for real data and use function to calculate what SD should be&lt;/p&gt;&#10;&#10;&lt;p&gt;But theres two problems with this: &#10;1. it assumes that the mean of the ratio is 1 (so that the real data has a true value equal to the fit value +/- the computed SD).&#10;2. The SD for the parameters is not just a function of the fitted peaks - i.e. the sources with a low peak and an oversized value for the major axis size will not have the SD as sources with just low peak but size close to true value.&lt;/p&gt;&#10;&#10;&lt;p&gt;So then I thought to try binning by fitted peak and fitted size - but this parameter space is not very evenly filled out (generally mainly its only sources with low peak values that have large size values). And just not sure best way to interpolate or fit a function at that point. &lt;/p&gt;&#10;&#10;&lt;p&gt;So I'm wondering if anyone has any thoughts on something like this. Maybe I'm approaching this wrong, and Im sure theres better ways. I'm just getting hung up on the best way to utilize all this simulated data for my real data. Any input would be appreciated.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-09T20:17:17.403" Id="127368" LastActivityDate="2014-12-09T20:17:17.403" OwnerUserId="29604" PostTypeId="1" Score="1" Tags="&lt;simulation&gt;&lt;fitting&gt;&lt;uncertainty&gt;" Title="estimating model fit parameters using simulations" ViewCount="15" />
  <row AnswerCount="0" Body="&lt;p&gt;I’d like to model repeating peaks of various periodicity of a time series as a curve.&#10;&lt;strong&gt;Here’s the general scenario:&lt;/strong&gt; &#10;A device under measurement experiences reasonably regular voltage spikes every N minutes/hours. These spikes vary in amplitude by their periodicity. These spike cycles come and go.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;For example:&lt;/strong&gt;&#10;A device may experience a low amplitude spike every 8 minutes and a medium amplitude spike every 40 minutes.&lt;br/&gt; &#10;This 8 minute cycle may go on for a few hours/days and then never return.&lt;br/&gt; &#10;A new voltage spike cycle may appear at any time. For example, in addition to the 8 and 40 minute spike cycle a 12 minute cycle may appear and continue for a while.&lt;/p&gt;&#10;&#10;&lt;p&gt;Autocorrelation does a reasonably good job at finding the spike cycles once they appear. Given the spike cycles are of different amplitude and periodicity I’d like to project them forward and visualize them as a curve where peaks would indicate time periods when several coincident spikes are projected to occur. I.e. times of high coincident spike cycle convergence. I can therefore be prepared days/weeks in advance for the voltage spike anomalies.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt;: What would you suggest as effective methods to model and visualize these spike cycles (maybe as a curve) and project them forward in time.&lt;br/&gt;&#10;Thanks&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-09T20:45:32.070" Id="127371" LastActivityDate="2014-12-09T20:45:32.070" OwnerUserId="62459" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;forecasting&gt;" Title="How to model and forecast spike cycles in a time series" ViewCount="24" />
  
  <row Body="&lt;p&gt;One way of thinking about why least squares regression (and other methods, but I'm assuming this is what you're asking about) is useful is thinking about the problem of &lt;em&gt;distinguishing different effects&lt;/em&gt;. In other words, regression allows us to determine the &lt;em&gt;unique&lt;/em&gt; effect that X has on Y and the &lt;em&gt;unique&lt;/em&gt; effect that Z has on Y. If X and Z are related together statistically, then simply regressing Y on X will give an erroneous estimate of the effect of X on Y because some of the effect of Z will be caught up in the regression. The same thing happens if you only regress Y on Z. The cool thing about regression is that it allows us to see the unique effect that each predictor has on the response variable, even if our predictors are themselves related.&lt;/p&gt;&#10;&#10;&lt;p&gt;That being said, it looks like you need to read up or review the basics of regression itself. This is especially true if you're using regression methods to make arguments in a thesis.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-09T21:35:03.470" Id="127381" LastActivityDate="2014-12-09T21:35:03.470" OwnerUserId="53655" ParentId="127370" PostTypeId="2" Score="1" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm not really sure where to begin with this question, and I'm not sure if I am supplying all the resources necessary to answer this question, so just let me know and I will update. I'm going to copy and paste the question, then write up the relevant information needed as the question progresses.&lt;/p&gt;&#10;&#10;&lt;p&gt;$\textit{Test for independence}$. If the $p$ variables in a MVN sample are independently distributed&#10;then&lt;/p&gt;&#10;&#10;&lt;p&gt;$ \Sigma = diag(\sigma_{11},...,\sigma_{pp})$&lt;/p&gt;&#10;&#10;&lt;p&gt;Show that the m.l.e. of $\sigma_{ii}$ is $s_{ii}$ the i&#10;th diagonal element of the sample covariance matrix $S$:&#10;Hence that&lt;/p&gt;&#10;&#10;&lt;p&gt;$\hat{\Sigma} = diag(s_{11},...s_{pp})= D$&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, we are given earlier on that&lt;/p&gt;&#10;&#10;&lt;p&gt;$S = &#10;\begin{pmatrix}&#10;91.48&amp;amp;66.88\\&#10;66.88&amp;amp;96.78\\&#10;\end{pmatrix}$&lt;/p&gt;&#10;&#10;&lt;p&gt;So where do I begin?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-12-09T23:48:51.293" Id="127396" LastActivityDate="2014-12-09T23:48:51.293" OwnerUserId="57064" PostTypeId="1" Score="1" Tags="&lt;self-study&gt;&lt;multivariate-analysis&gt;" Title="Need help understanding hypothesis tests in multivariate statistics" ViewCount="21" />
  
  
  
  
  <row Body="&lt;p&gt;Correlation is $\rho=\frac{Cov[x,y]}{\sqrt{Var[x]Var[y]}}$&lt;/p&gt;&#10;&#10;&lt;p&gt;I would compute everything based off the table you gave like this in MATLAB:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x=[5    6    7]&#10;y=[10:10:50]'&#10;&#10;a= [   550  168  333    ;...&#10;       390  133  299;...&#10;        280  135  255;...&#10;        145  100  34;...&#10;        130  54   12]&#10;&#10; n =sum(sum(a))&#10; mu_x = sum(a)*x'/n&#10; mu_y = sum(a,2)'*y/n&#10; mu_xy = sum(sum(y*x.*a))/n&#10; cov_xy = mu_xy - mu_x*mu_y&#10;&#10; mu_x2 = sum(a)*x.^2'/n&#10; var_x = mu_x2 - mu_x^2&#10; mu_y2 = sum(a,2)'*y.^2/n&#10; var_y = mu_y2 - mu_y^2&#10;&#10; corr = cov_xy/sqrt(var_x*var_y)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;OUTPUT:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x =&#10;&#10;     5     6     7&#10;&#10;&#10;y =&#10;&#10;    10&#10;    20&#10;    30&#10;    40&#10;    50&#10;&#10;&#10;a =&#10;&#10;   550   168   333&#10;   390   133   299&#10;   280   135   255&#10;   145   100    34&#10;   130    54    12&#10;&#10;&#10;n =&#10;&#10;        3018&#10;&#10;&#10;mu_x =&#10;&#10;   5.813783962889331&#10;&#10;&#10;mu_y =&#10;&#10;  22.534791252485089&#10;&#10;&#10;mu_xy =&#10;&#10;     1.302120609675282e+02&#10;&#10;&#10;cov_xy =&#10;&#10;  -0.800347023228426&#10;&#10;&#10;mu_x2 =&#10;&#10;  34.569913850231941&#10;&#10;&#10;var_x =&#10;&#10;   0.769829883082771&#10;&#10;&#10;mu_y2 =&#10;&#10;     6.538436050364480e+02&#10;&#10;&#10;var_y =&#10;&#10;     1.460267882433695e+02&#10;&#10;&#10;corr =&#10;&#10;  -0.075485699630891&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-12-10T04:12:40.907" Id="127429" LastActivityDate="2014-12-10T04:12:40.907" OwnerUserId="36041" ParentId="127416" PostTypeId="2" Score="4" />
  <row AcceptedAnswerId="127462" AnswerCount="1" Body="&lt;p&gt;Hi I am currently trying to simulate an AR(4) process $y_t=0.67y_{t-1}-0.51y_{t-4}+\epsilon_t$ given that the initial value $y_1=1,y_2=2,y_3=3,y_4=4$ and $\epsilon_t\sim N(0,1)$. My code is given as below&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; y &amp;lt;- arima.sim(34,model=list(ar=c(0.6,0,0,-0.5)),&#10;start.innov=c(1,2,3,4),n.start=4,innov=c(0,0,0,0,rnorm(30,mean=0,sd=1)))&#10;&#10;&amp;gt; y&#10;Time Series:&#10;Start = 1 &#10;End = 34 &#10;Frequency = 1 &#10; [1]  3.541600000  0.824960000 -1.785024000 -4.439014400 -5.947049868 -4.222318979 -2.326217678  1.144088356  5.150246659  4.159775404  5.767793661&#10;[12]  0.967415814 -1.472489589 -3.311269208 -3.235976179 -1.787193697  0.855102209  1.619000292  3.488545520  2.288206839  1.247018920 -0.197260086&#10;[23] -1.554881352 -2.202977785 -1.038750927 -1.255710320 -0.006073465  0.080668247  0.461419926 -0.274830999  0.806213750  0.216685532 -0.398132839&#10;[34]  0.122593066&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;While what I am expecting is that the first four values of $y$ to be 1,2,3,4 say since I've set the first four entries of innov to be zero. Can someone help me with this problem? Thanks in advance. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-12-10T06:46:21.530" Id="127441" LastActivityDate="2014-12-10T09:19:16.227" LastEditDate="2014-12-10T07:49:22.213" LastEditorUserId="35989" OwnerUserId="62418" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;regression&gt;&lt;time-series&gt;" Title="Initialize AR(p) process by using Arima.sim" ViewCount="53" />
  <row AnswerCount="1" Body="&lt;p&gt;I am reading a paper named &quot;EPRI-GTC Overhead Electric Transmission Line Siting Methodology&quot;, and in this paper the author uses &quot;the first statistical break in its histogram&quot; to determine the corridor boundary. And there is no description about the meaning of 'statistical break' and how to calculate it in this paper. &lt;/p&gt;&#10;&#10;&lt;p&gt;I can't find any info about the term 'statistical break' by Google. Please help me if you know something about it.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here are some sentences in this paper:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;In each scenario, the Macro Corridor boundary is determined by the first statistical break in its histogram. &lt;/p&gt;&#10;  &#10;  &lt;p&gt;The boundaries of these corridors are chosen by the first statistical break in the histogram.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="3" CreationDate="2014-12-10T08:37:15.323" Id="127458" LastActivityDate="2014-12-10T10:14:56.930" LastEditDate="2014-12-10T10:14:56.930" LastEditorUserId="17230" OwnerUserId="62492" PostTypeId="1" Score="1" Tags="&lt;summary-statistics&gt;&lt;descriptive-statistics&gt;" Title="What's the meaning of 'statistical break'? And how to calculate it?" ViewCount="24" />
  <row Body="&lt;p&gt;You could try &lt;a href=&quot;http://www-bcf.usc.edu/~gareth/ISL/&quot; rel=&quot;nofollow&quot;&gt;Introduction to Statistical Learning&lt;/a&gt; by Gareth James et al. It's freely available, contains introductory-level review and discusion of all the topics you mention (in particular Chapter 6 deals with regularization), is well-supported by the ISLwR package in R and provides a gateway to the more advanced counterpart &lt;a href=&quot;http://statweb.stanford.edu/~tibs/ElemStatLearn/&quot; rel=&quot;nofollow&quot;&gt;The Elements of Statistical Learning&lt;/a&gt; by Hastie et al.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-12-10T08:40:21.593" Id="127459" LastActivityDate="2014-12-10T08:50:10.780" LastEditDate="2014-12-10T08:50:10.780" LastEditorUserId="61519" OwnerUserId="61519" ParentId="127444" PostTypeId="2" Score="1" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;We have contracted out collection of survey questions for the group of our customers. These customers even when sampling all of these represent less than 1% our our customer base.  &lt;/p&gt;&#10;&#10;&lt;p&gt;We would like to know how they perceive our offering and how they selected this product. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have prepared sample with two subgroups: &lt;/p&gt;&#10;&#10;&lt;p&gt;1) All customers with this rare product&lt;br&gt;&#10;2) Same number of customers who do not have the product but which are matched by certain attributes (gender, age, region etc)  &lt;/p&gt;&#10;&#10;&lt;p&gt;Matching is frequency based, there is no x strata with 2 observations each. In this case there is 2-20 observations in each strata. &lt;/p&gt;&#10;&#10;&lt;p&gt;Now, how I can use classification algorithm for this data set after I have survey questions in hand? Can I use unconditional logistic regression or must I only use conditional logistic regression with those variables which are not used in matching?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-10T11:29:00.387" Id="127476" LastActivityDate="2014-12-10T11:29:00.387" OwnerUserId="28732" PostTypeId="1" Score="0" Tags="&lt;logistic&gt;&lt;sampling&gt;&lt;survey&gt;&lt;case-control-study&gt;&lt;clogit&gt;" Title="Inference possibilities for matched case-control study" ViewCount="31" />
  <row AnswerCount="1" Body="&lt;p&gt;Consider two &lt;a href=&quot;http://en.wikipedia.org/wiki/Poisson_binomial_distribution&quot; rel=&quot;nofollow&quot;&gt;Poisson binomial&lt;/a&gt; distributions (distributions of sums of independent Bernoulli variables) as below:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;The distribution of the sum of 99 variables with probability 0.01 (of taking the value 1), and 1 variable with probability 0.05.&lt;/li&gt;&#10;&lt;li&gt;The distribution of the sum of 999 variables with probability 0.01, and 1 variable with probability 0.05.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;We expect that each distribution will approximate to a Poisson distribution:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$Pr(k)=\frac{\lambda^ke^{-\lambda}}{k!}.......(k=0,1,2,...)&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;with $\lambda = 1.04$ for Distribution 1 and $\lambda = 10.04$ for Distribution 2.&lt;/p&gt;&#10;&#10;&lt;p&gt;Intuitively, to me at least, the approximation should be better for Distribution 2, because there are more variables with probability 0.01 to &quot;swamp&quot; the effect of the single variable with a different probability.  However, applying &lt;a href=&quot;http://en.wikipedia.org/wiki/Le_Cam%27s_theorem&quot; rel=&quot;nofollow&quot;&gt;Le Cam's Theorem&lt;/a&gt;, the sums to infinity of the absolute differences in probabilities of particular $k$-values under these distributions from their respective Poisson approximations have the following upper bounds:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$Distribution 1: 2[(99*0.01^2)+(1*0.05^2)] = 0.0248$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$Distribution 2: 2[(999*0.01^2)+(1*0.05^2)] = 0.2048$$&lt;/p&gt;&#10;&#10;&lt;p&gt;If these sums can be taken as measures of goodness of approximation, they suggest that the approximation is much better for Distribution 1.&lt;/p&gt;&#10;&#10;&lt;p&gt;How can this puzzle be resolved?  Is there any result that goes beyond Le Cam's Theorem by saying something about the absolute differences in probabilities of particular $k$-values,  and not merely about their sum? It would be helpful for example if it could be shown that at most only a small part of the 0.2048 for Distribution 2 can relate to any particular $k$-value.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;&#10;I calculated the differences between the exact probabilities of $k$-values and their Poisson approximations, obtaining the results below:&lt;/p&gt;&#10;&#10;&lt;p&gt;Distribution 1: Sum of absolute differences for $k=0,...,10 = 0.0066$.  Largest absolute difference for a particular $k$ is $0.0022$ for $k=0$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Distribution 2: Sum of absolute differences for $k=0,...,30 = 0.0050$.  Largest absolute difference for a particular $k$ is $0.0006$ for $k=10$.&lt;/p&gt;&#10;&#10;&lt;p&gt;I did not calculate beyond 10 and 30 respectively as the probabilities then became very small and appeared unlikely to contribute much to the sums to infinity.  These results suggest that the Poisson approximation is indeed better for Distribution 2.  They also suggest that the upper bounds obtained from Le Cam's Theorem (though perfectly correct) do not necessarily provide useful measures of goodness of approximation because for particular distributions the actual sums of absolute differences may lie well within those bounds.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-10T13:19:52.963" Id="127485" LastActivityDate="2014-12-12T13:13:56.653" LastEditDate="2014-12-12T13:13:56.653" LastEditorUserId="11060" OwnerUserId="11060" PostTypeId="1" Score="4" Tags="&lt;distributions&gt;&lt;poisson&gt;" Title="Which of these distributions approximates better to a Poisson distribution?" ViewCount="104" />
  <row AnswerCount="0" Body="&lt;p&gt;Suppose I have a collection of sets of numbers, lets say that each set has twenty five numbers in it (one for each day in a twenty five day period). Lets say I have 200 of these sets.&lt;/p&gt;&#10;&#10;&lt;p&gt;Are there any statistical methods which will help me to identify trends in the data which can predict (based on my existing sets) whether a new sets number on the 25th day day will be lower than on the first day?&lt;/p&gt;&#10;&#10;&lt;p&gt;For instance, suppose I am on the fifth day. Can I calculate a percentage chance that my last day will be lower than the first based on the number of observations I have so far?&lt;/p&gt;&#10;&#10;&lt;p&gt;Any guidance on methods that would point me in the right direction would be much appreciated. I'm using SAS to analyze my datasets.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;&#10;&lt;p&gt;Edit ...&lt;/p&gt;&#10;&#10;&lt;p&gt;I'll try to be more clear ...&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a dataset with 200 rows and 25 columns. Each row represents time series data for a stock that was added to the S&amp;amp;P 500 index. Each column represents the nth day the price was recorded for. So if row one represents Whole Foods then row one column five represents the price for whole foods on the fifth day after it was added to the index. &lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose a new company is added to the index. Each day a price is recorded for the new stock. Each day I want to look at all of the days available to me for the new stock and make a prediction about whether the last days price (column 25) will be lower than the first days price. I want to make that prediction based on the whatever trends exist in the 200 rows of historical data I have in my dataset.&lt;/p&gt;&#10;&#10;&lt;p&gt;Based on some investigation it looks like an autoregression problem for which SAS has proc autoreg. Any experience with that procedure and how it might apply to my particular problem would be appreciated.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;&#10;&lt;p&gt;Brian&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-10T14:09:44.500" FavoriteCount="1" Id="127497" LastActivityDate="2014-12-10T20:58:33.037" LastEditDate="2014-12-10T20:58:33.037" LastEditorUserId="919" OwnerUserId="62517" PostTypeId="1" Score="2" Tags="&lt;time-series&gt;&lt;sas&gt;" Title="Statistical Analysis of a set of numbers to predict final outcome" ViewCount="49" />
  <row AnswerCount="1" Body="&lt;p&gt;Assume following model:&lt;/p&gt;&#10;&#10;&lt;p&gt;$Sales_i = a +b_i*Price*Dummy+Advertsing*Dummy$&lt;/p&gt;&#10;&#10;&lt;p&gt;$Sales_i$ represent the sales in retailer i (either 1 or 2) and $Dummy$ is a dummy variable giving value 1 to retailer 2 and 0 to retailer 1.&lt;/p&gt;&#10;&#10;&lt;p&gt;Basically, I want to test formally whether the effects of price and advertising on the sales in retailer 1 differs significantly from the effects of price and advertising on sales in store 2.&lt;/p&gt;&#10;&#10;&lt;p&gt;I thought I could do this using an F-test. So I fitted a model $Sales = a+b_1*Price+b_2Advertising$ (no distinction between retailers here) and requested an anova table:&lt;/p&gt;&#10;&#10;&lt;p&gt;anova(model1, model2)&lt;/p&gt;&#10;&#10;&lt;p&gt;I am not sure if this is the right way to test this. Also, the anova table did not provide any P-value, so I don't know whether te reject or accept the F-test.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can somebody please help me solving this?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-12-10T14:14:05.087" FavoriteCount="1" Id="127498" LastActivityDate="2015-01-28T22:28:18.573" LastEditDate="2014-12-10T17:40:20.823" LastEditorUserId="62109" OwnerUserId="62109" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;f-test&gt;" Title="F-test for dummies" ViewCount="86" />
  <row AcceptedAnswerId="127510" AnswerCount="1" Body="&lt;p&gt;I've read through the most popular threads concerning confounding variables, but I haven't been able to find an answer to my specific question. Sorry for the wall of text, I hope it's clear enough. Thanks in advance for your time!&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm attempting to build a predictive multiple regression model for $weight$ using a dataset with a large number of (mostly categorical) variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;During my exploratory analysis I stumbled across the following problem. A simple scatter plot of my response vs $age$ revealed that there was no simple linear relation between these two variables. In fact, it seems like it would be possible to fit a straight line with a positive slope through the observations below a certain age. For older observations, there does not seem to be a relation between $age$ and $weight$. Of course this makes sense if you consider the fact that human beings stop growing after a certain age.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is: will it always be possible to perform a transformation on my response/predictors, so that I can fit a linear model on my data? Is there a boxcox-like procedure for predictor variables? I've attempted a log, square root and even cubed root transformation of age, but none of those seem to be the magic fix.&lt;/p&gt;&#10;&#10;&lt;p&gt;If I cannot find a useful transformation, does this then indicate that there is a confounding variable I should include in the model (or subset my data for)? There are plenty of examples on how adding a categorical variable (e.g. gender) could reveal a hidden association. In my case however, one possible categorical variable would simply be 'adult vs child'. Would it make sense to use an age category (adult/child) as a categorical factor in a model where $age$ is also my continuous predictor? While it would be strange to use the same variable twice (in a sense), this would allow me to properly capture the relationship I'm seeing. &lt;/p&gt;&#10;&#10;&lt;p&gt;Simple regression models of my other predictor variables were often only significant when performed on a certain age group. If I cannot fit an adequate curve on $age$, I will not be able to account for these differences between adults and children in my final multiple regression model, unless I use the categorical 'adult-child' variable. I would however lose out on the positive relation between $weight$ and $age$ (continuous) for children.&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT: Some additional information on why I believe I need to transform $age$: while the simple linear regression model of $weight$ ~ $age$ is significant, it has a low $R^2$ and the residual plot shows a parabola-like pattern.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-12-10T15:10:50.823" Id="127508" LastActivityDate="2014-12-10T17:19:14.640" LastEditDate="2014-12-10T17:19:14.640" LastEditorUserId="28500" OwnerUserId="62518" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;data-transformation&gt;&lt;confounding&gt;" Title="Transformation necessary or look for confounding variables" ViewCount="22" />
  <row Body="&lt;p&gt;I think I understand your problem now. You alluded to your assumption that somehow KS test should show that all bootstrapped samples should be shown to be from the original sample. However, consider this: what does it mean to show that they're from the same distribution?&lt;/p&gt;&#10;&#10;&lt;p&gt;It usually means that p-value is over some $\alpha$ confidence. If bootstrapping is done properly you'll get p-value sometimes over, sometimes under the $\alpha$. Build the distribution of test statistics you get from running KS test on bootstrapped samples. Observe p-values for various critical values, they should match the theoretical values for which KS test was designed.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-12-10T16:18:19.680" Id="127520" LastActivityDate="2014-12-10T16:18:19.680" OwnerUserId="36041" ParentId="127492" PostTypeId="2" Score="2" />
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I would like to know how 'loess' and 'locfit' functions in R choose their smoothing parameters.&lt;/p&gt;&#10;&#10;&lt;p&gt;'loess' help file states that &quot;For α &amp;lt; 1, the neighbourhood includes proportion α of the points, and these have tricubic weighting (proportional to (1 - (dist/maxdist)^3)^3).&quot; which is quite clear.&lt;/p&gt;&#10;&#10;&lt;p&gt;For 'locfit', it is not so straightforward. In the help file of 'locfit.raw' function, it reads that &quot;A single number (e.g. alpha=0.7) is interpreted as a nearest neighbor fraction. With two componentes (e.g. alpha=c(0.7,1.2)), the first component is a nearest neighbor fraction, and the second component is a fixed component. A third component is the penalty term in locally adaptive smoothing.&quot; in which a single number case seems to be same as 'loess'.&lt;/p&gt;&#10;&#10;&lt;p&gt;But the Section 7 of this document reads that (&lt;a href=&quot;http://www.statistik.lmu.de/~leiten/Lehre/Material/GLM_0708/Tutorium/locfit.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.statistik.lmu.de/~leiten/Lehre/Material/GLM_0708/Tutorium/locfit.pdf&lt;/a&gt;)&#10;&quot;Locfit differs from Loess in the way points are selected for direct fitting. While Loess&#10;bases its choice on the density of data points, Locfit uses the bandwidths at fitting points. This allows Locfit to adapt to different bandwidth schemes: fixed, nearest-neighbor, and locally adaptive.&quot;&#10;It states that there are different bandwidth schemes (fix, nearest-neighbor and locally adaptive). Does this 'fix' means that fixed bandwidths for all fitting points?&#10;I'm especially interested in how 'locfit' decides the bandwidths when the user uses no option (default). &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-10T21:03:24.923" Id="127564" LastActivityDate="2014-12-10T21:03:24.923" OwnerUserId="15997" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;loess&gt;" Title="How does 'locfit' choose its smoothing parameter?" ViewCount="20" />
  <row AcceptedAnswerId="127587" AnswerCount="3" Body="&lt;p&gt;When I was writing R code to plot two empirical cumulative density curves, I came across &lt;code&gt;ks.test&lt;/code&gt; in R. I looked into the code of &lt;code&gt;ks.test&lt;/code&gt; and ran into this question. &lt;/p&gt;&#10;&#10;&lt;p&gt;I am wondering whether anyone can explain to me why this expression &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;z &amp;lt;- cumsum(ifelse(order(w) &amp;lt;= n.x, 1/n.x, -1/n.y))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;gives the maximum $D$ values, specifically why &lt;code&gt;order(w)&amp;lt;=n.x&lt;/code&gt;? Thanks!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-10T22:24:06.723" FavoriteCount="2" Id="127576" LastActivityDate="2014-12-11T18:26:47.410" LastEditDate="2014-12-10T22:28:41.113" LastEditorUserId="805" OwnerUserId="62475" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;algorithms&gt;&lt;kolmogorov-smirnov&gt;" Title="what is the algorithm of getting maximum $D$ value in ks.test?" ViewCount="84" />
  
  <row Body="&lt;p&gt;This is clever code; it deserves to be understood well.  I will therefore unravel it carefully, explaining each part.  As with much functional code, we have to work from the inside out.&lt;/p&gt;&#10;&#10;&lt;p&gt;The calculation is &lt;em&gt;apropros&lt;/em&gt; the &lt;a href=&quot;http://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test#Two-sample_Kolmogorov.E2.80.93Smirnov_test&quot; rel=&quot;nofollow&quot;&gt;two-sample Kolmogorov-Smirnov test.&lt;/a&gt;  One sample has been stored in array $\mathrm{x}=(x_1,x_2,\ldots, x_n)$ and the other in $\mathrm{y}=(y_1,y_2,\ldots,y_m)$ (where to avoid towers of subscripts I write $n$ for &lt;code&gt;n.x&lt;/code&gt; and $m$ for &lt;code&gt;n.y&lt;/code&gt;).  The array $\mathrm w$ is their concatenation&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\mathrm{w} = (x_1, x_2, \ldots, x_n,\, y_1, y_2, \ldots, y_m).$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The purpose of &lt;code&gt;order&lt;/code&gt; is to give the &lt;em&gt;indexes&lt;/em&gt; of $\mathrm{w}$ in increasing sequence.  Writing the output of &lt;code&gt;order&lt;/code&gt; as $\sigma = (\sigma(1), \sigma(2), \ldots, \sigma(n+m))$, this means that&lt;/p&gt;&#10;&#10;&lt;p&gt;$$w_{\sigma(i)} \le w_{\sigma(j)}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;whenever $i \lt j$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The expression &lt;code&gt;order(w) &amp;lt;= n.x&lt;/code&gt; creates an indicator vector with only zeros and ones for its entries.  It will have ones exactly at those places where $\sigma(i)\le n$.  Since $\sigma(i)$ is an entry into $\mathrm w$, and the first $n$ entries of $\mathrm w$ come from $\mathrm x$, this is tantamount to creating an indicator of &lt;em&gt;where the elements of $\mathrm x$ would be after sorting $\mathrm w$.&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The effect of &lt;code&gt;cumsum(ifelse&lt;/code&gt; is to use this indicator to walk around on a vertical line starting at zero.  &lt;strong&gt;Every time an element of $\mathrm x$ is encountered, move $1/n$ up; every time an element of $\mathrm y$ is encountered, move $1/m$ down.&lt;/strong&gt;  &lt;/p&gt;&#10;&#10;&lt;p&gt;Compare this recipe to the following illustration of the empirical distribution functions (posted in an answer to a related question at &lt;a href=&quot;http://stackoverflow.com/questions/27233738&quot;&gt;http://stackoverflow.com/questions/27233738&lt;/a&gt;, but with the roles of $\mathrm x$ and $\mathrm y$ reversed there):&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/JEZ9e.png&quot; alt=&quot;Figure&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The red graph is the empirical CDF (ECDF) of $\mathrm{x}$: by construction, as you scan from left to right, it leaps upward by $1/n$ each time an element of $\mathrm{x}$ is encountered on the horizontal axis.  Similarly, the blue graph is the empirical CDF of $\mathrm{y}$; its leaps occur in quantum units of $1/m$.  The vertical black line shows one stage of this left-to-right scan; its vertical length represents the difference between the red graph and the blue graph.  (If the red graph drops below the blue graph, the length is considered &lt;em&gt;negative&lt;/em&gt;.)&lt;/p&gt;&#10;&#10;&lt;p&gt;We can know the length of the vertical line in a simple fashion.  At the outset, it has zero length.  Every time we scan across an element of $\mathrm {x}$ it lengthens by $1/n$, but every time we scan across an element of $\mathrm{y}$ it is &lt;em&gt;shortened&lt;/em&gt; by $1/m$.  As we saw above, that's precisely what the magic &lt;code&gt;R&lt;/code&gt; code does!&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Consequently,&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;strong&gt;cumsum(ifelse(order(w) &amp;lt;= n.x, 1/n.x, -1/n.y))&lt;/strong&gt;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;is an array that contains all the distinct (signed) lengths attained by the vertical black line throughout the scan.&lt;/strong&gt;  &lt;em&gt;Assuming there are no ties&lt;/em&gt; between some value of $\mathrm x$ and some value of $\mathrm y$ (which is an assumption of this test which is explicitly checked in the code beforehand), its maximum is the greatest distance reached by the ECDF of $\mathrm{x}$ &lt;em&gt;above&lt;/em&gt; the ECDF of $\mathrm {y}$, while its minimum is the negative of the greatest distance reached by the ECDF of $\mathrm{x}$ &lt;em&gt;below&lt;/em&gt; the ECDF of $\mathrm {y}$.  Their absolute values are precisely the two Kolmogorov-Smirnov statistics $D^{+}$ and $D^{-}$ and the larger of those two absolute values is the K-S statistic $D$.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-12-11T00:33:27.550" Id="127591" LastActivityDate="2014-12-11T18:26:47.410" LastEditDate="2014-12-11T18:26:47.410" LastEditorUserId="919" OwnerUserId="919" ParentId="127576" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;This one needs to be put to bed.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;I am interested in learning more about applying Bayesian linear models for covariates some of which are continuous and some are binary. &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;As people have pointed out in comments, there's no distinction - linear regression is linear regression whether your x's are continuous or binary. This applies to Bayesian linear regression as well. &lt;/p&gt;&#10;&#10;&lt;p&gt;If they were &lt;em&gt;all&lt;/em&gt; binary, you might call it Bayesian ANOVA perhaps, but it's still also Bayesian regression.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;What is the appropriate terminology for such models so that I can search online for literature related to these models.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;It's all just Bayesian linear regression.&lt;/p&gt;&#10;&#10;&lt;p&gt;(There are also a number of questions on site that discuss aspects of the topic.)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-12-11T02:11:03.063" Id="127599" LastActivityDate="2014-12-11T02:11:03.063" OwnerUserId="805" ParentId="126822" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;I assume this is vanilla Gibbs. You don't say if you're programming from scratch or what. I'll start with some basic hints as if you were writing the whole thing.&lt;/p&gt;&#10;&#10;&lt;p&gt;The Gibbs sampler relies on sampling from full conditional distributions, so you need to start trying to write down all your conditional posteriors (so you can see how to sample from them)&lt;/p&gt;&#10;&#10;&lt;p&gt;So can you write down the conditional posterior for say $\left[\mu_1|\mathbf{x,\theta}\right]$?&lt;/p&gt;&#10;&#10;&lt;p&gt;(If you know $\mathbf{\theta}$, what distribution do you think it should look like?)&lt;/p&gt;&#10;&#10;&lt;p&gt;Or $\left[\theta_i|\mathbf{X,\mu,...}\right]$? &lt;/p&gt;&#10;&#10;&lt;p&gt;(What family would you expect the posterior to be from?)&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-12-11T04:23:42.980" Id="127608" LastActivityDate="2014-12-11T04:23:42.980" OwnerUserId="805" ParentId="127604" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="128629" AnswerCount="1" Body="&lt;p&gt;I wish to model the retransmission time of a file that divided into K blocks. I know the successful blocks of first transmission obey the binomial distribution &#10;$$&#10;X_1 \sim \text B(K,p)&#10;$$&#10;, p is the successful probability of packet delivery. Then, the first retransmission(second transmission) try to complete the file delivery with retransmitting the $N_2 = K-X_1$ packets, and the number of successful packets received is $X_2$.&#10;$$&#10;X_2 \sim \text B(K-X_1,p)&#10;$$&#10;The remainder of retransmission follows:&#10;$$&#10;N_i = K - \sum_{m=1}^{i-1}X_m \\&#10;X_i \sim \text B(N_i,p)&#10;$$&#10;We stop the file transaction when $\sum_{m=1}^L X_m = K$. &#10;&lt;br&gt;&#10;Eventually, I wish to derive the probability density function of $L$. I think the hard point to model $L$ is the parameter of $X_i$'s distribution depends on $X_{i-1}$.&#10;&lt;br&gt;&#10;I know the &quot;multi stage binomial tree model&quot; might be suffice for deriving the PDF of $L$. I also consider if there is a existing stochastic process models this process. But i can't find any. &#10;&lt;br&gt;&#10;Any suggestion about the theoretical pdf, stochastic process or approximation is appreciated.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-12-11T05:12:04.800" FavoriteCount="1" Id="127614" LastActivityDate="2014-12-11T09:48:00.383" OwnerUserId="62561" PostTypeId="1" Score="5" Tags="&lt;distributions&gt;&lt;binomial&gt;&lt;stochastic-processes&gt;&lt;non-independent&gt;&lt;summations&gt;" Title="multi stage binomial &quot;process&quot;" ViewCount="31" />
  <row Body="&lt;p&gt;A logit model is a linearization of a nonlinear relationship between the X's and the dichotomous outcome. The slope is therefore constant everywhere in X. &lt;/p&gt;&#10;&#10;&lt;p&gt;As far as the predicted probabilities, the average predicted probability is just the proportion of 1's in your data. So you want to identify for what value of X does an observation have a higher probability of being 1 than the overall proportion. You can do that a number of ways - one way would be to estimate the predicted probabilities for each level of X (keeping other variables in your model at their means), sort them and see at which point is it higher than the overall. (You can find this out by looking at the coefficients and doing some arithmetic, but this may be easier if you have trouble with converting log-odds to p).&lt;/p&gt;&#10;&#10;&lt;p&gt;You can also estimate a confidence interval around the marginal effect (i.e. average probability) at each level. I haven't done this in R though it probably isn't that complicated - in Stata, you run your logit and then do margins. For example, I simulated some data where the overall sample probability is 0.391, and one standard normal predictor x, where the true logit coefficient was 2. In Stata, here is my output:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    logit y x&#10;&#10;Logistic regression                               Number of obs   =       1000&#10;                                                  LR chi2(1)      =     504.19&#10;                                                  Prob &amp;gt; chi2     =     0.0000&#10;Log likelihood = -417.09877                       Pseudo R2       =     0.3767&#10;&#10;------------------------------------------------------------------------------&#10;           y |      Coef.   Std. Err.      z    P&amp;gt;|z|     [95% Conf. Interval]&#10;-------------+----------------------------------------------------------------&#10;           x |   2.192613   .1399275    15.67   0.000      1.91836    2.466866&#10;       _cons |  -.8138537   .0933571    -8.72   0.000    -.9968302   -.6308772&#10;&#10;margins&#10;&#10;             |            Delta-method&#10;             |     Margin   Std. Err.      z    P&amp;gt;|z|     [95% Conf. Interval]&#10;-------------+----------------------------------------------------------------&#10;       _cons |       .391   .0116073    33.69   0.000     .3682501    .4137499&#10;------------------------------------------------------------------------------&#10;&#10;margins, at(x==0.2)&#10;&#10;------------------------------------------------------------------------------&#10;             |            Delta-method&#10;             |     Margin   Std. Err.      z    P&amp;gt;|z|     [95% Conf. Interval]&#10;-------------+----------------------------------------------------------------&#10;       _cons |   .4072535    .020886    19.50   0.000     .3663176    .4481893&#10;------------------------------------------------------------------------------&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Look at the confidence intervals for the two margins commands and you will see that they overlap, so even though at x=0.2 you see a predicted probability higher than the average, it isn't statistically significant. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt;: Per whuber's comments below, this approach doesn't account for the sampling variation of the mean from the data. It will tell you a difference from a particular value, which may happen in this instance to be the mean of your sample. But it may not address your specific question.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-12-11T06:20:20.490" Id="128615" LastActivityDate="2014-12-12T14:06:02.883" LastEditDate="2014-12-12T14:06:02.883" LastEditorUserId="42898" OwnerUserId="42898" ParentId="127385" PostTypeId="2" Score="0" />
  
