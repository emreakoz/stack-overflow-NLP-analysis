  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I fitted a linear mixed model with R as follows.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lmme3cs = lme(LAZ ~ group_k + x1 + x2 + x4 + x6 + x1_k + x2_k + x4_k + x6_k, random = ~1|SECTORID/CHILDUID,&#10;        correlation = corCompSymm(form = ~1|SECTORID/CHILDUID),data=dat2c)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;It has 50043 observations, but the object 'lmme3cs$fitted' is a 50043 by 3 matrix which has three columns fixed, SECTORID, and CHILDUID.&#10;What are these three columns?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-04T15:07:48.407" Id="101139" LastActivityDate="2015-02-10T20:11:24.777" OwnerUserId="15997" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;mixed-model&gt;&lt;mixed-effect&gt;" Title="Fitted values of 'lme' function result" ViewCount="78" />
  <row AcceptedAnswerId="101194" AnswerCount="1" Body="&lt;p&gt;I am an analyst on a paper and, in writing up methods and results, noted that one of the proposed (logistic regression) models did not converge due to separation. I noted this in the &lt;em&gt;results&lt;/em&gt; section of the paper, but the authors have asked that this be moved to the &lt;em&gt;methods&lt;/em&gt; section. &lt;/p&gt;&#10;&#10;&lt;p&gt;It doesn't seem right to me, as their only justification for doing so is that it is a somewhat technical concept. But it turns out that several of the proposed regression variables were highly correlated. We wouldn't have known that before looking at the data. By that reasoning, it seems intuitive to report this in the results. But I think their issue is that even making mention of the technical aspects detracts from the results (to me, it enhances them).&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a resource to which I can defer that would argue in favor of presenting this evidence one way versus another? Either a methods type article or even an applied article where the authors did an exceptionally good job at describing a proposed analysis that failed to converge?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-04T15:14:33.713" FavoriteCount="1" Id="101144" LastActivityDate="2014-06-04T19:54:39.060" OwnerUserId="8013" PostTypeId="1" Score="3" Tags="&lt;logistic&gt;&lt;convergence&gt;&lt;application&gt;" Title="How to report results from analyses that did not converge" ViewCount="52" />
  <row Body="&lt;p&gt;A better approach would be to compare the chi-square tests' confidence intervals of A and B. If the confidence intervals (for example, 95% confidence intervals) do not overlap, you can conclude that you are 95% confident that A's classes are more homogeneous than B's classes. Otherwise, you do not have evidence to conclude that either groups' classes are more/less homogeneous than the other group's. I hope this helps!&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-06-04T15:31:46.327" Id="101147" LastActivityDate="2014-06-04T15:31:46.327" OwnerUserId="29068" ParentId="100893" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;If there is a significant &lt;em&gt;p&lt;/em&gt;-value for the &lt;em&gt;t&lt;/em&gt;-test (for example, &lt;em&gt;p&lt;/em&gt; &amp;lt; 0.05), the &lt;a href=&quot;http://en.wikipedia.org/wiki/Confidence_interval&quot; rel=&quot;nofollow&quot;&gt;confidence intervals&lt;/a&gt; (CI) will NOT overlap. For example, let's say Group A has an average understanding of 3.5 (95% CI: 2.5-3.75), and Group B has an average understanding of 5.5 (95% CI: 4.5-6.5), we can conclude that we are 95% confident that the mean understanding of subjects from Group B is higher than the mean understanding of subjects from Group A.&lt;/p&gt;&#10;&#10;&lt;p&gt;On the other hand, if there is not a statistically significant &lt;em&gt;p&lt;/em&gt;-test value for the &lt;em&gt;t&lt;/em&gt;-test (for example, p &gt; 0.05), the CIs will overlap. For example, let's say Group A has an average understanding of 3.5 (95% CI: 2.5-3.75), and Group B has an average understanding of 4.5 (95% CI: 3.5-6) -- In other words, there is overlap in the confidence intervals! -- Thus we do NOT have evidence to conclude that the mean understanding of subjects from Group B is different from the mean understanding of subjects from Group A. &lt;/p&gt;&#10;&#10;&lt;p&gt;I hope this helps!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-04T15:46:59.423" Id="101151" LastActivityDate="2014-06-04T15:46:59.423" OwnerUserId="29068" ParentId="100878" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;It is pretty easy to compare two dissimilarity matrices (assuming that is what you mean by compare?).&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, you could ordinate the dissimilarity matrices separately and compare them with Procrustes rotation. Or there is the method of co-intertia analysis which extracts axes that maximise the covariance between the two data sets (cf PCA which extracts axes of maximal variance in the one data set) subject to axes being orthogonal. Co-inertia is based on Euclidean distances so you could apply the Hellinger transformation to the species data and leave the environmental data untransformed, or you might transform some of the env data using say a log transformation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Mantel's (partial) test can also be used to compare associations between two or more dissimilarity matrices.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-04T16:09:49.117" Id="101155" LastActivityDate="2014-06-04T16:09:49.117" OwnerUserId="1390" ParentId="64769" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Is it a problem from a methodological point of view? Sort of, since strictly speaking, SEM assumes that the observed variables are normally distributed, which a fortiori, your likert items are not.&lt;/p&gt;&#10;&#10;&lt;p&gt;So what to do? You could hold your nose and pretend that everything is normal, trusting in the Central Limit Theorem. I would probably do that, at least as a preliminary, to see if there's anything going on.&lt;/p&gt;&#10;&#10;&lt;p&gt;A cleaner solution is to use a SEM method adjusted for likert items. Instead of working with the correlation matrix, these methods treat the likert responses as cut points for an underlying continuous variable, whose correlations one then seeks to estimate. Any time I've done this, all variables have had the same number of likert responses, so I don't know if there's an off-the-shelf package for estimating these correlations with discordant likert items. However it should be possible in principle, and it has probably been done in practice somewhere, by someone. If you are using R, you could check out the &lt;a href=&quot;https://groups.google.com/forum/#!forum/lavaan&quot;&gt;user group&lt;/a&gt; for package lavaan. &lt;/p&gt;&#10;&#10;&lt;p&gt;In answer to your final question, of course you report all this. In the Methods section of your paper, you will have described the data you are using, including it's Likertship and other issues. You can then explain how you addressed the difficulty.&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT. I did some googling and came up with &lt;a href=&quot;http://www.john-uebersax.com/stat/tetra.htm&quot;&gt;this&lt;/a&gt;. There is software that does polychoric correlations with mixed levels. That's what I would advise. Be aware, however, that you need more subjects for polychoric correlations than you would if you could observe the continuous latent variables directly.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-06-04T16:43:24.850" Id="101163" LastActivityDate="2014-06-04T16:43:24.850" OwnerUserId="14188" ParentId="101159" PostTypeId="2" Score="4" />
  <row AnswerCount="1" Body="&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/WbiL9.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I have 10 question, based on 5  Likert scale choices.&#10;How do I find degree of freedom here in Chi square?&#10;Some one says use $(r-1)\cdot (c-1)$ but I think it will apply on categorical data, my data is not that one.&lt;/p&gt;&#10;" ClosedDate="2014-06-07T20:48:39.220" CommentCount="4" CreationDate="2014-06-04T16:53:05.373" Id="101165" LastActivityDate="2014-06-04T20:37:00.317" LastEditDate="2014-06-04T18:20:22.430" LastEditorUserId="88" OwnerUserId="46716" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;hypothesis-testing&gt;&lt;self-study&gt;&lt;chi-squared&gt;&lt;p-value&gt;" Title="Which method of df for chi square I have to apply either $(c-1)\cdot (r-1)$ or df$=n-1$?" ViewCount="45" />
  
  <row Body="&lt;p&gt;You can do the same thing that you did to the columns of the matrix to make them correlated, just do it to the rows instead.  This will adjust the correlation on the observations within a column without affecting the correlations between the columns much:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;L = chol(OCRMt)# Cholesky decomposition&#10;p = dim(L)[1]&#10;&#10;M2 &amp;lt;- t(L) %*% M1&#10;&#10;hist( cor(M2)[ lower.tri(cor(M2), diag=FALSE)])&#10;hist( cor(t(M2))[ lower.tri(cor(t(M2)), diag=FALSE)])&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;You can also create one observation from a distribution with n times p columns, then wrap that into your matrix.  The correlation matrix is the Kronecker product of your other correlation matrices.  My computer ran out of memory for your example, but works for a smaller matrix:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(MASS)&#10;vcmat &amp;lt;- matrix( 0.5, 10,10 )&#10;diag(vcmat) &amp;lt;- 1&#10;&#10;ocmat &amp;lt;- matrix( 0.3, 20,20 )&#10;diag(ocmat) &amp;lt;- 1&#10;&#10;cmat &amp;lt;- kronecker( ocmat, vcmat )&#10;&#10;obs &amp;lt;- matrix( mvrnorm(1, mu=rep(0,10*20), Sigma=cmat), 20, 10 )&#10;&#10;hist(cor(obs))&#10;hist(cor(t(obs))) &#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2014-06-04T17:32:24.797" Id="101171" LastActivityDate="2014-06-04T17:32:24.797" OwnerUserId="4505" ParentId="100770" PostTypeId="2" Score="5" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I know PSO is intended for optimization, but I saw in an article that survey techniques of ML to the Intrusion detection problem (KDD 99 dataset):&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Chen et al. [55] demonstrated a ‘‘divide-and-conquer’’ approach to incrementally learning a classiﬁcation rule set using a standard PSO algorithm. This algorithm starts with a full training set. One run of the PSO is expected to produce the best classiﬁer, which is added to the rule set. Meanwhile, data covered by this classiﬁer is deleted from the training dataset. This process is repeated until the training dataset is empty.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;May someone clarify those lines to me? (the article of Chen et al. &quot;PSO BASED APPROACH TO RULE LEARNING, 2007&quot; isn't available to me).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-04T22:10:09.763" Id="101205" LastActivityDate="2014-06-04T22:35:35.143" LastEditDate="2014-06-04T22:35:35.143" LastEditorUserId="22468" OwnerUserId="46735" PostTypeId="1" Score="0" Tags="&lt;classification&gt;" Title="Using particle swarm optimization (PSO) as classifier" ViewCount="28" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I've found a discrepancy between the output of the fitted() function and adding the residuals to the original data set. Is the fitted() function not doing what I think it should be doing?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-06-05T03:41:49.220" Id="101229" LastActivityDate="2014-06-05T05:50:00.723" LastEditDate="2014-06-05T05:50:00.723" LastEditorUserId="805" OwnerUserId="46751" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;regression&gt;&lt;residuals&gt;&lt;fitting&gt;" Title="fitted() function in R vs adding the residuals to the original data" ViewCount="73" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Dividing sample data into several sets seems to be a common approach in statistics. This is especially evident in predictive modeling, where samples are traditionally divided into two sets, usually called &lt;em&gt;training set&lt;/em&gt; and &lt;em&gt;test set&lt;/em&gt; (&lt;strong&gt;two-way partition&lt;/strong&gt;). Training sets are supposed to be used for &lt;em&gt;developing models&lt;/em&gt; and test sets for &lt;em&gt;evaluating models&lt;/em&gt;. Recently, I became aware that a &lt;strong&gt;three-way partitioning&lt;/strong&gt; scheme is also used, where data set for developing models is called &lt;em&gt;training set&lt;/em&gt;, for evaluating models - &lt;em&gt;validation set&lt;/em&gt;, and for evaluating the selected model - &lt;em&gt;test set&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have also read about specific implementations of two-way partitioning, such as &lt;em&gt;multi-fold cross-validation&lt;/em&gt;, as well as &lt;em&gt;bootstrapping&lt;/em&gt;, which is popular in PLS-SEM.&lt;/p&gt;&#10;&#10;&lt;p&gt;My &lt;strong&gt;question&lt;/strong&gt; is threefold:&lt;/p&gt;&#10;&#10;&lt;p&gt;1) What &lt;strong&gt;sampling approach&lt;/strong&gt; for data analysis is &lt;strong&gt;correct/best/optimal&lt;/strong&gt;, considering that I plan to perform EFA and CFA, with following SEM?&lt;/p&gt;&#10;&#10;&lt;p&gt;2) What are the &lt;strong&gt;guidelines (heuristics)&lt;/strong&gt; for training/test and training/validation/test &lt;strong&gt;ratios&lt;/strong&gt; in terms of the &lt;em&gt;size&lt;/em&gt; of each subgroup of a sample? Any specific &lt;strong&gt;strategy&lt;/strong&gt;?&lt;/p&gt;&#10;&#10;&lt;p&gt;3) Do I need to care about &lt;strong&gt;specific partitioning implementations&lt;/strong&gt; (cross-validation, bootstrapping, etc.), or just good old &lt;strong&gt;simple random sampling&lt;/strong&gt; will be sufficient?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Please include references, if possible. Thank you much in advance!&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;UPDATE: After giving these ideas some additional thought, I came to the following realization. It seems to me that a combination EFA/CFA basically represents a two-way partitioning approach (use training set for EFA and test set for CFA), while SEM can benefit from a three-way partitioning, when using alternative models variant of SEM. Please comment on this realization as well.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-05T08:56:44.327" Id="101251" LastActivityDate="2014-06-05T09:32:18.907" LastEditDate="2014-06-05T09:32:18.907" LastEditorUserId="31372" OwnerUserId="31372" PostTypeId="1" Score="0" Tags="&lt;sampling&gt;&lt;factor-analysis&gt;&lt;sem&gt;&lt;resampling&gt;&lt;confirmatory-factor&gt;" Title="Determining characteristics of sampling sets for EFA/CFA/SEM" ViewCount="59" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I am trying to make some sense out of the results of a linear regression model. I have a dependent variable X, and, say, 3 independent variables Y1 Y2 Y3. I set up 5 models :&lt;/p&gt;&#10;&#10;&lt;p&gt;(m1) X ~ Y1&lt;/p&gt;&#10;&#10;&lt;p&gt;(m2) X ~ Y1 + Y2&lt;/p&gt;&#10;&#10;&lt;p&gt;(m3) X ~ Y1 + Y3&lt;/p&gt;&#10;&#10;&lt;p&gt;(m4) X ~ Y1 + Y2 + Y3&lt;/p&gt;&#10;&#10;&lt;p&gt;(m5) X ~ Y1 + Y2*Y3&lt;/p&gt;&#10;&#10;&lt;p&gt;In models 1 to 3, all independent variables are significant. &lt;/p&gt;&#10;&#10;&lt;p&gt;In model 4, when I include both Y2 and Y3 in the model, they both turn un-significant. However, when I include an interaction between Y2 and Y3 (m5), all variables main effects and the interaction turn significant. &lt;/p&gt;&#10;&#10;&lt;p&gt;I am wondering whether I am overfitting the model, or if there might be one logical way to interpret the changes of significances for Y2 and Y3 in these analyses (m4 vs m5).&lt;/p&gt;&#10;&#10;&lt;p&gt;With regards&lt;/p&gt;&#10;&#10;&lt;p&gt;[edit] Y2 is numeric, Y3 is factor (3 levels). The parameters' sign are the following :&#10;(m5) main Y2 : +, main Y3 : +, Y2*Y3[2] : -, Y2*Y3[3] : -,   &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-06-05T09:17:34.090" Id="101257" LastActivityDate="2014-06-05T12:15:48.887" LastEditDate="2014-06-05T12:04:07.123" LastEditorUserId="46764" OwnerUserId="46764" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;predictor&gt;" Title="Significance of independent variables in linear regression models" ViewCount="49" />
  <row AcceptedAnswerId="102644" AnswerCount="1" Body="&lt;blockquote&gt;&#10;  &lt;p&gt;Question: Suppose $X_1, \cdots, X_n$ are $iid$ normal random variables with unknown mean $\mu$ and known variance $\sigma^2$. Find the UMVUE for $\Phi(\mu)$, where $\Phi(\cdot)$ is the cdf of a standard normal random variable.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I used to guess the desired UMVUE is or similar to $\Phi(\bar X)$ since it is a function of complete and sufficient statistic. However, it is not really unbiased (see &lt;a href=&quot;http://math.stackexchange.com/questions/820211/how-to-find-the-following-integration&quot;&gt;here&lt;/a&gt;). &lt;strong&gt;Could anyone shed light on how to find the UMVUE, please? Thank you!&lt;/strong&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-05T09:25:35.993" Id="101258" LastActivityDate="2014-06-09T20:20:09.617" OwnerUserId="36575" PostTypeId="1" Score="3" Tags="&lt;probability&gt;&lt;estimation&gt;&lt;mathematical-statistics&gt;&lt;inference&gt;" Title="Find the UMVUE for $\Phi(\mu)$" ViewCount="122" />
  <row AnswerCount="0" Body="&lt;p&gt;I was trying to use the nlme package in r to do a multilevel linear model.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have yield as response variable and rainfall as predictor variable for 60 years for 6 different locations (State). I am trying to see whether rainfall has same level of effect on yield in all locations or different effects. In principle, I am trying to see if slope of yield vs rainfall significantly varies between locations. Therefore rainfall is my random effect. I built my model like this: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; # baseline model which only includes intercept&#10;mdl1&amp;lt;-gls(yield ~ 1,data = data, method=&quot;ML&quot;)&#10;&#10;#intercept as random effect&#10;mdl2&amp;lt;-lme(yield ~ 1,data=data,random = ~1|state,method=&quot;ML&quot;)  &#10;&#10; # slope as random effect&#10;mdl3&amp;lt;-lme(yield ~ rain, data = data, random = ~rain|state,method=&quot;ML&quot;)&#10;&#10;##compare the three model&#10;anova(mdl1,mdl2,mdl3)&#10;##this shows me when I add slope as random effect, my model shows better fit compared to baseline model (mdl1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;this is all working fine. The problem starts when I do the same analysis using an another predictor variable (a count data)&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; # baseline model which only includes intercept: Works fine&#10;mdl4&amp;lt;-gls(yield ~ 1,data = data, method=&quot;ML&quot;)&#10;&#10;#intercept as random effect - works fine&#10;mdl5&amp;lt;-lme(yield ~ 1,data=data,random = ~1|state,method=&quot;ML&quot;)  &#10;&#10; # include different predictor (break) this time instead of rain&#10;mdl6&amp;lt;-lme(yield ~ break, data = data, random = ~break|state,method=&quot;ML&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;when i run the mdl 6, this gives me the error&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Error in lme.formula(res_yld ~ brk, data = data, random = ~brk | state,  : &#10;nlminb problem, convergence error code = 1&#10;message = iteration limit reached without convergence (10)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I have absolutely no clue why is this happening. Everything worked fine for my first predictor but this does not work on another predictor. What am I doing wrong here? I tried reading about this online but the posts are not very clear to me. I would really appreciate of anyone could me out on this.&#10;Thanks&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-06-05T10:13:51.457" Id="101265" LastActivityDate="2014-06-05T10:13:51.457" OwnerUserId="46228" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;nlme&gt;" Title="nlme() convergence error code = 1" ViewCount="93" />
  
  <row Body="&lt;p&gt;I couldn't find the previous post I was seeking, so here's what QQ-plots look like (for particular choices of distribution) &lt;em&gt;on average&lt;/em&gt;:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ZXRkL.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;But randomness tends to obscure things, especially with small samples:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/eq4uy.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that at $n=21$ the results may be much more variable than shown there - I generated several such sets of six plots and chose a 'nice' set where you could kind of see the shape in all six plots at the same time. Sometimes straight relationships look curved, curved relationships look straight, heavy-tails just look skew, and so on - with such small samples, often the situation may be much less clear:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/BXx1k.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;It's possible to discern more features than those (such as discreteness, for one example), but with $n=21$, even such basic features may be hard to spot; we shouldn't try to 'over-interpret' every little wiggle. As sample sizes become larger, generally speaking the plots 'stabilize' and the features become more clearly interpretable rather than representing noise. [With some very heavy-tailed distributions, the rare large outlier might prevent the picture stabilizing nicely even at quite large sample sizes.]&lt;/p&gt;&#10;" CommentCount="14" CreationDate="2014-06-05T12:49:59.247" Id="101290" LastActivityDate="2015-01-23T04:10:51.587" LastEditDate="2015-01-23T04:10:51.587" LastEditorUserId="805" OwnerUserId="805" ParentId="101274" PostTypeId="2" Score="44" />
  <row Body="&lt;p&gt;It is important to understand that logistic regression parameters are &lt;strong&gt;only true locally&lt;/strong&gt;. This means that the relative impact of each estimate will change depending on the value of your independent variables. Here is a &lt;a href=&quot;http://www.unm.edu/~schrader/biostat/bio2/Spr06/lec11.pdf&quot; rel=&quot;nofollow&quot;&gt;paper&lt;/a&gt; that can helps explain things. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-06-05T15:02:28.550" Id="101303" LastActivityDate="2014-06-05T15:02:28.550" OwnerUserId="46794" ParentId="101233" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;if $y = F(\beta x) $ and $y\:  \epsilon\: (1,0) $ &#10;where $F(x) $ is a logit function then  $dy/dx_1 \neq dy/dx_2 $. This is because of the chain rule as $dy/dx = \frac{dy}{dF(x)}\frac{ dF(x)}{dx} $. If you look at a logit function  &lt;img src=&quot;http://i.stack.imgur.com/5vpYa.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;You will see that the slope of x changes depending on where you are on the curve. In practice this means that you only interpret rank and sign in a logistic regression as the magnitude of the impact will depend on how you parameters and variables interact with the logit function&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-05T15:53:32.307" Id="101314" LastActivityDate="2014-06-05T15:53:32.307" OwnerUserId="46794" ParentId="101233" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;I think this is actually an example where you would want to use either a &lt;a href=&quot;https://en.wikipedia.org/wiki/Repeated_measures_design&quot; rel=&quot;nofollow&quot;&gt;repeated measures ANOVA&lt;/a&gt;, or a nonparametric analog to repeated measures ANOVA, such as the &lt;a href=&quot;https://en.wikipedia.org/wiki/Friedman_test&quot; rel=&quot;nofollow&quot;&gt;Friedman Test&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is my attempt to amplify on your data structure correct below correct?&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Subject Scan1  Scan2  Scan3 ... Scan12&#10;Subj1   Mean1  Mean1  Mean1 ... Mean1&#10;Subj2   Mean2  Mean2  Mean2 ... Mean2&#10;...     ...    ...    ...   ... ...&#10;SubjN   MeanN  MeanN  MeanN ... MeanN&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;If so, and if your mean scores meet the repeated measures ANOVA assumptions (sphericity, normality, i.i.d.), then you can use the canned repeated measures ANOVA commands available in the popular statistical software packages.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you are not comfortable with these assumptions, then you can use a nonparametric test like Friedman's, which should also be available in your statistical software.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am making the software suggestions, because calculating repeated measures ANOVA by hand is a chore. Yes it is.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-06-05T16:22:21.760" Id="101319" LastActivityDate="2014-06-05T16:22:21.760" OwnerUserId="44269" ParentId="101307" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;I am unfamiliar with the term of art &lt;em&gt;non-differentiation&lt;/em&gt;, but two ideas come up.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Equivalence testing provides a framework for testing whether quantities are not more different than some &lt;em&gt;a priori&lt;/em&gt; specified level. So one approach would be to test for equivalence of items on your seventh page (or whatever), with the usual caveats about &lt;a href=&quot;https://en.wikipedia.org/wiki/Multiple_comparisons_problem&quot; rel=&quot;nofollow&quot;&gt;multiple comparisons&lt;/a&gt;. You can read about one kind of equivalence testing &lt;a href=&quot;http://stats.stackexchange.com/tags/tost/info&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;If non-differentiation means that responses to items on the seventh page (or whatever) tend toward a specific value (e.g. the middle of the scale), then you could also test whether each item is different than that middle. Again, multiple comparisons yadda yadda...&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2014-06-05T16:31:44.337" Id="101320" LastActivityDate="2014-06-05T16:31:44.337" OwnerUserId="44269" ParentId="101302" PostTypeId="2" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;I found &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/eval/&quot; rel=&quot;nofollow&quot;&gt;this tool&lt;/a&gt; that is providing some well known evaluation functions like precision and recall. Unfortunately this tool supports only binary-class C-SVM. I was wondering where I can find similar evaluation functions that are known to be appropriate for probability estimation SVM? If there is a LIBSVM extension tool like the above one would be even better. Otherwise I'd be happy to read more about evaluation functions for probability estimation SVM.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-05T16:32:46.823" Id="101321" LastActivityDate="2014-06-05T16:32:46.823" OwnerUserId="46622" PostTypeId="1" Score="0" Tags="&lt;probability&gt;&lt;libsvm&gt;&lt;accuracy&gt;" Title="LIBSVM evaluation functions for probability estimation" ViewCount="81" />
  <row AnswerCount="0" Body="&lt;p&gt;Is there a Matlab code for the probit random effects model for panel data? I observe there are Stata codes(a couple of versions) available but I cannot not find any Matlab codes. I would really appreciate it if someone could tell me some resources where I could find what I need.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-06-05T16:34:44.953" Id="101322" LastActivityDate="2014-06-05T16:34:44.953" OwnerUserId="46799" PostTypeId="1" Score="0" Tags="&lt;matlab&gt;&lt;panel-data&gt;&lt;probit&gt;" Title="Is there a Matlab code for the probit random effects model for panel data?" ViewCount="112" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a panel of 8 sectors across 24 OECD countries. Im estimating aggregate equations across all sectors, and 8 separate equations for each sector. Part of the analysis is to make comparisons across sectors. A few countries have all or  about half of the observations missing for a single sector. This means that e.g. for one sector a whole country is missing. &lt;/p&gt;&#10;&#10;&lt;p&gt;Are comparisons across sector equations and therefore slightly different samples still valid? &lt;/p&gt;&#10;&#10;&lt;p&gt;I would have guessed yes, as long as the nature of the missing observations or country-sectors in unsystematic. However, it seems to me that in general practice, countries which have a missing sector would be dropped.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-05T19:07:36.047" Id="101339" LastActivityDate="2014-06-05T19:07:36.047" OwnerUserId="45958" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;missing-data&gt;&lt;panel-data&gt;" Title="Missing data in 3 dimensional Panel: Are comparisons across different samples valid?" ViewCount="18" />
  
  
  
  <row Body="&lt;p&gt;Based on your phrasing, it seems you are &lt;a href=&quot;https://en.wikipedia.org/wiki/Entropy_%28information_theory%29#Relationship_to_thermodynamic_entropy&quot; rel=&quot;nofollow&quot;&gt;equating thermodynamic entropy with information entropy&lt;/a&gt;. The concepts are related, but you have to be careful because they are used differently in the two fields.&lt;/p&gt;&#10;&#10;&lt;p&gt;Shannon entropy measures &lt;em&gt;unpredictability&lt;/em&gt;. You are correct that entropy is maximum when the outcome is the most uncertain. An unbiased coin has maximum entropy (among coins), while a coin that comes up Heads with probability 0.9 has less entropy. Contrary to your next statement, however, max entropy = &lt;strong&gt;maximum&lt;/strong&gt; information content.&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose we flip a coin 20 times. If the coin is unbiased, the sequence might look like this:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;TTHTHHTTHHTHHTHTHTTH&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;If the coin comes up Heads with probability 0.9, it might look more like this:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;HHHHHHHHHHTHHHHHHTHH&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;The second signal contains &lt;em&gt;less&lt;/em&gt; information. Suppose we encode it using run length encoding, like this:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;10T6T2&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;which we interpret as &quot;10 heads, then 1 tail, then 6 heads, then a tail, then 2 heads&quot;. Compare this to the same encoding method applied to the first signal:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;TT1T2TT2T2T1T1TT1&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;We can't compress the signal from the maximum entropy coin as much, because it contains more information.&lt;/p&gt;&#10;&#10;&lt;p&gt;As for your specific questions:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;A thermodynamic system in equilibrium has maximum entropy in the sense that its microstate is maximally uncertain given its macrostate (e.g. its temperature, pressure, etc). From our perspective as observers, this means that our knowledge of its microstate is less certain than when it was not in equilibrium. But the system in equilibrium contains more information because its microstate is maximally unpredictable. The quantity that has decreased is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Mutual_information&quot; rel=&quot;nofollow&quot;&gt;mutual information&lt;/a&gt; between the macrostate and the microstate. This is the sense in which we &quot;lose (mutual) information&quot; when entropy increases. The loss is &lt;em&gt;relative&lt;/em&gt; to the observer.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;As long as the process is random, each new symbol adds information to the sequence. The symbols are random variables, so each one has a distribution for which we can calculate entropy. The information content of the sequence is measured with &lt;em&gt;joint entropy&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="2" CreationDate="2014-06-05T23:49:19.830" Id="101366" LastActivityDate="2014-06-05T23:49:19.830" OwnerUserId="46762" ParentId="101351" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;One could construct a polarization index; exactly how one defines it depends on what constitutes being more polarized (i.e. what, exactly do you mean, in particular edge cases, by more or less polarized?):&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, if the mean is '4', is a 50-50 split between  '3' and '5' more, or less polarized than 25% '1' and 75% '5'?&lt;/p&gt;&#10;&#10;&lt;p&gt;Anyway, in the absence of that kind of specific definition of what you mean,  I'll suggest a measure based off variance:&lt;/p&gt;&#10;&#10;&lt;p&gt;Given a particular mean, define the most polarized possible split as the one that maximizes variance*.&lt;/p&gt;&#10;&#10;&lt;p&gt;*(NB that would say that 25% '1' and 75% '5' is substantially &lt;em&gt;more&lt;/em&gt; polarized than 50-50 split of '3's and '5's; if that doesn't match your intuition, don't use variance)&lt;/p&gt;&#10;&#10;&lt;p&gt;So this polarization index is the proportion of the largest possible variance (&lt;em&gt;with the observed mean&lt;/em&gt;) in the observed variance.&lt;/p&gt;&#10;&#10;&lt;p&gt;Call the average rating $m$ ($m=\bar x$).&lt;/p&gt;&#10;&#10;&lt;p&gt;The maximum variance occurs when a proportion $p=\frac{m-1}{4}$ is at $5$ and $1-p$ is at $1$; this has a variance of &#10; $(m-1)(5-m) \cdot \frac{n}{n-1}$. &lt;/p&gt;&#10;&#10;&lt;p&gt;So simply take the sample variance and divide by $(m-1)(5-m) \cdot \frac{n}{n-1}$; this gives a number between $0$ (perfect agreement) and $1$ (completely polarized).&lt;/p&gt;&#10;&#10;&lt;p&gt;For a number of cases where the mean rating is 4, this would give the following:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/wXAaU.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;You might instead prefer &lt;em&gt;not&lt;/em&gt; to compute them relative to the biggest possible variance with the same mean, but instead as a percentage of the biggest possible variance &lt;em&gt;for any mean rating&lt;/em&gt;. That would involve dividing instead by $4 \cdot \frac{n}{n-1}$, and again yields a value between 0 (perfect agreement) and $1$ (polarized at the extremes in a 50-50 ratio). This would yield the same relativities&#10;as the diagram above, but all the values would be 3/4 as large (that is, from left to right, top to bottom they'd be  0, 16.5%, 25%, 25%, 50% and 75%).&lt;/p&gt;&#10;&#10;&lt;p&gt;Either of the two is a perfectly valid choice - as is any other number of alternative ways of constructing such an index.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-06-06T00:17:07.080" Id="101368" LastActivityDate="2014-06-06T01:25:24.033" LastEditDate="2014-06-06T01:25:24.033" LastEditorUserId="805" OwnerUserId="805" ParentId="101354" PostTypeId="2" Score="11" />
  <row AnswerCount="1" Body="&lt;p&gt;I am kind of confused with what I should actually do with predicted volatility values that I obtained via a ARCH/GARCH model other than feeling happy that I know when volatility rises/falls. Is there a way that I can incorporate the predicted volatility values via a GARCH/ARCH model into a prediction model for my actual time series or is what I am saying erroneous? I use R as my primary tool.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-06T01:21:02.907" Id="101372" LastActivityDate="2014-06-06T17:19:58.770" OwnerUserId="46705" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;time-series&gt;&lt;forecasting&gt;&lt;garch&gt;" Title="How can I use the results of GARCH in order to improve a forecast?" ViewCount="60" />
  
  
  <row Body="&lt;p&gt;Well, if you are looking &quot;for any pointers&quot;...&lt;/p&gt;&#10;&#10;&lt;p&gt;The (scaled)(inverse)Wishart distribution is often used because it is conjugate&#10;to the multivariate likelihood function and thus simplifies Gibbs sampling.&lt;/p&gt;&#10;&#10;&lt;p&gt;In &lt;a href=&quot;http://mc-stan.org/rstan.html&quot; rel=&quot;nofollow&quot;&gt;Stan&lt;/a&gt;, which uses Hamiltonian Monte Carlo sampling, there is no restriction for multivariate priors. The recommended approach is the separation strategy suggested by &lt;a href=&quot;http://www3.stat.sinica.edu.tw/statistica/oldpdf/A10n416.pdf&quot; rel=&quot;nofollow&quot;&gt;Barnard, McCulloch and Meng&lt;/a&gt;:&#10;$$\Sigma=\text{diag_matrix}(\sigma)\;\Omega\;\text{diag_matrix}(\sigma)$$&#10;where $\sigma$ is a vector of std devs and $\Omega$ is a correlation matrix.&lt;/p&gt;&#10;&#10;&lt;p&gt;The components of $\sigma$ can be given any reasonable prior. As to $\Omega$, the recommended prior is&#10;$$\Omega\sim\text{LKJcorr}(\nu)$$&#10;where &quot;LKJ&quot; means &lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0047259X09000876&quot; rel=&quot;nofollow&quot;&gt;Lewandowski, Kurowicka and Joe&lt;/a&gt;. As $\nu$ increases, the prior increasingly concentrates around the unit correlation matrix, at $\nu=1$ the LKJ correlation distribution reduces to the identity distribution over correlation matrices. The LKJ prior may thus be used to control the expected amount of correlation among the parameters.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, I've not (yet) tried non-normal distributions of random effects, so I hope I've not missed the point ;-)&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-06-06T09:16:05.257" Id="101408" LastActivityDate="2014-06-06T09:16:05.257" OwnerUserId="44965" ParentId="77038" PostTypeId="2" Score="4" />
  
  
  <row Body="&lt;p&gt;I don't know details of your model, but in my opinion you need to deal with the large amount of &quot;zero responses&quot;. Look into compound models with a mass point at zero. Something like the &quot;Tweedie model&quot;. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-06T16:15:49.803" Id="101444" LastActivityDate="2014-06-06T16:36:20.687" LastEditDate="2014-06-06T16:36:20.687" LastEditorUserId="7290" OwnerUserId="46850" ParentId="88825" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I am working with a random forest model and would like to better understand how the variables impact the forest output.  Using partial plots is useful, but this is basically a linear treatment of the model, because it assumes there is no interaction between the variables.  Are there known methods for evaluating the non-linear-ness of the a random forest model or for analyzing how variables interact with each other within the model?  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-06T18:05:43.843" FavoriteCount="2" Id="101457" LastActivityDate="2014-06-06T18:05:43.843" OwnerUserId="45856" PostTypeId="1" Score="1" Tags="&lt;random-forest&gt;&lt;nonlinear&gt;" Title="Analyzing the behavior of a non-linear model" ViewCount="16" />
  
  <row Body="&lt;p&gt;&lt;a href=&quot;https://tuvalabs.com&quot; rel=&quot;nofollow&quot;&gt;https://tuvalabs.com&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I am sure you have found what you were looking for long back, but for anyone else who come across thread - TuvaLabs is nice source for the datasets for Classrooms. It curates datasets, story, description, small exercise and visualization capability also you can requests datasets on it.&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2014-06-06T20:02:00.957" CreationDate="2014-06-06T20:02:00.957" Id="101470" LastActivityDate="2014-06-06T20:02:00.957" OwnerUserId="46888" ParentId="5937" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;I ended up with what I think is a practically useful suggestion (1(b)), and some other ideas that might be pursued if that's not suitable.&lt;/p&gt;&#10;&#10;&lt;p&gt;1) We can make a parametric assumption (though we cannot check it for reasonableness). We might, for example, assume that the payments are exponentially distributed - we can certainly base a test off that if we know the means and sample sizes. The sensitivity to that assumption may be an issue though. At least it's &lt;em&gt;something&lt;/em&gt;. (If we're prepared to assume a shape - which might be garnered from external data of similar type, say - we might even be able to use a gamma rather than an exponential.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Now I don't know that an exponential distribution will be a great approximation, but I'll give a suitable statistic for that case, and then suggest something that may work better - better enough as to perhaps be actually useful.&lt;/p&gt;&#10;&#10;&lt;p&gt;a) Exponential assumption. If we assume both samples are exponential, that all observations are independent, and that under the null they have the same mean, then&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac{\bar x_1}{\bar x_2}\sim F_{2n_1,2n_2}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;So you can do a (two-tailed) F-test of that hypothesis&lt;/p&gt;&#10;&#10;&lt;p&gt;(Indeed, with your sample, this would be $F_{30,20}=1.25$, and this would not lead to rejection at typical significance levels; but samples fifteen times as large would lead to rejection at the 5% level).&lt;/p&gt;&#10;&#10;&lt;p&gt;b) Here's what I think is the the better choice. If there's a &lt;em&gt;minimum&lt;/em&gt; payment (such as in places with a minimum wage and on tasks where you can reasonably know or guess the time or minimum time), then a shifted exponential would probably be a pretty good approximation. If you know the minimum, $m$, say (and if it's a legislated amount, you probably do), then you can instead do:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac{\bar x_1-m}{\bar x_2-m}\sim F_{2n_1,2n_2}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;And again the test is two-tailed.&lt;/p&gt;&#10;&#10;&lt;p&gt;(For example, if $m$, the minimum payment for this task, was say $17.50$, then for the data you have, the statistic would be significant at the 5% level.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that equality of the original means implies equality of the mean above the minimum, so this still speaks to the original hypothesis. If you have the information to pursue this option, I suggest trying it; in the right circumstances, it may be possible to justify it as a reasonable approximation to the actual distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;c) Rather than exponential as in (a), we might assume a gamma distribution with some other shape parameter (exponential is gamma with shape parameter 1).&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, if we assumed the shape parameter were 3 rather than 1, the test would work the same way but the degrees of freedom for the $F$ statistics (numerator and denominator) would also be multiplied by 3. So: &lt;/p&gt;&#10;&#10;&lt;p&gt;(i) this approach could accommodate any external shape information (such as for similar samples).&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's say that with similar samples we concluded that the shape parameter was typically around 10. Then (for your data), the test statistic would still be 1.25 but the df would be around $(300,200)$ ... but the result would still not be significant at the 5% level.&lt;/p&gt;&#10;&#10;&lt;p&gt;(ii) this opens up the possibility of considering some worst cases - with a given statistic you could compute the &lt;em&gt;smallest&lt;/em&gt; shape parameter for a gamma distribution that would lead to rejection. With a large ratio of means and enough observations, it would sometimes be the case that only very heavily skew payment distributions should the test fail to reject.&lt;/p&gt;&#10;&#10;&lt;p&gt;d) of course there are other possible parametric assumptions than the exponential/gamma; similar approaches to the above may be fruitful.&lt;/p&gt;&#10;&#10;&lt;p&gt;2) Even without such an assumption, there's at least &lt;em&gt;something&lt;/em&gt; here we can work with: We know the data are positive (payment received for doing work; presumably zero is excluded, but if the payment can be zero we need also to allow for that). That does tell us something, since it places some sort of limits on where the data can be relative to the mean.&lt;/p&gt;&#10;&#10;&lt;p&gt;So for example, the Markov inequality tells us that for any non-negative $X$ and $t&amp;gt;0$, &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\text{P}(X \geq t) \leq \frac{\text{E}(X)}{t}\,.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;It may be possible to use an analysis similar to that here: &lt;a href=&quot;http://stats.stackexchange.com/questions/82419/does-a-sample-version-of-the-one-sided-chebyshev-inequality-exist/82694#82694&quot;&gt;Does a sample version of the one-sided Chebyshev inequality exist?&lt;/a&gt; and derive some kind of lower bound on tail probabilities for a difference in sample means.&lt;/p&gt;&#10;&#10;&lt;p&gt;If we can make an additional assumption, such as unimodality within the groups, we may be able to do more.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;3) Indeed, since it's possible to do hypothesis testing with a &lt;em&gt;single observation&lt;/em&gt;, at the very least one can take the difference in means and test that as a single observation. But in that case, the bounds are not likely to be useful very often.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-07T03:04:29.523" Id="101494" LastActivityDate="2014-06-07T09:57:55.357" LastEditDate="2014-06-07T09:57:55.357" LastEditorUserId="805" OwnerUserId="805" ParentId="101477" PostTypeId="2" Score="3" />
  <row AnswerCount="2" Body="&lt;p&gt;If I know that the population is normally distributed, and then take &lt;em&gt;small&lt;/em&gt; samples from this population, is it more correct to claim that the sampling distribution is &lt;strong&gt;normal&lt;/strong&gt; or instead follows the &lt;strong&gt;t distribution&lt;/strong&gt;?&lt;/p&gt;&#10;&#10;&lt;p&gt;I understand that small samples tend to be t distributed, but does this only apply when the underlying population distribution is unknown?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" ClosedDate="2014-06-07T22:17:39.897" CommentCount="3" CreationDate="2014-06-07T04:38:07.467" FavoriteCount="1" Id="101496" LastActivityDate="2014-06-08T08:46:09.390" OwnerUserId="46902" PostTypeId="1" Score="6" Tags="&lt;distributions&gt;&lt;normal-distribution&gt;&lt;sampling&gt;&lt;small-sample&gt;&lt;t-distribution&gt;" Title="Is the sampling distribution for small samples of a normal population normal or t distributed?" ViewCount="435" />
  <row AnswerCount="0" Body="&lt;p&gt;How best can I use the Granger causality test in time series data and understand it better because I have never used it. I want to analyze long run relationship and bi-directional relationship between two variables.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-06-07T09:43:20.207" Id="101510" LastActivityDate="2014-06-07T10:07:45.810" LastEditDate="2014-06-07T10:07:45.810" LastEditorUserId="26338" OwnerUserId="46911" PostTypeId="1" Score="0" Tags="&lt;time-series&gt;&lt;granger-causality&gt;" Title="Granger causality test" ViewCount="64" />
  
  
  <row Body="&lt;p&gt;I think the model is incomplete.  Why not consider her hands as a forest of SIR models?  There are some things that &quot;die on the vine&quot; and for that &quot;R&quot; applies.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Here are links on SIR:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://www.maa.org/publications/periodicals/loci/joma/the-sir-model-for-spread-of-disease-the-differential-equation-model&quot; rel=&quot;nofollow&quot;&gt;http://www.maa.org/publications/periodicals/loci/joma/the-sir-model-for-spread-of-disease-the-differential-equation-model&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://www.iaeng.org/IJAM/issues_v44/issue_2/IJAM_44_2_06.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.iaeng.org/IJAM/issues_v44/issue_2/IJAM_44_2_06.pdf&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Epidemic_model&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Epidemic_model&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;It seems that lognormal distributions may apply to some limited set of special cases of disease, but they are not general enough for unilateral applicability.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I have my head in random forests and a forest of SIR models (see IJAM link) is going to account for variation, give you robust answers, and have a basis that is generally applicable to the problem.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-07T20:35:10.687" Id="101544" LastActivityDate="2014-06-07T20:35:10.687" OwnerUserId="22452" ParentId="55684" PostTypeId="2" Score="0" />
  
  <row AcceptedAnswerId="101559" AnswerCount="1" Body="&lt;p&gt;I am reading the paper available at this link:&#10;&lt;a href=&quot;https://drive.google.com/file/d/0B2_rKFnvrjMARnU1QjB4anR3RDA/edit?usp=sharing&quot; rel=&quot;nofollow&quot;&gt;https://drive.google.com/file/d/0B2_rKFnvrjMARnU1QjB4anR3RDA/edit?usp=sharing&lt;/a&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;I am having trouble understanding section 5.1 (page 2741).&lt;/p&gt;&#10;&#10;&lt;p&gt;Essentially it says the following: &lt;/p&gt;&#10;&#10;&lt;p&gt;$\theta_{ABi} \sim \mathrm{N}(\mu_{AB}, \tau^2)$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\theta_{ACi} \sim \mathrm{N}(\mu_{AC}, \tau^2)$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\theta_{BCi} \sim \mathrm{N}(\mu_{BC}, \tau^2)$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\mu_{BC} = \mu_{AC}-\mu_{AB}$&lt;/p&gt;&#10;&#10;&lt;p&gt;implies &lt;/p&gt;&#10;&#10;&lt;p&gt;$ \begin{pmatrix}\theta_{ABi} \\ \theta_{ACi}\end{pmatrix} \sim \mathrm{N} \left(\begin{pmatrix}\mu_{ABi} \\ \mu_{ACi}\end{pmatrix}, \begin{pmatrix}\tau^2 &amp;amp; \tau^2/2 \\ \tau^2 /2  &amp;amp; \tau^2\end{pmatrix} \right)$&lt;/p&gt;&#10;&#10;&lt;p&gt;I do not understand how $\mathrm{Cov} \left[ \theta_{ABi}, \theta_{ACi} \right] = \tau^2 /2$ ? Could someone please explain this?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-08T03:50:07.117" Id="101557" LastActivityDate="2014-06-08T04:33:28.303" OwnerUserId="18098" PostTypeId="1" Score="1" Tags="&lt;normal-distribution&gt;&lt;covariance&gt;&lt;variance-covariance&gt;&lt;multivariate&gt;" Title="Univariate Normal Converted to Multivariate Normal: Covariance Derivation" ViewCount="39" />
  <row AnswerCount="0" Body="&lt;p&gt;I want to use the following truncated normal distribution in WinBUGS to estimate parameters of SEM using Bayesian analysis.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;for(j in 1:P){y1[i,j]~djl.dnorm.trunc(mu1[i,j],psi1[j],thd[j,z1[i,j]],thd[j,z1[i,j]+1])} &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I have a problem in that the last step in &quot;updating tools,&quot; the updating tools are not responding.&lt;/p&gt;&#10;&#10;&lt;p&gt;After I also reduced the value of refresh from 100 to 1 at iteration 1, the updating tools stopped. How can I solve this problem? Any guidance would be greatly appreciated.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-06-08T04:25:52.627" Id="101558" LastActivityDate="2014-06-08T04:39:54.523" LastEditDate="2014-06-08T04:39:54.523" LastEditorUserId="32036" OwnerUserId="41067" PostTypeId="1" Score="2" Tags="&lt;probability&gt;&lt;bayesian&gt;&lt;normal-distribution&gt;&lt;winbugs&gt;&lt;truncation&gt;" Title="Truncated normal in WinBUGS" ViewCount="47" />
  <row AnswerCount="1" Body="&lt;p&gt;tanh activation function is nothing but  2*sigmoid - 1. Does it really matter between using those two activation functions. Which function is better in which cases?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-06-08T06:11:24.523" FavoriteCount="4" Id="101560" LastActivityDate="2014-06-08T06:47:38.883" OwnerUserId="46194" PostTypeId="1" Score="2" Tags="&lt;machine-learning&gt;&lt;neural-networks&gt;&lt;cost-maximization&gt;" Title="tanh activation function vs sigmoid activation function" ViewCount="1989" />
  
  <row AcceptedAnswerId="102592" AnswerCount="1" Body="&lt;p&gt;I am fitting a &lt;code&gt;glmer&lt;/code&gt; model in the &lt;code&gt;lme4&lt;/code&gt; R package. I'm looking for an anova table with p-value shown therein, but I cannot find any package that fits it. Is it possible to do it in R?&lt;/p&gt;&#10;&#10;&lt;p&gt;The model I am fitting is of the form:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;model1&amp;lt;-glmer(dmn~period*teethTreated+(1|fullName), &#10;   family=&quot;poisson&quot;, &#10;   data=subset(dataset, &#10;          group=='Four times a year'),&#10;   control=glmerControl(optimizer=&quot;bobyqa&quot;))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-06-08T08:58:30.333" FavoriteCount="1" Id="101566" LastActivityDate="2014-09-02T21:08:31.057" LastEditDate="2014-09-02T21:07:53.443" LastEditorUserId="2126" OwnerUserId="6547" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;anova&gt;&lt;lmer&gt;&lt;mixed-effect&gt;" Title="anova type III test for a GLMM" ViewCount="433" />
  <row AcceptedAnswerId="108222" AnswerCount="1" Body="&lt;p&gt;Suppose to have a valid correlation function on $R^2$ that depends on the distance. For example if  the distance between two point is $h$, the correlation function can be $\exp(-\phi h)$, where $\phi$ is a parameter.&#10;Now if instead of  $R^2$ we are on a cylinder: $R \times D$ where $D$ is a circle. My question is:&#10;The correlation function based on the distance is again a valid correlation function on the cylinder?&lt;/p&gt;&#10;&#10;&lt;p&gt;Or maybe the question can be expressed in a different way: how can I prove that a correlation function on a generic space is a valid correlation function?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-06-08T14:38:14.300" FavoriteCount="1" Id="101586" LastActivityDate="2014-07-16T22:46:53.980" LastEditDate="2014-06-08T14:43:38.830" LastEditorUserId="27213" OwnerUserId="27213" PostTypeId="1" Score="3" Tags="&lt;correlation&gt;&lt;spatial&gt;" Title="Is a valid correlation function on $R^2$ (a plane) also a valid correlation function on a cylinder" ViewCount="88" />
  <row AnswerCount="1" Body="&lt;p&gt;Which statistical tools are best suited for this problem:&lt;/p&gt;&#10;&#10;&lt;p&gt;Team A goes and measures 3 magnitudes in 200 locations (let's imagine the volume of a room, the insulation coefficient and the average temperature outside the room).  For each room they calculate the energy this room will consume to keep the desired temperature for a week.&lt;/p&gt;&#10;&#10;&lt;p&gt;So after the measurement campaign they have measured these 3 magnitudes in 200 different rooms (each room is only measured once).  So the data looks like:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; (Room_volume, r_coefficient, T, estimated_energy)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Team A is assumed to be very good at getting their results. Now team B measures the same 200 rooms, and calculates all the estimated energies for each room.&lt;/p&gt;&#10;&#10;&lt;p&gt;How should we measure the performance of team B vs team A (where A is assumed to be accurate)? &lt;/p&gt;&#10;&#10;&lt;p&gt;Forgetting the physics, as it's just an illustrative example, how do you compare the accuracy of a measurement campaign measuring 3 values from which a 4th one can be calculated deterministically?  Note that none of these values are distributed in any special way among the 200 samples (they don't look gaussian)&lt;/p&gt;&#10;&#10;&lt;p&gt;I started looking at the distribution of errors, so:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; {Estimated_energy_Ai - Estimated_energy_Bi} with i = 1..200&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And seeing how many results are within a standard deviation.  What other tools are appropriate?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-08T18:11:22.793" FavoriteCount="2" Id="102595" LastActivityDate="2014-06-12T13:01:07.497" LastEditDate="2014-06-10T14:51:00.573" LastEditorUserId="47959" OwnerUserId="47959" PostTypeId="1" Score="1" Tags="&lt;distributions&gt;&lt;paired-comparisons&gt;&lt;benchmark&gt;" Title="Comparing the performance of two teams measuring many variables once" ViewCount="85" />
  <row Body="&lt;p&gt;I figured this out, in case anyone ever needs it.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;=TINV(x*2, df)&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;So, in my example:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;=TINV(0.2*2, 10)&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;The answer is 0.879058.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-06-08T18:22:16.080" Id="102596" LastActivityDate="2014-06-08T18:22:16.080" OwnerUserId="46958" ParentId="101592" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;It is beneficial to look at the motivation for the BCa interval and it mechanisms (i.e. the so called &quot;correction factors&quot;). The BCa intervals are one of the most important aspects of the bootstrap because they are the more general case of the Bootstrap Percentile Intervals (i.e. the confidence interval based solely upon the bootstrap distribution itself).&lt;/p&gt;&#10;&#10;&lt;p&gt;In particular, look at the relationship between BCa intervals and the Bootstrap Percentile Intervals: when the adjustment for acceleration (the first &quot;correction factor&quot;) and skewness (the second &quot;correction factor&quot;) are both zero, then the BCa intervals revert back to the typical Bootstrap Percentile Interval. &lt;/p&gt;&#10;&#10;&lt;p&gt;I do not think that it would be a good idea to ALWAYS use bootstrapping. Bootstrapping is a robust technique that has a variety of mechanisms (ex: confidence intervals and there are different variations of the bootstrap for different types of problems such as the wild bootstrap when there is heteroscedasticity) for adjusting for different problems (ex: non-normality), but it is relies upon one crucial assumption: the data accurately represent the true population. &lt;/p&gt;&#10;&#10;&lt;p&gt;This assumption, although simple in nature, can be difficult to verify especially in the context of small sample sizes (it could be though that a small sample is an accurate reflection of the true population!). If the original sample on which the bootstrap distribution (and hence all of the results that follow from it) is not adequately accurate, then your results (and hence your decision based upon those results) will be flawed. &lt;/p&gt;&#10;&#10;&lt;p&gt;CONCLUSION: There is a lot of ambiguity with the bootstrap and you should exercise caution before applying it.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-06-08T19:06:10.307" Id="102598" LastActivityDate="2014-06-08T19:06:10.307" OwnerUserId="46705" ParentId="99988" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;You cannot do this. Not in SPSS nor any other package. Doing an ANOVA requires the raw data.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, if you are willing to assume that each variable has a particular distribution that is defined by the mean and sd (e.g. Normal, t with a certain df, etc) and if you are also willing to specify the relationships among the repeated data, then you can use the information you have to create simulated data and then do ANOVA on those data. &lt;/p&gt;&#10;&#10;&lt;p&gt;I would be very very hesitant to do that. If I did do that, I would be sure to do lots of sensitivity analyses.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-06-08T20:00:00.763" Id="102602" LastActivityDate="2014-06-08T20:00:00.763" OwnerUserId="686" ParentId="102597" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to implement a panel regression model, but there is one issue: both fixed effects estimation (FE) and random effects estimation (RE) require that the cross section sample be random. For my situation this requirement has not been met (I selected each city based upon a particular characteristic). What can I do about this? I know that in experimental design there was a way of accounting for this (i.e. the Bonferroni, Scheffe, and Tukey comparisons), but is there something similar in panel regression? Would a &quot;mixed&quot; panel regression model be appropriate here?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-08T20:39:29.343" Id="102605" LastActivityDate="2014-06-08T21:13:30.620" LastEditDate="2014-06-08T21:13:30.620" LastEditorUserId="46705" OwnerUserId="46705" PostTypeId="1" Score="1" Tags="&lt;panel-data&gt;&lt;random-effects-model&gt;&lt;longitudinal&gt;&lt;fixed-effects-model&gt;&lt;mixed-effect&gt;" Title="What do I do if my sample for cross sections is not random in a panel regression model?" ViewCount="15" />
  <row Body="&lt;p&gt;A simple test for changes in paired samples is the &lt;a href=&quot;http://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test&quot; rel=&quot;nofollow&quot;&gt;Wilcoxon signed-rank test&lt;/a&gt;. It's nonparametric, so Likert data won't be a problem. You can use this on each item if you like (and correct for &lt;a href=&quot;http://en.wikipedia.org/wiki/Familywise_error_rate&quot; rel=&quot;nofollow&quot;&gt;familywise error&lt;/a&gt; inflation if appropriate), or if your items are all supposed to measure the same construct, you can adopt &lt;a href=&quot;http://en.wikipedia.org/wiki/Classical_test_theory&quot; rel=&quot;nofollow&quot;&gt;classical test theory&lt;/a&gt; assumptions and sum or average them...&lt;em&gt;or&lt;/em&gt; you could estimate factor scores using a &lt;a href=&quot;https://en.wikipedia.org/wiki/Polytomous_Rasch_model&quot; rel=&quot;nofollow&quot;&gt;rating scale model&lt;/a&gt;. This is an &lt;a href=&quot;http://en.wikipedia.org/wiki/Item_response_theory&quot; rel=&quot;nofollow&quot;&gt;item response theory&lt;/a&gt; approach that produces index scores as continuous data, which might then permit a paired-samples &lt;em&gt;t&lt;/em&gt;-test as well. However, there's no guarantee that these factor scores would be normally distributed, and the whole process may take a fair amount of data. There are also bootstrap and Bayesian methods for these analyses, though I'm less familiar with them.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-08T21:47:02.417" Id="102607" LastActivityDate="2014-06-08T21:47:02.417" OwnerUserId="32036" ParentId="101585" PostTypeId="2" Score="2" />
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I have seen the term &lt;em&gt;deep autoencoders&lt;/em&gt; in a couple of articles such as &lt;a href=&quot;http://www.cs.toronto.edu/~fritz/absps/esann-deep-final.pdf&quot; rel=&quot;nofollow&quot;&gt;Krizhevsky, Alex, and Geoffrey E. Hinton. &quot;Using very deep autoencoders for content-based image retrieval.&quot; ESANN. 2011.&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;What's the difference between autoencoders and deep autoencoders?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-09T03:19:17.283" Id="102627" LastActivityDate="2015-01-02T08:20:07.743" OwnerUserId="47983" PostTypeId="1" Score="2" Tags="&lt;terminology&gt;&lt;deep-learning&gt;" Title="What's the difference between autoencoders and deep autoencoders?" ViewCount="86" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a question about using BATS/TBATS models implemented in the &lt;em&gt;forecast&lt;/em&gt; package for R. In &lt;a href=&quot;http://robjhyndman.com/papers/complex-seasonality/&quot; rel=&quot;nofollow&quot;&gt;De Liv­era, Hyndman &amp;amp; Snyder (2011)&lt;/a&gt; the models are used without any following analysis. Is it OK to use these models this way? Shouldn't there be at least verification that the residuals are a Gaussian white noise, which is an assumption for BATS/TBATS models? &#10;Many thanks&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-09T08:57:00.313" Id="102651" LastActivityDate="2014-06-09T08:57:00.313" OwnerUserId="47996" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;forecasting&gt;" Title="Verification of assumptions in TBATS model" ViewCount="44" />
  <row Body="&lt;p&gt;Here's one possibility:&lt;/p&gt;&#10;&#10;&lt;p&gt;If you assume the answers are like independent coin-flips, you can work out the probability of getting any number correct. For example if the answers were 50-50 coin flips the change of getting at least 15 correct is about 2%.  You might say &quot;well, that's pretty unlikely, let's say that 15 or more is 'above chance' since there's such a low probability of it happening by chance&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Indeed, that's how hypothesis tests are constructed.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's a table of values so you can choose which cutoff appeals to you:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Number    Probability of &#10;correct   at least that &#10;          many correct (%)&#10;  13       13.1588&#10;  14        5.7659&#10;  15        2.0695&#10;  16        0.5909&#10;  17        0.1288&#10;  18        0.0201&#10;  19        0.0020&#10;  20        0.0001&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I'd suggest choosing one of 14, 15 or 16.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-09T09:18:17.300" Id="102652" LastActivityDate="2014-06-09T09:18:17.300" OwnerUserId="805" ParentId="102643" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;Not sure which approach is better but I think if we have binary responses and two categories male-female, you can do something simple like a difference in proportions z-test.  This will basically answer the question if there is a difference in proportion of agree between females and males for a question.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-09T15:52:38.587" Id="102685" LastActivityDate="2014-06-09T15:52:38.587" OwnerUserId="48019" ParentId="102680" PostTypeId="2" Score="0" />
  <row AnswerCount="1" Body="&lt;p&gt;I fit several two level models in SAS using &lt;code&gt;PROC MIXED&lt;/code&gt;: an empty model with multilevel structure (null), a model with a level 2 covariate (partial model), and a model with level 1 and level 2 covariates (full model).&lt;/p&gt;&#10;&#10;&lt;p&gt;SAS estimated the random intercepts for all models, and the random effect covariate in the partial model, but in the full model the estimates for the random effect were all 0. My colleague suggested this because the covariate is a poor predictor in the presence of the level 1 covariates, that within group variation may be larger than between group variation, and that I should do a Hausman's test to see whether the multilevel structure is needed.&lt;/p&gt;&#10;&#10;&lt;p&gt;As I understand it, Hausman's test is&lt;/p&gt;&#10;&#10;&lt;p&gt;$W = \frac{(\hat{\beta}_{FE}^\star - \hat{\beta}_{RE}^\star)}{Var(\hat{\beta}_{FE}^\star) - Var(\hat{\beta}_{RE}^\star)}$&lt;/p&gt;&#10;&#10;&lt;p&gt;which approximates a $\chi^2$ distribution with 1 degree of freedom, where $\hat{\beta}_{FE}^\star$ is the estimated beta coefficient from the fixed effects model, $\hat{\beta}_{RE}^\star$ is the estimated beta coefficient from the random effects model, and $Var(\hat{\beta}_{FE}^\star)$ and $Var(\hat{\beta}_{RE}^\star)$ are their variances. The relationship to the chi-square distribution is intuitive and it is easy to see how to implement this for a single coefficient.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is whether I should calculate $W$ for each $\beta$ in my model (i.e. level 1 covariates and the level 2 covariate) or just the random effects covariate.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-09T16:11:42.253" FavoriteCount="1" Id="102688" LastActivityDate="2015-02-17T09:27:11.543" LastEditDate="2015-02-17T09:27:11.543" LastEditorUserId="26338" OwnerUserId="29109" PostTypeId="1" Score="2" Tags="&lt;mixed-model&gt;&lt;model-selection&gt;&lt;random-effects-model&gt;&lt;fixed-effects-model&gt;&lt;hausman&gt;" Title="Hausman's test for all $\beta$s – comparing FE vs RE models" ViewCount="134" />
  <row AnswerCount="1" Body="&lt;p&gt;I'm looking for an intuitive explanation of Bayesian Logistic Regression (I'm using it for texts if that's relevant). It seems that &lt;a href=&quot;http://www.stat.columbia.edu/~madigan/PAPERS/techno.pdf&quot; rel=&quot;nofollow&quot;&gt;this&lt;/a&gt; article presents it, but it's, uh, &lt;em&gt;way&lt;/em&gt; too mathy.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-06-09T17:05:18.430" FavoriteCount="1" Id="102692" LastActivityDate="2014-06-24T11:07:07.153" OwnerUserId="37686" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;machine-learning&gt;&lt;logistic&gt;&lt;bayesian&gt;&lt;bayes&gt;" Title="Intuitive explanation of Bayesian logistic regression?" ViewCount="164" />
  
  <row Body="&lt;p&gt;This is a balanced randomized block design with replicates. You just need to do&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;mymodel &amp;lt;- aov(y ~ treatment + Block, data=mydata)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;You could treat the Block as a random effect, but with only 3 blocks, I would leave it as fixed. There is no nesting, because the 60 plants were randomly assigned to their 3 blocks and 4 treatments, without any particular restrictions.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-09T17:53:56.177" Id="102698" LastActivityDate="2014-06-09T17:53:56.177" OwnerUserId="14188" ParentId="50418" PostTypeId="2" Score="1" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I am working on developing a logistic regression model that uses qualitative variables only ($n=990$). My remit is to define the equation that can identify the most relevant characteristics of a survey respondent that is favorable towards Company X. The proposed equation is similar to the following:&#10;\begin{align}&#10;{\rm Fav} = &amp;amp;a*{\rm age} + b*{\rm CoAware} + c*{\rm IssueAware} + d*{\rm readnewspaper} + e*{\rm region} + \\ &#10;            &amp;amp;f*{\rm income}\ldots&#10;\end{align}&#10;The dependent variable is &quot;Company Favorability&quot; (0 = Unfavorable/Neither | 1 = Favorable). There are currently 25 independent variables, 20 of which are binary IVs that range from highly correlated to the DV (awareness of Company) to not significant (gender). I also have 5 categorical variables that indicate region of the country, age (in categories), party affiliation, income level, and education.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I am almost certain that I need to use a logistic regression model for this approach. However, when I test my assumptions, I have having a very difficult time proving a linear relationship between the dichotomous independent variables and the logit transformation of the DV.&lt;/p&gt;&#10;&#10;&lt;p&gt;My other problem is that, I am somewhat overwhelmed by possible interaction effects. There are 34 possible options using 25 variables - leading me to over 50 million possible combinations.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have three questions:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Is there a better method to model with a binary dependent variable? &lt;/li&gt;&#10;&lt;li&gt;Am I missing something in the assumptions? (ie: Do I need to indeed prove the linear relationship if all of my variables are dichotomous)&lt;/li&gt;&#10;&lt;li&gt;Would it be better to approach this by looking at multicollinearity first, to reduce the number of variables overall, and then look at linear relationships with the logit of the DV?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="1" CreationDate="2014-06-09T20:27:55.563" Id="102722" LastActivityDate="2014-06-09T21:00:21.180" LastEditDate="2014-06-09T21:00:21.180" LastEditorUserId="7290" OwnerUserId="48038" PostTypeId="1" Score="3" Tags="&lt;regression&gt;&lt;logistic&gt;&lt;interaction&gt;&lt;assumptions&gt;" Title="Logistic regression assumptions for a model with many binary independent variables" ViewCount="1168" />
  <row Body="&lt;p&gt;Here are some thoughts:  &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;If you only want to identify relevant variables (e.g., you don't want to test pre-existing hypotheses, and you aren't trying to build the optimal model for predicting favorability later), you might try either CART, or combining logistic regression with LASSO penalization.  &lt;/li&gt;&#10;&lt;li&gt;There is no assumption of 'linearity' in logistic regression.  In general, people consider LR to be a &lt;em&gt;non-linear&lt;/em&gt; model.  There is some sense in which any model assumes it is properly specified, and it is possible (with a continuous variable) that you could need a squared term (e.g.) for the model to be reasonably well specified, but even that isn't true with only dichotomous variables.  &lt;/li&gt;&#10;&lt;li&gt;It is often best to try to reduce your independent variables first.  However, you still won't have to worry about 'linearity'.  &lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="8" CreationDate="2014-06-09T20:56:26.427" Id="102727" LastActivityDate="2014-06-09T20:56:26.427" OwnerUserId="7290" ParentId="102722" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm fairly well acquainted with using &lt;a href=&quot;http://en.wikipedia.org/wiki/Reservoir_sampling&quot; rel=&quot;nofollow&quot;&gt;Reservoir Sampling&lt;/a&gt; to sample from a set of undetermined length in a single pass over the data. One limitation of this approach, in my mind, is that it still requires a pass over the entire data set before any results can be returned. Conceptually this makes sense, since one has to allow items in the entirety of the sequence the opportunity to replace previously encountered items to achieve a uniform sample.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a way to be able to yield some random results &lt;strong&gt;before&lt;/strong&gt; the entire sequence has been evaluated? I'm thinking of the kind of lazy approach that would fit well with python's great &lt;a href=&quot;https://docs.python.org/2/library/itertools.html&quot; rel=&quot;nofollow&quot;&gt;itertools&lt;/a&gt; library. Perhaps this could be done within some given error tolerance? I'd appreciate any sort of feedback on this idea!&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-06-09T21:03:40.943" Id="102728" LastActivityDate="2014-06-11T21:07:29.447" OwnerUserId="47988" PostTypeId="1" Score="2" Tags="&lt;sampling&gt;&lt;algorithms&gt;" Title="Iterative or Lazy Reservoir Sampling" ViewCount="43" />
  
  <row Body="&lt;p&gt;It's unusual to not fit an intercept and generally inadvisable - one should only do so if you know it's 0, but I think that (and the fact that you can't compare the $R^2$ for fits with and without intercept) is well and truly covered already (if possibly a little overstated in the case of the 0 intercept); I want to focus on your main issue which is that you need the fitted function to be positive, though I do return to the 0-intercept issue in part of my answer.&lt;/p&gt;&#10;&#10;&lt;p&gt;The best way to get an always positive fit is to fit something that will always be positive; in part that depends on what functions you need to fit.&lt;/p&gt;&#10;&#10;&lt;p&gt;If your linear model was largely one of convenience (rather than coming from a known functional relationship that might stem from a physical model, say), then you might instead work with log-time; the fitted model is then guaranteed to be positive in $t$. As an alternative, you might work with speed rather than time - but then with linear fits you may get a problem with small speeds (long times) instead.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you know your response is linear in the predictors, you can attempt to fit a &lt;a href=&quot;http://en.wikipedia.org/wiki/Ordinary_least_squares#Constrained_estimation&quot;&gt;constrained regression&lt;/a&gt;, but with multiple regression the exact form you need will depend on your particular x's (there's no one linear constraint that will work for all $x's$), so it's a bit ad-hoc.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can also look at GLMs which can be used to fit models which have non-negative fitted values and can (if required) even have $E(Y)=X\beta$. &lt;/p&gt;&#10;&#10;&lt;p&gt;For example, one can fit a gamma GLM with identity link. You should not end up with a negative fitted value for any of your x's (but you might perhaps have convergence issues in some cases if you force the identity link where it really won't fit).&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's an example: the &lt;code&gt;cars&lt;/code&gt; data set in R, which records speed and stopping distances (the response).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/XpoUZ.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;One might say &quot;oh, but the distance for speed 0 is guaranteed to be 0, so we should omit the intercept&quot; but the problem with that reasoning is that the model is misspecified in several ways, and that argument only works well enough when the model is not misspecified - a linear model with 0 intercept doesn't fit at all well in this case, while one with an intercept is actually a half-decent approximation even though it's not actually &quot;correct&quot;. &lt;/p&gt;&#10;&#10;&lt;p&gt;The problem is, if you fit an ordinary linear regression, the fitted intercept is quite a way negative, which causes the fitted values to be negative. &lt;/p&gt;&#10;&#10;&lt;p&gt;The blue line is the OLS fit; the fitted value for the smallest x-values in the data set are negative. The red line is the gamma GLM with identity link -- while having a negative intercept, it only has positive fitted values. This model has variance proportional to mean, so if you find your data are more spread as the expected time grows, it may be especially suitable.&lt;/p&gt;&#10;&#10;&lt;p&gt;So that's one possible alternative approach that may be worth a try. It's almost as easy as fitting a regression in R.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you don't need the identity link, you might consider other link functions, like the log-link and the inverse link, which relate to the transformations already discussed, but without the need for actual transformation. &lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Since people usually ask for it, here's the code for my plot:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;plot(dist~speed,data=cars,xlim=c(0,30),ylim=c(-5,120))&#10;abline(h=0,v=0,col=8)&#10;abline(glm(dist~speed,data=cars,family=Gamma(link=identity)),col=2,lty=2)&#10;abline(lm(dist~speed,data=cars),col=4,lty=2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;(The ellipse was added by hand afterward, though it's easy enough to do in R as well)&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-06-09T23:06:55.437" Id="102753" LastActivityDate="2014-06-10T04:18:23.403" LastEditDate="2014-06-10T04:18:23.403" LastEditorUserId="805" OwnerUserId="805" ParentId="102709" PostTypeId="2" Score="9" />
  
  
  <row Body="&lt;p&gt;I would consider these tests at a minimum:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Hausman or Small-Hsiao tests of the IIA &lt;/li&gt;&#10;&lt;li&gt;Confusion matrices of predicted vs. actual outcome&lt;/li&gt;&#10;&lt;li&gt;Information criteria (AIC, BIC) &lt;/li&gt;&#10;&lt;li&gt;Various scalar measures of fit (like McFadden's $R^2$)&lt;/li&gt;&#10;&lt;li&gt;Wald or LR tests for combining alternatives&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;With Stata, check out the &lt;a href=&quot;http://www.indiana.edu/~jslsoc/spost.htm&quot; rel=&quot;nofollow&quot;&gt;SPost&lt;/a&gt; from Long and Freese as their as their &lt;a href=&quot;http://www.stata.com/bookstore/regression-models-categorical-dependent-variables/&quot; rel=&quot;nofollow&quot;&gt;categorical variables book&lt;/a&gt; for code and a nice intro to all of these tests with examples.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-06-10T01:05:40.993" Id="102762" LastActivityDate="2014-06-10T01:05:40.993" OwnerUserId="7071" ParentId="83899" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="104821" AnswerCount="1" Body="&lt;p&gt;Based on $N$ realizations of two random variables $X \sim N(0,\sigma_X^2)$ and $Y \sim N(0, \sigma_Y^2)$ with correlation $\rho$, I conduct a simple linear regression $Y = \beta_0 + X\beta_1 + \epsilon$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Using the known distributions of $X$ and $Y$, their correlation $\rho$ and the number of realizations $N$, I am trying to find the sampling distribution of the regression coefficients. &lt;/p&gt;&#10;&#10;&lt;p&gt;Estimates of the coefficients can be caluclated as:&lt;br&gt;&#10;$\hat \beta_1=r\frac{s_Y}{s_X}$ where $r$ is an estimate of $\rho$ and $s_X, s_Y$ are estimates of $\sigma_X, \sigma_Y$&lt;br&gt;&#10;$\hat \beta_0=\overline{Y} - \hat \beta_1 \overline{X}$ where $\overline{X}, \overline{Y}$ are estimates of the means of $X$ and $Y$&lt;/p&gt;&#10;&#10;&lt;p&gt;Intuitively, I thought the regression coefficients should follow a normal distribution. However, after some additional thoughts, I am not sure of anymore. Plotting the distribution of $\beta_1$ from 10,000 experimental runs with $N=12, \sigma_X^2=1, \sigma_Y^2=2, \rho =0.6$ (black) vs. fitted normal distribution, a deviation from normal distribution can be seen. This is also the case more iterations (I tested up to 100,000) for which the normal distribution should be quite stable. For more detail on the experiment, see below.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/CDFjN.png&quot; alt=&quot;Distribution of $\beta_1$ from 10,000 experimental runs (black) vs. fitted normal distribution&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Finding the specific distribution of $\beta_0, \beta_1$ - if there is one - is however only the second step. In the first step, I am trying to find the expected value and variance of the regression coefficients.&lt;/p&gt;&#10;&#10;&lt;p&gt;It is clear that $E[\hat \beta_0]= 0$ and $E[\hat \beta_1] = \rho \frac{\sigma_Y}{\sigma_X}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, I have trouble deriving the variance of both parameters. I have found some definitions for the coefficient variance in the literature. $Var[\hat \beta_0] = \frac{s_\epsilon^2\sum x_i^2}{N \sum (x_i - \overline{x})^2}$ and $Var[\hat \beta_1] = \frac{s_\epsilon^2}{\sum (x_i - \overline{x})^2} = \frac{s_\epsilon^2}{N \sigma_X^2}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, as stated correctly by whuber, these definitions are conditioned on $X$. A calculation is therefore only possible for specific relizations and the definitions are therefore not applicable to my use case. In the literature, I found that the restriction on fixed $X$ is going back to Fisher (e.g. his work 'Asymptotic distribution of the reduced-rank regression estimator under general conditions' from 1922). However, I did not find a consideration of the case with random $X$ in more recent literature. I have only found refrences to the (non central) Wishart distribution. I'm not sure whether and how the Wishart distribution can be used in my use case, though.&lt;/p&gt;&#10;&#10;&lt;p&gt;Overall, I am completly stuck on how to derive the variances in the described case, $Var[\hat \beta_0], Var[\hat \beta_0]$ with $X, Y$ being random variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;For cases with conditioning on $X$, I have found various answers on how to derive the variance of the regression coefficients, for instance &lt;a href=&quot;http://stats.stackexchange.com/a/89155/48067&quot;&gt;http://stats.stackexchange.com/a/89155/48067&lt;/a&gt;. However, since not only $\epsilon_i$ is a random variable, the approach described in the answer is not easily transferable to my problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;Consdering that $\hat \beta = r \frac{cov(X, Y)}{s_X^2}= r \frac{s_Y}{s_X}$, I assume that the distributions of $r, s_Y, s_X$ may for some reason be problematic for the calculation of the variance. I have found that $s_X^2, s_Y^2$ follow a gamma distribution. $s_X, s_Y$ should therefore follow a generalized gamma distribution, which have a well defined variance. However, I am not sure about the quotient of two generalized gamma distributed random variables. $r$ has a skewed distribution on which the Fisher transformation can be used - I am however not sure whether this helps to calculate the variance. Lastly, the sample covariance has a strange distribution, approximately like a shifted gamma distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;Overall, I have already derived some information about the distributions of characteristics of the samples. Yet, I have not found a way to use these information to derive the variance of the sample coefficients in my use case. &lt;/p&gt;&#10;&#10;&lt;p&gt;Can someone point me in the right direction? &lt;/p&gt;&#10;&#10;&lt;p&gt;--&lt;br&gt;&#10;Some additional detail about my &quot;simulations&quot;, as requested:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt; Samples of size N (e.g. N=12) are drawn from $X \sim N(0, \sigma_X^2)$ and $Y \sim N(0,\sigma_Y^2)$ (e.g. $\sigma_X^2 = 1$, $\sigma_Y^2 = 2$) so that the samples have a specific correlation $\rho$ (e.g. $\rho = 0.6$). This is achieved by using two uncorrelated random variables $A, B \sim N(0,1)$, and constructing a new (correlated) one as $C = \rho^2 A + \sqrt{1- \rho^2} B$ and correcting the variance of $A$ and $C$ in order to have $X$ and $Y$&#10;&lt;li&gt; A regression of $Y$ on $X$ is executed leading to coefficients $b_0$ and $b_1$&#10;&lt;li&gt; Steps one and two are repeated (for instance 10,000 times)&#10;&lt;li&gt; The distribution resulting of $b_0$ and $b_1$ resulting from the runs are analyzed&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;The distributions resulting from the last step are the distributions for which I want to derive exact distribution parameters in order to analyze the uncertainty in the regression parameters resulting from the sampling of the random variables.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-06-10T09:12:09.647" Id="102786" LastActivityDate="2014-06-26T06:54:54.047" LastEditDate="2014-06-12T14:28:12.393" LastEditorUserId="48067" OwnerUserId="48067" PostTypeId="1" Score="3" Tags="&lt;sampling&gt;&lt;regression-coefficients&gt;" Title="Sampling distribution of regression coefficients for normally distributed random variables" ViewCount="355" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Which test to rely on for a time series data of 30 years for instance(when they offer different cointegrating equations in Johansen's cointegration test)? Trace test or maximum eigen value? &lt;/p&gt;&#10;&#10;&lt;p&gt;Say trace test says there are 8 cointegrating equations. Is this the number we input in the &quot;number of cointegrating&quot; box when estimating the VECM? Does it make a huge difference if we put 1?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks! &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-10T09:31:17.477" Id="102788" LastActivityDate="2014-06-10T09:31:17.477" OwnerUserId="48050" PostTypeId="1" Score="0" Tags="&lt;vecm&gt;" Title="Trace test or maximum eigen value in Johansen's cointegration test?" ViewCount="95" />
  
  
  <row AcceptedAnswerId="102815" AnswerCount="2" Body="&lt;p&gt;Following &lt;a href=&quot;http://stats.stackexchange.com/q/101407/29911&quot;&gt;this question&lt;/a&gt;, I am interested on the best way to display four distributions with spikes.&lt;/p&gt;&#10;&#10;&lt;p&gt;Indeed, I have four distributions with circa 75% of elements in it at the maximum value (1). I am searching for a method that is able to display graphically which distribution is more clustered near the maximum value.&lt;/p&gt;&#10;&#10;&lt;p&gt;I thought about boxplots, but, as you can see in the plot below, it is not easy to decide which one of the second and the fourth boxes has the most quantity of data clustered near the maximum value (to the right).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/cgkz8.png&quot; alt=&quot;boxplot&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I thought about violin plots, but, with data with such spikes I cannot understand how the density of the violins are computed, because at least 75% of data has the maximum value.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/AaZaE.png&quot; alt=&quot;violin&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I tried to use a barplot. But even if you use fine binning near the maximum value, you have always the 75% at top and it is difficult to me to compare these four distributions.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/legoI.png&quot; alt=&quot;barplot&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I thought about qqplot between each couple of distributions, but I am unsure if this method can be used to the scope of defining which distribution is better clustered to the maximum.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/kNQWa.png&quot; alt=&quot;qqplot&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there any other data visualization method to represent this data in such a way that I can decide which of the four distribution is more clustered near the maximum value?&lt;/p&gt;&#10;&#10;&lt;p&gt;I don't want to use the mean because can be affected by a far outlier. Neither the median because here the median is always the maximum value.&lt;/p&gt;&#10;&#10;&lt;p&gt;I was thinking about counting how many points are above a certain threshold, but, how to define a good threshold?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-06-10T11:45:59.410" Id="102802" LastActivityDate="2014-07-01T11:44:50.240" LastEditDate="2014-06-10T13:32:40.753" LastEditorUserId="29911" OwnerUserId="29911" PostTypeId="1" Score="3" Tags="&lt;distributions&gt;&lt;data-visualization&gt;&lt;boxplot&gt;&lt;method-comparison&gt;&lt;barplot&gt;" Title="Best way to compare distributions with spikes" ViewCount="88" />
  
  
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;1) Is it actually so easy to empirically verify whether the aisle's historical sales is a Pareto distribution? &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;You cannot identify that sample data come from &lt;em&gt;any&lt;/em&gt; specific distribution. After all, even the tiniest deviation from that distributional form would make it from some other distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can, however, often identify that data isn't from some distribution, since many distributions will be extremely unlikely to have generated some samples, sometimes so unlikely (compared to alternatives) that one is prepared to conclude that it's not the case.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Does fitting the data to a Pareto curve mean anything?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Sure, it tells you the best-fitting Pareto.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;I've seen that, broadly, fitting data to a Pareto curve does not necessarily imply that the data is actually Pareto distributed.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Correct. Indeed, even without looking at any set of data, you can say it almost certainly isn't exactly Pareto.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, you &lt;em&gt;can&lt;/em&gt; see whether a Pareto is a plausible model for the data, and it's possible to explore how much the kind of deviations the data might have from being Pareto might impact your inferences/conclusions.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;To respond to the question in your title, I think it &lt;em&gt;can&lt;/em&gt; make sense, as long as one doesn't make the mistake of believing your model is actually true.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-10T13:27:23.043" Id="102824" LastActivityDate="2014-06-10T13:44:55.083" LastEditDate="2014-06-10T13:44:55.083" LastEditorUserId="805" OwnerUserId="805" ParentId="102820" PostTypeId="2" Score="1" />
  
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Can I run a Wilcoxon matched pair signed rank test after using Coarsened Exact Matching to match data? If yes, please guide me through the process. I use STATA. &lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-06-10T15:53:34.320" Id="102856" LastActivityDate="2014-07-19T15:05:11.393" LastEditDate="2014-06-11T02:43:49.227" LastEditorUserId="-1" OwnerUserId="48111" PostTypeId="1" Score="1" Tags="&lt;stata&gt;&lt;wilcoxon&gt;&lt;paired-data&gt;" Title="Wilcoxon matched pair signed rank test after using CEM" ViewCount="97" />
  <row Body="&lt;p&gt;Sometimes the Pareto is used to represent an exponential with gamma mixture. This is especially common in corporate statistics such as marketing and risk management. In other words people (or items in this case) do something or other according to an exponential distribution (in this case get sold) but because items differ from each other the rate parameter of the exponential differs and therefore the statistician uses a gamma distribution (because gamma has many shapes) to represent those differing rate parameters.&lt;/p&gt;&#10;&#10;&lt;p&gt;This is explained in &lt;a href=&quot;http://statisticalmodeling.wordpress.com/2011/06/23/the-pareto-distribution/&quot; rel=&quot;nofollow&quot;&gt;this blog post&lt;/a&gt; but reproduced here for longevity. To illustrate consider the exponential that is intended to represents when an item will sell:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$f(x)= \theta e^{-\theta x}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;This pdf can described as a conditional pdf just by changing the notation like this.&lt;/p&gt;&#10;&#10;&lt;p&gt;$$f_{X|\Theta}(x|\theta)= \theta e^{-\theta x}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;If we grant that $\theta$ itself is distributed according to a gamma distribution with scale $\alpha$ and shape $\beta$ then the pdf of $\theta$ looks like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$f_{\Theta}(\theta)=\frac{\alpha^{\beta}}{\Gamma(\beta)}\theta^{\beta-1}e^{-\alpha\theta}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;If you are not familiar with the gamma distribution, $\Gamma(x)$ is the &lt;a href=&quot;http://en.wikipedia.org/wiki/Gamma_function&quot; rel=&quot;nofollow&quot;&gt;gamma function&lt;/a&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;Constructing the unconditional exponential from the conditional plus the gamma means we have a 'density' of weights assigned to each $\theta$ in the exponential.&lt;/p&gt;&#10;&#10;&lt;p&gt;$$f_X(x) = \int^{\infty}_0 = f_{X|\Theta}(x|\theta) f_{\Theta}(\theta) d\theta$$&lt;/p&gt;&#10;&#10;&lt;p&gt;If you compute this integral (which I will leave as an exercise worth doing) you will get the Pareto defined by:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$f_X(x)= \frac{\beta \alpha^{\beta}}{(x+\alpha)^{\beta+1}}$$ &lt;/p&gt;&#10;&#10;&lt;p&gt;So the following two assumptions worth questioning are made.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;items are sold according to an exponential rate.&lt;/li&gt;&#10;&lt;li&gt;differences in the rate of sales of items is gamma.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Interesting note: For &lt;a href=&quot;http://en.wikipedia.org/wiki/Overdispersion&quot; rel=&quot;nofollow&quot;&gt;overdispersed&lt;/a&gt; poisson the negative binomial distribution (NBD) is often used. Often this overdispersion is attributed to heterogeneity in whatever is being modeled with a poisson. The NBD is therefore used because it is a similar mixture distribution also known as the gamma-poisson where the gamma is the distribution over the poisson's $\lambda$. The assumptions are very similar.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-10T16:36:43.107" Id="102862" LastActivityDate="2014-06-11T17:16:34.663" LastEditDate="2014-06-11T17:16:34.663" LastEditorUserId="13950" OwnerUserId="13950" ParentId="102820" PostTypeId="2" Score="2" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I am learning statistical analysis, and am now confused about calculating the standard error of the mean. &lt;/p&gt;&#10;&#10;&lt;p&gt;My dataset looks like..&lt;/p&gt;&#10;&#10;&lt;p&gt;Condition A:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;animal 1 - 3,4,4,3,6&#10;animal 2 - 5,5,5,8,7&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Condition B: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;animal 3 - 3,4,5,1,6&#10;animal 4 - 3,1,1,4,8&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;How can I calculate the standard error of the mean for condition A? I am confused by two methods&lt;/p&gt;&#10;&#10;&lt;p&gt;1) Simply taking the mean for two animals and use the means of them, like&lt;/p&gt;&#10;&#10;&lt;p&gt;mean for animal 1: 4 &lt;/p&gt;&#10;&#10;&lt;p&gt;mean for animal 2: 6&lt;/p&gt;&#10;&#10;&lt;p&gt;overall mean : 5&lt;/p&gt;&#10;&#10;&lt;p&gt;SD: $\sqrt{\frac{\left[\left(6-5\right)^{2} + \left(4-5\right)^{2}\right]}{2-1}} = \sqrt{2}$&lt;/p&gt;&#10;&#10;&lt;p&gt;SE: $\frac{\sqrt{2}}{\sqrt{2}} = 1$&lt;/p&gt;&#10;&#10;&lt;p&gt;or maybe using all the measurements like&lt;/p&gt;&#10;&#10;&lt;p&gt;SD: $\sqrt{\frac{\left[\left(3-5\right)^{2} + \left(4-5\right)^{2} + \left(4-5\right)^2 + ...... + \left(8-5\right)^2 + \left(7-5\right)^{2}\right]}{\left(10-1\right)}} = \sqrt{\frac{24}{9}}$&lt;/p&gt;&#10;&#10;&lt;p&gt;SE: $\frac{\sqrt{\frac{24}{9}}}{\sqrt{10}} = 0.51$&lt;/p&gt;&#10;&#10;&lt;p&gt;Whichi is right? Thanks in advance. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-06-10T16:48:37.790" FavoriteCount="1" Id="102865" LastActivityDate="2014-06-10T20:28:23.750" LastEditDate="2014-06-10T17:36:56.207" LastEditorUserId="44269" OwnerUserId="48115" PostTypeId="1" Score="2" Tags="&lt;mean&gt;&lt;error&gt;" Title="Standard error of mean" ViewCount="92" />
  <row AnswerCount="0" Body="&lt;p&gt;I want to estimate parameters of VARMA  model using maximum likelihood estimation using real data. The problem I face with is that I don't know how to set the initial  values for the parameters. I tried to choose randomly but the objective function is always undefined at initial point.&lt;/p&gt;&#10;&#10;&lt;p&gt;Please, is there any theory about this? I googled and found out that using this program &lt;a href=&quot;http://www.mathworks.com/help/econ/vgxvarx.html&quot; rel=&quot;nofollow&quot;&gt;http://www.mathworks.com/help/econ/vgxvarx.html&lt;/a&gt; &lt;strong&gt;it is not necessary to have initial values for any parameters to be estimated.&lt;/strong&gt;. But I don't understnd the theory behind this.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-10T18:07:52.317" Id="102878" LastActivityDate="2014-06-10T18:07:52.317" OwnerUserId="31718" PostTypeId="1" Score="1" Tags="&lt;var&gt;" Title="Initial Parameters for VARMA models?" ViewCount="21" />
  <row Body="&lt;p&gt;To me the question speaks of straight-up mixed models where the typical homogeneous (homoscedastic) error term is (possibly) decomposed into levels and (possibly) explained at each level using functions.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, suppose you have a model that looks like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;(1) $y_{i} = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} + \varepsilon_{i},$&lt;/p&gt;&#10;&#10;&lt;p&gt;where, say, $x_{1}$ is some continuous predictor, and $x_{2}$ is a nominal factor (for sake of simplicity).&lt;/p&gt;&#10;&#10;&lt;p&gt;Now suppose, as you suggest that the variance of $y$ depends on $x_{2}$. You might revise your model as in (2):&lt;/p&gt;&#10;&#10;&lt;p&gt;(2) $y_{i} = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} + \varepsilon_{i0} + \varepsilon_{i2},$&lt;/p&gt;&#10;&#10;&lt;p&gt;where:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\left[\begin{array}{c}\varepsilon_{0,i}\\&#10;\varepsilon_{2,i}\end{array}\right] \sim \mathcal{N}\left(0,\Omega_{\varepsilon}\right):\Omega_{\varepsilon}=\left[\begin{array}{cc}\sigma^{2}_{\varepsilon 0} &amp;amp;  \\&#10;0 &amp;amp; \sigma^{2}_{\varepsilon 2} \end{array}\right]$&lt;/p&gt;&#10;&#10;&lt;p&gt;(The covariance in $\Omega_{\varepsilon}$ is assumed zero here, since you've got factors with what I am assuming are mutually exclusive categories. If you are not comfortable with that assumption, and think you can estimate a covariance term, $\sigma_{\varepsilon02}$, go ahead and include that.)&lt;/p&gt;&#10;&#10;&lt;p&gt;You can even use this model if there is no fixed effect of $x_{2}$ on $y$ (i.e. if $x_{2}$ only contributes a random effect). This could be accomplished implicitly by letting a near zero estimate of $\beta_{2}$ be part of the model as in (2), or explicit as in (3):&lt;/p&gt;&#10;&#10;&lt;p&gt;(3) $y_{i} = \beta_{0} + \beta_{1}x_{1} + \varepsilon_{0,i} + \varepsilon_{2,i}.$&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-10T18:30:49.407" Id="102880" LastActivityDate="2014-06-10T18:30:49.407" OwnerUserId="44269" ParentId="102859" PostTypeId="2" Score="1" />
  
  
  <row AcceptedAnswerId="102966" AnswerCount="2" Body="&lt;p&gt;Just wanted your opinion on whether you think its appropriate to have a 2nd hypothesis when you have included in your study a secondary outcome? And also do you think its appropriate to have more than one secondary outcome? I believe it should be fine but wanted to double check with you guys.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-10T23:53:48.733" Id="102920" LastActivityDate="2014-06-11T13:41:03.430" LastEditDate="2014-06-11T13:41:03.430" LastEditorUserId="22311" OwnerUserId="48092" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;" Title="How many hypotheses? And secondary outcomes?" ViewCount="77" />
  
  <row AcceptedAnswerId="104162" AnswerCount="1" Body="&lt;p&gt;$$F_X(x)=\begin{cases}&#10;\quad\dfrac{\alpha}{\alpha+\theta}\left(\dfrac x\omega  \right)^\theta &amp;amp;\text{ if } x&amp;lt;\omega \\ \\&#10;1-\dfrac{\theta}{\alpha+\theta}\left(\dfrac\omega x\right)^{\alpha} &amp;amp;\text{ if } x&amp;gt;\omega \end{cases}\quad\text{ where } 0\le x&amp;lt; +\infty.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I derived it by having $F_{X|Y}(x|y)=\left(\dfrac{x}{y}\right)^\theta$, which is $Y$ with a multiplicative &quot;shock/noise&quot; give by a $\mathrm{Beta}(\theta,1)$ (max of $\theta$ uniforms)&#10;and $Y$ follows a Pareto distribution&#10;$F_Y(y)=1-\left(\dfrac{\omega}{y}\right)^\alpha$. &lt;/p&gt;&#10;&#10;&lt;p&gt;I was not able to find a classification for it. Does it have a name/family?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-11T00:29:00.610" FavoriteCount="2" Id="102922" LastActivityDate="2014-06-20T16:55:13.317" OwnerUserId="43681" PostTypeId="1" Score="0" Tags="&lt;distributions&gt;&lt;bayesian&gt;" Title="Conjugated priors (Pareto and Beta), name of the unconditional distribution?" ViewCount="77" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am using a Kruskal Wallis test for my thesis and want to report the results from this test. However, one of the plots from this test is a pairwise comparison test, which I do not understand. Can someone explain me how I can report this in a scientific paper and how the numbers in the table correspond with the plotted graph.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/sfBXB.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-11T01:39:00.267" Id="102930" LastActivityDate="2014-06-11T01:39:00.267" OwnerUserId="46342" PostTypeId="1" Score="0" Tags="&lt;spss&gt;&lt;paired-comparisons&gt;&lt;kruskal-wallis&gt;" Title="How to report the Pairwise comparison in SPSS" ViewCount="478" />
  <row AcceptedAnswerId="103199" AnswerCount="1" Body="&lt;p&gt;&lt;a href=&quot;http://www.ece.uc.edu/~mazlack/dbm.w2011/Mazlack_NAFIPS_2009.pdf&quot; rel=&quot;nofollow&quot;&gt;Representing causality using fuzzy cognitive maps&lt;/a&gt; presents a cognitive model which is a graphical model consisting of weighted directed graph. To me it looks like a state transition machine. Can somebody explain the difference between causal map and markov chain? Thank you&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-06-11T02:00:35.217" Id="102931" LastActivityDate="2014-06-16T23:48:09.957" OwnerUserId="21160" PostTypeId="1" Score="1" Tags="&lt;graphical-model&gt;&lt;markov-chain&gt;" Title="Difference between graphical model and markov chain" ViewCount="85" />
  <row AnswerCount="1" Body="&lt;p&gt;Suppose I have two discrete independant random variables $X$ and $Y$, and that I'm interested in the expected value of the random variable $W$, where:&#10;$$&#10;W= \text{sign}(X-Y).&#10;$$&#10;So, W is 1 if $X&amp;gt;Y$, -1 if $Y&amp;gt;X$ and 0 otherwise.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I sample the distributions of $X$ and $Y$ ten times each, giving me $\{X_1, \dots, X_{10}\}$ and $\{Y_1, \dots, Y_{10}\}$.&lt;br&gt;&#10;Consider these two ways to estimate $\text{E}\{W\}$&#10;$$&#10;\quad\quad\bar{W} = \frac{1}{10}\sum_{i=1}^{10} W_{i,i}, \\&#10;\text{and, } \quad\quad&#10;\bar{W}' = \frac{1}{100}\sum_{i=1}^{10}\sum_{j=1}^{10} W_{i,j}, \\&#10;\text{where } \quad W_{i,j} = \text{sign}(X_i - Y_j)&#10;$$&#10;I know that $\text{Var}\{\bar{W}\} = \frac{1}{10}\text{Var}\{W\}$, but what is $\text{Var}\{\bar{W}'\}$, and how can I estimate it from my 20 samples?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-06-11T02:40:35.420" Id="102933" LastActivityDate="2015-02-25T18:01:27.857" LastEditDate="2014-06-11T17:52:40.430" LastEditorUserId="2116" OwnerUserId="48144" PostTypeId="1" Score="2" Tags="&lt;variance&gt;&lt;mean&gt;&lt;covariance&gt;&lt;expected-value&gt;" Title="Variance of sample mean for dependent samples" ViewCount="106" />
  <row Body="&lt;p&gt;A standard feed forward network is conceptually not meant to generate samples. It is a discriminative model.&#10;There are, however, generative models like restricted Boltzmann machines that allow you to generate samples.&lt;/p&gt;&#10;&#10;&lt;p&gt;That being said, it's not completely impossible to get something sample-like from a feed forward neural network.&#10;You could fix the output of the model and also keep all the weights fixed and then optimize for the inputs. &#10;If it's a binary classifier, dog=1 vs not-dog=0, you could fix the output to one and do your usual backprop-gradient descent but instead of optimizing the weights you only optimize for the network's input. That will give you input values that will lead to a high score for the classifier and if the network was trained properly it might look like a dog.&lt;/p&gt;&#10;&#10;&lt;p&gt;Maybe you can get some inspiration from&#10;&lt;a href=&quot;http://arxiv.org/pdf/1311.2901v3.pdf&quot; rel=&quot;nofollow&quot;&gt;Visualizing and Understanding Convolutional Networks&#10;by Zeiler et al.&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-06-11T05:29:01.487" Id="102946" LastActivityDate="2014-06-11T05:29:01.487" OwnerUserId="48143" ParentId="102935" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;The big problem is of course that you cannot interpret this as the effect of a change in one percentage while keeping the remainder constant: if the percentage in one group increases at least one of the remaining groups must decrease otherwise you would end up with more than 100%. &lt;/p&gt;&#10;&#10;&lt;p&gt;One solution that can help is to pick various racial compositions and compare the predicted outcomes of those. For example, you can take the racial compositions of two or more real cities and compare the predicted outcomes of those. Taking the compositions in real cities often helps the communication with the audience; It is easier to talk about comparing Berlin to Stuttgart (I am living in Germany), than comparing a city with $x_1\%$ Blacks, $y_1\%$ Hispanics and $z_1\%$ Whites with a city with $x_2\%$ Blacks, $y_2\%$ Hispanics and $z_2\%$ Whites.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can also look at Chapter 12 of J. Aitchison (2003) &lt;em&gt;The Statistical Analysis of Compositional Data&lt;/em&gt; The Blackburn Press.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;On a different note: beware of the interpretation of such a regression using city level data. The results cannot tell you much about the behaviour of individuals. This is known as the ecological fallacy often attributed to: &lt;/p&gt;&#10;&#10;&lt;p&gt;W. S. Robinson (1950) &quot;Ecological Correlations and the Behavior of Individuals&quot; &lt;em&gt;American Sociological Review&lt;/em&gt;, Vol. 15, No. 3, pp. 351-357. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-06-11T07:59:06.287" Id="102957" LastActivityDate="2014-06-11T09:31:55.670" LastEditDate="2014-06-11T09:31:55.670" LastEditorUserId="17230" OwnerUserId="23853" ParentId="102927" PostTypeId="2" Score="2" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have two time series: (1) daily mean water temperature from 1988 to 2014 and (2) daily mean air temperature from 1968 to 2010. The water temperature time series has missing data, occurring on individual days and for periods of consecutive days (e.g., data for all of 1993 is missing). The air temperature data has data missing from 1990-02-01 - 1991-09-30. The data are plotted below:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/7c2bF.jpg&quot; alt=&quot;Water temperatures. Red bands indicate missing data. Smooth is annual GAM.&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/12Wur.jpg&quot; alt=&quot;Airtemperatures. Red bands indicate missing data. Smooth is annual GAM.&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to use the air temperature series to inform missing values in the water temperature series.&lt;/p&gt;&#10;&#10;&lt;p&gt;From my understanding, this could be achieved using a state-space or Dynamic Linear model (DLM). I’m imagining a Seemingly Unrelated Time Series Equations (SUTSE) DLM in which both temperature series follow a stochastic linear trend and annual seasonal component.&lt;/p&gt;&#10;&#10;&lt;p&gt;So far (I’m only learning DLM), I have managed to fit a simple 2nd order stochastic linear trend SUTSE DLM to the water and air time series (both reduced to the same period 1986-08-19 to 2010-04-15) as follows (I’m afraid a data-sharing agreement forbids me to provide the data):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# the 'base' univariate model&#10;uni &amp;lt;- dlmModPoly()&#10;&#10;# to get the matrices of the correct dim&#10;temps_mod &amp;lt;- uni %+% uni &#10;&#10;## now redefine matrices to keep levels together and slopes together&#10;FF(temps_mod) &amp;lt;- FF(uni) %x% diag(2)&#10;GG(temps_mod) &amp;lt;- GG(uni) %x% diag(2)&#10;W(temps_mod)[] &amp;lt;- 0 # 'clean' the system variance&#10;&#10;## define a build function for MLE&#10;buildSUTSE &amp;lt;- function(psi) {&#10;  U &amp;lt;- matrix(0, nrow = 2, ncol = 2)&#10;  U[upper.tri(U)] &amp;lt;- psi[1:2]&#10;  diag(U) &amp;lt;- exp(0.5 * psi[3:4])&#10;  W(temps_mod)[3:4, 3:4] &amp;lt;- crossprod(U)&#10;  diag(V(temps_mod)) &amp;lt;- exp(0.5 * psi[5:6])&#10;  temps_mod&#10;}&#10;&#10;## estimation&#10;temps_est &amp;lt;- dlmMLE(temps, rep(-2, 9), buildSUTSE,&#10;                 control = list(maxit = 500)) ## few iters for test&#10;&#10;## check convergence&#10;print(temps_est$conv)&#10;&#10;## set up fitted model&#10;temps_mod &amp;lt;- buildSUTSE(temps_est$par)&#10;&#10;## look at the estimated variance / covariance matrices&#10;print(W(temps_mod)[2:3, 2:3])&#10;print(cov2cor(W(temps_mod)[2:3, 2:3])) # slopes&#10;print(sqrt(diag(V(temps_mod)))) # observation standard deviations&#10;&#10;## smooths&#10;tempsSmooth &amp;lt;- dlmSmooth(temps, temps_mod)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;which results in the following fits:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/e4O2j.jpg&quot; alt=&quot;SUTSE DLM linear stochastic trend fits&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;While it is encouraging to see that the fitting has worked, the 1993 water temperature estimates look poor / too low relative to the surrounding values.&#10;Here are my questions:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Am I taking the right approach, or is there a better / simpler way to estimate missing water temperature data from air temperature?&lt;/li&gt;&#10;&lt;li&gt;How do I extend the above code to include a seasonal trend, with (presumably) a frequency of 365 (even though leap years are present)? Should I be considering trigonometric functions, e.g., dlmModTrig?&lt;/li&gt;&#10;&lt;li&gt;Has anyone seen this attempted before, and if so, where can I read about it (preferably with an R example)?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Thanks in advance for any help.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-06-11T08:46:14.143" Id="102963" LastActivityDate="2014-06-11T08:46:14.143" OwnerUserId="48080" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;multivariate-analysis&gt;&lt;interpolation&gt;&lt;dlm&gt;" Title="SUTSE DLM on daily mean water &amp; air temperature TS" ViewCount="108" />
  <row AcceptedAnswerId="102996" AnswerCount="2" Body="&lt;p&gt;Suppose a model,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$x_{i} \sim N(\theta_{i}, \phi), \text{ for } i=1,\ldots,n$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Furthermore, suppose the variance parameter, $\phi$, is some known constant.&lt;/p&gt;&#10;&#10;&lt;p&gt;The multidimensional Jeffreys prior is defined as,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\pi(\vec{\theta}) \propto \sqrt{\text{det}I(\vec{\theta})}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\text{det}I(\vec{\theta)}$ is the determinant of the multidimensional Fisher information.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can a multidimensional Jeffreys prior be used to formulate a prior distribution for the posterior distribution, $p(\vec{\theta}|x_{1},\ldots,x_{n})$?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-06-11T11:57:58.913" Id="102983" LastActivityDate="2014-06-11T13:24:21.980" OwnerUserId="9171" PostTypeId="1" Score="1" Tags="&lt;bayesian&gt;&lt;prior&gt;&lt;jeffreys-prior&gt;" Title="Use of the Jeffreys prior in multidimensional models" ViewCount="66" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I used 5-point Likert-type scales as a dependent variables (1 strongly agree, 2 agree, 3 neutral,  4 disagree, 5 strongly disagree), but, as you are aware, using binary logistic regression requires me to create a binary variable from these 5-level responses. So, strongly agree and agree would become category 1, and disagree and strongly disagree would become category 2, while we exclude the neutral response option (although this significantly cuts down my sample size).&lt;/p&gt;&#10;&#10;&lt;p&gt;One more issue in this regard, my questionnaire has 12 variables and each variable is measured by a group of items. For example, I have education (measured by 7 items), Gender inequality (measured by 6 items), trust (measured by 6 items), career development (measured by 7 items) and so on. Now, and in order to create the binary category do I need to take the high and the low score of each variables and find the mean and standard deviation, and exclude the cases in the middle? but this again will significantly cuts down my sample size? &lt;/p&gt;&#10;&#10;&lt;p&gt;Finally is it better to use multinomial logistic regression instead of binary logistic regression when dependent variables are measured by 5-point Likert scale, which means there are 5 groups with each variable? &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-06-11T13:16:17.927" Id="102994" LastActivityDate="2014-06-11T13:50:49.277" LastEditDate="2014-06-11T13:50:49.277" LastEditorUserId="930" OwnerUserId="48179" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;logistic&gt;" Title="How can I create binary variables from Likert scale for binary logistic regression model?" ViewCount="267" />
  <row Body="&lt;p&gt;Some key differences, preceding a longer explanation below, are that:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Crucially: the Jeffries-Matusita distance applies to distributions, rather than vectors in general.&lt;/li&gt;&#10;&lt;li&gt;The J-M distance formula you quote above only applies to vectors representing discrete probability distributions (i.e. vectors that sum to 1).&lt;/li&gt;&#10;&lt;li&gt;Unlike the Euclidean distance, the J-M distance can be generalised to any distributions for which the Bhattacharrya distance can be formulated.&lt;/li&gt;&#10;&lt;li&gt;The J-M distance has, via the Bhattacharrya distance, a probabilistic interpretation.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;The Jeffries-Matusita distance, which seems to be particularly popular in the Remote Sensing literature, is a transformation of the &lt;a href=&quot;http://en.wikipedia.org/wiki/Bhattacharyya_distance&quot;&gt;Bhattacharrya distance&lt;/a&gt; (a popular measure of the dissimilarity between two distributions, denoted here as $b_{p,q}$) from the range $[0, \inf)$ to the fixed range $[0, \sqrt{2}]$:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;JM_{p,q}=\sqrt{2(1-\exp(-b(p,q))}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;A practical advantage of the J-M distance, according to &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.32.9829&quot;&gt;this paper&lt;/a&gt; is that this measure &quot;tends to suppress high separability values, whilst overemphasising low separability values&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;The Bhattacharrya distance measures the dissimilarity of two distributions $p$ and $q$ in the following abstract continuous sense:&#10;$$&#10;b(p,q)=-\ln\int{\sqrt{p(x)q(x)}}dx&#10;$$&#10;If the distributions $p$ and $q$ are captured by histograms, represented by unit length vectors (where the $i$th element is the normalised count for $i$th of $N$ bins) this becomes:&#10;$$&#10;b(p,q)=-\ln\sum_{i=1}^{N}\sqrt{p_i\cdot q_i}&#10;$$&#10;And consequently the J-M distance for the two histograms is:&#10;$$&#10;JM_{p,q}=\sqrt{2\left(1-\sum_{i=1}^{N}{\sqrt{p_i\cdot q_i}}\right)}&#10;$$&#10;Which, noting that for normalised histograms $\sum_{i}{p_i}=1$, is the same as the formula you gave above:&#10;$$&#10;JM_{p,q}=\sqrt{\sum_{i=1}^{N}{\left(\sqrt{p_i} - \sqrt{q_i}\right)^2}}=\sqrt{\sum_{i=1}^{N}{\left(p_i -2 \sqrt{p_i}\sqrt{q_i} + q_i \right)}}=\sqrt{2\left(1-\sum_{i=1}^{N}{\sqrt{p_i\cdot q_i}}\right)}&#10;$$&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-06-11T14:03:57.613" Id="103000" LastActivityDate="2014-06-11T14:52:34.553" LastEditDate="2014-06-11T14:52:34.553" LastEditorUserId="28620" OwnerUserId="28620" ParentId="102810" PostTypeId="2" Score="5" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have simulation data from 1000 runs, plotting some measurable (in this case convergence of the algorithm) as a function of simulation time. Each run produces a discrete set of points &lt;code&gt;(t, f(t))&lt;/code&gt; and the superimposed image of all the runs looks like:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/UyZ44.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Clearly there is some structure here. How can I generate a curve that somehow captures the average value of f(t) for a given t? In addition I'd like to be able to estimate the spread as well, though I'm not sure what the underlying distribution is.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's say that this plot was generated from simulation parameters &lt;strong&gt;A&lt;/strong&gt;. The goal of this project is to show that another set of simulation parameters &lt;strong&gt;B&lt;/strong&gt;, produces a different set of curves, and have a nice way to visualize this.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-06-11T14:37:50.570" Id="103004" LastActivityDate="2014-06-11T14:37:50.570" OwnerUserId="600" PostTypeId="1" Score="4" Tags="&lt;time-series&gt;&lt;data-visualization&gt;&lt;average&gt;&lt;lognormal&gt;" Title="&quot;Average&quot; line over irregular log series data" ViewCount="46" />
  
  
  
  <row AcceptedAnswerId="103034" AnswerCount="1" Body="&lt;p&gt;I am running simulations to compare different weighting methods to estimate the mean of y (with missing values). I use bias, RMSE and 95% CI Coverage as my performance metrics. However, looking at RMSE and coverage, I arrive at different conclusions. For example, for method A and B, bias(x10^3) are 75 and -134 respectively, the corresponding RMSEs(x10^3) are 158 and 159, while coverages are 85% and 34%. Given that the variance of method A is higher than method B, and the weighting model is non-linear, is it normal to have such RMSE-coverage inconsistency?&lt;/p&gt;&#10;&#10;&lt;p&gt;I have been searching for references which point out to this inconsistency. But all I could find was this statement:&lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;In selecting a best single model or model ensemble, the three types of assessments may have to be balanced. The model with the lowest root mean squared error (RMSE) may do worse on trend or have coverage that is too high or too low...&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.pophealthmetrics.com/content/pdf/1478-7954-10-1.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.pophealthmetrics.com/content/pdf/1478-7954-10-1.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;And if this is the case, can we think of Coverage as a more important performance metric?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-11T16:35:46.687" Id="103024" LastActivityDate="2014-06-19T09:41:46.053" LastEditDate="2014-06-19T09:41:46.053" LastEditorUserId="28671" OwnerUserId="28671" PostTypeId="1" Score="0" Tags="&lt;rms&gt;&lt;coverage-probability&gt;" Title="Inconsistency between RMSE and 95% CI Coverage" ViewCount="31" />
  
  <row Body="&lt;p&gt;Why would there be a general answer to the question which performance measure is to be prefered? It is just a trade-off between different properties, and a trade-off depends on the specific context in which the method is to be used.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-11T18:39:23.750" Id="103034" LastActivityDate="2014-06-11T18:39:23.750" OwnerUserId="23853" ParentId="103024" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm looking for a dataset on which a diffusion kernel (also called heat kernel), used via SVM, would get better accuracy than other kernels for the classification task. I want to use such a dataset to make sure my implementation of the diffusion kernel is working, and to get a better understanding of the diffusion kernel (does it classify as &quot;similar&quot; graphs that have the same expression on the same zones (doesn't care about the level expression as long as it's approximatively the same value on some part of the graph) ? Or can it understand substructure of the graphs, and say that for instance if 2 out of 3 connected features is present, it belongs in the same class ?).&lt;/p&gt;&#10;&#10;&lt;p&gt;Does anyone know of such a dataset, or know how to construct it ?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;&#10;&lt;p&gt;Jess&lt;/p&gt;&#10;&#10;&lt;p&gt;PS : For now, I've tried creating my own dataset in R, but the linear and gaussian kernel get similar results for classification. Here was the idea behind it :&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm trying to predict whether a pokemon is dragon type, based on this underlying graph :&#10;&lt;img src=&quot;http://i.stack.imgur.com/zTP2H.jpg&quot; alt=&quot;feature graph&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is the dataset, in R :&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    charizard = c(70, 100, 80, 100, 80, 10, 95)&#10;    dragonite = c(80, 80, 80, 60, 20, 20, 50)&#10;    tyranitar = c(90, 0, 100, 80, 40, 5, 70)&#10;    altaria = c(30, 95, 10, 80, 60, 70, 80)&#10;    aerodactyl = c(65, 100, 100, 50, 60, 10, 40)&#10;    salamence = c(100, 100, 100, 100, 90, 0, 70)&#10;    rayquaza = c(100, 0, 100, 100, 80, 10, 40)&#10;    lugia = c(100, 80, 90, 80, 70, 40, 30)&#10;    gyarados = c(80, 0, 90, 80, 70, 2, 75)&#10;    hydreigon = c(95, 90, 90, 90, 70, 15, 70)&#10;    dratini = c(20, 0, 90, 60, 20, 60, 70)&#10;    hooh = c(100, 90, 5, 90, 70, 20, 35)&#10;&#10;    vibrava = c(60, 90, 20, 10, 100, 20, 50)&#10;    entei = c(95, 0, 5, 100, 60, 10, 60)&#10;    scyther = c(40, 40, 60, 0, 100, 20, 75)&#10;    magikarp = c(0, 0, 40, 0, 20, 20, 10)&#10;    butterfree = c(30, 100, 10, 5, 40, 90, 90)&#10;    milotic = c(90, 0, 100, 20, 80, 80, 90)&#10;    ponyta = c(15, 0, 30, 70, 70, 80, 60)&#10;    blastoise = c(90, 0, 40, 10, 40, 50, 80)&#10;    alakazam = c(95, 0, 60, 40, 100, 20, 60)&#10;    sudowoodo = c(50, 0, 60, 5, 30, 40, 40)&#10;&#10;&#10;    train = rbind(charizard, dragonite, tyranitar, altaria, aerodactyl, salamence, rayquaza,&#10;                  vibrava, entei, scyther, magikarp, butterfree, milotic, ponyta)&#10;&#10;    test = rbind(lugia, gyarados, hydreigon, dratini, hooh,&#10;                 blastoise, alakazam, sudowoodo)&#10;&#10;    cl_train = as.factor(c(rep('dragon', 7), rep('not_dragon', 7)))&#10;&#10;    cl_test = as.factor(c(rep('dragon', 5), rep('not_dragon', 3 )))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Please note that I'm fully aware that Ho-oh is not a dragon pokemon, and neither is tyranitar nor lugia. It's just an example (and they kind of look like a dragon...).&lt;/p&gt;&#10;&#10;&lt;p&gt;As mentionned earlier, this small dataset gets perfect accuracy for linear kernel, for instance, and not for the diffusion kernel, so it's not what I'm looking for even though I tried to buid it for this purpose...&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-11T18:39:58.987" Id="103035" LastActivityDate="2014-06-11T18:39:58.987" OwnerUserId="48203" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;classification&gt;&lt;svm&gt;&lt;kernel&gt;" Title="construct/load dataset that performs better with diffusion kernel than other kernel" ViewCount="19" />
  
  
  <row Body="&lt;p&gt;You want to perform an intervention analysis, which is a subset of time series analysis. To perform this analysis you will do 2 separate regressions, one to measure the impact of advertising on clicks and the second to measure the impact of advertising on signups.  &lt;/p&gt;&#10;&#10;&lt;p&gt;To accomplish this you regress the dependent variable (clicks or signups) for each day on the independent variable which is a dummy variable (or indicator function) which takes the value of 1 if the particular data points was collected with advertising and 0 if the particular dependent data point was collected during the period without advertising.  &lt;/p&gt;&#10;&#10;&lt;p&gt;The beta coefficient estimated from the regression would reflect the estimate decrease in average clicks or signups from foregoing advertising.  Further the p-value for the beta estimate can be used to determine if there is a statistically significant decline in clicks/signups as a result of excluding advertising, i.e., if the average number of clicks/signups during the non-advertising period is statistically significantly less than during the period with advertising.&lt;/p&gt;&#10;&#10;&lt;p&gt;A caveat would be that the model described above includes no other exogenous variables besides the existence of advertising.  If any other non-included variables also changed during this period, the beta estimate produced by this regression model would be biased by the marginal impact of the excluded exogenous variables.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-11T21:12:00.530" Id="103052" LastActivityDate="2014-06-11T21:12:00.530" OwnerUserId="48215" ParentId="102902" PostTypeId="2" Score="1" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am having difficulties with the interpretation of the regression results from estimating a growth regression in a dynamic panel data set-up, estimated using Stata. (I'm using difference GMM and system GMM estimators by Arellano-Bond and Arellano-Bover/Blundell-Bond respectively).&lt;/p&gt;&#10;&#10;&lt;p&gt;For my thesis I am interested in the effect of a set of regressors $X$ on growth. My specification is a follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\ln Y_{i,t} = a \ln Y_{i,t-1} + bX_{i,t} + e_{i,t}$$&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;$Y$ measures income per capita&lt;/li&gt;&#10;&lt;li&gt;$X$ is the independent variable of interest, which represents a percentage where 1% is written as 0.01 for example.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;My difficulty lies in the interpretation of coefficient $b$, therefore I would like to ask for your help.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Is it correct that b represents the effect on the growth rate(as the lagged income per capita is also incorporated in the model)?&lt;/li&gt;&#10;&lt;li&gt;Also if the outocome is for example 0.5 how do i calculate the actual effect?&lt;/li&gt;&#10;&lt;li&gt;How do i calculate the rate of convergence in this model?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2014-06-12T10:11:45.753" Id="103100" LastActivityDate="2014-06-12T10:30:01.073" LastEditDate="2014-06-12T10:30:01.073" LastEditorUserId="26338" OwnerUserId="48238" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;panel-data&gt;&lt;regression-coefficients&gt;&lt;convergence&gt;&lt;dynamic-regression&gt;" Title="Coefficients from a Dynamic Panel Data Model of Economic Growth" ViewCount="142" />
  <row Body="&lt;p&gt;Got too long for a comment, so I guess it's an answer now.&lt;/p&gt;&#10;&#10;&lt;p&gt;Use long, descriptive names for anything you need to keep around for more than a couple of minutes. &lt;/p&gt;&#10;&#10;&lt;p&gt;The long names might be a nuisance to type, but if you're working by running scripts, writing functions and so on, the actual typing is not that much anyway. &lt;/p&gt;&#10;&#10;&lt;p&gt;But even a lot of typing is far easier than trying to figure out what you did with variables you can't remember. &lt;/p&gt;&#10;&#10;&lt;p&gt;Imagine you need to figure out what you did in 6 months or a year - or imagine some other poor person in the same position. Write your code for poor future-you. Help future-you out by making everything as obvious as possible. Imagine future-you can't remember anything about what you're doing.&lt;/p&gt;&#10;&#10;&lt;p&gt;It's nearly always better to have scripts that generate your calculations, and document them as well as making them readable. To that end, also delete variables you don't need, it will help keep you disciplined about making everything generatable again.&lt;/p&gt;&#10;&#10;&lt;p&gt;Keep related variables in data frames or lists (and keep related matrices, data frames and so in inside lists when there are more than a couple of them), and name all your data frame columns and your list components with names that explain what they are.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-06-12T10:14:57.857" Id="103101" LastActivityDate="2014-06-12T10:14:57.857" OwnerUserId="805" ParentId="103074" PostTypeId="2" Score="7" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I have a data set, have two interval variables and lots of categorical (ordinal and nominal) sociodemographic variables. &lt;br/&gt;&#10;Lets say one of may interval variable is $x$ and the other is $y$, I want to test  effect of $x$ and a demographic variable such as gender or educational level on $y$. In other words, how can I compare or find a relationship of $x$ scores with different educational levels and $y$ scores (dependent variable)? I want to say that there is a difference between $x$ scores from high school and $y$ scores. Or I want to say that there is a relationship between $x$ scores from high school and $y$ scores. Or I want to say that x scores from university educational level is related to y scores.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-12T12:49:00.743" Id="103119" LastActivityDate="2014-06-12T15:19:40.093" LastEditDate="2014-06-12T14:40:24.700" LastEditorUserId="26338" OwnerUserId="40575" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;" Title="Please help I am confused to decide which statistical test?" ViewCount="65" />
  
  
  <row AcceptedAnswerId="103155" AnswerCount="1" Body="&lt;p&gt;I've read:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/64991/model-selection-and-cross-validation-the-right-way&quot;&gt;Model selection and cross-validation: The right way&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/99108/crossvalidation-and-or-testdata-always-use-both-or-can-one-exclude-the-other&quot;&gt;Crossvalidation and/or testdata. Always use both or can one exclude the other?&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;but I still don't get it. &lt;/p&gt;&#10;&#10;&lt;p&gt;My problem is to construct a simple SVM, tune it's parameters and calculate the generalization error. Assume I have a dataset with e.g. 10 features and 200 samples. I don't want to waste data, since it's relatively small. My approach would be:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Split the dataset (e.g., 70/30) with holdout method into training / validation and test set.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Make a repeated n-fold cross-validation on the training set. I calculate the error rate (misclassification rate) after the folds are done (simply counting all misclassified samples). I try to minimize the error rate (or any other loss function?) each time the complete n fold is done. I store the error rate and choose the parameters with the lowest error rate. Then I retrain the model on the complete training set with the estimated parameters.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;Calculate generalization error with the test set.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;The problems: &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;The larger my test set is, the smaller gets the train set, so I discard potential information. Can this be solved via a &quot;stacked&quot; n-fold cv?&lt;/li&gt;&#10;&lt;li&gt;Do I really have to make a REPEATED n-fold cv? Are there other possibilities?&lt;/li&gt;&#10;&lt;li&gt;Is the error rate an appreciate loss function or should I choose another one (eg. the empirical error function or mse, but then I'd need a probability output, right?)?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2014-06-12T14:07:02.037" Id="103137" LastActivityDate="2014-06-13T10:32:40.133" LastEditDate="2014-06-12T14:29:53.803" LastEditorUserId="7290" OwnerUserId="48253" PostTypeId="1" Score="2" Tags="&lt;svm&gt;&lt;cross-validation&gt;" Title="SVM parameter selection and model testing with cross-validation" ViewCount="140" />
  <row AnswerCount="0" Body="&lt;p&gt;I am looking for a way to graph a triple optimization used in boosting. I did individual cross-validation on shrinkage, number of trees, and interaction depth and found the boundaries of interesting values. Now I am trying to figure out &quot;the best boost&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;My dataset contains 1515 rows, 1010 are training and 505 are test data.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;intd &amp;lt;- c(5,6,7,8,9) # interaction.depth parameters&#10;nt &amp;lt;- seq(1:10)*500  # number of trees&#10;shrink &amp;lt;- seq(from = 0.001, to = 0.010, by = 0.001) # shrinkage factor&#10;   opt &amp;lt;- cbind(rep(nt, each=50),rep(shrink,each=5,times=10),rep(intd,times=100))&#10;&#10;m &amp;lt;- matrix(0,nrow=500,ncol=505) # build empty matrix to fill with predictions&#10;for (i in 1:500)   {&#10;m[i,] &amp;lt;- predict((gbm(branch2, data=df[train,],distribution=&quot;gaussian&quot;,n.trees=opt[i,1],interaction.depth=opt[i,3], shrinkage = opt[i,2])), newdata=df[-train,],n.trees=opt[i,1])&#10; cat(i,&quot; &quot;)        &#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So the question is, what is a sensible way to graphically represent the results? I'm guessing calculating the MSE will be necessary but then given there are three parameters I'm not sure how to best visualise it&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;boost.err=with(df[-train,],apply( (t(opt)-v10)^2,2,mean))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;df&lt;/code&gt; contains all my data and &lt;code&gt;v10&lt;/code&gt; is my response variable&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-12T14:14:11.260" Id="103139" LastActivityDate="2014-06-12T14:14:11.260" OwnerUserId="42884" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;data-visualization&gt;&lt;ggplot2&gt;" Title="Plot triple optimization boosting procedure" ViewCount="19" />
  <row AnswerCount="0" Body="&lt;p&gt;In my dataset, I have 2 labels, positive and negative. Most samples belong to only one class, either positive or negative. A small fraction of samples take both labels i.e. both positive and negative. Thus, making it a &lt;a href=&quot;http://en.wikipedia.org/wiki/Multi-label_classification&quot; rel=&quot;nofollow&quot;&gt;multi-label&lt;/a&gt; classification problem. Usually, this could be solved in many different ways including &lt;a href=&quot;http://www.clei.cl/cleiej/papers/v14i1p4.pdf&quot; rel=&quot;nofollow&quot;&gt;problem transformation&lt;/a&gt; or using multi-label classifiers. My question is, &lt;strong&gt;can I consider the samples that take both labels as a third class and consider this as a &lt;em&gt;multi-class classification problem with 3 classes&lt;/em&gt;?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;As far as I understand, this method is not preferred in multi-label classification scenarios where the no. of individual labels is large. That is, if $n$ is the no. individual labels, then a sample could take any of the $2^n-1$ combination of labels. Hence for large $n$, opting for a multi-class classification (with $2^n-1$ classes) could be a bad idea. But, in my case $n=2$, which is a trivial case. Hence, I am considering multi-class classification as an option. Please advise on this and let me know if I am right. Also, if possible please point to literature where such trivial case of multi-label classification problems are dealt with. &lt;/p&gt;&#10;" ClosedDate="2014-12-03T16:16:37.233" CommentCount="0" CreationDate="2014-06-12T15:45:24.573" FavoriteCount="1" Id="103154" LastActivityDate="2014-06-12T16:15:00.370" LastEditDate="2014-06-12T16:15:00.370" LastEditorUserId="40917" OwnerUserId="40917" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;classification&gt;&lt;data-mining&gt;&lt;multi-class&gt;&lt;multilabel&gt;" Title="What should I use - Multi label classification or Multi class classification?" ViewCount="98" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;For every year, for eight years, I have a set of projects (n=332). The number of projects that exist in each year varies (for example, some years there are 10 projects others there are 60).   In each year, projects are either approved or not approved based on a score.  I want to compare the means for approved and not approved projects for each year as well as compare the scores for approved and not approved projects over time.  A repeated measures ANOVA with contrasts lets me do this but because the projects are not the same every year I believe this is not the right test.  Thoughts? &lt;/p&gt;&#10;&#10;&lt;p&gt;Also, the scores used to approve or not approve a project is a sum of several sub-scores.  I want to compare the standard deviations about the project score for both approved and not approved project and over time.   &lt;/p&gt;&#10;&#10;&lt;p&gt;PS a mixed model, projects within years in STATA won't converge  &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-06-12T17:27:25.753" Id="103167" LastActivityDate="2014-06-12T17:52:33.437" LastEditDate="2014-06-12T17:52:33.437" LastEditorUserId="7290" OwnerUserId="48274" PostTypeId="1" Score="0" Tags="&lt;anova&gt;&lt;mixed-model&gt;&lt;repeated-measures&gt;&lt;stata&gt;" Title="repeated measures ANOVA - non-experimental data" ViewCount="50" />
  
  <row Body="&lt;p&gt;I'm sure that you would have thought of this already if this were the case, but if the test set has actual values and you're using the test set for cross validation purposes, then re-splitting the dataframe into training and test datamframes where the two are balanced on these factors would avoid your problem.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-12T17:49:33.483" Id="103174" LastActivityDate="2014-06-12T17:49:33.483" OwnerUserId="26356" ParentId="29446" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;$Q$ is &quot;wrong&quot; distribution, while $P$ is the &quot;right&quot; distribution. The length of a code $i$ under the &quot;wrong&quot; coding is $-log(q_{i})$. So, the average message length under the &quot;wrong&quot; coding is:&lt;/p&gt;&#10;&#10;&lt;p&gt;$-\sum p_{i} log(q_{i}) = H(P,Q)$&lt;/p&gt;&#10;&#10;&lt;p&gt;Under the &quot;right&quot; coding, the average message length is $H(P)$. Imagine that we are using entropy coding like arithmetic coding, but we have not estimated the probability distribution right. Then the average message length is a bit larger then the theoretical limit $H(P)$. The difference is Kullback–Leibler divergence.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-06-12T19:50:35.220" Id="103186" LastActivityDate="2014-06-12T19:50:35.220" OwnerUserId="48284" ParentId="103175" PostTypeId="2" Score="0" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am currently planning a survey. The data will be used to test which of three metrics best captures some &quot;true&quot; underlying characteristic of the respondents. Getting data on the &quot;true&quot; underlying characteristic is costly, so I want a simplified metric that best approximates the &quot;true&quot; measure. &lt;/p&gt;&#10;&#10;&lt;p&gt;For the current survey, I am gathering four data points for each observation.&lt;/p&gt;&#10;&#10;&lt;p&gt;1) metric 1 &lt;/p&gt;&#10;&#10;&lt;p&gt;2) metric 2&lt;/p&gt;&#10;&#10;&lt;p&gt;3) metric 3&lt;/p&gt;&#10;&#10;&lt;p&gt;4) the &quot;true&quot; underlying characteristic&lt;/p&gt;&#10;&#10;&lt;p&gt;In the future, I won't have data on the &quot;true&quot; characteristic, so I will rely on one metric. For future work we will collect just the data for the &quot;best&quot; metric.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am not clear about what effect size (aka difference in prevalence of indicator) to use to calculate sample size. Specifically, the hypothesized effect that I seek to identify, I believe is the difference between the metrics in their correlation to the true measure. For instance, if metric 1 has correlation 0.7 with the true characteristic, and metric 2 has a correlation 0.9, then the &quot;effect size&quot; of switching from metric 1 to metric 2 is 0.2.&lt;/p&gt;&#10;&#10;&lt;p&gt;Am I on track?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-13T09:25:03.303" Id="103238" LastActivityDate="2014-06-13T10:50:39.203" LastEditDate="2014-06-13T10:50:39.203" LastEditorUserId="22088" OwnerUserId="22088" PostTypeId="1" Score="1" Tags="&lt;sampling&gt;&lt;sample-size&gt;&lt;survey&gt;&lt;power-analysis&gt;" Title="Calculating power for a survey to compare different metrics" ViewCount="31" />
  
  <row Body="&lt;p&gt;The median is the point at which 1/2 the observations fall below and 1/2 above. Similarly, the 25th perecentile is the median for data between the min and the median, and the 75th percentile is the median between the median and the max, so yes, I think you're on solid ground applying whatever median algorithm you use first on the entire data set to partition it, and then on the two resulting pieces.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://stackoverflow.com/questions/10657503/find-running-median-from-a-stream-of-integers&quot;&gt;This question&lt;/a&gt; on stackoverflow leads to this paper: &lt;a href=&quot;http://www.cs.wustl.edu/~jain/papers/ftp/psqr.pdf&quot; rel=&quot;nofollow&quot;&gt;Raj Jain, Imrich Chlamtac: The P² Algorithm for Dynamic Calculation of Quantiiles and Histograms Without Storing Observations. Commun. ACM 28(10): 1076-1085 (1985)&lt;/a&gt; whose abstract indicates it's probably of great interest to you:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;A heuristic algorithm is proposed for dynamic calculation qf the&#10;  median and other quantiles. The estimates are produced dynamically as&#10;  the observations are generated. The observations are not stored;&#10;  therefore, the algorithm has a very small and fixed storage&#10;  requirement regardless of the number of observations. This makes it&#10;  ideal for implementing in a quantile chip that can be used in&#10;  industrial controllers and recorders. The algorithm is further&#10;  extended to histogram plotting. The accuracy of the algorithm is&#10;  analyzed.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="4" CreationDate="2014-06-13T14:11:07.590" Id="103266" LastActivityDate="2014-06-13T15:36:37.947" LastEditDate="2014-06-13T15:36:37.947" LastEditorUserId="29617" OwnerUserId="29617" ParentId="103258" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Just to note, that if you get a huge run of heads or tails in a row, you may be better off &lt;a href=&quot;https://en.wikipedia.org/wiki/Bayesian_probability&quot; rel=&quot;nofollow&quot;&gt;revisiting your prior assumption&lt;/a&gt; assumption that the coin was fair. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-13T14:16:21.607" Id="103267" LastActivityDate="2014-06-13T14:16:21.607" OwnerUserId="29617" ParentId="101590" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Assuming that the sample variance is also $4$ (since I believe we need to the sample variance to solve this, please correct me if I'm wrong), the standard error of the sample mean would be $2/\sqrt{400}=0.1$. Since the population mean is $4$, using a t-test we get $t=(4.5-4)/0.1=5$, which is well within standard ranges of rejection values of the null hypothesis (that the sample mean is equal to the population mean).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-13T14:41:02.757" Id="103271" LastActivityDate="2014-06-13T14:41:02.757" OwnerUserId="48257" ParentId="99690" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;As far as I can tell, Bayesian Networks do not claim to be able to estimate causal effects in non-directed acyclic graphs, whereas SEM does. That's a generalization in favor of SEM... if you believe it.&lt;/p&gt;&#10;&#10;&lt;p&gt;An example of this might be measuring cognitive decline among people where cognition is a latent effect estimated using a survey instrument like 3MSE, but some people may decreased cognition as a function of pain meds usage. Their pain meds may have been a consequence of injuring themselves due to cognitive decline (falling for example). And so, in a cross sectional analysis, you would see a graph that has a circular shape. SEM analysts like to tackle problems like that. I steer clear.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the Bayes network world, you have very general methods of assessing conditional independence/dependence of nodes. One can use a fully parametric approach with any number of distributions, or go about the Bayesian nonparametric approaches I've heard about. SEM estimated using ML are (usually) assumed to be normal, which means that conditional independence is equivalent to having zero covariance for 2 nodes in the graph. I personally believe that's a rather strong assumption and would have very little robustness to model misspecification.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-06-13T15:16:24.487" Id="103278" LastActivityDate="2014-06-13T15:16:24.487" OwnerUserId="8013" ParentId="103183" PostTypeId="2" Score="4" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm attempting a project where I need to statistically rank available cars based on several variables such as cost, mpg, seating, milage, etc.. I wish to rank these cars in order decide which car would be the best choice (highest &quot;worth&quot;) to buy (or best several cars if I was informing multiple people of the best cars to get). As the list of available cars changes from day to day, I will also need to re-run the code every day to allow the rankings to give me the best decision for this new day. &lt;/p&gt;&#10;&#10;&lt;p&gt;What statistical methods should I use to go about this ranking system? I plan on determining which factors I find most important so the variables used will be subjective in choice. I thought about trying MDS or clustering but I didn't know if that would be relevant since I'm already subjectively determining what variables are to be used. I don't see how regression can be used since I can't get a handle on the &quot;worth&quot; of previous cars as that is what I'm trying to rank by. Also, I will be attempting this in R so any helpful packages/functions would be great to know as well.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any help with how to go about this ranking scheme would be helpful as I'm at a loss.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks so much&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-06-13T15:37:35.077" Id="103280" LastActivityDate="2014-06-13T15:37:35.077" OwnerUserId="48288" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;regression&gt;&lt;forecasting&gt;&lt;computational-statistics&gt;" Title="What method(s) should I use to achive statistical ranking?" ViewCount="33" />
  
  <row Body="Situations where a level of a categorical variable occurs very rarely (eg, a rare disease). This can be a problem especially when the variable is the response variable in a model." CommentCount="0" CreationDate="2014-06-13T16:23:45.600" Id="103290" LastActivityDate="2014-06-13T16:23:45.600" LastEditDate="2014-06-13T16:23:45.600" LastEditorUserId="7290" OwnerUserId="7290" PostTypeId="4" Score="0" />
  <row AcceptedAnswerId="103294" AnswerCount="1" Body="&lt;p&gt;I have a large data set (&gt;1000 obs) and i'm performing regressions tests, both linear and logistic, on a series of clinical outcomes.&lt;/p&gt;&#10;&#10;&lt;p&gt;In this test I verify the effect of interactions between two cat variables by creating a dummy interaction variable with the two vars joined in one (with R is done using the interaction() fuction). In the equation are present also other vars for adjustment.&lt;/p&gt;&#10;&#10;&lt;p&gt;After every test I perform a post-hoc to verify if there is difference between every level of this interaction variable. The test is performed via glht() of the multcomp package in R. I believe, (i'm not 100% sure) that the post-hoc is performed using Tukey methodology.&lt;/p&gt;&#10;&#10;&lt;p&gt;Of course applying the post-hoc procedure I loose significance. Often appears that results that were significant in the regression test become not significant. It's ok. I always think that p&amp;lt;0.05 it's a very shallow threshold, especially when you have large numbers like I have. Some times it even happens that some relevant effects between levels that were not show in the regression become visibile in the post hoc and this is a good thing.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is therefore, when you perform a post hoc of a regression analysis, using Tukey, and with these large numbers, it's still easier to make a type 1 error and accept no real results, or a type 2 error, being too hard on data and loosing interesting results?&lt;/p&gt;&#10;&#10;&lt;p&gt;Or it's impossible to know? (probably this is the correct answer)&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-06-13T16:28:09.047" Id="103292" LastActivityDate="2014-06-13T16:41:33.253" OwnerUserId="6479" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;post-hoc&gt;&lt;type-i-errors&gt;&lt;type-ii-errors&gt;" Title="post-hoc test after logistic regression with interaction. Risk higher for type 1 or type 2 error?" ViewCount="204" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;One can prove it by a straightforward computation that the sample variance of two samples combined is&#10;$\sigma^2=\frac{N_1-1}{N-1}\sigma_1^2+\frac{N_1}{N-1}\mu_1^2+ \frac{N_2-1}{N-1}\sigma_2^2+\frac{N_2}{N-1}\mu_2^2-&#10;\frac{N}{N-1}{\mu^2},$&#10;where $N=N_1+N_2.$&#10;I need to add a reference to this formula in my paper and have a difficulty finding it in a published book. Would anybody know a good reference to it?&lt;/p&gt;&#10;" ClosedDate="2015-01-09T17:18:46.140" CommentCount="1" CreationDate="2014-06-13T17:55:15.403" Id="103308" LastActivityDate="2014-06-13T17:55:15.403" OwnerUserId="48342" PostTypeId="1" Score="0" Tags="&lt;mathematical-statistics&gt;&lt;summary-statistics&gt;" Title="Reference to the formula for sample variance of combined samples" ViewCount="94" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Say I have multiple normal or beta distributions.&lt;/p&gt;&#10;&#10;&lt;p&gt;So I have two questions.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;How do I Draw a random sample from a distribution?&lt;/li&gt;&#10;&lt;li&gt;How do I compare determine which of my distributions has the largest draw sample?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;More Info:&lt;/p&gt;&#10;&#10;&lt;p&gt;Im trying to decipher this &lt;a href=&quot;http://engineering.richrelevance.com/recommendations-thompson-sampling/&quot; rel=&quot;nofollow&quot;&gt;blog&lt;/a&gt; on Thompson sampling and Multi armed bandits.  In it the author describes each arm as having a Beta Distribution.  How do I run a random sample on each distribution and what is meant by largest drawn sample.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-06-13T22:39:32.787" Id="103337" LastActivityDate="2014-06-13T23:43:48.223" LastEditDate="2014-06-13T23:43:48.223" LastEditorUserId="26338" OwnerUserId="46201" PostTypeId="1" Score="0" Tags="&lt;distributions&gt;&lt;normal-distribution&gt;&lt;sampling&gt;&lt;beta-distribution&gt;&lt;multiarmed-bandit&gt;" Title="Drawing a random Sample on a Probability Distribution" ViewCount="65" />
  <row AnswerCount="1" Body="&lt;p&gt;I have a question about SAS and R. For a research, I used a longitudinal data and I initially used SAS (&lt;code&gt;GLIMMIX&lt;/code&gt;) and then I analyzed the data with R (&lt;code&gt;glmer&lt;/code&gt;) programming. There are differences between p-values of SAS and R. I expected that regression coefficient and standard error could be different for R and SAS. But there are differences for p value for some variables, which are significant in R, are not significant in SAS. &lt;/p&gt;&#10;&#10;&lt;p&gt;My R model and SAS model are respectively :&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#R&#10;m3.glmm &amp;lt;- glmer(y ~ timebefore + timeafter + x1 + x2 +...+ x11 +      &#10;                     (1+timebefore+timeafter|id), &#10;                 data=data, family=binomial(link=&quot;logit&quot;), nAGQ=3)&#10;&#10;#SAS&#10;proc glimmix data=data METHOD=QUAD(QPOINTS=3) NOCLPRINT ;&#10;  class id x2 x3 x4 x5;&#10;  model y(event='1')=timebefore timeafter x1 x2 x3 x4 x5 &#10;        x6 x7  x8 x9 x10 x11 /solution CL link = logit dist = binary;&#10;  random intercept timebefore timeafter/subject = id GCORR SOLUTION;&#10;run;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Eg: variable &quot;x1&quot;(defined as age) was significant (p val= 0.04) in SAS but not in R (p val=0.1). But others were similar. It means that significant variables in SAS are found significant in R, or insignificant variables in SAS are insignificant in R. &lt;/p&gt;&#10;&#10;&lt;p&gt;Does anybody know about the differences?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-06-14T00:58:10.053" FavoriteCount="1" Id="103340" LastActivityDate="2014-06-14T19:01:23.733" LastEditDate="2014-06-14T17:36:45.990" LastEditorUserId="7290" OwnerUserId="45836" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;p-value&gt;&lt;sas&gt;&lt;panel-data&gt;&lt;regression-coefficients&gt;" Title="Regression coefficients with longitudinal data yields different results in R and SAS" ViewCount="184" />
  <row AnswerCount="1" Body="&lt;p&gt;I wonder if it is fair is to compare the gini coefficient of two discrete distributions with different number of elements. If not, how can I adjust the coefficients for a fair comparison. &lt;/p&gt;&#10;&#10;&lt;p&gt;In particular, I'm interested in the case when the sum of values in both distributions are equal. For example, minutes played by players of two teams in a basketball game if they have different number of players (10 players in one team, 12 players in the other team) but for both teams sums plays 48 minutes x 5. &lt;/p&gt;&#10;&#10;&lt;p&gt;Other example, different partitioning of some surface in different regions, how to compare inequality for different partitioning if they have different number of subareas. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-14T03:33:09.207" Id="103348" LastActivityDate="2014-06-14T13:09:00.113" LastEditDate="2014-06-14T13:09:00.113" LastEditorUserId="26338" OwnerUserId="48358" PostTypeId="1" Score="1" Tags="&lt;distributions&gt;&lt;gini&gt;" Title="Comparing Gini coefficient of 2 distributions" ViewCount="45" />
  
  <row Body="&lt;h3&gt;A counterexample to your generalized claim&lt;/h3&gt;&#10;&#10;&lt;p&gt;Consider tossing fair coin once, and define the events as $A = \textrm{heads}$, $B = \textrm{tails}$. Now, $P(A)=0.5=P(B)$, so the assumption holds. However, here $A = \overline{B}$, i.e. the events are mutually exclusive and their union is the sample space $S$. Therefore,&#10;\begin{equation}&#10;P(A\overline{B} + \overline{A}B) = P(A + B) = 1.&#10;\end{equation}&#10;That is, almost certainly either the result of the toss is heads and not tails or the result is tails and not heads. Based on this counterexample, we know that your generalized result is false and may proceed to locating the error in the argument. &lt;/p&gt;&#10;&#10;&lt;p&gt;The error is in Eq. (6) as pointed out by @Stat in the comments.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-14T06:49:19.410" Id="103354" LastActivityDate="2014-06-14T06:49:19.410" OwnerUserId="24669" ParentId="103347" PostTypeId="2" Score="0" />
  <row AnswerCount="1" Body="&lt;p&gt;I have a dataset where I extract the mean and the standard deviation, I want to generate a new synthetic dataset which is lognormal distributed based on the original dataset paramaters. Like a montecarlo simulation.&lt;/p&gt;&#10;&#10;&lt;p&gt;My problem is that I need that synthetic dataset is truncated, with the maximum and minimum value of the orignal set, in other words I don't want any synthetic value greater than the maximum value and minimum value of the original datset.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks @whuber, following your guidelines it works perfectly with the positive side of my original dataset, long right tail distribution within the max and min limits.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/63ddE.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="10" CreationDate="2014-06-14T07:30:25.050" FavoriteCount="1" Id="103356" LastActivityDate="2014-06-19T14:13:55.533" LastEditDate="2014-06-19T14:13:55.533" LastEditorUserId="48366" OwnerUserId="48366" PostTypeId="1" Score="3" Tags="&lt;distributions&gt;&lt;simulation&gt;&lt;excel&gt;" Title="Truncate lognormal distribution with excel" ViewCount="1072" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to build GAM model to see the effect of several environmental variables on the total abundance of one species. I have collected samples from three sites with three replicates from each site. How is the best for me to see the effect of each variable on the model after removing the variation cause by the three sites and considering the three replicates as random effect. My try is the following :&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;m01&amp;lt;- gam(TotalInd ~ s(dayinyear, bs=&quot;cr&quot;) + Site2 + s(Temp) + s(Mud) + s(Chlorophyll) + s(Salinity) ,random=list(Replicate2=~ 1), data=Data)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Do you think the format of this model is correct?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-14T09:06:25.303" Id="103366" LastActivityDate="2014-06-14T09:06:25.303" OwnerUserId="48325" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;mixed-model&gt;&lt;mixed-effect&gt;&lt;gam&gt;" Title="Site effect in GAM model" ViewCount="18" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I try to study differences in health care cost between counties over a several years period, using a fixed effects model. I use a cost measure as dependent variable and have some demographic, geographical, social and size-related factors as explanatory variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;What are some of the factors you think could be contained in the unobserved effect, $a_i$?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-06-14T12:48:52.147" Id="103380" LastActivityDate="2014-06-14T12:58:41.643" LastEditDate="2014-06-14T12:51:43.663" LastEditorUserId="26338" OwnerUserId="48372" PostTypeId="1" Score="3" Tags="&lt;panel-data&gt;&lt;fixed-effects-model&gt;" Title="Factors contained in the unobserved effect?" ViewCount="39" />
  
  <row Body="&lt;p&gt;My personal answer to the question as asked is &quot;yes&quot;. You may view it as a pro or a con that there are an infinite number of choices of features to describe the past.Try to pick features that correspond to how you might concisely describe to someone what the market has just done [eg &quot;the price is at 1.4&quot; tells you nothing if it is not related to some other number]. As for the target of the SVM, the simplest are the difference in prices and the ratio of prices for two consecutive days. As these correspond directly to the fate of a hypothetical trade, they seem good choices.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have to pedantically disagree with the first statement by Jason: you can do k-fold cross-validation in situations like that described by raconteur and it is useful (with a proviso I will explain). The reason it is statistically valid is that the instances of the target in this case have no intrinsic relationship: they are disjoint differences or ratios. If you choose instead to use data at higher resolution than the scale of the target, there would be reason for concern that correlated instances might appear in the training set and validation set, which would compromise the cross-validation (by contrast, when applying the SVM you will have no instances available whose targets overlap the one you are interested in).  &lt;/p&gt;&#10;&#10;&lt;p&gt;The thing that does reduce the effectiveness of cross-validation is if the behavior of the market is changing over time. There are two possible ways to deal with this. The first is to incorporate time as a feature (I've not found this very useful, perhaps because the values of this feature in the future are all new). A well-motivated alternative is to use walk-forward validation (which means testing your methodology on a sliding window of time, and testing it on the period just after this window. If behaviour is changing over time, the saying attributed to Niels Bohr &quot;Prediction is very difficult, especially about the future&quot; is especially appropriate. There is some evidence in the literature that the behaviour of financial markets does change over time, generally becoming more efficient, which typically means that successful trading systems deteriorate in performance over time.&lt;/p&gt;&#10;&#10;&lt;p&gt;Good luck!&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-06-14T15:44:53.730" Id="103397" LastActivityDate="2014-06-14T18:09:00.920" LastEditDate="2014-06-14T18:09:00.920" LastEditorUserId="21296" OwnerUserId="21296" ParentId="14482" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;The gbm package in R documents the following deviance formula for CoxPH models&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://gradientboostedmodels.googlecode.com/git/gbm/inst/doc/gbm.pdf&quot; rel=&quot;nofollow&quot;&gt;http://gradientboostedmodels.googlecode.com/git/gbm/inst/doc/gbm.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;$-2 \sum w_i \delta_i (f(x_i) - \log(R_i / w_i)$, where $w_i$ is a weight for the observation, $\delta_i$ is the right censor indicator and $R_i = \sum_{j=1}^N 1(t_j &amp;gt;= t_i) e^{f(x_i)}$ is the total hazard of the risk set&lt;/p&gt;&#10;&#10;&lt;p&gt;However this formula does not seem to agree with the author's implementation.  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;https://github.com/harrysouthworth/gbm/issues/22&quot; rel=&quot;nofollow&quot;&gt;https://github.com/harrysouthworth/gbm/issues/22&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm wondering how this was derived so I can figure out if there is a bug in the implementation or if there is a mistake with the way the formula was written&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-15T05:47:48.677" Id="103425" LastActivityDate="2014-06-20T20:58:49.917" LastEditDate="2014-06-20T20:58:49.917" LastEditorUserId="17230" OwnerUserId="8067" PostTypeId="1" Score="1" Tags="&lt;survival&gt;&lt;cox-model&gt;&lt;deviance&gt;" Title="How was this deviance formula for Cox Proportional Hazard derived?" ViewCount="45" />
  
  <row AcceptedAnswerId="103457" AnswerCount="2" Body="&lt;p&gt;I'm getting confused about what I read about covariance. I know how to calculate covariance between two different variables, but not between two observations of the same variable. Imagine you have many observations of a variable $X$. What does this mean and how to calculate it?  &lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;{\rm Cov}[x_i,x_j]&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Or, which seems the same problem to me, in a simple linear regression when it is said that  &lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;{\rm Cov}[\varepsilon_i,\varepsilon_j] = 0&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;for $i \neq j$. How do you calculate this?  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-15T10:29:27.337" Id="103439" LastActivityDate="2014-06-15T16:47:59.850" LastEditDate="2014-06-15T14:54:29.733" LastEditorUserId="7290" OwnerUserId="48410" PostTypeId="1" Score="4" Tags="&lt;covariance&gt;" Title="How to calculate the covariance between two observations of the same variable?" ViewCount="230" />
  <row AnswerCount="0" Body="&lt;p&gt;I want to use Relevance Vector Machines and need to define my custom made kernel.&#10;I was wondering if it is allowed for the kernel to depend on the full dataset. For exampe, I can calculate a certain weight vector from all the data and call this vector $w$ and then use the kernel K(x1,x2|w) (i.e. $w$ is a vector of parameters for the kernel). Are there any potential problems with this idea?&lt;/p&gt;&#10;&#10;&lt;p&gt;Mbk &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-15T12:59:17.007" Id="103449" LastActivityDate="2014-06-15T12:59:17.007" OwnerUserId="20227" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;svm&gt;&lt;kernel&gt;" Title="In RVM, is the kernel allowed to depend on the full dataset?" ViewCount="10" />
  <row AnswerCount="0" Body="&lt;p&gt;This might be a better fit in the Mathematics or Philosophy StackExchanges, as I think I'm fundamentally facing a problem of logic, but I'll throw it out here first to avoid cross-posting. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have two surveys - Survey A and Survey B. I want to validate Survey A's findings against Survey B's.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have one indicator: average meals eaten per day in the household.&lt;/p&gt;&#10;&#10;&lt;p&gt;Survey A divides the question into two sub-questions:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Daily average household meals &lt;/li&gt;&#10;&lt;li&gt;Daily average meals for children (aged 6 months to 5 years)&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Now, (1) could be a superset of (2), but not necessarily. Kids might eat with their parents, and thus as much as their parents. Kids might eat less than their parents. Kids might eat more - say, if parents give up a meal in order to feed their kid. Some households might not have kids. So I imagine it as a Venn diagram, where the little &quot;kids&quot; bubble overlaps (a lot) with the big &quot;household&quot; bubble, but might not be completely subsumed by it.&lt;/p&gt;&#10;&#10;&lt;p&gt;Survey B, instead, divides this into three sub-questions:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Daily average meals for adults&lt;/li&gt;&#10;&lt;li&gt;Daily average meals for children (aged 6 months to 5 years)&lt;/li&gt;&#10;&lt;li&gt;Daily average meals for children (aged 5 years to 13 years)&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Here, it seems cleaner. These are three distinct groups (ignoring, for now, the fact that both (2) and (3) could capture 5 year olds). &lt;/p&gt;&#10;&#10;&lt;p&gt;I'd like to run t-tests on the survey means of, say, daily average meals for the household, and daily average meals for children (6 months - 5 years). But I'm not sure how I can combine and disentangle these different sets from each other to make them comparable. &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-06-15T12:59:30.630" Id="103450" LastActivityDate="2014-06-15T12:59:30.630" OwnerUserId="46721" PostTypeId="1" Score="1" Tags="&lt;survey&gt;" Title="Running a t-test on different (sub?)sets [survey logic]" ViewCount="17" />
  <row AcceptedAnswerId="103492" AnswerCount="1" Body="&lt;p&gt;Suppose a random intercept model is to be fitted, like:&#10;$$y_{ij}=\beta_0 + \beta_1x_{1ij} + \beta_2x_{2ij} + \beta_3x_{3ij}+ u_{0j} + \epsilon_{ij}$$&#10;where $x_{1ij}$ and $x_{2ij}$ are continuous variables, $x_{3ij}$ can only assume values of 1 and 0, while $u_{0j}$, $\epsilon_{ij}$ are mutually independent and normal.&lt;/p&gt;&#10;&#10;&lt;p&gt;In a paper I saw such a model being fit using the following two commands in Stata:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;xtmixed y x1 x2 x3 || groupid:, mle&#10;xtmixed y x1 x2 x3 || groupid:, mle, residuals(independent, by(x3))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;What does the second command add compared to the first? The estimated coefficients are different amongst the models, and there is some extra output for the random effect parameters.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-15T18:33:16.500" Id="103476" LastActivityDate="2014-06-16T09:13:42.970" LastEditDate="2014-06-16T09:13:42.970" LastEditorUserId="27581" OwnerUserId="27581" PostTypeId="1" Score="0" Tags="&lt;stata&gt;&lt;random-effects-model&gt;" Title="Residual specification for xtmixed" ViewCount="86" />
  <row Body="&lt;p&gt;Number of cigarettes smoked in a period of time: this requires a zero-inflated process (e.g. zero-inflated Poisson or zero-inflated negative binomial) because not everyone smokes cigarettes.&lt;/p&gt;&#10;" CommentCount="3" CommunityOwnedDate="2014-06-16T18:42:37.393" CreationDate="2014-06-15T18:47:58.010" Id="103477" LastActivityDate="2014-06-15T21:12:34.970" LastEditDate="2014-06-15T21:12:34.970" LastEditorUserId="44269" OwnerUserId="44269" ParentId="37589" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="103484" AnswerCount="1" Body="&lt;p&gt;Instead of perusing through many more textbooks, I thought it's easier to ask here. Say I would like to express the mean for a variable with a certainty of 95 percent, how exactly do (&quot;should&quot;) I do this?&lt;/p&gt;&#10;&#10;&lt;p&gt;Maybe like:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;$x_{P = 95\%} = 373.334$ mm $\pm 8.82$ or&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;$x (P = 95\%) = 373.334$ mm $\pm 8.82$&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I know I could always write &lt;/p&gt;&#10;&#10;&lt;p&gt;$x = 373.334$ mm $\pm 8.82$ with a probability of \SI{95}{\percent} but I'd prefer a more compact way.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2014-06-15T19:31:55.240" Id="103483" LastActivityDate="2014-06-16T20:03:27.267" LastEditDate="2014-06-16T20:03:27.267" LastEditorUserId="29265" OwnerUserId="29265" PostTypeId="1" Score="1" Tags="&lt;notation&gt;" Title="Correct way to write values which have a certainty of 95 percent?" ViewCount="62" />
  
  <row Body="&lt;p&gt;While I second Hastie et al.s ESL. They do assume some statistical background. Like the OP, I had absolutely no background in statistics and that made some parts of the their discussion on SVMs difficult (especially the bit on tying it with some sort of regularized logistic regression).&lt;/p&gt;&#10;&#10;&lt;p&gt;A much simpler option is James et als. &lt;a href=&quot;http://www-bcf.usc.edu/~gareth/ISL/&quot; rel=&quot;nofollow&quot;&gt;Introduction to Statistical Learning&lt;/a&gt;. This is a simplified version of ESL. The best thing about the book, however, is that it is a very good and rapid introduction to R.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have found the relevant lecture notes in the following MIT-OCW a good launching pad to understanding SVMs: &lt;a href=&quot;http://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/&quot; rel=&quot;nofollow&quot;&gt;Prediction: Machine Learning and Statistics.&lt;/a&gt; Though, unforunately, this too assumes some basic statistical knowledge. It also assumes some mathematics background. &lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2014-06-15T21:33:54.523" CreationDate="2014-06-15T21:33:54.523" Id="103488" LastActivityDate="2014-06-15T21:33:54.523" OwnerUserId="46597" ParentId="90159" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;I'm not sure that this is an appropriate question for this forum since it deals only with the use of a particular software package, but the option&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;residuals(independent, by(x3))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;specifies that the residuals are independent, but that their variance may differ according to variable &lt;code&gt;x3&lt;/code&gt;.  IOW, residuals for observations with different values of &lt;code&gt;x3&lt;/code&gt; may have different variance.  This is a way to model heteroskedasticity.  It is explained in detail in the section of the &lt;a href=&quot;http://www.stata.com/manuals13/me.pdf&quot; rel=&quot;nofollow&quot;&gt;User's Manual [ME]&lt;/a&gt; dealing with &lt;code&gt;mixed&lt;/code&gt; (FYI, the command &lt;code&gt;xtmixed&lt;/code&gt; was renamed &lt;code&gt;mixed&lt;/code&gt; in Stata 13).&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-06-15T22:21:16.990" Id="103492" LastActivityDate="2014-06-16T03:10:36.687" LastEditDate="2014-06-16T03:10:36.687" LastEditorUserId="36755" OwnerUserId="36755" ParentId="103476" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;Consider a classification problem in which the raw data are snippets of a larger 1-D time series signal. In my application, the signal is the response of a motion sensor as a function of time (the raw measurements themselves are voltages).&lt;/p&gt;&#10;&#10;&lt;p&gt;One &quot;sample&quot; for classification purposes will be some subsampled window from the original data. So if the original data is some 1-D array like:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;original_data = [v_{1}, v_{2}, ..., v_{N}]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;then a single &quot;sample&quot; could be anything of the form&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;sample = [v_{s}, v_{s+1}, ..., v_{s+k}]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;for some length &lt;code&gt;k &amp;lt; N-s&lt;/code&gt;. Simply put, the samples are sub-windows of the original time-series... just bootstrapped by picking random contiguous ranges of the original data set, of varying lengths.&lt;/p&gt;&#10;&#10;&lt;p&gt;The goal is to classify a given &lt;code&gt;sample&lt;/code&gt; value as +1 if it contains a certain event and -1 if it does not. I have ground truth data on the ranges of indices that contain events.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am working now on methods to characterize the &lt;code&gt;sample&lt;/code&gt; values -- that is, transforming a given &lt;code&gt;sample&lt;/code&gt; value into a common, fixed set of features organized into a feature vector which can be used for classification.&lt;/p&gt;&#10;&#10;&lt;p&gt;Among the types of features I would like to use, frequency decompositions could be helpful and I am especially interested in the coefficients of a wavelet decomposition of each given &lt;code&gt;sample&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;But the problem is that these concepts are defined in part based on the length of the sample supplied. So the number of wavelet coefficients (much like the number of Fourier coefficients) will be a function of the number of individual time samples (number of indices) contained in one given &lt;code&gt;sample&lt;/code&gt; snippet of the data.&lt;/p&gt;&#10;&#10;&lt;p&gt;But I need for these coefficients to be comparable across snippets of differing lengths. &lt;/p&gt;&#10;&#10;&lt;p&gt;In particular, I don't want to pad my &lt;code&gt;sample&lt;/code&gt; sequences, nor downsample, nor anything else, to make them superficially have a common length for the purposes of the analysis.&lt;/p&gt;&#10;&#10;&lt;p&gt;Interpolating the signals into a finer grid could be possible -- except that I won't know how fine a grid to use. If I pick a fixed size, but then randomly a &lt;code&gt;sample&lt;/code&gt; is chosen that is larger than my fixed size, I would have to truncate or downsample, etc. &lt;/p&gt;&#10;&#10;&lt;p&gt;But it will be expensive to compute the largest (or shortest) sample length ahead of time and interpolate (or downsample) to get every training example to match in length for the purposed of the wavelet part.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, the question is: how to compute a frequency-decomposition-like representation of each variable-length signal window in a stream of such windows &lt;em&gt;without&lt;/em&gt; doing any interpolating, resampling, etc. ... such that the calculated representation is directly comparable between any two of the variable-length signal windows.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-15T22:46:03.163" Id="103496" LastActivityDate="2014-06-16T02:44:10.133" LastEditDate="2014-06-16T02:44:10.133" LastEditorUserId="8927" OwnerUserId="8927" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;linear-algebra&gt;&lt;feature-construction&gt;&lt;wavelet&gt;" Title="Computing directly comparable wavelet features on variable-length training examples" ViewCount="43" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I have an idea to use ECDF conversion of data to their uniform distribution of equal sample sizes.  So, for say 1000 pieces of data, each value should more or less correlate with a .1% representative value, if a value is repeated, that repeated value is translated into another correlated 1/1000%.&lt;/p&gt;&#10;&#10;&lt;p&gt;So if the number 2 appears 2 times out of 1000 entries.&lt;/p&gt;&#10;&#10;&lt;p&gt;The overall % of that value is .2%&lt;/p&gt;&#10;&#10;&lt;p&gt;However, whatever value that was before it, say 1 , that appeared once would be .1%.&lt;/p&gt;&#10;&#10;&lt;p&gt;so 1 - Maps to .1%&#10;and 2 - Maps to .3% (i.e. 2 counts of 2 out of 1000 = .2%, add prior values for cumulative distribution function. aka .1% and .2% = .3%)&lt;/p&gt;&#10;&#10;&lt;p&gt;I also have a formula that flags for skewed distributions by testing the ECDF converted mean of a distribution for a max error of .275 from .5 mean.  If it is, we do a frequency check on the dataset for the median value, and if the frequency is &gt;50% of the values, then we treat the distribution differently.  We don't want the 0% to be valued at a high value.&lt;/p&gt;&#10;&#10;&lt;p&gt;Instead we remove all 0's from the list, convert to ECDF rating.  Then re add back 0's.  That way we normalize the tail end of the skew.&lt;/p&gt;&#10;&#10;&lt;p&gt;So would this work to say compare varying sets of data to each other on an additive scale by normalizing them in this method?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-16T03:12:45.903" Id="103509" LastActivityDate="2014-06-20T03:36:44.710" LastEditDate="2014-06-16T09:59:31.030" LastEditorUserId="603" OwnerUserId="48197" PostTypeId="1" Score="0" Tags="&lt;summary-statistics&gt;" Title="Can I use Empirical Cumulative Distribution Function to derive standardized values of various data sets?" ViewCount="199" />
  <row Body="&lt;p&gt;First of all, it only makes sense to consider a situation in which both teams are equally good. Then, losses and victories are equally probable, so consider the comparison between winning and drawing for illustrative purposes. Then outcomes for which the two teams draw are for instance 0-0, 1-1, 2-2, ..., whereas outcomes for which team 1 wins are for instance 1-0, 2-0, 3-0, 4-0,..., 2-1, 3-1, 4-1,... As you can see, the number of outcomes for draws and the number of winning outcomes is not equal (and hence the probabilities are also unequal). You could formalize this idea by assuming that team $X$ and team $Y$ both score a number of goals in a time period of for instance 90 minutes that is Poisson distributed with a certain parameter $p$. Then you could compute the probabilities that $X-Y&amp;gt;1$ (so that team one wins) and that $X=Y$, so that the teams draw.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-16T08:25:37.210" Id="103514" LastActivityDate="2014-06-16T08:25:37.210" OwnerUserId="31563" ParentId="103512" PostTypeId="2" Score="1" />
  
  
  
  
  
  <row Body="&lt;p&gt;As an property/casualty actuary, I deal with real-life examples of discrete processes which are non-Poisson all the time. For high-severity, low-frequency lines of business, the Poisson distribution is ill-suited as it demands a variance-to-mean ratio of 1. The negative binomial distribution, mentioned above, is much more commonly used, and the Delaporte distributions is used in some of the literature, though less often in standard North American actuarial practice.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Why&lt;/strong&gt; this is so is a deeper question. Is the negative binomial so much better because it represents a Poisson process for which the mean parameter is itself gamma distributed? Or is it because loss occurrences fail independence (as earthquake events do under current theory that the longer one waits for the earth to slip, the more likely it is due to the build-up in pressure), is it non-stationary (the intervals cannot be subdivided into sequences, each of which is stationary, which would allow the use of a non-homogeneous Poisson), and certainly some lines of business allow for simultaneous occurrences (e.g. medical malpractice with multiple doctors covered by the policy).&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2014-06-16T18:42:37.393" CreationDate="2014-06-16T14:50:35.397" Id="103553" LastActivityDate="2014-06-16T14:50:35.397" OwnerUserId="29617" ParentId="37589" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;What sort of model are you fitting?  It sounds to me like you want to fit the regression coefficients as random effects.&lt;/p&gt;&#10;&#10;&lt;p&gt;If that is the case, I'd recommend REML (REstricted Maximum Likelihood) estimation and an F-test (which will probably be an approximate F test, but since there are no Normal distributions outside computers, they are all approximate anyway.)&lt;/p&gt;&#10;&#10;&lt;p&gt;In the event that your computer and software are incapable of fitting mixed models, you could also do a weighted analysis of variance of the regression coefficients classified by your groups.  The weights should be proportional to the inverse of the standard errors of the regression coefficients.&lt;/p&gt;&#10;&#10;&lt;p&gt;With all that said, remember that main effects models can be deceiving when there are non-additive effects among the classification variables.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-06-16T15:55:26.043" Id="103563" LastActivityDate="2014-06-16T15:55:26.043" OwnerUserId="48148" ParentId="103560" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;I used the Breusch-Pagan test for heteroscedasticity, but I have many observations ($\approx 500,\! 000$) and the Breusch-Pagan test uses $nR^2$ as a test statistic where $n$ is the number of observations. But with the Breusch-Pagan test I will always get heteroscedasticity unless my $R^2$ is almost 0. From the residual graphs there seems to be no heteroscedasticity, but according to the Breusch-pagan test there is. So is there any test that is good to use in the case of many observations?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-06-16T16:29:52.193" Id="103569" LastActivityDate="2014-06-16T17:12:41.567" LastEditDate="2014-06-16T16:52:38.287" LastEditorUserId="7290" OwnerUserId="44023" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;multiple-regression&gt;&lt;heteroscedasticity&gt;&lt;large-data&gt;&lt;breusch-pagan&gt;" Title="Testing for heteroscedasticity with many observations" ViewCount="64" />
  
  
  
  
  <row AcceptedAnswerId="103597" AnswerCount="6" Body="&lt;p&gt;Two friends, A and B, are tossing a fair coin together. Each of the first three coin tosses yields a &quot;head&quot;, and they're about to toss it for a fourth time. A says the probability that the next coin toss yields a head is $0.5$. B says that since the probability that four consecutive coin tosses all turn up heads is $.0625$, the probability that the next toss yields a head should be $0.0625$ given the prior three heads.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;I know intuitively that B is wrong and A is right, but can someone provide a mathematical explanation of why? Also, does Bayes' Theorem have any relevance here? &lt;/p&gt;&#10;" CommentCount="9" CreationDate="2014-06-16T18:56:44.183" Id="103592" LastActivityDate="2014-06-17T22:49:42.120" LastEditDate="2014-06-17T22:49:42.120" LastEditorUserId="7290" OwnerUserId="29742" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;bayesian&gt;&lt;intuition&gt;" Title="Explain intuition regarding probability of &quot;next flip&quot; when tossing a fair coin" ViewCount="738" />
  <row AnswerCount="0" Body="&lt;p&gt;A general question I'd like to ask. I have two systems, one has complete data, the other one we found only has 80% of data, the rest 20% is missing for reasons yet to be found. But for short term, I still want to work on those 80% of data for analysis, data mining or whatever. But first I want to figure out if missing 20% of data is significant or not?&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm using the chisq.test in R, I pulled the probability distribution from the complete system which I use as my expected distribution. and use the data from the system that has 20% of missing data to run a goodness of fit test given the p. &lt;/p&gt;&#10;&#10;&lt;p&gt;Does the procedure make sense? Any suggestions?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-06-16T20:25:49.093" Id="103599" LastActivityDate="2014-06-16T20:25:49.093" OwnerUserId="48464" PostTypeId="1" Score="1" Tags="&lt;statistical-significance&gt;&lt;chi-squared&gt;" Title="Chi Square Test - Goodness of fit to test significance of incomplete system" ViewCount="40" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am arguing that I can control error vs. coverage by modifying a certain parameter. After running an experiment with leave-many-out validation I have a set of errors along with the parameter value for each error. I want to visualize how I can trade coverage for accuracy by placing restrictions on my parameter.&lt;/p&gt;&#10;&#10;&lt;p&gt;If I didn't have a parameter to modify I could use a &lt;a href=&quot;http://machinelearning.wustl.edu/mlpapers/paper_files/icml2003_BiB03.pdf&quot; rel=&quot;nofollow&quot;&gt;regression error characteristic curve&lt;/a&gt;. These curves plot the error tolerance on the $x$-axis vs. the fraction of test points within that tolerance on the $y$-axis.&lt;/p&gt;&#10;&#10;&lt;p&gt;Drawing several REC curves using different values of my parameter leads to a plot with many overcrossing lines which is hard to interpret. I've also found some work on &quot;&lt;a href=&quot;http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0040598&quot; rel=&quot;nofollow&quot;&gt;ROC surfaces&lt;/a&gt;&quot; but I don't know how widespread it is.&lt;/p&gt;&#10;&#10;&lt;p&gt;Right now I'm using a scatter of mean error vs. test points retained with points colored by the parameter value:&#10;&lt;img src=&quot;http://i.stack.imgur.com/OUQCp.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;It's clear to me what is going on in my figure (e.g. I can set the parameter to 10^2 and get 80% coverage at mean error 200). However, I still feel like I've run into a very common problem that has standardized solutions. Is there is another visualization technique I should be looking into?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-16T21:29:29.877" Id="103612" LastActivityDate="2014-06-16T21:29:29.877" OwnerUserId="2822" PostTypeId="1" Score="1" Tags="&lt;data-visualization&gt;&lt;roc&gt;" Title="How to visualize the effect of a regression parameter" ViewCount="41" />
  <row AnswerCount="1" Body="&lt;p&gt;I pose a very similar question to &lt;a href=&quot;http://stats.stackexchange.com/questions/26531/do-zero-inflated-continuous-covariates-cause-problems-in-binary-logistic-regre/&quot;&gt;this&lt;/a&gt;, although I felt the advice given does not apply to my particular situation;&lt;/p&gt;&#10;&#10;&lt;p&gt;I am using logistic regression models for an animal habitat occupancy study, and all the predictor variables I am interested in contain &gt;50% zeros (although they have a decent range of values in the higher percentiles). Can this cause bias or influence how I should interpret the estimated coefficients?&lt;/p&gt;&#10;&#10;&lt;p&gt;A 2-stage analysis, as suggested in the linked question, doesn't seem to make sense because all the predictors share this zero-inflated distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for any insights&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT Clarifications suggested by Peter Flom;&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Sample size ~ 500 (300 &quot;0&quot;s, 200 &quot;1&quot;s)&lt;/p&gt;&#10;&#10;&lt;p&gt;There are 5 IV's; a typical five-number summary looks like this;  &lt;/p&gt;&#10;&#10;&lt;p&gt;min= 0.000  lower= 0.000  median=0.000 upper= 0.289 max= 16.887&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, Mean= 0.468, SD= 1.467&lt;/p&gt;&#10;&#10;&lt;p&gt;correlations between the 5 IV's all absolute r &amp;lt; 0.3&lt;/p&gt;&#10;&#10;&lt;p&gt;The IV's are hectares of specific habitat types.  Every sample has &gt;0 hectare(s) for at least 1 of the IV's.&lt;/p&gt;&#10;&#10;&lt;p&gt;An example run of the model in R;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    Call:&#10;    glm(formula = use ~ x.1 + x.2 + x.3 + x.4, family = binomial, &#10;        data = mydata)&#10;&#10;Deviance Residuals: &#10;    Min       1Q   Median       3Q      Max  &#10;-2.2338  -0.9312  -0.8679   1.3231   1.6432  &#10;&#10;Coefficients:&#10;        Estimate Std. Error z value Pr(&amp;gt;|z|)    &#10;(Intercept) -0.78412    0.12814  -6.119 9.41e-10 ***&#10;x.1          0.19866    0.06366   3.121  0.00181 ** &#10;x.2          0.06956    0.02618   2.657  0.00788 ** &#10;x.3          0.05238    0.02265   2.313  0.02074 *  &#10;x.4         -0.09995    0.13814  -0.724  0.46935    &#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1&#10;&#10;(Dispersion parameter for binomial family taken to be 1)&#10;&#10;    Null deviance: 634.10  on 473  degrees of freedom&#10;Residual deviance: 611.18  on 469  degrees of freedom&#10;AIC: 621.18&#10;&#10;Number of Fisher Scoring iterations: 4&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="3" CreationDate="2014-06-16T21:30:52.730" Id="103613" LastActivityDate="2014-06-16T22:09:08.327" LastEditDate="2014-06-16T22:09:08.327" LastEditorUserId="38125" OwnerUserId="38125" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;logistic&gt;&lt;generalized-linear-model&gt;&lt;zero-inflation&gt;" Title="&quot;Zero-inflated continuous covariates&quot;, Can they cause problems in logistic regression?" ViewCount="135" />
  <row AnswerCount="0" Body="&lt;p&gt;In Machine Learning, there are various models. I tried to learn few probabilistic models of Machine Learning. I read the theory, worked on problems and tested my results and could analyze my data according to the need like classification, etc.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am a Computational Linguist and I am trying to post many questions on the models(Naive Bayes and HMM) I learnt and going to learn (CRF,MEM). &lt;/p&gt;&#10;&#10;&lt;p&gt;I was thinking if there are any set of questions that I should be able to answer after learning a model, upon answering whom I may say I know a model? &lt;/p&gt;&#10;&#10;&lt;p&gt;Is there any set of questionairre available in the web or in any text book( preferably with the answers)? &lt;/p&gt;&#10;&#10;&lt;p&gt;If any one of the esteemed members of the group may kindly let me know?&lt;/p&gt;&#10;&#10;&lt;p&gt;Regards,&#10;Subhabrata Banerjee. &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-06-16T21:54:16.263" Id="103616" LastActivityDate="2014-06-16T21:54:16.263" OwnerUserId="2329" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;bayesian&gt;&lt;nlp&gt;" Title="What does it take to learn a Model?" ViewCount="35" />
  <row AcceptedAnswerId="103720" AnswerCount="1" Body="&lt;p&gt;I'm reading Schaum's outline of probability, random var. and random processes.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the second chapter they make it clear that a random variable $X$ is not a variable in the traditional sense but rather a function $X(\zeta)$ where $\zeta$ is a sample point in sample space $S$. So $S$ is the domain of r.v. $X$ and collection of all numbers $X(\zeta)$ is the range of r.v. $X$ &lt;/p&gt;&#10;&#10;&lt;p&gt;I'm trying to relate this to distribution of features used on audio. Take MFCCs for example. Let's assume we have two different audio signatures each having a range of MFCC values, so we could have a MFCC distribution for each of the two classes.&lt;/p&gt;&#10;&#10;&lt;p&gt;If we define the numerical value of r.v. as the values taken by MFCCs, it's clean that range of r.v. $X$ is from $-\infty$ to $+\infty$ (in N dimensions, N = dimensions of MFCC), but what is the sample space here? And what is the $\zeta$? i.e. If $X(\zeta)$ = MFCC_n, what is this $\zeta$ we are referring to? &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-06-17T01:20:56.180" Id="103630" LastActivityDate="2014-06-17T19:00:54.137" OwnerUserId="13107" PostTypeId="1" Score="2" Tags="&lt;probability&gt;&lt;random-variable&gt;&lt;notation&gt;" Title="Random variable as a function" ViewCount="66" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am working in a BPO with lots of projects coming in. I would like to know whether is there any book (statistical) through which I can tell whether the incoming project is viable and the company should take it.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-06-17T08:45:21.527" Id="103652" LastActivityDate="2014-06-17T08:45:21.527" OwnerUserId="48107" PostTypeId="1" Score="0" Tags="&lt;project-management&gt;" Title="Whether project is feasible?" ViewCount="15" />
  <row Body="&lt;p&gt;Are x and y vectors of random variables? If so you can define the  positive-semidefinite symmetric correlation matrix:&lt;/p&gt;&#10;&#10;&lt;p&gt;$(\textrm{diag}(\mathbf{\Sigma}^{-\frac{1}{2}})) \mathbf{\Sigma} (\textrm{diag}(\mathbf{\Sigma}^{-\frac{1}{2}})) $&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\mathbf{\Sigma}$ is the covariance matrix of the data. &lt;/p&gt;&#10;&#10;&lt;p&gt;i.e. the $i^{\textrm{th}}$, $j^{\textrm{th}}$ component of the corrlation matrix is the correlation coefficient for the $i^{\textrm{th}}$, $j^{\textrm{th}}$ data in x and y. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-17T09:52:54.563" Id="103665" LastActivityDate="2014-06-17T10:06:53.773" LastEditDate="2014-06-17T10:06:53.773" LastEditorUserId="46653" OwnerUserId="46653" ParentId="103659" PostTypeId="2" Score="0" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Each sample in my data set is an $N$-bit bit-vector (say $N$=200). The bits in each sample are not uncorreleated within the sample.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I built a matrix $S_{N \times N}$ with each $s_{ij}$ being the proportion of samples where the $i$-th bit and the $j$-th bit were identical. This is a symmetric similarity matrix (obviously, with $1$s on the diagonal).  &lt;/p&gt;&#10;&#10;&lt;p&gt;Now I want to throw some of the bits away with as little loss of information of possible.&lt;br&gt;&#10;What is the best way to choose $K$ bits, i.e. $K$ columns+rows of $S$, that when removed, the data-set retains as much of its discrimination as before.  &lt;/p&gt;&#10;&#10;&lt;p&gt;In other words, how do I choose the top $K$ bits to remove, that can be best predicted by the remaining bits for each sample.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-06-17T12:55:53.067" Id="103687" LastActivityDate="2014-06-17T12:55:53.067" OwnerUserId="2631" PostTypeId="1" Score="0" Tags="&lt;correlation&gt;" Title="Removing minimally informative bits" ViewCount="8" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Im having trouble with a multivariate probit model with partial observability/sample selection (written in GAUSS). In this model there is a probit at each of multiple stages, and only one of the two outcomes for each stage goes on to a new stage. In other words, its basically a Heckman model, where you observe an outcome equation (wages) but only for a subgroup passing the selection equation (only those who decide to work).&lt;/p&gt;&#10;&#10;&lt;p&gt;My problem is with specifying the covariance matrix. It basically needs to satisfy 3 things: 1) must be positive definite 2) all diagonal entries are = 1 for identification 3) off-diagonal entries are thus correlations, and must be in [-1,1]. &lt;/p&gt;&#10;&#10;&lt;p&gt;To ensure the cov matrix is positive definite (PD), people usually just parameterize a lower triangular Cholesky-type matrix, C. Then define the cov matrix=CC', which is guaranteed PD. However, in this decomposition C, the diagonals take the form sqrt(1-x). These values often wind up imaginary in my estimation, even if I brute force them back into good values (presumably since the canned optimization does its thing after my code evaluates).&lt;/p&gt;&#10;&#10;&lt;p&gt;Secondly, I am unable to restrict the off-diagonal entries within [-1,1], so I wind up with values of magnitude &gt;1, yet they are supposed to be correlations. &lt;/p&gt;&#10;&#10;&lt;p&gt;Does anyone have any advice or experience with these issues? Much appreciated.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-17T14:28:56.090" Id="103704" LastActivityDate="2014-06-17T14:28:56.090" OwnerUserId="48511" PostTypeId="1" Score="0" Tags="&lt;multivariate-analysis&gt;&lt;covariance&gt;&lt;probit&gt;&lt;multivariate-regression&gt;" Title="Covariance matrix specification in multivariate probit" ViewCount="23" />
  <row Body="&lt;p&gt;I'm stealing this wholesale from the Stan users group. Michael Betancourt provided this &lt;a href=&quot;https://groups.google.com/forum/#!searchin/stan-users/identifiability/stan-users/zK6fCA_zPVY/gRRq6YA3qU4J&quot; rel=&quot;nofollow&quot;&gt;really good discussion&lt;/a&gt; of identifiability in Bayesian inference, which I believe bears on your request for a contrast of the two statistical schools.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The first difference with a Bayesian analysis will be the presence of priors which, even when weak, will constrain the posterior mass for those 4 parameters into a finite neighborhood (otherwise you wouldn't have had a valid prior in the first place).  Despite this, you can still have non-identifiability in the sense that the posterior will not converge to a point mass in the limit of infinite data.  In a very real sense, however, that doesn't matter because (a) the infinite data limit isn't real anyways and (b) Bayesian inference doesn't report point estimates but rather distributions.  In practice such non-identifiability will result in large correlations between the parameters (perhaps even non-convexity) but a proper Bayesian analysis will identify those correlations.  Even if you report single parameter marginals you'll get distributions that span the marginal variance rather than the conditional variance at any point (which is what a standard frequentist result would quote, and why identifiability is really important there), and it's really the marginal variance that best encodes the uncertainty regarding a parameter.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;Simple example:  consider a model with parameters $\mu_1$ and $\mu_2$ with likelihood $\mathcal{N}(x | \mu_1 + \mu_2, \sigma)$.  No matter how much data you collect, the likelihood will not converge to a point but rather a line $\mu_1 + \mu_2 = 0$.  The conditional variance of $\mu_1$ and $\mu_2$ at any point on that line will be really small, despite the fact that the parameters can't really be identified.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;Bayesian priors constrain the posterior distribution from that line to a long, cigar shaped distribution.  Not easily to sample from but at least compact.  A good Bayesian analysis will explore the entirety of that cigar, either identifying the correlation between $\mu_1$ and $\mu_2$ or returning the marginal variances that correspond to the projection of the long cigar onto the $\mu_1$ or $\mu_2$ axes, which give a much more faithful summary of the uncertainty in the parameters than the conditional variances.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="0" CreationDate="2014-06-17T14:52:39.583" Id="103710" LastActivityDate="2014-06-17T16:16:45.177" LastEditDate="2014-06-17T16:16:45.177" LastEditorUserId="22311" OwnerUserId="22311" ParentId="103625" PostTypeId="2" Score="2" />
  <row AnswerCount="0" Body="&lt;p&gt;In this experiment we collect $N$ samples and each sample yields a pair of survival curves. The two survival curves are hypothesized to be identical up until time $t$ and diverge thereafter. What would be an appropriate method by which such a point $t$ can be estimated and statistically validated?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-17T15:12:20.433" Id="103712" LastActivityDate="2014-06-18T16:40:31.313" LastEditDate="2014-06-18T16:40:31.313" LastEditorUserId="44995" OwnerUserId="44995" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;survival&gt;" Title="How to estimate the point of divergence between two continuous time survival curves?" ViewCount="38" />
  <row AnswerCount="1" Body="&lt;p&gt;Hi I have two questions related to a previous question I asked here:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/103105/simplex-random-walk&quot;&gt;Simplex Random Walk&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In this link it describes how to perform a random walk on the simplex.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/User%3aSkinnerd/Simplex_Point_Picking&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/User:Skinnerd/Simplex_Point_Picking&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;It says that &quot;This procedure effectively samples $x_{new}$ from a gamma random variable with mean of $x_{old}$ and standard deviation of $h*x_{old}$.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;My first question is why would the mean be $x_{old}$? &lt;/p&gt;&#10;&#10;&lt;p&gt;As per the answer to my previous question the proposal distribution is a log-normal with parameters $(\ln(x_{old}),h)$. Wiki says the mean of a log-normal distribution is $e^{\mu + \frac{\sigma^2}{2}}$, which would evaluate to $e^{\ln(x_{old}) + \frac{h^2}{2}} = x_{old}e^{\frac{h^2}{2}}$. Am I missing something?&lt;/p&gt;&#10;&#10;&lt;p&gt;My second question is: What other ways are there to perform a random walk on a simplex? Is there a &quot;standard&quot; method? I've tried searching but am curious if I've missed something.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for any help.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-06-17T15:24:38.703" FavoriteCount="2" Id="103715" LastActivityDate="2014-06-19T12:31:03.927" OwnerUserId="48244" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;lognormal&gt;&lt;metropolis-hastings&gt;&lt;random-walk&gt;" Title="Simplex Random Walk Mean" ViewCount="52" />
  <row AnswerCount="0" Body="&lt;p&gt;Let's say I have a dataset with scores on a bunch of questionnaire items, which are theoretically comprised of a smaller number of scales, like in psychology research.&lt;/p&gt;&#10;&#10;&lt;p&gt;I know a common approach here is to check the reliability of the scales using Cronbach's alpha or something similar, then aggregate the items in the scales to form scale scores and continue analysis from there.&lt;/p&gt;&#10;&#10;&lt;p&gt;But there's also factor analysis, which can take all of your item scores as input and tell you which of them form consistent factors. You can get a sense of how strong these factors are by looking at loadings and communalities, and so on. To me this sounds like the same kind of thing, only much more in-depth.&lt;/p&gt;&#10;&#10;&lt;p&gt;Even if all your scale reliabilities are good, an EFA might correct you on which items fit better into which scales, right? You're probably going to get cross loadings and it might make more sense to use derived factor scores than simple scale sums.&lt;/p&gt;&#10;&#10;&lt;p&gt;If I want to use these scales for some later analysis (like regression or ANOVA), should I just aggregate the scales so long as their reliability holds up? Or is something like CFA (testing to see if the scales hold up as good factors, which seems to be measuring the same thing as 'reliability').&lt;/p&gt;&#10;&#10;&lt;p&gt;I've been taught about both approaches independently and so I really don't know how they relate, whether they can be used together or which one makes more sense for which context. Is there a decision tree for good research practice in this case? Something like:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;Run CFA according to predicted scale items  &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;If CFA shows good fit, calculate factor scores and use those for analysis&lt;/li&gt;&#10;&lt;li&gt;If CFA shows poor fit, run EFA instead and take exploratory approach (or something)&lt;/li&gt;&#10;&lt;/ul&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Are factor analysis and reliability testing indeed separate approaches to the same thing, or am I misunderstanding somewhere?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-06-17T15:58:27.003" Id="103723" LastActivityDate="2014-07-21T18:07:08.920" LastEditDate="2014-06-17T16:20:08.910" LastEditorUserId="7290" OwnerUserId="48524" PostTypeId="1" Score="3" Tags="&lt;factor-analysis&gt;&lt;reliability&gt;&lt;psychometrics&gt;&lt;best-practices&gt;" Title="What is the relationship between scale reliability measures (Cronbach's alpha etc.) and component/factor loadings?" ViewCount="142" />
  <row AnswerCount="3" Body="&lt;p&gt;I'm looking to generate fake data to fit a multinomial logit in R? Any code/suggestions on material to look at would be very much appreciated...&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-06-17T17:10:34.027" Id="103728" LastActivityDate="2014-12-28T15:43:52.053" OwnerUserId="48526" PostTypeId="1" Score="0" Tags="&lt;simulation&gt;&lt;logit&gt;" Title="Simulating Multinomial Logit Data with R" ViewCount="213" />
  
  
  <row AnswerCount="3" Body="&lt;p&gt;I have a study where many outcomes are represented like percentages and I'm using multiple linear regressions to asses the effect of some categorical variables on thes outcomes.&lt;/p&gt;&#10;&#10;&lt;p&gt;I was wondering, since a linear regression assume that the outcome is a continuous distribution, are there methodological problems in applying such model to percentages, which are limited between 0 and 100?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-06-17T17:32:47.863" Id="103731" LastActivityDate="2014-06-18T21:34:02.937" LastEditDate="2014-06-18T07:34:33.183" LastEditorUserId="22047" OwnerUserId="6479" PostTypeId="1" Score="3" Tags="&lt;regression&gt;&lt;linear-model&gt;&lt;model&gt;&lt;lm&gt;&lt;percentage&gt;" Title="Percentage outcomes in linear regressions" ViewCount="257" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Below is the output from a model of novel object test scores fit with the &lt;code&gt;nbinom1&lt;/code&gt; (quasi-Poisson) option in &lt;code&gt;glmmADMB&lt;/code&gt;. I used this package/method because: &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;the Poisson mean is &amp;lt; 5, so according to Bolker et al. 2009 I should not use &lt;code&gt;glmmPQL&lt;/code&gt;&lt;/li&gt;&#10;&lt;li&gt;there was overdipersion with the Poisson model run with &lt;code&gt;glmer&lt;/code&gt;. The NB1 model fit with &lt;code&gt;glmmADMB&lt;/code&gt; was the best of all fits I tried.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;There are 3 fixed factors as below, as well as a &lt;code&gt;Sex&lt;/code&gt;–&lt;code&gt;Age&lt;/code&gt; interaction term, and individual (&lt;code&gt;ID&lt;/code&gt;) is the random factor.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Call:&#10;glmmadmb(formula = Score ~ Sex * Age + Object + (1 | ID), data = PGS, &#10;    family = &quot;nbinom1&quot;)&#10;&#10;AIC: 394.5&#10;&#10;Coefficients:&#10;              Estimate Std. Error z value Pr(&amp;gt;|z|)&#10;(Intercept)      0.480      0.195    2.47   0.0137&#10;SexM             0.797      0.262    3.05   0.0023&#10;AgeS             1.224      0.254    4.82  1.4e-06&#10;ObjectPLA       -0.434      0.194   -2.24   0.0248&#10;ObjectSCO       -0.822      0.203   -4.05  5.1e-05 &#10;SexM:AgeS       -0.825      0.353   -2.33   0.0196 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Number of observations: total=105, ID=40 &#10;&#10;Random effect variance(s):&#10;Group=ID&amp;lt;br/&amp;gt;&#10;            Variance StdDev&amp;lt;br/&amp;gt;&#10;(Intercept)  0.03738 0.1933&#10;&#10;Negative binomial dispersion parameter: 1.5958 (std. err.: 0.34873)&#10;&#10;Log-likelihood: -189.228&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; how can I test if the random effect is significant? I have looked around online, and where there are multiple random effects it seems straightforward, but I know I can't compare the &lt;code&gt;GLMM&lt;/code&gt; to a &lt;code&gt;GLM&lt;/code&gt; without my single random effect according to info on &lt;a href=&quot;http://glmm.wikidot.com/faq&quot; rel=&quot;nofollow&quot;&gt;http://glmm.wikidot.com/faq&lt;/a&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have tried Ben Bolker's &lt;code&gt;varprof&lt;/code&gt; function in the reef-fish example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;varprof(PGS.nbinom1)&#10;Error in varprof(PGS.nbinom1) : &amp;lt;br/&amp;gt;&#10;  trying to get slot &quot;deviance&quot; from an object (class &quot;glmmadmb&quot;) that is not an S4 object&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;As well as his &lt;code&gt;simulate.glm(PGS.nbinom1)&lt;/code&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;simulate.glm(PGS.nbinom1)&#10;Error in simulate.glm(M1) : family nbinom not implemented&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And also Jeff Evans’ way, posted on &lt;a href=&quot;http://web.archiveorange.com/archive/v/rOz2zzUOluek2dZ4lcUh&quot; rel=&quot;nofollow&quot;&gt;archiveorange.com&lt;/a&gt;, where I ran the model with an uninformative dummy variable (a column with the word &quot;dummy&quot; in every row) and compared that with my model with ANOVA, but also got a warning message here:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;anova(M1,M2)&#10;Analysis of Deviance Table&#10;&#10;Model 1: Score ~ Sex * Age + Object&amp;lt;br/&amp;gt;&#10;Model 2: Score ~ Sex * Age + Object&amp;lt;br/&amp;gt;&#10;  NoPar  LogLik Df Deviance Pr(&amp;gt;Chi)&amp;lt;br/&amp;gt;&#10;1     8 -189.23 &amp;lt;br/&amp;gt;                    &#10;2     8 -189.32  0    -0.19        1&amp;lt;br/&amp;gt;&#10;&#10;Warning message:&#10;In anova.glmmadmb(M1, M2) :&amp;lt;br/&amp;gt;&#10;  something's wrong: models should be nested, increasing complexity should imply increasing log-likelihood&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;What else can I try?&#10;Any help would be much appreciated! &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-17T18:58:13.940" Id="103742" LastActivityDate="2014-06-17T21:09:54.400" LastEditDate="2014-06-17T19:24:48.327" LastEditorUserId="32036" OwnerUserId="48533" PostTypeId="1" Score="2" Tags="&lt;inference&gt;&lt;random-effects-model&gt;&lt;glmm&gt;" Title="Testing significance of a random effect glmmADMB model" ViewCount="478" />
  <row AnswerCount="1" Body="&lt;p&gt;I have a list of 20 QTL from an experiment involving a certain phenotype related to bone development.   Of all the candidate genes within the 20 QTL intervals, 5 are associated with a certain disease in humans.  I tested the set of 20 QTL to see if they are enriched for the disease-associating genes.  Each of the 20 QTL has a certain length in base pairs.  I took that set of 20 lengths, and randomly placed them in the mouse genome.  I forced them not to overlap and to land no closer than 500,000 bases on either side.  Then I took note of the genes inside each interval and counted how many matched the names contained in the disease-associating list of genes.  I repeated the process 10,000 times.  This provides the expectations given randomness... how many times can I expect to see disease-associating genes inside a set of 20 randomly placed QTL?  I compare that to my actual observed number.  Then I calculate the probability.&lt;/p&gt;&#10;&#10;&lt;p&gt;To do that, I simply counted how many times 1 disease-associating gene appeared within each set of 20 randomly placed QTL and divide by 10,000.  That gives me the frequency.  I did that for 2 disease-associating genes, and 3, and so on up to 5, which is my observed value.  I find that the frequency of 5's in the random data is less than 0.05.  Is this appropriate?&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, I see online some might suggest a parametric method, like Fishers' exact test, but that is supposed to be used for very small data sets, am I correct?  Am I missing something there?  How can I implement a parametric calculation for p?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-17T19:03:32.610" Id="103744" LastActivityDate="2015-02-28T09:20:32.720" OwnerUserId="20883" PostTypeId="1" Score="0" Tags="&lt;probability&gt;&lt;simulation&gt;&lt;biostatistics&gt;&lt;fishersexact&gt;&lt;bioinformatics&gt;" Title="How do I calculate the p-value in a gene enrichment analysis, using parametric and non-parametric methods?" ViewCount="92" />
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Would it make sense to apply a measure of association test like Cramér's V on a 1xk table? &lt;/p&gt;&#10;&#10;&lt;p&gt;I am a bit puzzled because in the literature phi and Cramér V are normally used on 2x2 or bigger contingency tables, but while I was looking for some more info, I found the following statement on Wikipedia:&lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;Cramér's V may also be applied to goodness of fit chi-squared models when there is a 1×k table (e.g.: r=1). In this case k is taken as the number of optional outcomes and it functions as a measure of tendency towards a single outcome.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;No reference is provided though. What do you think about it? Could you please give me some bibliographic reference?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-06-17T20:10:31.503" Id="103753" LastActivityDate="2014-06-17T20:16:10.083" LastEditDate="2014-06-17T20:16:10.083" LastEditorUserId="22390" OwnerUserId="22390" PostTypeId="1" Score="1" Tags="&lt;chi-squared&gt;&lt;association-measure&gt;" Title="One-way Chi-Square goodness of fit and Cramér's V" ViewCount="20" />
  
  <row AcceptedAnswerId="103823" AnswerCount="2" Body="&lt;p&gt;I have a data set with two columns, the first of which is to be used as a response variable and the second of which is to be used as a predictor variable. The predictor variable, however, is populated by integer values, many of which are zero.  When I place data points for the predictor and response variables into groups corresponding groups, no group has a value of zero anymore.  Performing a linear regression on the data when it is in this form produces a significantly higher $R^2$ than in the ungrouped regression. My question is:  Is something seriously wrong with my methodology if the change in $R^2$ is massive between the grouped and ungrouped models?  Can this be explained by the large number of zeros?&lt;/p&gt;&#10;&#10;&lt;p&gt;To provide a slightly better idea of what I mean by grouping consider the following data:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;A  B&#10;&#10;0  .2&#10;0  .3&#10;0  .3&#10;2  .6&#10;4  .7&#10;6  .8&#10;7  .7&#10;4  .4&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;A would be grouped as follows:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;0-3&#10;4-7&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;with the new B values being the group averages.&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2014-06-17T20:19:41.363" Id="103755" LastActivityDate="2014-06-21T05:06:23.323" LastEditDate="2014-06-17T21:31:36.843" LastEditorUserId="40120" OwnerUserId="40120" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;linear-model&gt;&lt;zero-inflation&gt;" Title="Linear Regression With Groups vs. Points - Issue with Influence of Zeros" ViewCount="151" />
  
  <row Body="&lt;p&gt;These are often known as cophenetic distances and can be extracted using the &lt;code&gt;cophenetic()&lt;/code&gt; function. This function works with &lt;code&gt;hclust()&lt;/code&gt; objects, any object with a &lt;code&gt;as.hclust()&lt;/code&gt; method (so hierarchical cluster methods in the &lt;strong&gt;cluster&lt;/strong&gt; package) and object of class &lt;code&gt;&quot;dendrogram&quot;&lt;/code&gt;.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-06-17T22:41:47.977" Id="103770" LastActivityDate="2014-06-17T22:41:47.977" OwnerUserId="1390" ParentId="103767" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;Check out the work: Nonparametric belief propagation by Sudderth, Ihler, Isard, Freeman, Willsky. They talk about a general graph, with the nodes taking continuous values, instead of discrete states. They show a way to perform loopy belief propagation in this context, using MCMC.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.ics.uci.edu/~ihler/papers/_cacm10.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.ics.uci.edu/~ihler/papers/_cacm10.pdf&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-18T00:52:50.367" Id="103778" LastActivityDate="2014-06-18T00:52:50.367" OwnerUserId="44731" ParentId="83266" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;&lt;strong&gt;Details to take care of the $N=0$ case.&lt;/strong&gt;&#10;$$&#10;\begin{align}&#10;  \{Z\geq\theta+\epsilon\} &amp;amp;= \left(\{Z\geq\theta+\epsilon\} \cap \{N=0\}\right) \cup \left(\{Z\geq\theta+\epsilon\} \cap \{N&amp;gt;0\}\right) \\&#10;&amp;amp;=  \left(\{0\geq\theta+\epsilon\} \cap \{N=0\}\right) \cup \left(\{Z\geq\theta+\epsilon\} \cap \{N&amp;gt;0\}\right) \\&#10;&amp;amp;=  \left(\emptyset \cap \{N=0\}\right) \cup \left(\{Z\geq\theta+\epsilon\} \cap \{N&amp;gt;0\}\right) \\&#10;&amp;amp;= \left\{\sum_{i=1}^n X_iY_i\geq(\theta+\epsilon)\sum_{i=1}^n Y_i\right\} \cap \{N&amp;gt;0\} \\&#10;&amp;amp;\subset \left\{\sum_{i=1}^n X_iY_i\geq(\theta+\epsilon)\sum_{i=1}^n Y_i\right\} \\&#10;&amp;amp;= \left\{\sum_{i=1}^n (X_i-\theta-\epsilon)Y_i\geq 0\right\} \\&#10;&amp;amp;= \left\{\sum_{i=1}^n \left((X_i-\theta-\epsilon)Y_i+\epsilon/2\right)\geq n\epsilon/2\right\} \, .&#10;\end{align}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;For Alecos.&lt;/strong&gt;&#10;$$&#10;\begin{align}&#10;\mathrm{E}\!\left[\sum_{i=1} ^n W_i\right]&amp;amp;=\mathrm{E}\!\left[I_{\{\sum_{i=1}^n Y_i=0\}}\sum_{i=1} ^n W_i\right] + \mathrm{E}\!\left[I_{\{\sum_{i=1}^n Y_i&amp;gt;0\}}\sum_{i=1} ^n W_i\right] \\&#10;&amp;amp;=\mathrm{E}\!\left[I_{\{\sum_{i=1}^n Y_i&amp;gt;0\}}\frac{\sum_{i=1} ^n Y_i}{\sum_{i=1}^n Y_i}\right]=\mathrm{E}\!\left[I_{\{\sum_{i=1}^n Y_i&amp;gt;0\}}\right]=1-1/2^n \, .&#10;\end{align}&#10;$$&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-18T04:03:45.220" Id="103785" LastActivityDate="2015-01-25T20:24:14.687" LastEditDate="2015-01-25T20:24:14.687" LastEditorUserId="9394" OwnerUserId="9394" ParentId="103456" PostTypeId="2" Score="5" />
  <row Body="&lt;h1&gt;UPDATE&lt;/h1&gt;&#10;&#10;&lt;p&gt;I found a relevant paper!&lt;/p&gt;&#10;&#10;&lt;p&gt;Leisenring W, Alonzo T, Pepe MS. &lt;em&gt;Comparisons of predictive values of binary medical diagnostic tests for paired designs.&lt;/em&gt; Biometrics 2000; 56:345-51.&lt;/p&gt;&#10;&#10;&lt;p&gt;It's not an easy one to read, I think it would be best to get a statistician with experience of hierarchical regression models to read the paper and advise. If this is not an option open to you then I will try to find time to have a go with your data, if you can provide it in the following format:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;+--+-----+-----+&#10;|  |  D+ |  D- |&#10;|  |B+ B-|B+ B-|&#10;+--+-----+-----+&#10;|A+|a  b |e  f |&#10;|A-|c  d |g  h |&#10;+--+-----+-----+&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Where D is your reference standard and A and B are your two index tests (RIPASA and Alvarado). The full cross-tabulation is necessary.&lt;/p&gt;&#10;&#10;&lt;h2&gt;(Previous answer)&lt;/h2&gt;&#10;&#10;&lt;p&gt;In this example the test on the left is superior according to the diagnostic likelihood ratios (labeled PLR &amp;amp; NLR) which are both further from 1.&lt;/p&gt;&#10;&#10;&lt;p&gt;Diagnostic likelihood ratios, sensitivity and specificity are all significantly &lt;em&gt;less&lt;/em&gt; sensitive to prevalence than PPV and NPV (which are both directly sensitive to prevalence) but they can still be affected if different prevalence in studies is due to spectrum bias.&lt;/p&gt;&#10;&#10;&lt;p&gt;Of course, a simple comparison of point estimates is not satisfactory - the better test might have only been tested in a small population and there could be no statistically significant difference. Also you want to know whether they were both evaluated in representative samples (using a single gate [cohort or nested case-control] design).&lt;/p&gt;&#10;&#10;&lt;p&gt;If you have multiple studies of each technique then a meta-analysis (using bivariate or HSROC method) may be appropriate. See chapter 10 (I think) of the Cochrane handbook for systematic reviews of diagnostic test accuracy.&lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, it could be that the diagnostically superior test is not acceptable to the same population as the inferior test. Better tests are often more expensive and more invasive, both factors which can render such tests poor for, e.g., screening.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-06-18T07:08:32.240" Id="103795" LastActivityDate="2014-07-13T21:35:50.403" LastEditDate="2014-07-13T21:35:50.403" LastEditorUserId="13158" OwnerUserId="13158" ParentId="103759" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a data set with label &quot;Y&quot; and &quot;N&quot;, which is splitted into training and testing sets. The ratio the labels in training and testing sets are 3:1 and 1:1, respectively. If I build model using the training data with random forest and 10-CV for model tuning, my question is, when the model is used predict the testing set, on average will the ratio of predicted labels be close to 1:1 or 3:1? I just want to understand how the unbalance in the training data influence the new data for prediction.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-18T08:16:28.257" Id="103807" LastActivityDate="2014-06-18T08:16:28.257" OwnerUserId="39796" PostTypeId="1" Score="0" Tags="&lt;classification&gt;&lt;unbalanced-classes&gt;" Title="the effect of unbalanced classes" ViewCount="21" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I was recommended on StackOverflow to ask this question here: for me it's dataviz question, but perhaps it's more algorithmic in nature.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have an ordered set of points in 2D, and would like to join them using some sort of spline to form a closed loop that does not intersect itself. This is, in fact, a single contour line, and I have additional sets of points marking out further contour lines that should be sequentially nested inside each other. Do any dataviz procedures exist to do this without intersection? For instance, take the 23 black points in this picture as the outer contour and the 12 blue points as an inner contour. &lt;img src=&quot;http://i.stack.imgur.com/RHK3K.jpg&quot; alt=&quot;Points to join&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I would want to join them vaguely like this:&lt;img src=&quot;http://i.stack.imgur.com/TsqH1.jpg&quot; alt=&quot;Lines through points&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;If it helps, I'll probably be trying to visualise this in R.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2014-06-18T13:58:48.713" Id="103844" LastActivityDate="2014-06-18T13:58:48.713" OwnerUserId="48582" PostTypeId="1" Score="2" Tags="&lt;data-visualization&gt;&lt;splines&gt;" Title="Contour lines by joining a set of 2D ordered points without line intersection" ViewCount="70" />
  
  
  <row Body="&lt;p&gt;I think the way to go is actually only estimate some of the parameters and then use the delta method to get the asymptotic distribution.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-18T15:31:02.990" Id="103860" LastActivityDate="2014-06-18T15:31:02.990" OwnerUserId="48478" ParentId="103634" PostTypeId="2" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;So in everything I've found, they tell you have to calculate $\rho$, or how to test for confidence interval for it. What I am trying to figure out is how to calculate the SE which would get us our critical value, i.e. if you get a $\rho$ of 0.8. To test it, you would do $(\rho-1)/(\text{SE}(\rho))$. To clarify, rho is our original coefficient in the AR model, it is not the transformed $(\rho-1)$ variable. Moving on, how does one get this $\text{SE}(\rho)$ manually?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-18T15:54:01.647" Id="103868" LastActivityDate="2014-06-18T17:25:37.440" LastEditDate="2014-06-18T17:25:37.440" LastEditorUserId="930" OwnerUserId="48590" PostTypeId="1" Score="0" Tags="&lt;standard-error&gt;&lt;autoregressive&gt;&lt;spearman-rho&gt;&lt;augmented-dickey-fuller&gt;" Title="How do you calculate standard error in a Dickey-Fuller test?" ViewCount="43" />
  
  <row AcceptedAnswerId="103888" AnswerCount="1" Body="&lt;p&gt;I am using a generalized formula for normalizing one data range to another but am having difficulty finding its formal name, if it even exists (sorry if my notation is strange):&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;x_b = min_b + \frac{(x_a - min_a)(max_b-min_b)}{max_a - min_a}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;This converts an $x$ in range $a$ to its respective value in range $b$.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, if you are mapping the value $2$ from range $[0,\ldots,10]$ into $[100,\ldots,200]$ then&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;120 = 100 + \frac{(2 - 0)(200-100)}{10 - 0}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I chose these values because it is easy to validate in my head; it works for general ranges.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-06-18T17:14:07.577" Id="103882" LastActivityDate="2014-06-18T18:43:55.080" LastEditDate="2014-06-18T17:49:41.307" LastEditorUserId="44746" OwnerUserId="44746" PostTypeId="1" Score="3" Tags="&lt;normalization&gt;&lt;terminology&gt;" Title="Is there a formal name for this data normalization formula?" ViewCount="61" />
  <row Body="&lt;h3&gt;Margins&lt;/h3&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Margins&lt;/em&gt; here refers to the values on the edges (margins!) of the table, that is, the total number of reds, total number of blacks, total number of drawn, and total number of not drawn. The related term &lt;a href=&quot;http://en.wikipedia.org/wiki/Marginal_distribution&quot; rel=&quot;nofollow&quot;&gt;marginal distribution&lt;/a&gt; refers to the distribution of a single variable obtained from a joint distribution of several variables by averaging over the other variables (etymologically, the term indeed comes from the values written on the margins of tables).&lt;/p&gt;&#10;&#10;&lt;h3&gt;Conditioning&lt;/h3&gt;&#10;&#10;&lt;p&gt;Conditioning refers  to computing &lt;a href=&quot;http://en.wikipedia.org/wiki/Conditional_probability_distribution&quot; rel=&quot;nofollow&quot;&gt;conditional distributions&lt;/a&gt;, that is, probability distributions given some information. Here, conditioning on the margins means that the margins are fixed, i.e., we assume that there are in total 6680 red balls (and 12160 black balls), as well as 382 drawn balls (and 18458 balls not drawn). So that, for example&lt;code&gt;&#10;      Drawn Not drawn Total&#10;Red   200   6480      6680 &#10;Black 182   11978     12160&#10;Total 382   18458     18840&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;would be a possible realization of our random distribution (the margins are the same). Under the null hypothesis that getting drawn and the color of the ball are independent, conditioning on the margins leads to the hypergeometric distribution. &lt;/p&gt;&#10;&#10;&lt;p&gt;Alternatively, if the experiment were such that one draws balls until 160 reds are obtained, it would not make sense to condition on the margins (as the total number of drawn balls could have turned out something else than 382). In this case, one could obtain realizations like&#10;&lt;code&gt;&#10;      Drawn Not drawn Total&#10;Red   160   6520      6680 &#10;Black 182   11978     12160&#10;Total 342   18498     18840&lt;/code&gt;,&#10;which would have different margins.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-18T17:44:47.443" Id="103887" LastActivityDate="2014-06-18T17:44:47.443" OwnerUserId="24669" ParentId="103876" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;Besides my comment above, according to &lt;a href=&quot;http://faculty.washington.edu/ezivot/econ584/notes/unitroot.pdf&quot; rel=&quot;nofollow&quot;&gt;this&lt;/a&gt;  the maximum lag length can be chosen by rule of a thumb formula:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;P_{max}=[12.(\frac{T}{100})^{1/4}]&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;See page 121 in the above referenced link:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;An important practical issue for the implementation of the ADF test is&#10;  the speciﬁcation of the lag length p. If p is too small then the&#10;  remaining serial correlation in the errors will bias the test. If p is&#10;  too large then the power of the test will suﬀer. Ng and Perron (1995)&#10;  suggest the following data dependent lag length selection procedure&#10;  that results in stable size of the test and minimal power loss. First,&#10;  set an upper bound pmax for p. Next, estimate the ADF test regression&#10;  with p = pmax. If the absolute value of the t-statistic for testing&#10;  the signiﬁcance of the last lagged diﬀerence is greater than 1.6 then&#10;  set p = pmax and perform the unit root test. Otherwise, reduce the lag&#10;  length by one and repeat the process. A useful rule of thumb for determining pmax, &#10;  suggested by Schwert(1989), is&lt;/p&gt;&#10;  &#10;  &lt;p&gt;$$&#10;P_{max}=[12.(\frac{T}{100})^{1/4}]&#10;$$&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="0" CreationDate="2014-06-18T19:33:56.893" Id="103900" LastActivityDate="2014-06-18T19:33:56.893" OwnerUserId="29137" ParentId="103865" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;The reason is that as sample size increases your ability to detect deviations from the null (no effect) increases [standard errors of the estimate shrink].  Therefore, at sufficiently large sample sizes, even tiny effects may become statistically significant (i.e. $p &amp;lt; \alpha$).  It is always (or at least almost nearly always) an appropriate thing to do to look at the effect size of your statistic.  The effect size will quantify how large your effect is, hopefully in terms you care about (e.g. proportion of variance accounted for or magnitude of average difference scaled by error in measurement).  If you know how to interpret your effect size, then you will also know how to judge whether this is a tiny effect of no theoretical interest, or an effect large enough to be worthy of consideration.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-06-18T20:12:28.133" Id="103908" LastActivityDate="2014-06-18T20:12:28.133" OwnerUserId="196" ParentId="103907" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;Let's say we have the following matrix (typical VSM example):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;         word1    word2    word3    word4&#10;doc1     0.838    0.918    0.001    0.482&#10;doc2     0.311    0.948    0.001    0.183&#10;doc3     0.001    0.928    0.592    0.381&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I want to initially get rid of some words with a low tf-idf weighting, but I am not sure what to do with words as word1 in my example. word1 is worthless for doc3 but looks important for doc1. &lt;/p&gt;&#10;&#10;&lt;p&gt;Should I take an average of these weights and put a threshold there?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-18T20:52:25.447" Id="103916" LastActivityDate="2014-06-18T20:52:25.447" OwnerUserId="44513" PostTypeId="1" Score="1" Tags="&lt;classification&gt;&lt;feature-selection&gt;" Title="How to apply feature selection based on tf-idf threshold" ViewCount="137" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Consider a given dataset on which I performed a set of independent tests on different (non-independent) variables, and obtained several p-values. I estimated the number of independent tests as being $n$ and corrected the p-values obtained using Sidak or Bonferroni correction.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now let us say I obtained for one of these a marginaly significant result and acquired an extended dataset to conclude if the correlation is significant or not. What is tne number of independent tests I should use for the correction on this larger dataset, $n$ or $n+1$?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-19T00:46:32.547" Id="103937" LastActivityDate="2014-06-19T12:25:32.617" LastEditDate="2014-06-19T12:25:32.617" LastEditorUserId="44760" OwnerUserId="44760" PostTypeId="1" Score="0" Tags="&lt;multiple-comparisons&gt;&lt;bonferroni&gt;" Title="Multiple testing correction when confirming a correlation with an extended dataset" ViewCount="23" />
  
  
  
  <row Body="&lt;p&gt;A more positive fact is the following.&lt;br&gt;&#10;If you drop the requirement that the probability measure be countably additive, and only require, instead, that it be finitely additive (just for the sake of this question), then for the rational numbers the answer is &quot;yes&quot;.&lt;br&gt;&#10;The rational numbers are an additive group since one can add two rational numbers, there is a neutral element, zero, and any $z\in\mathbb{Q}$ has an additive inverse $-z\in\mathbb{Q}$.&lt;br&gt;&#10;Now, one can equip the rational numbers with the discrete topology so that they are a &lt;a href=&quot;http://en.wikipedia.org/wiki/Discrete_group&quot; rel=&quot;nofollow&quot;&gt;discrete group&lt;/a&gt;.  (This is important because in other contexts it is more convenient not to do so and put another topology on them.)&lt;br&gt;&#10;Viewed as a discrete group, they are even a countable discrete group because there are only countably many rational numbers.&lt;br&gt;&#10;Also, they are an abelian group because $z +y =y+z$ for any pair of rational numbers.&lt;br&gt;&#10;Now, the rational numbers, viewed as a countable discrete group, are an amenable group. See &lt;a href=&quot;http://en.wikipedia.org/wiki/Amenable_group#Case_of_discrete_groups&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; for the definition of an amenable discrete group. &lt;a href=&quot;http://mathoverflow.net/questions/27965/countable-discrete-abelian-group-amenable&quot;&gt;Here&lt;/a&gt; it is shown that every countable abelian discrete group is amenable. In particular, this applies to the group of rational numbers.&lt;br&gt;&#10;Therefore, by the very definition of an amenable discrete group, there exists a finitely additive probability measure $\mu$ on the rational numbers that is translation invariant, meaning that $\mu(z + A) = \mu(A)$ for any subset $A\subset\mathbb{Q}$ and any rational number $z\in\mathbb{Q}$.&lt;br&gt;&#10;This property encompasses the intuitive way of defining &quot;uniformity&quot;.&lt;br&gt;&#10;$\mu$ necessarily vanishes on all finite subsets: $\mu(\{z\})=0$ for all $z\in\mathbb{Q}$.&lt;br&gt;&#10;If you seek a random variable instead of a probability measure, then just consider the identity function on the probability space $(\mathbb{Q}, \mu)$. This gives such a required random variable.&lt;br&gt;&#10;Therefore, if you relax your definition of probability measure a bit, you end up with a positive answer for the rational numbers.&lt;br&gt;&#10;Perhaps, the existence of $\mu$ seems a bit counter-intuitive. One can get a better idea of $\mu$ by taking into account that a direct consequence of the translation-invariance is that the measure of all rational numbers whose floor is even, is one half; also, the measure of those with odd floor is one half, and so on.&lt;br&gt;&#10;That measure $\mu$ that we just showed to exist, also necessarily vanishes on all bounded subsets (as one can show with a similar argument), in particular on the unit interval.&lt;br&gt;&#10;Therefore, $\mu$ does not immediately give an answer for the rational numbers in the unit interval. One would have thought that the answer is easier to give for the rational numbers in the unit interval instead of all rational numbers, but it seems to be the other way around.&lt;br&gt;&#10;(However, it also seems that one can cook up a probability measure on the rational numbers in the unit interval with similar properties, but the answer would then require a more precise definition of &quot;uniformity&quot; - maybe something along the lines of &quot;translation-invariant whenever translation does not lead outside the unit interval&quot;.)&lt;br&gt;&#10;UPDATE: You immediately obtain a measure on the unit interval rationals that is uniform in that sense, by considering the push-forward measure of the one on the rationals, that we constructed, along the map from the rationals to the unit interval rationals that maps each rational to its fractional part.&lt;br&gt;&#10;Therefore, after relaxing the requirement to finite additivity, you obtain such measures in both cases you mentioned.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-06-19T08:22:33.513" Id="103962" LastActivityDate="2014-07-05T13:19:44.617" LastEditDate="2014-07-05T13:19:44.617" LastEditorUserId="48637" OwnerUserId="48637" ParentId="103930" PostTypeId="2" Score="6" />
  
  <row AcceptedAnswerId="103976" AnswerCount="2" Body="&lt;p&gt;Suppose I have a dataframe consisting of six time series. In this dataframe, some observations are missing, meaning at some timepoints all time series contain a NA-value. In R, one possible imputation package that can be used to impute time series data is Amelia. However, this package does not work for observations that are completely missing. Are there other ways to impute my data? For what it's worth, the amount of missing observations is less than 20% of all observations.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-19T09:31:02.197" FavoriteCount="1" Id="103968" LastActivityDate="2014-06-19T10:21:41.173" OwnerUserId="5795" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;time-series&gt;&lt;multivariate-analysis&gt;&lt;data-imputation&gt;" Title="Imputing missing observation in multivariate time series" ViewCount="105" />
  
  
  
  <row AcceptedAnswerId="104021" AnswerCount="2" Body="&lt;p&gt;I'm working on 2D time series data where two attributes are depth and temperature. When I plotted depth-vs-temp curve and saw its variation over time, the fluctuation occurs at few places only.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm not saying temperature is dependent on depth. But given these data, what should I look into to establish relationships between depth, temperature, time?&lt;/p&gt;&#10;&#10;&lt;p&gt;Ideas:&lt;br&gt;&#10;Look for depth regions where fluctuation in temperatures occurs over time. And develop time series prediction for these given regions.&lt;/p&gt;&#10;&#10;&lt;p&gt;What models/papers should I study to get insights out of the data? I want to understand and study what are the various methods that could be tested and visualizations built over these data. It's a data exploration problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;Data shape: (2000,2,100) (samples,depth-temperature,time-sample) .i.e 2000 depth,temperature samples for each of 100 time stamps.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-06-19T11:49:28.340" Id="103989" LastActivityDate="2014-06-19T15:23:27.777" LastEditDate="2014-06-19T13:10:16.773" LastEditorUserId="48651" OwnerUserId="48651" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;data-visualization&gt;&lt;variance&gt;&lt;generalized-linear-model&gt;&lt;forecasting&gt;" Title="Ways to understand 2-dimensional time-series data" ViewCount="97" />
  <row AnswerCount="0" Body="&lt;p&gt;Let $c$ be a positive constant. Suppose I have $X$~$Unif(0,\theta)$ and I wish to test the null hypothesis $H_0: \theta \leq c$ against the alternative hypothesis $H_1:\theta &amp;gt; c$. Suppose I did a sample $X_1,...,X_n$ but I obsrerve them with perturbation - that is, I actually have on hand $Y_1:=X_1+\epsilon_1,...,Y_n:=X_n+\epsilon_n$ where $\epsilon_i$ are i.i.d. ~ $N(0,\sigma^2)$. How should I conduct a hypothesis testing?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Idea:&lt;/strong&gt; I was thinking of using Likelihood ratio test, since that allows me to do hypothesis testing on a composite null hypothesis. I can take into account the errors in my likelihood function by convolving the uniform with the normal. However, when I want to define critical values, I have difficulty because I have no idea how to approximate the test statistic to any distribution and hence derive rejection regions.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Idea 2:&lt;/strong&gt; Rao's score test, since here I know how to approximate the test statistic distribution for large sample size, but the only form of Rao's test I know involves point null hypothesis (i.e. testing $\theta = c$) - sadly, my bound $c$ is conservative upper bound, so there is a large chance that $\theta$ is smaller than $c$ and I dont want this case to be a rejection case.&lt;/p&gt;&#10;&#10;&lt;p&gt;Other ideas would also be appreciated.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-06-19T13:26:09.240" Id="104003" LastActivityDate="2014-06-19T13:26:09.240" OwnerUserId="48490" PostTypeId="1" Score="2" Tags="&lt;hypothesis-testing&gt;&lt;likelihood-ratio&gt;&lt;scores&gt;" Title="Hypothesis Testing of endpoint of Uniform distribution" ViewCount="141" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Could you please recommend methods for combining classifiers, which were trained on different patients? For example, there are 10 patients and for each of them I trained a binary classifier. Now I want to combine the classifiers I already have by taking weighted average of their predictions, so my question is how to learn these weights? It resembles stacking a bit (like &lt;a href=&quot;http://jair.org/media/594/live-594-1797-jair.pdf&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;), but I'm not sure. I would be grateful for any other proposals.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-19T15:00:34.493" Id="104017" LastActivityDate="2014-06-19T15:00:34.493" OwnerUserId="35873" PostTypeId="1" Score="0" Tags="&lt;classification&gt;&lt;ensemble&gt;" Title="combining classifiers trained on different data" ViewCount="24" />
  <row AnswerCount="0" Body="&lt;p&gt;I am curious, given a data set that has a design with two within subjects factors, which  plots would work the best to generalize the the observations?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-19T16:54:25.717" Id="104033" LastActivityDate="2014-06-19T16:54:25.717" OwnerUserId="46872" PostTypeId="1" Score="0" Tags="&lt;anova&gt;&lt;repeated-measures&gt;" Title="Plots for Two Within Subjects ANOVA" ViewCount="11" />
  
  <row AcceptedAnswerId="104064" AnswerCount="1" Body="&lt;p&gt;I am trying to learn ttest and ANOVA for my project (i am a computer science student). I need to know that my subjects answers &quot;effected&quot; by the feedback they get from the first question.&lt;br&gt;&#10;What should i use to show &quot;sth did effect or not sth&quot;? &#10;ANOVA or ttest?  &lt;/p&gt;&#10;&#10;&lt;p&gt;I am sorry if it sounds very stupid. Thanks in advance.&lt;/p&gt;&#10;&#10;&lt;p&gt;My data is:&lt;/p&gt;&#10;&#10;&lt;p&gt;A person answers a question and receives a feedback as &quot;good/bad&quot; than he/she answers another question. Does the second answer effected by the feedback or not? That is what i want to learn.   &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-06-19T20:05:47.493" Id="104061" LastActivityDate="2014-06-19T20:42:46.913" LastEditDate="2014-06-19T20:42:46.913" LastEditorUserId="48685" OwnerUserId="48685" PostTypeId="1" Score="0" Tags="&lt;anova&gt;&lt;t-test&gt;" Title="Show &quot;Does it affect?&quot; with hypothesis test" ViewCount="27" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am working on the following problem. Given N days of stock returns, I compute the covariance matrix for stocks. I then use Probabilistic PCA to &quot;shrink&quot; the covariance matrix. I am trying different number of principal components: 1, 2, etc. I then compute the log likelihood of observing the next-day's returns under the multivariate normal distribution using the &quot;shrinked&quot; covariance matrix (inverse of it to be precise). Just to make sure, the matrix is non-singular, as in PPCA the &quot;remaining&quot; principal components are not zeroed out, but instead are assumed to be uncorrelated.&lt;/p&gt;&#10;&#10;&lt;p&gt;I run this process over some period of time, to compute the total log-likelihood of the next-day's returns prediction under this method (every day re-estimating the covariance matrix, computing next-day's returns log-likelihood, and adding it up). Just to make sure, the computed log-likelihood is always out-of-sample.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, I would expect that the highest log-likelihood would be for a small number of principal components. However, what I find is that the highest log-likelihood is in fact for the model with 1 principal component, gradually decreasing as more principal components are used.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, this could mean two things:&lt;/p&gt;&#10;&#10;&lt;p&gt;1) The best model is the one using 1 principal component (aka CAPM)&lt;/p&gt;&#10;&#10;&lt;p&gt;2) I need to adjust the computed log-likelihood to make it properly Bayesian. I know that if I would be fitting the model in-sample, then I would need to actually estimate the posterior for the model parameters (including the number of principal components), and then integrate it out. However, because I am calculating log-likelihood out-of-sample, it's in fact something like cross-validation. And so I don't really see the problem with this approach. Are there any problems I might have missed?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-20T08:08:14.263" Id="104098" LastActivityDate="2014-06-20T08:08:14.263" OwnerUserId="48705" PostTypeId="1" Score="1" Tags="&lt;pca&gt;&lt;prediction&gt;&lt;covariance&gt;" Title="Stock Returns Covariance Prediction - Number of Principal Components" ViewCount="48" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;Please explain the difference between a parametric and a non-parametric test.&lt;/p&gt;&#10;&#10;&lt;p&gt;Do all data mining techniques come under the non-parametric category?&lt;/p&gt;&#10;" ClosedDate="2014-06-20T14:01:33.693" CommentCount="1" CreationDate="2014-06-20T10:44:35.630" Id="104110" LastActivityDate="2014-06-20T13:32:36.570" LastEditDate="2014-06-20T11:15:59.687" LastEditorUserId="930" OwnerUserId="48712" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;data-mining&gt;&lt;nonparametric&gt;&lt;parametric&gt;&lt;definition&gt;" Title="What is the difference between a parametric test and non-parametric tests?" ViewCount="4545" />
  
  <row Body="&lt;p&gt;Please refer to the &lt;a href=&quot;http://en.wikipedia.org/wiki/Cross-validation_%28statistics%29#k-fold_cross-validation&quot; rel=&quot;nofollow&quot;&gt;wikipedia page&lt;/a&gt; for the method definitions (they do a far better job than I could do here). &lt;/p&gt;&#10;&#10;&lt;p&gt;After you have had a look at that page, the following may be of help to you. Let me focus on the part of the question where one wants to pick one of these methods for their modeling process. Since this is pretty frequent choice that one makes, and they could benefit from additional knowledge, here is my answer for two situations:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Any situation&lt;/strong&gt;: Use &lt;code&gt;k-fold cross validation&lt;/code&gt; with some suitable number of repeats (say 5 or 10). &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;Splitting the data into 1 half, training on the first half and validating on the other is one step in 2-fold cross validation anyway (the other step being repeating the same exercise with the two halfs interchanged). Hence, rule out 'splitting the data into half' strategy.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Many machine learning and data mining papers use &lt;strong&gt;k-fold cross validation&lt;/strong&gt; (don't have citation), so use it unless you have to be very careful in this step.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Now, leave one out method and other methods like '&lt;strong&gt;leave p out&lt;/strong&gt;' and '&lt;strong&gt;random split and repeat&lt;/strong&gt;' (essentially &lt;strong&gt;bootstrap&lt;/strong&gt; like process described above) are defintely good contenders. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;If your data size is N, then N-fold cross validation is essentially the same as leave one out. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;'leave p out' and 'bootstrap' are a bit more different than k fold cross validation, but the difference is essentially in how folds are defined and the number of repetitions 'k' that happen.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;As the wiki page says, both k-fold and '&lt;strong&gt;leave p out&lt;/strong&gt;' are decent estimators of the '&lt;strong&gt;expected performance/fit&lt;/strong&gt;' (although the bets are off with regards to the variance of these estimators).&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Your situation:&lt;/em&gt;&lt;/strong&gt; You only have a sample size of 200 compared to number of features (100). I think there is a very high chance that there are multiple linear models giving the same performance. &lt;strong&gt;&lt;em&gt;I would suggest using k-fold cross validation with &gt; 10 repeats&lt;/em&gt;&lt;/strong&gt;. Pick a k value of 3 or 5.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;Reason for k value: generic choice. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Reason for repeat value: A decently high value for repetition is probably critical here because the output of a single k-fold cross validation computation may be suceptible to fold splitting variability/randomness that we introduce.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Additional thoughts: &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;Maybe I would also employ '&lt;strong&gt;leave p out&lt;/strong&gt;' and '&lt;strong&gt;bootstrap like random split repeat&lt;/strong&gt;' methods (in addition to k-fold cross validation) for the same performance/fit measure to check if my k-fold cross validation method's outputs look alright.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Although you want to use all the 100 features, as someone suggested, pay attention to &lt;strong&gt;multicollinearity/correlation&lt;/strong&gt; and  maybe reduce the number of features.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="1" CreationDate="2014-06-20T12:43:59.987" Id="104126" LastActivityDate="2014-06-24T15:51:15.970" LastEditDate="2014-06-24T15:51:15.970" LastEditorUserId="19762" OwnerUserId="30815" ParentId="103459" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Assuming you want Gaussians: here is a dataframe in R:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; data.frame(pop1 = rnorm(1000,mean=5.5,sd=0.65), pop2 = rnorm(1000,mean=5.9,sd=0.32), pop3 = rnorm(1000,mean=5.4,sd=0.49))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2014-06-20T12:57:41.540" Id="104128" LastActivityDate="2014-06-20T12:57:41.540" OwnerUserId="30815" ParentId="104103" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="104317" AnswerCount="2" Body="&lt;p&gt;I am running a simulation. One of my parameters is sampled from a normal distribution. I would like to perform a sensitivity analysis using a right skewed distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;This is what I had hoped to do: specify a log normal distribution, with a leftward translation so that it has the same cumulative density below zero as the reference normal, and have the same mean and variance (on the log scale) as the reference normal.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have played around with these parameters. I am stuck.&#10;1) this is even possible?&#10;2) if it is possible, I may need to solve for the parameters numerically... But I am having trouble specifying an equation(s). I am having trouble with the algebra&lt;/p&gt;&#10;&#10;&lt;p&gt;If it is impossible to have the exact same expectation, variance, and cumulative density below zero, I'd be happy with letting the variance be unconsteained.&lt;/p&gt;&#10;&#10;&lt;p&gt;Alternatively, I'd be happy using any right skewed distribution, I only started with the log normal because  I thought it would be simple.&lt;/p&gt;&#10;&#10;&lt;p&gt;I can post the equations I have worked on, but I suspect they are not useful. I've been using the definitions posted at this link (&lt;a href=&quot;http://www.mathworks.com/help/stats/lognpdf.html&quot; rel=&quot;nofollow&quot;&gt;http://www.mathworks.com/help/stats/lognpdf.html&lt;/a&gt;)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-20T14:26:04.277" Id="104140" LastActivityDate="2014-06-24T13:50:01.090" OwnerUserId="18098" PostTypeId="1" Score="1" Tags="&lt;normal-distribution&gt;&lt;simulation&gt;&lt;skewness&gt;&lt;lognormal&gt;&lt;sensitivity-analysis&gt;" Title="Match Right Skewed Distribution to Normal" ViewCount="73" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm analysing data with a nested structure with the lmer-function of the Lme4 package in R. I'm interested in the estimation of the confidence intervals of the random effects (is the score of class1 higher than class2 nested within school A).&lt;/p&gt;&#10;&#10;&lt;p&gt;I found very small standard errors using ranef(mod, postVar=TRUE), that I did not expect. With a quick simulation study I found the following results: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(lme4); library(plyr)&#10;   N &amp;lt;- seq(100,10000,by=500)&#10;se_sim &amp;lt;- se_lmer &amp;lt;- rep(NA,length(N))&#10;&#10;for(i in 1:length(N))&#10;{  &#10;  n = N[i]&#10;  dat &amp;lt;- data.frame(x=c(rnorm(n, 0, 1), rnorm(n, 1, 1)), y=as.factor(rep(1:2, each=n)))&#10;  sum_dat &amp;lt;- ddply(dat, .(y), summarize, m=mean(x), se=sd(x)/sqrt(length(x))) # se = 1/sqrt(1000)&#10;  se_sim[i] &amp;lt;- sum_dat$se[1]&#10;&#10;  mod &amp;lt;- lmer(x ~ 1 + (1|y), data=dat)&#10;  r &amp;lt;- ranef(mod, condVar = TRUE)&#10;  se_lmer[i] &amp;lt;- attr(r[[1]], 'postVar')[1, 1, 1] # ??&#10;}&#10;&#10;plot(N,se_sim, type='b')&#10;plot(N,se_lmer, type='b')&#10;plot(se_lmer, se_sim)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;For n = 1000, the se's equal .03 (as expected, 1/sqrt(n)), but when estimated with the ranef function of lmer I get about .001 (1/n) Hence, the resulting confidence intervals are too small. The plots indicate that the se from lmer need to be squared to get the corresponding estimates.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I'm I missing something here? I could not find any related issues elsewhere.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-06-20T14:31:14.140" FavoriteCount="1" Id="104147" LastActivityDate="2014-06-20T14:31:14.140" OwnerUserId="48724" PostTypeId="1" Score="1" Tags="&lt;confidence-interval&gt;&lt;random-effects-model&gt;&lt;lmer&gt;&lt;lme4&gt;" Title="Recovery of Standard Errors of Random Effects in Lmer" ViewCount="160" />
  <row AnswerCount="2" Body="&lt;p&gt;I'm working on protein multi-classification problem. I'm using libsvm and the edit distance kernel. This kernel depends from a parameter (gamma). I'm able to get the best parameters (gamma and C) through grid search. If i use a kernel that depends from 3 or more parameters, the grid search is computationally heavy, so i'm thinking to approach the problem with an evolution strategy (for instance CMA-ES). Is there a way to do it in libsvm?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-20T15:48:31.233" Id="104158" LastActivityDate="2014-06-25T13:32:42.333" OwnerUserId="40952" PostTypeId="1" Score="0" Tags="&lt;classification&gt;&lt;svm&gt;&lt;optimization&gt;&lt;kernel&gt;&lt;libsvm&gt;" Title="Evolution strategies in libsvm" ViewCount="46" />
  <row AcceptedAnswerId="104462" AnswerCount="1" Body="&lt;p&gt;I need some help understanding the relation between the maximum likelihood and Gaussian mixture models. I have seen that there is a relationship between the expectation maximization algorithms and these two terms, but I could not quite understand the relation well enough. Could some please give me an example of how these two terms are related or different?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-20T20:59:28.520" Id="104183" LastActivityDate="2014-06-23T20:28:46.930" LastEditDate="2014-06-23T19:50:05.417" LastEditorUserId="32036" OwnerUserId="48746" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;maximum-likelihood&gt;&lt;expectation-maximization&gt;&lt;gaussian-mixture&gt;" Title="Relation between Gaussian mixture models and maximum likelihood?" ViewCount="78" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am stuck with the correlated and independent data combined in one study. Here's my dilemma:&lt;/p&gt;&#10;&#10;&lt;p&gt;Say X is a drug(explanatory variable) and Y is a gene expression(response variable). &lt;/p&gt;&#10;&#10;&lt;p&gt;Normally, you would give out drugs to a half of your group(randomly chosen) and placebo to the rest, measure the gene expression for each person in the group, and conduct the differential expression analysis between two conditions using a standard package such as deseq. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, let's say between-subjects variability is huge, so you also measured the gene expression prior to taking the pill as well.  In other words, each subject has two data points(before and after the taking the pill).  How do you incorporate this information into the analysis? I tried to subtract the 'before' value from after 'value' for each patient, and conduct t-test on those statistics between conditions(drug &amp;amp; placebo), but I am not sure if I am doing it right. &lt;/p&gt;&#10;&#10;&lt;p&gt;I am curious if there is a standard, already established way to do this?&#10;You can assume Y follows Poisson or negative binomial.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-20T21:59:48.003" FavoriteCount="0" Id="104192" LastActivityDate="2014-06-20T22:33:35.810" LastEditDate="2014-06-20T22:33:35.810" LastEditorUserId="11013" OwnerUserId="11013" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;multivariate-analysis&gt;&lt;genetics&gt;&lt;correlated-predictors&gt;" Title="How to conduct the two sample test with extra data to eliminate between-subjects variability" ViewCount="15" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I want to find the moment generating function (mfg) and mean deviation of this distribution:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$f(x,\epsilon,k,\theta) = k\theta^{(1+1/k+\epsilon/k)}x^{(k+\epsilon)}\exp{(-\theta x^k )}/(\Gamma(1+(1+\epsilon))/k)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\epsilon, k, \theta$ are the three parameters of this distribution. HERE x is from 0 to infinity.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-06-21T17:17:42.510" Id="104241" LastActivityDate="2014-06-22T01:34:34.737" LastEditDate="2014-06-22T01:34:34.737" LastEditorUserId="48769" OwnerUserId="48769" PostTypeId="1" Score="0" Tags="&lt;pdf&gt;&lt;moments&gt;" Title="Moment generating function of a distribution" ViewCount="64" />
  <row AnswerCount="0" Body="&lt;p&gt;Does anyone know the best way of carrying out a mediation analysis when the IV, mediator and DV have all been measured at two time points? I'm wanting to do this using SEM in AMOs. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-21T17:32:45.283" Id="104243" LastActivityDate="2014-06-21T17:32:45.283" OwnerUserId="46261" PostTypeId="1" Score="0" Tags="&lt;panel-data&gt;&lt;sem&gt;&lt;mediation&gt;&lt;amos&gt;" Title="Mediation analysis with two waves of data in AMOS" ViewCount="76" />
  
  <row Body="&lt;p&gt;Multiple Imputation is never a bad approach. You could also do Full Information Maximum Likelihood. Good review and comparison &lt;a href=&quot;https://www.amstat.org/sections/srms/webinarfiles/ModernMethodWebinarMay2012.pdf&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;http://www.people.ku.edu/~kylelang/content/Graham%20et%20al.%20%282007%29.pdf&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;But if you're going that route, consider using &lt;a href=&quot;http://mc-stan.org/&quot; rel=&quot;nofollow&quot;&gt;Stan&lt;/a&gt; to fit the ML imputation simultaneously with your regression as a single Bayesian model, since &lt;a href=&quot;http://www.stat.ufl.edu/archived/casella/Papers/Lasso.pdf&quot; rel=&quot;nofollow&quot;&gt;LASSO is a special case of Bayesian regression anyway&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-06-21T21:09:53.257" Id="104257" LastActivityDate="2014-06-21T21:09:53.257" OwnerUserId="36229" ParentId="104194" PostTypeId="2" Score="3" />
  
  <row AcceptedAnswerId="104272" AnswerCount="1" Body="&lt;p&gt;I ran across this exercise: &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Let $T$ be a random variable distributed as a $\text{Bernoulli}(p)$, $U$ be a random variable distributed as a $\text{Bernoulli}(q)$ and $W$ be a random variable distributed as a $\text{Poisson}(\lambda)$. Find the probability function of $X =T \cdot U\cdot W\,$ . &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I first calculated:   &lt;/p&gt;&#10;&#10;&lt;p&gt;$ \ \  P(T\cdot U = 1) = P(T=1)\cdot P(U=1) = pq \\ P(T \cdot U = 0) = 1 - P(T\cdot U=1) = 1 - pq $ &lt;/p&gt;&#10;&#10;&lt;p&gt;Then I made a mistake thinking that:&lt;/p&gt;&#10;&#10;&lt;p&gt;$ P(X=0) =P(T\cdot U = 0 , W&amp;gt; 0) +P(T \cdot U = 1, W= 0) $ &lt;/p&gt;&#10;&#10;&lt;p&gt;was the correct formula for the probability of  $ X = 0$, this seemed reasonable and symmetrical to me, my intuition lead me astray.&lt;/p&gt;&#10;&#10;&lt;p&gt;I then thought that the $ P(X = 0) = P(T\cdot U= 0) + P(W = 0)$  &lt;/p&gt;&#10;&#10;&lt;p&gt;For the second time, my intuition lead me astray.&lt;/p&gt;&#10;&#10;&lt;p&gt;The solutions highlight as the correct solution : $P(X= 0) = P(T\cdot U = 0) + P(T\cdot U = 1, W = 0) $&lt;/p&gt;&#10;&#10;&lt;p&gt;Intuitively I explained this result to myself as: if $ P(T\cdot U = 0) $ then I do not need to check the other random variable because anything times 0 is 0. If $ P(T\cdot U = 1) $ then I need the Poisson to be 0 to have $ P(X = 0) $.&lt;/p&gt;&#10;&#10;&lt;p&gt;But because my intuition failed me I would like to understand which probability law the solution is using to solve this exercise, It seems to me not the Total Probability law or any form of Bayes rule. A Formal proof of the equality being used would be helpful to me. Also If you want to point out in witch way my first two attempts where wrong I would be grateful.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-06-22T02:15:16.750" Id="104271" LastActivityDate="2014-06-22T02:56:33.270" LastEditDate="2014-06-22T02:30:09.180" LastEditorUserId="805" OwnerUserId="48259" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;self-study&gt;&lt;random-variable&gt;&lt;pdf&gt;" Title="Probability function of three Random Variables multiplied, solidifying intuition" ViewCount="83" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I am looking for publicly accessible data sets to explore some wisdom of the crowd ideas. &lt;/p&gt;&#10;&#10;&lt;p&gt;An ideal data set would contain predictions from a number of individuals (who were not collaborating with each other, or at least only minimally) on events with known outcomes.&lt;/p&gt;&#10;&#10;&lt;p&gt;Predictions in terms of probability of events occurring (e.g., 80% chance of depression in the US in 20xx), binary-valued predictions (e.g., there will be a depression in the US in 20xx), or real values of quantities (e.g., average number of goals per match in World Cup 2014 is 2.75) are all okay. These predictions need to be paired with their corresponding objective ground truth, so that I can evaluate how different methods of aggregating data can produce estimates that are the closest to the ground truth.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any domain are okay. This includes but not limited to politics, economy, business, military, sports.&lt;/p&gt;&#10;&#10;&lt;p&gt;Missing data (some predictors did not make predictions for all questions) are okay.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'd like the data to be readily downloadable (e.g., I don't want to write code to scrap a web site).&lt;/p&gt;&#10;" ClosedDate="2014-08-29T10:35:11.030" CommentCount="0" CommunityOwnedDate="2014-08-29T04:17:37.607" CreationDate="2014-06-22T16:53:02.927" Id="104298" LastActivityDate="2014-08-28T22:54:28.100" LastEditDate="2014-06-23T08:32:32.527" LastEditorUserId="17186" OwnerUserId="17186" PostTypeId="1" Score="2" Tags="&lt;data-mining&gt;&lt;dataset&gt;" Title="Are there any good publicly accessible wisdom of crowd data sets?" ViewCount="67" />
  <row Body="&lt;p&gt;Prediction markets such as &lt;a href=&quot;http://tippie.uiowa.edu/iem/&quot; rel=&quot;nofollow&quot;&gt;IEM&lt;/a&gt;, &lt;a href=&quot;http://tradesports.com/&quot; rel=&quot;nofollow&quot;&gt;Tradesports&lt;/a&gt; (and its defunct parent &lt;a href=&quot;https://www.intrade.com/v4/home/&quot; rel=&quot;nofollow&quot;&gt;InTrade&lt;/a&gt;), &lt;a href=&quot;http://home.inklingmarkets.com/&quot; rel=&quot;nofollow&quot;&gt;Inkling&lt;/a&gt;, and &lt;a href=&quot;https://www.ipredict.co.nz/&quot; rel=&quot;nofollow&quot;&gt;iPredict&lt;/a&gt; might be good places to start. Some price histories are available for IEM &lt;a href=&quot;https://iemweb.biz.uiowa.edu/WebEx/marketoverview.cfm&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="1" CommunityOwnedDate="2014-08-29T04:17:37.607" CreationDate="2014-06-22T17:32:24.423" Id="104302" LastActivityDate="2014-06-22T17:32:24.423" OwnerUserId="36229" ParentId="104298" PostTypeId="2" Score="0" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I am trying to find a statistical or machine learning tool that replicates this analysis I am doing manually in excel.&lt;/p&gt;&#10;&#10;&lt;p&gt;Each row in my data set is a user.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;User | sites visits | pages viewed | items added to cart | Purchased (1 or 0)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I set up a pivot table so Purchased is in the columns field.  I have it set to % of row.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Purchased   Didn't Purchase         Total&#10; 80%              20%                100%&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I then add all the other variables as filters in the Pivot Table.&lt;/p&gt;&#10;&#10;&lt;p&gt;I start by filtering out the people who didn't view 1 page and see how that impacts that % of people who purchased.  I then filter out the people who didn't view 2 pages, etc. I then start filtering out other variables to see how the interactions work together.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am looking for a combination of inputs that leads to an 80% purchase rate.  So maybe it's 2 visits, 6 pageviews, and 2 add to carts.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a tool that automates this type of analysis? &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-06-22T21:32:25.970" Id="104319" LastActivityDate="2014-06-22T23:20:00.870" LastEditDate="2014-06-22T22:26:49.673" LastEditorUserId="805" OwnerUserId="46489" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;machine-learning&gt;&lt;logistic&gt;" Title="Is there a stats tool for this analysis I run in excel?" ViewCount="34" />
  <row Body="&lt;p&gt;I think it would be easier with a small (toy) example, fully worked; it might be possible to show you what can be done in other things. That is if under your headings, you have maybe 15 or so lines of data values (indent them all 4 spaces), and then how you get the 80% (or whatever value it might be); and give the corresponding value for that data, and then the solution (combination) you'd produce using Excel. Then we might be able to show you how to reproduce that automatically.&lt;/p&gt;&#10;&#10;&lt;p&gt;I believe one approach would be to fit a model (e.g. an ANOVA-like model in R) that fits the % at the combinations of values - a saturated model for the variables not aggregated over (so it exactly reproduces the observed values); you can then find and extract the fitted combinations that come closest to 80% (or whatever). I imagine the whole thing would be a few lines of R that could be made into a function that would load your data, run the analysis and produce as output the required combination or combinations.&lt;/p&gt;&#10;&#10;&lt;p&gt;While I don't think you need to produce a pivot table to do any part of that, the equivalent of pivot tables themselves can easily be done in vanilla R, or in several R packages. Two answers to &lt;a href=&quot;http://stackoverflow.com/questions/11597542/r-how-to-create-pivot-table-like-data-frame-while-3-variables-are-involved&quot;&gt;this question&lt;/a&gt; shows how to do a pivot table in R via &lt;code&gt;tapply&lt;/code&gt; (plain R) or with &lt;code&gt;reshape2&lt;/code&gt;; I think several other functions in base R and in popular R packages have the same functionality (e.g. see &lt;a href=&quot;http://stackoverflow.com/questions/11627754/r-summation-pivot-table&quot;&gt;here&lt;/a&gt;, or &lt;a href=&quot;http://digitheadslabnotebook.blogspot.com.au/2010/01/pivot-tables-in-r.html&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;http://marcoghislanzoni.com/blog/2013/10/11/pivot-tables-in-r-with-melt-and-cast/&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;http://news.mrdwab.com/2010/08/08/using-the-reshape-packagein-r/&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;. Searches in &lt;a href=&quot;http://stackoverflow.com/search?q=pivot+table+%5br%5d&quot;&gt;StackOverflow&lt;/a&gt; or in &lt;a href=&quot;https://www.google.com/search?q=pivot+table+in+r&quot; rel=&quot;nofollow&quot;&gt;google&lt;/a&gt; provide many more examples.&lt;/p&gt;&#10;&#10;&lt;p&gt;Lots of programs other than R could do exactly the same things I discuss here.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-22T23:05:04.243" Id="104325" LastActivityDate="2014-06-22T23:20:00.870" LastEditDate="2014-06-22T23:20:00.870" LastEditorUserId="805" OwnerUserId="805" ParentId="104319" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="104332" AnswerCount="1" Body="&lt;p&gt;I have a dataset of ~400 subjects. For each subject, I have about 18 variables (V1...V18), all of those continuous and measured at the same timepoint.&lt;/p&gt;&#10;&#10;&lt;p&gt;A paper I'm working on would benefit from showing that one of those variables (say V1) has a significant correlation with one of three other variables (say V2, V3, or V4) – all of these are measuring different aspects of the same illness. And I have found some significant correlations between the pairs (V1–V2, V1–V3, V1–V4). However, I have another variable, say V5, that is more strongly correlated to all of them (V1–V4). When I correct (linearly) for V5, the correlations between V1–(V2/V3/V4) is no longer significant/there.&lt;/p&gt;&#10;&#10;&lt;p&gt;So at this point, any critic could say that the V1 is correlated to V5, V5 is correlated to V2/3/4, and that I can't make the point I want to make. However, I have reason to believe that subgroups of the subjects (more susceptible to the illness), will show strong correlation between V1 and V2–4 that is independent of V5. &lt;em&gt;Is there any way to find these subgroups?&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to find some partition(s), A &amp;lt; Vx &amp;lt; B, in which there is a significant correlation between V1 and V2/V3/V4, that I can show is independent of V5. Alternatively, if there's a way to quantify how much of the correlation between V1 and V2–4 is due to correlation between V1–V5 and how much is independent, that would be a good solution as well.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have been reading a lot about two-way ANOVA, thinking about doing it between V1 as rows, V5 as columns, and one of V2–4 as outcome. But even if I do this (which would take significant effort, since I couldn't find a simple Python implementation), I'm not sure if it would advance me towards my goals. Any help is appreciated.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-06-22T23:36:48.170" Id="104328" LastActivityDate="2014-06-23T17:52:43.953" LastEditDate="2014-06-22T23:58:22.887" LastEditorUserId="32036" OwnerUserId="48581" PostTypeId="1" Score="2" Tags="&lt;correlation&gt;&lt;multivariate-analysis&gt;&lt;cross-correlation&gt;" Title="Finding a subset of the data in which two variables are independent" ViewCount="87" />
  <row AcceptedAnswerId="104389" AnswerCount="2" Body="&lt;p&gt;I have been reading &lt;a href=&quot;http://faculty.sites.uci.edu/mdlee/bgm/&quot; rel=&quot;nofollow&quot;&gt;a book&lt;/a&gt; that cites an example where a uniform distribution is the initial prior, and then a person scores 9/10 on a test. Then the resulting posterior becomes the prior distribution. &lt;/p&gt;&#10;&#10;&lt;p&gt;The book provides the following explanation for what happens, but despite consulting the Wikipedia page on integration I am struggling to understand exactly what is going on, and why this process is necessary.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/STt6A.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-23T01:52:00.113" FavoriteCount="3" Id="104333" LastActivityDate="2014-06-23T12:25:13.853" LastEditDate="2014-06-23T02:02:31.587" LastEditorUserId="45005" OwnerUserId="45005" PostTypeId="1" Score="3" Tags="&lt;bayesian&gt;&lt;prior&gt;&lt;posterior&gt;" Title="What does it mean to integrate over the posterior?" ViewCount="200" />
  
  <row Body="&lt;p&gt;I know this is an old question, but it's relatively popular and has a simple answer, so hopefully it'll be helpful to others in the future.  For a more in-depth take, take a look at Christoph Lippert's course on Linear Mixed Models which examines them in the context of genome-wide association studies &lt;a href=&quot;http://www.mixed-models.org/lectures-ucla/winter-2014-current-topics-in-computational-biology/&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;. In particular see &lt;a href=&quot;http://www.mixed-models.org/wp-content/uploads/2014/01/5_LinearMixedModels.pdf&quot; rel=&quot;nofollow&quot;&gt;Lecture 5&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;The reason that the mixed model works so much better is that it's designed to take into account exactly what you're trying to control for: population structure. The &quot;populations&quot; in your study are the different sites using, for example, slightly different but consistent implementations of the same protocol.  Also, if the subjects of your study are people, people pooled from different sites are less likely to be related than people from the same site, so blood-relatedness may play a role as well.&lt;/p&gt;&#10;&#10;&lt;p&gt;As opposed to the standard maximum-likelihood linear model where we have $\mathcal{N}(Y|X\beta,\sigma^2) $, linear mixed models add in an additional matrix called the kernel matrix $K$, which estimates the similarity between individuals, and fits the &quot;random effects&quot; so that similar individuals will have similar random effects. This gives rise to the model $\mathcal{N}(Y|X\beta + Zu,\sigma^2I + \sigma_g^2K)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Because you are trying to control for population structure explicitly, it's therefore no surprise that the linear mixed model outperformed other regression techniques.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-23T04:23:55.957" Id="104340" LastActivityDate="2014-06-23T15:14:39.307" LastEditDate="2014-06-23T15:14:39.307" LastEditorUserId="20775" OwnerUserId="20775" ParentId="35414" PostTypeId="2" Score="3" />
  <row AnswerCount="0" Body="&lt;p&gt;My question is specific to transportation modelling using a nested logit (NL) model. &#10;I wonder how to make a program including a &lt;em&gt;t&lt;/em&gt;-parameter, &lt;em&gt;t&lt;/em&gt;-value, and likelihood in R. &#10;I did estimation in a multinomial logit (MNL) model, and now I am planning to advance it using NL. Can anybody help do this?&lt;/p&gt;&#10;&#10;&lt;p&gt;Notes for my previous estimation:&lt;br&gt;&#10;We have revealed preference (RP) data about mode choice. The modes available are car, minibus, and motorbike. In the survey we only ask the respondent's current mode and alternative mode that they use for commuting. Say we have 400 samples. Because each respondent has a different combination of current mode and alternative mode, we estimate modal choice by employing a dummy mode and using a MNL model instead of binary logit. For this reason the &lt;code&gt;glm&lt;/code&gt; package doesn't work in this case.&lt;/p&gt;&#10;&#10;&lt;p&gt;In stated preference (SP) data for train, we have 5 experiments for each respondent, thus we have 2000 data. Then we combine RP data and the SP model into a &quot;combined RPSP model&quot; by using the additional variable &lt;em&gt;µ&lt;/em&gt; (scale parameter). Variables considered are travel cost, travel time, frequency and income.&lt;/p&gt;&#10;&#10;&lt;p&gt;I don't think the &lt;code&gt;glm&lt;/code&gt; package for NL works in this case. I'm planning to divide the modes into 2 nests: public transport (minibus and train) and private vehicle (car and motorbike), and of course with additional variable &lt;em&gt;λ&lt;/em&gt; (scale parameter) for the nest.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-23T05:05:16.313" FavoriteCount="1" Id="104343" LastActivityDate="2014-06-23T05:37:20.113" LastEditDate="2014-06-23T05:37:20.113" LastEditorUserId="48814" OwnerUserId="48814" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;modeling&gt;&lt;nested&gt;" Title="Nested logit program in R" ViewCount="53" />
  
  <row Body="&lt;p&gt;Intuitively, it should be obvious that a point whose coordinates are sampled at random from the uniform distribution should have small modulus due to the curse of dimensionality. As $d$ increases, the probability that a point sampled at random from the volume of the $d$-dimensional unit ball will have distance less than or equal to $\epsilon$ from the center is $\epsilon^{d}$, which drops exponentially fast.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'll give the full version of cardinal's solution.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $X_i$ be one independent copy of a discrete, uniform distribution over the integers $-n \leqslant k \leqslant n$.  Clearly, $\mathbb{E}[X] = 0$, and it is &lt;a href=&quot;http://www.wolframalpha.com/input/?i=1%2F%282n%2B1%29+%2a+sum+i+%3D+-n...n+%28i+-+0%29%5E2&quot; rel=&quot;nofollow&quot;&gt;easily computed&lt;/a&gt; that $\text{Var}(X_i) = \frac{n(n+1)}{3}$&lt;/p&gt;&#10;&#10;&lt;p&gt;Recall that $\mathbb{E}[X_i^2] = \text{Var}(X_i) + \mathbb{E}[X_i]^2$ and that $\text{Var}(X_i^2) =  \mathbb{E}[X_i^4] - \mathbb{E}[X_i^2]^2$&lt;/p&gt;&#10;&#10;&lt;p&gt;Thus, $\mathbb{E}[X_i^2] = \text{Var}(X_i) = \frac{n(n+1)}{3}$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\text{Var}(X_i^2) =  \mathbb{E}[X_i^4] - \mathbb{E}[X_i^2]^2 = \frac{n(n+1)(3n^2 + 3n + 1)}{15} - \left( \frac{n(n+1)}{3} \right)^2$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.wolframalpha.com/input/?i=1/%282n%2b1%29%20%2a%20sum%20i%20=%20-n...n%20%28i%20-%200%29%5E4&quot; rel=&quot;nofollow&quot;&gt;$\mathbb{E}[X_i^4]$ computation&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $Y_i = X_i^2$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\sum_{i=1}^d Y_i = (\text{Distance of Randomly Sampled Point to Origin})^2$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I'll finish this tomorrow, but you can see that this variable has a mean of about $\frac{n^2}{3}$, while less than $2^{-d}$ fraction of points have distances less than half the maximum distance $\frac{dn^2}{2}$ &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-23T05:59:34.950" Id="104349" LastActivityDate="2014-06-23T05:59:34.950" OwnerUserId="20775" ParentId="61612" PostTypeId="2" Score="0" />
  <row AnswerCount="1" Body="&lt;p&gt;I have many point clouds of a small size (say &amp;lt;200 points) in 2D. Some of them are isotropic and can be modelled as a single point. Others are elongated and curved so that they can reasonably be fit to the apex of a parabola. The dispersion seems to be of the same order for all clouds and there can be a fraction of outliers (say 10%).&lt;/p&gt;&#10;&#10;&lt;p&gt;Here are four samples, of diminishing size. The bottom right is an isotropic case.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/0ckvy.png&quot; alt=&quot;enter image description here&quot;&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/ilkCN.png&quot; alt=&quot;enter image description here&quot;&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/v0sYg.png&quot; alt=&quot;enter image description here&quot;&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/N8Euq.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I am trying to distinguish the circular clouds from the parabolic ones. I can probably use the regression coefficient, but I am looking for alternatives, with two purposes:&lt;/p&gt;&#10;&#10;&lt;p&gt;1) have a low computational cost (as the test has to be repeated hundred thousand times), and &lt;/p&gt;&#10;&#10;&lt;p&gt;2) go as far as possible as regards the classification of banana vs. &quot;cherry&quot; for small clouds or for larger dispersion, i.e. distinguishing cases like 3 and 4.&lt;/p&gt;&#10;&#10;&lt;p&gt;The method can require the knowledge of the dispersion, but it is better if not.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-06-23T08:19:18.880" Id="104357" LastActivityDate="2014-06-23T09:04:14.923" LastEditDate="2014-06-23T09:04:14.923" LastEditorUserId="22047" OwnerUserId="37306" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;&lt;pattern-recognition&gt;" Title="Testing a 2D point cloud for banana shape" ViewCount="94" />
  
  <row AcceptedAnswerId="104376" AnswerCount="2" Body="&lt;p&gt;Given are $Z_1, Z_2$ i.i.d. standard normal.&lt;/p&gt;&#10;&#10;&lt;p&gt;Find&lt;/p&gt;&#10;&#10;&lt;p&gt;$P[Z_1 &amp;lt; t &amp;lt; Z_2]$&lt;/p&gt;&#10;&#10;&lt;p&gt;I have difficulties with working out how I should split the condition.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is $P[Z_1 &amp;lt; t &amp;lt; Z_2] = P[Z_1 &amp;lt; t, t &amp;lt; Z_2]$ or is some additional condition required?&lt;/p&gt;&#10;&#10;&lt;h2&gt;My question&lt;/h2&gt;&#10;&#10;&lt;p&gt;How should I approach this problem?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-06-23T09:10:42.713" FavoriteCount="0" Id="104363" LastActivityDate="2014-06-23T13:05:56.033" OwnerUserId="44086" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;conditional-probability&gt;" Title="Probability of x between two random variables" ViewCount="245" />
  <row AnswerCount="0" Body="&lt;p&gt;I am running some Poisson (or Neg.Binomial depending on overdispersion) models and i want to check for residual autocorrelation due to the nature of the data (monthly cases).&lt;/p&gt;&#10;&#10;&lt;p&gt;I am using R and i am producing correlograms with the use of the &quot;acf&quot; command, where i can check if the autocorrelation for the different lags falls outside the produced confidence intervals. &#10;On the other hand based on published papers that have done similar studies to mine i see that they have used the Durbin-Watson test to check for presense of first or second order autocorrelation which is of interest here.&#10;In R i have found two commands for this test: the &quot;dwtest&quot; in package lmtest (based on what is called the pan algorithm) and &quot;durbinWatsonTest&quot; in package car which uses bootstrap.&#10;In both these tests the documentation suggests they are used for lm objects which is not the case for my models (nor the published ones).&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is: Is the D-W test valid to use for residual autocorrelation in Poisson (Neg.Bin) models? Is there some other test for these models?&#10;If the results between the &quot;acf&quot; and the two D-W tests matched i would feel more comfortable but they don't and to my understanding the &quot;acf&quot; is more reliable. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-23T09:19:41.900" Id="104365" LastActivityDate="2014-06-23T09:19:41.900" OwnerUserId="25032" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;autocorrelation&gt;&lt;poisson-regression&gt;" Title="Residual autocorrelation in Poisson (Neg.Bin) models - Durbin Watson test" ViewCount="140" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm trying to train a randomForest model in R in a 500k+ row dataset. So far so good, but now I'm trying to include factors person_id and company_id (non-unique) which both have a huge amount of levels (in the thousands, many more than R can handle). &lt;/p&gt;&#10;&#10;&lt;p&gt;There is an overlap between training and test set for these attributes of 40% and 70%, so I'm thinking these variables could be useful. &lt;/p&gt;&#10;&#10;&lt;p&gt;I'm trying to classify succes or not (0,1) and have therefore generated new features like frequency of the id's in training set and percentage of success per id in training set. I merged these attributes into the test set. &lt;/p&gt;&#10;&#10;&lt;p&gt;I replaced all the &quot;new&quot; ID's in the testSet by either 0's or 1's. After training with the new variables, it results in a huge overfit (due to the new unknown ID's). &lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a way I could benefit from the attributes which I think could be valuable. &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-06-23T10:26:44.333" FavoriteCount="1" Id="104374" LastActivityDate="2014-06-23T10:26:44.333" OwnerUserId="48825" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;random-forest&gt;&lt;overfitting&gt;" Title="RandomForest: how to use a person_id and company_id attribute in machine learning" ViewCount="43" />
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;Can regression be used for outlier detection. &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Yes. This answer and Glen_b's answer address this.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The primary aim here is not to fit a regression model but &lt;em&gt;find out out liers using regression&lt;/em&gt;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Building on Roman Lustrik's comment, here is a heuristic to find outliers using (multiple linear) regression.&lt;/p&gt;&#10;&#10;&lt;p&gt;Lets say you have sample size $n$. Then, do the following:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Fit a regression model on the $n$ examples. Note down its &lt;a href=&quot;http://en.wikipedia.org/wiki/Ordinary_least_squares#Estimation&quot; rel=&quot;nofollow&quot;&gt;residual&#10;sum of squares&#10;error&lt;/a&gt;&#10;$r_{total}$.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;For each sample i, fit a regression model on the n-1 examples&#10;(excluding example i) and note down the corresponding residual sum&#10;of squares error $r_i$.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Now, compare $r_i$ with $r_tot$ for each $i$, if $r_i &amp;lt;&amp;lt; r_{total}$,&#10;then $i$ is a candidate outlier.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Setting these candidate outlier points aside, we can repeat the whole exercise again with the reduced sample. In the algorithm, we are picking examples in the data which are influencing the regression fit in a bad way (which is one way to label an example as an outlier).&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-06-23T11:39:09.603" Id="104379" LastActivityDate="2014-06-23T11:39:09.603" OwnerUserId="30815" ParentId="104348" PostTypeId="2" Score="-2" />
  <row Body="&lt;p&gt;Yes it makes sense, however frequencies of single words may lead to trivial topics. You need to do some kind of normalisation by using TF-IDF and finding the most informative word(s) in the cluster (IDFs should be computed on the whole corpus). In other words you want the most frequent word(s) in the cluster that are in-frequent in other clusters.&lt;/p&gt;&#10;&#10;&lt;p&gt;It would be better to go for sets of 2-3 consecutive words (easily discoverable by running some adaptation of a-priori on each cluster). Also search for meme tracking algorithms as an alternative option.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-23T12:02:43.317" Id="104387" LastActivityDate="2014-06-23T12:02:43.317" OwnerUserId="37188" ParentId="104385" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;I have two different regression models which I learned from two different data sets.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there any statistical method which shows the significance of models based on the number of parameters and cross validation errors?&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem is that I used two different data sets and I worry that it might not make sense to compare two different models from two different data sets.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-23T14:01:51.563" FavoriteCount="1" Id="104401" LastActivityDate="2014-06-24T01:30:22.867" LastEditDate="2014-06-24T01:30:22.867" LastEditorUserId="805" OwnerUserId="32397" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;machine-learning&gt;&lt;statistical-significance&gt;&lt;model-selection&gt;" Title="How to compare the significance of two models from two different datasets?" ViewCount="73" />
  <row AnswerCount="0" Body="&lt;p&gt;For 50 patients, 2 measurements were done: 1 on a phantom, 1 on a reference point. Ideally, the difference between the measurements would be zero. I want to test whether the difference is significant. &lt;/p&gt;&#10;&#10;&lt;p&gt;Can anybody help me with this problem? Statistics isn't my strongest side. I hope I stated the situation well. &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-06-23T14:17:14.817" Id="104405" LastActivityDate="2014-06-23T14:17:14.817" OwnerUserId="48833" PostTypeId="1" Score="0" Tags="&lt;statistical-significance&gt;" Title="Is the difference between phantom and reference significant?" ViewCount="8" />
  <row Body="&lt;p&gt;In my view, everything boils down to assumptions, that is, how well the models fits them. If it does agree with all of them, treat the p-value as a probability. Then you can compare p-value of 0.06 with 0.99 by concluding which of the two are more likely. Also, a lot depend on circumstances: in some cases, marginal significance shouldn't be ignored, because as you stated, rejection region can be set rather arbitrarily. But if the models satisfies the assumptions, then you should not seek to reject your hypothesis to some arbitrary level but rather investigate, how likely is the outcome that you got. &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-06-23T14:34:09.000" Id="104407" LastActivityDate="2014-06-23T14:34:09.000" OwnerUserId="30586" ParentId="104403" PostTypeId="2" Score="0" />
  
  
  
  
  <row Body="&lt;p&gt;You are making 3 comparisons instead of one (multiple testing), so you have to account for it in computing you confidence intervals.&lt;/p&gt;&#10;&#10;&lt;p&gt;Instead of 5% as an acceptable risk, you have to take a lower value like 5%/3=1,66% (Bonferonni correction). In that case you would calculate the 98.34% confidence interval.&#10;That way, the global risk to make a wrong conclusion rejecting HO (no difference) across the 3 comparison is truly 5%.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-06-23T16:48:41.697" Id="104431" LastActivityDate="2014-06-24T16:18:22.983" LastEditDate="2014-06-24T16:18:22.983" LastEditorUserId="45605" OwnerUserId="45605" ParentId="104427" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;In this &lt;a href=&quot;http://www.jseepub.com/EN/article/downloadArticleFile.do?attachType=PDF&amp;amp;id=9027&quot; rel=&quot;nofollow&quot;&gt;paper&lt;/a&gt; on particle filtering with gradient descent, the authors sample &lt;em&gt;X&lt;/em&gt;&lt;sub&gt;&lt;em&gt;k&lt;/em&gt;+1&lt;/sub&gt; through gradient descent, then update the covariance matrix P associated with &lt;em&gt;X&lt;/em&gt;&lt;sub&gt;k+1&lt;/sub&gt; as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;P&lt;sup&gt;i+1&lt;/sup&gt;&lt;sub&gt;(k + 1|k + 1)&lt;/sub&gt; = P&lt;sup&gt;i&lt;/sup&gt;&lt;sub&gt;(k + 1|k)&lt;/sub&gt;−&#10;P&lt;sup&gt;i&lt;/sup&gt;&lt;sub&gt;(k + 1|k)&lt;/sub&gt;[H&lt;sup&gt;i+1&lt;/sup&gt;&lt;sub&gt;x&lt;/sub&gt;&lt;sub&gt;(k + 1)&lt;/sub&gt; P&lt;sup&gt;i&lt;/sup&gt;&lt;sub&gt;(k + 1|k)&lt;/sub&gt; ·&#10;(H&lt;sup&gt;i+1&lt;/sup&gt;&lt;sub&gt;x&lt;/sub&gt;&lt;sub&gt;(k + 1)&lt;/sub&gt;)' +R&lt;sub&gt;(k + 1)&lt;/sub&gt;] H&lt;sup&gt;i+1&lt;/sup&gt;&lt;sub&gt;x&lt;/sub&gt;&lt;sub&gt;(k + 1)&lt;/sub&gt; P&lt;sup&gt;i&lt;/sup&gt;&lt;sub&gt;(k + 1|k)&lt;/sub&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;where &lt;em&gt;H&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;+1&lt;/sup&gt;&lt;sub&gt;&lt;em&gt;x&lt;/em&gt;&lt;/sub&gt;&lt;sub&gt;(&lt;em&gt;k&lt;/em&gt; + 1)&lt;/sub&gt; = &lt;em&gt;h&lt;sub&gt;x&lt;/sub&gt;&lt;/em&gt;[&lt;em&gt;k&lt;/em&gt; + 1, &lt;em&gt;x&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;+1&lt;/sup&gt;(&lt;em&gt;k&lt;/em&gt; + 1)] &lt;br/&gt;&#10;and &lt;em&gt;h&lt;sub&gt;x&lt;/sub&gt;&lt;/em&gt; is the Jacobian matrix of transition function &lt;em&gt;h&lt;/em&gt;(·).[Equns 13/14 of cited paper]&lt;/p&gt;&#10;&#10;&lt;p&gt;To paraphrase, I think they mean if &lt;em&gt;P&lt;/em&gt; is the covariance of state &lt;em&gt;X&lt;/em&gt; of dimensions (dimx,dimx), &lt;em&gt;R&lt;/em&gt; is the covariance of the observed &lt;em&gt;Y&lt;/em&gt; of dimensions (&lt;code&gt;dimy&lt;/code&gt;, &lt;code&gt;dimy&lt;/code&gt;), and &lt;em&gt;H&lt;/em&gt; is the gradients of &lt;em&gt;Y&lt;/em&gt; w/r/t &lt;em&gt;X&lt;/em&gt; (at &lt;em&gt;x&lt;/em&gt;), of dimensions (&lt;code&gt;dimy&lt;/code&gt;, &lt;code&gt;dimx&lt;/code&gt;) then &lt;br/&gt;&lt;br/&gt;&#10;  $P = P - P\cdot(H\cdot P\cdot H' +R)\cdot H\cdot P$&#10;&lt;br/&gt;&lt;br/&gt;&#10;However, if I set &lt;code&gt;P = rand(5,5); R= rand(2,2); H = rand(2,5)&lt;/code&gt;; I get an inner matrix multiplication dimensions error evaluating the above. My question is: &lt;br/&gt;&#10;&lt;ul&gt;&#10;&lt;li&gt; is the &quot;Jacobian matrix of transition function &lt;em&gt;h&lt;/em&gt;&quot; really the dh/dx matrix of dimensions (&lt;em&gt;y,x&lt;/em&gt;) ?  If so how do I calculate this formula so the dimensions match? &lt;br/&gt;&lt;br/&gt;&#10;&lt;li&gt; if not what is the correct interpretation, or if the formula is incorrect, how can I best calculate the posterior covariance for each particle?&lt;br&gt;&#10;&lt;/ul&gt; Thank you!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-23T18:54:29.153" Id="104447" LastActivityDate="2014-06-23T19:58:15.933" LastEditDate="2014-06-23T19:58:15.933" LastEditorUserId="48850" OwnerUserId="48850" PostTypeId="1" Score="1" Tags="&lt;covariance&gt;&lt;mcmc&gt;&lt;posterior&gt;&lt;gradient-descent&gt;&lt;particle-filter&gt;" Title="Covariance update from Jacobian of transition function" ViewCount="45" />
  <row Body="&lt;p&gt;LOF uses k-distance neighborhoods.&lt;/p&gt;&#10;&#10;&lt;p&gt;Doing clustering to detect outliers has been attempted several times, but none of these methods seems to be very popular; definitely not as popular as LOF.&lt;/p&gt;&#10;&#10;&lt;p&gt;The reason may be that &lt;strong&gt;outliers can make clustering harder&lt;/strong&gt;; so you may also want to do the opposite: first remove local outliers, &lt;em&gt;then&lt;/em&gt; cluster.&lt;/p&gt;&#10;&#10;&lt;p&gt;The clustering method DBSCAN also has a notion of &quot;noise&quot;, so it &lt;em&gt;does&lt;/em&gt; also &quot;detect&quot; density outliers. But then it does not make sense to run LOF anymore, when DBSCAN already flagged &quot;outliers&quot;... and other clustering methods such as k-means are sensitive to outliers, so you can't use them well either.&lt;/p&gt;&#10;&#10;&lt;p&gt;It seems that LOF and similar methods yield higher quality outliers than clustering based methods.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-23T19:23:51.267" Id="104452" LastActivityDate="2014-06-23T19:23:51.267" OwnerUserId="7828" ParentId="104373" PostTypeId="2" Score="0" />
  <row AnswerCount="1" Body="&lt;p&gt;I have a binomial variable that I regress against different categorical variables which I have contrasted to build a reference of an individual Female, Married, aged 35-45, High education :&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Call:&#10;glm(formula = wpti ~ gender.f + is.married.f + age.f + edu.f, &#10;family = binomial(link = &quot;logit&quot;))&#10;&#10;Coefficients:&#10;                   Estimate Std. Error z value Pr(&amp;gt;|z|)    &#10;(Intercept)        -1.303107   0.046522 -28.010  &amp;lt; 2e-16 ***&#10;gender.fMale       -0.537958   0.036833 -14.605  &amp;lt; 2e-16 ***&#10;is.married.fSingle  0.012196   0.040584   0.301 0.763792    &#10;age.f&amp;lt;25           -0.298081   0.078040  -3.820 0.000134 ***&#10;age.f25-35         -0.121670   0.051283  -2.373 0.017667 *  &#10;age.f45-55         -0.006033   0.049078  -0.123 0.902162    &#10;age.f&amp;gt;55            0.239855   0.058727   4.084 4.42e-05 ***&#10;edu.fLow           -0.340115   0.049879  -6.819 9.18e-12 ***&#10;edu.fMedium        -0.298925   0.041729  -7.163 7.86e-13 ***&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I was expecting to see all the coefficients (and not only the one for gender) to change if I contrast the gender factor with Female as reference, but they all remain the same, as if being a man or woman has no effect. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Call:&#10;glm(formula = wpti ~ gender.f + is.married.f + age.f + edu.f, &#10;family = binomial(link = &quot;logit&quot;)) &#10;&#10;Coefficients:&#10;                Estimate Std. Error z value Pr(&amp;gt;|z|)    &#10;(Intercept)        -1.841064   0.048257 -38.151  &amp;lt; 2e-16 ***&#10;gender.fFemale      0.537958   0.036833  14.605  &amp;lt; 2e-16 ***&#10;is.married.fSingle  0.012196   0.040584   0.301 0.763792    &#10;age.f&amp;lt;25           -0.298081   0.078040  -3.820 0.000134 ***&#10;age.f25-35         -0.121670   0.051283  -2.373 0.017667 *  &#10;age.f45-55         -0.006033   0.049078  -0.123 0.902162    &#10;age.f&amp;gt;55            0.239855   0.058727   4.084 4.42e-05 ***&#10;edu.fLow           -0.340115   0.049879  -6.819 9.18e-12 ***&#10;edu.fMedium        -0.298925   0.041729  -7.163 7.86e-13 ***&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;However, if I remove the gender in the regression, the coefficients for the other variables are different : &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Call:&#10;glm(formula = wpti ~ is.married.f + age.f + edu.f, family = binomial(link = &quot;logit&quot;))&#10;&#10;Coefficients:&#10;                    Estimate Std. Error z value Pr(&amp;gt;|z|)    &#10;(Intercept)        -1.562816   0.043369 -36.035  &amp;lt; 2e-16 ***&#10;is.married.fSingle  0.071780   0.040378   1.778 0.075455 .  &#10;age.f&amp;lt;25           -0.330773   0.077982  -4.242 2.22e-05 ***&#10;age.f25-35         -0.134088   0.051091  -2.625 0.008677 ** &#10;age.f45-55          0.004803   0.048860   0.098 0.921694    &#10;age.f&amp;gt;55            0.224112   0.058419   3.836 0.000125 ***&#10;edu.fLow           -0.411232   0.049424  -8.321  &amp;lt; 2e-16 ***&#10;edu.fMedium        -0.332353   0.041488  -8.011 1.14e-15 ***&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So I am confused : I was under the impression that the coefficients could show the difference of likeliness of having a result positive or negative if only ONE variable would vary. Now I dont know what to think. Can somebody explain how to interpret the results, and why it is different if I remove one categorical variable (eg. gender here) althought it seems it has no effect on the others ?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-06-23T20:24:19.513" Id="104460" LastActivityDate="2014-11-21T08:05:35.543" OwnerUserId="48860" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;generalized-linear-model&gt;&lt;interpretation&gt;" Title="GLM with multiple categorical variables in R : how to interpret the result?" ViewCount="106" />
  <row AcceptedAnswerId="104477" AnswerCount="1" Body="&lt;p&gt;I have found that in much of the data that I am looking through, after sorting from largest to smallest, there is a pattern similar to a rotated logistic function.  That is, it declines steeply along the vertical axis at first, evens out, and then continues to decline along the vertical axis at the same rate as it declined to start with.  Does this tell me anything about the nature of the data underlying the graph outside the obvious?  I feel like I might be missing something from this.  If this is a ridiculous thought just let me know and I can remove the question.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-06-23T21:41:07.637" Id="104464" LastActivityDate="2014-06-24T03:41:38.920" LastEditDate="2014-06-24T00:27:03.460" LastEditorUserId="1191" OwnerUserId="40120" PostTypeId="1" Score="0" Tags="&lt;logistic&gt;&lt;normal-distribution&gt;&lt;data-visualization&gt;" Title="Frequent Occurrence of Rotated Graph of Logistic Function" ViewCount="35" />
  <row Body="&lt;p&gt;Q-learning also permits an agent to choose an action stochastically (according to some distribution).   In this case, the reward is the expected reward given that distribution of actions.  I think this fits your case above.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Q-learning also permits actions that may fail.  Hence, $Q(s, Left)$ might lead you to a state $s'$ that is not the to the left $s$ (e.g. the action &quot;fails&quot; with some probability).  In that case, the model (MDP, table of Q-values, automaton) will encode the possibility of failure directly and no distributions or expected values are needed.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-24T00:40:16.440" Id="104479" LastActivityDate="2014-06-24T00:40:16.440" OwnerUserId="48872" ParentId="34357" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="104505" AnswerCount="3" Body="&lt;p&gt;I'm using a service (Optimizely, as it happens), which show a confidence interval but doesn't explain what it is.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm assuming it is a Binomial Confidence Interval, but I have no idea what the % certainty of the range is, or the detailed meaning. Searching the Internet doesn't help - there is nobody with a clear explanation of what the default for this kind of thing is.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, for 97 success out of 939 trials, Optimizely says &quot;10.33% (±1.95)&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;What does that ±1.95 likely mean, &lt;em&gt;exactly&lt;/em&gt;?&lt;/p&gt;&#10;&#10;&lt;p&gt;And yes, I've already complained to them that they should document it. But clearly, there is some default that statisticians habitually use. What is it?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-06-24T07:06:18.617" Id="104499" LastActivityDate="2014-06-24T14:57:17.837" OwnerUserId="48885" PostTypeId="1" Score="3" Tags="&lt;confidence-interval&gt;&lt;binomial&gt;" Title="What is the most likely meaning of a confidence interval?" ViewCount="98" />
  <row AnswerCount="1" Body="&lt;p&gt;My supervisor once told me that before I run any classifier or do anything with a dataset I must fully understand the data and make sure that the data is clean and correct.&lt;/p&gt;&#10;&#10;&lt;p&gt;My questions:&lt;/p&gt;&#10;&#10;&lt;p&gt;What are the best practices to understand a dataset (high dimensional with numerical and nominal attributes)?&lt;/p&gt;&#10;&#10;&lt;p&gt;Practices to make sure the dataset is clean?&lt;/p&gt;&#10;&#10;&lt;p&gt;Practices to make sure the dataset doesn't have wrong values or so?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-24T08:11:08.047" Id="104509" LastActivityDate="2014-06-24T08:31:52.927" OwnerUserId="27779" PostTypeId="1" Score="0" Tags="&lt;data-mining&gt;&lt;dataset&gt;" Title="Best practice to understand a dataset" ViewCount="59" />
  
  
  
  
  
  
  
  
  
  <row AnswerCount="3" Body="&lt;p&gt;I have a longitudinal data set in which my predictor/Independent Variable is dichotomous and was assessed at age 4 years, and my outcome/Dependent Variable (depressive symptoms) was assessed at 16 years.  I would rather not arbitrarily dichotomize the DV, and I would prefer to avoid non-parametric stats so that I could include some covariates.&lt;/p&gt;&#10;&#10;&lt;p&gt;Naively, I would think that one could use a dichotomous IV but continuous DV in logistic regression, though I would assume that technically I would be using depressive symptoms to predict my IV?  Is there any precedence for this in the literature (or alternative analyses to consider)?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-06-24T15:28:36.497" Id="104573" LastActivityDate="2014-06-24T15:58:17.220" OwnerUserId="48925" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;logistic&gt;&lt;categorical-data&gt;" Title="Can I use logistic regression when my IV is dichotomous but my DV is continuous?" ViewCount="99" />
  <row Body="&lt;p&gt;I am assuming that you realize logistic regression is only suitable for binary outcome. What I think you're asking is if you can flip the identify of a binary IV and a continuous DV, and fit them into the outcome and exposure of a logistic model accordingly.&lt;/p&gt;&#10;&#10;&lt;p&gt;The answer is probably not. Because regression model assumes the independent variables are measured without error. If you swap their identity, you would attribute the residual to the exposure, and worse, you're assuming the current depressive symptom score is measured without error.&lt;/p&gt;&#10;&#10;&lt;p&gt;Try to use model closer to linear regression, and put them IV and DV in their right spots as best as you can.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-24T15:40:36.483" Id="104575" LastActivityDate="2014-06-24T15:40:36.483" OwnerUserId="13047" ParentId="104573" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;The purpose of statistical models is to model (approximate) an unknown, underlying reality.  When you discretize something that is naturally continuous, you are saying that all the responses for a range of predictor variables are exactly the same, then there is a sudden jump for the next interval.  Do you really believe that the natural world works by having a large difference in the response between x-values of 9.999 and 10.001 while having no difference between 9.001 and 9.999 (assuming one of the intervals is 9-10)?  I cannot think of any natural processes that I would consider plausibly working that way.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now there are many natural processes that act in a non linear manner, the change from 8 to 9 in the predictor may make a very different change in the response than a change from 10 to 11.  And therefore a discretized predictor may fit better than a linear relationship, but that is because it is allowed more degrees of freedom.  But, there are other ways to allow additional degrees of freedom, such as polynomials or splines, and these options allow us to penalize to get a certain level of smoothness and maintain something that is a better approximation of the underlying natural process.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-24T20:28:39.697" Id="104610" LastActivityDate="2014-06-24T20:28:39.697" OwnerUserId="4505" ParentId="104402" PostTypeId="2" Score="4" />
  
  <row AnswerCount="3" Body="&lt;p&gt;I would like to calculate $E[log{(a-b*x)^2}]$ where $a$ is known scaler, $b$ is a known row vector and $x$ is random column vector following a multivariate normal distribution with a known mean and covariance matrix. What is a quick way to calculate or approximate this expectation numerically [without using simulation or difficult multivariate integration]?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-25T00:40:28.643" Id="104636" LastActivityDate="2014-06-25T09:12:55.863" LastEditDate="2014-06-25T01:35:11.783" LastEditorUserId="41838" OwnerUserId="41838" PostTypeId="1" Score="2" Tags="&lt;mathematical-statistics&gt;" Title="Expectation related to a multivariate normally distributed random vector" ViewCount="86" />
  
  <row AcceptedAnswerId="104797" AnswerCount="1" Body="&lt;p&gt;I have a distribution that is skewed to the left (i.e. a lot of low values, aka positive skew - verified)&lt;/p&gt;&#10;&#10;&lt;p&gt;Basically, I have either a lot of similar low [~minimum] values, or 0 values, or something like 1 value that is the only value below the median value.&lt;/p&gt;&#10;&#10;&lt;p&gt;The data has already been normalized to 0 to 100% based on an ECDF run [transformation].  AKA an Empirical Cumulative Distribution Function, very similar to a rank transformation, it's very simple.  Is value equal to or less than the rest of the values, if not, then count the number of values less than or equal to current value, and count that towards that values overall Cumulative Distribution Function.&lt;/p&gt;&#10;&#10;&lt;p&gt;So... is there a simple way to mean adjust the ecdf distribution to a .5 mean, and slope the data from 0 to 100%&lt;/p&gt;&#10;&#10;&lt;p&gt;My first guess was to track the difference from a .5 mean.&lt;/p&gt;&#10;&#10;&lt;p&gt;My second guess was to derive the # of unique #'s in my [skewed] distribution to establish categories I needed to divide by from my difference from mean.&lt;/p&gt;&#10;&#10;&lt;p&gt;Sample set of data:&#10;0&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&#10;2&lt;/p&gt;&#10;&#10;&lt;p&gt;After ecdf conversion&#10;0.05&#10;0.55&#10;0.55&#10;0.55&#10;0.55&#10;0.55&#10;0.55&#10;0.55&#10;0.55&#10;0.55&#10;0.55&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&#10;1&lt;/p&gt;&#10;&#10;&lt;p&gt;mean: 0.7275&lt;/p&gt;&#10;&#10;&lt;p&gt;I had one idea to factor DOWN anything equal to or below median by .5&lt;/p&gt;&#10;&#10;&lt;p&gt;our desire to achieve .5 mean is because we are combining the distribution with other distributions that after ecdf conversion already have a ~.5 mean.  The problem with our skewed data set is it's promoting low skewed values to high %'s and we don't want to do that.  So, we would prefer to achieve a .5 with the skewed data.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-06-25T00:51:30.937" Id="104637" LastActivityDate="2014-08-30T08:08:38.020" LastEditDate="2014-06-25T13:50:14.070" LastEditorUserId="48197" OwnerUserId="48197" PostTypeId="1" Score="1" Tags="&lt;descriptive-statistics&gt;" Title="ECDF [skewed] distribution, wish to mean adjust to .5" ViewCount="216" />
  <row Body="&lt;p&gt;I know this is a partial answer and I'm not an expert, but this might help: if one of two unimodal pdfs is log-concave, then their convolution is unimodal. Due to &lt;a href=&quot;http://epubs.siam.org/doi/abs/10.1137/1101021&quot; rel=&quot;nofollow&quot;&gt;Ibragimov (1956)&lt;/a&gt;, via &lt;a href=&quot;http://www-personal.umd.umich.edu/~fmassey/notes/Convolutions%20of%20Unimodal%20Functions.doc&quot; rel=&quot;nofollow&quot;&gt;these notes&lt;/a&gt;. Apparently, if &lt;em&gt;both&lt;/em&gt; are log-concave, then the convolution is also log-concave.&lt;/p&gt;&#10;&#10;&lt;p&gt;As far as product closure, the only &quot;clean&quot; result I know of for product distributions is the limit theorem described in &lt;a href=&quot;http://math.stackexchange.com/questions/728406/central-limit-theorem-for-a-product&quot;&gt;this math.se answer&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;How about a truncated version of &lt;a href=&quot;http://en.wikipedia.org/wiki/Generalized_normal_distribution#Version_1&quot; rel=&quot;nofollow&quot;&gt;these&lt;/a&gt;? The bounded uniform distribution is a limiting case of its shape parameter, and as far as I'm aware they're unimodal and log-concave so they have unimodal, log-concave convolutions. I have no clue about their products . When I have more time later this week I could try and run some simulations to see if I get log-concave products of truncated error distributions. Maybe &lt;a href=&quot;http://projecteuclid.org/euclid.aoms/1177699380&quot; rel=&quot;nofollow&quot;&gt;Govindarajulu (1966)&lt;/a&gt; would help.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm not sure what the policy on crossposting is, but it seems like the math.se people might be able to help you as well. Out of curiosity, are you trying to build an algebraic structure out of probability distributions?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-06-25T04:56:44.917" Id="104653" LastActivityDate="2014-06-25T04:56:44.917" OwnerUserId="36229" ParentId="104633" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;In short, the probability of a 7-card straight when drawing 7 random cards from a standard deck of 52 is $0.000979$.&lt;/p&gt;&#10;&#10;&lt;p&gt;To calculate this value, we note that all 7-card hands are equally likely, of which there are ${52 \choose 7} = 133,784,560$ possibilities.&lt;/p&gt;&#10;&#10;&lt;p&gt;Next, we compute the number of 7-card straights. Ignoring suit, we note that there are $8$ possible straights (starting with {A, 2, 3, 4, 5, 6, 7} through {8, 9, 10, J, Q, K, A}). For each card in the straight, there are 4 possibilities for the suit, such that there are $4^7 = 16384$ ways to assign the suits to the 7 cards. However, $4$ of these suit assignments yield straight flushes (all clubs, all diamonds, etc.), so the actual number of suit assignments that can yield a straight (but not a straight flush) is $16384 - 4 = 16380$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Putting all this together, there are $8 \times 16380 = 131,040$ possible 7-card straights out of $133,784,560$ possible 7-card hands, yielding a probability of $\approx 0.000979$.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-06-25T04:59:28.273" Id="104654" LastActivityDate="2014-06-25T05:26:53.990" LastEditDate="2014-06-25T05:26:53.990" LastEditorUserId="35588" OwnerUserId="35588" ParentId="104652" PostTypeId="2" Score="5" />
  
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;The following output is given:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/cjWmV.png&quot; alt=&quot;t1&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The task is to state, which variables, among thoses that are statistically significnat at 0.05, have the greatest and least relative importance on the fitted model?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;First question&lt;/strong&gt;:&lt;/p&gt;&#10;&#10;&lt;p&gt;I think I have to look at the absolute value of the standardized estimates to get an approximate ranking of the relative importance of the input variables on the fitted model, right?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Second question&lt;/strong&gt;:&#10;How can I calculate these values? I thought about dividing the estimate by the standard error, e.g. $-0.9699/0.0385$ but this is not equal to -0.2074?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-25T10:50:15.303" Id="104689" LastActivityDate="2014-06-25T10:50:15.303" OwnerUserId="25675" PostTypeId="1" Score="1" Tags="&lt;estimation&gt;&lt;standard-deviation&gt;&lt;standard-error&gt;&lt;importance&gt;" Title="Relative importance of variables?" ViewCount="41" />
  <row AnswerCount="0" Body="&lt;p&gt;$(X_1,X_2,\ldots,X_n)$ is a random sample from $\mathrm{N}(θ, 1)$. We know sample mean is a unbiased estimator that is consistent. What would be a biased but consistent estimator for θ? Would it be sample median? If so, can anyone prove that? &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-06-25T13:12:54.773" Id="104702" LastActivityDate="2014-06-25T13:43:24.790" LastEditDate="2014-06-25T13:43:24.790" LastEditorUserId="24669" OwnerUserId="48559" PostTypeId="1" Score="0" Tags="&lt;distributions&gt;&lt;estimation&gt;&lt;point-estimation&gt;" Title="Biased but consistent estimator for the mean of Gaussian distribution?" ViewCount="56" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm wondering how I should do the following in SPSS.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a dataset with missing data (at random).&#10;Some values are blank, because the question was &quot;not applicable&quot; to that person (eg. questions about walking asked to people in a wheelchair). I assume I shouldnt want those values filled in with Multiple Imputations. How can I exclude those values?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-06-25T13:43:56.703" FavoriteCount="2" Id="104714" LastActivityDate="2014-06-25T13:43:56.703" OwnerUserId="48654" PostTypeId="1" Score="1" Tags="&lt;multiple-imputation&gt;" Title="Multiple Imputations exclude &quot;not applicable&quot;" ViewCount="41" />
  <row Body="&lt;p&gt;I wouldn't say it is a hypothesis testing problem. What you describe is a classification or regression problem, depending of the type of the output variable (categorial or real). This is the field of Machine Learning: you feed learning algorithm with historical data and you create a model. That model is used to make predictions on unseen data.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-25T14:03:45.823" Id="104717" LastActivityDate="2014-06-25T14:03:45.823" OwnerUserId="37188" ParentId="104709" PostTypeId="2" Score="0" />
  
  <row AnswerCount="1" Body="&lt;p&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I've been doing a little bit of work lately on maximum likelihood estimation (MLE), for cases where the data is normally-distributed and also for cases where the data is Poisson distributed. For these two cases, the likelihood $L$ is given by:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;L\left ( \mathbf{a} \right ) = \prod_{i} \mathrm{P}\left ( c_{i};m_{\mathbf{a}}\left ( x_{i} \right ) \right )&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Following the notation and steps of the first reference below, $\mathrm{P}\left ( c_{i};m_{\mathbf{a}}\left ( x_{i} \right ) \right )$ is the probability that a measurement gives $c_{i}$ if the true value is given by the model $m_{\mathbf{a}}\left ( x_{i} \right )$, where $\mathbf{a}$ is the set of parameters for the model. &lt;/p&gt;&#10;&#10;&lt;p&gt;For the two different distributions, the reference gives:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;L_{G}\left ( \mathbf{a} \right ) = \prod_{i} \frac{1}{\sqrt{2\pi \sigma_{i} ^{2}}} \mathrm{e}^{-\frac{\left ( c_{i}-m_{\mathbf{a}}\left ( x_{i} \right ) \right )^{2}}{2\sigma_{i} ^{2}}}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;for Gaussian distributed data, and&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;L_{P}\left ( \mathbf{a} \right ) = \prod_{i} \frac{\left [m_{\mathbf{a}}\left ( x_{i} \right )\right ]^{c_{i}}}{c_{i}!}\mathrm{e}^{-m_{\mathbf{a}}\left ( x_{i} \right )}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;for Poisson-distributed data.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm happy following the subsequent steps to get a likelihood ratio test out. The steps involve calculating the negative-log of the likelihood, then finding a ratio based on maximising $L$ for a given model with respect to the global maximum of the likelihood $L$, i.e.&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;\frac{\mathrm{max}_{\mathbf{a}}\;L(\mathbf{c}|m_{\mathbf{a}})}{\mathrm{max}\;L(\mathbf{c}|\mathbf{m})}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;This question is related: &lt;a href=&quot;http://stats.stackexchange.com/questions/25966/what-is-distribution-of-z-x-y-where-x-is-poisson-distributed-and-y-is?rq=1&quot;&gt;What is distribution of $Z = X + Y$ where $X$ is Poisson distributed and $Y$ is normally distributed?&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm comfortable with the MLE for Gaussian and Poisson data. The scenario I'm interested in is for mixed Poisson-Gaussian data. An example might be photons arriving at a detector, which will exhibit Poisson noise, and the signal from the detector then subsequently being corrupted by Gaussian noise (e.g. thermal noise in the electronics). The noise model is then a mix of Poisson and Gaussian noise.&lt;/p&gt;&#10;&#10;&lt;p&gt;What I want to do is test whether a Poisson, Gaussian or mixed model is most appropriate for various parameters (I don't necessarily know whether the Poisson data can be approximated by a Gaussian by the way, hence the question - it's what I'm trying to test!).&lt;/p&gt;&#10;&#10;&lt;p&gt;What would an appropriate likelihood function $L$ be? Is it just a sum of the Poisson and Gaussian likelihoods? If so, the log-likelihood will be a bit tricky to simplify, I believe? &lt;/p&gt;&#10;&#10;&lt;p&gt;I'm sure I'm missing something with this problem, I just haven't been able to make the leap from the two cases described above (I should add that it's been a bit of time since I did statistics in any serious depth). Any help much appreciated!&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;For this problem, I've been following the paper &lt;em&gt;&quot;Comparison of maximum likelihood estimation and chi-square statistics applied to counting experiments&quot;&lt;/em&gt; by T. Hauschild and M. Jentschel (2001).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0168900200007567&quot; rel=&quot;nofollow&quot;&gt;Link&lt;/a&gt; and&#10;&lt;a href=&quot;http://dx.doi.org/10.1016/S0168-9002%2800%2900756-7&quot; rel=&quot;nofollow&quot;&gt;DOI: 10.1016/S0168-9002(00)00756-7&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="15" CreationDate="2014-06-25T14:49:26.363" FavoriteCount="1" Id="104725" LastActivityDate="2014-06-27T09:01:36.110" LastEditDate="2014-06-27T09:01:36.110" LastEditorUserId="42134" OwnerUserId="42134" PostTypeId="1" Score="3" Tags="&lt;normal-distribution&gt;&lt;maximum-likelihood&gt;&lt;poisson&gt;" Title="Maximum likelihood estimation for mixed Poisson and Gaussian data" ViewCount="205" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;It's been a long time since basic statistics. I have a financial time-series that exhibits exponential growth. &lt;/p&gt;&#10;&#10;&lt;p&gt;Before I standardize, do I have to make the time-series stationary?&lt;/p&gt;&#10;&#10;&lt;p&gt;Before I standardize, do I have to normalize?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-06-25T17:45:43.977" Id="104755" LastActivityDate="2014-06-25T19:59:30.050" OwnerUserId="48997" PostTypeId="1" Score="0" Tags="&lt;normal-distribution&gt;&lt;normalization&gt;&lt;standardization&gt;" Title="Does a time-series have to be stationary before you calculate a z score or t score?" ViewCount="25" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I’m trying to implement a program in C# to get the Greenhouse-Geisser Epsilon for a 2 factor ANOVA design with 3 levels each factor.  Following the procedure on &lt;a href=&quot;http://www.real-statistics.com/anova-repeated-measures/sphericity/additional-information-sphericity/&quot; rel=&quot;nofollow&quot;&gt;&lt;/a&gt;&lt;a href=&quot;http://www.real-statistics.com/anova-repeated-measures/sphericity/additional-information-sphericity/&quot; rel=&quot;nofollow&quot;&gt;http://www.real-statistics.com/anova-repeated-measures/sphericity/additional-information-sphericity/&lt;/a&gt; I was able to get the GG Epsilon for the single term but for the interaction, I have no idea how to compute the sample covariance matrix.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have tried to partition the data similar to what SPSS requires:&lt;/p&gt;&#10;&#10;&lt;p&gt;Eg.  For factors A and B with 3 levels each:&lt;/p&gt;&#10;&#10;&lt;p&gt;A1B1 A1B2 A1B3 A2B1 A2B2 A2B3 A3B1 A3B2 A3B3&lt;/p&gt;&#10;&#10;&lt;p&gt;4     6    4    5    3    4    9    6    3&lt;/p&gt;&#10;&#10;&lt;p&gt;5     5    3    3    5    3    6    2    2&lt;/p&gt;&#10;&#10;&lt;p&gt;6     8    6    4    8    1    6    3    9&lt;/p&gt;&#10;&#10;&lt;p&gt;7     3    0    4    0    8    3    4    8&lt;/p&gt;&#10;&#10;&lt;p&gt;4     4    5    3    5    6    2    8    5&lt;/p&gt;&#10;&#10;&lt;p&gt;3     3    4    2    3    6    2    5    8&lt;/p&gt;&#10;&#10;&lt;p&gt;Then compute the covariance matrix for each group:&lt;/p&gt;&#10;&#10;&lt;p&gt;2.166667 0.7 0.333333 0.566667 -0.4 -1.66667 -1.46667 -0.06667 1.366666667&lt;/p&gt;&#10;&#10;&lt;p&gt;0.7 1.1 2.2 1.1 -0.2 0 -0.2 -0.6 -0.5&lt;/p&gt;&#10;&#10;&lt;p&gt;0.333333 2.2 7.866667 3.933333 1.8 -1.73333 1.066667 -4.53333 -4.066666667&lt;/p&gt;&#10;&#10;&lt;p&gt;0.566667 1.1 3.933333 3.766667 4 -1.26667 2.533333 -4.46667 -0.233333333&lt;/p&gt;&#10;&#10;&lt;p&gt;-0.4 -0.2 1.8 4 7.2 -1 4.8 -5.8 0&lt;/p&gt;&#10;&#10;&lt;p&gt;-1.66667 0 -1.73333 -1.26667 -1 4.666667 1.066667 2.466667 -0.466666667&lt;/p&gt;&#10;&#10;&lt;p&gt;-1.46667 -0.2 1.066667 2.533333 4.8 1.066667 4.266667 -3.53333 0.133333333&lt;/p&gt;&#10;&#10;&lt;p&gt;-0.06667 -0.6 -4.53333 -4.46667 -5.8 2.466667 -3.53333 6.266667 1.133333333&lt;/p&gt;&#10;&#10;&lt;p&gt;1.366667 -0.5 -4.06667 -0.23333 0 -0.46667 0.133333 1.133333 8.566666667&lt;/p&gt;&#10;&#10;&lt;p&gt;Then if I perform the procedure as if it was a one within subject factor ANOVA, I then get GGEpsilon = 0.3594.&#10;While SPSS Two Way repeated measures gives GGEpsilon = 0.603.&lt;/p&gt;&#10;&#10;&lt;p&gt;This is obviously not the right way to do this.  Can anyone point me to the right direction?  Or direct me to some reference material where I can find the information required.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks much&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-25T19:43:58.673" Id="104774" LastActivityDate="2014-06-25T19:43:58.673" OwnerUserId="49004" PostTypeId="1" Score="0" Tags="&lt;anova&gt;&lt;repeated-measures&gt;&lt;sphericity&gt;" Title="How to compute the Greenhouse-Geisser Epsilon for the interaction term of the 2 repeated factors in a Repeated Measures ANOVA" ViewCount="36" />
  <row Body="&lt;p&gt;The answer is the latter: Might as well do regular ANOVA. If the DV and the covariate are not correlated then there isn't much point to doing an analysis of covariance: the covariate will not usefully extract variance from the DV.  After all, the analysis is based on the assumption that the covariate is introducing variability into to DV in such a way that those who score low on the covariate score low on the DV; moderate on the covariate = moderate on the DV; high on the covariate = high on the DV (or the inverse if the DV and covariate are significantly negatively correlated). That's what makes something a &quot;covariate&quot;. So one reason the various treatment groups may differ on the DV is the effect of the covariate.  The analysis of covariance partials this effect of the covariate on the DV out of the effect(s) that the IVs have on the DV, giving you a &quot;purer&quot; look at the IVs' unique effect(s).&lt;/p&gt;&#10;&#10;&lt;p&gt;In addition to checking whether the DV and covariate are correlated at all it is important to look at the DV/covariate bivariate correlation to make sure they are LINEARLY correlated.  If they are curvilinearly related then the covariate will not remove variance from the DV in a consistent way across the treatment groups. If this is the case, choose a different covariate, one that is strongly and linearly correlated with the DV.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-06-25T21:00:39.400" Id="104782" LastActivityDate="2014-06-25T21:00:39.400" OwnerUserId="49008" ParentId="95036" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;--passes : think of this as the number of times you want to loop over the data&lt;/p&gt;&#10;&#10;&lt;p&gt;features : if you a have a dataset with column names &quot;name&quot;,&quot;id&quot;,&quot;age&quot; etc., then &quot;name&quot;,&quot;id&quot;,&quot;age&quot; etc., are the features that you can use as predictors or a target variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;Q 3,4: I think these depend on what you want to predict. You might want to tweak the learning rate and double check the values returned. I am trying to figure this out as well, so am not cent percent sure.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-25T21:53:53.403" Id="104785" LastActivityDate="2014-06-26T19:14:44.957" LastEditDate="2014-06-26T19:14:44.957" LastEditorUserId="49011" OwnerUserId="49011" ParentId="95125" PostTypeId="2" Score="0" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Let's say that $F(k; n, p) = \Pr(k \leq X)$ is the cumulative binomial distribution for $X \sim B(n, p)$. I am trying to solve for the smallest $i$ such that $F(i; n + i, p) &amp;gt; \alpha$ for some $\alpha$.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would use the inverse binomial distribution function to find this, except the $n$ parameter changes.&lt;/p&gt;&#10;&#10;&lt;p&gt;Does anyone know if this follows any known distribution?&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-06-26T00:39:10.180" Id="104800" LastActivityDate="2014-06-26T06:07:14.583" LastEditDate="2014-06-26T04:26:52.903" LastEditorUserId="49020" OwnerUserId="49020" PostTypeId="1" Score="0" Tags="&lt;binomial&gt;" Title="Binomial-like distribution" ViewCount="51" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have made a new algorithm that is specifically crafted for clustering very large datasets.  In order to document it as a research paper, I have to choose one or two &lt;strong&gt;internal (no-label) cluster evaluation measures&lt;/strong&gt; to evaluate my algorithm. Which algorithm do you think is generally the best choice for &lt;strong&gt;big datasets&lt;/strong&gt;? And &lt;strong&gt;why?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;My algorithm is a modified version of Expectation-Maximization Gaussian mixture models. In other words, the whole data is described by &lt;/p&gt;&#10;&#10;&lt;p&gt;$$P_(x) = \sum_{k=1}^{K}\pi(c_k)\mathcal{N}(\mu_k,\Sigma_k)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\pi(c_1),\ldots,\pi(c_K)$ are mixture weights. The main difference between my algorithm and regular EM is that it uses some sampling and approximation tricks that accelerate EM. The objective function is the same (the log-likelihood which is to be maximized).&lt;/p&gt;&#10;&#10;&lt;p&gt;Should I use log-likelihood as the evaluation metric? or use other (which?) internal measures for such task? Is it rational?&lt;/p&gt;&#10;" CommentCount="12" CreationDate="2014-06-26T06:31:05.777" Id="104819" LastActivityDate="2014-06-29T11:03:45.867" LastEditDate="2014-06-26T07:41:39.487" LastEditorUserId="46416" OwnerUserId="46416" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;clustering&gt;&lt;large-data&gt;" Title="Best metric for evaluation of mixture-of-Gaussian clusters on big-data" ViewCount="89" />
  <row AnswerCount="0" Body="&lt;p&gt;I am fairly new to multivariate statistics and have run into the following situation:&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a data set of 12 response sets based on a Likert scale (1-5), data which is commonly (in social research) treated as more-or-less interval scale / quantitative. Only about half of the responses show a normal distribution, many show a right-skewed distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;The aim is to identify correlations within this response data set. Using XLSTAT for Excel, I started out with multiple regression and identified two main influential IVs (R2 over all variables = 0,32) for a particular DV (selected from among the 12 variables). I then re-ran the calculations for 2 and 3 interaction levels. While the R2 of the results understandably increased considerably (to 0,85 and 0,94), the variables that now were most influential (according to standardized coefficient value for significant results) changed with each interaction level. While for no interactions, multicollinearity was low (tolerance 0.6-0.8, VIF always under 2), with 2 or 3 interaction levels multicollinearity soared (2 levels: VIFs over 400, 3 levels: VIFs in the millions). &lt;/p&gt;&#10;&#10;&lt;p&gt;Assuming that multicollinearity and probably suppressor variables were the problem, I then changed to PLS regression, again 11 IVs for one DV. With no interactions accounted for, I got a Q2 cumulated of 0.12, R2Y cum=0.26 and R2X cum=0.25. The most influential IV according to VIP value was the same as at the non-interactions multiple regression (with VIP value of 1.4). The same with 2 interaction levels again showed that same IV (VIP now 2.0) at the top (and the previously second-most significant IV still at second place), but Q2cum slightly down at 0.09, R2Y cum improved at 0.41, R2X cum lower at 0.09. Trying now for 3 interaction levels, the rank of most influential IVs shifted completely, with a combination of three IVs showing a VIP of 2.4, the previously most important IV still being in third place (VIP of 2.1). However, model quality is now terrible, with a Q2cum of -0.03 (R2Y/X cum about the same as for 2 interaction levels). &lt;/p&gt;&#10;&#10;&lt;p&gt;What I would like to understand from this:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;I assume that higher VIP values for increased interaction levels come from more complex modeling due to more interactions being accounted for, but the Q2cum for 3 interaction levels shows that model predictive quality has dropped. Why would this be the case? Which of the three PLS models (0/2/3 interaction levels) will serve me best, or does the low Q2 mean that none of them is significant at all? Assuming that Q2 and R2 were the same for all three, would the higher interaction model have the greatest explanatory power?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;How should I interpret the model quality values (Q2/R2x/y cum)? In particular, is Q2cum (predictive power of model) the primary quality factor to consider, or are reasonably R2x/y cum levels (actual explanative power of model) sufficient? &#10;Since R2y reflects the variation of the DV explained by the IVs (if I understood correctly), then I assume that the R2y value is more significant for the quality of my model than R2x, correct?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;How bad is it that my IVs do not all show a good normal distribution for all this? How robust is PLS regression in this case?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Thanks for your patience with a newbie, I appreciate simple and understandable explanations, thanks a lot!!&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-06-26T07:38:30.323" Id="104827" LastActivityDate="2014-10-04T20:01:32.680" LastEditDate="2014-06-26T08:21:54.453" LastEditorUserId="49035" OwnerUserId="49035" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;multiple-regression&gt;&lt;multivariate-analysis&gt;&lt;model&gt;&lt;pls&gt;" Title="Quality of PLS Regression at different interaction levels" ViewCount="84" />
  <row AnswerCount="2" Body="&lt;p&gt;What is affine transformation? Which distribution families are closed under affine transformation?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-06-26T07:38:47.490" FavoriteCount="1" Id="104828" LastActivityDate="2015-02-17T04:57:33.920" OwnerUserId="48559" PostTypeId="1" Score="4" Tags="&lt;probability&gt;&lt;distributions&gt;" Title="What is Affine Transformation?" ViewCount="208" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am performing a time series analysis on dataset ranging from 1974-2008. I have performed Augmented Dickey Fuller tests and Phillips Perron tests to check the stationarity/ order of integration of my series. These tests both confirmed that all series are I(1) with the exception of GDP per capita which is I(2). The country in question is Ireland. &#10;For the purposes of my analysis I had been planning on using the ARDL Bounds Testing approach to cointegration (Pesaran &amp;amp; Shin, 1999) but this method is not possible given the presence of an I(2) variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is: Does anyone know of any approach to cointegration which allows for the mix of I(1) and I(2) variables? I am unsure as to how to proceed with my analysis given this issue and thus far have not come across any solutions.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any help at all would be greatly appreciated. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-26T11:11:47.050" Id="104843" LastActivityDate="2014-06-26T11:11:47.050" OwnerUserId="49045" PostTypeId="1" Score="0" Tags="&lt;time-series&gt;&lt;econometrics&gt;&lt;cointegration&gt;&lt;unit-root&gt;" Title="Time Series Econometrics: Cointegration methods for series with mixed degree of integration" ViewCount="61" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm working on multiclass classification problem (precisely 4 classes). I want to use the most simple approach for this problem: one-vs-all! I have 4 different test set (only labels are different). Using one-vs-all approach, during test, for each input pattern, I have to compute 4 different objective function values from 4 different SMVs. So, the pattern will belong to the class with the greatest objective function value.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, I tried this:&lt;/p&gt;&#10;&#10;&lt;p&gt;./svm-train -s 0 -t 5 -c 16 -g 0.05 -b 1 'traindata'&lt;/p&gt;&#10;&#10;&lt;p&gt;./svm-predict -b 1 'testdata' 'traindata.model' 'outfile'&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm not sure about the means of &lt;strong&gt;b parameter&lt;/strong&gt;. Opening the file 'outfile' generated from svm-predict, I get probabilistic values. I suppose that comparing these probabilistic values between all SVMs, I'm able to classify each pattern in the class with the greatest probabilistic value. My assumption is correct? In other words, I want to be sure that these probabilistic values are directly proportional to the relative objective function values.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-26T15:29:09.767" Id="104877" LastActivityDate="2015-01-30T11:32:14.270" LastEditDate="2014-06-26T15:39:52.027" LastEditorUserId="22311" OwnerUserId="40952" PostTypeId="1" Score="0" Tags="&lt;classification&gt;&lt;svm&gt;&lt;libsvm&gt;" Title="One vs All appraoch to multiclassification in LIBSVM" ViewCount="123" />
  
  
  <row Body="&lt;p&gt;Your shifted, average score is:&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{align}&#10;M(X,\beta,\delta,\alpha) &amp;amp;= \frac{\sum_{p(X\beta+\alpha)&amp;gt;\delta} p(X\beta+\alpha)}{\sum \mathbf{1}_{p(X\beta+\alpha)&amp;gt;\delta}}&#10;\end{align}&lt;/p&gt;&#10;&#10;&lt;p&gt;Your question is basically &quot;Is $M$ monotonically increasing in $\alpha$?&quot;  The answer is that it is not.  Once you put the question this way, it is easy to see why it is not.  At &quot;most&quot; values of $\alpha$, $M$ is going to be increasing.  The numerator is increasing in $\alpha$ and the denominator is unaffected by $\alpha$.  At a very few special values of $\alpha$ (the points for which $X_i\beta+\alpha=\delta$ for some $i$, $M$ will decrease in $\alpha$.  Why?  Because the numerator will increase by exactly $\delta$ at one of these points, and the denominator will also increase by exactly 1 at one of these points.  Since the fraction is higher than $\delta$ all the time, this will drag it down at that point (in effect, you are suddenly including a new data point in your average which you know is less than the average was before you included it).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/L8juh.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The little down-ticks are where the data lie.  To make the graph, I slightly modified your R program.  I generate a single 100-observation dataset.  Then I shift it using $\alpha=0$ and take the truncated mean of the scores.  Then I shift it using $\alpha=0.001$ and use the truncated mean of the scores.  And so on.  Here is the &lt;code&gt;R&lt;/code&gt; script:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# This program written in response to a Cross Validated question&#10;# http://stats.stackexchange.com/questions/41267/shifted-intercepts-in-logistic-regression&#10;&#10;# The program graphs the expectation of a shifted logit score conditional on the score passing&#10;# some threshold.  The conditional mean is not monotonic in the shift.&#10;&#10;library(faraway)&#10;library(plyr)&#10;&#10;set.seed(12344321)&#10;&#10;# simulation parameters&#10;vBeta &amp;lt;- rbind(0.1, 0.2, 0.3, 0.4)  # vector of coefficients&#10;sDelta &amp;lt;- 0.16  # threshold for the scores&#10;&#10;# simulate the data&#10;mX &amp;lt;- matrix(rnorm(400, 4, 1), 100, 4)&#10;vY &amp;lt;- (0.4 + mX%*%vBeta + rt(n=100, df=7)&amp;gt;=5)&#10;data &amp;lt;- as.data.frame(cbind(vY,mX))&#10;&#10;# logistic regression&#10;resLogitFit &amp;lt;- glm(V1~V2+V3+V4+V5, binomial(link = &quot;logit&quot;), data=data)&#10;raw.scores &amp;lt;- resLogitFit$fitted.values&#10;&#10;# mean of scores bigger than delta:&#10;mean(raw.scores[raw.scores&amp;gt;sDelta])&#10;&#10;&#10;# Create mean greater than delta for a variety of alphas&#10;&#10;shift.logit.mean &amp;lt;- function(alpha,delta,scores){&#10;  alpha &amp;lt;- as.numeric(alpha)&#10;  shifted &amp;lt;- ilogit(logit(scores) + alpha)&#10;  return(mean(shifted[shifted&amp;gt;delta]))&#10;}&#10;&#10;results &amp;lt;- ddply(data.frame(alpha=seq(from=0,to=1000,by=1)/1000),.(alpha),&#10;                    shift.logit.mean,delta=sDelta,scores=raw.scores)  &#10;names(results)[2]&amp;lt;-&quot;shifted.scores&quot;&#10;&#10;plot(results,type=&quot;l&quot;,main=&quot;Scores not monotonic in alpha&quot;)&#10;&#10;# Now. let's artificially pile up the data right near the delta cut point:&#10;&#10;raw.scores[1:10] &amp;lt;- sDelta - 1:10/1000&#10;&#10;results &amp;lt;- ddply(data.frame(alpha=seq(from=0,to=1000,by=1)/1000),.(alpha),&#10;                 shift.logit.mean,delta=sDelta,scores=raw.scores)  &#10;names(results)[2]&amp;lt;-&quot;shifted.scores&quot;&#10;&#10;plot(results,type=&quot;l&quot;,main=&quot;With scores piled up near delta&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now that we know that the graph is going to go down wherever there are data, it is easy to make it go down a lot.  Just modify the data so that a whole bunch of scores are just a little less than $\delta$ (or, really, just grouped close together anywhere to the left of the original cut point).  The &lt;code&gt;R&lt;/code&gt; script above does that, and here is what you get:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/1NHPl.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I got it to go down really fast for low values of $\alpha$ by piling up scores just to the left of $\delta$, the cut point.&lt;/p&gt;&#10;&#10;&lt;p&gt;OK, so we know that, theoretically, there is not going to be any monotonicity result.  What can we say?  Not much, I think.  Obviously, once $\alpha$ gets big enough that all the scores pass the cut point, the function is going to be monotonic and is going to aymptote at 1.  That's about it, though.  You can make the function go down, on average, locally by putting a lot of data points there.  You can make the function go up locally by not putting any data points there.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, suppose we have a really big dataset, so that it would be OK to approximate the sums by integrals ($f(X)$ is the multivariate density of $X$):&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{align}&#10;M(X,\beta,\delta,\alpha) &amp;amp;= \frac{\int_{p(X\beta+\alpha)&amp;gt;\delta} p(X\beta+\alpha) f(X)}{\int_{p(X\beta+\alpha)&amp;gt;\delta}f(X)}&#10;\end{align}&lt;/p&gt;&#10;&#10;&lt;p&gt;The derivative of this in $\alpha$ is kind of ugly.  However, you get the same result.  At $\alpha$s for which the denominator is increasing a lot (where $f(X)$ is high), you can get a negative derivative.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-06-26T19:48:15.573" Id="104913" LastActivityDate="2014-06-26T19:48:15.573" OwnerUserId="25212" ParentId="41267" PostTypeId="2" Score="4" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a triaxial accelerometer mounted on my index finger and I want to differentiate in real time when it taps on a touch screen vs when the middle finger taps. For this I take the accelerometer data in a time window when the tap is detected on the touch screen. The problem is that the data is not clearly distinguishing the two cases, because even when the middle finger does not tap, it is following the downward motion when the index finger goes to tap. There are also dependencies on whether the 1st and 2nd finger go alternately or same finger again and again. I am new to this and am unable to figure out how do I go about analysing. First of all, how exactly should I mount the accelerometer so that it gives me the most clear data? - for instance if I mount it loosely, it might jiggle more when tapped but it might give increase noise as well. Secondly, do I need to try and integrate the data to distance values, or get it into Fourier transform? Thirdly, if I need to use machine learning (decision tree/svm etc.) which features will help me the most? There are just too many things to try out and I can't code every possibility in time - some guidance will help.       &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-26T20:53:05.610" Id="104921" LastActivityDate="2014-06-26T21:01:15.607" LastEditDate="2014-06-26T21:01:15.607" LastEditorUserId="49084" OwnerUserId="49084" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;signal-processing&gt;&lt;real-time&gt;" Title="How to process triaxial accelerometer data for finger taps in real time?" ViewCount="44" />
  <row Body="&lt;p&gt;&lt;strong&gt;Being able to generalize well&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;This is the essence of a good model.  And it is the essence of what makes the best practitioners of the art of machine learning stand out from the crowd.&lt;/p&gt;&#10;&#10;&lt;p&gt;Understanding that the goal is to optimize performance on unseen data, not to minimize training loss.  Knowing how to avoid both over-fitting and under-fitting. Coming up with models that are not too complex yet not too simple in describing the problem.  Extracting the gist of a training-set, rather than the maximum possible.&lt;/p&gt;&#10;&#10;&lt;p&gt;It is surprising how often, even experienced machine learning practitioners, fail to follow this principle.  One reason is that humans fail to appreciate two vast &lt;strong&gt;theory-vs-practice&lt;/strong&gt; &lt;em&gt;magnitude differences&lt;/em&gt;:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;How much larger is the space of &lt;strong&gt;&lt;em&gt;all possible examples&lt;/em&gt;&lt;/strong&gt; compared to the training-data at hand, even when the training data is very large.&lt;/li&gt;&#10;&lt;li&gt;How much larger is the &lt;strong&gt;&lt;em&gt;full &quot;hypothesis space&quot;&lt;/em&gt;&lt;/strong&gt;: number of possible models for a problem, compared to the practical &quot;solution space&quot;: everything you can think of, and everything your software/tools are capable of representing.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;The 2nd is especially incomprehensible because even for the simplest problem with $N$ inputs and a binary outcome, there are $2^N$ possible input-examples, and an exponentially larger number 2^$2^N$ of possible models.&lt;/p&gt;&#10;&#10;&lt;p&gt;It is also what most of the above answers said in more specific and concrete ways.  &lt;strong&gt;to generalize well&lt;/strong&gt; is just the shortest way I could think of, to put it.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-26T21:53:34.610" Id="104931" LastActivityDate="2014-12-14T21:57:03.780" LastEditDate="2014-12-14T21:57:03.780" LastEditorUserId="11236" OwnerUserId="11236" ParentId="104500" PostTypeId="2" Score="3" />
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I was reading this &lt;a href=&quot;http://eprints.soton.ac.uk/259182/1/gp2.pdf&quot; rel=&quot;nofollow&quot;&gt;paper&lt;/a&gt; related to online Gaussian processes. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/3MWl9.png&quot; alt=&quot;enter image description here&quot;&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/liJHA.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I didn't get how equation set 2 was derived. Any suggestions?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-06-27T03:38:57.523" Id="104964" LastActivityDate="2014-06-27T03:38:57.523" OwnerUserId="12331" PostTypeId="1" Score="0" Tags="&lt;gaussian-process&gt;" Title="Confusion related to derivation of a formula in gaussian process" ViewCount="27" />
  <row Body="&lt;p&gt;The effect of uninformative features depends largely on your modeling strategy. For some approaches they are irrelevant while for others they can dramatically decrease overall performance.&lt;/p&gt;&#10;&#10;&lt;p&gt;Your intuition that using more features should necessarily yield a better model is wrong.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-06-27T08:27:51.773" Id="104983" LastActivityDate="2014-06-27T08:27:51.773" OwnerUserId="25433" ParentId="104978" PostTypeId="2" Score="3" />
  <row AnswerCount="1" Body="&lt;p&gt;Let $\phi_1,\ldots,\phi_n$ denote characteristic functions for distributions on the real line. Let $a_1,\ldots,a_n$ denote nonnegative constants such that $a_1+\ldots+a_n = 1$. Show that&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\hspace{20mm} \phi(t) = \sum_{j=1}^{n}a_j\phi_j(t), \hspace{8mm}-\infty&amp;lt;t&amp;lt;\infty$$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Attempt:&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;Let $a = 1$.&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\phi(t) = \mathrm{E}\left[\exp(aitX)\right] = \int_{-\infty}^{\infty} \exp(aitX)\times p(x)\,\mathrm{d}x $$&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm not sure how to proceed. I would assume that the additive property of the characteristic functions has something to do with the exponential components.  &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-06-27T08:34:58.287" Id="104984" LastActivityDate="2014-10-13T19:02:03.297" LastEditDate="2014-06-27T08:40:47.640" LastEditorUserId="21054" OwnerUserId="37743" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;distributions&gt;&lt;self-study&gt;&lt;statistical-significance&gt;&lt;mathematical-statistics&gt;" Title="Characteristic Function proof" ViewCount="107" />
  <row AnswerCount="0" Body="&lt;p&gt;I am quite confused with the distinction between a latent variable and model parameters.&lt;/p&gt;&#10;&#10;&lt;p&gt;So say I have two observed variables $x$ and $y$ and they have some unknown relationship between them i.e. $y = f(x)$. Now based on that I have a generative model with $ y = f(x) + e$ where I model $e$ as zero mean independent and identically distributed noise with some variance $\sigma$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Am I correct in saying that:&lt;/p&gt;&#10;&#10;&lt;p&gt;1: The relationship $f$ that we try to estimate is the latent variable in this case.&lt;/p&gt;&#10;&#10;&lt;p&gt;2: $\sigma$ is a model parameter. Now, our estimate of $\sigma$ will also have some uncertainty associated with it. With these model parameters, are we saying that it has a fixed but unknown value that we try and estimate, but due to limited data size our estimates have uncertainty. However, underlying the model is the assumption that the value of a $\sigma$ takes a fixed value or is $\sigma$ also a random variable type entity?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-06-27T12:55:32.950" Id="105023" LastActivityDate="2014-06-27T12:55:32.950" OwnerUserId="36540" PostTypeId="1" Score="2" Tags="&lt;bayesian&gt;&lt;modeling&gt;&lt;latent-variable&gt;" Title="latent variables versus model parameters" ViewCount="53" />
  <row AnswerCount="0" Body="&lt;p&gt;Consider the companion papers by Clayton &amp;amp; Schifflers (1987) here: &lt;a href=&quot;http://www.pauldickman.com/cancerepi/handouts/handouts_time_trends/clayton_schifflers_1987a.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.pauldickman.com/cancerepi/handouts/handouts_time_trends/clayton_schifflers_1987a.pdf&lt;/a&gt;, &lt;a href=&quot;http://www.pauldickman.com/cancerepi/handouts/handouts_time_trends/clayton_schifflers_1987b.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.pauldickman.com/cancerepi/handouts/handouts_time_trends/clayton_schifflers_1987b.pdf&lt;/a&gt;.  Pages 464 and 474 discuss the drift parameter, but it's not clear how to compute the difference with the reference value using grouped data.  Suppose my reference is 1960-64 and my period or cohort of interest is 1970-74.  It seems like the difference in the midpoint, 10, is the proper choice here since no distinction is made between birth years falling within the intervals. Is that the value entered into the analysis for persons falling into these groups? &lt;/p&gt;&#10;&#10;&lt;p&gt;Now, if one has individual level data, the exact differences could be computed, but would still need to be grouped in a manner similar to the rest of the data before conducting the analysis, correct?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-27T13:40:07.810" Id="105025" LastActivityDate="2014-06-27T13:40:07.810" OwnerUserId="28835" PostTypeId="1" Score="0" Tags="&lt;age-period-cohort&gt;" Title="Age period cohort shift parameter" ViewCount="10" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Is there a probability distribution for which the standard deviation is proportional to the mean?&lt;/p&gt;&#10;&#10;&lt;p&gt;More generally, I have been searching the web for a nice table comparing the properties of probability distributions (including mean and std). Any good reference?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-06-27T16:03:53.073" Id="105041" LastActivityDate="2014-06-27T17:06:15.063" LastEditDate="2014-06-27T17:05:26.150" LastEditorUserId="805" OwnerUserId="31975" PostTypeId="1" Score="5" Tags="&lt;probability&gt;" Title="Probability distribution with standard deviation proportional to mean" ViewCount="175" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm trying to apply the bag of visual words approach to make scene classification. I started to use k-means to generate my codebook, but rapidly discovered its limitations. From one codebook generation to another, I get fluctuations of 10% on the classification ratio (data dimension: 8, number of clusters: 50). In addition, k-means is slow (I'm working on Matlab).&lt;/p&gt;&#10;&#10;&lt;p&gt;So I did some research and found that I can do unsupervised learning with random forests (paper: &quot;Unsupervised Learning With Random Forest Predictors&quot; by T. Shi and S. Horvath). It looked nice, so I decided to give it a try.&lt;/p&gt;&#10;&#10;&lt;p&gt;What I did:&lt;/p&gt;&#10;&#10;&lt;p&gt;(1) I created a synthetic data set from my original data set. To generate the coordinates of my synthetic samples, I take the corresponding coordinate of one of my original samples, chosen randomly (like explained here:&lt;a href=&quot;http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#unsup&quot; rel=&quot;nofollow&quot;&gt;http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#unsup&lt;/a&gt;). I also tried the Addcl2 method like explained in the paper that I mentioned above.&lt;/p&gt;&#10;&#10;&lt;p&gt;(2) The original data is labeled 1, the synthetic data is labeled 2. Then I train a random forest (I use Matlab's TreeBagger class) on this 2 classes. Then I compute the proximity matrix, but only for the original data.&lt;/p&gt;&#10;&#10;&lt;p&gt;(3) I use this proximity matrix as a distance matrix to perform unsupervised learning with hierarchical clustering.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is what I get for 2D data:&#10;&lt;img src=&quot;http://i.stack.imgur.com/Hw0yV.png&quot; alt=&quot;Example for well separated data&quot;&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/CqbRG.png&quot; alt=&quot;Example for badly separated data&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;It works approximately well for good separated data points, while the clustering result depends from one time to another, sometimes I get the same result as k-means. For badly separated data, the result is awfull.&lt;/p&gt;&#10;&#10;&lt;p&gt;What am I doing wrong? I could need some advice ;-)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-27T16:29:14.910" Id="105045" LastActivityDate="2014-06-27T16:29:14.910" OwnerUserId="49138" PostTypeId="1" Score="1" Tags="&lt;clustering&gt;&lt;random-forest&gt;&lt;unsupervised-learning&gt;" Title="Unsupervised Random Forest for Visual Codebook generation" ViewCount="121" />
  
  <row Body="&lt;p&gt;Your data are apparently not paired. You should not be attempting to pair unpaired data.&lt;/p&gt;&#10;&#10;&lt;p&gt;With (presumably) independent samples, the usual form of permutation test simply permutes the group labels.&lt;/p&gt;&#10;&#10;&lt;p&gt;With your sample sizes, a full permutation test would usually be impractical (unless the sample difference is fairly extreme, in which case a complete enumeration of the tail may be feasible).&lt;/p&gt;&#10;&#10;&lt;p&gt;As such we'd usually be looking at a randomization test.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the independent samples case, when testing for a difference in means that consists of any statistic yielding equivalent p-values (i.e. providing the same ordering of samples as a difference in means). &lt;/p&gt;&#10;&#10;&lt;p&gt;The sum of the values in the smaller sample would be sufficient (the difference in means is a simple linear transformation of this).&lt;/p&gt;&#10;&#10;&lt;p&gt;So the randomization test would consist of selecting random sets of 579 values from the combined sample of (579+1289) points and computing their sum, and then locating the sample value in that distribution and identifying the proportion of statistics at least as extreme as the observed one - counting the observed one. &lt;/p&gt;&#10;&#10;&lt;p&gt;With a two-tailed test, you can compute a sum of values in the smaller sample for the difference in means having the opposite sign to the observed and make teh same count of the proportion more extreme in the other tail.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;The independent-samples randomization test is a pretty standard test. In R you should be able to do it using the &lt;code&gt;coin&lt;/code&gt; package, but writing code for it is pretty simple (though possibly a good deal slower than using a function in a well-built package).&lt;/p&gt;&#10;&#10;&lt;p&gt;Consider the following data (this is small enough to do complete enumeration of the permutation distribution, but we'll do it as a randomization test):&lt;/p&gt;&#10;&#10;&lt;p&gt;Sample A: 23.194 28.027 37.487 31.180 30.430 38.424&lt;/p&gt;&#10;&#10;&lt;p&gt;Sample B: 34.623 32.936 36.885 &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; a &amp;lt;- scan()&#10; 23.194 28.027 37.487 31.180 30.430 38.424&#10;&#10; b &amp;lt;- scan()&#10; 34.623 32.936 36.885 &#10;&#10; (mean.diff &amp;lt;- mean(b)-mean(a))&#10; [1] 3.357667&#10;&#10; sumb &amp;lt;- sum(b)&#10; alldata &amp;lt;- c(a,b)&#10;&#10; res &amp;lt;- replicate(10000,sum(sample(alldata,3)))&#10;&#10; res &amp;lt;- c(res,sumb)&#10; extreme1 &amp;lt;- sum(res &amp;gt;= sumb)&#10; extreme2 &amp;lt;- sum(res &amp;gt;= sumb)+sum(res &amp;lt;= 3*(2*mean(alldata)-mean(b)))&#10; p.value.1 &amp;lt;- extreme1/10000&#10; p.value.2 &amp;lt;- extreme2/10000&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In my example it gives an (upper) one tailed p-value of $0.19$. and a two-tailed p-value of $0.37$.&lt;/p&gt;&#10;&#10;&lt;p&gt;You'd have to make a few adjustments for your sample data but that's the gist of how it works.&lt;/p&gt;&#10;&#10;&lt;p&gt;The exact permutation test p-value in this case is computed thusly:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;permsum = combn(alldata,3,sum)&#10;pp.value.1 = sum(sumb&amp;lt;=permsum)/length(permsum)&#10;pp.value.2 = pp.value.1+sum(3*(2*mean(alldata)-mean(b))&amp;gt;=permsum)/length(permsum)&#10;hist(permsum,n=50)&#10;abline(v=sumb,lty=2,col=6)&#10;abline(v=3*(2*mean(alldata)-mean(b)),lty=2,col=8)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The corresponding exact 1- and 2- tailed p-values here are $0.19$ and $0.37$.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-06-27T17:44:48.523" Id="105057" LastActivityDate="2014-06-28T06:48:44.047" LastEditDate="2014-06-28T06:48:44.047" LastEditorUserId="805" OwnerUserId="805" ParentId="105051" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I struggle performing a MANOVA for a mixed model. I have the following columns:&lt;/p&gt;&#10;&#10;&lt;p&gt;S - subject (1,...,N)&#10;B - in between group (B1, B2)&#10;W - repeated measurement - within (W1, W2)&#10;M - measured dependent variable 1&#10;N - measured dependent variable 2&lt;/p&gt;&#10;&#10;&lt;p&gt;My data is organized the following way:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;S; B; W;   M;   N&#10;1;B1;W1;12.3;14.4&#10;1;B1;W2;35.4;13.5&#10;2;B1;W1;11.1;22.2&#10;2;B1;W2;33.3;11.1&#10;3;B2;W1;15.3;12.3&#10;3;B2;W2;20.3;18.8&#10;...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;If I had only one dependent variable (M), I could obtain the results of an ANOVA in one of these ways:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;dat=read.csv(file=&quot;data.csv&quot;, sep=&quot;;&quot;)&#10;library(ez)&#10;ezANOVA(dat, dv=.(M), wid=.(S), between=.(B), within=.(W), type=3, detailed=TRUE)&#10;&#10;library(lmerTest)&#10;results = lmerTest::lmer(M ~ B*W + (1 | S), data=dat)&#10;anova(results, ddf=&quot;Kenward-Roger&quot;, type=3, method.grad=&quot;Richardson&quot;)&#10;&#10;library(car)&#10;options(contrasts=c(&quot;contr.sum&quot;, &quot;contr.poly&quot;))&#10;mod1 &amp;lt;- lme4::lmer(M ~ B*W + (1 | S), data=dat)&#10;car::Anova(mod1, test.statistic=&quot;F&quot;, type=3)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I wonder, how the code for the MANOVA has to look like? I tried&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;dat=read.csv(file=&quot;vor_actions_new.csv&quot;, sep=&quot;;&quot;)&#10;results = manova(cbind(M, N) ~ B*W, data=dat)&#10;summary.aov(results)&#10;summary(results)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;but I doubt, the within-factor is handled correctly. I also tried&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;mod1 &amp;lt;- lme4::lmer(cbind(M, N) ~ B*W + (1 | S), data=dat)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;but, this gave an error, probably because multiple responses are not supported.  ezAnova does also not support multiple responses, I think. Is there a way to execute the planned MANOVA?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-06-27T18:51:59.087" Id="105064" LastActivityDate="2014-06-27T18:51:59.087" OwnerUserId="37609" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;mixed-model&gt;&lt;repeated-measures&gt;&lt;manova&gt;" Title="MANOVA for mixed model with multiple repeated measurements" ViewCount="270" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am interested in taking the following model to my data:&lt;/p&gt;&#10;&#10;&lt;p&gt;M = X + Z + XZ&lt;/p&gt;&#10;&#10;&lt;p&gt;Y = X + M&lt;/p&gt;&#10;&#10;&lt;p&gt;Where Y = outcome; M = mediator; X = treatment; Z = moderator; XZ is treatment-moderator interaction&lt;/p&gt;&#10;&#10;&lt;p&gt;This corresponds with Hayes' model 2 (&lt;a href=&quot;http://www.ats.ucla.edu/stat/stata/faq/modmed.htm#model2&quot; rel=&quot;nofollow&quot;&gt;http://www.ats.ucla.edu/stat/stata/faq/modmed.htm#model2&lt;/a&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;I have theoretical reasons to &lt;em&gt;not&lt;/em&gt; include an interaction effect of Z with the direct effect, neither do I expect treatment/mediator interaction, nor do I expect an interaction of Z with M.&lt;/p&gt;&#10;&#10;&lt;p&gt;I seem to have problems estimating with [R] &lt;em&gt;mediation&lt;/em&gt;; it seems to force me to include the Z variable in the Y equation, whereas I don't have theoretical reasons to do so.&lt;/p&gt;&#10;&#10;&lt;p&gt;The following example will show what I mean.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(mediation)&#10;&#10;data(framing)&#10;&#10;# the model I want&#10;med.fit &amp;lt;- lm(emo ~ treat*age, data = framing)&#10;out.fit &amp;lt;- glm(cong_mesg ~ emo + treat, data = framing, family = binomial(link = &quot;probit&quot;))&#10;&#10;# mediation effect&#10;med.out &amp;lt;- mediate(med.fit, out.fit, treat = &quot;treat&quot;, mediator = &quot;emo&quot;, data = framing)&#10;summary(med.out)&#10;&#10;# investigate the moderation&#10;med.age20 &amp;lt;- mediate(med.fit, out.fit, treat = &quot;treat&quot;, mediator = &quot;emo&quot;, covariates = list(age=20))&#10;&#10;# let's try again&#10;out.fit &amp;lt;- glm(cong_mesg ~ emo + treat + age, data = framing, family = binomial(link = &quot;probit&quot;))&#10;&#10;med.age20 &amp;lt;- mediate(med.fit, out.fit, treat = &quot;treat&quot;, mediator = &quot;emo&quot;, covariates = list(age = 20))&#10;summary(med.age20)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;An error appears when trying to estimate the effects for a given value of the moderator. This makes sense as it tries to assign the covariate value to the Y equation but can't find the variable. &lt;/p&gt;&#10;&#10;&lt;p&gt;How can I estimate the model above using the &lt;em&gt;mediation&lt;/em&gt; package?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-27T20:14:32.973" FavoriteCount="1" Id="105075" LastActivityDate="2014-06-30T22:48:08.887" LastEditDate="2014-06-30T22:47:34.977" LastEditorUserId="10210" OwnerUserId="10210" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;mediation&gt;" Title="Moderated mediation: moderator only in M equation" ViewCount="59" />
  <row Body="&lt;p&gt;What you did isn't the way you run a chi-squared test.  You need a contingency table.  For each group, you have some people in STEM fields and some people who aren't.  Thus, you will have two rows of counts, or two cells per group.  Then you run a chi-squared test of the independence of the rows and columns.  Here is a slightly edited version of your data:  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;totals                      &amp;lt;- c(195, 134, 38)&#10;stems                       &amp;lt;- c(22,16,9)&#10;group_stem_counts           &amp;lt;- matrix(c(stems, totals-stems),ncol=3,byrow=TRUE)&#10;rownames(group_stem_counts) &amp;lt;- c(&quot;stem&quot;, &quot;non-stem&quot;)&#10;colnames(group_stem_counts) &amp;lt;- c(&quot;Group One&quot;,&quot;Group Two&quot;,&quot;Group Three&quot;)&#10;group_stem_counts&#10;#          Group One Group Two Group Three&#10;# stem            22        16           9&#10;# non-stem       173       118          29&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now you can run your test:  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;chisq.test(group_stem_counts)&#10;# &#10;#         Pearson's Chi-squared test&#10;# &#10;# data:  group_stem_counts&#10;# X-squared = 4.5225, df = 2, p-value = 0.1042&#10;# &#10;# Warning message:&#10;# In chisq.test(group_stem_counts) :&#10;#   Chi-squared approximation may be incorrect&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This yields the warning that you saw.  As a rule of thumb, it is generally recommended that the expected count for each cell under the null hypothesis to be at least 5.  However, it has been shown that this is overly conservative, and the chi-squared test is robust even if that isn't exactly the case.  We can examine your expected counts like so:  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;chisq.test(group_stem_counts)$expected&#10;#          Group One Group Two Group Three&#10;# stem      24.97275  17.16076    4.866485&#10;# non-stem 170.02725 116.83924   33.133515&#10;# Warning message:&#10;# In chisq.test(group_stem_counts) :&#10;#   Chi-squared approximation may be incorrect&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Your minimum expected count is &lt;code&gt;4.866485&lt;/code&gt;, and all the others are &gt;5.  Realistically, this is nothing to bother over.  However, if you are concerned about it, you can just simulate the p-value instead of using the chi-squared approximation.  Here is the chi-squared test using that option:  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;chisq.test(group_stem_counts, simulate.p.value=TRUE)&#10;# &#10;#         Pearson's Chi-squared test with simulated p-value (based on 2000&#10;#         replicates)&#10;# &#10;# data:  group_stem_counts&#10;# X-squared = 4.5225, df = NA, p-value = 0.1184&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;As you can see, the p-value is essentially the same.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-27T20:53:57.743" Id="105079" LastActivityDate="2014-06-27T20:53:57.743" OwnerUserId="7290" ParentId="105070" PostTypeId="2" Score="2" />
  <row AnswerCount="1" Body="&lt;p&gt;When calculating the coefficient of determination for a square, why is it that if you use the data set for the side length of as X= (1,2,3,4) and the perimeter as Y=(4,8,12,16) the Coefficient of Determination equals 100%.&lt;/p&gt;&#10;&#10;&lt;p&gt;But when doing the same calculation for using the area of the square with the X points (1,2,3,4) and the Y data point (1,4,9,16) the Coeff of Determ decreases to 96%?&lt;/p&gt;&#10;&#10;&lt;p&gt;Any insight would be great.  The simpler the language the better too.  Thanks!! &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-06-27T23:08:50.213" Id="105090" LastActivityDate="2014-06-27T23:48:38.580" OwnerUserId="49016" PostTypeId="1" Score="2" Tags="&lt;linear-model&gt;&lt;regression-coefficients&gt;" Title="Coefficient of Determination: For the perimeter and area of a square: Why different?" ViewCount="58" />
  <row Body="&lt;p&gt;You did not reject your hypothesis.&lt;/p&gt;&#10;&#10;&lt;p&gt;But here you have the opportunity for a few insights:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;em&gt;Any&lt;/em&gt; non-zero effect size is significant with a large enough sample.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;One might interpret the &lt;em&gt;significance&lt;/em&gt; of results, absent other considerations, as pretty meaningless beyond &quot;will my editor accept these results?&quot;&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;One might take the approach that, &lt;em&gt;a priori&lt;/em&gt; an effect size needs be be &lt;em&gt;so large&lt;/em&gt; in order for one to consider it &lt;em&gt;relevant&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;3a. One can combine &lt;em&gt;tests for difference&lt;/em&gt; with &lt;em&gt;tests for equivalence&lt;/em&gt; to produce inferences within a hypothesis testing framework that differentiate &lt;em&gt;relevant&lt;/em&gt; difference from &lt;em&gt;trivial&lt;/em&gt; difference (i.e. an over-powered test) from &lt;em&gt;equivalence&lt;/em&gt; from &lt;em&gt;indeterminacy&lt;/em&gt; (i.e. an underpowered test).&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="10" CreationDate="2014-06-28T00:59:13.270" Id="105096" LastActivityDate="2014-06-28T01:09:25.400" LastEditDate="2014-06-28T01:09:25.400" LastEditorUserId="44269" OwnerUserId="44269" ParentId="105094" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;I'm not sure what you mean exactly by support here, but if X lies in $[-M,M]$ a.s. then the following works:&lt;/p&gt;&#10;&#10;&lt;p&gt;First, have a look at the statement of Theorem 6.6 (pg 107) of: &lt;a href=&quot;http://stat.uconn.edu/~boba/stat6894/probabilityI.pdf&quot; rel=&quot;nofollow&quot;&gt;http://stat.uconn.edu/~boba/stat6894/probabilityI.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;As X is bounded a.s., $E|X|^k$ is finite for every $k&amp;gt;0$. Hence, by the theorem, &#10;$\phi^{(k)}(t)$ exists for every $k&amp;gt;0$.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-28T05:04:02.690" Id="105112" LastActivityDate="2014-06-28T05:04:02.690" OwnerUserId="29770" ParentId="105110" PostTypeId="2" Score="0" />
  
  
  
  
  <row AcceptedAnswerId="105163" AnswerCount="1" Body="&lt;p&gt;In some binary classification problem I assume that the probability for a positive is exactly linearly dependent on the features $P(y=1|x_1,x_2,x_3,\ldots)=\beta_1x_1+\beta_2x_2+\beta_3x_3+\cdots$. Now for the training I only observe the realization 0 or 1 for $y$. Therefore the probability isn't directly observable, but would rather be some local density (which is hard to estimate for high dimensions).&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there some type of regression that can determine the coefficients $\beta_i$?&lt;/p&gt;&#10;&#10;&lt;p&gt;I could try normal regression, but it doesn't seem obvious that any loss function will be able to converge to the same $\beta_i$. What type of regularization and loss function would be most appropriate?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-06-28T20:48:05.493" Id="105149" LastActivityDate="2014-07-01T14:48:46.750" OwnerUserId="8694" PostTypeId="1" Score="0" Tags="&lt;regression&gt;" Title="How to recover linear probability model for binary classification?" ViewCount="101" />
  <row AnswerCount="0" Body="&lt;p&gt;I have two questions.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;I am running an experiment where I am interested in determining the &lt;strong&gt;sample size&lt;/strong&gt; required for a certain CI (confidence interval) and error, where values range between $&amp;lt;1$ and $&amp;gt;-1$. However, I am only interested in the minimum sample size required as per the sign + or - (positive or negative) not as per the specific values. Could this problem be framed as a &lt;strong&gt;binary&lt;/strong&gt; problem (+ or -) as to calculate the minimum sample size (or should cathegorical values be applied instead and how) and what would be the correct method? &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;I have a model where I split the data into &lt;strong&gt;train&lt;/strong&gt; and &lt;strong&gt;test&lt;/strong&gt; samples. In this case and considering the above (interest is in + and - sign only not on the specific values) I would like to find out what the optimal breakdown sample size would be for train / test. In other words, at what &lt;strong&gt;training set size&lt;/strong&gt; the test error becomes significant when compared with the null hypothesis of a random prediction. In this respect I have reviewed power analysis, however I have doubts if this can be applied to a &lt;strong&gt;binary test&lt;/strong&gt; (or for the same token if values are cathegorical) given that the size effect calculation involves a difference in means that could be different from the one calculated applying the actual values or result in 0 and also there is the issue of on what values to calculate the standard deviation. In this situation how would I determine the sample size for training.     &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;EDIT&lt;/p&gt;&#10;&#10;&lt;p&gt;in summary &#10;Question 1 - can apply a method of calculation of sample size for a dichotomus outcome to an output that is not &lt;/p&gt;&#10;&#10;&lt;p&gt;Question 2- What is the minimum training set size where the test error becomes significant when compared with the null hypothesis of a random prediction as to know the minimum training size for a model to train &lt;/p&gt;&#10;" ClosedDate="2015-01-01T19:49:00.017" CommentCount="5" CreationDate="2014-06-29T01:13:56.707" FavoriteCount="2" Id="105162" LastActivityDate="2015-01-13T15:44:09.687" LastEditDate="2015-01-13T15:44:09.687" LastEditorUserId="36428" OwnerUserId="36428" PostTypeId="1" Score="5" Tags="&lt;r&gt;&lt;cross-validation&gt;&lt;sample-size&gt;&lt;power-analysis&gt;" Title="Minimum sample size for a dichotomous outcome" ViewCount="139" />
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to improve accuracy of my logistic regression model by selecting the best features. I did an FPR test and ranked the features based on their F-score. &lt;/p&gt;&#10;&#10;&lt;p&gt;The problem is that selecting the best say 3 features doesn't perform as well due to what I suspect is a cross correlation between these features. I noticed this after I saw better performance by picking the best 1 or 2 in combination with ones ranked about 9 or 10.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am a bit of a newbie, but is there a way I can systematically find what the best combination is?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-29T08:06:39.850" Id="105169" LastActivityDate="2014-06-29T08:06:39.850" OwnerUserId="49194" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;" Title="Feature Selection Limitations - Machine Learning FPR Test" ViewCount="21" />
  <row AnswerCount="1" Body="&lt;blockquote&gt;&#10;  &lt;p&gt;Studies indicated that drinking water supplied by some old lead-lined city piping systems contain harmful levels of lead. It appears that the distribution of lead content readings for individual water specimens has mean .033 mg/L and SD .10 mg/L.  &lt;/p&gt;&#10;  &#10;  &lt;p&gt;Explain why is it obvious that content readings are &lt;strong&gt;not&lt;/strong&gt; normally distributed.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I cannot see the answer instantly. Why can't we perceive this distribution as normal with very high variance?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-06-29T12:59:58.680" Id="105182" LastActivityDate="2014-06-29T17:17:46.197" LastEditDate="2014-06-29T14:52:24.633" LastEditorUserId="930" OwnerUserId="43163" PostTypeId="1" Score="2" Tags="&lt;distributions&gt;&lt;self-study&gt;" Title="Whether lead content readings follow a Normal distribution or not" ViewCount="94" />
  <row AnswerCount="0" Body="&lt;p&gt;Pardon my ignorance, for I have very basic training in statistics, but have never taken a regression course. &lt;/p&gt;&#10;&#10;&lt;p&gt;I ran an experiment where participants are in one of two conditions (between subjects). They categorized an object that had values for rotation and spacing, both ranging from 1 to 100. Each participant saw 40 of those values twice. There were a lot of participants. &lt;/p&gt;&#10;&#10;&lt;p&gt;What I want to know is for what values of rotation (or spacing) were the participants as a whole closest to chance (.5, is it or is it not in the category) in each condition, and whether those values differ significantly from one another. &lt;/p&gt;&#10;&#10;&lt;p&gt;What is this process called, and what are some recommended packages or methods in SAS, R, SPSS, Mathematica, or Stata (or really anything) I could use to go about figuring it out. &lt;/p&gt;&#10;&#10;&lt;p&gt;Any help rephrasing the question is also very welcome. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-29T13:11:10.010" Id="105183" LastActivityDate="2014-06-30T15:15:30.900" LastEditDate="2014-06-30T15:15:30.900" LastEditorUserId="12803" OwnerUserId="12803" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;regression&gt;&lt;sas&gt;&lt;methodology&gt;" Title="Help finding method of estimating regression parameters for a given performance" ViewCount="43" />
  <row AnswerCount="0" Body="&lt;p&gt;Different methods of meta-analysis have different assumptions and one of the popular assumption that has been invoked so often is that of normal distribution. can we work out meta-analytic results without this assumption especially when a meta-analysis utilizes a number of studies scattered over a range of populations.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-06-29T13:23:26.027" FavoriteCount="1" Id="105184" LastActivityDate="2014-06-29T13:23:26.027" OwnerUserId="10619" PostTypeId="1" Score="0" Tags="&lt;normal-distribution&gt;&lt;meta-analysis&gt;" Title="Does normal distribution assumption improve the quality of results of a meta-analysis of effect-sizes?" ViewCount="43" />
  <row AnswerCount="0" Body="&lt;p&gt;As I understand, there are two ways to compute Kendall Tau for a data matrix $X$ (n rows of data points in $\mathbb{R}^p$):&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;corr(X,'type','kendall')&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://www.mathworks.com/matlabcentral/fileexchange/27361-weighted-kendall-rank-correlation-matrix/content/kendalltau.m&quot; rel=&quot;nofollow&quot;&gt;http://www.mathworks.com/matlabcentral/fileexchange/27361-weighted-kendall-rank-correlation-matrix/content/kendalltau.m&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;The latter works very, very fast for &quot;small&quot; $X$ (say 300x400). But it runs out of memory when $X$ is say $100\times 1000$. Corr() works very, very slowly in this setting. It can take even an hour sometimes.&lt;/p&gt;&#10;&#10;&lt;p&gt;Are there any better solutions? Ideally in Matlab, but perhaps in another language?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-06-29T16:01:42.520" Id="105192" LastActivityDate="2014-06-29T16:45:44.120" OwnerUserId="21051" PostTypeId="1" Score="3" Tags="&lt;kendall-tau&gt;" Title="Fast Kendall Tau in Matlab (and if necessary another language)" ViewCount="182" />
  <row Body="&lt;p&gt;What you want to do is to model daily data and the roll the forecasts upto weekly/monthly/quarterly/annual projections. You will need to incorporate day-of-the-week effects; week-of-the-year effects; month-of-the-year effects; level shifts ; local time trends ; holiday effects around each of the knowm holidays and special events and any needed ARIMA structure while taking into account unusual values/changes in parameters over time and changes in error variance over time. You might want to look at &lt;a href=&quot;http://www.autobox.com/cms/index.php/news/47-can-forecasting-help-me-staff-a-specific-call-center-&quot; rel=&quot;nofollow&quot;&gt;http://www.autobox.com/cms/index.php/news/47-can-forecasting-help-me-staff-a-specific-call-center-&lt;/a&gt; for some guidance. In terms or transparency I was one of the developers of AUTOBOX.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-06-29T16:52:44.083" Id="105196" LastActivityDate="2014-06-29T16:52:44.083" OwnerUserId="3382" ParentId="105078" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;With the numbers you are talking about, you don't need a chi-squared table. You can use a normal approximation. If $X$ is chi-squared with $r$ degrees of freedom, then&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \frac{x-(r-1)}{\sqrt{2(r-1)}} \sim N(0,1) $$&lt;/p&gt;&#10;&#10;&lt;p&gt;Another approach would be to do a quantile plot of your observations against the theoretical quantiles of the uniform.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, I'm not sure these tests are really going to answer your question about the merits of your random number generator. It's not sufficient that your numbers be uniformly distributed - you also want to show that they are independent and that are no periodicities in your set.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-06-29T17:41:21.983" Id="105198" LastActivityDate="2014-06-29T18:57:53.080" LastEditDate="2014-06-29T18:57:53.080" LastEditorUserId="14188" OwnerUserId="14188" ParentId="105194" PostTypeId="2" Score="3" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I have what is a very basic question about meta-analysis. If data from individual studies about the relationship of interest are presented in both bivariate and multivariate analyses, which should I use? Would it be appropriate to calculate effect sizes based on multivariate analyses (e.g., adjusted odds ratios), or should only data from bivariate analyses be used?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-06-30T00:36:17.790" Id="105224" LastActivityDate="2014-08-03T23:09:10.200" OwnerUserId="49222" PostTypeId="1" Score="2" Tags="&lt;meta-analysis&gt;" Title="Extracting data from bivariate vs multivariate analyses for the purposes of meta-analysis" ViewCount="82" />
  
  
  <row Body="&lt;p&gt;As answered in comments, the canonical link function in the binomial model is the logit. So the probit link is &lt;strong&gt;not&lt;/strong&gt; canonical.&lt;/p&gt;&#10;&#10;&lt;p&gt;That should not stop you from using it, however.  A link function being canonical is a purely mathematical concept without much statistical impact.  You should use the link function which are appropriate for the context you are modelling.   I  once had extensive bioassay data (goal was to model LD50) where it was actually possible to see that the probit link fitted the data (marginally) better than the logit link.  The difference was not large enough to have any practical significance, though.  Probably the difference came from the probit &quot;kissing&quot; its asymptotes much faster than the logit, which is natural for some bioassay data.  Simply, when the dose gets too high, all the animals die!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-30T09:05:06.090" Id="105246" LastActivityDate="2014-06-30T11:27:06.230" LastEditDate="2014-06-30T11:27:06.230" LastEditorUserId="11887" OwnerUserId="11887" ParentId="62782" PostTypeId="2" Score="3" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I would like to validate a questionnaire for my population, using a &quot;think-aloud&quot; (protocol analysis) method. The study looks at IT user satisfaction. Do you think my participants have to use the software for a while to be able to assess that they understand the questionnaire? Also, is it OK to use the same people to answer the questionnaire later on?   &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-30T11:17:31.910" Id="105257" LastActivityDate="2014-06-30T11:17:31.910" OwnerUserId="49248" PostTypeId="1" Score="0" Tags="&lt;psychometrics&gt;" Title="Testing face validity of a questionnaire" ViewCount="33" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Does anyone know how the values on the z-score table are created, either with R code or anything else?&lt;/p&gt;&#10;&#10;&lt;p&gt;thank you&lt;/p&gt;&#10;" ClosedDate="2014-06-30T14:05:28.273" CommentCount="2" CreationDate="2014-06-30T13:33:27.857" Id="105277" LastActivityDate="2014-06-30T13:33:27.857" OwnerUserId="49208" PostTypeId="1" Score="0" Tags="&lt;z-statistic&gt;" Title="How to create a z-score table" ViewCount="20" />
  <row AnswerCount="1" Body="&lt;p&gt;I'm rather new to statistics, so please bear with me.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to be able to say &quot;to within 95% confidence, this data is exponentially-distributed.&quot; In this case, I know how the data &lt;em&gt;should&lt;/em&gt; be distributed and I want to be able to say it &lt;em&gt;is&lt;/em&gt; to some confidence.&lt;/p&gt;&#10;&#10;&lt;p&gt;In other words:&lt;/p&gt;&#10;&#10;&lt;p&gt;(Usually)&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;$H_0$ = The data are exponentially-distributed.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;$H_a$ = The data are not exponentially-distributed.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Choose a test statistic $T_\alpha$ such that Pr(T &gt; $T_\alpha$) = $\alpha$ (under $H_0$).&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;If T &gt; $T_\alpha$, one can reject H_0 with confidence 1-$\alpha$.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;(What I'd like)&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;$H_0$ = The data are not exponentially-distributed.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;$H_a$ = The data are exponentially-distributed.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;???&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Reject H_0 with confidence 1-$\alpha$.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I've also considered doing linear regression on the empirical CDF for the data, but I'm not sure how I would interpret the results of that in terms of confidence.&lt;/p&gt;&#10;&#10;&lt;p&gt;I apologize if this has been answered before, but I've searched considerably and haven't found anything helpful. Thank you all for your help.&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2014-06-30T16:02:06.327" Id="105290" LastActivityDate="2014-06-30T18:22:34.203" LastEditDate="2014-06-30T18:22:34.203" LastEditorUserId="49265" OwnerUserId="49265" PostTypeId="1" Score="0" Tags="&lt;hypothesis-testing&gt;&lt;distributions&gt;&lt;goodness-of-fit&gt;" Title="Unknown null-distribution, known alternative distribution" ViewCount="55" />
  <row AcceptedAnswerId="105307" AnswerCount="1" Body="&lt;p&gt;I am using &lt;strong&gt;linear-mixed effect models&lt;/strong&gt; to analyse my data (the interaction between &lt;code&gt;on&lt;/code&gt; and &lt;code&gt;arc&lt;/code&gt;; both &lt;code&gt;arc&lt;/code&gt; and &lt;code&gt;on&lt;/code&gt; are continuous variables). I also used the &lt;code&gt;effects&lt;/code&gt; library to plot the interaction of &lt;code&gt;on*arc&lt;/code&gt;. However, I could not understand the figure: &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ZQP0l.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Could you please help in understanding what the 5 panels in the figure represent? Do they represent &lt;strong&gt;quantiles&lt;/strong&gt; (e.g., the first 20% lowest arc)? &lt;/p&gt;&#10;&#10;&lt;p&gt;Why there are &lt;strong&gt;five panels&lt;/strong&gt; instead of say &lt;strong&gt;four panels&lt;/strong&gt;? I understand that grey band represent the 95% &lt;strong&gt;confidence intervals&lt;/strong&gt;, and that orange bars represent ranges of arc (high arc to low arc). &lt;/p&gt;&#10;&#10;&lt;p&gt;I also wanted to check whether the location of the orange bars represent where the quantiles fall in arc.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-06-30T16:52:34.467" Id="105294" LastActivityDate="2014-06-30T18:03:58.217" LastEditDate="2014-06-30T17:17:41.913" LastEditorUserId="7290" OwnerUserId="49269" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;data-visualization&gt;&lt;interaction&gt;&lt;lme4&gt;" Title="I don't understand the figure output from package lme4 in R using the effects library?" ViewCount="161" />
  <row Body="&lt;p&gt;Yes, you should definitely adjusted the nutrient with total energy intake first (based on Willett's method). To choose which method to derive the nutrient intake pattern depends on what you want to accomplish, but principle component analysis and cluster analysis are frequently used.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-06-30T17:08:28.723" Id="105296" LastActivityDate="2014-06-30T17:08:28.723" OwnerUserId="14665" ParentId="101105" PostTypeId="2" Score="0" />
  
  
  
  
  <row Body="&lt;p&gt;The suggestion of using a zero-inflated Poisson model is an interesting start. It has some benefits of &lt;em&gt;jointly&lt;/em&gt; modeling the probability of having any illness-related costs as well as the process of what those costs turn out to be should you have any illness. It has the limitation that it imposes some strict structure on what the shape of the outcome is, conditional upon having accrued any costs (e.g. a specific mean-variance relationship and a positive integer outcome... the latter of which can be relaxed for some modeling purposes).&lt;/p&gt;&#10;&#10;&lt;p&gt;If you are okay with treating the &lt;em&gt;illness-related admission&lt;/em&gt; and &lt;em&gt;illness-related costs conditional upon admission&lt;/em&gt; processes independently, you can extend this by first modeling the binary process of y/n did you accrue any costs related to illness? This is a simple logistic regression model and allows you to evaluate risk factors and prevalence. Given that, you can restrict an analysis to the subset of individuals having accrued any costs and model the actual cost process using a host of modeling techinques. Poisson is good, quasi-poisson would be better (accounting for small unmeasured sources of covariation in the data and departures from model assumptions). But sky's the limit with modeling the continuous cost process. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you absolutely need to model the correlation of parameters in the process, you can use bootstrap SE estimates. I see no reason why this would be invalid, but would be curious to hear others' input if this might be wrong. In general, I think those are two separate questions and should be treated as such to have valid inference.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-06-30T21:14:43.037" Id="105321" LastActivityDate="2014-06-30T21:14:43.037" OwnerUserId="8013" ParentId="105320" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;A stationary distribution is such a distribution $\pi$ that if the distribution over states at step $k$ is $\pi$, then also the distribution over states at step $k+1$ is $\pi$. That is, &#10;\begin{equation}&#10;\pi = \pi P.&#10;\end{equation}&#10;A limiting distribution is such a distribution $\pi$ that no matter what the initial distribution is, the distribution over states converges to $\pi$ as the number of steps goes to infinity:&#10;\begin{equation}&#10;\lim_{k\rightarrow \infty} \pi^{(0)} P^k = \pi,&#10;\end{equation}&#10;independent of $\pi^{(0)}$.&#10;For example, let us consider a Markov chain whose two states are the sides of a coin, $\{heads, tails\}$. Each step consists of turning the coin upside down (with probability 1). Note that when we compute the state distributions, they are not conditional on previous steps, i.e., the guy who computes the probabilities does not see the coin. So, the transition matrix is &#10;\begin{equation}&#10;P = \begin{pmatrix} 0 &amp;amp; 1 \\ 1 &amp;amp; 0 \end{pmatrix}.&#10;\end{equation}&#10;If we first initialize the coin by flipping it randomly ($\pi^{(0)} = \begin{pmatrix}0.5 &amp;amp; 0.5\end{pmatrix}$), then also all subsequent time steps follow this distribution. (If you flip a fair coin, and then turn it upside down, the probability of heads is still $0.5$). Thus, $\begin{pmatrix} 0.5 &amp;amp; 0.5 \end{pmatrix}$ is a stationary distribution for this Markov chain. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, this chain does not have a limiting distribution: suppose we initialize the coin so that it is heads with probability $2/3$. Then, as all subsequent states are determined by the initial state, after an even number of steps, the state is heads with probability $2/3$ and after an odd number of steps the state is heads with probability $1/3$. This holds no matter how many steps are taken, thus the distribution over states has no limit.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, let us modify the process so that at each step, one does not necessarily turn the coin. Instead, one throws a die, and if the result is $6$, the coin is left as is. This Markov chain has transition matrix&#10;\begin{equation}&#10;P = \begin{pmatrix} 1/6 &amp;amp; 5/6 \\ 5/6 &amp;amp; 1/6 \end{pmatrix}.&#10;\end{equation}&#10;Without going over the math, I will point out that this process will 'forget' the initial state due to randomly omitting the turn. After a huge amount of steps, the probability of heads will be close to $0.5$, even if we know how the coin was initialized. Thus, this chain has the limiting distribution $\begin{pmatrix} 0.5 &amp;amp; 0.5 \end{pmatrix}$.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-07-01T08:10:24.067" Id="105371" LastActivityDate="2014-07-01T08:10:24.067" OwnerUserId="24669" ParentId="48262" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;If you have no reason to believe that there's any correlation between sampling times (i.e. that &lt;code&gt;Monday 00:00&lt;/code&gt; and &lt;code&gt;Monday 00:30&lt;/code&gt; are no more similar than &lt;code&gt;Monday 00:00&lt;/code&gt; and &lt;code&gt;Friday 12:30&lt;/code&gt;), I think a simple &lt;a href=&quot;http://en.wikipedia.org/wiki/One-way_analysis_of_variance&quot; rel=&quot;nofollow&quot;&gt;one-way ANOVA&lt;/a&gt; should do the job. This tests the null hypothesis that all groups have the same mean, and if $p \lt .05$, tells you that it's unlikely that your samples are totally random.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, in my own field I've never come across an ANOVA with so many groups (336 from your example), so I can't honestly say if that's a problem or not.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-07-01T11:03:43.077" Id="105390" LastActivityDate="2014-07-01T13:52:39.620" LastEditDate="2014-07-01T13:52:39.620" LastEditorUserId="42952" OwnerUserId="42952" ParentId="105263" PostTypeId="2" Score="1" />
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have this shifted time series data. For one set consists of features from week1-5 and labels at week6. Another set features from week2-6 and labels and week7 and so on. I have like four sets of them. If I consider the individual set, then the positive labels are very scarce. So I was thinking of concatenating these four sets so  that I have abundant labels. However, in each dataset, the features are almost the same except for a week. So lets say this particular example, lets say 5th in each of the four datasets which belong to the same user has positive label in each of the four datasets&lt;/p&gt;&#10;&#10;&lt;p&gt;We can say that this example, is repeated four times or weighted four times if I concatenate all the four sets. This may lead to overfitting as well. What is the best way to handle this issue?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-01T16:52:07.843" Id="105440" LastActivityDate="2014-07-01T16:52:07.843" OwnerUserId="12329" PostTypeId="1" Score="0" Tags="&lt;time-series&gt;&lt;machine-learning&gt;" Title="How to correctly concatenate time series data" ViewCount="39" />
  
  
  <row Body="&lt;p&gt;As kjetil said in his comment it should not be a problem to use a differenced variable in a fixed effects regression. The question is: why would you want to do it? First differencing removes information from your variables and you lose one observation per panel. If the sole purpose is to remove the country specific fixed effects you might be throwing out the baby with the bath water.&lt;/p&gt;&#10;&#10;&lt;p&gt;I also think there is some misconception with respect the statistical programing part of your problem. What do you mean by &quot;when we use fixed effects model, it automatically uses first differences of the data&quot;. I don't know what statistical package you use, but for instance in Stata the command &lt;code&gt;xtreg lp hc fdi hc_fdi, fe&lt;/code&gt; uses the within transformation and not first differences. Conversely, when you first difference your data and then use the &lt;code&gt;regress&lt;/code&gt; command, this will give you a first difference regression. Both are ways to eliminate the unobserved country specific effects and do not need to be done together as they are distinct concepts.&lt;br/&gt;&#10;It's probably worthwhile to review these two concepts (for instance in the &lt;a href=&quot;http://www2.warwick.ac.uk/fac/soc/economics/staff/academic/waldinger/teaching/ec9a8/slides/lecture_2_-_panel_data.pdf&quot; rel=&quot;nofollow&quot;&gt;lecture&lt;/a&gt; here).&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-07-01T17:25:46.683" Id="105446" LastActivityDate="2014-07-01T17:25:46.683" OwnerUserId="26338" ParentId="105404" PostTypeId="2" Score="3" />
  <row AnswerCount="1" Body="&lt;p&gt;This is for a simple psychology report.&lt;/p&gt;&#10;&#10;&lt;p&gt;I was wondering if I could hypothesise an insignificant mediation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Eg: &quot;...based on this research it is predicted that group identity is not a significant mediator (or does not sognificantly control) narcissism's relationship with aggression.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;Then I would run a mediation analysis using indirect syntax for SPSS and report that narcissism has a significant relationship with aggression, which is not significantly influenced by (does not significantly overlap with) group identity.&lt;/p&gt;&#10;&#10;&lt;p&gt;Or should i just include it in a regression and that would be clearer? But then my main question remains, can I predict in my hypothesis that a variable will have an insignificant correlation value? Or a weak relationship?&lt;/p&gt;&#10;&#10;&lt;p&gt;I basically want to say that i predict that narcissism will interact with rumination on aggression (using modprobe syntax and correlational data) . But i would also like to add in my initial prediction (based on research) that narcissism's relationship with aggression has nothing to do with group identity.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you all for any input. As I said this is a basic psychology report, not worrying too much about null hypothesis and too much other hardcore statistical stuff. I just need a basically worded prediction (which i think i have) and a basic way to test it.&lt;/p&gt;&#10;&#10;&lt;p&gt;So the basic question is: can I speciffically predict insignificant correlations, mediations or even interactions? And would confirming this prediction support the notion that the two are unrelated in their relationship with aggression? (Or even that group identity has no relationship with aggression, not even via narcissism?)&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you for reading.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-07-01T17:28:19.977" Id="105447" LastActivityDate="2014-07-01T17:48:04.390" OwnerUserId="49287" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;&lt;correlation&gt;&lt;statistical-significance&gt;&lt;experiment-design&gt;" Title="Can I hypothesise an insignificant mediation? (Very basic stats)" ViewCount="66" />
  
  <row AcceptedAnswerId="105469" AnswerCount="2" Body="&lt;p&gt;I am currently reading up on assumptions for Pearson correlations. An important assumption for the ensuing t-test seems to be that both variables come from normal distributions; if they don't, then the use of alternative measures such as the Spearman rho is advocated. The Spearman correlation is computed like the Pearson correlation, only using the ranks of X and Y instead of X and Y themselves, correct?&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is: If the input variables into a Pearson correlation need to be normally distributed, why is the calculation of a Spearman correlation valid even though the input variables are ranks? My ranks certainly don't come from normal distributions...&lt;/p&gt;&#10;&#10;&lt;p&gt;The only explanation I have come up with so far is that rho's significance might be tested differently from that of the Pearson correlation t-test (in a way that does not require normality), but so far I have found no formula. However, when I ran a few examples, the p-values for rho and for the t-test of the Pearson correlation of ranks always matched, save for the last few digits. To me this does not look like a groundbreakingly different procedure.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any explanations and ideas you might have would be appreciated!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-01T22:18:25.723" FavoriteCount="1" Id="105466" LastActivityDate="2014-07-01T23:59:41.503" OwnerUserId="49343" PostTypeId="1" Score="6" Tags="&lt;correlation&gt;&lt;normality&gt;&lt;spearman&gt;&lt;ranks&gt;" Title="Why is a Pearson correlation of ranks valid despite normality assumption?" ViewCount="386" />
  <row AcceptedAnswerId="105474" AnswerCount="1" Body="&lt;p&gt;I am using &#10;    spec.pgram(tsobj,spans=6, plot=TRUE)&#10;to obtain a periodogram for my univariate time series of monthly observations which were sampled over 86 years (so I have 1032 observations in total). It's defined as an object of class &quot;ts&quot; with frequency=12. The periodogram nicely depicts the seasonality for 1 year and has further peaks on following years. However, it stops at 6 years. I have reason to believe that there might be a seasonality with longer phase, say 8 years. How can I change the command to obtain values for longer seasonalities?&lt;/p&gt;&#10;&#10;&lt;p&gt;I am sorry, it sounds like a stupid question and possibly it is, but starring further on the help files, playing around with the available options and googling around didn't brought me any inside.&#10;Thanks alot for your help!&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Here is the plot that I obtain:&#10;&lt;img src=&quot;http://i62.tinypic.com/7286kz.png&quot; alt=&quot;graph&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-01T23:06:05.660" Id="105468" LastActivityDate="2014-07-02T09:28:03.637" LastEditDate="2014-07-02T09:28:03.637" LastEditorUserId="19691" OwnerUserId="19691" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;spectral-analysis&gt;" Title="spec.pgram in R: how to check for seasonality with longer phase?" ViewCount="213" />
  <row AcceptedAnswerId="105481" AnswerCount="1" Body="&lt;p&gt;I am fitting a model in the frequency domain, and my fit looks as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://s3.postimg.org/8aop8zepv/fit.jpg&quot; alt=&quot;Model fit&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;As you can see, the model function does not fit the data perfectly, especially in the higher frequencies. So, I examined the residuals and found that a polynomial of the first degree is the missing term in the model function.&lt;/p&gt;&#10;&#10;&lt;p&gt;What concerns me more is that variance of the data decreases with frequency. What implications does that have on my analysis? &lt;/p&gt;&#10;&#10;&lt;p&gt;(It seems that looking at the autocorrelation of the residuals also doesn't prove very useful.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Any ideas would be very helpful.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-07-01T23:14:07.213" Id="105471" LastActivityDate="2014-07-02T02:06:31.287" OwnerUserId="40390" PostTypeId="1" Score="3" Tags="&lt;modeling&gt;&lt;model-selection&gt;&lt;residuals&gt;&lt;residual-analysis&gt;" Title="Non-constant standard deviation in residuals" ViewCount="40" />
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Assume that I have made a coverage log ratios signal by dividing the &#10;coverage data coming from a tumour sample over the reference sample and then &#10;taking the log2 of this division. My question is: &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;How could the noise level be estimated from the data statistically?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Normally, the log ratio's signal should be centred around zero if there is no&#10;problem in the genome. However, some parts of the genome change (deleted or&#10;amplified) because of cancer, and this shows itself in the log ratio signal by&#10;representing deviations from zero. But the signal is not completely clean,&#10;and not all the deviation from zero is caused by cancer; there is noise as well.&#10;So, the question is how we can estimate the noise level assuming the noise has a &#10;normal distribution. &lt;/p&gt;&#10;&#10;&lt;p&gt;Some terminology:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Coverage data for a point in the genome is the number of reads that have aligned &#10;to the reference genome at that specific point.&lt;/li&gt;&#10;&lt;li&gt;Reads are the output of sequencing tools using over DNA or RNA, etc.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2014-07-02T07:29:15.903" Id="105503" LastActivityDate="2014-07-10T06:33:11.067" LastEditDate="2014-07-10T06:33:11.067" LastEditorUserId="49251" OwnerUserId="49251" PostTypeId="1" Score="0" Tags="&lt;estimation&gt;&lt;variance&gt;&lt;mean&gt;&lt;bioinformatics&gt;&lt;noise&gt;" Title="Estimation of Noise Level in the High-Throughput Cancer Data" ViewCount="35" />
  
  
  <row Body="&lt;pre&gt;&lt;code&gt;## Just repeating what you did for completion&#10;&amp;gt; wt &amp;lt;-  c(2.2,6.4,3.4,10.2,4.45)&#10;&amp;gt; p  &amp;lt;-  ppois(q=0, lambda=wt)&#10;&amp;gt; &#10;&amp;gt; sum(ppois(q=0,lambda=wt))&#10;[1] 0.1575537&#10;&amp;gt; &#10;&amp;gt; tol &amp;lt;- .Machine$double.eps # 2.220446e-16&#10;&amp;gt; wt  &amp;lt;- seq(0,250, by=0.01)&#10;&amp;gt; wt[which( ppois(q=0, lambda=wt) &amp;lt; tol )[1]]&#10;[1] 36.05&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So when the poisson rate is about 36, the probability that the random variable is 0 is minuscule. Of course you can consider different tolerance levels (I used the smallest number that my computer's R could handle, but that is pretty extreme)&lt;/p&gt;&#10;&#10;&lt;p&gt;This is a pretty ad hoc way of going about it, and you can probably formalize it with a hypothesis test. &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-07-02T12:30:33.867" Id="105528" LastActivityDate="2014-07-02T14:41:52.977" LastEditDate="2014-07-02T14:41:52.977" LastEditorUserId="17661" OwnerUserId="17661" ParentId="105525" PostTypeId="2" Score="4" />
  
  <row AnswerCount="0" Body="&lt;p&gt;See I was under the impression that when we are calculating the error with respect to the weights connecting the last hidden layer with the output layer we are suppose to get the error of the output layer multiply it with the derivative of sigmoid, the current weights and the outputs of the last hidden layer. &lt;/p&gt;&#10;&#10;&lt;p&gt;But if you go through the Andrew NG's course in &lt;a href=&quot;https://www.coursera.org/course/ml&quot; rel=&quot;nofollow&quot;&gt;Coursera&lt;/a&gt; you will see that he doesn't multiply the sigmoid derivative for the last layer!! This is even mentioned in the &lt;a href=&quot;https://spark-public.s3.amazonaws.com/ml/exercises/ml-005/mlclass-ex4-005.zip&quot; rel=&quot;nofollow&quot;&gt;pdf&lt;/a&gt; he provided page 9. &lt;/p&gt;&#10;&#10;&lt;p&gt;Also I find that when I do multiply it with a sigmoid derivative I get multiple peaks in the regularization vs training error plot, the NN converges to very high values of training errors in few iterations on the MNSIT handwritten digits dataset. Without the sigmoid derivative the regularization vs error is a smooth singular peak plot and goes on decreasing till 1000 iterations!&lt;/p&gt;&#10;&#10;&lt;p&gt;Am I missing something here ? Cause from the calculus I learnt it seems obvious that sigmoid derivative needs to be multiplied ! &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-02T13:18:07.970" Id="105531" LastActivityDate="2014-07-02T13:25:15.067" LastEditDate="2014-07-02T13:25:15.067" LastEditorUserId="49379" OwnerUserId="49379" PostTypeId="1" Score="0" Tags="&lt;neural-networks&gt;" Title="Do we multiply the derivative of the sigmoid function when computing the errors of the weights connecting the last hidden layer to the output layer?" ViewCount="54" />
  <row Body="&lt;p&gt;The answer depends on your question.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's how I've interpreted it:  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;You're got 5 chromosomes, you're interested in the event that &lt;strong&gt;any one of them&lt;/strong&gt; is zero independent of the rest of them, and you're calling this the &quot;total probability&quot;.  You'd like to know what combination of individual rates ensures that this total probability is zero or negligible.&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The key is what you mean by negligible.  &lt;/p&gt;&#10;&#10;&lt;p&gt;If you are willing to accept a &lt;strong&gt;1%&lt;/strong&gt; chance that none of them are zero, then your answer is &lt;code&gt;lambda=6.3&lt;/code&gt;, with all chromosomes having a uniform rate.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; &amp;gt; sum(dpois(x=0,lambda=rep(6.3,5)))    # uniform rate for all 5 chromosomes&#10; [1] 0.009182           # less than 1% total probability&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;For a &lt;strong&gt;5%&lt;/strong&gt; chance, the answer is &lt;code&gt;lambda=4.7&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;The individual rates, however, could be smaller or larger, provided they contribute to keep the total within tolerance.  For example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; &amp;gt; sum(dpois(x=0,lambda=c(6,6,7,8,8)))  # non-uniform rates&#10; [1] 0.00654   # also less than 1% total probability&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Solution (uniform rates):&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Define a function that does the calculation.  Here I've assumed all rates are equal, though you can set lambda directly to &lt;code&gt;rate&lt;/code&gt; and pass in any vector of rates.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; all_chrom_zero &amp;lt;- function(rate) {&#10;     sum(dpois(x=0,lambda=rep(rate,5)))&#10;  }&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Then use &lt;code&gt;sapply&lt;/code&gt; to run it over a list of trial rates, and choose the smallest rate that comes in under your confidence threshold -- in this case 1%&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; tol &amp;lt;- 0.01&#10; wt&amp;lt;-seq(0,10,0.1)   # candidate rates&#10; min(wt[which(sapply(wt,all_chrom_zero)&amp;lt;0.01)])&#10; [1] 6.3&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;The above code has been structured to take a matrix without too much difficulty (using &lt;code&gt;sapply&lt;/code&gt;). So you could do away with the uniform assumption entirely and actually run your code to simulate over various 5-tuples of rates.&lt;/p&gt;&#10;&#10;&lt;p&gt;This would be a plus if your biological model has information about the distribution of the rates of the individual chromosomes.  If not, you could draw random samples from a Gaussian distribution for each chromosome's rate, each centred at the uniform solution, and run the calculations over these, or if you like brute force calculations, you could (just for fun!) also run the calculation over a 5D grid and look for the &quot;equiprobable&quot; curves at various tolerance levels.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Background for &lt;code&gt;sapply&lt;/code&gt;&lt;/em&gt;&#10;Using &lt;code&gt;sapply&lt;/code&gt; allows one to avoid explicit looping in R and is quite powerful for building simulations.  If you want a bit of background, I've referenced some &lt;a href=&quot;http://stackoverflow.com/a/23282110/181638&quot;&gt;useful primers in an answer here&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-07-02T13:21:03.353" Id="105532" LastActivityDate="2014-07-02T16:43:07.930" LastEditDate="2014-07-02T16:43:07.930" LastEditorUserId="14640" OwnerUserId="14640" ParentId="105525" PostTypeId="2" Score="3" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Apologies in advance if this is very basic; I am quite new to statistics.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a large set of 7 dimensional data, for two groups.&lt;/p&gt;&#10;&#10;&lt;p&gt;In my example, they are express relative preferences over a certain bundle of goods (for men, and for women). (Weightings of the parameters in a representative utility function, so for example X1=&amp;lt;2,3,1,4,5,2,7&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;As such, one could interpret them as either two sets of multivariate data, or equivalently, two sets of 7-dimensional directional unit vectors, without any loss of relevant information.&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to see if the populations have different means.&lt;/p&gt;&#10;&#10;&lt;p&gt;First question, should I try to use a multivariate t test, or delve into the literature on directional statistics? (the latter seems to represent the intuition behind the data structure better)&lt;/p&gt;&#10;&#10;&lt;p&gt;Secondly, can you recommend R packages that might be able to do either of the above? The 'circular' statistics package, as far as I can tell, does not extend to N dimensional vectors.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thirdly, and I'm throwing this out there more if anyone could indicate which area of literature to examine, suppose the sets of data represent ordered variables? And supposing they were paired instead of independent (say representing twin 1 and twin 2)? What would be the best topic to investigate to help answer questions such as, for example, does a skew in the 'twin 1' data towards the first few ('lower order' variables) correlate with a similar skew towards the first few in the corresponding 'twin 2' data? Or if twin 1 highly prefers 'higher order' goods twin 2 prefers 'lower order' goods? And so on.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance!&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;EDIT   (adding information given in comment)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So a little bit of context - the data arises from a simulation exercise where users are given a budget of say £10 and a menu of prices for different items. From each 'market' we assume their choice is their optimal bundle and we use a generic utility function = $U = \alpha(\ln (x_1))+\beta(\ln (x_2) +\dots$. For $k=7$ items to compute their relative weights for each item. That is, the solution can be represented by the 7D unit vector; magnitude is irrelevant. I'm trying to see if men and women differ systematically in their preferences. &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-07-02T14:32:42.227" Id="105540" LastActivityDate="2014-07-20T16:11:27.973" LastEditDate="2014-07-20T16:11:27.973" LastEditorUserId="11887" OwnerUserId="48754" PostTypeId="1" Score="2" Tags="&lt;t-test&gt;&lt;multivariate-analysis&gt;&lt;directional-statistics&gt;" Title="Comparing Multivariate Means" ViewCount="49" />
  <row AcceptedAnswerId="105555" AnswerCount="1" Body="&lt;p&gt;Suppose we have a sequence of items, from an alphabet of 2: {0,1}.  For example, this 8-bit sequence: 10001010&lt;/p&gt;&#10;&#10;&lt;p&gt;You generate a random 8-bit string.  The probability of you matching the first bit is $1/2$ or 50%, the last bit also $1/2$ or 50%, and of matching both: $1/2 * 1/2$ or 25%. &lt;/p&gt;&#10;&#10;&lt;p&gt;Part 1 concerns how many attempts someone would have to make to have the first and last bit match.  &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Intuitively, it seems like 4 attempts would yield a high probability of achieving this match.  But statistics is not always intuitive...&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Part 2 concerns any particular bit in the sequence, but trying to match the most number possible. &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;What is the probability of matching &lt;em&gt;any&lt;/em&gt; bit correctly? (Of matching any $n$ bits correctly?)  &lt;/li&gt;&#10;&lt;li&gt;How many random generations would someone need to make to reach $n$ bit matches with probability=$1/2$? (With probability=$3/4$?)&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="1" CreationDate="2014-07-02T15:48:25.500" Id="105546" LastActivityDate="2014-07-02T16:39:35.943" OwnerUserId="4703" PostTypeId="1" Score="0" Tags="&lt;probability&gt;" Title="How would you calculate the probability of matching the value of a sequenced list?" ViewCount="59" />
  <row Body="&lt;p&gt;In order to determine the optimal model you'll need to estimate an optimal penalization parameter $\lambda$.  This can be done using the built in cross-validation function in &lt;code&gt;glmnet&lt;/code&gt; called &lt;code&gt;cv.glmnet&lt;/code&gt;.  Once you've fit your cross validated model with &lt;code&gt;cv.glmnet&lt;/code&gt; the model object will contain all sorts of useful values.  These are described under the &lt;em&gt;Values&lt;/em&gt; header in the &lt;code&gt;cv.glmnet&lt;/code&gt; help file.  They can be extracted with the typical extractors, for example &lt;code&gt;coef()&lt;/code&gt;, or using the &lt;code&gt;$&lt;/code&gt; operator on the &lt;code&gt;cv.glmnet&lt;/code&gt; object.&lt;/p&gt;&#10;&#10;&lt;p&gt;A word of warning though: be &lt;strong&gt;very&lt;/strong&gt; careful with the default &lt;code&gt;glmnet&lt;/code&gt; arguments.  Make sure you understand what &lt;strong&gt;each one&lt;/strong&gt; is doing and why it's appropriate for your specific problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;So almost verbatim from the cv.glmnet help file:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; ## simulate data&#10;&amp;gt; set.seed(1010)&#10;&amp;gt; n=1000;p=100&#10;&amp;gt; nzc=trunc(p/10)&#10;&amp;gt; x=matrix(rnorm(n*p),n,p)&#10;&amp;gt; beta=rnorm(nzc)&#10;&amp;gt; fx= x[,seq(nzc)] %*% beta&#10;&amp;gt; eps=rnorm(n)*5&#10;&amp;gt; y=drop(fx+eps)&#10;&amp;gt; ## run cross-validation with all the default argumnets&#10;&amp;gt; cvob1=cv.glmnet(x,y)&#10;&amp;gt; ## one way to print the coefficients of the optimal model&#10;&amp;gt; coef(cvob1)&#10;101 x 1 sparse Matrix of class &quot;dgCMatrix&quot;&#10;                     1&#10;(Intercept) -0.1162737&#10;V1          -0.2171531&#10;V2           0.3237422&#10;V3           .        &#10;V4          -0.2190339&#10;V5          -0.1856601&#10;V6           0.2530652&#10;V7           0.1874832&#10;V8          -1.3574323&#10;V9           1.0162046&#10;V10          0.1558299&#10;V11          .        &#10;V12          .        &#10;V13          .        &#10;V14          .        &#10;V15          .        &#10;V16          .        &#10;V17          .        &#10;V18          .        &#10;V19          .        &#10;V20          .        &#10;V21          .        &#10;V22          .        &#10;V23          .        &#10;V24          .        &#10;V25          .        &#10;V26          .        &#10;V27          .        &#10;V28          .        &#10;V29          .        &#10;V30          .        &#10;V31          .        &#10;V32          .        &#10;V33          .        &#10;V34          .        &#10;V35          .        &#10;V36          .        &#10;V37          .        &#10;V38          .        &#10;V39          .        &#10;V40          .        &#10;V41          .        &#10;V42          .        &#10;V43          .        &#10;V44          .        &#10;V45          .        &#10;V46          .        &#10;V47          .        &#10;V48          .        &#10;V49          .        &#10;V50          .        &#10;V51          .        &#10;V52          .        &#10;V53          .        &#10;V54          .        &#10;V55          .        &#10;V56          .        &#10;V57          .        &#10;V58          .        &#10;V59          .        &#10;V60          .        &#10;V61          .        &#10;V62          .        &#10;V63          .        &#10;V64          .        &#10;V65          .        &#10;V66          .        &#10;V67          .        &#10;V68          .        &#10;V69          .        &#10;V70          .        &#10;V71          .        &#10;V72          .        &#10;V73          .        &#10;V74          .        &#10;V75         -0.1420966&#10;V76          .        &#10;V77          .        &#10;V78          .        &#10;V79          .        &#10;V80          .        &#10;V81          .        &#10;V82          .        &#10;V83          .        &#10;V84          .        &#10;V85          .        &#10;V86          .        &#10;V87          .        &#10;V88          .        &#10;V89          .        &#10;V90          .        &#10;V91          .        &#10;V92          .        &#10;V93          .        &#10;V94          .        &#10;V95          .        &#10;V96          .        &#10;V97          .        &#10;V98          .        &#10;V99          .        &#10;V100         .        &#10;&amp;gt; ## a view of the cvob1 values&#10;&amp;gt; str(cvob1)&#10;List of 10&#10; $ lambda    : num [1:74] 1.78 1.62 1.48 1.35 1.23 ...&#10;     $ cvm       : num [1:74] 31.3 30.8 30.3 29.8 29.3 ...&#10; $ cvsd      : num [1:74] 0.994 1.008 1.022 1.045 1.054 ...&#10;     $ cvup      : num [1:74] 32.2 31.8 31.4 30.9 30.3 ...&#10; $ cvlo      : num [1:74] 30.3 29.8 29.3 28.8 28.2 ...&#10;     $ nzero     : Named int [1:74] 0 1 1 2 2 2 2 2 2 2 ...&#10;  ..- attr(*, &quot;names&quot;)= chr [1:74] &quot;s0&quot; &quot;s1&quot; &quot;s2&quot; &quot;s3&quot; ...&#10; $ name      : Named chr &quot;Mean-Squared Error&quot;&#10;      ..- attr(*, &quot;names&quot;)= chr &quot;mse&quot;&#10;     $ glmnet.fit:List of 12&#10;  ..$ a0       : Named num [1:74] -0.157 -0.156 -0.155 -0.153 -0.149 ...&#10;      .. ..- attr(*, &quot;names&quot;)= chr [1:74] &quot;s0&quot; &quot;s1&quot; &quot;s2&quot; &quot;s3&quot; ...&#10;      ..$ beta     :Formal class 'dgCMatrix' [package &quot;Matrix&quot;] with 6 slots&#10;  .. .. ..@ i       : int [1:4266] 7 7 7 8 7 8 7 8 7 8 ...&#10;  .. .. ..@ p       : int [1:75] 0 0 1 2 4 6 8 10 12 14 ...&#10;  .. .. ..@ Dim     : int [1:2] 100 74&#10;  .. .. ..@ Dimnames:List of 2&#10;  .. .. .. ..$ : chr [1:100] &quot;V1&quot; &quot;V2&quot; &quot;V3&quot; &quot;V4&quot; ...&#10;      .. .. .. ..$ : chr [1:74] &quot;s0&quot; &quot;s1&quot; &quot;s2&quot; &quot;s3&quot; ...&#10;  .. .. ..@ x       : num [1:4266] -0.1565 -0.2991 -0.4299 0.0459 -0.5505 ...&#10;  .. .. ..@ factors : list()&#10;  ..$ df       : int [1:74] 0 1 1 2 2 2 2 2 2 2 ...&#10;      ..$ dim      : int [1:2] 100 74&#10;  ..$ lambda   : num [1:74] 1.78 1.62 1.48 1.35 1.23 ...&#10;      ..$ dev.ratio: num [1:74] 0 0.0172 0.0315 0.0475 0.0676 ...&#10;  ..$ nulldev  : num 31247&#10;      ..$ npasses  : int 298&#10;  ..$ jerr     : int 0&#10;      ..$ offset   : logi FALSE&#10;  ..$ call     : language glmnet(x = x, y = y)&#10;      ..$ nobs     : int 1000&#10;  ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;elnet&quot; &quot;glmnet&quot;&#10; $ lambda.min: num 0.23&#10;     $ lambda.1se: num 0.402&#10; - attr(*, &quot;class&quot;)= chr &quot;cv.glmnet&quot;&#10; &amp;gt; cvob1$lambda.min&#10; [1] 0.2300227&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-07-02T15:54:01.463" Id="105548" LastActivityDate="2014-07-02T15:54:01.463" OwnerUserId="21683" ParentId="105541" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Binning of continuous variables is never the answer.  Your post raises a huge number of issues.  Many of these are covered in my handouts at &lt;a href=&quot;http://biostat.mc.vanderbilt.edu/CourseBios330&quot; rel=&quot;nofollow&quot;&gt;http://biostat.mc.vanderbilt.edu/CourseBios330&lt;/a&gt;.  Note that the Hosmer-Lemeshow test is no longer competitive with continuous calibration curve methods.  Data splitting requires perhaps 15,000 observations to be reliable.  Look at resampling methods in my handout.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-02T16:03:26.863" Id="105549" LastActivityDate="2014-07-02T16:03:26.863" OwnerUserId="4253" ParentId="105298" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="105558" AnswerCount="2" Body="&lt;p&gt;Let's say that I have two samples:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;A    B  C    D&#10;100  5  17   12  #&amp;lt;- Method 1&#10;90   2  4    15  #&amp;lt;- Method 2&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And want to compare to test whether method 1 is different from method 2. However, that observations in the samples are tied together, say &lt;code&gt;100&lt;/code&gt; and &lt;code&gt;90&lt;/code&gt; come from the same market &lt;code&gt;A&lt;/code&gt;. What is the proper way to test this?&lt;/p&gt;&#10;&#10;&lt;p&gt;I am pretty sure that lumping everything together and doing a t-test or Mann-Whitney U test is not the way to go.&lt;/p&gt;&#10;" CommentCount="12" CreationDate="2014-07-02T16:11:16.577" Id="105550" LastActivityDate="2014-07-02T23:43:11.180" OwnerUserId="11708" PostTypeId="1" Score="1" Tags="&lt;statistical-significance&gt;" Title="Significance test between two samples when positions matter" ViewCount="55" />
  <row AcceptedAnswerId="105559" AnswerCount="1" Body="&lt;p&gt;Suppose that $X_1,X_2,\ldots,X_n$ is a random sample from a distribution with probability function:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$p_X(x)=\begin{cases} 1/2 &amp;amp;x=-1,1 \\ 0 &amp;amp; \text{otherwise} \end{cases}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Now if we define the average &lt;/p&gt;&#10;&#10;&lt;p&gt;$\bar{X}= \frac{1}{n} \sum X_i$, I need to show that that for an odd $n$ the probability function for $\bar{X}$ is:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$p_{\bar{X}} (x)= \frac{\binom{n}{\frac{n}{2} (x+1)}}{2^n}$$ for $x=\pm 1/n,\pm 3/n,\ldots,\pm 1$, $0$ otherwise.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Since $P[\sum X_i=k] =P [\bar{X}=k/n ]$, I thought it would be easier to first derive the probability function for the sum. Since they are still $2^n$ n-tuples and each one is equilikely, I need to count the ones whose sum is $k$ for:&lt;/p&gt;&#10;&#10;&lt;p&gt;$k=\pm 1,\pm 3,\ldots \pm n-2,\pm n$ (since even numbers are not a feasible combination)&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem here though is that I do not immediately recognize how I could count all these tuples. Could you please help me with this? If it is easier to proceed another way, I am of course all ears.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-07-02T16:14:18.100" FavoriteCount="1" Id="105552" LastActivityDate="2014-07-02T23:48:02.787" LastEditDate="2014-07-02T23:48:02.787" LastEditorUserId="34826" OwnerUserId="31420" PostTypeId="1" Score="6" Tags="&lt;probability&gt;&lt;self-study&gt;&lt;binomial&gt;&lt;combinatorics&gt;" Title="Probability mass function of the sample mean" ViewCount="228" />
  <row Body="&lt;p&gt;Two observations per month is not much information to carry out seasonal adjustment.&lt;/p&gt;&#10;&#10;&lt;p&gt;While you gather more data, you may be interested in measuring how consumption changes with temperature. For that, you can fit the following model (a quadratic relationship between consumption and temperature seems appropriate at first glance but see comment by whuber below):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;cons &amp;lt;- structure(c(156, 199, 173, 69, 63, 9, 9, 9, 15, 19, 83, 62), .Tsp = c(1, &#10;  1.91666666666667, 12), class = &quot;ts&quot;)&#10;temp &amp;lt;- structure(c(1.4, 0.3, 2.3, 9.6, 12.2, 16.9, 20.8, 18.5, 14.3, &#10;  11.4, 5.2, 4.2), .Tsp = c(1, 1.91666666666667, 12), class = &quot;ts&quot;)&#10;&#10;fit &amp;lt;- lm(c(cons) ~ poly(c(temp), 2))&#10;summary(fit)&#10;#Residuals:&#10;#    Min      1Q  Median      3Q     Max &#10;#-52.340  -7.920  -2.102  17.963  34.661 &#10;#Coefficients:&#10;#                  Estimate Std. Error t value Pr(&amp;gt;|t|)    &#10;#(Intercept)         72.167      7.451   9.686 4.66e-06 ***&#10;#poly(c(temp), 2)1 -201.911     25.810  -7.823 2.65e-05 ***&#10;#poly(c(temp), 2)2   69.987     25.810   2.712   0.0239 *  &#10;#---&#10;#Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1&#10;#Residual standard error: 25.81 on 9 degrees of freedom&#10;#Multiple R-squared:  0.8839,   Adjusted R-squared:  0.8582 &#10;#F-statistic: 34.28 on 2 and 9 DF,  p-value: 6.18e-05&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Display observed data (black points) and fitted values (in blue):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;plot(c(temp), c(cons), pch = 16)&#10;lines(c(sort(temp)), predict(fit)[order(temp)], type = &quot;b&quot;, col = &quot;blue&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Upon this regression you can obtain the predicted value for consumption given for example temperatures of 5 and 15 Celsius degrees (along with 95% confidence intervals).&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;p &amp;lt;- predict(fit, newdata = data.frame(temp = c(5, 15)), se.fit = TRUE)&#10;res &amp;lt;- cbind(p$fit - 1.96 * p$se, p$fit, p$fit + 1.96 * p$se)&#10;colnames(res) &amp;lt;- c(&quot;lower limit&quot;, &quot;pred&quot;, &quot;upper limit&quot;)&#10;res&#10;#  lower limit     pred upper limit&#10;#1   83.072304 102.5629   122.05356&#10;#2   -5.487951  14.9201    35.32816&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;As you obtain more data you can think of further methods. See, for instance, &#10;the documentation and examples for function &lt;code&gt;decompose&lt;/code&gt;. You could also include seasonal dummies in the above regression. These dummies are indicator variables pointing each one to observations recorded at a given month, e.g. [1 0 0 0 0 0 0 0 0 0 0 0], [0 1 0 0 0 0 0 0 0 0 0 0] and so on. (Note: one of the the 12 seasonal dummies should be omitted in the regression if an intercept is included.)&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-07-02T20:06:22.040" Id="105583" LastActivityDate="2014-07-03T06:49:08.213" LastEditDate="2014-07-03T06:49:08.213" LastEditorUserId="48766" OwnerUserId="48766" ParentId="105574" PostTypeId="2" Score="2" />
  
  
  <row AcceptedAnswerId="105665" AnswerCount="1" Body="&lt;blockquote&gt;&#10;  &lt;p&gt;Suppose $X_1, \cdots, X_n$ are independent, nonnegative continuous functions, each $X_i$ has hazard function $\lambda_i(x)$.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;If $V=\max\{X_1, \cdots, X_n\}$, I need to show that $\lambda_V(x)\leq \min\{\lambda_1(x),\cdots, \lambda_n(x)\}$.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I know that $\displaystyle\lambda_V(x)=\frac{f_V(x)}{1-F_V(x)}=\frac{\sum_j f_j(x)\prod_{i\neq j}F_i(x)}{1-\prod_{i=1}^n F_i(x)}$. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, I'm stuck here.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any help would be appreciated.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-02T22:58:20.877" FavoriteCount="0" Id="105600" LastActivityDate="2014-07-03T13:45:59.120" OwnerUserId="40252" PostTypeId="1" Score="3" Tags="&lt;survival&gt;&lt;stochastic-processes&gt;&lt;hazard&gt;" Title="Showing $ λ_V(x) \leq min\{λ_1(x),\cdots,λ_n(x)\}$ Hazard function" ViewCount="51" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a training dataset with a binary response variable, 6 independent variables, and 21,000 observations.&lt;/p&gt;&#10;&#10;&lt;p&gt;I've fit both an ordinary regression tree and a random forest (mtry = 2, ntree = 2000) and there is almost no difference between the two when each model is validated, using RMSE and predicted to actual ratio as goodness of fit metrics.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is this to be expected with a small number of independent variables, or am I not using the right metrics to measure goodness of fit?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-07-03T01:11:46.213" Id="105608" LastActivityDate="2014-07-03T01:11:46.213" OwnerUserId="46522" PostTypeId="1" Score="3" Tags="&lt;random-forest&gt;&lt;goodness-of-fit&gt;&lt;cart&gt;" Title="Why will a random forest not outperform a regression tree?" ViewCount="133" />
  <row Body="&lt;p&gt;The main source is Stephen M. Stigler, &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/067440341X&quot;&gt;&lt;em&gt;The History of Statistics&lt;/em&gt;&lt;/a&gt;, Part One, &quot;The Development of Mathematical Statistics in Astronomy and Geodesy before 1827&quot;. Another useful source is John Aldrich, &lt;a href=&quot;http://www.economics.soton.ac.uk/staff/aldrich/Figures.htm&quot;&gt;Figures from the History of Probability and Statistics&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;You could also look at Searle, Casella and McCulloch, &lt;a href=&quot;http://eu.wiley.com/WileyCDA/WileyTitle/productCd-0470009594.html&quot;&gt;Variance Components&lt;/a&gt;, chap. 2:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;p. 23: The method of least squares was independently discovered by Legendre and Gauss. The story is told by R.L. Plackett, &quot;&lt;a href=&quot;http://www.jstor.org/stable/2334569&quot;&gt;Studies in the History of Probability and Statistics. XXIX: The Discovery of the Method of Least Squares&lt;/a&gt;&quot;, &lt;em&gt;Biometrika&lt;/em&gt;, 59, 239-251.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;p. 24: According to R.D. Anderson, &quot;astronomers understood the concept of degrees of freedom (but without using the term) as early as the year 1852&quot;. He refers to B. J. Peirce, &quot;Criterion for the rejection of doubtful observations&quot;, &lt;em&gt;The Astronomical Journal&lt;/em&gt;, 2, 161-163 (see &lt;a href=&quot;http://articles.adsabs.harvard.edu/full/1855AJ......4...81G&quot;&gt;here&lt;/a&gt;), who specified &quot;the sum of squares of all errors' as being $(N-m)\varepsilon^2$, where $N$ is the&#10;total number of observations, $m$ is the number of unknown quantities contained&#10;in the observations and $\varepsilon^2$ is the mean error (sample variance).&quot;&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;pages 23-24: The first formulation of a random effects model is that of &lt;a href=&quot;http://en.wikipedia.org/wiki/George_Biddell_Airy&quot;&gt;George Biddell Airy&lt;/a&gt;, in a monograph published in 1861. See also Marc Nerlove, &quot;The History of Panel Data Econometrics, 1861-1997&quot;, in &lt;a href=&quot;http://www.cambridge.org/it/academic/subjects/economics/natural-resource-and-environmental-economics/essays-panel-data-econometrics&quot;&gt;&lt;em&gt;Essays in Panel Data Econometrics&lt;/em&gt;&lt;/a&gt;: &quot;what Airy calls a &lt;em&gt;Constant error&lt;/em&gt;, we would call a random day effect&quot;. It is the error that remains even when every known instrumental correction has been applied.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;pages 24-25: The second use of a random effects model appears in W. Chauvenet, &lt;em&gt;A Manual of Spherical and Practical Astronomy, 2: Theory and Use of&#10;Astronomical Instruments&lt;/em&gt;, 1863. He derived the variance of $\bar{y}_{..}=\sum_{i=1}^a\sum_{j=1}^n y_{ij}/an$ as &#10;$$\text{var}(\bar{y}_{..})=\frac{\sigma^2_a+\sigma^2_e/n}{a}$$&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2014-07-03T07:06:07.633" Id="105622" LastActivityDate="2014-07-03T07:31:12.467" LastEditDate="2014-07-03T07:31:12.467" LastEditorUserId="22047" OwnerUserId="44965" ParentId="105601" PostTypeId="2" Score="12" />
  <row Body="&lt;p&gt;I assume you are not limited to R, since this is a big data problem you probably shouldn't be. You can try &lt;a href=&quot;https://spark.apache.org/mllib/&quot; rel=&quot;nofollow&quot;&gt;MLlib&lt;/a&gt;, which is Apache Spark's scalable machine learning library.&lt;/p&gt;&#10;&#10;&lt;p&gt;Apache &lt;a href=&quot;http://spark.apache.org/&quot; rel=&quot;nofollow&quot;&gt;Spark&lt;/a&gt;, in turn, is a fast and general engine for in-memory large-scale data processing. These operate on a Hadoop framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models. It is designed to scale up from single servers to thousands of machines, each offering local computation and storage.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that 'thousands of machines' is optional(!), you can set it up on your local work/home desktop as well.&lt;/p&gt;&#10;&#10;&lt;p&gt;Going back to MLlib, it comes with the below algorithms out of the box:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;K-means clustering with K-means|| initialization.&lt;/li&gt;&#10;&lt;li&gt;L1- and L2-regularized linear regression.&lt;/li&gt;&#10;&lt;li&gt;&lt;strong&gt;L1- and L2-regularized logistic regression.&lt;/strong&gt;&lt;/li&gt;&#10;&lt;li&gt;Alternating least squares collaborative filtering, with explicit ratings or implicit feedback.&lt;/li&gt;&#10;&lt;li&gt;Naive Bayes multinomial classification.&lt;/li&gt;&#10;&lt;li&gt;Stochastic gradient descent.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;If you are regularly working with big data, you may need to adopt a Hadoop solution. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-03T08:10:07.960" Id="105630" LastActivityDate="2014-07-03T08:10:07.960" OwnerUserId="28740" ParentId="101003" PostTypeId="2" Score="0" />
  
  <row AnswerCount="1" Body="&lt;p&gt;This question is in relation to a dataset I've previously discussed &lt;a href=&quot;http://stats.stackexchange.com/questions/105611/dealing-with-non-normal-distribution-in-big-datasets-when-do-we-throw-out-the/105614#105614&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm trying to determine whether a treatment affects not only the number of visits a patient has to the doctor, but also how long it takes for the injury to heal. The two are inherently interlinked. &lt;/p&gt;&#10;&#10;&lt;p&gt;Now the structure of the cohort changes over time because of censoring; when a wound is healed, the patient doesn't have to see the doctor and those left in the cohort are those with longer healing times and thus more doctor visits. Because of this &quot;intuitive&quot; interpretation of the data, I thought a cox proportional hazards model (problem below) and/or &quot;inverse&quot; Kaplan-Meier Curves would be good to show how the initial treatment affects outcome. &lt;/p&gt;&#10;&#10;&lt;p&gt;First I looked at the median and mean number of visits all patients had, which was around three. Then I stratified the whole cohort into those patients requiring $&amp;gt;3$ visits and $\leq 3$ visits. I then used the following function in R&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(survival)&#10;km &amp;lt;- km(Surv(Time, Visits&amp;gt;3)~Treatment, data=mydata)&#10;plot(km, fun=&quot;event&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This produced the following plot&#10;&lt;img src=&quot;http://i.stack.imgur.com/Ou7Wt.png&quot; alt=&quot;KM Plot&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I then wanted to basically do the same for a coxph hazards model, but have realized that its interpretation is a bit wonky as Treatment 2 was resulting in low HR, which getting your head around is a bit of task and I honestly don't think is right, because I'm trying to look at cumulative hazards.&lt;/p&gt;&#10;&#10;&lt;p&gt;The code used in R was: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;cox &amp;lt;- coxph(Surv(Time, Visits&amp;gt;3)~Treatment, data=mydata)&#10;summary(cox)&#10;cox(formula = Surv(Time, Visits&amp;gt;3) ~ Treatment, data=mydata)&#10;&#10;  n= 4302, number of events= 1514 &#10;   (41 observations deleted due to missingness)&#10;&#10;                                   coef exp(coef) se(coef)      z Pr(&amp;gt;|z|)    &#10;Treatment 2                    -0.36705   0.69278  0.05318 -6.902 5.12e-12 ***&#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1&#10;&#10;                               exp(coef) exp(-coef) lower .95 upper .95&#10;Treatment 2                       0.6928      1.443    0.6242    0.7689&#10;&#10;Concordance= 0.541  (se = 0.008 )&#10;Rsquare= 0.011   (max possible= 0.99 )&#10;Likelihood ratio test= 48.43  on 1 df,   p=3.419e-12&#10;Wald test            = 47.64  on 1 df,   p=5.119e-12&#10;Score (logrank) test = 48.13  on 1 df,   p=3.986e-12&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So I'm wondering &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Is there a cumulative hazards function in R for this?   &lt;/li&gt;&#10;&lt;li&gt;Is it even ok to stratify patients into two groups based on a time-dependent&#10;variable?  &lt;/li&gt;&#10;&lt;li&gt;How would you try to actually interpret (in &quot;lay-terms&quot;) the hazard ratio resulting from this model?  &lt;/li&gt;&#10;&lt;li&gt;Have I totally got this wrong?  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Cheers for your opinions and help. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-07-03T08:42:49.103" FavoriteCount="3" Id="105635" LastActivityDate="2015-02-04T17:01:21.553" LastEditDate="2014-07-04T07:07:08.997" LastEditorUserId="40785" OwnerUserId="40785" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;regression&gt;&lt;cox-model&gt;&lt;kaplan-meier&gt;" Title="Cumulative Hazard Function where &quot;status&quot; is dependent on &quot;time&quot;" ViewCount="114" />
  
  
  <row Body="&lt;p&gt;No problem, you can try plain vanilla OLS.&lt;/p&gt;&#10;&#10;&lt;p&gt;A toy example in R:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; set.seed(1235321)&#10;&amp;gt; x &amp;lt;- c(rep(1,50), rep(0,50))&#10;&amp;gt; epsilon &amp;lt;- rnorm(100)&#10;&amp;gt; y &amp;lt;- 1 + 3 * x + epsilon&#10;&amp;gt; coef(lm(y ~ x))&#10;(Intercept)           x &#10;   1.069068    2.987035 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Of course, you'll get poor results if your &quot;error&quot; contains relevant omitted variables. E.g.:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; z &amp;lt;- runif(100, 2, 6)&#10;&amp;gt; y &amp;lt;- 1 + 0.5 * x + 3 * z + epsilon  # y depends on x _and_ z&#10;&amp;gt; coef(lm(y ~ x))                     # you omit z&#10;(Intercept)           x &#10;  12.317986    1.201527 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This should be your main concern.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-07-03T16:31:50.017" Id="105692" LastActivityDate="2014-07-03T16:31:50.017" OwnerUserId="44965" ParentId="105688" PostTypeId="2" Score="6" />
  
  <row Body="&lt;p&gt;With no additional information, I am left to assume that the author is referring to the difficulty in performing statistical inference on a sample of 'voluntary' subjects, opposed to randomly selected subjects. Basic statistical inference rests on the assumption that a population sample is a random and representative selection from the population proper. If subjects are voluntary, then you cannot be sure that a voluntary patient is truly representative of the population. For example, a patient may be a more willing volunteer for the survey if their medical treatment was successful. Therefore your sample, and inferences therefrom, are statistically biased.&lt;/p&gt;&#10;&#10;&lt;p&gt;The basic premise of the statement appears to be that samples must be representative in order to 'generalize' statistical conclusions. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-03T17:16:36.713" Id="105700" LastActivityDate="2014-07-03T17:16:36.713" OwnerUserId="26357" ParentId="105693" PostTypeId="2" Score="4" />
  
  
  
  
  
  <row Body="&lt;p&gt;The mean of the hypergeometric distribution can be interpreted as the finite sampling equivalent of $\mu = np$ from the binomial, taking $p=\frac{K}{N}$.  The variance can be expressed as $\sigma^2 = \left(n\frac{K}{N}\frac{N-K}{N}\right)\left(\frac{N-n}{N-1}\right)$, which is exactly analogous to the binomial $\sigma^2=np(1-p)$, except that there is a correction factor of $\frac{N-n}{N-1}$ added.  So, in the limit as $N \to \infty$ and $\frac{K}{N} \to p$, the hypergeometric's moments approach the binomial.  It's better to show via a generating function approach (several are possible, I think the easiest is the pgf) that the random variable itself (rather than the moment sequence) converges.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-03T21:07:36.273" Id="105735" LastActivityDate="2014-07-03T21:07:36.273" OwnerUserId="48148" ParentId="89943" PostTypeId="2" Score="2" />
  <row AnswerCount="0" Body="&lt;p&gt;The study: Comparing two groups (intervention vs control) across time using a behavioral measure as the outcome variable of interest.  I will have baseline behavioral measurements as well as more than 2 time points throughout treatment.  I'm interested in finding any significant improvements from baseline across time (within each group) and whether or not there were significant differences between groups.  Would this be a repeated measures ANOVA or MLM? Would I need to control for baseline behavioral measurement? Thank you in advance for any help with this!!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-03T22:14:31.827" Id="105741" LastActivityDate="2014-07-03T22:14:31.827" OwnerUserId="49477" PostTypeId="1" Score="1" Tags="&lt;intervention-analysis&gt;" Title="Need to find appropriate statistical test for a longitudinal intervention study" ViewCount="19" />
  <row AcceptedAnswerId="105766" AnswerCount="2" Body="&lt;p&gt;Can one just fit a linear model between $Y$ and $X$, and then take the p-values of the $t$-test done on the slope (or beta) of the linear model? Since presumably correlation would be in the same direction as the slope/beta of the linear regression model?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-07-04T06:40:01.993" Id="105762" LastActivityDate="2014-07-04T07:20:52.387" LastEditDate="2014-07-04T06:53:14.943" LastEditorUserId="21054" OwnerUserId="5288" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;correlation&gt;" Title="How do you calculate the statistical significance of a correlation between $Y$ and $X$?" ViewCount="64" />
  <row AcceptedAnswerId="105781" AnswerCount="1" Body="&lt;p&gt;In below example, Is it possible to calculate new SD for one additional observation?&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;In a class of 25 students, 24 of them took an exam in class and 1&#10;  student took a make-up exam the following day. The professor graded&#10;  the first batch of 24 exams and found an average score of 74 points&#10;  with a standard deviation of 8.9 points. The student who took the&#10;  make-up the following day scored 64 points on the exam.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;New mean is 73.6, What is new SD? How to calculate it?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-07-04T07:53:11.133" FavoriteCount="1" Id="105773" LastActivityDate="2014-07-04T08:55:29.503" OwnerUserId="46253" PostTypeId="1" Score="2" Tags="&lt;standard-deviation&gt;" Title="How to calculate SD of sample for one new observation?" ViewCount="367" />
  <row Body="&lt;p&gt;I don´t know if I understood well your question but here's my point of view about this... In my experience with real data ,even though with nothing related with health data, I usually prefer not to deal with cox models because I never met a case with proportional hazards. I've been  always in front of data where hazard rates are not constant at all with the pass of time.&lt;/p&gt;&#10;&#10;&lt;p&gt;The solutions I've been taken are basically AFT (accelerated failure time) models, wich are parametric regression models in case you are not familiar with..&lt;/p&gt;&#10;&#10;&lt;p&gt;I use both survreg and the psm family functions (based on survreg) at the excelent package called rms. There you'll be able to find functions associated with the survival objects to calculate hazard ratios, quantiles, means etc of the parametric function you choose for your model.&lt;/p&gt;&#10;&#10;&lt;p&gt;Even taking all of this into account as far as I know there's an important area of improvement modelling events where teoretical distributions don't fit well the data (multimodal distributions of time, for example) wich require an almost artesanal modelling strategies to fit good models.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-07-04T07:55:28.800" Id="105774" LastActivityDate="2014-07-04T07:55:28.800" OwnerUserId="16242" ParentId="105635" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;If your classifier produces only factor outcomes (only labels), without scores, you still can draw a ROC curve. However this ROC curve is only a point. Considering the ROC space, this points is $(x,y) = (FPR,TPR)$, where $FPR$ - false positive rate and $TPR$ - true positive rate.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/p8hYh.png&quot; width=&quot;500&quot; height=&quot;500&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;See more on how this is computed on &lt;a href=&quot;http://en.wikipedia.org/wiki/Receiver_operating_characteristic&quot; rel=&quot;nofollow&quot;&gt;Wikipedia page&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can extend this point to look like a ROC curve by drawing a line from $(0,0)$ to your point, and from there to $(1,1)$. Thus you have a curve.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, for a decision tree is easy to extend from an label output to a numeric output. Note that when you predict with a decision tree you go down from the root node to a leaf node, where you predict with majority class. If instead of that class you would return the proportion of classes in that leaf node, you would have a score for each class. Suppose that you have two classes T and F, and in your leaf node you have 10 instances with T and 5 instances with F, you can return a vector of scores: $(score_T,score_F) = ( \frac{count_T}{count_T+count_F}, \frac{count_F}{count_T+count_F}) = (10/15,5/15) = (0.66, 0.33)$. Take care that this is really note a proper scoring rule (this are not the best estimator for probabilities), but is better than nothing I believe, and this is how usually scores are retrieved for decision trees.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-04T09:24:36.010" Id="105786" LastActivityDate="2014-07-04T09:24:36.010" OwnerUserId="16709" ParentId="105760" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="105899" AnswerCount="1" Body="&lt;blockquote&gt;&#10;  &lt;p&gt;Consider the estimator $b_1=\frac{\sum y_i}{\sum x_i}$. Suppose that $y_i = \beta x_i + \epsilon_i$, $E[\epsilon_i]=0$, $E[\epsilon_i \epsilon_j] (i \neq j)$ and $E[\epsilon_i^2]=\sigma_i^2$. Find a model for the variance of $b_1$ for which th estimator is BLUE.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;The answer is supposed to be:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;$v_i=x_i$ and $\sigma_i^2=\sigma^2 x_i$&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;However, I am not sure how they got to this answer. Could anyone please help?&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Update:&lt;/em&gt;&lt;br&gt;&#10;I tried computing the variance of $b_1$. I get:&#10;$$&#10;{\rm Var}(b_1)=\frac 1 {(∑x^2_i)^2}∑x^2_i\ {\rm Var}(\varepsilon_i)&#10;$$&#10;I don't see however how I could proceed from here or whether this was the right thing to do.&lt;/p&gt;&#10;" CommentCount="14" CreationDate="2014-07-04T09:53:57.807" Id="105787" LastActivityDate="2014-07-05T15:30:30.613" LastEditDate="2014-07-05T15:00:55.473" LastEditorUserId="49504" OwnerUserId="49504" PostTypeId="1" Score="4" Tags="&lt;regression&gt;&lt;self-study&gt;&lt;least-squares&gt;&lt;weighted-regression&gt;" Title="Weighted least squares" ViewCount="110" />
  
  <row Body="&lt;p&gt;If you end up with the [0,1] averaged distribution, you could use a &lt;a href=&quot;http://arxiv.org/abs/1103.2372&quot; rel=&quot;nofollow&quot;&gt;zero-one-inflated beta&lt;/a&gt; model as well. Flexible, easy to implement.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-04T15:17:52.653" Id="105812" LastActivityDate="2014-07-04T15:17:52.653" OwnerUserId="36229" ParentId="105804" PostTypeId="2" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;So I have collected data for 50 years of my dependent variable (continuous) data (x), independent (continuous) variable (y) across 10 locations. &lt;/p&gt;&#10;&#10;&lt;p&gt;My null hypothesis is that the regression slopes between x and y is homogeneous across 10 locations (i.e. slopes of y against x is similar across all 10 locations) against the alternate hypothesis that slopes are heterogeneous (i.e. slopes of y against x is different for different States).&lt;/p&gt;&#10;&#10;&lt;p&gt;I thought I could just do this by doing:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; mdl1&amp;lt;-y ~ x*location       # includes interaction x and location&#10; mdl2&amp;lt;-y ~ x + location     # no interaction between x and location&#10; anova(mdl1,mdl2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;If the model with the interaction term is a better fit, then I can deduce that at least two locations have different slopes. But I really want to know which slopes are different for which I need to do a posthocs test. In this case what posthocs test should I do?&lt;/p&gt;&#10;&#10;&lt;p&gt;Also someone told me that this analysis is wrong and that I have to use a mixed effect models with locations as a random factor. I tried reading about the mixed effect models but could not understand it. What I want to know is whether the analysis above is wrong and do I actually need to use a mixed effect model?   &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-04T15:41:32.373" Id="105813" LastActivityDate="2014-07-04T15:41:32.373" OwnerUserId="46228" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;lmer&gt;&lt;ancova&gt;" Title="difference between mixed effect and a simple ANCOVA" ViewCount="30" />
  <row AnswerCount="0" Body="&lt;p&gt;I need your advice to know how to restore certain market maker algorithm based on historical data.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have dataset with limit order book information at discrete time periods. For every time moment it contains best bid/ask prices and volume of fist 10 asks and 10 bids. Also it has bid and ask prices - the choice of the market maker. It's known that market maker's decision at time $t$ depends on order book condition at $t$ and maybe previous time moments.&lt;/p&gt;&#10;&#10;&lt;p&gt;I tried to use machine learning regression methods like reg.tree ensembles, neural nets and SVR to educate a model based on given features but when I tried to check prediction quality on cross-validation it showed very bad quality.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I think that I should properly extract features from my data but I don't know how to do it.&#10;Or maybe should I use different model and not regression algorithms?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-04T19:33:16.957" Id="105837" LastActivityDate="2014-07-04T19:33:16.957" OwnerUserId="49528" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;prediction&gt;&lt;finance&gt;" Title="How to restore market making algorithm?" ViewCount="15" />
  <row Body="&lt;p&gt;That´s a big task for one person. I can suggest you only some books related to 1) and 3).&#10;If you want to analyse your data in R a good reference is &lt;a href=&quot;http://www.asdar-book.org/&quot; rel=&quot;nofollow&quot;&gt;Applied Spatial Data Analysis with R&lt;/a&gt; and the&lt;a href=&quot;http://cran.r-project.org/web/packages/sp/sp.pdf&quot; rel=&quot;nofollow&quot;&gt;&lt;code&gt;sp&lt;/code&gt; package&lt;/a&gt;. On the book´s webpage you can find sample data and the code used thorughout the book.&#10;Another books are:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.plantsciences.ucdavis.edu/plant/sda.htm&quot; rel=&quot;nofollow&quot;&gt;Spatial Data Analysis in Ecology and Agriculture using R&lt;/a&gt;, a free book &lt;a href=&quot;http://spatial-analyst.net/book/&quot; rel=&quot;nofollow&quot;&gt;A Practical Guide to Geostatistical Mapping&lt;/a&gt;, or &lt;a href=&quot;http://oscarperpinan.github.io/spacetime-vis/&quot; rel=&quot;nofollow&quot;&gt;Displaying time series, spatial and space-time data with R&lt;/a&gt;. If you will work with rasters in R, than there is a &lt;a href=&quot;http://cran.r-project.org/web/packages/raster/&quot; rel=&quot;nofollow&quot;&gt;&lt;code&gt;raster&lt;/code&gt; package &lt;/a&gt;. Or you can do it in GIS software, for example ArcGIS or SAGA GIS, and check the &lt;a href=&quot;http://gis.stackexchange.com/&quot;&gt;GIS Stackexchange&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;My answer did not cover the time series analysis though...try the &lt;a href=&quot;http://www.statmethods.net/advstats/timeseries.html&quot; rel=&quot;nofollow&quot;&gt;Quick R&lt;/a&gt; page.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-07-04T20:19:43.127" Id="105840" LastActivityDate="2014-07-04T20:19:43.127" OwnerUserId="49427" ParentId="105785" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Your hypotheses look good to me, but, &quot;You can imagine that it could cause stress to sit around oinking and mooing,&quot; sounds like a directional hypothesis, so a one-sided hypothesis is also worth considering. Then again, one-sided hypothesis tests seem to be rather controversial, so maybe not!&lt;/p&gt;&#10;&#10;&lt;p&gt;Your calculation of &lt;em&gt;t&lt;/em&gt; is incorrect. You're right that $\bar X=4.5, \mu=4$ and $N=16$, but the population standard deviation in the question is sort of a red herring. You need to use the sample standard deviation for a one-sample &lt;em&gt;t&lt;/em&gt;-test.&lt;/p&gt;&#10;&#10;&lt;p&gt;I don't understand why a $Z_{\rm critical}$ is requested. A &lt;em&gt;Z&lt;/em&gt;-test is inappropriate without knowledge of the experimental population's standard deviation. It doesn't seem justified to assume the given &lt;em&gt;SD&lt;/em&gt; parameter for the control population would apply equally to the test population. For one thing, without controlling for personality differences like extraversion or self-monitoring, making animal noises might increase variability in stress levels by allowing some people to release stress by acting silly while making others more stressed due to embarrassment. By the way, the question implies that the ordinal data from the Likert scale for stress assessment can be treated as continuous, which is also fishy...&lt;/p&gt;&#10;&#10;&lt;p&gt;Furthermore, even if a $t_{\rm critical}$ was requested instead, the appropriate value to provide would depend on the choice of false positive error rate $\alpha$ for the test. It's conventional to choose $\alpha=.05$, so if you haven't been given any other reason to choose a different error rate, you probably won't go wrong (for grading purposes at least) with $t_{(\alpha=.95,df=15)}=2.131$ as long as you don't call it a $Z_{\rm critical}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;You don't seem to have attempted the last part, &quot;Provide a statement of a significant or nonsignificant effect.&quot; Give it a shot. It shouldn't be too hard as long as you choose your words carefully (don't get too creative, and be careful not to claim more than your results actually tell you) and understand which conclusion is appropriate when your test statistic is higher vs. lower than the critical value.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-07-05T01:18:40.820" Id="105852" LastActivityDate="2014-07-05T01:18:40.820" OwnerUserId="32036" ParentId="105846" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;The paper from Clauset et al. warns (Section 4.2) against small sample sizes  (&amp;lt; 100) which are much easier to fit. You may want to consider using the direct comparisons of models.&lt;/p&gt;&#10;&#10;&lt;p&gt;While the p-value of the KS statistic with estimated parameters is an overestimate, the bootstrapping procedure you described is able to tackle this and provides a correct p-value given enough simulations.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, the way the goodness of fit is computed in your code is not correct as it does not strictly follow the procedure described in the paper, and implemented in the &lt;code&gt;poweRlaw&lt;/code&gt; package.&lt;/p&gt;&#10;&#10;&lt;p&gt;Specifically: the synthetic data generation procedure is half implemented, it does not search for the best &lt;code&gt;xmin&lt;/code&gt; as provided by the &lt;code&gt;extimate_xmin&lt;/code&gt; function of the &lt;code&gt;poweRlaw&lt;/code&gt; package, and finally the &lt;code&gt;ks.test&lt;/code&gt; discards all the ties, which the package doesn't with its built-in KS test. &lt;/p&gt;&#10;&#10;&lt;p&gt;On this page is provided code that takes into account these issues using &lt;code&gt;poweRlaw&lt;/code&gt;; as a consequence it is significantly slower than the code you suggested: &lt;a href=&quot;http://notesnico.blogspot.com/2014/07/goodness-of-fit-test-for-log-normal-and.html&quot; rel=&quot;nofollow&quot;&gt;http://notesnico.blogspot.com/2014/07/goodness-of-fit-test-for-log-normal-and.html&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-05T09:39:04.513" Id="105870" LastActivityDate="2014-07-05T10:07:19.150" LastEditDate="2014-07-05T10:07:19.150" LastEditorUserId="32036" OwnerUserId="49548" ParentId="82530" PostTypeId="2" Score="2" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I want to simulate survival times from the model&lt;/p&gt;&#10;&#10;&lt;p&gt;$\lambda(t|X_1,X_2) = \lambda_0(t) \exp(\gamma_1X_1+\gamma_2X_2)$&lt;/p&gt;&#10;&#10;&lt;p&gt;where covariates are given by $X_1\sim$Binomial(1,0.5), $X_2\sim$Uniform(0,1), the true parameter values are&lt;/p&gt;&#10;&#10;&lt;p&gt;$\gamma_1=-0.5$, $\gamma_2=1.5$ and the true baseline hazard function is as follows&lt;/p&gt;&#10;&#10;&lt;p&gt;$ \lambda_0(t)=  \left\{&#10;\begin{array}{ll}&#10;      \exp(-0.3t); &amp;amp; 0&amp;lt;t\leq 1 \\&#10;      \exp(-0.3); &amp;amp; 1&amp;lt;t\leq2.5 \\&#10;      \exp(0.3(t-3.5)); &amp;amp; t&amp;gt;2.5&#10;\end{array} &#10;\right. $&lt;/p&gt;&#10;&#10;&lt;p&gt;The censoring time is from the exponential distribution with mean 2.5.&lt;/p&gt;&#10;&#10;&lt;p&gt;For a simple baseline hazard function(e.g. $\lambda_0(t)=\lambda$), i used the following algorithm to generate survival times&lt;/p&gt;&#10;&#10;&lt;p&gt;$$T=H_0^-\{-\log(u) \times \exp(\gamma_1X_1+\gamma_2X_2)\}&#10;=\lambda^-\{-\log(u) \times \exp(\gamma_1X_1+\gamma_2X_2))\}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $u\sim$Uniform(0,1). The R codes are as follows&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;gamma &amp;lt;- c(-0.5, 1.5)&#10;X1 &amp;lt;- rbinom(n=200,size=1,prob=.5)&#10;X2 &amp;lt;- round(runif(n=200,0,1),2)&#10;time &amp;lt;- (1/.3)*(-log(runif(200))/exp(gamma[1]*X1+gamma[2]*X2))&#10;cen &amp;lt;- rexp(200,rate=1/2.5)&#10;event &amp;lt;- as.numeric(time &amp;lt;= cen)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;But i'm struggling to generate for the piecewise baseline hazard function that i mentioned above. Would anyone please help me to generate survival times for the above settings using R.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-07-05T12:31:46.737" Id="105881" LastActivityDate="2014-07-05T12:31:46.737" OwnerUserId="48785" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;survival&gt;&lt;simulation&gt;" Title="How to simulate survival times using true base line hazard function" ViewCount="143" />
  
  
  
  
  <row Body="&lt;p&gt;Affinity propagation clustering could be an interesting method for you to try. But it is more important to pick a binary metric that matches you requirements. If you have an appropriate similarity metric, it would also be helpful to visualize the data with MDS methods (or non-linear dimensionality reduction) in 2D or 3D space.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-07-05T15:59:08.217" Id="105904" LastActivityDate="2014-07-05T15:59:08.217" OwnerUserId="14411" ParentId="105838" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;I used hierarchical clustering with cosine distance for a similar problem and it worked well. If they have no services in common the distance will be 1. If they have exacltly the same services the distance will be 0.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-05T17:19:25.947" Id="105909" LastActivityDate="2014-07-05T17:19:25.947" OwnerUserId="44027" ParentId="105838" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="106092" AnswerCount="1" Body="&lt;p&gt;I have a kalman filter (a recursive least square filter, really) regressing over real-time streams of data. Because the data-generating process varies slightly over time I add an exponential forgetting factor to the kalman filter.&lt;br&gt;&#10;It all fits nicely with good prediction.&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem I have is that the variables in input streams sometimes enter long periods of equilibrium. In those periods of time all I receive in the input streams are the same $y_t$ and same $x_t$ (except for background white noise). If the equilibrium remains for long enough the kalman fit starts degrading as it forgets old useful data and gives more and more importance to the background noise in equilibrium.&lt;/p&gt;&#10;&#10;&lt;p&gt;What I'd like to know is if there is any method to avoid overfitting the filter to useless new data. Right now I am using very ad-hoc methods (tallying a moving standard deviation to use as weight in the regression), but there must be better way to regularize a regression in an online setting.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-05T20:45:47.800" Id="105924" LastActivityDate="2014-07-07T18:35:13.987" OwnerUserId="6793" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;model-selection&gt;&lt;kalman-filter&gt;&lt;online&gt;&lt;overfitting&gt;" Title="How to deal with bouts of equilibrium in an online learning setting?" ViewCount="44" />
  <row AnswerCount="0" Body="&lt;blockquote&gt;&#10;  &lt;p&gt;Given 2 sets of entities from 2 different classes described by properties $f_1...f_n$,&#10;  any 2 entities from the 2 different classes together have a score: ${\rm Score}(c_1e_1,c_2e_1) = y$.&#10;  How can one derive a function / model predicting a score of any pair of the two different classes given its properties?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;What's the term for this problem in research?&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2014-07-05T22:40:59.380" Id="105929" LastActivityDate="2014-07-05T23:10:54.697" LastEditDate="2014-07-05T23:10:54.697" LastEditorUserId="32036" OwnerUserId="49568" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;terminology&gt;&lt;similarities&gt;&lt;matching&gt;" Title="What is this problem called in machine learning research?" ViewCount="55" />
  <row AnswerCount="0" Body="&lt;p&gt;For each function (f or g) I have some sampled X data (of about 1000 dimensions) and the associated (noisy) y value. From these two data sets I would like to estimate f-g, whose magnitude I expect to be small relative to f and g.  The X data for both datasets are drawn from the same distribution.  I have tried estimating f and g separately and merely taking the difference between the estimates, but it seems that is leaving some power on the table.  After all, if for each data point I had samples from both f and g, it seems reasonable that the best way would be to subtract the respective y's and regress the differences, but in general I don't.  Anyone have any advice, references, or just recommendations on what to google for? &lt;/p&gt;&#10;" ClosedDate="2014-07-06T22:24:00.353" CommentCount="1" CreationDate="2014-07-06T04:24:09.283" FavoriteCount="0" Id="105942" LastActivityDate="2014-07-06T17:09:21.513" LastEditDate="2014-07-06T17:09:21.513" LastEditorUserId="12090" OwnerUserId="12090" PostTypeId="1" Score="2" Tags="&lt;regression&gt;" Title="Estimating the difference between two functions" ViewCount="32" />
  
  
  
  <row Body="&lt;p&gt;No reason why you can't in principle. Stuff goes up, stuff goes down.&lt;/p&gt;&#10;&#10;&lt;p&gt;That said, did you plot your data? Just because it's statistically &quot;significant&quot; (that's really an evil term) doesn't mean it's substantively significant. And even if there's a curve, does it make sense to fit the curve? Does it make sense to fit a &lt;em&gt;quadratic&lt;/em&gt; curve? You have one time series over 33 points. Plot the thing.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-07-06T21:19:27.893" Id="105990" LastActivityDate="2014-07-06T21:19:27.893" OwnerUserId="36229" ParentId="105989" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;Volatility also has an application that is rather important, which is in options pricing. The famous Black-Scholes model incorporates future volatility in its equation. So being able to forecast volatility with a high degree of accuracy is crucial in everyday life of derivative trade desk professionals. That is why so much attention is being paid in academia to the research and better understanding of volatility.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-07T03:58:32.833" Id="106017" LastActivityDate="2014-07-07T03:58:32.833" OwnerUserId="49611" ParentId="34132" PostTypeId="2" Score="0" />
  <row AnswerCount="2" Body="&lt;p&gt;I want to graph some data, containing of an event ID and a date (minimum required resolution is one day). every event ID has multiple dates connected to it, so the min and max date give me a time span. Another useful information would be where in time most of these events took place (like a density graph over this time span).&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem is, that i have about 1500 different events, with each more than 100 dates connected. In sum i have 1.1 million event/date pairs. (the whole data set is much larger: 200k events with an average of 10 dates)&lt;/p&gt;&#10;&#10;&lt;p&gt;Are there any good ways to visualize this data? I tried using dot-plots, but it is more a mess and you see more a black wall than any thing else.&lt;/p&gt;&#10;&#10;&lt;p&gt;If there is no such solution i can break down the data into smaller chunks, but for example the top 10 events (having the most dates connected) sum up to 500k tuples.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you have concrete examples in ggplot2 or D3.js it would be perfect!&lt;/p&gt;&#10;&#10;&lt;p&gt;This is an image, how a scatter plot of start date vs end date for the top 1500 events look like. The problem is that most of the events have the same (or similar) span.&#10;&lt;img src=&quot;http://i.stack.imgur.com/5c76Z.png&quot; alt=&quot;Sample Scatter plot of the top 1500 events&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;This is the same plot, but with more data. As you can see, the less common events have shorter span.&#10;&lt;img src=&quot;http://i.stack.imgur.com/hGikC.png&quot; alt=&quot;Sample Scatter plot of the top 10000 events&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Also the same data as in the last plot, but with a marker for the count of dates inside the span. As you can see there are two big blobs and the rest is quite small.&#10;&lt;img src=&quot;http://i.stack.imgur.com/BErXb.png&quot; alt=&quot;scatter plot with count of events in span as size&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;now a scatter plot for all the data, you can clearly see there is a line with lots of events with only one date&#10;&lt;img src=&quot;http://i.stack.imgur.com/UCekU.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;and as last image, this is the whole data set as density2d:&#10;&lt;img src=&quot;http://i.stack.imgur.com/ohBq8.png&quot; alt=&quot;density2d map of the whole data set&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;and the same for all events with more than 10 dates:&#10;&lt;img src=&quot;http://i.stack.imgur.com/kG2xH.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem with the last two plots is, that they do not count the occurences inside the span. they just show me where a lot of time spans are. How can i put the count of events inside the time span also into this calculation?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-07-07T06:54:56.013" Id="106023" LastActivityDate="2014-07-08T00:04:34.350" LastEditDate="2014-07-07T08:05:27.767" LastEditorUserId="20839" OwnerUserId="20839" PostTypeId="1" Score="1" Tags="&lt;data-visualization&gt;" Title="How to visualize a lot (&gt;1k) of time spans?" ViewCount="89" />
  
  <row AcceptedAnswerId="106050" AnswerCount="2" Body="&lt;p&gt;I know that this is probably a question that's been asked plenty of times, but i haven't seen an answer that's both accurate and simple. How do you estimate the appropriate forecast model for a time series by visual inspection of the ACF and PACF plots? Which one, ACF or PACF, tells the AR or the MA (or do they both?) Which part of the graphs tell you the seasonal and non seasonal part for a seasonal ARIMA?&lt;/p&gt;&#10;&#10;&lt;p&gt;Take for instance these functions:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/E64Sd7p.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;They show the ACF and PCF of a log transformed series that's been differenced twice, one simple difference and one seasonal.&lt;/p&gt;&#10;&#10;&lt;p&gt;How would you caracterize it? What model best fits it?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance!&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; Added raw data&lt;/p&gt;&#10;&#10;&lt;p&gt;Original data: &lt;a href=&quot;http://pastebin.com/KRJnXzXp&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Log transformed data: &lt;a href=&quot;http://pastebin.com/JR3bkctv&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; Corrected ACF and PACF functions (previous ones were overdifferentiated)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-07T08:38:09.727" FavoriteCount="6" Id="106038" LastActivityDate="2014-07-07T22:21:14.340" LastEditDate="2014-07-07T12:52:26.917" LastEditorUserId="35881" OwnerUserId="35881" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;regression&gt;&lt;time-series&gt;&lt;machine-learning&gt;&lt;mathematical-statistics&gt;" Title="Estimate ARMA coefficients through ACF and PACF inspection" ViewCount="1803" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Working on a problem of extracting a minimal sub-set of criteria in a siting problem, I resorted to PCA, I have only 26 individuals (measures), I naturally thought it would be wise to ask whether that number is sufficient or not. &#10;I do obtain results, that's not the issue, I just wonder if I can trust them or not. &lt;/p&gt;&#10;&#10;&lt;p&gt;So I guess, a complementary question would be : &lt;strong&gt;&lt;em&gt;what is the ideal number of measures to have in order of insuring a solid PCA analysis ?&lt;/em&gt;&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for your replies, have a great day !&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-07-07T10:37:31.867" Id="106042" LastActivityDate="2014-07-07T10:37:31.867" OwnerUserId="49371" PostTypeId="1" Score="0" Tags="&lt;pca&gt;" Title="Is there an ideal number of measures in PCA ?" ViewCount="25" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am quantifying a measure of correlation in a graph/network under two different experimental conditions to determine whether there is a significant difference in the measure given by the change of condition. I am repeating this computation for a number (N=5) of heterogeneous graphs/networks in the sense that they may have different number of nodes/links. To avoid any dependency on these graphs/networks I am normalizing each measure by the summation of both measures under each condition.  I am now simply using a t-test between both sets of proportions because of the limited number of samples, but I have concerns given the nature of my computations. Which statistical test would be more convenient in this case?&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2014-07-07T10:45:02.393" Id="106045" LastActivityDate="2014-07-07T10:45:02.393" OwnerUserId="49626" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;&lt;t-test&gt;" Title="Statistical test on pairs of percentages" ViewCount="40" />
  <row AcceptedAnswerId="106299" AnswerCount="1" Body="&lt;p&gt;I have a problem with a number of inputs and one binary output, which I have tried to train several classifiers to solve. Unfortunately, none of the classifiers (MLP, SVM, bagging) have achieved the required level of accuracy. &lt;/p&gt;&#10;&#10;&lt;p&gt;I am thinking of subdividing the problem by splitting the dataset into two, and using a different classifier on each half, or the same classifier but with different parameters (or the same parameters, but it will be learning a different problem).  &lt;/p&gt;&#10;&#10;&lt;p&gt;One approach is to choose the attribute which has the highest correlation with the output, and split the data so that the lowest values are in one set, and the highest in another.  I fond that one set gave very good accuracy, whereas the other had a lower accuracy than the combined data set.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there an alternative approach to subdividing the problem that anyone can suggest?&lt;/p&gt;&#10;" CommentCount="10" CreationDate="2014-07-07T10:58:18.743" Id="106049" LastActivityDate="2014-07-09T09:19:49.990" OwnerUserId="48636" PostTypeId="1" Score="0" Tags="&lt;classification&gt;" Title="How to subdivide a classification problem" ViewCount="64" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm interested in using a wavelet transform, Haar for example, to create classification variables from time series data to use in logistic regression.&lt;/p&gt;&#10;&#10;&lt;p&gt;Simple example. Let's say I'm trying to predict payment defaults and I have a person's monthly expense data and I know that someone with consistent expenses is better than someone with increasing expenses in the the most recent 4 months.&lt;/p&gt;&#10;&#10;&lt;p&gt;If I have two sample borrowers:&lt;/p&gt;&#10;&#10;&lt;p&gt;Borrower A - Good - expensesA = c(100,110,95,105), default = 0&lt;/p&gt;&#10;&#10;&lt;p&gt;Borrower B - Bad - expensesB = c(75,100,150,200), default = 1&lt;/p&gt;&#10;&#10;&lt;p&gt;If I am using logistic regression to create a classification model, using the R &lt;code&gt;wavelets&lt;/code&gt; package &lt;code&gt;dwt()&lt;/code&gt; function for a &quot;haar&quot; transform of the time series I get the features below.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;W&lt;/code&gt;s are wavelet coefficients and the &lt;code&gt;V&lt;/code&gt;s the scaling coefficients.&lt;/p&gt;&#10;&#10;&lt;p&gt;Do I need to use all four &lt;code&gt;W1&lt;/code&gt; and &lt;code&gt;V1&lt;/code&gt; values as variables to properly model this or is it okay to try just the &lt;code&gt;W1&lt;/code&gt;s without &lt;code&gt;V1&lt;/code&gt;s (or vice versa)?&lt;/p&gt;&#10;&#10;&lt;p&gt;Is it worthwhile to try only the single &lt;code&gt;W2&lt;/code&gt; and &lt;code&gt;V2&lt;/code&gt;s as variables?&lt;/p&gt;&#10;&#10;&lt;p&gt;Or is it better to try to use a clustering algorithm and label them based on clusters created from these coefficients?&lt;/p&gt;&#10;&#10;&lt;p&gt;I know it of course also depends on the data, but I'm looking for a starting point regarding best practices.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;tr = dwt(expensesA, filter = &quot;haar&quot;)&#10;tr&#10;&#10;An object of class &quot;dwt&quot;&#10;Slot &quot;W&quot;:&#10;$W1&#10;[,1]&#10;[1,] 7.071068&#10;[2,] 7.071068&#10;&#10;$W2&#10;[,1]&#10;[1,]   -5&#10;&#10;&#10;Slot &quot;V&quot;:&#10;$V1&#10;[,1]&#10;[1,] 148.4924&#10;[2,] 141.4214&#10;&#10;$V2&#10;[,1]&#10;[1,]  205&#10;&#10;&#10;Slot &quot;filter&quot;:&#10;Filter Class: Daubechies&#10;Name: HAAR&#10;Length: 2&#10;Level: 1&#10;Wavelet Coefficients: 7.0711e-01 -7.0711e-01&#10;Scaling Coefficients: 7.0711e-01 7.0711e&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-07-07T12:32:20.297" FavoriteCount="1" Id="106057" LastActivityDate="2014-07-07T12:32:20.297" OwnerUserId="29070" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;logistic&gt;&lt;classification&gt;&lt;dimensionality-reduction&gt;&lt;wavelet&gt;" Title="Creating classification features from wavelet transformed time series" ViewCount="82" />
  <row Body="&lt;p&gt;The statement is related to the fact that the ACF of a stationary AR process of order p goes to zero at an exponential rate, while the PACF becomes zero after lag p. For an MA process of order q the theoretical ACF and PACF show the reverse behaviour, the ACF truncates after lag q and the PACF goes to zero at an exponential rate.&lt;/p&gt;&#10;&#10;&lt;p&gt;These properties can be used as a guide to choose the orders of an ARMA model.&#10;See for instance, Chapter 3 in &lt;a href=&quot;http://www.springer.com/statistics/statistical+theory+and+methods/book/978-0-387-97429-3&quot; rel=&quot;nofollow&quot;&gt;Time Series: Theory and Methods&lt;/a&gt; by Peter J. Brockwell and Richard A. Davis and &lt;a href=&quot;http://en.wikipedia.org/wiki/Box%E2%80%93Jenkins#Identify_p_and_q&quot; rel=&quot;nofollow&quot;&gt;this&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-07T12:51:26.393" Id="106059" LastActivityDate="2014-07-07T12:51:26.393" OwnerUserId="48766" ParentId="79833" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;Let $X_1,X_2,...,X_n$ be a random sample from a normal distribution with mean $\mu$ and variance $\sigma^2$.&lt;br&gt;&#10;I showed that $(\bar X,S^2)$ is jointly sufficient for estimating ($\mu$,$\sigma^2$) where $\bar X$ is the sample mean and $S^2$ is the sample variance.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Then assuming that$(\bar X,S^2)$ is also complete I have to show that $$\sqrt{ n-1\over 2}{\Gamma ({ n-1\over 2})\over\Gamma (\frac n2)} S$$&#10;is a Uniformly Minimum Variance Unbiased Estimator for $\sigma$.&lt;/p&gt;&#10;&#10;&lt;p&gt;I think I have to use Lehman Scheffe theorem as $(\bar X,S^2)$ is jointly sufficient and complete for $\sigma$.&#10;But how can I find a function which is unbiased for  $\sigma$ that contains both $(\bar X,S^2)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;I don't understand how to work when there's  a &lt;strong&gt;joint&lt;/strong&gt; sufficiency and completeness.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-07-07T12:58:58.470" Id="106060" LastActivityDate="2014-07-08T00:51:44.653" LastEditDate="2014-07-08T00:51:44.653" LastEditorUserId="30557" OwnerUserId="30557" PostTypeId="1" Score="2" Tags="&lt;self-study&gt;&lt;normal-distribution&gt;&lt;estimation&gt;&lt;inference&gt;&lt;umvue&gt;" Title="UMVUE for normal distribution $\sigma$" ViewCount="251" />
  <row AnswerCount="1" Body="&lt;p&gt;I am trying to fit an exponential model to some data. The data is:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Wavelength  aCDOM&#10;350.01  0.80605&#10;350.22  0.78302&#10;350.43  0.78302&#10;350.64  0.78302&#10;350.85  0.78302&#10;351.06  0.78302&#10;351.27  0.78302&#10;351.48  0.78302&#10;351.68  0.75999&#10;351.89  0.75999&#10;352.1   0.75999&#10;352.31  0.75999&#10;352.52  0.75999&#10;352.73  0.73696&#10;352.94  0.73696&#10;353.15  0.73696&#10;353.36  0.73696&#10;353.57  0.73696&#10;353.78  0.73696&#10;353.99  0.73696&#10;354.2   0.73696&#10;354.41  0.73696&#10;354.62  0.73696&#10;354.83  0.73696&#10;355.04  0.73696&#10;355.25  0.71393&#10;355.46  0.71393&#10;355.67  0.71393&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I know that the best model fit this type of data is an exponential function in the for &lt;code&gt;y ~ a * exp(-b * c)&lt;/code&gt; where &lt;code&gt;a&lt;/code&gt; is the absorbance estimate at the reference wavelength, &lt;code&gt;b&lt;/code&gt; is the spectral slope which is the value I am looking for, and &lt;code&gt;c&lt;/code&gt; is the wavelength minus the reference wavelength (440nm). I have tried using the &lt;code&gt;nls&lt;/code&gt; function by estimating the starting parameters following the method outlined in Fox (2002) but I keep getting the following error in &lt;code&gt;nlsModel(formula, mf, start, wts)&lt;/code&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;singular gradient matrix at initial parameter estimates&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The formula I am using is &lt;code&gt;model1 &amp;lt;- nls(St0104 ~ a * exp(-b * (Wavelength-440)), start = list(a=0.1, b=0.0012), trace=T)&lt;/code&gt;, I'm not even if I have calculated the starting parameters correctly. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have tried instead to plot the log of &lt;code&gt;Y&lt;/code&gt; and do linear as well as polynomial regression, but I know this is not correct for this data and the residuals are horrible. I'm pretty new to R but I have tried everything I can but this is driving me nuts can anybody help please?&lt;/p&gt;&#10;" ClosedDate="2014-07-22T15:01:12.503" CommentCount="6" CreationDate="2014-07-07T13:48:38.267" Id="106064" LastActivityDate="2014-07-22T15:02:14.243" LastEditDate="2014-07-22T15:02:14.243" LastEditorUserId="919" OwnerUserId="49637" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;regression&gt;&lt;nonlinear&gt;" Title="Non linear regression in R Exponential curve fitting" ViewCount="1509" />
  <row AnswerCount="0" Body="&lt;p&gt;In the solution to &lt;a href=&quot;http://stats.stackexchange.com/questions/105745/extreme-value-theory-show-normal-to-gumbel&quot;&gt;this question&lt;/a&gt; (Extreme Value Theory - Show: Normal to Gumbel), the OP asked for the sequence $(a_n, b_n)$ such that $\Phi(a_nx+b_n)$ converges to the Gumbel CDF. Not only did I not able to understand the derivation in the accepted answer (I don't see the derivation for the last paragraph), I am also curious to know how one would go about deriving the sequence for distributions other than the standard normal.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, the sequences in examples 7.5 and 7.6 in &lt;a href=&quot;http://www.math.ethz.ch/~embrecht/RM/chap7.pdf&quot; rel=&quot;nofollow&quot;&gt;this document&lt;/a&gt; (page 6) seems to be pulled out of thin air.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-07T14:33:42.773" Id="106068" LastActivityDate="2014-07-07T14:33:42.773" OwnerUserId="48490" PostTypeId="1" Score="3" Tags="&lt;extreme-value&gt;" Title="How to find the $(a_n,b_n)$ for extreme value theory" ViewCount="57" />
  <row AnswerCount="1" Body="&lt;p&gt;I am using Matlab to forecast time series data using ARIMA algorithm. I am able to get forecasted values, but unable to get the fitted values. &#10;This is what I mean. In R:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; model &amp;lt;- auto.arima(data)&quot;);&#10; forecasted &amp;lt;- forecast.Arima(model, h=36)&#10; forecasted$fitted&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;forecasted$fitted&lt;/code&gt; will give the data which has been fitted to the original data by the ARIMA algorithm. But I cannot see anything like that in Matlab. This is what I have done so far:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;model = arima(1,1,0);&#10;[est, ~, logL, info] = estimate(model, data);&#10;[output, YMSE] = forecast(est, 10);&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;output gives me the 10 values Arima has forecasted, but not the original data values it has fitted. I would like to know those. &lt;/p&gt;&#10;&#10;&lt;p&gt;Any help is appreciated.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-07T14:53:07.913" Id="106071" LastActivityDate="2015-01-27T09:08:17.087" LastEditDate="2014-07-07T15:11:28.293" LastEditorUserId="805" OwnerUserId="49644" PostTypeId="1" Score="2" Tags="&lt;matlab&gt;&lt;arima&gt;" Title="Get fitted values estimated in ARIMA in Matlab" ViewCount="519" />
  <row AcceptedAnswerId="111613" AnswerCount="1" Body="&lt;p&gt;I am analyzing student performance on a pre/post test designed to assess gains for a week long high school biology curriculum on natural selection.&lt;/p&gt;&#10;&#10;&lt;p&gt;After comparing pre/post performance using a variety of different metrics (test score, Rasch measure, chi-square per item) between the paired samples, I conducted an ANOVA to gauge whether there were differences between teachers. Two of the (seven) teachers are significantly different from the others (their students have drastically lower mean Rash measures and did worse overall), so I'd like to account for these differences in the analysis.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is a nested ANOVA model appropriate here? I know nested models aren't in the SPSS dialogs, but I'm not much of a programmer and thus do not know what syntax to produce.&lt;/p&gt;&#10;&#10;&lt;p&gt;Moreover, each teacher has a different number of students. Would a random sample (ensuring the same amount from each teacher) be useful for paired comparisons of means (like pre vs. post), or does it not matter since it's a comparison of means between groups anyway?&lt;/p&gt;&#10;&#10;&lt;p&gt;Apologies for any obvious questions. Education is not my original field (I'm a bio-anthropologist), and it's been a bit since I took multivariate stats in grad school.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-07-07T15:25:30.233" Id="106074" LastActivityDate="2014-08-12T18:57:03.957" OwnerUserId="26757" PostTypeId="1" Score="1" Tags="&lt;anova&gt;&lt;nested&gt;&lt;paired-comparisons&gt;&lt;education&gt;&lt;rasch&gt;" Title="Nested ANOVA in SPSS / Science Education" ViewCount="297" />
  <row Body="&lt;p&gt;Stratified sampling is one option, but you don't end up using all your data. Alternatively, you could update your loss function to proportionally add weight to less represented classes. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-07T16:13:37.187" Id="106077" LastActivityDate="2014-07-07T16:13:37.187" OwnerUserId="49645" ParentId="106075" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Perhaps you would benefit from an exploratory tool.  Splitting the data into deciles of the x coordinate appears to have been performed in that spirit.  With modifications described below, it's a perfectly fine approach.&lt;/p&gt;&#10;&#10;&lt;p&gt;Many bivariate exploratory methods have been invented.  A simple one proposed by John Tukey (&lt;em&gt;EDA&lt;/em&gt;, Addison-Wesley 1977) is his &quot;wandering schematic plot.&quot;  You slice the x-coordinate into bins, erect a vertical boxplot of the corresponding y data at the median of each bin, and connect the key parts of the boxplots (medians, hinges, etc.) into curves (optionally smoothing them).  These &quot;wandering traces&quot; provide a picture of the bivariate distribution of the data and allow immediate visual assessment of correlation, linearity of relationship, outliers, and marginal distributions, as well as robust estimation and goodness-of-fit evaluation of any nonlinear regression function.&lt;/p&gt;&#10;&#10;&lt;p&gt;To this idea Tukey added the thought, consistent with the boxplot idea, that a good way to probe the distribution of data is to start at the middle and work outwards, halving the amount of data as you go.  That is, the bins to use need not be cut at equally-spaced quantiles, but instead should reflect the quantiles at the points $2^{-k}$ and $1-2^{-k}$ for $k=1, 2, 3, \ldots$.&lt;/p&gt;&#10;&#10;&lt;p&gt;To display the varying bin populations we can make each boxplot's width proportional to the amount of data it represents.&lt;/p&gt;&#10;&#10;&lt;p&gt;The resulting wandering schematic plot would look something like this.  Data, as developed from the data summary, are shown as gray dots in the background.  Over this the wandering schematic plot has been drawn, with the five traces in color and the boxplots (including any outliers shown) in black and white.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Vv8x0.png&quot; alt=&quot;Figure&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The nature of the near-zero correlation becomes immediately clear: the data twist around.  Near their center, ranging from $x=-4$ to $x=4$, they have a strong positive correlation.  At extreme values, these data exhibit curvilinear relationships that tend on the whole to be negative.  The net correlation coefficient (which happens to be $-0.074$ for these data) is close to zero.  However, insisting on interpreting that as &quot;nearly no correlation&quot; or &quot;significant but low correlation&quot; would be the same error spoofed in the old joke about the statistician who was happy with her head in the oven and feet in the icebox because on average the temperature was comfortable.  Sometimes a single number just won't do to describe the situation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Alternative exploratory tools with similar purposes include robust smooths of windowed quantiles of the data and fits of quantile regressions using a range of quantiles. With the ready availability of software to perform these calculations they have perhaps become easier to execute than a wandering schematic trace, but they do not enjoy the same simplicity of construction, ease of interpretation, and broad applicability.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;The following &lt;code&gt;R&lt;/code&gt; code produced the figure and can be applied to the original data with little or no change.  (Ignore the warnings produced by &lt;code&gt;bplt&lt;/code&gt; (called by &lt;code&gt;bxp&lt;/code&gt;): it complains when it has no outliers to draw.)&lt;/p&gt;&#10;&#10;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;#&#10;# Data&#10;#&#10;set.seed(17)&#10;n &amp;lt;- 1449&#10;x &amp;lt;- sort(rnorm(n, 0, 4))&#10;s &amp;lt;- spline(quantile(x, seq(0,1,1/10)), c(0,.03,-.6,.5,-.1,.6,1.2,.7,1.4,.1,.6),&#10;            xout=x, method=&quot;natural&quot;)&#10;#plot(s, type=&quot;l&quot;)&#10;e &amp;lt;- rnorm(length(x), sd=1)&#10;y &amp;lt;- s$y + e # ($ interferes with MathJax processing on SE)&#10;#&#10;# Calculations&#10;#&#10;q &amp;lt;- 2^(-(2:floor(log(n/10, 2))))&#10;q &amp;lt;- c(rev(q), 1/2, 1-q)&#10;n.bins &amp;lt;- length(q)+1&#10;bins &amp;lt;- cut(x, quantile(x, probs = c(0,q,1)))&#10;x.binmed &amp;lt;- by(x, bins, median)&#10;x.bincount &amp;lt;- by(x, bins, length)&#10;x.bincount.max &amp;lt;- max(x.bincount)&#10;x.delta &amp;lt;- diff(range(x))&#10;cor(x,y)&#10;#&#10;# Plot&#10;#&#10;par(mfrow=c(1,1))&#10;b &amp;lt;- boxplot(y ~ bins, varwidth=TRUE, plot=FALSE)&#10;plot(x,y, pch=19, col=&quot;#00000010&quot;, &#10;     main=&quot;Wandering schematic plot&quot;, xlab=&quot;X&quot;, ylab=&quot;Y&quot;)&#10;for (i in 1:n.bins) {&#10;  invisible(bxp(list(stats=b$stats[,i, drop=FALSE],&#10;                         n=b$n[i],&#10;                     conf=b$conf[,i, drop=FALSE],&#10;                         out=b$out[b$group==i],&#10;                         group=1,&#10;                         names=b$names[i]), add=TRUE, &#10;                boxwex=2*x.delta*x.bincount[i]/x.bincount.max/n.bins, &#10;                at=x.binmed[i]))&#10;}&#10;&#10;colors &amp;lt;- hsv(seq(2/6, 1, 1/6), 3/4, 5/6)&#10;temp &amp;lt;- sapply(1:5, function(i) lines(spline(x.binmed, b$stats[i,], &#10;                                             method=&quot;natural&quot;), col=colors[i], lwd=2))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2014-07-07T17:09:21.237" Id="106083" LastActivityDate="2014-07-07T17:09:21.237" OwnerUserId="919" ParentId="106016" PostTypeId="2" Score="7" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm considering a two-part, member-level model for predicting Year 2 inpatient health care costs based on health care utilization, demographics, and diagnosis data from Year 1. A logistic regression to predict Year 2 inpatient utilization (yes/no) + a loglinear OLS to predict Year 2 log cost (with perhaps a Duan Smear factor adjustment) given Year 2 costs &gt; $0 seems appropriate. I'll train my model on 2011-2012 data, then score 2013 records to make 2014 predictions.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, I've encountered a major limitation with using a two-part predictive model:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;How can I make severity predictions with the OLS component of the two-part model when I don't know a priori which members will have non-zero health care costs in Year 2 (2014)?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I've read a few forum posts recommending &lt;strong&gt;out of sample prediction&lt;/strong&gt; for the zero utilization records in a two-part model - how is this done?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-07T19:30:11.930" Id="106102" LastActivityDate="2014-07-07T19:30:11.930" OwnerUserId="13634" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;logistic&gt;&lt;out-of-sample&gt;" Title="How to use out of sample prediction for zero utilization records in two-part health care cost model?" ViewCount="39" />
  
  <row Body="&lt;p&gt;As whuber points out, it doesn't mean a whole lot to ask about the probability of something after it's already happened. But in the spirit of your question, we could ask &quot;What's the probability that if we pick 4 random people then 2 of them will share a birthday and the other 2 will also share a birthday?&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;Here, it depends on whether you assume these events are independent and if it's equally likely to be born on any day of the year. If we assume that any day is equally likely (I don't think this is strictly true, but it's a reasonable assumption to make), and that these 4 birthdays are all independent of each other (and we ignore leap years), then the chance of this happening is just:$$({\frac 1{365})^2}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;This is true because person 1 can be born on any day, then there's a $\frac 1{365}$ chance that person 2 shares that birthday. Same goes for persons 3 and 4 (the calculation would be a little different if you specified that you wanted the two days to be different days). This may not quite fit with what you're really trying to ask - if you specify exact dates beforehand and then pick 4 random people the chance would be $$({\frac 1{365})^4}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;However, this is misleading as this is the probability of ANY combination of $4$ birthdates. In other words, it's also the chance that if you take $4$ people their birthdays will be 1/1, 1/1, 1/1, and 1/1. Or 1/1, 1/2, 1/3, and 1/4. OR 1/2, 3/4, 5/6 and 7/8. Or something with no discernible pattern like 1/7, 10/3, 4/23 and 7/31.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-07-07T20:00:48.457" Id="106108" LastActivityDate="2014-07-07T20:16:54.853" LastEditDate="2014-07-07T20:16:54.853" LastEditorUserId="45561" OwnerUserId="45561" ParentId="106103" PostTypeId="2" Score="3" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have one dependent variable (continuous data) and 4 independent data (mix of continuous and count) collected over 35 years across several states. I am using a linear mixed effect models with a random intercept and a random slope.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;model&amp;lt;-lmer(y ~ x1 + x2 + x3 + x4 (1+x1+x2+x3+x4|state),data=data,method=&quot;ML&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;If some of my independent variables are correlated, what is the procedure of reducing the collinearity issue in a linear mixed effect model?  I could spot collinerity  using VIF and retain the most significant independent variables but I can do this for each factor level (levels of state) individually. But won't it result in retaining some independent variables in one factor level while deleting the same in other factor level? I guess the main question is how to spot collinearity in a mixed effect models and what to do with it when you have 5 or 6 independent variables?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-07-07T20:50:31.790" Id="106112" LastActivityDate="2014-07-07T20:50:31.790" OwnerUserId="46228" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;lmer&gt;&lt;glmer&gt;" Title="collinearity in linear mixed effects model" ViewCount="108" />
  <row Body="&lt;p&gt;The problem here seems to be not the occurence of $(0,0)$-values (since any other points $(x,y)$ will behave similarly), but rather a problem of model selection. If understand you right, you want to assume a linear model and then let the fitting procedure state doubts about either the fitted parameters or about the linear model itself.&lt;/p&gt;&#10;&#10;&lt;p&gt;Some points into three different directions:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;First, within the linear model, you could use Bayesian regression and then find that the slope has a much larger variance than the intercept (of course this depends also on the prior). Adding more (0,0) values will mainly narrow the distribtion of the intercept and not the distribution of the slope.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Second, also assuming a linear model, you can use weighted least squares regression. If you have doubts about the (2,2) values, you can assume them to have a large variance and thus a small weight. Similar to the Bayesian setup, this will widen the error bars and draw higher doubt on the result for the intercept.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Third, and more generally, if you want a procedure which doubts the model based on the training data, you can use model comparison schemes which will show that other models beside a straight line are possible and probably equally likely (e.g., a parabola will also fit the data well).&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;However, in the end you are stuck with the model you assume. Hence, you should choose it carefully and with regard to the data.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-07-07T22:24:41.053" Id="106118" LastActivityDate="2014-07-07T22:24:41.053" OwnerUserId="49279" ParentId="106111" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="109810" AnswerCount="2" Body="&lt;p&gt;Assume I have a dataset for a supervised statistical classification task, e.g., via a Bayes' classifier. This dataset consists of 20 features and I want to boil it down to 2 features via dimensionality reduction techniques such as Principal Component Analysis (PCA) and/or Linear Discriminant Analysis (LDA).&lt;/p&gt;&#10;&#10;&lt;p&gt;Both techniques are projecting the data onto a smaller feature subspace: with PCA, I would find the directions (components) that maximize the variance in the dataset (without considering the class labels), and with LDA I would have the components that maximize the between-class separation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, I am wondering if, how, and why these techniques can be combined and if it makes sense.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;transforming the dataset via PCA and projecting it onto a new 2D subspace&lt;/li&gt;&#10;&lt;li&gt;transforming (the already PCA-transformed) dataset via LDA for max. in-class separation&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;or &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;skipping the PCA step and using the top 2 components from a LDA.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;or any other combination that makes sense.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-07-07T23:25:30.227" FavoriteCount="2" Id="106121" LastActivityDate="2015-01-13T15:43:14.057" LastEditDate="2015-01-13T15:27:53.583" LastEditorUserId="28666" OwnerUserId="39663" PostTypeId="1" Score="5" Tags="&lt;classification&gt;&lt;pca&gt;&lt;dimensionality-reduction&gt;&lt;discriminant-analysis&gt;" Title="Does it make sense to combine PCA and LDA?" ViewCount="529" />
  <row AnswerCount="0" Body="&lt;p&gt;I have never been so confused in my life. Could anyone please tell me what is the difference between the following mixed effects models:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;mdl1&amp;lt;-lme(y ~ x,random=~1|state,method=&quot;ML&quot;,data=df) # does this allow intercepts to vary across States but keeping the slope fixed (i.e. relationship between y and  x is same across all States)&#10;&#10;mdl2&amp;lt;-lme(y ~ x,random=~x|state,method=&quot;ML&quot;,data=df) # does this allow slopes (i.e. relationship between y and x) to vary across States but the intercept is fixed (mean y is similar across all States) &#10;&#10;mdl3&amp;lt;-lme(y ~ x,random=~1+x|state,method=&quot;ML&quot;,data=df) # does this allow both intercepts and slopes to vary across States (baseline y is different for different State as well as relationship between y and x is different for different States) &#10;&#10;mdl4&amp;lt;-lme(y ~ x*state,random=~1|state,method=&quot;ML&quot;,data=df) ### and I have no clue what this does??&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;These different model structure has confused me a lot. I would be really grateful if someone could guide me through this&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-07T23:32:06.683" Id="106122" LastActivityDate="2014-07-07T23:32:06.683" OwnerUserId="46228" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;glmm&gt;&lt;lme&gt;" Title="differences between different notations in mixed effects model" ViewCount="30" />
  
  <row AcceptedAnswerId="107750" AnswerCount="1" Body="&lt;p&gt;I'm planning a study using Structural Equation Modelling to test different accounts of language learning. I would like to study a group of children with language difficulties. However, they are very hard to recruit and I'm aware that these models typically require at least 200 participants. One possibility would be&lt;/p&gt;&#10;&#10;&lt;p&gt;(1) Test SEMs on a large cohort of typically developing children - who are much easier to recruit (n = 500). (NB I'm imagining 3 or 4 different models each with about 3 or 4 latent variables).&lt;/p&gt;&#10;&#10;&lt;p&gt;(2) Apply these models to a smaller cohort of language impaired individuals (n = 80)&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm imagining that in the second stage one would keep the coefficients from stage one, and would use some kind of procedure to manipulate the values of the latent (exogenous) variables to maximise model fit. Research questions would be&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Which model best describes the data for the typically developing children?&lt;/li&gt;&#10;&lt;li&gt;Does this model also best describe the data for the language impaired children?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I'm wondering if this is statistically feasible and whether anyone has tried this approach before. Ta.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-08T08:25:29.383" Id="106157" LastActivityDate="2014-07-12T23:58:22.017" OwnerUserId="24109" PostTypeId="1" Score="0" Tags="&lt;model&gt;&lt;sem&gt;" Title="Structural Equation Models - building with a large sample, and testing with a small one?" ViewCount="52" />
  
  <row Body="&lt;p&gt;It's the lumpy bit in the middle (normally) of a distribution&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-07-08T09:27:09.653" Id="106161" LastActivityDate="2014-07-08T09:27:09.653" OwnerUserId="49693" ParentId="2499" PostTypeId="2" Score="-6" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a data set derived from administrative registers covering the population of a small European country, containing a large number of defined groups (15 000+). For each of these groups I have multiple observations (different years), and two explanatory variables that characterize the exposure of each group to a policy intervention.&lt;/p&gt;&#10;&#10;&lt;p&gt;In STATA I would estimate this using the regression command, with aweight=group_size and using absorb(group_identifier). I've done this before, and it is extremely fast. How do I do this in R?  In R, my problem is that I cannot find a package that allows me to both weight by group size AND include fixed effects.&lt;/p&gt;&#10;&#10;&lt;p&gt;Using the standard lm function I could include the group-size weight, but would have to estimate the group-effects explicitly (simply adding the groups identifier as a factor variable). This is extremely time consuming (I cancelled it after it had run for several minutes).&lt;/p&gt;&#10;&#10;&lt;p&gt;Alternatively, I could turn to fixed-effects packages - such as the lfe package. These allow for fixed effects, but I have not found any such package that also accepts weighting by group size.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any suggestions?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-07-07T10:33:21.257" Id="106165" LastActivityDate="2014-07-08T10:04:26.420" OwnerDisplayName="user39748" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;regression&gt;" Title="How can I estimate a weighted linear regression with fixed effects and many groups in R?" ViewCount="97" />
  
  <row Body="&lt;p&gt;I do not believe that binning is a scientific approach to the problem.   It is information losing and arbitrary.  Rank (ordinal; semiparametric) methods are far better and do not lose information.  Even if one were to settle on decile binning, the method is still arbitrary and non-reproducible by others, simply because of the large number of definitions that are used for quantiles in the case of ties in the data.  And as alluded to in the nice data torture comment above,  Howard Wainer has a nice paper showing how to find bins that can produce a positive association, and find bins that can produce a negative association, from the same dataset:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; @Article{wai06fin,&#10;   author =          {Wainer, Howard},&#10;   title =       {Finding what is not there through the unfortunate&#10;    binning of results: {The} {Mendel} effect},&#10;   journal =     {Chance},&#10;   year =        2006,&#10;   volume =      19,&#10;   number =      1,&#10;   pages =       {49-56},&#10;   annote =      {can find bins that yield either positive or negative&#10;    association;especially pertinent when effects are small;``With four&#10;    parameters, I can fit an elephant; with five, I can make it wiggle its&#10;    trunk.'' - John von Neumann}&#10; }&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="3" CreationDate="2014-07-08T12:12:56.477" Id="106181" LastActivityDate="2014-07-08T12:12:56.477" OwnerUserId="4253" ParentId="106016" PostTypeId="2" Score="5" />
  <row AcceptedAnswerId="107733" AnswerCount="1" Body="&lt;p&gt;This question regards the problem of &lt;a href=&quot;http://en.wikipedia.org/wiki/Generalized_least_squares&quot; rel=&quot;nofollow&quot;&gt;Generalized Least Squares&lt;/a&gt;. Vectors and matrices will be denoted in bold.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Premises&lt;/strong&gt;. Let $N,K$ be given integers, with $K \gg N &amp;gt; 1$. The transpose of matrix $\mathbf{A}$ will be denoted with $\mathbf{A}^T$. Suppose the following statistical model holds&#10;$$&#10;(*) \quad \mathbf{y} = \mathbf{Hx + n}, \quad \mathbf{n} \sim \mathcal{N}_{K}(\mathbf{0}, \mathbf{C}) &#10;$$&#10;where $\mathbf{y} \in \mathbb{R}^{K \times 1}$ are the observables, $\mathbf{H} \in \mathbb{R}^{K \times N}$ is a known full-rank matrix, $\mathbf{x} \in \mathbb{R}^{N \times 1}$ is a deterministic vector of unknown parameters (which we want to estimate) and finally $\mathbf{n} \in \mathbb{R}^{K \times 1}$ is a disturbance vector (noise) with a known (positive definite) covariance matrix $\mathbf{C} \in \mathbb{R}^{K \times K}$. The &lt;a href=&quot;http://en.wikipedia.org/wiki/Maximum_likelihood&quot; rel=&quot;nofollow&quot;&gt;Maximum Likelihood&lt;/a&gt; (ML) estimate of $\mathbf{x}$, denoted with $\hat{\mathbf{x}}_{ML}$, is given by&#10;$$&#10;(1) \quad \hat{\mathbf{x}}_{ML} = (\mathbf{H}^T \mathbf{C^{-1}} \mathbf{H})^{-1} \mathbf{H}^T \mathbf{C}^{-1} \mathbf{y}&#10;$$&#10;and this is also the standard formula of Generalized Linear Least Squares (GLLS). Consider the standard formula of Ordinary Least Squares (OLS) for a linear model, i.e.&#10;$$&#10;(2) \quad \hat{\mathbf{x}}_{OLS} = (\mathbf{H}^T \mathbf{H})^{-1} \mathbf{H}^T \mathbf{y}&#10;$$&#10;As a final note on notation, $\mathbf{I}_K$ is the $K \times K$ identity matrix and $\mathbf{O}$ is a matrix of all zeros (with appropriate dimensions).&#10;&lt;hr&gt;Now, for the problem at hand, assume that $\mathbf{C}^{-1} = \mathbf{I}_K + \mathbf{X}$, where $\mathbf{X} \in \mathbb{R}^{K \times K}$ is a symmetric, invertible matrix (and, for the formalism to make sense, $\mathbf{X}$ is such that $\mathbf{I}_K + \mathbf{X}$ is invertible and the inverse is positive definite). In this case, it can be proven (using &lt;em&gt;matrix inversion lemma&lt;/em&gt;) that&#10;$$&#10;(3) \quad (\mathbf{H}^T \mathbf{C^{-1}} \mathbf{H})^{-1} \mathbf{H}^T \mathbf{C}^{-1} = (\mathbf{H}^T \mathbf{H})^{-1} \mathbf{H}^T + \mathbf{Q}&#10;$$&#10;where the expression for the matrix $\mathbf{Q} \in \mathbb{R}^{N \times K}$ can be found (I will omit it here). Now, finally,&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Proposition 1&lt;/strong&gt;. &lt;em&gt;If $\mathbf{H}^T\mathbf{X} = \mathbf{O}_{N,K}$, then equation $(1)$ degenerates in equation $(2)$, i.e., there exists no difference between GLLS and OLS&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;The proof is straigthforward and is valid even if $\mathbf{X}$ is singular. Now, my question is&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Question&lt;/em&gt;&lt;/strong&gt;: &lt;em&gt;Can an equation similar to eq. $(3)$ (which &quot;separates&quot; an OLS-term from a second term) be written when $\mathbf{X}$ is a singular matrix?&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I am not interested in a closed-form of $\mathbf{Q}$ when $\mathbf{X}$ is singular. But I do am interested in understanding the concept beyond that expression: what is the actual role of $\mathbf{Q}$? In which space does it operate?&lt;/p&gt;&#10;&#10;&lt;p&gt;I found this problem during a numerical implementation where both OLS and GLLS performed roughly the same (the actual model is $(*)$), and I cannot understand why OLS is not strictly sub-optimal. I found this slightly counter-intuitive, since you know a lot more in GLLS (you know $\mathbf{C}$ and make full use of it, why OLS does not), but this is somehow &quot;useless&quot; if some conditions are met. What are these conditions?&lt;/p&gt;&#10;&#10;&lt;p&gt;As a final note, I am rather new to the world of Least Squares, since I generally work within a ML-framework (or MMSE in other cases) and never studied the deep aspects of GLLS vs OLS, since, in my case, they are just intermediate steps during the derivation of MLE for a given problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;If the question is, in your opinion, a bit too broad, or if there is something I am missing, could you please point me in the right direction by giving me references? Preferably well-known books written in standard notation.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-07-08T13:33:12.030" Id="106197" LastActivityDate="2014-07-12T21:31:52.163" OwnerUserId="43926" PostTypeId="1" Score="2" Tags="&lt;multivariate-analysis&gt;&lt;least-squares&gt;&lt;generalized-least-squares&gt;" Title="Generalized Least Squares vs Ordinary Least Squares under a special case" ViewCount="455" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Hopefully this is an easily answerable question, though I've been searching for an answer without any luck.&lt;/p&gt;&#10;&#10;&lt;p&gt;I've run a 2x2x2x2 mixed-design ANOVA with two within-subjects and two between-subjects factors. I'm trying to follow up the only significant interaction, which is between the two between-subjects factors. These are scary image vs. non-scary image, and high-fear vs. low-fear. The interaction is significant because the low-fear group barely differ between the scary and non-scary images, but the high-fear group have both a lower score than the low-fear group for the non-scary image, and a higher score than the low-fear group for the scary image. I would like to know whether each of these differences is significant, that is, whether the difference between fear groups is significant at each level of the image variable, but I'm not sure how to test it.&lt;/p&gt;&#10;&#10;&lt;p&gt;Many thanks.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-08T14:13:12.533" Id="106204" LastActivityDate="2014-07-08T14:13:12.533" OwnerUserId="48494" PostTypeId="1" Score="0" Tags="&lt;anova&gt;&lt;categorical-data&gt;&lt;interaction&gt;&lt;post-hoc&gt;&lt;mixed-design&gt;" Title="Follow up a significant interaction in ANOVA with dichotomous variables" ViewCount="44" />
  <row AnswerCount="0" Body="&lt;p&gt;I want to interpret the goodness of attributes using feature selection with 10-fold cross validation.&lt;/p&gt;&#10;&#10;&lt;p&gt;With ChiSquared I get something like this (deletet attributes with merrit was 0 in all folds):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;average merit      average rank  attribute&#10;13.632 +- 1.6        1   +- 0        8 E8&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;or&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;average merit      average rank  attribute&#10; 3.957 +- 0.306      2.1 +- 0.54     5 E5&#10; 8.015 +- 2.672      4.2 +- 8.94    23 E23&#10; 1.331 +- 3.994      7   +- 2.1     14 E14&#10; 1.095 +- 3.285      9   +- 2.68     8 E8&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;With SVM i get:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;average merit      average rank  attribute&#10;34     +- 0          1   +- 0        1 class (perfect attribute to compare merit)&#10;31.5   +- 2.335      3.5 +- 2.33    13 E12&#10;30.8   +- 1.887      4.2 +- 1.89     4 E3&#10;30.6   +- 1.908      4.4 +- 1.91     9 E8&#10;28.7   +- 3.407      6.3 +- 3.41    10 E9&#10;27.6   +- 2.01       7.4 +- 2.01    20 E19&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I couldn't find descriptions about what the numbers are telling me. Is it just a relative value to say which attribute is better than others or is there a meaning behind?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-08T14:34:49.383" Id="106208" LastActivityDate="2014-07-08T14:34:49.383" OwnerUserId="44027" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;svm&gt;&lt;chi-squared&gt;&lt;feature-selection&gt;&lt;weka&gt;" Title="How to interpret merits in Weka with ChiSquaredAttributeEval and SVMAttributeEval?" ViewCount="94" />
  <row AnswerCount="0" Body="&lt;p&gt;Mean shift is a procedure for locating the maxima of a density function given discrete data sampled from that function. It is useful for detecting the modes of this density. This is an iterative method. How accurate sum of kernels needs to be so that this algorithm works? &#10;For example - Is it okay if it performs poorly at tails and better at mode?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-08T15:09:04.810" Id="106213" LastActivityDate="2014-07-08T15:09:04.810" OwnerUserId="49722" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;kernel&gt;&lt;kernel-trick&gt;&lt;kernel-density-estimate&gt;" Title="How accurate sum of kernel function needs to be, so that we can use it in Mean shift algorithm (may be for image segmentation)?" ViewCount="8" />
  
  <row Body="&lt;p&gt;For pointers to the literature, see the nice answer of @M. Berk. I have just a small comment, which might explain why such methods are not ubiquituous and probably won't be.&lt;/p&gt;&#10;&#10;&lt;p&gt;From the first I would be unsure whether the result pays the effort. The standard argument for Bagging shows that the variance decreases, as long as the results are uncorrelated. More detailed (citing Hastie et. al., chapter 15), if you have $B$ i.i.d. random variables each with variance $\sigma$ and pair-wise correlation $\rho$, the variance of the average is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\rho \sigma^2 + \frac{1-\rho}{B} \sigma^2.$$ &lt;/p&gt;&#10;&#10;&lt;p&gt;If you now have a well tuned random forest ensemble, I guess any of those universal methods you mentioned will probably have a large correlation with the random forest prediction, and as a consequence you will hardly get an improvement. The same will probably hold for any other well tuned ensemble (let it be ANNs, SVMs, etc.).&lt;/p&gt;&#10;&#10;&lt;p&gt;Moreover, putting more and more models in your ensemble can also lead to overfitting (if the ensemble is not properly regularized).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-08T15:47:06.073" Id="106220" LastActivityDate="2014-07-08T15:47:06.073" OwnerUserId="49279" ParentId="106211" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;It seems that the only way this problem has been addressed is by an unweighted least-squares regression of cdf on quantile, using the CDF formula as the model.  I thought that there might be some better Bayesian approach that had already been worked out.&lt;/p&gt;&#10;&#10;&lt;p&gt;The least-squares approach appears sub-optimal.  Not only is it unlikely that the residuals are, in general, Gaussian but an unweighted regression seems almost certainly incorrect since the discrepancy between empirical and theoretical CDF diminishes to zero at the extremes.&lt;/p&gt;&#10;&#10;&lt;p&gt;I guess I'll keep thinking about this.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-08T16:05:41.187" Id="106221" LastActivityDate="2014-07-08T16:05:41.187" OwnerUserId="49085" ParentId="104923" PostTypeId="2" Score="0" />
  <row AnswerCount="1" Body="&lt;p&gt;I am looking for a way to test whether a boundary threshold exists in a physiological response – a sample of the data is plotted below. My hypothesis is that the X-variable imposes a physiological constraint on Y-values, thus producing a boundary 'ceiling' for maximum Y-values that decreases at higher X-values (indicated by the red line on figure). I assume any Y-values below the boundary are limited by some other factor not included in this model.&lt;/p&gt;&#10;&#10;&lt;p&gt;Essentially, my goal is to determine if the boundary exists and if so to derive a confidence interval for the boundary line model – similar to a linear regression model, but describing the upper bound of the Y-values, rather than the center of mass.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm sure something like this exists, but I haven't come across it before. Also, I would appreciate any suggestions on a better title or tags for this post – I assume there are more accurate terms for what I'm describing that would help folks find this post.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/1BQBk.jpg&quot; alt=&quot;threshold&quot;&gt;&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-07-08T17:00:45.833" FavoriteCount="1" Id="106225" LastActivityDate="2015-02-04T13:56:57.283" LastEditDate="2015-02-04T13:56:57.283" LastEditorUserId="22228" OwnerUserId="42170" PostTypeId="1" Score="3" Tags="&lt;regression&gt;&lt;linear-model&gt;" Title="Boundary or threshold test for regression-type scatter plot" ViewCount="109" />
  <row AnswerCount="0" Body="&lt;p&gt;I have the following data with factor 1 (A, B and C) and factor 2 (D and E):&#10;$$&#10;\begin{array}{ccc}&#10;\hline&#10; &amp;amp; D &amp;amp; E\\&#10;\hline&#10;A &amp;amp; 68 &amp;amp; 59\\&#10; &amp;amp; 65 &amp;amp; 57\\&#10; &amp;amp; 63 &amp;amp; 54\\&#10; &amp;amp; 59 &amp;amp; 56\\&#10; &amp;amp; 67 &amp;amp; \\&#10;\hline&#10;B &amp;amp; 59 &amp;amp; 51\\&#10; &amp;amp; 50 &amp;amp; 45\\&#10; &amp;amp; 51 &amp;amp; 46\\&#10; &amp;amp;    &amp;amp; 48\\&#10; &amp;amp;    &amp;amp; 49\\&#10;\hline&#10;C &amp;amp; 40 &amp;amp; 47\\&#10; &amp;amp; 39 &amp;amp; 39\\&#10; &amp;amp; 35 &amp;amp; 40\\&#10; &amp;amp; 36 &amp;amp; 40&#10;\end{array}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Now I would like to make two-way ANOVAs of type I, II and III with R and interpret them.&lt;/p&gt;&#10;&#10;&lt;p&gt;I made the following:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;&#10;rm(list=ls(all=T))&#10;data &amp;lt;- c(68,65,63,59,67,59,50,51,40,39,35,36,59,57,54,56,51,45,46,48,49,47,39,40,40)&#10;f1 &amp;lt;- c(0,0,0,0,0,1,1,1,2,2,2,2,0,0,0,0,1,1,1,1,1,2,2,2,2) ### A=0,B=1,C=2&#10;f2 &amp;lt;- c(3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,4,4,4) ### D=3,E=4&#10;summary(aov(data ~ f1*f2)) ### type 1&#10;install.packages(&quot;car&quot;)&#10;library(car)&#10;Anova(lm(data~f1*f2),type=c(&quot;II&quot;)) ### type 2&#10;Anova(lm(data~f1*f2),type=c(&quot;III&quot;)) ### type 3&#10;&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;For type 1 I get&#10;&lt;code&gt;&#10;Df Sum Sq Mean Sq F value   Pr(&gt;F)&lt;br&gt;&#10;f1           1 1941.9  1941.9 193.420 4.58e-12 &lt;strong&gt;*&#10;f2           1   65.5    65.5   6.527  0.01845 *&lt;br&gt;&#10;f1:f2        1  145.9   145.9  14.537  0.00102 ** &#10;Residuals   21  210.8    10.0&lt;br&gt;&#10;Signif. codes:  0 ‘&lt;em&gt;&lt;/strong&gt;’ 0.001 ‘*&lt;/em&gt;’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1&#10;&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;For type 2 I get&#10;&lt;code&gt;&#10;Response: data&#10;           Sum Sq Df F value    Pr(&gt;F)&lt;br&gt;&#10;f1        1901.11  1 189.355 5.607e-12 &lt;strong&gt;*&#10;f2          65.53  1   6.527  0.018450 *&lt;br&gt;&#10;f1:f2      145.95  1  14.537  0.001016 ** &#10;Residuals  210.84 21&lt;br&gt;&#10;Signif. codes:  0 ‘&lt;em&gt;&lt;/strong&gt;’ 0.001 ‘*&lt;/em&gt;’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1&#10;&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;For type 3 I finally get&#10;&lt;code&gt;&#10;Response: data&#10;             Sum Sq Df F value    Pr(&gt;F)&lt;br&gt;&#10;(Intercept) 1802.06  1 179.489 9.308e-12 &lt;strong&gt;*&#10;f1           329.87  1  32.856 1.089e-05 &lt;em&gt;&lt;/strong&gt;&#10;f2           208.54  1  20.771 0.0001714 &lt;strong&gt;&lt;/em&gt;&#10;f1:f2        145.95  1  14.537 0.0010157 ** &#10;Residuals    210.84 21&lt;br&gt;&#10;Signif. codes:  0 ‘&lt;em&gt;&lt;/strong&gt;’ 0.001 ‘*&lt;/em&gt;’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1&#10;&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Is that allright? &#10;How would you interpret these results?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-07-08T17:39:42.557" Id="106227" LastActivityDate="2014-07-08T18:34:32.493" LastEditDate="2014-07-08T18:34:32.493" LastEditorUserId="26338" OwnerUserId="46727" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;anova&gt;&lt;sums-of-squares&gt;" Title="How can I make 2-way ANOVA (Type I, II and III) with R?" ViewCount="66" />
  <row AcceptedAnswerId="106231" AnswerCount="1" Body="&lt;p&gt;I have 2 groups of patients (control and treated) and 1 variable measured at two time moments (before and after treatment).&#10;Using paired T test I am able to determine the mean (with 95%CI) difference in each group (at baseline and after treatment). Using split-plot ANOVA I can say that the effect of the treatment is not different between the two groups (from the Test of Between Subjects Effects). But I am not able to determine the difference in difference (mean with 95%CI).&#10;I forgot to say that I am using SPSS.&#10;Thank you&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-08T17:39:56.830" Id="106228" LastActivityDate="2014-07-08T19:22:15.890" OwnerUserId="43947" PostTypeId="1" Score="1" Tags="&lt;spss&gt;&lt;difference-in-difference&gt;" Title="Difference in difference using spss" ViewCount="607" />
  <row AnswerCount="1" Body="&lt;p&gt;I want to combine data from different sources.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's say I want to estimate a chemical property (e.g. a &lt;a href=&quot;http://en.wikipedia.org/wiki/Partition_coefficient&quot; rel=&quot;nofollow&quot;&gt;partitioning coefficient&lt;/a&gt;):&lt;/p&gt;&#10;&#10;&lt;p&gt;I have some empirical data, varying due to measurement error around the mean.&lt;/p&gt;&#10;&#10;&lt;p&gt;And, secondly, I have a model predicting an estimate from other information (the model has also some uncertainty).&lt;/p&gt;&#10;&#10;&lt;p&gt;How can I combine those two datasets? [The combined estimate will be used in another model as predictor].&lt;/p&gt;&#10;&#10;&lt;p&gt;Meta-analysis and bayesian methods seem to be suitable. However, haven't found many references and ideas how to implement it (I am using R, but also familiar with python and C++).&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Ok, here's a more real example:&lt;/p&gt;&#10;&#10;&lt;p&gt;To estimate the toxicity of a chemical (typically expressed as $LC_{50}$ = concentration where 50% of animals die) lab-experiments are conducted. Happily the results of the experiments are gathered in a database &lt;a href=&quot;http://cfpub.epa.gov/ecotox/advanced_query.htm&quot; rel=&quot;nofollow&quot;&gt;(EPA)&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here are some values for the insecticide &lt;a href=&quot;http://en.wikipedia.org/wiki/Lindane&quot; rel=&quot;nofollow&quot;&gt;Lindane&lt;/a&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;### Toxicity of Lindane in ug/L&#10;epa &amp;lt;- c(850 ,6300 ,6500 ,8000, 1990 ,516, 6442 ,1870, 1870, 2000 ,250 ,62000,&#10;         2600,1000,485,1190,1790,390,1790,750000,1000,800&#10;)&#10;hist(log10(epa))&#10;&#10;# or in mol / L&#10;# molecular weight of Lindane&#10;mw = 290.83 # [g/mol]&#10;hist(log10(epa/ (mw * 1000000)))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;However, there are also some models available to predict toxicity from chemical properties (&lt;a href=&quot;http://en.wikipedia.org/wiki/Quantitative_structure%E2%80%93activity_relationship&quot; rel=&quot;nofollow&quot;&gt;QSAR&lt;/a&gt;). One of these models predicts toxicity from the octanol/water partition coefficient ($log~K_{OW}$):&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ log~LC_{50} [mol/L] = 0.94~(\pm 0.03)~log~K_{OW}~-~1.33 (\pm~0.1)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The partitioning coefficient of Lindane is $log~K_{OW} = 3.8$ and the predicted toxicity is $ log~LC_{50} [mol/L] = -4.902 $.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lkow = 3.8&#10;mod1 &amp;lt;- -0.94 * lkow - 1.33&#10;mod1&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Is there a nice way to combine these two different informations (lab experimens and model predictions)?&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;hist(log10(epa/ (mw * 1000000)))&#10;abline(v = mod1, col = 'steelblue')&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The combined $LC_{50}$ will be used later on in a model as predictor. Therefore, a single (combined) value would be a simple solution. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, a distribution might be also handy - if this is possible in modelling (how?).&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-07-08T18:15:11.857" FavoriteCount="2" Id="106233" LastActivityDate="2014-07-10T21:58:44.570" LastEditDate="2014-07-10T15:19:48.530" LastEditorUserId="1381" OwnerUserId="1050" PostTypeId="1" Score="7" Tags="&lt;r&gt;&lt;bayesian&gt;&lt;meta-analysis&gt;&lt;jags&gt;" Title="Combining data from different sources" ViewCount="177" />
  
  <row Body="&lt;p&gt;Fit an MLE by maximizing&#10;$$&#10;l(\mathbf{\theta};\mathbf{y})=\sum_{i=1}^Nl{\left(\theta;y_i\right)}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $l$ is the log-likelihood. Fitting an MLE with inverse-probability (i.e. frequency) weights entails modifying the log-likelihood to:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;l(\mathbf{\theta};\mathbf{y})=\sum_{i=1}^Nw_i~l{\left(\theta;y_i\right)}.&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;In the GLM case, this reduces to solving&#10;$$&#10;\sum_{i=1}^N w_i\frac{y_i-\mu_i}{V(y_i)}\left(\frac{\partial\mu_i}{\partial\eta_i}x_{ij}\right)=0,~\forall j&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Source: page 119 of &lt;a href=&quot;http://www.ssicentral.com/lisrel/techdocs/sglim.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.ssicentral.com/lisrel/techdocs/sglim.pdf&lt;/a&gt;, linked at &lt;a href=&quot;http://www.ssicentral.com/lisrel/resources.html#t&quot; rel=&quot;nofollow&quot;&gt;http://www.ssicentral.com/lisrel/resources.html#t&lt;/a&gt;. It's the &quot;Generalized Linear Modeling&quot; chapter (chapter 3) of the LISREL &quot;technical documents.&quot;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-07-08T20:01:13.153" Id="106246" LastActivityDate="2014-07-08T23:28:55.897" LastEditDate="2014-07-08T23:28:55.897" LastEditorUserId="36229" OwnerUserId="36229" ParentId="105230" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;It is obviously not correct to rerun models until by chance they give what you expect...&lt;/p&gt;&#10;&#10;&lt;p&gt;Ideally, the model structure (i.e. selection of predictors, transformations, interactions) is chosen based on a number of points before computing the model:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;Expert knowledge such as publications and research questions. This includes &lt;em&gt;thinking&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Example: if you are mainly interested in the effect of a particular&#10;variable $X$ on the response $Y$ and you know that $X$ has a strong&#10;causal effect on $Z$, then it would be quite stupid to include $Z$ in&#10;the model along with $X$, because this would partially hide the&#10;effect of $X$ onto $Y$. That is maybe the case in your model.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;Univariate distributions of variables (e.g. excluding potential predictor &quot;sex&quot; if there is only one male or if most values are missing; log-transform some right-skewed&#10;variables with outliers if it makes scientific sense etc.)&lt;/li&gt;&#10;&lt;li&gt;Bivariate distributions of the predictors (e.g. if both potential predictors &quot;age&quot; and &quot;experience&quot; are highly correlated, it might suffice to include just one of&#10;the two)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;The hidden message of the above points: Don't take into consideration the association between the response variable and the potential predictors at this point. This will tend to bias the model to fit your expectations. It also answers Question 1: Such univariate screening is not suitable for variable selection. It might be part of the analysis though as complement to the multivariate model. This depends on the research question.&lt;/p&gt;&#10;&#10;&lt;p&gt;The answer to Question 2 depend on what the research question or the objective of the analysis is:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;You could, for instance, be interested in testing some specific hypotheses. Then the &quot;borderline significance&quot; and multiple testing problem becomes an issue. &lt;/li&gt;&#10;&lt;li&gt;(And) or you might want to have a good predictive model. Then cross-validation (or similar) of the performance of the model is of much higher relevance than p values. &lt;/li&gt;&#10;&lt;li&gt;(And) or you might be interested in estimating the effects of some particular predictor. &lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="3" CreationDate="2014-07-08T20:13:48.027" Id="106248" LastActivityDate="2014-07-08T20:24:46.097" LastEditDate="2014-07-08T20:24:46.097" LastEditorUserId="30351" OwnerUserId="30351" ParentId="106235" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;&lt;strong&gt;There is a simple elegant algebraic method&lt;/strong&gt;.  It amounts to little more than repeated pattern matching and replacement (with very simple patterns), making it efficient (at least for small problems like this one).&lt;/p&gt;&#10;&#10;&lt;p&gt;The aim is to compute the generating function for the number of faces appearing $k$ or more times out of $n$ rolls of a generalized &quot;die&quot; that has a finite number (say $m$) of faces with probabilities $\mathbf{p}=(p_1, p_2, \ldots, p_m)$ of appearing.  This, by definition, is a polynomial &lt;/p&gt;&#10;&#10;&lt;p&gt;$$g(u; n,k,\mathbf{p}) = g_0 + g_1u + \cdots + g_ju^j + \cdots$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $g_j$ is the chance of &lt;em&gt;exactly&lt;/em&gt; $j$ faces appearing $k$ or more times.&lt;/p&gt;&#10;&#10;&lt;p&gt;The particular question concerns the case $k=3$, $n=10$, and $\mathbf{p}=\left(\frac{1}{6},\frac{1}{6},\frac{1}{6},\frac{1}{6},\frac{1}{6},\frac{1}{6}\right)$. To illustrate the process, though, consider a two-sided &quot;die&quot; with &quot;faces&quot; indexed by $1$ and $2$ having probabilities $p=\left(\frac{1}{2},\frac{1}{2}\right)$ of appearing: a &quot;fair coin.&quot;  Let's decrease $n$ to $5$ and $k$ to $2$ to keep the expressions short.&lt;/p&gt;&#10;&#10;&lt;p&gt;Begin with a generating function of the die itself,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$f(\mathbf{x}; \mathbf{p}) = p_1 x_1 + p_2 x_2 + \cdots +p_m x_m.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;This is a polynomial in the $m$ variables $\mathbf{x}=\left(x_1, \ldots, x_m\right)$.  For the fair coin,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$f\left(\left(x_1,x_2\right); \left(\frac{1}{2},\frac{1}{2}\right)\right) = \frac{1}{2}x_1 + \frac{1}{2}x_2.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;We can read &lt;em&gt;all&lt;/em&gt; possible outcomes of $n$ throws by expanding $f(\mathbf{x},\mathbf{p})$ to the $n^\text{th}$ power.&lt;/p&gt;&#10;&#10;&lt;p&gt;$$f(\mathbf{x},\mathbf{p})^5 = \frac{x_1^5}{32}+\frac{5}{32} x_2 x_1^4+\frac{5}{16} x_2^2 x_1^3+\frac{5}{16} x_2^3 x_1^2+\frac{5}{32} x_2^4 x_1+\frac{x_2^5}{32}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;This says there is a $\frac{1}{32}$ chance of seeing five ones, a $\frac{5}{32}$ chance of four ones and one two, and so on.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;The trick is to avoid doing this full calculation,&lt;/strong&gt; which is pure brute force.  Notice that we don't need most of it: we only need to recognize the terms where the power of $x_1$ or $x_2$ is $k=2$ or greater.  The rest we can throw away.  One way to accomplish this--which is made rigorous by the algebraic concept of a &lt;em&gt;polynomial ideal&lt;/em&gt;--is to introduce $m$ new variables, say $u_1, u_2, \ldots, u_m$.  We simply declare that $u_i = x_i^k$ for each $i$ and use this to simplify the expansion.  Moreover, the rules of algebra imply we may perform the simplification by computing $f(\mathbf{x},\mathbf{p})^n$ recursively as&lt;/p&gt;&#10;&#10;&lt;p&gt;$$f(\mathbf{x},\mathbf{p})^n = f(\mathbf{x},\mathbf{p})f(\mathbf{x},\mathbf{p})^{n-1}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and doing the simplification at each step.  In the example this procedure yields&lt;/p&gt;&#10;&#10;&lt;p&gt;$$f(\mathbf{x},\mathbf{p})^5 = \frac{1}{32} u_1^2 x_1+\frac{5}{32} u_1^2 x_2+\frac{5}{16} u_2 u_1 x_1+\frac{5}{16} u_2 u_1 x_2+\frac{5}{32} u_2^2 x_1+\frac{1}{32} u_2^2 x_2.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;We may polish this off with a few more conversions.  First, the appearance of any power of $u_i$ of $2$ or greater just reflects the fact that $x_i^k$ was found multiple times.  For instance, $u_1^2$ means $x_1^k$ appeared twice--that is, $x_1$ appeared at least $2k$ times.  Furthermore, after having computed $f\left(\mathbf{x},\mathbf{p}\right)^n$ we can ignore any powers of the $x_i$ still lying around.  That is easily done by setting all the $x_i=1$.  In the example the result of these two operations is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac{5 u_2 u_1}{8}+\frac{3 u_1}{16}+\frac{3 u_2}{16}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;All that remains to do is track the total degree of each monomial: $u_2u_1$ has degree $2$, indicating that two separate faces appeared at least $k$ times.  $u_1$ and $u_2$ have degree $1$, indicating that exactly one face appeared at least $k$ times.  The degree is easy to compute: set all the variables $u_i=u$ for a single variable $u$.  In the example we have thereby found that&lt;/p&gt;&#10;&#10;&lt;p&gt;$$g(u;n,k,\mathbf{p}) = \frac{5 u^2}{8}+\frac{3 u}{8}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;(Interpretation: after five flips of a fair coin, there is a $3/8$ chance that exactly one face will be observed two or more times and a $5/8$ chance that both faces will be observed two or more times.  These results are easily checked.)&lt;/p&gt;&#10;&#10;&lt;p&gt;For the setting in the question, with ten tosses of a fair die, these calculations yield&lt;/p&gt;&#10;&#10;&lt;p&gt;$$g\left(u; 10, 3, \left(\frac{1}{6},\frac{1}{6},\frac{1}{6},\frac{1}{6},\frac{1}{6},\frac{1}{6}\right)\right)=\frac{4375 u^3}{209952}+\frac{52415 u^2}{139968}+\frac{225559 u}{419904}+\frac{175}{2592}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;From this we may easily compute any probabilities we wish.  For instance, the event where two distinct faces each appear three or more times (and all other faces two times or fewer) has probability equal to the coefficient of $u^2$ and therefore is $52415/139968 \approx 0.374478.$&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Using a computer algebra system, the code is remarkably short.  The following is a &lt;em&gt;Mathematica&lt;/em&gt; solution.  All the work is done in the three middle lines, exactly following the foregoing description.&lt;/p&gt;&#10;&#10;&lt;pre class=&quot;lang-mathematica prettyprint-override&quot;&gt;&lt;code&gt;g[y_, p_List, n_Integer, k_Integer] /; n &amp;gt;= 1 &amp;amp;&amp;amp; k &amp;gt;= 1 := &#10; Module[{f, x, u, a, i}, &#10;   f = Sum[p[[i]] Subscript[x, i], {i, 1, Length[p]}]/Total[p];&#10;   Nest[(Expand[f #] /. {Power[Subscript[x,i_], a_] /; a == k -&amp;gt; Subscript[u,i]}) &amp;amp;, 1, n] &#10;    /. {Subscript[x, _] -&amp;gt; 1, Power[Subscript[u, _], _] -&amp;gt; y, Subscript[u, _] -&amp;gt; y}  &#10;  ]&#10;g[u, ConstantArray[1/6, 6], 10, 3]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Execution took 0.2 seconds.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-07-08T21:31:04.333" Id="106251" LastActivityDate="2014-07-09T13:18:30.410" LastEditDate="2014-07-09T13:18:30.410" LastEditorUserId="919" OwnerUserId="919" ParentId="106247" PostTypeId="2" Score="9" />
  <row AnswerCount="0" Body="&lt;p&gt;I have implemented Binary RBM in Matlab. I am using 60000 images as an input to train RBM. It takes approximately 11.3 minutes. I used tic and toc functions to evaluate above mentioned elapsed time. Is that,  looks reasonable time? or it is slow?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-07-08T22:52:00.050" Id="106255" LastActivityDate="2014-07-10T21:00:32.080" LastEditDate="2014-07-10T21:00:32.080" LastEditorUserId="48880" OwnerUserId="48880" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;" Title="How much time is reasonable for training Restricted Boltzmann Machine?" ViewCount="23" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;&lt;strong&gt;The typical instrumental variable setup&lt;/strong&gt; seeks a consistent estimate of $\beta$ from &#10;$$&#10;y = \alpha + \beta x + \epsilon&#10;$$&#10;where $cor(x,\epsilon) \neq 0$, in the univariate case, without loss of generality, by instrumenting with some variable $z$ that is uncorrelated with $y$ conditional on $x$ (the &quot;exclusion restriction&quot;).  $\hat\beta_{IV} = (Z'X)^{-1}Z'y$.  It can also be motivated as two-stage least squares, etc.  In the multivariate case, one generally needs at least as many instruments as endogenous variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;I've got a somewhat exotic case&lt;/strong&gt;, involving nodes in a network.  $y_i$ is an outcome for node $i$, and $x_i$ is an endogenous predictor of $y_i$.  I've got a single $z_i$ that is randomly assigned, and which predicts $x_i$ but not $y_i$, except through its effect on $x_i$.  It gets tricky when I consider that $y$ is also predicted by some $f(x_{-i})$ (i.e.: values of $x$ at other nodes).  &lt;/p&gt;&#10;&#10;&lt;p&gt;The first stage model is something like &#10;$$&#10;x_i = f(z_i) + \eta_i&#10;$$&#10;where $cor(z,\eta) = 0$&lt;/p&gt;&#10;&#10;&lt;p&gt;The second stage, represented as two-stage least squares, is something like&#10;$$&#10;y_i = \alpha + \beta_1\hat x_i + \beta_2 \hat u_i + \epsilon_i&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $u_i = f(x_{-i},D_{-i})$ -- basically some function of the $x$'s of nearby $i$'s, and their distances $D$.  $\hat u$ is therefore $f(\hat x_{-i},D_{-i})$&lt;/p&gt;&#10;&#10;&lt;p&gt;So, I've got more endogenous variables than instruments, but my endogenous variables are all derived from a single endogenous variable, for which I have one instrument.  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;My question&lt;/strong&gt;:  is my second stage underidentified?  Or just identified?  And why?  &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-07-08T23:49:54.960" Id="106266" LastActivityDate="2014-07-08T23:49:54.960" OwnerUserId="17359" PostTypeId="1" Score="2" Tags="&lt;econometrics&gt;&lt;instrumental-variables&gt;&lt;networks&gt;" Title="More than one function of a single instrumented endogenous variable -- is the model still underidentified?" ViewCount="32" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm coding a &lt;strong&gt;McMC&lt;/strong&gt; algorithm for geophysical applications. &lt;/p&gt;&#10;&#10;&lt;p&gt;Using the Metropolis–Hastings scheme to accept/reject the proposed models is something that I thought I understood completely, but I don't. To be clear:&#10;normally you have a current model $m$, you perturb one parameter (in my case velocity $v_i$), and in this way you get a trial model $m'$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The acceptance probability for such a perturbation is:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\alpha(m'|m)=min\left[ 1, \frac{p(m')}{p(m)} \cdot \frac{p(d|m')}{p(d|m)} \cdot \frac{q(m|m')}{q(m|m)} \cdot |J|\right]$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Where $p(m)$ are the priors, $p(d|m)$ are the likelihoods, $q(m|m')$ are the proposals, and $J$ the Jacobian.&lt;/p&gt;&#10;&#10;&lt;p&gt;So assuming that the priors are symmetric, and the Jacobian is 1, I remain with:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\alpha(m'|m)=min\left[ 1,\frac{p(d|m')}{p(d|m)} \cdot \frac{q(m|m')}{q(m|m)} \right]$$ &lt;/p&gt;&#10;&#10;&lt;p&gt;The likelihood ratio is not a problem, so now I want to deal only with the proposal ratio:&lt;/p&gt;&#10;&#10;&lt;p&gt;Supposing that the way I make a perturbation is choosing one parameter, and perturb it with Gaussian probability, so that the new velocity value will be $$v'_i=v_i + u \cdot \sigma$$ ($u$ is normal distributed with mean = 0 and var. = 1)&lt;br&gt;&#10;This perturbation can be described by a proposal in the form:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$q(v'|v)= \frac{1}{\sigma \sqrt{2 \pi}} \cdot \exp \left[- \frac{(v'-v)^2}{2 \sigma ^2} \right]$$&lt;/p&gt;&#10;&#10;&lt;p&gt;It's easy to see that for this kind of perturbation the proposals are symmetric, so the acceptance probability is just the ratio of the likelihoods.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Questions:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;If instead of perturbing one parameter, I want to change 3 of them at the same time, what happens to my proposals, and so to my acceptance probability $\alpha$? How should I compute them? ($v_1, v_2, v_3$ are not correlated.)&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;If I want to include some deterministic constraints because I know that the 3 parameters I want to change are correlated, i.e. if I increase $v_1$ of $v_p$, then I want to decrease $v_2$ and $v_3$ of $v_p / 3$ (a sort of compensation)...how am I supposed to come up with a representation of the proposal?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2014-07-09T08:23:39.920" Id="106294" LastActivityDate="2014-07-09T09:03:08.067" LastEditDate="2014-07-09T09:03:08.067" LastEditorUserId="32036" OwnerUserId="49038" PostTypeId="1" Score="1" Tags="&lt;monte-carlo&gt;&lt;markov-chain&gt;&lt;metropolis-hastings&gt;" Title="Simultaneous multiple perturbations in Markov chain Monte Carlo" ViewCount="26" />
  <row AcceptedAnswerId="110114" AnswerCount="1" Body="&lt;p&gt;I have been training MLPs for a binary classification problem in the Weka explorer and now have one with an acceptable level of accuracy.  I've written some code to parse the text of the model which is output, and write it out as array literals (in my C# code) for the weights and bias terms.  I then have my own code to implement the neural network output.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;The problem is that my confusion matrix differs from Weka's; for example one of the values was 86.8% in Weka and 91.3% in my code. How can I explain this difference?&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I've downloaded someone else's C# neural network code and their matrix was the same as mine.  So I doubt it is because my implementation of the neural network output is incorrect.&lt;/p&gt;&#10;&#10;&lt;p&gt;I've manually checked that the weights and the normalisation ranges are the same.&lt;/p&gt;&#10;&#10;&lt;p&gt;Weka is running 10-fold cross validation, whereas my code is evaluating the error on the whole data set.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Is there a way in Weka to run the classifier against a data set, and obtain the confusion matrix, but without changing the classifier?  Or, to run the classifier and get the actual output values&lt;/em&gt;, so I can compare them with those in my code? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-09T10:22:22.890" Id="106307" LastActivityDate="2014-07-31T10:45:20.583" OwnerUserId="48636" PostTypeId="1" Score="0" Tags="&lt;neural-networks&gt;&lt;weka&gt;&lt;c#&gt;" Title="How do I ensure my implementation of a MLP from Weka is correct?" ViewCount="131" />
  
  <row AcceptedAnswerId="107506" AnswerCount="3" Body="&lt;p&gt;I have following type of &lt;strong&gt;associated&lt;/strong&gt; data. The following example step to generate associated variable. &lt;strong&gt;p&lt;/strong&gt; number of variables and &lt;strong&gt;n&lt;/strong&gt; is number of observations. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;p = 500&#10;n = 200&#10;mat &amp;lt;- matrix(NA, ncol = 500, nrow = 200)&#10;&#10;for (i in 1:p){&#10;if(i ==1){&#10;     fs &amp;lt;- sample (c(&quot;AA&quot;, &quot;AB&quot;, &quot;AB&quot;, &quot;BB&quot;), n, replace=TRUE) &#10;     mat[,i] &amp;lt;- fs &#10;     fs1 &amp;lt;- fs &#10;     } &#10;rechr &amp;lt;- sample(1:n, 1)&#10;fs1[rechr] &amp;lt;- sample (c(&quot;AA&quot;, &quot;AB&quot;, &quot;AB&quot;, &quot;BB&quot;), 1)&#10;mat[,i] &amp;lt;- fs1 &#10;}&#10;&#10;dim(mat)&#10;mat[1:10,1:20]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The above data is complete, but in real dataset I have missing values (which are randomly distributed within each variables).&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to predict them using random forest (or any other appropriate algorithms). Let's randomly put 10% missing values (the approximate number of variables) to the above data.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;rowind &amp;lt;- sample(1:n, 20)&#10;colind &amp;lt;- sample(1:p, 20)&#10;&#10;for (i in 1:length(rowind)){&#10;                mat[rowind[i],colind[i]] &amp;lt;- NA&#10;}&#10;mat[1:20,1:10]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;How can do this and &lt;strong&gt;what issues&lt;/strong&gt; I need to think up in doing so - considering &lt;strong&gt;categorical variables&lt;/strong&gt;, number of observations and variables ? &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt;&#10;As all variables are correlated and none are response (but can serve as response) while prediction. for example mat[,1] missing value prediction can be based on rest of variables, mat[,2] prediction can be based on prediction of rest of variables. So the objective here is prediction all missing values in all variables (all of them are categorical). &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit 2&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I am interested in &lt;a href=&quot;http://support.sas.com/rnd/app/papers/multipleimputation.pdf&quot; rel=&quot;nofollow&quot;&gt;multiple imputation&lt;/a&gt; of this data so that I can perform other analysis that requires complete dataset without missing value.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-09T12:34:31.470" FavoriteCount="1" Id="106318" LastActivityDate="2014-07-16T03:10:23.660" LastEditDate="2014-07-10T19:08:37.917" LastEditorUserId="7244" OwnerUserId="7244" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;random-forest&gt;&lt;missing-data&gt;&lt;data-imputation&gt;" Title="using random forest for missing data imputation in categorical variables ( in R)" ViewCount="688" />
  <row AnswerCount="1" Body="&lt;p&gt;Let $z$  a random variable with PDF : $f_z= Cz^{k-1}(1-\frac{z}{d})^bF(-a+k+1,b;b+1;1-\frac{z}{d})$, where $0\leq z \leq d$, $F$ is the Hypergeometric function, $k$ is a positive integer, $-a+k+1 &amp;gt;0$, $C$ is a constant and $a ,b$ are 2 real positive numbers.&lt;/p&gt;&#10;&#10;&lt;p&gt;Please I need to compute a closed form of the Moment generating function of $z$.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-09T12:56:20.200" Id="106324" LastActivityDate="2014-07-10T14:52:47.543" LastEditDate="2014-07-09T15:51:16.223" LastEditorUserId="48775" OwnerUserId="48775" PostTypeId="1" Score="2" Tags="&lt;probability&gt;&lt;distributions&gt;&lt;moments&gt;&lt;hypergeometric&gt;&lt;integral&gt;" Title="Moment generating function if the PDF is $f_z= Cz^{k-1}(1-\frac{z}{d})^bF(-a+k+1,b;b+1;1-\frac{z}{d})$" ViewCount="62" />
  <row Body="&lt;p&gt;Is your goal model parsimony or the predictive power of the model?  If parsimony, then use AIC, if predictive power then $R^2$.  Usually the answer is similar, but if you are comparing models with very similar $R^2$ or a number of low quality predictors the answers can be different.  This is why in regular regression we tend to look at adjusted $R^2$ rather than just $R^2$, that is, because the adjusted value penalizes $R^2$ to adjust for the variance one might expect to be explained by chance if a predictor was not really effective at all.  As the author says in the blog post &quot;although I should note that [$R^2$ is] a poor tool for model selection, since it almost always favors the most complex models&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;P.S.  If you are interested in the fixed effects relative to the overall variance (regardless of nesting factor), then you probably want to be looking at the marginal $R^2$.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-09T12:58:26.090" Id="106326" LastActivityDate="2014-07-09T13:16:45.127" LastEditDate="2014-07-09T13:16:45.127" LastEditorUserId="196" OwnerUserId="196" ParentId="106320" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="106349" AnswerCount="1" Body="&lt;p&gt;Wikipedia provides the &lt;a href=&quot;http://en.wikipedia.org/wiki/Confidence_interval#Statistical_theory&quot; rel=&quot;nofollow&quot;&gt;following definition&lt;/a&gt; for a confidence interval for a parameter $\theta$:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;strong&gt;A confidence interval&lt;/strong&gt; for the parameter θ, with confidence level or&#10;  confidence coefficient γ, is an interval with random endpoints (u(X),&#10;  v(X)), determined by the pair of random variables u(X) and v(X), with&#10;  the property:&lt;/p&gt;&#10;  &#10;  &lt;p&gt;${\Pr}_{\theta,\varphi}(u(X)&amp;lt;\theta&amp;lt;v(X))=\gamma\text{ for all&#10;&amp;gt; }(\theta,\varphi). $&lt;/p&gt;&#10;  &#10;  &lt;p&gt;Here $Pr(θ,φ)$ indicates the probability distribution of X&#10;  characterised by $(θ, φ)$. &lt;/p&gt;&#10;  &#10;  &lt;p&gt;In a specific situation, when x is the outcome of the sample X, &lt;strong&gt;the&#10;  interval $(u(x), v(x))$ is also referred to as a confidence interval&lt;/strong&gt; for&#10;  $θ$. Note that it is no longer possible to say that the (observed)&#10;  interval $(u(x), v(x))$ has probability γ to contain the parameter $θ$.&#10;  This observed interval is just one realization of all possible&#10;  intervals for which the probability statement holds.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I presume that $X$ here is the random variable representing the sample and that $u(x)$ and $v(X)$ refer to not only random variables, but to &lt;strong&gt;methods&lt;/strong&gt; for constructing the upper and lower bounds of the confidence interval from one sample.&lt;/p&gt;&#10;&#10;&lt;p&gt;So it looks like there are &lt;strong&gt;two&lt;/strong&gt; confidence interval definitions. In the first one, one can say that the interval has probability $\gamma$ of containing&#10; $\theta$, while in the second one, one cannot. Why is this the case? &lt;/p&gt;&#10;&#10;&lt;p&gt;Perhaps more specifically, I fail to see how one reaches the following conclusion &lt;strong&gt;given the second definition of a confidence interval&lt;/strong&gt; provided: &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;... Note that it is &lt;strong&gt;no&lt;/strong&gt; longer possible to say that the (observed)&#10;  interval $(u(x), v(x))$ has probability $\gamma$ to contain the parameter $θ$.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="3" CreationDate="2014-07-09T14:11:13.300" Id="106338" LastActivityDate="2014-07-09T15:19:50.157" LastEditDate="2014-07-09T15:19:50.157" LastEditorUserId="2798" OwnerUserId="2798" PostTypeId="1" Score="2" Tags="&lt;confidence-interval&gt;&lt;frequentist&gt;" Title="Confidence interval definitions" ViewCount="48" />
  <row AnswerCount="1" Body="&lt;p&gt;My question is if data are in paired groups (before and after treatment), where the pre-treatment group is normally distributed and the post-treatment group is not normally distributed (i.e. it is skewed), then which test should be applied? Is the Wilcoxon sign rank test appropriate?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-09T14:32:45.620" Id="106342" LastActivityDate="2014-07-09T17:18:52.400" LastEditDate="2014-07-09T17:08:31.013" LastEditorUserId="44269" OwnerUserId="49789" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;" Title="Which test should be used when paired data (pre/post treatment) have different distributions?" ViewCount="32" />
  <row AnswerCount="0" Body="&lt;p&gt;I have an interview with a top company for a data scientist position.   I was made aware that they will be testing probability/statistical theory concepts.  &lt;/p&gt;&#10;&#10;&lt;p&gt;So the question:&lt;/p&gt;&#10;&#10;&lt;p&gt;If you had 1 hour tops to do an all-inclusive interview about a range of topics, and wanted to test someone's knowledge of probability/statistical theory as well.... what concepts would you test?&lt;/p&gt;&#10;&#10;&lt;p&gt;Another way of phrasing the question:&lt;/p&gt;&#10;&#10;&lt;p&gt;What are the most important concepts to know from probability/statistical theory?&lt;/p&gt;&#10;&#10;&lt;p&gt;(I can't study everything!)&lt;/p&gt;&#10;" ClosedDate="2014-07-09T15:42:23.873" CommentCount="2" CreationDate="2014-07-09T14:45:07.713" Id="106346" LastActivityDate="2014-07-09T14:45:07.713" OwnerUserId="49790" PostTypeId="1" Score="0" Tags="&lt;probability&gt;&lt;mathematical-statistics&gt;&lt;data-mining&gt;&lt;theory&gt;&lt;mathematics&gt;" Title="Most Important Stat Theory Concepts -- Interview" ViewCount="39" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Let me explain my scenario in which I need to calculate absolute error.&lt;/p&gt;&#10;&#10;&lt;p&gt;Lets say the X is the actual value. And X' is the value of X with some error 'e'. So X' = X + e'.&lt;/p&gt;&#10;&#10;&lt;p&gt;Lets say i = 1 to 10000. I have X(i) as well as X'(i). Now I want to know (also want to plot on graph) that How much X' differs from X. One way to do is to calculate Absolute error and plot it on the graph.&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to know if there are other ways to see how much X differs from X' besides absolute error? and Which one will be more appropriate in my case as I have to plot it as a graph? And also easier to follow for those who will see that plotted graph?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks! :)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-09T15:31:04.000" Id="106353" LastActivityDate="2014-07-09T17:09:35.850" OwnerUserId="41988" PostTypeId="1" Score="0" Tags="&lt;mathematical-statistics&gt;&lt;data-mining&gt;&lt;computational-statistics&gt;&lt;functional-data-analysis&gt;" Title="Alternatives to absolute error?" ViewCount="29" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;All common treatment of PAC bounds based on Rademacher complexity assume a bounded loss function (for a self-contained treatemnt, &lt;a href=&quot;http://www.cs.princeton.edu/courses/archive/spring13/cos511/scribe_notes/0305.pdf&quot; rel=&quot;nofollow&quot;&gt;see this handout by Schapire&lt;/a&gt;. However, I could not find any result for unbounded losses (with finite moments or possibly subgaussian). The only &lt;a href=&quot;http://arxiv.org/abs/1310.5796&quot; rel=&quot;nofollow&quot;&gt;paper&lt;/a&gt; I know of is Cortes et al, but it is about &lt;em&gt;relative&lt;/em&gt; bounds, not absolute ones. Does anyone know of a result that explicitly covers Rademacher bounds for general loss functions?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-09T17:56:57.097" FavoriteCount="1" Id="106373" LastActivityDate="2014-07-09T17:56:57.097" OwnerUserId="30" PostTypeId="1" Score="2" Tags="&lt;machine-learning&gt;" Title="Rademacher bounds for unbounded loss functions" ViewCount="91" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have a question regarding the use of logistic/log-linear regression vs. contingency test statistics, such as chi-square.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can someone explain to me why it would ever be preferable to use test statistics over logistic (or log-linear) regression modeling?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-09T20:54:17.917" Id="106386" LastActivityDate="2014-07-10T00:44:35.530" OwnerUserId="49818" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;logistic&gt;&lt;chi-squared&gt;&lt;contingency-tables&gt;&lt;log-linear&gt;" Title="Why use chi-square (or other) statistic over regression modeling for 2x2 contingency tables?" ViewCount="111" />
  <row AnswerCount="1" Body="&lt;p&gt;I'd like to review the results of a recent survey I conducted.  I'd like to see how one set of answers may/may not have influenced another set of answers.  &lt;/p&gt;&#10;&#10;&lt;p&gt;For example:&lt;/p&gt;&#10;&#10;&lt;p&gt;If users who answered question #1 on a 1-8 Likert scale with answers from 6-8, does this correlate to how they answered questions 2-8?  &lt;/p&gt;&#10;&#10;&lt;p&gt;Or:&lt;/p&gt;&#10;&#10;&lt;p&gt;Users who answered false for question 10 did/did not similarly answer questions 11-12.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-09T21:59:15.553" Id="106392" LastActivityDate="2014-07-09T23:38:56.440" OwnerUserId="49824" PostTypeId="1" Score="0" Tags="&lt;survey&gt;&lt;likert&gt;" Title="How to see if one (or one set of commonly answered) question(s) impacts/statistically influences another question/set of questions?" ViewCount="13" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have 15 subjects each with 200 trials &amp;amp; I'd like to run separate regressions for each subject.&lt;/p&gt;&#10;&#10;&lt;p&gt;If I just run the regression on the whole dataset I am able to generate odds ratios / confidence intervals from it:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;acc.log = glm(response ~ phdot + sdot, data=fullerr, family=&quot;binomial&quot;)&#10;summary(acc.log)&#10;&#10;exp(acc.log$coefficients)&#10;exp(confint(acc.log))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;But then when I fit each subject separately (I've tried using &lt;code&gt;by&lt;/code&gt;), I am an unsure how to then extract information like odds ratios (standard errors &amp;amp; p values are also no longer shown in the output).&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;split.acc &amp;lt;- by(fullerr, fullerr[,&quot;subject&quot;], function(fullerr){ &#10;                  glm(response ~ phdot + sdot, data=fullerr, family=&quot;binomial&quot;)&#10;                })&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-07-09T22:15:53.557" Id="106394" LastActivityDate="2014-07-09T22:42:58.687" LastEditDate="2014-07-09T22:42:58.687" LastEditorUserId="7290" OwnerUserId="49825" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;logistic&gt;&lt;repeated-measures&gt;" Title="Odds ratios from logistic regression on subsets of data using R" ViewCount="48" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am building a regression model and I'm having an issue with how to incorporate geographical information into the model as a predictor in the model. I have multiple geographical levels of granularity: ZIP, CBSA (Core Based Statistical Area), and state. The idea is to reduce geography to as granular a level as possible without splitting up geography into a bajillion different factors. Ideally, we'd just break everything down to the ZIP level, but we don't have enough observations at that level. Basically the problem is balancing the number of factors with observations per factor. &lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a mathematical way to figure out the right number of observations at the ZIP / CBSA / State level to lead to the best model performance? Right now I'm doing a hackish way trying different combinations of observations per geographical level to see what leads to the best predictions. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-07-10T00:28:13.103" Id="106406" LastActivityDate="2014-07-10T00:28:13.103" OwnerUserId="26356" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;categorical-data&gt;&lt;prediction&gt;" Title="Nested factor variable: How to balance the number of observations with number of levels of a factor?" ViewCount="19" />
  <row Body="&lt;p&gt;$X\neq x$ --&gt; The error in your reasoning is to think that $x$ and $X$ stand for the same object: $x$ is just the variable of integration, we can use any letter. In the first case $x$ is a realization/value of $X$ but in the second case $x$ is a realization/value of $2X$ when you write $f_{2X}(x)$.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-07-10T01:44:30.993" Id="106412" LastActivityDate="2014-07-10T01:44:30.993" OwnerUserId="43681" ParentId="106410" PostTypeId="2" Score="3" />
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to perform an attrition analysis on a company with an average size fo around 250 to 300. I have monthly attrition data for the last 30 months or so. Now if i want to go for a predictive approach where i can predict the attrition for next few months, say 3 months, which one would be the best approach.&lt;/p&gt;&#10;&#10;&lt;p&gt;After some research I am juggling between poisson's distribution and time series analysis. Which approach is the better way to go ?? and why ?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-10T05:15:18.703" FavoriteCount="1" Id="107424" LastActivityDate="2014-07-10T05:15:18.703" OwnerUserId="51836" PostTypeId="1" Score="0" Tags="&lt;time-series&gt;&lt;poisson&gt;" Title="What approach to use for attrition analysis?" ViewCount="29" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Im not understanding the following;&lt;/p&gt;&#10;&#10;&lt;p&gt;suppose $y \sim N (\mu,\sigma^2)$&#10;and we have a prior $\mu \sim N (\mu_0, \sigma^2_1)$&lt;/p&gt;&#10;&#10;&lt;p&gt;Then we  can figure out the posterior distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;What i dont understand is suppose&lt;/p&gt;&#10;&#10;&lt;p&gt;$y=\mu+e$ where $e \sim N(0,\sigma^2_e)$&#10;how do I know that  $y \sim N (\mu,\sigma^2)$ &#10;i.e. how do I know that Y is centered around $\mu$?&#10;Won't it be centered around $\mu_0$? Because;&lt;/p&gt;&#10;&#10;&lt;p&gt;$E(y)=E(\mu)+E[e]=\mu_0$&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-07-10T10:04:44.117" FavoriteCount="1" Id="107449" LastActivityDate="2014-07-15T23:54:03.190" LastEditDate="2014-07-10T10:06:30.077" LastEditorUserId="26338" OwnerUserId="51851" PostTypeId="1" Score="0" Tags="&lt;distributions&gt;&lt;bayesian&gt;&lt;posterior&gt;" Title="Posterior distribution of a random variable" ViewCount="79" />
  <row AcceptedAnswerId="107751" AnswerCount="2" Body="&lt;p&gt;I understood that Holt Winters forecasting may results in negative values due to trending. I did reduce trending component value, but still forecast values are negative territory. Our data set will never be in negative values (like electricity data set, which never falls below ZERO).&lt;/p&gt;&#10;&#10;&lt;p&gt;What sort of post algorithm techniques we can apply to make this value as non-negative value? Any help would be appreciated.&lt;/p&gt;&#10;&#10;&lt;p&gt;This implementation is in Java, so I can't use &lt;code&gt;ets()&lt;/code&gt; package at this point of time (and I am assuming &lt;code&gt;ets()&lt;/code&gt; also can't avoid negative values).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/7DJ5z.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-10T14:12:50.307" Id="107467" LastActivityDate="2014-07-12T23:58:32.450" LastEditDate="2014-07-10T14:31:35.273" LastEditorUserId="24778" OwnerUserId="24778" PostTypeId="1" Score="3" Tags="&lt;time-series&gt;&lt;forecasting&gt;&lt;exponential-smoothing&gt;" Title="Avoid negative results in Holt Winters forecasting" ViewCount="291" />
  <row AcceptedAnswerId="107478" AnswerCount="1" Body="&lt;p&gt;I know what the z-value of a single observation is, that is explained in Wikipedia. But what is the z-value of a parameter in a glm model?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-07-10T15:25:07.847" Id="107473" LastActivityDate="2014-07-10T23:31:04.567" LastEditDate="2014-07-10T23:31:04.567" LastEditorUserId="805" OwnerUserId="6193" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;generalized-linear-model&gt;&lt;summary-statistics&gt;" Title="What is the z-value of glm model parameters?" ViewCount="190" />
  <row Body="&lt;p&gt;Reporting back with what I've found out:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edge cut&lt;/strong&gt;: This is the number of edges that stretch between the partitions, and are &quot;cut&quot; when the partitioning is done. If the edges are weighted, it is the sum of weights. Naturally you want this to be as low as possible.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Communication volume&lt;/strong&gt;: This is the number of nodes, or sum of weights for the nodes that are connected to nodes in other partitions. Those nodes connected by &quot;edge cut&quot;-edges if you will.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Subdomain connectivity&lt;/strong&gt;: The number of other partitions each partition is connected to (using the &quot;edge cut&quot;-edges).&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The original graph had 1243 connected components and the resulting &#10;     partitioning after removing the cut edges has 2362 components.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;This means that the partitioning has split connected components in the graph, resulting in an increased number of components.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Furthermore, I've found that plotting the edge cut for different partitionings produce a graph similar to that used to decide on the optimal number of cluster in k-Means clustering. A &quot;knee&quot; can be found where for the optimal partitioning.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-10T15:35:12.757" Id="107476" LastActivityDate="2014-07-10T15:35:12.757" OwnerUserId="49360" ParentId="105502" PostTypeId="2" Score="0" />
  <row AnswerCount="1" Body="&lt;p&gt;This question is from a confused novice.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a data set with where each point is located in a 2-D space defined by two objectives (say, X and Y). I wish to identify a set of points from this space such that their distance from X AND y is minimum. &lt;/p&gt;&#10;&#10;&lt;p&gt;What can be the most efficient / simple / targeted approach to achieve this ? I started with k-means clustering but eventually ventured into material related to multiobjective optimization, multiobjective clustering, clustering ensemble, etc. &lt;/p&gt;&#10;&#10;&lt;p&gt;K-means CA will try to identify clusters via unspecified leaning of the data based on internal criteria (compactness and separation), but is there a way to identify clusters using external criteria such as distance of a cluster member from the above X and Y axes?&lt;/p&gt;&#10;&#10;&lt;p&gt;Will the multiobjective methods be an overkill ? Are the advanced methods unobtrusive? &#10;Can these methods work with the dataset that already exists ?&lt;/p&gt;&#10;&#10;&lt;p&gt;A lead in the right direction will be greatly appreciated. Thank you.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-10T15:36:52.570" Id="107477" LastActivityDate="2014-07-11T07:56:15.123" OwnerUserId="51861" PostTypeId="1" Score="0" Tags="&lt;clustering&gt;&lt;optimization&gt;&lt;ensemble&gt;" Title="What method to use for cluster identification ?" ViewCount="28" />
  <row Body="&lt;p&gt;It is the test-statistic for the Wald-test that the parameter is 0. It is the parameter divided by the standard error. If the null-hypothesis is true (i.e. the parameter is 0 in the population) and we were to draw many samples from our population and compute the z-statistic in each of them, then those z-statistics will follow a standard normal distribution.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-07-10T15:38:32.533" Id="107478" LastActivityDate="2014-07-10T15:38:32.533" OwnerUserId="23853" ParentId="107473" PostTypeId="2" Score="2" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a basic question regarding pattern learning, or pattern representation. Assume I have a  complex pattern of this form, could you please provide me with some research directions or concepts that I can follow to learn how to represent (mathematically describe) these forms of patterns? in general the pattern does not have a closed contour nor it can be represented with analytical objects like boxes, circles etc. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/nuSqr.jpg&quot; alt=&quot;binary discrete pattern&quot;&gt;&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-07-10T18:33:39.833" FavoriteCount="1" Id="107503" LastActivityDate="2014-07-10T18:33:39.833" OwnerUserId="28761" PostTypeId="1" Score="0" Tags="&lt;discrete-data&gt;&lt;pattern-recognition&gt;&lt;image-processing&gt;&lt;segmentation&gt;" Title="Conceptual question on image pattern representation" ViewCount="30" />
  
  <row Body="&lt;p&gt;This can be accomplished with hierarchical clustering using the &lt;code&gt;hclust&lt;/code&gt; function in R. Here's some example code. The &lt;code&gt;cutree&lt;/code&gt; function allows you to specify the distance (height) at which the tree should be cut and returns the cluster for each data point.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;numbers_to_cluster &amp;lt;- c(0:10, 20:30, 40:50)&#10;distances_between_numbers &amp;lt;- dist(x = numbers_to_cluster, method = &quot;euclidean&quot;)&#10;hierarchical_clustering &amp;lt;- hclust(d = distances_between_numbers, method = &quot;single&quot;)&#10;clusters &amp;lt;- cutree(tree = hierarchical_clustering, h = 1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-07-10T20:38:32.433" Id="107518" LastActivityDate="2014-07-10T20:38:32.433" OwnerUserId="4812" ParentId="107515" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am fitting a lot of elastic net models, simultaneously tuning $\lambda$ and $\alpha$. I am often coming to the following conclusions:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Cross-validation error is much more sensitive to changes in $\lambda$ than $\alpha$&lt;/li&gt;&#10;&lt;li&gt;In fact, the differences in cross-validation error are negligible between $L_1$, $L_2$, and $\alpha$ = 0.5&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Without looking at my data or knowing anything about it, even the family of GLM I'm using, are there any generalizable results about the conditions when this would be the case?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-07-10T22:16:09.180" Id="107532" LastActivityDate="2014-07-10T22:16:09.180" OwnerUserId="7616" PostTypeId="1" Score="0" Tags="&lt;cross-validation&gt;&lt;glmnet&gt;" Title="Are there any well-understood circumstances when we should expect cross-validation to be indifferent to $\alpha$ in elastic net tuning?" ViewCount="51" />
  
  
  <row Body="&lt;p&gt;PMFs are associated with discrete random variables, PDFs with continuous random variables.  For &lt;em&gt;any&lt;/em&gt; type of random of random variable, the CDF always exists (and is unique), defined as $$F_X(x) = P\{X \leq x\}.$$&#10;Now, depending on the support set of the random variable $X$, the density (or mass function) need not exist.  (Consider the &lt;a href=&quot;http://en.wikipedia.org/wiki/Cantor_function&quot; rel=&quot;nofollow&quot;&gt;Cantor Set and Cantor Function&lt;/a&gt;, the set is recursively defined by removing the center 1/3 of the unit interval, then repeating the procedure for the intervals (0, 1/3) and (2/3, 1), etc.  The function is defined as $C(x) = x$, if $x$ is in the Cantor set, and the greatest lower bound in the Cantor Set if $x$ is not a member.)  The Cantor Function is a perfectly good distribution function, if you tack on $C(x)= 0$ if $x &amp;lt; 0$ and $C(x) = 1$ if $1 &amp;lt; x$.  But this cdf has no density: $C(x)$ is continuous everywhere but its derivative is 0 almost everywhere.  No density with respect to any useful measure.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, the answer to your question is, &lt;em&gt;if&lt;/em&gt; a density or mass function exists, then it is a derivative of the CDF with respect to some measure.  In that sense, they carry the &quot;the same&quot; information.  BUT, PDFs and PMFs don't have to exist.  CDFs must exist.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-07-11T07:57:37.063" Id="107566" LastActivityDate="2014-07-12T13:41:44.213" LastEditDate="2014-07-12T13:41:44.213" LastEditorUserId="48148" OwnerUserId="48148" ParentId="107563" PostTypeId="2" Score="10" />
  <row AnswerCount="1" Body="&lt;p&gt;I adjust the partial least squares regression for one categorical factor (2 levels – &lt;code&gt;be&lt;/code&gt; or &lt;code&gt;nottobe&lt;/code&gt;) with with the &lt;code&gt;pls&lt;/code&gt; package in R. I try to use &lt;code&gt;round()&lt;/code&gt; function in the predict values for take the decision if the result are the first or second level in my factor. Does this approach sound correct?&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;require(pls) &#10;&#10;#Artificial data  &#10;&#10;T&amp;lt;-as.factor(sort(rep(c(&quot;be&quot;, &quot;nottobe&quot;), 100))) &#10;&#10;y1 &amp;lt;- c(rnorm(100,1,0.1),rnorm(100,1,0.1)) &#10;y2 &amp;lt;- c(rnorm(100,10,0.3),rnorm(100,10,0.6)) &#10;y3 &amp;lt;- c(rnorm(100,10,2.3),rnorm(100,11,2.6)) &#10;y4 &amp;lt;- c(rnorm(100,5,0.5),rnorm(100,7,0.5)) &#10;y5 &amp;lt;- c(rnorm(100,0,0.1),rnorm(100,0,0.1)) &#10;&#10;#Create the data frame &#10;avaliacao &amp;lt;- as.numeric(T) &#10;espectro &amp;lt;- cbind(y1,y2,y3,y4,y5) &#10;dados &amp;lt;- data.frame(avaliacao = I(as.matrix(avaliacao)), bands = I(as.matrix(espectro))) &#10;&#10;#PLS regression&#10;taumato &amp;lt;- plsr(avaliacao ~ bands, ncomp = 5, validation = &quot;LOO&quot;, data=dados) &#10;summary(taumato) &#10;&#10;#Components analysis &#10;plot(taumato, plottype = &quot;scores&quot;, comps = 1:5) &#10;&#10;&#10;#Cross validation &#10;taumato.cv &amp;lt;- crossval(taumato, segments = 10) &#10;plot(MSEP(taumato.cv), legendpos = &quot;topright&quot;) &#10;summary(taumato.cv, what = &quot;validation&quot;) &#10;plot(taumato, xlab =&quot;medição&quot;, ylab=&quot;predição&quot;, ncomp = 3, asp = 1, main=&quot; &quot;, line = TRUE) &#10;&#10;&#10;#Predition for 3 components &#10;T&amp;lt;-as.factor(sort(rep(c(&quot;be&quot;, &quot;nottobe&quot;), 50))) &#10;&#10;y1 &amp;lt;- c(rnorm(100,1,0.1),rnorm(100,1,0.1)) &#10;y2 &amp;lt;- c(rnorm(100,10,0.3),rnorm(100,10,0.6)) &#10;y3 &amp;lt;- c(rnorm(100,10,2.3),rnorm(100,11,2.6)) &#10;y4 &amp;lt;- c(rnorm(100,5,0.5),rnorm(100,7,0.5)) &#10;y5 &amp;lt;- c(rnorm(100,0,0.1),rnorm(100,0,0.1)) &#10;&#10;espectro2 &amp;lt;- cbind(y1,y2,y3,y4,y5) &#10;new.dados &amp;lt;- data.frame(bands = I(as.matrix(espectro2))) &#10;round(predict(taumato, ncomp = 3, newdata = new.dados))##&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-07-11T10:43:11.570" Id="107584" LastActivityDate="2014-12-25T13:48:47.027" LastEditDate="2014-07-11T11:05:27.577" LastEditorUserId="32036" OwnerUserId="51910" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;categorical-data&gt;&lt;multivariate-analysis&gt;&lt;pls&gt;" Title="Partial least squares regression for categorical factor in R" ViewCount="236" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm having problem understanding how big sample size is needed for a situation where there are pairs in the before/after groups but these have all multiple data points. &lt;/p&gt;&#10;&#10;&lt;h2&gt;Problem&lt;/h2&gt;&#10;&#10;&lt;p&gt;Pose a situation where you have X dart throwers. Each dart thrower have thrown $n_i$ darts for some average $m_i$ points with a &quot;bad&quot; dart. &#10;Now an expected better dart is introduced, so the dart throwers all start throwing with the new dart. How many throws are needed in order to be able to say that the new darts are better at say 5% significance with 80% power? (Note that this will be a function of the actual difference in average points.)&lt;/p&gt;&#10;&#10;&lt;h2&gt;Thoughts on solution&lt;/h2&gt;&#10;&#10;&lt;p&gt;I am aware of the &lt;code&gt;power.t.test&lt;/code&gt;, which can be used for to compare all the points before and after. However, as the dart throwing expertise could vary substantially amongst the throwers, it should probably be a paired test. But, when reading up on the &lt;code&gt;power.t.test(..., type = &quot;paired&quot;)&lt;/code&gt;, it tells you how many pairs (throwers) are needed (and presumably assumes only one data point with the bad and good dart). &lt;/p&gt;&#10;&#10;&lt;p&gt;So, I'm considering to do a &lt;code&gt;power.t.test&lt;/code&gt; for each of the authors but how would I assemble the results into one? Is there a paired power test which could answer how big sample is needed in the situation where you have pairs but also multiple data points for each pair?&lt;/p&gt;&#10;&#10;&lt;p&gt;Does the original distribution matter? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-11T15:23:49.150" Id="107615" LastActivityDate="2014-07-13T07:30:09.717" OwnerUserId="10437" PostTypeId="1" Score="0" Tags="&lt;sample-size&gt;&lt;power-analysis&gt;" Title="How many samples are needed for a paired trial with multiple tries?" ViewCount="20" />
  
  <row Body="&lt;p&gt;Actually, during CV you try to find the best parameters on a &lt;strong&gt;validation&lt;/strong&gt; set, which is distinct from the test set. You split your entire data into three sets : training set, validation set, and test set. When you do cross-validation properly, you never look at the test end until the end, so there is no snooping at all. Doing cross-validation on the test set is a serious (yet frequent) methodological error.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-07-11T16:16:43.320" Id="107624" LastActivityDate="2014-07-11T16:16:43.320" OwnerUserId="45348" ParentId="107617" PostTypeId="2" Score="0" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Thanks in advance for the help.&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose I have a set of data of the form [feature, rating].  For the sake of argument lets assume feature can be movie A or movie B and rating is a ranked, continuous value on the interval [0,100].  Suppose also that the data set is very large.  In reality I have several more features but I intend to isolate each pair of features so I believe this is a comparable example (still one rating though).&lt;/p&gt;&#10;&#10;&lt;p&gt;Under the naive assumption that having equal ratings implies features are the same, how might I be able to determine if feature A and B are different or the same ( I acknowledge it may not be possible to determine either).  Given that I know the standard deviation of how ratings are assigned (suppose I have a user or set of users that assign the scores and I know the std of the ratings they give), how might I go about this?&lt;/p&gt;&#10;&#10;&lt;p&gt;I apologize if I put the wrong tags on this question, I'm not sure what this should go under.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-11T18:52:37.723" Id="107647" LastActivityDate="2014-07-11T19:11:35.733" OwnerUserId="42003" PostTypeId="1" Score="1" Tags="&lt;distributions&gt;&lt;correlation&gt;&lt;confidence-interval&gt;&lt;equivalence&gt;" Title="How can I determine if two features, given a set of data, are the same feature or not?" ViewCount="40" />
  <row AnswerCount="0" Body="&lt;p&gt;My question is as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;Ratio=Ratios + log (numbers) +dummy variables + volatility&lt;/p&gt;&#10;&#10;&lt;p&gt;I have this type of regression in a paper published by the Federal Reserve Bank.Can someone tell me why we took the log of variables that represent numbers (such as size,and fees) whereas for the fractions such as (market value of equity/book value of equity) we didnd't take log. Also we didn't take it for the dummies .Finally the volatility which is a number was without log .&#10;Best,&#10;Wesso &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-07-11T19:38:54.303" Id="107652" LastActivityDate="2014-07-11T19:38:54.303" OwnerUserId="51943" PostTypeId="1" Score="0" Tags="&lt;data-transformation&gt;" Title="taking the log of (Only) some variables in a regression" ViewCount="26" />
  
  
  <row Body="&lt;p&gt;One common way to determine the relative contribution of each factor to a model is to remove the factor and compare the relative likelihood with something like a chi-squared test:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;pchisq(logLik(model1) - logLik(model2), 1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;As the way that likelihoods are calculated between functions may be slightly different, I typically will only compare them between the same method.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-12T03:15:30.320" Id="107677" LastActivityDate="2014-07-12T03:15:30.320" OwnerUserId="51954" ParentId="31118" PostTypeId="2" Score="1" />
  
  
  
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;I'm not even sure if my data follows a poisson distribution and am not sure how to find out.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Let's start there. You can't say &quot;my data are definitely Poisson&quot;. It's more a question of whether it's a reasonable model.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are two main approaches.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;is to investigate whether requirements that will yield a Poisson distribution for the data are met or likely to be met or that you are prepared to assume are met, or are sufficiently closely met in some sense.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;is to see whether this kind of data appear to be close enough to Poisson that inferences obtained by assuming it for your data would be 'close enough' for your purposes. (How close it needs to be depends on your needs, preferences and so on.)&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;In case 1, the obvious thing to consider is whether you can treat it like a Poisson process. You need: &lt;/p&gt;&#10;&#10;&lt;p&gt;(1) constant intensity of events within a single variable&lt;/p&gt;&#10;&#10;&lt;p&gt;(2) independence&lt;/p&gt;&#10;&#10;&lt;p&gt;(3) &quot;rare&quot; events (so that, for example, the chance of more than one event occurring in a very small interval of time is correspondingly small) -- they don't actually have to be rare overall, just rare in small intervals of time.&lt;/p&gt;&#10;&#10;&lt;p&gt;The second approach would seek to identify how non-Poisson data like yours is and so what specific forms of non-Poissonness you might have, and to consider the degree to which that might affect your inference. There are some suitable alternatives to consider (such as the negative binomial, which might be more suitable if the intensity varies from observation to observation).&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;totalled over a population of patients&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;This would be one potential source of 'non-constant rate' (heterogeneity of people) that might lead you to consider whether the variable is 'overdispersed' (tends to show more variation relative to the mean than you'd expect from a Poisson).&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;I just want to compare to see if the rates I'm getting are significantly different from each other. That is the whole goal.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Then it may be better to start with that goal. A Poisson may be appropriate, but that goal can be approached even if it isn't.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Am I even looking in the right direction by googling poisson distribution? &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;It's a pretty good place to start.&lt;/p&gt;&#10;&#10;&lt;p&gt;What's a typical expected count?&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;There are a number of approaches to comparing two Poisson counts. &lt;/p&gt;&#10;&#10;&lt;p&gt;Perhaps the most common is to condition on the total count and test whether the counts are in proportion to the ratio of the specific gene to all other genes. The conditioning converts the test to a binomial proportion.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are a number of other ways to approach it even with a simple Poisson comparison.&lt;/p&gt;&#10;&#10;&lt;p&gt;Some other issues:&lt;/p&gt;&#10;&#10;&lt;p&gt;If there are any other variables to account for, you might consider a GLM.&lt;/p&gt;&#10;&#10;&lt;p&gt;You might also consider whether patients should be treated as random effects.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you're not prepared to assume it's Poisson, you might consider a variety of other possibilities; perhaps a permutation test.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-12T07:56:29.127" Id="107694" LastActivityDate="2014-07-12T09:58:37.627" LastEditDate="2014-07-12T09:58:37.627" LastEditorUserId="805" OwnerUserId="805" ParentId="107634" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I cannot suppose age to be normal, since age can only be positive, while normal distributions are on the range $-\infty$ to $\infty$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Since 41.25 is only 3.43 standard deviations (12.02) from 0, you will get some negatives by supposing it to be actually normal.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you simply want some set of random numbers with mean 41.25 and standard deviation 12.02 that looks &lt;em&gt;vaguely&lt;/em&gt; normalish, you might consider generating a &lt;em&gt;gamma&lt;/em&gt; random variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's a histogram of a sample of 100000 ages from a gamma distribution with mean 41.25 and standard deviation 12.02:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/PzgIy.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The smallest age generated was about 7, while the largest was 110.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you want a strict upper limit as well as lower limit, you might consider using a beta distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;It looks like in SAS you could do it something like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x = beta*RAND('GAMMA',alpha)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Where the shape parameter &lt;code&gt;alpha&lt;/code&gt; can be set to $(\frac{\text{mean}}{\text{sd}})^2$ and the scale parameter &lt;code&gt;beta&lt;/code&gt; can be set to $\frac{\text{sd}^2}{\text{mean}}$. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-12T10:39:14.840" Id="107701" LastActivityDate="2014-07-12T10:39:14.840" OwnerUserId="805" ParentId="107679" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Why do you expect the oob error to be unbiased? &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;There's (at least) 1 training case less available for the trees used in the surrogate forest compared to the &quot;original&quot; forest. I'd expect this to lead to a small pessimistic bias roughly comparable to leave-one-out cross-validation. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;There are roughly $1 - \frac{1}{e} \approx \frac{2}{3}$ of the number of trees of the &quot;original&quot; forest in the surrogate forest that is actually evaluated with the left-out case. Thus, I'd expect higher variance in the prediction, which will cause further pessimistic bias. &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Both thoughts are closely related to the learning curve of the classifier and application/data in question: the first to the average performance as function of training sample size and the second to the variance around this average curve. &lt;/p&gt;&#10;&#10;&lt;p&gt;All in all, I'd expect you'll at most be able to show formally that oob is an unbiased estimator of the performance of random forests containing $1 - \frac{1}{e} \approx \frac{2}{3}$ of the number of trees of the &quot;original&quot; forest, and being trained on $n - 1$ cases of the original training data. &lt;/p&gt;&#10;&#10;&lt;p&gt;Note also that &lt;a href=&quot;http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#ooberr&quot; rel=&quot;nofollow&quot;&gt;Breiman&lt;/a&gt; uses &quot;unbiased&quot; for out-of-bootstrap the same way as he uses it for cross validation, where we also have a (small) pessimistic bias. &#10;Coming from an experimental field, I'm OK with saying that both are practically unbiased as the bias is usually much less of a problem than the variance (you're probably not using random forests if you have the luxury of having plenty of cases).&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-07-12T13:10:19.607" Id="107706" LastActivityDate="2014-07-12T13:17:10.787" LastEditDate="2014-07-12T13:17:10.787" LastEditorUserId="4598" OwnerUserId="4598" ParentId="105811" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="107727" AnswerCount="1" Body="&lt;p&gt;I need to understand the Rayleigh distribution for a homework assignment in computer networks. Unfortunately, I lack the background knowledge in the field of statistics and probability theory to understand the descriptions that are given elsewhere about the Rayleigh Distribution, as it leads me from one page to another and I get more confused.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you could use an example to illustrate the case it would be even more useful. &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-07-12T13:22:47.457" Id="107707" LastActivityDate="2014-07-13T13:19:14.240" LastEditDate="2014-07-12T14:04:10.790" LastEditorUserId="7290" OwnerUserId="32343" PostTypeId="1" Score="2" Tags="&lt;probability&gt;&lt;self-study&gt;&lt;basic-concepts&gt;&lt;intuition&gt;&lt;rayleigh&gt;" Title="Plain english explanation of the Rayleigh distribution?" ViewCount="158" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a cognitive architecture solving a set of tasks. Also I have data of human subjects solving the same set of tasks.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now I want to see whether I can find a relationship between the performance of the architecture and that of human subjects (e.g: if its hard for the architecture its also hard for human subjects). &lt;/p&gt;&#10;&#10;&lt;p&gt;I have 3 summary statistics describing the performance of the architecture, call them a,b,c. They all measure slightly different things. Think about: uncertainty, structure of the solution given etc.&#10;For the human data I only have access to one statistic (data taken from a published experiment ), which is related but not identical to a,b,c.&lt;/p&gt;&#10;&#10;&lt;p&gt;If I compute the correlation coefficient, I see that b shows a high correlation to the performance of human subjects. However a,b,c have correlation (~0.2-0.7) in themselves.&lt;/p&gt;&#10;&#10;&lt;p&gt;I read that one can compute the partial correlation coefficient which measures the relationship of two variables excluding the influence of the other. If I do this, c has the highest partial correlation to human data, although its correlation to the human data was the lowest&lt;/p&gt;&#10;&#10;&lt;p&gt;I have trouble interpreting these results, and I am not sure whether I should do my interpretation based on correlation or partial correlation and if the use of partial correlation makes sense in this context. Also note: this is purely explorative, I don't aim to do hypothesis testing, I merely search for relationships. &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-07-12T14:51:58.210" Id="107712" LastActivityDate="2014-07-12T14:51:58.210" OwnerUserId="51968" PostTypeId="1" Score="2" Tags="&lt;correlation&gt;&lt;partial&gt;" Title="Correlation vs Partial Correlation to explore relationship in data" ViewCount="55" />
  <row AnswerCount="0" Body="&lt;p&gt;Is this the correct formula for combining the standard deviations of 3 groups?&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;\sigma = \sqrt{\frac{n_1\sigma_1^2 + n_2 \sigma_2^2+ n_3 \sigma_3^2+ n_1(\mu_1-\mu)^2 +n_2(\mu_2-\mu)^2+n_3(\mu_3-\mu)^2}{n_1 + n_2 +n_3 }}&#10;$$&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-07-12T15:15:19.073" Id="107713" LastActivityDate="2014-07-12T15:15:19.073" OwnerUserId="51969" PostTypeId="1" Score="0" Tags="&lt;standard-deviation&gt;&lt;group-differences&gt;" Title="Combining Standard deviation 3 groups" ViewCount="17" />
  
  
  <row Body="&lt;p&gt;To address the part of your question related to R, the ets function from the &lt;a href=&quot;http://cran.r-project.org/web/packages/forecast/index.html&quot; rel=&quot;nofollow&quot;&gt;forecast&lt;/a&gt; package includes a lambda argument -- when true, a Box-Cox transformation is used that will keep the forecasts strictly positive. You may be able to use the same general approach in Java.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-07-12T21:38:35.263" Id="107735" LastActivityDate="2014-07-12T21:38:35.263" OwnerUserId="39702" ParentId="107467" PostTypeId="2" Score="4" />
  <row AnswerCount="0" Body="&lt;p&gt;The application here is interpreting the correlation coefficient $r$ in terms of $X$'s ability to predict $Y$ for extreme values of $X$. For example, if $r = .8$, then what is $p(Y &amp;lt; 0 | X &amp;lt; -2)$?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-07-12T22:08:08.897" Id="107740" LastActivityDate="2014-07-12T22:08:08.897" OwnerUserId="14076" PostTypeId="1" Score="0" Tags="&lt;correlation&gt;&lt;normal-distribution&gt;&lt;bivariate&gt;" Title="If $X, Y$ are jointly standard normal with correlation $r$, and $a, b$ are constants, what is $p(Y &lt; b | X &lt; a)$?" ViewCount="61" />
  
  <row Body="&lt;p&gt;There's no need to combine cells or try some continuity correction, you can compute the likelihood - and hence the statistic - quite readily.&lt;/p&gt;&#10;&#10;&lt;p&gt;$G = 2\sum_{i} {O_{i} \cdot \ln\left(\frac{O_i}{E_i}\right)}$&lt;/p&gt;&#10;&#10;&lt;p&gt;Since $\lim_{x\to 0}x \ln x=0\,$, each cell with a 0 count contributes $0 \log (0/E_i)=0$  to the sum in the formula for $G$. &lt;/p&gt;&#10;&#10;&lt;p&gt;So you can just skip the zero cells, or equivalently you can replace their contribution with 0.&lt;/p&gt;&#10;&#10;&lt;p&gt;See also Wikipedia &lt;a href=&quot;http://en.wikipedia.org/wiki/G-test&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;, where it says of this test:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;the sum is taken over all non-empty cells.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Consequently the R code that just omits the &lt;code&gt;NA&lt;/code&gt;s is correct.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;On the other hand, you can very often compare nested models via chi-squared, since the chi-square will generally partition into components that allow for testing the change - you just get another chi-square test.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-13T05:29:27.870" Id="107765" LastActivityDate="2014-07-13T07:07:48.983" LastEditDate="2014-07-13T07:07:48.983" LastEditorUserId="805" OwnerUserId="805" ParentId="107718" PostTypeId="2" Score="3" />
  
  <row AcceptedAnswerId="107771" AnswerCount="1" Body="&lt;p&gt;What is the difference between multi-label classification and multiclass classfication.  Speficially, what is the difference between a label and a class?&#10;Please provide a clear example.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;&quot;Multiclass classification should not be confused with multi-label classification, where multiple labels are to be predicted for each instance.&quot; -wikipedia&lt;/em&gt;  ... was not very helpful.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-13T06:02:26.930" FavoriteCount="1" Id="107768" LastActivityDate="2014-07-13T14:51:27.193" LastEditDate="2014-07-13T14:51:27.193" LastEditorUserId="43890" OwnerUserId="51992" PostTypeId="1" Score="2" Tags="&lt;machine-learning&gt;&lt;classification&gt;&lt;multi-class&gt;&lt;multilabel&gt;" Title="What is the difference between a multi-label and a multi-class classification?" ViewCount="196" />
  <row AnswerCount="0" Body="&lt;p&gt;When I am reading Murphy's Machine Learning A Probabilistic Perspective. In chapter 3.2. I have some doubt. &lt;/p&gt;&#10;&#10;&lt;p&gt;I think the author want to express is two things. First, we can use Bayes formula to computer posterior, combine likelihood and prior. Second, if data is big enough, we can compute it directly. $1$ or $0$ depend on the hypothesis which maximum itself($p(h|\mathcal{D})$) is exist or not. I can't see these conclusion clearly. And I feel indigestion. the below is some detail I can't go through.   &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;li&gt; Likelihood. He give an formula:$$p(\mathcal{D}|h)=\left[\frac{1}{\text{size}(h)}\right]^N=\left[\frac{1}{|h|}\right]^N$$&lt;/p&gt;&#10;&#10;&lt;p&gt;And the author say this is the probability of independently sampling $N$ items(with replacement) from $h$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Yes, if the probability mass function is uniform. this is equal to $\left[\frac{1}{|h|}\right]^N$. but I think the the notation $p(\mathcal{D}|h)$ is not reasonable. It represents the probability of event $\mathcal{D}$ when $h$ is happen. under the assumption &lt;strong&gt;pmf&lt;/strong&gt; is uniform. it must equal to $$\frac{\text{size}(\mathcal{D})}{\text{size}(h)}$$. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;li&gt; Posterior , the formula is $$p(h|\mathcal{D})=\frac{p(\mathcal{D}|h)p(h)}{p(\mathcal{D})}$$ It is exactly Bayes formula. Then ,the author say if we have enough data, the prior is not important. the likelihood almost determine everything.then he give a formula: $$p(h|\mathcal{D})\rightarrow \delta_{\hat{h}^{MAP}}(h)$$ where $\hat{h}^{MAP} = \max_{h}p(h|\mathcal{D})$ and $\delta$ is Dirac measure. so does it mean if exist hypothesis $\hat{h}$ in $\{h\}$ maximum $p(h|\mathcal{D})$. then $p(h|\mathcal{D}) \rightarrow 1$?&lt;/p&gt;&#10;&#10;&lt;p&gt;To find $\hat{h}$ maximum $p(h|\mathcal{D})$ is equal to maximum $\log p(\mathcal{D}|h) + \log h$. Then the author say if we have enough data, only to find $\hat{h}$ maximum $\log p(\mathcal{D}|h)$ is enough. Since the likelihood term depends exponentially on $N$, and the prior stays constant.  But I don't think so. Intuitively, If we have a lot of data. the prior is not important. But for this formula, both $p(\mathcal{D}|h)$ and $p(h)$ are probability. they $\in[0,1]$. Although $\mathcal{D}$ is big. for different hypothesis $h$, $p(\mathcal{D}|h)$ is different. but it $\in [0,1]$ the same as $p(h)$. So I think we can't ignore $p(h)$. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-07-13T06:56:18.513" Id="107769" LastActivityDate="2014-07-13T06:56:18.513" OwnerUserId="46504" PostTypeId="1" Score="2" Tags="&lt;machine-learning&gt;&lt;conditional-probability&gt;&lt;bayes&gt;" Title="Some doubt in reading Machine Learning A Probabilistic Perspective ( chapter 3.2 )" ViewCount="91" />
  
  
  
  
  
  <row Body="&lt;p&gt;The most natural representation would be three heat maps with the same coloring (&lt;em&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Small_multiple&quot; rel=&quot;nofollow&quot;&gt;small multiples&lt;/a&gt;&lt;/em&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/9T70l.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;If the distance scales are too different for a single color scale, you can either use different colors for each or standardize the measures to put them on the same scale.&lt;/p&gt;&#10;&#10;&lt;p&gt;If there is some consistent ranking of pairs, then overlaid line plots or dot plots may help show the pattern. However, it's no good in these examples with random data.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/YgWvI.png&quot; alt=&quot;enter image description here&quot;&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/BNhTx.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-13T14:45:15.990" Id="107791" LastActivityDate="2014-07-13T16:21:20.120" LastEditDate="2014-07-13T16:21:20.120" LastEditorUserId="1191" OwnerUserId="1191" ParentId="107773" PostTypeId="2" Score="7" />
  <row Body="&lt;p&gt;It depends on what you are trying to accomplish.  What distribution do you want the imputation to reflect? $$\mathrm{N}(10, 25)$$ or $$\mathrm{N}(10,\frac{25}{\sqrt{1000}})?$$ &lt;/p&gt;&#10;&#10;&lt;p&gt;Your second task could be accomplished in $0.1\%$ of the time by implementing my second suggestion.&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to emphasize that I am &lt;strong&gt;not&lt;/strong&gt; endorsing &lt;em&gt;either&lt;/em&gt; of these approaches for your intended application (imputing missing values).  My point is that sampling $n$ i.i.d $\mathrm{N}(\mu, \sigma^2)$ and computing $\bar{X}_n$ is exactly equivalent (and indistinguishable) from drawing a single value from $\mathrm{N}(\mu, \frac{\sigma^2}{n})$, provided you discard the observations in the first case.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-13T16:53:59.100" Id="107797" LastActivityDate="2014-07-13T22:59:17.927" LastEditDate="2014-07-13T22:59:17.927" LastEditorUserId="48148" OwnerUserId="48148" ParentId="107795" PostTypeId="2" Score="2" />
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;(i) Is it true to say that if the sample frame is not a random&#10;  selection (such as a convenience sample), then the sample frame is not&#10;  representative of the true population?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;In short, yes. Elaborating: &quot;representative&quot; doesn't really have a clear definition - Kruskall &amp;amp; Mosteller tracked 9(!) distinct meanings of it in a series of papers in 1980. I think the consensus in modern statistics is that &quot;representative&quot; means that every item was sampled with known probability. So a convenience sample is not representative &lt;em&gt;by definition.&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;(ii) We can always do a G*Power analysis to determine the sample size.&#10;  But how do we determine if the sample frame represents the true&#10;  population or not?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;There are a number of ways to approach this problem. Bethlehem, Schouten &amp;amp; Cobben (in the &lt;a href=&quot;http://www.risq-project.eu/&quot; rel=&quot;nofollow&quot;&gt;RISQ project&lt;/a&gt;) suggest one such indicator. &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;(iii) How is sample size important in order to avoid making Type I or Type II errors?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I'll refer you to whuber's excellent reply in this &lt;a href=&quot;http://stats.stackexchange.com/questions/9653/can-a-small-sample-size-cause-type-1-error&quot;&gt;thread&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-13T18:31:05.867" Id="107806" LastActivityDate="2014-07-13T18:31:05.867" OwnerUserId="11878" ParentId="107803" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;The &lt;code&gt;positive=TRUE&lt;/code&gt; argument for &lt;code&gt;forecast.gts&lt;/code&gt; and &lt;code&gt;forecast.hts&lt;/code&gt; ensures the starting forecasts to be positive, but not the final reconciled forecasts. Even when the starting forecasts are positive, it is possible for the reconciled forecasts to be negative. When you use &lt;code&gt;combinef&lt;/code&gt;, you provide your own starting forecasts, so it is up to you to make them positive.&lt;/p&gt;&#10;&#10;&lt;p&gt;It would be possible to use a non-linear least squares reconciliation procedure to produce positively constrained reconciled forecasts, but that would be much slower.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-07-14T00:41:38.723" Id="107822" LastActivityDate="2014-07-14T00:41:38.723" OwnerUserId="159" ParentId="107730" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;The most common way to determine whether an AR or MA is appropriate is by looking at the autocorrelation functions of the data. My answer in &lt;a href=&quot;http://stats.stackexchange.com/questions/106038/estimate-arma-coefficients-through-acf-and-pacf-inspection/106044#106044&quot;&gt;this post&lt;/a&gt; describes the strategy to do so. It is basically based on the following features of AR and MA processes: 1) the ACF of a stationary AR process of order p goes to zero at an exponential rate, while the PACF becomes zero after lag p. 2) For an MA process of order q the theoretical ACF and PACF exhibit the reverse behaviour (the ACF truncates after lag q and the PACF goes to zero relatively quickly).&lt;/p&gt;&#10;&#10;&lt;p&gt;For processes that contain both AR and MA parts, the autocorrelation functions may not be clear. One way to proceed is to fit first an AR or MA model of low order and look at the autocorrelation functions of the residuals to see if there is some AR or MA structure that should be added to the initial model.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-07-14T07:19:53.563" Id="107838" LastActivityDate="2014-07-14T07:19:53.563" OwnerUserId="48766" ParentId="107834" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to calculate the power that I will have to detect an interaction between 2 different interventions in a 2X2 factorial design. We aim to have about 850 participants in each of the four boxes, or roughly 1700 per arm for a total of 3400. The study outcome is prevalence at 2 years, with expected prevalence of 2-8% depending on the arm. I was wondering if someone had a good equation to calculate what power we would have to detect an interaction, and what size of interaction? I have not had any luck with an internet search, although have been playing around with factorialsim in stata 12.0, using my proportions (or prevalence) as means, which feels wrong. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-07-14T08:31:51.600" Id="107841" LastActivityDate="2014-07-14T08:31:51.600" OwnerUserId="52034" PostTypeId="1" Score="0" Tags="&lt;interaction&gt;&lt;power-analysis&gt;" Title="Power estimates for an interaction in a 2X2 factorial design with a proportion outcome" ViewCount="65" />
  <row AnswerCount="1" Body="&lt;p&gt;I have a problem where I want to use a classifier for it. So I defined a set of features and created a dataset. Now I want to generate some plots to understand the features. I came across the Scatterplot Matrix or the Correlation Matrix, but I'm so confused about if they are the same thing or not.&lt;/p&gt;&#10;&#10;&lt;p&gt;1 - &lt;a href=&quot;http://glowingpython.blogspot.de/2012/10/visualizing-correlation-matrices.html&quot; rel=&quot;nofollow&quot;&gt;Visualizing correlation matrices&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;2- &lt;a href=&quot;http://www.r-bloggers.com/scatterplot-matrices/&quot; rel=&quot;nofollow&quot;&gt;Scatterplot Matrices&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;So my questions:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;Are they the same thing? &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Does plotting both make sense? i.e. each plot convey a different message?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="1" CreationDate="2014-07-14T09:23:03.987" Id="107843" LastActivityDate="2014-07-14T10:15:37.013" LastEditDate="2014-07-14T09:41:52.867" LastEditorUserId="27779" OwnerUserId="27779" PostTypeId="1" Score="1" Tags="&lt;classification&gt;&lt;data-visualization&gt;&lt;data-mining&gt;&lt;dataset&gt;" Title="Plotting Scatterplot Matrix or Correlation matrix or both?" ViewCount="36" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Consider the constraint optimization $\text{argmin}_{\beta}(f(\beta)+\lambda g(\beta))$ can someone define $\beta(\lambda)$. That is, what is the relationship between $\lambda$ and $\beta$?.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-07-14T10:46:59.100" Id="107852" LastActivityDate="2014-07-14T11:44:38.630" LastEditDate="2014-07-14T11:11:49.260" LastEditorUserId="31261" OwnerUserId="31261" PostTypeId="1" Score="0" Tags="&lt;optimization&gt;&lt;constrained-regression&gt;" Title="Constraint optimisation" ViewCount="34" />
  <row AnswerCount="1" Body="&lt;p&gt;I want to compare the frequency of retweets between 2 groups of unequal size. I've got the total tweet count and the retweet count for each user. I was hoping the R Cookbook webpage would help but the tests presented there seem to be more suited for repeated measures and equal sample sizes for the two groups: &lt;a href=&quot;http://www.cookbook-r.com/Statistical_analysis/Frequency_tests/&quot; rel=&quot;nofollow&quot;&gt;http://www.cookbook-r.com/Statistical_analysis/Frequency_tests/&lt;/a&gt;  I will be grateful for advice on what test to use here. My dataset looks a bit like this:    &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;userid tweetCount ReTweetcount Group&#10;1      45        3             A&#10;2      100       25            A&#10;3      23        0             B&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2014-07-14T10:47:36.817" Id="107853" LastActivityDate="2014-07-14T12:55:24.580" LastEditDate="2014-07-14T12:55:24.580" LastEditorUserId="22311" OwnerUserId="52040" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;frequency&gt;" Title="Difference in frequencies in two groups of unequal size" ViewCount="62" />
  <row Body="&lt;p&gt;Using PCA for feature selection (removing non-predictive features) is an extremely expensive way to do it.  PCA algos are often O(n^3).  Rather a much better and more efficient approach would be to use a measure of inter-dependence between the feature and the class - for this Mutual Information tends to perform very well, furthermore it's the only measure of dependence that a) fully generalizes and b) actually has a good philosophical foundation based on Kullback-Leibler divergence.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, we compute (using maximum likelihood probability approx with some smoothing)&lt;/p&gt;&#10;&#10;&lt;p&gt;MI-above-expected = MI(F, C) - E_{X, N}[MI(X, C)]&lt;/p&gt;&#10;&#10;&lt;p&gt;where the second term is the 'expected mutual information given N examples'.  We then take the top M features after sorting by MI-above-expected.&lt;/p&gt;&#10;&#10;&lt;p&gt;The reason why one would want to use PCA is if one expects that many of the features are in fact dependent.  This would be particularly handy for Naive Bayes where independence is assumed.  Now the datasets I've worked with have always been far too large to use PCA, so I don't use PCA and we have to use more sophisticated methods.  But if your dataset is small, and you don't have the time to investigate more sophisticated methods, then by all means go ahead and apply an out-of-box PCA.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-14T12:16:16.727" Id="107862" LastActivityDate="2014-07-14T12:16:16.727" OwnerUserId="36915" ParentId="107845" PostTypeId="2" Score="2" />
  
  <row AnswerCount="3" Body="&lt;p&gt;I am doing data analysis in the mobile ad targeting domain. I have around &lt;strong&gt;18 features&lt;/strong&gt; and  for a combination of these features, the result is either True or False &lt;strong&gt;(1/0)&lt;/strong&gt; depending on whether the impression was clicked or not. The problem here is that the &lt;strong&gt;output class is highly skewed&lt;/strong&gt;. Click though rate is around &lt;strong&gt;0.4%&lt;/strong&gt;. (i.e value is 1  only 4 out 1000 times). I have a data set of &lt;strong&gt;2 million rows&lt;/strong&gt; and I am using 90% as train set and 10% as test set. I have used &lt;strong&gt;logistic regression&lt;/strong&gt; from &lt;strong&gt;sckit-learn&lt;/strong&gt; package in &lt;strong&gt;python&lt;/strong&gt;. Now after training my model I get all values for test set as 0. Please tell me what the problem could be and what should I do to solve it? &lt;/p&gt;&#10;&#10;&lt;p&gt;P.S. : I have tried increasing my data set size and also reducing the number of features(even to just one feature). If I see the probability of each class(0/1) in the test set, I get around 0.002 - 0.005 for a 1.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-14T13:09:14.653" FavoriteCount="1" Id="107874" LastActivityDate="2015-03-01T12:26:40.820" OwnerUserId="52043" PostTypeId="1" Score="0" Tags="&lt;logistic&gt;&lt;binary-data&gt;&lt;skewness&gt;&lt;unbalanced-classes&gt;&lt;scikit-learn&gt;" Title="How to deal with a skewed class in binary classification having many features?" ViewCount="177" />
  
  <row AcceptedAnswerId="107924" AnswerCount="1" Body="&lt;p&gt;Given the following ANCOVA model:&lt;/p&gt;&#10;&#10;&lt;p&gt;$Y_{ij}=\mu+\alpha_i+\beta X_{ij}+\epsilon_{ij}$,&lt;/p&gt;&#10;&#10;&lt;p&gt;$e_{ij}\sim N(0,\sigma^2)$ i.i.d., &lt;/p&gt;&#10;&#10;&lt;p&gt;$\alpha_1=0$, &lt;/p&gt;&#10;&#10;&lt;p&gt;$i=1,..., m$,&lt;/p&gt;&#10;&#10;&lt;p&gt;$j=1,..., n$&lt;/p&gt;&#10;&#10;&lt;p&gt;Given I have already computed&lt;/p&gt;&#10;&#10;&lt;p&gt;$SC_{W}=\sum_i\sum_j(y_{ij}-\bar{y}_{i.})(x_{ij}-\bar{x}_{i.})$&lt;/p&gt;&#10;&#10;&lt;p&gt;and &lt;/p&gt;&#10;&#10;&lt;p&gt;$SS_W(X)=\sum_i\sum_j(x_{ij}-\bar{x}_{i.})^2$&lt;/p&gt;&#10;&#10;&lt;p&gt;how can the coefficient of determination $R^2$ in this model be computed and what is the formula for doing so?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-14T13:21:25.193" Id="107878" LastActivityDate="2014-07-21T14:07:02.460" LastEditDate="2014-07-14T18:25:55.230" LastEditorUserId="43750" OwnerUserId="43750" PostTypeId="1" Score="0" Tags="&lt;ancova&gt;&lt;r-squared&gt;" Title="How is the coefficient of determination in ANCOVA computed?" ViewCount="30" />
  
  <row AnswerCount="0" Body="&lt;p&gt;The two most popular types of loss functions are&lt;/p&gt;&#10;&#10;&lt;p&gt;1) squared error: $L(y,f(x))=(y-f(x))^2$ --&gt; best estimate is the $E(Y|x) $&lt;br&gt;&#10;2) absolute error: $L(y,f(x))=|y-f(x)|$ --&gt; best estimate is the $median(Y|x)$&lt;/p&gt;&#10;&#10;&lt;p&gt;I have two questions.&lt;/p&gt;&#10;&#10;&lt;p&gt;1) Why do people use the squared error method? The absolute error method makes much more intuitive sense. You get the difference between the actual and the estimate. Plain and simple. If you square the difference, then won't you get &quot;warped&quot; values depending on the size of the difference?&lt;/p&gt;&#10;&#10;&lt;p&gt;2) This also got me thinking about what is &quot;expected value.&quot; Expected value is defined as the mean. However, the best estimate under the absolute error loss function is the median. So is the &quot;expected value&quot; the median? I would very much appreciate it if someone can help me clarify my thinking. Thanks.&lt;/p&gt;&#10;" ClosedDate="2014-07-14T17:59:19.200" CommentCount="4" CreationDate="2014-07-14T14:17:13.893" FavoriteCount="1" Id="107883" LastActivityDate="2014-07-14T14:17:13.893" OwnerUserId="20866" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;loss-functions&gt;" Title="Squared Error vs Absolute Error loss functions" ViewCount="40" />
  <row AnswerCount="1" Body="&lt;p&gt;I have conducted Pearson's correlation in R and need help interpreting the results.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Pearson's product-moment correlation&#10;&#10;data:  A.C$Average.tortuosity and A.C$Area&#10;t = -0.6168, df = 14, p-value = 0.5473&#10;alternative hypothesis: true correlation is not equal to 0&#10;95 percent confidence interval:&#10; -0.6092369  0.3622606&#10;sample estimates:&#10;       cor &#10;-0.1626531&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I am interested in reporting the &lt;em&gt;r&lt;/em&gt; and &lt;em&gt;p&lt;/em&gt; values. Would it be fair to say based on the the above output that:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;There was a non-significant correlation between the two variables, &lt;em&gt;r&lt;/em&gt; (14) = -.16, &lt;em&gt;p&lt;/em&gt; &gt; .05.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="1" CreationDate="2014-07-14T14:27:03.127" Id="107885" LastActivityDate="2014-07-14T15:31:33.690" LastEditDate="2014-07-14T15:31:33.690" LastEditorUserId="7290" OwnerUserId="37190" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;correlation&gt;&lt;interpretation&gt;" Title="Interpret Pearsons correlation output from R" ViewCount="121" />
  
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I was sure that these are the same things but I do not get the difference reading about &lt;a href=&quot;http://en.wikipedia.org/wiki/Probability_mass_function#Formal_definition&quot; rel=&quot;nofollow&quot;&gt;mass probability function&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Suppose that $X: S → A (A \subseteq R)$ is a discrete random variable&#10;  defined on a sample space S. Then the probability mass function $f_X: A → [0, 1]$ for X is defined as&lt;/p&gt;&#10;  &#10;  &lt;p&gt;$$f_X(x) = \Pr(X = x) = \Pr(\{s \in S: X(s) = x\})$$&lt;/p&gt;&#10;  &#10;  &lt;p&gt;Thinking of probability as mass helps to avoid mistakes since the&#10;  physical mass is conserved as is the total probability for all&#10;  hypothetical outcomes ''x'': $\sum_{x\in A} f_X(x) = 1$&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Well, I just do not get &lt;a href=&quot;http://www.mathgoodies.com/lessons/vol6/sample_spaces.html&quot; rel=&quot;nofollow&quot;&gt;this&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The sample space of an experiment is the set of all&#10;  possible outcomes of that experiment&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Sample set, the set of samples is the same as set of outcomes, so we can also call it outcome set. I just do not get why to denote it by two different letters A and S above if these are the same things and why does the function maps the samples to outcomes if it is the same thing?&lt;/p&gt;&#10;&#10;&lt;p&gt;Is outcome a value of the sample? What is the value of the sample then? The functions map values to values. I need a value (of sample) so that function (aka random variable) could produce the value of outcome. &lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit1&lt;/strong&gt; I would like to &lt;a href=&quot;http://stats.stackexchange.com/a/107936/26140&quot;&gt;thank&lt;/a&gt; who started to elaborate my question. It is not answered, however. I cannot accept it unless the following is resolved.&lt;/p&gt;&#10;&#10;&lt;p&gt;I did not get how events/observations are different from samples/outcomes. Also, I see that you identify samples with &lt;em&gt;any type&lt;/em&gt; whereas outcomes are values (you wanted to say &lt;em&gt;num types&lt;/em&gt; instead since &lt;em&gt;any type&lt;/em&gt; also has values and variables). We, programmers, write functions in terms of arguments/variables. For readability and reliability, we constrain every variable to some type. The values/arguments are placeholders that take concrete values during realization at runtime. The types simply constrain the domain of the variables (types are the domains/set of available values for that variable/argument). I think that it is sorta the same in math, you just skip the name of variable when define function like $f: Int \rightarrow Real$. So, by 'anything', you mean 'value of any, not necessarily numerical, type'.&lt;/p&gt;&#10;&#10;&lt;p&gt;It remains unclear however what is the point of introducing the S domain (aka separating samples and outcomes). You take heads/tails and convert it into a number. Why not to sample real numbers right away (identify S with A, why heads/tails, how introducing S improves our understanding of mass probability function) or why not to make the conversion chain even longer introducing $S_{interm}$ for instance, so that you could sample and use a series of random variables to turn the sample into A, e.g. $S \xrightarrow {randomvar_1} S_{interm} \xrightarrow {randomvar_2} A$? Why not to say that I have 3 pennies and 1 dime and sample A={1,10} right away? Why play around with heads/tails instead?&lt;/p&gt;&#10;&#10;&lt;p&gt;It is also not clear why 'random variables' appear in the S -&gt; A stage rather than in the sampling, to obtain values of S? Does it mean that generation of heads/tails is deterministic whereas mapping them to real domain, {1,10}, is random? &lt;/p&gt;&#10;&#10;&lt;p&gt;Back to the &lt;code&gt;sample=outcome&lt;/code&gt;. I see that Wikipedia says&#10;similarly&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;A random variable is a real-valued function defined on a set of&#10;  possible outcomes, the sample space Ω.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;So, how can it be that random variable maps samples to outcomes if the outcomes are the domain rather than range of random variables? I think that all this confusion deserves clarification.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-07-14T17:27:06.460" FavoriteCount="1" Id="107912" LastActivityDate="2014-07-15T15:11:26.523" LastEditDate="2014-07-15T15:11:26.523" LastEditorUserId="26140" OwnerUserId="26140" PostTypeId="1" Score="1" Tags="&lt;sample&gt;&lt;terminology&gt;&lt;basic-concepts&gt;&lt;definition&gt;" Title="What is the difference between sample and outcome? (plus events and observations)" ViewCount="117" />
  
  
  <row Body="&lt;p&gt;Perhaps you can jigger with your data structure a little bit like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;You say that each assay is produced twice on each tissue type. If both samples receive precisely the same assay, then it should not matter to you to arbitrarily call, say, one assay on the tissue &quot;group A&quot; and the &lt;em&gt;other&lt;/em&gt; assay on the same tissue &quot;group B&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, if you code your outcome/success=1, and no outcome/failure = 0, your data could be thought of as looking like:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Tissue  Outcome A  Outcome B&#10;&#10;     1       0         0&#10;     2       1         1&#10;     3       1         1&#10;     .       .         .&#10;     .       .         .&#10;     .       .         .&#10;    49       0         1&#10;    50       0         0&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;These look like &lt;em&gt;paired&lt;/em&gt; data (because one observation of success/no success in a tissue is &lt;em&gt;paired&lt;/em&gt; with a second observation of success/no success &lt;em&gt;in the same tissue type&lt;/em&gt;), and when someone says &quot;I wanna examine the correlation between paired data with binomial outcomes&quot; I head straight over to &lt;a href=&quot;https://en.wikipedia.org/wiki/McNemar%27s_test&quot; rel=&quot;nofollow&quot;&gt;McNemar's test&lt;/a&gt;. This require rejiggering your data, because what you are analyzing are &lt;em&gt;pairs&lt;/em&gt;, and there are four kinds of pairs, and you wanna set up the counts of each (&lt;strong&gt;totally&lt;/strong&gt; made up numbers, yo):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;   Group A    Group B  Kind of Pair  Count of Pairs&#10;no success  no success   concordant        15&#10;   success  no success   discordant        11&#10;no success     success   discordant        19&#10;   success     success   concordant        5&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;McNemar's test is a $\chi^{2}$ equivalent to the sign test. The test statistic only examines &lt;em&gt;discordant pairs&lt;/em&gt;. Let's call the count of Group A successful, but Group B not successful $r$, and then let's call the count of Group A not successful, but Group B successful $s$ (which discordant pair you call which is arbitrary). The test statistic (which is including a &lt;a href=&quot;https://en.wikipedia.org/wiki/Yates%27s_correction_for_continuity&quot; rel=&quot;nofollow&quot;&gt;Yates continuity correction&lt;/a&gt; in that &quot;$-1$&quot; business) is:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\chi^{2}_{\text{df=1}} = \frac{\left(\left|r-s\right|-1\right)^{2}}{r+s}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\chi^{2}_{\text{df=1}} = \frac{\left(\left|11-19\right|-1\right)^{2}}{11+19} = \frac{49}{30} = 1.6\bar{3}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;These made up data give a $p$-value of about 0.20, so unless you are being &lt;strong&gt;outrageously libertine&lt;/strong&gt; with your Type I error preferences, you would not reject the null hypothesis. As it so happens, the null hypothesis you are testing is that there is no association between group membership and probability of success.&lt;/p&gt;&#10;&#10;&lt;p&gt;R can perform McNemar's test using the, ahem, &lt;strong&gt;mcnemar.test()&lt;/strong&gt; function&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-07-14T21:43:44.200" Id="107942" LastActivityDate="2014-07-14T22:45:04.853" LastEditDate="2014-07-14T22:45:04.853" LastEditorUserId="44269" OwnerUserId="44269" ParentId="107925" PostTypeId="2" Score="1" />
  <row AnswerCount="2" Body="&lt;p&gt;Why does SAS random and repeated both produce the same result? &lt;/p&gt;&#10;&#10;&lt;p&gt;Can someone explain this in detail?&lt;/p&gt;&#10;&#10;&lt;p&gt;For example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;proc mixed data=test;&#10;class variable1 ..... variableN;&#10;model outcome=variable1+...+variableN;&#10;random intercept/ subject=cluster type=cs;&#10;run;&#10;&#10;proc mixed data=test;&#10;class variable1 ..... variableN;&#10;model outcome=variable1+...+variableN;&#10;repeated/ subject=cluster type=cs;&#10;run;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Why do both program produce the same result?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-07-14T23:58:02.147" FavoriteCount="2" Id="107953" LastActivityDate="2015-02-03T07:45:15.110" LastEditDate="2014-07-15T12:34:55.433" LastEditorUserId="8413" OwnerUserId="52024" PostTypeId="1" Score="2" Tags="&lt;sas&gt;&lt;mixed-effect&gt;" Title="Why are 'random' and 'repeated' in mixed models in SAS both producing the same result?" ViewCount="83" />
  
  <row Body="&lt;p&gt;The coefficient of variation is the standard deviation divided by the mean.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;But Cv also deals with dispersion, can we use it to understand the degree of HOV?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;If by dispersion, you mean 'standard deviation' or 'variance', then not unless you fix the mean at a constant: then changes is coefficient of variation would reflect changes in spread (and so changes in variance). &lt;/p&gt;&#10;&#10;&lt;p&gt;Consider:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;You can have constant coefficient of variation while variance changes. &lt;/p&gt;&#10;&#10;&lt;p&gt;Consider a family of random variables with &lt;a href=&quot;http://en.wikipedia.org/wiki/Gamma_distribution&quot; rel=&quot;nofollow&quot;&gt;gamma distributions&lt;/a&gt; with fixed shape parameter, $\alpha=\alpha_0$ but varying scale parameter, $\beta$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Since the coefficient of variation is only a function of $\alpha$, it will be constant, while the variance ($=\alpha\beta$) changes as $\beta$ changes.  &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/thUK0.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;You can even have constant variance while coefficient of variation changes. &lt;/p&gt;&#10;&#10;&lt;p&gt;Consider another gamma family, this time with $\alpha$ varying and with $\beta=k/\alpha$ for some constant $k$. Then the variance of every member of the family is $k$, while the coefficient of variation is different for each different $\alpha$.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/wGHpL.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;So looking at (say) Levene's test won't directly tell you about coefficient of variation and differences in coefficient of variation won't tell you directly about variance. In either case, you can only tell by bringing the mean (or something that determines the mean given the other information) into it.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;For example, if&lt;/p&gt;&#10;  &#10;  &lt;p&gt;$C_{v_1} &amp;gt; C_{v_2}$&lt;/p&gt;&#10;  &#10;  &lt;p&gt;where $C_{v_1}$ is the coefficient of variation for sample 1 and $C_{v_2}$ is the coefficient of variation for sample 2, can we say that the assume that samples 1 and 2 are differently dispersed?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Again, it depends on what you mean by 'dispersion'. As indicated in case 2. above, they might have identical spread and still differ in coefficient of variation, so if you mean 'different standard deviation' (or different variance), then no. &lt;/p&gt;&#10;&#10;&lt;p&gt;If instead you &lt;em&gt;define&lt;/em&gt; 'dispersion' in terms of CV, then clearly you can say that, yes.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-07-15T04:56:39.697" Id="107967" LastActivityDate="2014-07-15T04:56:39.697" OwnerUserId="805" ParentId="107756" PostTypeId="2" Score="2" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Here is my problem:&lt;/p&gt;&#10;&#10;&lt;p&gt;I have 3 variables $X,Y,Z$ : &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;$X$ is the number of clicks we observed on an web advertisement; &lt;/li&gt;&#10;&lt;li&gt;$Y$ is the number of time a customer do a sign-up on the website after clicking on that advertisement; &lt;/li&gt;&#10;&lt;li&gt;$Z$ is the &quot;conversion rate&quot;, i.e. the number of sign-up / number of clicks on that advertisement.   &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;$Z$ is an unobserved variable and $X,Y$ are the two which I can observe. &lt;/p&gt;&#10;&#10;&lt;p&gt;So for each ad $i$ there is a latent variable $Z_i$ that represents the &quot;conversion rate&quot; of the ad $i$ where the set of $x_i,y_i$ (number of clicks and number of sign-up for ad $i$) is assumed to be observations of random samples from a Poisson distribution with parameter $Z_i$. &lt;/p&gt;&#10;&#10;&lt;p&gt;And we assume that these $Z_i$ are from a Gamma distribution with unknown but &lt;strong&gt;common&lt;/strong&gt; (all the advertisements are in a same campaign) parameters of shape and rate.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Ok for the hypothesis. Questions : &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;What is the law of $X|Z$ or $Y|Z$ ? Write the density $f_{X,Y|Z}$. &lt;/li&gt;&#10;&lt;li&gt;I observe a lot of $x_i,y_i$ with $i = 1,...,n$. I try to estimate the average &quot;conversion rate&quot;. Is $E[Z]$ the same thing that $E[Z|X=x,Y=y]$? I guess not. I made a confusion because I can only observe realization of $X$ and $Y$. &lt;/li&gt;&#10;&lt;li&gt;How can I estimate the unknow parameters of my Gamma just by observing the realizations of Poisson with parameter the realization of the Gamma? &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I am quite confuse with all of that. In turn when I observe $x_i$ and $y_i$ I observe a couple $(X,Y)$ where the two variables are dependent. &lt;/p&gt;&#10;&#10;&lt;p&gt;So if anybody can give me some advice for that problem (I'm sure that if I can write down my problem properly with the link with maximum likelihood - or EM - I can solve a part of my questions). &lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you in advance, &lt;/p&gt;&#10;&#10;&lt;p&gt;BimBimBap&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-15T08:55:46.853" Id="107984" LastActivityDate="2014-07-15T08:55:46.853" OwnerUserId="52106" PostTypeId="1" Score="0" Tags="&lt;modeling&gt;&lt;maximum-likelihood&gt;&lt;poisson&gt;&lt;expectation-maximization&gt;&lt;latent-variable&gt;" Title="Estimation parameters for latent (unobserved) variable" ViewCount="63" />
  
  
  <row Body="&lt;p&gt;As an opening comment, the word &quot;correlation&quot; sometimes is used in order to indicate the existence of an unspecified form of stochastic dependence -some authors will write &quot;the variables are correlated&quot; and they mean &quot;the variables are dependent&quot;. So when one reads such a general statement, one must be careful to interpret it in the context that it is used.  &lt;/p&gt;&#10;&#10;&lt;p&gt;To the other extreme, &quot;correlation&quot; sometimes is used as a shortcut for &lt;a href=&quot;http://en.wikipedia.org/wiki/Pearson%27s_product&quot; rel=&quot;nofollow&quot;&gt;Pearson's product moment correlation coefficient&lt;/a&gt; which is the most widely used related measure, and for which the notation $\operatorname{Corr}(X,Y)$ is employed.  But other measures do exist, so again, one should be alert.  &lt;/p&gt;&#10;&#10;&lt;p&gt;The phrase &quot;correlation only measures linear relationships&quot; can be misleading, if one thinks hard enough about it. If it is zero, then we usually say &quot;the relation between the two variables, if it exists is not linear&quot;. But if it is not zero, we &lt;em&gt;cannot&lt;/em&gt; say that &quot;their relation is linear&quot;. It can very well be non-linear &lt;em&gt;and&lt;/em&gt; the correlation to be non-zero. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;AN EXAMPLE&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;Consider a random variable $X$ that is continuous uniform $U(0,1)$ and define another random variable by  $Y = X^3$. This is a clear non-linear relationship.&#10;Their covariance will be&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\operatorname{Cov}(X,Y) = E(XY) - E(X)E(Y) = E(X^4) - E(X)E(X^3)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;For a $U(0,1)$ random variable we have&lt;/p&gt;&#10;&#10;&lt;p&gt;$$E(X) = \frac 12,\;\; E(X^3) = \int_{0}^{1}x^3dx = \frac 14,\;\;E(X^4) = \int_{0}^{1}x^4dx = \frac 15$$&lt;/p&gt;&#10;&#10;&lt;p&gt;So &#10;$$\operatorname{Cov}(X,Y) = \frac 15 - \frac 12\cdot \frac 14 = \frac 3{40} \qquad [1]$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The Pearson's correlation coefficient is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\operatorname{Corr}(X,Y) = \frac {\operatorname{Cov}(X,Y)}{\sqrt {\operatorname{Var}(X)\operatorname{Var}(Y)}}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;We have&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\operatorname{Var}(X) = \frac 1{12},\;\; \operatorname{Var}(Y)= E(X^6) - \left(E(X^3)\right)^2 = \frac 17 - \frac 1{16} = \frac 9{112}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Bringing it all together we get&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\operatorname{Corr}(X,Y) = \frac {\frac 3{40}}{\sqrt {\frac 1{12}\cdot \frac 9{112}}} \approx 0.9165$$&lt;/p&gt;&#10;&#10;&lt;p&gt;... a very high value. Can we adequately express $Y$ as a linear function of $X$? Let's have a look: Based on a random sample of $1.000$ observations from a $U(0,1)$ we get&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/OpEcB.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;(note that the theoretical $\beta$ coefficient is $0.9$).  &lt;/p&gt;&#10;&#10;&lt;p&gt;Is the blue straight line an &quot;adequate representation&quot; of the relation between $Y$ and $X$?&lt;/p&gt;&#10;&#10;&lt;p&gt;I would tentatively say that, if anything, the absolute value of the correlation coefficient measures the &quot;uniformity&quot; of the &lt;em&gt;direction&lt;/em&gt; of covariance, rather than its &quot;linear&quot; nature. In our example, when $X$ tends to rise $Y$ rises too, &lt;em&gt;in all cases&lt;/em&gt; (this is the nature of the non-linear relationship between them, given also the support of $X$). &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-15T10:09:55.733" Id="107997" LastActivityDate="2014-07-15T10:09:55.733" OwnerUserId="28746" ParentId="107929" PostTypeId="2" Score="2" />
  
  <row AnswerCount="4" Body="&lt;p&gt;I have a dataframe with many observations and many variables. Some of them are categorical (unordered) and the others are numerical.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm looking for associations between these variables. I've been able to compute correlation for numerical variables (Spearman's correlation) but :&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;I don't know how to measure correlation between unordered categorical variables.&lt;/li&gt;&#10;&lt;li&gt;I don't know how to measure correlation between unordered categorical variables and numerical variables.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Does anyone know how this could be done? If so, are there R functions implementing these methods?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-15T12:18:27.247" FavoriteCount="3" Id="108007" LastActivityDate="2015-02-16T06:20:08.607" LastEditDate="2014-08-20T21:12:04.037" LastEditorUserId="7290" OwnerUserId="52117" PostTypeId="1" Score="6" Tags="&lt;r&gt;&lt;correlation&gt;&lt;categorical-data&gt;" Title="Correlations with categorical variables" ViewCount="7383" />
  
  
  <row Body="&lt;p&gt;SPSS isn't free, so you won't find a legal version of SPSS 16 that's free. There is a free, open-source alternative to SPSS, called &lt;a href=&quot;http://www.gnu.org/software/pspp/&quot; rel=&quot;nofollow&quot;&gt;PSPP&lt;/a&gt;. PSPP is designed to look like SPSS, though it still has fewer functions available than SPSS.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-07-15T15:45:49.457" Id="108040" LastActivityDate="2014-07-15T15:45:49.457" OwnerUserId="24808" ParentId="108039" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Consider the basic definition of a derivative according to Leibniz:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \frac{d}{dx} f(x)\Big|_{x=x_0} = \lim_{x\rightarrow x_0}\frac{f(x)-f(x_0)}{x-x_0} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;From this you see that if $f(x)$ has unit $u$, and $x$ has unit $v$, the derivative has unit $\frac{u}{v}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;More generally, the $n$-th derivative $\frac{d^n}{dx^n} f(x)$ has unit $\frac{u}{v^n}$ (this is easily shown by repeating the above argument).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; In addition to the mathematics consider a basic example: let a function $s(t)$ denote the distance travelled by a car versus time, i.e. $s(t)$ has unit meters $m$ and the time $t$ has unit seconds $s$. The first derivative then has unit $m/s$ and descibes the velocity=distance per time. The second derivative has unit $m/s^2$ and describes acceleration, which is [change in velocity] per time.&lt;/p&gt;&#10;&#10;&lt;p&gt;Translated to your graph, the first derivative corresponds to [change in gap probability per height] or [slope of gap probability]. The sought second derivate then denotes [change in [slope of gap probability] per height]&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-15T15:46:05.347" Id="108041" LastActivityDate="2014-07-17T10:42:31.247" LastEditDate="2014-07-17T10:42:31.247" LastEditorUserId="49279" OwnerUserId="49279" ParentId="107959" PostTypeId="2" Score="7" />
  <row AnswerCount="0" Body="&lt;p&gt;I am a computer science graduate with a minor in mathematics, however it has been a long while since taking any statistical courses.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I am looking at calculating the Mean Percent Error and Mean Absolute Percent Error between expected scores and actual scores.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Lets use the school setting for a familiar analogy. If I calculate the MPE and MAPE for each school as a whole based on various test scores against expected test scores, could I then use those MPE and MAPE results (summed and averaged) to get the MPE and MAPE for larger subsets such as combining multiple School's MPE/MAPE results into a district level MPE/MAPE, or would that skew the results from a loss of proper weighting of data?&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there another piece of data I could retain alongside the school MPE/MAPE to address combining them into larger subsets, such as the number of students, or number of tests taken to maintain proper weighting?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-07-15T15:34:58.723" Id="108057" LastActivityDate="2014-07-15T16:50:13.650" OwnerDisplayName="cbradsh1" PostTypeId="1" Score="0" Tags="&lt;mathematical-statistics&gt;" Title="MPE and MAPE averages" ViewCount="101" />
  <row AnswerCount="0" Body="&lt;blockquote&gt;&#10;  &lt;p&gt;Number of obs     =       501&#10;  , Method: Fixed-effects regression&lt;br&gt;&#10;  Number of groups  =       101&lt;/p&gt;&#10;  &#10;  &lt;blockquote&gt;&#10;    &lt;p&gt;F( 10,   100)     =   3422.31&lt;/p&gt;&#10;    &#10;    &lt;blockquote&gt;&#10;      &lt;p&gt;, Prob &gt; F          =    0.0000&#10;      ,  within R-squared  =    0.6325&lt;/p&gt;&#10;    &lt;/blockquote&gt;&#10;  &lt;/blockquote&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;My model (panel data with fixed effects) has six variables and four dummies for five years and as you see above, the F-value is too high with a value of 3422. Is this normal? Or what could be the problem?&lt;/p&gt;&#10;&#10;&lt;p&gt;I should also mention that when I add one variable to the six, the f stat turn from 3422 to 86. &#10;Thank you in advance &lt;/p&gt;&#10;" CommentCount="17" CreationDate="2014-07-15T17:13:41.643" Id="108060" LastActivityDate="2014-07-15T20:01:12.930" LastEditDate="2014-07-15T20:01:12.930" LastEditorUserId="45745" OwnerUserId="45745" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;time-series&gt;&lt;statistical-significance&gt;&lt;p-value&gt;&lt;panel-data&gt;" Title="F-statistic value is too high for my model" ViewCount="231" />
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;In my multivariate dataset I have over 100 objects/cases that have been coded on 20 different variables. Several variables are ordinal and several variables are nominal. Is it possible to use non-linear principal components analysis as a dimension reduction technique on this dataset? I am not certain how to approach optimal scaling with the two types of variables simultaneously.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-07-15T19:17:31.577" Id="108076" LastActivityDate="2014-11-10T16:17:39.330" OwnerUserId="52149" PostTypeId="1" Score="1" Tags="&lt;pca&gt;&lt;multivariate-analysis&gt;&lt;nonlinear&gt;" Title="Can I use non-linear PCA (CATPCA?) on my multivariate dataset that contains nominal AND ordinal data?" ViewCount="60" />
  <row Body="&lt;p&gt;You don't choose a one-tailed test based on near-significance in a two-tailed test.&lt;/p&gt;&#10;&#10;&lt;p&gt;You don't choose the direction of a one-tailed test based on directional information from the data.&lt;/p&gt;&#10;&#10;&lt;p&gt;Or at the least, if you do those things, you must also double the resulting p-value.&lt;/p&gt;&#10;&#10;&lt;p&gt;A one tailed test - if you do one at all - must be based on prior considerations, in place before you know what is in the data. If this is not the case, the significance levels (and p-values) are meaningless.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-07-15T19:48:19.927" Id="108082" LastActivityDate="2014-07-15T19:48:19.927" OwnerUserId="805" ParentId="108078" PostTypeId="2" Score="11" />
  <row AnswerCount="0" Body="&lt;p&gt;I am building a high-dimensional Bayesian spike-and-slab model to study the association between several organic compounds and a continuous outcome. The goal is to select the most influential compounds that are most explanatory for the continuous outcome. A number of compounds in my data are unclassified, i.e. they do not have a known nomenclature. &lt;/p&gt;&#10;&#10;&lt;p&gt;When I completed the models, some of these unclassified compounds were selected as influential variables. I cannot let the unclassified compounds just stay in my results because they will be tough to interpret by the downstream audience of this analysis. So I am now thinking of running the unclassified compounds that made it to the influential shortlist through another database and retrieve nomenclatures that the original database did not assign. The two ways of classification are fundamentally different; one assigns nomenclature by relying on clustering and structural similarity and another compares it to standards.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I can think of a tricky issue that might crop up here. There might be some compounds which are labeled as say, A by the new database. If the old database had assigned some compounds the tag A, then I have a situation where some of the compounds tagged A are influential and some are not. How do I deal with this bias? Any ideas on how I can structure my thoughts about this problem are most welcome. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-15T22:49:56.970" FavoriteCount="1" Id="108098" LastActivityDate="2014-07-15T22:49:56.970" OwnerUserId="6250" PostTypeId="1" Score="0" Tags="&lt;classification&gt;&lt;bias&gt;" Title="Exposure classification bias, post-modeling" ViewCount="13" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to use the liblinear library for logistic regression with L2 regularization. However, I am finding some issues with it. For eg when choosing the cost parameter, I chose&#10;the C parameter to be from 0.5 to 10 with 10 fold cross validation. However, when I tried with higher values of C upto even 32000, the cross validation accuracy is more with C=32000. This is really strange&lt;/p&gt;&#10;&#10;&lt;p&gt;I normally see people using up to 10 but in my case even 32000 seems to be the best. Isn't this weird?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-15T23:02:37.227" Id="108101" LastActivityDate="2014-07-15T23:02:37.227" OwnerUserId="12329" PostTypeId="1" Score="0" Tags="&lt;logistic&gt;&lt;classification&gt;&lt;cross-validation&gt;" Title="Liblinear logisitic regression with L2 regularization for classification" ViewCount="40" />
  
  <row Body="&lt;p&gt;Here goes:&lt;/p&gt;&#10;&#10;&lt;p&gt;In my field (developmental science) we apply DFA to intensive multivariate time-series data for an individual. &lt;em&gt;Intensive small samples are key.&lt;/em&gt; DFA allows us to examine both the structure and time-lagged relationships of latent factors. Model parameters are constant across time, so stationary time-series (i.e., probability distributions of stationarity of stochastic process is constant) is really what you are looking at with these models. However, researchers have relaxed this a bit by including time-varying covariates. There are many ways to estimate the DFA, most of which involve the Toeplitz matrices: maximum likelihood (ML) estimation with block Toeplitz matrices (Molenaar, 1985), generalized least squares estimation with block Toeplitz matrices (Molenaar &amp;amp; Nesselroade, 1998), ordinary least squares estimation with lagged correlation matrices (Browne &amp;amp; Zhang, 2007), raw data ML estimation with the Kalman filter (Engle &amp;amp; Watson, 1981; Hamaker, Dolan, &amp;amp; Molenaar, 2005), and the Bayesian approach (Z. Zhang, Hamaker, &amp;amp; Nesselroade, 2008).&lt;/p&gt;&#10;&#10;&lt;p&gt;In my field DFA has become an essential tool in modeling nomothetic relations at a latent level, while also capturing idiosyncratic features of the manifest indicators: the &lt;em&gt;idiographic filter.&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The P-technique was a precursor to DFA, so you might want to check that out, as well as what came after... state-space models. &lt;/p&gt;&#10;&#10;&lt;p&gt;Read any of the references in the list for estimation procedures for nice overviews.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-16T02:21:15.367" Id="108110" LastActivityDate="2014-07-16T02:21:15.367" OwnerUserId="46638" ParentId="29085" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;My experiment is devised as follow: There are two different types of stimuli-- cellphones: android (android-1; android-2)  and iOS (iOS-1 and iOS-2). A sample size of 60 subjects are divided into two groups of Group A:30 - Group B:30. The first group (Group A: 30) is further divided into two groups of (GroupA1- 15 and GroupA2- 15. The second group (Group B:30) is also divided into two groups of (GroupB1 -15 and GroupB2- 15).&lt;/p&gt;&#10;&#10;&lt;p&gt;The following assignment occurs:&#10;subjects: stimuli&lt;/p&gt;&#10;&#10;&lt;p&gt;GroupA1: android-1 (version1)&#10;GroupA2: android-2 (version2)&#10;GroupB1: iOS-1     (version1)&#10;GroupB2: iOS-2     (version2)&lt;/p&gt;&#10;&#10;&lt;p&gt;case (i) I want to conduct a within-subject ANOVA test with the two groups of participants - GroupA1 and GroupA2. &lt;/p&gt;&#10;&#10;&lt;p&gt;case (ii) I want to conduct another within-subject ANOVA test with the two groups of participants - GroupB1 and GroupB2.&lt;/p&gt;&#10;&#10;&lt;p&gt;case (iii) I want to conduct a mixed-ANOVA: within-subject test cellphone version (1, 2) &amp;amp; in-between subject test (Groups [A, B]). &lt;/p&gt;&#10;&#10;&lt;p&gt;(a) What numbers do I input, based on the above brief, into G*Power Analysis to determine the sample size needed to conduct this study in each case scenario?&lt;/p&gt;&#10;&#10;&lt;p&gt;case (i) and (ii)&#10;Effect size f:0.25?&#10;alpha: 0.05;&#10;Power:: 0.95 ?&#10;Number of groups:2&#10;Number of measurements:2&lt;/p&gt;&#10;&#10;&lt;p&gt;case (iii) &#10;Effect size f:0.25?&#10;alpha: 0.05;&#10;Power:: 0.95 ?&#10;Number of groups:4&#10;Number of measurements:2?&lt;/p&gt;&#10;&#10;&lt;p&gt;I would be grateful for any advice. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-16T02:55:27.590" Id="108116" LastActivityDate="2014-07-16T16:05:47.673" LastEditDate="2014-07-16T16:05:47.673" LastEditorUserId="39531" OwnerUserId="39531" PostTypeId="1" Score="0" Tags="&lt;anova&gt;&lt;sample-size&gt;&lt;sample&gt;&lt;manova&gt;" Title="G*Power analysis -sample size calculation" ViewCount="58" />
  
  <row Body="&lt;p&gt;&lt;a href=&quot;http://davegiles.blogspot.ca/2013/06/ardl-models-part-ii-bounds-tests.html&quot; rel=&quot;nofollow&quot;&gt;http://davegiles.blogspot.ca/2013/06/ardl-models-part-ii-bounds-tests.html&lt;/a&gt;&#10;see in the above link for a detail discussion. Its very useful.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-07-16T06:34:47.607" Id="108129" LastActivityDate="2014-07-16T06:34:47.607" OwnerUserId="52175" ParentId="94353" PostTypeId="2" Score="-1" />
  <row AcceptedAnswerId="108187" AnswerCount="1" Body="&lt;p&gt;I would like to get the most important variables from a PCA result. I see two clusters in the plot. I now that is possible that there is no only one variable causing this, so maybe I would have to get more than one variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm using &quot;&lt;a href=&quot;http://adegenet.r-forge.r-project.org&quot; rel=&quot;nofollow&quot;&gt;Adegenet&lt;/a&gt;&quot; R package. My original data is a matrix where rows are PubMed papers and columns are MeSH keywords. Data has been transformed into SNP-like to adapt the method to the new input data. Please point me the the correct R package if you think I'm doing the incorrect work with this package, I just chose it because I already knew how it works.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/1LhUv.png&quot; alt=&quot;First two dimensions of PCA based on 1359 metagenomcs papers and 3459 MeSH terms&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#R code&#10;library(adegenet)&#10;#Load SNPs&#10;myPath &amp;lt;- &quot;pubmed_result_metagenomics_ALL_parsed.fasta&quot;#core SNPs retrieved with kSNP from 188 H. parasuis strains, removing from the analysis the strains tagged with ‘NK’ phenotype. kSNP k-mer sizes tested were 25, 20 and 15, selecting the run that gave more SNPs, i.e., 15.&#10;core_SNPs_matrix &amp;lt;- fasta2genlight(myPath, chunk=1000, multicore=FALSE)#&#10;core_SNPs_matrix &amp;lt;- as.matrix(core_SNPs_matrix)&#10;&#10;# Principal Component Analysis (PCA)&#10;pca1 &amp;lt;- glPca(core_SNPs_matrix) # 10 components saved&#10;pca1&#10;&#10;# Draw PCA colorplot&#10;myCol &amp;lt;- colorplot(pca1$scores,pca1$scores, transp=TRUE, cex=4)&#10;abline(h=0,v=0, col=&quot;grey&quot;)&#10;add.scatter.eig(pca1$eig[1:40],2,1,2, posi=&quot;topright&quot;, inset=.05, ratio=.3)&#10;title(&quot;First two dimensions of PCA \n based on 1359 metagenomcs papers \n and 3459 MeSH terms&quot;)&#10;dev.copy2pdf(file = &quot;Figure_12.pdf&quot;) #Save as .pdf#&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2014-07-16T11:25:10.590" FavoriteCount="2" Id="108148" LastActivityDate="2015-01-23T16:09:54.747" LastEditDate="2014-07-16T14:32:30.743" LastEditorUserId="43706" OwnerUserId="43706" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;clustering&gt;&lt;pca&gt;" Title="Pull out most important variables from PCA" ViewCount="284" />
  
  <row AnswerCount="1" Body="&lt;p&gt;The &lt;code&gt;lmerTest&lt;/code&gt; package provides an ANOVA function for linear mixed effects models with optionally Satterthwaite's (default) or Kenward-Roger's approximation of the degrees of freedom. What is the difference between these two approaches? When to chose which?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-16T13:30:53.737" FavoriteCount="3" Id="108161" LastActivityDate="2015-01-05T00:20:52.910" LastEditDate="2014-07-16T13:45:13.317" LastEditorUserId="7290" OwnerUserId="52198" PostTypeId="1" Score="8" Tags="&lt;r&gt;&lt;mixed-effect&gt;&lt;approximation&gt;&lt;degrees-of-freedom&gt;" Title="Satterthwaite vs Kenward-Roger approximations for the df in mixed effects models" ViewCount="341" />
  
  
  
  <row Body="&lt;p&gt;The KDE is a mixture of Normal distributions.  Let's look at a single one of them.&lt;/p&gt;&#10;&#10;&lt;p&gt;The definitions of $P(A)$ and $P(B)$ show their values are invariant under translations and rescalings in the plane, so it suffices to consider the standard Normal distribution with PDF $f$.  The inequality&lt;/p&gt;&#10;&#10;&lt;p&gt;$$f(x,y) \le f(r,s)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;is equivalent to &lt;/p&gt;&#10;&#10;&lt;p&gt;$$x^2 + y^2 \ge r^2 + s^2.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Introducing polar coordinates $\rho, \theta$ allows the integral to be rewritten&lt;/p&gt;&#10;&#10;&lt;p&gt;$$P(r,s) = \frac{1}{2\pi}\int_0^{2\pi}\int_\sqrt{r^2+s^2}^\infty \rho \exp(-\rho^2/2) d\rho d\theta= \exp(-(r^2+s^2)/2) = 2\pi f(r,s).$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Now consider the mixture.  Because it is linear, &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\eqalign{&#10;P(r,s) &amp;amp;= \frac{1}{n}\sum_i 2\pi f((r-x_i)/h, (s-y_i)/h) \\&#10;  &amp;amp;= 2\pi h^2\left(\frac{1}{n}\sum_i \frac{1}{h^2} f((r-x_i)/h, (s-y_i)/h)\right) \\&#10;  &amp;amp;=2\pi h^2 KDE(r,s).&#10;}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Indeed, &lt;strong&gt;$f$ and $P$ are proportional.&lt;/strong&gt;  The constant of proportionality is $2\pi h^2$.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;That such a proportionality relationship between $P$ and $f$ is special&lt;/strong&gt; can be appreciated by contemplating a simple counterexample.  Let $f_1$ have a uniform distribution on a measurable set $A_1$ of unit area and $f_2$ have a uniform distribution on a measurable set $A_2$ which is disjoint from $A_1$ and has area $\mu\gt 1$.  Then the mixture with PDF $f=f_1/2 + f_2/2$ has constant value $1/2$ on $A_1$, $1/(2\mu)$ on $A_2$, and is zero elsewhere.  There are three cases to consider:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;$(r,s)\in A_1$.  Here $f(r,s)=1/2$ attains its maximum, whence $P(r,s)=1$.  The ratio $f(r,s)/P(r,s) = 1/2$.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;$(r,s)\in A_2$.  Here $f(r,s)$ is strictly less than $1/2$ but greater than $0$.  Thus the region of integration is the complement of $A_1$ and the resulting integral must equal $1/2$.  The ratio $f(r,s)/P(r,s) = (1/(2\mu))/(1/2) = 1/\mu$.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Elsewhere, $f$ is zero and the integral $P$ is zero.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Evidently the ratio (where it is defined) is not constant and varies between $1$ and $1/\mu \ne 1$.  Although this distribution is not continuous, it can be made so by adding a Normal$(0,\Sigma)$ distribution to it.  By making both eigenvalues of $\Sigma$ small, this will change the distribution very little and produce qualitatively the same results--only now the values of the ratio $f/P$ will include &lt;em&gt;all&lt;/em&gt; the numbers in the interval $[1,1/\mu]$.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;This result also does not generalize to other dimensions.&lt;/strong&gt;  Essentially the same calculation that started this answer shows that $P$ is an incomplete Gamma function and that clearly is not the same as $f$.  That two dimensions are special can be appreciated by noting that the integration in $P$ essentially concerns the &lt;em&gt;distances&lt;/em&gt; and when those are Normally distributed, the distance function has a $\chi^2(2)$ distribution--which is the &lt;em&gt;exponential distribution.&lt;/em&gt;  The exponential function is unique in being proportional to its own derivative--whence the integrand $f$ and integral $P$ must be proportional.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-07-16T17:16:49.797" Id="108191" LastActivityDate="2014-07-16T17:16:49.797" OwnerUserId="919" ParentId="81132" PostTypeId="2" Score="3" />
  
  <row AcceptedAnswerId="108229" AnswerCount="1" Body="&lt;p&gt;I've developed a ARIMA model with exogenous variable. Before fitting the model, I made every time series stationary by differencing (each variable had a different order of integration). For simplicity, let's say there was only one exogenous variable. Finally I ended up with a model. Keeping in mind that all my data are stationary to create the model, I received the following coefficient estimates:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ARIMA(1,0,0) with zero mean     &#10;&#10;Coefficients:&#10;       ar1      X&#10;      -0.6     -0.002&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;There were no MA terms. Note that Y was second-differenced, and X was first differenced. So, my formula is really like:&lt;/p&gt;&#10;&#10;&lt;p&gt;$Y_t'' = -0.6 Y_{t-1}'' - 0.002 X'$&lt;/p&gt;&#10;&#10;&lt;p&gt;Where the primes ( $'$ ) represent the level of differencing.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'd like an equivalent representation of the model using the original non-differenced variables. So I perform the following:&lt;/p&gt;&#10;&#10;&lt;p&gt;$(Y_t - Y_{t-1}) - (Y_{t-1} - Y_{t-2}) = -0.6 [(Y_{t-1} - Y_{t-2}) - (Y_{t-2} - Y_{t-3})] - 0.002(X_{t} - X_{t-1}) $&lt;/p&gt;&#10;&#10;&lt;p&gt;$Y_{t} - 2Y_{t-1} + Y_{t-2} = -0.6Y_{t-1} + 1.2Y_{t-2} - 0.6Y_{t-3} - 0.002X_{t} + 0.002X_{t-1}$&lt;/p&gt;&#10;&#10;&lt;p&gt;$Y_{t} = 1.4Y_{t-1} - 0.2Y_{t-2} - 0.6Y_{t-3} - 0.002X_{t} + 0.002X_{t-1}$&lt;/p&gt;&#10;&#10;&lt;p&gt;So, I have 2 questions.&lt;/p&gt;&#10;&#10;&lt;p&gt;1) Is my interpretation correct? Seems straight-forward, but I just want to double check. Is there a quicker way to perform these conversions, besides the tedious expanding &amp;amp; combining I performed above? Some of the models I'm working with are quite more complicated. Perhaps using the backshift operator would help? I don't have much practice with algebra involving the backshift operator, and am not sure whether it would help do what I'd like.&lt;/p&gt;&#10;&#10;&lt;p&gt;2) How do I perform a similar procedure when there are MA terms? I know that an MA term is equivalent to infinite AR terms. So, is it still be possible to get some representation like $Y_t = ...$? For example, suppose my fit produced ARIMA(1,0,1), with an MA coefficient estimate of 0.3.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-16T20:00:22.017" FavoriteCount="1" Id="108212" LastActivityDate="2014-07-16T23:09:49.693" LastEditDate="2014-07-16T23:09:49.693" LastEditorUserId="43805" OwnerUserId="43805" PostTypeId="1" Score="1" Tags="&lt;arima&gt;&lt;stationarity&gt;&lt;arma&gt;&lt;equivalence&gt;&lt;non-stationary&gt;" Title="Inverse Differencing and ARIMA Model Equivalence" ViewCount="50" />
  <row AnswerCount="0" Body="&lt;p&gt;In the &lt;code&gt; caret &lt;/code&gt; package, there is the option to impute missing data using decision trees. I already imputed my data by sampling the missing data from a distribution. Also, I determined the important variables to include in the model instead of relying on the &lt;code&gt; caret &lt;/code&gt; package. In general, is it better to clean the data by yourself instead of relying on the &lt;code&gt; caret &lt;/code&gt; package?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-16T20:26:07.033" Id="108214" LastActivityDate="2014-07-16T20:26:07.033" OwnerUserId="49811" PostTypeId="1" Score="0" Tags="&lt;r&gt;" Title="Imputation using caret package vs imputation without it" ViewCount="111" />
  <row Body="&lt;p&gt;About the subsampling strategies: just for example consider  to have two observations $X_1 \sim N(\mu_1, \sigma_1^2)$ and $X_2 \sim N(\mu_2,\sigma_2^2)$ and consider to put some priors on the mean and variance. Let $\theta = (\mu_1, \mu_2, \sigma_1^2, \sigma_2^2)$, the posterior we want to evaluate is &#10;$$&#10;f(\theta|X_1, X_2) \propto f(X_1|\theta)f(X_2 | \theta)f(\theta)&#10;$$&#10;COnsider now a binomial variable $\delta \sim B(0.5)$. If $\delta=0$  we chose $X_1$, if $\delta =1$ we chose $X_2$, the new posterior is$$&#10;f(\theta, \delta|X_1, X_2) \propto f(X_1, X_2|\delta,\theta)f(\theta)f(\delta)&#10;$$&#10;where $f(X_1, X_2|\delta,\theta) = f(X_1|\theta)^{\delta} f(X_2|\theta)^{1-\delta}$ and $f(\delta) = 0.5$. Now if you want to sample $\delta$ with a Gibbs step you have to compute  $f(X_1|\theta)$ and $f(X_2|\theta)$ because $P(\delta=1)= \frac{f(X_1|\theta) }{f(X_1|\theta) +f(X_2|\theta) }$. If you otherwise use the Metropolis Hastings then you propose a new state $\delta^*$ and you have to compute only one between $f(X_1|\theta)$ and $f(X_2|\theta)$, the one associated with the proposed states but you have to compute one between $f(X_1|\theta)$ and $f(X_2|\theta)$ even for the last accepted state of $\delta$. Then I am not sure that the metropolis will give you some advantage. Moreover here we are considering a bivariate process, but with a multivariate process the sampling of the $\delta$s can be very complicated with the metropolis.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-16T22:13:21.760" Id="108225" LastActivityDate="2014-07-16T22:13:21.760" OwnerUserId="27213" ParentId="100591" PostTypeId="2" Score="0" />
  <row AnswerCount="1" Body="&lt;p&gt;Say I want to know what sample size I need for an experiment in which I'm seeking to determine whether or not the difference in two proportions of success is statistically significant. Here is my current process:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Look at historical data to establish baseline predictions. Say that in the past, taking an action results in a 10% success rate whereas not taking an action results in a 9% success rate. Assume that these conclusions have not been statistically validated but that they are based on relatively large amounts of data (10,000+ observations).&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Plug these assumptions into power.prop.test to get the following:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; power.prop.test(p1=.1,p2=.11,power=.9)&#10;&#10; Two-sample comparison of proportions power calculation &#10;&#10;          n = 19746.62&#10;         p1 = 0.1&#10;         p2 = 0.11&#10;  sig.level = 0.05&#10;      power = 0.9&#10;alternative = two.sided&#10;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;So this tells me that I would need a sample size of ~20000 in each group of an A/B test in order to detect a significant difference between proportions.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The next step is to perform the experiment with 20,000 observations in each group. Group B (no action taken) has 2300 successes out of 20,000 observations, whereas Group A (action taken) has 2200 successes out of 20,000 observations.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Do a prop.test&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;prop.test(c(2300,2100),c(20000,20000))&#10;&#10;2-sample test for equality of proportions with continuity correction&#10;&#10;data:  c(2300, 2100) out of c(20000, 20000)&#10;X-squared = 10.1126, df = 1, p-value = 0.001473&#10;alternative hypothesis: two.sided&#10;95 percent confidence interval:&#10;0.003818257 0.016181743&#10;sample estimates:&#10;prop 1 prop 2 &#10;0.115  0.105&#10;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;So we say that we can reject the null hypothesis that the proportions are equal.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Questions&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Is this method sound or at least on the right track?&lt;/li&gt;&#10;&lt;li&gt;Could I specify &lt;code&gt;alt=&quot;greater&quot;&lt;/code&gt; on prop.test and trust the p-value even though power.prop.test was for a two-sided test?&lt;/li&gt;&#10;&lt;li&gt;What if the p-value was greater than .05 on prop.test? Should I assume that I have a statistically significant sample but there is no statistically significant difference between the two proportions? Furthermore, is statistical significance inherent in the p-value in prop.test - i.e. is power.prop.test even necessary?&lt;/li&gt;&#10;&lt;li&gt;What if I can't do a 50/50 split and need to do, say, a 95/5 split? Is there a method to calculate sample size for this case?&lt;/li&gt;&#10;&lt;li&gt;What if I have no idea what my baseline prediction should be for proportions? If I guess and the actual proportions are way off, will that invalidate my analysis?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Any other gaps that you could fill in would be much appreciated - my apologies for the convoluted nature of this post. Thank you!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-16T22:17:48.583" FavoriteCount="4" Id="108226" LastActivityDate="2015-02-10T23:31:20.577" OwnerUserId="48679" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;hypothesis-testing&gt;&lt;statistical-significance&gt;&lt;proportion&gt;" Title="R - power.prop.test, prop.test, and unequal sample sizes in A/B tests" ViewCount="785" />
  <row AnswerCount="1" Body="&lt;p&gt;I have this web application where I need to map consumer preferences based on some input information and individual choices. My goal is to create a list of product recommendations and evaluate the level of “importance” of these products with respect to the user.  &lt;/p&gt;&#10;&#10;&lt;p&gt;From my research so far I realize that there are several ways to address this problem. For instance there is the classical marketing research approach that involves modeling individual utility functions and econometric models. Alternatively,  there’s  the machine learning approach with learning based algorithms. There might be others I’m not aware of. &lt;/p&gt;&#10;&#10;&lt;p&gt;Which would be the best approach in this case?  Are there other alternatives? I really could use some direction on the best way to go.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-07-16T23:03:11.283" Id="108230" LastActivityDate="2014-07-22T05:15:53.997" LastEditDate="2014-07-17T13:08:22.530" LastEditorUserId="52227" OwnerUserId="52227" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;data-mining&gt;&lt;econometrics&gt;" Title="Approach for mapping consumer preferences" ViewCount="50" />
  
  
  <row AcceptedAnswerId="108252" AnswerCount="1" Body="&lt;p&gt;I am reading a paper about the Dirichlet process and image segmentation.&#10;(&lt;a href=&quot;http://dl.acm.org/citation.cfm?id=2404806&quot; rel=&quot;nofollow&quot;&gt;http://dl.acm.org/citation.cfm?id=2404806&lt;/a&gt;)&lt;/p&gt;&#10;&#10;&lt;p&gt;How is (3) below derived? Do any articles or posts explain image segmentation and the Dirichlet process?&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/22HD7.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="0" CreationDate="2014-07-17T04:37:56.900" Id="108251" LastActivityDate="2014-07-17T05:10:43.327" LastEditDate="2014-07-17T04:50:39.160" LastEditorUserId="32036" OwnerUserId="52197" PostTypeId="1" Score="1" Tags="&lt;image-processing&gt;&lt;dirichlet-process&gt;" Title="Image clustering and Dirichlet process" ViewCount="36" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I am given a time series data vector (ordered by months and years),which contains only &lt;code&gt;0&lt;/code&gt;s and &lt;code&gt;1&lt;/code&gt;s. &lt;code&gt;1&lt;/code&gt; s represent a person changes his job at a particular a month. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Questions:&lt;/strong&gt; What model can i use to determine model how frequently this person change his job ? In addition, this model should be able to predict the probability of this person changing his in the next 6 months.  &lt;/p&gt;&#10;&#10;&lt;p&gt;A poisson process ? (I have studied poisson process before however I have no idea when and how to apply it). Any assumptions that data need to meet before applying the poisson process ? &lt;/p&gt;&#10;&#10;&lt;p&gt;Would love to gather more information on how to model something like this. Thanks&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-17T09:30:09.147" Id="108276" LastActivityDate="2014-07-24T17:43:14.007" OwnerUserId="20917" PostTypeId="1" Score="2" Tags="&lt;time-series&gt;&lt;poisson-process&gt;" Title="Given time series data, how to model the frequency of someone changes his job?" ViewCount="63" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I know that there are already a host of questions about nested designs but many of them haven't been answered or come from biological domains which I sometimes find hard to transfer to my domain.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am currently trying to analyse the data from a psychological study. Participants read four statements and indicated how appropriate they found each on a 5-point Likert-scale. But I'll just call the outcome variable &quot;out&quot; and so the content will not distract from the statistical problem at hand.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are two IVs, one that was varied on three levels between subjects (let's call it &quot;between&quot; and the levels 1, 2 and 3) and one on two levels within subjects (&quot;within&quot; with its levels a and b). The within subjects factor is realised in such a manner that two of the four statements correspond to one of the levels and the other two to the other. &lt;/p&gt;&#10;&#10;&lt;p&gt;Correspondingly, for every participant, I now have four &quot;out&quot; data points, two for the a statements and two for the b statements, like this (in long format):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;subj    between within  item    out&#10;123     1       a       a1      3&#10;123     1       a       a2      4&#10;123     1       b       b1      1&#10;123     1       b       b2      2&#10;124     2       a       a1      5&#10;124     2       a       a2      4&#10;124     2       b       b1      2&#10;124     2       b       b2      3&#10;125     3       a       a1      1&#10;125     3       a       a2      1&#10;125     3       b       b1      2&#10;125     3       b       b2      3&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and so on&#10;What is the right way to analyse the effect and interaction of between and within? I assume that the two different a and the two different b statements do not differ systematically but are just two instances of the same thing. Or do I need an additional factor that tells R which of the four statements it is in order to allow for that error? Anyway, I have tried these options:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;m1 &amp;lt;- aov(out~between*within+Error(subj/within))&#10;summary(m1)&#10;&#10;m2 &amp;lt;- lm(out~between*within+within/subj)&#10;anova(m2)&#10;&#10;m3 &amp;lt;- lmer(out~between*within+(between|subj))&#10;Anova(m3)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;They produce different results but I'm not entirely sure about which one is right or what the differences are. Could anyone enlighten me on this? I assume this is about fixed and random effects which always gets my head in a twist. I have read other posts about nested designs and I think &lt;a href=&quot;https://stats.stackexchange.com/questions/94882/repeated-measures-mixed-model-using-lmer-in-r&quot;&gt;this&lt;/a&gt; one comes nearest. But unfortunately the question ID being nested groups wasn't answered. Any help would be very much appreciated!&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt;&lt;br&gt;&#10;I greatly appreciate the answers so far! The comments prompted me to read some more tutorials on mixed effects models as well as answers to similar questions. This enables me to clarify my question: I know that I will need to specify random intercepts at least for subjects and probably also the four different items representing the two levels of the within factors. However, I am unsure about the specific fixed/random effects structure I would have to model, and specifically what the maximal model in my case would look like.&#10;Right now, I have tried some code which roughly looks like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;m1 &amp;lt;- lmer(out ~ between * within + (between|subj) + (within|subj) + (between|within/item))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This is probably wrong on several levels so any feedback would be greatly appreciated! Just for clarity: The item factor has no intrinsic meaning, but both levels of the within factor is realised by two items each (which every participant sees) and I am unsure whether this is relevant to the model.&lt;/p&gt;&#10;&#10;&lt;p&gt;Apart from the model specification, I am still getting confused about the concepts nested and crossed and which one applies to my setup. Lastly, most of my models fail to converge but that is probably a matter for another question (and has been discussed here in length.)&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2014-07-17T09:49:24.287" FavoriteCount="0" Id="108280" LastActivityDate="2014-07-23T11:13:11.137" LastEditDate="2014-07-22T07:29:56.447" LastEditorUserId="52253" OwnerUserId="52253" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;mixed-model&gt;&lt;random-effects-model&gt;&lt;nested&gt;&lt;psychology&gt;" Title="What is the right way to analyze a nested design in R?" ViewCount="157" />
  <row AnswerCount="1" Body="&lt;p&gt;I conduct some experiments on continuous speech recognition. After the initial application of the recognizer I have the phoneme sequence that includes some errors (three types of errors: substitution, deletion and inclusion). For instance:&#10;I obtain&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;h e l' l o n y k f r a n d s t&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;instead of&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;h e l l o m y f r i e n d s&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;So, the task is use the information about language (correct transcription of every word, valid word order etc.) in order to recover the correct transcription from the phoneme sequence corrupted by noise.&lt;/p&gt;&#10;&#10;&lt;p&gt;I wondered about Hidden Markov Models, but I am not shore how to use it here.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any idea is appreciated. Thanks in advance!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-17T10:33:31.940" Id="108286" LastActivityDate="2014-09-25T06:27:57.307" OwnerUserId="42452" PostTypeId="1" Score="0" Tags="&lt;hidden-markov-model&gt;" Title="What is the most appropriate method to recover words transcription from the phoneme sequence with errors?" ViewCount="8" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a problem with Missing Value Analysis. I am using SPSS version 20. I am trying to test whether missing values are at complete random. As I know in order to ensure missing values are completely random (MCAR: missing completely at random), Sig. value should be greater than 0.05. I have got two cases.&lt;/p&gt;&#10;&#10;&lt;p&gt;First case, I have got Sig value less than 0.05 as illustrated by the picture&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/lVIdV.png&quot; alt=&quot;EM (Missing Value Analysis by SPSS)&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Second case: I have got degree of freedom is zero and Sig is unspecified for other variable. (See, picture below)&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/E2Bip.png&quot; alt=&quot;EM (Missing Value Analysis by SPSS&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Please, what should I do?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-17T11:02:54.503" Id="108291" LastActivityDate="2014-07-19T03:11:19.237" LastEditDate="2014-07-19T03:11:19.237" LastEditorUserId="48151" OwnerUserId="48151" PostTypeId="1" Score="0" Tags="&lt;statistical-significance&gt;&lt;mathematical-statistics&gt;&lt;spss&gt;&lt;missing-data&gt;&lt;expectation-maximization&gt;" Title="How to treat &quot;Missing Value Analysis&quot; test results (problems)" ViewCount="41" />
  <row AnswerCount="0" Body="&lt;p&gt;I have CO2 (in parts per million) data of a closed room. The CO2 data is recorded along with timestamps. Typical difference between two samples is around five minutes.&#10;&lt;strong&gt;My aim is to find occupancy of the room.&lt;/strong&gt; &#10;Theoretically, while the room is occupied the CO2 levels increase. And, when the room is unoccupied - CO2 levels decrease.  So, I take 1st derivative of CO2 data with respect to time which gives me rate of change in CO2. At this stage, I judge the room to be occupied with positive rate of change (rate of change &gt; 0) in CO2 and not occupied otherwise.&#10;I have some other CO2 data as well where I don't have validation head-counts. I use the same above approach to analyze this data and I believe there are many false positives in results (I have validation data of the room with actual Boolean head-counts) corresponding lower positive rate of change. The thought behind false positives being certainly there is because I know for sure the room was unoccupied for those particular duration.&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to derive a positive cutoff from training data (the one with validation set) so that I can apply the same approach on new data without any validation set being present. The room will be judged to be occupied when (&lt;strong&gt;rate of change&lt;/strong&gt; &gt; &lt;strong&gt;positive_cutoff&lt;/strong&gt;). This can decrease the number of false positives in results. &lt;/p&gt;&#10;&#10;&lt;p&gt;Have a look sample graphs: &lt;/p&gt;&#10;&#10;&lt;p&gt;Smooth CO2 (Data with validation set) :&lt;img src=&quot;http://i.stack.imgur.com/fRy6c.jpg&quot; alt=&quot;enter image description here&quot;&gt;&#10;Results after rate of change analysis: &lt;img src=&quot;http://i.stack.imgur.com/hXeVS.jpg&quot; alt=&quot;enter image description here&quot;&gt;&#10;CO2 data without validation set : &lt;img src=&quot;http://i.stack.imgur.com/4P9CV.jpg&quot; alt=&quot;enter image description here&quot;&gt;&#10;Results after rate of change analysis on above data : &lt;img src=&quot;http://i.stack.imgur.com/etZLE.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, I realize that for all actual occupancies, the rate of change is significantly higher than zero. Hence, the idea of deriving a positive threshold from training data (co2 values + actual headcounts) and use that threshold for predicting occupancy of room in test data. &lt;/p&gt;&#10;&#10;&lt;p&gt;How should I approach the problem from here on?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-17T11:41:30.990" Id="108296" LastActivityDate="2014-07-17T11:41:30.990" OwnerUserId="52261" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;dataset&gt;&lt;validation&gt;" Title="Derive minimum positive rate of change for co2 data" ViewCount="17" />
  
  
  
  <row Body="&lt;p&gt;If you are using R you can use the caret package which has a built in method to give variable importance. See this link (&lt;a href=&quot;http://caret.r-forge.r-project.org/varimp.html&quot; rel=&quot;nofollow&quot;&gt;http://caret.r-forge.r-project.org/varimp.html&lt;/a&gt;)&lt;/p&gt;&#10;&#10;&lt;p&gt;You basically will just have to do&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; varImp(mod, scale = FALSE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="3" CreationDate="2014-07-17T13:06:52.293" Id="108306" LastActivityDate="2014-07-17T13:06:52.293" OwnerUserId="39953" ParentId="108302" PostTypeId="2" Score="2" />
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to forecast data using ARIMA. I have read articles which say that I should first take the log of the series, if my series follows an exponential trend. &lt;/p&gt;&#10;&#10;&lt;p&gt;My question is: Is there a mathematical approach to find what kind of trend (linear or exponential) my data is following? &lt;/p&gt;&#10;&#10;&lt;p&gt;Im working with MATLAB and R. So any builtin functions would be really helpful. &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-07-17T13:52:52.977" FavoriteCount="0" Id="108309" LastActivityDate="2014-07-17T13:52:52.977" OwnerUserId="49644" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;matlab&gt;&lt;trend&gt;" Title="Trend: Linear or exponential" ViewCount="57" />
  <row AnswerCount="0" Body="&lt;p&gt;If I have multiple data series,&lt;/p&gt;&#10;&#10;&lt;p&gt;a = [a1, a2, ... a100] ~ bimodal with mu_a1, mu_a2, sigma_a1, sigma_a2, &lt;/p&gt;&#10;&#10;&lt;p&gt;b = [b1, b2, ... b100] ~ bimodal with mu_b1, mu_b2, sigma_b1, sigma_b2, &lt;/p&gt;&#10;&#10;&lt;p&gt;c = [c1, c2, ... c100] ~ bimodal with mu_c1, mu_c2, sigma_c1, sigma_c2, &lt;/p&gt;&#10;&#10;&lt;p&gt;how do I normalize a, b, and c to follow the same distribution? Say, for the purposes of combining the data?&lt;/p&gt;&#10;&#10;&lt;p&gt;If a, b, and c followed a Gaussian distribution, for example, all I'd have to do is subtract the means and scale to the same standard deviation. But what about in bimodal or multimodal distributions with multiple means and standard deviations?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-07-17T14:27:12.897" Id="108311" LastActivityDate="2014-07-17T14:27:12.897" OwnerUserId="52271" PostTypeId="1" Score="1" Tags="&lt;distributions&gt;&lt;normalization&gt;&lt;expectation-maximization&gt;" Title="How to normalize bimodal (or multimodal) distributions?" ViewCount="183" />
  <row AnswerCount="0" Body="&lt;p&gt;An analysis was implemented in SPSS 22 that uses the &quot;Generalized Linear Mixed Models&quot; feature of the program. Now I am looking for a way to port this to R. I use the glmmPQL() function of the MASS package and got the same results as in SPSS when using &quot;model-based covariances&quot;. But, the data requires to use the &quot;robust covariances&quot; instead.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the SPSS manual I discovered what is the difference between both methods of estimating the covariances: &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Covariance matrix. The model-based estimator is the negative of the generalized inverse of the Hessian matrix. The robust (also called the Huber/White/sandwich) estimator is a &quot;corrected&quot; model-based estimator that provides a consistent estimate of the covariance, even when the specification of the variance and link functions is incorrect. &lt;a href=&quot;http://pic.dhe.ibm.com/infocenter/spssstat/v22r0m0/index.jsp?topic=%2Fcom.ibm.spss.statistics.help%2Fspss%2Fadvanced%2Fidh_idd_genlin_estimation.htm&quot; rel=&quot;nofollow&quot;&gt;Source: SPSS manual&lt;/a&gt;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;How can I use the Huber/White estimator in R?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-17T15:38:19.257" FavoriteCount="1" Id="108320" LastActivityDate="2014-07-17T15:38:19.257" OwnerUserId="52279" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;glmm&gt;&lt;variance-covariance&gt;" Title="How to use the Huber/White estimator of covariances in a generalized linear mixed model (glmmPQL) in R?" ViewCount="135" />
  <row AcceptedAnswerId="108359" AnswerCount="1" Body="&lt;p&gt;For a set A, I'm running 8 independent random samples, each with a probability of 1/8=12.5% and is without replacement. I know that the set formed by the union of these 8 samples will be of a size between 12.5% and 100%.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there an expected value for the size of this union?&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-07-17T17:04:17.940" Id="108329" LastActivityDate="2014-07-18T02:16:58.350" LastEditDate="2014-07-18T01:55:33.460" LastEditorUserId="49453" OwnerUserId="49453" PostTypeId="1" Score="2" Tags="&lt;probability&gt;&lt;sampling&gt;" Title="Expected coverage of a set of random samples" ViewCount="36" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have done the first part of the question already I just need help on the second part.&lt;/p&gt;&#10;&#10;&lt;p&gt;You read in a U.S Census Bureau that a 99% confidence interval for the mean income in 2005 of American households headed by a college educated person at least 25 years old was 100,272±1651. Based on this interval, can you reject the null hypothesis that the mean income in this group is $95,000?  What is the alternate hypothesis of the test? What is its significance level?&lt;/p&gt;&#10;&#10;&lt;p&gt;I answered that you can reject the null hypothesis at the 0.01 level as confidence interval is between 98621 and 101923, and 95,000 is not in that in that range.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am confused about the alternate hypothesis is it Ha&gt;95,000 or is it Ha does not equal 95,000. Also How do I get the signicance level of the null hypothesis?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you very much everyone.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-17T18:36:48.353" Id="108340" LastActivityDate="2014-07-17T18:46:33.657" OwnerUserId="46725" PostTypeId="1" Score="1" Tags="&lt;self-study&gt;&lt;statistical-significance&gt;&lt;confidence-interval&gt;&lt;t-test&gt;" Title="Confidence Interval and Signifcance test Question" ViewCount="45" />
  
  
  <row Body="&lt;p&gt;Don't abuse clustering for classification.&lt;/p&gt;&#10;&#10;&lt;p&gt;How do you plan on directing the algorithm to not produce two clusters that correspond to&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Males vs. Females?&lt;/li&gt;&#10;&lt;li&gt;Blondes vs. Brunettes (with red haired noise)&lt;/li&gt;&#10;&lt;li&gt;Short vs. Tall people?&lt;/li&gt;&#10;&lt;li&gt;People whose first name has even length vs. odd length?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;All of these would be meaningful clusters, wouldn't they?&lt;/p&gt;&#10;&#10;&lt;p&gt;If you have a particular objective to solve - in your case, introverts vs. extroverts - you &lt;em&gt;need&lt;/em&gt; to direct your algorithm accordingly. Algorithms cannot do magic, they need direction. So most likely &lt;strong&gt;you need training data&lt;/strong&gt;.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-07-17T19:35:54.517" Id="108350" LastActivityDate="2014-07-17T19:40:59.693" LastEditDate="2014-07-17T19:40:59.693" LastEditorUserId="7828" OwnerUserId="7828" ParentId="108261" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;The chapter on stochastically dependent effect sizes by Gleser and Olkin (2009) in &lt;em&gt;The handbook of research synthesis and meta-analysis&lt;/em&gt; (2nd ed.) describes how multiple-treatment and multiple-endpoint studies can be meta-analyzed. Your case can be covered by the &quot;multiple-treatment&quot; part -- that is, you can look at the two disease stages as two different 'treatments' that are both compared against a common control group. In essence, you will have to compute the covariance between the two (log) ORs and take that into consideration when you want to include the two ORs in your meta-analysis.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you work with R, you may find the &lt;code&gt;metafor&lt;/code&gt; package useful. On the metafor package website, you can find code that will reproduce the methods described in the Gleser and Olkin chapter: &lt;a href=&quot;http://www.metafor-project.org/doku.php/analyses:gleser2009&quot; rel=&quot;nofollow&quot;&gt;http://www.metafor-project.org/doku.php/analyses:gleser2009&lt;/a&gt;. That should help to get you started.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-17T20:29:29.333" Id="108353" LastActivityDate="2014-07-17T20:29:29.333" OwnerUserId="1934" ParentId="108346" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;The computation of the lasso solutions is a quadratic programming problem, and can be tackled by standard numerical analysis algorithms. But the least angle regression procedure is a better approach. This algorithm exploits the special structure of the lasso problem, and provides an efficient way to compute the solutions simultaneously for all values of  $\lambda$.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Here is my opinion:&lt;/p&gt;&#10;&#10;&lt;p&gt;Your question can be divided to two parts. High dimensional cases and low dimensional cases. &#10;On the other hand it depends on what criteria are you going to use for selecting the optimal model. in the original paper of LARS, it is proved a $C_p$ criteria for selecting the best model and also you can see a SVS and CV criteria in the 'Discussion' of the paper as well. Generally, there are tiny differences between LARS and Lasso and can be ignored completely.&lt;/p&gt;&#10;&#10;&lt;p&gt;In addition LARS is computationally fast and reliable. Lasso is fast but there is a tiny difference between algorithm that causes the LARS win the speed challenge. On the other hand there are alternative packages for example in R, called 'glmnet' that work more reliable than lars package(because it is more general). &lt;/p&gt;&#10;&#10;&lt;p&gt;To sum up, there is nothing significant that can be considered about lars and lasso. It depended on the context you are going to use model. &lt;/p&gt;&#10;&#10;&lt;p&gt;I personally advise using glmnet in R in both high and low dimensional cases. or if you are interested in different criteria, you can use &lt;a href=&quot;http://cran.r-project.org/web/packages/msgps/&quot; rel=&quot;nofollow&quot;&gt;http://cran.r-project.org/web/packages/msgps/&lt;/a&gt; package.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-17T22:37:45.140" Id="108362" LastActivityDate="2014-07-18T08:54:33.567" LastEditDate="2014-07-18T08:54:33.567" LastEditorUserId="46139" OwnerUserId="46139" ParentId="4663" PostTypeId="2" Score="1" />
  
  
  <row AcceptedAnswerId="108433" AnswerCount="1" Body="&lt;p&gt;I would like to fit &lt;code&gt;LASSO&lt;/code&gt; in the following type of data where there are large number of variables (p &gt; n). My y variable is &lt;code&gt;y&lt;/code&gt; and would like to fit rest of variables in &lt;code&gt;myd&lt;/code&gt; as x variables. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#random population of 200 subjects with 1000 variables &#10;M &amp;lt;- matrix(rep(0,200*100),200,1000)&#10;for (i in 1:200) {&#10;set.seed(i)&#10;  M[i,] &amp;lt;- ifelse(runif(1000)&amp;lt;0.5,-1,1)&#10;}&#10;rownames(M) &amp;lt;- 1:200&#10;&#10;#random yvars &#10;set.seed(1234)&#10;u &amp;lt;- rnorm(1000)&#10;g &amp;lt;- as.vector(crossprod(t(M),u))&#10;h2 &amp;lt;- 0.5 &#10;set.seed(234)&#10;y &amp;lt;- g + rnorm(200,mean=0,sd=sqrt((1-h2)/h2*var(g)))&#10;&#10;myd &amp;lt;- data.frame(y=y, M)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;&lt;em&gt;My Question:&lt;/em&gt;&lt;/strong&gt; &#10; What is best method for this type of data and is there any &lt;code&gt;R&lt;/code&gt; program that implements this? &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-07-18T00:03:37.340" FavoriteCount="1" Id="108371" LastActivityDate="2014-07-18T15:50:14.517" OwnerUserId="7244" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;lasso&gt;" Title="LASSO in the data with large number of variables (p) with lower number of samples (n)" ViewCount="111" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I would like to compute the power of two distributions which are poisson with $n_1$ being large and $n_2$ being medium sized.&#10;I have used the formula given by &lt;a href=&quot;http://www.ucs.louisiana.edu/~kxk4695/JSPI-04.pdf&quot; rel=&quot;nofollow&quot;&gt;Krishnamoorthy and Thomson&lt;/a&gt; for the C-test (see below).&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\sum^\infty_{k_1=0}\sum^\infty_{k_2=0}\frac{e^{-n_1\lambda_1}(n_1\lambda_1)^{k_1}}{k_1!}\frac{e^{-n_2\lambda_2}(n_2\lambda_2)^{k_2}}{k_2!}I[P(X_1\geq k_1|k_1+k_2, p(\lambda_1/ \lambda_2))\leq\alpha]$$&lt;/p&gt;&#10;&#10;&lt;p&gt;However, as $n_1$ or $n_2$ get large the exponential function goes to $0$ which makes taking the sum of the probability rather pointless. &lt;/p&gt;&#10;&#10;&lt;p&gt;Is there anyway I can compute the power with having an large uneven sample size?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks a lot for your help&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-07-18T05:32:19.620" FavoriteCount="1" Id="108385" LastActivityDate="2014-07-18T05:45:13.663" LastEditDate="2014-07-18T05:45:13.663" LastEditorUserId="25843" OwnerUserId="25843" PostTypeId="1" Score="2" Tags="&lt;poisson&gt;&lt;power&gt;" Title="power of poisson test with large sample size" ViewCount="40" />
  
  <row AcceptedAnswerId="108425" AnswerCount="1" Body="&lt;p&gt;I have data for 1000 students' performance over 10 different tests on a scale of 0 -100 (a 1000 rows X 10 col matrix).&#10;I calculated the mean score and the associated coefficient of variation for each student.&#10;Now I wish to sort / rank students who consistently perform better, that is, have a high mean score and low coeff. of variation.&#10;Can anyone suggest a ranking scheme based on two objective variables (mean score and coeff. of variation).&#10;Thanks.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-18T12:13:50.650" FavoriteCount="1" Id="108418" LastActivityDate="2014-07-18T14:52:52.797" OwnerUserId="51861" PostTypeId="1" Score="1" Tags="&lt;multivariate-analysis&gt;&lt;ranking&gt;" Title="multivariate sorting / ranking" ViewCount="67" />
  <row AnswerCount="0" Body="&lt;p&gt;This is my first question here. I have experimental results for two datasets where I want to check if the difference is significant or not. Here is a description of my experiment:&lt;/p&gt;&#10;&#10;&lt;p&gt;Experiment: I have a set of 70 genes where I showed that 12 of them are evolving in a particular manner. Now I wanted to check if these 70 genes are biased to carry that evolutionary characteristics compared to all the genes in that species. Unfortunately, due to experimental restriction I could not replicate this experiment on all the genes. So I selected a random set of 100 genes (without replacement) and got the results that 8 out of 100 genes were carrying the same characteristics.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now to check if this difference is statistically significant or not, I initially used binomial test on the data (using R prop.test). But someone told me that this is not the right test for your data. Can someone please help me with the correct test&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks,&#10;RT&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-18T12:29:06.263" Id="108419" LastActivityDate="2014-07-18T12:29:06.263" OwnerUserId="52326" PostTypeId="1" Score="0" Tags="&lt;proportion&gt;" Title="Test for relative proportions" ViewCount="21" />
  <row Body="&lt;p&gt;The natural logarithm of $1 + r/m$ is defined to be the area under the curve $t\to 1/(1+t)$ between $0$ and $r/m$.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/sVRSm.png&quot; alt=&quot;Figure&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;When $r/m$ is small, this region (shown in blue) is approximated moderately well by a rectangle of height $1$ and base $r/m$ (shown in gray underneath), whose area is $r/m$. The relative error in the approximation is no greater than the decrease in the value of $1/(1+t)$ over the region, which for small $t$ becomes arbitrarily close to zero.  That answers the question.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;From the figure it is clear that the region is much better approximated by a trapezoid with vertices at the origin, $(x, 0)$, $(x, 1/(1+x))$, and $(0,1)$, where $x=r/m$. Its area is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$x\left(1 + \frac{1}{1 +x}\right)/2 = x\left(1 - \frac{x}{2+2x}\right).$$&lt;/p&gt;&#10;&#10;&lt;p&gt;In the next figure, these two approximations are plotted (in red for the trapezoid, gold for the rectangle) along with a graph of $\log(1+x)$ (in blue) for smallish values of $x$.  There is scarcely any visible difference between the trapezoidal approximation and the logarithm. All three values converge to one another as $x$ approaches $0$.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/CVZ5O.png&quot; alt=&quot;Figure 2&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-18T13:50:29.790" Id="108423" LastActivityDate="2014-07-18T17:04:09.890" LastEditDate="2014-07-18T17:04:09.890" LastEditorUserId="805" OwnerUserId="919" ParentId="108382" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;You don't state if the events that each of the prizes are won are independent of each other (and if not, what the form of dependence is), which you need if you want to work out the probability that the total value of prizes exceeds some amount. I'll assume independence for now.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are also problems with the notation, since you define $k$ to be a set and then treat it like a sum of random variables. Let's construct a better notation first, or we'll have some problems.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let there be 5 prizes, of amounts $a_1 = 65, a_2=25, a_3=30, a_4=54, a_5=30$. Let $a=(a_1, a_2, a_3, a_4, a_5)'$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $X_1=1$ is prize 1 is won and $0$ otherwise. Define $X_2,...,X_5$ similarly, and let $X=(X_1, X_2, X_3, X_4, X_5)'$. &lt;/p&gt;&#10;&#10;&lt;p&gt;So if $X=(1,0,0,1,0)$ then the amount won is $65+54 = 119$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $p_1, p_2,...,p_5$ be the respective probabilities that $X_1=1$, $X_2=1$ and so on and let $p=(p_1, p_2, p_3, p_4, p_5)'$. Then $p=(0.80,0.50,0.25,0.20,0.40)'$&lt;/p&gt;&#10;&#10;&lt;p&gt;There are $2^5=32$ possible outcomes, corresponding to the $2^5$ values $X$ can take.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $Y$ be the total amount of prize won; $Y=a'X$. We need to be able to work out the probability distribution of $Y$.&lt;/p&gt;&#10;&#10;&lt;p&gt;One way to proceed is simply to list all the outcomes, with their probabilities. The probability of a particular outcome, $P(X=x)$ is $p^x\cdot (1-p)^{(1-x)}$ :&lt;/p&gt;&#10;&#10;&lt;p&gt;$X=(0,0,0,0,0)'$, $Y=0$, probability = $(1-p_1)(1-p_2)...(1-p_5)=0.036$&lt;/p&gt;&#10;&#10;&lt;p&gt;$X=(0,0,0,0,1)'$, $Y=30$, probability = $(1-p_1)(1-p_2)...p_5=0.024$&lt;/p&gt;&#10;&#10;&lt;p&gt;$X=(0,0,0,1,0)'$, $Y=54$, probability = $(1-p_1)(1-p_2)...p_4(1-p_5)=0.009$&lt;/p&gt;&#10;&#10;&lt;p&gt;$X=(0,0,0,1,1)'$, $Y=84$, probability = $(1-p_1)(1-p_2)...p_4p_5=0.006$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\vdots$&lt;/p&gt;&#10;&#10;&lt;p&gt;and so on up to &lt;/p&gt;&#10;&#10;&lt;p&gt;$X=(1,1,1,1,1)'$, $Y=204$, probability = $p_1 p_2...p_5=0.008$&lt;/p&gt;&#10;&#10;&lt;p&gt;If we collect them all up (summing the probability when you can get the same prize amount in more than one way), the probability (mass) function looks like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/v2jTO.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;and the cdf like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/2u7jk.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;(though strictly speaking step functions don't have vertical parts); or in table form:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;     y  prob   cdf&#10;1    0 0.036 0.036&#10;2   25 0.036 0.072&#10;3   30 0.036 0.108&#10;4   54 0.009 0.117&#10;5   55 0.036 0.153&#10;6   60 0.008 0.161&#10;7   65 0.144 0.305&#10;8   79 0.009 0.314&#10;9   84 0.009 0.323&#10;10  85 0.008 0.331&#10;11  90 0.144 0.475&#10;12  95 0.144 0.619&#10;13 109 0.009 0.628&#10;14 114 0.002 0.630&#10;15 119 0.036 0.666&#10;16 120 0.144 0.810&#10;17 125 0.032 0.842&#10;18 139 0.002 0.844&#10;19 144 0.036 0.880&#10;20 149 0.036 0.916&#10;21 150 0.032 0.948&#10;22 174 0.036 0.984&#10;23 179 0.008 0.992&#10;24 204 0.008 1.000&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;from which we can find $P(Y\leq 85)$ and $P(Y\leq 90)$, and hence obtain $P(Y&amp;gt;85)$ and $P(Y&amp;gt;90)$.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-07-18T15:48:02.947" Id="108432" LastActivityDate="2014-07-18T17:22:49.277" LastEditDate="2014-07-18T17:22:49.277" LastEditorUserId="805" OwnerUserId="805" ParentId="108424" PostTypeId="2" Score="4" />
  <row AnswerCount="0" Body="&lt;p&gt;Can dependent variable for PLS be based on Likert scale? &lt;/p&gt;&#10;&#10;&lt;p&gt;As i understand, dependent variable needs to be continuous. I wish to multiply frequency (i.e. integer value 3) with rating coming from likert scale (for example 5) to get value of Y as 15. is this acceptable as a continuous variable? If not, appreciate any alternate approach to calculate Y's.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks a lot for your support in advance. &lt;/p&gt;&#10;" CommentCount="8" CreationDate="2014-07-18T16:27:50.230" Id="108437" LastActivityDate="2014-07-18T16:27:50.230" OwnerUserId="52335" PostTypeId="1" Score="0" Tags="&lt;pls&gt;" Title="PLS regression analysis based on Likert scale" ViewCount="44" />
  <row AcceptedAnswerId="108469" AnswerCount="1" Body="&lt;p&gt;I am currently conducting a two-way ANOVA on a dataset and have noticed how significance and F values change dependent on the type of ANOVA I use in R. Why is this?&lt;/p&gt;&#10;&#10;&lt;p&gt;For instance, if I run a two-way ANOVA the factor I am analysing becomes outputs F (1,28) = 7.2, P &amp;lt;.05.&lt;/p&gt;&#10;&#10;&lt;p&gt;When I run it by itself however in a one-way ANOVA it becomes F (1, 30) = 21.5, p &amp;lt; .001.&lt;/p&gt;&#10;&#10;&lt;p&gt;If someone could explain to me why the difference, that would be much appreciated.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-18T18:15:49.003" Id="108445" LastActivityDate="2014-07-20T09:54:31.393" OwnerUserId="37190" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;anova&gt;" Title="Why do ANOVA p and F values change?" ViewCount="82" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a doubt related with Recursive State Estimation using Bayes Filter (actually using an aproximation to that through Particle Filters)&lt;/p&gt;&#10;&#10;&lt;p&gt;This algorithm is explained in several sources with sampling every $\Delta t$, and assumes a fixed $\Delta t$.&lt;/p&gt;&#10;&#10;&lt;p&gt;My questions are:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;how to choose a $\Delta t$ that is good enough for the problem?&lt;/li&gt;&#10;&lt;li&gt;Is is feasible to have a $\Delta t$ that is not homogenous, so let say we sample at $t=1$ then $t=3$  then $t=10$ etc?.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2014-07-18T18:44:07.307" Id="108451" LastActivityDate="2014-07-18T18:44:07.307" OwnerUserId="28148" PostTypeId="1" Score="1" Tags="&lt;nonparametric-bayes&gt;&lt;particle-filter&gt;" Title="Is it a problem to have non homogeneus sampling time in Bayes Filter?" ViewCount="24" />
  <row Body="&lt;p&gt;I'm going to give an answer to a different question! Because I am unsure if the original one is tractable, but it is how I would frame the problem - so I hope it is helpful.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's just start with the simpler task of &lt;em&gt;decomposing&lt;/em&gt; a set of covariances between two spatial weights matrices for the same units of analysis. Lets say that $W = W^a + W^b$, and we want to know how $\text{Cov}(y,Wy)$ relates to the covariances of $\text{Cov}(y,W^ay)$ and $\text{Cov}(y,W^by)$. &lt;/p&gt;&#10;&#10;&lt;p&gt;So assuming a mean centered $y$, the &lt;a href=&quot;http://en.wikipedia.org/wiki/Moran&amp;#39;s_I&quot; rel=&quot;nofollow&quot;&gt;sample covariance&lt;/a&gt; can be written as:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\text{Cov}(y,Wy) = \frac{N}{\sum_i \sum_j w_{ij}} \frac{\sum_i \sum_j w_{ij}(y_i \cdot y_j)}{\sum_i y_i^2}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Where $N$ is the total number of observations and $w_{ij}$ are the weights between units $i$ and $j$. Because I specified that $W$ is the sum of two separate weight matrices we can decompose the numerator in the above equation to the covariance contributed to by $W^a$ and the contribution of $W^b$:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\sum_i \sum_j w_{ij}(y_i \cdot y_j) = \sum_i \sum_j (w_{ij}^a + w_{ij}^b)(y_i \cdot y_j)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;So if you formulated the problem like this, we have a whole to part relationship. One slightly different way to do this with the county level model is to have you binary contiguity matrix be $W$, but then have $W^a$ be the within-state weights and $W^b$ be the between state weights. Then you can estimate a model of the form&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{align}&#10;y &amp;amp;= \rho_a W^a y + \rho_b W^b y + X \beta \\&#10;y &amp;amp;= \rho W y + X \beta&#10;\end{align}&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm not sure if $\rho_a + \rho_b = \rho$, but hopefully this is a good start.&lt;/p&gt;&#10;&#10;&lt;p&gt;So there are a few difficulties/moving parts with the aggregation that make me think that a direct decomposition is not going to happen:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;If you consider contiguity matrices for the state level compared to the county level, the weights do not overlap nicely like they do in this exposition. For example, at the state level Maryland is a neighbor of Pennsylvania. At the county level, any county in northern PA is not going to be first order neighbors of any county in Maryland.&lt;/li&gt;&#10;&lt;li&gt;The variance of the aggregated state level series is not the same as the variance of the county level. In most social science lit. when you aggregate areas up the correlations tend to be higher. I would expect the spatial autocorrelation with the smaller units to actually be smaller. In applied social science research a spatial auto-correlation of 0.2 is about the highest I've seen.&lt;/li&gt;&#10;&lt;li&gt;The variance of the aggregated series is partially a function of the within state correlation. So I don't believe you can decompose the within state spatial auto-correlation from the aggregate between state spatial auto-correlation as stated. &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I would like to see it if I am wrong though!&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-07-18T19:00:29.850" Id="108454" LastActivityDate="2014-07-18T19:00:29.850" OwnerUserId="1036" ParentId="108440" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;An almost optimal approach can be to fit ordinary binary regression model, possibly with updated (time-dependent) covariates, to all the records, and to just not trust the standard errors that result.  You can use the cluster sandwich covariance matrix estimator or cluster bootstrap to get &quot;honest&quot; standard errors.  I say &quot;almost&quot; because a mixed effects or other full likelihood approach that takes the particular form of intra-subject correlation into account can result in more efficient estimates of $\beta$ coefficients.  But this &quot;working independence model&quot; approach is often not far from optimal, as long as you compute the variance-covariance matrix so as to take into account redundancies across records within subject.  In the R &lt;code&gt;rms&lt;/code&gt; package the pertinent functions are &lt;code&gt;lrm&lt;/code&gt;, &lt;code&gt;robcov&lt;/code&gt;, &lt;code&gt;bootcov&lt;/code&gt;.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-07-18T19:16:29.757" Id="108458" LastActivityDate="2014-07-18T19:16:29.757" OwnerUserId="4253" ParentId="108201" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;In general: while the treatment sum of squares (SS) and degrees of freedom (df) will not change, the SSerror and dferror are calculated in different ways for one- and two-way ANOVA-s which is why the corresponding F- and p-values differ.&lt;/p&gt;&#10;&#10;&lt;p&gt;The error sum of squares (SS) is calculated as the sum of squared deviations from the cell mean for all cells in the design table. If you alter the design table, the error SS changes too. This alters F since for a treatment effect:&lt;/p&gt;&#10;&#10;&lt;p&gt;F = (SStreat / df1) / (SSerror / df2)&lt;/p&gt;&#10;&#10;&lt;p&gt;df1 = treatment df; df2 = error df;&lt;/p&gt;&#10;&#10;&lt;p&gt;For a &lt;strong&gt;one-way ANOVA&lt;/strong&gt; with equal sample sizes of all treatment groups: &#10;df1 = p - 1 (where p is the number of groups);&#10;df2 = p(n - 1) (where n is the number of persons per group);&lt;/p&gt;&#10;&#10;&lt;p&gt;If you subdivide the design table from above by a second factor (let's call it a 'row factor', as opposed to the 'column factor' from above) you get a &lt;strong&gt;two-way ANOVA&lt;/strong&gt;:&#10;df1 = p - 1 (where p is the number of columns);&#10;df2 = pk(n -1) (where k is the number of rows, n and p as above);&lt;/p&gt;&#10;&#10;&lt;p&gt;Btw., do you have a 2x2 design with 8 persons per group, i.e., 32 persons in total?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-07-18T20:44:49.817" Id="108469" LastActivityDate="2014-07-20T09:54:31.393" LastEditDate="2014-07-20T09:54:31.393" LastEditorUserId="16444" OwnerUserId="16444" ParentId="108445" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;You could use conditional logistic regression.  That would answer if, within each subject, on average, if Variable1 is associated with Variable2.  This will only work if you have enough subjects who have concordant Variable1 and Variable2 measurements.  In R,&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; library(survival)&#10; clogit(Variable1 ~ Variable2 + strata(UID))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Note that this only accounts for the correlation within subjects, not the correlation between time points, within subjects.  I think you're going to be stuck using a mixed model if you need to model the correlation across time, within subject.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-19T02:39:26.267" Id="108489" LastActivityDate="2014-07-19T02:39:26.267" OwnerUserId="52092" ParentId="106371" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;One of my variables has a low Cronbach's Alpha. I continued to run an planned contrast to test my hypothesis and the results were significant. &lt;/p&gt;&#10;&#10;&lt;p&gt;I understand that the results should be interpreted lightly. My question is however, what does it mean if the Cronbach's Alpha is low yet significant results have been found? I am writing an honours thesis so I am still learning. &lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-19T03:48:26.753" Id="108491" LastActivityDate="2014-07-19T07:02:35.600" OwnerUserId="52359" PostTypeId="1" Score="2" Tags="&lt;statistical-significance&gt;&lt;reliability&gt;&lt;consistency&gt;" Title="Cronbach's Alpha low- significant results" ViewCount="65" />
  
  <row AnswerCount="2" Body="&lt;p&gt;If I use the LARS algorithm to fit the LASSO path, is it sufficient to cross-validate using the values of $\lambda$ at each step in LARS or is it better to use a finer grid of $\lambda$ values? I guess I can ask this in two parts:  &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;Is the prediction optimal model found at one of the LARS steps?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Is the correct subset of variables found at one of the LARS steps?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I would have done the cross-validation over the parameter $s = \left\Vert \beta \right\Vert _{1}\left/ \max \left\Vert \beta \right\Vert&#10;_{1}\right.$ on a fine grid with $s\in \left( 0,1\right) $... but then I thought that defeats the purpose of using LARS, which is supposed to be attractive for its lower computational expense. Some clarification would be greatly appreciated.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-19T14:07:35.533" Id="108515" LastActivityDate="2014-10-28T08:23:12.387" OwnerUserId="52367" PostTypeId="1" Score="2" Tags="&lt;cross-validation&gt;&lt;lasso&gt;&lt;lars&gt;" Title="CV for LASSO tuning parameter using LARS" ViewCount="124" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Can we run this regression:&lt;/p&gt;&#10;&#10;&lt;p&gt;$Y_{it} = a + bX_{it} + c_2Time_2 + ... + c_TTime_T + ht + U_{it}$                     &lt;/p&gt;&#10;&#10;&lt;p&gt;$i = 1,2,..., N;$ $t = 1,2,3...,T$&lt;/p&gt;&#10;&#10;&lt;p&gt;Where&#10;​&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;$Time_T$ are time dummies &lt;/li&gt;&#10;&lt;li&gt;$t$ is the time trend.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;In other words, can we run a regression with both time dummies and and a time trend? I suspect that we cannot, but am looking for a more formal explanation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-19T16:00:21.807" FavoriteCount="0" Id="108526" LastActivityDate="2014-07-19T16:33:50.540" LastEditDate="2014-07-19T16:14:42.537" LastEditorUserId="24808" OwnerUserId="52382" PostTypeId="1" Score="0" Tags="&lt;panel-data&gt;" Title="Time Dummies and Time Trend in the same equation" ViewCount="27" />
  <row Body="&lt;p&gt;In conjunction with the &lt;code&gt;ols&lt;/code&gt; function in the R &lt;code&gt;rms&lt;/code&gt; package you can use the &lt;code&gt;effective.df&lt;/code&gt; function to get what you want.  This also tells you how many d.f. are effectively going to different types of terms (nonlinear, interaction, nonlinear interaction).  The &lt;code&gt;pentrace&lt;/code&gt; function helps you choose a penalty, or more than one penalty.  For example, you may decide to penalize complex terms more than simple linear terms.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-07-19T16:02:14.707" Id="108528" LastActivityDate="2014-07-19T16:02:14.707" OwnerUserId="4253" ParentId="108524" PostTypeId="2" Score="3" />
  
  
  <row Body="&lt;p&gt;Try this one:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(12345)&#10;library(mvtnorm)&#10;mu  &amp;lt;- c(1,2,3)&#10;Sig &amp;lt;- matrix(c(4,2,1,2,4,-1,1,-1,4), nrow=3, ncol=3)&#10;Y   &amp;lt;- rmvnorm(20, mean=mu, sigma=Sig) #generate multivariate normal distribution&#10;y3  &amp;lt;- lm(Y[,3]~Y[,1] + Y[,2])&#10;y2  &amp;lt;- lm(Y[,2]~Y[,1])&#10;Y23 &amp;lt;- lm(cbind(Y[,2], Y[,3])~Y[,1])&#10;summary(Y23)&#10;anova(Y23)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Output&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;summary(Y23)&#10;Response Y[, 2] :&#10;&#10;Call:&#10;lm(formula = `Y[, 2]` ~ Y[, 1])&#10;&#10;Residuals:&#10;    Min      1Q  Median      3Q     Max &#10;-4.0151 -1.1028 -0.2606  0.9836  4.1341 &#10;&#10;Coefficients:&#10;            Estimate Std. Error t value Pr(&amp;gt;|t|)   &#10;(Intercept)   2.1441     0.7060   3.037  0.00709 **&#10;Y[, 1]        0.3161     0.2780   1.137  0.27049   &#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1&#10;&#10;Residual standard error: 2.137 on 18 degrees of freedom&#10;Multiple R-squared:  0.067,     Adjusted R-squared:  0.01516 &#10;F-statistic: 1.293 on 1 and 18 DF,  p-value: 0.2705&#10;&#10;&#10;Response Y[, 3] :&#10;&#10;Call:&#10;lm(formula = `Y[, 3]` ~ Y[, 1])&#10;&#10;Residuals:&#10;    Min      1Q  Median      3Q     Max &#10;-5.1493 -1.2435 -0.1305  1.5748  4.3708 &#10;&#10;Coefficients:&#10;            Estimate Std. Error t value Pr(&amp;gt;|t|)  &#10;(Intercept)   2.4869     0.8722   2.851   0.0106 *&#10;Y[, 1]        0.2232     0.3435   0.650   0.5241  &#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1&#10;&#10;Residual standard error: 2.64 on 18 degrees of freedom&#10;Multiple R-squared:  0.02292,   Adjusted R-squared:  -0.03137 &#10;F-statistic: 0.4222 on 1 and 18 DF,  p-value: 0.5241&#10;&#10;&#10;&amp;gt; anova(Y23)&#10;Analysis of Variance Table&#10;&#10;            Df  Pillai approx F num Df den Df    Pr(&amp;gt;F)    &#10;(Intercept)  1 0.88361   64.533      2     17 1.149e-08 ***&#10;Y[, 1]       1 0.17884    1.851      2     17    0.1873    &#10;Residuals   18                                             &#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2014-07-19T19:53:46.297" Id="108546" LastActivityDate="2014-07-19T19:53:46.297" OwnerUserId="3903" ParentId="108517" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;This answer is based on the clarification offered in comments:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;I'd like to make a statement such as ... &quot;I am 68% sure the mean is between $3.1−σ_−$ and $3.1+σ_+$&quot;, and I want to calculate $σ_+$ and $σ_−$. &lt;/p&gt;&#10;  &#10;  &lt;p&gt;I think that at least in the physics world this is called a confidence interval.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Let's take it as given that in response to my question &quot;confidence interval for what?&quot; you responded that want a confidence interval for the &lt;em&gt;mean&lt;/em&gt; (and as made clear, not some other interval for values from the distribution).&lt;/p&gt;&#10;&#10;&lt;p&gt;There's one issue to clear up first - &quot;&lt;em&gt;I am 68% sure the mean is between&lt;/em&gt;&quot; isn't really the usual interpretation placed on a confidence interval. Rather, it's that if you repeated the procedure that generated the interval many times, 68% of such intervals would contain the parameter.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now to address the confidence interval for the mean.&lt;/p&gt;&#10;&#10;&lt;p&gt;I agree with your calculation of mean and sd of the data:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; x=c(1,2,3,5,1,2,2,3,7,2,3,4,1,5,7,6,4,1,2,2,3,9,2,1,2,2,3)&#10;&amp;gt; mean(x);sd(x)&#10;[1] 3.148148&#10;[1] 2.106833&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;However, the &lt;em&gt;mean&lt;/em&gt; doesn't have the same sd as the population the data was drawn from.&lt;/p&gt;&#10;&#10;&lt;p&gt;The standard error of the mean is $\sigma/\sqrt{n}$. We could estimate that from the sample sd (though if the data were truly Poisson, this isn't the most efficient method): &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; sd(x)/sqrt(length(x))&#10;[1] 0.4054603&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;If we assumed that the sample mean was approximately normally distributed (but did not take advantage of the possible Poisson assumption for the original data), &lt;em&gt;and&lt;/em&gt; assumed that $\sigma=s$ (invoking Slutsky, in effect) then an approximate 68% interval for the mean would be $3.15\pm 0.41$. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, the sample isn't really large enough for Slutsky. A better interval would take account of the uncertainty in $\hat \sigma$, which is to say, a 68% t$_{26}$-interval for the mean would be &lt;/p&gt;&#10;&#10;&lt;p&gt;$3.15\pm 1.013843\times 0.41$&lt;/p&gt;&#10;&#10;&lt;p&gt;which is just a fraction wider.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, as for whether the sample size is large enough to apply the normal theory CI we just used, that depends on your criteria. Simulations at similar Poisson means (in particular, ones somewhat smaller than the observed one) at this sample size suggest that using a t-interval will work quite well for similar Poisson rates and 27 observations or more.&lt;/p&gt;&#10;&#10;&lt;p&gt;If we take account of the fact that the data is (supposedly) Poisson, we can get a more efficient estimate of the standard deviation and an interval for $\mu$, but if there's any risk the Poisson assumption could be wrong - a chance of overdispersion caused by some homogeneity of Poisson parameters, say - then the t-interval would probably be better.&lt;/p&gt;&#10;&#10;&lt;p&gt;Nevertheless, we should consider that specific question - &quot;how to get a confidence interval for the mean of Poisson data&quot; -- but this more specific question has already been answered here on CV - for example, see the fine answers &lt;a href=&quot;http://stats.stackexchange.com/questions/15371/how-to-calculate-a-confidence-level-for-a-poisson-distribution&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-07-20T00:26:43.900" Id="108557" LastActivityDate="2014-07-20T09:40:51.950" LastEditDate="2014-07-20T09:40:51.950" LastEditorUserId="805" OwnerUserId="805" ParentId="108457" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;I believe usually &quot;shallow&quot; means only one hidden layer. &#10;For example: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;1. MLP with one hidden layer: data --&amp;gt; hidden --&amp;gt; softmax (class label)&#10;2. SVM: data --&amp;gt; feature (can be considered as hidden) --&amp;gt; class label&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Anything with more than 2 hidden layers (inclusive) can be called deep.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-20T00:49:36.290" Id="108559" LastActivityDate="2014-07-20T00:49:36.290" OwnerUserId="52395" ParentId="102706" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;This is a fairly basic question but I can't seem to find an answer on the net (perhaps I'm searching the wrong things).&lt;/p&gt;&#10;&#10;&lt;p&gt;Regression is trying to predict continuous outputs. Since a neural network uses a clamping function (typically giving a value between 0 and 1) before the output.. if the output can only be between 0 and 1, how can a neural network learn a regression function?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-20T04:52:52.463" Id="108570" LastActivityDate="2014-12-04T19:37:27.880" OwnerUserId="46925" PostTypeId="1" Score="0" Tags="&lt;neural-networks&gt;" Title="Basic Question: how does feed-forward neural network solve regression?" ViewCount="36" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to do maximum likelihood estimation and trying to see if the problem can be formulated using a random effect model. Here is the problem description:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;There are $100$ pairs $(N_i, D_i)$ where each $N_i$ is the number of trials and $D_i$ the number of successes in the $i$-th Binomial experiment (with $100$ trials). The probability $p$ of the binomial depends on the NormCDF through a parameter $\rho$ (which needs to be estimated using MLE). The probability $p$ is really a CONDITIONAL probability, conditional on $y$ = some constant, where $y$ can range over $(0,1)$.&#10;  Thus, I have a likelihood function $L$ which is being integrated over a variable $y$, $0 &amp;lt; y &amp;lt; 1$. That is,  &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;$L$ = int over $(0,1)$ of Binomial(n,d,p)dy,&lt;br&gt;&#10; or equivalently,&lt;br&gt;&#10;$L$ = int over $(0,1)$ of $\{ k1 * p^d * (1-p)^ (n-d) dy \},0&amp;lt;y&amp;lt;1$&lt;br&gt;&#10; where $p$ is  a function of $y$, and $k1$,$n$,$d$ are constants, $k1 = {n \choose d}$.&#10;Also&lt;br&gt;&#10;&lt;code&gt;p(y) = NORMCDF(  (gamma -  {sqrt(rho)*INVNORMCDF(y)}  ) / sqrt(1-rho)  )&lt;/code&gt;, where gamma is a constant (over all 100 trials).&lt;/p&gt;&#10;&#10;&lt;p&gt;I am trying to maximize $L$ to obtain an estimate for $\rho$.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to know if I can think of the $y$ as a random effect, and also, if specifying $y$ as a random effect in &lt;code&gt;SAS proc nlmixed&lt;/code&gt; implies that the likelihood $L$ is automatically integrated over $y$. Currently I am trying to maximize the likelihood in &lt;code&gt;SAS / IML&lt;/code&gt; and the problem is the optimization for $\rho$ does not work (&lt;code&gt;IML&lt;/code&gt; just keeps running, and I think it is because the likelihood function is flat enough that convergence takes too long).&lt;/p&gt;&#10;&#10;&lt;p&gt;I am trying to understand what it means for a variable $y$ to be a random effect: does it mean that the likelihood is integrated over all possible values of $y$? If that is true, can I estimate $\rho$ using &lt;code&gt;PROC NLmixed&lt;/code&gt; as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;&#10;    gamma = 0.5;&#10;    PROC NLMIXED DATA=bindata MAXITER=200000 QPOINTS=10 GCONV=1e-15;&#10;    PARMS rho = 0.5;&#10;    * Define likelihood;&#10;    w1 = INVNORMCDF(y);  &#10;    p  = NORMCDF(  (gamma -  sqrt(rho)*w1 ) / sqrt(1-rho)  );&#10;    L  = (n CHOOSE d)*((1-p)**(n-d)) + log(p**d);&#10;    MODEL d ~ GENERAL(L);&#10;    RANDOM y~ NORMAL(0,1) SUBJECT=ID;&#10;    RUN;&#10;&lt;/code&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;In this way, I don't need to integrate the binomial density over $y$ in $(0,1)$ myself in &lt;code&gt;IML&lt;/code&gt; using a &lt;code&gt;QUAD&lt;/code&gt; function as I am doing now. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-07-20T10:43:50.300" Id="108587" LastActivityDate="2014-07-20T13:55:35.247" LastEditDate="2014-07-20T13:55:35.247" LastEditorUserId="7290" OwnerUserId="52413" PostTypeId="1" Score="1" Tags="&lt;maximum-likelihood&gt;&lt;sas&gt;&lt;random-effects-model&gt;" Title="Random effects models / Integrate over the random effect" ViewCount="74" />
  <row Body="&lt;p&gt;The Spearman's rank c. c. is the Pearson' c.c. of the ranked variables; in its turn the Pearson's c.c. is defined as the mean of the product of the paired standardized scores $z(X_i)$, $z(Y_i)$. &lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{equation}&#10;r(X,Y) = \Sigma_i[z(X_i) z(Y_i)]/(n-1)&#10;\end{equation}&lt;/p&gt;&#10;&#10;&lt;p&gt;in which $n$ is the sample size and the standard scores&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{equation}&#10; z(X_i) = [X_i - \bar{X}]/std(X)&#10;\end{equation} &lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{equation}&#10; z(Y_i) = [Y_i - \bar{Y}]/std(Y)&#10;\end{equation} &lt;/p&gt;&#10;&#10;&lt;p&gt;are relative to the ranked variables ($X_i$, $Y_i$). Squaring $r(X_i, Y_i)$ we obtain the coefficient of determination $r²$, which we can equate to the fraction of explained variance. So if my Spearman's rank c.c. is of 0.6, I can deduce that the variance of the ranked variables is shared at 36%.&lt;/p&gt;&#10;&#10;&lt;p&gt;From the first equation and attempting at a simpler way of explaining $r(X,Y)$, I would say is the average value of concordance of z-score variations. For instance, let us say I repeat an experiment by increasing the sample size $n$ and calculate $r(X,Y)$ for both the small sample and the larger one. Let us say that associated to an increase in n of $~3$ I get a decrease in $r(X,Y)$ of roughly 50%; this corresponds to a decrease in standard scores concordance of 50%. My interpretation should be then that the latest dataset provides weaker evidence for the presence of a correlation in the data. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-20T13:07:18.330" Id="108593" LastActivityDate="2014-07-20T13:07:18.330" OwnerUserId="44760" ParentId="106356" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="108638" AnswerCount="2" Body="&lt;p&gt;Lets say I have an R value of .5 and therefore an $R^2$ value of .25, and a p value &amp;lt; 0.001. (All through the stock cor.test function in R).&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a way to isolate those cases/observations whose variation is most explained by my $R^2$? My goal is two fold, find the group for which this correlation is maximized, and take the other group too explore separately for other interesting interactions instead if this one.&lt;/p&gt;&#10;&#10;&lt;p&gt;Sorry for the imprecise wording here, clearly I'm not quite an expert in this area, and hence dont quite know how or what question to ask, but thank you for any direction you can provide.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-07-20T15:20:38.267" Id="108602" LastActivityDate="2014-07-20T21:59:13.967" LastEditDate="2014-07-20T17:39:01.963" LastEditorUserId="88" OwnerUserId="52372" PostTypeId="1" Score="0" Tags="&lt;correlation&gt;&lt;pearson&gt;" Title="Next steps after obtaining a significant Pearson coefficient" ViewCount="77" />
  
  
  <row AcceptedAnswerId="108623" AnswerCount="1" Body="&lt;p&gt;I am writing a naive bayes classifier for a text classification problem. I have a bunch of words and an associated label:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;[short,snippet,text], label1&#10;[slightly,different,snippet,text], label2&#10;...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I am able to train the naive bayes fine. However, when I am classifying unseen data, sometimes there are unseen features (words). In that case, what happens to the naive bayes formula to determine the probability of a class $C$ given features $F_1,F_2,...$?&lt;/p&gt;&#10;&#10;&lt;p&gt;$$P(C|F_1,F_2,...) = \frac{P(F_1,F_2,...|C)P(C)}{P(F_1,F_2,...)} = \frac{P(C)\prod_{i}P(F_i|C)}{P(F_1,F_2,...)}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Say feature $F_k$ never occured in the training data, then isn't $P(F_k|C)=\frac{0}{0}$?&lt;/p&gt;&#10;&#10;&lt;p&gt;How is this typically handled in classification problems?&lt;/p&gt;&#10;&#10;&lt;p&gt;One option is to simply ignore unseen features. However, I would not like to do that, since I am trying to calculate the actual probability score associated with classes. Probabilities should take a hit when there are unseen features, but I am not sure how to do that mathematically.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any insights, links to reseach articles, etc would really help! Thanks in advance.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-20T19:14:35.293" FavoriteCount="1" Id="108618" LastActivityDate="2014-07-20T22:03:52.983" LastEditDate="2014-07-20T19:22:19.013" LastEditorUserId="52391" OwnerUserId="52391" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;naive-bayes&gt;&lt;natural-language&gt;" Title="How to handle unseen features in a Naive Bayes classifier?" ViewCount="110" />
  <row Body="&lt;p&gt;Causal inference requires a causal model.  Such a model can be used to infer (predict) some variables given observations &lt;strong&gt;and interventions&lt;/strong&gt; at other variables.  Regression and classification have no such causal requirement and therefore have nothing to do with interventional (counterfactual) reasoning.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-20T19:46:55.923" Id="108624" LastActivityDate="2014-07-23T02:58:06.503" LastEditDate="2014-07-23T02:58:06.503" LastEditorUserId="858" OwnerUserId="858" ParentId="56909" PostTypeId="2" Score="1" />
  <row Body="&lt;ol&gt;&#10;&lt;li&gt;You can consider local minima &lt;em&gt;L&lt;/em&gt; &lt;strong&gt;bad&lt;/strong&gt; if a) your model does not overfit on &lt;em&gt;L&lt;/em&gt; and b) there's some other minima &lt;em&gt;L'&lt;/em&gt; which has significantly lower CV error rate than &lt;em&gt;L&lt;/em&gt;.&lt;/li&gt;&#10;&lt;li&gt;Global minima in NN is not &lt;em&gt;usually&lt;/em&gt; a bad thing. It is bad only if your model overfits, but you can use always proper regularization and stop early.&lt;/li&gt;&#10;&lt;li&gt;Overfitting has nothing to do with convergence, a model can overfit long before convergence.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2014-07-20T21:21:04.293" Id="108633" LastActivityDate="2014-07-20T21:21:04.293" OwnerUserId="25134" ParentId="108631" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;There is a betting game, which seems to have &quot;positive sum&quot; situations, and I'd like to find out the optimal strategy.&lt;/p&gt;&#10;&#10;&lt;p&gt;Multiple players place bets against the bank, for and bets that fit winning criteria earn players points of the game.&#10;There are two sets of criteria:&#10;The first: bet 1 point with 0.475 chance of doubling it or 0.525 – losing it. This is obviously a negative sum game.&#10;The second part is the &quot;Jackpot&quot;: you can add a fixed amount: 2pt, to your prior bet, to have the bet included in roll for the jackpot. The chance for winning the jackpot is 0.0001.&lt;/p&gt;&#10;&#10;&lt;p&gt;It's all about how big the jackpot is.&lt;/p&gt;&#10;&#10;&lt;p&gt;All points you lose, be it to the 0.525 chance of loss in the first part, or the 0.999 chance of loss in the second part, are accumulating in the jackpot. Additionally, if the amount in the jackpot is less than 10,000, the house covers the difference between the amount and 10,000 (you can't win less than 10,000 if you hit the 0.0001 chance). When a winning bet for the jackpot is placed, the jackpot is emptied, and the process of collecting begins anew.&lt;/p&gt;&#10;&#10;&lt;p&gt;When always playing the first part as bet 1, with house edge of 5%, and jackpot bet cost of 2, the expected value for a bet is (-2.05 + (jackpot &amp;times; 0.0001)) which means it becomes a positive sum game when jackpot value exceeds 20,500. When that happens I should keep placing bets until I win the jackpot.&lt;/p&gt;&#10;&#10;&lt;p&gt;But I don't quite grasp my chances for jackpot in the two lower ranges: 0–10,000 and 10,000–20,500. If it was a simple case of a fixed-amount jackpot, that would be plainly a negative sum game, but considering all my bets increase the jackpot value, I'm not so sure.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, does the fact that the jackpot is shared between all players (their losses add to it, one takes it all) change the optimal strategy?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-20T21:25:58.347" Id="108634" LastActivityDate="2014-07-20T21:40:08.660" LastEditDate="2014-07-20T21:40:08.660" LastEditorUserId="32036" OwnerUserId="52428" PostTypeId="1" Score="1" Tags="&lt;games&gt;" Title="Optimal strategy in growing jackpot scenario" ViewCount="11" />
  
  
  
  <row Body="&lt;p&gt;Well, one &quot;Lexy&quot; to another (even if you spell yours with an &quot;I&quot; ;):&lt;/p&gt;&#10;&#10;&lt;p&gt;You won't go wrong by using a nonparametric test. Without seeing the distributions of the absolute values of your actual data, one cannot really say whether normality is a good assumption or not. However, a paired test, like the &lt;a href=&quot;https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test&quot; rel=&quot;nofollow&quot;&gt;sign-rank test&lt;/a&gt;, which is a test for stochastic dominance, with the null hypothesis H$_{0}\text{: P}(X_{\text{A}} &amp;gt; X_{\text{B}}) = 0.5$ with H$_{\text{A}}\text{: P}(X_{\text{A}} &amp;gt; X_{\text{B}}) \ne 0.5$, should work just fine. What is your sample size?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-20T22:25:17.057" Id="108646" LastActivityDate="2014-07-20T22:25:17.057" OwnerUserId="44269" ParentId="69972" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I am new in this forum.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am beginning to work with time series, I have a daily (25,000+ observations) temperature dataset (01/01/1946 - 07/01/2014) &lt;/p&gt;&#10;&#10;&lt;p&gt;I want to test for the following:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;strong&gt;Trends:&lt;/strong&gt; So far I used OLS, but I have heard about using&#10;Mann-Kendall test may be useful.&lt;/li&gt;&#10;&lt;li&gt;&lt;strong&gt;seasonality:&lt;/strong&gt; In the same way that in the previous point so far I tried to analyze them with OLS, any recommendation is very welcome.&lt;/li&gt;&#10;&lt;li&gt;&lt;strong&gt;structural breaks:&lt;/strong&gt; the problem with this aspect is that graphically it cannot be observed any breaks, therefore I have been trying with CUSUM and CUSUMSQ to identify this structural breaks, some additional recommendations?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;As additional point I need to perform this on STATA. I saw in the Web about Kendall package for R, but I cannot use R. Currently I use Gretl and STATA, I hope someone might help me.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance for your help.&lt;/p&gt;&#10;&#10;&lt;p&gt;Jorge&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-07-20T22:39:55.073" Id="108649" LastActivityDate="2014-12-16T05:38:44.817" OwnerUserId="52431" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;stata&gt;&lt;gretl&gt;" Title="Mann-Kendall test STATA" ViewCount="172" />
  <row Body="&lt;p&gt;(While not presently an answer, this is too long for a comment; I plan to either edit it into an answer when the question improves, or eventually to delete it)&lt;/p&gt;&#10;&#10;&lt;p&gt;You have several terminology problems that render this question almost unanswerable, and certainly confusing for many readers&lt;/p&gt;&#10;&#10;&lt;p&gt;1) &quot;OLS&quot; is almost universally understood to mean 'ordinary least squares', not 'orthogonal least squares'.&lt;/p&gt;&#10;&#10;&lt;p&gt;2) Orthogonal least squares, &lt;a href=&quot;http://en.wikipedia.org/wiki/Total_least_squares&quot; rel=&quot;nofollow&quot;&gt;as conventially understood&lt;/a&gt; is not what you seem to be talking about.&lt;/p&gt;&#10;&#10;&lt;p&gt;You seem to be instead talking about linear transformations to orthogonalize the predictors, perhaps something like a Gram-Schmidt algorithm.&lt;/p&gt;&#10;&#10;&lt;p&gt;What are you doing? What is the algorithm you're trying to describe? What language is your code supposed to be in? Please clarify your question via edits, taking care to be as clear as you can.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-07-20T22:57:15.380" Id="108651" LastActivityDate="2014-07-21T00:48:40.513" LastEditDate="2014-07-21T00:48:40.513" LastEditorUserId="805" OwnerUserId="805" ParentId="108641" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;Since the goal is prediction you can choose some distance measure and then calculate the distance between your predictions $\hat{Y}$ and the true values $Y$, choosing the method with the minimum distance. The most common distance measurement for this setting is the mean squared error: MSE = $\sum_{i=1}^n (\hat{Y_i} - Y_i)$.  The hardest part is appropriately choosing your training and testing sets and then performing model selection procedure required by the Lasso and Ridge regularization.&lt;/p&gt;&#10;&#10;&lt;p&gt;First, split your data into two parts: training $X_{training}$ and testing $X_{testing}$.  A common choice is 66% training, 34% testing but the choice of proportion is influenced by the size of your data set.  Now forget about the testing set for a while.  Train, or fit, the three models using just the training data. Model selection for the regularization parameter $\lambda$ should be chosen via cross-validation since prediction is your goal here.  Finally, using the best $\lambda$, perform prediction on the testing data to obtain values for $\hat{Y}$.  I haven't used the &lt;code&gt;Lasso2&lt;/code&gt; or &lt;code&gt;MASS&lt;/code&gt; implementations of regularized regression but the &lt;code&gt;glmnet&lt;/code&gt; package makes the process very straightforward because it provides a cross-validatiion function. Here's some R code to do this on the &lt;code&gt;Prostate&lt;/code&gt; data:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;require(glmnet)&#10;data(Prostate, package = &quot;lasso2&quot;)&#10;## Split into training and test&#10;n_obs = dim(Prostate)[1]&#10;proportion_split = 0.66&#10;train_index = sample(1:n_obs, round(n_obs * proportion_split))&#10;y = Prostate$lpsa&#10;X = as.matrix(Prostate[setdiff(colnames(Prostate), &quot;lpsa&quot;)])&#10;Xtr = X[train_index,]&#10;Xte = X[-train_index,]&#10;ytr = y[train_index]&#10;yte = y[-train_index]&#10;## Train models&#10;ols = lm(ytr ~ Xtr)&#10;lasso = cv.glmnet(Xtr, ytr, alpha = 1)&#10;ridge = cv.glmnet(Xtr, ytr, alpha = 0)&#10;## Test models&#10;y_hat_ols = cbind(rep(1, n_obs - length(train_index)), Xte) %*% coef(ols)&#10;y_hat_lasso = predict(lasso, Xte)&#10;y_hat_ridge = predict(ridge, Xte)&#10;## compare&#10;sum((yte - y_hat_ols)^2)&#10;sum((yte - y_hat_lasso)^2)&#10;sum((yte - y_hat_ridge)^2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Note that the &lt;code&gt;sample()&lt;/code&gt; function randomly chooses the rows for training and testing so the MSE will change every time you run it. And since the &lt;code&gt;Prostate&lt;/code&gt; data is really just meant as a demonstration dataset, no clear winner is likely to emerge.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-20T23:39:38.753" Id="108652" LastActivityDate="2014-07-20T23:39:38.753" OwnerUserId="21683" ParentId="108529" PostTypeId="2" Score="2" />
  <row AnswerCount="2" Body="&lt;p&gt;Wouldn't any decision tree trained on a training data set have no errors in classification? In other words, wouldn't every data point be classified correctly in the training data set? How would this tie in with the misclassification rate?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-21T00:06:27.770" Id="108655" LastActivityDate="2014-10-31T16:38:58.837" OwnerUserId="52429" PostTypeId="1" Score="2" Tags="&lt;cart&gt;" Title="Decision Trees on training data" ViewCount="51" />
  
  <row Body="&lt;p&gt;Oftentimes you stop the training process before it gets to the point where it has zero errors on the training set. One way this can be done is to not expand any leaf nodes unless more than a certain number of training examples fall on that node. &lt;/p&gt;&#10;&#10;&lt;p&gt;After the first pass of training, a held-out validation set can be used to prune the tree to prevent over-fitting.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-21T02:40:26.777" Id="108664" LastActivityDate="2014-07-21T02:40:26.777" OwnerUserId="41389" ParentId="108655" PostTypeId="2" Score="0" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have used lme4 for mixed effects models of reaction times and accuracy rates. I could not use lmerTest because the type of model I was using are not yet implemented there (problem with predictors that are factors). I was able to get p-values for the models ran on accuracy rates (based on Wald z-values) but not for the models built on reaction times.&#10;In order to get p-values for all models, I used Anova in the car package which gives me Wald chisquare values and probability of significance based on those chisquares. &#10;My concern is that sometimes for the accuracy rates, the effects indicated as significant in the analysis of deviance table (with the Wald chisquare values) are not significant in the mixed effect models. That is even for main effects of a factor with only 2-levels.&#10;Does anyone know why this could be the case?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-07-21T05:44:05.763" Id="108677" LastActivityDate="2014-07-21T06:26:41.770" LastEditDate="2014-07-21T06:26:41.770" LastEditorUserId="24808" OwnerUserId="52444" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;chi-squared&gt;&lt;mixed-effect&gt;&lt;lme4&gt;&lt;car&gt;" Title="Different p-values between Wald Z and Wald Chisquare" ViewCount="133" />
  <row Body="&lt;p&gt;For the general case, the answer is no.  For the specific cases, it is also no.&lt;/p&gt;&#10;&#10;&lt;p&gt;A simple counter example is take $ y\sim U (0,1) $ and take $ x\sim Gamma (a, a) $ such that we have $ E (x)=1 $ and $ var (x)=a^{-1}$ .  Take $ x $ and $ y $ as independent, and we have:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ var (z) =E [var (z|y)]+var [E (z|y)]=E [y^2a^{-1}]+var [y]=var (y) + E (y^2)a^{-1}=\frac {1}{12} +\frac {1}{3} var (x)=var (x)\frac {a+4}{12} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;Now we just choose any value for $ a $ such that $ a&amp;gt; 8$ and we will have $ var (z)&amp;gt; var (x) $&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-07-21T07:38:04.250" Id="108682" LastActivityDate="2014-07-21T07:38:04.250" OwnerUserId="2392" ParentId="108676" PostTypeId="2" Score="4" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have used the STDRATE procedure to get the estimates and standard errors for 5 different timeperiods between 1980-2000 in 5 years interval (1980, 1985, 1990, 1995 and 2000). &lt;/p&gt;&#10;&#10;&lt;p&gt;This means that I have point estimate and 95% CI for $X_1, X_2, X_3, X_4$ and $X_5$ which spans between the years 1980-2000.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example i have the estimates with 95%Cl for &lt;/p&gt;&#10;&#10;&lt;p&gt;X1 = 22 (18 to 25), &lt;/p&gt;&#10;&#10;&lt;p&gt;X2= 30 (24 to 33) &lt;/p&gt;&#10;&#10;&lt;p&gt;X3= 14 (12 to 16) &lt;/p&gt;&#10;&#10;&lt;p&gt;X4= 10 (8 to 11) &lt;/p&gt;&#10;&#10;&lt;p&gt;X5= 6 (2-7)&lt;/p&gt;&#10;&#10;&lt;p&gt;Now if I wanted to calculate the absolute change with point estimates and 95% CI between the start of the study (i.e for X1) and the end (i.e X5), how do I do this? &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;A similar study have done this and this is how it looks&lt;/strong&gt;, their variables are a1, a2, a3, a4 and a5.&lt;/p&gt;&#10;&#10;&lt;p&gt;This is their data. a1= 4.2 (3.8 to 4.5), &lt;/p&gt;&#10;&#10;&lt;p&gt;a2= 3.7(3.3 to 4.0), &lt;/p&gt;&#10;&#10;&lt;p&gt;a3= 2.4(2.3 to 2.5) &lt;/p&gt;&#10;&#10;&lt;p&gt;a4= 1.9(1.8 to 2.0) &lt;/p&gt;&#10;&#10;&lt;p&gt;a5= 1.5(1.4 to 1.6). &lt;/p&gt;&#10;&#10;&lt;p&gt;Their absolute change was -2.7 (-3.0 to -2.4)&lt;/p&gt;&#10;&#10;&lt;p&gt;I would preciate any help that I can get!&lt;/p&gt;&#10;" ClosedDate="2014-07-22T14:44:20.873" CommentCount="2" CreationDate="2014-07-21T08:34:25.600" Id="108689" LastActivityDate="2014-07-21T10:25:00.150" LastEditDate="2014-07-21T10:25:00.150" LastEditorUserId="52455" OwnerUserId="52455" PostTypeId="1" Score="2" Tags="&lt;confidence-interval&gt;" Title="Confidence interval for absolute changes" ViewCount="25" />
  <row AnswerCount="0" Body="&lt;p&gt;I am new in research and have just started. My anticipated study is on ''determinants of community participation in planning for HIV and AIDS non medical interventions. Now the question is if the model is relevant to my study. The index scale for my dependent variable is 1 and 2 which means low and high participation in planning respectively. There are several factors which are thought to influence the participation positively of negatively.....i.e J=0,1....j. Pls kindly advise if it's relevant to use.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-07-21T10:20:11.350" Id="108693" LastActivityDate="2014-07-21T10:20:11.350" OwnerUserId="52458" PostTypeId="1" Score="0" Tags="&lt;regression&gt;" Title="Ordinal Logistic Model" ViewCount="9" />
  
  
  <row AcceptedAnswerId="108713" AnswerCount="2" Body="&lt;p&gt;I've recently encountered the bivariate Poisson distribution, but I'm a little confused as to how it can be derived.&lt;/p&gt;&#10;&#10;&lt;p&gt;The distribution is given by:&lt;/p&gt;&#10;&#10;&lt;p&gt;$P(X = x, Y = y) = e^{-(\theta_{1}+\theta_{2}+\theta_{0})} \displaystyle\frac{\theta_{1}^{x}}{x!}\frac{\theta_{2}^{y}}{y!} \sum_{i=0}^{min(x,y)}\binom{x}{i}\binom{y}{i}i!\left(\frac{\theta_{0}}{\theta_{1}\theta_{2}}\right)^{i}$&lt;/p&gt;&#10;&#10;&lt;p&gt;From what I can gather, the $\theta_{0}$ term is a measure of correlation between $X$ and $Y$; hence, when $X$ and $Y$ are independent, $\theta_{0} = 0$ and the distribution simply becomes the product of two univariate Poisson distributions.&lt;/p&gt;&#10;&#10;&lt;p&gt;Bearing this in mind, my confusion is predicated on the summation term - I'm assuming this term explains the the correlation between $X$ and $Y$.&lt;/p&gt;&#10;&#10;&lt;p&gt;It seems to me that the summand constitutes some sort of product of binomial cumulative distribution functions where the probability of &quot;success&quot; is given by $\left(\frac{\theta_{0}}{\theta_{1}\theta_{2}}\right)$ and the probability of &quot;failure&quot;  is given by $i!^{\frac{1}{min(x,y)-i}}$, because $\left(i!^{\frac{1}{min(x,y)-i!}}\right)^{(min(x,y)-i)} = i!$, but I could be way off with this.&lt;/p&gt;&#10;&#10;&lt;p&gt;Could somebody provide some assistance on how this distribution can be derived? Also, if it could be included in any answer how this model might be extended to a multivariate scenario (say three or more random variables), that would be great!&lt;/p&gt;&#10;&#10;&lt;p&gt;(Finally, I have noted that there was a similar question posted before (&lt;a href=&quot;http://stats.stackexchange.com/questions/47764/understanding-the-bivariate-poisson-distribution&quot;&gt;Understanding the bivariate Poisson distribution&lt;/a&gt;), but the derivation wasn't actually explored.)&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-07-21T12:16:06.160" FavoriteCount="2" Id="108705" LastActivityDate="2014-07-21T22:45:05.757" LastEditDate="2014-07-21T22:45:05.757" LastEditorUserId="49647" OwnerUserId="9171" PostTypeId="1" Score="7" Tags="&lt;distributions&gt;&lt;mathematical-statistics&gt;&lt;multivariate-analysis&gt;&lt;poisson&gt;&lt;proof&gt;" Title="Deriving the bivariate Poisson distribution" ViewCount="607" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am calculating the R.M.S. of a periodic signal that has a stochastic component to it. &lt;/p&gt;&#10;&#10;&lt;p&gt;For every period I am able to calculate a value for R.M.S. using the following function:&lt;/p&gt;&#10;&#10;&lt;p&gt;$R.M.S. = \sqrt{\frac{1}{N} \sum_{i=1}^N x_i^2 }$,&lt;/p&gt;&#10;&#10;&lt;p&gt;which I understand I can use only if the time series has a mean of zero, in which case the above is equivalent to the standard deviation of that time series.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, I have multiple periods, each of which has a value for R.M.S. Presumably, I can calculate the mean and standard deviation over many periods. Mathematically, what would be the correct way to express this?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-21T15:42:09.370" Id="108730" LastActivityDate="2014-07-21T15:42:09.370" OwnerUserId="40390" PostTypeId="1" Score="0" Tags="&lt;time-series&gt;&lt;standard-deviation&gt;" Title="Standard deviation of multiple root mean square values" ViewCount="39" />
  <row AcceptedAnswerId="108963" AnswerCount="1" Body="&lt;p&gt;I have a data set where many of the actual values are zero, so I can't use MAPE. It's not a time series, so I can't use MASE ala our very own Rob Hyndman.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there another alternative to MAPE that I could use?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-21T16:03:02.857" Id="108734" LastActivityDate="2014-07-23T03:26:34.303" OwnerUserId="46522" PostTypeId="1" Score="0" Tags="&lt;modeling&gt;" Title="Alternative to MAPE when the data is not a time series" ViewCount="27" />
  <row AcceptedAnswerId="108764" AnswerCount="1" Body="&lt;p&gt;I am wondering what approaches are commonly used for validating machine learning models designed for classification or prediction tasks: &lt;/p&gt;&#10;&#10;&lt;p&gt;Approaches that am using at the moment:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Using truth-sets:&lt;/strong&gt; &#10;- ROCs, Bootstrapping, Accuracy, Sensitivity, Specificity, Cross-validation &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Orthogonal validation:&lt;/strong&gt; &#10;- Use a different class of algorithm that can perform prediction or classification task and compare results &lt;/p&gt;&#10;&#10;&lt;p&gt;Any other suggestions ? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-21T16:48:06.757" FavoriteCount="2" Id="108738" LastActivityDate="2014-09-07T14:04:32.433" LastEditDate="2014-09-07T14:04:32.433" LastEditorUserId="529" OwnerUserId="529" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;svm&gt;&lt;random-forest&gt;&lt;validation&gt;" Title="How do you validate your machine learning models?" ViewCount="185" />
  <row AnswerCount="3" Body="&lt;p&gt;What are the available methods/implementation in R/Python to discard/select unimportant/important features in data? My data does not have labels (unsupervised).&lt;/p&gt;&#10;&#10;&lt;p&gt;The data has ~100 features with mixed types. Some are numeric while others are binary (0/1).&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-07-21T17:41:48.213" FavoriteCount="1" Id="108743" LastActivityDate="2014-09-20T13:18:14.310" LastEditDate="2014-08-20T06:44:37.250" LastEditorUserId="183" OwnerUserId="31017" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;feature-selection&gt;&lt;python&gt;" Title="Methods in R or Python to perform feature selection in unsupervised learning" ViewCount="477" />
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;Cross correlation measures the similarity of two signals / images A,B&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Only in a very particular sense. Two things can be highly correlated but very different in size (mean, say) and scale (variation around the mean).&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;I aim at expressing this in the probability that signal A is equal to signal B. &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;There's quite literally no direct relationship here. &lt;/p&gt;&#10;&#10;&lt;p&gt;Consider some $B\geq 0$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now take $A_1=B$. Obviously the correlation is $1$ and $P(A_1=B)=1$ by construction.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now consider $A_2= 12 B + 10000$. The correlation is still 1 but $P(A_2=B)=0$.&lt;/p&gt;&#10;&#10;&lt;p&gt;So correlation is next to useless as an indication of how close to equal two things are. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you want to measure closeness to equality, don't use correlation. (This is an error that seems to come up very frequently, and is the subject of a number of questions on this site.)&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;My suggestion is measure closeness much more directly: something like $\frac{_1}{^n}\sum (A_i-B_i)^2$, perhaps normalized to a &lt;em&gt;common&lt;/em&gt; scale if you wish.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-07-21T22:45:27.017" Id="108766" LastActivityDate="2015-02-18T22:00:57.577" LastEditDate="2015-02-18T22:00:57.577" LastEditorUserId="805" OwnerUserId="805" ParentId="108479" PostTypeId="2" Score="4" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Suppose a data set contains the numbers $1-100$ (i.e. $\{1, \dots, 100 \}$). In $10$-fold cross validation, the data set is divided into $10$ subsets with one used as the validation data set. For example, one such division is $S_1 = \{1, \dots, 10 \}$, $S_2 = \{11, \dots, 20 \}, \dots, S_{10} = \{91, \dots, 100 \}$. &lt;/p&gt;&#10;&#10;&lt;p&gt;In &quot;regular&quot; 10-fold cross validation, each subset is used as the test data set. In 10-repeated 10-fold cross validation (for example), the part that is repeated is how $S_1, \dots, S_{10}$ are constructed?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-21T23:37:41.900" Id="108771" LastActivityDate="2014-07-21T23:37:41.900" OwnerUserId="52429" PostTypeId="1" Score="0" Tags="&lt;cross-validation&gt;" Title="Repeated CV vs. CV" ViewCount="50" />
  
  <row AnswerCount="3" Body="&lt;p&gt;I was reading over Naive Bayes Classification today.  I read, under the heading of &lt;strong&gt;Parameter Estimation with add 1 smoothing&lt;/strong&gt;:  &lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;Let $c$ refer to a class (such as Positive or Negative), and let $w$ refer to a tolken or word.&quot; &lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;The maximum likelihood estimator for $P(w|c)$ is $$\frac{count(w,c)}{count(c)} = \frac{\text{counts w in class c}}{\text{counts of words in class c}}.&quot;$$  &lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;This estimation of $P(w|c)$ could be problematic since it would give us probability $0$ for documents with unknown words. A common way of solving&#10;this problem is to use Laplace smoothing.&quot;   &lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;Let V be the set of words in the training set, add a new element $UNK$ (for unknown) to the set of words.&quot;  &lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;Define $$P(w|c)=\frac{\text{count}(w,c) +1}{\text{count}(c) + |V| + 1},$$  &lt;/p&gt;&#10;&#10;&lt;p&gt;where $V$ refers to the vocabulary (the words in the training set).&quot;  &lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;In particular, any unknown word will have probability  $$\frac{1}{\text{count}(c) + |V| + 1}.&quot;$$  &lt;/p&gt;&#10;&#10;&lt;p&gt;First off, thanks for reading this far.  Second, my question is this:  why do we bother with this Laplacian smoothing at all?  If these unknown words that we encounter in the testing set have a probability that is obviously almost zero, ie, $\frac{1}{\text{count}(c) + |V| + 1}$,   what is the point of including them in the model?  Why not just disregard and delete them?  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-22T04:29:18.017" FavoriteCount="1" Id="108797" LastActivityDate="2014-07-23T09:50:10.333" OwnerUserId="34738" PostTypeId="1" Score="3" Tags="&lt;machine-learning&gt;&lt;text-mining&gt;&lt;naive-bayes&gt;&lt;smoothing&gt;" Title="In Naive Bayes, why bother with Laplacian smoothing when we have unknown words in the test set?" ViewCount="546" />
  <row Body="&lt;p&gt;Moderating variables (aka effect modifier, interacting variable, etc.) &lt;em&gt;alter the effect&lt;/em&gt; of another variable. In your example increased stress causes decreased well-being, but this effect &lt;em&gt;is altered&lt;/em&gt; in the presence of locus of control.&lt;/p&gt;&#10;&#10;&lt;p&gt;Of course, nothing prevents a moderating variable from having its own direct effect (aka main effect). An example here would be respiratory exposure to tobacco smoke, and respiratory exposure to asbestos: tobacco smoke exposure increases risk of mesothelioma; however, the micro-scale mechanical damage wrought by asbestos exposure actually increases the toxic effect of tobacco smoke (asbestos exposure moderates the effect of tobacco smoke exposure on mesothelioma risk), &lt;em&gt;and&lt;/em&gt; asbestos on its own and in the absence of tobacco smoke exposure &lt;em&gt;also&lt;/em&gt; increases risk of mesothelioma. So asbestos exposure increases mesothelioma risk, and moderates the effect of tobacco smoke exposure on same.&lt;/p&gt;&#10;&#10;&lt;p&gt;Mediating effects are different. Mediating effects are &quot;in the middle&quot; of a causal chain from a distal cause to some effect. For example ingesting cyanide causes death (from hypoxia). However, the effect of cyanide ingestion on hypoxia is &lt;em&gt;mediated&lt;/em&gt; by the inactivation of cytochrome oxidase. Put another way, ingestion of cyanide has an effect on cytochrome oxidase (making it inactive), and this inactivation of cytochrome oxydase in turn causes hypoxia.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-22T05:38:41.923" Id="108804" LastActivityDate="2014-07-22T05:38:41.923" OwnerUserId="44269" ParentId="108795" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;I understand your example such that The 2 is the population (&quot;true&quot;) mean. An unbiased estimate of the mean would result in a sampling distribution which is centered about that population value. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, in practice we don't know the population value. What we see is the mean in the sample, and that mean could have any possible value. It is more likely to be near the population mean, but there is no guarantee that that is the case in your particular sample. If you perform many statistical tests (as most of us will do in their lifetime) you will most of the time be near the population mean, but some of the times you will be way off. &lt;/p&gt;&#10;&#10;&lt;p&gt;Taken in isolation, you have no way of knowing when you are reasonably near and when not. However, usually we participate in a debate where multiple people/teams try different ways of answering the same or &quot;sameish&quot; question. In time it will become clear which study is the outlier and which study is not.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;As an aside: You are right that a standard error is the standard deviation of the sampling distribution. However, a standard deviation is not the average deviation from the mean but the square root of the average squared deviations. Since squaring is a non-linear transformation: $\sqrt{\mathrm{E}(dev^2)} \neq \mathrm{E}(dev)$. In fact, in case of the mean we know that $\mathrm{E}(dev)= 0$, and the estimate of central tendency that minimizes $\mathrm{E}(|dev|)$ is the median. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-22T09:03:55.187" Id="108821" LastActivityDate="2014-07-22T09:03:55.187" OwnerUserId="23853" ParentId="108808" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Just to add to what the others said already. &lt;/p&gt;&#10;&#10;&lt;p&gt;I think your confusion is well founded. There is no clear-cut answer to your question as far as I can see. Answering your question depends on your ability to clearly delineate the role locus of attention plays in the connection between stress and well-being. Hypothetical constructs are often too fuzzy for this. The confusion in your example arises because &quot;locus of control&quot; can act as a moderator or a mediator depending on the context. &lt;/p&gt;&#10;&#10;&lt;p&gt;Moderators serve to influence/modify the relationship between two other variables by either amplifying or attenuating their interaction. But it doesn't serve as the &lt;em&gt;necessary&lt;/em&gt; &lt;em&gt;causal mechanism&lt;/em&gt; for their interaction. In your example, locus of control would be a moderator if it influences the relationship between stress and well-being, but isn't a necessary link to establish that relationship. Stress will probably still have an influence on well-being and there are numerous other psychological factors that could influence the relationship between well-being and stress quite independently of locus of control.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Mediating variables, on the other hand, are vital in establishing the relationship because they make the interaction possible; take them away and the interaction stops. &#10;The line is very blurry in your case, but my gut tells me that locus of control masquerades as a mediator because of the phrasing, but it is actually a moderator.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-22T11:55:47.887" Id="108835" LastActivityDate="2014-07-22T11:55:47.887" OwnerUserId="49037" ParentId="108795" PostTypeId="2" Score="0" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I sent a postal questionnaire to 1618 schools. In total, 220 responded (13.6% response rate). For one question, which had Yes, No and Don't know response options, 217 schools responded (Yes=85, No=127, Don't know=5). I'd like to find the margin of error for Yes, No and Don't know (with 95% confidence) so I can work out what the responses for each would have been if all 1618 schools had responded. &lt;/p&gt;&#10;&#10;&lt;p&gt;Do I work out the overall margin of error based on the response rate for the whole questionnaire (i.e., 220 of 1618 schools) and apply the upper and lower % to this particular question, or  do I use the response rate for each answer in this particular question to find out the margin of error? For example, for Yes, should I use 85/1618, 85/220 or 85/217 to work out the margin of error? There is a big difference in results depending on which one I use. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-22T13:53:35.497" Id="108852" LastActivityDate="2014-08-06T14:55:41.050" LastEditDate="2014-07-22T14:44:52.120" LastEditorUserId="7290" OwnerUserId="52539" PostTypeId="1" Score="0" Tags="&lt;survey&gt;&lt;finite-population&gt;&lt;non-response&gt;" Title="Margin of error (questionnaire response)" ViewCount="55" />
  
  
  
  
  
  <row AcceptedAnswerId="108914" AnswerCount="1" Body="&lt;p&gt;I was just reading &lt;a href=&quot;http://www.bayesian-inference.com/bayesfactors&quot;&gt;this article&lt;/a&gt; on the Bayes factor for a completely unrelated problem when I stumbled upon this passage&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Hypothesis testing with Bayes factors is more robust than frequentist hypothesis testing, since the Bayesian form avoids model selection bias, evaluates evidence in favor the null hypothesis, includes model uncertainty, and allows non-nested models to be compared (though of course the model must have the same dependent variable). &lt;strong&gt;Also, frequentist significance tests become biased in favor of rejecting the null hypothesis with sufficiently large sample size.&lt;/strong&gt; [emphasis added]&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I've seen this claim before in &lt;a href=&quot;http://www.fil.ion.ucl.ac.uk/~karl/Ten%20ironic%20rules%20for%20non-statistical%20reviewers.pdf&quot;&gt;Karl Friston's 2012 paper in NeuroImage&lt;/a&gt;, where he calls it the &lt;em&gt;fallacy of classical inference&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;I've had a bit of trouble finding a truly pedagogical account of why this should be true.  Specifically, I'm wondering:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;why this occurs&lt;/li&gt;&#10;&lt;li&gt;how to guard against it&lt;/li&gt;&#10;&lt;li&gt;failing that, how to detect it&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="5" CreationDate="2014-07-22T20:06:58.397" FavoriteCount="15" Id="108911" LastActivityDate="2014-07-27T13:58:04.683" LastEditDate="2014-07-22T20:25:40.260" LastEditorUserId="17054" OwnerUserId="17054" PostTypeId="1" Score="19" Tags="&lt;hypothesis-testing&gt;&lt;frequentist&gt;" Title="Why does frequentist hypothesis testing become biased towards rejecting the null hypothesis with sufficiently large samples?" ViewCount="1104" />
  <row AcceptedAnswerId="108920" AnswerCount="1" Body="&lt;p&gt;I have large correlation matrix in Excel that I'd like to use to inform my choice of explanatory variables in a multiple linear regression model.  One problem is that the initial data was very sparse, and some columns had significantly more zeroes than others. How would I go about choosing the variables with the lowest correlation without falling into the trap of choosing those that have the lowest pairwise correlation solely because they were both very sparse?  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-22T20:39:40.637" Id="108912" LastActivityDate="2014-07-22T21:03:38.400" OwnerUserId="40120" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;correlation&gt;&lt;sparse&gt;" Title="Utilizing A Correlation Matrix Derived from a Sparse Matrix" ViewCount="20" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I was going to build an ANCOVA model with two treatment groups (climbed/unclimbed) and a set of 10 potential environmental covariates which may collectively remove the significance of the climbing treatment, if environmental factors are correlated with the treatment groups (there is uncertainty here because the sample was taken in a heterogenous environment and it is possible that the apparent effect size of climbing on the ecosystem was due to environmental differences and not climbing itself).&lt;/p&gt;&#10;&#10;&lt;p&gt;Specifically, I'm wondering if a preliminary MANOVA of the 10 covariates by treatment group can indicate the potential confounded nature of these two variables (climbing treatment and environment).&lt;/p&gt;&#10;&#10;&lt;p&gt;In such a MANOVA, I found no significance difference in the mean environment by treatment group. &lt;/p&gt;&#10;&#10;&lt;p&gt;My Question: Does this result imply that there is no mathematical possibility that these covariates, when used in an ANCOVA model with the treatment groups, could be confounded with the climbing impact? In other words, their inclusion in the ANCOVA model would not explain mean ecosystem response in the two groups? (thereby negating the statistical significance of climbing impact)&lt;/p&gt;&#10;&#10;&lt;p&gt;I recognize this is not how covariates are typically used in ANCOVA, but that is the goal of my analysis, to more confidently conclude a significant impact of climbing, despite environmental variability (we did do a paired sampling design, but in the field such pairing does not create identical environments for the side-by-side paired climbed/unclimbed sample units).&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you for your time ~ If there is a more elegant way to reach this goal, I'm interested to hear any suggestions.&#10;T&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-22T23:44:45.817" Id="108944" LastActivityDate="2014-07-22T23:44:45.817" OwnerUserId="52585" PostTypeId="1" Score="0" Tags="&lt;ancova&gt;&lt;manova&gt;&lt;paired-comparisons&gt;&lt;paired-data&gt;&lt;covariate&gt;" Title="Seeking covariates that can negate the effect size of treatment groups in an ANCOVA model" ViewCount="34" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a longitudinal data with multiple time points (measured twice every year). There was seasonal difference on the outcome variable of interest (BMIz) if using mixed model (where time points were nested within kids, and kids were nested within schools). &lt;/p&gt;&#10;&#10;&lt;p&gt;I also calculated the percentages of children transitioning between weight categories from assessment to assessment. In that graph, there were clear seasonal differences in the transition percentages. However, if I use 1st order Markov Chain transitional model and with the meaningful categorized outcome variable (healthy weight,  overweight, and obese), there was no seasonal difference on the transition probability matrices. Is Markov Chain use some smoothing function, so the seasonal differences get smoothed out?&lt;/p&gt;&#10;&#10;&lt;p&gt;Does anyone could provide some explanations? I appreciate any inputs. &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-23T03:15:02.780" Id="108961" LastActivityDate="2014-07-23T03:15:02.780" OwnerUserId="48628" PostTypeId="1" Score="0" Tags="&lt;probability&gt;&lt;model&gt;&lt;mixed&gt;" Title="mixed model vs. markov chain" ViewCount="32" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I have data from an experiment in which participants played a repeated game 50 times. That is, I have 50 data points per participant. I want to run a logistic regression and I have of course to take into account that I have data points that are correlated, but I don't know &lt;strong&gt;if I should use repeated measures or if I should work with clustered sample&lt;/strong&gt;. I have found some very clear articles on why clusters are better than other techniques (Williams, 2000; Petersen, 2009), but I have found nothing on repeated measures or on the comparison between the two. I would appreciate it if you could help me on this one!&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-07-23T06:08:13.930" Id="108972" LastActivityDate="2014-07-23T08:19:30.380" LastEditDate="2014-07-23T06:12:59.503" LastEditorUserId="3277" OwnerUserId="52000" PostTypeId="1" Score="1" Tags="&lt;repeated-measures&gt;&lt;cluster-sample&gt;" Title="Repeated measures vs. cluster in logistic regression" ViewCount="52" />
  <row AcceptedAnswerId="108996" AnswerCount="1" Body="&lt;p&gt;I've collected data from my website.&#10;The website is about cars. The data are about user reviews and the cars.&#10;what we see in the graphs is the probability of some car type (Ford Focus 2008) to have &lt;em&gt;X&lt;/em&gt; reviews.&lt;/p&gt;&#10;&#10;&lt;p&gt;The interesting thing is that when I limit the types of cars to a certain % of the cars population, the distribution looks like the Poisson distribution.&#10;To be clear about it:&lt;br&gt;&#10;100% of cars on roads are made of 850 car types (Israel)&lt;br&gt;&#10;99% of cars are only made of 360 car types&lt;br&gt;&#10;95% are 256,&#10;and so on.&lt;/p&gt;&#10;&#10;&lt;p&gt;These are the relevant graphs:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/7ztG2.png&quot; alt=&quot;P(X=k)&quot;&gt;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;You can see the at 80% it looks like Normal Distribution, as expected from high values of Poisson.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ZxBNB.png&quot; alt=&quot;P(X&amp;lt;k)&quot;&gt;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;It looks exactly like the behavior of the &lt;a href=&quot;http://en.wikipedia.org/wiki/Poisson_distribution&quot; rel=&quot;nofollow&quot;&gt;Poisson distribution&lt;/a&gt;. &#10;Why is it, and what is the meaning of lambda (the distribution parameter) here?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-07-23T08:45:48.130" Id="108984" LastActivityDate="2014-07-24T03:37:20.523" LastEditDate="2014-07-23T08:57:45.107" LastEditorUserId="32036" OwnerUserId="52617" PostTypeId="1" Score="2" Tags="&lt;normal-distribution&gt;&lt;poisson&gt;&lt;summary-statistics&gt;" Title="Is it Poisson distributed, and if so, what's its meaning?" ViewCount="91" />
  <row AnswerCount="0" Body="&lt;p&gt;I need to do the similar kind of test as Kruskal-Wallis test, but for a multidimensional vector. Would you please kindly suggest such kind of test or provide some references for N-dimensional case&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-07-23T09:11:33.923" Id="108987" LastActivityDate="2014-07-23T09:11:33.923" OwnerUserId="52623" PostTypeId="1" Score="0" Tags="&lt;hypothesis-testing&gt;&lt;statistical-significance&gt;" Title="Test similar to Kruskal-Wallis test but for multidimensional vector" ViewCount="28" />
  
  
  <row AcceptedAnswerId="109505" AnswerCount="2" Body="&lt;p&gt;This might come as a very trivial thing and way below standards of this group but I am struggling to figure out what do the authors mean when they say that they have used initial level of income in growth regressions? assuming that the first obervation in the sample is 120, is it something like this:&lt;br/&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Year GDP&lt;br/&gt;&#10;1991 120&lt;br/&gt;&#10;1992 120&lt;br/&gt;&#10;1993 120&lt;/p&gt;&#10;&#10;&lt;p&gt;or something like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;Year GDP&lt;br/&gt;&#10;1991 120&lt;br/&gt;&#10;1992 .&lt;br/&gt;&#10;1993 .&lt;/p&gt;&#10;&#10;&lt;p&gt;or is it just the lagged value of levels?&lt;/p&gt;&#10;&#10;&lt;p&gt;an example of this type of models is the following:&lt;/p&gt;&#10;&#10;&lt;p&gt;Benhabib, J., &amp;amp; Spiegel,M.M. (1994). The role of human capital in economic development: Evidence from aggregate cross-country data. Journal of Monetary Economics, 34(2), 143–173.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you very much in advance.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-23T10:43:35.497" Id="109003" LastActivityDate="2014-07-26T20:41:02.387" LastEditDate="2014-07-23T11:09:40.390" LastEditorUserId="44969" OwnerUserId="44969" PostTypeId="1" Score="0" Tags="&lt;modeling&gt;&lt;econometrics&gt;&lt;growth-model&gt;" Title="What does initial level of GDP mean?" ViewCount="247" />
  
  
  
  
  <row Body="&lt;p&gt;I don't think the &quot;confusables&quot; problem is as big as you think it is. Supervized learning algorithms are designed to find the differences between classes, even if the classes have resembling symbols. For instance, the MNIST data, a standard optical recognition dataset used to test many techniques, contains handwritten digits, and to my knowledge none of the standard methods (random forests, boosting, SVM, MLP, deep networks...)  require any specific pre-treatment because of the fact that handwritten &quot;6&quot; resemble &quot;8&quot;. Or &quot;1&quot; and &quot;7&quot;, for that matter.&lt;/p&gt;&#10;&#10;&lt;p&gt;You might get a problem if your classes are of uneven sizes : if you have much more &quot;7&quot; than &quot;1&quot; in your training data, the model might be tempted to class all 7s and 1s as &quot;7&quot; to reduce error. But that is a different question, not directly related to the problem you raised.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-07-23T17:04:52.830" Id="109060" LastActivityDate="2014-07-23T17:04:52.830" OwnerUserId="52432" ParentId="109043" PostTypeId="2" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;February usually has 28 days, unlike it's neighbors January and March which have 31.&lt;/p&gt;&#10;&#10;&lt;p&gt;It seems that most countable things will exhibit, on average, a 10% dip in February for the missing 3 days.&lt;/p&gt;&#10;&#10;&lt;p&gt;This isn't a matter of pure statistics, but it's something I just noticed. Is this known? Is it ever adjusted for charts?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-07-23T17:10:00.820" Id="109061" LastActivityDate="2014-07-23T17:10:00.820" OwnerUserId="52658" PostTypeId="1" Score="3" Tags="&lt;count-data&gt;&lt;histogram&gt;" Title="10% dip in February for metrics that count?" ViewCount="40" />
  <row Body="&lt;p&gt;To me, your question doesn't seem to be a programming question but more of statistics one as to how much to weight good opinions vs bad. Here's how I would approach it:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Use a &lt;a href=&quot;http://en.wikipedia.org/wiki/Cluster_analysis#Algorithms&quot; rel=&quot;nofollow&quot;&gt;clustering algorithm&lt;/a&gt; to classify your reviewers according&#10;to their bias  &lt;/li&gt;&#10;&lt;li&gt;Use a classification algorithm to assign new reviewers to a cluster (eg k-NN)&lt;/li&gt;&#10;&lt;li&gt;Take into account cluster membership when calculating your review score. Note your example assumes assymetric bias - always overestimating the true score of a book. You could have a cluster of people who randomly give reviews wide of the true score in both + and - directions, and so your weighting algorithm could be a regression that excludes this cluster. You would to have look at the residuals to see if your adjustments are appropriate (like in any regression).&lt;/li&gt;&#10;&lt;li&gt;Look at your &lt;a href=&quot;http://en.wikipedia.org/wiki/Lift_%28data_mining&quot; rel=&quot;nofollow&quot;&gt;lift curve&lt;/a&gt; to see if your forecast model is any good&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Or the short answer is to just increase the number of reviewers per title and rely on the central limit theorem.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-07-17T11:09:39.077" Id="109065" LastActivityDate="2014-07-17T11:09:39.077" OwnerDisplayName="Escher" OwnerUserId="48493" ParentId="108295" PostTypeId="2" Score="2" />
  
  
  
  
  
  <row Body="&lt;p&gt;So this doesn't remain unanswered.&lt;/p&gt;&#10;&#10;&lt;p&gt;1) You can't determine that observations &lt;em&gt;are&lt;/em&gt; from a given distribution. &lt;/p&gt;&#10;&#10;&lt;p&gt;2) You may sometimes be able to be pretty confident they aren't - and in some circumstances, completely certain of it (e.g. for a binomial, if n=10, none of 11, -5 or 3.4 are even possible, so if you see them, you can reject), but since something that's not binomial may be arbitrarily close to binomial, no test can tell you that they are certainly binomial&lt;/p&gt;&#10;&#10;&lt;p&gt;Alexis points out the existence of equivalence tests, which could allow you to see whether you have something that's only trivially different ('close enough') from some base case. It might be worth considering these tests, but to my knowledge we don't presently have some nice TOST procedure for our equivalence test for binomialness.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-23T23:59:31.170" Id="109118" LastActivityDate="2014-07-24T22:31:53.430" LastEditDate="2014-07-24T22:31:53.430" LastEditorUserId="805" OwnerUserId="805" ParentId="105679" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm trying to apply one of the Framingham cardiovascular event risk scores to a new dataset in order to get absolute risk. D'Agostino 2013 &quot;Cardiovascular Disease Risk Assessment: Insights from Framingham&quot; says the procedure is&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Set means $M_1,\dots,M_p$ to be the mean of each of your new study risk factors $1,\dots,p$&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Replace the Framingham baseline survival estimate $S_0(t)$ with Kaplan-Meier survival estimate at time t of new study&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The final absolute survival probabilities are $$S(t|X) = S_0(t)^{exp(\beta_1(X_1-M_1)+\dots+\beta_p(X_p-M_p))},$$ X are the risk factors.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;My questions:&lt;/p&gt;&#10;&#10;&lt;p&gt;Doesn't this KM curve essentially assume that the average survival across the entire new cohort is equal to the true baseline risk (assumed here to be the risk for an individual with each risk factor being at average level), which may not be the case. For example, if you have a dataset with lots of people of high HDL, your average in the data won't be the true population-wide average.&lt;/p&gt;&#10;&#10;&lt;p&gt;Would this be a better alternative: use the predicted scores, after subtracting the per risk factor means, to form a single predictor (one vector), then do Cox regression in the new data, and use exp(-hazard) where hazard is from survival::basehaz() to get the baseline survival?&lt;/p&gt;&#10;&#10;&lt;p&gt;Would an offset here be useful, to prevent re-tuning of the predictor in the new data (keep the model honest).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-07-24T02:43:30.087" Id="109135" LastActivityDate="2014-07-24T02:43:30.087" OwnerUserId="10119" PostTypeId="1" Score="0" Tags="&lt;survival&gt;&lt;cox-model&gt;&lt;validation&gt;&lt;calibration&gt;" Title="Applying Cox proportional hazards model to new data to get absolute risk (Framingham risk score)" ViewCount="69" />
  
  <row Body="&lt;p&gt;I always enjoy reading this &lt;a href=&quot;https://en.wikipedia.org/wiki/Sankey_diagram&quot;&gt;Sankey diagram&lt;/a&gt; (a type of &lt;a href=&quot;http://en.wikipedia.org/wiki/Flow_map&quot;&gt;flow map&lt;/a&gt;) on the &lt;a href=&quot;http://en.wikipedia.org/wiki/French_invasion_of_Russia&quot;&gt;French invasion of Russia&lt;/a&gt; by &lt;a href=&quot;http://en.wikipedia.org/wiki/Charles_Joseph_Minard&quot;&gt;Charles Joseph Minard&lt;/a&gt; in 1812:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Charles Joseph Minard's famous graph showing the decreasing size of&#10;  the Grande Armée as it marches to Moscow (brown line, from left to&#10;  right) and back (black line, from right to left) with the size of the&#10;  army equal to the width of the line. Temperature is plotted on the&#10;  lower graph for the return journey (multiply Réaumur temperatures by&#10;  1¼ to get Celsius, e.g. −30 °R = −37.5 °C).&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/French_invasion_of_Russia#mediaviewer/File:Minard.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/O4ETT.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;(click on image to zoom)&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;In 2nd position, this 3D pie makes me laugh each time I see it:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/4X7S5.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;It is the perfect example of how &lt;a href=&quot;http://visualgadgets.blogspot.com/2008/05/misleading-reader-with-pie-charts.html&quot;&gt;misleading&lt;/a&gt; a 3D visualization can be: Steve Jobs clearly used a 3D pie chart to make Apple's market share look much larger than it was:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The 19.5% market share slice for Apple's iPhone somehow looks bigger&#10;  than the 21.2% market share for the mish-mash of &quot;Other&quot; brands.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Same Steve Jobs 3D trick on another slide:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ZSS7i.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2014-07-24T04:15:28.827" CreationDate="2014-07-24T04:15:28.827" Id="109141" LastActivityDate="2014-08-08T03:44:05.927" LastEditDate="2014-08-08T03:44:05.927" LastEditorUserId="12359" OwnerUserId="12359" ParentId="109076" PostTypeId="2" Score="25" />
  
  <row Body="&lt;p&gt;This means that a linear model isn't a good model for your data. You could transform it, for example, take log of the score. Or use another model that takes into account that the scores are all positive.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-24T05:42:31.740" Id="109148" LastActivityDate="2014-07-24T05:42:31.740" OwnerUserId="10119" ParentId="109143" PostTypeId="2" Score="0" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I am using &lt;code&gt;ccf&lt;/code&gt; to find a correlation between 2 time series. I am getting a plot that looks like that:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/S7f65.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that I am mainly interested in correlation for the lag=0. &#10;Questions:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Do interpret it correctly that there is a cross-correlation for the lag=0, as for this lag the cross-correlation is above the dotted line?&lt;/li&gt;&#10;&lt;li&gt;How should I interpret the level of cross-correlation in this example, is this significant (as I interpret it right now, there is a small cross-correlation)?&lt;/li&gt;&#10;&lt;li&gt;How can I extract only &lt;code&gt;acf&lt;/code&gt; value for lag=0? &lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2014-07-24T06:09:44.460" Id="109152" LastActivityDate="2014-12-28T10:08:42.400" OwnerUserId="42246" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;interpretation&gt;&lt;cross-correlation&gt;" Title="Interpreting CCF correlation in R" ViewCount="1276" />
  
  <row Body="&lt;p&gt;The minimal approach for text based features is to used bag of words. Basically the whole data set allows you to define the vocabulary of term used and then each item has a 1/0 value for the presence/absence of each term. This lead to pretty large sparse vector of 0/1 but it's then easy to compute distance using any kind of cosine similarity approach.&lt;/p&gt;&#10;&#10;&lt;p&gt;To improve over the bag of words, you can refine the 0/1 using term frequency (or even better tf/idf).&lt;/p&gt;&#10;&#10;&lt;p&gt;At the end, you will still need to aggregate the distance computed over the text data with the other features. Since the range and dynamic might be very different over these feature distances, it might be tricky. Try to start with basic linear combination (weighted sum). If not to clumsy, the weights night be easy to fix. If not, then you can try optimization approach to approximate good values.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-07-24T09:04:25.487" Id="109165" LastActivityDate="2014-07-24T09:04:25.487" OwnerUserId="49609" ParentId="109162" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;The expected degree correlation of (i.e., the assortativity of the node degrees) in an ER network is 0. &lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;In the ER model, since edges are placed at random without regard to vertex degree, it follows that r = 0 in the limit of large graph size.&quot; - &lt;a href=&quot;http://en.wikipedia.org/wiki/Assortativity&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Assortativity&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-24T10:19:53.680" Id="109178" LastActivityDate="2014-07-24T10:19:53.680" OwnerUserId="52701" ParentId="86565" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="109200" AnswerCount="1" Body="&lt;p&gt;I am analyzing an experiment comparing the effect of treatment A vs. B on the matched subject. Here are the measurements on 34 subjects:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; A     B&#10;-1.15 -1.16&#10;-1.13 -0.94&#10;-0.16 -1.18&#10;-0.37 -1.20&#10;-1.09 -1.20&#10;-1.20 -1.20&#10;-0.94 -1.20&#10;-0.84 -1.16&#10;-1.18 -1.17&#10;-1.20 -1.11&#10;-0.78 -0.68&#10;-0.83 -0.73&#10;-1.05 -1.20&#10;-0.71 -1.20&#10; 0.07  0.12&#10;-1.20 -0.98&#10;-1.20 -1.20&#10;-1.02 -1.17&#10;-0.28 -0.84&#10; 1.33  1.47&#10;-1.19 -1.20&#10;-1.20 -1.17&#10;-0.40 -1.20&#10; 0.66 -0.21&#10;-0.63  0.21&#10;-0.88 -1.16&#10;-0.46 -1.20&#10;-0.76 -1.20&#10;-0.38 -1.20&#10;-0.67 -0.97&#10;-0.90 -1.20&#10;-0.90 -1.20&#10;-1.20 -1.15&#10;-1.01 -0.79&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The differences between the two treatment (dat[,&quot;A&quot;]-dat[,&quot;B&quot;]) looks normally distributed. I first applied a paired t-test:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;t.test(dat$A, dat$B, alternative = c(&quot;two.sided&quot;), mu = 0, paired = TRUE)&#10;&#10;Paired t-test&#10;data:  dat$A and dat$B &#10;t = 2.894, df = 33, p-value = 0.006692&#10;alternative hypothesis: true difference in means is not equal to 0 &#10;95 percent confidence interval:&#10; 0.05870022 0.33659390 &#10;sample estimates:&#10;mean of the differences &#10;              0.1976471 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The paired t-test indicates that on average treatment A has a significantly higher measurement than treatment B.&lt;/p&gt;&#10;&#10;&lt;p&gt;On the other hand, I applied a linear model on A~B:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;mod1 &amp;lt;- lm(A ~ B, data=dat)&#10;&#10;Coefficients:&#10;        Estimate Std. Error t value Pr(&amp;gt;|t|)    &#10;(Intercept) -0.03345    0.12739  -0.263    0.795    &#10;B            0.75111    0.11801   6.365 3.79e-07 ***&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The 95% confidence interval of the coefficient for B (0.51-0.99) does not cover 1. This result indicates that treatment A has on average a smaller measurement than treatment B, which is contradictory to the findings from the t-test. &lt;/p&gt;&#10;&#10;&lt;p&gt;Can anyone help me to explain these contradictory findings?&lt;/p&gt;&#10;&#10;&lt;p&gt;and to expand my question: Does a paired t-test (test mean of difference against 0) equal to a linear regression without intercept (test the coefficient against 1)? I mean in terms of testing against the null hypothesis, rather than the estimate or the meaning of the coefficient. Because both tests are testing against the null hypothesis that $B_{i}-A_{i}=\epsilon_{i}$.&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2014-07-24T11:52:47.263" Id="109191" LastActivityDate="2014-07-24T14:16:59.163" LastEditDate="2014-07-24T12:13:55.223" LastEditorUserId="13702" OwnerUserId="13702" PostTypeId="1" Score="0" Tags="&lt;t-test&gt;&lt;linear-model&gt;" Title="Inconsistent x-y relationship from paired t-test and linear regression" ViewCount="94" />
  <row AnswerCount="0" Body="&lt;p&gt;I am a beginner in Machine Learning. For my project I need a regression algorithm that can estimate the 3D position of a device based on some constraints (moreover inputs). I know how to implement regression algorithms (using gradient descent) for scalar outputs. But in this case the output is a position vector. Can someone provide me any material or book chapter. Any quick theoretical reference?&lt;/p&gt;&#10;&#10;&lt;p&gt;Regards.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-07-24T12:32:51.223" Id="109195" LastActivityDate="2014-07-24T21:33:52.747" LastEditDate="2014-07-24T20:35:48.160" LastEditorUserId="52713" OwnerUserId="52713" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;machine-learning&gt;&lt;nonlinear-regression&gt;" Title="how to implement linear or non linear regression for 3d position estimation?" ViewCount="25" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am running an analysis where I have 2500 cases and 2500 controls. The cases have disease A, and the controls do not. I am trying to see if having disease A increases the odds of various diseases. For the sake of simplicity, we can focus on one disease, call it disease B.&lt;/p&gt;&#10;&#10;&lt;p&gt;D = 1 if disease B present, 0 otherwise&lt;/p&gt;&#10;&#10;&lt;p&gt;E = 1 if disease A present, 0 otherwise&lt;/p&gt;&#10;&#10;&lt;p&gt;I am also including in the model a measure of healthcare utilization. &lt;/p&gt;&#10;&#10;&lt;p&gt;F is a positive integer proportional to an individual's utilization of healthcare.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am running the logistic regression model as such in R:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;glm(D ~ E + F, family = &quot;binomial&quot;) &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now, this works fine. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, when I try to run conditional logistic regression, it gives me an error:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(survival)&#10;clogit(D ~ E + F, strata(matched.pairs))&#10;Error in fitter(X, Y, strats, offset, init, control, weights = weights,  :&#10;  NA/NaN/Inf in foreign function call (arg 5)&#10;In addition: Warning message:&#10;In fitter(X, Y, strats, offset, init, control, weights = weights,  :&#10;  Ran out of iterations and did not converge&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I have tried different strata, including dividing the individuals into quantile bins based on F. It does not seem to change anything. (note: pairs are matched on age, gender, race, and F)&lt;/p&gt;&#10;&#10;&lt;p&gt;This occurs only when I run it on a larger sample size. I ran this same analysis on a sample size of 200 (100 cases and 100 controls) and it worked fine. When I use a sample size of 5000, I get the above error. &lt;/p&gt;&#10;&#10;&lt;p&gt;I also made sure that at least 10 cases and 10 controls had the disease in question (disease B, for this example). &lt;/p&gt;&#10;&#10;&lt;p&gt;I am not sure why logistic regression runs fine when conditional logistic regression does not. Can anyone offer me any advice?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you all in advance.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-07-24T15:21:27.037" Id="109222" LastActivityDate="2014-07-24T15:41:23.353" LastEditDate="2014-07-24T15:41:23.353" LastEditorUserId="52726" OwnerUserId="52726" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;regression&gt;&lt;logistic&gt;" Title="Conditional logistic regression model does not converge but logistic regression model does" ViewCount="174" />
  
  
  
  
  <row Body="&lt;p&gt;I agree with the other answers. I would just emphasize that one common way (at least in the US) for people like you who hesitate between continuing with a PhD or doing the industry after their undergrad degrees is to apply for PhD, then take a leave (one year or more) if things aren't as great as they expected or simply want to explore industry. It is generally easier to apply for PhD right after undergrad: you haven't forgotten yet the habit to cram exams (GRE), professors who are going to write recommendation letters for you still remember you well, etc.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, in your comparison between PhD and industry, amongst the opportunities you have, you might want to compare the access to interesting datasets, computer cluster availability, software engineering skills of the place and how many people are assigned for each project.&lt;/p&gt;&#10;&#10;&lt;p&gt;Lastly, you can find a lot of intellectually challenging stuff in the industry as well, e.g. check out IBM/Google/Microsoft/Nuance/Facebook/etc. research department (just like you can find a lot of intellectually unchallenging stuff academia). E.g. the folks behind SVM were working at AT&amp;amp;T, IBM Watson is at IBM, Google Translate is one of the best machine translation system, Nuance and Google have the top voice recognition system, and those are very far from isolated examples. In fact I've always wondered who among industry and academia contribute the most toward machine learning research (I had asked the same question regarding the database research on Quora: &lt;a href=&quot;https://www.quora.com/Database-Systems/Has-database-research-been-mostly-driven-by-the-industry-over-the-last-decade?share=1&quot; rel=&quot;nofollow&quot;&gt;Has database research been mostly driven by the industry over the last decade?&lt;/a&gt;).&lt;/p&gt;&#10;" CommentCount="1" CommunityOwnedDate="2014-07-24T18:11:42.990" CreationDate="2014-07-24T18:11:42.990" Id="109246" LastActivityDate="2014-07-24T18:26:46.890" LastEditDate="2014-07-24T18:26:46.890" LastEditorUserId="12359" OwnerUserId="12359" ParentId="109210" PostTypeId="2" Score="2" />
  <row AnswerCount="1" Body="&lt;p&gt;I'm analyzing a simple linear regression $Y_{i}$~$a+b*X_{i}+e_{i}$, with $e$ being normally distributed with known variance and where I have normal priors on $a$ and $b$. I'm trying to piece together an approximate closed form solution for the variance of my posterior of $a$ as a function of sample size for the purposes of an optimization I'm trying to do. &lt;/p&gt;&#10;&#10;&lt;p&gt;I know that in general, there's a closed form conjugate prior for Bayesian regression in the multivariate case in matrix form. So I can do some sort of moment matching and then calculate out the matrix algebra for the bivariate case and probably can end up with a nice looking closed form. I've tried that, but I keep getting bogged in algebra, and I feel like this has to be a well-known problem that's been fleshed out already. Does anybody have a citation or link to point me to?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-07-24T18:39:11.287" FavoriteCount="1" Id="109251" LastActivityDate="2014-07-24T22:33:32.220" LastEditDate="2014-07-24T20:45:20.953" LastEditorUserId="996" OwnerUserId="996" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;bayesian&gt;&lt;linear-model&gt;" Title="Closed form posteriors for a simple bivariate Bayesian regression" ViewCount="86" />
  <row Body="&lt;p&gt;I'm not quite sure I understand your approach or the problem you are trying to solve.&lt;/p&gt;&#10;&#10;&lt;p&gt;Seems like the only way your &quot;hot streak&quot; effect can exist is if players change their strength level overtime (meaning you can't just characterize their strength by taking an average of their strength). Is this the question you are really asking? Otherwise you need to define the &quot;hot streak&quot; more.&lt;/p&gt;&#10;&#10;&lt;p&gt;However one thing to note is that you can absolutely extend logistic regression to the multivariate case, so you may not have to do the &quot;difference in strengths&quot; thing. You can actually just have the two strengths as separate parameters.&lt;/p&gt;&#10;&#10;&lt;p&gt;Logistic regression is really just a form of linear regression. You are finding a function that takes in inputs &lt;code&gt;x&lt;/code&gt;, and predicts the output &lt;code&gt;P&lt;/code&gt;. The output &lt;code&gt;P&lt;/code&gt; represents the two outcomes, win and lose. The logistic function takes the form: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;P=\frac{1}{1+e^{-\omega x + c}}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;If you extend it the multinomial case, &lt;code&gt;x&lt;/code&gt; is just a vector of parameters and you are looking for the weights &lt;code&gt;\omega&lt;/code&gt; and &lt;code&gt;c&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;It seems to me like you need to do the following:&lt;/p&gt;&#10;&#10;&lt;p&gt;1) Train using existing statistics about strengths of the two players and the outcome of the game to find your &lt;code&gt;\omega&lt;/code&gt; and &lt;code&gt;c&lt;/code&gt;. You can add more parameters you want even stuff like weather, etc. if you have enough of it&lt;/p&gt;&#10;&#10;&lt;p&gt;2) Use your random generator to find the strengths of your two players. Not sure if this distribution should be completely normal, since the strength distribution of tennis players is not normal (you have a bunch of people who are mediocre and then a few extraordinary players and not really any terrible players). Then use your logistic function to simulate outcomes of the games.&lt;/p&gt;&#10;&#10;&lt;p&gt;3) Record the number of hotstreaks&lt;/p&gt;&#10;&#10;&lt;p&gt;If you provide more information on where you are confused I can try to help more.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2014-07-24T19:10:57.137" Id="109255" LastActivityDate="2014-07-24T19:10:57.137" OwnerUserId="52740" ParentId="109249" PostTypeId="2" Score="1" />
  
  <row Body="&lt;ol&gt;&#10;&lt;li&gt;If it is indeed Cohen's effect size, you can get the SD by computing the difference between the means and dividing it by the effect size. &lt;/li&gt;&#10;&lt;li&gt;Otherwise, you do need the sample size also. If it is $n$ observations per group, then the SD is $($(diff of means)$/t)\times\sqrt{n/2}$&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Now that you have the SD, &lt;strong&gt;forget you ever saw any of those past results&lt;/strong&gt;. The goal is to power your future study to meet its scientific objectives, not to replicate the old one. So examine (and discuss with others) the scientific goals of the study: How big a difference would be considered important, from a scientific perspective? Use that as the target value for $\mu_1-\mu_2$ in your sample-size calculator. (Your calculator asks for both means, so put in two values that differ by this amount).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-07-24T20:45:13.797" Id="109269" LastActivityDate="2014-07-24T20:45:13.797" OwnerUserId="52554" ParentId="109263" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;Please, take a look at Section 3.2.6 of the following document:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.biostat.uzh.ch/teaching/master/previous/seminarbayes/SimonKunz_article.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.biostat.uzh.ch/teaching/master/previous/seminarbayes/SimonKunz_article.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;There, you will find the posterior of $(a,b)$ (which is a bivariate normal distribution in your case with a certain known variance structure) from where you can extract the marginal distributions, which are normal, and derive the corresponding variances.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, page 19 of the following document:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://fisher.osu.edu/~schroeder.9/AMIS900/ech6.pdf&quot; rel=&quot;nofollow&quot;&gt;http://fisher.osu.edu/~schroeder.9/AMIS900/ech6.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Bürgin, Reto: Bayesian linear model - Basics, Part 1. 2009&lt;/p&gt;&#10;  &#10;  &lt;p&gt;Gelman et al.: Bayesian data analysis, third edition.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="2" CreationDate="2014-07-24T22:19:52.910" Id="109277" LastActivityDate="2014-07-24T22:33:32.220" LastEditDate="2014-07-24T22:33:32.220" LastEditorUserId="52761" OwnerUserId="52761" ParentId="109251" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Try this toy example&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;conversion_set &amp;lt;- data.frame(Country=c(&quot;United States&quot;, &quot;Canada&quot;, &quot;Mexico&quot;, &#10;                             &quot;Guatemala&quot;, &quot;Belize&quot;, &quot;Honduras&quot;), &#10;                             Var1=c(10,  5, 65, 10, 40, 70),&#10;                             Var2=c(20, 30, 60, 80, 25, 90) ) &#10;numbers_only &amp;lt;- conversion_set[,-1]&#10;rownames(numbers_only) &amp;lt;- conversion_set[,1]&#10;# Applying Ward Hierarchical Clustering&#10;d   &amp;lt;- dist(numbers_only, method=&quot;euclidean&quot;)&#10;fit &amp;lt;- hclust(d, method=&quot;ward&quot;)&#10;plot(fit)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;which puts Belize closer to the United States and Canada than Guatemala is, and also puts Mexico and Honduras closer together than to Guatemala, as in &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/AdykN.png&quot; alt=&quot;enter image description here&quot;&gt;  &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-07-24T22:22:58.367" Id="109278" LastActivityDate="2014-07-24T22:22:58.367" OwnerUserId="2958" ParentId="109273" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;A large part of the problem seems to be that the data structure is not amenable to analysis. Is there any reason you can't collapse the data into one record per person-year (e.g you're using more than just the variables from the subrecords)? The result may have a heck of a lot of indicator variables, but it would make the analysis a lot easier.&lt;/p&gt;&#10;&#10;&lt;p&gt;Due to the different requirements of the data at various stages at my company, I've had to collapse data in that manner countless times.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-07-24T23:04:32.890" Id="109281" LastActivityDate="2014-07-24T23:04:32.890" OwnerUserId="46522" ParentId="108201" PostTypeId="2" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;This is something I have been thinking about for sometime. Consider the random effects model&lt;/p&gt;&#10;&#10;&lt;p&gt;$y= Zu + e$ &lt;/p&gt;&#10;&#10;&lt;p&gt;where $u \sim N(0, \sigma^{2}I)$ and $ e \sim N(0, \epsilon^{2}I)$&lt;/p&gt;&#10;&#10;&lt;p&gt;I now want to compare 2 models. In the first model I use an overfit matrix $Z_{1}$ in which one singular value is large and the rest are close to zero. In the second model I use a matrix $Z_{2}$ which is a rank-1 approximation of $Z_{1}$ (i.e. de-noised). &lt;/p&gt;&#10;&#10;&lt;p&gt;Intutively speaking, how would we expect the estimate of the means and standard errors of Variance($Z_{1}u_{1} + e_{1}$) and Variance($Z_{2}u_{2} + e_{2}$) to compare. Is there some paper which makes this hand wavy argument concrete?&lt;/p&gt;&#10;&#10;&lt;p&gt;Although this is a little far fetched, I have been thinking if the estimates of $u$ show something equivalent to the bias-variance tradeoff seen in linear regression&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-07-25T01:54:29.437" Id="109295" LastActivityDate="2014-07-25T04:37:11.977" LastEditDate="2014-07-25T04:37:11.977" LastEditorUserId="49398" OwnerUserId="49398" PostTypeId="1" Score="0" Tags="&lt;mixed-effect&gt;&lt;lme&gt;&lt;intuition&gt;" Title="Intuitive feel in mixed effect models" ViewCount="46" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Suppose I have 64 columns that I have chosen out of 500+ columns based on the fact that they have the highest pairwise correlation (is this a good way?).  I take 16 of these columns and run a simple multi-variate regression.  I find that 4 of these columns have P values less than .05.  Can I go back, do this for the next set of 16 (mutually exclusive from the first set), check the P values, and add the successful columns to the model without issue?  Moreover, is it okay to only pick the four mutually exclusive sets of 16 columns, or would I have to actually pick 64 choose 16 times and find commonalities between each set?  &lt;/p&gt;&#10;&#10;&lt;p&gt;I have a feeling there is something wrong with the way I am doing the above, so I hope someone can shed some light on the proper way to go about this.  As well, please let me know if there is anything I can do to improve this question.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-07-25T13:04:23.880" Id="109351" LastActivityDate="2014-08-21T00:27:06.703" LastEditDate="2014-08-21T00:27:06.703" LastEditorUserId="40120" OwnerUserId="40120" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;correlation&gt;&lt;model-selection&gt;&lt;p-value&gt;" Title="Iterative Addition of Variables to Model Based on P Value" ViewCount="43" />
  
  
  
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;One of the assumptions of the Mann Whitney test U test is that the two variables must not be normally distributed. &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;This assertion is quite false. Not only is that &lt;em&gt;not&lt;/em&gt; an assumption of the Mann-Whitney, it's actually a very good test at the normal, having an asymptotic relative efficiency of $3/\pi$ - around 95.5%. Do you have a book or other reference that makes the claim here?&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The nutrient intake of males is normal &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I doubt you have any basis on which to assert this as a fact. I doubt it's true. What makes you think it is? &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Would this be a problem?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;If the distribution shapes differ, it &lt;em&gt;might&lt;/em&gt; be a problem, depending on how you wanted to interpret the test results.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Also, the data is on intake of seven nutrients. &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;This is very important. You should probably lead with it.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;To do the comparison wouldn't I have to do the Mann whitney test 7 times? Is there any other alternative?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;One of the issues with doing multiple univariate tests is that you can easily miss clear changes in the multivariate distribution that are not strong on any one variable. Which you do depends on which questions are important to answer.&lt;/p&gt;&#10;&#10;&lt;p&gt;How non-normal are the data? What's the dependence structure like?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-07-25T15:52:27.143" Id="109373" LastActivityDate="2014-07-26T01:06:13.117" LastEditDate="2014-07-26T01:06:13.117" LastEditorUserId="805" OwnerUserId="805" ParentId="109368" PostTypeId="2" Score="9" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;This is a very basic question about cross-validation. Say that I have a sample size of 2901(or any difficult to divide number). How do I split this up into &lt;strong&gt;equal&lt;/strong&gt; partitions (other than n=1)? And how big should I make each partition?&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, if I make each partition size 300 (which gives me approximately 10 partitions), I will have some data points that are in more than one partition, giving it an unfair weight. Is this acceptable/what do people normally do about this?&lt;/p&gt;&#10;&#10;&lt;p&gt;By the way, I wanted to split it into equal partitions so that I can easily write code that will perform cross validation for any number of partitions.&lt;/p&gt;&#10;&#10;&lt;p&gt;Background: this is going to be used for SVM&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-25T17:42:35.353" Id="109382" LastActivityDate="2014-07-26T01:59:55.057" LastEditDate="2014-07-26T01:59:55.057" LastEditorUserId="52740" OwnerUserId="52740" PostTypeId="1" Score="3" Tags="&lt;machine-learning&gt;&lt;svm&gt;&lt;cross-validation&gt;&lt;libsvm&gt;&lt;partitioning&gt;" Title="Assigning even partitions for Cross-Validation" ViewCount="81" />
  
  <row Body="&lt;p&gt;Going to preface my answer with this: the procedure I describe below is NOT a valid model selection procedure... It is an exploratory tool.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;How do I choose which to apply in my regression formula?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Unless there is reasoning based on findings in your field, usually the analyst can only guess what relationships may exist. One way to do this is to throw all sorts of non-linear transformations into the regression model to see how well they do in explaining the output variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, &lt;strong&gt;plotting the data&lt;/strong&gt; can be extremely useful here. Consider a model given precisely by&#10;$$&#10;Y \sim \mathcal{N}(\mu, \sigma^2),\\&#10;\mu = \beta_0 + \beta_1X_1 + \beta_2 X_2 + \beta_3\log(X_3).&#10;$$&#10;If we suspect there is a nonlinear transform for $X_3$ that will help us model $Y$, then we can run a regression on $Y$ against just $X_1, X_2$ and look at the residuals. The residuals are the errors of the model (i.e., $y - \hat y$), so they represent how far off just using $X_1, X_2$ is. In this case, we would expect $y - \hat y \propto \log(X_3)$ because of the formula for $\mu$:&#10;$$&#10;\mu = \beta_0 + \beta_1X_1 + \beta_2 X_2 + \beta_3\log(X_3) \implies \mu - (\beta_0 + \beta_1X_1 + \beta_2 X_2) = \beta_3\log(X_3)&#10;$$&#10;Plotting the residuals, we observe a log trend in $X_3$.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/CfMpm.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;So we plot $\log(X_3)$ against the residuals and get a clearly linear trend.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/d141E.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Just plotting $X_3$ against $Y$ might not be that useful, since we wouldn't necessarily be able to uncover this trend without controlling for $X_1, X_2$. This approach allows you a better chance to uncover $X_3$'s involvement in predicting $Y$.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-07-25T18:27:23.213" Id="109388" LastActivityDate="2014-07-25T19:12:36.613" LastEditDate="2014-07-25T19:12:36.613" LastEditorUserId="44764" OwnerUserId="44764" ParentId="109387" PostTypeId="2" Score="3" />
  
  
  <row Body="&lt;p&gt;&lt;strong&gt;Pellagra&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;According to &lt;a href=&quot;http://psych.unl.edu/psycrs/450/e3/chapt/stanovich_c5.pdf&quot; rel=&quot;nofollow&quot;&gt;this book chapter&lt;/a&gt;, &lt;a href=&quot;http://en.wikipedia.org/wiki/Pellagra&quot; rel=&quot;nofollow&quot;&gt;pellagra&lt;/a&gt;, a disease characterized by dizziness, lethargy, running sores, vomiting, and severe diarrhea that had reached epidemic proportions in the US South by the early 1900s, was widely attributed to an unknown pathogen on the basis of a correlation with unsanitary living conditions. Dr. Joseph Goldberger was instrumental in showing experimentally that the disease was, in fact, caused by a poor diet, which (along with unsanitary living conditions) stemmed from widespread poverty in the postbellum South. His work was largely ignored until the late 1930s, when researchers finally proved that the disease was caused by a lack of niacin.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Ocular Literacy Training&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;From &lt;a href=&quot;http://psych.unl.edu/psycrs/450/e3/chapt/stanovich_c5.pdf&quot; rel=&quot;nofollow&quot;&gt;the same source&lt;/a&gt; - a correlation between reading (in)ability and erratic eye movements during reading was taken as evidence of a causal relationship &lt;em&gt;in the wrong direction&lt;/em&gt;, and &quot;eye movement training programs&quot; were implemented to improve literacy. These were ineffective, and later work showed that causality runs in the opposite direction; reading difficulties lead to the regressions and fixations observed in poor readers.&lt;/p&gt;&#10;" CommentCount="3" CommunityOwnedDate="2014-07-30T20:10:12.250" CreationDate="2014-07-25T20:21:05.757" Id="109395" LastActivityDate="2014-07-25T20:38:57.403" LastEditDate="2014-07-25T20:38:57.403" LastEditorUserId="52673" OwnerUserId="52673" ParentId="109129" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Perhaps some sort of multivariate outlier test is needed, but in this context it seems reasonable to me to think of trying to predict the 2012 indicators, $y$, based on the 1996 indicators, $x$ -- i.e., putting the data in roles of dependent and independent variables. If you fit a linear regression line using software that can compute the &quot;$t$ residuals&quot;, AKA &quot;Studentized deleted residuals&quot;, then that is the formal way of testing for an outlier in the $y$ direction. A absolute $t$ residual greater than $t_{\alpha/2,d}$ would be ``significant,'' where $d$ is the error degrees of freedom.&lt;/p&gt;&#10;&#10;&lt;p&gt;Another way to do the above is to fit a regression model with $y$ as the response and predictors $x$ and $I_i$, where $I_i$ is an indicator variable for the $i$th observation ($1$ for the $i$th observation, $0$ for all others). Then in the table of coefficients, the $t$ statistic for $I_i$ is the negative of the $i$th $t$ residual. This approach also reveals the substance of what is being done: this $t$ statistic measures the significance of accounting for the $i$th observation separately, over and above what is explained by $x$.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-25T21:03:05.850" Id="109400" LastActivityDate="2014-07-25T21:03:05.850" OwnerUserId="52554" ParentId="109393" PostTypeId="2" Score="0" />
  
  <row AnswerCount="2" Body="&lt;p&gt;Let ${X_t}$, $t=...-2,-1,0,1,2...$ be a stochastic process that satisfies:&lt;/p&gt;&#10;&#10;&lt;p&gt;$X_t=\rho X_{t-1}+\varepsilon_t$&lt;/p&gt;&#10;&#10;&lt;p&gt;with $|\rho|&amp;lt;1$ and $\varepsilon_t$ is a white noise. In that case, we also know that there is a $\beta$ such that $|\beta|&amp;gt;1$ and another white noise $\eta_t$ such that ${X_t}$ also satisfies:&lt;/p&gt;&#10;&#10;&lt;p&gt;$X_t=\beta X_{t-1}+\eta_t$&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose I have some observations of $X_t$. If I run OLS by data generated by those equivalent process, which one should I expect? Why?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-07-25T21:54:05.457" FavoriteCount="1" Id="109407" LastActivityDate="2014-07-26T01:12:38.783" LastEditDate="2014-07-26T01:12:38.783" LastEditorUserId="805" OwnerUserId="52826" PostTypeId="1" Score="1" Tags="&lt;estimation&gt;&lt;least-squares&gt;&lt;stochastic-processes&gt;" Title="Why representation of AR process comes up in estimation" ViewCount="42" />
  
  <row Body="&lt;p&gt;At that sort of sample size and probability, the C-test should be okay.&lt;/p&gt;&#10;&#10;&lt;p&gt;Since this is just a binomial test, you can test it using a binomial test in &lt;a href=&quot;http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.binom_test.html&quot; rel=&quot;nofollow&quot;&gt;Scipy&lt;/a&gt;. &lt;code&gt;x&lt;/code&gt; is 127, &lt;code&gt;n&lt;/code&gt; is 127+316 and &lt;code&gt;p&lt;/code&gt; is 35465/(35465+79076).&lt;/p&gt;&#10;&#10;&lt;p&gt;There are other tests for this situation. Vanilla R offers an exact Poisson test for example.&lt;/p&gt;&#10;&#10;&lt;p&gt;See also:&lt;/p&gt;&#10;&#10;&lt;p&gt;Krishnamoorthy K. and J. Thomson (2004),&lt;br&gt;&#10;&quot;A more powerful test for comparing two Poisson means,&quot;&lt;br&gt;&#10;&lt;em&gt;Journal of Statistical Planning and Inference&lt;/em&gt;, &lt;strong&gt;119&lt;/strong&gt;, pp 23–35&lt;/p&gt;&#10;&#10;&lt;p&gt;which indicates that an unconditional test will tend to have greater power (as is generally the case for unconditional tests in this sort of situation). &lt;/p&gt;&#10;&#10;&lt;p&gt;It is, of course, not exact.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;In response to comments:&lt;/p&gt;&#10;&#10;&lt;p&gt;You're now taking $X$ to be binomial, not Poisson. &lt;/p&gt;&#10;&#10;&lt;p&gt;I followed your assertion that $X$ was Poisson (by which the 35465 and 79076 are simply exposure) and showed how to do the corresponding C-test. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you want to treat the 35465 &amp;amp; 79076 as numbers of trials you don't need the C-test at all. You just do a straight two-sample binomial test on the trials and successes you have.&lt;/p&gt;&#10;&#10;&lt;p&gt;Like so (this is in R):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; prop.test(x=c(127,316),n=c(35465,79076 ))&#10;&#10;    2-sample test for equality of proportions with continuity correction&#10;&#10;data:  c(127, 316) out of c(35465, 79076)&#10;X-squared = 0.9902, df = 1, p-value = 0.3197&#10;alternative hypothesis: two.sided&#10;95 percent confidence interval:&#10; -0.0011970592  0.0003667388&#10;sample estimates:&#10;     prop 1      prop 2 &#10;0.003580995 0.003996156 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Incidentally, this p-value is very similar to what you get with the C-test. &lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-07-26T03:28:58.067" Id="109423" LastActivityDate="2014-07-29T01:21:11.650" LastEditDate="2014-07-29T01:21:11.650" LastEditorUserId="805" OwnerUserId="805" ParentId="109402" PostTypeId="2" Score="2" />
  <row AnswerCount="1" Body="&lt;p&gt;Respected  Fellows.&#10;I will thankful if someone help me to explain my model results.my model is as follows.&#10;Yit=αPFit+βPSit+δ (PF*PS) it+εit&#10;Where Y is GDP per capita &#10;PF=Political Freedom Index ranges from (0-10)&#10;PS is political stability Index ranges from (0-10)&#10;PF*PS=interaction term &#10;I=country and t represent time period.&#10;I know how to interpret the results of two continuous variables but I am confused how to interpret the results of two indexes interaction term.(for purpose of easiness i exclude other control variables here)&lt;/p&gt;&#10;&#10;&lt;p&gt;My estimated model results as follows.&#10;Y=  -0.009+0.009PF+0.015PS-0.001(PF*PS)&#10;Regard&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-26T08:41:05.267" Id="109435" LastActivityDate="2014-07-26T10:27:53.180" OwnerUserId="17428" PostTypeId="1" Score="0" Tags="&lt;multiple-regression&gt;&lt;interaction&gt;&lt;interpretation&gt;&lt;regression-coefficients&gt;" Title="Interpretation of two indexes Interaction Term" ViewCount="19" />
  <row Body="" CommentCount="0" CreationDate="2014-07-26T11:24:59.290" Id="109441" LastActivityDate="2014-07-26T11:24:59.290" LastEditDate="2014-07-26T11:24:59.290" LastEditorUserId="-1" OwnerUserId="-1" PostTypeId="5" Score="0" />
  
  <row Body="&lt;p&gt;Let's simplify the notation and write $e(n)$ for the expectation of the maximum of $n$ iid Normal$(0,1)$ variables $X_1, X_2, \ldots, X_i, \ldots, X_n$, with $n$ arbitrary.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;The first claim&lt;/strong&gt; is that $e$ is monotonically increasing; that is, $e(m) \lt e(n)$ whenever $0\lt m\lt n$.  &lt;strong&gt;This is immediate&lt;/strong&gt; from two simple observations:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;$\max\{X_1, \ldots, X_m\} \le \max\{X_1,\ldots, X_m,\ldots, X_n\}$ (implying $e(m)\le e(n)$) and&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;There is a positive chance that the maximum of all $n$ of the $X_i$ will strictly exceed the maximum of the first $m$ of them (implying the inequality is strict).&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;The second claim&lt;/strong&gt; is that all increments in $e$ are strictly decreasing; that is, whenever $0\lt n_1\lt n_2\lt n_3$ then $e(n_3)-e(n_2) \lt e(n_2) - e(n_1)$.  &lt;strong&gt;This is false.&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;Such a result would imply that $e(n) \lt 2e(n_2) - e(n_1)$ for all $n\gt n_2$, providing a set of finite upper bounds on the entire sequence $e(1), e(2), \ldots, e(n), \ldots$.  Yet, to the contrary, the maximum of a large number of (independent) Normal variables must be concentrated around an arbitrarily large value.  This is immediate from the &lt;a href=&quot;http://en.wikipedia.org/wiki/Fisher%E2%80%93Tippett%E2%80%93Gnedenko_theorem&quot; rel=&quot;nofollow&quot;&gt;Fisher-Tippett-Gnedenko theorem&lt;/a&gt; on extreme value distributions, but there is a nice elementary demonstration.  It relies on the fact that when random variables $X$ and $Y$ have distributions with CDFs $F$ and $G$, respectively, then the difference in their expectations equals the (signed) area between the graphs of $F$ and $G$, as shown by integrating that difference by parts:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\mathbb{E}(Y) - \mathbb{E}(X) = \int_\mathbb{R} x dG(x) - \int_\mathbb{R} x dF(x) = \int_\mathbb{R} (G(x)-F(x))dx.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;To apply this let $F$ be the common CDF of the $X_i$.   Pick any number $N\gt 0$.  Since for the standard Normal distribution $F(N+1)\lt 1$, we can find a positive integer $n$ for which $F(N+1)^n \le 1/(N+1)$, as in the figure.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/1R6e3.png&quot; alt=&quot;Figure&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;The gold curve is the graph of $F$ and the blue curve is the graph of $F^n$.&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $Y = \max(X_1, \ldots, X_n)$.  Because the $X_i$ are independent, $G(x) = F(x)^n$. &#10;The difference in expectations, $\mathbb{E}(Y) - \mathbb{E}(X) = e(n) - e(1)$, therefore equals the blue area in the figure.  The figure shows this area decomposed into four parts, $I$ (all the blue to the left of $0$), $III, IV$, and $V$.  Areas $I$ and $II$ are fixed and finite (because $X$ has finite expectation equal to the area of $II$ minus the area of $I$).  Areas $II$ and $III$ comprise a rectangle of base $N+1$ and height $1-1/(N+1)$, whence its area equals $N$.  Consequently--ignoring the contributions of $IV$ and $V$ altogether--an underestimate of $e(n)$ is given by $N$ plus the area of $II$ minus the area of $I$.  (The symmetry of the standard Normal distribution implies $I$ and $II$ are congruent, showing the difference of their areas is zero.)&lt;/p&gt;&#10;&#10;&lt;p&gt;This simple visualization shows there always exists an $n$ for which $e(n) \gt N$ no matter what value $N$ might have.  Therefore the sequence $(e(n))$ cannot be bounded above.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;h3&gt;Discussion and Comments&lt;/h3&gt;&#10;&#10;&lt;p&gt;This argument made no reference to the actual expression for $F$.  It needed only two assumptions:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;$F(x)\lt 1$ for all $x$.  (That is, the $X_i$ have unbounded support.)&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The $X_i$ are independent.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Consequently the conclusions hold for a large family of distributions, not just the Normal distribution, and these distributions need not even be continuous.&lt;/p&gt;&#10;&#10;&lt;p&gt;Incidentally, $n$ must be very large even for small $N$.  For the standard Normal distribution, the values of $n$ for $N=1, 2, 3, 4$ are $5, 48, 1027, 50817$.  Thus, it can be difficult to find counterexamples to claim (2) via simulations unless very large values of $n$ are involved.  This provides a nice example of how simulations alone can be misleading: some mathematical analysis is essential to make sure the simulations are useful and are properly interpreted.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-07-26T14:52:30.090" Id="109457" LastActivityDate="2014-07-27T16:01:19.870" LastEditDate="2014-07-27T16:01:19.870" LastEditorUserId="919" OwnerUserId="919" ParentId="109432" PostTypeId="2" Score="3" />
  
  <row AnswerCount="2" Body="&lt;p&gt;My final year project is on debt recovery data for a debt collection firm. Data such as original/current balances,payments made,DOB, number of contacts made,whether or not a debtor has made insuarance payments are given &lt;/p&gt;&#10;&#10;&lt;p&gt;We need to model the given problem in order to predict the collectability of debts of a given debtor. Only approach I started on was a logit model. Any other ideas or hint on possible ways to approach the problem would be appreciated.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-07-26T17:05:13.467" Id="109471" LastActivityDate="2015-02-08T22:46:41.863" OwnerUserId="52857" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;logistic&gt;" Title="Research on debt recovery" ViewCount="81" />
  <row Body="&lt;p&gt;If the performance with a linear kernel is bad, it means the data is not separable in input space (assuming a classication context). The feature spaces induced by more complex kernels (such as polynomial or RBF) &lt;em&gt;can&lt;/em&gt; lead to linear separability. &lt;/p&gt;&#10;&#10;&lt;p&gt;The feature space of an RBF kernel is more complex than both linear and polynomial (in fact, both linear and degree-2 polynomial are degenerate versions of RBF). Hence, if a polynomial kernel can make the data separable, an appropriate RBF kernel can too.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-07-26T18:02:26.727" Id="109481" LastActivityDate="2014-07-26T18:02:26.727" OwnerUserId="25433" ParentId="109470" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="109491" AnswerCount="1" Body="&lt;p&gt;&lt;a href=&quot;http://papers.nips.cc/paper/1765-learning-informative-statistics-a-nonparametnic-approach.pdf&quot; rel=&quot;nofollow&quot;&gt;Learning&#10;Informative&#10;Statistics:&#10;A&#10;Nonparametric&#10;Approach &lt;/a&gt; paper presents an approach to parameter estimation by entropy minimization. There are other related works &quot;Minimum-entropy estimation in semi-parametric models&quot;  download link ( &lt;a href=&quot;http://dl.acm.org/citation.cfm?id=1195853&quot; rel=&quot;nofollow&quot;&gt;http://dl.acm.org/citation.cfm?id=1195853&lt;/a&gt;). The rationale provided is that minimization of error entropy is equivalent to maximization of likelihood. I am new to this area and find it hard to understand the intuition behind why entropy of the error minimization will yield the parameters. What happens when entropy is minimized?&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;What happens when Shannon Entropy is maximized? Entropy (Shannon's) is the uncertainty  = average information or uncertainty (unsure). &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;And what happens when entropy is minimized and &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;What is the meaning of minimizing entropy of error? &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2014-07-26T18:17:40.640" Id="109484" LastActivityDate="2014-07-26T19:38:32.157" OwnerUserId="21961" PostTypeId="1" Score="1" Tags="&lt;self-study&gt;&lt;entropy&gt;&lt;information-theory&gt;&lt;estimators&gt;&lt;maximum-entropy&gt;" Title="Conceptual questions on Entropy and estimation" ViewCount="47" />
  
  
  
  
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;Let assume I have a dataset like this &lt;a href=&quot;http://archive.ics.uci.edu/ml/datasets/Adult&quot; rel=&quot;nofollow&quot;&gt;dataset&lt;/a&gt; where there are several textual attributes even continuos attributes like age. I have always encountered cases where k-nn is applied on just two attributes and all of those ones numerical. How can I properly apply k-nn in a dataset like that?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-26T22:31:11.927" Id="109510" LastActivityDate="2014-07-26T23:48:08.767" OwnerUserId="52694" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;classification&gt;" Title="How to apply properly k-nn algorithm when having several attributes" ViewCount="18" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I had read wiki and some sources. &#10;&lt;a href=&quot;http://jmanton.wordpress.com/2010/06/05/comments-on-james-stein-estimation-theory/&quot; rel=&quot;nofollow&quot;&gt;jmanton's blog&lt;/a&gt;,&#10;&lt;a href=&quot;http://normaldeviate.wordpress.com/2013/05/18/steins-paradox/&quot; rel=&quot;nofollow&quot;&gt;Wasserman's blog&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;the background is that:&lt;/p&gt;&#10;&#10;&lt;p&gt;You have Xi ∼ N(θi, 1), and we want to estimate the each θi.&lt;br&gt;&#10;Where Xi are &lt;code&gt;independent&lt;/code&gt;。&lt;/p&gt;&#10;&#10;&lt;p&gt;For &lt;code&gt;MSE&lt;/code&gt; risk of vector (X1,...,Xn), when n&gt;=3.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;the MLE estimator $\hat θ_i = x_i$ is inadmissable . &lt;/li&gt;&#10;&lt;li&gt;While James-Stein estimator is better than MLE in such case.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;h2&gt;Looking from Shrinkage&lt;/h2&gt;&#10;&#10;&lt;p&gt;quote from Larry Wasserman:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Note that the James-Stein estimator &lt;code&gt;shrinks&lt;/code&gt; {X} towards the origin. (In fact, you can shrink towards any point; there is nothing special about the origin.) &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I know the James-Stein estimator is special to MLE for its shrinkage behavior.&#10;I still dont get it, the variables are independent.  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Why James-Stein estimator &lt;code&gt;shrinkage&lt;/code&gt; to arbitrary point can improve the MSE risk versus where MLE do &lt;code&gt;not shrinkage&lt;/code&gt; at all ??&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;and seeing this from Larry Wasserman:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;This can be viewed as an empirical Bayes estimator ....&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;If view it from Empirical Bayes case, it is shrinkage to a overall mean, but that is nonsense if you have independent variable to estimate.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Is there better example or intuitive explanation for it?? THANKS.&lt;/p&gt;&#10;" ClosedDate="2014-11-02T20:17:42.077" CommentCount="2" CreationDate="2014-07-27T10:13:03.467" Id="109543" LastActivityDate="2014-07-27T10:20:18.740" LastEditDate="2014-07-27T10:20:18.740" LastEditorUserId="30310" OwnerUserId="30310" PostTypeId="1" Score="0" Tags="&lt;estimation&gt;&lt;decision-theory&gt;&lt;shrinkage&gt;" Title="Intuition behind the Stein's paradox" ViewCount="54" />
  <row Body="&lt;p&gt;Be careful of artifacts.&lt;/p&gt;&#10;&#10;&lt;p&gt;K-means assumes that every attribute has the same weight.&lt;/p&gt;&#10;&#10;&lt;p&gt;If, say, one attribute is the height in meters, and the other is the weight in g, then the result of k-means will depend almost exclusively on the weight.&lt;/p&gt;&#10;&#10;&lt;p&gt;If this attribute then is useful for separating your two classes, the outcome will look much more impressive than it is logically.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Visualize&lt;/strong&gt;, visualize, visualize! Often such artifacts can be seen already in a primitive visualization. In your case, I recommend looking at histograms as well as scatterplots; both with class labels and clusters visualized.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-27T10:24:06.410" Id="109545" LastActivityDate="2014-07-27T10:24:06.410" OwnerUserId="7828" ParentId="109443" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;You are correct about the link between these two functions, provided the MGF exists. This was discussed in another question in this site: &lt;a href=&quot;http://stats.stackexchange.com/questions/44201/link-between-moment-generating-function-and-characteristic-function&quot;&gt;Link between moment-generating function and characteristic function&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-27T13:12:31.880" Id="109558" LastActivityDate="2014-07-27T13:12:31.880" OwnerUserId="52891" ParentId="109551" PostTypeId="2" Score="0" />
  <row AnswerCount="1" Body="&lt;p&gt;Sorry, I'm not an expert and my question could be fundamentally wrong.&#10;I've read this interesting &lt;a href=&quot;http://stats.stackexchange.com/questions/11602/training-with-the-full-dataset-after-cross-validation&quot;&gt;question&lt;/a&gt; because I also was wondering whether to train the model again after cross-validation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, after boosting regression trees with this &lt;a href=&quot;http://github.com/tqchen/xgboost&quot; rel=&quot;nofollow&quot;&gt;library&lt;/a&gt; over 4/5 of the training set, I'm doing a 5-fold CV against a particular objective &lt;a href=&quot;http://www.kaggle.com/c/higgs-boson/details/evaluation&quot; rel=&quot;nofollow&quot;&gt;function&lt;/a&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;After re-training on the whole training dataset, I use that final model to predict the binary classification of the test dataset. I'm a bit surprised to see that the evaluation metric for the test set is significantly worse than expected from the validation results. &lt;/p&gt;&#10;&#10;&lt;p&gt;Do you have any suggestions about what could be going wrong? My second (maybe related) question is why does increasing the rounds of boosting result in worse predictions?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-07-27T13:42:25.563" Id="109559" LastActivityDate="2014-11-29T18:52:14.043" LastEditDate="2014-11-29T18:52:14.043" LastEditorUserId="26338" OwnerDisplayName="user52890" PostTypeId="1" Score="3" Tags="&lt;cross-validation&gt;&lt;cart&gt;&lt;boosting&gt;&lt;classification-tree&gt;" Title="Why should all Cross-Validation results be higher than the result on the test dataset?" ViewCount="148" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Im trying to estimate a &lt;strong&gt;momentum threshold model&lt;/strong&gt; for relationship between main stock index, political-risk and economic-risk.&#10;The equation I want to evaluate is&#10;StockIndex(t)=a*PoliticalRisk+b*EconomicRisk+error&#10;I've been using both &lt;strong&gt;Eviews add-in called TARCOINT&lt;/strong&gt; with the &lt;strong&gt;tsdyn package in R&lt;/strong&gt;. I have been facing some issues in understanding the steps&#10;I had a basic doubt- The R-squared values are really low and the coefficients are not significant. Can I continue to the MTAR model despite this? Do I have to try different ARIMA models and then perform MTAR on the residuals?&#10;I hope I was clear. Really need help!! &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-27T14:25:44.677" FavoriteCount="1" Id="109561" LastActivityDate="2014-07-27T14:25:44.677" OwnerUserId="52862" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;threshold&gt;&lt;eviews&gt;" Title="Threshold Models" ViewCount="35" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm going through Christopher Manning's tutorial from NAACL 2013 &quot;Deep Learning for NLP (without Magic)&quot; and he gets to the point where he's showing how to do unsupervised pre-training. He's saying that we compute two different scores $s$ and $s_c$ and then we want to train the network so that $s &amp;gt; s_c$. He claims that the objective function to use is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;J = \text{max}(0, 1 - s + s_c)&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I don't see how minimizing this function ensures that $s &amp;gt; s_c$. It seems to me that the value is greater than zero whenever the sum of $s$ and $s_c$ is greater than 1 and it doesn't matter which one is greater than the other. &lt;/p&gt;&#10;&#10;&lt;p&gt;Could someone help clear this up for me? What am I missing?&lt;/p&gt;&#10;&#10;&lt;p&gt;The video is here: &lt;a href=&quot;http://techtalks.tv/talks/deep-learning-for-nlp-without-magic-part-1/58414/&quot; rel=&quot;nofollow&quot;&gt;http://techtalks.tv/talks/deep-learning-for-nlp-without-magic-part-1/58414/&lt;/a&gt; and the part I'm asking about is at about the 50:30 mark. The slides are at &lt;a href=&quot;http://nlp.stanford.edu/courses/NAACL2013/NAACL2013-Socher-Manning-DeepLearning.pdf&quot; rel=&quot;nofollow&quot;&gt;http://nlp.stanford.edu/courses/NAACL2013/NAACL2013-Socher-Manning-DeepLearning.pdf&lt;/a&gt; and the slide in question is number 50.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-07-27T17:17:43.373" Id="109582" LastActivityDate="2014-07-27T17:17:43.373" OwnerUserId="118" PostTypeId="1" Score="0" Tags="&lt;neural-networks&gt;&lt;nlp&gt;&lt;deep-learning&gt;" Title="How to understand this objective function in deep learning" ViewCount="79" />
  <row AnswerCount="0" Body="&lt;p&gt;What I have is a treatment group consisting of 90 individuals. I have a pool of individuals eligible for my control group, which I want to be matched by age and gender in a 1:1 ratio. Is there any practical package or function in R to find such optimal control group?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-27T17:32:07.593" Id="109584" LastActivityDate="2014-07-27T17:32:07.593" OwnerUserId="16666" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;computational-statistics&gt;&lt;case-control-study&gt;" Title="Selecting from a bigger control group for 1:1 ratio" ViewCount="21" />
  <row Body="&lt;p&gt;you can use the Kolmogorov Smirnof test:&#10;Consider two samples ${x}$ and ${y}$ and let $F_n(x)$ be the empirical distribution function of $x$ and $G_m(y)$ be the one of $y$, where $n$ and $m$ are respectively the sample size of the first and second sample. The statistic is&lt;/p&gt;&#10;&#10;&lt;p&gt;$D = sup_{x,y} |F_n(x)-G_m(y)|$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The critical values (of the Kolmogorov distribution) are tabulated.&lt;/p&gt;&#10;&#10;&lt;p&gt;In compute  the critical value you have to take in consideration that you want to do many sequential test and then the critical values must be adjusted. I am not an expert in that field but you probably can find something on the internet. (the key words are &quot;sequential test&quot;, &quot;adjusted p-value&quot;..)&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT:&#10;I changed the notation accordingly to the suggestion of Russ Lenth&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-07-27T19:22:32.233" Id="109596" LastActivityDate="2014-07-27T20:03:29.087" LastEditDate="2014-07-27T20:03:29.087" LastEditorUserId="27213" OwnerUserId="27213" ParentId="109591" PostTypeId="2" Score="1" />
  
  
  
  <row Body="&lt;p&gt;You're probably best off treating the color palettes as ordinary numerical vectors and using any one of the popular distance metrics for vector spaces, i.e. euclidean distance, cosine distance or mahalanobis distance. &lt;/p&gt;&#10;&#10;&lt;p&gt;The only reason I wouldn't do this is if there is some way you want to incorporate the similarity of non-identical colors in your comparison. For example should the distance between {'Pink' : 100} and {'Purple' : 100} be different than the distance between {'Black' : 100} and {'White' : 100}. If so, then you might want to look at different color spaces for representing your vectors but it depends on what you're trying to achieve.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-27T22:37:05.593" Id="109619" LastActivityDate="2014-07-27T22:37:05.593" OwnerUserId="41389" ParentId="109618" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="109621" AnswerCount="1" Body="&lt;p&gt;I have two normal distributions, and I want to test whether they have the same standard deviation, I really don't care about the mean.&lt;/p&gt;&#10;&#10;&lt;p&gt;My idea is: de-mean both of them and then use &lt;a href=&quot;http://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test&quot; rel=&quot;nofollow&quot;&gt;Kolmogorov-Smirnov&lt;/a&gt; to test if the distributions are different, if they are then standard deviations also should be different. &lt;/p&gt;&#10;&#10;&lt;p&gt;I am wondering if I am missing anything, and if there is a better way to do this.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-07-27T22:59:43.317" FavoriteCount="3" Id="109620" LastActivityDate="2014-09-05T02:27:41.140" LastEditDate="2014-09-05T02:27:41.140" LastEditorUserId="805" OwnerUserId="11708" PostTypeId="1" Score="5" Tags="&lt;hypothesis-testing&gt;&lt;distributions&gt;&lt;statistical-significance&gt;&lt;normal-distribution&gt;&lt;standard-deviation&gt;" Title="Test that two normal distributions have same standard deviation" ViewCount="260" />
  
  <row Body="&lt;p&gt;The problem's a bit more complicated than you think. If you want to compute the perceived similarity when viewed, you're going to have to aren't going to be able to just work with pixel values as they are contained in an image file. The first thing you'll have to do is a gamma adjustment (&lt;a href=&quot;http://www.poynton.com/GammaFAQ.html&quot; rel=&quot;nofollow&quot;&gt;http://www.poynton.com/GammaFAQ.html&lt;/a&gt;), and although I'm a bit rusty on this stuff, I think you'll need to then convert to a perceptually uniform color space.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hopefully someone on this forum will know more.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-28T00:32:02.267" Id="109629" LastActivityDate="2014-07-28T00:32:02.267" OwnerUserId="46522" ParentId="109618" PostTypeId="2" Score="0" />
  
  
  <row Body="&lt;p&gt;Here is the answer to my question. There are two packages to try out:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://cran.r-project.org/web/packages/riverplot/index.html&quot; rel=&quot;nofollow&quot;&gt;riverplot&lt;/a&gt; - this package is very flexible - as it is flexible, will need some time to learn its format to master it.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://cran.r-project.org/web/packages/reshape2/index.html&quot; rel=&quot;nofollow&quot;&gt;ggparallel&lt;/a&gt; - easier regarding to data format which can be easily done using reshape2 package&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-28T03:22:33.260" Id="109640" LastActivityDate="2014-07-28T03:22:33.260" OwnerUserId="52610" ParentId="108974" PostTypeId="2" Score="0" />
  
  
  
  <row Body="&lt;p&gt;The point is to find the location of the zeros in the eigenvectors. You could use PCA or any other form of clustering.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-07-28T05:33:28.060" Id="109645" LastActivityDate="2014-07-28T05:33:28.060" OwnerUserId="49398" ParentId="109616" PostTypeId="2" Score="0" />
  <row AnswerCount="2" Body="&lt;p&gt;I am trying to conduct an EFA with a sample size of 150 respondents. I would also like to use cross-validation but my professor says that the sample is not big enough for that. Is that true?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-07-28T08:07:47.857" Id="109652" LastActivityDate="2014-07-28T10:10:45.340" OwnerUserId="52896" PostTypeId="1" Score="0" Tags="&lt;cross-validation&gt;&lt;sample-size&gt;&lt;factor-analysis&gt;&lt;eda&gt;" Title="Cross Vaidation in Factor Analysis" ViewCount="34" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I need to estimate (using cross-validation), the parameters $\sigma$ and $\lambda$ of the Gaussian kernel:&lt;/p&gt;&#10;&#10;&lt;p&gt;$K_G(x,y) = \sigma^2 \exp{(-\frac{1}{2\lambda^2}\sum_{i,j}(x_{ij}-y_{ij})^2})$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $x$ and $y$ are the adjacent matrices of a Markov chain. How can I do that?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-07-28T10:36:04.000" Id="109668" LastActivityDate="2014-07-28T11:54:01.013" LastEditDate="2014-07-28T11:54:01.013" LastEditorUserId="17908" OwnerUserId="52938" PostTypeId="1" Score="0" Tags="&lt;estimation&gt;&lt;cross-validation&gt;&lt;kernel&gt;&lt;markov-chain&gt;" Title="Parameter estimation of gaussian function kernel using cross-validation" ViewCount="25" />
  <row AnswerCount="0" Body="&lt;p&gt;I think that this is an interesting issue for all those who whan to not only adjust one prediction function, but also to obtain the prediction bands to asses the graphical or tabular error I'm trying to apply step by step the Delta Method to my data, however, as I would like to share with you my advances to get to the final. Sumarizing and reviewing the standard method using in comercial software above all, we find: &#10;&lt;img src=&quot;http://i.stack.imgur.com/h23aM.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Where the right square root involves the inverse of the Hessian matrix I think, although they call it design matrix. As well as the gradient matrix function. So I share here my code to try to fit a custom non-linear function and asseseing its prediction intervals (confidence and prediction, or int Mathematica language, mean and simple prediction intervals).&lt;/p&gt;&#10;&#10;&lt;p&gt;CODE WITH MY EXAMPLE DATA:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;(*Example data*)&#10;In[1]:= data = {31, 46, 70, 87, 87, 93, 114, 128, 133, 134, 143, 155, 161, &#10;161, 163, 177, 181, 207, 207, 226, 302, 315, 319, 347, 347, 362, &#10;375, 377, 413, 440, 447, 461, 464, 511, 524, 556, 800, 860, 880, &#10;954, 5200, 12000};&#10;&#10;In[2]:=dist = ProbabilityDistribution[{&quot;CDF&quot;, &#10;Exp[-a Exp[-x b] - c Exp[-x d]]}, {x, 0, Infinity}, &#10;Assumptions -&amp;gt; {{0 &amp;lt; a &amp;lt; 10}, {0 &amp;lt; b &amp;lt; 1}, {0 &amp;lt; c &amp;lt; 10}, {0 &amp;lt; d &amp;lt; &#10;1}}];&#10;&#10;In[3]:= res = FindDistributionParameters[data, &#10;     dist, {{a, 3.8}, {b, 0.006}, {c, 0.08}, {d, 0.0002}}];&#10;&#10;            (* ACOV is:*)&#10;In[4]:= acov[data_, dist_, paramlist_, mleRule_] := &#10;     Block[{len, infmat, cov}, len = Length[data];&#10;     infmat = -D[LogLikelihood[dist, data], {paramlist, 2}]/len /. &#10;     mleRule;&#10;cov = Inverse[infmat]];&#10;&#10;In[5]:= acov[data, dist, {a, b, c, d}, res];&#10;&#10;           (* Gradient function G' *)&#10;In[6]:=D[Exp[-a Exp[-x b] - c Exp[-x d]], {{a, b, c, d}}];&#10;&#10;In[7]:=Partition [%,4];&#10;In[8]:= Transpose[%7];&#10;&#10;            (* To multiply Transpose-Gradient by acov by Gradient*)&#10;&#10;In[9]:=%7 %5 %8&#10;&#10;            (*T-student critical value to alpha 0.05 and 42 data*)&#10;Quantile[StudentTDistribution[41], 0.95]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The problem now is to validate this code and to do it working with a numerical example to get the predicted y and the their confidence and prediction intervals. That is, the final matrix have to give a simple number and then we have to calculate the MSE as well as the t-student queantile.&lt;/p&gt;&#10;&#10;&lt;p&gt;Reference posts: http : //mathematica.stackexchange.com/questions/6498/standard - errors - for - maximum - likelihood - estimates - in - finddistributionparameters;&lt;/p&gt;&#10;&#10;&lt;p&gt;http : //stats.stackexchange.com/questions/15423/how - to - compute - prediction - bands - for - non - linear - regression?answertab = active #tab - top&lt;/p&gt;&#10;&#10;&lt;p&gt;http : //stats.stackexchange.com/questions/68080/basic - question - about - fisher - information - matrix - and - relationship - to - hessian - and - s&lt;/p&gt;&#10;&#10;&lt;p&gt;I look forward to your help, thank you.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-28T11:39:44.490" Id="109674" LastActivityDate="2014-07-28T11:39:44.490" OwnerUserId="52856" PostTypeId="1" Score="0" Tags="&lt;distributions&gt;&lt;fitting&gt;" Title="Applying Delta Method to generate prediction bands for custom distributions step by step" ViewCount="41" />
  <row Body="&lt;p&gt;If $i\neq j$, you are right. Formally, the expected value here is taken with respect to the joint distribution of $X_i$ and $X_j$,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ E[X_i \cdot I(X_j&amp;gt;\theta)]  = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f_{ij}(x_i,x_j)x_i\cdot I(x_j&amp;gt;\theta)dx_idx_j$$&#10;where due to independence&lt;/p&gt;&#10;&#10;&lt;p&gt;$$=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f_{i}(x_i)f_{j}(x_j)x_i\cdot I(x_j&amp;gt;\theta)dx_idx_j$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$=\int_{-\infty}^{\infty}f_{j}(x_j) I(x_j&amp;gt;\theta)\int_{-\infty}^{\infty}f_{i}(x_i)x_idx_idx_j$$&#10;$$=\int_{-\infty}^{\infty}f_{j}(x_j) I(x_j&amp;gt;\theta)E(X_i)dx_j$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$=E(X_i)\int_{-\infty}^{\infty}f_{j}(x_j) I(x_j&amp;gt;\theta)dx_j = E(X_i)\int_{\theta}^{\infty}f_{j}(x_j) dx_j$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$=E(X_i)\cdot[1-F_X(\theta)] = \frac 12E(X_i)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;If $i=j$, by the Law of Total Expectation&#10;$$E[X_i \cdot I(X_i&amp;gt;\theta)] = E[X_i \cdot I(X_i&amp;gt;\theta)\mid X_i&amp;gt;\theta]P( X_i&amp;gt;\theta) + E[X_i \cdot I(X_i&amp;gt;\theta)\mid X_i\leq \theta]P( X_i\leq\theta)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$=E(X_i \cdot 1\mid X_i&amp;gt;\theta)\frac 12 + E(X_i \cdot 0\mid X_i\leq \theta)\frac 12$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$=\frac 12E(X_i \mid X_i&amp;gt;\theta)=\frac 12\int_{\theta}^{\infty}\frac {x f_X(x)}{1-F_X(\theta)}dx = \int_{\theta}^{\infty}x f_X(x)dx$$&lt;/p&gt;&#10;&#10;&lt;p&gt;i.e. here we have &quot;half the value of the &lt;em&gt;truncated&lt;/em&gt; from below at $\theta$&quot; distribution, or, the expected value of the &quot;&lt;em&gt;restricted&lt;/em&gt; from below at $\theta$&quot; distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;One could also obtain this immediately by using the so-called &quot;Law of the Unconscious Statistician&quot;, &lt;/p&gt;&#10;&#10;&lt;p&gt;$$E[X_i \cdot I(X_i&amp;gt;\theta)] = \int_{-\infty}^{\infty}f_X(x)[x I(x&amp;gt;\theta)]dx = \int_{\theta}^{\infty}x f_X(x)dx$$&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-07-28T12:06:44.840" Id="109677" LastActivityDate="2015-01-15T17:04:48.900" LastEditDate="2015-01-15T17:04:48.900" LastEditorUserId="28746" OwnerUserId="28746" ParentId="109671" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;Good question.  The best metrics to use are in fact those defined at single thresholds - so Precision, Accuracy, Recall, Uplift (which is Precision / Prior).  The reason why you ought to consider these measures more important than, say ROC, AUC or Gini, is that these measures aggregate over all possible thresholds. This means the threshold is still to be chosen, something that will be necessary for your final product.&lt;/p&gt;&#10;&#10;&lt;p&gt;AUC, ROC, and Gini will &lt;em&gt;help you choose a threshold&lt;/em&gt; but not tell you how &quot;good&quot; your algorithm actually is.  Think if these measures as corresponding to &quot;the probability that your algorithm is good if you where to randomly pick a threshold&quot;, but of course in reality you don't randomly pick a threshold.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now what you &lt;strong&gt;really&lt;/strong&gt; want to do is relate some measure of accuracy, like precision, to the actual thing in the business you wish to optimize.  This is usually something that has currency as a unit.&lt;/p&gt;&#10;&#10;&lt;p&gt;So I'll give an example, in Adtech, the important business measure is CPA, which is &quot;Cost Per Acquisition&quot; - in other words the cost in $ to get someone to convert (buy a product). Usually this measure directly corresponds with Precision, but NOT accuracy.  In this business case Accuracy is meaningless as it rewards algorithms on how good they are at predicting Negatives as well as Positives, but in Adtech we just want to know how good we are at predicting the Positives.  For example we are not interested in predicting whether someone does NOT want to buy a car, we are interested in if they DO want to buy a car so we can give them an advert.&lt;/p&gt;&#10;&#10;&lt;p&gt;In other businesses other measures might be important.  In fraud detection, maybe Recall is the most important thing - i.e. how good are we at ensuring fraudsters do not slip through our net.&lt;/p&gt;&#10;&#10;&lt;p&gt;So the real answer to the question is the measure of &quot;good&quot; depends on what it is your trying to do - try to find a mathematical relationship between a measure of &quot;good&quot;ness and the business measure your trying to optimize.  This is why AUC and Gini are not that useful because there cannot exist a mathematical correspondence between concrete business measures and these numbers because a threshold has not yet been chosen, without a threshold you don't have a product.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hope that isn't too rambly&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-28T12:22:17.100" Id="109678" LastActivityDate="2014-07-28T12:22:17.100" OwnerUserId="36915" ParentId="109599" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a model where the $y$ is very skewed and I convert it to log and run a log linear model.&lt;/p&gt;&#10;&#10;&lt;p&gt;But, I have doubts about the way to measure the error, because in the original variables the error would be much bigger than in log variables. Are there any other ways to measure the error?&lt;/p&gt;&#10;&#10;&lt;p&gt;I would prefer to use GEP instead of log linear. But the idea is to find simple coefficients to explain. Is there another way to do it?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-07-28T13:15:07.103" Id="109688" LastActivityDate="2014-07-28T13:21:46.927" LastEditDate="2014-07-28T13:21:46.927" LastEditorUserId="22311" OwnerUserId="46741" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;goodness-of-fit&gt;&lt;log-linear&gt;" Title="Is it correct to measure R coefficient for LOG LIN models" ViewCount="22" />
  <row Body="&lt;p&gt;Remember that only the coefficient for the constant from logitsitc regression are log odds; the remaining coefficients are log odds &lt;em&gt;ratios&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Lets say your baseline odds ($\exp(constant)$) is .8. This would mean that there are 0.8 people who say yes for every person who says no when all explanatory variables are 0. Your odds ratio for A says that a unit increase in A is associated with a 1% decrease in the odds of saying yes, while the odds ratio for B says that a unit increas in B is associated with a 24% increase in the odds of saying yes.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you want your effects in terms of probabilities you can look up &quot;marginal effects&quot;. &lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-07-28T13:16:46.590" Id="109689" LastActivityDate="2014-07-28T13:16:46.590" OwnerUserId="23853" ParentId="109685" PostTypeId="2" Score="2" />
  <row AnswerCount="1" Body="&lt;p&gt;I am doing a case-control study analysis with 2500 cases and 2500 controls. I am interested in finding out if the cases have higher odds of having a particular disease than the controls, so I am calculating odds ratios for each of the 1000 diseases using a logistic regression model. &lt;/p&gt;&#10;&#10;&lt;p&gt;I want to know which odds ratios &gt; 1 are actually significant enough to the point where I can say that the case is at higher risk for the disease than the control. To do so, I employed Efron's method of estimating the empirical null distribution and local FDR values (link: &lt;a href=&quot;http://www.uni-leipzig.de/~strimmer/lab/courses/ss06/seminar/papers/B/efron2004.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.uni-leipzig.de/~strimmer/lab/courses/ss06/seminar/papers/B/efron2004.pdf&lt;/a&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;Most of the odds ratios should be 1 (betas are 0), corresponding to z-value of 0. When I estimate the empirical null distribution, it is not distributed ~ N(0, 1) but rather N(mu, sigma). So this allows me to identify significant odds ratios. However, I want to know if I can transform the odds ratios using the empirical null distribution as such:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;z-value for beta coefficients is equal to coefficient/standard error.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;z-values are distributed N(mu, sigma).&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Normalize the z-values: (z - mu)/sigma.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Multiply the new z-values by the standard error. &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Now, you have new coefficients and odds ratios, which have a more accurate distribution of odds ratios (most of them are around 1). &lt;/p&gt;&#10;&#10;&lt;p&gt;Is this a valid approach?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you in advance.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-28T14:05:50.657" Id="109694" LastActivityDate="2014-07-28T16:51:24.540" LastEditDate="2014-07-28T16:51:24.540" LastEditorUserId="4253" OwnerUserId="52726" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;&lt;logistic&gt;&lt;bootstrap&gt;&lt;odds-ratio&gt;&lt;regression-strategies&gt;" Title="Using empirical null distribution to adjust odds ratios" ViewCount="87" />
  <row AnswerCount="0" Body="&lt;p&gt;I am concerned with simulating data for a linear regression model. I need to control the means, variances, and correlations (covariances) between the predictors and the criterion variable. In addition, I need to be able to vary the explained variances ($R^2$). It is obvious to me that the latter must be a function of the earlier, so at least one correlation (covariance) in $\Sigma$ is perhaps dependent on the choice of $R^2$, where $\Sigma=E((Y,X)(Y,X)^T)$, for centered $X$ and $Y$, is the variance-covariance matrix of all variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;My plan is thus as follows:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Specify $\Sigma$, means, and $R^2$&lt;/li&gt;&#10;&lt;li&gt;Simulate data with these sufficient statistics, e.g. by sampling from the multivariate normal&lt;/li&gt;&#10;&lt;li&gt;Check estimated $\beta$ (regression coefficient) vector against population (theoretical) coefficients and use the model for unrelated tests/science.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Hence, my approach does not suggest specifying $\beta$ but letting the coefficients be a function of population $\Sigma$, means, and $R^2$. The reason I need to do this is to attribute some realisitc scale to $X$ and $Y$ (e.g., let $Y$ assume an 'income' scale and give $X$ a realistic scale for years of eduction). Therefore, I specify the sufficient statistics instead of regression coefficients. But maybe there is a better way.&lt;/p&gt;&#10;&#10;&lt;p&gt;Moreover, I have two specific questions:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Given the population variance-covariance matrix $\Sigma$ of one criterion variable $Y$ and a series of predictors (covariates) $X$ , I would like to calculate the vector of true population regression coefficients. Of course, I could simulate data $X$ and $Y$ and use the OLS estimator, but there should be a direct way to use $\Sigma$ in the estimation of population $\beta$?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Which options are there to specify covariances (correlations) in $\Sigma$ given I need a fixed $R^2$ of a linear regression of $Y$ on $X$? This, to systematically vary the explanatory power of the regression model.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="1" CreationDate="2014-07-28T14:16:47.737" FavoriteCount="1" Id="109696" LastActivityDate="2014-07-28T14:42:57.780" LastEditDate="2014-07-28T14:42:57.780" LastEditorUserId="24515" OwnerUserId="24515" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;multiple-regression&gt;&lt;linear-model&gt;&lt;simulation&gt;&lt;variance-covariance&gt;" Title="What is the best way to simulate data for a linear regression model?" ViewCount="91" />
  <row AnswerCount="0" Body="&lt;p&gt;I would like to know if there is a test for the difference of to means m1 and m2 (continuous variables) if I have only information for mean, 2.5%- and 97.5%-quantiles. For example:&lt;/p&gt;&#10;&#10;&lt;p&gt;$m1  \ \ \   = 10.5$&lt;br&gt;&#10;$q1_{025}= 8.3$&lt;br&gt;&#10;$q1_{975}= 12.5$&lt;br&gt;&#10;$m2 \ \ \ \   = 15.5$&lt;br&gt;&#10;$q2_{025}= 12.7$&lt;br&gt;&#10;$q2_{975}= 17.3$  &lt;/p&gt;&#10;&#10;&lt;p&gt;No further informations are available. One possible way to solve this is to assume that the quantiles range is 1.96*standard errors (se), to compute the se and to perform a t-test. Maybe there  is another test which is more suitable to this kind of data (and preferably as R function). Any idea? &lt;/p&gt;&#10;&#10;&lt;p&gt;Updates to clarify question:&lt;br&gt;&#10;The original question above was motivated from a regression analysis with a linear (x) and quadratic term ($x^2$), interaction z (factor with 7 levels) with x, other fixed variables and a random variable &#10;$$y &#10;\tilde~b_1x+b_2x²+b_3z+b_4xz+b_5x²z+...&#10;$$&#10;The parameter of interest is the x value at maximum y ($x_m$). I computed 500 $x_m^{\ast}$ through (s=500 resamplings of the residuals $\epsilon=y-\hat y$ and 500 regressions). From this 500 $x_m^{\ast}$ I computed $m1$, $q1_{025}$,$q1_{975}$,$sd1$,...,$m7$ etc.&lt;br&gt;&#10;My questions:  &lt;/p&gt;&#10;&#10;&lt;p&gt;(1) If I have only the quantiles of $x_m^{\ast}$ for z-levels=1...7 and no other informations how can I perform a test to compare $x_m$ of z-level 1 and 2? (This was my original question above).  &lt;/p&gt;&#10;&#10;&lt;p&gt;(2) I want to compare z-levels 1 and 2 and I have all $x_m^{\ast}$ available:   $$x_{{m_1}_1}^{\ast}$,$x_{{m_1}_2}^{\ast}$,...,$x_{{m_1}_{500}}^{\ast}&#10;$$ &#10;for level 1 and &#10;$$&#10;x_{{m_2}_1}^{\ast}$,$x_{{m_2}_2}^{\ast}$,...,$x_{{m_2}_{500}}^{\ast}&#10;$$&#10;for level 2.&lt;/p&gt;&#10;&#10;&lt;p&gt;(3) Additionally to question (2) I have the number of observation for  level 1 and 2 of z: $n_1$ and $n_2$.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-07-28T14:20:08.400" Id="109697" LastActivityDate="2014-07-30T01:01:01.520" LastEditDate="2014-07-30T01:01:01.520" LastEditorUserId="17147" OwnerUserId="17147" PostTypeId="1" Score="0" Tags="&lt;hypothesis-testing&gt;&lt;confidence-interval&gt;&lt;t-test&gt;&lt;quantiles&gt;" Title="Test for two confidence intervals" ViewCount="25" />
  <row AcceptedAnswerId="109944" AnswerCount="1" Body="&lt;p&gt;I have heard of the &quot;&lt;strong&gt;false discovery rate curve&lt;/strong&gt;&quot; (e.g &lt;a href=&quot;http://people.inf.ethz.ch/bkay/publications/Brodersen_2010a_ICPR.pdf&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;) but never seen an example.&lt;/p&gt;&#10;&#10;&lt;p&gt;If I recall correctly from a conversation with a colleague, the &lt;code&gt;y-axis&lt;/code&gt; in the FDR curve measures the FDR itself, defined as $FDR = \frac{FP}{TP+FP}$ (i.e. $1 - \text{Precision}$), but what goes into the x-axis? &lt;/p&gt;&#10;&#10;&lt;p&gt;What is a normal shape for this curve, and when is it a good idea to use it?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-07-28T15:16:26.393" FavoriteCount="1" Id="109701" LastActivityDate="2014-07-30T18:30:17.213" LastEditDate="2014-07-30T18:30:17.213" LastEditorUserId="30802" OwnerUserId="2798" PostTypeId="1" Score="2" Tags="&lt;classification&gt;&lt;precision-recall&gt;&lt;metric&gt;" Title="What is the False Discovery Rate curve?" ViewCount="127" />
  <row AcceptedAnswerId="109782" AnswerCount="2" Body="&lt;p&gt;I'm currently doing a modelling project. However, I haven't taken a bunch of statistics classes, so I have to teach myself generalized linear models. I'm reading &lt;em&gt;Generalized Linear Models for Insurance Data&lt;/em&gt; (Heller and de Jong, 2008, CUP), and I have two questions:&lt;/p&gt;&#10;&#10;&lt;p&gt;1. On page 64, it says: &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Given a response $y$, the generalized linear model is $f(y)=c(y,\phi)\exp{\frac{y\theta - a(\theta)}{\phi}}$. The equation for $f(y)$ specifies that the distribution of the response is in the exponential family.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Is that the equation for the distribution of $E[y_i|x_i]$ or some other thing? If it's the distribution for $y$ corresponding to a fixed $x_i$, is it possible that even if the plot of $y$ against $x$ looks like a straight line, I should still use GLM instead of simple regression?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;update&lt;/strong&gt;: I guess I should clarify myself a little bit. Currently I have a dataset and my dependent variable is $y$. I made a histogram of $y$ (with frequency on y-axix) and it looks like a gamma curve fits well. Does that essentially imply that I should choose $f(y)$ to be gamma? I kinda doubt it because I suppose $y_i|_{X=x_i}$ and $Y$ are essentially two different things. I hope I'm not confusing you guys.&lt;/p&gt;&#10;&#10;&lt;p&gt;2. The book suggests that when assuming response $y$ follows a gamma distribution, it is a common practise to use a logarithmic link function. I don't quite understand the reason behind that.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any suggestion would be great. Thanks!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-07-28T15:54:55.110" FavoriteCount="1" Id="109708" LastActivityDate="2014-07-31T02:21:15.310" LastEditDate="2014-07-31T02:21:15.310" LastEditorUserId="805" OwnerUserId="52874" PostTypeId="1" Score="4" Tags="&lt;regression&gt;&lt;generalized-linear-model&gt;&lt;linear-model&gt;&lt;assumptions&gt;" Title="Two simple questions regarding GLM" ViewCount="113" />
  <row AnswerCount="2" Body="&lt;p&gt;I have a pair of data sets, and I want to know whether the medians of these two sets differ significantly. I've calculated the median of each set and subtracted them from each other. How can I determine appropriate error bars for that point?&lt;/p&gt;&#10;&#10;&lt;p&gt;I was thinking I would take the median absolute deviation (MAD) of both and add them together. But should that be the entire length of the error bar? Or half of it? Or should I divide it by $\sqrt n$? Any advice is appreciated, thanks.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-07-28T15:58:40.287" Id="109709" LastActivityDate="2014-07-28T22:49:01.550" OwnerUserId="2488" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;&lt;error&gt;&lt;robust&gt;&lt;median&gt;" Title="Obtaining error bars for a difference" ViewCount="61" />
  
  <row AnswerCount="0" Body="&lt;p&gt;We're designing a survey where we ask students to indicate their level of agreement (likert scale 1 - Strongly agree....5 - Strongly disagree.  We ran factor analysis on the data and obtained 4 factors.  In order to get composite scores for each factor, this is what we did:&#10;For example, &#10;If factor 1 was made up of 3 items, each with raw scores of 3,4 and 5, we took the mean of 3 scores and then a z-score of the mean to get standardized scores for further analyses.  Now if we want to divide the construct being measured into high and low points, would z-scores &gt; 0 automatically imply a high measure of the construct?  In the original scale, if 3 was the 0 point, values above 3 would indicate disagreement (low measure of the construct).  So does the same thing translate to the z-score as well?  Would z-scores above 0 indicate low measures? &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-28T16:41:50.873" Id="109716" LastActivityDate="2014-07-28T16:41:50.873" OwnerUserId="28687" PostTypeId="1" Score="0" Tags="&lt;pca&gt;&lt;composite&gt;" Title="calculation of composite scores from factor analysis" ViewCount="26" />
  <row AnswerCount="1" Body="&lt;p&gt;Is using a 2 way MANOVA to check whether height and weight differ across age groups and gender, the same as using a 2 way ANOVA to check whether BMI (wt in kg/ht in m$^2$) differ across age groups and gender?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-07-28T16:47:59.110" Id="109719" LastActivityDate="2014-07-28T17:47:55.230" LastEditDate="2014-07-28T17:47:55.230" LastEditorUserId="28666" OwnerUserId="52672" PostTypeId="1" Score="1" Tags="&lt;anova&gt;&lt;biostatistics&gt;&lt;manova&gt;" Title="Two-way MANOVA vs. two-way ANOVA on a composite variable" ViewCount="179" />
  
  <row Body="&lt;p&gt;It's probably easiest to compute $\eta^2_p$ directly from the $t$- or $F$-statistic observed with the contrast. You can do that using the formula&#10;$$&#10;\eta^2_p = \frac{F}{F+v_d/v_n},&#10;$$&#10;where $v_d$ is the denominator degrees of freedom for the $F$-ratio and $v_n$ is the numerator degrees of freedom.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-07-28T17:33:01.577" Id="109727" LastActivityDate="2014-07-28T17:33:01.577" OwnerUserId="5829" ParentId="109724" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="109801" AnswerCount="1" Body="&lt;p&gt;I was curious what sort of time series models were the standard for doing this type of analysis. I have weekly sales data for the company - I could cook up my own time series model but would like to know what my options are.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-07-28T18:11:08.803" Id="109732" LastActivityDate="2014-07-29T12:19:04.367" OwnerUserId="10810" PostTypeId="1" Score="0" Tags="&lt;time-series&gt;&lt;forecasting&gt;&lt;econometrics&gt;" Title="Standard techniques for forecasting revenue growth of a company?" ViewCount="86" />
  <row Body="&lt;p&gt;How would you have analyzed the data if the 2 sample sizes had worked out to be the same?  A paired test usually requires you to know the pairs, some form of ID that links the 2 surveys of the same person.  A truly anonymous survey will not have this information available making the paired test impossible.  Some surveys will include an arbitrary ID number that the respondent is to include both times and is (hopefully) unique to each respondent, but this has to be designed into the survey up front (and may reduce it from anonymous to confidential).&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, are the 304 out of the 320 a random/representative sample? or could there be a bias?  Are the 156 a random/representative sample of the 304? or could there be a bias?  If those students who improved were more likely to answer the post survey than those who declined then that could greatly bias the results.&lt;/p&gt;&#10;&#10;&lt;p&gt;Are you planning on using a finite population correction?&lt;/p&gt;&#10;&#10;&lt;p&gt;These questions should be examined before the questions that you asked as they will probably have a much larger impact on your results than the bias of using an independent t-test.  It may be that your best approach is to report summary statistics and not attempt any formal inference.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Edit&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is some R code that simulates some data based on the original numbers and compares results:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(MASS)&#10;&#10;simfun &amp;lt;- function(r=0,d=0) {&#10;    x &amp;lt;- mvrnorm(320, c(0,d), matrix( c(1,r,r,1), 2 ))&#10;    x[ sample( 320, 16 ), 1 ] &amp;lt;- NA&#10;    x[ sample( 320, 164 ), 2 ] &amp;lt;- NA&#10;    c(paired = t.test( na.omit(x)[,1], na.omit(x)[,2], paired=TRUE)$p.value,&#10;     ind1 = t.test( na.omit(x)[,1], na.omit(x)[,2] )$p.value,&#10;    ind2 = t.test( na.omit(x[,1]), na.omit(x[,2])  )$p.value)&#10;}&#10;&#10;&#10;out &amp;lt;- replicate(10000, simfun(r=0,d=0))&#10;out &amp;lt;- t(out)&#10;&#10;pairs(out)&#10;&#10;mean( out[,2] &amp;gt; out[,1] )&#10;mean( out[,3] &amp;gt; out[,1] )&#10;mean( out[,3] &amp;gt; out[,2] )&#10;&#10;mean(out[,1] &amp;lt;= 0.05)&#10;mean(out[,2] &amp;lt;= 0.05)&#10;mean(out[,3] &amp;lt;= 0.05)&#10;&#10;&#10;&#10;out &amp;lt;- replicate(10000, simfun(r=0.7, d=0.2))&#10;out &amp;lt;- t(out)&#10;&#10;pairs(out)&#10;&#10;mean( out[,2] &amp;gt; out[,1] )&#10;mean( out[,3] &amp;gt; out[,1] )&#10;mean( out[,3] &amp;gt; out[,2] )&#10;&#10;mean(out[,1] &amp;lt;= 0.05)&#10;mean(out[,2] &amp;lt;= 0.05)&#10;mean(out[,3] &amp;lt;= 0.05)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Running this code (and you can change to different values of &lt;code&gt;r&lt;/code&gt; and &lt;code&gt;d&lt;/code&gt;) shows that when there is no correlation and no difference then all 3 tests give the correct type I error rate.  With correlation and no difference the proper paired test still gives the correct type I error rate and the other 2 give an error rate below what is specified (conservative).  When there is a difference then the paired test has the most power.&lt;/p&gt;&#10;&#10;&lt;p&gt;So if you are happy with all the assumptions about representative samples and independence between responses and likelihood of responding, then you could use an independent t-test (even though you don't have independence) and just realize that the results will be conservative, p-values to large, confidence interval too wide, on average.  If the test is significant you can be confident in a significant difference.  The problem comes with p-values that are a little large than $\alpha$, they could represent a significant difference with inflated p-value. &lt;/p&gt;&#10;" CommentCount="13" CreationDate="2014-07-28T18:49:21.360" Id="109738" LastActivityDate="2014-07-28T20:30:11.187" LastEditDate="2014-07-28T20:30:11.187" LastEditorUserId="4505" OwnerUserId="4505" ParentId="109731" PostTypeId="2" Score="4" />
  
  
  
  
  
  <row Body="&lt;p&gt;Let $\text{TP}=a, \text{FP}=b, \text{TN}=c, \text{FN}=d$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The given information is:&lt;/p&gt;&#10;&#10;&lt;p&gt;$a = 0.525\ (a+d)\\&#10;b = 0.925\ (b+c)\\&#10;a = 0.516\ (a+b)\\&#10;(a+c) = 0.907\ (a+b+c+d)$&lt;/p&gt;&#10;&#10;&lt;p&gt;So&lt;/p&gt;&#10;&#10;&lt;p&gt;$d = 0.475/0.525\ a = 0.90476\ a\\&#10;b = 0.484/0.516\ a = 0.93798\ a\\&#10;c = 0.075/0.925\ b = (0.075/0.925)\ \times\ (0.484/0.516)\ a = 0.07605\ a$&lt;/p&gt;&#10;&#10;&lt;p&gt;and substituting into $(a+c) = 0.907\ (a+b+c+d)$ we get: &lt;/p&gt;&#10;&#10;&lt;p&gt;$(1+0.07605)\ a = 0.907\times (1+0.93798+0.07605+0.90476)\ a$, but&lt;/p&gt;&#10;&#10;&lt;p&gt;$(1+.07605) \neq 0.907\times (1+0.93798+0.07605+0.90476)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;So yes, it's inconsistent - no set of TN, TP, FN, FP can satisfy those.&lt;/p&gt;&#10;&#10;&lt;p&gt;Looking at sensitivity and precision, we have $d = 0.475/0.525\ a = 0.90476\ a$ and &#10;$b = 0.484/0.516\ a = 0.93798\ a$. They together imply that accuracy lies between 0.351 and 1, so they're not inconsistent with that given accuracy.&lt;/p&gt;&#10;&#10;&lt;p&gt;Looking at specificity and precision, they together imply that $(a+c)/(a+b+c)=0.5343$, meaning that accuracy must be less than 0.5343.&lt;/p&gt;&#10;&#10;&lt;p&gt;Since sensitivity and precision were consistent with accuracy, and with specificity, but those two don't seem to go together, it looks like one of specificity or accuracy would be the likely culprit.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-29T04:59:39.013" Id="109794" LastActivityDate="2014-07-29T08:13:12.947" LastEditDate="2014-07-29T08:13:12.947" LastEditorUserId="805" OwnerUserId="805" ParentId="109659" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;Just integrate directly:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\frac{dm}{dt} = (\lambda -\mu) m$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\int \frac{dm}{m} = \int (\lambda - \mu) \,dt$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\ln m = (\lambda - \mu) t + C$ for some constant&lt;/p&gt;&#10;&#10;&lt;p&gt;$m = e^{(\lambda - \mu) t + C} = ae^{(\lambda - \mu) t}$ by solving for $C$ when $t=0$.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-29T05:18:10.343" Id="109795" LastActivityDate="2014-07-29T05:18:10.343" OwnerUserId="24868" ParentId="95326" PostTypeId="2" Score="1" />
  
  
  
  <row Body="&lt;p&gt;&lt;strong&gt;Summary: PCA can be performed before LDA to regularize the problem and avoid over-fitting.&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Recall that LDA projections are computed via eigendecomposition of $\boldsymbol \Sigma_W^{-1} \boldsymbol \Sigma_B$, where $\boldsymbol \Sigma_W$ and $\boldsymbol \Sigma_B$ are within- and between-class covariance matrices. If there are less than $N$ data points (where $N$ is the dimensionality of your space, i.e. the number of features/variables), then $\boldsymbol \Sigma_W$ will be singular and therefore cannot be inverted. In this case there is simply no way to perform LDA directly, but if one applies PCA first, it will work. @Aaron made this remark in the comments to his reply, and I agree with that (but disagree with his answer in general, as you will see now).&lt;/p&gt;&#10;&#10;&lt;p&gt;However, this is only part of the problem. The bigger picture is that LDA very easily tends to overfit the data. Note that within-class covariance matrix &lt;em&gt;gets inverted&lt;/em&gt; in the LDA computations; for high-dimensional matrices inversion is a really sensitive operation that can only be reliably done if the estimate of $\boldsymbol \Sigma_W$ is really good. But in high dimensions $N \gg 1$, it is really difficult to obtain a precise estimate of $\boldsymbol \Sigma_W$, and in practice one often has to have &lt;em&gt;a lot&lt;/em&gt; more than $N$ data points to start hoping that the estimate is good. Otherwise $\boldsymbol \Sigma_W$ will be almost-singular (i.e. some of the eigenvalues will be very low), and this will cause over-fitting, i.e. near-perfect class separation on the training data with chance performance on the test data.&lt;/p&gt;&#10;&#10;&lt;p&gt;To tackle this issue, one needs to &lt;em&gt;regularize&lt;/em&gt; the problem. One way to do it is to use PCA to reduce dimensionality first. There are other, arguably better ones, e.g. &lt;em&gt;regularized LDA&lt;/em&gt; (rLDA) method which simply uses $(1-\lambda)\boldsymbol \Sigma_W + \lambda \boldsymbol I$ with small $\lambda$ instead of $\boldsymbol \Sigma_W$ (this is called &lt;em&gt;shrinkage estimator&lt;/em&gt;), but doing PCA first is conceptually the simplest approach and often works just fine.&lt;/p&gt;&#10;&#10;&lt;h2&gt;Illustration&lt;/h2&gt;&#10;&#10;&lt;p&gt;Here is an illustration of the over-fitting problem. I generated 60 samples per class in 3 classes from standard Gaussian distribution (mean zero, unit variance) in 10-, 50-, 100-, and 150-dimensional spaces, and applied LDA to project the data on 2D:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/r3lC0.png&quot; alt=&quot;Overfitting in LDA&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Note how as the dimensionality grows, classes become better and better separated, whereas in reality there is &lt;em&gt;no difference&lt;/em&gt; between the classes.&lt;/p&gt;&#10;&#10;&lt;p&gt;We can see how PCA helps to prevent the overfitting if we make classes slightly separated. I added 1 to the first coordinate of the first class, 2 to the first coordinate of the second class, and 3 to the first coordinate of the third class. Now they are slightly separated, see top left subplot:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Gv4n7.png&quot; alt=&quot;Overfitting in LDA and regularization with PCA&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Overfitting (top row) is still obvious. But if I pre-process the data with PCA, always keeping 10 dimensions (bottom row), overfitting disappears while the classes remain near-optimally separated.&lt;/p&gt;&#10;&#10;&lt;p&gt;PS. To prevent misunderstandings: I am not claiming that PCA+LDA is a good regularization strategy (on the contrary, I would advice to use rLDA), I am simply demonstrating that it is a &lt;em&gt;possible&lt;/em&gt; strategy.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Update.&lt;/strong&gt; Very similar topic has been previously discussed in the following threads with interesting and comprehensive answers provided by @cbeleites:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/54547&quot;&gt;Should PCA be performed before I do classification?&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/62714&quot;&gt;Does it make sense to run LDA on several principal components and not on all variables?&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;See also this question with some good answers:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/52773&quot;&gt;What can cause PCA to worsen results of a classifier?&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="1" CreationDate="2014-07-29T09:39:30.693" Id="109810" LastActivityDate="2015-01-13T15:43:14.057" LastEditDate="2015-01-13T15:43:14.057" LastEditorUserId="28666" OwnerUserId="28666" ParentId="106121" PostTypeId="2" Score="12" />
  
  
  <row Body="&lt;p&gt;Supposed that in Excel you create z-score of random variable P and Q, using a built-in function&lt;/p&gt;&#10;&#10;&lt;p&gt;=norminv(rand(),0,1)&lt;/p&gt;&#10;&#10;&lt;p&gt;If ρ is specified, then&lt;/p&gt;&#10;&#10;&lt;p&gt;X = P and Y = P*ρ + Q*sqrt(1-ρ^2)&lt;/p&gt;&#10;&#10;&lt;p&gt;The generated X and Y will distributed as normal distribution with zero mean and sd 1 with correlation coefficient = ρ.&lt;/p&gt;&#10;&#10;&lt;p&gt;I saw this formula a long time ago but could not find the reference now. Luckily, it works.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-29T14:13:08.710" Id="109838" LastActivityDate="2014-07-29T14:13:08.710" OwnerUserId="53036" ParentId="109824" PostTypeId="2" Score="0" />
  
  
  <row Body="&lt;p&gt;They do give you a standard error and a $z$ value, which suggests that the SAS developers thought it reasonable to use the normal distribution to approximate the sampling distribution of the statistic involved. So, if you can confirm that your $z$ statistic is equal to the estimated log rate ratio (call it $LRR$) divided by the standard error, then I would be comfortable in using $LRR \pm 2\times SE$ as an approximate 95% CI for the true log rate ratio. Supposing that this results in an interval $(\ell,u)$, then the interval $(e^\ell, e^u)$ provides a 95% CI for the incidence rate ratio itself.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2014-07-29T18:19:15.970" Id="109884" LastActivityDate="2014-07-29T18:19:15.970" OwnerUserId="52554" ParentId="109872" PostTypeId="2" Score="3" />
  
  
  
  
  
  
  <row Body="&lt;p&gt;Assuming you're asking &quot;why might we use an independent, additive Gaussian assumption?&quot; rather than &quot;why maximize entropy?&quot; more generally ...&lt;/p&gt;&#10;&#10;&lt;p&gt;I don't think it's a matter of &lt;em&gt;preferring&lt;/em&gt; white noise or (even just additive noise) as such, though it depends on the sense in which you mean 'prefer'.&lt;/p&gt;&#10;&#10;&lt;p&gt;A vector is &lt;a href=&quot;http://en.wikipedia.org/wiki/White_noise#Mathematical_definitions&quot; rel=&quot;nofollow&quot;&gt;white noise&lt;/a&gt; if&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;its components each have a probability distribution with zero mean and finite variance, and are statistically independent&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;More specifically you ask about additive Gaussian white noise. I assume we can take zero mean and finite variance as a given, so that leaves additivity, independence and normality.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;identically distributed white noise is often a reasonable first approximation to reality&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;often we can relatively easily do calculations under the assumption of additive independent Gaussian noise that are more difficult under other assumptions&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;often the procedures we get under that assumption may still have good properties under a somewhat wider set of conditions (consider, for example, the Gauss-Markov theorem; as long as you're not in a situation where all linear estimators are bad, the best linear estimator may be quite useful)&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;often the resulting estimators may be relatively easy/efficient to compute or update. Sometimes (e.g. in some online situations) you may only have time to compute/update something that's very simple to compute or update.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;sometimes it may be difficult to know what else &lt;em&gt;to&lt;/em&gt; assume, for example if sample sizes are small and you have no particularly good idea what the errors might look like &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;It may be a good starting place for something that doesn't actually assume Gaussian noise (e.g. robust procedures which are designed to perform well under contaminated normality but which perform quite well in a much wider variety of situations). This may be particularly useful when in the situation described in item 5.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Addressing the edit (though an edit above pretty much covers it):&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Why do we need iid property?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;In general, we don't &lt;em&gt;need&lt;/em&gt; it. Points 1, 4, 5 and to some extent 2 above suggest some reasons why it might be used, but we can deal with both dependence and non-identical distributions if we know what form of dependence we mean and what distributions we have.&lt;/p&gt;&#10;&#10;&lt;p&gt;I don't quite follow what your part 3 is trying to ask. Independence is not a property of a univariate distribution; and if they're not identically distributed both cdf and density (assuming continuous r.v.s) will be different.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-07-30T00:44:51.713" Id="109924" LastActivityDate="2014-07-30T11:49:35.010" LastEditDate="2014-07-30T11:49:35.010" LastEditorUserId="805" OwnerUserId="805" ParentId="109923" PostTypeId="2" Score="4" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have two groups of people, namely A and B. We have the hourly mean heart rate for 45-80 hrs, length differed by individual. We are interested in the group*time effect on the heart rate. Since I have different repeats for different times in each subject, subject ID is considered a random effect. I tried lme in R and &quot;fit model&quot; in JMP, but get different results, the effect, such as time 10, 11, or interaction terms, such as group[A]*time[3], group[A]*time[4], are significant in JMP outputs, while not in R. Can somebody tell me if my model is correct and why are they different?&lt;/p&gt;&#10;&#10;&lt;p&gt;In R, my script is:&#10;lme.RR&amp;lt;-lme(meanRR~Group*timeofday, random=~1|SubjectID)&#10;(Time of day is specified as factors)&lt;/p&gt;&#10;&#10;&lt;p&gt;In JMP, I chose Analyze -&gt; fit model, chose &quot;meanRR&quot; as Y, &quot;group&quot;,&quot;time&quot;,&quot;group*time&quot; in model effects, and add &quot;subjectID&quot; and specify it as random effects.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/QzTHs.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-30T07:29:37.190" Id="109950" LastActivityDate="2014-07-30T07:29:37.190" OwnerUserId="53080" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;random-effects-model&gt;&lt;lme&gt;&lt;jmp&gt;" Title="Random effect model result of R and JMP" ViewCount="55" />
  <row AnswerCount="0" Body="&lt;p&gt;I am beginning to look at high frequency data sets. I have only ever dealt with regularly spaced data up until now so wondering what best practice is.&lt;/p&gt;&#10;&#10;&lt;p&gt;If I have two time series X and Y with Y being the dependent variable, if both time series are irregular is the only way to regress Y against X to perform some form of interpolation on both series? &lt;/p&gt;&#10;&#10;&lt;p&gt;Or is there a way to consider the time between data points as equal in duration and then perform a regression with X lagged against Y. I assume would have to consider that if you get a data point for  X at time stamp 0 mics then the next data point in Y must occur after  xx mics since it takes time for any response to occur in Y in relation to X.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a good way to determine that duration of response time? or is this just an assumption?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-30T08:00:35.240" FavoriteCount="1" Id="109955" LastActivityDate="2014-07-30T08:00:35.240" OwnerUserId="30341" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;dataset&gt;" Title="Regression of irregular time series" ViewCount="35" />
  <row Body="&lt;p&gt;This is just a meta-analysis of proportions (or transformed values thereof). A couple articles discussing methods for this are:&lt;/p&gt;&#10;&#10;&lt;p&gt;Stijnen, T., Hamza, T. H., &amp;amp; Ozdemir, P. (2010). Random effects meta-analysis of event outcome in the framework of the generalized linear mixed model with applications in sparse data. &lt;em&gt;Statistics in Medicine, 29,&lt;/em&gt; 3046-3067.&lt;/p&gt;&#10;&#10;&lt;p&gt;Chang, B.-H., Waternaux, C., &amp;amp; Lipsitz, S. (2001). Meta-analysis of binary data: Which within study variance estimate to use? &lt;em&gt;Statistics in Medicine, 20,&lt;/em&gt; 1947-1956.&lt;/p&gt;&#10;&#10;&lt;p&gt;Zhou, X.-H., Brizendine, E. J., &amp;amp; Pritz, M. B. (1999). Methods for combining rates from several studies. &lt;em&gt;Statistics in Medicine, 18,&lt;/em&gt; 557-566.&lt;/p&gt;&#10;&#10;&lt;p&gt;A reproduction of the analyses from Stijnen et al. (2010) using the &lt;em&gt;metafor&lt;/em&gt; package in R can be found here: &lt;a href=&quot;http://www.metafor-project.org/doku.php/analyses:stijnen2010&quot; rel=&quot;nofollow&quot;&gt;http://www.metafor-project.org/doku.php/analyses:stijnen2010&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;More examples can be found in the help files of the package. In particular:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.rdocumentation.org/packages/metafor/functions/dat.debruin2009&quot; rel=&quot;nofollow&quot;&gt;http://www.rdocumentation.org/packages/metafor/functions/dat.debruin2009&lt;/a&gt;&#10;&lt;a href=&quot;http://www.rdocumentation.org/packages/metafor/functions/dat.pritz1997&quot; rel=&quot;nofollow&quot;&gt;http://www.rdocumentation.org/packages/metafor/functions/dat.pritz1997&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;If you intend on using the package, you probably also want to take a look at the paper describing the package in more detail: &lt;/p&gt;&#10;&#10;&lt;p&gt;Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. &lt;em&gt;Journal of Statistical Software, 36(3)&lt;/em&gt;, 1-48. &lt;a href=&quot;http://www.jstatsoft.org/v36/i03/&quot; rel=&quot;nofollow&quot;&gt;http://www.jstatsoft.org/v36/i03/&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-07-30T11:41:27.727" Id="109978" LastActivityDate="2014-07-30T11:41:27.727" OwnerUserId="1934" ParentId="109223" PostTypeId="2" Score="0" />
  
  
  
  
  <row AcceptedAnswerId="110501" AnswerCount="1" Body="&lt;p&gt;Let's assume we have a training set with $y \in \mathbb{R}$. Thus all the data is between $y_{min}$ and $y_{max}$. If we built a decision tree model it cannot return $y_{pred}$ outside the given range (using any combination of input features). Thus decision tree cannot extrapolate in terms of predicted values. Can a neural net regression model extrapolate and return $y_{pred}$ values outside the $y$ range in a training set? Does it depend on the activation function or not?&lt;/p&gt;&#10;&#10;&lt;p&gt;Below is my attempt to answer this question.&lt;/p&gt;&#10;&#10;&lt;p&gt;The output neuron of the model is just $\sum \Theta_ia_i$, where $\Theta_i$ - weight of i-th neuron on the previous hidden layer, and $a_i$ - value of activation function of that neuron. If we use logistic function then $a \in (-1;1)$. Thus maximum possible $y_{pred} = \sum \Theta_i$, assuming that all $a$ reach their maximum value around 1. But if we will use linear activation function, which doesn't have restrictions on output values of $a$ ($a \in \mathbb{R}$) the model will return $y_{pred} \in \mathbb{R}$, which can be ouside $y$ range of the training set.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is my line of reasoning correct or there are some mistakes?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-30T17:45:40.997" FavoriteCount="0" Id="110026" LastActivityDate="2014-08-03T20:56:56.567" OwnerUserId="5918" PostTypeId="1" Score="2" Tags="&lt;neural-networks&gt;" Title="Can neural net extrapolate output value" ViewCount="73" />
  <row AcceptedAnswerId="110039" AnswerCount="1" Body="&lt;p&gt;As the title says I want to quantify the difference in the rates of sequence change between genes. When I measure the mutation rates between my subgenomes I can determine a significant but slight difference via wilcoxon test (the distributions are gaussian but with tails). This ends up being true for many of the different types of mutations I measure, but visually I can see that different types differ in the magnitude of their difference. What is the best way to quantify the magnitude of the difference? I don't think comparing the p-values is right, and the subtraction of the median seems crude but maybe I am overthinking this.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-30T18:30:00.243" Id="110032" LastActivityDate="2014-07-30T19:21:47.847" OwnerUserId="40065" PostTypeId="1" Score="2" Tags="&lt;distributions&gt;&lt;histogram&gt;&lt;biostatistics&gt;" Title="How to measure the magnitude of difference between two gaussian distributions? Specifically with rates of DNA mutations" ViewCount="63" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have two data sets. In one of them I am controlling the predictor variable $X_1$ and observe the response on $Y$. In the second one I control $X_2$ and observe the response on $Y$. I perform linear regression for each data set separately.&lt;/p&gt;&#10;&#10;&lt;p&gt;$X_1$ and $X_2$ are different by 6 orders of magnitude. In theory, are the estimated slopes between the two data sets comparable? I suppose the F-test corrects for the difference in orders of magnitude?&lt;/p&gt;&#10;&#10;&lt;p&gt;(There may be a correlation between $X_1$ and $X_2$, but I think in my case it is okay to assume their correlation is zero.)&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-07-30T19:26:54.593" Id="110041" LastActivityDate="2014-07-30T19:26:54.593" OwnerUserId="40390" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;f-test&gt;" Title="Compare linear regression slopes for two datasets, each with a different predictor variable" ViewCount="91" />
  
  <row AcceptedAnswerId="110096" AnswerCount="1" Body="&lt;p&gt;I have a sequence of observations e.g. [&quot;Click&quot;,&quot;Scroll&quot;,&quot;Hover&quot;,&quot;Zoom&quot;,&quot;Select&quot;]. I need to predict the next value of this observation sequence but not the next hidden state.&lt;/p&gt;&#10;&#10;&lt;p&gt;I know that there are three fundamental problems for HMMs:&#10;a)Given the model parameters and observed data, we can estimate the optimal sequence of hidden states.&#10;b)Given the model parameters and observed data, we can calculate the likelihood of the data.&#10;c)Given just the observed data, we can estimate the model parameters.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, for solutions to my kind of problem:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;I thought  by referencing to b) that if I make conditional sequences of data each of them ending with one of the possible values that could stand for the next observation and calculating the likelihood of each of them given the model, then can this be considered as prediction?  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;To be more specific, in my example (if I know that the possible observations can only be Click/Scroll/Hover/Zoom/Select) I will simulate the following sequences&lt;/p&gt;&#10;&#10;&lt;p&gt;[&quot;Click&quot;,&quot;Scroll&quot;,&quot;Hover&quot;,&quot;Zoom&quot;,&quot;Select&quot;,&quot;Scroll&quot;] &lt;/p&gt;&#10;&#10;&lt;p&gt;[&quot;Click&quot;,&quot;Scroll&quot;,&quot;Hover&quot;,&quot;Zoom&quot;,&quot;Select&quot;,&quot;Click&quot;]&lt;/p&gt;&#10;&#10;&lt;p&gt;[&quot;Click&quot;,&quot;Scroll&quot;,&quot;Hover&quot;,&quot;Zoom&quot;,&quot;Select&quot;,&quot;Select&quot;]&lt;/p&gt;&#10;&#10;&lt;p&gt;[&quot;Click&quot;,&quot;Scroll&quot;,&quot;Hover&quot;,&quot;Zoom&quot;,&quot;Select&quot;,&quot;Hover&quot;]&lt;/p&gt;&#10;&#10;&lt;p&gt;[&quot;Click&quot;,&quot;Scroll&quot;,&quot;Hover&quot;,&quot;Zoom&quot;,&quot;Select&quot;,&quot;Zoom&quot;] &lt;/p&gt;&#10;&#10;&lt;p&gt;and the sequence that gives higher probability is &quot;the predicted&quot;, so eventually I  also have the predicted next observation, which will be the last observation of the sequence that gives higher likelihod. Is this correct?   &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;Another way as it is referred &lt;a href=&quot;http://stackoverflow.com/questions/13859557/how-to-make-future-prediction-with-hidden-markov-models?rq=1&quot;&gt;in this link&lt;/a&gt; would be to predict the most likely hidden-state-sequence based on a) and then through the emission distribution of the last hidden state to calculate the mean of this distribution?&#10;The above link was never verified and I am wondering if anyone could verify it.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Other way would be to get the sum of the likelihoods of all states each of them multiplied with the mean of the state's distribution. Is it correct?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Thank you in advance for any feedback you can give me.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-07-30T20:44:51.497" FavoriteCount="2" Id="110049" LastActivityDate="2014-07-31T07:04:20.783" LastEditDate="2014-07-31T06:14:26.340" LastEditorUserId="53135" OwnerUserId="53135" PostTypeId="1" Score="3" Tags="&lt;machine-learning&gt;&lt;prediction&gt;&lt;hidden-markov-model&gt;" Title="Predict observation using Hidden Markov Models" ViewCount="170" />
  
  
  <row Body="&lt;p&gt;Your NLLS has four ortoghonality conditions, one per variable plus the constant (analogue to normal equations in standard OLS) to solve for three parameters ($\rho, \beta, \alpha$ ). The Non-linear algorithms often have heterogenous configurations for tolerance parameters across softwares. MAy I suggest you drop $X_{t-1}$ from your equation in order to get exactly identified system and then test Eviews against R ? if both agree, it probably means one of them has troubles with the overidentification.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-30T22:36:25.583" Id="110061" LastActivityDate="2014-07-30T22:36:25.583" OwnerUserId="12265" ParentId="43588" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;This is a question from a coursera course:&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose we have a set of examples and Brian comes in and duplicates every example, then randomly reorders the examples. We now have twice as many examples, but no more information about the problem than we had before. If we do not remove the duplicate entries, which one of the following methods will not be affected by this change, in terms of the computer time (time in seconds, for example) it takes to come close to convergence?&lt;/p&gt;&#10;&#10;&lt;p&gt;a) full-batch learning &lt;br&gt;&#10;b) online-learning where for every iteration we randomly pick a training case &lt;br&gt;&#10;c) mini-batch learning where for every iteration we randomly pick 100 training cases&lt;/p&gt;&#10;&#10;&lt;p&gt;The answer is b. But I wonder why c is wrong. Isn't online-learning a special case of mini-batch where each iteration contains only a single training case?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-31T02:07:42.053" FavoriteCount="1" Id="110078" LastActivityDate="2014-10-24T19:56:15.583" LastEditDate="2014-10-24T19:56:15.583" LastEditorUserId="22047" OwnerUserId="27783" PostTypeId="1" Score="2" Tags="&lt;machine-learning&gt;" Title="full batch vs online learning vs mini batch" ViewCount="274" />
  
  
  <row Body="&lt;p&gt;From your question, I understood (hopefully correctly) that you want to estimate the next observation, given the observations up to now. Let $y_{1:N} = Y$ the N observations you have seen until now and let $\Theta$ be the parameters of the HMM. Then you want to infer the probability of the next observation given the already observed data, which can be expressed as:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ P(y_{N+1}|y_{1:N}=Y,\Theta)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;If this is what you want, the above conditional expression is equal to :&#10;$$ P(y_{N+1}|y_{1:N}=Y,\Theta) = \dfrac{P(y_{1:N}=Y, y_{N+1}|\Theta)}{P(y_{1:N}=Y|\Theta)}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that the denominator is independent from $y_{N+1}$. So, it is:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ P(y_{N+1}|y_{1:N}=Y,\Theta) \propto P(y_{1:N}=Y, y_{N+1}|\Theta)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;A brute force approach is the following:&lt;/p&gt;&#10;&#10;&lt;p&gt;For each of your possible observations, $y_{N+1}=Click, y_{N+1}=Scroll$ etc, calculate the likelihood of the sequences $y_{1:N+1}$. So what you need to calculate is $P(y_{N+1}=Click,y_{1:N}=Y|\Theta)$ , $P(y_{N+1}=Scroll,y_{1:N}=Y|\Theta)$, etc. for each of your possible observation sequences. Then the $y_{N+1}$ which gives the maximum likelihood can be estimated as the best guess for the next observation. Note that each of these likelihood calculations is a straightforward application of the forward pass algorithm, which corresponds to one of the three problems of HMMs: The calculation of the likelihood of a observation sequence. You have stated this in b) in your question.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hope this helps.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-07-31T06:59:09.503" Id="110096" LastActivityDate="2014-07-31T07:04:20.783" LastEditDate="2014-07-31T07:04:20.783" LastEditorUserId="31611" OwnerUserId="31611" ParentId="110049" PostTypeId="2" Score="1" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I need to do a model with a generalized linear model.&#10;My data are these: habitat : 0 or 1, group : 1 or 0 , mortality : yes or no, and the numbers of individuals for each case (habitat, group and mortality are factors).&#10;My model is &#10;&lt;code&gt;glm(moratily~indiv+habitat+group,data,family=binomial(logit))&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to see the influence of groupe and habitat on mortality.&lt;/p&gt;&#10;&#10;&lt;p&gt;and I get this message error :&#10;Warning message:&#10;glm.fit: fitted probabilities numerically 0 or 1 occurred &lt;/p&gt;&#10;&#10;&lt;p&gt;So what's wrong? &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-07-31T09:00:59.967" Id="110103" LastActivityDate="2014-08-01T19:07:13.890" LastEditDate="2014-08-01T19:07:13.890" LastEditorUserId="22311" OwnerUserId="42822" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;categorical-data&gt;&lt;generalized-linear-model&gt;&lt;binomial&gt;" Title="GLM with categorical predictor on R" ViewCount="52" />
  <row Body="&lt;p&gt;There's a very straightforward means by which to use almost any correlation measure to fit linear regressions, and which reproduces least squares when you use the Pearson correlation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Consider that if the slope of a relationship is $\beta$, the correlation between $y-\beta x$ and $x$ should be expected to be $0$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Indeed, if it were anything &lt;em&gt;other&lt;/em&gt; than $0$, there'd be some uncaptured linear relationship - which is what the correlation measure would be picking up.&lt;/p&gt;&#10;&#10;&lt;p&gt;We might therefore estimate the slope by finding the slope, $\tilde{\beta}$ that makes the &lt;em&gt;sample&lt;/em&gt; correlation between $y-\tilde{\beta} x$ and $x$ be $0$. In many cases -- e.g. when using rank-based measures -- the correlation will be a step-function of the value of the slope estimate, so there may be an interval where it's zero. In that case we normally define the sample estimate to be the center of the interval. Often the step function jumps from above zero to  below zero at some point, and in that case the estimate is at the jump point.&lt;/p&gt;&#10;&#10;&lt;p&gt;This definition works, for example, with all manner of rank based and robust correlations. It can also be used to obtain an interval for the slope (in the usual manner - by finding the slopes that mark the border between just significant correlations and just insignificant correlations).&lt;/p&gt;&#10;&#10;&lt;p&gt;This only defines the slope, of course; once the slope is estimated, the intercept can be based on a suitable location estimate computed on the residuals $y-\tilde{\beta}$. With the rank-based correlations the median is a common choice, but there are many other suitable choices.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's the correlation plotted against the slope for the &lt;code&gt;car&lt;/code&gt; data in R:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/hHntr.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The Pearson correlation crosses 0 at the least squares slope, 3.932&lt;br&gt;&#10;The Kendall correlation crosses 0 at the Theil-Sen slope, 3.667&lt;br&gt;&#10;The Spearman correlation crosses 0 giving a &quot;Spearman-line&quot; slope of 3.714  &lt;/p&gt;&#10;&#10;&lt;p&gt;Those are the three slope estimates for our example. Now we need intercepts. For simplicity I'll just use the mean residual for the first intercept and the median for the other two (it doesn't matter very much in this case):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;           intercept&#10; Pearson:  -17.573 *     &#10; Kendall:  -15.667&#10; Spearman: -16.285&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;*(the small difference from least squares is due to rounding error in the slope estimate; no doubt there's similar rounding error in the other estimates)&lt;/p&gt;&#10;&#10;&lt;p&gt;The corresponding fitted lines (using the same color scheme as above) are:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/7jpjS.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Edit: By comparison, the quadrant-correlation slope is 3.333&lt;/p&gt;&#10;&#10;&lt;p&gt;Both the Kendall correlation and Spearman correlation slopes are substantially more robust to influential outliers than least squares. See &lt;a href=&quot;http://stats.stackexchange.com/questions/46229/fast-linear-regression-robust-to-outliers/51822#51822&quot;&gt;here&lt;/a&gt; for a dramatic example in the case of the Kendall.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-07-31T10:20:50.830" Id="110112" LastActivityDate="2014-08-03T23:52:09.000" LastEditDate="2014-08-03T23:52:09.000" LastEditorUserId="805" OwnerUserId="805" ParentId="64938" PostTypeId="2" Score="8" />
  <row AcceptedAnswerId="110116" AnswerCount="3" Body="&lt;p&gt;Flipping a fair coin 21 times. The probability of getting 8 heads in any order is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$p = \frac{21!}{8!(21-8)!}0.5^8(1-0.5)^{21-8} = 0.097$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I get that the Binomial formula takes into account the different sequences in which the heads can occur during the 21 flips. But in any case, I would at least had expected the Binomial formula to give a value a bit closer to $0.5$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The question is: What am I then calculating by $8/21 = 0.38$?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-07-31T10:25:15.913" Id="110113" LastActivityDate="2014-07-31T17:32:13.993" LastEditDate="2014-07-31T11:48:29.570" LastEditorUserId="44339" OwnerUserId="52669" PostTypeId="1" Score="4" Tags="&lt;probability&gt;" Title="Why is p for 8 times heads out of 21 flips not 8/21?" ViewCount="1325" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I need help with complete interpretation of this output(below). Can someone explain all (almost all) estimators in this output: what they mean and what is the substantive interpretation. &lt;/p&gt;&#10;&#10;&lt;p&gt;In addition: Which tools would you recommend to test for these estimators? &lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you! &lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;inter.coxph &amp;lt;- coxph( Surv(time,censor)~age+drug+age*drug, method=&quot;breslow&quot;)&#10;summary(inter.coxph)&#10;&#10; n= 100 &#10;&#10;        coef exp(coef) se(coef)      z       p &#10; age  0.0942     1.099   0.0229  4.110 0.00004&#10;drug  1.1859     3.274   1.2565  0.944 0.35000&#10;age:drug -0.0067     0.993   0.0337 -0.199 0.84000&#10;&#10;     exp(coef) exp(-coef) lower .95 upper .95 &#10; age     1.099      0.910     1.051      1.15&#10;drug     3.274      0.305     0.279     38.42&#10;age:drug     0.993      1.007     0.930      1.06&#10;&#10;&#10; Rsquare= 0.295   (max possible= 0.997 )&#10; Likelihood ratio test= 35    on 3 df,   p=1.21e-007&#10; Wald test            = 32.2  on 3 df,   p=4.83e-007&#10; Score (logrank) test = 35.2  on 3 df,   p=1.13e-007&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2014-07-31T11:51:01.817" Id="110124" LastActivityDate="2014-07-31T11:59:01.110" LastEditDate="2014-07-31T11:59:01.110" LastEditorUserId="805" OwnerUserId="53160" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;survival&gt;&lt;interpretation&gt;&lt;cox-model&gt;" Title="Complete interpretation of Cox regression output in R" ViewCount="100" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Given some fixed vector $\mathbf{x}_0\in\mathbb{R}^d$, I want to put a prior on a random variable $\mathbf{x}\in\mathbb{R}^d$ so that &quot;it's not very close to $\mathbf{x}_0$&quot;. For the moment I put a Cauchy prior on $\mathbf{x}$, which is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;\mathrm{pdf}(\mathbf{x};\mathbf{x}_0,\mathbf{a})\propto\frac{1}{(1+\|\mathbf{x}-(\mathbf{x}_0+\mathbf{a})\|^2)^{\frac{d+1}{2}}}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\mathbf{a}$ is the offset parameter. Are there any other &quot;less heuristic&quot; choices? Thank you.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-31T14:14:41.727" Id="110144" LastActivityDate="2014-07-31T19:19:18.817" OwnerUserId="4864" PostTypeId="1" Score="0" Tags="&lt;prior&gt;&lt;heavy-tailed&gt;" Title="Weakly informative priors for &quot;r.v. $\mathbf{x}$ is not close to $\mathbf{x}_0$&quot;?" ViewCount="32" />
  <row Body="&lt;p&gt;Do you use a nugget? If you don't, the prediction on the points used for the estimation of the model are just the observed points, then if you try to make prediction on then you will have a  correlation =1 and the matrix can be singular.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-31T16:33:13.223" Id="110163" LastActivityDate="2014-07-31T16:33:13.223" OwnerUserId="27213" ParentId="110156" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I have data from{ Erne /Soho}   for  time in unit( hour )on x-axis  and the intensity  in unit                    ( 1/s.sr.cm2.Mev/n)  on y axis the data so long  from 1500 points  to 2500 points  and the curves look like Gaussian curve  i need your help about :&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Plot and find the fitting , the polyvalue and  polynomial&lt;/li&gt;&#10;&lt;li&gt;Find the slope ( angle ) for the rising points from the point that beginning  to rise to the peak of the curve&lt;/li&gt;&#10;&lt;li&gt;Repeat step 2 for points from peak to the end decay&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;the data is:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x(time in hrs)  y(intensity)&#10;       X    y&#10;0.041667    0.000171&#10;0.125   0.000161&#10;0.208333    0.000101&#10;0.291667    0.000139&#10;0.375   0.0000565&#10;0.458333    0.000124&#10;0.541667    0.0000669&#10;0.625   0.0000908&#10;0.708333    0.0000869&#10;0.791667    0.000164&#10;0.875   0.000137&#10;0.958333    0.0000885&#10;1.041667    0.000162&#10;1.125   0.000141&#10;1.208333    0.000125&#10;1.291667    0.000104&#10;1.375   0.000067&#10;1.458333    0.0000658&#10;1.541667    0.000135&#10;1.625   0.0000691&#10;1.708333    0.0000571&#10;1.791667    0.0000895&#10;1.875   0.000149&#10;1.958333    0.000118&#10;2.041667    0.000162&#10;2.125   0.00016&#10;2.208333    0.0000777&#10;2.291667    0.0000882&#10;2.375   0.000156&#10;2.458333    0.0000706&#10;2.541667    0.000136&#10;2.625   0.000102&#10;2.708333    0.000143&#10;2.791667    0.0000921&#10;2.875   0.0000803&#10;2.958333    0.000106&#10;3.041667    0.0000803&#10;3.125   0.000117&#10;3.208333    0.000161&#10;3.291667    0.000183&#10;3.375   0.0000588&#10;3.458333    0.0000687&#10;3.541667    0.000138&#10;3.625   0.000101&#10;3.708333    0.000126&#10;3.791667    0.0000352&#10;3.875   0.0000344&#10;3.958333    0.0000684&#10;4.041667    0.000102&#10;4.125   0.000149&#10;4.208333    0.000147&#10;4.291667    0.0000222&#10;4.375   0.000034&#10;4.458333    0.0000929&#10;4.541667    0.000092&#10;4.625   0.00008&#10;4.708333    0.000101&#10;4.791667    0.000145&#10;4.875   0.0000794&#10;4.958333    0.000102&#10;5.041667    0.000283&#10;5.125   0.000341&#10;5.208333    0.000711&#10;5.291667    0.000892&#10;5.375   0.001269&#10;5.458333    0.001324&#10;5.541667    0.001479&#10;5.625   0.002049&#10;5.708333    0.001915&#10;5.791667    0.002428&#10;5.875   0.002894&#10;5.958333    0.003405&#10;6.041667    0.003135&#10;6.125   0.003133&#10;6.208333    0.003374&#10;6.291667    0.003532&#10;6.375   0.003844&#10;6.458333    0.003582&#10;6.541667    0.004157&#10;6.625   0.004822&#10;6.708333    0.005595&#10;6.791667    0.006492&#10;6.875   0.00688&#10;6.958333    0.007222&#10;7.041667    0.007582&#10;7.125   0.007961&#10;7.208333    0.007937&#10;7.291667    0.007869&#10;7.375   0.007804&#10;7.458333    0.007741&#10;7.541667    0.007369&#10;7.625   0.006984&#10;7.708333    0.00662&#10;7.791667    0.006275&#10;7.875   0.006338&#10;7.958333    0.006447&#10;8.041667    0.006536&#10;8.125   0.006673&#10;8.208333    0.006869&#10;8.291667    0.007084&#10;8.375   0.007309&#10;8.458333    0.007546&#10;8.541667    0.007243&#10;8.625   0.006907&#10;8.708333    0.006596&#10;8.791667    0.00631&#10;8.875   0.006351&#10;8.958333    0.006428&#10;9.041667    0.006509&#10;9.125   0.006592&#10;9.208333    0.006545&#10;9.291667    0.006488&#10;9.375   0.006433&#10;9.458333    0.006382&#10;9.541667    0.006437&#10;9.625   0.006511&#10;9.708333    0.006594&#10;9.791667    0.006685&#10;9.875   0.006784&#10;9.958333    0.006889&#10;10.04167    0.006998&#10;10.125  0.007113&#10;10.20833    0.007222&#10;10.29167    0.007335&#10;10.375  0.007453&#10;10.45833    0.007577&#10;10.54167    0.007719&#10;10.625  0.007867&#10;10.70833    0.00802&#10;10.79167    0.008178&#10;10.875  0.008258&#10;10.95833    0.008332&#10;11.04167    0.008407&#10;11.125  0.008483&#10;11.20833    0.008638&#10;11.29167    0.008805&#10;11.375  0.008976&#10;11.45833    0.009151&#10;11.54167    0.008961&#10;11.625  0.00874&#10;11.70833    0.008531&#10;11.79167    0.008333&#10;11.875  0.008155&#10;11.95833    0.007988&#10;12.04167    0.007829&#10;12.125  0.00768&#10;12.20833    0.00746&#10;12.29167    0.007242&#10;12.375  0.007034&#10;12.45833    0.006835&#10;12.54167    0.006705&#10;12.625  0.006584&#10;12.70833    0.006466&#10;12.79167    0.006351&#10;12.875  0.006259&#10;12.95833    0.006173&#10;13.04167    0.006093&#10;13.125  0.006017&#10;13.20833    0.005997&#10;13.29167    0.005985&#10;13.375  0.005975&#10;13.45833    0.005969&#10;13.54167    0.005572&#10;13.625  0.005167&#10;13.70833    0.0048&#10;13.79167    0.004464&#10;13.875  0.004469&#10;13.95833    0.004521&#10;14.04167    0.004583&#10;14.125  0.004694&#10;14.20833    0.00436&#10;14.29167    0.004653&#10;14.375  0.004479&#10;14.45833    0.0044&#10;14.54167    0.004331&#10;14.625  0.004178&#10;14.70833    0.004028&#10;14.79167    0.004093&#10;14.875  0.003896&#10;14.95833    0.003949&#10;15.04167    0.003895&#10;15.125  0.003843&#10;15.20833    0.003795&#10;15.29167    0.004257&#10;15.375  0.00382&#10;15.45833    0.003776&#10;15.54167    0.003662&#10;15.625  0.003628&#10;15.70833    0.00356&#10;15.79167    0.00358&#10;15.875  0.003705&#10;15.95833    0.00354&#10;16.04167    0.002857&#10;16.125  0.00322&#10;16.20833    0.003162&#10;16.29167    0.003105&#10;16.375  0.003049&#10;16.45833    0.003023&#10;16.54167    0.002811&#10;16.625  0.003027&#10;16.70833    0.002789&#10;16.79167    0.002864&#10;16.875  0.002708&#10;16.95833    0.002964&#10;17.04167    0.003224&#10;17.125  0.002653&#10;17.20833    0.002214&#10;17.29167    0.002505&#10;17.375  0.002372&#10;17.45833    0.00202&#10;17.54167    0.002151&#10;17.625  0.00267&#10;17.70833    0.002533&#10;17.79167    0.002248&#10;17.875  0.002042&#10;17.95833    0.002338&#10;18.04167    0.002448&#10;18.125  0.002201&#10;18.20833    0.001753&#10;18.29167    0.001924&#10;18.375  0.001724&#10;18.45833    0.0019&#10;18.54167    0.00188&#10;18.625  0.001866&#10;18.70833    0.001861&#10;18.79167    0.001848&#10;18.875  0.001671&#10;18.95833    0.002026&#10;19.04167    0.001911&#10;19.125  0.001747&#10;19.20833    0.002013&#10;19.29167    0.001459&#10;19.375  0.001561&#10;19.45833    0.001312&#10;19.54167    0.001475&#10;19.625  0.00186&#10;19.70833    0.001447&#10;19.79167    0.001411&#10;19.875  0.001464&#10;19.95833    0.001573&#10;20.04167    0.001883&#10;20.125  0.001447&#10;20.20833    0.001521&#10;20.29167    0.001419&#10;20.375  0.00148&#10;20.45833    0.001458&#10;20.54167    0.001428&#10;20.625  0.001622&#10;20.70833    0.001325&#10;20.79167    0.001302&#10;20.875  0.001308&#10;20.95833    0.001322&#10;21.04167    0.001437&#10;21.125  0.001255&#10;21.20833    0.0012&#10;21.29167    0.001379&#10;21.375  0.001139&#10;21.45833    0.001016&#10;21.54167    0.001307&#10;21.625  0.000904&#10;21.70833    0.00098&#10;21.79167    0.000771&#10;21.875  0.000852&#10;21.95833    0.001013&#10;22.04167    0.001236&#10;22.125  0.00102&#10;22.20833    0.000923&#10;22.29167    0.001115&#10;22.375  0.001144&#10;22.45833    0.001097&#10;22.54167    0.000909&#10;22.625  0.001157&#10;22.70833    0.00095&#10;22.79167    0.001014&#10;22.875  0.001137&#10;22.95833    0.00109&#10;23.04167    0.001046&#10;23.125  0.001032&#10;23.20833    0.000918&#10;23.29167    0.000785&#10;23.375  0.000938&#10;23.45833    0.00086&#10;23.54167    0.000814&#10;23.625  0.000818&#10;23.70833    0.000865&#10;23.79167    0.000951&#10;23.875  0.001015&#10;23.95833    0.000808&#10;24.04167    0.000782&#10;24.125  0.000609&#10;24.20833    0.000741&#10;24.29167    0.000756&#10;24.375  0.00084&#10;24.45833    0.000786&#10;24.54167    0.000659&#10;24.625  0.000725&#10;24.70833    0.000603&#10;24.79167    0.000748&#10;24.875  0.000664&#10;24.95833    0.000719&#10;25.04167    0.000648&#10;25.125  0.000688&#10;25.20833    0.000528&#10;25.29167    0.000644&#10;25.375  0.000612&#10;25.45833    0.000538&#10;25.54167    0.000597&#10;25.625  0.000653&#10;25.70833    0.000598&#10;25.79167    0.000632&#10;25.875  0.000594&#10;25.95833    0.000501&#10;26.04167    0.000652&#10;26.125  0.000739&#10;26.20833    0.000511&#10;26.29167    0.000611&#10;26.375  0.00053&#10;26.45833    0.000646&#10;26.54167    0.000533&#10;26.625  0.00063&#10;26.70833    0.000551&#10;26.79167    0.000406&#10;26.875  0.000699&#10;26.95833    0.000631&#10;27.04167    0.000674&#10;27.125  0.000469&#10;27.20833    0.000565&#10;27.29167    0.000397&#10;27.375  0.000589&#10;27.45833    0.000464&#10;27.54167    0.000573&#10;27.625  0.000482&#10;27.70833    0.000569&#10;27.79167    0.000526&#10;27.875  0.000482&#10;27.95833    0.000458&#10;28.04167    0.000569&#10;28.125  0.000515&#10;28.20833    0.000638&#10;28.29167    0.000516&#10;28.375  0.000525&#10;28.45833    0.000605&#10;28.54167    0.000469&#10;28.625  0.000492&#10;28.70833    0.000262&#10;28.79167    0.000347&#10;28.875  0.000434&#10;28.95833    0.000399&#10;29.04167    0.000448&#10;29.125  0.000473&#10;29.20833    0.000386&#10;29.29167    0.00043&#10;29.375  0.000403&#10;29.45833    0.000465&#10;29.54167    0.000441&#10;29.625  0.000375&#10;29.70833    0.000403&#10;29.79167    0.000374&#10;29.875  0.000328&#10;29.95833    0.000459&#10;30.04167    0.000365&#10;30.125  0.000403&#10;30.20833    0.00037&#10;30.29167    0.000587&#10;30.375  0.000441&#10;30.45833    0.000316&#10;30.54167    0.000379&#10;30.625  0.000477&#10;30.70833    0.000425&#10;30.79167    0.000488&#10;30.875  0.000358&#10;30.95833    0.000369&#10;31.04167    0.000392&#10;31.125  0.000399&#10;31.20833    0.000326&#10;31.29167    0.0004&#10;31.375  0.000283&#10;31.45833    0.00027&#10;31.54167    0.000323&#10;31.625  0.000356&#10;31.70833    0.000415&#10;31.79167    0.000388&#10;31.875  0.000259&#10;31.95833    0.000488&#10;32.04167    0.000304&#10;32.125  0.00041&#10;32.20833    0.000395&#10;32.29167    0.000344&#10;32.375  0.000382&#10;32.45833    0.000357&#10;32.54167    0.000431&#10;32.625  0.000289&#10;32.70833    0.000346&#10;32.79167    0.000302&#10;32.875  0.000237&#10;32.95833    0.000366&#10;33.04167    0.000272&#10;33.125  0.000205&#10;33.20833    0.000322&#10;33.29167    0.000299&#10;33.375  0.000315&#10;33.45833    0.000277&#10;33.54167    0.000204&#10;33.625  0.000421&#10;33.70833    0.000304&#10;33.79167    0.000281&#10;33.875  0.000289&#10;33.95833    0.000214&#10;34.04167    0.000281&#10;34.125  0.000266&#10;34.20833    0.000268&#10;34.29167    0.000267&#10;34.375  0.000288&#10;34.45833    0.000202&#10;34.54167    0.000309&#10;34.625  0.000299&#10;34.70833    0.000393&#10;34.79167    0.000211&#10;34.875  0.000286&#10;34.95833    0.000317&#10;35.04167    0.000224&#10;35.125  0.000233&#10;35.20833    0.000298&#10;35.29167    0.000234&#10;35.375  0.000202&#10;35.45833    0.000255&#10;35.54167    0.000287&#10;35.625  0.000274&#10;35.70833    0.000319&#10;35.79167    0.000254&#10;35.875  0.000297&#10;35.95833    0.000382&#10;36.04167    0.0003&#10;36.125  0.000263&#10;36.20833    0.000201&#10;36.29167    0.000243&#10;36.375  0.00018&#10;36.45833    0.00019&#10;36.54167    0.000284&#10;36.625  0.000232&#10;36.70833    0.000224&#10;36.79167    0.000199&#10;36.875  0.000317&#10;36.95833    0.00021&#10;37.04167    0.000233&#10;37.125  0.00019&#10;37.20833    0.000137&#10;37.29167    0.000127&#10;37.375  0.000254&#10;37.45833    0.000272&#10;37.54167    0.000335&#10;37.625  0.000221&#10;37.70833    0.000234&#10;37.79167    0.000178&#10;37.875  0.000283&#10;37.95833    0.000282&#10;38.04167    0.000179&#10;38.125  0.000136&#10;38.20833    0.000137&#10;38.29167    0.000147&#10;38.375  0.000178&#10;38.45833    0.000242&#10;38.54167    0.000136&#10;38.625  0.000136&#10;38.70833    0.000316&#10;38.79167    0.000284&#10;38.875  0.000199&#10;38.95833    0.000221&#10;39.04167    0.000168&#10;39.125  0.00021&#10;39.20833    0.000208&#10;39.29167    0.000271&#10;39.375  0.000199&#10;39.45833    0.000199&#10;39.54167    0.000251&#10;39.625  0.000188&#10;39.70833    0.000231&#10;39.79167    0.000147&#10;39.875  0.000105&#10;39.95833    0.000197&#10;40.04167    0.0000835&#10;40.125  0.00025&#10;40.20833    0.000229&#10;40.29167    0.000209&#10;40.375  0.000158&#10;40.45833    0.000147&#10;40.54167    0.000115&#10;40.625  0.000187&#10;40.70833    0.000221&#10;40.79167    0.00023&#10;40.875  0.000229&#10;40.95833    0.000157&#10;41.04167    0.000241&#10;41.125  0.000208&#10;41.20833    0.000178&#10;41.29167    0.000187&#10;41.375  0.000188&#10;41.45833    0.000187&#10;41.54167    0.000188&#10;41.625  0.000105&#10;41.70833    0.000199&#10;41.79167    0.000261&#10;41.875  0.000155&#10;41.95833    0.000166&#10;42.04167    0.000146&#10;42.125  0.000209&#10;42.20833    0.000177&#10;42.29167    0.000199&#10;42.375  0.000137&#10;42.45833    0.000168&#10;42.54167    0.000126&#10;42.625  0.000115&#10;42.70833    0.0000847&#10;42.79167    0.000167&#10;42.875  0.000189&#10;42.95833    0.000177&#10;43.04167    0.0000947&#10;43.125  0.000156&#10;43.20833    0.000178&#10;43.29167    0.000105&#10;43.375  0.000193&#10;43.45833    0.000157&#10;43.54167    0.000136&#10;43.625  0.000115&#10;43.70833    0.000179&#10;43.79167    0.000148&#10;43.875  0.000114&#10;43.95833    0.000127&#10;44.04167    0.000179&#10;44.125  0.000211&#10;44.20833    0.00022&#10;44.29167    0.000148&#10;44.375  0.000126&#10;44.45833    0.000106&#10;44.54167    0.000096&#10;44.625  0.000171&#10;44.70833    0.0000954&#10;44.79167    0.000157&#10;44.875  0.000213&#10;44.95833    0.000107&#10;45.04167    0.00017&#10;45.125  0.000136&#10;45.20833    0.000168&#10;45.29167    0.000162&#10;45.375  0.000117&#10;45.45833    0.000148&#10;45.54167    0.000178&#10;45.625  0.000116&#10;45.70833    0.000136&#10;45.79167    0.000159&#10;45.875  0.000116&#10;45.95833    0.000169&#10;46.04167    0.000191&#10;46.125  0.000104&#10;46.20833    0.000117&#10;46.29167    0.00016&#10;46.375  0.000201&#10;46.45833    0.000128&#10;46.54167    0.000127&#10;46.625  0.00016&#10;46.70833    0.000139&#10;46.79167    0.0000964&#10;46.875  0.000126&#10;46.95833    0.000137&#10;47.04167    0.000159&#10;47.125  0.000138&#10;47.20833    0.0000862&#10;47.29167    0.000129&#10;47.375  0.000108&#10;47.45833    0.0000846&#10;47.54167    0.000171&#10;47.625  0.00016&#10;47.70833    0.000149&#10;47.79167    0.00013&#10;47.875  0.000106&#10;47.95833    0.000181&#10;48.04167    0.000148&#10;48.125  0.000139&#10;48.20833    0.000128&#10;48.29167    0.000149&#10;48.375  0.0000754&#10;48.45833    0.000116&#10;48.54167    0.000194&#10;48.625  0.000158&#10;48.70833    0.000106&#10;48.79167    0.000179&#10;48.875  0.000139&#10;48.95833    0.000107&#10;49.04167    0.00018&#10;49.125  0.000116&#10;49.20833    0.000106&#10;49.29167    0.000161&#10;49.375  0.000106&#10;49.45833    0.00014&#10;49.54167    0.0000848&#10;49.625  0.000116&#10;49.70833    0.000141&#10;49.79167    0.000173&#10;49.875  0.000117&#10;49.95833    0.000128&#10;50.04167    0.0000848&#10;50.125  0.000182&#10;50.20833    0.00014&#10;50.29167    0.000105&#10;50.375  0.0000863&#10;50.45833    0.000174&#10;50.54167    0.000213&#10;50.625  0.000159&#10;50.70833    0.000118&#10;50.79167    0.000138&#10;50.875  0.000139&#10;50.95833    0.000191&#10;51.04167    0.000117&#10;51.125  0.000181&#10;51.20833    0.000169&#10;51.29167    0.000148&#10;51.375  0.000138&#10;51.45833    0.00017&#10;51.54167    0.000203&#10;51.625  0.000117&#10;51.70833    0.000126&#10;51.79167    0.00016&#10;51.875  0.000076&#10;51.95833    0.0000845&#10;52.04167    0.000128&#10;52.125  0.00015&#10;52.20833    0.000127&#10;52.29167    0.0000955&#10;52.375  0.000224&#10;52.45833    0.00013&#10;52.54167    0.0000851&#10;52.625  0.00012&#10;52.70833    0.000108&#10;52.79167    0.000116&#10;52.875  0.000129&#10;52.95833    0.000149&#10;53.04167    0.0000954&#10;53.125  0.000128&#10;53.20833    0.000097&#10;53.29167    0.000183&#10;53.375  0.0000856&#10;53.45833    0.000173&#10;53.54167    0.00014&#10;53.625  0.000128&#10;53.70833    0.0000852&#10;53.79167    0.000128&#10;53.875  0.0000954&#10;53.95833    0.000129&#10;54.04167    0.0000651&#10;54.125  0.000117&#10;54.20833    0.0000867&#10;54.29167    0.000164&#10;54.375  0.0000859&#10;54.45833    0.000194&#10;54.54167    0.000142&#10;54.625  0.0000988&#10;54.70833    0.000118&#10;54.79167    0.000176&#10;54.875  0.000141&#10;54.95833    0.000119&#10;55.04167    0.000106&#10;55.125  0.0000856&#10;55.20833    0.000107&#10;55.29167    0.0000965&#10;55.375  0.000129&#10;55.45833    0.000141&#10;55.54167    0.000176&#10;55.625  0.000108&#10;55.70833    0.0000647&#10;55.79167    0.000108&#10;55.875  0.000143&#10;55.95833    0.0000765&#10;56.04167    0.000108&#10;56.125  0.0000656&#10;56.20833    0.0000872&#10;56.29167    0.000129&#10;56.375  0.000101&#10;56.45833    0.0000883&#10;56.54167    0.000139&#10;56.625  0.000142&#10;56.70833    0.000128&#10;56.79167    0.000199&#10;56.875  0.0000539&#10;56.95833    0.0000549&#10;57.04167    0.000161&#10;57.125  0.000119&#10;57.20833    0.000137&#10;57.29167    0.000121&#10;57.375  0.000177&#10;57.45833    0.000134&#10;57.54167    0.000153&#10;57.625  0.000176&#10;57.70833    0.000166&#10;57.79167    0.00014&#10;57.875  0.000121&#10;57.95833    0.000108&#10;58.04167    0.0000757&#10;58.125  0.000164&#10;58.20833    0.000192&#10;58.29167    0.0000867&#10;58.375  0.000173&#10;58.45833    0.000129&#10;58.54167    0.0000863&#10;58.625  0.00015&#10;58.70833    0.000098&#10;58.79167    0.0000767&#10;58.875  0.00011&#10;58.95833    0.000111&#10;59.04167    0.000108&#10;59.125  0.000173&#10;59.20833    0.000117&#10;59.29167    0.0000442&#10;59.375  0.00011&#10;59.45833    0.00012&#10;59.54167    0.00014&#10;59.625  0.000076&#10;59.70833    0.000195&#10;59.79167    0.000165&#10;59.875  0.000163&#10;59.95833    0.000163&#10;60.04167    0.0001&#10;60.125  0.0000996&#10;60.20833    0.00011&#10;60.29167    0.0000649&#10;60.375  0.0000764&#10;60.45833    0.000172&#10;60.54167    0.0000871&#10;60.625  0.000121&#10;60.70833    0.000207&#10;60.79167    0.000128&#10;60.875  0.000109&#10;60.95833    0.000179&#10;61.04167    0.000144&#10;61.125  0.00012&#10;61.20833    0.0000646&#10;61.29167    0.0000764&#10;61.375  0.00013&#10;61.45833    0.00017&#10;61.54167    0.000184&#10;61.625  0.000153&#10;61.70833    0.000169&#10;61.79167    0.0000546&#10;61.875  0.000142&#10;61.95833    0.0000884&#10;62.04167    0.000113&#10;62.125  0.000153&#10;62.20833    0.000179&#10;62.29167    0.0000672&#10;62.375  0.0000989&#10;62.45833    0.00011&#10;62.54167    0.000175&#10;62.625  0.0000659&#10;62.70833    0.000121&#10;62.79167    0.000164&#10;62.875  0.000141&#10;62.95833    0.000146&#10;63.04167    0.000139&#10;63.125  0.000155&#10;63.20833    0.000145&#10;63.29167    0.000111&#10;63.375  0.000165&#10;63.45833    0.000111&#10;63.54167    0.000133&#10;63.625  0.0000764&#10;63.70833    0.0000883&#10;63.79167    0.000145&#10;63.875  0.000124&#10;63.95833    0.000214&#10;64.04167    0.000089&#10;64.125  0.000122&#10;64.20833    0.000144&#10;64.29167    0.000165&#10;64.375  0.00012&#10;64.45833    0.0000664&#10;64.54167    0.000165&#10;64.625  0.0000668&#10;64.70833    0.000111&#10;64.79167    0.000135&#10;64.875  0.000111&#10;64.95833    0.000122&#10;65.04167    0.000134&#10;65.125  0.0000989&#10;65.20833    0.000172&#10;65.29167    0.000167&#10;65.375  0.000145&#10;65.45833    0.000123&#10;65.54167    0.000195&#10;65.625  0.0000881&#10;65.70833    0.0000893&#10;65.79167    0.000146&#10;65.875  0.000123&#10;65.95833    0.000174&#10;66.04167    0.000119&#10;66.125  0.000122&#10;66.20833    0.000151&#10;66.29167    0.000177&#10;66.375  0.0001&#10;66.45833    0.0000795&#10;66.54167    0.0000768&#10;66.625  0.000124&#10;66.70833    0.000176&#10;66.79167    0.000168&#10;66.875  0.000133&#10;66.95833    0.0000772&#10;67.04167    0.000112&#10;67.125  0.0000895&#10;67.20833    0.0000801&#10;67.29167    0.000125&#10;67.375  0.000112&#10;67.45833    0.000144&#10;67.54167    0.0000975&#10;67.625  0.000101&#10;67.70833    0.0000664&#10;67.79167    0.000191&#10;67.875  0.000119&#10;67.95833    0.000168&#10;68.04167    0.000213&#10;68.125  0.0000803&#10;68.20833    0.00013&#10;68.29167    0.0000996&#10;68.375  0.000101&#10;68.45833    0.000122&#10;68.54167    0.000068&#10;68.625  0.000144&#10;68.70833    0.0000908&#10;68.79167    0.000157&#10;68.875  0.00011&#10;68.95833    0.0000796&#10;69.04167    0.000115&#10;69.125  0.000178&#10;69.20833    0.0000911&#10;69.29167    0.00009&#10;69.375  0.000149&#10;69.45833    0.000121&#10;69.54167    0.000135&#10;69.625  0.000113&#10;69.70833    0.000103&#10;69.79167    0.000145&#10;69.875  0.0000814&#10;69.95833    0.000111&#10;70.04167    0.000113&#10;70.125  0.000115&#10;70.20833    0.000133&#10;70.29167    0.0000561&#10;70.375  0.0000904&#10;70.45833    0.000124&#10;70.54167    0.0000662&#10;70.625  0.000101&#10;70.70833    0.0000769&#10;70.79167    0.0000889&#10;70.875  0.000125&#10;70.95833    0.000125&#10;71.04167    0.000147&#10;71.125  0.000132&#10;71.20833    0.0000882&#10;71.29167    0.000159&#10;71.375  0.000137&#10;71.45833    0.000135&#10;71.54167    0.0000879&#10;71.625  0.0000972&#10;71.70833    0.0000781&#10;71.79167    0.000157&#10;71.875  0.000103&#10;71.95833    0.000155&#10;72.04167    0.000155&#10;72.125  0.0000677&#10;72.20833    0.000101&#10;72.29167    0.000135&#10;72.375  0.000114&#10;72.45833    0.000211&#10;72.54167    0.000114&#10;72.625  0.000195&#10;72.70833    0.000225&#10;72.79167    0.000131&#10;72.875  0.000121&#10;72.95833    0.000143&#10;73.04167    0.000101&#10;73.125  0.0000552&#10;73.20833    0.000101&#10;73.29167    0.000123&#10;73.375  0.000134&#10;73.45833    0.0000788&#10;73.54167    0.00011&#10;73.625  0.0000572&#10;73.70833    0.0000805&#10;73.79167    0.0000909&#10;73.875  0.000203&#10;73.95833    0.000144&#10;74.04167    0.000156&#10;74.125  0.000144&#10;74.20833    0.0000785&#10;74.29167    0.00016&#10;74.375  0.000101&#10;74.45833    0.000145&#10;74.54167    0.000155&#10;74.625  0.0000895&#10;74.70833    0.000125&#10;74.79167    0.000167&#10;74.875  0.0000559&#10;74.95833    0.000128&#10;75.04167    0.000101&#10;75.125  0.0000892&#10;75.20833    0.0000912&#10;75.29167    0.000145&#10;75.375  0.000112&#10;75.45833    0.0000904&#10;75.54167    0.000167&#10;75.625  0.000137&#10;75.70833    0.000103&#10;75.79167    0.000172&#10;75.875  0.0000915&#10;75.95833    0.000103&#10;76.04167    0.0000977&#10;76.125  0.000101&#10;76.20833    0.0000871&#10;76.29167    0.0001&#10;76.375  0.00008&#10;76.45833    0.000069&#10;76.54167    0.000116&#10;76.625  0.0000796&#10;76.70833    0.0000669&#10;76.79167    0.000122&#10;76.875  0.000145&#10;76.95833    0.0000993&#10;77.04167    0.0000785&#10;77.125  0.000102&#10;77.20833    0.000113&#10;77.29167    0.000099&#10;77.375  0.000141&#10;77.45833    0.0000928&#10;77.54167    0.000114&#10;77.625  0.000194&#10;77.70833    0.000219&#10;77.79167    0.000223&#10;77.875  0.0000917&#10;77.95833    0.000124&#10;78.04167    0.0000465&#10;78.125  0.000182&#10;78.20833    0.000147&#10;78.29167    0.000091&#10;78.375  0.00017&#10;78.45833    0.000124&#10;78.54167    0.000112&#10;78.625  0.0000696&#10;78.70833    0.0000942&#10;78.79167    0.0000947&#10;78.875  0.000136&#10;78.95833    0.000161&#10;79.04167    0.000138&#10;79.125  0.000185&#10;79.20833    0.0000931&#10;79.29167    0.0000659&#10;79.375  0.000109&#10;79.45833    0.000079&#10;79.54167    0.000101&#10;79.625  0.000115&#10;79.70833    0.000184&#10;79.79167    0.000123&#10;79.875  0.0000787&#10;79.95833    0.000127&#10;80.04167    0.000122&#10;80.125  0.0000794&#10;80.20833    0.0000657&#10;80.29167    0.0000967&#10;80.375  0.000173&#10;80.45833    0.0000923&#10;80.54167    0.0000912&#10;80.625  0.000226&#10;80.70833    0.000135&#10;80.79167    0.000147&#10;80.875  0.000123&#10;80.95833    0.000159&#10;81.04167    0.0000683&#10;81.125  0.000172&#10;81.20833    0.000161&#10;81.29167    0.00013&#10;81.375  0.000147&#10;81.45833    0.000113&#10;81.54167    0.000149&#10;81.625  0.0000423&#10;81.70833    0.000128&#10;81.79167    0.000103&#10;81.875  0.000139&#10;81.95833    0.000191&#10;82.04167    0.000103&#10;82.125  0.000115&#10;82.20833    0.0000915&#10;82.29167    0.000147&#10;82.375  0.0000866&#10;82.45833    0.0000928&#10;82.54167    0.000114&#10;82.625  0.0000978&#10;82.70833    0.0000872&#10;82.79167    0.000113&#10;82.875  0.000181&#10;82.95833    0.000102&#10;83.04167    0.000103&#10;83.125  0.000154&#10;83.20833    0.000124&#10;83.29167    0.000154&#10;83.375  0.000162&#10;83.45833    0.000112&#10;83.54167    0.000105&#10;83.625  0.0000771&#10;83.70833    0.000113&#10;83.79167    0.000213&#10;83.875  0.000185&#10;83.95833    0.0000373&#10;84.04167    0.00024&#10;84.125  0.0000918&#10;84.20833    0.0000912&#10;84.29167    0.0000997&#10;84.375  0.0000915&#10;84.45833    0.0000695&#10;84.54167    0.0000896&#10;84.625  0.000113&#10;84.70833    0.000113&#10;84.79167    0.000127&#10;84.875  0.000103&#10;84.95833    0.000139&#10;85.04167    0.000105&#10;85.125  0.00021&#10;85.20833    0.000146&#10;85.29167    0.000198&#10;85.375  0.000106&#10;85.45833    0.000132&#10;85.54167    0.0001&#10;85.625  0.0000683&#10;85.70833    0.0000872&#10;85.79167    0.0000555&#10;85.875  0.000125&#10;85.95833    0.0000909&#10;86.04167    0.000102&#10;86.125  0.0000682&#10;86.20833    0.000112&#10;86.29167    0.0000699&#10;86.375  0.00012&#10;86.45833    0.0000919&#10;86.54167    0.000142&#10;86.625  0.000181&#10;86.70833    0.0000685&#10;86.79167    0.0000915&#10;86.875  0.000112&#10;86.95833    0.0000799&#10;87.04167    0.0000679&#10;87.125  0.0000343&#10;87.20833    0.000123&#10;87.29167    0.0000813&#10;87.375  0.0000914&#10;87.45833    0.000198&#10;87.54167    0.000162&#10;87.625  0.000205&#10;87.70833    0.000106&#10;87.79167    0.000153&#10;87.875  0.00017&#10;87.95833    0.0000789&#10;88.04167    0.000183&#10;88.125  0.0000906&#10;88.20833    0.000149&#10;88.29167    0.000106&#10;88.375  0.0000828&#10;88.45833    0.000206&#10;88.54167    0.000134&#10;88.625  0.000058&#10;88.70833    0.000122&#10;88.79167    0.000101&#10;88.875  0.000133&#10;88.95833    0.000101&#10;89.04167    0.000141&#10;89.125  0.000082&#10;89.20833    0.000149&#10;89.29167    0.000106&#10;89.375  0.0000949&#10;89.45833    0.000127&#10;89.54167    0.0000907&#10;89.625  0.00018&#10;89.70833    0.000068&#10;89.79167    0.000119&#10;89.875  0.0000567&#10;89.95833    0.000104&#10;90.04167    0.000104&#10;90.125  0.0000577&#10;90.20833    0.000151&#10;90.29167    0.000114&#10;90.375  0.000106&#10;90.45833    0.0000959&#10;90.54167    0.000149&#10;90.625  0.000171&#10;90.70833    0.000102&#10;90.79167    0.000128&#10;90.875  0.000112&#10;90.95833    0.0000881&#10;91.04167    0.00016&#10;91.125  0.000103&#10;91.20833    0.000105&#10;91.29167    0.000115&#10;91.375  0.000118&#10;91.45833    0.0000929&#10;91.54167    0.000181&#10;91.625  0.000101&#10;91.70833    0.000101&#10;91.79167    0.000117&#10;91.875  0.000149&#10;91.95833    0.0000908&#10;92.04167    0.0000987&#10;92.125  0.0000569&#10;92.20833    0.000133&#10;92.29167    0.000103&#10;92.375  0.000106&#10;92.45833    0.000103&#10;92.54167    0.000115&#10;92.625  0.0000816&#10;92.70833    0.000137&#10;92.79167    0.000146&#10;92.875  0.000138&#10;92.95833    0.000142&#10;93.04167    0.000169&#10;93.125  0.0000819&#10;93.20833    0.0000943&#10;93.29167    0.0000455&#10;93.375  0.000114&#10;93.45833    0.0000929&#10;93.54167    0.000186&#10;93.625  0.00024&#10;93.70833    0.000114&#10;93.79167    0.000136&#10;93.875  0.0000706&#10;93.95833    0.000129&#10;94.04167    0.000115&#10;94.125  0.000152&#10;94.20833    0.000148&#10;94.29167    0.000101&#10;94.375  0.000158&#10;94.45833    0.000159&#10;94.54167    0.000116&#10;94.625  0.000127&#10;94.70833    0.000112&#10;94.79167    0.000153&#10;94.875  0.000207&#10;94.95833    0.0000804&#10;95.04167    0.000143&#10;95.125  0.00015&#10;95.20833    0.000105&#10;95.29167    0.000117&#10;95.375  0.000127&#10;95.45833    0.000105&#10;95.54167    0.000114&#10;95.625  0.00014&#10;95.70833    0.000131&#10;95.79167    0.0000934&#10;95.875  0.0000589&#10;95.95833    0.0000782&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Most of my data is noisy and have large fluctuation my problem is: &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;i can’t find the fitting for the curve that pass through the onset time ( the point of curve that begin to increase until the peak point and then find the angle between the line and x-axes , and find the angle of the line that pass through  the peak point and the point of end decay on the curve)&lt;/li&gt;&#10;&lt;li&gt;i asked for best way in excel or matlab to write a program to do so.&lt;/li&gt;&#10;&lt;li&gt;i asked if possible separate the data to two parts : rising part and decay part and then find the slop of each line.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I tried only to plot the data put i have no idea about programming  to do so  and  what is the equation of the curve or the lines and errors.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-07-31T16:54:37.057" Id="110165" LastActivityDate="2014-07-31T17:08:06.627" LastEditDate="2014-07-31T17:08:06.627" LastEditorUserId="24808" OwnerUserId="53189" PostTypeId="1" Score="0" Tags="&lt;goodness-of-fit&gt;" Title="slope of non linear noisy data" ViewCount="14" />
  <row Body="&lt;p&gt;The OP urgently needs to clarify in what sense does he use the terms a) &quot;non-significant differences&quot; and b)&quot;correlation&quot; - but clarification a) is the most important.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Example:&#10;Assume we have two number series of equal length. Series $A$ is comprised of only $1$'s and series $B$ is comprised of only $2$'s. Is there, or is there not, a &quot;significant difference&quot; between the two?  &lt;/p&gt;&#10;&#10;&lt;p&gt;It depends on how we use the concept. We could argue that there is a significant difference, because the two series have no element in common.&lt;/p&gt;&#10;&#10;&lt;p&gt;But on another level, we could also argue that these are two very similar series because both have the characteristic that they are constant series of numbers (which is a very strong similarity trait, at certain levels of analysis).  &lt;/p&gt;&#10;&#10;&lt;p&gt;In short, the two series are totally different as regards their level, and at the same time they are totally similar as regards their structure.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-07-31T17:46:56.420" Id="110172" LastActivityDate="2014-07-31T17:46:56.420" OwnerUserId="28746" ParentId="110138" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I am building a multivariate regression model and trying to use the model to predict a High, Medium, and Low estimated outcome for each individual in a group of people. I am using the 95% confidence interval to get the High/Medium/Low estimates. This means constructing a High, Medium, and Low estimate for each variable in the individual's profile, and then summing the estimates together in the appropriate category to get a High, Medium, and Low overall outcome estimate. &lt;/p&gt;&#10;&#10;&lt;p&gt;To be explicit, here's how I see High/Medium/Low mapping to the 95% confidence interval, where B is the coefficient and SE is the standard error:&lt;/p&gt;&#10;&#10;&lt;p&gt;High = (B*X)+(1.96*SE)&#10;&lt;br&gt;Medium = (B*X)&#10;&lt;br&gt;Low = (B*X)-(1.96*SE)&lt;/p&gt;&#10;&#10;&lt;p&gt;Again, the High estimated outcome is the sum of all the High predictions for the input variables, the Medium outcome is the sum of all the Medium predictions, and so forth.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;The &quot;Problem&quot;:&lt;/strong&gt; Some of my input variables are binary indicators (with values of 0 or 1). I'm not sure how to handle the binary indicators with values of 0 when summing the input variables to generate the High and Low predictions.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;My Question:&lt;/strong&gt; When a binary indicator variable equals 0, can I leave their High and Low estimates as simply 0, or do I have to use the High/Low formulas listed above [e.g., (B*0)+(1.96*SE)]? Put another way, is the prediction for a 0-value indicator variable always 0, regardless of High/Medium/Low?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Why It Matters:&lt;/strong&gt; If I assume that the predictions for the 0-value binary indicators always equal 0, I get a much more accurate High/Medium/Low range of estimated outcomes. If I adhere to the formulas listed above regardless of the binary indicator's value, I get a much broader range of predictions. (This is probably because the first case basically assumes that SE=0 for 0-value binary indicators).&lt;/p&gt;&#10;&#10;&lt;p&gt;This problem has me puzzled, so I would be greatly appreciative of any insights. Happy to illustrate by example as well.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-07-31T21:23:47.780" Id="110199" LastActivityDate="2014-07-31T21:23:47.780" OwnerUserId="53203" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;confidence-interval&gt;&lt;binary&gt;&lt;predictor&gt;" Title="Calculating confidence intervals when binary input variable equals zero" ViewCount="60" />
  
  
  <row Body="&lt;p&gt;We are talking about &lt;em&gt;transitory&lt;/em&gt; (temporary) shocks. Permanent Shocks create structural shifts, and should not be captured by the error term.&lt;/p&gt;&#10;&#10;&lt;p&gt;For temporary shocks, take for example a toy-specificaton for demand for money (i.e for liquidity), under controlled interest rates and flexible money supply:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$M^d = AY^ke^{-a\mathbf i}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $Y$ is output and $\mathbf i$ is the nominal interest rate. In a linearized econometric setting we would get&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\ln M^d  = \ln A +k\ln Y -a\mathbf i +u$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $u$ is the error term -excuse me, the shock. What shock?&lt;/p&gt;&#10;&#10;&lt;p&gt;Our specification by design does not take into account fluctuations in  &lt;em&gt;precautionary&lt;/em&gt; demand for money. This comes about due to uncertainty / expectations for future events: if, say, news arrive that a banking sector crisis is imminent, people will tend to get their savings out of the banks, thus increasing abruptly the demand for money (interest bearing accounts are not money, they are assets). This abrupt fluctuation will be included in the &quot;error term&quot;.  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;ADDENDUM: Permanent Shocks&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In response to an OP' comment, assume now that a change in the technology of transactions happens. For example, &lt;em&gt;the telephone&lt;/em&gt; (I mean the &quot;traditional&quot; telephone, not cell phones). This implies that now you can &lt;em&gt;call&lt;/em&gt; a shop, find out if it has the product you need, order one and arrange for a specific date when you will appear in the shop, give the money, take the product. Previously, you had to have the money on you and to go physically in the market place to find what you wanted -and sometimes you wouldn't find it, and you would have to go again the next day: this means that the amount of money for the transaction would remain money and not being re-transformed into an interest-bearing asset, for a longer time period.&lt;/p&gt;&#10;&#10;&lt;p&gt;Overall, this reduces the demand for money, for given output and interest rates: it permanently reduces the constant term $A$. In that sense, it is a &lt;em&gt;structural shift&lt;/em&gt;, and &lt;em&gt;should not&lt;/em&gt; be left to be conceptually captured by the error term, which reflects something that affects the dependent variable only in a certain period. Even if the error term is autocorrelated (=&gt; current shocks propagate into the future), still this propagation will eventually die out, and so strictly speaking, again, it is not the right way to deal with structural shifts&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-08-01T02:51:12.693" Id="110223" LastActivityDate="2014-08-01T10:53:12.770" LastEditDate="2014-08-01T10:53:12.770" LastEditorUserId="28746" OwnerUserId="28746" ParentId="110216" PostTypeId="2" Score="5" />
  <row AnswerCount="0" Body="&lt;p&gt;I conducted a study in which I used sequencing of marker genes to study bacterial diversity and I would like to calculate the minimum abundance for a species to still be detected. &lt;/p&gt;&#10;&#10;&lt;p&gt;The &lt;strong&gt;total volume&lt;/strong&gt; of my experimental unit is &lt;strong&gt;2000 ml&lt;/strong&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;My &lt;strong&gt;sample size&lt;/strong&gt; is &lt;strong&gt;150 ml&lt;/strong&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;The &lt;strong&gt;total abundance&lt;/strong&gt; (all species confounded) is &lt;strong&gt;10^6 cells/ml&lt;/strong&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;I can identify &lt;strong&gt;4000 individuals in each sample&lt;/strong&gt; (4000 reads). &lt;/p&gt;&#10;&#10;&lt;p&gt;As I understand it, I sampled &lt;strong&gt;4000 individuals out of 150 ml&lt;/strong&gt; which is about &lt;strong&gt;27 (= 4000/150) ind./ml&lt;/strong&gt;. With a &lt;strong&gt;total abundance of 10^6 ind./ml&lt;/strong&gt; any given species needs to have a &lt;strong&gt;concentration of at least 10^6/27 = 3.7*10^4 ind./ml&lt;/strong&gt; to be represented (in average) with at least 1 ind. in my sample.   &lt;/p&gt;&#10;&#10;&lt;p&gt;Is that logic correct or do I need to take into account my total experimental volume, as having sampled &lt;strong&gt;4000 ind. out of 2000 ml&lt;/strong&gt;, hence only &lt;strong&gt;2 ind./ml&lt;/strong&gt; with an according detection limit of &lt;strong&gt;5*10^5 ind./ml&lt;/strong&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;sorry for that really simple question, but I get very confused and can't make up my mind. &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for your help!&lt;/p&gt;&#10;&#10;&lt;p&gt;Fabian&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-01T09:07:37.883" Id="110248" LastActivityDate="2014-08-01T09:07:37.883" OwnerUserId="53227" PostTypeId="1" Score="0" Tags="&lt;biostatistics&gt;" Title="minimum abundance of a bacterial species" ViewCount="24" />
  
  <row Body="&lt;p&gt;To complement the answer given by @Marc Claesen.&lt;/p&gt;&#10;&#10;&lt;p&gt;You don't need it for decision trees. To train a decision tree, e.g. by the ID-3 algorithm, you are interested in relative entropies with and without different variables. &lt;/p&gt;&#10;&#10;&lt;p&gt;For categorical variabels, e.g.&lt;code&gt;type=[brown, yellow, red]&lt;/code&gt;, the tree branches out for each category, you do not need to index the colours. The distance problem with the SVMs or logistic regression or k-NN, is not a problem for decision trees.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-01T09:38:38.637" Id="110250" LastActivityDate="2014-08-01T09:38:38.637" OwnerUserId="28740" ParentId="110217" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Any normal distribution is defined on the entire real line $(-\infty,\infty)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;This means that while we can choose some probability of being between 1 and 45, you can't have it both be actually normal and definitely in that range.&lt;/p&gt;&#10;&#10;&lt;p&gt;You have to decide which of the two of those can be compromised.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, it also sounds like perhaps you're after an integer result; if that's so, that, again, means you don't have normality, and have to compromise one or the other.&lt;/p&gt;&#10;&#10;&lt;p&gt;Are you after a distribution with mean 23 ($\frac{_{45+1}}{^2}$)?&lt;/p&gt;&#10;&#10;&lt;p&gt;Can you say any more about the situation?&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Whoever asked you to do this seems to be operating under a misunderstanding because the conditions are inconsistent.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;I have no idea how to choose the variance parameter.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;The smaller the variance, the less chance of going outside the range. But the smaller the variance, the less chance of taking up most of the range.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, if you choose $\sigma=5$, you'll have about 11 of your 1000 samples with at least one value outside the bounds, but on the other hand, most of the time you won't have any values very close to 1 or 45.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can use simulation to judge the tradeoff.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's a simulated distribution of the largest observation for n=1000 with mean 23 and sd 5:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/hgU44.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;As you see almost all of the time, it's less than 45 but then often it's less than 40. &lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;One possibility is to truncate the normal; e.g. try mean 45 sd around 6 (say) and then regenerate values that fall outside the range (or generate slightly too many and then omit what you don't need).&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's an example with sd=6 and another with sd=9:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Udy2f.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/8F56r.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;(This is not a single sample of 1000, but many samples of 1000 so you can see the distribution)&lt;/p&gt;&#10;&#10;&lt;p&gt;The narrower one looks more normal but you're less likely to get values near 1 or 45; the wider one is a little less normal looking but generally gives values right out near the ends.&lt;/p&gt;&#10;&#10;&lt;p&gt;You may want to choose something in between.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Another possibility: the person may actually want you to fit it to 1 to 45 post-hoc (that is to make the smallest value 1 and the largest 45). This is fine, but you don't actually have normality any more (though with n=1000 it will look very close). &lt;/p&gt;&#10;&#10;&lt;p&gt;For example, here's what happens when you use this strategy with n=6 instead of n=1000:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/PahyS.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;(1000 samples of n=6)&lt;/p&gt;&#10;&#10;&lt;p&gt;There are spikes at 1 and 45 because those values are certain to occur, but leaving that aside,  the distribution of values in between 1 and 45 is fairly flat, only gently bulging in the center. [Macro makes a similar point about the n=4 case &lt;a href=&quot;http://stats.stackexchange.com/a/30311/805&quot;&gt;here&lt;/a&gt;; it's even more nearly flat, though even the n=3 case still shows a slight bulge in the center.]&lt;/p&gt;&#10;&#10;&lt;p&gt;For small samples this isn't much use. As sample sizes increase, this improves. Here it is for samples of size 100:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/InVC1.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;(1000 samples of n=100)&lt;/p&gt;&#10;&#10;&lt;p&gt;It looks like a Winsorized normal (or ignoring the endpoints, it's like a truncated normal). This will still be having an impact at n=1000:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/6dAca.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that this isn't a single sample, but 1000 samples of 1000. I did that to show you what the actual distribution you're ending up with is like.&lt;/p&gt;&#10;&#10;&lt;p&gt;I think you're better off with a truncated normal than doing this. Nevertheless, if this is what they want, it is very easy to do.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-08-01T09:46:20.070" Id="110252" LastActivityDate="2014-08-08T00:42:46.653" LastEditDate="2014-08-08T00:42:46.653" LastEditorUserId="805" OwnerUserId="805" ParentId="110249" PostTypeId="2" Score="12" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I fit two gee models with two different correlation structures, exchangeable vs ar(1), which resulted in very different p.values. I'm wondering what reasons have led that. Would somebody offer an explanation?&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; summary(fit1)&#10;&#10;Call:&#10;geeglm(formula = LAU ~ SAMPLENO, data = bd, id = MAGE, corstr = &quot;exchangeable&quot;)&#10;&#10; Coefficients:&#10;             Estimate   Std.err   Wald Pr(&amp;gt;|W|)    &#10;(Intercept)  0.067349  0.004537 220.32   &amp;lt;2e-16 ***&#10;SAMPLENO    -0.002800  0.000947   8.74   0.0031 ** &#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1&#10;&#10;Estimated Scale Parameters:&#10;            Estimate  Std.err&#10;(Intercept) 0.000425 0.000166&#10;&#10;Correlation: Structure = exchangeable  Link = identity &#10;&#10;Estimated Correlation Parameters:&#10;      Estimate Std.err&#10;alpha    0.206   0.249&#10;Number of clusters:   9   Maximum cluster size: 8 &#10;&#10;&#10;&#10;&#10;&amp;gt; summary(fit2)&#10;&#10;Call:&#10;geeglm(formula = LAU ~ SAMPLENO, data = bd, id = MAGE, corstr = &quot;ar1&quot;)&#10;&#10; Coefficients:&#10;            Estimate  Std.err   Wald Pr(&amp;gt;|W|)    &#10;(Intercept)  0.06392  0.00588 118.25   &amp;lt;2e-16 ***&#10;SAMPLENO    -0.00142  0.00219   0.42     0.52    &#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1&#10;&#10;Estimated Scale Parameters:&#10;            Estimate  Std.err&#10;(Intercept) 0.000427 0.000172&#10;&#10;Correlation: Structure = ar1  Link = identity &#10;&#10;Estimated Correlation Parameters:&#10;      Estimate Std.err&#10;alpha    0.659   0.221&#10;Number of clusters:   9   Maximum cluster size: 8&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-08-01T14:21:06.880" Id="110278" LastActivityDate="2014-08-02T15:45:17.243" OwnerUserId="35736" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;regression&gt;" Title="How to explain the large p.value difference between these two models" ViewCount="37" />
  
  
  <row Body="&lt;p&gt;I believe it is called contemporaneous cross-equation error correlation.&lt;/p&gt;&#10;&#10;&lt;p&gt;The errors has can be interpreted as the effect of omitted variables, measurement error, or human indeterminacy. Here's one example of the first. Suppose your two outcomes are budget shares (rather than levels) for clothing and food. If someone puts on a lot of weight (typically unobserved to the econometrician, even in the days of big data), there will be large positive error in the clothing equation, and a large negative error in the food equation as the dieting commences.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-01T17:29:24.657" Id="110304" LastActivityDate="2014-08-01T17:29:24.657" OwnerUserId="7071" ParentId="110258" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;You don't need to normalize, but can get the p-value for a goodness-of-fit test by simulation. Here is some sample R code, taken from Greg Snow's answer to a similar question (&lt;a href=&quot;http://stats.stackexchange.com/questions/109110/ks-test-r-minitab-and-systat/109116#109116&quot;&gt;KS test - R, Minitab (and Systat)&lt;/a&gt;):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;data &amp;lt;- c(7.2,10.5,10.67,0.15,3.92,3.28,0.89,2.29,13.82,0.43)&#10;&#10;simp &amp;lt;- replicate(100000, {x &amp;lt;- rexp(length(data),rate=1/mean(data));&#10;     ks.test(x,&quot;pexp&quot;,rate=1/mean(x))$p.value} )&#10;&#10;mean(simp &amp;lt;= ks.test(data,&quot;pexp&quot;,1/mean(data))$p.value)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The method is described by Clauset et. al in a SIAM paper &quot;Power-Law Distributions in Empirical Data.&quot;&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-08-01T17:57:43.903" Id="110309" LastActivityDate="2014-08-01T17:57:43.903" OwnerUserId="24073" ParentId="110272" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;For iid data points $\boldsymbol{x}=x_i,\ldots,x_n$, the complete data log-likelihood is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\ell(\theta)=\mathrm{log}[\prod_{i=1}^nP(y,x_i|\theta)]=\sum_{i=1}^n\mathrm{log}P(y,x_i|\theta).$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The $Q$ function is defined as $$Q(\theta,\theta^{'})=E(\ell(\theta)|\boldsymbol{x},\theta^{'})=\sum_{i=1}^nE[\mathrm{log}P(y,x_i|\theta)|x_i,\theta^{'}].$$&lt;/p&gt;&#10;&#10;&lt;p&gt;For each data point $x_i$, you have understood that $$E[\mathrm{log}P(y,x_i|\theta)|x_i,\theta^{'}]=\sum_yP(y|x_i,\theta^{'})\mathrm{log}P(y,x_i|\theta).$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Thus $$Q(\theta,\theta^{'})=\sum_{i=1}^n\sum_yP(y|x_i,\theta^{'})P(y,x_i|\theta)=\sum_{i=1}^n\Sigma_yP_{\theta^{'}}(y|x_i)P_\theta(y,x_i).$$&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-01T21:09:36.913" Id="110330" LastActivityDate="2014-08-05T16:02:47.983" LastEditDate="2014-08-05T16:02:47.983" LastEditorUserId="21599" OwnerUserId="21599" ParentId="91783" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;I believe you may have a negative binomial distribution, as it is zero inflated.  &lt;a href=&quot;http://stat.ethz.ch/R-manual/R-patched/library/stats/html/NegBinomial.html&quot; rel=&quot;nofollow&quot;&gt;This link&lt;/a&gt; contains a description of the r functions for negative binomial distributions.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-01T22:47:43.880" Id="110339" LastActivityDate="2014-08-02T00:35:57.857" LastEditDate="2014-08-02T00:35:57.857" LastEditorUserId="7290" OwnerUserId="53262" ParentId="17697" PostTypeId="2" Score="0" />
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm doing a project on dog adoption patterns, and I realized that there are many (100 +) different breeds of dogs.  I'd like to build a predictive model using breed as covariate, but I'm not sure whether I should turn each breed into a dummy variable and run a quick and dirty supervised learning algorithm, or try to do some clustering (based on aggressiveness of breed, for example), and then try to run the algorithm.  My dataset is pretty large : 14000 entries.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Any advice?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-08-02T15:27:17.037" Id="110387" LastActivityDate="2014-08-02T15:27:17.037" OwnerUserId="43046" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;feature-selection&gt;&lt;feature-construction&gt;&lt;supervised-learning&gt;" Title="When to cluster features for supervised learning?" ViewCount="13" />
  
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have quarterly unbalanced Panel data and I want to de-trend my dependent variable to make it stationary. how do i do it? I don't want to take differences as it will shorten my observations. The residual series that I get after regressing my dependent variable on time trend does not remove unit root for data . It should be noted my independent variables are stationary? should I transform/detrend them as well.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also can I regress differences on levels in a panel data setting or all variables should be in the same order of integration?&lt;/p&gt;&#10;&#10;&lt;p&gt;Would Hodrick prescott filter be a good choice for detrending quarterly observations? 2002q2 to 2013q4&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-02T20:12:58.667" Id="110413" LastActivityDate="2014-08-02T20:12:58.667" OwnerUserId="53297" PostTypeId="1" Score="0" Tags="&lt;panel-data&gt;" Title="Detrending for quarterly Panel Data" ViewCount="29" />
  <row Body="&lt;p&gt;After looking around for a while without finding anything satisfactory, this is the best answer that seems to make sense to me:&lt;/p&gt;&#10;&#10;&lt;p&gt;Notice that the sampling distribution of the unknown $\sigma$ is not Normal, so $Q = \frac{\bar{Y} - \mu_{0}}{\sigma / \sqrt{n}}$ does not actually follow $N(0, 1)$, thus it cannot be a pivotal quantity, at least not a pivotal quantity with a Normal distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;The case for unknown $\mu$ however, is different because the sampling distribution of $\mu$ is Normal by Central Limit Theorem, so we know $Q = \frac{\bar{Y} - \mu}{\sigma_{0} / \sqrt{n}}$ follows $N(0, 1)$, which makes it a pivotal quantity.&lt;/p&gt;&#10;&#10;&lt;p&gt;This is why to find the confidence interval for $\sigma$, we have to use the pivotal quantity $$\frac{(n-1)S^2}{\sigma^2},$$ which follows a $\chi^2$ distribution with $n-1$ degrees of freedom.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-02T20:15:48.863" Id="110414" LastActivityDate="2014-08-02T20:15:48.863" OwnerUserId="49471" ParentId="105727" PostTypeId="2" Score="0" />
  
  
  <row Body="&lt;p&gt;&lt;strong&gt;The standard deviation is as applicable here as anywhere else:&lt;/strong&gt; it gives useful information about the dispersion of the data.  In particular, the sd divided by the square root of the sample size is one standard error: it estimates the dispersion of the sampling distribution of the mean.  Let's calculate:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$3.2\% / \sqrt{10000} = 0.032\% = 0.00032.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;That's &lt;em&gt;tiny&lt;/em&gt;--far smaller than the $\pm 0.50\%$ precision you seek.&lt;/p&gt;&#10;&#10;&lt;p&gt;Although the data are not Normally distributed, &lt;strong&gt;the sample mean is extremely close to Normally distributed because the sample size is so large.&lt;/strong&gt;  Here, for instance, is a histogram of a sample with the same characteristics as yours and, at its right, the histogram of the means of a thousand additional samples from the same population.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Zd5OO.png&quot; alt=&quot;Figure 1&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;It looks very close to Normal, doesn't it?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thus, &lt;strong&gt;although it appears you are bootstrapping correctly, bootstrapping is not needed:&lt;/strong&gt; a symmetric $100 - \alpha\%$ confidence interval  for the mean is obtained, as usual, by multiplying the standard error by an appropriate percentile of the standard Normal distribution (to wit, $Z_{1-\alpha/200}$) and moving that distance to either side of the mean.  In your case, $Z_{1-\alpha/200} = 2.5758$, so the $99\%$ confidence interval is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\left(0.977 - 2.5758(0.032) / \sqrt{10000},\  0.977 + 2.5758(0.032) / \sqrt{10000}\right) \\ = \left(97.62\%, 97.78\%\right).$$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;A sufficient sample size&lt;/strong&gt; can be found by inverting this relationship to solve for the sample size.  Here it tells us that you need a sample size around &lt;/p&gt;&#10;&#10;&lt;p&gt;$$(3.2\% / (0.5\% / Z_{1-\alpha/200}))^2 \approx 272.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;This is small enough that &lt;strong&gt;we might want to re-check the conclusion that the sampling distribution of the mean is Normal.&lt;/strong&gt;  I drew a sample of $272$ from my population and bootstrapped its mean (for $9999$ iterations):&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/f1KVH.png&quot; alt=&quot;Figure 2&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Sure enough, it looks Normal.  In fact, the bootstrapped confidence interval of $(97.16\%, 98.21\%)$ is almost identical to the Normal-theory CI of $(97.19\%, 98.24\%)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;As these examples show, &lt;strong&gt;the &lt;em&gt;absolute sample size&lt;/em&gt; determines the accuracy of estimates rather than the proportion of the population size.&lt;/strong&gt;  (An extreme but intuitive example is that a single drop of seawater can provide an accurate estimate of the concentration of salt in the ocean, even though that drop is such a tiny fraction of all the seawater.)  For your stated purposes, obtaining a sample of $10000$ (which requires more than $36$ times as much work as a sample of $272$) is overkill.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;R&lt;/code&gt; code to perform these analyses and plot these graphics follows. It samples from a population having a Beta distribution with a mean of $0.977$ and SD of $0.032$.&lt;/p&gt;&#10;&#10;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;set.seed(17)&#10;#&#10;# Study a sample of 10,000.&#10;#&#10;Sample &amp;lt;- rbeta(10^4, 20.4626, 0.4817)&#10;hist(Sample)&#10;hist(replicate(10^3, mean(rbeta(10^4, 20.4626, 0.4817))),xlab=&quot;%&quot;,main=&quot;1000 Sample Means&quot;)&#10;#&#10;# Analyze a sample designed to achieve a CI of width 1%.&#10;#&#10;(n.sample &amp;lt;- ceiling((0.032 / (0.005 / qnorm(1-0.005)))^2))&#10;Sample &amp;lt;- rbeta(n.sample, 20.4626, 0.4817)&#10;cat(round(mean(Sample), 3), round(sd(Sample), 3)) # Sample statistics&#10;se.mean &amp;lt;- sd(Sample) / sqrt(length(Sample))      # Standard error of the mean&#10;cat(&quot;CL: &quot;, round(mean(Sample) + qnorm(0.005)*c(1,-1)*se.mean, 5)) # Normal CI&#10;#&#10;# Compare the bootstrapped CI of this sample.&#10;#&#10;Bootstrapped.means &amp;lt;- replicate(9999, mean(sample(Sample, length(Sample), replace=TRUE)))&#10;hist(Bootstrapped.means)&#10;cat(&quot;Bootstrap CL:&quot;, round(quantile(Bootstrapped.means, c(0.005, 1-0.005)), 5))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-08-02T23:45:38.080" Id="110420" LastActivityDate="2015-01-09T17:00:56.253" LastEditDate="2015-01-09T17:00:56.253" LastEditorUserId="919" OwnerUserId="919" ParentId="110418" PostTypeId="2" Score="13" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;While searching for a job for data analyst I found out that many jobs where data analysis is widely used do not include &quot;data&quot; or &quot;statistics&quot; in position title, for example credit specialist, business analyst and so on. &lt;/p&gt;&#10;&#10;&lt;p&gt;What are suitable job positions for people who want to analyze data? Moreover, how they differ?&lt;/p&gt;&#10;" CommentCount="4" CommunityOwnedDate="2014-08-03T15:07:47.840" CreationDate="2014-08-03T09:30:04.463" FavoriteCount="1" Id="110443" LastActivityDate="2014-08-04T07:44:54.513" OwnerUserId="14730" PostTypeId="1" Score="3" Tags="&lt;careers&gt;" Title="List of suitable job positions for data analyst" ViewCount="152" />
  <row AcceptedAnswerId="110474" AnswerCount="1" Body="&lt;p&gt;Does anyone know the difference in calculation between these two AUC packages? They get different results when I add in positives with predicted value of 0 (simulating a prob model where many outputs will be zero e.g. randomForest). So I would like to know why there is the kink in the pROC results (red) and the AUC 'flips'. The plot below illustrates this behaviour.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, this stems from the problem of having to compute AUC's on models with many positives assigned probabilities of zero. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; library(ROCR)&#10; library(pROC)&#10;&#10;  start.ds &amp;lt;- data.frame(response=gl(2, 1000, labels = c(&quot;1&quot;, &quot;0&quot;)),pred=ifelse(as.numeric(gl(2, 1000, labels = c(&quot;0&quot;, &quot;1&quot;)))==2,0,1))&#10;  add.positives &amp;lt;- data.frame(response=gl(1, 1000, labels = c(&quot;1&quot;)),pred=0)&#10;  add.positives &amp;lt;- rbind(add.positives, add.positives)&#10;&#10;  nadd &amp;lt;- nrow(add.positives)&#10;  theSeq &amp;lt;- seq(1, nadd, 10)&#10;  allAUCs &amp;lt;- NULL&#10;  for(i in theSeq){&#10;       print(i)&#10;       temp.ds &amp;lt;- rbind(start.ds,add.positives[1:i,])&#10;       temp.ds &amp;lt;- temp.ds[order(temp.ds$pred,decreasing=TRUE),]&#10;&#10;       aucPROC &amp;lt;- as.numeric(auc(temp.ds[,1],temp.ds[,2]))&#10;       aucAUCPackage &amp;lt;- performance(prediction(temp.ds[,2],temp.ds[,1]),&quot;auc&quot;)@y.values[[1]]&#10;       allAUCs &amp;lt;- rbind(allAUCs, data.frame(obs=i, aucPROC=aucPROC, aucAUCPackage=aucAUCPackage))&#10;&#10;    }&#10;&#10;  plot(allAUCs$aucPROC,col=&quot;red&quot;,type=&quot;l&quot;)&#10;  lines(allAUCs$aucAUCPackage,col=&quot;black&quot;,type=&quot;l&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/IPjU8.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-03T10:35:41.707" Id="110447" LastActivityDate="2014-08-03T16:21:17.587" LastEditDate="2014-08-03T16:14:25.257" LastEditorUserId="50835" OwnerUserId="50835" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;machine-learning&gt;&lt;roc&gt;&lt;auc&gt;" Title="Differences in AUC calculation between pROC and ROCR" ViewCount="180" />
  <row Body="&lt;p&gt;As the question is tagged &lt;code&gt;t-test&lt;/code&gt;, I will therefore focus on it:&lt;/p&gt;&#10;&#10;&lt;p&gt;There is one miss-conception that I would like to pointed out: The t-test is about the &lt;strong&gt;sample mean&lt;/strong&gt; s and therefore it assumes that the means of the different samples are normally distributed &lt;strong&gt;instead of&lt;/strong&gt; assuming normal distribution of the populations in question. &lt;/p&gt;&#10;&#10;&lt;p&gt;Based on &lt;a href=&quot;http://en.wikipedia.org/wiki/Central_limit_theorem&quot; rel=&quot;nofollow&quot;&gt;Central limit theorem&lt;/a&gt;, &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;the arithmetic mean of a sufficiently large number of iterates of independent random variables, each with a well-defined expected value and well-defined variance, will be approximately normally distributed &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Therefore it would be very likely safe to assume that for large sample sizes, the sample means are normally distributed. Your sample size (always &gt;50) is probably 'large enough'. &lt;/p&gt;&#10;&#10;&lt;p&gt;Therefore, &lt;code&gt;t-test&lt;/code&gt; when you have homogeneity of variance, &lt;a href=&quot;http://en.wikipedia.org/wiki/Welch%27s_t_test&quot; rel=&quot;nofollow&quot;&gt;&lt;code&gt;Welch test&lt;/code&gt;&lt;/a&gt; otherwise.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-03T14:25:26.753" Id="110463" LastActivityDate="2014-08-03T14:25:26.753" OwnerUserId="23790" ParentId="110175" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;I know you wrote it as a premise, but I would not use the ordered LASSO without being absolutely sure that this is thing that is needed, because the assumptions of the ordered LASSO are not directly appropriate for time-series prediction. As a counter-example, consider the case where you have a delay-time of, say, ten time-steps between measurement and target. Obviously, the ordered LASSO constraints cannot handle such a effects without attributing nonsense to the first nine parameters.&lt;/p&gt;&#10;&#10;&lt;p&gt;In contrast, I would rather stick to the normal LASSO and include &lt;em&gt;all&lt;/em&gt; previous observation -- particularly because you wrote your model space is small, and coordinate-descent optimization routines for the LASSO (as described &lt;a href=&quot;https://www.google.de/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;uact=8&amp;amp;ved=0CCcQFjAA&amp;amp;url=http%3A%2F%2Fwww.jstatsoft.org%2Fv33%2Fi01%2Fpaper&amp;amp;ei=80zeU-KuK9Db4QShloHAAw&amp;amp;usg=AFQjCNEHiTrirITAlESPVPCAh83ht4xLzg&amp;amp;sig2=N_iz1ua34eHgI3vrNJvpSQ&amp;amp;bvm=bv.72197243,d.bGE&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;) are working efficiently also for large datasets. Then compute the path for the regularization strength parameter $\lambda$ and look which parameters get included as you go from large $\lambda$ to $\lambda=0$. Especially those included earlier are the important ones.&lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, you have to choose an appropriate criterion and optimize the parameter $\lambda$ using cross-validation, standard one-dimensional minimization or whatever. The criterion can for example be something as &quot;prediction error + number of included variables&quot; (--AIC criterion-like).&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-08-03T14:57:43.517" Id="110465" LastActivityDate="2014-08-03T14:57:43.517" OwnerUserId="49279" ParentId="74656" PostTypeId="2" Score="0" />
  
  <row AcceptedAnswerId="110514" AnswerCount="2" Body="&lt;p&gt;This question: &lt;a href=&quot;http://stats.stackexchange.com/questions/33453/comparing-and-contrasting-p-values-significance-levels-and-type-i-error#33500&quot;&gt;Comparing and contrasting, p-values, significance levels and type I error&lt;/a&gt; cites exceptions that arise when using discrete data. Are there other exceptions with continuous data as well?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-08-03T16:55:06.607" Id="110476" LastActivityDate="2014-08-04T00:16:53.577" LastEditDate="2014-08-03T17:01:14.267" LastEditorUserId="7290" OwnerUserId="9162" PostTypeId="1" Score="4" Tags="&lt;hypothesis-testing&gt;&lt;statistical-significance&gt;" Title="Does $\Pr(\text{Type I error})$ ever not equal $\alpha$ with continuous data?" ViewCount="75" />
  
  <row Body="&lt;p&gt;A picture is worth a thousand words. Null hypothesis: patient is &lt;em&gt;not pregnant&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/CLwMG.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Image via &lt;a href=&quot;http://effectsizefaq.com/2010/05/31/i-always-get-confused-about-type-i-and-ii-errors-can-you-show-me-something-to-help-me-remember-the-difference/&quot; rel=&quot;nofollow&quot;&gt;Paul Ellis&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2014-08-03T17:14:17.010" Id="110480" LastActivityDate="2014-08-03T22:09:02.813" LastEditDate="2014-08-03T22:09:02.813" LastEditorUserId="44269" OwnerUserId="44269" ParentId="110433" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;I don't know if you still need an answer to this question, but I have a similar problem in which I'd like to use Poisson regression.  In running your code, I found that if I set up the model as&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;model &amp;lt;- glm(y ~ b + x, family=binomial(logit)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;rather than as your Poisson regression model, the same result occurs: the estimated OR is ~1.5 as ce approaches 1.  So, I'm not sure that your example provides information on a possible problem with the use of Poisson regression for binary outcomes.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-03T17:57:14.077" Id="110483" LastActivityDate="2014-08-03T17:57:14.077" OwnerUserId="27346" ParentId="105346" PostTypeId="2" Score="0" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I am confused about the concept of cross validation and its usage.  &lt;/p&gt;&#10;&#10;&lt;p&gt;As I read about cross validation before, it is a way of validating a model. I did cross validation in my project (developing different regression models on a dataset, model validation and finally choosing best model). Cross validation just give me a model but no statistical criteria that shows ability of the model. By the way each time that I run cross validation (in R program) the result is different because train dataset is changed. For selecting the best model, I calculate AIC for the model obtained by cross validation but now I think it is wrong. Because first, train data used for each model is different, and second, even for a certain model, AIC will change by repeating cross validation. &lt;/p&gt;&#10;&#10;&lt;p&gt;Lately I read that cross validation is used for selecting the best model! All these confused me about utility of cross validation and the way of interpreting the result.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Could you please help me on figuring out all these?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-03T19:21:59.883" FavoriteCount="1" Id="110490" LastActivityDate="2014-08-03T21:39:14.823" LastEditDate="2014-08-03T19:41:44.900" LastEditorUserId="22468" OwnerUserId="53325" PostTypeId="1" Score="2" Tags="&lt;modeling&gt;&lt;cross-validation&gt;&lt;model-selection&gt;&lt;aic&gt;" Title="Is cross validation for validating a model or for selecting best model in different kinds of models?" ViewCount="77" />
  <row Body="&lt;p&gt;Here is one thing that might be a good solution. I looked at the clime R package but couldn't get it to run. Then I tried the fastclime package for R. It works and apparently is faster than clime (hence the name). This should solve my problem because it will take either the data X(n,p) or the covariance matrix, Cov(p,p). &lt;/p&gt;&#10;&#10;&lt;p&gt;Because it will take the covariance matrix directly, it will work with missing values. That's because you can always calculate the covariance matrix by ignoring missing values. In R you do cov.mat=cov(X,use='pairwise') using the built in cov function. The use='pairwise' (reads the docs) means that it will compute the dot product as best it can by ignoring any pairs that have NA. Any method that will take the sample covariance as input likewise will work. I'd guess that it would also generalize to include any weight on the p-parameters (not just 0,1). &lt;/p&gt;&#10;&#10;&lt;p&gt;Still open to other suggestions.    &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-03T19:29:10.433" Id="110491" LastActivityDate="2014-08-03T19:29:10.433" OwnerUserId="40967" ParentId="110461" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;I'm presuming you're doing a hypothesis test to check the significance of a covariate. In doing a hypothesis test, you would $\textit{always}$ use all the samples in one test, not do a test for each sample separately.&lt;/p&gt;&#10;&#10;&lt;p&gt;As you correctly point out, more samples is better. In a hypothesis test there are two errors: a type 1 error, $\alpha$, which is set before performing the hypothesis test (often set to be 5%). The other error you can make is a type 2 error, $\beta$, which is dependent on the test you are using, among other things. Using more samples, the probability of a type 2 error will decrease.&lt;/p&gt;&#10;&#10;&lt;p&gt;In terms of a ranking, the power ($=1-\beta$) could be used to rank a test. Clearly maximising the power means the smallest type 2 error. The power is normally considered when deciding between which hypothesis test to use. Tests are called uniformly most powerful if they give the best power and a test that is uniformly most powerful would be preferred over any other test.&lt;/p&gt;&#10;&#10;&lt;p&gt;It is important not to decide on the test after you have performed it, as this will bias your conclusions. You would, beforehand, decide on the acceptable probability of a type 1 error and a type 2 error and then collect your samples; you must not choose the number of samples after you've performed a test.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-08-03T20:11:17.267" Id="110496" LastActivityDate="2014-08-03T20:11:17.267" OwnerUserId="52756" ParentId="110494" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;I think if you plot lines connecting the three dots, in order (1st year, 2nd year, 3rd year), with a different color for each country, and the same shapes for each year across all countries (filled circle for year 1, filled square for year 2, filled triangle for year 3), it will be a lot simpler to follow. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-08-03T21:13:48.643" Id="110504" LastActivityDate="2014-08-03T21:13:48.643" OwnerUserId="36954" ParentId="110486" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="110546" AnswerCount="1" Body="&lt;p&gt;I am trying to understand some descriptions of PCA (the first two are from Wikipedia), emphasis added:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Principal components are guaranteed to be &lt;em&gt;independent&lt;/em&gt; only if the data set is &lt;em&gt;jointly normally distributed&lt;/em&gt;.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Is the independence of principal components very important? How can I understand this description?&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;PCA is sensitive to the relative &lt;em&gt;scaling&lt;/em&gt; of the original variables.  &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;What does 'scaling' mean there? Normalization of different dimensions?&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The transformation is defined in such a way that the first principal component has the largest possible variance and each succeeding component in turn has the highest variance under &lt;em&gt;the constraint that it be orthogonal to the preceding components&lt;/em&gt;.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Can you explain this constraint?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-08-03T22:03:19.650" FavoriteCount="3" Id="110508" LastActivityDate="2014-12-17T00:59:52.110" LastEditDate="2014-12-17T00:59:52.110" LastEditorUserId="28666" OwnerUserId="52219" PostTypeId="1" Score="3" Tags="&lt;pca&gt;" Title="Questions on PCA: when are PCs independent? why is PCA sensitive to scaling? why are PCs constrained to be orthogonal?" ViewCount="272" />
  <row AnswerCount="2" Body="&lt;p&gt;I was reading Koller's and Friedman's Probabilistic Graphical Models book and became confused about some of its notation because of a set of notes that either contradict it or express it differently. Whichever it is, I would like to clarify.&lt;/p&gt;&#10;&#10;&lt;p&gt;The notation is:&lt;/p&gt;&#10;&#10;&lt;p&gt;$(\textbf{X} \perp \textbf{Y} , \textbf{W}\mid \textbf{Z})$&lt;/p&gt;&#10;&#10;&lt;p&gt;The reason I got confused on what the notation means is because of the following slides online:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.stats.ox.ac.uk/~steffen/seminars/waldmarkov.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.stats.ox.ac.uk/~steffen/seminars/waldmarkov.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Particularly what confuses me is the way these notes express the decomposition rule on page 8 and how koller's book expresses this same decomposition rule a little differently. If I understand the two notations, I should understand the decomposition rule under both notations. &lt;/p&gt;&#10;&#10;&lt;p&gt;Koller's book expresses the decomposition rule as follow:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$(\textbf{X} \perp \textbf{Y} , \textbf{W}\mid \textbf{Z}) \implies (\textbf{X} \perp \textbf{Y}\mid \textbf{Z}, \textbf{W})$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Which I would express intuitively as; the set of random variables $\textbf{X}$ are independent of the set of random variables $\textbf{Y}$ and $\textbf{W}$ (i.e. interpreting the comma in between $\textbf{Y}$  and $\textbf{X}$ as an &quot;and&quot;) given the set of random variables $\textbf{Z}$. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, the slides seem to interpret this comma separation as an Union which confuses me (because I thought we were doing &quot;ands&quot;):&lt;/p&gt;&#10;&#10;&lt;p&gt;$$(A \perp_{\sigma} (B \cup D) \mid C) \implies (A \perp_{\sigma} B \mid C) \text{ and } (A \perp_{\sigma} D \mid C) $$&lt;/p&gt;&#10;&#10;&lt;p&gt;It confuses me because they seem to use union for the commas were I used &quot;ands&quot;. Furthermore, I can't see why the two statements are equivalent decomposition rules (which might explain why I've had a tough time proving it because I don't think I understand the statement!). Does someone know the difference in the notation and how they equivalently express the decomposition rule?&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;As a reference this is more background on the notation in Koller's Book:&lt;/p&gt;&#10;&#10;&lt;p&gt;Let capital non-bold stand for random variables say $X$ is a r.v. Let little non bold stand for the assignment to a random variable say $(X = x)$. Also, let me define captial bold letters as sets of random variables. For example $\textbf{X}, \textbf{Y}, \textbf{Z}$ are three sets of random variables. Let small bold letters denote assigments to these sets $\textbf{x}, \textbf{y}, \textbf{z}$ i.e. it denotes assigments of values to the variables in these sets. Let $Val(\textbf{X})$ be the values that the set of random variables can take.&lt;/p&gt;&#10;&#10;&lt;p&gt;On page 21 of Koller's book it explicitly says that Intersections are comma separated, so its fair to assume comma's means &quot;intersection&quot;. This can be appreciated on page 21 where it says:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Rather than write $P((X=x) \cap (Y=y))$, we write $P(X=x, Y = y)$ or&#10;  just $P(x,y)$.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I know this is only on the context of explicit probability distributions, however, if we note how it starts to prove the decomposition property, we can infer that koller probably mean a comma as a intersection in the confusing notation $(\textbf{X} \perp \textbf{Y} , \textbf{W}\mid \textbf{Z})$ too. She starts like this:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;For example, to prove Decomposition assume that $(X \perp Y , W \mid&#10; Z)$ holds. Then, from the definition of conditional independence, we&#10;  have that $P(X,Y,W|Z) = P(X|Z)P(Y,W|Z)$. Now using... proof&#10;  continues...&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;The equation:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$P(X,Y,W|Z) = P(X|Z)P(Y,W|Z)$$ &lt;/p&gt;&#10;&#10;&lt;p&gt;Makes me suspect that the slides are either wrong or sloppy in notation. Or both are sloppy. I am not sure.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-04T00:19:12.013" Id="110515" LastActivityDate="2014-08-04T15:29:08.827" LastEditDate="2014-08-04T01:50:11.853" LastEditorUserId="28986" OwnerUserId="28986" PostTypeId="1" Score="2" Tags="&lt;probability&gt;&lt;conditional-probability&gt;&lt;graphical-model&gt;&lt;notation&gt;" Title="What does the notation $(\textbf{X} \perp \textbf{Y} , \textbf{W}\mid \textbf{Z})$ mean?" ViewCount="144" />
  
  
  <row Body="&lt;p&gt;The answers here are pretty complete, but I still would like to add my 5 cents. &lt;a href=&quot;http://stats.stackexchange.com/a/110550/49130&quot;&gt;In this question&lt;/a&gt; you can find an example of R code for producing ROC Curves using One-Vs-All Approach and the &lt;a href=&quot;http://cran.r-project.org/web/packages/ROCR/index.html&quot; rel=&quot;nofollow&quot;&gt;ROCR&lt;/a&gt; R library.&lt;/p&gt;&#10;&#10;&lt;p&gt;This is the plot from that answer:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ejxF9.png&quot; alt=&quot;ROC Curve&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-04T08:49:57.770" Id="110551" LastActivityDate="2014-08-04T08:49:57.770" OwnerUserId="49130" ParentId="2151" PostTypeId="2" Score="0" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to do a Bayesian analysis in which my likelihood function is a probit function on two parameters. From various sources, I found out that Normal distribution is a conjugate prior to probit likelihood, but I am not being able to compute the posterior from the functions. Here are the details of the problem&lt;/p&gt;&#10;&#10;&lt;p&gt;Likelihood: $\Phi(\theta-\beta)$&lt;/p&gt;&#10;&#10;&lt;p&gt;Prior for $\beta$: $\phi(\beta-\mu_1)$&lt;/p&gt;&#10;&#10;&lt;p&gt;Prior for $\theta$: $\phi(\theta-\mu_2)$&lt;/p&gt;&#10;&#10;&lt;p&gt;Joint Posterior: $C\Phi(\theta-\beta)\phi(\beta-\mu_1)\phi(\theta-\mu_2)$&lt;/p&gt;&#10;&#10;&lt;p&gt;Now I want to get the marginal distributions from this joint posterior&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-08-04T09:40:14.773" Id="110557" LastActivityDate="2014-08-04T13:52:04.390" LastEditDate="2014-08-04T13:52:04.390" LastEditorUserId="22311" OwnerUserId="53355" PostTypeId="1" Score="1" Tags="&lt;bayesian&gt;&lt;prior&gt;&lt;probit&gt;" Title="Conjugate Prior for Probit likelihood function" ViewCount="51" />
  <row AnswerCount="0" Body="&lt;p&gt;I intend to assess Granger Causality between three endogenous variables, where one of these variables is a dummy variable, indicating specific events during the continuous time frame. &lt;/p&gt;&#10;&#10;&lt;p&gt;I was wondering whether normal Granger Causality tests can be used in the this case. I presume that I can, since the GC test is but an F-test of joint significance of other endogenous variables' lagged coefficients. So, if anything, the VAR model might not work with dummies. &lt;/p&gt;&#10;&#10;&lt;p&gt;Anyhow, I was hoping I could get a second opinion on this: Can I safely use dummies in a VAR and subsequent Granger Causality test?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-04T12:03:44.623" Id="110567" LastActivityDate="2014-08-04T12:03:44.623" OwnerUserId="20898" PostTypeId="1" Score="0" Tags="&lt;categorical-data&gt;&lt;var&gt;&lt;granger-causality&gt;" Title="Granger Causality test with dummy variables" ViewCount="78" />
  <row AnswerCount="1" Body="&lt;p&gt;I'm working through Rasmussen's Gaussian Processes book, and I have a question about the possibility of optimizing additional basis function hyperparameters (in section 2.7 &lt;a href=&quot;http://www.gaussianprocess.org/gpml/chapters/RW2.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.gaussianprocess.org/gpml/chapters/RW2.pdf&lt;/a&gt;).&#10;The text explains that the hyperparameters can be varied to maximize the log marginal likelihood, which is given in eq. 2.44 and 2.45.  The derivative for the general case is given in eq. 5.9.&lt;/p&gt;&#10;&#10;&lt;p&gt;I've looked in the gpml code that goes with the book and relevant literature, but I haven't been able to find the derivative wrt the hyperparameters including the basis function hyperparameters.&lt;/p&gt;&#10;&#10;&lt;p&gt;In other words, for anyone familiar with Gaussian Process Regression, what is the derivative of eq. 2.44 wrt the hyperprior? (again, &lt;a href=&quot;http://www.gaussianprocess.org/gpml/chapters/RW2.pdf&quot; rel=&quot;nofollow&quot;&gt;GPR Book, Ch. 2&lt;/a&gt;).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-04T13:07:04.047" Id="110571" LastActivityDate="2014-12-20T02:04:16.887" OwnerUserId="53259" PostTypeId="1" Score="0" Tags="&lt;bayesian&gt;&lt;maximum-likelihood&gt;&lt;gaussian-process&gt;&lt;marginal&gt;" Title="Gaussian Process Regression with additional Basis Functions" ViewCount="52" />
  
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;My question is simple: is there a function in &lt;code&gt;R&lt;/code&gt; which estimates the linear regresion model in a similar fashion as &lt;code&gt;lm&lt;/code&gt;, but only using the means, variances, and covariance (correlations), i.e. the sufficient statistics? I am looking for a function to which I can input these statistics (plus sample size) and it returns regression coefficients and tests.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-08-04T18:21:00.917" Id="110597" LastActivityDate="2015-02-20T11:04:39.970" LastEditDate="2015-02-20T11:04:39.970" LastEditorUserId="53618" OwnerUserId="24515" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;regression&gt;&lt;multiple-regression&gt;&lt;linear-model&gt;" Title="How can I use the sufficient statistics (variances, covariances, means) to estimate a linear regression model in R?" ViewCount="94" />
  
  <row Body="&lt;p&gt;Donbeo, a couple of pointers:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;When training a random forest model, you need to optimize the tuning parameter &lt;code&gt;mtry&lt;/code&gt;, which is the number of features randomly selected for each tree. Use five- or ten-fold cross-validation for this. The reason why &lt;code&gt;mtry&lt;/code&gt; could influence out-of-sample prediction error is that when you grow larger trees, the trees are going to be more correlated with one another.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;When training a random forest model, you should also grow a large enough forest. Perhaps 100 trees is not a big enough forest. Try growing a bigger forest in addition to optimize &lt;code&gt;mtry&lt;/code&gt;. You need not worry about the size of the forest leading to over-fitting. Actually, the bigger the forest, the better (although there are diminishing returns).&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2014-08-04T23:16:02.497" Id="110629" LastActivityDate="2014-08-04T23:16:02.497" OwnerUserId="7616" ParentId="110616" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Slutsky's Theorem.  Need I say more?&lt;/p&gt;&#10;" CommentCount="7" CommunityOwnedDate="2014-08-04T23:44:03.987" CreationDate="2014-08-04T23:44:03.987" Id="110631" LastActivityDate="2014-08-04T23:44:03.987" OwnerUserId="35150" ParentId="1337" PostTypeId="2" Score="-7" />
  <row AnswerCount="1" Body="&lt;p&gt;I am analyzing my data of my experiment and I have a question concerning the repeated measures ANOVA. I have 2 independent variables (temperature and salinity), 1 dependent which is length and 2 measuring points (3rd and 7th day) of the experiment. &lt;/p&gt;&#10;&#10;&lt;p&gt;So my question is, can I use repeated measures ANOVA when I only have 2 time points? I read somewhere that you need more than 3. &lt;/p&gt;&#10;&#10;&lt;p&gt;In case I can't do it, is it possible to run a 3-way ANOVA and set temperature, salinity and day (3rd and 7th) as a factor?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-08-04T23:54:19.277" Id="110633" LastActivityDate="2015-02-05T06:02:49.043" LastEditDate="2014-08-05T02:36:17.397" LastEditorUserId="183" OwnerUserId="53401" PostTypeId="1" Score="0" Tags="&lt;anova&gt;&lt;repeated-measures&gt;" Title="Can repeated measures ANOVA be used with only 2 time points?" ViewCount="62" />
  <row Body="&lt;p&gt;Your problem is here:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;  function(x) {cor(xx$Data1.MEAN, xx$Data2.MEAN)})&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The variable passed in is &lt;code&gt;x&lt;/code&gt;, not &lt;code&gt;xx&lt;/code&gt;. The function only knows &lt;code&gt;xx&lt;/code&gt; because it exists in the parent environment. You pass in the subsetted data but then ignore it in favor of the unsubsetted data. That is, because you reference a variable other than the one &lt;code&gt;by&lt;/code&gt; passes it, it evaluates the correlation on the &lt;em&gt;whole data&lt;/em&gt; every time.&lt;/p&gt;&#10;&#10;&lt;p&gt;Replace &lt;code&gt;xx&lt;/code&gt; with &lt;code&gt;x&lt;/code&gt; there and it looks right to me, though the last line might be superfluous depending on what you're trying to achieve.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-05T04:41:52.473" Id="110649" LastActivityDate="2014-08-05T04:41:52.473" OwnerUserId="805" ParentId="109534" PostTypeId="2" Score="1" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I heard somewhere that in astrophysics, there are more bayesians than frequentists (although in general, I think that bayesian community is smaller). Is there something special about &quot;astro&quot; data what makes it more suitable for bayesian methods?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt;: The motivation for my question was also &lt;a href=&quot;http://www.kaggle.com/c/DarkWorlds&quot; rel=&quot;nofollow&quot;&gt;this kaggle competition&lt;/a&gt;, where the bayesian approach outperformed others. Therefore it seems that there could be something special about astro data.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-08-05T12:15:23.560" Id="110702" LastActivityDate="2014-08-05T16:59:23.847" LastEditDate="2014-08-05T13:17:04.633" LastEditorUserId="14730" OwnerUserId="14730" PostTypeId="1" Score="2" Tags="&lt;bayesian&gt;&lt;astronomy&gt;" Title="Why are there more bayesians than frequentists in astrophysics?" ViewCount="81" />
  <row AnswerCount="0" Body="&lt;p&gt;I'd like to run a chi-squared test in Python with scipy. I've created code to do this, but I don't know if what I'm doing is right, because the scipy docs are quite sparse. &lt;/p&gt;&#10;&#10;&lt;p&gt;Background first: I have two groups of users. My null hypothesis is that there is no significant difference in whether people in either group are more likely to use desktop, mobile, or tablet.&lt;/p&gt;&#10;&#10;&lt;p&gt;These are the &lt;em&gt;observed&lt;/em&gt; frequencies in the two groups:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;[[u'desktop', 14452], [u'mobile', 4073], [u'tablet', 4287]]&#10;[[u'desktop', 30864], [u'mobile', 11439], [u'tablet', 9887]]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Here is my code using &lt;a href=&quot;http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2_contingency.html&quot; rel=&quot;nofollow&quot;&gt;&lt;code&gt;scipy.stats.chi2_contingency&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;obs = np.array([14452, 4073, 4287], [30864, 11439, 9887])&#10;chi2, p, dof, expected = stats.chi2_contingency(obs)&#10;print p&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This gives me a p-value of &lt;code&gt;2.02258737401e-38&lt;/code&gt;, which clearly is significant.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is: does this code look valid? In particular, I'm not sure whether I should be using &lt;a href=&quot;http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2_contingency.html&quot; rel=&quot;nofollow&quot;&gt;scipy.stats.chi2_contingency&lt;/a&gt; or &lt;a href=&quot;http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mstats.chisquare.html&quot; rel=&quot;nofollow&quot;&gt;scipy.stats.chisquare&lt;/a&gt;, given the data I have. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-05T13:39:40.853" Id="110718" LastActivityDate="2014-08-05T20:54:14.840" LastEditDate="2014-08-05T20:54:14.840" LastEditorUserId="36825" OwnerUserId="36825" PostTypeId="1" Score="0" Tags="&lt;hypothesis-testing&gt;&lt;statistical-significance&gt;&lt;chi-squared&gt;&lt;python&gt;&lt;scipy&gt;" Title="Chi-squared test with scipy: what's the difference between chi2_contingency and chisquare?" ViewCount="137" />
  <row Body="&lt;p&gt;Since your data is bounded, your variance will be bounded above. See &lt;a href=&quot;http://stats.stackexchange.com/q/18621/36229&quot;&gt;Maximum value of coefficient of variation for bounded data set&lt;/a&gt;. Therefore you should choose a prior for $\sigma^2$ that, at the very least, has support only from 0 to that upper bound.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, once you've chosen a prior, you can validate your choice by comparing the prior predictive distribution to the corresponding (marginal) distribution of your data.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-05T13:42:01.467" Id="110720" LastActivityDate="2014-08-05T13:42:01.467" OwnerUserId="36229" ParentId="100438" PostTypeId="2" Score="1" />
  <row Body="&lt;h2&gt;$\chi^2$ Test&lt;/h2&gt;&#10;&#10;&lt;p&gt;For a $\chi^2$ test of independence, we have to have a contingency table, so we assume two modalities for each variable: $\text{dom}(A) = \text{dom}(B) = \{`1`, `\text{not} 1`\}$&lt;/p&gt;&#10;&#10;&lt;p&gt;With little bit of calculation we have the following table:&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{array}{|c|l|l|r|}&#10;\hline&#10; &amp;amp; 1 &amp;amp; \text{not } 1 &amp;amp; \text{total} \\&#10;\hline&#10;1 &amp;amp; 2871 &amp;amp; 32300 &amp;amp; 35171 \\&#10;\text{not } 1 &amp;amp; 10812 &amp;amp; 154017 &amp;amp; 164829 \\&#10;\hline&#10; &amp;amp; 13683 &amp;amp; 186317 &amp;amp; 200000 \\&#10;\hline&#10;\end{array}&lt;/p&gt;&#10;&#10;&lt;p&gt;Given that we expect $P(A=1) = 0.068$ and $P(B=1) = 0.175$, we have the following table with expected values&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{array}{|c|l|l|r|}&#10;\hline&#10; &amp;amp; 1 &amp;amp; \text{not } 1 &amp;amp; \text{total} \\&#10;\hline&#10;1 &amp;amp; 2380 &amp;amp; 32620 &amp;amp; 35000 \\&#10;\text{not } 1 &amp;amp; 11220 &amp;amp; 153780 &amp;amp; 165000 \\&#10;\hline&#10; &amp;amp; 13600 &amp;amp; 186400 &amp;amp; 200000 \\&#10;\hline&#10;\end{array}&lt;/p&gt;&#10;&#10;&lt;p&gt;Now we calculate the normalized squared difference $X^2 = \sum_i \frac{(O_i-E_i)^2}{E_i}$ which follows the $\chi^2$ distribution with $\text{df} = 1$. &lt;/p&gt;&#10;&#10;&lt;p&gt;In this case, $X^2 = 119.6353$ which is very unusual ($p \approx 10^{-28}$) to observe under assumption that $A$ and $B$ are independent. So we conclude that the variables are not independent. &lt;/p&gt;&#10;&#10;&lt;p&gt;Also we could calculate Crammer's V, which, in this case, is $v = \sqrt{ X^2 / n } = \sqrt{119.6353 / 200000} \approx 0.02$, meaning that the correlation is strong&lt;/p&gt;&#10;&#10;&lt;h2&gt;R code&lt;/h2&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;obs = matrix(data=c(2871, 32300, 10812, 154017), nrow=2, ncol=2,&#10;             byrow=T, dimnames=list(A=c('1', 'not 1'), B=c('1', 'not 1')))&#10;&#10;exp = matrix(data=c(2380, 32620, 11220, 153780), nrow=2, ncol=2,&#10;             byrow=T, dimnames=list(A=c('1', 'not 1'), B=c('1', 'not 1')))&#10;&#10;chi2 = sum((obs - exp)^2 / exp)&#10;pchisq(q=chi2, df=1, lower.tail=F)&#10;&#10;sqrt(chi2 / sum(obs))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;h2&gt;References&lt;/h2&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://0agr.ru/wiki/index.php/Chi-square_Test_of_Independence&quot; rel=&quot;nofollow&quot;&gt;http://0agr.ru/wiki/index.php/Chi-square_Test_of_Independence&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://0agr.ru/wiki/index.php/Crammer%27s_Coefficient&quot; rel=&quot;nofollow&quot;&gt;http://0agr.ru/wiki/index.php/Crammer%27s_Coefficient&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="3" CreationDate="2014-08-05T17:07:09.597" Id="110745" LastActivityDate="2014-08-05T17:07:09.597" OwnerUserId="49130" ParentId="110690" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;Sorry to pick up on an old thread, but I have a concern with your polynomial developments of the two theta's, as I learned that the  MA operator (here both theta and Theta) were represented as polynomials with a &lt;strong&gt;minus sign&lt;/strong&gt;, &#10;therefore I would have written&lt;/p&gt;&#10;&#10;&lt;p&gt;θ1(B)=1 - θ1B,&lt;/p&gt;&#10;&#10;&lt;p&gt;Θ1(B12)=1 - Θ1B12, &lt;/p&gt;&#10;&#10;&lt;p&gt;which changes the sign in the final answer too&#10;Yt−2Yt−1+Yt−2=et - Θ1et−12 - θ1et−1 + θ1Θ1et−13...?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-05T18:01:39.997" Id="110752" LastActivityDate="2014-08-05T18:01:39.997" OwnerUserId="53208" ParentId="61510" PostTypeId="2" Score="0" />
  <row AnswerCount="2" Body="&lt;p&gt;When modeling time series one has the possibility to &#10;(1) model the correlational structure of the error terms as e.g. an AR(1) process&#10;(2) include the lagged dependent variable as an explanatory variable (on the right hand side)&lt;/p&gt;&#10;&#10;&lt;p&gt;I understand that their are sometimes substantial reasons to go for (2). &lt;/p&gt;&#10;&#10;&lt;p&gt;However, what are the &lt;strong&gt;methodological reasons&lt;/strong&gt; to do either (1) or (2) or even both?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-05T18:10:49.807" FavoriteCount="1" Id="110757" LastActivityDate="2015-01-26T18:25:41.203" OwnerUserId="13212" PostTypeId="1" Score="5" Tags="&lt;time-series&gt;&lt;autocorrelation&gt;&lt;residuals&gt;&lt;lags&gt;" Title="Residual autocorrelation versus lagged dependent variable" ViewCount="174" />
  
  
  <row AcceptedAnswerId="110785" AnswerCount="4" Body="&lt;p&gt;I am wondering if the following equality holds - $Var(X + U | X) = Var(U)$? where $X$ and $U$ are two independent random variables?&lt;/p&gt;&#10;&#10;&lt;p&gt;It seems can we say $Var(X + U | X) = Var(X|X) + Var(U|X) = Var(X|X) + Var(U)$ as $U, X$ independent? And then $Var(X|X) = 0$? But that doesn't seem right.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can someone clarify this for me, I'm not very experienced with conditional variance.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-05T20:46:00.413" Id="110777" LastActivityDate="2014-08-05T22:14:53.783" OwnerUserId="32524" PostTypeId="1" Score="3" Tags="&lt;variance&gt;&lt;random-variable&gt;&lt;expected-value&gt;" Title="Conditional variance - $Var(X + U | X) = Var(U)$?" ViewCount="52" />
  
  <row Body="&lt;p&gt;One way to pool the stepwise analyses of individual imputations is to identify the predictors that most frequently appear in them. &lt;/p&gt;&#10;&#10;&lt;p&gt;For example, &lt;code&gt;names(coef())&lt;/code&gt; applied to output from &lt;code&gt;stepAIC()&lt;/code&gt; (MASS package) applied to a linear regression gives the predictor names retained in the final stepwise regression.  Add up, over all the individual imputations analyzed this way, how often each predictor appears. That quickly shows which predictors are the best candidates for your further model building.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm assuming that you have a good reason for reducing the number of variables in your model. Stepwise selection loses information; for application to prediction, a full model even including &quot;non-significant&quot; predictors may be preferable.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-08-05T21:00:42.957" Id="110780" LastActivityDate="2014-08-05T21:00:42.957" OwnerUserId="28500" ParentId="110585" PostTypeId="2" Score="1" />
  
  
  
  <row Body="&lt;p&gt;Multinomial regression suits nominal outcome data like your second example (car, train, ship...tricycle...), but ignores any order in those categories. I suppose it &lt;em&gt;could&lt;/em&gt; work for any set of discrete outcomes, but it would waste information and estimate model parameters inefficiently if used to model ordinal (ranked) or cardinal (count) data. Ordinal logistic regression is better for ordered categorical outcome data.&lt;/p&gt;&#10;&#10;&lt;p&gt;Poisson regression is probably better for modeling cardinal data, but it isn't optimal for all count data. The &lt;a href=&quot;http://en.wikipedia.org/wiki/Poisson_distribution&quot; rel=&quot;nofollow&quot;&gt;Poisson distribution&lt;/a&gt;'s expected value is equal to its variance, which &lt;a href=&quot;http://en.wikipedia.org/wiki/Poisson_regression#Overdispersion_and_Zero_Inflation&quot; rel=&quot;nofollow&quot;&gt;can be a problem&lt;/a&gt; (e.g., with dependent observations or an excess of zeroes). Negative binomial regression can accommodate overdispersed counts better (see also &quot;&lt;a href=&quot;http://stats.stackexchange.com/q/20826/32036&quot;&gt;Poisson or quasi poisson in a regression with count data and overdispersion?&lt;/a&gt;&quot;). Zero-inflated models help handle datasets with lots of counts equal to zero, though this requires more statistical power to do well.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-06T05:56:35.393" Id="110835" LastActivityDate="2014-08-06T05:56:35.393" OwnerUserId="32036" ParentId="110832" PostTypeId="2" Score="3" />
  
  <row AnswerCount="1" Body="&lt;p&gt;We sent out a survey to find if educational material about a type of surgery would potentially influence the decision of subjects. Prior to giving them this educational material we asked them demographic questions (e.g. sex, job, etc.), several questions on a 5 point scale to assess their priorities (e.g. &quot;How important is cosmesis?&quot;). After receiving this educational material we then asked them which type of surgery they potentially would be interested in.&lt;/p&gt;&#10;&#10;&lt;p&gt;For this analysis, I would like to see if &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;the pre and post-educational material preferences changed&lt;/li&gt;&#10;&lt;li&gt;if demographic information (e.g. age, education, sex) has an influence on choice after receiving the educational material&lt;/li&gt;&#10;&lt;li&gt;if the pre-educational material priorities are associated with change in choice after receiving&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;This survey was given to a random group of medical professionals, and I am assuming that the distribution of the data collected from this population is non-normal.&lt;/p&gt;&#10;&#10;&lt;p&gt;This makes me think that I need to perform non-parametric tests, but I am having difficulty in figuring out which test would be best. So far I think I may have to perform a Friedman Test since I want to do a pre/post comparison between the same group. However I would appreciate if anyone could clarify the matter for me. Thanks!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-06T12:52:38.603" Id="110863" LastActivityDate="2014-08-16T14:43:31.943" OwnerUserId="9244" PostTypeId="1" Score="0" Tags="&lt;nonparametric&gt;&lt;survey&gt;" Title="Is the Friedman Test Suitable For an Analysis of Pre/Post-Intervention Choices From Survey Data?" ViewCount="49" />
  <row AcceptedAnswerId="110866" AnswerCount="1" Body="&lt;p&gt;Can we use duplicate data as an input to SVM? The duplicate data that I mean is, let say we have 50 of same data (maybe being duplicate) from total of 100 data. Will this kind of data effect the performance result that we obtained from SVM? Do we have to avoid any duplicate data for SVM?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-05T17:03:07.043" Id="110865" LastActivityDate="2014-08-06T13:02:37.843" OwnerDisplayName="user3847711" OwnerUserId="54605" PostTypeId="1" Score="0" Tags="&lt;svm&gt;" Title="Duplicate data for SVM" ViewCount="35" />
  <row Body="&lt;p&gt;You can, yes as long as the repeated data has the same label there will be no problem. It is not ideal though. The performance will be lower than if you trained with a more diverse dataset of the same size, and just as good as if you dropped the duplicates and trained.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-08-05T20:46:47.793" Id="110866" LastActivityDate="2014-08-05T20:46:47.793" OwnerDisplayName="carlosdc" OwnerUserId="1540" ParentId="110865" PostTypeId="2" Score="1" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;How can I get a good estimate of parameter $\nu$ of the &lt;a href=&quot;http://en.wikipedia.org/wiki/Rice_distribution&quot; rel=&quot;nofollow&quot;&gt;Rice distribution&lt;/a&gt; based on a set of $(x,y)$-coordinates? &lt;strong&gt;Edit&lt;/strong&gt;: Given whuber's excellent comment, I'm not looking for an &lt;em&gt;unbiased&lt;/em&gt; estimate. Instead, I'd like to know whether there is an estimate that takes advantage of having available the 2D points themselves (not just their Euclidean norm) as well as an estimate of $\sigma$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Background: &lt;/p&gt;&#10;&#10;&lt;p&gt;Given an uncorrelated bivariate normal variable $X$ with mean $\mu \neq 0$ with covariance matrix $\Sigma = \sigma^{2}I$, the Euclidean norm $R = \sqrt{X'X}$ (the radius, or distance to the origin) follows a Rice distribution with parameters $\nu = \sqrt{\mu'\mu}$ and $\sigma$.&lt;/p&gt;&#10;&#10;&lt;p&gt;When given a set of $N$ 2D-points $x = \mu + e$ from a Rice distribution, getting an unbiased estimate of $\sigma$ is straightforward and works as it does for the &lt;a href=&quot;http://ballistipedia.com/index.php?title=Closed_Form_Precision#Variance_Estimates&quot; rel=&quot;nofollow&quot;&gt;Rayleigh distribution&lt;/a&gt;. The obvious idea to estimate $\nu$ is $\sqrt{\bar{x}'\bar{x}}$, but if I'm not mistaken, the square has expectation $E(\bar{x}'\bar{x}) = \mu'\mu + E(e'e) = \mu'\mu + tr(\frac{1}{N}\Sigma) = \mu'\mu + \frac{2}{N} \sigma^{2}$. And this is before taking the square-root which introduces another bias.&lt;/p&gt;&#10;&#10;&lt;p&gt;The &lt;a href=&quot;http://en.wikipedia.org/wiki/Rice_distribution#Parameter_estimation_.28the_Koay_inversion_technique.29&quot; rel=&quot;nofollow&quot;&gt;Wikipedia page&lt;/a&gt; mentions methods for parameter estimation, but they seem to be for the situation when we don't have the original 2D points, but just their Euclidean norm. It's possible to apply these methods after calculating the distance of the 2D points, they are complicated and based on fewer information than what I have.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-08-06T15:05:23.910" Id="110890" LastActivityDate="2014-08-06T17:53:40.147" LastEditDate="2014-08-06T17:53:40.147" LastEditorUserId="1909" OwnerUserId="1909" PostTypeId="1" Score="3" Tags="&lt;normal-distribution&gt;&lt;euclidean&gt;" Title="Rice distribution: estimate $\nu$ from 2D-data" ViewCount="26" />
  <row Body="&lt;p&gt;I gave a similar answer here &lt;a href=&quot;http://stats.stackexchange.com/questions/29781/when-should-you-center-your-data-when-should-you-standardize/&quot;&gt;When should you center your data &amp;amp; when should you standardize?&lt;/a&gt; but thought it was sufficiently different context that an answer could go here.  &lt;/p&gt;&#10;&#10;&lt;p&gt;There is a great usenet resource &lt;a href=&quot;http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html&quot; rel=&quot;nofollow&quot;&gt;http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;It gives in simple terms some of the issues and considerations when one wants to normalize/standardize/rescale the data.  As it treats the subject from a machine learning perspective, and as your question is ML, it could have some relevance.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-06T15:07:34.657" Id="110891" LastActivityDate="2014-08-06T15:07:34.657" OwnerUserId="31564" ParentId="7757" PostTypeId="2" Score="2" />
  <row AnswerCount="0" Body="&lt;p&gt;Given a set of rule such as:&lt;br/&gt;&#10;$A \rightarrow B$; $B \rightarrow C$; that satisfy minimum confidence in the context of apriori algorithm meaning:&#10;$$\text{Conf}(A \rightarrow B) \geq \text{min. confidence}$$ and &#10;$$\text{Conf}(B \rightarrow C) \geq \text{min. confidence}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Is it possible to state that $\text{Conf}(A \rightarrow C) \geq \text{min. confidence}$?&lt;/p&gt;&#10;&#10;&lt;p&gt;I believe it's not possible to assert the later given that&#10;the relation between $\text{Support}(A \rightarrow B)$ and $\text{Support}(B \rightarrow C)$ doesn't give any information about the $\text{Support}(A \rightarrow C)$&lt;/p&gt;&#10;&#10;&lt;p&gt;Does this make sense? &lt;/p&gt;&#10;&#10;&lt;p&gt;Note: it's just a book exercise I'm trying to solve(not homework!)&#10;Here is the &lt;a href=&quot;http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf&quot; rel=&quot;nofollow&quot;&gt;chapter link&lt;/a&gt;, it's in page 405 exercise 3.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-06T15:13:35.420" Id="110893" LastActivityDate="2014-08-06T15:28:56.570" LastEditDate="2014-08-06T15:28:56.570" LastEditorUserId="26338" OwnerUserId="27691" PostTypeId="1" Score="1" Tags="&lt;confidence&gt;&lt;association-rules&gt;&lt;apriori&gt;" Title="Is confidence transitive in asociation rules?" ViewCount="20" />
  <row AnswerCount="0" Body="&lt;p&gt;Suppose ($h_1,h_2,...,h_n$) is an $n\times 1$ vector.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $h_i=g_iX_i$, where $g_i$ is a non-random variable which can vary across $i$ and $X_i$ is a random variable with Pareto Type I distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;$$f_X(x) =&#10;\begin{cases}&#10;{\alpha x_m^\alpha \over x^{\alpha +1}},  &amp;amp; \text{if $x\ge x_m$} \\[2ex]&#10;0, &amp;amp; \text{if $x\lt x_m$}  \\&#10;\end{cases}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;What is the probability, as a function of $g$'s, that any given $h_i$ will be among the top $k$ largest of the $n$ $h$'s?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-08-06T18:19:36.060" Id="110922" LastActivityDate="2014-08-07T00:28:19.450" LastEditDate="2014-08-07T00:28:19.450" LastEditorUserId="22311" OwnerUserId="53572" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;random-variable&gt;&lt;ranking&gt;&lt;order-statistics&gt;" Title="probability that a variable is ONE OF the top k out of n when ordered" ViewCount="47" />
  <row AnswerCount="2" Body="&lt;p&gt;I have two types of missing data (non-responses and &quot;Don't know or does not apply&quot;) and I want to replace only the non-responses (with mean or by using EM or by using Multiple Imputation) and to see if this is a good solution. How can this be done in SPSS? Even in the simplest case, when replacing missing values with mean for example, I can't find a way to impute only some of the missing data, in this case the non-responses?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-06T22:07:00.097" Id="110950" LastActivityDate="2014-08-06T23:29:10.150" OwnerUserId="49217" PostTypeId="1" Score="1" Tags="&lt;missing-data&gt;" Title="How to replace only SOME missing data" ViewCount="46" />
  
  <row Body="&lt;p&gt;I'm afraid not. No model is perfect, and those false positives are just misclassification errors. And when you apply your tree to real data, you'll have no idea what's actually 0 and what's actually 1 except for what the tree tells you.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-06T23:48:16.810" Id="110964" LastActivityDate="2014-08-06T23:48:16.810" OwnerUserId="46522" ParentId="110940" PostTypeId="2" Score="-1" />
  
  <row Body="&lt;p&gt;&lt;code&gt;mlogit&lt;/code&gt; has a predict function, but I found it very difficult to use. I wrote my own &lt;em&gt;very ugly&lt;/em&gt; set of functions for an implementation that I have. Anyone is welcome to use or improve them, stored on my &lt;a href=&quot;https://github.com/gregmacfarlane/MLogitTools&quot; rel=&quot;nofollow&quot;&gt;github&lt;/a&gt; profile.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-07T01:50:45.063" Id="110977" LastActivityDate="2014-08-07T01:50:45.063" OwnerUserId="10026" ParentId="6702" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;For each pair of examples, introduce both permutations to your training matrix. If necessary, at evaluation time average the results of both permutations.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-08-07T03:34:54.463" Id="110985" LastActivityDate="2014-08-07T03:34:54.463" OwnerUserId="9568" ParentId="110948" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;AUC isn't the same thing as accuracy.&lt;/p&gt;&#10;&#10;&lt;p&gt;The situation where you want to use a ROC curve or the AUC is when you're evaluating a continuous measure and don't have a clear choice of where to make the cutoff; it describes the range of possible behaviors between the true positive and false positive rates.&lt;/p&gt;&#10;&#10;&lt;p&gt;A confusion matrix is for when you have something divided into distinct categories, and tells you the full behavior of the classifier there.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you know the total number of positives and negatives in your test set, though, you can reconstruct a confusion matrix from any point on the ROC curve.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-07T03:52:49.520" Id="110989" LastActivityDate="2014-08-07T03:52:49.520" OwnerUserId="9964" ParentId="110986" PostTypeId="2" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;I want to ask about response rate. I have distributed 2500 posts. However, I have received back around 30. Then, I sent 200 posts as reminders with appealing letter. I have received one or two only. The total usable questionnaires: 21.&#10;After a while, I determined that I need to send by email. Therefore, I sent over 5,000 emails with around 6-time reminders. over 500 people have opened the questionnaire, but not all of them responded. Maybe people who showed an intention to reply by answering few questions are around 250. The usable ones around 165.&#10;Is there any problem with response rate? The informants were owner-manager of small and medium manufacturing enterprises.&#10;In addition, I have tested the homogeneity among the both groups (posts and emails) they could emerged except in DV they show significant difference?&#10;1. The question can I emerge the both groups even response rates of posts is too low at the first place?&#10;2. If yes (I can merge), what should I do as only both groups show difference regarding DV (not IVs, not demographic variables)&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-08-07T04:12:48.800" Id="110992" LastActivityDate="2014-08-07T04:12:48.800" OwnerUserId="48151" PostTypeId="1" Score="0" Tags="&lt;statistical-significance&gt;&lt;computational-statistics&gt;&lt;descriptive-statistics&gt;" Title="Response Rate in Research" ViewCount="16" />
  <row AcceptedAnswerId="111003" AnswerCount="1" Body="&lt;ul&gt;&#10;&lt;li&gt;Root mean square error&lt;/li&gt;&#10;&lt;li&gt;residual sum of squares&lt;/li&gt;&#10;&lt;li&gt;residual standard error&lt;/li&gt;&#10;&lt;li&gt;mean squared error&lt;/li&gt;&#10;&lt;li&gt;test error&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I thought I used to understand these terms but the more I do statistic problems the more I have gotten myself confused where I second guess myself. I would like some re-assurance &amp;amp; a concrete example&lt;/p&gt;&#10;&#10;&lt;p&gt;I can find the equations easily enough online but I am having trouble getting a 'explain like I'm 5' explanation of these terms so I can crystallize in my head the differences and how one leads to another.&lt;/p&gt;&#10;&#10;&lt;p&gt;If anyone can take this code below and point out how I would calculate each one of these terms I would appreciate it. R code would be great..&lt;/p&gt;&#10;&#10;&lt;p&gt;Using this example below:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;summary(lm(mpg~hp, data=mtcars))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Show me in R code how to find:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;rmse = ____&#10;rss = ____&#10;residual_standard_error = ______  # i know its there but need understanding&#10;mean_squared_error = _______&#10;test_error = ________&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Bonus points for explaining like i'm 5 the differences/similarities between these.  example: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;rmse = squareroot(mss)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2014-08-07T05:57:04.090" FavoriteCount="4" Id="110999" LastActivityDate="2014-08-07T08:20:31.033" LastEditDate="2014-08-07T08:20:31.033" LastEditorUserId="3556" OwnerUserId="53610" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;regression&gt;&lt;residuals&gt;&lt;residual-analysis&gt;" Title="R - Confused on Residual Terminology" ViewCount="674" />
  
  
  
  <row AcceptedAnswerId="111059" AnswerCount="2" Body="&lt;p&gt;I am using a MANOVA test to compare nine different dependent variables (from neuropsychological and neuropsychiatric assessment) between three groups. The output shows a significant influence from GROUP on my variables ($p &amp;lt; .001$). &lt;/p&gt;&#10;&#10;&lt;p&gt;Ofcourse, I am interested in how the three groups influence every dependent variable. I have studied Field's &quot;Discovering Statistics Using IBM SPSS Statistics&quot; chapter 16, and he states that the preferred post-hoc analysis is a discriminant analysis, because of the linear combination in which the dependent variables are related to group membership in a MANOVA. Discriminant analysis could account for this linear combination, so Field states.&lt;/p&gt;&#10;&#10;&lt;p&gt;Otherwise, I read some literature, on basic statistical sites, where is stated that I can use multiple univariate ANOVA's with Bonferroni correction, and use a post-hoc on these univariate ANOVA's when they are significant. &lt;/p&gt;&#10;&#10;&lt;p&gt;Which of these method's is better? That is, which will make my chance of a Type I or II error the least?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-07T13:49:35.667" Id="111044" LastActivityDate="2015-01-08T11:33:03.263" LastEditDate="2015-01-08T11:14:44.480" LastEditorUserId="28666" OwnerUserId="53099" PostTypeId="1" Score="2" Tags="&lt;power&gt;&lt;post-hoc&gt;&lt;manova&gt;&lt;discriminant-analysis&gt;&lt;bonferroni&gt;" Title="Post-hoc tests for MANOVA: univariate ANOVAs or discriminant analysis?" ViewCount="726" />
  <row Body="&lt;p&gt;Three points: &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;The null hypothesis of the Kruskal-Wallis &lt;strong&gt;is not&lt;/strong&gt; what you have written, but rather &lt;em&gt;stochastic equality&lt;/em&gt;, H$_{0}\text{: P}\left(X_{i} &amp;gt; X_{j}\right) = 0.5$ for all $i,j \in \{1,\dots,k\}$ for $k$ groups (assuming the CDFs of any two groups do not cross), so you are testing for &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_dominance&quot; rel=&quot;nofollow&quot;&gt;stochastic dominance&lt;/a&gt;. When more stringent assumptions that each treatment has identically shaped distributions, and differences are entirely in location-shift, then you can interpret the null hypothesis as equality of medians, and the test as a test for median difference; and &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Your data do not sound appropriate to the Kruskal-Wallis test, because you have a &lt;em&gt;blocked study design&lt;/em&gt; where the same individuals are measured repeatedly. Thus you are looking for a repeated measures test for 'treatment', given your scoring variable, &lt;a href=&quot;https://en.wikipedia.org/wiki/Repeated_measures_design#Repeated_measures_ANOVA&quot; rel=&quot;nofollow&quot;&gt;repeated measures ANOVA&lt;/a&gt; is perhaps not a good candidate. However, the nonparametric &lt;a href=&quot;https://en.wikipedia.org/wiki/Friedman_test&quot; rel=&quot;nofollow&quot;&gt;Friedman test&lt;/a&gt; may well suit your needs; plus&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Tests like Kruskal-Wallis and Friedman assume that the data (your scores) are &lt;em&gt;continuously&lt;/em&gt; measured. There are often 'corrections for ties' in nonparametric tests, but you should make sure that your statistical software &lt;em&gt;uses&lt;/em&gt; such, and bear in mind that lots of ties (as might happen when there are only five possible scores) may distort your results.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="12" CreationDate="2014-08-07T15:09:05.840" Id="111053" LastActivityDate="2014-08-07T20:17:02.303" LastEditDate="2014-08-07T20:17:02.303" LastEditorUserId="44269" OwnerUserId="44269" ParentId="111050" PostTypeId="2" Score="4" />
  <row AnswerCount="1" Body="&lt;p&gt;I am really stuck with this question: &#10;The Mann-Whitney test requires homogenity of variance if a median difference is suppossed to be statistically significant.&lt;/p&gt;&#10;&#10;&lt;p&gt;In case homogenity of variance is not met, but the test is significant: &#10;Which aspects of the test can I report? &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks a lot in advance. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-08-07T15:30:06.630" FavoriteCount="1" Id="111058" LastActivityDate="2014-08-07T17:17:11.367" OwnerDisplayName="user53632" PostTypeId="1" Score="2" Tags="&lt;mann-whitney-u-test&gt;" Title="Reporting Mann-Whitney U-Test without homogenity of variance" ViewCount="122" />
  
  <row Body="&lt;p&gt;Most classification models in R produce both a class prediction and the probabilities for each class. For binary data, in almost every case, the class prediction is based on a 50% probability cutoff. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;glm&lt;/code&gt; is the same. With &lt;code&gt;caret&lt;/code&gt;, using &lt;code&gt;predict(object, newdata)&lt;/code&gt; gives you the predicted class and &lt;code&gt;predict(object, new data, type = &quot;prob&quot;)&lt;/code&gt; will give you class-specific probabilities (when &lt;code&gt;object&lt;/code&gt; is generated by &lt;code&gt;train&lt;/code&gt;). &lt;/p&gt;&#10;&#10;&lt;p&gt;You can do things differently by &lt;a href=&quot;http://topepo.github.io/caret/custom_models.html&quot; rel=&quot;nofollow&quot;&gt;defining your own model&lt;/a&gt; and applying whatever cutoff that you want. The &lt;code&gt;caret&lt;/code&gt; &lt;a href=&quot;http://topepo.github.io/caret/&quot; rel=&quot;nofollow&quot;&gt;website&lt;/a&gt; also has an &lt;a href=&quot;http://topepo.github.io/caret/custom_models.html#Illustration5&quot; rel=&quot;nofollow&quot;&gt;example&lt;/a&gt; that uses resampling to optimize the probability cutoff. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;confusionMatrix&lt;/code&gt; uses the predicted classes and thus a 50% probability cutoff&lt;/p&gt;&#10;&#10;&lt;p&gt;Max&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-07T15:50:51.037" Id="111067" LastActivityDate="2014-08-07T15:50:51.037" OwnerUserId="3468" ParentId="110969" PostTypeId="2" Score="2" />
  
  <row Body="&lt;h2&gt;These two follow-up approaches have very different goals!&lt;/h2&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;Univariate ANOVAs (as follow-ups to MANOVA) aim at checking which individual variables (as opposed to all variables together) differ between groups.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Linear Discriminant Analysis, LDA, (as a follow-up to MANOVA) aims at checking which linear combination of individual variables leads to maximal group separability and at interpreting this linear combination.&lt;/p&gt;&#10;&#10;&lt;p&gt;This question asked about one-way MANOVA with only a single factor, but see here for the [more complicated] case of factorial MANOVA: &lt;a href=&quot;http://stats.stackexchange.com/questions/131241&quot;&gt;How to follow up a factorial MANOVA with discriminant analysis?&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;So e.g. if your individual variables are weight and height, then with univariate ANOVAs you can test if weight and height, separately, differ between groups. With LDA you can find out that the best group separability is given by, say, 2*weight+3*height. Then you can try to interpret this linear combination.&lt;/p&gt;&#10;&#10;&lt;p&gt;So the choice between these two follow-up approaches entirely depends on what you want to test.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;h2&gt;Two further remarks&lt;/h2&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;First,&lt;/strong&gt; if you are &lt;em&gt;&quot;interested in how the three groups influence every dependent variable&quot;&lt;/em&gt; (i.e. individual DVs are of primary interest), then you should arguably not run MANOVA at all, but go straight to univariate ANOVAs! Correct for multiple comparisons (note that Bonferroni is very conservative, you might prefer to control false discover rate instead; &lt;em&gt;but see comment below for another opinion&lt;/em&gt;), but proceed with univariate tests. After all, nine DVs are not a lot. If, instead, you are interested in whether groups differed &lt;em&gt;at all&lt;/em&gt; (and maybe in what respect they differed the most) but do not care so much about individual DVs, then you should use MANOVA. It all depends on your research hypothesis.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Second,&lt;/strong&gt; it sounds though as if you might have no pre-specified hypothesis about which DVs should be influenced by group, and what exactly this influence should be. Instead, you probably have a bunch of data that you wish to explore. It is a valid wish, but it means that you are doing &lt;em&gt;exploratory&lt;/em&gt; analysis. And in this case my best advice would be: plot the data and look at it!&lt;/p&gt;&#10;&#10;&lt;p&gt;You can plot a number of things. I would plot distributions of each DV for each of the three groups (i.e. nine plots; can be density plots or box plots). I would also run linear discriminant analysis (which is intimately related to MANOVA, &lt;a href=&quot;http://stats.stackexchange.com/questions/82959/how-is-manova-related-to-lda&quot;&gt;see e.g. my answer here&lt;/a&gt;), project the data onto the first two discriminant axes and plot all your data as a scatter plot with different groups marked in different colours. You can project original DVs onto the same scatter plot, obtaining a &lt;em&gt;biplot&lt;/em&gt; (here is &lt;a href=&quot;http://stats.stackexchange.com/questions/7860/visualizing-a-million-pca-edition/7862#7862&quot;&gt;a nice example&lt;/a&gt; done with PCA, but one can make a similar one with LDA too).&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-08-07T16:05:44.093" Id="111071" LastActivityDate="2015-01-08T11:33:03.263" LastEditDate="2015-01-08T11:33:03.263" LastEditorUserId="28666" OwnerUserId="28666" ParentId="111044" PostTypeId="2" Score="2" />
  
  <row AnswerCount="0" Body="&lt;p&gt;When I wish to compare 2D image histograms I can use methods like Chi-Square and Intersection but what are my options if I wish to compare 3D histograms (e.g, based on R,G,B values)?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks. &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-08-07T19:58:41.257" Id="111101" LastActivityDate="2014-08-07T19:58:41.257" OwnerUserId="29060" PostTypeId="1" Score="1" Tags="&lt;histogram&gt;" Title="Methods for 3D histograms comparison" ViewCount="22" />
  
  
  <row AcceptedAnswerId="111114" AnswerCount="2" Body="&lt;p&gt;I'm attempting a multiple regression model where the predicted variable is runoff ratio - the ratio of watershed discharge to the precipitation input. This should generally be bounded [0,1], however, due to measurement error some values &gt; 1 occur.&lt;/p&gt;&#10;&#10;&lt;p&gt;Originally, I modeled this with the predicted variable un-transformed, but logistic regression has been suggested to me, I also have heard Beta regression suggested. I'm not sure how to proceed, and if these transformations are appropriate to my data:&#10;&lt;img src=&quot;http://i.stack.imgur.com/hCOuF.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;My questions are:&#10;1) Is a logistic regression appropriate for these data? and &#10;2) If I were to proceed with logistic regression, would I need to convert the runoff ratios to proportions, or would I apply the logit to the values as they are?&lt;/p&gt;&#10;&#10;&lt;p&gt;Sorry if these are obtuse questions - I'm new to logit and most of the information I have found is for binary response variables.  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edited for suggested additions:&lt;/strong&gt;&#10;As a simple version: I am modeling runoff ratio (rr) as an effect of precipitation (pcp) and antecedent water table position (ant):&lt;/p&gt;&#10;&#10;&lt;p&gt;rr ~ pcp + ant&lt;/p&gt;&#10;&#10;&lt;p&gt;rr is a continuous variable. I am not interested in the &lt;em&gt;probability&lt;/em&gt; of specific values, rather I'm interested in the values themselves - both to assess the significance of the predictors and as a predictive model. &lt;/p&gt;&#10;&#10;&lt;p&gt;Conceptually, I was fine modeling it un-transformed. However, a simple linear regression allows predicted values outside of the physical range of [0,1]. As mentioned above, measurement error does lead to values &gt;1, which I'll eventually have to deal with.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-08-07T20:27:07.980" Id="111108" LastActivityDate="2014-08-07T21:24:52.150" LastEditDate="2014-08-07T20:43:38.487" LastEditorUserId="45770" OwnerUserId="45770" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;logistic&gt;" Title="model for continuous dependent variable bounded between 0 and 1" ViewCount="384" />
  
  <row Body="&lt;p&gt;Actually, your code will not generate values from a t-distribution (not quite, at least).&lt;/p&gt;&#10;&#10;&lt;p&gt;This is because &lt;code&gt;t.test&lt;/code&gt; by default uses the Welch test.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are two things that prevent your results having a t-distribution:&lt;/p&gt;&#10;&#10;&lt;p&gt;i) the Welch-statistic only has approximately a t-distribution -- the denominator is only approximately chi-square with the nominal df of the Welch test. &lt;/p&gt;&#10;&#10;&lt;p&gt;ii) the df are computed as a function the &lt;em&gt;sample values&lt;/em&gt;, and thus vary each time you generate a new sample. Thus you'll actually get some mixture of these &lt;em&gt;not-quite-t-distributions-that-are-conditional-on-sample-specific-variance-ratios&lt;/em&gt;. &lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Here's how to actually get a t-distribution; if you want a two-sample t-test, you'll need to use an equal-variance t-test:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;n &amp;lt;- 10 # this will get you a t with 18 df&#10;res &amp;lt;- replicate(5000,{x=rnorm(n);y=rnorm(n);(mean(x)-mean(y))/sqrt((var(x)+var(y))/n)})&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This statistic is a simplified version (because of equal sample size) of an ordinary equal variance two-sample t-statistic.&lt;/p&gt;&#10;&#10;&lt;p&gt;Alternatively, if you want to use &lt;code&gt;t.test&lt;/code&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;n &amp;lt;- 10 # this will get you a t with 18 df&#10;res &amp;lt;- replicate(5000,t.test(rnorm(n),rnorm(n),var.equal=TRUE)$statistic)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;You can check it gives a t-distribution with the supposed d.f. by transforming the output using the cdf and comparing the result with a uniform:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;plot(ecdf(pt(res,2*n-2)))&#10;abline(0,1,col=2,lty=3)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;giving&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/29Epz.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;However, it's probably simpler to start with a &lt;em&gt;one-sample test&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Who exactly made the tables of critical values for t distributions found in statistical text books?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Each set of tables - unless it credits someone else as a source - will likely have been constructed from scratch, in a variety of ways depending on when they were published.&lt;/p&gt;&#10;&#10;&lt;p&gt;Critical values are effectively tables of the inverse cdf, but if you can compute the cdf you can find the value that corresponds to a certain point of the cdf relatively quickly (e.g. via &lt;a href=&quot;http://stats.stackexchange.com/questions/64538/how-do-i-find-values-not-given-in-interpolate-in-statistical-tables&quot;&gt;interpolation&lt;/a&gt; and say a Newton or secant-method step); there were also approaches like the use of finite difference methods that could also be used to achieve smooth interpolated results in use before people used computers for everything.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that for moderate to low d.f. it's quite reasonable to compute integrals directly (you can use integration by parts to write it in terms of an integral with d.f. that is two lower than the original, and the two lowest d.f. yield to standard integration methods. There's also numerical integration, which can be made quite accurate. There are series approximations, and there are highly accurate functional approximations (if I recall correctly, some are found in Abramowitz and Stegun for example). There are particular (highly accurate) functional approximations that are suitable for computer implementation, which is how tables are constructed now.&lt;/p&gt;&#10;&#10;&lt;p&gt;A typical set of t-tables might contain 100 to 200 values to about 3 or 4 figure accuracy; these could be computed &lt;em&gt;by hand&lt;/em&gt; in a fairly short period of time - probably a couple of days at most if you have several people checking the computations (people who did such computations - often on mechanical calculators - were called &lt;em&gt;computers&lt;/em&gt;). Perhaps 3 or 4 days with a single person doing the calculations and the checking.&lt;/p&gt;&#10;&#10;&lt;p&gt;The t-distribution itself dates back a long way; to Helmert in the mid 1870s (who was also responsible for the chi-square). Its use for the t-test is due to Gosset (&quot;Student&quot;), but I think the first tables of the inverse cdf for that purpose would be due to Fisher. For example, he gave &lt;a href=&quot;http://psychclassics.yorku.ca/Fisher/Methods/tabIV.gif&quot; rel=&quot;nofollow&quot;&gt;t-tables&lt;/a&gt; in his 1925 book &lt;em&gt;&lt;a href=&quot;http://psychclassics.yorku.ca/Fisher/Methods/index.htm&quot; rel=&quot;nofollow&quot;&gt;Statistical Methods for Research Workers&lt;/a&gt;&lt;/em&gt; (see the bottom of the page). &lt;/p&gt;&#10;&#10;&lt;p&gt;However, as mentioned by Fisher &lt;a href=&quot;http://digital.library.adelaide.edu.au/dspace/bitstream/2440/15174/1/20.pdf&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; (1922)$^{[1]}$, Gosset produced tables of the probability integral (i.e. the cdf rather than the inverse cdf) in 1917$^{[2]}$ (up to 30df); it would be practically possible to do the t-test from those. Indeed, Gosset's 1908 paper $^{[3]}$ has* a table for an integral related to the cdf of the $t$ for a set of small d.f. from which (with a little additional work), it's actually possible to in effect do t-tests (as he does in the paper, though keep in mind the hypothesis testing framework we're used to isn't entirely in place at that time).&lt;/p&gt;&#10;&#10;&lt;p&gt;*(see p19 &lt;a href=&quot;http://www.york.ac.uk/depts/maths/histstat/student.pdf&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;; a link to a scan of the original paper is on the Wikipedia page for the &lt;a href=&quot;http://en.wikipedia.org/wiki/Student%27s_t-distribution#Notes&quot; rel=&quot;nofollow&quot;&gt;Student's t distribution&lt;/a&gt;, see note 9) &lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;[1] Fisher, R.A. (1922),&lt;br&gt;&#10;&quot;The goodness of fit of regression formulae, and the distribution of regression coefficients&quot;.&lt;br&gt;&#10;&lt;em&gt;Journal of the Royal Statistical Society&lt;/em&gt; &lt;strong&gt;85&lt;/strong&gt; (4): 597–612. &lt;/p&gt;&#10;&#10;&lt;p&gt;[2] Student, (1917),&lt;br&gt;&#10;&quot;Tables for estimating the Probability that the Mean of a unique Sample of Observations lies between $-\infty$ and any given Distance of the Mean of the Population from which the Sample is drawn&quot;,&lt;br&gt;&#10;&lt;em&gt;Biometrika&lt;/em&gt; &lt;strong&gt;11&lt;/strong&gt; (4): 414-417 &lt;/p&gt;&#10;&#10;&lt;p&gt;[3] Student, (1908),&lt;br&gt;&#10;&quot;The probable error of a mean&quot;,&lt;br&gt;&#10;&lt;em&gt;Biometrika&lt;/em&gt; &lt;strong&gt;6&lt;/strong&gt; (1): 1–25. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-07T23:37:45.633" Id="111131" LastActivityDate="2014-08-08T04:09:57.550" LastEditDate="2014-08-08T04:09:57.550" LastEditorUserId="805" OwnerUserId="805" ParentId="111077" PostTypeId="2" Score="8" />
  
  
  <row Body="&lt;p&gt;Inverting the chi-squared test seemed to work. This boils to down to the following. If &lt;em&gt;m&lt;/em&gt; males are infected the null hypothesis should be rejected at the 5% level if the number females &lt;em&gt;f&lt;/em&gt; infected is not within the range given by the roots of this quadratic:&#10;$$(200+r) f^2 + (2 r m - 400 m- 200 r) f + [(200+r)m^2 - 200rm] = 0$$&#10;Here r = 3.7 (which seemed to give a slightly tighter result than 3.84 – the 95th percentile of the chi-squared distribution with 1 degree of freedom). I am happy enough with this answer.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-08T01:22:01.227" Id="111143" LastActivityDate="2014-08-08T01:22:01.227" OwnerUserId="53404" ParentId="110926" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Since you're interested in prediction, here are my thoughts.&lt;/p&gt;&#10;&#10;&lt;p&gt;Random Forests are excellent at regression tasks with abrupt discontinuities. &lt;/p&gt;&#10;&#10;&lt;p&gt;See the figure on page 55 (60 of the PDF) of &lt;a href=&quot;http://research.microsoft.com/pubs/155552/decisionForests_MSR_TR_2011_114.pdf&quot; rel=&quot;nofollow&quot;&gt;Decision Forests for Classification, Regression, Density Estimation, Manifold Learning and Semi-Supervised Learning.&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Stochastic Gradient Boosting is a similar model worth considering. It's more sensitive to the hyperparameters, but it tends to be a top performer in regression tasks. GBM in R and Scikit-Learn in Python provide implementations of Stochastic Gradient Boosting that can be trained on quantile loss functions, giving you confidence intervals.&lt;/p&gt;&#10;&#10;&lt;p&gt;Following the advice given in &lt;a href=&quot;http://www.ulb.ac.be/di/map/gbonte/ftp/time_ser.pdf&quot; rel=&quot;nofollow&quot;&gt;this lecture,&lt;/a&gt; if you're forecasting beyond the immediate horizon then it is advisable to use multi-output models. Random Forests can do this well.&lt;/p&gt;&#10;&#10;&lt;p&gt;The idea behind nonparametric time-series prediction is that you create a function that approximates $$f(x_{i-1}, x_{i-2}, .., x_{i-n}) = x_i$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The choice of $X$ is dependent on the data at hand. If your data is strictly positive, then consider a log-transform. If your data changes in scale as a function of time, then consider &lt;a href=&quot;http://en.wikipedia.org/wiki/Lag_operator&quot; rel=&quot;nofollow&quot;&gt;differencing.&lt;/a&gt; If there's an obvious cyclical trend to your data, then consider building a simple regression model over the period of the trend.&lt;/p&gt;&#10;&#10;&lt;p&gt;In any case, the final model for a predictive pipeline is feeding the residuals of these functions onto subsequent functions.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you have side-information on market events, then it is advisable to include this into your model as an explanatory variable. A simple trick is include the event as a dummy variable.&#10;$$&#10;   k_i = \left\{&#10;     \begin{array}{lr}&#10;       \exp(\alpha(v - i)) &amp;amp; : i \ge v\\&#10;       0 &amp;amp; : i &amp;lt; v&#10;     \end{array}&#10;   \right.&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Where $\alpha$ is a scaling factor and $v$ is the time index of the event that occurred. This looks like the following function.&lt;img src=&quot;http://i.stack.imgur.com/db5YU.png&quot; alt=&quot;Dummy variable.&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;As with all forms of predictive modelling, cross-validation is your friend. In the time-series case, your training, validation and testing set should be sequential.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-08-08T02:26:55.270" Id="111144" LastActivityDate="2014-08-08T21:57:55.743" LastEditDate="2014-08-08T21:57:55.743" LastEditorUserId="9568" OwnerUserId="9568" ParentId="111128" PostTypeId="2" Score="0" />
  
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;Does it make sense to use cross-validation with factor variables that have 3+ levels? When using bestglm, I get an error saying that it doesn't work with categorical variables. In the documentation the reasoning is &quot;Cross-validation is not available when there are categorical variables since in this case it is likely that the training sample may not contain all levels and in this case we can’t predict the response in the validation sample.&quot; but I can't find anyone else discussing this issue.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-08T17:17:31.540" Id="111218" LastActivityDate="2014-08-08T18:11:16.320" OwnerUserId="7340" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;categorical-data&gt;&lt;cross-validation&gt;&lt;predictive-models&gt;&lt;model-selection&gt;" Title="Cross-validation with dummy variables?" ViewCount="99" />
  
  <row AnswerCount="0" Body="&lt;p&gt;This time, I have a more theoretical than computational predicament. I have a path model that I am interested in testing on a data set with two groups. It is a very simple two predictor model outlined below, and I am using the ML-SEM function on Mplus v.7 to analyze the data.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; y = x1 + x2&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;My main research question is (1.) whether my model works better between groups or within groups. I am also interested in (2.) the proportion of the variance accounted for between group as compared to within groups. Finally, I would like to know (3.) if x1 is a better between-group predictor and x2 is a better within-group predictor.&lt;/p&gt;&#10;&#10;&lt;p&gt;The model converges just fine and fits the data very well, but I do not know what type of information in the output I would use to answer these three research questions. What data do I use to derive the amount of variance accounted for by the model on each level? How can I compare the predictors functioning within and between groups?&lt;/p&gt;&#10;&#10;&lt;p&gt;Any pointers and suggestions would be very much appreciated.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-08T21:43:28.853" Id="111247" LastActivityDate="2014-08-08T21:43:28.853" OwnerUserId="26945" PostTypeId="1" Score="0" Tags="&lt;variance&gt;&lt;multilevel-analysis&gt;&lt;sem&gt;&lt;mplus&gt;" Title="Answering Research Questions with Multi-level Structural Equation Modeling (ML-SEM)" ViewCount="63" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I would like to run a semi-symmetric bi-directional case-crossover method on some generated data using conditional logistic regression. &lt;/p&gt;&#10;&#10;&lt;p&gt;I generated data from Poisson distribution Poiss(lambda) with &lt;/p&gt;&#10;&#10;&lt;p&gt;$\textrm{log}(\lambda)=\lambda_0 \textrm{exp}(x\beta), $&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\lambda_0$ and $\beta$ are known and $x$ is a known time series. Then for each timepoint I split generated value into pairs: case in this timepoint and control in referent period (that is if in generated data there are $5$ cases at day $11$, I get $10$ observations for this timepoint: $5$ $(1$, $x$ at day $11)$ and $5 (0$, $x$ at reference period for day $11)$). Now I run conditional logistic regression on this data, with separate stratum for each pair. &lt;/p&gt;&#10;&#10;&lt;p&gt;The question is, should the parameter estimates that I get from CLR be $\beta$ from parametrer of Poisson distribution? I found few articles with this procedure applied, but I can't figure out why estimetes from CLR and $\beta$ from Poisson distribution should be the same. &lt;/p&gt;&#10;&#10;&lt;p&gt;Reference articles:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;T.Bateson, J.Shwartz, &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/10468428&quot; rel=&quot;nofollow&quot;&gt;Control for Seasonal Variation and Time Trend in Case-Crossover Studies of Acute Effects of Environmental Exposures&lt;/a&gt;, Epidemiology 1999, 10(4), 539-544;&lt;/li&gt;&#10;&lt;li&gt;S.Wang, B.Coukk, J.Shwartz, M.Mittleman, G.Wellenius, &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/21540322&quot; rel=&quot;nofollow&quot;&gt;Potential for Bas in Case-Crossover Studies With Shared Exposures Analyzed Using SAS&lt;/a&gt;, American Journal of Epidemiology 2011, 174(1), 118-124;&lt;/li&gt;&#10;&lt;li&gt;K.Fung, D.Krewski, Y.Chen, R.Burnett, S.Cakmak, &lt;a href=&quot;http://ije.oxfordjournals.org/content/32/6/1064.full&quot; rel=&quot;nofollow&quot;&gt;Comparison of time-series and case-crossover analyses of air pollution and hospital admission data&lt;/a&gt;, International Journal of Epidemiology 2003, 32, 1064-1070;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Thanks for help&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-09T08:57:42.980" Id="111265" LastActivityDate="2015-02-06T15:13:35.257" OwnerUserId="45130" PostTypeId="1" Score="1" Tags="&lt;logistic&gt;&lt;econometrics&gt;&lt;epidemiology&gt;&lt;case-control-study&gt;&lt;clogit&gt;" Title="Case-crossover method with conditional logistic regression for poisson model generated data" ViewCount="97" />
  <row AnswerCount="1" Body="&lt;p&gt;With OLS regression the errors must be normally distributed and be homoscedastic. Does these rules apply to polynomial regression as well?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-08-09T09:24:12.583" Id="111266" LastActivityDate="2014-08-09T11:45:57.647" OwnerUserId="44023" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;least-squares&gt;&lt;polynomial&gt;" Title="Polynomial regression rules" ViewCount="43" />
  
  
  <row Body="&lt;p&gt;The proper way to approach this would be a mixed model. That is, model mood as a function of group, with a random intercept for each subject. A simple example is shown &lt;a href=&quot;http://stats.stackexchange.com/questions/33995/shouldnt-the-beta-estimates-always-be-the-same-using-lm-and-lmer&quot;&gt;here&lt;/a&gt;, and this same link also explains when a simple anova (i.e. just using the subject means as you mentioned in your original post) will give equivalent parameter estimates and inferences.&lt;/p&gt;&#10;&#10;&lt;p&gt;Specifically, the results will be the same if your study is balanced (i.e. same number of observations per subject). Since missing a few data points here or there is inevitable, you should gain a little power by using the mixed model approach. The more unbalanced your data, the more power you should gain.&lt;/p&gt;&#10;&#10;&lt;p&gt;Spend some time learning about mixed models; you'll find they are very very useful. &lt;a href=&quot;http://www.r-bloggers.com/random-regression-coefficients-using-lme4/&quot; rel=&quot;nofollow&quot;&gt;Here&lt;/a&gt; is a good link. Also, anything John Fox writes about mixed models (or nearly anything, really) I've found to be superb.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-08-09T17:30:51.510" Id="111293" LastActivityDate="2014-08-11T15:15:59.723" LastEditDate="2014-08-11T15:15:59.723" LastEditorUserId="28520" OwnerUserId="28520" ParentId="111189" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="111525" AnswerCount="1" Body="&lt;p&gt;I am working on a project that is looking at the discriminative properties of 6, 5-point Likert items in a scale. I am using the ltm package in R to examine this within a item response theory (irt) paradigm, using a graded response model (grm).&lt;/p&gt;&#10;&#10;&lt;p&gt;For each item, I am given a &quot;discrimination value&quot; that corresponds with the slope of the ICC at each level given the ability level. For my six items, the values are (approximately): 2.1, 1.8, 1.3, 0.8, 2.7, 2.3.&lt;/p&gt;&#10;&#10;&lt;p&gt;How would I interpret these? Is there a cut-off for &quot;good&quot; discriminative properties? Are there values related to &quot;weak&quot;, &quot;medium&quot;, or &quot;strong&quot; levels of discrimination?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for your help!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-09T18:19:21.507" FavoriteCount="0" Id="111301" LastActivityDate="2014-08-11T23:53:25.933" OwnerUserId="26945" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;irt&gt;&lt;discriminant&gt;" Title="Interpreting discrimination values in IRT polytomous (grm) models. Are there any cut-offs or anchors?" ViewCount="77" />
  
  
  
  <row Body="&lt;p&gt;The OP asked for &quot;intuitive&quot; explanations of the assumptions, but what's &quot;intuitive&quot; varies greatly from person to person. So I will provide &lt;strong&gt;a view of these assumptions as a &quot;reasonable framework&quot; in order to &lt;em&gt;examine&lt;/em&gt; consistency of the extremum estimator&lt;/strong&gt;, and not as a set of assumptions that are designed to be sufficient for consistency. In my eyes at least, this is an interesting way to gain &quot;intuition&quot; on them.&lt;/p&gt;&#10;&#10;&lt;p&gt;The extremum estimator $\hat \theta_n$ is defined as (I am indexing the estimator also with $n$ for a reason),&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\hat \theta_n : Q_n(\hat \theta_n) = \max_{\theta \in \Theta}Q_n(\theta) \tag{1}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;And how do we know that $\hat \theta_n$ exists? How do we know that $Q_n(\cdot) $ has a maximum in $\Theta$?&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;We &lt;em&gt;don't&lt;/em&gt;: In order to guarantee it we have to assume that a) $Q_n(\cdot) $ is continuous in $\theta$ and &lt;strong&gt;b) that $\Theta$ is compact.&lt;/strong&gt; (since compactness of the set from which the argument of the function takes its values is &lt;em&gt;equivalent&lt;/em&gt; to the existence of a maximum of a continuous function in this set).&#10;This is the first and foremost reason why we assume that the parameter space is compact (together with continuity of $Q_n(\cdot)$: to &lt;strong&gt;guarantee the &lt;em&gt;existence&lt;/em&gt; of the extremum estimator&lt;/strong&gt; -but it will serve us in other aspects too.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Moreover, since &quot;the limit function $Q_0(\theta)$ is uniquely maximized at $\theta_0$&quot; appears as an assumption, then, together with compactness of $\Theta$ &lt;em&gt;imply&lt;/em&gt; the continuity of $Q_0(\cdot)$ (&lt;strong&gt;so assumption (iii) in the question is in reality redundant&lt;/strong&gt;, although it is usually stated as an additional assumption).  &lt;/p&gt;&#10;&#10;&lt;p&gt;So why do we make the assumption that &lt;strong&gt;&quot;the limit function $Q_0(\cdot)$  is uniquely maximized at $\theta_0$?&quot;&lt;/strong&gt; I would offer the following intuition: &lt;strong&gt;In order to &quot;target&quot; the unknown parameter of interest, it must possess some distinctive characteristic, on which to base the targeting&lt;/strong&gt;. So we &lt;em&gt;start&lt;/em&gt; by finding a limit function $Q_0(\cdot)$ such that $\theta_0$ has a special role in relation to it, compared to other values in the parameter space. One such special role is for $\theta_0$ to be the unique $\text{argmax}$ of $Q_0(\cdot)$. Then in a sense we apply the &quot;sample analogue&quot; principle, and obtain $Q_n(\cdot)$ as the sample analogue of $Q_0(\cdot)$, and $\hat \theta_n$ as the sample analogue of $\theta_0$. And &lt;em&gt;then&lt;/em&gt; we examine what additional conditions may be needed in order to guarantee the desired convergence of the sample analogue to the population counterpart.  &lt;/p&gt;&#10;&#10;&lt;p&gt;What is left is the assumption &lt;strong&gt;&quot;$Q_n(\cdot)$ converges uniformly in probability to $Q_0(\cdot)$&quot;&lt;/strong&gt;. To assume convergence per se appears naturally necessary -if $Q_n(\cdot)$ did not converge, or if it converged to some other function than $Q_0(\cdot)$, how could we relate $\hat \theta_n$ and $\theta_0$? But why do we need &lt;em&gt;uniform&lt;/em&gt; convergence, and not just the weaker &lt;em&gt;pointwise&lt;/em&gt; convergence?&lt;/p&gt;&#10;&#10;&lt;p&gt;Pointwise convergence (in probability) is defined as&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\lim_{n\rightarrow \infty}P\left(\Big|Q_n(\theta)-Q_0(\theta)\Big|&amp;lt; \epsilon\right) =1,\;\;\epsilon&amp;gt;0,\;\; \forall\, \theta \in \Theta \tag{1}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;while Uniform convergence as&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\lim_{n\rightarrow \infty}P\left(\sup_{\theta \in \Theta}\Big|Q_n(\theta)-Q_0(\theta)\Big|&amp;lt; \epsilon\right) =1,\;\;\epsilon&amp;gt;0   \tag{2}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;In pointwise convergence, we &lt;em&gt;fix&lt;/em&gt; $\theta$ and then consider what happens as $n\rightarrow \infty$. If the limiting probability result holds for all $\theta$ (but &lt;em&gt;each examined separately&lt;/em&gt;) then we have pointwise convergence.&lt;/p&gt;&#10;&#10;&lt;p&gt;In Uniform convergence, we form a sequence with the suprema of the function, and examine whether this sequence converges. And as $n$ changes, $\hat \theta_n$ in general changes, a fact that places what we are interested in studying &lt;em&gt;outside the scope of pointwise convergence&lt;/em&gt;. Under pointwise convergence, we could not even write the probability statement using $\hat \theta_n$ -since the argument of $Q_n(\cdot)$ should remain fixed. &lt;/p&gt;&#10;&#10;&lt;p&gt;So we could say that &lt;strong&gt;the assumption of uniform convergence&lt;/strong&gt; is not a matter of &quot;how strong we need the convergence to be&quot; - but, to begin with, &lt;strong&gt;a matter of assuming a form of convergence in the context of which we could indeed examine what we want to examine.&lt;/strong&gt;  &lt;/p&gt;&#10;&#10;&lt;p&gt;Up to now therefore, all assumptions made (and some regarding measurability with respect to the data that have been left unstated), appear mostly to be assumptions that permit us to &lt;em&gt;consider whether&lt;/em&gt; we have convergence of $\hat \theta_n$ to $\theta_0$, and perhaps to have a &quot;reasonable belief that we &lt;em&gt;may&lt;/em&gt; obtain convergence&quot;... They do &lt;em&gt;not&lt;/em&gt; appear to be assumptions that guarantee (are sufficient for) the desired convergence result...  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;The impressive thing about it, is that these assumptions are also sufficient for consistency of the extremum estimator $\hat \theta_n$.&lt;/strong&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-09T20:15:36.807" Id="111311" LastActivityDate="2014-08-20T13:31:04.263" LastEditDate="2014-08-20T13:31:04.263" LastEditorUserId="28746" OwnerUserId="28746" ParentId="111219" PostTypeId="2" Score="2" />
  
  <row Body="&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;Is it advisable to remove such skewed features when doing&#10;classification?&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;In general, no. In various specific cases - yes. (If you hand engineered features, or used something very specific for example). Sometimes a strong feature is just that. (While in other cases it can be overfitting). &#10;What's the phrase? It might be &quot;Accepted&quot; or &quot;published&quot; for example :). &lt;/li&gt;&#10;&lt;/ul&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;How do you such handle features which are intending to predict only&#10;one class?&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Be VERY careful with your &quot;negative set&quot;, assuming you're using discriminative classification. (And not 1 class or unsupervised methods). &#10;How large and &quot;random&quot;/diverse is your data? (Sources, fields, journals, types? )&#10;Are oyu looking at the final text in the journal or the author submission? (See my question on how whether the feature is something like &quot;accepted&quot;, &quot;published&quot;, etc' ).&lt;/li&gt;&#10;&lt;/ul&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Is there a method to check skewed presence for every feature and then&#10;decide whether to keep it in the model or not?&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Best way would be Chi square (plenty of inbuilt methods. eg, sci kit learn), or Mutual Information (entropy, etc'. Maybe mrmr?), or P. correlation.&lt;/li&gt;&#10;&lt;/ul&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2014-08-10T08:28:50.767" Id="111333" LastActivityDate="2014-08-10T08:28:50.767" OwnerUserId="38427" ParentId="76808" PostTypeId="2" Score="0" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm running an experiment where subjects are viewing a stimulus, and they have to decide if the stimulus is brighter/darker/identical in respect to a standard image.&#10;Stimulus luminosity actually varies, and it can be brighter, darker or identical to the standard. What i' triyng to estimate is the threshold (or the just noticeable difference) needed to correctly identify (let's say 75% of times) the stimulus as darker or brighter then the standard. I prefer to run the experiment with the method of constant stimuli, randomizing the standard luminosity.&lt;/p&gt;&#10;&#10;&lt;p&gt;From dummy data, I suppose that the plot of probability of response will follow the graph here showed. On Y-axis the probability; on X-axis the luminosity of the stimulus (-7 is darker, +7 is brighter, 0 is identical luminosity).&#10;The red curve identify the probability of answering &quot;darker&quot;; the green curve the probability of answering &quot;brighter&quot;, and the blue line the probability of answering &quot;identical luminosity&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/gYzvI.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;How can I consider an experiment like this? What is the correct model of analysis?&#10;From google-searches I've found that it could be considered a &quot;multinomial ordered logit model&quot;, but I'm not sure. And by the way, I was not able to find how to determinate the threshold or the JND in experiments like that.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any references are welcome.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-08-10T09:02:58.657" Id="111334" LastActivityDate="2014-08-13T06:45:07.197" LastEditDate="2014-08-13T06:45:07.197" LastEditorUserId="930" OwnerUserId="6153" PostTypeId="1" Score="2" Tags="&lt;probability&gt;&lt;experiment-design&gt;&lt;psychometrics&gt;&lt;threshold&gt;" Title="Threshold for psychometric experiment with three (or more) possible choices" ViewCount="42" />
  <row Body="&lt;p&gt;The output of every layer of a neural network can be considered as features for a model (which usually consists of the following layers). Typically, neural networks use neurons in the output layer, but this may well be anything, including SVM as suggested in the paper.&lt;/p&gt;&#10;&#10;&lt;p&gt;In this sense, the deep network is used to construct features which allow linear separability by the SVM. That said, I don't see the appeal of using a &lt;em&gt;linear&lt;/em&gt; SVM as a drop-in replacement for a typical output layer, as they are almost equivalent.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-08-10T13:21:34.673" Id="111356" LastActivityDate="2014-08-10T13:21:34.673" OwnerUserId="25433" ParentId="111347" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;Not quite.&lt;/p&gt;&#10;&#10;&lt;p&gt;Your $P_b(x)$ in your notation (putting aside questions on function form) is $P(SAT | Admitted)$&lt;br&gt;&#10;$P(SAT | Admitted)P(MySAT)$ doesn't give you what you want.&lt;br&gt;&#10;What you are looking for is $P(Admitted | SAT)P(MySAT)$&lt;/p&gt;&#10;&#10;&lt;p&gt;Getting $P(Admitted | SAT)$ is an application of Bayes:&#10;$$ P(B|A) = {P(A|B)P(B) \over P(A)} = {P(A|B)P(B) \over P(A|B)P(B) + P(A|!B)P(!B)} $$&#10;Translating:&#10;$$ P(Admitted | SAT) = {P(SAT | Admitted)P(Admitted) \over P(SAT)}$$&#10;$$ = {P(SAT | Admitted)P(Admitted) \over P(SAT | Admitted)P(Admitted) + P(SAT | !Admitted)P(!Admitted)}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;You have  &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;$P(SAT | Admitted)$ - provided in your question as 25/75 percentiles, you need to assume a functional form for this  &lt;/li&gt;&#10;&lt;li&gt;$P(Admitted)$ - google says this is 0.18&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;You do not have:  &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;$P(SAT)$  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Note that while you have assumed a $P(My SAT)$ you actually need the distribution of the population of all UCB applicants, not just yourself. Specifically you do not have $P(SAT| !Admitted)$. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you are able to obtain that, then you can calculate $P(Admitted | SAT)$ and from there $P(Admitted | SAT)P(MySAT)$&lt;/p&gt;&#10;&#10;&lt;p&gt;There may be some abuse of notation above.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;To answer your follow-up, yes, that is referencing the un-admitted population. You cannot assume the complement. A simplified version of the problem may help. Let's take a look at SAT $\ge$ 2230.&lt;/p&gt;&#10;&#10;&lt;p&gt;Since the 75 percentile of admitted students is 2230, that means 25% of admitted students have an SAT $\ge$ 2300 and thus&lt;br&gt;&#10;$P(SAT \ge 2300 | Admitted) = 0.25$&lt;br&gt;&#10;You can easily see the incongruity with taking the compliment and saying that 75% of non-admitted students have an SAT score greater than 2300&lt;br&gt;&#10;$P(SAT \ge 2300 | !Admitted) = 0.75$&lt;/p&gt;&#10;&#10;&lt;p&gt;And yes, by functional form, I mean your assumed normal. So to clarify (and clean up my notation from above).&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;$f_{SAT}(SAT=x | Admitted)$ is a assumed probability distribution function with 25/75 percentiles 1870/2230.&lt;br&gt;&#10;If you assume this is a &lt;a href=&quot;http://en.wikipedia.org/wiki/Normal_distribution&quot; rel=&quot;nofollow&quot;&gt;normal&lt;/a&gt; (ignoring that SATs are capped at 2400), you can set the cumulative distribution equal to 0.25 and 0.75 (or integrate the pdf)&lt;sup&gt;1&lt;/sup&gt; and solve for the mean and standard deviation to arrive at $\approx N(2050,267)$  &lt;/li&gt;&#10;&lt;li&gt;$P(Admitted) = 0.18$&lt;/li&gt;&#10;&lt;li&gt;You will need to acquire or assume $f_{SAT}(SAT=x)$ or $f_{SAT}(SAT=x | !Admitted)$. For a start, you can consider the overall SAT distribution of all US students (though this is unlikely to be the distribution of UCB applicants)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;This results in a function&#10;$$ P(Admitted | SAT=x) = {{f_{SAT}(SAT=x | Admitted)P(Admitted)} \over f_{SAT}(SAT=x) }$$&lt;/p&gt;&#10;&#10;&lt;p&gt;And then you can calculate&#10;$$ P(Admitted | SAT=My SAT)P(SAT=My SAT) $$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; Solve the system of equations&lt;br&gt;&#10;$F_{SAT}(SAT = 1870 | Admitted) = \int_{-\infty}^{1870}f_{SAT}(SAT=x | Admitted)dx = 0.25$&lt;br&gt;&#10;$F_{SAT}(SAT = 2230 | Admitted) = \int_{-\infty}^{2230}f_{SAT}(SAT=x | Admitted)dx = 0.75$&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-08-10T17:31:41.063" Id="111367" LastActivityDate="2014-08-10T22:33:05.203" LastEditDate="2014-08-10T22:33:05.203" LastEditorUserId="4485" OwnerUserId="4485" ParentId="111214" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;I would like to fit linear mixed effects model to my dataset, but I was wondering if quantity of observations in groups matter? I have some groups with about 60 observations in each, but there are also some with only 1, so Im curious if there will be any influence on my linear mixed effects model because of that.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-10T22:06:09.810" FavoriteCount="1" Id="111389" LastActivityDate="2014-11-25T01:25:49.777" OwnerUserId="46703" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;mixed-model&gt;&lt;lmer&gt;&lt;mixed-effect&gt;&lt;lme4&gt;" Title="Number of observations in groups - linear mixed effects model" ViewCount="139" />
  
  <row Body="&lt;p&gt;The &lt;code&gt;Keep it maximal&lt;/code&gt; proposal is not to be taken as a dogma. Be more pragmatic, and try to determine what level of model complexity your data will support (or at least a maximal level that will be supported).&lt;/p&gt;&#10;&#10;&lt;p&gt;Computationally speaking: The IWRLS estimation procedure used might not converge to the optimal parameter values;  as a result your inference will be wrong. In addition, a large number of parameters in a model may results in a very flat (conceptually speaking) log-likelihood surface and as a consequence the optimization problem you were previously solving &quot;easily&quot; just become extremely hairy.&lt;/p&gt;&#10;&#10;&lt;p&gt;The reasonable thing to do is to reduce the number of groups you are estimating. Right now you have an over-parametrized LME models; as you assume a model $y\sim N(X\beta,ZDZ^T+\sigma^2I)$ what is written essentially has more variance parameters than data-points.&lt;/p&gt;&#10;&#10;&lt;p&gt;Check for starters something like: &lt;code&gt;y ~ x1*x2*z1*z2+ (1+z1|ID) + (1+z2|ID))&lt;/code&gt; if you feel you should have correlated random slopes and intercepts in your model. This is already quite explicit for a covariance structure anyway. You can modify the model later if it does not fit your modeling assumptions.&lt;/p&gt;&#10;&#10;&lt;p&gt;And to get back to you original final question: No, there is no way to automatically specify the maximal random effects structure of your model; unfortunately there is no $silver$ $bullet$ for that statistical question.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-08-10T23:21:49.253" Id="111398" LastActivityDate="2014-08-10T23:21:49.253" OwnerUserId="11852" ParentId="108422" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;I &lt;em&gt;think&lt;/em&gt; you are asking for 2 different distributions that share some common 'essence', but that differ in how heavy their tails are, so that when you plot them the nature of 'heavy-tailedness' can be demonstrated. Is that correct?  If so, why not use the $t$ distribution with $1$ and $\infty$ degrees of freedom?  Here is a plot of several $t$ distributions from the &lt;a href=&quot;http://en.wikipedia.org/wiki/Student%27s_t-distribution&quot; rel=&quot;nofollow&quot;&gt;Wikipedia page&lt;/a&gt;:  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ACVgm.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-10T23:59:29.970" Id="111403" LastActivityDate="2014-08-10T23:59:29.970" OwnerUserId="7290" ParentId="111385" PostTypeId="2" Score="2" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm doing Cox proportional hazards analysis using Frank Harrell's rms package (4.2.0), with 3 strata.&lt;/p&gt;&#10;&#10;&lt;p&gt;My model seems reasonably predictive on training data (Dxy=-0.537), but somehow in bootstrap validation Dxy is exactly zero, which doesn't seem likely. I'm specifying &lt;code&gt;u=10&lt;/code&gt; in &lt;code&gt;validate.cph&lt;/code&gt; due to the use of strata, but other values of u don't make a difference.&lt;/p&gt;&#10;&#10;&lt;p&gt;Am I doing something wrong?&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# some data stats   &#10;&amp;gt; summary(d$time)&#10;   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.&#10;  0.019   9.839  14.780  12.430  14.880  19.940 &#10;&amp;gt; with(d, tapply(time, project, summary))&#10;$`1`&#10;   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.&#10;  0.019   9.725   9.818   8.561   9.875   9.941&#10;&#10;$`2`&#10;   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.&#10;  0.312  10.370  19.770  14.990  19.860  19.940&#10;&#10;$`3`&#10;   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.&#10;   0.191  14.740  14.820  13.380  14.880  14.940&#10;&amp;gt; s &amp;lt;- cph(Surv(time, event) ~ predictor + strat(project), data=d,&#10;     x=TRUE, y=TRUE, surv=TRUE)&#10;&amp;gt; s&#10;Cox Proportional Hazards Model&#10;&#10;cph(formula = Surv(time, event) ~ predictor + strat(project),&#10;    data = d, x = TRUE, y = TRUE, surv = TRUE)&#10;&#10;       Status&#10;Stratum     No Event Event&#10;  project=1      702   123&#10;  project=2      313    78&#10;  project=3     2126   190&#10;&#10;                 Model Tests       Discrimination&#10;                                      Indexes&#10;Obs       3532    LR chi2    382.77    R2       0.131 &#10;Events     391    d.f.            1    Dxy     -0.537&#10;Center 39.0395    Pr(&amp;gt; chi2) 0.0000    g        1.695&#10;              Score chi2 279.06    gr       5.445&#10;              Pr(&amp;gt; chi2) 0.0000&#10;&#10;          Coef   S.E.   Wald Z Pr(&amp;gt;|Z|)&#10;predictor 0.2341 0.0153 15.29  &amp;lt;0.0001&#10;&#10;&amp;gt; validate(s, B=20, u=10)&#10;&#10;      index.orig training   test optimism index.corrected  n&#10;Dxy       0.0000   0.0000 0.0000   0.0000          0.0000 20&#10;R2        0.1306   0.1300 0.1306  -0.0006          0.1312 20&#10;Slope     1.0000   1.0000 1.0056  -0.0056          1.0056 20&#10;D         0.0700   0.0698 0.0700  -0.0002          0.0702 20&#10;U        -0.0004  -0.0004 0.0002  -0.0005          0.0002 20&#10;Q         0.0704   0.0702 0.0698   0.0004          0.0700 20&#10;g         1.6948   1.6939 1.6948  -0.0009          1.6957 20&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2014-08-11T03:34:29.520" Id="111414" LastActivityDate="2015-03-07T14:20:57.973" LastEditDate="2014-08-15T03:51:45.603" LastEditorUserId="10119" OwnerUserId="10119" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;cox-model&gt;&lt;validation&gt;&lt;regression-strategies&gt;" Title="rms package, getting zero Dxy in validation of cph model" ViewCount="83" />
  <row AcceptedAnswerId="111560" AnswerCount="2" Body="&lt;p&gt;I'm struggling with projection points in linear discriminant analysis (LDA). Many books on multivariate statistical methods illustrate the idea of the LDA with the figure below.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/9k7iT.png&quot; alt=&quot;figure-1&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem description is as follows. First we need to draw decision boundary, add perpendicular line and than plot projections of data points on it. I wonder how to add projection points to the perpendicular line.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any suggestions / pointers? &lt;/p&gt;&#10;" CommentCount="16" CreationDate="2014-08-11T06:08:07.467" Id="111421" LastActivityDate="2014-10-07T13:37:10.443" LastEditDate="2014-08-12T12:05:33.410" LastEditorUserId="609" OwnerUserId="609" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;machine-learning&gt;&lt;classification&gt;&lt;discriminant-analysis&gt;" Title="Reproduce linear discriminant analysis projection plot" ViewCount="498" />
  
  <row Body="&lt;p&gt;Beyond subsampling and divide-and-conquer distributed computing, both important and useful, there are many other ways of solving such problems. To name just a couple, parallel coordinate descent (iterate on each variable independently, combine solutions later), and online methods, like stochastic gradient descent (SGD).&lt;/p&gt;&#10;&#10;&lt;p&gt;Have a look at &lt;a href=&quot;https://github.com/JohnLangford/vowpal_wabbit&quot;&gt;https://github.com/JohnLangford/vowpal_wabbit&lt;/a&gt; for quite a widely used approach to online learning with SGD. Also Alex Smola has done quite a bit of work on large-scale learning.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-08-11T09:46:50.567" Id="111437" LastActivityDate="2014-08-11T09:46:50.567" OwnerUserId="10119" ParentId="111422" PostTypeId="2" Score="6" />
  
  
  <row Body="&lt;p&gt;The data are not usually capable of telling you what the variables that &quot;really&quot; affect Y are. It would be a good approach to use data reduction techniques (variable clustering, etc.) to reduce the number of potential predictors by grouping them so that collinear predictors are not competing against one another. You can use a smaller number of cluster scores (e.g., first principal components) as predictors in a smaller model. Don't try to remove variables.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-11T11:53:10.887" Id="111446" LastActivityDate="2014-08-11T11:53:10.887" OwnerUserId="4253" ParentId="111424" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;First, the use of &lt;code&gt;step&lt;/code&gt; is irrelevant so I'll ignore that below.&lt;/p&gt;&#10;&#10;&lt;p&gt;Second, your dependent variable is integer valued and bounded above and below, so it's rather unlikely that your residuals would ever look Normal.  You might therefore try a model that does not assume that they are, such as&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;mod.bin &amp;lt;- glm(cbind(day_count_jun, 30-day_count_jun) ~ vars, &#10;               family=binomial, data=ml)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;or if you have mostly low counts, a Poisson assumption might be reasonable:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;mod.pois &amp;lt;- glm(day_count_jun ~ vars, &#10;                family=poisson, data=ml)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And if you have a lot of zeros that you think are driven by a separate process, then a zero-inflated or hurdle version of one of these two models might be better.  For what it's worth, that's what the histogram says to me.&lt;/p&gt;&#10;&#10;&lt;p&gt;One of those alternatives should get you a better model, or at least one whose forecasts will not be nonsensical.&lt;/p&gt;&#10;&#10;&lt;p&gt;Back to the residuals: even when you fit these models, the residuals will always look a bit bizarre relative to a model that assumes conditional Normality. The key is to look rather at the smoothing line than the dots.&lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, be a little careful as you add lagged variables in these models (as I see you have done from the graph).  At the very least you'd want the log of &lt;code&gt;day_count_april&lt;/code&gt; for an autoregressive model with a log link.  &lt;/p&gt;&#10;&#10;&lt;p&gt;There's good short discussion of all these things in Cameron's &lt;a href=&quot;http://www.econ.ucdavis.edu/faculty/cameron/research/CTE01preprint.pdf&quot; rel=&quot;nofollow&quot;&gt;note on count regression&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-08-11T13:19:10.903" Id="111450" LastActivityDate="2014-08-11T13:19:10.903" OwnerUserId="1739" ParentId="111443" PostTypeId="2" Score="3" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have two sets of data, say sales and profit, and I have calculated the correlation between these two data over different months using &lt;code&gt;R&lt;/code&gt;. So currently I have somethings like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Month       | Jan | Feb | Mar | Apr | ....&#10;sales       |  X  |  Y  |  Z  |  P  | ....&#10;profit      |  A  |  B  |  C  |  D  | ....&#10;correlation | .1  | .35 | .28 | -.47| ....&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This correlation calculation has been done over period of years and now I want to develop a prediction model which can provide me prediction for coming months of coming years,say what will be the profit for month of Aug 2014? How can I develop such a prediction model and what else, apart from what I have as of now, will I need to develop the prediction model? I am new to statistics and predictive analysis and thus if anyone could provide me a guide of required steps and some info about how can I do those steps then it would be very helpful for me to get the understanding also.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; If I could get help in respect to some non parametric bayesian method then it will be great. &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-08-11T13:39:05.003" Id="111455" LastActivityDate="2014-08-11T17:11:30.710" LastEditDate="2014-08-11T17:11:30.710" LastEditorUserId="919" OwnerUserId="52120" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;time-series&gt;&lt;correlation&gt;&lt;predictive-models&gt;&lt;prediction&gt;" Title="How to develop a prediction model based on correlation in R?" ViewCount="84" />
  
  
  
  <row Body="&lt;p&gt;If Δ1 and Δ2 can be viewed as two separate samples, you could use a two-sample Kolmogorov-Smirnov test to see if they have different distributions (&lt;code&gt;kstest2&lt;/code&gt; function). Or there are different functions to see if they have different means or medians or whatever.&lt;/p&gt;&#10;&#10;&lt;p&gt;But I am not sure if I understand correctly. This would not require that Δ1 and Δ2 have the same number of observations, but it would require that X1 and Y1 have the same number and can be subtracted to get Δ1.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-08-11T16:42:16.510" Id="111481" LastActivityDate="2014-08-11T16:42:16.510" OwnerUserId="22414" ParentId="111267" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have several files, each of which contains unique terms which are related to each other(without sentence structure). So for finding the word relationships I created a dictionary of bi-grams for every two words occurring in the document, and for every match found I increment the count for the bi-gram occurrence making it more important. For example - if I have football, goal, messi. I have 6 bi-grams football-goal, goal-football, messi-football, football-messi, goal-messi, messi-goal. Doing so in a large set of document I might get several occurrences of football-goal which might tell me that these words are related. And I can further develop the co-relation with 'goal' related words. I have two questions here: &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Are there better algorithms to achieve this?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;If I have to use this is algorithm. What are the possible way of visualizing the co-related terms?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2014-08-11T17:24:30.187" FavoriteCount="1" Id="111486" LastActivityDate="2014-08-11T17:27:19.140" LastEditDate="2014-08-11T17:27:19.140" LastEditorUserId="7290" OwnerUserId="53865" PostTypeId="1" Score="2" Tags="&lt;data-visualization&gt;&lt;data-mining&gt;&lt;algorithms&gt;&lt;text-mining&gt;" Title="Finding related words" ViewCount="28" />
  
  
  
  
  
  <row Body="&lt;p&gt;There are two ways to interpret your statement $\epsilon \sim \mathcal N(0, 1)$. If $\epsilon \sim \mathcal N(0, 1)$ is taken to mean that $\epsilon$ is &lt;em&gt;marginally&lt;/em&gt; $\mathcal N(0, 1)$ then your logic falls apart at concluding that $[\epsilon - y \mid Y = y] \sim \mathcal N(-y, 1)$. This is because there is no reason whatsover to conclude that $[\epsilon \mid Y = y] \sim \mathcal N(0, 1)$; the assumption is made about the marginal distribution of $\epsilon$, but if $\epsilon$ and $Y$ are dependent we cannot make such a strong conclusion about the  &lt;em&gt;conditional&lt;/em&gt; distribution of $[\epsilon \mid Y]$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, on the other hand, if one interprets $\epsilon \sim \mathcal N(0, 1)$ to mean that $[\epsilon \mid Y = y] \sim \mathcal N(0, 1)$ then your logic works. This is a standard assumption in parametric linear regression, after relabeling the variables slightly; one assumes $Y = X\beta + \epsilon$ and assumes that $[\epsilon \mid X] \sim \mathcal N(0, \sigma)$, from which is follows that $[Y \mid X] \sim \mathcal N(X\beta, \sigma)$. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-12T03:12:28.387" Id="111537" LastActivityDate="2014-08-12T03:12:28.387" OwnerUserId="5339" ParentId="111532" PostTypeId="2" Score="4" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have two groups of data, each contains five sets.&#10;Each of them has a standard score.&#10;I would like to compare them.&lt;/p&gt;&#10;&#10;&lt;p&gt;However I don't know if it is good to directly compare standard scores.&#10;As you know, the difference in standard score -2 and -1 is different from -1 and 0.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, should I use p value instead?&#10;Sorry for this silly question.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-08-12T07:10:55.307" Id="111545" LastActivityDate="2014-08-12T07:10:55.307" OwnerUserId="53900" PostTypeId="1" Score="0" Tags="&lt;p-value&gt;" Title="standard score or p value" ViewCount="46" />
  
  
  <row Body="&lt;p&gt;As you can see in the website, samples are splitted in two files: a training file and a test file. Accuracy is given on the test set, so that results are comparable.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, in practice one usually employs &lt;a href=&quot;http://en.wikipedia.org/wiki/Cross-validation_%28statistics%29&quot; rel=&quot;nofollow&quot;&gt;k-fold cross validation&lt;/a&gt; in order to average away those effects you mention. The average value over the folds is reported.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-08-12T12:13:51.487" Id="111563" LastActivityDate="2014-08-12T12:13:51.487" OwnerUserId="17908" ParentId="111554" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;You can use either a simple t-test as proposed by Glen_b, or a more general Wald test. &lt;/p&gt;&#10;&#10;&lt;p&gt;The Wald test allows to test multiple hypotheses on multiple parameters. It is formulated as: $R\beta=q$ where R selects (a combination of)  coefficients, and q indicates the value to be tested against, $\beta$ being the standard regresison coefficients. &lt;/p&gt;&#10;&#10;&lt;p&gt;In your example, where you have just one hypothesis on one parameter, R is a row vector, with a value of one for the parameter in question and zero elsewhere, and q is a scalar with the restriction to test. &lt;/p&gt;&#10;&#10;&lt;p&gt;In R, you can run a Wald test with the function &lt;em&gt;linearHypothesis()&lt;/em&gt; from package &lt;em&gt;car&lt;/em&gt;. Let us say you want to check if the second coefficient (indicated by argument &lt;strong&gt;hypothesis.matrix&lt;/strong&gt;) is different than 0.1 (argument &lt;strong&gt;rhs&lt;/strong&gt;):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;reg &amp;lt;- lm(freeny)&#10;coef(reg)&#10;&#10;# wald test for lag.quarterly.revenue =0.1&#10;&amp;gt;library(car)&#10;&amp;gt;linearHypothesis(reg, hypothesis.matrix = c(0, 1, rep(0,3)), rhs=0.1)&#10;#skip some result, look at last value on last row, of Pr(&amp;gt;F) &#10;  Res.Df       RSS Df  Sum of Sq      F Pr(&amp;gt;F)&#10;1     35 0.0073811                            &#10;2     34 0.0073750  1 6.0936e-06 0.0281 0.8679&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;For the t-test,  this function implements the t-test shown by Glen_b: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ttest &amp;lt;- function(reg, coefnum, val){&#10;  co &amp;lt;- coef(summary(reg))&#10;  tstat &amp;lt;- (co[coefnum,1]-val)/co[coefnum,2]&#10;  2 * pt(abs(tstat), reg$df.residual, lower.tail = FALSE)&#10;}&#10;&#10;&amp;gt; ttest(reg, 2,0.1)&#10;[1] 0.8678848&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Let us make sure we got the right procedure by comparing the Wald, our t-test, and R default t-test, for the standard hypothesis that the second coefficient is zero:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; linearHypothesis(reg, hypothesis.matrix = c(0, 1, rep(0,3)), rhs=0)[[&quot;Pr(&amp;gt;F)&quot;]][2]&#10;[1] 0.3904361&#10;&amp;gt; ttest(reg, 2,0)&#10;[1] 0.3904361&#10;## The 'right' answer from R:&#10;&amp;gt; coef(summary(reg))[2,4]&#10;[1] 0.3904361&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;You should get the same result with the three procedures. &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-08-12T12:46:07.067" Id="111566" LastActivityDate="2014-08-26T14:40:07.293" LastEditDate="2014-08-26T14:40:07.293" LastEditorUserId="35398" OwnerUserId="35398" ParentId="111559" PostTypeId="2" Score="3" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm trying to compare the predictive strenght of four different algorithms:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;support vector machines&lt;/li&gt;&#10;&lt;li&gt;k-NN&lt;/li&gt;&#10;&lt;li&gt;decision trees &lt;/li&gt;&#10;&lt;li&gt;neural networks&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I've got a few questions concerning the parameter tuning:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Some papers like [1]state that k-nearest neighbour does not need parameter tuning, don't you need to decide what k will be?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;If you want to tune the parameters of SVM (gridsearch) and NN (different hidden neurons), on what data set do you do that if you plan on using tenfold cross-validation? If you're using your data set with tenfold, isn't there any data left to use to validate your parameters?   &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;[1] Xiao, W., Qian, Z., &amp;amp; Fei, Q. (2006). A comparative study of data mining methods in consumer loans credit scoring management. Journal of Systems Science and Systems Engineering, 419-435.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-12T14:50:28.890" Id="111580" LastActivityDate="2014-08-12T15:12:26.913" OwnerUserId="53844" PostTypeId="1" Score="0" Tags="&lt;parameterization&gt;" Title="Tuning paramaters SVM, DT, k-NN, NN" ViewCount="27" />
  <row AnswerCount="0" Body="&lt;p&gt;I am looking for help in the following situation: &lt;/p&gt;&#10;&#10;&lt;p&gt;I have a set of numbers $A=\{1, 2, \dots, 100\}$, and I am drawing subsets of 10 of these numbers $\{a_1, a_2, \dots, a_{10}\}$ according to an underlying distribution that I do not know. After doing this procedure $N$ times (which can be very large), I would like to know if certain subsets of $A$ (of 9 or fewer numbers) are drawn as part of a single larger subset of 10 more often than would be expected by chance. &lt;/p&gt;&#10;&#10;&lt;p&gt;Essentially, I am trying to find subsets of $A$ the co-occur together more often than would be expected by chance when A is sampled in sets of 10.&lt;/p&gt;&#10;&#10;&lt;p&gt;Are there any well established statistics for this sort of thing? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-12T14:59:35.257" Id="111581" LastActivityDate="2014-08-12T14:59:35.257" OwnerUserId="51872" PostTypeId="1" Score="2" Tags="&lt;correlation&gt;&lt;clustering&gt;&lt;multivariate-analysis&gt;&lt;combination&gt;" Title="Co-occurrence statistics for sets" ViewCount="31" />
  <row Body="&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;In kNN k is a parameter, so it can be selected via cross-validation, or it can just be chosen arbitrary, e.g. k=3 is a popular choice. If somebody says that kNN is parameterless, probably they mean 1NN, i.e. k=1.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Ideally you would have 3 datasets: one for choosing model structure and/or parameters (via cross-validation or hold out if you have enough data), one for training the models with the chosen structure/parameters, and  nominating the best model (also may be cross validation or hold out), and the third one for final testing (should be used once and once only). &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Practically, if you have a small dataset this is often not feasible. Then the main idea is to make sure that after you chose the parameters, you train a new model on other dataset with the chosen parameters, and only then report the performance.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-08-12T15:12:26.913" Id="111584" LastActivityDate="2014-08-12T15:12:26.913" OwnerUserId="39588" ParentId="111580" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;Highly discrete and skew variables can exhibit some particular issues in their t-statistics:&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, consider something like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/M0kp1.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;(it has a bit more of a tail out to the right, that's been cut off, going out to 90-something)&lt;/p&gt;&#10;&#10;&lt;p&gt;The distribution of two-sample t-statistics for samples of size 50 look something like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/F5wPO.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In particular, there are somewhat short tails and a noticeable spike at 0.&lt;/p&gt;&#10;&#10;&lt;p&gt;Issues like these suggest that simulation from distributions that look something like your sample might be necessary to judge whether the sample size is 'large enough'&lt;/p&gt;&#10;&#10;&lt;p&gt;Your data seems to have somewhat more of a tail than in my above example, but your sample size is much larger (I was hoping for something like a frequency table). It may be okay, but you could either simulate form some models in the neighborhood of your sample distribution (or you could resample your data) to get some idea of whether those sample sizes would be sufficient to treat the distribution of your test statistics as approximately $t$.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Simulation study A&lt;/strong&gt; - t.test significance level (based on the supplied frequency tables)&lt;/p&gt;&#10;&#10;&lt;p&gt;Here I resampled your frequency tables to get a sense of the impact of distributions like you have on the inference from a t-test. I did two simulations, both using your sample sizes for the UsersX and UsersY groups, but in the first instance sampling from the X-data for both and in the second instance sampling from the Y-data for both (to get the H0 true situation)&lt;/p&gt;&#10;&#10;&lt;p&gt;The results were (not surprisingly given the similarity in shape) fairly similar:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/a4HyS.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The distribution of p-values should look like a uniform distribution. The reason why it doesn't is probably for the same reason we see a spike in the histogram of the t-statistic I drew earlier - while the general shape is okay, there's a distinct possibility of a mean difference of exactly zero. This spike inflates the type 1 error rate - lifting a 5% significance level to roughly 7.5 or 8 percent:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; sum(tpres1&amp;lt;.05)/length(tpres1)&#10;[1] 0.0769&#10;&#10;&amp;gt; sum(tpres2&amp;lt;.05)/length(tpres2)&#10;[1] 0.0801&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This is not necessarily a problem - if you know about it. You could, for example, (a) do the test &quot;as is&quot;, keeping in mind you will get a somewhat higher type I error rate; or (b) drop the nominal type I error rate by about half (or even a bit more, since it affects smaller significance levels relatively more than larger ones).&lt;/p&gt;&#10;&#10;&lt;p&gt;My suggestion - if you want to do a t-test - would instead be to use the t-statistic but to do a resampling-based test (do a permutation/randomization test or, if you prefer, do a bootstrap test).&lt;/p&gt;&#10;&#10;&lt;p&gt;--&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Simulation study B&lt;/strong&gt; - Mann-Whitney test significance level (based on the supplied frequency tables)&lt;/p&gt;&#10;&#10;&lt;p&gt;To my surprise, by contrast, the Mann-Whitney is quite level-robust at this sample size. This contradicts a couple of sets of published recommendations that I've seen (admittedly conducted at lower sample sizes).&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; sum(mwpres1&amp;lt;.05)/length(mwpres1)&#10;[1] 0.0509&#10;&#10;&amp;gt; sum(mwpres2&amp;lt;.05)/length(mwpres2)&#10;[1] 0.0482&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;(the histograms for this case appear uniform, so this should work similarly at other typical significance levels)&lt;/p&gt;&#10;&#10;&lt;p&gt;Significance levels of 4.8 and 5.1 percent (with standard error 0.22%) are excellent with distributions like these. &lt;/p&gt;&#10;&#10;&lt;p&gt;On this basis I'd say that - on significance level at least - the Mann Whitney is performing quite well. We'd have to do a power study to see the impact on power, but I don't expect it would do too badly compared to say the t-test (if we adjust things so they're at about the same actual significance level).&lt;/p&gt;&#10;&#10;&lt;p&gt;So I have to eat my previous words - my caution on the Mann-Whitney looks to be unnecessary at this sample size.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;My R code for reading in the frequency tables &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#metric1 sample1&#10;UsersX=data.frame(&#10;     count=c(182L, 119L, 41L, 11L, 7L, 5L, 5L, 3L, 1L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L),&#10;     value=c(0L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 12L, 17L, 18L, 20L, 29L, 35L, 42L)&#10;             )&#10;&#10;#metric 1 sample2&#10;UsersY=data.frame(&#10;    count=c(5098L, 2231L, 629L, 288L, 147L, 104L, 50L, 39L, 28L, 22L, 12L, 14L, 8L, 8L, &#10;     9L, 5L, 2L, 5L, 5L, 4L, 1L, 3L, 2L, 1L, 1L, 4L, 1L, 4L, 1L, 1L, 1L, 1L, 1L, 1L),&#10;    value=c(0L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, &#10;     17L, 18L, 19L, 20L, 21L, 22L, 25L, 26L, 27L, 28L, 31L, 33L, 37L, 40L, 44L, 50L, 76L)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;My R code for doing simulations&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;resample=function(tbl,n=sum(tbl$count))                                           #$&#10;                  sample(tbl$value,size=n,replace=TRUE,prob=tbl$count)            #$&#10;&#10;n1=sum(UsersX$count)                                                              #$&#10;n2=sum(UsersY$count)                                                              #$&#10;tpres1=replicate(10000,t.test(resample(UsersX),resample(UsersX,n2))$p.value)      #$&#10;tpres2=replicate(10000,t.test(resample(UsersY,n1),resample(UsersY))$p.value)      #$&#10;&#10;mwpres1=replicate(10000,wilcox.test(resample(UsersX),resample(UsersX,n2))$p.value)#$&#10;mwpres2=replicate(10000,wilcox.test(resample(UsersY,n1),resample(UsersY))$p.value)#$&#10;&#10;# &quot;#$&quot; at end of each line avoids minor issue with rendering R code containing &quot;$&quot;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="7" CreationDate="2014-08-12T16:19:09.917" Id="111597" LastActivityDate="2014-08-13T04:36:21.317" LastEditDate="2014-08-13T04:36:21.317" LastEditorUserId="805" OwnerUserId="805" ParentId="111320" PostTypeId="2" Score="10" />
  <row Body="&lt;p&gt;It seems that your task can be solved with standard clustering algorithms. Try k-means. You choose the number of clusters (k), choose k observations at random and say that they are cluster centers, compute the distance of each observation to each of the centers and assign each to the closest cluster, when done, compute the mean of each cluster, these would be the new centers. Now compute the distances of eachobservation to each center again, and reassing to the closest center. Repeat until no observation moves anymore. Distance function could be the Euclidean distance. &#10;It is advisable to scale the features before starting, since, if for instance, one feature is measured in 1-1000 and another in 0-1 scale, then the first one would dominate the distance computation, and the second would contribute nearly nothing.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-12T16:30:15.037" Id="111600" LastActivityDate="2014-08-12T16:30:15.037" OwnerUserId="39588" ParentId="111547" PostTypeId="2" Score="0" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm having a hard time proving that $R^2$ is equal to the square of the sample correlation between $Y$ and $\hat{Y}$. Every book I search tells me that's very easy, like verbeek. They just state that from the definition of $R^2$ and knowing that $SST=SSR+SSE$, it's very easy to prove the claim. However, I've spent a lot of time thinking about it, with no sucess.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any help would be appreciated.&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT (My try): $R^2=\frac{SSE}{SST}=1-\frac{SSR}{SST}$&lt;/p&gt;&#10;&#10;&lt;p&gt;Sample correlation$^2=\frac{\left(\sum(\hat{y_i}-\bar{y})(y_i-\bar{y})\right)^2}{ SST\cdot SSE }$&lt;/p&gt;&#10;&#10;&lt;p&gt;From then on, I have no idea...&lt;/p&gt;&#10;" ClosedDate="2014-08-13T01:43:09.923" CommentCount="12" CreationDate="2014-08-12T22:46:33.077" Id="111645" LastActivityDate="2015-01-13T21:51:00.167" LastEditDate="2015-01-13T21:51:00.167" LastEditorUserId="22228" OwnerUserId="40252" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;self-study&gt;&lt;multiple-regression&gt;&lt;r-squared&gt;" Title="$R^2$ equals square of correlation between observed and fitted responses" ViewCount="40" />
  <row AnswerCount="0" Body="&lt;p&gt;I have come to understand that a 3x5 MA (moving average) is equivalent to a 7 term MA. However, on applying the same to a time series such as below, I did not get the exact final time series values (one value, in this case) but approximately close. Made me wonder, if I did it correctly. The series I considered was:&lt;/p&gt;&#10;&#10;&lt;p&gt;0.067, 0.133, 0.2, 0.2, 0.2, 0.133, 0.067&lt;/p&gt;&#10;&#10;&lt;p&gt;Could someone highlight how exactly are the two calculated. My approach:&lt;/p&gt;&#10;&#10;&lt;p&gt;7-MA : add them up and divide by 7 = 0.1423&lt;/p&gt;&#10;&#10;&lt;p&gt;3x5 MA: &#10;a. Get the 5-MA as 0.16, 0.1732,0.16&#10;b. Get the 3-MA of above as: 0.1644   (!= 0.1423)&lt;/p&gt;&#10;&#10;&lt;p&gt;Am I doing it correctly?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-08-12T22:53:41.023" Id="111648" LastActivityDate="2014-08-12T22:53:41.023" OwnerUserId="53955" PostTypeId="1" Score="0" Tags="&lt;moving-average&gt;" Title="Moving average confusion" ViewCount="32" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;Do the &lt;code&gt;ranef&lt;/code&gt; and &lt;code&gt;fixef&lt;/code&gt; functions in &lt;code&gt;lmer&lt;/code&gt; give the random and fixed effect coefficients? If not what do they really give?&lt;/p&gt;&#10;&#10;&lt;p&gt;Data looks something like (this is a fake data):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;id      1  1  1  2  2  2 &#10;weight 34 45 56 78 12 45&#10;count  23 12 13 16 14 22&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Model looks like &lt;code&gt;mod &amp;lt;- lmer(weight ~ count + (1+count|id), data=d1)&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;where &lt;code&gt;id&lt;/code&gt; is a random effect in the model and &lt;code&gt;count&lt;/code&gt; is fixed effect. &lt;/p&gt;&#10;&#10;&lt;p&gt;I believed that the statement &lt;code&gt;coef(model)$id[,&quot;count&quot;]&lt;/code&gt; in R will give the random coefficients for &lt;code&gt;count&lt;/code&gt; by each &lt;code&gt;id&lt;/code&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;Am I correct? &lt;/p&gt;&#10;&#10;&lt;p&gt;If yes then what does &lt;code&gt;ranef(mod)&lt;/code&gt; give?&lt;/p&gt;&#10;&#10;&lt;p&gt;Any help is much appreciated. Sorry for the confusion.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-08-13T12:14:10.757" Id="111702" LastActivityDate="2014-08-17T10:20:30.910" LastEditDate="2014-08-17T10:20:30.910" LastEditorUserId="1739" OwnerUserId="46050" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;lmer&gt;" Title="I am confused with ranef function in R" ViewCount="434" />
  <row Body="&lt;p&gt;This answer will be written in a several regressors setting. You can directly apply it to the your simple case of only one regressor.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $N=I_n-\frac{1}{n}\mathbf{1}\mathbf{1}'$&lt;/p&gt;&#10;&#10;&lt;p&gt;$R^2=\frac{SSE}{SST}=\frac{(Xb)'NXb}{Y'NY}$&lt;/p&gt;&#10;&#10;&lt;p&gt;The square of the sample correlation is $\displaystyle r^2(\hat{Y},Y)=\frac{\left(\sum(\hat{y_i}-\bar{\hat{y}})(y_i-\bar{y})\right)^2}{ SST\cdot SSE}=\frac{(b'X'NY)'b'X'NY}{(Xb)'NXb \cdot Y'NY}$&lt;/p&gt;&#10;&#10;&lt;p&gt;Note: $N$ is symmetric and idempotent and $Ne=e$, and $X'e=0$.&lt;/p&gt;&#10;&#10;&lt;p&gt;$b'X'NY=b'X'NXb+b'X'Ne=b'X'NXb$. &lt;/p&gt;&#10;&#10;&lt;p&gt;So $r^2(\hat{Y},Y)=\frac{(b'X'NXb)'b'X'NXb}{(Xb)'NXb \cdot Y'NY}=\frac{(Xb)'NXb}{Y'NY}=R^2$&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-08-13T12:38:57.583" Id="111707" LastActivityDate="2015-01-14T15:14:15.157" LastEditDate="2015-01-14T15:14:15.157" LastEditorUserId="40252" OwnerUserId="40252" ParentId="70190" PostTypeId="2" Score="1" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I am attempting to fit a GLM to rate data. In my case it is the number of provisioning visits to a bird nest per hour. I model the data like in the example below including time as an offset.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;number of visits ~ x , offset=log(time), family=poisson(link = log)&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;However this is complicated by the fact that my nests also have different numbers of chicks and I want to include this in my model. This suggests to me that I need to adjust my offset to account for both time and number of chicks. Am I thinking about this the correct way or do I need a different approach?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-08-13T13:01:55.357" FavoriteCount="1" Id="111712" LastActivityDate="2014-08-13T14:25:52.210" LastEditDate="2014-08-13T14:22:22.057" LastEditorUserId="22311" OwnerUserId="35305" PostTypeId="1" Score="1" Tags="&lt;generalized-linear-model&gt;&lt;offset&gt;" Title="glm with offset for rate data" ViewCount="72" />
  <row Body="&lt;p&gt;I know nothing about ornithology, but I can give you some general advice. An offset is used to, effectively, include a feature with a coefficient fixed at precisely $1$. That is, the coefficient is assumed to be $1$ and is not estimated. I do not know if this is plausible in your field. &lt;/p&gt;&#10;&#10;&lt;p&gt;In a Poisson regression, the most common use of offsets is to adjust for periods of different lengths (recall that a Poisson model has several requirements, one of which is that all periods are of the same length). For example, you might look at the number of car accidents per month; because not all months have the same length, you could include an offset to account for the brevity of February compared to January. Naturally, we would expect that the additional number of days in January would slightly increase the average number of car accidents in that month, since people have more opportunities to get into wrecks. The offset corrects for this.&lt;/p&gt;&#10;&#10;&lt;p&gt;Incidentally, your code does not include an offset in any way; instead, it includes the effect of either &lt;code&gt;x&lt;/code&gt; or &lt;code&gt;log(time)&lt;/code&gt; as an additional feature. In R's GLM command, the offset is declared like this&#10;&lt;code&gt;glm(y~x,offset=chicks)&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Alternatively, you could include number of chicks as an additional feature and estimate a coefficient for it. This would express that there is some uncertainty in the effect of a chick on visits, and that you would like to estimate that effect from the data. I don't entirely understand why you believe that the number of chicks requires an offset, instead of being modeled as its own feature. Moreover, the number of chicks aren't in the same units as &lt;code&gt;log(time)&lt;/code&gt;, which makes me concerned that there's no sensible interpretation of the model.&lt;/p&gt;&#10;&#10;&lt;p&gt;Correcting for variable-length periods of time in your data collection makes perfect sense, though, and is standard practice.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-13T13:37:06.030" Id="111720" LastActivityDate="2014-08-13T14:25:52.210" LastEditDate="2014-08-13T14:25:52.210" LastEditorUserId="22311" OwnerUserId="22311" ParentId="111712" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I wonder if anyone can help me.&lt;/p&gt;&#10;&#10;&lt;p&gt;My data has 3 main variables: proportions at 2 time periods, and an additional predictor.&#10;For example:  &lt;/p&gt;&#10;&#10;&lt;pre&gt; Item Type    Y(t)      X        Y(t+1)  &#10;1             .05        .2          .7  &#10;1             .07        .14         .06  &#10;1              0         .11         0  &#10;1              0         .05         .01  &#10;1              0         .04         0  &#10;etc.  &#10;2             .2         .1          .1  &#10;2              0         .23          0  &#10;etc  &#10;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I am interested to know how the 4th column depends on the 2nd and 3rd columns. The issues are:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;columns 2 and 4 measure the same thing at two times  &lt;/li&gt;&#10;&lt;li&gt;all data are within [0,1] &lt;/li&gt;&#10;&lt;li&gt;for each item, the values in each column sum to 1  &lt;/li&gt;&#10;&lt;li&gt;99% of the data has a 0 in both columns 2 and 4&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I thought of first calculating the difference between columns 4 and 2, to represent the rise/fall of each proportion, and regressing that on the 3rd column, i.e. the predictor. This is conceptually what I'm after. But still, when the value in column 2 was zero -- which is 99% of the data -- then that difference cannot be negative, meaning that the d.v. will have many small positive values and a few large negative values. Is that OK for linear regression? &lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you for any advice!&#10;David&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-13T13:59:17.383" Id="111726" LastActivityDate="2014-08-13T14:57:19.920" LastEditDate="2014-08-13T14:57:19.920" LastEditorUserId="30997" OwnerUserId="30997" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;time-series&gt;&lt;proportion&gt;&lt;hierarchical&gt;" Title="proportions at two times with predictor" ViewCount="10" />
  
  <row Body="&lt;p&gt;It is not &lt;em&gt;exactly&lt;/em&gt; symmetrical. Look where the autocorrelation has a maximal absolute value:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lags(abs(c)==max(abs(c)))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This returns &lt;code&gt;-3&lt;/code&gt;, which corresponds to your delay of 0.003 s. The negative sign simply means that your second signal is delayed, not the first. If you swap them when calling &lt;code&gt;xcorr&lt;/code&gt;, like that:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;[c,lags] = xcorr(s2,s1);&#10;lags(abs(c)==max(abs(c)))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;then the answer will be &lt;code&gt;3&lt;/code&gt;. Now it's positive.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-13T16:43:49.450" Id="111740" LastActivityDate="2014-08-13T16:43:49.450" OwnerUserId="28666" ParentId="111739" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Weights per the theory of Generalized Least-Square should be constructed as relating to an empirically estimated or theoretical based model of the distribution of error terms. &lt;/p&gt;&#10;&#10;&lt;p&gt;Most often applied when the absolute magnitude of the error terms are assumed, for example, to be related to the size of the predictor (also called independent) variable. In such a case, transforming both Y and X by dividing by the square root of X will make the error term distribution 'homogeneous', which is an underlying assumption of standard least squares theory. Note, all predictor variables are divided by the square root of the size variable(Xi) so the intercept term, which has always a value of 1, is now not constant, but has a value for the ith term of 1/sqrt(Xi). As a result, the transformed model no longer has an intercept term.&lt;/p&gt;&#10;&#10;&lt;p&gt;Constrainted least-squares can be mechanically performed by solving the min of the sum of squares of the actual minus fitted subject to a linear constraint (technique of lagrange multipliers). The nature of statistical estimation of the standard deviation of such computed regression coefficients is complex, perhaps Baynesian regression theory or just run your own simulations.&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT: In practice I have used Box-Cox Analysis of Transformations as a guide to suggest the proper transformation to employ. The latter is best applied by a random sampling of the dataset, and the gathering of a range of suggested transformations. Linking the transformation to the suspected mechanism/error distribution linked to the nature of the data, for example, survival data, is also advised.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-08-13T16:57:45.393" Id="111744" LastActivityDate="2014-08-13T17:41:24.133" LastEditDate="2014-08-13T17:41:24.133" LastEditorUserId="54013" OwnerUserId="54013" ParentId="111738" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;In random forest algorithms, variable importance is not a measure of effect size. It is a measure of the contribution of a variable to out-of-bag predictive performance. In random forest, a variable can be important due to the way it interacts with other variables, and to the way it separates the data on its own. Nothing prevents a variable with a small linear effect size as estimated in a logistic regression model from having high importance in a random forest fit. Indeed, nothing prevents the inclusion or exclusion of a variable with a small effect size from having a large effect on predictive accuracy even within the framework of logistic regression, especially if there are strong confounding or mediation effects of that variable on other predictors, and including those confounding or mediation effects leads to better predictions.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-13T19:47:51.217" Id="111761" LastActivityDate="2014-08-14T06:58:20.587" LastEditDate="2014-08-14T06:58:20.587" LastEditorUserId="7616" OwnerUserId="7616" ParentId="111746" PostTypeId="2" Score="2" />
  
  
  <row AcceptedAnswerId="111878" AnswerCount="1" Body="&lt;p&gt;Why the expected value $\int h(x)f(x)$ is desired for inference in Dirichlet Process Mixture?&#10;What is the intuition for MCMC in Dirichlet Process Mixture?&lt;/p&gt;&#10;&#10;&lt;p&gt;$f(x)$ is the probability density function, and $h(x)$ is the outcome of the Dirichlet Process Mixture (I am not sure).&lt;/p&gt;&#10;&#10;&lt;p&gt;This formula is originally from page 40 of &lt;a href=&quot;http://people.cs.pitt.edu/~milos/courses/cs3750/lectures/class23.pdf&quot; rel=&quot;nofollow&quot;&gt;slides&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-08-13T23:09:39.057" Id="111779" LastActivityDate="2014-08-14T17:00:53.847" LastEditDate="2014-08-14T14:50:58.010" LastEditorUserId="53144" OwnerUserId="53144" PostTypeId="1" Score="1" Tags="&lt;gaussian-mixture&gt;&lt;dirichlet-distribution&gt;&lt;nonparametric-bayes&gt;&lt;dirichlet-process&gt;" Title="For inference of Dirichlet Process Mixture, why the expected value $\int h(x)f(x)$ is desired?" ViewCount="55" />
  <row Body="&lt;p&gt;Firstly, your supervisor should explain factor analysis to you. That's why he gets paid the big bucks.&lt;/p&gt;&#10;&#10;&lt;p&gt;But I guess it's up to old CV to plug the gaps of the educational system. &lt;/p&gt;&#10;&#10;&lt;p&gt;It would be nice if you could get AMOS with your SPSS system, or possibly use sem or lavaan in R, since I think your research question should probably be addressed through confirmatory factor analysis. What SPSS offers is just an exploratory analysis. So far, that seems to have worked well, since it looks as if the analysis produced the three categories that you believe are operative. Note that Varimax will always produce uncorrelated factors. That's what it does.&lt;/p&gt;&#10;&#10;&lt;p&gt;So what is factor analysis doing? You have a questionnaire with items, but what really interests you are certain underlying characteristics or &quot;categories&quot; that you can't measure directly. You measure these indirectly through the items of the questionnaire. You want the questionnaire to detect those categories. So perhaps questions 1-3 target the first category; 4-6 target the second.&lt;/p&gt;&#10;&#10;&lt;p&gt;If this model is correct, then the variance matrix of the 9 items will have a particular structure, reflective of the underlying categories. Confirmatory factor analysis lets you test that hypothesis.&lt;/p&gt;&#10;&#10;&lt;p&gt;Alternative hypotheses could be that all 9 items reflect only 1 category ... or at the other extreme, that there are no underlying categories that can simplify the variance structure. Confirmatory factor analysis would then check that these categories are relevant to the demographic you have.&lt;/p&gt;&#10;&#10;&lt;p&gt;Factor loadings are sort of the regression coefficients of the items against the underlying factors or categories, if in fact, you could measure those underlying factors. What you get from SPSS, I believe, assumes that the factors are scaled to have variance 1.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm not sure that high loadings from a category mean that the category is &quot;important&quot; to your demographic. It does suggest that the factor is present and well manifested by the questions. It also implies that people's responses are very much governed by the factor, and less by randomness. It might help if you specified what these categories are.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-08-14T03:09:37.323" Id="111797" LastActivityDate="2014-08-14T03:09:37.323" OwnerUserId="14188" ParentId="111789" PostTypeId="2" Score="3" />
  
  
  
  <row AcceptedAnswerId="111897" AnswerCount="1" Body="&lt;p&gt;I have the following type of dataset:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&#10;        |          |             |            variable_r         |&#10;subject |  gender  |  age_group  |     Cond_1    |     Cond_2    |&#10;--------|----------|-------------|---------------|---------------|&#10;   1    |    m     |      1      | r (A) | r (B) | r (A) | r (B) |&#10;   2    |    f     |      2      | r (A) | r (B) | r (A) | r (B) |&#10;...&#10;   8    |    f     |      2      | r (A) | r (B) | r (A) | r (B) |&#10;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So two genders, two age groups, two conditions (Cond_1 and Cond_2) under which the experiment was done and two ways the subjects were prompted (A and B). r is the numerical result from each experiment. So two within-subject variables (prompt A/B and Cond 1/2) and two between-subject variables (age group 1/2 and gender m/f) (right?). I should calculate the statistically significant effects of each variable and their interactions.&lt;/p&gt;&#10;&#10;&lt;p&gt;How can I do this in R (or Python)? My googling found a lot of information about different types of ANOVA analyses, but I wasn't able to apply that information to my case.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;&#10;&lt;p&gt;e: the subjects were tested 4 times&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-08-14T07:25:38.480" Id="111817" LastActivityDate="2014-08-15T17:39:27.557" LastEditDate="2014-08-14T09:29:26.220" LastEditorUserId="54055" OwnerUserId="54055" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;anova&gt;&lt;repeated-measures&gt;&lt;python&gt;" Title="How do I perform this complicated ANOVA type analysis in R?" ViewCount="78" />
  <row AnswerCount="0" Body="&lt;p&gt;I am a bit confused on how to proceed using the MFA analysis from FactoMineR in my data set. I am currently working with activity results of 15 bacteria (b1, b2, b3, b4, b5,.., b15), divided into 3 groups (A, B, C); all groups were subjected to 4 different types of environmental treatments (heat, cold, pressure, UV) . Within each type of treatment, there are several sub-treatments (level 1, 2, 3, 4,...)&lt;/p&gt;&#10;&#10;&lt;p&gt;I have been using MFA to find the sub-treatments that have the highest cos2 on the 2 dimensions; with this procedure I aimed to find the sub-treatments that are most linked to both dimensions and that are the actual sub-treatments that have an effect on the activity of all bacteria.&lt;/p&gt;&#10;&#10;&lt;p&gt;I also found the elements with the highest contributions. I used this procedure to find the sub-treatments that participate in both the first and second dimension and that are the extreme endpoints of the two dimensions.&lt;/p&gt;&#10;&#10;&lt;p&gt;The final objective of my analysis is to spot treatments that trigger similar activity in different bacteria and in particular groups of bacteria. For instance, if heat inhibits activity in A and B but not in C, or if UV affects b1, b5 (which belong to group A), b7 (from group B) and b13 (which belong to group C).&lt;/p&gt;&#10;&#10;&lt;p&gt;I also want to find out if particular sub-treatments are important for the activity of groups of bacteria. For instance, if level1 affects activity of C and if level1 affects b4 (from group A), b9 and b15 (from group C).&lt;/p&gt;&#10;&#10;&lt;p&gt;I am confused on which option is the best to use in the MFA to achieve the above or if it is even possible to find it out using this analysis. I will appreciate your kind advice on how to proceed. Thank you very much for your time!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-14T09:08:29.723" Id="111828" LastActivityDate="2014-08-14T09:08:29.723" OwnerUserId="54065" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;pca&gt;&lt;multivariate-analysis&gt;&lt;factor-analysis&gt;" Title="Multiple Factor analysis and squared cosines" ViewCount="26" />
  <row Body="&lt;p&gt;If you have a number of variables $\{x_1, x_2, \dots x_N\}$ and you consider a new variable that is a linear combination of the original ones: $$y=\alpha_1 x_1 + \alpha_2 x_2 + \dots + \alpha_N x_N = \sum \alpha_i x_i$$&#10;Then, obviously, all original variables are &quot;used to construct&quot; the new variable (unless some of the coefficients $\alpha_i$ are equal to zero). However, if some coefficients are $\approx 0$, then you can say that the new variable &quot;mostly&quot; depends only on a subset of the original variables --- on those that have large coefficients.&lt;/p&gt;&#10;&#10;&lt;p&gt;When you perform PCA, your e.g. first principal component is a linear combination of the original variables. So what you want to do, is to look at the coefficients $\alpha_i$ and see which ones have large absolute values (and which ones are close to zero).&lt;/p&gt;&#10;&#10;&lt;p&gt;How to actually do it, depends on your software, programming language, etc.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2014-08-14T09:41:37.823" Id="111831" LastActivityDate="2014-08-14T11:33:34.113" LastEditDate="2014-08-14T11:33:34.113" LastEditorUserId="28666" OwnerUserId="28666" ParentId="111824" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to get my head around the best way to analyse a study.&lt;/p&gt;&#10;&#10;&lt;p&gt;The study comprises of persons that get given a number of up to 10 different interventions, whereby hospital admission frequency is measured as an outcome.&lt;/p&gt;&#10;&#10;&lt;p&gt;Each intervention are not given out randomly, and some persons are not required to have all 10. Also, the interventions can be given out in different orders for each person, but a given intervention may only be used once for each person. &lt;strong&gt;As such, it is a given that a majority of these people will be living with, and experiencing the effects of multiple interventions at the same time&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The questions is then to measure the effect of each individual intervention by looking at the hospital outcome data?&lt;/p&gt;&#10;&#10;&lt;p&gt;I am finding this a tricky concept to get my head around as for example if I want to measure the effect of intervention 1, I might say what is the total amount of person time where people are living before intervention, and the total amount of person time people are living after the intervention - where I measure the health outcome of both periods. The problem is that some of these persons years will also have had persons years living before/after a different intervention.&lt;/p&gt;&#10;&#10;&lt;p&gt;Does anybody have any ideas on how I can use the data to measure the effect of each individual variable?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-14T11:15:41.977" Id="111839" LastActivityDate="2014-08-14T11:25:31.657" LastEditDate="2014-08-14T11:25:31.657" LastEditorUserId="26338" OwnerUserId="54071" PostTypeId="1" Score="0" Tags="&lt;statistical-significance&gt;&lt;summary-statistics&gt;&lt;intervention-analysis&gt;&lt;case-cohort&gt;" Title="How do I perform a statistical analysis on an intervention based study?" ViewCount="23" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm using a multinomial logistic regression analysis to examine deer behavioural responses to camera traps in terms of 7 predictors (both singly and their interactions). &lt;/p&gt;&#10;&#10;&lt;p&gt;I have found that the model with the lowest AIC value is season and vegetation (AIC = 1005.023). However, the AIC value for season as predictor variable on its own is 1005.103. From what I have read I gather that this means that these 2 models are nominally equivalent in indicating the processes influencing reactions as the AIC value difference is less than 2 or 3. I'm wondering if it is okay to report both AIC models and say that the model season and vegetation indicates that these variables when combined have a strong influence on the underlying processes which influence behavioural responses to camera traps even though the AIC values are nearly the same? The AIC value for vegetation as an isolated predictor variable is 1008. 289.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, when I run various models based on my hypothses, the best models seem to centre around the 1008 AIC value mark. As the difference in AIC value between the model season and vegetation and these models is approximately 3, does this means that the models with an AIC value of ~1008 are equally as good at indicating the underlying processes influencing behavioural responses as the model season and vegetation?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks a mil!&lt;/p&gt;&#10;" ClosedDate="2014-08-14T12:36:34.507" CommentCount="2" CreationDate="2014-08-14T11:44:56.513" Id="111843" LastActivityDate="2014-08-14T11:44:56.513" OwnerUserId="53182" PostTypeId="1" Score="1" Tags="&lt;model-selection&gt;&lt;multinomial&gt;&lt;aic&gt;" Title="Choosing models with similar AIC values" ViewCount="16" />
  
  
  
  <row Body="&lt;p&gt;Odds ratio for a success rate of $p$ is defined as $\frac{p}{1-p}$. A nice feature of this ratio is the plain English explanation you can do with it in a betting scenario. As an example, assume $p = 0.8$. Your odds ratio is $\frac{0.8}{1 - 0.8} = 4$. So you could say you are four times more likely to win than loose or vice versa. Lots of horse race betting (and their pay off) also works in the same way.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-08-14T14:27:08.543" Id="111859" LastActivityDate="2014-08-14T14:27:08.543" OwnerUserId="13516" ParentId="111751" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Without knowing the true goal of the study, it seems that what your supervisor suggested is closer to what's probably needed.  The way you phrased the question suggests an indirect analysis.  We are usually interested in knowing whether comorbidities predict the probability of a specific disease (here, a mental disorder that is strangely assumed to be all-or-nothing) after accounting for how other variables are associated with the mental disease.  One issue that frequently arises is that there are too many comorbidities to model all of them as separate variables, and you may want to include the more frequent ones or summarize them using a publish comorbidity index such as Elixhauser's.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-14T14:47:12.007" Id="111861" LastActivityDate="2014-08-14T14:47:12.007" OwnerUserId="4253" ParentId="111852" PostTypeId="2" Score="1" />
  
  
  
  
  <row Body="&lt;p&gt;Package mvtnorm in R produces random multivariate normals. You can specify the correlations.&lt;/p&gt;&#10;&#10;&lt;p&gt;If M is your matrix of random normals, do write.csv(M, file=&quot;mydata.csv&quot;) to write it out to a file.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-08-14T16:17:07.337" Id="111885" LastActivityDate="2014-08-14T16:17:07.337" OwnerUserId="14188" ParentId="111865" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;After having viewed the loadings in each factor, I use PCA as a tool to propose how to construct a new variable (that could be used elsewhere) that intuitively encompasses a shared attribute that have been outlined by a purely statistical technique. I do not recommend embracing a new factor without understanding why those variables could be possibly associated with the underlying factor. Even then, replication with new data, is needed to verify the new construct.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-08-14T16:46:23.743" Id="111893" LastActivityDate="2014-08-14T16:56:01.067" LastEditDate="2014-08-14T16:56:01.067" LastEditorUserId="54013" OwnerUserId="54013" ParentId="111824" PostTypeId="2" Score="-1" />
  <row Body="&lt;p&gt;Example from the lme4 package:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;fm1 &amp;lt;- lmer(Reaction ~ Days + (Days | Subject), sleepstudy)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;For this model &lt;code&gt;coef(fm1)&lt;/code&gt; gives &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;$Subject&#10;    (Intercept)       Days&#10;308    253.6637 19.6662581&#10;309    211.0065  1.8475834&#10;310    212.4449  5.0184067&#10;...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This is the 'summed up' version, assuming you want to know the subject-specific intercept and subject specific slope for &lt;code&gt;Day&lt;/code&gt;.  This is constructed by combining the fixed effects, which provide the mean, with the random effects, which are zero centred and provide the variation.  &lt;code&gt;fixef(fm1)&lt;/code&gt; gives&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;(Intercept)        Days &#10;  251.40510    10.46729&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and &lt;code&gt;ranef(fm1)&lt;/code&gt; gives &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;$Subject&#10;    (Intercept)        Days&#10;308   2.2585637   9.1989722&#10;309 -40.3985802  -8.6197026&#10;310 -38.9602496  -5.4488792 &#10;...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Looking at subject 308 we see that their personal intercept 253.6637 is equal to the grand mean 251.40510 plus 2.2585637 and their personal slope 19.6662581 is equal to the fixed effect slope 10.46729 plus their personal slope 9.1989722.&lt;/p&gt;&#10;&#10;&lt;p&gt;The advantage of &lt;code&gt;ranef&lt;/code&gt; in all this is that you can get the posterior uncertainty (or whatever it is that lme4 actually computes) over the random effects using &lt;code&gt;ranef(fm1, condVar = TRUE)&lt;/code&gt;.  What you got before were only point estimates of random variables.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-08-14T17:33:33.693" Id="111896" LastActivityDate="2014-08-14T17:33:33.693" OwnerUserId="1739" ParentId="111702" PostTypeId="2" Score="2" />
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;I would like to check the Time:Group interaction, to know which groups&#10;  are different AND where those differences are.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;If I understand your question correctly, you would like to tease apart the interaction effect between Group and Time. One possible approach is to perform various tests for all the combinations of the two factors (and maybe plot them out with the effect estimates and their standard errors). For example,&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(phia)    &#10;testInteractions(lme_data, custom=list(Group=c(1,0,0), Time=c(1,0,0)))&#10;testInteractions(lme_data, custom=list(Group=c(0,1,0), Time=c(1,0,0)))&#10;testInteractions(lme_data, custom=list(Group=c(0,0,1), Time=c(1,0,0)))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;show the effects for each group at 12h of Time and their significance. Similarly, with&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;testInteractions(lme_data, custom=list(Group=c(1,0,0), Time=c(1,0,0)))&#10;testInteractions(lme_data, custom=list(Group=c(1,0,0), Time=c(0,1,0)))&#10;testInteractions(lme_data, custom=list(Group=c(1,0,0), Time=c(0,0,1)))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;you obtain the effects at each time point for group G1. Furthermore, you can test all the pairwise comparisons,&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;testInteractions(lme_data, custom=list(Group=c(1,-1,0), Time=c(1,0,0)))&#10;testInteractions(lme_data, custom=list(Group=c(1,0,-1), Time=c(1,0,0)))&#10;testInteractions(lme_data, custom=list(Group=c(0,1,-1), Time=c(1,0,0)))&#10;...&#10;&#10;testInteractions(lme_data, custom=list(Group=c(1,0,0), Time=c(1,-1,0)))&#10;testInteractions(lme_data, custom=list(Group=c(1,0,0), Time=c(1,0,-1)))&#10;testInteractions(lme_data, custom=list(Group=c(1,0,0), Time=c(0,1,-1)))&#10;...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;With all these effects combined, you should be able to have a detailed picture about the interaction.&lt;/p&gt;&#10;&#10;&lt;p&gt;To visualize these effects, plot them out with:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(effects)&#10;plot(allEffects(lme_data))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(lsmeans)&#10;lsmip(lme_data, Group~Time)&#10;lsmip(lme_data, Time~Group)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2014-08-14T18:00:53.413" Id="111903" LastActivityDate="2014-08-14T18:33:06.077" LastEditDate="2014-08-14T18:33:06.077" LastEditorUserId="1513" OwnerUserId="1513" ParentId="111815" PostTypeId="2" Score="2" />
  
  <row AcceptedAnswerId="112232" AnswerCount="1" Body="&lt;p&gt;I am trying to analyze a dataset in which there are three measures on patients within areal units, however I am having trouble in how I am thinking about random/fixed effects and including covariates at different levels of the model.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Dataset&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Dependent variable:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Presence of disease (1=Yes, 0=No)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Independent variables:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Disease indicator (1=Disease 1, 2=Disease 2, 3=Disease 3)&lt;/li&gt;&#10;&lt;li&gt;Patient age&lt;/li&gt;&#10;&lt;li&gt;Patient race/ethnicity&lt;/li&gt;&#10;&lt;li&gt;Patient sex&lt;/li&gt;&#10;&lt;li&gt;Areal unit exposure&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Example of my data:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ArealID | PatientID | DV | DV_Indicator | Patient_Age | Patient_Race | Patient_Sex | ArealExposure&#10;      A |      0001 |  1 |            1 |          57 |        White |        Male | 5&#10;      A |      0001 |  0 |            2 |          57 |        White |        Male | 5&#10;      A |      0001 |  0 |            3 |          57 |        White |        Male | 5&#10;      A |      0002 |  1 |            1 |          43 |        White |      Female | 5&#10;      A |      0002 |  1 |            2 |          43 |        White |      Female | 5&#10;      A |      0002 |  0 |            3 |          43 |        White |      Female | 5&#10;      A |      0003 |  0 |            1 |          60 |        Black |        Male | 5&#10;      A |      0003 |  0 |            2 |          60 |        Black |        Male | 5&#10;      A |      0003 |  0 |            3 |          60 |        Black |        Male | 5&#10;    ... |       ... | ...|          ... |         ... |          ... |         ...&#10;      Z |      5678 |  1 |            1 |          77 |        Black |      Female | 12&#10;      Z |      5678 |  1 |            2 |          77 |        Black |      Female | 12&#10;      Z |      5678 |  1 |            3 |          77 |        Black |      Female | 12&#10;      Z |      5679 |  1 |            1 |          70 |        White |      Female | 12&#10;      Z |      5679 |  0 |            2 |          70 |        White |      Female | 12&#10;      Z |      5679 |  1 |            3 |          70 |        White |      Female | 12&#10;      Z |      5680 |  0 |            1 |          64 |     Hispanic |        Male | 12&#10;      Z |      5680 |  1 |            2 |          64 |     Hispanic |        Male | 12&#10;      Z |      5680 |  0 |            3 |          64 |     Hispanic |        Male | 12&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Note that the areal exposure is the same within each areal unit and that patient age, race, and sex are the same within each patient.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Model specification&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I am trying to specify my model using the &lt;code&gt;glmer&lt;/code&gt; function in the &lt;code&gt;lme4&lt;/code&gt; package in R.&lt;/p&gt;&#10;&#10;&lt;p&gt;Since I have multiple observations per patient and multiple patients in each areal unit, I am specifying my model as:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;m1 &amp;lt;- glmer(DV ~ DV_Indicator + Patient_Age + Patient_Race + Patient_Sex + ArealExposure + (1 | ArealID/PatientID), data=mydata, family=binomial)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;If I understand the syntax correctly, this model should have random intercepts for both patients and areal units, but I am not sure that I have included the variables correctly since they all seem to be included at the disease level (rather than patient or areal unit).&lt;/p&gt;&#10;&#10;&lt;p&gt;The way I thought to indicate which level each variable should be included at was like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;m2 &amp;lt;- glmer(DV ~ DV_Indicator + (1 + Patient_Age + Patient_Race + Patient_Sex | PatientID) + (1 + Areal_Exposure | ArealID), data=mydata, family=binomial)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;but I think this is actually including these terms as random slopes(?). I have had trouble finding clear examples of this since it seems many people mix using other packages with &lt;code&gt;lme4&lt;/code&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-14T18:27:11.687" Id="111907" LastActivityDate="2014-10-13T02:53:58.903" OwnerUserId="29109" PostTypeId="1" Score="1" Tags="&lt;repeated-measures&gt;&lt;multilevel-analysis&gt;&lt;lmer&gt;" Title="Specifying variable levels in multilevel repeated measures in R using lme4" ViewCount="73" />
  
  
  <row AcceptedAnswerId="111964" AnswerCount="1" Body="&lt;p&gt;In the first step of modeling a regression equation I came up with the following model:&lt;/p&gt;&#10;&#10;&lt;p&gt;$T_c = 26.73 + 0.042{\rm Sc} + 0.247{\rm Lc} - 14.709{\rm Lf} + 1.41{\rm Lu} - 0.214{\rm Fc} + 0.041{\rm Ad} - 64.308{\rm Sr} - 20.341{\rm Rc}$&lt;/p&gt;&#10;&#10;&lt;p&gt;Analyzing the residuals it is obviously noticed it does not follow a normal distribution; the relationship between the response variable and predictor is also found to be non-linear. After the attempt of some transformation in both predictors and response variable, and also stepwise procedures (to minimize the number of variables), I came up with the following model:&lt;/p&gt;&#10;&#10;&lt;p&gt;$T_c^{1.5}= -9.868 + 1.845\ln⁡({\rm Sc}) + 10.23\sqrt{{\rm Lu}} + 6.866 \times 10^{-5}&#10; {\rm Ad}^{2.5} + 7.772 \times 10^{-3} {\rm R_c}^{-4.47}$&lt;/p&gt;&#10;&#10;&lt;p&gt;I am using the method of least square to estimate coefficients and parametric statistic tests based on normality assumptions to test the performance of the model. &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;My question is how the residual must be calculated and analysed: I am using R language and using function &lt;code&gt;lm&lt;/code&gt;, it returns the residual as residual = $Tc^{1.5}$ predicted - $T_c^{1.5}$ observed. However, I feel the residual should be: residual = original $T_c$ - the result of the right side of the above equation to the power of 2/3. Could you shed a light on this please? Am I missing some simple theory on the model that I am not being able to visualize?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The same question is valid to r squared: which one should I choose to calculate r squared, the result of the right side of the above equation to the power of 2/3 (as the predicted variable) or $T_c$ predicted, that is, $T_c^{1.5}$?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2014-08-14T22:23:17.573" Id="111935" LastActivityDate="2014-08-15T03:45:58.513" LastEditDate="2014-08-15T03:03:36.493" LastEditorUserId="53387" OwnerUserId="53387" PostTypeId="1" Score="0" Tags="&lt;multiple-regression&gt;&lt;data-transformation&gt;&lt;r-squared&gt;&lt;residual-analysis&gt;" Title="Multlinear regression: analysis of residual of transformed response and predictor variables" ViewCount="71" />
  
  
  
  <row Body="&lt;p&gt;I think your model still falls within the category of Multiple Regression Analysis.  Such models can include autoregressive variables as you depict.  Translating your model in plain English you have: Energy consumption is a function of Energy consumption in the previous period and the change in temperature.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Depending on how your dependent variable is structured, your model could have a Unit-Root issue.  This means that your dependent variable is nonstationary, and is not mean-reverting.  In such a case, both the Variance and the Average of (a smaller section of the time series) of the dependent variable can drift in the same direction for too long.  If this is the case, your dependent variable is mispecified.  And, your model's results are spurious even if the overall R Square is very high and the variables are very statistically significant (this is a very common result with mispecified models).  &lt;/p&gt;&#10;&#10;&lt;p&gt;I think if your variable is Energy consumption for a specific geographical area, you may run into such a Unit-Root situation.  As, Energy consumption goes up forever.  If your variable is Energy consumption per capita, maybe it is fine as is.&lt;/p&gt;&#10;&#10;&lt;p&gt;Investigating whether your model has a Unit-Root issue or not, may dictate whether your results are statistically meaningful or not.  If it does have a Unit-Root, you need to transform the dependent variable.  If you transform it to represent the % change in Energy consumption per capita, you will most probably avoid any Unit-Root issue.       &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-08-15T00:43:24.347" Id="111954" LastActivityDate="2014-08-15T00:43:24.347" OwnerUserId="1329" ParentId="111951" PostTypeId="2" Score="1" />
  
  <row AnswerCount="1" Body="&lt;p&gt;This is a quite simple question but I don't find any good, clear, precise answers:&#10;I'm looking for a way to perform &lt;em&gt;post hoc&lt;/em&gt; test on a chi$^2$ test.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have 2 variables : var1 : good/fair/poor and var2: a/b/c&lt;/p&gt;&#10;&#10;&lt;p&gt;This is the contingency table :&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;      a   b  c&#10;good 120  70 13&#10;fair 230 130 26&#10;poor  84  83 18&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;with R :&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;mat &amp;lt;- matrix(c(120,230,84,70,130,83,13,26,18),3)&#10;dimnames(mat) &amp;lt;- list(c(&quot;good&quot;,&quot;fair&quot;,&quot;poor&quot;),c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;))&#10;mat&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I perform $\chi^2$&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;chisq.test(mat)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I obtain p =0.022, so there is a relation between var1 and var2... but which one? How can I handle this? (contribution analysis?, pairwise prop test?)&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-08-15T09:15:54.290" Id="111986" LastActivityDate="2014-08-15T17:53:49.023" LastEditDate="2014-08-15T14:02:33.223" LastEditorUserId="22311" OwnerUserId="54129" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;chi-squared&gt;&lt;post-hoc&gt;" Title="Post hoc $\chi^2$ test with R" ViewCount="273" />
  <row AnswerCount="1" Body="&lt;p&gt;I'm trying to understand the difference between &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;testing the null hypothesis (i.e. testing that the probability of a &quot;goal&quot; is the same across 2 different populations, similar to prop.test in R) &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;an A/B test using a bayesian formula such as described here: &lt;a href=&quot;http://www.evanmiller.org/bayesian-ab-testing.html&quot; rel=&quot;nofollow&quot;&gt;http://www.evanmiller.org/bayesian-ab-testing.html&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Is there a difference? Is one preferable?&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem I'm facing looks something like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;control group has 100,000 impressions and 100 reactions&#10;test group has 50,000 impressions and 55 reactions&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-15T15:57:51.657" Id="112024" LastActivityDate="2014-08-17T14:37:14.683" LastEditDate="2014-08-15T17:30:37.650" LastEditorUserId="35639" OwnerUserId="35639" PostTypeId="1" Score="2" Tags="&lt;hypothesis-testing&gt;&lt;bayesian&gt;&lt;proportion&gt;&lt;ab-test&gt;" Title="AB testing vs testing the null hypothesis" ViewCount="77" />
  <row Body="&lt;p&gt;This problem falls to the class called &lt;a href=&quot;https://en.wikipedia.org/wiki/Structured-output_learning&quot; rel=&quot;nofollow&quot;&gt;structured-output learning&lt;/a&gt;. First, you need to define a function $F(\mathbf{y}; \mathbf{x}, \mathbf{w})$ that scores predictions. It is supposed to return large values for the correct $\mathbf{y}$ and those similar to it. It is parametrized by the vector $\mathbf{w}$; usually $F$ depends linearly on $\mathbf{w}$. At test time, the decision is thus made by maximization:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\hat{\mathbf{y}} = \arg\!\max_{\bar{\mathbf{y}}} F(\bar{\mathbf{y}}; \mathbf{x}, \mathbf{w})$.&lt;/p&gt;&#10;&#10;&lt;p&gt;So $F$ should be chosen the way to perform this maximization efficiently. See below for an example.&lt;/p&gt;&#10;&#10;&lt;p&gt;At training time, you need to find the parameters $\mathbf{w}^*$ such that for each training example $\mathbf{x}_i$ the corresponding prediction $\hat{\mathbf{y}}_i$ should be close $\mathbf{y}_i$ in some sense. Similarly to regression, it is unlikely to match perfectly, so one needs to define a loss function. &lt;/p&gt;&#10;&#10;&lt;p&gt;One such method is provided by &lt;a href=&quot;https://en.wikipedia.org/wiki/Structured_SVM&quot; rel=&quot;nofollow&quot;&gt;structured SVM&lt;/a&gt;. The methods tries to find parameters $\mathbf{w}$ to maximize the margin &lt;/p&gt;&#10;&#10;&lt;p&gt;$F(\mathbf{y}_i; \mathbf{x}_i, \mathbf{w}) - \max_{\bar{\mathbf{y}}: \bar{\mathbf{y}} \ne \mathbf{y}_i} F(\bar{\mathbf{y}}; \mathbf{x}_i, \mathbf{w})$&lt;/p&gt;&#10;&#10;&lt;p&gt;for each training example $(\mathbf{x}_i, \mathbf{y}_i)$. To account for similarity in $\mathbf{y}$'s, it is usually augmented with the empirical loss function $\Delta(\bar{\mathbf{y}}; \mathbf{y})$ that measures some sort of distance; for discrete vectors it may measure &lt;a href=&quot;https://en.wikipedia.org/wiki/Hamming_distance&quot; rel=&quot;nofollow&quot;&gt;Hamming distance&lt;/a&gt;. Thus, for each training example we want to maximize the following:&lt;/p&gt;&#10;&#10;&lt;p&gt;$F(\mathbf{y}_i; \mathbf{x}_i, \mathbf{w}) - \max_{\bar{\mathbf{y}}} \{ F(\bar{\mathbf{y}}; \mathbf{x}_i, \mathbf{w}) + \Delta(\bar{\mathbf{y}}; \mathbf{y}_i) \}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The idea is to strive for the larger margin if $\bar{\mathbf{y}}$ is more different from $\mathbf{y}_i$. We can put it together and add $L_2$ regularization to get the SVM-style optimization problem:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\textrm{minimize}_{\mathbf{w}, \boldsymbol{\xi}} \quad \frac{\|\mathbf{w}\|^2}{2} + C \sum_{i=0}^N \xi_i,$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\textrm{s.t.} \quad F(\mathbf{y}_i; \mathbf{x}_i, \mathbf{w}) - \max_{\bar{\mathbf{y}}} \{ F(\bar{\mathbf{y}}; \mathbf{x}_i, \mathbf{w}) + \Delta(\bar{\mathbf{y}}; \mathbf{y}_i) \} \ge -\xi_i, \quad \forall i.$&lt;/p&gt;&#10;&#10;&lt;p&gt;It is not trivial to solve, but there are well-studied methods that perform it, including the cutting-plane algorithm and subgradient descent. For implementations, you can try &lt;a href=&quot;http://www.cs.cornell.edu/people/tj/svm_light/svm_struct.html&quot; rel=&quot;nofollow&quot;&gt;SVMstruct&lt;/a&gt; and &lt;a href=&quot;https://pystruct.github.io/&quot; rel=&quot;nofollow&quot;&gt;pystruct&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;What is left is the design of the scoring function $F$. It really depends on the problem you solve, but often some kind of graphical model is useful. Given that your data are sequential, something like &lt;a href=&quot;https://en.wikipedia.org/wiki/Hidden_Markov_model&quot; rel=&quot;nofollow&quot;&gt;Hidden Markov Model&lt;/a&gt; or &lt;a href=&quot;http://repository.upenn.edu/cgi/viewcontent.cgi?article=1162&amp;amp;context=cis_papers&quot; rel=&quot;nofollow&quot;&gt;some its generalization&lt;/a&gt; may be useful. For example, you may consider the following “factorization”:&lt;/p&gt;&#10;&#10;&lt;p&gt;$F(\mathbf{y}; \mathbf{x}, \mathbf{w}) = \psi(y_{0}; \mathbf{x}, \mathbf{w}) + \sum_{j=1}^q \Big( \phi(y_{j-1}, y_{j}; \mathbf{w}) + \psi(y_{j}; \mathbf{x}, \mathbf{w}) \Big).$&lt;/p&gt;&#10;&#10;&lt;p&gt;It allows performing maximization over $\mathbf{y}$ in $\mathcal{O}(q)$ time using dynamic programming. The potential functions $\psi$ and $\phi$ should depend linearly on (different parts of) $\mathbf{w}$, but specific form depends on your problem (in particular, if the output variables are discrete or continuous). You also need to figure out the components of $\mathbf{x}$ the potential function depends on. &lt;/p&gt;&#10;&#10;&lt;p&gt;This form of scoring functions accounts for the “correlations” of neighbouring $y$'s, but it is possible to add higher-order connections at some computational cost (i.e. potentials like $\phi_4(y_{j-4}, y_{j}; \mathbf{w})$). SVMstruct library contains an example of training HMM.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that in case of such sequential models training can be done by maximizing likelihood, which leads to a different objective function, but in general case it is infeasible. See &lt;a href=&quot;http://repository.upenn.edu/cgi/viewcontent.cgi?article=1162&amp;amp;context=cis_papers&quot; rel=&quot;nofollow&quot;&gt;this paper&lt;/a&gt; for more details on probabilistic training of sequential models.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-15T16:28:54.157" Id="112031" LastActivityDate="2014-08-15T16:28:54.157" OwnerUserId="3617" ParentId="111515" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;The answer to your question depends on the reason you are doing the research.&lt;br&gt;&#10;(1) If you want to make statements about the total (or mean) cost, then you must use the mean of all your data.  Only if you think the outliers will never reoccur would omitting the outliers make sense.&lt;br&gt;&#10;(2) If you want to describe the vast majority of your health care users, then a median or a trimmed mean would make sense. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-15T17:50:56.003" Id="112046" LastActivityDate="2014-08-15T17:50:56.003" OwnerUserId="8137" ParentId="111879" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;You don't really need to bother. In most packages (like glmnet) if you do not specify $\lambda$, the software package generates its own sequence (which is often recommended). The reason I stress this answer is that during the running of the LASSO  the solver generates a sequence of $\lambda$, so while it may counterintuitive providing a single $\lambda$ value may actually slow the solver down considerably (When you provide an exact parameter the solver resorts to solving a semi definite program which can be slow for reasonably 'simple' cases.)&lt;/p&gt;&#10;&#10;&lt;p&gt;As for the exact value of $\lambda$ you can potentially chose whatever you want from $[0,\infty[$. Note that if your $\lambda$ value is too large the penalty will be too large and hence none of the coefficients can be non-zero. If the penalty is too small you will overfit the model and this will not be the best cross validated solution  &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-08-15T19:24:33.687" Id="112066" LastActivityDate="2014-08-15T19:52:00.527" LastEditDate="2014-08-15T19:52:00.527" LastEditorUserId="49398" OwnerUserId="49398" ParentId="112056" PostTypeId="2" Score="5" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I use bootstrapping to generate the distribution / histogram of my sample statistic and find out that the value of my real sample statistic is way out in the tail. What does this mean?&lt;/p&gt;&#10;&#10;&lt;p&gt;Does it mean that my sample is unrepresentative of the population? But if it is indeed unrepresentative of the population, then wouldn't bootstrapping give the incorrect sampling distribution in the first place?&lt;/p&gt;&#10;&#10;&lt;p&gt;This is a &quot;what if&quot; question of mine, so if it is somehow impossible for my real sample statistic to be way out in the tail, I'd appreciate an explanation about why as well.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-08-15T22:18:37.527" Id="112080" LastActivityDate="2014-08-16T03:24:35.350" LastEditDate="2014-08-16T00:56:41.873" LastEditorUserId="20148" OwnerUserId="20148" PostTypeId="1" Score="2" Tags="&lt;sampling&gt;&lt;bootstrap&gt;&lt;representative&gt;" Title="How to interpret if my sample statistic is way out in the tail of the bootstrap distribution" ViewCount="37" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have data about popularity of some products in some stores. For each of the stores I have only a ordered rank of best-selling products in them. I need help how to find information about which products are popular only in certain stores versus products that are popular in all stores.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-16T14:10:16.760" Id="112117" LastActivityDate="2014-08-16T14:10:16.760" OwnerUserId="54190" PostTypeId="1" Score="2" Tags="&lt;nonparametric&gt;&lt;dataset&gt;&lt;ranking&gt;" Title="Stores and ordered rank of products" ViewCount="22" />
  <row AnswerCount="1" Body="&lt;p&gt;I have a input data and an output binary variable . The y value is 1 if the patient get ill.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; summary(x_or)&#10;   Symscore1        Symscore2        exercise3     exerciseduration3                       groupchange        age3      &#10; Min.   :0.0000   Min.   :0.0000   Min.   :1.000   Min.   :0.000     Regular to Regular          : 340   Min.   :45.00  &#10; 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000     Regular to Menopausal       : 360   1st Qu.:49.00  &#10; Median :0.0000   Median :0.0000   Median :4.000   Median :3.000     Transitional to Transitional: 171   Median :54.00  &#10; Mean   :0.5504   Mean   :0.5941   Mean   :3.651   Mean   :2.545     Transitional to Menopausal  :1492   Mean   :54.07  &#10; 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:5.000   3rd Qu.:3.000     Menopausal to Menopausal    : 246   3rd Qu.:59.00  &#10; Max.   :5.0000   Max.   :5.0000   Max.   :5.000   Max.   :4.000                                         Max.   :66.00  &#10;   packyears           bmi3            education3  &#10; Min.   : 0.000   Min.   :16.77   Basic     : 348  &#10; 1st Qu.: 0.000   1st Qu.:22.32   Highschool:1013  &#10; Median : 0.000   Median :24.84   University:1248  &#10; Mean   : 4.397   Mean   :25.60                    &#10; 3rd Qu.: 5.714   3rd Qu.:27.72                    &#10; Max.   :97.143   Max.   :57.09                    &#10;&amp;gt; summary(y_or)&#10;   0    1 &#10;2129  480 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I have fitted a random forest model and computed the outlier measure&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    rf = randomForest(x = x_or, y = y_or, proximity = T, ntree = 1000)&#10;out = outlier(x = rf$proximity, cls = y_or)&#10;&#10;plot(out, col=y_or)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;From the outlier plot I can see that some of the samples with y=1 have a very low outlier measure. &lt;/p&gt;&#10;&#10;&lt;p&gt;The general prediction performance of my model are very low. &#10;Can I deduce from this plot that there is a subgroup of patient that will very likely get ill?  &lt;/p&gt;&#10;&#10;&lt;p&gt;Basically I would like to say at the medical team that it is difficult to obtain a general model but that we are able to classify people with this characteristics...&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/oIg7P.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-16T15:18:47.053" Id="112123" LastActivityDate="2014-08-16T19:15:11.807" OwnerUserId="25392" PostTypeId="1" Score="0" Tags="&lt;classification&gt;&lt;random-forest&gt;&lt;outliers&gt;" Title="Use random forest outliers to detect group of variables" ViewCount="49" />
  
  
  <row AcceptedAnswerId="113484" AnswerCount="2" Body="&lt;p&gt;Suppose one performs the so-called non-parametric bootstrap by drawing $B$ samples of size $n$ each from the original $n$ observations with replacement. I believe this procedure is equivalent to estimating the cumulative distribution function by the empirical cdf:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Empirical_distribution_function&quot;&gt;http://en.wikipedia.org/wiki/Empirical_distribution_function&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;and then obtaining the bootstrap samples by simulating $n$ observations from the estimated cdf $B$ times in a row.&lt;/p&gt;&#10;&#10;&lt;p&gt;If I am right in this, then one has to address the issue of overfitting, because the empirical cdf has about N parameters. Of course, asymptotically it converges to the population cdf, but what about finite samples? E.g. if I were to tell you that I have 100 observations and I am going to estimate the cdf as $N(\mu, \sigma^2)$ with two parameters, you wouldn't be alarmed. However, if the number of parameters were to go up to 100, it wouldn't seem reasonable at all.&lt;/p&gt;&#10;&#10;&lt;p&gt;Likewise, when one employs a standard multiple linear regression, the distribution of the error term is estimated as $N(0, \sigma^2)$. If one decides to switch to bootstrapping the residuals, he has to realize that now there are about $n$ parameters used just to handle the error term distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;Could you please direct me to some sources that address this issue explicitly, or tell me why it's not an issue if you think I got it wrong.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-08-16T19:37:27.403" FavoriteCount="2" Id="112142" LastActivityDate="2014-08-28T08:45:56.043" LastEditDate="2014-08-27T14:15:31.013" LastEditorUserId="54099" OwnerUserId="54099" PostTypeId="1" Score="9" Tags="&lt;bootstrap&gt;&lt;sample-size&gt;&lt;sample&gt;&lt;small-sample&gt;&lt;finite-population&gt;" Title="Bootstrap: the issue of overfitting" ViewCount="273" />
  <row AnswerCount="0" Body="&lt;p&gt;How is the $A(\cdot) = \int\frac{du}{V^{1/3}(\mu)}$ normalizing transform for the exponential family derived?  &lt;/p&gt;&#10;&#10;&lt;p&gt;More specifically: I tried to follow the Taylor expansion sketch on page 3, slide 1 of &lt;a href=&quot;http://bit.ly/VtM1UR&quot; rel=&quot;nofollow&quot;&gt;http://bit.ly/VtM1UR&lt;/a&gt; but have several questions. With $X$ from an exponential family, transformation $h(X)$, and $\kappa _i$ denoting the $i^{th}$ cumulant, the slides argue that&#10;$$&#10;\kappa _3(h(\bar{X})) \approx h'(\mu)^3\frac{\kappa _3(\bar{X})}{N^2} + 3h'(\mu)^2h''(\mu)\frac{\sigma^4}{N} + O(N^{-3})&#10;$$&#10;and it remains to simply find $h(X)$ such that the above evaluates to 0. My first question is about arithmetic: my Taylor expansion has different coefficients, and I can't justify their having dropped many of the terms. Since $h(x) \approx h(\mu) + h'(\mu)(x - \mu) + \frac{h''(x)}{2}(x - \mu)^2$, we have&#10;$$&#10;\begin{align}&#10;h(\bar{X}) - h(u) &amp;amp;\approx h'(u))(\bar{X} - \mu) + \frac{h''(x)}{2}(\bar{X} - \mu)^2 \\&#10;E\left(h(\bar{X}) - h(u)\right)^3 &amp;amp;\approx h'(\mu)^3E(\bar{X}-\mu)^3 + \frac{3}{2}h'(\mu)^2h''(\mu)E(\bar{X} - \mu)^4 + \frac{3}{4}h'(\mu)h''(\mu)^2E(\bar{X}-\mu)^5 + \frac{1}{8}h''(\mu)^3E(\bar{X} - \mu)^6.&#10;\end{align}&#10;$$&#10;I can get to something similar by replacing the central moments by their cumulant equivalents, but it still doesn't add up. The second question: why does the analysis start with $\bar{X}$ instead of $X$, the quantity we actually care about?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-08-16T19:47:10.373" Id="112143" LastActivityDate="2014-08-17T14:47:26.830" LastEditDate="2014-08-17T14:47:26.830" LastEditorUserId="10464" OwnerUserId="10464" PostTypeId="1" Score="3" Tags="&lt;generalized-linear-model&gt;&lt;data-transformation&gt;&lt;residuals&gt;" Title="Derivation of normalizing transform for GLMs" ViewCount="25" />
  
  
  <row Body="&lt;p&gt;This is the first time I actually answer a question, so do not pin me down on it .. but I do think I can answer your question:&lt;/p&gt;&#10;&#10;&lt;p&gt;If you are indeed only interested in model performance and not in thing like interpretability random forest are indeed often a very good learning algorithm, but do perform slightly worse in the following cases:&lt;/p&gt;&#10;&#10;&lt;p&gt;1.) When the dimensionality (number of features) is very high with respect to the number of training samples, in those cases a regularized linear regression or SVM would be better.&lt;/p&gt;&#10;&#10;&lt;p&gt;2.) In the case there are higher order representations/convolutional structures in the data, like e.g. in computer vision problems. In those computer vision cases a convolutional neural network will outperform a random forest (In general if there is knowledge one can incorporate into the learning that is a better thing).&lt;/p&gt;&#10;&#10;&lt;p&gt;That being said random forest are a very good starting point. One of the person I admire for his Machine Learning skills always starts with learning a random forest and a regularized linear regressor.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, if you want the best possible performance I believe nowadays neural networks aka. Deep Learning is looking like a very attractive approach. More and more winners on data-challenge websites like Kaggle use Deep Learning models for the competition. Another pro of neural networks is that they can handle very large numbers of samples (&gt;10^6 one can train them using stochastic gradient descend, feeding bits of data at a time).&#10;Personally I find this a very attractive pro for Deep Learning.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-08-16T21:13:13.373" Id="112149" LastActivityDate="2014-08-16T21:13:13.373" OwnerUserId="54024" ParentId="112148" PostTypeId="2" Score="1" />
  
  
  
  <row AnswerCount="2" Body="&lt;p&gt;this is a conceptual question. In data mining, the problem often arises that scientists/data engineers are using an expert guess for the underlying distribution of their data. Often their assumption is based on previous results of academic research and does not really reflect the real distribution of the dataset. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, I would like to get the real distribution of my dataset? Any recommendations where to start? Does this even make sense to get the real distribution? Can thisv task be automated?&lt;/p&gt;&#10;&#10;&lt;p&gt;I appreciate your answer!&lt;/p&gt;&#10;&#10;&lt;p&gt;btw I am mostly using R as my data analysis tool&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-08-17T09:03:14.683" FavoriteCount="1" Id="112174" LastActivityDate="2014-08-17T10:18:14.713" OwnerUserId="37155" PostTypeId="1" Score="2" Tags="&lt;distributions&gt;&lt;data-mining&gt;&lt;dataset&gt;" Title="Get the distribution of a dataset" ViewCount="34" />
  
  
  <row AnswerCount="3" Body="&lt;p&gt;I need to analyze a data-set, with a very messy design, I am not sure how. I will try to make it simple. A new kind of stitches was invented, and is tested vs. 2 old kind of stitches. I will call this: Treatment, control 1 and control 2.&lt;/p&gt;&#10;&#10;&lt;p&gt;A few patients were selected, needing stitches. Each patient was assigned (by the need, not by random) to one of two stitching techniques. There are others, but only these two were tested. I call this variable &quot;procedure type&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Each patient, needed one stitch or more, up to 5 stitches. The problem is, that those patients in need for more than one stitch, received (sometimes) a mix of different stitches. Meaning, that one patient could have treatment and control 1, and other could have 2 treatment and 2 control 2, etc...&lt;/p&gt;&#10;&#10;&lt;p&gt;I hope I am not mixing terms here, but my experimental unit is the patient and the observations unit is the stitch within the patient. I have 40 treatment stitches, and 20 of each control, all coming from 25 patients.&lt;/p&gt;&#10;&#10;&lt;p&gt;The measure being tested, is how many minutes did it take for each stitch until the bleeding stopped. The variable can take values: 0,0.5,1,1.5,2,2.5,...&lt;/p&gt;&#10;&#10;&lt;p&gt;Clinicians claim that the procedure type, i.e. the technique being used is of no clinical importance and should not have any affect. Thus they think the analysis could ignore this factor.&lt;/p&gt;&#10;&#10;&lt;p&gt;I wanted to ask you, how would you analyze this data, with and without taking into account the technique. I think it should be some sort of a mixed model or a generalized estimating equations model, but I am not sure to define this design exactly in terms of  &quot;nesting&quot;, &quot;blocking&quot; and so on, and I would like to also fit a SAS code to model the differences between the means of the 3 different treatments, and I wouldn't mind verifying it with R.&lt;/p&gt;&#10;&#10;&lt;p&gt;For your convenience, I attach a diagram I made of the design: the blue circles are treatment stitches, the red ones are control 1 stitches, and the green are control 2 stitches. Each yellow rectangle is a patient, and the big brown rectangles are the techniques,  which again, I would like to try to fit and to try to ignore the see the outcome of that.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/l7O51.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-08-17T10:06:53.573" FavoriteCount="1" Id="112179" LastActivityDate="2014-11-07T16:44:45.860" LastEditDate="2014-08-17T10:09:46.113" LastEditorUserId="26338" OwnerUserId="54215" PostTypeId="1" Score="2" Tags="&lt;mixed-model&gt;&lt;experiment-design&gt;&lt;sas&gt;&lt;gee&gt;" Title="How to analyze this messy design?" ViewCount="77" />
  
  
  
  <row Body="&lt;p&gt;If you only have binary inputs this will only give you one point on the ROC curve. Usually you need something like a probability estimate. Many algorithms (random forests) that aren't meant to produce probability estimates have been modified to provide some estimate.    &lt;/p&gt;&#10;&#10;&lt;p&gt;Another alternative method is to use cost sensitive learning to produce k models which will give k points on the ROC curve. By changing your cost of FN/FP for each model you will produce different number of FP/FN.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-08-17T15:46:45.340" Id="112202" LastActivityDate="2014-08-17T15:46:45.340" OwnerUserId="34658" ParentId="112199" PostTypeId="2" Score="1" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am working with longitudinal data, where there are a different number of &quot;items&quot; per time point. I am working with the &lt;code&gt;MCMCdynamicIRT1d&lt;/code&gt; function in &lt;code&gt;MCMCpack&lt;/code&gt; in R to estimate a dynamic IRT model with binary items. However, now I am interested in modeling ordinal items within the same model.&lt;/p&gt;&#10;&#10;&lt;p&gt;I found an example of a dynamic IRT model with ordinal data in BUGS and the code is based on a CSV data file looks like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;CTRY            YEAR    Item1 Item2 Item3 Item4&#10;Afghanistan     1981        0     1     0     0&#10;Afghanistan     1982        2     0     1     2&#10;....&#10;Albania         1981        0     2     0     0&#10;Albania         1982        1     0     0     1&#10;....&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The issue is that my data have a different number of &quot;items&quot; per timepoint. I am working with US Supreme Court cases, and the Court sees a different number of cases per year. So my data might look something like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Justice1 Justice2   year  time&#10;       0        0   1960     1&#10;       1        0   1960     1&#10;       0        1   1960     1&#10;       0        0   1961     2&#10;       0        0   1961     2&#10;       0        0   1961     2&#10;       0        0   1961     2&#10;       0        0   1962     3&#10;       0        0   1962     3&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Any thoughts on how I can accommodate my data structure with a different number of &quot;items&quot; per time point?&lt;/p&gt;&#10;&#10;&lt;p&gt;I appreciate any advice you can offer!!!&lt;/p&gt;&#10;&#10;&lt;p&gt;===== BUGS code I am trying to work with ====&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;model{&#10;for(i in 1:n){# n is the number of obs&#10;for(item in 1:4){&#10;logit(Z[i, item, 1]) &amp;lt;- alpha[item, 1] - beta[item]*x[i]&#10;logit(Z[i, item, 2]) &amp;lt;- alpha[item, 2] - beta[item]*x[i]&#10;Pi[i, item, 1] &amp;lt;- Z[i, item, 1]&#10;Pi[i, item, 2] &amp;lt;- Z[i, item, 2] - Z[i, item, 1]&#10;Pi[i, item, 3] &amp;lt;- 1 - Z[i, item, 2]&#10;y[i, item] ~ dcat(Pi[i, item, 1:3])&#10;}&#10;&#10;x[i] &amp;lt;- mu[country[i], year[i]]&#10;&#10;}&#10;&#10;sigma ~ dunif(0,1)&#10;kappa &amp;lt;- pow(sigma, -1)&#10;&#10;for(c in 1:n.country){&#10;mu[c, 1] ~ dnorm(0, 1)&#10;&#10;for(t in 2:n.year){ #n.year is number of years&#10;mu[c, t] ~ dnorm(mu[c, t-1], kappa)&#10;&#10;}&#10;}&#10;&#10;for(j in 1:4){&#10;beta[j] ~ dgamma(4, 3)&#10;alpha0[j, 1] ~ dnorm(0, .25)&#10;alpha0[j, 2] ~ dnorm(0, .25)&#10;alpha[j, 1:2] &amp;lt;- sort(alpha0[j, 1:2])&#10;}&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2014-08-17T18:33:50.697" Id="112215" LastActivityDate="2014-08-17T18:53:37.780" LastEditDate="2014-08-17T18:53:37.780" LastEditorUserId="24808" OwnerUserId="54231" PostTypeId="1" Score="1" Tags="&lt;dataset&gt;&lt;ordinal&gt;&lt;winbugs&gt;&lt;irt&gt;&lt;open-bugs&gt;" Title="Longitudinal data format in WinBUGS or OpenBUGS, different number of items per time point" ViewCount="47" />
  
  <row Body="&lt;p&gt;I had the same situation and the following R code solved my problem. Although I was using linear regression (&lt;code&gt;lm&lt;/code&gt;), you can replace it with ARIMA if you want. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;dd          &amp;lt;- read.csv(&quot;SCALE8_data_ready_4_BN_exp1.csv&quot;)&#10;windowsSize &amp;lt;- 4000  # training data size&#10;testsize    &amp;lt;- 70    # number of observation to forecast&#10;# load variables from the data set &#10;delta1   &amp;lt;- dd$delta_t_discrete_3  #$&#10;OSVOS1   &amp;lt;- dd$OSV_OS_discrete     #$&#10;nbAlpha1 &amp;lt;- dd$nb_alpha_corrected  #$&#10;OSVEXT1  &amp;lt;- dd$OSVEXT              #$&#10;h1       &amp;lt;- dd$hidden_state        #$&#10;RMSE     &amp;lt;- matrix(0, 50, 1)&#10;for(k in 0:33)  # run 34 experiments&#10;{&#10;  A         &amp;lt;- k*testsize + 1&#10;  B         &amp;lt;- A + windowsSize - 1&#10;  start_obs &amp;lt;- A&#10;  end_obs   &amp;lt;- B&#10;&#10;  delta &amp;lt;- delta1[A:B]&#10;  NbAlpha &amp;lt;- nbAlpha1[A:B]&#10;  OSVOS   &amp;lt;- OSVOS1[A:B]&#10;  OSVEXT  &amp;lt;- OSVEXT1[A:B]&#10;&#10; # ddata  &amp;lt;- data.frame(delta=delta, NbAlpha=NbAlpha, OSVOS=OSVOS, OSVEXT=OSVEXT)&#10; # output &amp;lt;- paste(&quot;Gold_theta0.2per_alpha_0.1_CPTs_from&quot;, A, &quot;to&quot;, B, &#10; #                 &quot;RollingLMTraining.csv&quot;)&#10; # write.csv(ddata, file=output)&#10;&#10;  llmm &amp;lt;- lm(OSVEXT~delta + NbAlpha + OSVOS)  # initiate linear regression&#10;  intercept  &amp;lt;- coef(llmm)[1]&#10;  co_delta   &amp;lt;- coef(llmm)[2]&#10;  co_NbAlpha &amp;lt;- coef(llmm)[3]&#10;  co_OSVOS   &amp;lt;- coef(llmm)[4]&#10;&#10;&#10;  A           &amp;lt;- B + 1&#10;  B           &amp;lt;- B + testsize&#10;  delta       &amp;lt;- delta1[A:B]&#10;  NbAlpha     &amp;lt;- nbAlpha1[A:B]&#10;  OSVOS       &amp;lt;- OSVOS1[A:B]&#10;  OSVEXT      &amp;lt;- OSVEXT1[A:B]&#10;  predict_EXT &amp;lt;- matrix(0, testsize, 1)&#10;  SSE         &amp;lt;- 0&#10;  for(i in 1:testsize)  # do the forecast based on LM results&#10;  {&#10;    predict_EXT[i] &amp;lt;- intercept + delta[i]*co_delta + NbAlpha[i]*co_NbAlpha + &#10;                      OSVOS[i]*co_OSVOS&#10;    SSE            &amp;lt;- SSE + (predict_EXT[i] - OSVEXT[i])^2&#10;  }&#10;  RMSE[k+1] &amp;lt;- sqrt(SSE/testsize)&#10; # ddata    &amp;lt;- data.frame(Predicted_EXT=predict_EXT, Real_OSVEXT=OSVEXT)&#10; # output   &amp;lt;- paste(&quot;from&quot;, A, &quot;to&quot;, B, &quot;RollingLMTesting.csv&quot;)&#10; # write.csv(ddata, file=output)&#10; print(RMSE[k+1])&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-08-17T21:24:11.347" Id="112230" LastActivityDate="2014-08-18T02:26:15.143" LastEditDate="2014-08-18T02:26:15.143" LastEditorUserId="7290" OwnerUserId="54237" ParentId="20725" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;R functions typically return answers on the same scale as the inputs, so I would guess it's difference in logits. Moreover, functions like this typically don't/can't check where their inputs came from (i.e. whether it's a &lt;code&gt;glm&lt;/code&gt; or an &lt;code&gt;lm&lt;/code&gt; or something else) so they operate agnostically, which in this case would be to just take a difference in mean $Y$.&lt;/p&gt;&#10;&#10;&lt;p&gt;When in doubt in situations like this you can always check the source code. Type the name of the function without &lt;code&gt;()&lt;/code&gt; after it. Or in Rstudio, highlight the name of the function and ctrl+click.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-18T02:15:32.830" Id="112252" LastActivityDate="2014-08-18T02:15:32.830" OwnerUserId="36229" ParentId="112247" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="112257" AnswerCount="1" Body="&lt;p&gt;I am trying to fit a multi-level model to some longitudinal data that I have. As an example, let's pretend participants had to make 10 basketball free throws, and I measured how long it took them to make each one (in seconds). In this case, the time variable is severely positively skewed because although there is a minimum amount of time it takes to make 10 throws, some participants could take much longer than others.&lt;/p&gt;&#10;&#10;&lt;p&gt;When trying to fit the model with the original data, I ran into all sorts of convergence errors using lmer in R. After transforming the time variable (in seconds) by applying a fourth-root transformation, the model fits fine, and the coefficients are interpretable.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm just wondering if (a) this is a valid thing to do, and (b) what I should be aware of/careful about when doing this.&lt;/p&gt;&#10;&#10;&lt;p&gt;edit: If someone could direct me to further reading on why the non-transformed model might fail to converge, that would be great!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-18T02:30:21.540" Id="112256" LastActivityDate="2014-08-18T02:46:26.230" LastEditDate="2014-08-18T02:46:26.230" LastEditorUserId="48137" OwnerUserId="48137" PostTypeId="1" Score="0" Tags="&lt;panel-data&gt;&lt;multilevel-analysis&gt;" Title="How to transform time-varying covariate measure of response time in a multi-level model of longitudinal data?" ViewCount="47" />
  
  
  <row AcceptedAnswerId="112269" AnswerCount="2" Body="&lt;p&gt;I would like to know what are the  common techniques to compare two histograms?&#10;I have histogram of two images and I want to see are they similar or not meaning that is there any correlation between them or not. The histograms are for two different parts of tissue.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/octv6.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-08-18T06:28:47.080" FavoriteCount="1" Id="112267" LastActivityDate="2014-08-18T10:26:47.323" LastEditDate="2014-08-18T10:26:47.323" LastEditorUserId="88" OwnerUserId="54252" PostTypeId="1" Score="2" Tags="&lt;histogram&gt;&lt;image-processing&gt;" Title="Comparing two histograms" ViewCount="690" />
  <row Body="&lt;p&gt;In addition to what has been said, you might want to consider structural time series models. They account explicitly for one or more seasonalities and trend, and are very tolerant of missing data. A good starting point might be the R function StructTS(). More complex models can be fit with packages such as &lt;code&gt;dlm&lt;/code&gt;, &lt;code&gt;KFAS&lt;/code&gt;, and several others.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-18T08:33:00.613" Id="112274" LastActivityDate="2014-08-18T08:33:00.613" OwnerUserId="892" ParentId="97019" PostTypeId="2" Score="0" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have created a contingency table and an association plot in R and I am not quite sure how to interpret it. Here is my contingency table:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;             No Dental Dental Sum&#10;  No Medical        39     13  52&#10;  Medical           18    146 164&#10;  Sum               57    159 216&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and here is my observed association plot:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/3KerN.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I also have a chi square test statistic:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    Pearson's Chi-squared test with Yates' continuity correction&#10;&#10;data:  t&#10;X-squared = 80.051, df = 1, p-value &amp;lt; 2.2e-16&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I know this test is for determining if my categorical data is independent. &#10;My question is:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;How to interpret the plot?&lt;/li&gt;&#10;&lt;li&gt;How do I choose my null hypothesis? &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Is my null hypothesis dental is independent of medical? Since my p-value is extremely small I can reject the null?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you in advance!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-18T13:58:59.450" Id="112312" LastActivityDate="2014-08-18T13:58:59.450" OwnerUserId="53098" PostTypeId="1" Score="0" Tags="&lt;hypothesis-testing&gt;&lt;statistical-significance&gt;&lt;categorical-data&gt;&lt;chi-squared&gt;&lt;p-value&gt;" Title="How To Determine Association Plot in R and p-value" ViewCount="50" />
  <row Body="&lt;p&gt;Zero-inflation is about the shape of the distribution. Therefore, you will have to specify the distribution for the non-zero part (Poisson, Negative Binomial, etc), if you want a formal test. Then you can use a likelihood ratio test to see if the zero-inflated parameters can be dropped from the model. This can be done in R.&lt;/p&gt;&#10;&#10;&lt;p&gt;In cruder terms, zero inflation is defined not only by proportion of zeros but also by the total number of observations. Say, if you assume a zero-inflated Poisson model and your data contain 50% of zeros, you still won't be able to say with certainty that it's zero inflated if the total number of points is only 4. On the other hand, 10% of zeros in 1000 observations can result in a positive test for zero-inflation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Zero-inflated property is associated with count-based data, so I haven't heard of &quot;zero-inflated normal&quot;. E.g. in this package: &lt;/p&gt;&#10;&#10;&lt;p&gt;cran.r-project.org/web/packages/pscl/pscl.pdf &lt;/p&gt;&#10;&#10;&lt;p&gt;they only consider Poisson, Negative Binomial and Geometric. What I would do is fit, say, a Poisson model where the zero and non-zero components contain only the intercept and then check if the intercept from the zero component has a significant p-value.&lt;/p&gt;&#10;&#10;&lt;p&gt;P.S. I also managed to find a reference to the (log) normal zero-inflated distribution, but I don't know if it's obtainable for free:&lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;Analysis of repeated measures data with clumping at zero&quot;, Stat Methods Med Res, August 2002 &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://smm.sagepub.com/content/11/4/341.full.pdf+html&quot; rel=&quot;nofollow&quot;&gt;http://smm.sagepub.com/content/11/4/341.full.pdf+html&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-08-18T14:41:50.433" Id="112320" LastActivityDate="2014-08-25T16:37:51.060" LastEditDate="2014-08-25T16:37:51.060" LastEditorUserId="54099" OwnerUserId="54099" ParentId="112292" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;Since the procedure type is of no clinical importance, it can be ignored or included as a random factor to see if it is statistically significant. So the random factors would be either patients only or patients nested within the procedures.&lt;/p&gt;&#10;&#10;&lt;p&gt;The dependent variable can be modeled as the geometric distribution because it is the number of minutes it took until the first success in stopping bleeding. You can specify it as a negative binomial distribution with $\theta = 1$. I know that this is an unbalanced design, the number and types of stitches vary across patients, but (restricted) maximum likelihood estimation should be able to estimate population parameters accurately, assuming that stitch types and numbers are distributed across the patients and procedures (completely) randomly. &lt;/p&gt;&#10;&#10;&lt;p&gt;Sample size permitting, here are some other random effects you may want to include in the model are: the order in which stitching was performed within each patient and surgeons who performed stitches (if there were two or more). &lt;/p&gt;&#10;&#10;&lt;p&gt;I am sure that you can model mixed effects negative-binomial regression with SAS, and I know you can with R (e.g., the &lt;code&gt;glmmADMB&lt;/code&gt; or &lt;code&gt;lme4&lt;/code&gt; package).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-08-18T16:23:21.270" Id="112335" LastActivityDate="2014-08-18T16:39:04.037" LastEditDate="2014-08-18T16:39:04.037" LastEditorUserId="53057" OwnerUserId="53057" ParentId="112179" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="112468" AnswerCount="2" Body="&lt;p&gt;I'm trying to better understand some of the theory behind fitting models that have a nonlinear link between the response and the predictors. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(1)&#10;&#10;#Create a random exponential sample&#10;n&amp;lt;-1000&#10;y&amp;lt;-rexp(n=n,rate=.01)&#10;y&amp;lt;-y[order(y)]&#10;x&amp;lt;-seq(n)&#10;df1&amp;lt;-data.frame(cbind(x,y))&#10;plot(x,y)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now I will attempt 4 different models fits and explain what I expect as a result versus the actual result.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;m1&amp;lt;-lm(I(log(y))~x,data=df1)&#10;summary(m1)&#10;out1&amp;lt;-exp(predict(m1,type='response'))&#10;lines(out1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I expect the m1 model to be the worst fit.  This is OLS with the log of the response taken before fitting the model.  By taking the log of the response I have a model that, when exponentiated, cannot be negative.  Therefore, the assumptions of normally distributed residuals with a constant variance cannot hold.  The graph appears to under weight the tails of the distribution significantly.&lt;/p&gt;&#10;&#10;&lt;p&gt;Next, I will fit the model using a GLM.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;m2&amp;lt;-glm(y~x,data=df1,family=gaussian(link='log'))&#10;summary(m2)&#10;out2&amp;lt;-predict(m2,type='response')&#10;lines(out2,col='red')&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I expect the m2 model to be of a slightly different fit than m1.  This is due to the fact we are now modeling log(y+ϵ)=Xβ rather than modeling log(y)=Xβ+ϵ.  I cannot justify if this fit should be better or worse from a theoretical standpoint.  R gives an R-squared figure for lm() functions calls but not glm() (rather it gives and AIC score).  Looking at the plot it appears that m2 is more poorly matched in the tails of the distribution than m1.  The issues with normally distributed residuals should still be the same.&lt;/p&gt;&#10;&#10;&lt;p&gt;For m3, I relax the residual distribution assumptions by changing to a Gamma family residual distribution.  Since an exponential distribution is a Gamma family distribution, I expect this model to very closely match the sampled data.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;m3&amp;lt;-glm(y~x,data=df1,family=Gamma(link='log'))&#10;summary(m3)&#10;out3&amp;lt;-predict(m3,type='response')&#10;lines(out3,col='blue')&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;AIC suggests that m3 is a better fit than m2, but it still does not appear to be a very good fit.  In fact, it appears to be a worse estimate in the tails of the distribution than m1.  &lt;/p&gt;&#10;&#10;&lt;p&gt;The m4 model will use nls just for a different approach.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;m4&amp;lt;-nls(y~exp(a+b*x),data=df1, start = list(a = 0, b = 0))&#10;summary(m4)&#10;out4&amp;lt;-predict(m4,type='response')&#10;lines(out4,col='yellow')&#10;&#10;#for direct comparison&#10;t1&amp;lt;-cbind(out1,out2,out3,out4,y)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This appears to very closely match m2.  I also expected this to very closely fit the data.&lt;/p&gt;&#10;&#10;&lt;p&gt;My questions are - how come does m3 not more closely match my data?  Is there a way using glm() to more closely fit this model?  I realize that if I am randomly sampling then the tails of my sample data will not fit my model exactly, but these appear to be nowhere close.  Perhaps letting n approach infinity would all one (or more) of the models to converge, but rerunning the code with n=100,000 actually appears to be worse fitting models (perhaps that's because with 100,000 random samples more outliers were selected and due to how the graph is present an undue amount of focus is given to these outliers?).  &lt;/p&gt;&#10;&#10;&lt;p&gt;I also realize that an exponential distribution is not the same as exponentiating a normal distribution.  That is to say, I realize that just because I took the log the of response doesn't mean that I should get a &quot;perfect model&quot; as a result; however, in m3 when I am fitting the models to a Gamma family distribution, I expected to be able to get a very close fit. &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-18T16:35:06.753" FavoriteCount="1" Id="112338" LastActivityDate="2014-08-19T14:26:17.713" OwnerUserId="45603" PostTypeId="1" Score="1" Tags="&lt;generalized-linear-model&gt;&lt;lm&gt;&lt;exponential-family&gt;" Title="Best Fit for Exponential Data" ViewCount="128" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I've got two sets of data from some fluorescent cells.&lt;/p&gt;&#10;&#10;&lt;p&gt;The first set is when the cells don't have their fluorescence switched on, but they are still faintly glowing.&lt;/p&gt;&#10;&#10;&lt;p&gt;The second set is when they do have their fluorescence switched on.&lt;/p&gt;&#10;&#10;&lt;p&gt;I ran each of the two experiments three times, so n=3, and I therefore have a mean for each dataset and a SD.&lt;/p&gt;&#10;&#10;&lt;p&gt;I need to subtract the non-switched on set from the switched on set in order to determine the amount of fluorescence that arises as a result of being switched on.&lt;/p&gt;&#10;&#10;&lt;p&gt;How do I then calculate the SD of the final value?&lt;/p&gt;&#10;&#10;&lt;p&gt;I know that for discrete random variables, &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ E[X+Y] = E[X] + E[Y] $$&lt;/p&gt;&#10;&#10;&lt;p&gt;So I assume that holds true for subtraction as well, but I can't find the rules for continuous random variables, as these are.&lt;/p&gt;&#10;&#10;&lt;p&gt;Example data for one data point:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;         ON            OFF&#10;MEAN:  33956.6666    3835.66667&#10;SD:    457.47301     38.0905&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="12" CreationDate="2014-08-18T17:51:56.923" Id="112351" LastActivityDate="2014-08-19T10:53:48.450" OwnerUserId="54279" PostTypeId="1" Score="2" Tags="&lt;standard-deviation&gt;&lt;mean&gt;&lt;biostatistics&gt;" Title="Standard Deviation After Subtracting One Mean From Another" ViewCount="583" />
  <row Body="&lt;p&gt;The problem with this approach in practice is that most data sets are small enough that many distributions will adequately fit the data.  If you arbitrarily pick a distribution that happens to fit the data and then proceed to do calculations or a simulation under this assumption, you can be badly mislead.  &lt;/p&gt;&#10;&#10;&lt;p&gt;This problem occurs frequently in discrete event simulation modeling.  One practical approach is to run the simulation model using a variety of distributions to see whether the results are sensitive to the distributional assumptions.  &lt;/p&gt;&#10;&#10;&lt;p&gt;If you're doing statistical analysis, then nonparametric statistics can often be used to analyze your data without making distributional assumptions.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-18T18:17:54.113" Id="112356" LastActivityDate="2014-08-18T18:17:54.113" OwnerUserId="8200" ParentId="112349" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;In my experiences, I have found that using Levene's Test works quite well.  It does not require the sample sets to have the same number data points and it is not sensitive to non-normality. &#10;&lt;a href=&quot;http://www.itl.nist.gov/div898/handbook/eda/section3/eda35a.htm&quot; rel=&quot;nofollow&quot;&gt;http://www.itl.nist.gov/div898/handbook/eda/section3/eda35a.htm&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-18T18:55:25.827" Id="112363" LastActivityDate="2014-08-18T18:55:25.827" OwnerUserId="26756" ParentId="111267" PostTypeId="2" Score="0" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I am working with a time series data and found that $a_t$ Granger causes $b_{t+1}$ and $b_t$ granger causes $a_{t+1}$. The results were obtained through Stata. &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;With the coefficients obtained, is it possible to determine which Granger causal direction is stronger? If it can't be determined by just looking at the coefficients, is there a method to do so?  &lt;/p&gt;&#10;&#10;&lt;p&gt;Also, Granger causality was tested on 2 conditions. In one subset, a&#10;policy was adopted. In the other subset, the policy was not adopted.&#10;The results was that a granger causes $b_{t+1}$ in both cases.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Is there a way to test whether the Granger causality coefficient in condition one was significantly different in condition two? One idea is to introduce an interaction term, but is this possible with Granger causality tests?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2014-08-18T19:23:38.327" Id="112366" LastActivityDate="2014-08-18T20:20:45.903" LastEditDate="2014-08-18T19:52:37.383" LastEditorUserId="1739" OwnerUserId="43929" PostTypeId="1" Score="0" Tags="&lt;granger-causality&gt;" Title="Granger Causality coefficient comparison" ViewCount="73" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Say I have 10 subjects, coming from three groups.  In each subject I make two measurements (one on the left, one on the right).&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ID &amp;lt;- rep(1:10, 2)&#10;myData &amp;lt;- data.frame(ID)&#10;myData$Group &amp;lt;- rep(c('A','B'), 10)&#10;    myData$Measurement &amp;lt;- rnorm(20, mean = 10, sd = 1)&#10;myData$MeasurementLocation &amp;lt;- c(rep('Left', 10), rep('Right', 10))&#10;myData &amp;lt;- myData[with(myData, order(ID)),]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Using the &lt;code&gt;gee()&lt;/code&gt; function from the &lt;code&gt;gee&lt;/code&gt; package in R, I can compare &lt;code&gt;Measurement&lt;/code&gt; by &lt;code&gt;Group&lt;/code&gt; while controlling for intrasubject correlation using &lt;code&gt;gee()&lt;/code&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;gee(Measurement ~ Group, id = ID, data = myData)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;However, I would like to specify &lt;em&gt;where the measurement came from&lt;/em&gt; (i.e., right vs left).  I am told that in SPSS you can specify a &lt;code&gt;WITHINSUBJECT&lt;/code&gt; argument (which I assume would be &lt;code&gt;myData$MeasurementLocation&lt;/code&gt; in my data).  Is there a way to do this using &lt;code&gt;gee()&lt;/code&gt;?  If not, is there another function/package that would allow for this?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-08-18T21:10:49.073" Id="112382" LastActivityDate="2014-08-18T21:10:49.073" OwnerUserId="30231" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;gee&gt;" Title="GEE Repeated Measurements: Specifying Measurement ID" ViewCount="28" />
  <row Body="&lt;p&gt;The biggest impediment is unfamiliarity with R. The Internet is overflowing with R tutorials in flavors for every academic discipline. Stack Overflow has thousands of questions about how to do things in R; searching the archives, using keywords from this answer, will set you on the right track.&lt;/p&gt;&#10;&#10;&lt;p&gt;1) The technical name for your solution for missing values is called &lt;em&gt;casewise deletion.&lt;/em&gt; This can create problems when the missingness of the observation depends on its value. The classic example of this is survey questions about a person's income. People with very high or very low income might be less likely to answer those questions, but simply omitting these people can bias estimates which depend on income as a predictor. &lt;/p&gt;&#10;&#10;&lt;p&gt;Handling missing data is a huge topic in statistics. One approach is called imputation, which attempts to fill in the missing values by using observed values as a guide. But there are many others. Search the missing data tag for more information.&lt;/p&gt;&#10;&#10;&lt;p&gt;2) You didn't actually convert &lt;code&gt;airbags&lt;/code&gt; to a numerical value, you just made it a factor with labels &quot;0&quot;, &quot;1&quot;, and &quot;2&quot;. If you converted it to a factor and labeled it &quot;A&quot;, &quot;B&quot; and &quot;C&quot; or &quot;Tom&quot;, &quot;Dick&quot;, and &quot;Harry&quot;, R would treat it the same way, because factors are regarded as &lt;em&gt;categories&lt;/em&gt; in R: political parties or religions or brands of cars are all logically represented as factors; numbers are not.&lt;/p&gt;&#10;&#10;&lt;p&gt;You'll have to coerce those values to numeric quantities (reals or integers) in order to treat them as numbers.&lt;/p&gt;&#10;&#10;&lt;p&gt;3) Again, this has to do with data types. For a factor with $k$ levels, R will estimate coefficients for $k-1$ of the levels because the $k^{\text{th}}$ level will be collinear with the intercept. Each of the $k$ levels is a binary vector, $1$ when the observation is of category $k$ and $0$ otherwise. The first category alphabetically is used as the reference category and is included with the intercept. The remaining levels are reported in the model summary.&lt;/p&gt;&#10;&#10;&lt;p&gt;4) Your last questions are far too broad to be answered here. The literal answer to why your model has only a few &quot;signficiant&quot; features is that the coefficents are too small relative to their standard errors, so they are plausibly zero. More detail can be found in a standard regression textbook, or in the CV archives.&lt;/p&gt;&#10;&#10;&lt;p&gt;Another answer is that you either need a better model, more data, or both. If this model was selected with stepwise AIC, it can be shown that the selection procedure is roughly the same as only keeping features with p-values $\le 0.16$. It's widely acknowledged in the statistical community that stepwise feature selection is a &lt;em&gt;minimum logic estimator,&lt;/em&gt; rather than a useful tool to pick out meaningful relationships in data.&lt;/p&gt;&#10;&#10;&lt;p&gt;My recommendation to improving your model is to take a step back and really think about what features might increase or decrease a car's price. Do you &lt;em&gt;really&lt;/em&gt; think that many people decide to buy a car based on its gas tank volume? Or length? On the other hand, for the same reason that a designer purse commands an exorbitant price, a car's brand may be important, but this doesn't appear to figure into your model.&lt;/p&gt;&#10;&#10;&lt;p&gt;The final question, how to check your assumptions, can only reasonably be answered by reading up on regression. One resource is &lt;a href=&quot;http://people.duke.edu/~rnau/testing.htm&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;. But I can't overstate the value of reading a good textbook, especially when getting started.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-08-18T21:21:47.277" Id="112384" LastActivityDate="2014-08-18T22:09:04.720" LastEditDate="2014-08-18T22:09:04.720" LastEditorUserId="22311" OwnerUserId="22311" ParentId="112380" PostTypeId="2" Score="6" />
  <row AnswerCount="0" Body="&lt;p&gt;I read 1) &lt;a href=&quot;http://stats.stackexchange.com/questions/29121/intuitive-explanation-of-unit-root&quot;&gt;Intuitive explanation of unit root&lt;/a&gt; and 2) &lt;a href=&quot;http://www.r-bloggers.com/unit-root-tests/&quot; rel=&quot;nofollow&quot;&gt;http://www.r-bloggers.com/unit-root-tests/&lt;/a&gt; for doing unit root test. I have basic questions:&lt;/p&gt;&#10;&#10;&lt;p&gt;1)should I check for unit root on both 'x' and 'y' variables of my lm equation or just one of them? For AR process, it doesnt matter (I suppose), but my &quot;lm&quot; equation has 1) few variables(x's) on right hand side, and also 2)one lag of y. Should I check ur.lm for all x's as well?&lt;/p&gt;&#10;&#10;&lt;p&gt;2)I am confused  on what parameters should I pass to ur.df?  what does type parameter mean(none/trend/drift) and how do I decide which of none/trend/drift is the right one&lt;/p&gt;&#10;&#10;&lt;p&gt;3)How would I interpret the following df test result(what is z.diff.lag? Is it correct to say z.diff.lag does NOT exhibit unit root given its high statistic, hence z.diff.lag is stationary; but what is z.diff.lag anyway)&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    &amp;gt; summary(ur.df(temp3))&#10;&#10;    ############################################### &#10;     Augmented Dickey-Fuller Test Unit Root Test # &#10;     ############################################### &#10;&#10;    Test regression none &#10;    Call:&#10;          lm(formula = z.diff ~ z.lag.1 - 1 + z.diff.lag)&#10;    Residuals:&#10;          Min      1Q  Median      3Q     Max &#10;           -5.7048 -0.7167 -0.1592  0.4686  7.4964 &#10;     Coefficients:&#10;          Estimate Std. Error t value Pr(&amp;gt;|t|)    &#10;           z.lag.1    -0.0002636  0.0001407  -1.874   0.0609 .  &#10;           z.diff.lag -0.3607210  0.0048720 -74.040   &amp;lt;2e-16 ***&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-08-18T21:36:24.413" Id="112386" LastActivityDate="2014-08-18T22:20:17.287" LastEditDate="2014-08-18T22:20:17.287" LastEditorUserId="45817" OwnerUserId="45817" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;multiple-regression&gt;&lt;regression-coefficients&gt;" Title="How to interpret Dickey Fuller (DF) test results in R (for unit test)" ViewCount="81" />
  <row Body="&lt;p&gt;Several ideas and references are discussed in:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://dl.acm.org/citation.cfm?id=513543&quot; rel=&quot;nofollow&quot;&gt;A simple generalization of the area under the ROC curve to multiple class classification problems&lt;/a&gt;. &lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;https://www.cs.bris.ac.uk/~flach/ICML04tutorial/ROCtutorialPartIII.pdf&quot; rel=&quot;nofollow&quot;&gt;Multi-class ROC (a tutorial)&lt;/a&gt; (using &quot;volumes&quot; under ROC)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Other approaches include computing &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;macro-average ROC curves (average per class in a 1-vs-all fashion) &lt;/li&gt;&#10;&lt;li&gt;micro-averaged ROC curves (consider all positives and negatives together as  single class)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;You can see &lt;a href=&quot;http://scikit-learn.org/stable/auto_examples/plot_roc.html&quot; rel=&quot;nofollow&quot;&gt;examples&lt;/a&gt; in some libraries like scikit-learn.&lt;/p&gt;&#10;&#10;&lt;p&gt;See also this other thread in CrossValidated: &#10;&lt;a href=&quot;http://stats.stackexchange.com/questions/21551/how-to-compute-precision-recall-for-multiclass-multilabel-classification&quot;&gt;How to compute precision/recall for multiclass-multilabel classification?&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-08-18T21:59:21.213" Id="112389" LastActivityDate="2014-08-18T22:05:25.517" LastEditDate="2014-08-18T22:05:25.517" LastEditorUserId="27838" OwnerUserId="27838" ParentId="112383" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;Dynamic models that update the abilities at each time step are generally better, but hard to implement. If you want to use lots of things like number of passes, then you will generally want to be using some sort of regression somewhere in your model.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://onlinelibrary.wiley.com/doi/10.1111/rssa.12015/abstract&quot; rel=&quot;nofollow&quot;&gt;http://onlinelibrary.wiley.com/doi/10.1111/rssa.12015/abstract&lt;/a&gt; is a paper that predicts a player's goal scoring ability with a mixed effects model.&lt;/p&gt;&#10;&#10;&lt;p&gt;State-space models give a way of estimating the abilities of football teams, given the number of goals scored in a match, over time. An example is this paper &lt;a href=&quot;http://onlinelibrary.wiley.com/doi/10.1111/rssa.12042/abstract&quot; rel=&quot;nofollow&quot;&gt;http://onlinelibrary.wiley.com/doi/10.1111/rssa.12042/abstract&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are a number of ordinal regression models. A dynamic one, which aims to rate teams, is &lt;a href=&quot;http://onlinelibrary.wiley.com/doi/10.1111/1467-9884.00236/abstract&quot; rel=&quot;nofollow&quot;&gt;http://onlinelibrary.wiley.com/doi/10.1111/1467-9884.00236/abstract&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;There is the also pi-football model, which uses a detailed Bayesian network that uses lots of information: &lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0950705112001967&quot; rel=&quot;nofollow&quot;&gt;http://www.sciencedirect.com/science/article/pii/S0950705112001967&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hopefully these papers will give you some good ideas on how to tackle the problem.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-19T02:56:17.757" Id="112401" LastActivityDate="2014-08-19T02:56:17.757" OwnerUserId="52756" ParentId="112307" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;If it is a general approach, then it applies generally. A neural network is a universal approximator.  Converging on a particular number of neurons means only that it is not universal anymore.&lt;/p&gt;&#10;&#10;&lt;p&gt;So your equation results in the following plot.  I am assuming a negative value for Neuron1 is meaningless.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/QpnfX.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;So it is a function that maps the continuous range of 0...1 to a discrete range of 4 to 24.&lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;Best&quot; does not exist without a measure of goodness.  There are infinite numbers of candidate bests.  For every candidate rubric there are infinite candidate anti-rubrics, and perpendicular rubriks.  You have not specified a measure of goodness so there is no best to exist.&lt;/p&gt;&#10;&#10;&lt;p&gt;It is likely that the underlying system is better fit by a higher number of neurons.  There are networks with billions of neurons.  Think about your own brain, for example.  Assuming it is efficient, not a bad assumption, then it should take about a billion artificial neurons to make a good approximation of what the real neurons are doing.  If you tried to approximate it using 24 neurons, it would fall stunningly short - for all currently used transfer functions.&lt;/p&gt;&#10;&#10;&lt;p&gt;My suggestion:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;use something like a random forest to determine something about the actual complexity of your data.  &lt;/li&gt;&#10;&lt;li&gt;select neuron counts appropriate to that complexity, train properly, and cull as appropriate.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit/Elaboration:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;A random forest subsets the data.  It builds trees of random subsets of the data.  &lt;/p&gt;&#10;&#10;&lt;p&gt;When I build a random forest, of course I use CV, but then I look at total error and total complexity as a function of tree size  to determine proper tree size.  I look at variable importance to reduce dimensionality, and train using the important variables.  Once you have a reasonable tree, you can iterate on training-validation using progressively smaller chunks of the data for training until you hit a &quot;cliff&quot; or &quot;knee&quot; in representation.  At that point you are at minimum rows and minimum columns.  You can double-check leaf size with an eye for minimization, but you are reasonably well situated to apply the fitting with a NN.  You want to do a few reruns with random row-selection at every level of split, of course, to make sure that your mean error at the split-size is not an artifact.  &lt;/p&gt;&#10;&#10;&lt;p&gt;This leaves you several estimates of parameter:&lt;/p&gt;&#10;&#10;&lt;p&gt;$ #rows x #colums = minimum data element count$&#10;$ #leaves x #trees = tree parameter count $&lt;/p&gt;&#10;&#10;&lt;p&gt;Then you can look at the parameters count for your neurons and put a ceiling there.  If you can't beat the trees with the NN you have a problem.  You add up all the &quot;weights&quot; and &quot;biases&quot; in your ANN and that is your parameter count.  You can also look at this to inform your training set, and to subsample your original data if training would take a prohibitively long time.&lt;/p&gt;&#10;&#10;&lt;p&gt;So you start your ANN with the same number of parameters as the RF.  You train until suitable representation occurs.  Then you perform a cull operation, where you cull the least informative neuron, and update the training.  You track the fully-trained error as a function of parameter count and make a good judgment call as to where you are comfortable leaving your NN.  I would suggest evaluating performance from RF-base to at least 25% fewer parameters than RF-base as a domain to evaluate fully-trained performance.&lt;/p&gt;&#10;&#10;&lt;p&gt;In my personal experience, many things can be approximated with Random Forests of less than 300 trees, otherwise built to the recommendations of &lt;a href=&quot;https://www.stat.berkeley.edu/users/breiman/RandomForests/cc_papers.htm&quot; rel=&quot;nofollow&quot;&gt;Breiman&lt;/a&gt; and &lt;a href=&quot;http://www.math.usu.edu/adele/RandomForests/index.htm&quot; rel=&quot;nofollow&quot;&gt;company&lt;/a&gt;.  Sometimes fewer than 50 trees will do the job well.  This means some ballpark complexity ceilings are between 300-1800 parameters.  Your actual mileage is going to depend entirely on your data.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;aside:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;This would make a great (winning) science fair (ISEF) project.  As a former judge - I would have liked to see some of this there, in the applied mathematics area.  If someone wanted to apply this to a dozen or so &lt;a href=&quot;https://archive.ics.uci.edu/ml/index.html&quot; rel=&quot;nofollow&quot;&gt;textbook data sets&lt;/a&gt; and compare to textbook results. Given that &quot;machine learning&quot; and &quot;data-mining&quot; are pretty popular right now this could be good.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-08-19T03:14:39.367" Id="112403" LastActivityDate="2014-08-19T15:40:42.740" LastEditDate="2014-08-19T15:40:42.740" LastEditorUserId="22452" OwnerUserId="22452" ParentId="112211" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;A simple question to which I don't seem to find the answer anywhere. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have a response variable &lt;code&gt;duration of time spent doing A&lt;/code&gt; of individuals tested for $\text{max duration}=X$. Therefore the variance is likely to be much wider at $x=0.5$ compared to $X=0$ and $X=1$. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, I am confused what family and link function I need as time is continuous but can not be less than $0$ and more than $X$.&lt;/p&gt;&#10;&#10;&lt;p&gt;ps: I am using R so any code examples would be appreciated&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;&lt;br/&gt;&#10;I have heard about the tobit() function from &quot;AER&quot; which &quot;&lt;em&gt;is a convenience interface to 'survreg' setting different defaults and providing a more convenient interface for specification of the censoring information.&lt;/em&gt;&quot; &lt;/p&gt;&#10;&#10;&lt;p&gt;Therefore, it seems for my data I could run the model&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;tobit(duration.A ~ factor1*factor2, left=0, right=X)&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;However, it is unclear how A) I could do this for a mixed model i.e. a model with random factors, B) what it's assumptions are (they are not clear from the R documentation &lt;a href=&quot;http://cran.r-project.org/web/packages/betareg/vignettes/betareg.pdf&quot; rel=&quot;nofollow&quot;&gt;http://cran.r-project.org/web/packages/betareg/vignettes/betareg.pdf&lt;/a&gt;), and C) how I could get residuals and fitted values for plotting.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2014-08-19T11:00:57.653" Id="112441" LastActivityDate="2014-08-19T11:54:17.100" LastEditDate="2014-08-19T11:54:17.100" LastEditorUserId="20112" OwnerUserId="20112" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;generalized-linear-model&gt;&lt;glmm&gt;&lt;link-function&gt;" Title="What GLM family and link function for &quot;proportion of time&quot;?" ViewCount="73" />
  <row Body="&lt;p&gt;&lt;code&gt;Cross validation&lt;/code&gt; has two purposes : &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;when you don't use cross validation and randomly select a part of data as train and other part as test, you may have a high accuracy in that part for train and test but when you select another train and test data you may have lower accuracy. Cross validation methods like &lt;code&gt;n-fold cross validation&lt;/code&gt; or etc. will help to find best fit model based on your database. with lowest error on all parts of data.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;In some cases cross validation will help to find some parameters of model like &lt;code&gt;C&lt;/code&gt; in logistic regression that you can find some documentation about it in &lt;code&gt;MATLAB&lt;/code&gt; help center or in &lt;code&gt;R&lt;/code&gt; documentation files.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;So as we discoursed &lt;code&gt;cross validation&lt;/code&gt; has a critical rule to finding a reliable model for your database. You should select best cross-validation technique based on your model structure and your sample size. &lt;code&gt;5-fold cross validation&lt;/code&gt; is a well known technique. You can increase the &lt;code&gt;k&lt;/code&gt; in k-fold cross validation If you have more sample size. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-19T11:38:02.373" Id="112445" LastActivityDate="2014-08-19T11:38:02.373" OwnerUserId="43534" ParentId="108199" PostTypeId="2" Score="1" />
  
  
  
  <row Body="&lt;p&gt;Maximum Likelihood Estimation (MLE) is a technique to find the &lt;em&gt;most likely&lt;/em&gt; &#10;function that explains observed data. I think math is necessary, but don't let it&#10;scare you!&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's say that we have a set of points in the $x,y$ plane, and we want to know &#10;the function parameters $\beta$ and $\sigma$ that most likely fit the data &#10;(in this case we know the function because I specified it to create this &#10;example, but bear with me).&lt;/p&gt;&#10;&#10;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;data   &amp;lt;- data.frame(x = runif(200, 1, 10))&#10;data$y &amp;lt;- 0 + beta*data$x + rnorm(200, 0, sigma2)&#10;plot(data$x, data$y)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Qz2aZ.png&quot; alt=&quot;data points&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In order to do a MLE, we need to make assumptions about the form of the function.&#10;In a linear model, we assume that the points follow a normal (Gaussian) probability&#10;distribution, with mean $x\beta$ and standard deviation $\sigma^2$:&lt;br&gt;&#10;$y = \mathcal{N}(x\beta, \sigma^2)$.  The equation of this probability density function is:&#10;$$\frac{1}{\sqrt{2\pi\sigma^2}}\exp{\left(-\frac{(y_i-x_i\beta)^2}{2\sigma^2}\right)}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;What we want to find is the parameters $\beta$ and $\sigma$ that &lt;em&gt;maximize&lt;/em&gt; this &#10;probability for all points $(x_i, y_i)$. This is the &quot;likelihood&quot; function, $\mathcal{L}$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\mathcal{L} = \prod_{i=1}^n y_i = \prod_{i=1}^n \dfrac{1}{\sqrt{2\pi\sigma^2}}&#10;                \exp\Big({-\dfrac{(y_i - x_i\beta)^2}{2\sigma^2}}\Big)$$&#10;For various reasons, it's easier to use the log of the likelihood function:&#10;$$\log(\mathcal{L}) = \sum_{i = 1}^n-\frac{n}{2}\log(2\pi) -\frac{n}{2}\log(\sigma^2) -&#10;      \frac{1}{2\sigma^2}(y_i - x_i\beta)^2$$&lt;/p&gt;&#10;&#10;&lt;p&gt;We can code this as a function in R with $\theta = (\beta,\sigma^2)$.&lt;/p&gt;&#10;&#10;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;linear.lik &amp;lt;- function(theta, y, X){&#10;  n      &amp;lt;- nrow(X)&#10;  k      &amp;lt;- ncol(X)&#10;  beta   &amp;lt;- theta[1:k]&#10;  sigma2 &amp;lt;- theta[k+1]&#10;  e      &amp;lt;- y - X%*%beta&#10;  logl   &amp;lt;- -.5*n*log(2*pi)-.5*n*log(sigma2) - ( (t(e) %*% e)/ (2*sigma2) )&#10;  return(-logl)&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This function, at different values of $\beta$ and $\sigma^2$, creates a surface.&lt;/p&gt;&#10;&#10;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;surface &amp;lt;- list()&#10;k &amp;lt;- 0&#10;for(beta in seq(0, 5, 0.1)){&#10;  for(sigma2 in seq(0.1, 5, 0.1)){&#10;    k &amp;lt;- k + 1&#10;    logL &amp;lt;- linear.lik(theta = c(0, beta, sigma2), y = data$y, X = cbind(1, data$x))&#10;    surface[[k]] &amp;lt;- data.frame(beta = beta, sigma2 = sigma2, logL = -logL)&#10;  }&#10;}&#10;surface &amp;lt;- do.call(rbind, surface)&#10;library(lattice)&#10;wireframe(logL ~ beta*sigma2, surface, shade = TRUE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/2iVZv.png&quot; alt=&quot;likelihood surface&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;As you can see, there is a maximum point somewhere on this surface. &#10;We can find parameters that specify this point with R's built-in &#10;optimization commands. This comes reasonably close to uncovering the true parameters&#10;$0, \beta = 2.7, \sigma^2 = 1.3$&lt;/p&gt;&#10;&#10;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;linear.MLE &amp;lt;- nlm(f=linear.lik, p=c(1,1,1), hessian=TRUE, y=data$y, X=cbind(1, data$x))&#10;linear.MLE$estimate&#10;&#10;## [1] -0.04481  2.70841  1.71870&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Ordinary least squares &lt;em&gt;is&lt;/em&gt; the maximum likelihood for a linear model, so it &#10;makes sense that &lt;code&gt;lm&lt;/code&gt; would give us the same answers. (Note that $\sigma^2$ is used&#10;in determining the standard errors).&lt;/p&gt;&#10;&#10;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;summary(lm(y ~ x, data))&#10;&#10;## &#10;## Call:&#10;## lm(formula = y ~ x, data = data)&#10;## &#10;## Residuals:&#10;##    Min     1Q Median     3Q    Max &#10;## -3.106 -0.893  0.032  0.880  3.281 &#10;## &#10;## Coefficients:&#10;##             Estimate Std. Error t value Pr(&amp;gt;|t|)    &#10;## (Intercept)  -0.0449     0.2334   -0.19     0.85    &#10;## x             2.7084     0.0387   69.98   &amp;lt;2e-16 ***&#10;## ---&#10;## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1&#10;## &#10;## Residual standard error: 1.32 on 198 degrees of freedom&#10;## Multiple R-squared:  0.961,  Adjusted R-squared:  0.961 &#10;## F-statistic: 4.9e+03 on 1 and 198 DF,  p-value: &amp;lt;2e-16&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-08-19T14:30:21.897" Id="112469" LastActivityDate="2014-08-25T10:48:34.793" LastEditDate="2014-08-25T10:48:34.793" LastEditorUserId="10026" OwnerUserId="10026" ParentId="112451" PostTypeId="2" Score="22" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a series of about 1000 points on a chromosome and I want to know if they are clumped, over-dispersed, or neither.  The chromosome can be viewed as a 1-dimensional.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I've looked over &lt;a href=&quot;http://www.seas.upenn.edu/~ese502/NOTEBOOK/Part_I/3_Testing_Spatial_Randomness.pdf&quot; rel=&quot;nofollow&quot;&gt;some spatial stats&lt;/a&gt; and found 2 methods that look promising.  &lt;/p&gt;&#10;&#10;&lt;p&gt;First one can chop the line into M segments, and the number of points in each segment should be Poisson distributed with &amp;lambda; = N/M, where N is the total number of points. Then it isn't too hard to construct a chi-squared test, where the observed is the point count in each segment, and the expected is &amp;lambda;.  The issue here of course is how to choose M.  I guess you try a range of values and see how stable the result is.&lt;/p&gt;&#10;&#10;&lt;p&gt;The second method is to compute the distance to the nearest neighbor (NN) for each point. The NN values are said to be Rayleigh distributed. I'm not very comfortable with this distribution and I'm struggling to construct a good test statistic for the NN values. Similarly, it would be easy to simulate a null distribution though Monte Carlo methods, but I'm not sure what the appropriate summary statistic is to compare my observed distribution against (maybe the variance?). Any help appreciated!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-08-19T14:33:59.743" Id="112470" LastActivityDate="2014-08-19T14:33:59.743" OwnerUserId="54335" PostTypeId="1" Score="2" Tags="&lt;hypothesis-testing&gt;&lt;monte-carlo&gt;&lt;spatial&gt;&lt;poisson-process&gt;&lt;rayleigh&gt;" Title="Testing if points on a line are over/under dispersed" ViewCount="30" />
  
  <row AnswerCount="1" Body="&lt;p&gt;For dealing with multiple testing, Benjamini and Hochberg (1995) assume that the tests are independent. Benjamini and Yekutieli (2001) show that (1) the 1995 result also holds under (I think) weaker conditions and that (2) including the factor $\Sigma^m_{i=1}\frac{1}{i}$ makes the procedure applicable for any dependence structure (see Theorem 1.3). Since this summation is greater than or equal to 1, it would make the procedure more conservative.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is why is independent tests not the worst case assumption? Is there some simple example to illustrate why not? Does it involve a negative dependence structure of some sort?&lt;/p&gt;&#10;&#10;&lt;p&gt;Maybe my understanding is faulty, but I thought that the Bonferroni adjustment &#10;(for the familywise error) worked for any dependence and that independence is the worst case assumption. Perhaps this is not correct and hence my confusion with BH. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-19T18:31:35.567" FavoriteCount="2" Id="112506" LastActivityDate="2014-08-19T19:54:38.353" LastEditDate="2014-08-19T19:54:38.353" LastEditorUserId="54351" OwnerUserId="54351" PostTypeId="1" Score="4" Tags="&lt;hypothesis-testing&gt;&lt;p-value&gt;&lt;multiple-comparisons&gt;&lt;intuition&gt;&lt;bonferroni&gt;" Title="What is the intuition for dependence assumption in Benjamini and Hochberg (1995)?" ViewCount="53" />
  <row Body="&lt;p&gt;Independence is more like a best-case assumption than a worst-case assumption.  Loosely, when data are independent, each datum contains as much information as possible.  If data were dependent, because their values can be predicted from other data, each additional datum must have less new information to contribute (the part that could have been predicted you already knew in some sense).  The situation can be similar with multiple testing.  In terms of simple alpha correction strategies, if the tests are independent, the &lt;a href=&quot;http://en.wikipedia.org/wiki/%C5%A0id%C3%A1k_correction&quot; rel=&quot;nofollow&quot;&gt;Dunn-Sidak correction&lt;/a&gt; can be used:&lt;br&gt;&#10;$$&#10;\alpha_{\rm DS} = 1 - (1-\alpha)^{1/k}&#10;$$&#10;but if the tests are not independent, the &lt;a href=&quot;http://en.wikipedia.org/wiki/Bonferroni_correction&quot; rel=&quot;nofollow&quot;&gt;Bonferroni correction&lt;/a&gt; must be used:&lt;br&gt;&#10;$$&#10;\alpha_{\rm B} = \frac{\alpha}{k}&#10;$$&#10;As is clear from the formulas, $\alpha_{\rm DS}\ge \alpha_{\rm B}$.  &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-08-19T18:48:48.207" Id="112509" LastActivityDate="2014-08-19T18:48:48.207" OwnerUserId="7290" ParentId="112506" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;This seems to be a question using the &lt;a href=&quot;https://en.wikipedia.org/wiki/Multinomial_distribution&quot;&gt;multinomial distribution&lt;/a&gt;. In essence you have 4 trials (the family members) to be split into 3 categories (A, B, C). Each category has an equal chance of success (i.e. the family member being assigned there). Let the number of members assigned to A, B and C be $n_A$, $n_B$ and $n_C$ respectively. Then the relevant probability mass function is:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$f(n_A,n_B,n_C) = \frac{4!}{n_A!n_B!n_C!}\left(\frac{1}{3}\right)^4$$&lt;/p&gt;&#10;&#10;&lt;p&gt;You want two members assigned to one station, and the other two assigned to separate stations. This can be done in three ways, so the required probability is:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$3\times \frac{4!}{2!1!1!}\times \left(\frac{1}{3}\right)^4 = \frac{4}{9} \approx 0.444$$&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-19T21:06:06.343" Id="112523" LastActivityDate="2014-08-19T21:06:06.343" OwnerUserId="27581" ParentId="112290" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;Looking at the same thing in two different but equivalent ways offers insight.&lt;/p&gt;&#10;&#10;&lt;p&gt;A Binomial$(n,p)$ variable is the sum of $n$ independent Bernoulli$(p)$ variables.  A Bernoulli variable works exactly like drawing one ticket from a box in which all tickets have either a $0$ or $1$ written on them; the proportion of the latter is $p$.&lt;/p&gt;&#10;&#10;&lt;p&gt;To say that $X=x$ means that $n$ such tickets were drawn from such an &quot;$X$ box&quot; (with replacement each time) and $x$ of them had a $1$ on it.  To say that $Y$ has a Binomial$(X,q)$ distribution amounts to performing a second follow-on experiment in which $x$ draws (with replacement) are made from a separate box, the &quot;$Y$ box,&quot; in which the proportion of tickets with $1$s is $q$.  The value of $Y$ is the count of the $1$s that are drawn.&lt;/p&gt;&#10;&#10;&lt;p&gt;An alternative way to carry out the same procedure is not to wait until all $n$ tickets are drawn from the $X$ box.  Instead, after drawing each ticket, immediately read its value.  If it says $X=0$, do nothing more.  If it says $X=1$, though, &lt;em&gt;immediately&lt;/em&gt; draw a ticket from the $Y$ box and read its value.&lt;/p&gt;&#10;&#10;&lt;p&gt;This alternative procedure can be described by drawing a single ticket from a new box.  Up to two numbers are written on each ticket, called &quot;$X$&quot; and &quot;$Y$&quot;, to record a single sequence of up to two draws.  According to the foregoing description, which has three outcomes, there must be three kinds of corresponding tickets:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;$X=0$. These tickets model drawing a value of $0$ from the $X$ box.  Their proportion within the new box, in order to emulate the properties of the first step, must equal $1-p$.  Don't bother to write any value for $Y$, because $Y$ will not be observed when such a ticket is drawn.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;$X=1, Y=0$.  These tickets model drawing a $1$ from the $X$ box and then a $0$ from the $Y$ box.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;$X=1, Y=1$.  These tickets model drawing a $1$ from the $X$ box and then a $1$ from the $Y$ box.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;The total proportion of tickets of types (2) and (3) must equal the proportion of $1$s in an $X$ box, namely $p$.  Since $Y$ is drawn independently of $X$, the fraction of the tickets with $X=1$ for which $Y=1$ must be $q$.  The fraction of the tickets with $X=1$ for which $Y=0$ similarly must be $1-q$.&lt;/p&gt;&#10;&#10;&lt;p&gt;To summarize, the three tickets and their proportions in the new box must be&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;$X=0$, proportion $p$.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;$X=1, Y=0$, proportion $p(1-q)$.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;$X=1, Y=1$, proportion $pq$.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;What kind of variable is $Y$?  According to our new (but equivalent) description, it is obtained by drawing $n$ tickets from the new box (with replacement) and counting the number of times a value of $1$ for $Y$ is observed.  The only way this can happen is when the third type of ticket is drawn.  These occupy a fraction $pq$ of all the tickets.  This exhibits $Y$ as the sum of $n$ independent Bernoulli$(pq)$ variables, whence $Y$ has a Binomial$(n, pq)$ distribution.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-19T21:33:00.760" Id="112525" LastActivityDate="2014-08-19T21:33:00.760" OwnerUserId="919" ParentId="112516" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="113374" AnswerCount="1" Body="&lt;p&gt;I have a problem in finding negdata value. In particular multiplying with sigma.  Could someone help in  representing this equation vishid*sigma*poshidstates + visbias in matlab.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; poshidstates=poshidprobs &amp;gt; rand(numcases,numhid);&#10; negdatapart=poshidstates*vishid'; %bsxfun(@rdivide,,sigmas));&#10; negdatapart2= (sigmas)*negdatapart';&#10; negdata= bsxfun(@plus,negdatapart2,repmat(visbiases,numcases,1));&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-08-19T23:23:36.697" Id="112537" LastActivityDate="2014-08-27T03:00:29.093" LastEditDate="2014-08-21T03:46:43.913" LastEditorUserId="48880" OwnerUserId="48880" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;matlab&gt;&lt;rbm&gt;" Title="Whats going wrong in Implementation of this gaussian bernoulli RBM?" ViewCount="105" />
  <row AnswerCount="1" Body="&lt;p&gt;What kind of Machine Learning method does the function &quot;classify&quot; in Matlab use for the multi-class classification? Is it SVM? If so, how does it use the classifier for multiple classes?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-20T01:25:19.403" Id="112546" LastActivityDate="2014-08-20T16:05:48.267" OwnerUserId="54365" PostTypeId="1" Score="0" Tags="&lt;matlab&gt;" Title="Matlab classify function for multi-class classification" ViewCount="159" />
  
  <row Body="&lt;p&gt;It looks like your &lt;code&gt;identity&lt;/code&gt; variable is treated like a numerical variable instead of a factor (categorical) variable. Using your code, one way to fix this would be:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;mydata$identity &amp;lt;- as.factor(mydata$identity)&#10;aov4 &amp;lt;- aov(NO3.means ~ temperature*identity, data=mydata)&#10;summary(aov4)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-08-20T03:27:49.427" Id="112554" LastActivityDate="2014-08-20T03:27:49.427" OwnerUserId="24808" ParentId="112552" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="112699" AnswerCount="1" Body="&lt;p&gt;I have a Cox proportional hazards model. Judging by Schoenfeld residual vs. time plots and corresponding tests for zero slope, there is clear violation of the PH assumption for several of the variables (too many to stratify). &lt;/p&gt;&#10;&#10;&lt;p&gt;If we take the single-covariate case for simplicity, the original model looks like:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \hat\lambda(t) = \lambda_0(t)*\exp(X_1\hat\beta_{X_1}) $$&lt;/p&gt;&#10;&#10;&lt;p&gt;Given the violation of the PH assumption, I want to try fitting time-dependent coefficients for the problem variables. It seems intuitive to fit this:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \hat\lambda(t) = \lambda_0(t)*\exp(X_1\hat\beta_{X_1}+X_1t*\hat\beta_{X_1t}) $$&lt;/p&gt;&#10;&#10;&lt;p&gt;which simplifies to&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \hat\lambda(t) = \lambda_0(t)*\exp(X_1(\hat\beta_{X_1}+t*\hat\beta_{X_1t})) $$&lt;/p&gt;&#10;&#10;&lt;p&gt;So, effectively, this would parametrically fit a coefficient $\hat\beta^\prime_{X_1}$ that is a linear function of time:&#10;$$ \hat\beta^\prime_{X_1} = \hat\beta_{X_1}+t*\hat\beta_{X_1t} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;Before I could happily dance off into the sunset with this model, I learned it's invalid (&lt;a href=&quot;http://grokbase.com/t/r/r-help/105n8xnbr9/r-time-dependent-cox-model&quot; rel=&quot;nofollow&quot; title=&quot;Terry Therneau #1&quot;&gt;Terry Therneau #1&lt;/a&gt; and &lt;a href=&quot;http://r.789695.n4.nabble.com/Re-Survival-prediction-td4645433.html&quot; rel=&quot;nofollow&quot;&gt;#2&lt;/a&gt;). But WHY is it invalid? Why do we have to construct a dataset with start-stop times instead?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-20T03:40:54.943" Id="112555" LastActivityDate="2014-08-20T23:59:14.343" OwnerUserId="11511" PostTypeId="1" Score="2" Tags="&lt;survival&gt;&lt;cox-model&gt;&lt;assumptions&gt;" Title="What’s wrong with this way of fitting time-dependent coefficients in a Cox regression?" ViewCount="117" />
  
  
  
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I am working on a data set of about 34K rows trying to predict an ordinal response variable using R. I have tried association rules, random forest and ordinal regression. Does anyone have experience with other algorithms which are worth a try?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-20T10:41:44.100" Id="112594" LastActivityDate="2014-08-20T11:58:59.027" OwnerUserId="2753" PostTypeId="1" Score="0" Tags="&lt;classification&gt;&lt;ordinal&gt;" Title="Best algorithms for ordinal classification" ViewCount="76" />
  
  
  <row Body="&lt;p&gt;Removing correlations is a best practise (whitening), but not required.&lt;/p&gt;&#10;&#10;&lt;p&gt;Non-continuous variables however tend to yield bad results with k-means, even after whitening.  Due to the clearly cut gaps in non-continuous data, these gaps tend to dominate the k-means clustering result much more than any structure in continuous attributes.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-08-20T11:29:31.197" Id="112602" LastActivityDate="2014-08-20T11:29:31.197" OwnerUserId="7828" ParentId="112277" PostTypeId="2" Score="1" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;Suppose we have $K$ subjects and a treatment with two levels, &quot;Before&quot; and &quot;After&quot;. A paired t-test is equivalent to fitting a fixed effects ANOVA:&lt;/p&gt;&#10;&#10;&lt;p&gt;$Y = Subject + Treatment +\epsilon$&lt;/p&gt;&#10;&#10;&lt;p&gt;It is also equivalent Repeated Measures ANOVA or Mixed ANOVA, where Treatment is fixed and Subject is random. It tried all four methods for this simple dataset and the Treatment p-value is exactly the same. For the sake of this question, let us consider the fixed effects version only.&lt;/p&gt;&#10;&#10;&lt;p&gt;The total number of observations is $2K$, while the number of parameters is:&lt;/p&gt;&#10;&#10;&lt;p&gt;$ p = intercept + (K-1)$  subject effects + (2 - 1) treatment effects = $K + 1$&lt;/p&gt;&#10;&#10;&lt;p&gt;That is, there are about 2 observations per parameter for any $K$. To me it suggests that the model is likely to be very overfitted, unless there is a very substantial subject effect. &lt;/p&gt;&#10;&#10;&lt;p&gt;In practice, how often have you seen the subject effect so large that it justifies pairing the observations by subject?&lt;/p&gt;&#10;" CommentCount="12" CreationDate="2014-08-20T15:22:36.697" FavoriteCount="1" Id="112640" LastActivityDate="2014-09-04T18:20:24.127" LastEditDate="2014-09-03T18:07:10.593" LastEditorUserId="54099" OwnerUserId="54099" PostTypeId="1" Score="1" Tags="&lt;mixed-model&gt;&lt;paired-comparisons&gt;&lt;paired-data&gt;" Title="Bias-variance tradeoff in the paired t-test" ViewCount="172" />
  
  
  
  
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm working for object detection(computer vision) and have some problems in SVM training.&#10;My training configuration is as below.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Balanced training set (positive 3998/ negative 3998)&lt;/li&gt;&#10;&lt;li&gt;The dimension of my feature set is 2592&lt;/li&gt;&#10;&lt;li&gt;Training using LIBSVM with default options, except kernel function &lt;/li&gt;&#10;&lt;li&gt;Kernel function is intersection kernel, or min-kernel; &#10;$K(x,y)=\sum_i\min(x_i,y_i)$. &#10;For more information, see &#10;&lt;a href=&quot;http://ttic.uchicago.edu/~smaji/projects/fiksvm&quot; rel=&quot;nofollow&quot;&gt;ikSVM&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;After training, I plotted density of the scores(decision values) of training samples.&#10;I expected the density function is unbiased, i.e. the center of the density function is near 0.&#10;But as you see &#10;&lt;a href=&quot;http://cfile220.uf.daum.net/image/225ED94053F309C43217C5&quot; rel=&quot;nofollow&quot;&gt;figure&lt;/a&gt;, &#10;the distribution is biased to right.&lt;/p&gt;&#10;&#10;&lt;p&gt;If the distribution is shitfed to left, then the training accuracy would be 100%.&#10;I can't understand the result and want to get a proper result.&#10;Is there anyone who can comment on that?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-21T06:06:00.483" Id="112728" LastActivityDate="2014-08-21T06:13:40.730" LastEditDate="2014-08-21T06:13:40.730" LastEditorUserId="805" OwnerUserId="54451" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;svm&gt;&lt;libsvm&gt;" Title="Examine SVM result by plotting histogram of decision values of training samples" ViewCount="39" />
  
  
  
  <row Body="&lt;p&gt;Maybe some consider it old-fashioned, but if you only want to test the null hypothesis of all groups having equal success probability, then you can define&#10;$X_k$ as number of successes in group $k$, $n_k$ as number of trials in group $k$, the estimated probability in group $k$ will be $\hat{p}_k=X_k/n_k$, and then use the variance-stabilizing transformation for the binomial, which is&#10;$$&#10;    g(p) = \arcsin \sqrt(p)&#10;$$&#10;Such an approach was (at times) good enough for Fisher, so can be useful also today!&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-08-21T11:51:41.233" Id="112764" LastActivityDate="2014-08-27T12:28:01.137" LastEditDate="2014-08-27T12:28:01.137" LastEditorUserId="17230" OwnerUserId="11887" ParentId="5935" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;There is no need to use matching, as the variables you are matching on are exceedingly easy to handle as covariates.  You can easily allow for nonlinearity in age and non-additivity in age and sex by expanding age into a spline and interacting all spline terms with sex, and likewise for duration.&lt;/p&gt;&#10;&#10;&lt;p&gt;Ways to check that the matching method worked in a good and reproducible fashion include&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;randomly reorder the dataset and see if the same matches are produced&lt;/li&gt;&#10;&lt;li&gt;check that the matched ages are within one year of each other and likewise for duration of disease&lt;/li&gt;&#10;&lt;li&gt;show that the matching did not discard any observations that could have possibly matched&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;It is unlikely that all three of these conditions are satisfied, hence matching has problems.&lt;/p&gt;&#10;&#10;&lt;p&gt;To obtain the Cox model analysis I suggested above:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;require(rms)&#10;dd &amp;lt;- datadist(mydata); options(datadist='dd')&#10;f &amp;lt;- cph(Surv( ) ~ (rcs(age,5) + rcs(duration,5)) * sex + immigrant,&#10;         data=mydata)&#10;anova(f)     # all meaningful hypothesis tests&#10;summary(f)   # hazard ratios &#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="8" CreationDate="2014-08-21T12:29:11.647" Id="112767" LastActivityDate="2014-08-21T15:22:14.513" LastEditDate="2014-08-21T15:22:14.513" LastEditorUserId="4253" OwnerUserId="4253" ParentId="112662" PostTypeId="2" Score="3" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a question about Adaboost and neural networks. Given the recent development in neural networks (dropout, maxout, or rectified linear units) is there a significant benefit of performing Adaboost in order to increase accuracy?&lt;/p&gt;&#10;&#10;&lt;p&gt;I am asking the question also having in mind that in order for Adaboost to perform well you need to train multiple neural networks which is not very time efficient and if the improvement in accuracy is not substantial then maybe it is not worth it.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-21T13:13:57.427" Id="112771" LastActivityDate="2014-08-21T13:13:57.427" OwnerUserId="41850" PostTypeId="1" Score="0" Tags="&lt;neural-networks&gt;&lt;boosting&gt;" Title="Adaboost for neural networks. Is it still worth it?" ViewCount="42" />
  <row Body="&lt;p&gt;The result you claim to be true is not true in general, not even for the case when &lt;em&gt;all&lt;/em&gt; that is known is that $X_1$ and $X_2$ are normal random variables with identical variance, but the result &lt;em&gt;does&lt;/em&gt; hold for the &lt;em&gt;usual&lt;/em&gt; interpretation of the condition you stated later:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The subscripts do not indicate Order Statistics but observations from the standard normal distrubution.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;The usual interpretation of the last few words in this statement is, of course, that $X_1$ and $X_2$ are &lt;em&gt;independent&lt;/em&gt;&#10;(normal) random variables, and hence &lt;strong&gt;jointly&lt;/strong&gt; normal random variables.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;For &lt;em&gt;jointly normal&lt;/em&gt; random variables with identical variance, it is&#10;  true that $X_1+X_2$ and $X_1-X_2$ are &lt;em&gt;independent&lt;/em&gt; (normal) random&#10;  variables (with, in general, unequal variances), and the intuitive explanation for this is best given in Glen_b's answer. For &lt;em&gt;your&lt;/em&gt; special case of&#10;  $X_1$ and $X_2$ being independent as well, dobiwan's answer, which you&#10;  have accepted, is simplest, and indeed reveals that &lt;em&gt;any&lt;/em&gt; rotation of&#10;  the axes, not just by the $\pm \frac{\pi}{4}$ implicit in the transformation $(X_1,X_2)\to (X_1+X_2, X_1-X_2)$, will yield independent random variables.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;What can be said in general? In everything I say below, please bear in&#10;mind that $X$ and $Y$ have the &lt;em&gt;same variance&lt;/em&gt;, no matter what other&#10;properties might be attributed to them.&lt;/p&gt;&#10;&#10;&lt;p&gt;If $X$ and $Y$ are &lt;em&gt;any&lt;/em&gt; random variables (note: not necessarily&#10;normal) with &lt;strong&gt;identical variance,&lt;/strong&gt; then&#10;$X+Y$ and $X-Y$ are &lt;em&gt;uncorrelated&lt;/em&gt; random variables (that is, they have&#10;zero covariance). This is because&#10;the covariance function is &lt;em&gt;bilinear&lt;/em&gt;:&#10;$$\begin{align}&#10;\operatorname{cov}(X+Y, X-Y) &amp;amp;= \operatorname{cov}(X,X) - \operatorname{cov}(X,Y) + \operatorname{cov}(Y,X) - \operatorname{cov}(Y,Y)\\&#10;&amp;amp;= \operatorname{var}(X) - \operatorname{cov}(X,Y) + \operatorname{cov}(X,Y) - \operatorname{var}(Y)\\&#10;&amp;amp;= 0.&#10;\end{align}$$&#10;Here we have used the fact that $\operatorname{cov}(X,X)$ is just &#10;the variance $\operatorname{var}(X)$ of $X$ (and similarly&#10;for $Y$) and, of course,&#10;$\operatorname{cov}(Y,X) = \operatorname{cov}(X,Y)$.   Note that&#10;this result holds when $X$ and $Y$ are (marginally) normal random variables&#10;but not necessarily &lt;em&gt;jointly&lt;/em&gt; normal random variables. &#10;(If you are not familiar with this notion of marginal normality&#10;not being the same as joint normality, see &#10;&lt;a href=&quot;http://stats.stackexchange.com/a/30205/6633&quot;&gt;this great answer&lt;/a&gt; by cardinal).&#10;In the special case when $X$ and $Y$ are &lt;em&gt;jointly&lt;/em&gt;  normal&#10;(but not necessarily independent) normal random variables,&#10;so are $X+Y$ and $X-Y$ jointly normal, and since their covariance&#10;is $0$, $X+Y$ and $X-Y$ are independent random variables. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-08-21T14:14:46.667" Id="112782" LastActivityDate="2014-08-21T14:26:52.263" LastEditDate="2014-08-21T14:26:52.263" LastEditorUserId="6633" OwnerUserId="6633" ParentId="112692" PostTypeId="2" Score="6" />
  
  <row Body="&lt;p&gt;The first equality $E[\epsilon^{2}_{i}] =E[E[\epsilon^{2}_{i}|X_{i}]]$ is just the &lt;a href=&quot;http://en.wikipedia.org/wiki/Law_of_iterated_expectation&quot; rel=&quot;nofollow&quot;&gt;law of total expectation&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Recall that the &lt;a href=&quot;http://en.wikipedia.org/wiki/Variance&quot; rel=&quot;nofollow&quot;&gt;variance&lt;/a&gt; of $X$ is the expected squared deviance of $X$ from its expected value:&#10;\begin{equation}&#10;\textrm{Var}(X) = \mathbb{E}\left( \left(X - \mathbb{E}(X)\right)^2 \right).&#10;\end{equation}&#10;The &lt;a href=&quot;http://en.wikipedia.org/wiki/Conditional_variance&quot; rel=&quot;nofollow&quot;&gt;conditional variance&lt;/a&gt; is defined similarly, but now both expectations are conditional:&#10;\begin{equation}&#10;\textrm{Var}(X \mid Y) = \mathbb{E}\left[\left( X - \mathbb{E}(X \mid Y)\right)^2 \mid Y\right].&#10;\end{equation}&lt;/p&gt;&#10;&#10;&lt;p&gt;The second equality $E[\epsilon^{2}_{i}|X_{i}] = V[y_{i}|X_{i}]$ is obtained by&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Applying the aforementioned definition of conditional variance to $\textrm{Var}[y_i \mid X_i]$&lt;/li&gt;&#10;&lt;li&gt;Use the given part to express this in terms of  $\epsilon_i$.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2014-08-21T15:35:06.423" Id="112794" LastActivityDate="2014-08-21T15:58:50.803" LastEditDate="2014-08-21T15:58:50.803" LastEditorUserId="24669" OwnerUserId="24669" ParentId="112785" PostTypeId="2" Score="3" />
  
  <row AnswerCount="0" Body="&lt;p&gt;When using bootstrapping for model evaluation, I always thought the out-of-bag samples were directly used as a test set. However, this appears not to be the case for the &lt;a href=&quot;http://scikit-learn.org/0.14/modules/generated/sklearn.cross_validation.Bootstrap.html&quot;&gt;scikit-learn bootstrap function&lt;/a&gt;, which seems to build the test set from drawing with replacement from the out-of-bag data subset. What is the statistical reasoning behind this? Are there specific scenarios where this technique is better than just evaluating on the out-of-bag-sample or vice versa?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-21T16:49:52.093" FavoriteCount="1" Id="112804" LastActivityDate="2014-08-21T17:06:42.580" LastEditDate="2014-08-21T17:06:42.580" LastEditorUserId="7290" OwnerUserId="54477" PostTypeId="1" Score="7" Tags="&lt;cross-validation&gt;&lt;bootstrap&gt;&lt;random-forest&gt;&lt;scikit-learn&gt;&lt;bagging&gt;" Title="Why does the scikit-learn bootstrap function resample the test set?" ViewCount="138" />
  <row AnswerCount="0" Body="&lt;p&gt;I faced difficulty with my data analysis output. I had two factors  with split plot design. Separately, the main effect of the two factors on soil pH was significant. But their interaction effect was non significant on soil pH mean value. Thus, how this could be and how can I construct the table, separately for the two factors or both in one? &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-08-21T16:59:59.147" Id="112805" LastActivityDate="2014-08-21T17:12:23.300" LastEditDate="2014-08-21T17:12:23.300" LastEditorUserId="1739" OwnerUserId="54478" PostTypeId="1" Score="0" Tags="&lt;statistical-significance&gt;&lt;interaction&gt;" Title="main effect and interaction effect" ViewCount="35" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am using a simple UIP model to forecast exchange rates using interest rates with a twelve month horizon. The equation I use is: $E(t+12) - E(t) = α + β(I*(t) - I(t))$. I apply OLS linear regression on the in-sample period to get $α$ and $β$ and then use these values to calculate the forecast. For example, if I use data points $t=1$ to $t=24$ to get $α$ and $β$ with OLS, then $E(t=25) - E(t=13) = α + β(I*(t=13) - I(t=13))$. This gives me a forecast for $E(t=25) - E(t=13)$. Once I have a whole time series of forecasts (using an automated rolling regression, but nevermind that), I can use another linear regression to compare the predicted values with the actual values and see how well I did after the fact based on the $R^2$ value of this new regression.&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem I see with this approach is that I am using using $E(t=1)$ to $E(t=24)$ in order to forecast $E(t=25) - E(t=13)$: That's an overlap of 11 months between the forecast and the in-sample period! So I'm worried that this will make my forecasting ability look much better than it actually is. But maybe I'm not thinking about this right. Is this actually an issue or does it not affect the result at all?&lt;/p&gt;&#10;&#10;&lt;p&gt;If it really is an issue, I can get rid of the overlap by reducing the in-sample period by 12 months and forecasting $E(t=25) - E(t=13)$ using $E(t=1)$ to $E(t=12)$. However this will weaken my forecasting ability so I'd rather not. Please tell me what I should do! Many thanks in advance to anyone who can help me out.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-21T19:10:38.087" Id="112824" LastActivityDate="2014-08-22T01:45:11.987" LastEditDate="2014-08-22T01:45:11.987" LastEditorUserId="22311" OwnerUserId="54493" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;forecasting&gt;&lt;least-squares&gt;" Title="Does this kind of overlap between in-sample data and forecast cause inflated $R^2$?" ViewCount="31" />
  
  <row AnswerCount="0" Body="&lt;p&gt;If I have a model where the set of features where a cosign distance measure makes sense for some of the features, and a Euclidean distance measure makes sense for the others for example using a BOW feature set with additional features like number publications made by the author, etc., how do I pick which metric I should use or split the feature set to make more sense?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-08-21T20:52:42.977" Id="112836" LastActivityDate="2014-08-21T21:09:44.113" LastEditDate="2014-08-21T21:09:44.113" LastEditorUserId="7290" OwnerUserId="54498" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;svm&gt;&lt;distance-functions&gt;" Title="How do you deal with different distance based features?" ViewCount="20" />
  
  <row Body="&lt;p&gt;&lt;strong&gt;Spatial Regression Models&lt;/strong&gt;, by Michael D. Ward and Kristian Skrede Gleditsch is an exellent short introduction. Moran's I is explained on p.23.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Applied Spatial Data Analysis with R&lt;/strong&gt; by Bivand, Pebesma, Gomez-Rubio is a popular choice for doing spatial modelling in R.  &lt;/p&gt;&#10;&#10;&lt;p&gt;In terms packages, R , as usual, has many packages to do all sorts of spatial analysis. One package that computes Moran's I is ape&#10;For spatial kringing and the likes: gstat, akima, fields, gss&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2014-08-21T23:01:04.380" CreationDate="2014-08-21T23:01:04.380" Id="112852" LastActivityDate="2014-08-21T23:01:04.380" OwnerUserId="21967" ParentId="112795" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;I want to make a 4 question, multiple choice survey in which each question asks about an analogous range of actions for slightly different scenarios.  Each participant would only be administered the survey once.  For example:  &lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;In situation W how would you behave? (5[very aggressively] -- 1 [not aggressively]).&quot;&#10;&quot;In situation X how would you behave? (5[very aggressively] -- 1 [not aggressively]).&quot;&#10;&quot;In situation Y how would you behave? (5[very aggressively] -- 1 [not aggressively]).&quot;&#10;&quot;In situation Z how would you behave? (5[very aggressively] -- 1 [not aggressively]).&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;I then wanted to sum the responses to the 4 questions, and use that total score as my DV.&lt;/p&gt;&#10;&#10;&lt;p&gt;I plan to have 2 IVs.  &lt;/p&gt;&#10;&#10;&lt;p&gt;The first will be relationship status, and consist of 7 levels (e.g. Married, single, divorced, dating, widowed, etc).&lt;/p&gt;&#10;&#10;&lt;p&gt;The second will be, say, partner hair-color preference, and will consist of 3 levels (e.g. blond, brunet, redhead).&lt;/p&gt;&#10;&#10;&lt;p&gt;So, my understanding is that this will be a 3x7 factorial design, with a total number of 21 individual conditions, and an ordinal DV.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Ideally, I'd like to do a 3x7 factorial ANOVA to test for main effects and interaction effects, but I don't think my summed, ordinal survey response DV is amenable to this.  Is there some other analysis i could use in this situation?  Would the Kruskal-Wallis be appropriate?  Thanks!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-08-22T02:00:18.747" FavoriteCount="0" Id="112856" LastActivityDate="2015-02-14T23:36:44.057" LastEditDate="2014-08-22T17:13:50.360" LastEditorUserId="22318" OwnerUserId="22318" PostTypeId="1" Score="0" Tags="&lt;hypothesis-testing&gt;&lt;anova&gt;&lt;interaction&gt;&lt;ordinal&gt;&lt;kruskal-wallis&gt;" Title="Factorial design for interactions with ordinal DV" ViewCount="66" />
  <row AnswerCount="0" Body="&lt;p&gt;I am doing a time series regression between 2 variables. I used the dynlm library in R. I'm trying to understand how to interpret the results. &lt;/p&gt;&#10;&#10;&lt;p&gt;Could you please point out where I am getting it wrong: &lt;/p&gt;&#10;&#10;&lt;p&gt;1) The R squared seems very low -- this indicates a weak linear relationship. Or too many outliers. &#10;2) Normal QQ plot shows that there are a significant number of outliers -- at a later date in the time series? &#10;3) Does the 'Residuals vs Fitted' plot show that the model is a pretty good fit for most of the data, except for those outliers? &lt;/p&gt;&#10;&#10;&lt;p&gt;Any suggested readings (esp open-sourced materials available online) also appreciated. &lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Call:&#10;dynlm(formula = P ~ L(B))&#10;Residuals:&#10;    Min      1Q  Median      3Q     Max &#10;-63.711 -27.687 -14.907   2.364 146.157 &#10;Coefficients:&#10;             Estimate Std. Error t value Pr(&amp;gt;|t|)    &#10;(Intercept) 17.485051  13.500833   1.295    0.197     &#10;L(B)         0.019422   0.002384   8.146 5.84e-14&#10;---&#10;Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 &#10;Residual standard error: 48.17 on 182 degrees of freedom&#10;Multiple R-squared:  0.2672,    Adjusted R-squared:  0.2632 &#10;F-statistic: 66.36 on 1 and 182 DF,  p-value: 5.838e-14&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/xJbHF.png&quot; alt=&quot;enter image description here&quot;&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/6cpT2.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/JZbmX.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-22T03:06:39.450" FavoriteCount="0" Id="112860" LastActivityDate="2014-08-22T03:06:39.450" OwnerUserId="54123" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;regression&gt;&lt;time-series&gt;&lt;residuals&gt;" Title="How to interpret residual plots from time series regression" ViewCount="189" />
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;Should they be calculated based on the data or are they completely&#10;  random?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Of course based on data.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://people.sabanciuniv.edu/berrin/cs512/reading/rabiner-tutorial-on-hmm.pdf&quot; rel=&quot;nofollow&quot;&gt;This paper&lt;/a&gt; by Rabiner explains how they are calculated. The answer to your question is on equations &lt;code&gt;(40a)&lt;/code&gt;, &lt;code&gt;(40b)&lt;/code&gt;, and &lt;code&gt;(40c)&lt;/code&gt; on Pg 265. You may have to read the whole paper unto that point (9 pages) to fully udnerstand what is going on.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-08-22T07:58:18.060" Id="112871" LastActivityDate="2014-08-22T07:58:18.060" OwnerUserId="28740" ParentId="112866" PostTypeId="2" Score="0" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I estimated a Transfer Entropy value TE(XY). Now I want to establish the statistical significance of the estimated value. Therefore I used the method of shuffled surrogates to estimate a shuffled surrogate TEss(XssYss) which should be close to zero because there should be no casual relationship between the shuffled time series. This is my null hypothesis.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now I want to test the null hypothesis (of zeros TEss) by using the log-likelihood ratio and compute it in MatLab.&lt;/p&gt;&#10;&#10;&lt;p&gt;I know this is the code in Matlab I should use:&lt;/p&gt;&#10;&#10;&lt;p&gt;[h,pValue,stat,cValue] = lratiotest(uLL,rLL,dof) &lt;/p&gt;&#10;&#10;&lt;p&gt;But how do I get the unrestricted and restricted log likelihood maxima supposed TEss&#10;is my result vector of estimated TE using so shuffled time-series. And what are good model parameters?&lt;/p&gt;&#10;&#10;&lt;p&gt;I appreciate any help:)&#10;Thank you in advance&lt;/p&gt;&#10;&#10;&lt;p&gt;-klaus&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-22T12:23:32.453" FavoriteCount="0" Id="112890" LastActivityDate="2014-12-09T00:37:17.403" LastEditDate="2014-12-09T00:37:17.403" LastEditorUserId="805" OwnerUserId="54527" PostTypeId="1" Score="1" Tags="&lt;statistical-significance&gt;&lt;matlab&gt;&lt;entropy&gt;&lt;likelihood-ratio&gt;" Title="Likelihood Ratio as statistical test for Transfer Entropy with MatLab" ViewCount="56" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I have Beta as my independent variable and Economic value added (EVA) as my dependent variable. &#10;To calculate EVA I need to use Cost of capital and to calculate that I have to use Beta, so is it possible to use EVA as the dependent variable. thank you &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;EVA= Net Operating Profit After Taxes (NOPAT) - (Capital * Cost of Capital) &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="3" CreationDate="2014-08-22T13:11:46.663" Id="112899" LastActivityDate="2014-08-22T14:00:03.970" OwnerUserId="48932" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;regression&gt;&lt;time-series&gt;&lt;correlation&gt;&lt;repeated-measures&gt;" Title="Is it possible to measure the independent variable with part of the dependent variable" ViewCount="24" />
  <row Body="&lt;p&gt;Thanks for the help, all. I figured it out. &lt;/p&gt;&#10;&#10;&lt;p&gt;I first created a vector that listed EVERY unique visitID. Then, I created a vector that sampled four random IDs from that list (we'll call it 'sample') and then used the subset function:&lt;/p&gt;&#10;&#10;&lt;p&gt;newdataframe &amp;lt;- subset(entiredataframe, visitID %in% sample)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-22T14:30:09.047" Id="112908" LastActivityDate="2014-08-22T14:30:09.047" OwnerUserId="54483" ParentId="112810" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Another standard alternative is to calculate the CI with the Wilcoxon test. In R&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;wilcox.test(your-data, conf.int = TRUE, conf.level = 0.95)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Unfortunately, it gives you the CI around the (pseudo)median not the mean, but then if the data is haevily non-normal maybe the media is a more informative measure.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-22T14:30:44.297" Id="112909" LastActivityDate="2014-08-22T14:30:44.297" OwnerUserId="8226" ParentId="112829" PostTypeId="2" Score="1" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have been working with some time-series theory and I noticed something that I can understand &quot;mathematically&quot;, but not based on the intuitive explanations of what the partial auto-correlation function (PACF) is supposed to represent: The correlation between points of a given lag &lt;strong&gt;with the effects of smaller correlations removed&lt;/strong&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;For pure autoregressive time series (AR), the autocorrelation function (ACF) shows the expected slow decay while the PACF is truncated at the largest lag involved in the AR process. This all makes sense to me intuitively.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now...when we get to a pure moving average process (MA), the behaviors are &quot;reversed&quot; with the ACF being truncated and the PACF decaying. However, under the intuitive interpretation of the ACF and PACF, I cannot (inutuively) see why the PACF would show positive autocorrelations beyond the lags involved in the construction of the MA time series?&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, take an MA(2) process:&lt;/p&gt;&#10;&#10;&lt;p&gt;$Z_{t+1}=\varepsilon_{t-1} + \varepsilon+{t}\;\; [\varepsilon_i \sim \mathcal{N}(0,1)]$&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, with this, I can see why the ACF would truncate for lags &gt; 1. Howver, the PACF will slowsly decay, so the partial autocorreleation will be &lt;strong&gt;positive&lt;/strong&gt; even for lags that &lt;strong&gt;do not contribute&lt;/strong&gt; to the same $Z_t$...therefore, &lt;strong&gt;what is the PACF really telling us for an MA process&lt;/strong&gt;? It can't be telling us &quot;the relationship between the lags not accounted for by earlier lags&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;My initial thought is that since an AR and MA process are dual, the actual results of the ACF and PACF calculations are also dual, such that applying the PACF calculations to an MA process is equivalent to applying the ACF calculations to the dual AR process. Therefore, the names are really only intuitively correct for AR processes. Could this be it?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-08-22T17:35:03.203" Id="112932" LastActivityDate="2014-08-22T17:35:03.203" OwnerDisplayName="user31668" PostTypeId="1" Score="4" Tags="&lt;time-series&gt;&lt;arima&gt;&lt;autocorrelation&gt;&lt;stochastic-processes&gt;" Title="Interpretation of the partial autocorrelation function for a pure MA process" ViewCount="116" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I am trying to compare the performance of two devices designed to cool anaesthetised patients. This is a substudy analysis and the patient allocation between the groups is not even. Once cooling was started, they were cooled to 33 degrees before being re-warmed. Temperature measurements were taken once every hour. Both devices work on a negative feedback principle and I am investigating the device's ability to maintain patient temperature around the target temperature (33 degrees) for the set time period (28 hours). Below is plotted the mean time course of the groups of patients with the standard deviation. As demonstrated, the standard deviation is less for the IV33 group once cooling is established, although there were fewer patients in this group. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/REIAg.png&quot; alt=&quot;MeanSD33&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;What I am trying to show is that there is a statistically significant difference in the variance once cooling is established. To do that I have applied Levene's Test to the results from each hour which give me the following results:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Hour - p value&#10;00 - 0.8949&#10;01 - 0.8954&#10;02 - 0.2522&#10;03 - 0.01618 &#10;04 - 0.03234 &#10;05 - 0.004928 &#10;06 - 0.000227 &#10;07 - 0.0001289&#10;08 - 0.0002498 &#10;09 - 0.001403 &#10;10 - 0.001158 &#10;11 - 0.0001553 &#10;12 - 0.01084 &#10;13 - 0.0003181&#10;14 - 0.001402 &#10;15 - 0.005558 &#10;16 - 0.01849 &#10;17 - 0.001601 &#10;18 - 0.003469 &#10;19 - 0.01291 &#10;20 - 0.02297 &#10;21 - 0.09245&#10;22 - 0.02421 &#10;23 - 0.03829 &#10;24 - 0.03653 &#10;25 - 0.05466&#10;26 - 0.1282&#10;27 - 0.1982&#10;28 - 0.3297&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;As you can see, these correlate well with the graph, starting to become significant as soon as cooling takes effect. However, I am obviously applying the same test multiple times. Using a standard Bonferroni correction I would need to use a p value of around 0.0014 (36 hours) which I think is overly conservative as Bonferroni aims at some negative correlation-type dependence whereas my results are rather positively correlated. &lt;/p&gt;&#10;&#10;&lt;p&gt;Is this correct and if so, how can I calculate a more appropriate p value that takes into account the multiple testing?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-08-22T18:18:30.780" Id="112936" LastActivityDate="2014-08-22T20:52:14.093" LastEditDate="2014-08-22T20:13:10.500" LastEditorUserId="7290" OwnerUserId="45449" PostTypeId="1" Score="2" Tags="&lt;multiple-comparisons&gt;" Title="Multiple comparisons when investigating consistency of a device" ViewCount="20" />
  
  <row AcceptedAnswerId="112954" AnswerCount="1" Body="&lt;p&gt;I am doing some post-hoc comparisons (in lme4, but here I'll just present a simple linear model), and I am having a hard time making sure that I am building the right matrix of contrasts to test differences between some combinations of factors. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Here is my model:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;dataset &amp;lt;- data.frame(response=rnorm(1200), factorX=rep(c(&quot;1&quot;, &quot;2&quot;), 600), factor2=rep(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), 400))&#10;&#10;model &amp;lt;- lm(response ~ factorX*factor2,data = dataset)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In my dataset, I measured a response variable depending on two factors: factorX (which has 2 levels: 1 and 2, 1 being the reference level) and factor2 (which has 3 levels: A, B and C, A being the reference level). &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;I get this model output:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Coefficients:&#10;                  Estimate Std. Error t value Pr(&amp;gt;|t|)&#10;(Intercept)        0.02918    0.07183   0.406    0.685&#10;factorX2          -0.03780    0.10158  -0.372    0.710&#10;factor2B           0.02027    0.10158   0.200    0.842&#10;factor2C          -0.10972    0.10158  -1.080    0.280&#10;factorX2:factor2B  0.01409    0.14366   0.098    0.922&#10;factorX2:factor2C  0.08925    0.14366   0.621    0.535&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt;: &lt;/p&gt;&#10;&#10;&lt;p&gt;I want to know if there is a significant difference between some combinations of factor levels, so I build the following contrasts, and wanted to know if they corresponded to the right comparisons:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;c(0, 0, 0, 0, 0, 1)&lt;/code&gt; =&gt; factorX1_factor2A vs. factorX2_factor2C&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;c(0, 0, 1, 0, -1, 0)&lt;/code&gt; =&gt; factorX1_factor2B vs. factorX2_factor2B&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;c(0, -1, -1, 0, 0, 1)&lt;/code&gt; =&gt; factorX2_factor2B vs. factorX2_factor2C&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;c(0, 0, 0, 1, 0, 0)&lt;/code&gt; =&gt; factorX1_factor2A vs. factorX1_factor2C&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;c(0, 1, 0, 0, 0, 0)&lt;/code&gt; =&gt; factorX1_factor2A vs. factorX2_factor2A&lt;/p&gt;&#10;&#10;&lt;p&gt;Is it correct?!&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-08-22T19:18:22.977" Id="112944" LastActivityDate="2014-08-22T20:52:25.847" LastEditDate="2014-08-22T20:06:57.087" LastEditorUserId="5829" OwnerUserId="44439" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;mixed-model&gt;&lt;linear-model&gt;&lt;matrix&gt;&lt;contrasts&gt;" Title="Design of matrix of contrasts in R" ViewCount="156" />
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to fit a model in R that doesn't have censoring. In other words, I know that each event will eventually happen. There are several experimental groups with people in each group receiving surgery. However, each group receives the surgery on different dates. I only have data for who died. Hence, my data set so happens to look like:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; data.frame&#10;Group     Person   D = Day of Death since GROUP received surgery     Age&#10;1         7        5                                                   45&#10;1         8        12                                                  62&#10;1         9        85                                                  55&#10;1         10       102                                                 34&#10;2         11       3                                                   44&#10;2         12       7                                                   72&#10;2         13       25                                                  81&#10;2         14       56                                                  63&#10;2         15       78                                                  57&#10;3         16       2                                                   66&#10;3         17       15                                                  28&#10;3         18       88                                                  77&#10;3         19       150                                                 54&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The above is different than usual survival analysis data in that the days till death are dependent on the group a person belongs to. Meaning, each person within a group received the surgery on the same day, with each group's surgery date possibly different than another group's surgery date. &lt;/p&gt;&#10;&#10;&lt;p&gt;Also, what is different here is that I only have data for people who died. Hence, there isn't any censoring (I believe). &lt;/p&gt;&#10;&#10;&lt;p&gt;The Age variable is a covariates. &lt;/p&gt;&#10;&#10;&lt;p&gt;If I were to try to model the hazard function as exponential, would it make sense to do this in R?&lt;/p&gt;&#10;&#10;&lt;p&gt;Meaning, if I thought the hazard function looked like:&lt;/p&gt;&#10;&#10;&lt;p&gt;$h_i(t) = \beta_0 + \beta_1Age_i + \beta_2log(Age_i)$&lt;/p&gt;&#10;&#10;&lt;p&gt;where the log is to account for any potential logarithmic effects.&lt;/p&gt;&#10;&#10;&lt;p&gt;And if my survial function were:&lt;/p&gt;&#10;&#10;&lt;p&gt;$S(t) = e^{-h_i(t)*t}$, &lt;/p&gt;&#10;&#10;&lt;p&gt;would the following R model do just that?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;survreg(Surv(D) ~ Age+log(Age), data = data.frame,  dist =&quot;exponential&quot;)&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Would I have any troubles? I ask this because this isn't really your standard survival analysis problem. Thank you!&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-08-23T13:37:12.090" FavoriteCount="1" Id="112983" LastActivityDate="2014-08-23T13:37:12.090" OwnerUserId="54207" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;survival&gt;&lt;exponential&gt;" Title="Would this be a problem that Survival Analysis can tackle? The problem deals with localized durations and uncensored data" ViewCount="24" />
  
  
  
  
  
  
  <row Body="&lt;p&gt;From an answer on math.SE&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $Q(x) = 1 - \Phi(x)$ denote the complementary standard Gaussian distribution function, and $\phi(x)$ the standard Gaussian density function. Many&#10;scientific calculators evaluate $Q(x)$ for $x \geq 0$ via a&#10;rational function approximation:&#10;$$Q(x) \approx \phi(x)(b_1t + b_2t^2 + b_3t^3 + b_4t^4 + b_5t^5) &#10;~~ \mbox{where}~ t = \frac{1}{1 + 0.2316419x},$$&#10;$b_1 = 0.319381530$, $b_2 =  0.356563782$,&#10;$b_3 = 1.781477937,$&#10;$b_4 = 1.821255978,$ and&#10;$b_5 = 1.330274429.$  The magnitude of the error in the approximation is smaller&#10;than $7.5 \times 10^{-8}$ for all $x \geq 0$. This suffices to calculate $\Phi(z)$&#10;to the desired accuracy.&lt;/p&gt;&#10;&#10;&lt;p&gt;The formula stated above is essentially Formula 26.2.17 in &lt;a href=&quot;http://people.math.sfu.ca/~cbm/aands/page_932.htm&quot; rel=&quot;nofollow&quot;&gt;Abramowitz and Stegun&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-24T03:09:21.033" Id="113025" LastActivityDate="2014-08-24T03:09:21.033" OwnerUserId="6633" ParentId="90885" PostTypeId="2" Score="1" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I am confused by the behaviour of ks.test (package stat) a) in the presence of ties and b) if one-sided while doing a two-sample test. Documentation: &quot;Exact p-values are not available for the two-sample case if one-sided or in the presence of ties.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;I ask if black (experiment) and red (control) follow the same distribution function  without knowing the underlying distribution function.&lt;/p&gt;&#10;&#10;&lt;p&gt;In my hands exact p-values are computed if one-sided and in the presence of ties (according to the warning message). But two-sided the p-value is just &amp;lt; 2.2e-16 but not an &quot;exactly&quot; reported.&lt;/p&gt;&#10;&#10;&lt;p&gt;If interested you may download the data as .Rda (length of the vector ~ 9000):&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;https://www.dropbox.com/s/xl29jvpurkbwqpm/black.Rda?dl=0&quot; rel=&quot;nofollow&quot;&gt;https://www.dropbox.com/s/xl29jvpurkbwqpm/black.Rda?dl=0&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;https://www.dropbox.com/s/5biptm1xet36v3v/red.Rda?dl=0&quot; rel=&quot;nofollow&quot;&gt;https://www.dropbox.com/s/5biptm1xet36v3v/red.Rda?dl=0&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ks.test (black, red)&#10;&#10;    Two-sample Kolmogorov-Smirnov test&#10;&#10;data:  black and red&#10;D = 0.0731, p-value &amp;lt; 2.2e-16&#10;alternative hypothesis: two-sided&#10;&#10;ks.test (black, red)$p.value &#10;&#10;[1] 0&#10;Warnmeldung: # means warning message&#10;In ks.test(black, red) :&#10;  im Falle von Bindungen sind die p-Werte approximativ # &quot;Bindungen&quot; means ties&#10;&#10;ks.test (black, red, alternative=&quot;g&quot;)$p.value # not as expected&#10;&#10;[1] 1.235537e-23&#10;Warnmeldung:&#10;In ks.test(black, red, alternative = &quot;g&quot;) :&#10;  im Falle von Bindungen sind die p-Werte approximativ&#10;&#10;ks.test (black, red, alternative=&quot;l&quot;)$p.value&#10;&#10;[1] 0.0005651143&#10;Warnmeldung:&#10;In ks.test(black, red, alternative = &quot;l&quot;) :&#10;  im Falle von Bindungen sind die p-Werte approximativ&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I tried &lt;code&gt;ks.boot&lt;/code&gt; (package &quot;Matching&quot;) that claims to work for two.sample tests with ties and  &quot;provides correct coverage even when the distributions being compared are not entirely continuous.&quot; Same story. I get exact p-values for one-sided conditions only. For instanche:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ks.boot (black, red, alternative=&quot;l&quot;)&#10;&#10;$ks.boot.pvalue&#10;[1] 0.001&#10;&#10;$ks&#10;&#10;Two-sample Kolmogorov-Smirnov test&#10;&#10;data:  Tr and Co&#10;D^- = 0.0275, p-value = 0.0005651&#10;alternative hypothesis: the CDF of x lies below that of y&#10;&#10;$nboots&#10;[1] 1000&#10;&#10;attr(,&quot;class&quot;)&#10;[1] &quot;ks.boot&quot;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Did I missunderstand the sentence &quot;exact p-values are not available for the two-sample case if one-sided or in the presence of ties?&quot; I thought the sense was: No exact p-value if one-sided or ...&lt;/p&gt;&#10;&#10;&lt;p&gt;Are the p-values of ks.test (two.sample, one sided) &quot;correct&quot;?&lt;/p&gt;&#10;&#10;&lt;p&gt;In terms of delivering exact p-values ks.boot was not superior.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can anybody please comment on this?&#10;Thanks&#10;Hermann&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-08-23T20:50:07.290" Id="113032" LastActivityDate="2014-08-24T22:08:07.693" OwnerDisplayName="Hermann Norpois" OwnerUserId="54597" PostTypeId="1" Score="1" Tags="&lt;r&gt;" Title="ks.test and ks.boot - exact p-values and ties" ViewCount="1118" />
  
  <row Body="&lt;p&gt;I'm not clear on why you are starting a new topic on the subject, nor am I clear on the motivation for pursuing a $\sin$ link.  A binary model with a $\sin$ link is no longer a logistic model.  And there are no boundary problems with ordinary logistic regression.  You seem to find something wrong with infinite parameter estimates in order to achieve probabilities of zero or one but in fact this does not present a problem for the logistic model.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you wanted to fit a variance-stabilizing model you would use the $X\beta = \arcsin(\sqrt p)$ model, yielding $p = \sin^{2}(X\beta)$.  This model looks very appealing (information matrix = $X'X$ times a constant) but when you plot the log-likelihood surface you find it is too flat to lead to efficient estimates of $\beta$.&lt;/p&gt;&#10;" CommentCount="13" CreationDate="2014-08-24T13:24:01.613" Id="113044" LastActivityDate="2014-08-24T13:24:01.613" OwnerUserId="4253" ParentId="113043" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;The non-parametric analog of the paired $t$-test is the &lt;a href=&quot;http://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test&quot; rel=&quot;nofollow&quot;&gt;Wilcoxon&lt;/a&gt;.  &lt;/p&gt;&#10;" CommentCount="7" CreationDate="2014-08-24T15:47:19.683" Id="113054" LastActivityDate="2014-08-24T15:47:19.683" OwnerUserId="7290" ParentId="113053" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;See a simpler answer in &lt;a href=&quot;http://www.public.iastate.edu/~alicia/stat328/Multiple%20regression%20-%20nested%20models.pdf&quot; rel=&quot;nofollow&quot;&gt;this pdf&lt;/a&gt;. Essentially, a nested model is a model with less variables than a full model. One intention is to look for more parsimonious answers.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-08-24T16:31:46.273" Id="113058" LastActivityDate="2014-08-24T16:50:46.830" LastEditDate="2014-08-24T16:50:46.830" LastEditorUserId="7290" OwnerUserId="54608" ParentId="4717" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a specific regression problem of which I am not sure how to solve it. &#10;We have a dependent variable which varies depending on a certain area that is electrically stimulated in the brain. The stimulation location is represented as 3D coordinates. A multiple regression with the three coordinates (i.e. axes) as predictors yield that two axes are significantly correlated with the DV. Hence, we should be able to find a vector that explains most of the variance and is not one of the „c ardinal“ axes but somehow diagonal.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can anybody recommend any statistical methods (maybe somehow similar to PCA) to obtain this vector? I am searching for an automatic way of finding the direction that is decisive in modulating (i.e. explains most of the variance) the DV.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for any advice!&#10;Petra&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-24T18:06:33.023" Id="113065" LastActivityDate="2014-08-24T18:06:33.023" OwnerUserId="54612" PostTypeId="1" Score="1" Tags="&lt;regression&gt;" Title="Regression: Finding the vector explaining most variance in 3D space" ViewCount="42" />
  
  
  <row Body="&lt;p&gt;I hope I can help you! &lt;/p&gt;&#10;&#10;&lt;p&gt;I suggest you to look the following references to have more information about hierarchical models and hyperparameters:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Edward Greenberg - Introduction to Bayesian Econometrics, 4.6 Hierarchical Models&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Gary Koop - Bayesian Econometrics (several explainations from page 140)&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Giannone, Lenza, Primiceri - Prior Selection for Vector Autoregressions&lt;/em&gt;&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-08-24T22:05:45.410" Id="113083" LastActivityDate="2014-08-24T22:05:45.410" OwnerUserId="40899" ParentId="112273" PostTypeId="2" Score="0" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I need your help interpreting my model's results. To make story short, I have a continuous dependent variable $Y$, and two factors $A$ and $B$, each with $2$ levels $(A_1, A_2, B_1, B_2)$. My main interest is the differences between $A_1$ and $A_2$, however I am taking into account the affect of $B$. The observations are not independent, in other words, each &quot;subject&quot; contributed more than $1$ observation, therefore, I used a mixed model with $A$, $B$ and the interaction $AB$ as fixed effects, and the &quot;subject&quot; as a random effect. All $3$ fixed effects were statistically significant, A with a $\text{p -value}$ of $0.0001$, $B$ with a $\text{p -value}$ of $0.0038$ and $AB$ with a $\text{p -value}$ of $0.00032$. I asked the computer to make &quot;slicing&quot; and to test a pair, $A_1$ vs $A_2$ for $B_1$ and for $B_2$. For $B_2$ it was statistically significant, however, for $B_1$ not despite seeing a mean difference, with a $\text{p -value}$ of $0.057$...&lt;/p&gt;&#10;&#10;&lt;p&gt;So for $B_1$ the differences are not statistically significant (could be due to lack of power, I won't use bad phrases like &quot;almost significant&quot;), but overall, the factor $A$ IS statistically significant ! Now what should be my conclusion from this analysis ? What should I report and how ?&lt;/p&gt;&#10;&#10;&lt;p&gt;For your convenience I attach the two plots that the computer gave me, will help you see my problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/gtpVo.jpg&quot; alt=&quot;Interatcion plot&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/9VUuY.jpg&quot; alt=&quot;Means plot&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;All the means you see are adjusted means (what SAS, NCSS and others call least square means). Again, my &quot;goal&quot; is to look if $A_1$ is superior over $A_2$, where lower values of $Y$ are better.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you in advance.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-25T07:28:42.360" Id="113120" LastActivityDate="2014-08-25T08:23:30.723" LastEditDate="2014-08-25T08:23:30.723" LastEditorUserId="49647" OwnerUserId="54647" PostTypeId="1" Score="0" Tags="&lt;interaction&gt;&lt;interpretation&gt;" Title="Interpreting results from a mixed model with interactions" ViewCount="37" />
  <row AnswerCount="0" Body="&lt;p&gt;I have four groups of plant treated with different temperatures and I conducted repeated measurement on their growth rate.  I use ANOVA with Mixed Model to analyze the data by specifying both temperature (TEMP), time (TIME) and their interaction (Temp*Time) as fixed factor, I also include TIME as repeated factor using plant as subject. &lt;/p&gt;&#10;&#10;&lt;p&gt;I used Levene's test to test the assumption of homogeneity of variance.  For the temperature factor, the result is not significant.  However, for the Time factor, the result is significant (p=0.012).  &lt;/p&gt;&#10;&#10;&lt;p&gt;May I know do I need to have homogeneity of variance for Time as well?  Further more, do I need to do Levene's test on the interaction (Temp*Time) as well?&lt;/p&gt;&#10;&#10;&lt;p&gt;If I could not have homogeneity of variance for Time even after transformation, should I do One-Way ANOVA separately for each time point? Or should I use a non-parametric test? Or is there any other methods to analyze the data?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you so much for all the generous help!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-25T08:30:18.000" Id="113127" LastActivityDate="2014-08-25T08:30:18.000" OwnerUserId="53975" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;anova&gt;&lt;variance&gt;" Title="Homogeneity of variance for time variable" ViewCount="19" />
  <row AnswerCount="1" Body="&lt;p&gt;Since a kernel, such as Gaussian, is often used to smooth out the distribution of discrete points in 1D, 2D or 3D, I believe there must be some study materials or research work that have used this, and I need this for references in my work.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would appreciate if you can point me to any book/research papers/study materials that use a kernel to smooth out some distributions. &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="1" CommunityOwnedDate="2014-08-25T13:56:50.650" CreationDate="2014-08-25T08:37:43.220" Id="113128" LastActivityDate="2014-08-25T08:48:43.033" OwnerUserId="54620" PostTypeId="1" Score="1" Tags="&lt;distributions&gt;&lt;references&gt;&lt;kernel&gt;" Title="References to papers/books that uses a kernel to smooth a discrete distribution" ViewCount="29" />
  <row AnswerCount="0" Body="&lt;p&gt;First of all, excuse me if this is not a good place to ask this question.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can anyone explain to me how streaming kmeans algorithm implemented in Mahout works? And can it be used for anomaly detection?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-25T10:35:06.723" Id="113137" LastActivityDate="2014-08-25T12:29:36.433" LastEditDate="2014-08-25T12:29:36.433" LastEditorUserId="26338" OwnerUserId="54659" PostTypeId="1" Score="0" Tags="&lt;clustering&gt;" Title="Streaming k means clustering Mahout" ViewCount="59" />
  <row AnswerCount="0" Body="&lt;p&gt;cty year qtr tl&lt;/p&gt;&#10;&#10;&lt;p&gt;Argentina 2009 Q4 3&lt;/p&gt;&#10;&#10;&lt;p&gt;Argentina 2010 Q1 2&lt;/p&gt;&#10;&#10;&lt;p&gt;Argentina 2010 Q2 7&lt;/p&gt;&#10;&#10;&lt;p&gt;Argentina 2010 Q3 7&lt;/p&gt;&#10;&#10;&lt;p&gt;Argentina 2010 Q4 10&lt;/p&gt;&#10;&#10;&lt;p&gt;Argentina 2011 Q1 7&lt;/p&gt;&#10;&#10;&lt;p&gt;Argentina 2011 Q2 7&lt;/p&gt;&#10;&#10;&lt;p&gt;Argentina 2011 Q3 1&lt;/p&gt;&#10;&#10;&lt;p&gt;Argentina 2011 Q4 7&lt;/p&gt;&#10;&#10;&lt;p&gt;Argentina 2012 Q1 5&lt;/p&gt;&#10;&#10;&lt;p&gt;The data set has around 40 countries with each country having data for 5 years and four quarters. cty=country, year=Year, qtr Quarter, var=count of people. As I am new to R, can someone assist me with how to prepare the dataset for time series and fit a ARIMA/ARMA models etc. followed by the forecast in R? Some basic codes to prepare the data, to perform analysis and forecast for this series as an example would be helpful.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-25T10:46:50.193" Id="113138" LastActivityDate="2014-08-25T10:46:50.193" OwnerUserId="46050" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;time-series&gt;&lt;forecasting&gt;" Title="Time series Data Analysis and Forecasting by country and time factor" ViewCount="38" />
  <row AcceptedAnswerId="113144" AnswerCount="1" Body="&lt;p&gt;Suppose I have one time series and two competing models that describe it. Model 1 is ARMA$(p_1,q_1)$, model 2 is ARMA$(p_2,q_2)$-GARCH$(r,s)$. &lt;/p&gt;&#10;&#10;&lt;p&gt;I obtain AIC values of model 1 and model 2. I would like to conclude that the model with the lower AIC value is the prefered model. However, I am not sure whether I can compare AIC values coming from two different classes of models such as ARMA and ARMA-GARCH.&#10;&lt;br&gt;&#10;&lt;br&gt;&#10;My intuition is the following: if we find a model that nests both model 1 and model 2, we should be able to use AIC to compare the former two. Indeed, ARMA$(p_1,q_1)$ and ARMA$(p_2,q_2)$-GARCH$(r,s)$ are both nested within ARMA$(\max\{p_1,p_2\},\max\{q_1,q_2\})$-GARCH$(r,s)$ and may be obtained from the latter model by applying zero restrictions on coefficients.&#10;&lt;br&gt;&#10;&lt;br&gt;&#10;However, I am not sure whether my approach makes sense. Could someone please elaborate on this issue and answer the question: can we, or can we not, use AIC to compare ARMA models with ARMA-GARCH models?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-25T11:07:48.677" Id="113139" LastActivityDate="2014-08-25T12:20:53.473" LastEditDate="2014-08-25T12:20:53.473" LastEditorUserId="27581" OwnerUserId="53690" PostTypeId="1" Score="1" Tags="&lt;aic&gt;&lt;arma&gt;&lt;garch&gt;" Title="Can AIC be used to compare an ARMA model to an ARMA-GARCH model?" ViewCount="67" />
  <row Body="&lt;p&gt;You have to calculate the distance of your test samples (out sample) from the previously defined centroids (second output in Matlab kmeans function) using the same metric (see pdist) and then apply the same cutoff values on the distances obtained.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-08-25T11:34:32.223" Id="113143" LastActivityDate="2014-08-25T11:34:32.223" OwnerUserId="48968" ParentId="113142" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Yes. AIC can be used to compare an ARMA model to an ARMA-GARCH model.&lt;/p&gt;&#10;&#10;&lt;p&gt;GARCH part is adding the variance equation. Specifying GARCH does not change the dependent variable and the loglikelihood is comparable. Hence AIC.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-25T11:53:14.753" Id="113144" LastActivityDate="2014-08-25T11:53:14.753" OwnerUserId="7788" ParentId="113139" PostTypeId="2" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;I was wondering if someone could provide reference (if such exists) for the theoretical properties of empirical Bayes(EB) point estimates, in the sense of what can we say about their risk under certain regularity conditions, for general case.&lt;br&gt;&#10;For example, assume we have a prior which is structurally correct, but has an unknown hyperparameter. So using EB approach, we estimate the hyperparameter and plug it in, finally getting, let's say, the EB &quot;MMSE&quot; for the needed parameter. Can we say it'll always be better than just using the maximum likelihood (in the mse sense) ?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-25T12:22:07.347" Id="113149" LastActivityDate="2014-08-25T13:07:05.973" LastEditDate="2014-08-25T13:07:05.973" LastEditorUserId="42514" OwnerUserId="42514" PostTypeId="1" Score="0" Tags="&lt;bayesian&gt;&lt;mathematical-statistics&gt;&lt;estimation&gt;&lt;references&gt;&lt;hierarchical-bayesian&gt;" Title="General theoretical properties of empirical Bayes estimates" ViewCount="50" />
  <row AnswerCount="0" Body="&lt;p&gt;We have 7 classification models generated from 30 subsets sampled from the same data-set (via cross-validation). We would like to know if there are significant differences among the accuracy of the 7 models.&lt;/p&gt;&#10;&#10;&lt;p&gt;Apparently, in a comment from &lt;a href=&quot;http://stats.stackexchange.com/questions/17208/usage-of-the-friedman-test&quot;&gt;here&lt;/a&gt;, there is no independence assumption for the Friedman test. Is that true?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-25T13:40:39.603" Id="113156" LastActivityDate="2014-08-25T13:40:39.603" OwnerUserId="36979" PostTypeId="1" Score="0" Tags="&lt;hypothesis-testing&gt;" Title="Is it correct to apply Friedman+Nemenyi tests to several runs at the same dataset?" ViewCount="26" />
  <row Body="&lt;p&gt;If you want to model the covariance structure simply as compound symmetric, the results from-mixed effect modeling and repeated-measures ANOVA should match if the data are balanced and not missing. If you want to model the covariance structure as something else (e.g., unstructured or autoregressive), then you need to use mixed-effects modeling.&lt;/p&gt;&#10;&#10;&lt;p&gt;How do you know which covariance structure to use? First, plot the data and see is the variance/correlation noticeably changes over nights. Second, compute a covariance matrix to see whether the (co)variance remains constant over nights. Third, check the journal you are thinking of submitting your article to. Does it prefer advanced or conventional techniques? If the journal publishes only articles using ANOVAs, then you may want to stick with ANOVA as well (unless you feel comfortable justifying the use of an alternative covariance structure based on mixed-effects modeling).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-25T14:23:41.940" Id="113160" LastActivityDate="2014-08-25T14:23:41.940" OwnerUserId="53057" ParentId="32894" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;While it is true that the t-distribution takes into account the small sample size, I would assume that your referee was thinking about the difficulty of establishing that the population is normally distributed, when the only information you have is a relatively small sample?  This may not be a huge issue with a sample of size 15, since the sample hopefully is large enough to show some signs of being vaguely normally distributed?  If this is true, then &lt;em&gt;hopefully&lt;/em&gt; the population is somewhere near normal too and, combined with Central Limit Theorem, that ought to give you sample means that are well enough behaved.&lt;/p&gt;&#10;&#10;&lt;p&gt;But I'm dubious about recommendations to use t-tests for tiny samples (such as size four) unless the normality of the population can be established by some external information or mechanical understanding?  There cannot surely be anywhere near enough information in a sample of size four to have any clue as the shape of the population distribution.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-25T15:33:23.290" Id="113169" LastActivityDate="2014-08-25T15:33:23.290" OwnerUserId="54668" ParentId="37993" PostTypeId="2" Score="3" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;This is my first question!&lt;/p&gt;&#10;&#10;&lt;p&gt;My firm takes boxes of documents, preps them (removing staples, taping torn documents), and then scans the documents. I want to calculate what effect staples and torn documents have on the time it takes to prep a box of documents. &lt;/p&gt;&#10;&#10;&lt;p&gt;We measured a series of boxes for: the # of total documents, the percentage of staples (# of staples / # of paper), the percentage of torn documents (# of torn docs / # of total docs), and the time it took to prep the box. &lt;/p&gt;&#10;&#10;&lt;p&gt;The issue I'm having is- how do you treat the  percentages? (Staples%, TornDoc%) I tried calculating the Staples% * #ofDocuments to get the number of staples, but that results in a high correlation w/ #ofDocuments (multicollinearity). However, if you use it as a percentage, since each box has a completely different number of documents, it seems to me that you first have to convert all % to a common / nominal variable. &lt;/p&gt;&#10;&#10;&lt;p&gt;Please advise! &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-08-25T18:17:55.093" Id="113188" LastActivityDate="2014-08-25T18:17:55.093" OwnerUserId="54677" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;multicollinearity&gt;&lt;percentage&gt;" Title="Need to use a percentage as a independent var in regression" ViewCount="48" />
  <row Body="&lt;p&gt;It looks like underdispersion, which Efron described in a number of papers here:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://statweb.stanford.edu/~ckirby/brad/papers/&quot; rel=&quot;nofollow&quot;&gt;http://statweb.stanford.edu/~ckirby/brad/papers/&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In particular, in this paper:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://statweb.stanford.edu/~ckirby/brad/papers/2006Size.pdf&quot; rel=&quot;nofollow&quot;&gt;http://statweb.stanford.edu/~ckirby/brad/papers/2006Size.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;in Figure 1 we can see that the right histogram of z-values is too &quot;narrow&quot;, which is equivalent to saying that the distribution of p-values is not uniform because the proportion of small p-values is lower than expected. &lt;/p&gt;&#10;&#10;&lt;p&gt;In that case, you have to adjust for underdispersion by, say, using Efron's package locfdr in R.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-25T19:20:14.650" Id="113198" LastActivityDate="2014-08-25T19:20:14.650" OwnerUserId="54099" ParentId="113181" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;The LARS algorithm has the property that can detect the exact value of lambda that allows a new variable to join the active set. This does not mean that obtained lambda will be the one that minimize the cross validation. &lt;/p&gt;&#10;&#10;&lt;p&gt;Two close values of lambda can lead to the same set of active variables but the estimated regression coefficients can be slightly different.  &lt;/p&gt;&#10;&#10;&lt;p&gt;A possible approach that leads to the same solutions of LARS computed on a smaller grid is to fit a linear interpolation between the point obtained with the LARS algorithm&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-25T22:51:34.133" Id="113223" LastActivityDate="2014-08-25T22:51:34.133" OwnerUserId="25392" ParentId="108515" PostTypeId="2" Score="0" />
  
  <row AcceptedAnswerId="113564" AnswerCount="2" Body="&lt;p&gt;My question is simply, why is the &lt;a href=&quot;http://en.wikipedia.org/wiki/Geometric_median&quot; rel=&quot;nofollow&quot;&gt;geometric median&lt;/a&gt; called the $L_1$ estimator? This always reminds of $L_p$ spaces but the distance being minimized in the geometric median's definition isn't $L_1$ but rather the $L_2$ (Euclidean) norm. What does the $L_1$ refer to?&lt;/p&gt;&#10;&#10;&lt;p&gt;Is it just a misnomer/an accident/a freak of nature/something historical?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-08-26T03:01:33.767" FavoriteCount="2" Id="113239" LastActivityDate="2015-02-11T14:51:07.673" LastEditDate="2014-08-28T12:46:18.437" LastEditorUserId="28666" OwnerUserId="26352" PostTypeId="1" Score="6" Tags="&lt;estimation&gt;&lt;median&gt;&lt;point-estimation&gt;" Title="Why is the geometric median called the $L_1$ estimator?" ViewCount="225" />
  <row AnswerCount="0" Body="&lt;p&gt;The problem is defined as &#10;$$&#10;\min_{x} \Bigg\{ a{\|x\|}^2+\frac{b}{2}{\|x-c\|}^2 \Bigg\}&#10;$$&#10;where $x\in R^{n \times 1}, c \in R^{n \times 1}$ and $a,b$ are scalars.&#10;Equations 2.5 to 2.8 of &lt;a href=&quot;http://www.caam.rice.edu/~optimization/L1/GroupSparsity/group110419.pdf&quot; rel=&quot;nofollow&quot;&gt;this paper&lt;/a&gt; say this could be solved by soft-thresholding, and the $\arg \min$ is&#10;$$&#10;x ^*=\frac{c}{\|c\|_2}\max \Bigg\{ {\|c\|}_2-\frac{a}{b}, 0 \Bigg\}&#10;$$&#10;Could anyone provide some references on how to derivative this solution, please?&lt;/p&gt;&#10;&#10;&lt;p&gt;My derivation is as follows:&#10;$$&#10;ax^Tx+\frac{b}{2}(x^Tx-2x^Tc+c^Tc)=\left(a+\frac{b}{2}\right)x^Tx-bx^Tc+\frac{b}{2}c^Tc&#10;$$&#10;Then, the minimum is at the point that gradient is zero, i.e.&#10;$$&#10;(2a+b)x-bc=0&#10;$$&#10;then, $x=\frac{b}{2a+b}c$. &lt;/p&gt;&#10;&#10;&lt;p&gt;What's wrong with my derivation?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-08-26T07:32:53.490" Id="113249" LastActivityDate="2014-08-26T10:19:22.893" LastEditDate="2014-08-26T10:19:22.893" LastEditorUserId="30834" OwnerUserId="30834" PostTypeId="1" Score="0" Tags="&lt;mathematical-statistics&gt;&lt;optimization&gt;&lt;lasso&gt;&lt;signal-processing&gt;" Title="How to derive this problem with soft-thresholding method?" ViewCount="55" />
  <row AcceptedAnswerId="113275" AnswerCount="1" Body="&lt;p&gt;Let a~$\mathcal{N}(\mu_a,{\sigma_a}^2)$,b~$\mathcal{N}(\mu_b,{\sigma_b}^2)$ and c~$\mathcal{N}(\mu_c,{\sigma_c}^2)$. &lt;/p&gt;&#10;&#10;&lt;p&gt;We construct two normal variables x~$a-b$ and y~$a-c$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can we find the covariance between these two random variables i.e. $\textrm{cov}(x,y)$ by hand? Sorry if my question is trivial. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-08-26T08:36:14.763" Id="113261" LastActivityDate="2014-08-26T11:24:19.177" LastEditDate="2014-08-26T10:34:54.647" LastEditorUserId="31874" OwnerUserId="31874" PostTypeId="1" Score="1" Tags="&lt;correlation&gt;&lt;normal-distribution&gt;&lt;covariance&gt;&lt;variance-covariance&gt;" Title="Correlation between two normally distributed variables" ViewCount="73" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Could you recommend some examples of published articles that used general mixed-models or glmer() function with only crossed random effects and no random slopes.&lt;/p&gt;&#10;&#10;&lt;p&gt;Using glmer function, the model will be like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;model &amp;lt;- glmer(response ~ V1 + V2 + (1|SITE) + (1|TIME), family = binomial, data =data))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2014-08-26T09:19:18.673" Id="113264" LastActivityDate="2014-08-26T09:19:18.673" OwnerUserId="46168" PostTypeId="1" Score="0" Tags="&lt;glmm&gt;&lt;lme4&gt;&lt;cross-section&gt;" Title="References for crossed random effects" ViewCount="16" />
  <row AnswerCount="0" Body="&lt;p&gt;here is my scenario:&lt;/p&gt;&#10;&#10;&lt;p&gt;I have N triangles and N probability distributions (of independent variables), and the support of the ith probability distribution is just the ith triangle.&lt;/p&gt;&#10;&#10;&lt;p&gt;The triangles may differ in size and shape, but the idea is that the different probability distributions are kind of &quot;similar&quot; (e.g. in the sense that they all have a maximum somewhere in the center of their appropriate triangle and decrease towards the edges...)&lt;/p&gt;&#10;&#10;&lt;p&gt;Now I have a completely new triangle and I want to assign to it a probability distribution that best fits in with the distributions I already have.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, intuitively, the more a triangle i among the N given ones is similar to my new triangle, the more I would want the new distribution to be close to the ith given one.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;My first approach would be to define a metric on the set of triangles (probably Hausdorff distance), and to define the new probability distribution in terms of a weighted sum of the given distributions, where the weights depend on the 'distance' of the new triangle to the approriate given one. What remains to be done is to &quot;squash and stretch&quot; the new distribution function so as to adjust its support to the new triangle.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am not sure if this is the most generic way to handle the problem or if there is a more beautiful theory that I am not aware of.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you very much for your feedback.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-08-26T10:16:54.340" Id="113270" LastActivityDate="2014-08-26T10:16:54.340" OwnerUserId="54718" PostTypeId="1" Score="3" Tags="&lt;distributions&gt;&lt;spatial&gt;" Title="'Extrapolate' a probability density to a different support" ViewCount="48" />
  
  <row Body="&lt;p&gt;If you leave out the censored observations, your results might be very wrong. The example below follows ten people. Two died during the course of the study, and eight were alive when data collection ended (so their data are censored). The blue survival curve accounts for the censoring; the red one does not. They are very different!&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/9gmUH.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-26T14:53:01.800" Id="113298" LastActivityDate="2014-08-26T14:53:01.800" OwnerUserId="25" ParentId="113280" PostTypeId="2" Score="2" />
  
  
  
  
  <row AcceptedAnswerId="113335" AnswerCount="2" Body="&lt;p&gt;I'm Stata-proficient but learning SPSS for my new position. I am using a simple dataset to do very basic regressions and comparing to see if the results are the same. They're not. I'm close, but the magnitudes of the betas and significance are slightly different. The data was copy and pasted into each from an Excel; I didn't use a Stata file in SPSS, or vice versa. For SPSS, I did not weight it, it's using listwise deletion, and it's on the &quot;enter&quot; method. I presume Stata is doing the same, as its default (but correct me if I'm wrong and it's a different default!).&lt;/p&gt;&#10;&#10;&lt;p&gt;Any ideas on what else to check? I'm doing just a simple linear regression.&lt;/p&gt;&#10;&#10;&lt;p&gt;Data: &lt;a href=&quot;https://www.dropbox.com/s/8g31cjf8vr69i44/rwj%20county%20data.xls?dl=0&quot; rel=&quot;nofollow&quot;&gt;https://www.dropbox.com/s/8g31cjf8vr69i44/rwj%20county%20data.xls?dl=0&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;h3&gt;Syntax&lt;/h3&gt;&#10;&#10;&lt;p&gt;SPSS&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;REGRESSION&#10;/MISSING LISTWISE&#10;/STATISTICS COEFF&#10;/DEPENDENT FreeLunch&#10;/METHOD=ENTER FoodInsecure Rural Female @18 Hispanic.&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;(Or, for point and click, analyze-&gt;regression-&gt;linear; forces a choice under &quot;method&quot; for enter/stepwise/remove/backward/forward.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Stata&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;reg percentfreelunch percentfoodinsecure rural female under18 hispanic&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Data is in Excel and was pasted into both.&lt;/p&gt;&#10;&#10;&lt;h3&gt;Results&lt;/h3&gt;&#10;&#10;&lt;p&gt;SPSS&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;           Var. |   Unst.B | Std.Err. |   St.B |     t | Sig.&#10;     (Constant) | -139.616 |   66.652 | -2.095 | .045&#10;% Food Insecure |    2.785 |     .674 |   .546 | 4.131 | .000&#10;          Rural |     .131 |     .048 |   .404 | 2.701 | .011&#10;         Female |    2.657 |    1.170 |   .372 | 2.270 | .031&#10;           &amp;lt; 18 |    -.416 |     .583 |  -.145 | -.715 | .480&#10;       Hispanic |    1.156 |     .236 |  1.092 | 4.905 | .000&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Stata&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;               Var. |     Coef. | Std.Err. |     t | P&amp;gt;|t|&#10;percentfoodinsecure |   2.76532 | .6741544 |  4.10 | 0.000&#10;              rural |  .1378976 | .0495354 |  2.78 | 0.009&#10;             female |  2.826711 | 1.204272 |  2.35 | 0.026&#10;            under18 | -.3799895 |  .588423 | -0.65 | 0.523&#10;           hispanic |  1.168375 | .2398765 |  4.87 | 0.000&#10;              _cons | -149.3858 |  69.0891 | -2.16 | 0.039&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="17" CreationDate="2014-08-26T16:33:17.883" FavoriteCount="4" Id="113314" LastActivityDate="2014-08-26T21:43:08.383" LastEditDate="2014-08-26T21:33:42.683" LastEditorUserId="44269" OwnerUserId="54736" PostTypeId="1" Score="7" Tags="&lt;regression&gt;&lt;spss&gt;&lt;stata&gt;" Title="SPSS and Stata output different" ViewCount="549" />
  <row Body="&lt;p&gt;The problem that I see with your question is as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;31 is not a VERY large number of variables, at least not so large that you could not by-hand cluster similar variables into 4 or 5 latent variables using sum-scores, as you aim to do. This should give very approximately similar results to the factor analysis. If it doesn't, I would trust the by-hand scores more. The benefit of doing this is:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Scoring is done by nature of the research question, not the structure of the collected data. &lt;/li&gt;&#10;&lt;li&gt;The usual assumptions and very large &quot;p&quot; of data mining hardly apply here so the structure of the data is dubious to begin with. I am not confident that a number of &quot;orthogonal&quot; components would summarize something that school board educators would be interested in.&lt;/li&gt;&#10;&lt;li&gt;0 reproducibility error. Very easy to replicate and understand results. Could potentially benchmark and compare results between districts.&lt;/li&gt;&#10;&lt;li&gt;People reviewing such an analysis will agree that, while the measure may not be perfect, it should have good power to go about conducting a confirmatory factor analysis.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I am not advocating that you should inspect, say, a heirarchical clustering and/or heatmap or use other analyses to show the interdependence of variables, and/or that you shouldn't try to, say, run a univariate factor analysis and create latent varaible scores using &lt;em&gt;these&lt;/em&gt; as independent predictors in a regression model (note that the standard errors here aren't correct because they don't account for uncertainty in the scores). These types of analyses can help to better understand the confirmatory analysis above.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-08-26T16:33:52.390" Id="113315" LastActivityDate="2014-08-26T16:33:52.390" OwnerUserId="8013" ParentId="112464" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;It is possible to find the distribution of $Z=\frac{X}{Y}$ from first principles, where $X\sim U[0,1]$ and $Y \sim N(\mu,\sigma^2)$. Consider the cumulative probability function of $Z$:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$F_Z(z) = P(Z\le z) = P\left(\frac{X}{Y} \le z \right)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Consider the two cases $Y&amp;gt;0$ and $Y&amp;lt;0$. If $Y&amp;gt;0$, then $\frac{X}{Y}\le z\implies X \le zY $. Similarly if $Y&amp;lt;0$ then $\frac{X}{Y}\le z\implies X \ge zY $.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now we know $-\infty&amp;lt;Z&amp;lt;\infty$. To find the above probability, consider the cases $z&amp;gt;0$ and $z&amp;lt;0$.&lt;/p&gt;&#10;&#10;&lt;p&gt;If $z&amp;gt;0$, then the probability can be expressed as an integration of the joint distribution of $(X,Y)$ over the below shown region. (using the inequalities)&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/nJzYh.png&quot; alt=&quot;Integration Region&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;So &#10;$$F_Z(z) = \int_0^1 \int_{x/z}^\infty f_Y(y) dy dx + \int_0^1 \int_{-\infty}^0 f_Y(y) dy dx $$&#10;where $f_Y(y)$ is the distribution function of $Y$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Find the distribution function of $Z$ by differentiating the above.&#10;$$\begin{align*} f_Z(z) &amp;amp;= \frac{d}{dz}\int_0^1 \left[ F_Y(\infty) - F_Y\left(\frac{x}{z}\right) \right] dx \\&#10;&amp;amp;= \int_0^1 \frac{\partial}{\partial z} \left[ F_Y(\infty) - F_Y\left(\frac{x}{z}\right) \right] dx &#10;\\&#10;&amp;amp;= \int_0^1  \frac{x}{z^2} f_Y\left(\frac{x}{z}\right) dx \\&#10;&amp;amp;= \int_0^1  \frac{x}{\sqrt{2\pi}\sigma z^2} \exp \left( - \frac{\left( \frac{x}{z}-\mu\right)^2}{2\sigma^2} \right) dx&#10;\end{align*}$$ &lt;/p&gt;&#10;&#10;&lt;p&gt;The integral above can be evaluated using the following sequence of transformations:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Let $u=\frac{x}{z}$&lt;/li&gt;&#10;&lt;li&gt;Let $v=u-\mu$&lt;/li&gt;&#10;&lt;li&gt;Separate the resulting integral into two integrals, one with $v$ only in the exponential, and one with $v$ multiplying with the exponential.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;The resulting integrals can be simplified to yield&#10;$$f_Z(z) = \frac{\sigma}{\sqrt{2\pi}}\left[ \exp\left(\frac{-\mu^2}{2\sigma^2}\right)-\exp\left(\frac{-\left(\frac{1}{z}-\mu\right)^2}{2\sigma^2}\right) \right] + \mu \left[ \Phi\left(\frac{\frac{1}{z}-\mu}{\sigma}\right)-\Phi\left(\frac{-\mu}{\sigma}\right) \right]$$ &lt;/p&gt;&#10;&#10;&lt;p&gt;Here $\Phi(x)$ is the cumulative distribution function of the standard normal. An identical result is obtained for the case $z&amp;lt;0$.&lt;/p&gt;&#10;&#10;&lt;p&gt;This answer can be verified by simulation. The following script in R performs this task.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;n &amp;lt;- 1e7&#10;mu &amp;lt;- 2&#10;sigma &amp;lt;- 4&#10;&#10;X &amp;lt;- runif(n)&#10;Y &amp;lt;- rnorm(n, mean=mu, sd=sigma)&#10;&#10;Z &amp;lt;- X/Y&#10;# Constrain range of Z to allow better visualization &#10;Z &amp;lt;- Z[Z&amp;gt;-10]&#10;Z &amp;lt;- Z[Z&amp;lt;10] &#10;&#10;# The actual density &#10;hist(Z, breaks=1000, xlim=c(-10,10), prob=TRUE)&#10;&#10;# The theoretical density&#10;r &amp;lt;- seq(from=-10, to=10, by=0.01)&#10;p &amp;lt;- sigma/sqrt(2*pi)*( exp( -mu^2/(2*sigma^2)) - exp(-(1/r-mu)^2/(2*sigma^2)) ) + mu*( pnorm((1/r-mu)/sigma) - pnorm(-mu/sigma) )&#10;&#10;lines(r,p, col=&quot;red&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Here are a few graphs for verification:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;For $Y\sim N(0,1)$ &lt;img src=&quot;http://i.stack.imgur.com/rRhuF.png&quot; alt=&quot;Check 1&quot;&gt;&lt;/li&gt;&#10;&lt;li&gt;For $Y\sim N(1,1)$ &lt;img src=&quot;http://i.stack.imgur.com/j8Xk6.png&quot; alt=&quot;Check 2&quot;&gt;&lt;/li&gt;&#10;&lt;li&gt;For $y\sim N(1,2)$ &lt;img src=&quot;http://i.stack.imgur.com/K0c2T.png&quot; alt=&quot;Check 3&quot;&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;The undershooting of the theoretical answer seen in the graphs around $z=0$ is probably because of the constrained range. Otherwise the theoretical answer seems to follow the simulated density.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-08-26T16:37:44.923" Id="113317" LastActivityDate="2014-08-26T16:37:44.923" OwnerUserId="27581" ParentId="91511" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;The Mann-Whitney test first ranks all your values from high to low, computes the mean rank in each group, and then computes the probability than random shuffling of those values between two groups would end up with the mean ranks as far apart as, or further apart, than you observed. No assumptions about distributions are needed so far. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you want to make further inferences about difference between medians, you need to assume that the two populations have about the same shape distributions, so the same variances, (even if the medians are different so the two distributions are shifted from one another). &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-08-26T19:29:22.970" Id="113337" LastActivityDate="2014-08-27T01:27:46.190" LastEditDate="2014-08-27T01:27:46.190" LastEditorUserId="25" OwnerUserId="25" ParentId="113334" PostTypeId="2" Score="5" />
  
  
  <row Body="&lt;p&gt;You should probably use a linear mixed model to fit a random-slopes model:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(nlme)&#10;m1 &amp;lt;- lme(RT ~ span_num*group,&#10;          random = ~span_num|subject, data=data)&#10;summary(m1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;(You need to be careful here as I believe &lt;code&gt;summary.lme&lt;/code&gt; gets the denominator degrees of freedom wrong for random-slopes models. If you have a large (&gt;40) total number of subjects it won't matter much.)&lt;/p&gt;&#10;&#10;&lt;p&gt;or&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(lme4)&#10;m2 &amp;lt;- lmer(RT ~ span_num*group+(span_num|subject),&#10;           data=data, method=&quot;ML&quot;)&#10;drop1(m2,test=&quot;Chisq&quot;)  &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The latter case ignores the denominator DF issue entirely. &lt;code&gt;pbkrtest::KRmodcomp&lt;/code&gt; gets it right.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-26T23:21:51.640" Id="113357" LastActivityDate="2014-08-26T23:21:51.640" OwnerUserId="2126" ParentId="113251" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I think minimizing $\| Ax -y \|^2 + \lambda x ^\top A^\top Ax$ does make sense. think of it as just a particular non-diagonal covariance gaussian prior. then you can vary $\lambda$ (and cross validate) to achieve different error/feature support tradeoffs.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-08-27T00:05:49.360" Id="113360" LastActivityDate="2014-08-27T00:05:49.360" OwnerUserId="21480" ParentId="47438" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;These arguments from &lt;a href=&quot;http://www.indiana.edu/~kruschke/articles/Kruschke2013JEPG.pdf&quot; rel=&quot;nofollow&quot;&gt;&quot;Bayesian Estimation Supersedes the t-Test&quot;&lt;/a&gt; seem relevant:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;This article introduces an intuitive Bayesian approach to the analysis of data from two groups. In particular, the analysis reveals the relative credibility of every possible difference of means, every possible difference of standard deviations, and all possible effect sizes. From this explicit distribution of credible parameter values, inferences about null values can be made without ever referring to &lt;em&gt;p&lt;/em&gt; values as in null hypothesis significance testing (NHST). Unlike NHST, the Bayesian method can accept the null value, not only reject it, when certainty in the estimate is high.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Meaning, were the statistician Bayesian, she would accept the posterior as the best possible inference given the available data, assumptions, and prior knowledge. She'd then interpret the posterior as the credibility of both opposed theories.&lt;/p&gt;&#10;&#10;&lt;p&gt;If the pair could agree ahead of time on a &lt;a href=&quot;http://doingbayesiandataanalysis.blogspot.com/2013/08/how-much-of-bayesian-posterior.html&quot; rel=&quot;nofollow&quot;&gt;region of practical equivalence&lt;/a&gt;—a ROPE is basically a range of negligible differences—then only one of the three outcomes can be credible under the posterior.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;The entire ROPE falls within the 95% highest-density interval (HDI) of the posterior of difference of the parameters; the groups are practically equivalent.&lt;/li&gt;&#10;&lt;li&gt;The ROPE and HDI do not overlap; the groups meaningfully differ.&lt;/li&gt;&#10;&lt;li&gt;The two overlap; the pair must agree to disagree, or to gather more data.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2014-08-27T01:47:47.350" Id="113369" LastActivityDate="2014-08-27T01:47:47.350" OwnerUserId="28462" ParentId="112430" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;The problem with a chi-square is it ignores the ordering, leading to a loss of power. &lt;/p&gt;&#10;&#10;&lt;p&gt;One possibility: there is a &lt;a href=&quot;http://www.jstor.org/stable/2238210&quot; rel=&quot;nofollow&quot;&gt;k-group version of a Kolmogorov-Smirnov test&lt;/a&gt;$^{[1]}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Another is a k-sample Anderson-Darling test. E.g. see &lt;a href=&quot;http://en.wikipedia.org/wiki/Anderson%E2%80%93Darling_test#Non-parametric_k-sample_tests&quot; rel=&quot;nofollow&quot;&gt;Wikipedia&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;A third possibility might be to look at an orthogonal-polynomial decomposition of a chi-square (or rather the first few terms in one), which would then be taking account of the ordering. See, for example chapter 6 of Best (1999)$^{[2]}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;[1]:  Conover, W. J.  (1965),&lt;br&gt;&#10;&quot;Several k-Sample Kolmogorov-Smirnov Tests&quot;,&lt;br&gt;&#10;&lt;em&gt;The Annals of Mathematical Statistics&lt;/em&gt;, &lt;strong&gt;36&lt;/strong&gt;:3 (Jun.), pp. 1019-1026&lt;/p&gt;&#10;&#10;&lt;p&gt;[2]:  Best, D.J. (1999),&lt;br&gt;&#10;&lt;em&gt;Tests of fit and other nonparametric data analysis&lt;/em&gt;,&lt;br&gt;&#10;PhD Thesis, School of Mathematics and Applied Statistics, University of Wollongong&lt;br&gt;&#10;&lt;a href=&quot;http://ro.uow.edu.au/theses/2061&quot; rel=&quot;nofollow&quot;&gt;http://ro.uow.edu.au/theses/2061&lt;/a&gt;   (&lt;a href=&quot;http://ro.uow.edu.au/cgi/viewcontent.cgi?article=3061&amp;amp;context=theses&quot; rel=&quot;nofollow&quot;&gt;direct link&lt;/a&gt;)&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-08-27T07:51:41.957" Id="113385" LastActivityDate="2014-08-27T08:18:32.380" LastEditDate="2014-08-27T08:18:32.380" LastEditorUserId="805" OwnerUserId="805" ParentId="113002" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;Converting my comments to an answer:&lt;/p&gt;&#10;&#10;&lt;p&gt;One possibility is to work on the log-scale (since on the log scale the mean difference of pairs and the difference of unpaired means should both estimate the same location difference). &lt;/p&gt;&#10;&#10;&lt;p&gt;Since you have quadruplets of observations that share a &quot;position&quot;, you could consider either 'pairs of pair-differences' or an ANOVA, with the position as a blocking factor and test a contrast that corresponds to the comparison of ratios. The advantage of the ANOVA approach is that you can do more general things (e.g. treat the positions as a random effect rather than fixed effect, or test other form of hypothesis relatively easily).&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-08-27T09:36:10.777" Id="113391" LastActivityDate="2014-08-27T09:36:10.777" OwnerUserId="805" ParentId="113012" PostTypeId="2" Score="2" />
  <row AnswerCount="3" Body="&lt;p&gt;I do not have hypotheses for my research and I have got only categorical data. what kind of data analysis can i conduct for correlation between variables? also is chi-square used  only for hypotheses testing? can it be used for exploratory analysis? and how to test if there is moderation relationship between variables?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-08-27T10:53:31.820" Id="113394" LastActivityDate="2014-08-29T23:10:09.683" LastEditDate="2014-08-29T23:04:37.770" LastEditorUserId="805" OwnerUserId="54773" PostTypeId="1" Score="1" Tags="&lt;correlation&gt;&lt;categorical-data&gt;&lt;chi-squared&gt;&lt;interaction&gt;&lt;eda&gt;" Title="I do not have hypotheses for my research and I have got only categorical data. what kind of data analysis can i conduct? also" ViewCount="100" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm doing a linear mixed effects analysis in which I'm really only interested in one of the fixed effects. I have several other fixed effects and a random intercept term, but none of them are relevant, nor is the overall goodness-of-fit of the model (other than the fact that adding my regressor of interest (ROI) improves it significantly). &lt;/p&gt;&#10;&#10;&lt;p&gt;So, I now want to report the size of my interesting fixed effect in an intuitive way. In a simple multiple regression analysis, I would compute the partial correlation between my ROI and the outcome measure by regressing out all other factors from both the ROI and the outcome measure, and then computing the correlation between the residuals from these two &quot;nuisance regressions&quot;. Or, equivalently, taking the beta coefficient on the ROI and multiplying it by the ratio of the standard deviations of the residuals from the nuisance regressions, i.e. SD(ROI_res)/SD(Outcome_res).&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to do something similar here (with linear mixed effects), but I have trouble finding the right approach. Certainly, just &quot;copy-pasting&quot; the approach from multiple regression doesn't seem to yield sensible results (as the 2 ways of doing this which were previously equivalent now give different answers). So all of this boils down to 2 questions:&lt;/p&gt;&#10;&#10;&lt;p&gt;1) Is there a way to obtain partial correlation coefficients for fixed effects in a linear mixed model? &lt;/p&gt;&#10;&#10;&lt;p&gt;2) If not, then what would be appropriate way to report my effect of interest? Reporting the estimated fixed effect coefficient seems valid but not very insightful in terms of effect size/strength of association (as it is entirely dependent on the scale of the two variables), so my guess is there should be a better way...&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-27T13:32:26.963" Id="113419" LastActivityDate="2014-08-27T13:32:26.963" OwnerUserId="38185" PostTypeId="1" Score="1" Tags="&lt;correlation&gt;&lt;mixed-model&gt;&lt;mixed-effect&gt;&lt;reporting&gt;&lt;partial-correlation&gt;" Title="Reporting Fixed Effects as (partial) correlations?" ViewCount="68" />
  
  
  
  
  <row Body="&lt;p&gt;Have you considered using graphical solutions to show there is a correlation? Something as simple as a box plot would probably convey what you want.  For example, an illustration as simple as the following&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/pMYFy.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;would clearly show that there appears to be a correlation between opinion and gender.  These type of solutions don't give a numerical value for the correlation (although I am unaware of a correlation measure for categorical variables) but they still make the point that you want.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-08-27T16:29:09.513" Id="113460" LastActivityDate="2014-08-27T16:29:09.513" OwnerUserId="54300" ParentId="113441" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;You should proceed in two steps. (1) Data reduction and (2) Clustering.&lt;/p&gt;&#10;&#10;&lt;p&gt;For step (1), you should carefully inspect your data and determine a reasonable probability distribution for your data. You seem to have thought about this step already. The next step is to estimate the parameters of these distributions. You might fit a model separately for each unit to be clustered, or it may be appropriate to use a more sophisticated model such as a generalized linear mixed model.&lt;/p&gt;&#10;&#10;&lt;p&gt;For step (2), you can then cluster based on these parameter estimates. At this stage you should have a small number of parameter estimates per unit. As described in the answer to &lt;a href=&quot;http://stats.stackexchange.com/questions/68130/identifying-clusters-ordination-based-on-correlation-statistic/82140#82140&quot;&gt;this post&lt;/a&gt;, you can then cluster on these parameter estimates.&lt;/p&gt;&#10;&#10;&lt;p&gt;This answer is necessarily somewhat vague -- there is no &quot;canned&quot; solution here, and a great deal of statistical insight is needed for each step to select from a nearly infinite number of methods that may be relevant, depending on your unique problem. The statement of your question shows that you have self-tought yourself a good deal of statistical knowledge, which is commendable, but you still have some fundamental misunderstandings of core statistical concepts, such as the distinction between a probability distribution and observations from a probability distribution. Consider taking/auditing a mathematical statistics course or two.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-27T16:44:23.687" Id="113463" LastActivityDate="2014-08-27T16:44:23.687" OwnerUserId="28520" ParentId="13186" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="113999" AnswerCount="1" Body="&lt;p&gt;I am trying to use write my own &lt;code&gt;stochastic&lt;/code&gt; and &lt;code&gt;deterministic&lt;/code&gt; variables with &lt;code&gt;pymc3&lt;/code&gt;, but old published recipe for &lt;code&gt;pymc2.3&lt;/code&gt; explained how we can parametrize our variables no longer works. For instance I tried to use this direct approach and it failed:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;def x_logp(value, x_l, x_h):&#10;    if ((value&amp;gt;x_h) or (value&amp;lt;x_l)):&#10;        return -np.inf&#10;    else:&#10;        return -np.log(x_h-x_l+1)&#10;def x_rand(x_l,x_h):&#10;    return np.round((x_h-x_l)*np.random.random_sample())+x_l&#10;&#10;Xpos=pm.stochastic(logp=x_logp,&#10;                   doc=&quot;X position of halo center &quot;,&#10;                   observed=False, &#10;                   trace=True,&#10;                   name='Xpos',&#10;                   random=x_rand,&#10;                   value=25.32,&#10;                   parents={'x_l':0,'x_h'=500},&#10;                   dtype=float64,&#10;                   plot=True,&#10;                   verbose=0)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I got the following error message:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ERROR: AttributeError: 'module' object has no attribute 'Stochastic' [unknown]&#10;Traceback (most recent call last):&#10;  File &quot;&amp;lt;stdin&amp;gt;&quot;, line 1, in &amp;lt;module&amp;gt;&#10;AttributeError: 'module' object has no attribute 'Stochastic'&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I am wondering how could I define my own variables with &lt;code&gt;pymc3&lt;/code&gt; without using the available &lt;code&gt;pymc&lt;/code&gt; distributions?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-08-27T18:17:17.437" Id="113477" LastActivityDate="2014-09-01T19:12:28.670" LastEditDate="2014-08-27T18:28:09.347" LastEditorUserId="45941" OwnerUserId="45941" PostTypeId="1" Score="0" Tags="&lt;python&gt;&lt;mcmc&gt;&lt;pymc&gt;" Title="Defining the Stochastic and Deterministic variables with pymc3" ViewCount="223" />
  
  <row Body="&lt;p&gt;1) The time series auto-correlation you refer is the correlation between a time series and the time-shifted series; &quot;time&quot; is observed when the data are collected. In your example, auto-correlation by shifting car maker or model is not very meaningful. For new cars, shifting year (comparing year over year sales of the same type of car) makes sense, but for used cars it would be less meaningful, since the random usage the car would have being exposed to would erase correlations if there was one. I think you are fine going ahead applying the OLS technology  &lt;/p&gt;&#10;&#10;&lt;p&gt;2) You would be fitting an unbiased linear estimator, a special case of an M-estimator. If your objective is to build a predictive model (as oppose to testing hypothesis expresable in terms of model parameter), then the OSL is appropiate. To cover for the possibility un-met model assumptions, use a training to build your model, and a validation sample to assess its performance on out-of-sample cases. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-27T18:36:06.993" Id="113480" LastActivityDate="2014-08-28T02:45:41.123" LastEditDate="2014-08-28T02:45:41.123" LastEditorUserId="53211" OwnerUserId="53211" ParentId="113439" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;i am not completely sure i understand your question right... i am assuming you are interested in the order of convergence?&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;because the empirical cdf has about N parameters. Of course, asymptotically it converges to the population cdf, but what about finite samples?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Have you read any of the basics on bootstrap theory?&#10;The Problem is that it gets pretty wild (mathematically) pretty quickly.&lt;/p&gt;&#10;&#10;&lt;p&gt;Anyway, i recommend having a look at&lt;/p&gt;&#10;&#10;&lt;p&gt;van der Vaart &quot;Asymptotic Statistics&quot; chapter 23.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hall &quot;Bootstrap and Edgeworth expansions&quot; (lengthy but concise and less handwaving than van der Vaart i'd say)&lt;/p&gt;&#10;&#10;&lt;p&gt;for the basics. &lt;/p&gt;&#10;&#10;&lt;p&gt;Chernick &quot;Bootstrap Methods&quot; is more aimed at users rather than mathematicians but has a section on &quot;where bootstrap fails&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;The classical Efron/Tibshirani has little on why bootstrap actually works...&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-27T19:25:48.837" Id="113484" LastActivityDate="2014-08-27T19:25:48.837" OwnerUserId="45192" ParentId="112142" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;It depends on what exactly your weights apply to.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $\mathbf{X}$ be the data matrix with variables in columns and observation in rows. If each observation has an associated weight $w_i$, then it is indeed straightforward to incorporate these weights into PCA: one simply has to multiply each row by the corresponding $\sqrt{w_i}$ and proceed with the standard PCA (I believe that is what you are referring to, when you mention &quot;weighted covariance matrix&quot;). Note that this is mathematically equivalent to (even though conceptually different from) rescaling the variables (e.g. standardizing them), when one multiplies each &lt;em&gt;column&lt;/em&gt; of the data matrix by a certain value.&lt;/p&gt;&#10;&#10;&lt;p&gt;The paper by &lt;a href=&quot;http://arxiv.org/pdf/astro-ph/0502056v1.pdf&quot; rel=&quot;nofollow&quot;&gt;Tamuz et al., 2013&lt;/a&gt;, that you found, considers a more complicated case when different weights $w_{ij}$ are applied to each &lt;em&gt;element&lt;/em&gt; of the data matrix. Then indeed there is no analytical solution and one has to use an iterative method. Note that, as acknowledged by the authors, they reinvented the wheel, as such general weights have certainly been considered before, e.g. in &lt;a href=&quot;http://www.ma.huji.ac.il/~zamir/documents/lower.pdf&quot; rel=&quot;nofollow&quot;&gt;Gabriel and Zamir, 1979, Lower Rank Approximation of Matrices by Least Squares With Any Choice of Weights&lt;/a&gt;. This was also &lt;a href=&quot;http://stats.stackexchange.com/questions/99323&quot;&gt;discussed here&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;As an additional remark: if the weights $w_{ij}$ vary with both variables and observations, but are symmetric, so that $w_{ij}=w_{ji}$, then analytic solution is possible again, see &lt;a href=&quot;http://www.research.att.com/groups/infovis/res/legacy_papers/DBLP-journals-tvcg-KorenC04.pdf&quot; rel=&quot;nofollow&quot;&gt;Koren and Carmel, 2004, Robust Linear Dimensionality Reduction&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-08-27T20:41:26.560" Id="113488" LastActivityDate="2014-08-28T08:04:07.437" LastEditDate="2014-08-28T08:04:07.437" LastEditorUserId="28666" OwnerUserId="28666" ParentId="113485" PostTypeId="2" Score="3" />
  <row AnswerCount="1" Body="&lt;p&gt;I can't find literature to understand this table. &#10;What I know doing my research is that an effect size should be between 0 and 1. when 0.2 is slow, 0,5 medium and 0.8 and higher , high. But In an article of Cochrane database I found these results and I can't really make a sense of this. I need this really badly so anyone who can help me to understand these values (including those ranges in brackets) I'd really appreciate it.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/k4uR3.png&quot; alt=&quot;Effect size&quot;&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-08-27T22:55:17.613" Id="113501" LastActivityDate="2014-08-28T05:14:10.020" LastEditDate="2014-08-27T23:04:49.387" LastEditorUserId="26338" OwnerUserId="54829" PostTypeId="1" Score="0" Tags="&lt;effect-size&gt;" Title="How are these negative values understood?" ViewCount="24" />
  
  <row Body="&lt;p&gt;The motivation of smoothing methods is to &quot;shift&quot; the probability mass in order to provide a higher probability to unseen events.  The total probability mass remains therefore the same (it must sum to one, as for any probability distribution), so that means that the probability of seen events must be slightly reduced.  &lt;/p&gt;&#10;&#10;&lt;p&gt;That's exactly what Good-Turing discounting does: if you try to sum up the probabilities for all possible values, you'll see it sums up exactly to one -- if the calculations are tricky, it might be easier to convince yourself of this on a more basic smoothing method such as Laplace smoothing.&lt;/p&gt;&#10;&#10;&lt;p&gt;The value $N_1/N$ is the &lt;strong&gt;total&lt;/strong&gt; probability for the collection of all possible unseen events. Although it might seem like a big number at first, the total number of possible unseen n-grams is also very large (orders of magnitude larger than the number of seen n-grams), so the probability of a single unseen event is not very big, namely $\frac{N_1}{N_0 N}$, where $N_1$ is the number of n-grams that appear once, $N$ the total number of n-grams, and $N_0$ the number of unseen n-grams.  The $N_0$ value can be derived by calculating the number possible combinations under a particular vocabulary, and substracting the n-grams that were observed.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-28T00:07:33.643" Id="113506" LastActivityDate="2014-08-28T00:07:33.643" OwnerUserId="12793" ParentId="91581" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Suppose I observe binary $Y_{ij}$ for $i = 1, ..., N$ and $j = 1, ...,&#10;J$ and I want to model &#10;$$\Pr(Y_{ij} = 1 \mid \lambda_{i}) = \Phi(\lambda_{ij}), \qquad [Y_{ij} \perp Y_{ij'} \mid \lambda_i]$$ &#10;where the&#10;vector $\lambda_{i} = (\lambda_{i1}, \ldots, \lambda_{iJ})$ is a multivariate-normal random&#10;effect, $\lambda_i \sim \mathcal N(\mu, \Sigma)$ and $\Phi(\cdot)$ is the&#10;probit link function (in general this could be any link function and we would run into approximately the same issues, but probit is easier to analyze with normal random effects). $\Sigma$ is not necessarily full-rank so this specification also covers random effects models of the form $\lambda_{ij} = x_j^T b_i$ where $\dim(b_i) \ll J$. &lt;/p&gt;&#10;&#10;&lt;p&gt;My question is: under what conditions on $(\mu, \Sigma)$ are they identified? My concern is due to the fact that it is well-known that the related multivariate probit model is unidentified in the absence of further restrictions (such as requiring $\Sigma$ to be a correlation matrix). The model as written is equivalent to a multivariate probit with mean $\mu$ and variance $\mathbf I + \Sigma$ so similar concerns should apply here. &lt;/p&gt;&#10;&#10;&lt;p&gt;For example, I feel confident heuristically that setting&#10;$$&#10;\Sigma(\theta)_{jj'} = \theta_1 e^{-\theta_2|t_j - t_{j'}|}&#10;$$&#10;for known $t$ leaves $\theta_1$ unidentified. On the other hand $\Sigma = X \Sigma_b X^T$ is a standard random effects covariance matrix and so is apparently identified as long as $\Sigma_b$ has small dimension. What can one say about (say)&#10;$$&#10;\Sigma = \Sigma(\theta) + X\Sigma_b X^T&#10;$$&#10;where $\Sigma(\theta)$ is defined as above?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-28T02:54:56.283" Id="113513" LastActivityDate="2014-09-01T05:10:01.833" OwnerUserId="5339" PostTypeId="1" Score="5" Tags="&lt;generalized-linear-model&gt;&lt;binomial&gt;&lt;binary-data&gt;&lt;probit&gt;" Title="Identifiability in generalized linear random effect model?" ViewCount="61" />
  <row AnswerCount="1" Body="&lt;p&gt;Have 4 groups of results from 4 different treatments.  Want to compare pre- and post treatment within groups and between groups&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-08-28T03:17:32.307" Id="113515" LastActivityDate="2014-08-28T10:17:49.093" OwnerUserId="54834" PostTypeId="1" Score="0" Tags="&lt;frequency&gt;" Title="What test(s) can I use to compare 4 percentages for both pre- and post-treatment conditions?" ViewCount="19" />
  
  
  
  <row Body="&lt;p&gt;Following the discussion in the comments, assume that there is one random variable, and that $n=2$.  Then&lt;/p&gt;&#10;&#10;&lt;p&gt;$$Z_2 = (1-k_1 e^{a_1 X})(1-k_2 e^{a_2 X}) = 1-k_2 e^{a_2 X}-k_1 e^{a_1 X}+k_1k_2 e^{(a_1+a_2) X}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;And&lt;/p&gt;&#10;&#10;&lt;p&gt;$$Z_2^2 = (1-k_2 e^{a_2 X}-k_1 e^{a_1 X}+k_1k_2 e^{(a_1+a_2) X})(1-k_2 e^{a_2 X}-k_1 e^{a_1 X}+k_1k_2 e^{(a_1+a_2) X}) \\$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\begin{align}&amp;amp; =1-k_2 e^{a_2 X}-k_1 e^{a_1 X}+k_1k_2 e^{(a_1+a_2) X} \\&#10;&amp;amp;-k_2 e^{a_2 X}+k_2^2 e^{2a_2 X}+k_1k_2 e^{(a_1+a_2) X}-k_1k_2^2 e^{(a_1+2a_2) X}\\&#10;&amp;amp;-k_1 e^{a_1 X}+k_1k_2 e^{(a_1+a_2) X}+k_1^2 e^{2a_1 X}-k_1^2k_2 e^{(2a_1+a_2) X}\\&#10;&amp;amp;+k_1k_2 e^{(a_1+a_2) X}-k_1k_2^2 e^{(a_1+2a_2) X}-k_1^2k_2 e^{(2a_1+a_2) X} +k_1^2k_2^2 e^{(2a_1+2a_2) X}\end{align}  $$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \begin{align}\Rightarrow Z_n^2 &amp;amp;= 1-2k_2 e^{a_2 X}-2k_1 e^{a_1 X}+k_2^2 e^{2a_2 X}+ k_1^2 e^{2a_1 X}\\&#10;&amp;amp;+ 4k_1k_2 e^{(a_1+a_2) X} -2k_1^2k_2 e^{(2a_1+a_2) X}-2k_1k_2^2 e^{(a_1+2a_2) X}\\&#10;&amp;amp;+k_1^2k_2^2 e^{(2a_1+2a_2) X} \end{align}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;In general if $X\sim N(\mu,\sigma^2)$ then  $b_iX=Y \sim N(b_i\mu, b_i^2\sigma^2)$&lt;/p&gt;&#10;&#10;&lt;p&gt;Also $W=e^{X}$ is a log-normal random variable, with $E(W)=e^{\mu + \sigma^2/2}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Finally $E(cW) = cE(W)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;We see therefore that both $Z_2$ and $Z_2^2$ are just sums of scaled log-normal random variables, of which we want to calculate the expected value, nothing more. This won't change as $n$ increases, so&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\text{Var}(Z_n) = E(Z_n^2) - [E(Z_n)]^2$$ &lt;/p&gt;&#10;&#10;&lt;p&gt;will be computable as a matter of increasingly and exceedingly tedious arithmetic. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;ADDENDUM&lt;/strong&gt;&lt;br&gt;&#10;The above procedure will also work if we assume that there are many $X$'s, that are independent and jointly normal. In such a case, instead of, say, $(a_1+2a_2)X$ we will have $a_1X_1+2a_2X_2$ which will again be a normal random variable.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-28T13:15:54.153" Id="113565" LastActivityDate="2014-08-28T15:38:09.287" LastEditDate="2014-08-28T15:38:09.287" LastEditorUserId="28746" OwnerUserId="28746" ParentId="113405" PostTypeId="2" Score="4" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a neural network that i want optimize number of hidden layers and neurons in every layer using an optimization algorithm like &lt;code&gt;Imperialist competitive algorithm&lt;/code&gt;. As you know i should set limits of every variable for optimization. I can have maximum 2 layers. Besides this my system select best composition of features (among 21 features) based on cost function of my neural network (5-fold cross validation on test data) using that optimization algorithm. So my optimization algorithm has &lt;code&gt;21+2&lt;/code&gt; variables. &lt;code&gt;21&lt;/code&gt; for features and &lt;code&gt;2&lt;/code&gt; for number of neurons in every layer. What is your idea and what do you recommend for boundary of number of neurons in every layer? I'm using &lt;code&gt;patternnet&lt;/code&gt; in &lt;code&gt;MATLAB&lt;/code&gt;. As you know we have difference composition of features and neural structure in calling cost function by optimization algorithm.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-28T13:29:34.343" FavoriteCount="1" Id="113567" LastActivityDate="2014-08-28T13:29:34.343" OwnerUserId="43534" PostTypeId="1" Score="1" Tags="&lt;classification&gt;&lt;matlab&gt;&lt;optimization&gt;&lt;genetic-algorithms&gt;" Title="Optimize number of layers and neurons with an optimization algorithm" ViewCount="43" />
  <row AnswerCount="0" Body="&lt;p&gt;Everything I've read about Buhlmann-Straub credibility assumes a fixed $\theta_i$ (the unknown parameter that $X_{ij}$, the variable of interest, depends on). Does anyone know of a version where theta can vary, say with time?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-08-28T13:31:33.163" Id="113569" LastActivityDate="2014-08-28T13:34:27.587" LastEditDate="2014-08-28T13:34:27.587" LastEditorUserId="22311" OwnerUserId="46522" PostTypeId="1" Score="1" Tags="&lt;survival&gt;&lt;loss-functions&gt;" Title="Is there a version of Buhlmann-Straub credibility that uses an non-fixed $\theta_i$?" ViewCount="21" />
  
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Is it safe to do my one-hot encoding of categorical variables before splitting the data into training and test and cross validation sets? I ask because this process alters the dimension of the data, and it would be more complicated to get my test/cv sets into the same schema as the training set if they were one-hot encoded separately. My intuition tells me that it is safe.&lt;/p&gt;&#10;&#10;&lt;p&gt;For an example, let's say there's a categorical feature for marriage status- it can be Married, Single, Divorced or Widowed. If when I partition into training and test and cv sets, no Widowed samples make it into the training set, then my one-hot encoding will produce only 3 columns (Married, Single, Divorced) in the training data. Then I will need to convert my test and cv data to this format in order to test the model. Whereas if I one-hot encoded before splitting, I would have an extra column in the training set with all zeroes. As far I can tell this shouldn't affect the performance of the NN.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-28T16:45:34.847" Id="113595" LastActivityDate="2014-08-28T16:45:34.847" OwnerUserId="54868" PostTypeId="1" Score="0" Tags="&lt;categorical-data&gt;&lt;partitioning&gt;" Title="One-hot encoding before or after split into training and test sets?" ViewCount="25" />
  <row AnswerCount="0" Body="&lt;p&gt;Let us suppose that I have a number of features. I design pdfs for every feature and every class, some of them by smoothing some histogram of training samples, others just by introducing the prior knowledge on how the feature should look like.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now I want to know which is the most likely class for a given observation, based on the pdfs. I can state the following:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$P(p_i\in C_i | F_i) = \displaystyle\frac{P(F_i | p_i\in C_i ) P(p_i\in C_i)}{\displaystyle \sum_{C_j} P(F_i | p_i\in C_j ) P(p_\in C_j)}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;being $p_i$ a particular observation with feature $F_i$, $C_i$ the class I'm assigning to it.&#10;What I've doing so far is just replacing $P(\cdot|\cdot)$ by its density function $\delta$ given that $P(\cdot|\cdot)$ is about $2\epsilon \delta(\cdot|\cdot) $ for a neighborhood $[F_i-\epsilon,F_i+\epsilon]$ and I can cancel out the $2\epsilon$, so I suppose it's just legitimate to directly take the value at the density function.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, there's one feature in particular in which I want to convey that I just don't care about its value in a very wide range, for one of the classes. It's an area, so say the area can be between zero and a large value. In other words, in this class we can find any area. In another class, there is a shorter dynamic range of the values the feature should take. The problem that I find is that the result of the formula before is always lower for the 'don't care' class because the uniform distribution extends far away and hence the value of the pdf is low at every point. My desired behavior would be that since I don't care much about the area, there should be a more fair competition among the two classes, and probably some other features in the classification process will make the final decision, but this way I just say that the problematic class is very unlikely for any input of this feature.&lt;/p&gt;&#10;&#10;&lt;p&gt;I could tune $P(p\in C_i)$ to be higher for the problematic class. However, in other features this is not what I expect. Any clues on how to handle this or pose this problem?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-28T16:50:34.303" Id="113597" LastActivityDate="2014-08-28T17:27:28.020" LastEditDate="2014-08-28T17:27:28.020" LastEditorUserId="21599" OwnerUserId="54867" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;pdf&gt;&lt;kernel-density-estimate&gt;" Title="Comparing densities of a feature for different classes when the feature is irrelevant to one class" ViewCount="17" />
  
  
  
  <row AcceptedAnswerId="113625" AnswerCount="1" Body="&lt;p&gt;If I take some sequences of random numbers generated by a random number generator with uniform distribution, will the resulting sequences be uniformly distributed as well?&lt;/p&gt;&#10;&#10;&lt;p&gt;By example, if I have a generator that returns 1, 2, or 3, what are the probabilities to get [1, 1, 1] and [1, 2, 3]?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-28T19:19:53.560" Id="113621" LastActivityDate="2014-08-28T20:20:35.267" OwnerUserId="54880" PostTypeId="1" Score="2" Tags="&lt;random-generation&gt;&lt;uniform&gt;" Title="Are all sequences of of random (uniform) numbers also uniformly distributed?" ViewCount="44" />
  
  
  <row AcceptedAnswerId="115673" AnswerCount="1" Body="&lt;p&gt;I want to estimate the effect of randomly assigned intervention.  The outcome is measured at the individual level, but the individuals are assigned to groups which influence eachother a lot, and it is the groups which are assigned to treatment or control.&lt;/p&gt;&#10;&#10;&lt;p&gt;I need to test the null hypothesis that the intervention had no effect.&lt;/p&gt;&#10;&#10;&lt;p&gt;I think this is a case where I want to estimate random effects at the group level and a fixed effect for the treatment (I've used lmer before), but I'm not totally sure, and even if that's right, I'm not sure how to take the next step from there to test the null I want to test.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-29T03:09:26.603" Id="113661" LastActivityDate="2014-09-16T18:46:54.050" OwnerUserId="54897" PostTypeId="1" Score="1" Tags="&lt;mixed-model&gt;&lt;cluster-sample&gt;" Title="Cluster-randomized trial: mixed model hypothesis testing" ViewCount="47" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Suppose I aim to examine if treatment A is better than treatment B.&#10;My dataset consists of 10,000 individuals with a total of 100,000 observations. Thus each individual is observed 10 times and his covariates (blood pressure, blood lipids etc) are updated each visit. To provide more robust estimates, I use all observations in the Cox regression, by counting process syntax. I would like to adjust for propensity score as well but the problem is:&lt;/p&gt;&#10;&#10;&lt;p&gt;(1) Should the propensity score be calculated for each observation (i.e 10 times for each individual)?&lt;/p&gt;&#10;&#10;&lt;p&gt;(2) Should the propensity score be calculated once (per individual) and then held fixed?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;My second question&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I have read R D'Agostinos review (&lt;a href=&quot;http://www.stat.ubc.ca/~john/papers/DAgostinoSIM1998.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.stat.ubc.ca/~john/papers/DAgostinoSIM1998.pdf&lt;/a&gt;) on use of propensity score but after reading a paper (which investigated differences in treatment A and treatment B on risk of death), where authors stated that their covariance adjustment was done only by stratification with a propensity score; is it not &quot;better&quot; to include the covariates in the Cox regression as well, despite being included in the prop score?&lt;/p&gt;&#10;&#10;&lt;p&gt;I did not attach a data set because this is a methodological discussion.&lt;/p&gt;&#10;&#10;&lt;p&gt;Greetings&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-08-29T07:27:30.593" Id="113670" LastActivityDate="2014-08-29T09:13:06.337" LastEditDate="2014-08-29T09:13:06.337" LastEditorUserId="35413" OwnerUserId="35413" PostTypeId="1" Score="1" Tags="&lt;cox-model&gt;&lt;propensity-scores&gt;" Title="Propensity score in time-updated (counting process) Cox regression" ViewCount="83" />
  <row Body="&lt;p&gt;Since predicted and observed values cannot be negatively correlated, you do not lose any information by squaring the linear correlation coefficient here.&lt;/p&gt;&#10;&#10;&lt;p&gt;One advantage of the R-squared is its nice objective interpretation as &quot;Proportion of variance (of the response) explained by (differences in) predictors&quot;, which follows directly of its definition&#10;$$&#10;  1 - \frac{Var(e)}{Var(Y)},&#10;$$&#10;where $e$ is the vector of residuals, $Y$ the vector of observed values of the response and $Var$ is the sample variance. The numerator can be called &quot;Unexplained variance&quot;, thus the interpretation above.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2014-08-29T07:53:46.190" Id="113672" LastActivityDate="2014-08-30T14:11:54.490" LastEditDate="2014-08-30T14:11:54.490" LastEditorUserId="30351" OwnerUserId="30351" ParentId="113666" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;I recommend reading these two before doing anything. Hope they can shape your mind:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://www.oreilly.com/data/free/analyzing-the-analyzers.csp&quot; rel=&quot;nofollow&quot;&gt;analyzing the analyzers&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://www.oreilly.com/data/free/stratasurvey.csp&quot; rel=&quot;nofollow&quot;&gt;strata survey&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="2" CommunityOwnedDate="2014-08-29T08:52:11.093" CreationDate="2014-08-29T08:52:11.093" Id="113676" LastActivityDate="2014-08-29T15:55:11.497" LastEditDate="2014-08-29T15:55:11.497" LastEditorUserId="7290" OwnerUserId="41879" ParentId="30129" PostTypeId="2" Score="0" />
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I am bit confused in terms of analyzing my data. I have done an experiment which involves testing of detection systems with controlled footage.The footage has been grouped based on scene properties (or factors). &lt;/p&gt;&#10;&#10;&lt;p&gt;I have about 7 factors and each of the factors has different levels. For example, Distance factor (see below in the data example) has 3 levels (i.e. medium, close,and far), Contrast factor has 9 levels (i.e. 0.1 to 0.2, 0.2 to 0.3) and so on. &lt;/p&gt;&#10;&#10;&lt;p&gt;The analysis of system performance is done based on events that have been correctly detected (e.g. in the table: Event No. 1, 2, 3 and so on). The obtained results have scores of 1 to the correctly detected events and 0 to the un-detected events.  To estimate the consistency of experimental results, each event was repeated 10 times (e.g. event was put through the detection system 10 times). There are some small variations on the results between the repeated times - the event was not always detected or un-detected but for some events was varied (e.g. see data table SysA_Response column, which represents the proportion/average of detection for events 1, 2, 3 and so on). &lt;/p&gt;&#10;&#10;&lt;p&gt;I want to find out the most influential scene features (factor levels) that affect the performance of the detection systems. I guess i need to do factorial analysis for binary data but my problem is that my data are now proportions (because of the repeated testing of the same event). Any help in terms even for a bit of direction will be appreciated! &lt;/p&gt;&#10;&#10;&lt;p&gt;Many thanks,&#10;Ana  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/FIgUO.png&quot; alt=&quot;Example of data &quot;&gt;     &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-29T12:59:31.063" Id="113695" LastActivityDate="2014-08-29T14:07:33.120" LastEditDate="2014-08-29T13:16:22.767" LastEditorUserId="54912" OwnerUserId="54912" PostTypeId="1" Score="1" Tags="&lt;repeated-measures&gt;&lt;factor-analysis&gt;" Title="Variation of binary responses between repeated measures" ViewCount="25" />
  <row AcceptedAnswerId="113702" AnswerCount="1" Body="&lt;p&gt;I have two IVs that are highly correlated with each other at 0.979 (Pearson) &amp;amp; 0.919 (Kendall's).&lt;/p&gt;&#10;&#10;&lt;p&gt;IV1: Quality of response&lt;br&gt;&#10;IV2: Quality of Technical Advice&lt;/p&gt;&#10;&#10;&lt;p&gt;Sample Size: 252&lt;/p&gt;&#10;&#10;&lt;p&gt;Considering the similarity in the phrasing of the IVs, could it mean that the respondents do not perceive a difference between the two measurements? &lt;/p&gt;&#10;&#10;&lt;p&gt;Would it be appropriate to drop one of the items?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-08-29T13:12:22.793" Id="113697" LastActivityDate="2014-08-29T13:59:32.503" OwnerUserId="13879" PostTypeId="1" Score="2" Tags="&lt;correlation&gt;&lt;similarities&gt;&lt;predictor&gt;" Title="Drop highly correlated items?" ViewCount="59" />
  <row AnswerCount="0" Body="&lt;p&gt;I am finding my way around in the different text mining tools, but I can't find the technique that does the following:&lt;/p&gt;&#10;&#10;&lt;p&gt;Extract out of the follwing advertisement texts&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&quot;apartment with bedrooms: 2, bathrooms: 2&quot; &#10;&quot;appartment with 2 bathrooms&quot; &#10;&quot;appartment with 2 beaufitul batrooms&quot;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;that these three appartments all have 2 bathrooms.&lt;/p&gt;&#10;&#10;&lt;p&gt;The text of the advertisement is already tokenized. What are my options now? Do I have to start writing algorotimhs like &lt;em&gt;&quot;find the word bathroom and a number just before it&quot;&lt;/em&gt;? Or are there better techniques?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-08-29T13:57:56.607" Id="113701" LastActivityDate="2014-08-29T13:57:56.607" OwnerUserId="36462" PostTypeId="1" Score="0" Tags="&lt;text-mining&gt;" Title="How do I extract a keyword plus its corresponding number out of a text" ViewCount="18" />
  <row AnswerCount="0" Body="&lt;p&gt;I would like to use a greedy nearest neighbour method to do propensity score matching. Though I've little experience here, it seems that the distance measure used is generally a propensity score generated from a logistic regression. My question is: why logistic regression? Why not a random forest, SVM or another method? Is there some logic to suggest this would be unfruitful?&lt;/p&gt;&#10;&#10;&lt;p&gt;My plan is currently to use the &lt;code&gt;MatchIt&lt;/code&gt; package in R and input my own distance measure calculated off the back of a random forest (you can input your own propensity score into the argument &lt;code&gt;distance&lt;/code&gt; of the &lt;code&gt;matchit&lt;/code&gt; function).&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm deterred by the fact that modern propensity scoring packages such as&#10;&lt;a href=&quot;http://htmlpreview.github.io/?https://github.com/jbryer/PSAboot/blob/master/inst/doc/PSAboot.html&quot; rel=&quot;nofollow&quot;&gt;PSAboot&lt;/a&gt; don't have built in facility to do this &lt;em&gt;for nearest neighbour methods&lt;/em&gt;. They do use &lt;code&gt;party&lt;/code&gt; and &lt;code&gt;rpart&lt;/code&gt; but only to match using &lt;em&gt;strata&lt;/em&gt; (additionally they only use single trees). &lt;/p&gt;&#10;&#10;&lt;p&gt;I'm intrigued that methods with (typically) greater predictive power than logistic regression do not appear to be harnessed to create a 'better' propensity score for one-to-one matching. Is there someone out there that can shed some light on this?&lt;/p&gt;&#10;&#10;&lt;p&gt;Linked question: &lt;a href=&quot;http://stats.stackexchange.com/questions/73309/propensity-score&quot;&gt;Propensity Score&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-08-29T16:00:56.540" Id="113717" LastActivityDate="2014-08-29T17:15:32.777" LastEditDate="2014-08-29T17:15:32.777" LastEditorUserId="34653" OwnerUserId="34653" PostTypeId="1" Score="3" Tags="&lt;logistic&gt;&lt;random-forest&gt;&lt;matching&gt;&lt;rpart&gt;&lt;propensity-scores&gt;" Title="Propensity score matching: using alternative methods to create a distance measure" ViewCount="99" />
  <row Body="&lt;p&gt;The word order in the question might be a bit confusing. You are asked to find an expression for the event &quot;that the price of the stock is the same as today after three trading days&quot;. A more clear wording might have been &quot;the price of the stock after three trading days is the same as [the price of the stock] today.&quot; That is, the event in question is defined by stating equality of two values . These values are &quot;the price today&quot; and &quot;the price after three days&quot;. What the price is between these days, or before today, or after the third trading day, does not matter. &lt;/p&gt;&#10;&#10;&lt;p&gt;Some numeric examples (3 made up histories for the price of the stock):&#10;\begin{equation}&#10;\begin{array}{cccc|c}&#10;\textrm{Today} &amp;amp; \textrm{After 1st day} &amp;amp; \textrm{After 2nd day} &amp;amp; \textrm{After 3rd day} &amp;amp; \textrm{Comparison} \\&#10;1 &amp;amp; 1 \frac{1}{8} &amp;amp; 1 \frac{1}{4} &amp;amp; 1 \frac{3}{8} &amp;amp; 1\neq 1\frac{3}{8} \\&#10;1 &amp;amp; 1 \frac{1}{8} &amp;amp; 1 \frac{1}{4} &amp;amp; 1 &amp;amp; 1 = 1   \\&#10;1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 5  &amp;amp; 1 \neq 5 &#10;\end{array}&#10;\end{equation}&lt;/p&gt;&#10;&#10;&lt;p&gt;The event defined in part d of the question occurs if the stock price behaves according to the second row, but does not occur if the stock price behaves according to the first/third row. (Note that my example price histories in the 2nd and 3rd rows are not compatible with the process defined in the question. This is deliberate to highlight that the problem here seems to be about the interpretation of the wording, which is unrelated to the details of the process specification)&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-08-29T20:16:19.747" Id="113743" LastActivityDate="2014-08-29T20:16:19.747" OwnerUserId="24669" ParentId="113739" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;You seem to be relatively feature poor in my opinion. There are many traits which are well known to be important in Dota that you might want to include in explicitly.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;How many supports/carries does r/d have?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;There are some hero combinations that are known to be powerful (tiny+wisp comes to mind) and it might be worth including an indicators of these events.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I would suggest really thinking what you know about the data and trying to use some domain knowledge to inform feature creation. Of course make sure to have set up training/validation/test sets so that you ensure your features aren't just pretending to help predict.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-29T20:47:54.387" Id="113749" LastActivityDate="2014-08-29T20:47:54.387" OwnerUserId="52477" ParentId="113748" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;In the feedforward case, the function form is given by the following equation. $$f(a_{t-1}, a_{t-2}, .., a_{t-n}, b_{t-1}, b_{t-2},..b_{t-b}) = y_t$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The design matrix in the space of an observations by features matrix is the column-wise concatenation of the individual autoregressive design matrices. In R, this is cbind. In Python, this is hstack. In Octave, this is horzcat.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-29T21:38:13.793" Id="113752" LastActivityDate="2014-08-29T21:38:13.793" OwnerUserId="9568" ParentId="113747" PostTypeId="2" Score="0" />
  
  
  
  <row Body="&lt;p&gt;Here the answers to the three questions (in same order):&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;The independence of the random effects ensure that responses coming from different individuals are independent. (And responses from the same individual are correlated.)&lt;/li&gt;&#10;&lt;li&gt;Each individual has one single (unobserved) random effect. How to test a hypothesis with a single unobserved value?&lt;/li&gt;&#10;&lt;li&gt;Because it simplifies the numeric computations by quite some amount. There are many extensions for non-normal random effects though, e.g. Baysian hierarchical models or frailty models in survival analysis.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="3" CreationDate="2014-08-30T09:58:14.530" Id="113782" LastActivityDate="2014-08-30T09:58:14.530" OwnerUserId="30351" ParentId="113769" PostTypeId="2" Score="2" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Suppose that we are searching for best &lt;code&gt;features&lt;/code&gt; using an optimization algorithm for a classification model (MLP,SNM,Regression,etc...). We should set a cost function for this propose. This is one of the well-known cost functions:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;NN = Selected_f&#10;Z = MSE(1+beta*NN)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;We should set &lt;code&gt;beta&lt;/code&gt; in this equations. What &lt;code&gt;beta&lt;/code&gt; value (between 0 and 1) do you think is suitable? I know this depends on problem and data set that I'm using, but as you know likes optimization algorithm parameters, maybe we have some proposed value for this. ( we don't know anything about how many features is good for this problem - we have 21 features)&lt;/p&gt;&#10;&#10;&lt;p&gt;Can we only use &lt;code&gt;MSE&lt;/code&gt; for cost function?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-30T12:23:56.653" Id="113788" LastActivityDate="2014-08-30T12:23:56.653" OwnerUserId="43534" PostTypeId="1" Score="0" Tags="&lt;classification&gt;&lt;feature-selection&gt;&lt;optimization&gt;&lt;genetic-algorithms&gt;&lt;cost-maximization&gt;" Title="Feature selection based on cost function" ViewCount="19" />
  <row Body="&lt;p&gt;I would put out there the notion of &quot;soft skills&quot;.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;recognising who the &quot;expert&quot; is for method X, and being able to tap into their knowledge (you shouldn't be able to or expected to know everything about erything).  The ability and willingness to collaborate with others.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;the ability to translate or represent &quot;the real world&quot; with the mathematics used in ML.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;the ability to explain your methods in different ways to different audiences - knowing when to focus on details and when to step back and view the wider context.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;systems thinking, being able to see how your role feeds into other areas of the business, and how these areas feed back into your work.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;an appreciation and understanding of uncertainty, and having some structured methods to deal with it.  Being able to state clearly what your assumptions are.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2014-08-30T12:54:06.027" Id="113791" LastActivityDate="2014-08-30T12:54:06.027" OwnerUserId="2392" ParentId="104500" PostTypeId="2" Score="2" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Is there a use of using a Randomized Block design when you only have one treatment and you only want to test the effects of this single treatment?&lt;/p&gt;&#10;&#10;&lt;p&gt;The experiment in question is to see whether a training exercise (the treatment) effects the grades of a number of students. Students are grouped in a control and experimental group and are subject to a test. The experimental group are then given training and then asked to re take the exam.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-30T17:23:35.917" Id="113812" LastActivityDate="2014-08-30T18:08:43.187" OwnerUserId="54967" PostTypeId="1" Score="0" Tags="&lt;experiment-design&gt;" Title="Experimental design considerations for Randomized Block design" ViewCount="26" />
  
  
  
  <row AcceptedAnswerId="113830" AnswerCount="1" Body="&lt;p&gt;I am reviewing a paper where I found this sentence repeatedly:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;A is statistically insignificantly worse than B&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I consider this to be confusing since, for me at least, it is not immediately clear if it is the same as:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;A is not statistically significantly worse than B&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Maybe the authors are trying to convey the fact that the data for A are worse, but that this is not statistically significant. If this is the case though, it is not clear whether they mean that the sample average is worse or sample median is worse or something else.&lt;/p&gt;&#10;&#10;&lt;p&gt;Should I recommend that they reformulate this sentence or is it common practice to say &quot;statistically insignificantly worse&quot;?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-08-30T20:02:24.950" Id="113829" LastActivityDate="2014-08-31T00:39:42.540" LastEditDate="2014-08-31T00:39:42.540" LastEditorUserId="28666" OwnerUserId="28978" PostTypeId="1" Score="6" Tags="&lt;hypothesis-testing&gt;&lt;statistical-significance&gt;" Title="What does &quot;statistically insignificantly worse&quot; mean?" ViewCount="166" />
  <row Body="&lt;p&gt;The case of &quot;attenuation bias&quot; can be more clearly presented if we examine the &quot;probit&quot; model -but the result carry over to the logistic regression also.&lt;/p&gt;&#10;&#10;&lt;p&gt;Underneath the Conditional Probability Models (Logistic (logit), &quot;probit&quot;, and &quot;Linear Probability&quot; models) we can postulate a &lt;em&gt;latent&lt;/em&gt; (unobservable) linear regression model:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$y^* = X\beta + u$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $y^*$ is a continuous unobservable variable (and $X$ is the regressor matrix). The error term is assumed to be &lt;em&gt;independent&lt;/em&gt; from the regressors, and to follow a distribution that has &lt;em&gt;a density symmetric around zero&lt;/em&gt;, and in our case,  the standard normal distribution  $F_U(u)= \Phi(u)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;We assume that what we observe, i.e. the binary variable $y$, is an Indicator function of the unobservable $y^*$:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ y = 1 \;\;\text{if} \;\;y^*&amp;gt;0,\qquad y = 0 \;\;\text{if}\;\; y^*\le 0$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Then we ask &quot;what is the probability that $y$ will take the value $1$ given the regressors?&quot; (i.e. we are looking at a conditional probability). This is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$P(y =1\mid X ) = P(y^*&amp;gt;0\mid X) = P(X\beta + u&amp;gt;0\mid X) = P(u&amp;gt; - X\beta\mid X) \\= 1- \Phi (-Χ\beta) = \Phi (X\beta) $$&lt;/p&gt;&#10;&#10;&lt;p&gt;the last equality due to the &quot;reflective&quot; property of the standard cumulative distribution function, which comes from the symmetry of the density function around zero. Note that although we have assumed that $u$ is independent of $X$, conditioning on $X$ is needed in order to treat the quantity $X\beta$ as non-random.&lt;/p&gt;&#10;&#10;&lt;p&gt;If we assume that $X\beta = b_0+b_1X_1 + b_2X_2$, then we obtain the theoretical model&lt;/p&gt;&#10;&#10;&lt;p&gt;$$P(y =1\mid X ) = \Phi (b_0+b_1X_1 + b_2X_2) \tag{1}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Let now $X_2$ be independent of $X_1$ and erroneously excluded from the specification of the underlying regression. So we  specify&lt;/p&gt;&#10;&#10;&lt;p&gt;$$y^* = b_0+b_1X_1 + \epsilon$$&#10;Assume further that $X_2$ is also a normal random variable $X_2 \sim N(\mu_2,\sigma_2^2)$. But this means that&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\epsilon = u + b_2X_2  \sim N(b_2\mu_2, 1+b_2^2\sigma_2^2)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;due to the closure-under-addition of the normal distribution (and the independence assumption). Applying the same logic as before, here we have&lt;/p&gt;&#10;&#10;&lt;p&gt;$$P(y =1\mid X_1 ) = P(y^*&amp;gt;0\mid X_1) = P(b_0+b_1X_1 + \epsilon&amp;gt;0\mid X_1) = P(\epsilon&amp;gt; - b_0-b_1X_1\mid X_1) $$&lt;/p&gt;&#10;&#10;&lt;p&gt;Standardizing the $\epsilon$ variable we have&lt;/p&gt;&#10;&#10;&lt;p&gt;$$P(y =1\mid X_1 )= 1- P\left(\frac{\epsilon-b_2\mu_2}{\sqrt {1+b_2^2\sigma_2^2}}\leq - \frac {(b_0 + b_2\mu_2)}{\sqrt {1+b_2^2\sigma_2^2}}- \frac {b_1}{\sqrt {1+b_2^2\sigma_2^2}}X_1\mid X_1\right)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\Rightarrow  P(y =1\mid X_1) = \Phi\left(\frac {(b_0 + b_2\mu_2)}{\sqrt {1+b_2^2\sigma_2^2}}+ \frac {b_1}{\sqrt {1+b_2^2\sigma_2^2}}X_1\right) \tag{2}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and one can compare models $(1)$ and $(2)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The above theoretical expression, tells us &lt;em&gt;where&lt;/em&gt; our maximum likelihood estimator of $b_1$ is going to converge, since it remains a consistent estimator, in the sense that &lt;em&gt;it will converge to the theoretical quantity that really exists in the model&lt;/em&gt; (and of course, not in the sense that it will find the &quot;truth&quot; in any case):&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\hat b_1 \xrightarrow{p} \frac {b_1}{\sqrt {1+b_2^2\sigma_2^2}} &amp;lt; |b_1|$$&lt;/p&gt;&#10;&#10;&lt;p&gt;which is the &quot;bias towards zero&quot; result.  &lt;/p&gt;&#10;&#10;&lt;p&gt;We used the probit model, and not the logit (logistic regression), because only under normality can we derive the distribution of $\epsilon$. The logistic distribution is not closed under addition. This means that if we omit a relevant variable in logistic regression, we also create distributional misspecification, because the error term (that now includes the omitted variable) no longer follows a logistic distribution. But this does not change the bias result (see footnote 6 in the paper linked to by the OP).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-30T23:02:51.437" Id="113832" LastActivityDate="2014-08-30T23:02:51.437" OwnerUserId="28746" ParentId="113766" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;You can try Random Forest. You can have categorical variables with up to 32 distinct values. It is an ensemble method,a quick and relatively precise way for prediction.&lt;/p&gt;&#10;&#10;&lt;p&gt;if you are comfortable with R, I recommend using Rattle GUI. You can install it like any other package. In Rattle you can do data mining in a point and click way and get the code afterwards, so you do not have to worry about spending too much time on different packages.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can try many of the algorithms including Random Forest there.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-31T06:58:12.203" Id="113847" LastActivityDate="2014-08-31T06:58:12.203" OwnerUserId="41879" ParentId="93474" PostTypeId="2" Score="1" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm working on a small project in which I try to compare directed a-cyclic graphs. Say I have (directed) three graphs:&lt;/p&gt;&#10;&#10;&lt;p&gt;1)&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;                    X &#10;                  /   \&#10;                 /     \&#10;START - X - X - X - X - END&#10;         \       &#10;          \   &#10;            X  &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;2) &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;         X - X - X - X&#10;       /              \&#10;      /                \&#10;START -  X - X - X - X - END&#10;      \                /&#10;       \              /&#10;         X - X - X - X &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;3)&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;                    X - X           X - X - X &#10;                  /               /          \&#10;                 /               /            \&#10;START - X - X - X - X - X - X - X - X - X - X - END&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Note, that the labels (X) are of no importance; I'm only interested in the structural properties and similarities between the graphs. Do you have any pointers to literature or any clever ideas how to compute the structural distance or similarity between such graphs?&lt;/p&gt;&#10;&#10;&lt;p&gt;Many thanks!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-08-31T10:16:17.030" Id="113855" LastActivityDate="2015-03-03T15:48:09.740" OwnerUserId="30206" PostTypeId="1" Score="2" Tags="&lt;distance&gt;&lt;metric&gt;&lt;dag&gt;" Title="Metric to compute structural similarity between two directed graphs" ViewCount="83" />
  <row AnswerCount="3" Body="&lt;p&gt;I am a soon-to-be physician. During my studies I have taken a class in biostatistics. I own Martin Bland's &quot;An introduction to medical statistics&quot;, which was a required textbooks at the time, and Harvey Motulsky's &quot;Intuitive biostatistics&quot;, which I've purchased on my own initiative based on positive reviews on Amazon. They served me well, but I'm interested in clinical research and I feel that I need to stop scratching the surface and delve deeper to understand statistics. &lt;/p&gt;&#10;&#10;&lt;p&gt;I'm looking for one or more textbooks I can use to self study. Ideally the textbook(s) should go beyond the basics, and explain modern statical techniques used in biomedical research (e.g reading these forums I've seen a lot of mentions of bayesian statistics, which seems to be the Next Big Thing). If possible, it should feature examples, and not assume a particular statistical package (unless it is R, that is the software I use). &lt;/p&gt;&#10;&#10;&lt;p&gt;I should probably mentions that my math skills are... rusty. So feel free to list the kind of math skills required to understand the book(s) you are suggesting.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-08-31T11:15:09.027" Id="113857" LastActivityDate="2014-09-01T13:23:42.027" LastEditDate="2014-09-01T02:11:38.150" LastEditorUserId="805" OwnerUserId="53356" PostTypeId="1" Score="2" Tags="&lt;self-study&gt;&lt;references&gt;&lt;biostatistics&gt;" Title="Beyond the basics: intermediate medical statistics textbooks suggestions" ViewCount="112" />
  <row Body="&lt;p&gt;Clustering is not the appropriate method to &lt;em&gt;test&lt;/em&gt; an hypothesis. Clustering is a way to &lt;em&gt;find&lt;/em&gt; hypotheses to study and test carefully afterwards. Clustering is &lt;em&gt;exploratory&lt;/em&gt; and a &lt;em&gt;discovery&lt;/em&gt; technique.&lt;/p&gt;&#10;&#10;&lt;p&gt;The reason is that clustering is unsupervised. It will just try to find some interesting hypothesis; but that does &lt;em&gt;not&lt;/em&gt; imply that hypotheses not found by clustering aren't sound. Any clustering method makes some assumptions on the hypotheses to be found (e.g. least variance partitionings, as found by k-means). &lt;strong&gt;At most you can measure how similar your hypothesis is to the assumptions made by a particular algorithm.&lt;/strong&gt; e.g. if your result is similar to those found by k-means, it means your result has a low variance.&lt;/p&gt;&#10;&#10;&lt;p&gt;What you essentially want is to see if your hypothesis is a good &lt;em&gt;predictor&lt;/em&gt;, isn't it? Treat it as a &lt;strong&gt;classification problem&lt;/strong&gt;, which are much easier to evaluate (e.g. cross-validation, leave-one-out, etc.)&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-08-31T11:52:52.557" Id="113861" LastActivityDate="2014-08-31T11:52:52.557" OwnerUserId="7828" ParentId="113669" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;when building a kernel function for a Gaussian-Process-Regression I am asking myself whether the kernel function is allowed to contain information from the measurements.&lt;/p&gt;&#10;&#10;&lt;p&gt;To ask a little more general, how important is it to prove Mercer's conditions for a kernel?&lt;/p&gt;&#10;&#10;&lt;p&gt;Best regards&#10;Damian&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-31T11:54:58.467" Id="113862" LastActivityDate="2014-08-31T11:54:58.467" OwnerUserId="48032" PostTypeId="1" Score="0" Tags="&lt;kernel&gt;&lt;gaussian-process&gt;" Title="Can a kernel function for GP-regression use measurement information?" ViewCount="8" />
  
  <row Body="&lt;p&gt;Sensitivity and specificity are in backwards time order (i.e., use reverse conditioning or Prob(X|Y)).  Hence they are not relevant to decision making.  I recommend developing a well-calibrating direct probability model then using standard decision theory.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-08-31T13:04:40.527" Id="113865" LastActivityDate="2014-08-31T13:04:40.527" OwnerUserId="4253" ParentId="113850" PostTypeId="2" Score="3" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm using optimization algorithm to find best structure+inputs of a &lt;code&gt;patternnet&lt;/code&gt; neural network in &lt;code&gt;MATLAB R2014a&lt;/code&gt; using &lt;code&gt;10-fold cross validation&lt;/code&gt;. Where should i initialize weights of my neural network?&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;*Position_1(rng,configure,randperm?)*&#10;&#10;for i=1:number_of_loops&#10;&#10;*Position_2(rng,configure,randperm?)* &#10;- repeating cross validation&#10;&#10;for i=1:number_of_kfolds&#10;&#10;*Position_3(rng,configure,randperm?)*&#10;- Cross validation loop&#10;&#10;end&#10; end&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I'm repeating &lt;code&gt;5-fold cross validation&lt;/code&gt; (because random selection of cross validation) to have more reliable outputs (average of neural network outputs). Which part is better for weight initialization for neural network (&lt;code&gt;Position_1&lt;/code&gt;,&lt;code&gt;Position_2&lt;/code&gt; or &lt;code&gt;Position_3&lt;/code&gt;) and why?&lt;/p&gt;&#10;&#10;&lt;p&gt;There is a discussion here that may help :&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.mathworks.com/matlabcentral/answers/152947-finding-best-neural-network-structure-using-optimization-algorithms-and-cross-validation&quot; rel=&quot;nofollow&quot;&gt;Classification+Optimization Mathworks community&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;We should set &lt;code&gt;rng&lt;/code&gt; function in a appropriate position to have unbiased designed and errors. Where is appropriate position for this function? As you know we are calling these codes as cost function of optimization algorithm that is searching for best neurons,layers and inputs combination so i think the logical solution is that we only should have one &lt;code&gt;rng&lt;/code&gt; structure in all calling of cost function by optimization algorithm but with that we can't search all space (maybe some initial weights or indexes for train/test/validation are poor and we don't change it in all system. What is your recommendation based on the above link? &lt;/p&gt;&#10;&#10;&lt;p&gt;My revised solution is that have &lt;code&gt;rng&lt;/code&gt; in position 1 and weight initialization (&lt;code&gt;configure&lt;/code&gt;) in position 3. It is better than removing &lt;code&gt;rng&lt;/code&gt; from all system but can optimization algorithm compare the cost functions because different random number generator of total cost calling (different weights and indexes of train/test/validation) but same random generator in every calling of cost function by optimization algorithm (cost function loops) ?&lt;/p&gt;&#10;&#10;&lt;p&gt;finally we will use all models from best cost function that will find by optimization algorithm as a pack (&lt;code&gt;number_of_loops*number_of_kfolds models&lt;/code&gt;) and average between outputs for out-sample data so we are searching for best pack of models not only structures. What do you think about this?&lt;/p&gt;&#10;&#10;&lt;p&gt;** &lt;em&gt;So briefly we should set position of &lt;code&gt;configure&lt;/code&gt; for neural network weight initialization, &lt;code&gt;rng&lt;/code&gt; for random number generator (that we will save and restore it in every calling of cost function) and &lt;code&gt;index=randperm(number_of_samples)&lt;/code&gt; for &lt;code&gt;cross validation&lt;/code&gt; (after that we separate it to train/test/validation in inner loop).&lt;/em&gt;&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-08-31T14:28:05.267" FavoriteCount="1" Id="113868" LastActivityDate="2014-09-07T07:35:54.907" LastEditDate="2014-09-07T07:35:54.907" LastEditorUserId="43534" OwnerUserId="43534" PostTypeId="1" Score="2" Tags="&lt;cross-validation&gt;&lt;matlab&gt;&lt;neural-networks&gt;&lt;pattern-recognition&gt;" Title="Finding best neural network structure and inputs using optimization algorithm and cross-validation" ViewCount="413" />
  <row Body="&lt;p&gt;The problem here is that you are not &lt;strong&gt;thinning&lt;/strong&gt; and &lt;strong&gt;burning&lt;/strong&gt; your posterior sample. The first $N$ iterations in your MCMC simulation have to be removed (because the MCMC works asymptotically) and then, you need to obtain a subsample (every $k$ iterations) in order to reduce correlation/dependence (MCMC samples are correlated). The MCMC itself doesn't seem to be problematic in this specific case.  See the following code, based on your own code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(MCMCpack)&#10;&#10;# Simulated data&#10;dat &amp;lt;- vector()&#10;&#10;sigma=0.2&#10;&#10;for(i in 1:1000){&#10;if(runif(1)&amp;lt;0.7) dat[i] = rlnorm(1,meanlog=2,sdlog=sigma)&#10;else dat[i] = rlnorm(1,meanlog=8,sdlog=sigma)&#10;    }&#10;&#10;hist(dat)&#10;&#10;d.lognorm.mix &amp;lt;- function(x,p=0.7,mu1,mu2,sigma=0.2) {&#10;dens &amp;lt;- p*dlnorm(x,meanlog=mu1,sdlog=sigma) + (1-p)*dlnorm(x,meanlog=mu2,sdlog=sigma)&#10;return(log(dens))&#10;}&#10;&#10;loglikeli &amp;lt;- function(theta) {  #theta = mu1, mu2&#10;mu1 &amp;lt;- theta[1]&#10;mu2 &amp;lt;- theta[2]&#10;return(sum(d.lognorm.mix(dat, p=0.7, mu1=mu1, mu2=mu2, sigma=0.2)))   &#10;}&#10;&#10;library(MCMCpack)&#10;post.samp &amp;lt;- MCMCmetrop1R(loglikeli, theta.init=c(1,1),&#10;                          thin=25, mcmc=60000, burnin=10000,&#10;                     #     tune=c(1.5, 1.5),  &#10;                          verbose=FALSE, logfun=TRUE)&#10;&#10;&#10;summary(post.samp)&#10;&#10;# Posterior samples burned and thinned N=10000, k=25&#10;&#10;hist(post.samp[,1])&#10;hist(post.samp[,2]) &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Here is a link to a paper on the label switching problem for your joy:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;A. Jasra, C. C. Holmes and D. A. Stephens. &lt;a href=&quot;http://projecteuclid.org/euclid.ss/1118065042&quot; rel=&quot;nofollow&quot;&gt;Markov Chain Monte Carlo Methods and the Label Switching Problem in Bayesian Mixture Modeling&lt;/a&gt;. Statist. Sci. Volume 20, Number 1 (2005), 50-67.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="2" CreationDate="2014-08-31T18:17:04.973" Id="113884" LastActivityDate="2014-08-31T20:50:02.550" LastEditDate="2014-08-31T20:50:02.550" LastEditorUserId="54999" OwnerUserId="54999" ParentId="113870" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;I think you are looking for a &lt;a href=&quot;https://en.wikipedia.org/wiki/Graph_kernel&quot; rel=&quot;nofollow&quot;&gt;graph kernel&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;A quick web search gave me a couple of papers to start:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;strong&gt;Graph Kernels&lt;/strong&gt;. S.V.N. Vishwanathan, Nicol N. Schraudolph, Risi Kondor, Karsten M. Borgwardt; 11(Apr):1201−1242, 2010&lt;/li&gt;&#10;&lt;li&gt;Sato et al. &lt;strong&gt;Directed acyclic graph kernels for structural RNA analysis&lt;/strong&gt;. BMC Bioinformatics 2008, 9:318  doi:10.1186/1471-2105-9-318&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="1" CreationDate="2014-08-31T20:37:29.957" Id="113890" LastActivityDate="2014-08-31T20:37:29.957" OwnerUserId="8960" ParentId="113855" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;I'm not an expert. I also don't know if your song variables are measured at the individual or species-level. Assuming the latter (if former, a simple thing would be to average to species and pick up directions from here), I would suggest running a PCA where rows are species and song elements are columns. Then pull out each species' position in however many dimensions you care to use (you'll have as many as you have song elements I think). That matrix would just be rows as species, columns as position along each successive PC axis.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then you can simply run&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;distancesAmong &amp;lt;- dist(pca.position.matrix)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And a Mantel test between these distances and the geographic distances. Make sure the matrices are in the same order, obviously. There are additional distance measures you can use, the default is Euclidean.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-08-31T21:37:50.970" Id="113897" LastActivityDate="2014-08-31T21:37:50.970" OwnerUserId="41792" ParentId="113491" PostTypeId="2" Score="0" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I am trying to follow up the example code in the &quot;Building Predictive Models in R Using the caret Package&quot; paper from Max Kuhn[1].&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is the part of the code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(&quot;caret&quot;)&#10;set.seed(1)&#10;inTrain &amp;lt;- createDataPartition(mutagen, p = 3/4, list = FALSE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;However, after the following,&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;trainDescr&amp;lt;-descr[ inTrain, ]&#10;&#10;Error: object 'descr' not found&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I get the &quot;Error: object 'descr' not found&quot; message. Looks  like 'descr' is not recognized. Can anyone tell me how to fix this please?&lt;/p&gt;&#10;&#10;&lt;p&gt;[1]: Kuhn, M. (2008),&lt;br&gt;&#10;&quot;Building Predictive Models in R Using the caret Package,&quot;&lt;br&gt;&#10;&lt;em&gt;Journal of Statistical Software&lt;/em&gt;, November, Volume &lt;strong&gt;28&lt;/strong&gt;, Issue 5.&lt;br&gt;&#10;&lt;a href=&quot;http://www.jstatsoft.org/v28/i05/paper&quot; rel=&quot;nofollow&quot;&gt;link&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-08-31T23:35:23.593" Id="113907" LastActivityDate="2014-09-01T01:02:46.997" LastEditDate="2014-09-01T00:38:23.697" LastEditorUserId="805" OwnerUserId="35577" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;caret&gt;" Title="Error: object 'descr' not found" ViewCount="96" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to &lt;code&gt;createFolds&lt;/code&gt; function in &lt;code&gt;caret&lt;/code&gt; to use k-fold cross-validation in R. But I came across this warning:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; Warning messages:&#10;1: In foldVector[which(y == dimnames(numInClass)$y[i])] &amp;lt;- sample(seqVector) :&#10;  number of items to replace is not a multiple of replacement length&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Has anyone come across this before &amp;amp; solved it?&lt;/p&gt;&#10;&#10;&lt;p&gt;I am using this R code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; flds&amp;lt;-createFolds(dataset,k=5, list=TRUE, returnTrain=FALSE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The data-set includes my response variables that I am going to use with Linear Discriminant Analysis&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;str(dataset)&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;'data.frame':   200 obs. of  162 variables:&lt;/p&gt;&#10;&#10;&lt;p&gt;$ V2  : Factor w/ 201 levels &quot;6.74361083760967&quot;,..: 150 190 108 157 180 7 139 116 175 49 ...&lt;/p&gt;&#10;&#10;&lt;p&gt;$ V3  : Factor w/ 201 levels &quot;1.88828398539874&quot;,..: 114 177 42 155 143 173 144 81 142 117 ...&lt;/p&gt;&#10;&#10;&lt;p&gt;[list output truncated]&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2014-09-01T08:24:29.440" Id="113933" LastActivityDate="2014-09-01T11:38:12.147" LastEditDate="2014-09-01T11:38:12.147" LastEditorUserId="55018" OwnerUserId="55018" PostTypeId="1" Score="0" Tags="&lt;cross-validation&gt;&lt;caret&gt;" Title="A question about warning that performing k-fold CV with caret" ViewCount="96" />
  <row Body="&lt;p&gt;Yes, you can simply moment match this with a Beta distribution and divide.  The moments that you mention can be computed in closed form.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-01T13:50:38.817" Id="113958" LastActivityDate="2014-09-01T13:50:38.817" OwnerUserId="2074" ParentId="109134" PostTypeId="2" Score="1" />
  
  <row AnswerCount="1" Body="&lt;p&gt;PredictionIO is good to be enough for content discovery and recommendation but it seems  it does not support classification. Then I should use a different tool then Prediction IO for my prediction server. Do you know good alternatives?&lt;/p&gt;&#10;&#10;&lt;p&gt;I know Google Prediction Server, Azure ML, BigML and looking for more alternatives to compare them.&lt;/p&gt;&#10;&#10;&lt;p&gt;Best :)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-01T13:54:17.033" Id="113960" LastActivityDate="2014-10-29T15:00:29.257" OwnerUserId="14289" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;prediction&gt;" Title="what are the alternative open source tools for PredictionIO?" ViewCount="263" />
  
  
  
  <row AnswerCount="2" Body="&lt;p&gt;Assume I have a small sample size, e.g. N=100, and two classes. How should I choose the training, cross-validation, and test set sizes for machine learning?&lt;/p&gt;&#10;&#10;&lt;p&gt;I would intuitively pick&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Training set size as 50&lt;/li&gt;&#10;&lt;li&gt;Cross validation set size 25, and&lt;/li&gt;&#10;&lt;li&gt;Test size as 25.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;But probably this makes more or less sense. How should I really decide these values? May I try different options (though I guess it is not so preferable... increased possibility of over learning)?&lt;/p&gt;&#10;&#10;&lt;p&gt;What if I had more than two classes?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-09-01T18:35:39.283" FavoriteCount="2" Id="113994" LastActivityDate="2014-09-03T15:35:02.897" OwnerUserId="55048" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;sampling&gt;&lt;svm&gt;&lt;cross-validation&gt;&lt;sample-size&gt;" Title="How to choose the training, cross-validation, and test set sizes for small sample-size data?" ViewCount="631" />
  
  
  <row Body="&lt;p&gt;Well, here's an idea...&lt;/p&gt;&#10;&#10;&lt;p&gt;First standardize all of the variables (the model outputs as well as the reference variable).&lt;/p&gt;&#10;&#10;&lt;p&gt;Then fit a multivariate model with those standardized model outputs as a multivariate &lt;em&gt;response&lt;/em&gt; variable (not predictors), and the common human-performance variable (standardized) as the predictor. Do not include an intercept, as it will be zero anyway. Then the regression coefficients will be equal to the correlation coefficients, due to the standardization, and their covariance matrix will be available; so you can estimate each pairwise difference and its standard error.&lt;/p&gt;&#10;&#10;&lt;h3&gt;R example&lt;/h3&gt;&#10;&#10;&lt;p&gt;This is using the &lt;code&gt;swiss&lt;/code&gt; dataset provided with R. Here is the standardized dataset&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; swiss.std = as.data.frame(lapply(swiss, function(x) (x-mean(x))/sd(x)))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Note the covariances are just the correlations&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; cov(swiss.std[1:4])&#10;             Fertility Agriculture Examination  Education&#10;Fertility    1.0000000   0.3530792  -0.6458827 -0.6637889&#10;Agriculture  0.3530792   1.0000000  -0.6865422 -0.6395225&#10;Examination -0.6458827  -0.6865422   1.0000000  0.6984153&#10;Education   -0.6637889  -0.6395225   0.6984153  1.0000000&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Fit the multivariate model, looking at correlations with &lt;code&gt;Fertility&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; swiss.mlm = lm(cbind(Agriculture,Examination,Education) ~ Fertility - 1, data = swiss.std)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Here are the coefficients and variance-covariance matrix thereof&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; coef(swiss.mlm)&#10;          Agriculture Examination  Education&#10;Fertility   0.3530792  -0.6458827 -0.6637889&#10;&#10;&amp;gt; vcov(swiss.mlm)&#10;                      Agriculture:Fertility Examination:Fertility Education:Fertility&#10;Agriculture:Fertility           0.019029024          -0.009967271        -0.008807663&#10;Examination:Fertility          -0.009967271           0.012670338         0.005862729&#10;Education:Fertility            -0.008807663           0.005862729         0.012160529&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So to compare, say, the 2nd and 3rd correlation, here's the estimate&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; con = c(0,1,-1)&#10;&amp;gt; coef(swiss.mlm) %*% con&#10;                [,1]&#10;Fertility 0.01790615&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and its SE&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; sqrt(sum(con * vcov(swiss.mlm) %*% con))&#10;[1] 0.1144789&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I can trick the lsmeans package into doing it:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; library(lsmeans)&#10;&amp;gt; swiss.lsm = lsmeans(swiss.mlm, &quot;rep.meas&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;rep.meas&lt;/code&gt; is the default name for the levels of the multivariate response. So far, &lt;code&gt;swiss.lsm&lt;/code&gt; is just estimating the mean, $(0,0,0,)$, but I'll change the linear function that it's using to be $1$ times each regression coefficient&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; swiss.lsm@linfct = diag(c(1,1,1))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now, here is the summary&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; swiss.lsm&#10; rep.meas        lsmean        SE df    lower.CL   upper.CL&#10; Agriculture  0.3530792 0.1379457 46  0.07540884  0.6307495&#10; Examination -0.6458827 0.1125626 46 -0.87245946 -0.4193060&#10; Education   -0.6637889 0.1102748 46 -0.88576050 -0.4418172&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and the pairwise comparisons:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; pairs(swiss.lsm)&#10; contrast                    estimate        SE df t.ratio p.value&#10; Agriculture - Examination 0.99896189 0.2272309 46   4.396  0.0002&#10; Agriculture - Education   1.01686804 0.2209183 46   4.603  0.0001&#10; Examination - Education   0.01790615 0.1144789 46   0.156  0.9866&#10;&#10;P value adjustment: tukey method for a family of 3 means &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;If I want to compare the absolute correlations, just change the linear function&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; swiss.lsm@linfct = diag(c(1,-1,-1))&#10;&amp;gt; swiss.lsm&#10; rep.meas       lsmean        SE df   lower.CL  upper.CL&#10; Agriculture 0.3530792 0.1379457 46 0.07540884 0.6307495&#10; Examination 0.6458827 0.1125626 46 0.41930596 0.8724595&#10; Education   0.6637889 0.1102748 46 0.44181722 0.8857605&#10;&#10;Confidence level used: 0.95 &#10;&#10;&amp;gt; pairs(swiss.lsm)&#10; contrast                     estimate        SE df t.ratio p.value&#10; Agriculture - Examination -0.29280352 0.1084658 46  -2.700  0.0257&#10; Agriculture - Education   -0.31070967 0.1165085 46  -2.667  0.0279&#10; Examination - Education   -0.01790615 0.1144789 46  -0.156  0.9866&#10;&#10;P value adjustment: tukey method for a family of 3 means &#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="11" CreationDate="2014-09-01T23:44:40.177" Id="114011" LastActivityDate="2014-09-02T19:43:47.800" LastEditDate="2014-09-02T19:43:47.800" LastEditorUserId="52554" OwnerUserId="52554" ParentId="114001" PostTypeId="2" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;In a book, I've found SPSS syntax for performing an aligned rank transformation in a two way ANOVA. &lt;/p&gt;&#10;&#10;&lt;p&gt;The procedure is:&#10;- save residuals by performing a standard ANOVA&#10;- use Aggregate to determine effects for group means (mij for interaction, ai as first factor, bj for second factor)&#10;- eliminate interaction effect from residual to determine interaction (ran), eliminate single effects to determine main effects (ra)&#10;- transform residuals into ranks&#10;- run complete model on these ranks for interaction / main effects, respectively&lt;/p&gt;&#10;&#10;&lt;p&gt;The syntax is:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Unianova  x by group drugs&#10;  /save=resid (rab)&#10;  /design=group drugs group*drugs.&#10;Compute ra=rab.&#10;Aggregate&#10;  /outfile=* mode=addvariables&#10;  /break=group drugs    /mij=mean(x).&#10;Aggregate&#10;  /outfile=* mode=addvariables&#10;  /break=group    /ai=mean(x).&#10;Aggregate&#10;  /outfile=* mode=addvariables&#10;  /break=drugs    /bj=mean(x).&#10;Aggregate&#10;  /outfile=* mode=addvariables&#10;  /break=         /mm=mean(x).&#10;Compute rab=rab-(mij - ai - bj + mm).&#10;Compute ra=ra-(ai + bj - 2*mm).&#10;Rank variables=ra rab (A)&#10;  /rank into rar rabr.&#10;Unianova  rabr by group drugs&#10;  /design=group drugs group*drugs.&#10;Unianova  rar by group drugs&#10;  /design=group drugs group*drugs.&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now I would like to adapt this to my design, which has an additional random factor &quot;subjects&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;My initial ANOVA syntax would look like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Unianova x by condition repetition subject&#10; /random=subject&#10; /method=SSTYPE(3)&#10; /intercept=INCLUDE&#10; /save=RESID (rab)  &#10; /criteria=ALPHA(.05)&#10; /design=condition repetition subject condition*repetition condition*subject repetition*subject.&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;However now I'm not sure &lt;strong&gt;how to include the third random effect&lt;/strong&gt; in the subsequent aligned rank transform procedure. &lt;/p&gt;&#10;&#10;&lt;p&gt;Any tips are greatly appreciated!&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;&#10;&lt;p&gt;(Source: &lt;a href=&quot;http://www.uni-koeln.de/~a0032/statistik/buch/nonpar-anova-buch.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.uni-koeln.de/~a0032/statistik/buch/nonpar-anova-buch.pdf&lt;/a&gt;, in German)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-02T04:08:30.900" Id="114024" LastActivityDate="2014-09-02T04:08:30.900" OwnerUserId="54643" PostTypeId="1" Score="0" Tags="&lt;anova&gt;&lt;ranks&gt;" Title="Aligned Rank Transform for mixed effects ANOVA in SPSS" ViewCount="148" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I would like to evaluate the goodness-of-fit of the following (&lt;em&gt;Pareto&lt;/em&gt;-like) distribution:&#10;$$&#10;f(r) = \sigma \centerdot r^{-\rho}&#10;$$&#10;The function estimates the population of cities given the rank $r$ in a popularity ranking.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have not estimated the parameters ($\sigma$ and $\rho$) from the sample. However, I am unsure how to apply &lt;em&gt;Kolmogorov-Smirnov&lt;/em&gt; (or similar tests) because I do not know the &lt;em&gt;CDF&lt;/em&gt; of $f(r)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;How can I solve this problem?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-09-02T07:23:17.803" Id="114032" LastActivityDate="2014-09-02T09:05:57.653" LastEditDate="2014-09-02T09:05:57.653" LastEditorUserId="53561" OwnerUserId="53561" PostTypeId="1" Score="1" Tags="&lt;goodness-of-fit&gt;&lt;kolmogorov-smirnov&gt;&lt;cdf&gt;&lt;pareto-distribution&gt;" Title="Evaluate goodness-of-fit of estimation of Pareto-like distribution" ViewCount="55" />
  
  <row Body="&lt;p&gt;I had the same problematic with LDA: discrimination worked a lot better when the number of observations in each class were matching.&lt;/p&gt;&#10;&#10;&lt;p&gt;I ended up artificially growing the numbers by adding more observations to the classes through &lt;a href=&quot;http://en.wikipedia.org/wiki/Boosting_%28machine_learning%29&quot; rel=&quot;nofollow&quot;&gt;boosting&lt;/a&gt;. There are several ways to do that, the easiest (and probably worst) being duplicating existing observations, a better one would create random observations according to the covariance matrix.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-02T08:41:38.323" Id="114036" LastActivityDate="2014-09-02T08:41:38.323" OwnerUserId="35998" ParentId="63740" PostTypeId="2" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;I am working with a survey dataset which contains hundreds of variables. The item-missing data rate ranges from 0.2% to 10%. In order to retain study units with missing values and to maintain a reasonable statistical power for my analyses, I attempted to do multiple imputation using sequential regression (chained equations) in Stata. However, with all the several variables with missing items in the equations, the models could never attain convergence. I spent days re-specifying  the equations and the options within the equations  but all in vain. In the end, I simplified the equations by selecting only variables with high item-missing  data rates and variables which apply to all study units in the dataset. This time I succeeded in multiply imputing the data.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, I have two questions.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Since I did multiple imputation using only some of the variables in the dataset, there are still variables with missing values (though small). Is it acceptable to still have variables with missing values while analyzing multiply-imputed data?&lt;/li&gt;&#10;&lt;li&gt;I am also planning to do sub-population analysis. However, due to the convergence problem I stated earlier, variables pertaining only to sub-populations are excluded from the multiple imputation I did. Is it acceptable to do a separate multiple imputation to the sub-population of interest?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="2" CreationDate="2014-09-02T09:03:05.413" Id="114038" LastActivityDate="2014-09-02T09:03:05.413" OwnerUserId="52898" PostTypeId="1" Score="0" Tags="&lt;stata&gt;&lt;convergence&gt;&lt;multiple-imputation&gt;" Title="Multiple imputation to part of a dataset" ViewCount="60" />
  <row Body="&lt;p&gt;Here is how to do the necessary algebra. Others can add an example of how to do it in R. Suppose you response variable of interest is $Y$, which is the response to a question with answer &quot;yes&quot; or &quot;no&quot; (We code &quot;yes&quot; as 1 and &quot;no&quot; as 0).  Suppose we have a valid logistic regression model for $Y$:&#10;$$&#10;P(Y=\text{yes} | x) = g^{-1}(x) =\frac{e^{\beta' x}}{1+e^{\beta' x}}&#10;$$&#10;where $g$ is the link function, $g^{-1}$ the mean function.  We get &#10;$$&#10;g(y) = \ln(\frac{y}{1-y})&#10;$$&#10;No, with the randomized resaponse technique, instead you observe a response variable $Y^*$&#10;which is defined as $Y^* = \begin{cases} Y, ~~\text{with probability $1-q$} \\  &#10;                                         \text{yes}, ~~\text{with probability $q$}.     \end{cases}$&lt;br&gt;&#10;Since now, in all cases, regardless of the values of the regressor variable(s) $x$, the probability of a &quot;yes&quot; response is at least $q$, so effectively we obtain the link function by restricting the probability parameter to range in $(q,1)$, not in $(0,1)$. Calculate:&#10;$$&#10;P(Y^*=\text{yes}|x)=q+(1-q)P(Y=\text{yes}|x)=\frac{q+e^{\beta'x}}{1+e^{\beta'x}}&#10;$$&#10;(where $q$ here is a known probability). Inverting this gives the link function&#10;$g(y)=\ln(\frac{y-q}{1-y})$.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-09-02T11:34:15.407" Id="114049" LastActivityDate="2014-09-02T11:34:15.407" OwnerUserId="11887" ParentId="113935" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;I am investigating the effect of genomic dysmethylation on cancer survival time, with data from multiple different cancers with very different survival curves. &lt;/p&gt;&#10;&#10;&lt;p&gt;Normally, I would split the cases into high and low dysmethylation, and do a log-rank test on the Kaplan-Meier curves. This isn't possible because for each cancer I have only 5 or so cases where methylation was measured.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, for every cancer, I do have 100+ cases where survival time was measured, even if methylation wasn't.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can I do a statistical test where I measure the survival times of the cases where methylation was measured against this &quot;background&quot; survival curve?&lt;/p&gt;&#10;&#10;&lt;p&gt;Is it possible for me to combine the different groups, so I get a single answer across all the different cancers?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks,&lt;/p&gt;&#10;&#10;&lt;p&gt;Steph     &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-09-02T12:04:45.587" Id="114050" LastActivityDate="2014-09-02T13:37:06.553" OwnerUserId="55093" PostTypeId="1" Score="1" Tags="&lt;survival&gt;&lt;kaplan-meier&gt;" Title="Survival - comparing Kaplan-Meier curve to handful of points" ViewCount="45" />
  <row AnswerCount="1" Body="&lt;p&gt;From Wikipedia&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The Net Promoter Score is obtained by asking customers a single&#10;  question on a 0 to 10 rating scale, where 10 is &quot;extremely likely&quot; and&#10;  0 is &quot;not at all likely&quot;: &quot;How likely is it that you would recommend&#10;  our company to a friend or colleague?&quot; Based on their responses,&#10;  customers are categorized into one of three groups: Promoters (9–10&#10;  rating), Passives (7–8 rating), and Detractors (0–6 rating). The&#10;  percentage of Detractors is then subtracted from the percentage of&#10;  Promoters to obtain a Net Promoter score (NPS). NPS can be as low as&#10;  -100 (everybody is a detractor) or as high as +100 (everybody is a promoter).&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Now we want to scale it down to 1-5 for our purposes, because frankly our users will get confused on the 0-10 scale. &lt;code&gt;Now we are considering 1,2,3 to be detractors , 4 to be passive and 5 to be a promoter&lt;/code&gt;. Now In the &lt;code&gt;Standard 0-10 scale, it is actually a 11 point scale – so promoters (9,10) is 2/11th of the distribution which is 18%. In the 1-5 scale we use promoters (5) is 1/5 which is 20%&lt;/code&gt;. So more chances of getting a promoter score. &lt;code&gt;By same logic, 5 point scale will also show less detractors than 0-10 point scale (3/5=60% vs 7/11 = 63.6%)&lt;/code&gt;. So a positive skew looks possible mathematically. But the fact is a user has only 1 option to choose from on the 1-5 scale to be a promoter, but 2 on the 1-10 scale. So someone who might be a 9 on a 10 scale might be a 4 on the 5 scale and become a passive and never show up and kill our NPS score to go to -100(if no one votes 5 , and a lot of people vote 4) . Three questions&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;blockquote&gt;&#10;    &lt;p&gt;1.) Which one of the two math arguments should be considered for scaling down&lt;/p&gt;&#10;    &#10;    &lt;p&gt;2.) What should be the right passive pivot for the 1-5 scale?&lt;/p&gt;&#10;  &lt;/blockquote&gt;&#10;  &#10;  &lt;p&gt;3.) Are there other tests that can be done on this data to get an accurate measure of customer feedback?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="5" CreationDate="2014-09-02T13:23:46.207" Id="114060" LastActivityDate="2015-01-14T21:37:53.240" OwnerUserId="32534" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;&lt;distributions&gt;&lt;standard-error&gt;" Title="Calculating the Net Promoter Score for poll data on a 1-5 scale" ViewCount="766" />
  <row Body="&lt;p&gt;Why not just use the numbers from the box plot, that is a table with the minimum, 25 percentile, 50 percentile (median), 75 percentile, and maximum? Alternatively, you could simply say what percentage of X values lie in the range of observed Y values. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-02T13:42:46.463" Id="114064" LastActivityDate="2014-09-02T13:42:46.463" OwnerUserId="17246" ParentId="114059" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;In cases where the likelihood function is maximized at $\rho=-1$, using Beta(1,1) over an open interval doesn't help since then there is no well-defined posterior mode.  There is no point in the open interval that you could say is the posterior mode.  &lt;/p&gt;&#10;&#10;&lt;p&gt;The book wants the prior to be linear at the boundaries, which uniquely selects Beta(2,2) out of all Beta distributions.  But the book never precisely explains why linearity is so important.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-09-02T14:52:20.837" Id="114075" LastActivityDate="2014-09-02T14:52:20.837" OwnerUserId="2074" ParentId="86484" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="114846" AnswerCount="1" Body="&lt;p&gt;What is the difference between the classic meta-analysis (that aggregates effect sizes from a sample of studies to a summary effect size), meta-regression analysis and moderator analysis?&lt;/p&gt;&#10;&#10;&lt;p&gt;As I understood moderator analysis is used to explain heterogeneity in a meta-analysis by regressing a vector containing the effect sizes of each study on specific variables that could explain the heterogeneity (e.g. publication year,...). But what's the difference between moderator-analysis and meta-regression?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for your help!&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-09-02T18:32:01.593" FavoriteCount="1" Id="114104" LastActivityDate="2014-09-09T15:12:22.270" OwnerUserId="55116" PostTypeId="1" Score="2" Tags="&lt;meta-analysis&gt;&lt;meta-regression&gt;" Title="Difference between Meta-Analysis, Meta-Regression and Moderator-Analysis" ViewCount="128" />
  <row Body="&lt;p&gt;It is not easy to get an intuition behind standard deviation $\sigma$, as one gets easily about mean as soon as one sees them.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now why variance is squared of standard deviation is perhaps an outcome of the definition of the variance ($Var[X] = \Sigma_i (x-u)^2 f(x) dx)$ which requires that the difference term $(x - u)$ should be squared. Now one can protest that one need not use $(x - u)^2$ in this definition and live with some other metric such as $|(x-u)|$. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you define variance in this way than standard deviation ceases to be squared for the variance. Now one can wonder why use square function in the definition in variance at all and not mod function. IMHO, I suspect since mod changes its direction rapidly at 0, it is not as useful as square function. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-09-02T19:08:11.227" Id="114109" LastActivityDate="2014-09-02T19:08:11.227" OwnerUserId="54851" ParentId="70238" PostTypeId="2" Score="0" />
  
  
  <row Body="&lt;p&gt;Start with the equation known to you, viz. &#10;$\operatorname{var}(Z\mid X) = E[Z^2\mid X] - \left(E[Z\mid X]\right)^2$&#10;and re-write it as follows.&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\begin{align}&#10;E[Z^2\mid X] &amp;amp;= \operatorname{var}(Z\mid X) + \left(E[Z\mid X]\right)^2\\&#10;E[(Y-f(X))^2\mid X] &amp;amp;= \operatorname{var}((Y-f(X))\mid X) + \left(E[(Y-f(X))\mid X]\right)^2 &amp;amp;\scriptstyle{\text{substitute}~ Y-f(X)~\text{for}~Z}\\&#10;\end{align}$$ &lt;/p&gt;&#10;&#10;&lt;p&gt;Now, notice that the first term on the right is a &lt;em&gt;conditional&lt;/em&gt; variance&#10;-- the (conditional) variance of $Y-f(X)$, in fact, --  &lt;em&gt;given&lt;/em&gt; the value of $X$. But if $X$&#10;has value $x$, say, then $f(X)$ is a constant (let's call it $a$), and&#10;not a random variable at all. Thus, the &lt;em&gt;conditional&lt;/em&gt; variance of $Y-f(X)$ &#10;&lt;em&gt;given&lt;/em&gt; that $X = x$ is the same as the  &lt;em&gt;conditional&lt;/em&gt; variance of &#10;$Y-a$ given that $X=x$.&#10;But, variance is invariant to translation of the data set (that is, $Y$ and&#10;$Y-a$ have the same variance), and thus we get&#10;$$E[(Y-f(X))^2\mid X] = \operatorname{var}(Y\mid X) + \left(E[(Y-f(X))\mid X]\right)^2 $$&#10;which is what your lecture note asserts.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-03T03:39:33.793" Id="114140" LastActivityDate="2014-09-03T21:12:37.333" LastEditDate="2014-09-03T21:12:37.333" LastEditorUserId="6633" OwnerUserId="6633" ParentId="114136" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="114157" AnswerCount="6" Body="&lt;p&gt;In this current &lt;a href=&quot;http://millhauser.chemistry.ucsc.edu/courses/Chem_163C_handouts_2014/Physicists_say_it_is_simple.pdf&quot;&gt;article in SCIENCE&lt;/a&gt; the following is being proposed: &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Suppose you randomly divide 500 million in income among 10,000&#10;  people. There's only one way to give everyone an equal, 50,000 share.&#10;  So if you're doling out earnings randomly, equality is extremely&#10;  unlikely. But there are countless ways to give a few people a lot of&#10;  cash and many people a little or nothing. In fact, given all the ways&#10;  you could divvy out income, most of them produce an exponential&#10;  distribution of income.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I have done this with the following R code which seems to reaffirm the result:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(MASS)&#10;&#10;w &amp;lt;- 500000000 #wealth&#10;p &amp;lt;- 10000 #people&#10;&#10;d &amp;lt;- diff(c(0,sort(runif(p-1,max=w)),w)) #wealth-distribution&#10;h &amp;lt;- hist(d, col=&quot;red&quot;, main=&quot;Exponential decline&quot;, freq = FALSE, breaks = 45, xlim = c(0, quantile(d, 0.99)))&#10;&#10;fit &amp;lt;- fitdistr(d,&quot;exponential&quot;)&#10;curve(dexp(x, rate = fit$estimate), col = &quot;black&quot;, type=&quot;p&quot;, pch=16, add = TRUE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/L9Qw4.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;My question&lt;/strong&gt;&lt;br&gt;&#10;How can I analytically prove that the resulting distribution is indeed exponential?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Addendum&lt;/strong&gt;&lt;br&gt;&#10;Thank you for your answers and comments. I have thought about the problem and came up with the following intuitive reasoning. Basically the following happens (Beware: oversimplification ahead): You kind of go along the amount and toss a (biased) coin. Every time you get e.g. heads you divide the amount. You distribute the resulting partitions. In the discrete case the coin tossing follows a binomial distribution, the partitions are geometrically distributed. The continuous analogues are the poisson distribution and the exponential distribution respectively! (By the same reasoning it also becomes intuitively clear why the geometrical and the exponential distribution have the property of memorylessness - because the coin doesn't have a memory either).&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2014-09-03T07:12:29.197" FavoriteCount="16" Id="114152" LastActivityDate="2014-09-08T22:27:57.153" LastEditDate="2014-09-04T14:27:40.487" LastEditorUserId="230" OwnerUserId="230" PostTypeId="1" Score="29" Tags="&lt;distributions&gt;&lt;exponential&gt;&lt;proof&gt;" Title="How can I analytically prove that randomly dividing an amount results in an exponential distribution (of e.g. income and wealth)?" ViewCount="2297" />
  
  <row Body="&lt;p&gt;This sort of pattern is to be expected with count data and log-links. The count data makes the residual vs fitted plot have &quot;lines&quot; diagonally across it (and gaps between), corresponding to counts of 0, 1, 2,... and the log-link makes those &quot;lines&quot; curved.&lt;/p&gt;&#10;&#10;&lt;p&gt;So you will expect to see this with Poisson and negative binomial regression and log links if the typical count is fairly small, in particular with simple models (such as say one continuous predictor, or one continuous predictor and a factor with few levels).&lt;/p&gt;&#10;&#10;&lt;p&gt;So nothing seems to be amiss, at least not from the appearance of the plots.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-03T10:28:30.710" Id="114161" LastActivityDate="2014-09-03T10:28:30.710" OwnerUserId="805" ParentId="114045" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Define $L(a)$ to be the likelihood that the changepoint is $a$, i.e., the probability of observing the data we saw, given that the changepoint was $a$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now an obvious method is to maximize $L(a)$.  For instance, you could compute $L(a)$ for all possible values of $a$ from $0.1$ to $0.9$, in increments of $1/n$.  (There may be more efficient methods.)&lt;/p&gt;&#10;&#10;&lt;p&gt;This might be suitable if you have a uniform prior on the value of $a$.  If you have a non-uniform prior, Bayesian methods might be more suitable.&lt;/p&gt;&#10;&#10;&lt;p&gt;How accurate will this be?  One way to find out would be to implement it and try it out, and see how well it performs.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Are you familiar with the estimate that we can distinguish a $\text{Ber}(p)$ random variable from a $\text{Ber}(q)$ random variable with roughly $\sim 1/|p-q|^2$ observations, when $p,q$ are close to each other?  This heuristic suggests that you should be able to get accuracy on the order of $\sim 100$ observations, i.e., to infer the change point $an$ to within $\pm 100$ (or some constant factor thereof), or in other words, to infer $a$ to within $\pm 100/n$ (possibly times some constant factor).&lt;/p&gt;&#10;&#10;&lt;p&gt;For more details on this estimate, see e.g., &lt;a href=&quot;http://cstheory.stackexchange.com/q/22328/5038&quot;&gt;http://cstheory.stackexchange.com/q/22328/5038&lt;/a&gt;.  Using the more precise estimate there suggests we might be able to replace $100$ with $25$ or so.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-03T07:38:45.010" Id="114173" LastActivityDate="2014-09-03T07:44:03.130" OwnerDisplayName="D.W." OwnerUserId="2921" ParentId="114172" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm running Gauss Seidel value iteration on a weapon-target assignment problem (I can explain this more later if necessary, but i don't think it is).  My VI is converging &lt;em&gt;exactly&lt;/em&gt; in 2 iterations.  I'm not sure if it is because my problem has ~1/2 absorbing states (i.e. if you expend all the weapons or if you destroy the primary target you no longer transition, nor do you obtain a further reward).  Being VI, I don't think it should be converging exactly (meaning that the inf-norm(V_2 - V_1) = 0).  I've hand-computed a couple of the states and things seem to be working properly, but I can't figure out theoretically why it would happen.  Matlab code posted below.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lambda = 0.95;&#10;epsilon = 0.001;&#10;tol = epsilon*(1-lambda)/2*lambda;&#10;MaxDelta = 1000;&#10;&#10;%%%%%%% Gauss-Seidel Value iteration %%%%%%%%&#10;&#10;%Initialization&#10;VS = zeros(cardS,1);&#10;VS_up = zeros(cardS,2);&#10;argmax = VS;&#10;count = 1;&#10;Vsa_T = cell(cardS,1);&#10;&#10;while MaxDelta &amp;gt; tol&#10;&#10;for sindex = Sindex&#10;    for aindex = 1:Aindex(sindex)&#10;        Vsa_T{sindex}(aindex) = r{sindex,aindex} + lambda*(Pij{sindex,aindex}'*VS);&#10;    end&#10;    [VS(sindex), argmax(sindex)] = max(Vsa_T{sindex});&#10;end&#10;&#10;VS_up(:,2) = VS;&#10;MaxDelta = norm(VS_up(:,1) - VS_up(:,2),inf);&#10;MD(count) = MaxDelta;&#10;VS_up(:,1) = VS_up(:,2);&#10;count=count+1&#10;end&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2014-09-03T14:56:28.677" Id="114193" LastActivityDate="2014-09-03T15:02:32.857" LastEditDate="2014-09-03T15:02:32.857" LastEditorUserId="7290" OwnerUserId="55172" PostTypeId="1" Score="0" Tags="&lt;matlab&gt;&lt;convergence&gt;" Title="Value Iteration exact convergence" ViewCount="23" />
  <row AnswerCount="0" Body="&lt;p&gt;Suppose I have 1 year of hourly streamflow values - approx. 8760 values.  I am developing a model that outputs an estimated streamflow value for each of the 8760 values.  I examine the sum of squared errors (for different predicted and observed outputs) and try to minimize it during calibration. What differences may result from these three different focuses during calibration:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;I calibrate the model using the differences between the observed and predicted values at the hourly timestep, examining every timestep individually.&lt;/li&gt;&#10;&lt;li&gt;I calibrate the model using the differences between the observed and predicted values at the daily timestep, summing the hourly predicted and observed values for each day, examining every day individually.&lt;/li&gt;&#10;&lt;li&gt;I calibrate the model using the differences between the observed and predicted values at the weekly timestep, summing the hourly predicted and observed values for each weekly, examining every week individually.&lt;/li&gt;&#10;&lt;li&gt;I calibrate to the mean of the 8760 values only.&lt;/li&gt;&#10;&lt;li&gt;I calibrate to the mean and skew of the 8760 values only.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2014-09-03T16:46:21.940" Id="114205" LastActivityDate="2014-09-03T16:46:21.940" OwnerUserId="36954" PostTypeId="1" Score="0" Tags="&lt;model&gt;&lt;calibration&gt;" Title="Calibrating a model using different timesteps, mean/skew" ViewCount="6" />
  
  
  <row Body="&lt;p&gt;The algorithm that you describe is treating $D$ like a variable in the problem, but using a method other than Bayesian inference to deal with $D$.  A Bayesian solution requires handling $D$ in a Bayesian manner, i.e. giving it a prior and integrating it out to get the marginal for $p_i$.  For example, you could use a Dirichlet process as the prior for $D$ and do inference for $p_i$ via Gibbs sampling.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-03T17:54:08.160" Id="114216" LastActivityDate="2014-09-03T17:54:08.160" OwnerUserId="2074" ParentId="114139" PostTypeId="2" Score="3" />
  <row AnswerCount="1" Body="&lt;p&gt;Dear statistics experts,&lt;/p&gt;&#10;&#10;&lt;p&gt;I have trouble to find a sensible statistical approach to back up some very obvious (at least to my eyes) interpretation of a dataset (see descriptive plot below).&lt;/p&gt;&#10;&#10;&lt;p&gt;I measure a response to two different stimulus (Stimulus 1 and 2) at 3 different intensities (weak medium high), in 2 different populations (population 1 and 2).&lt;/p&gt;&#10;&#10;&lt;p&gt;What I see and would like to support with a statistical test, is that overall the response exhibit a dynamic range along the intensity range, but more importantly, that this range is very steep for stimulus 1 in the population 2 but, not in population 1.&lt;/p&gt;&#10;&#10;&lt;p&gt;In terms of design, I have&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;a continuous dependent variable (my response).&lt;/li&gt;&#10;&lt;li&gt;two categorical variables with two levels each (stimulus and population)&lt;/li&gt;&#10;&lt;li&gt;one ordinal variable with 3 levels (stimulus strength)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Samples are not independent, as response to each stimulus/strength pair is assessed on the same subjects in the two populations.&lt;/p&gt;&#10;&#10;&lt;p&gt;I thought of using a mixed effect ANOVA with subject as a random variable to account for the repeated measure.&lt;/p&gt;&#10;&#10;&lt;p&gt;First problem : How to measure the departure from the homoscedasticity assumption ? A Levene's test reject the null hypothesis of equality of variance between groups. Anova is said to be robust to violations of its assomptions, but is there a way of judging the severity of this violation, especially given my unbalanced design? &lt;/p&gt;&#10;&#10;&lt;p&gt;Second, would my biological statements be reinforced if I was to find a significant 3-way interaction population x stimulus x strength ? I have two concerns :&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;Does a 3-way interaction p-value have any meaning on such a design and especially given my first point (disparity in variance) ?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;even if this effect was real, is it allowed (not to say feasible), to conclude anything biologically relevant from a three-way significant interaction, which essentially mean, (correct me if I am wrong), that at least one of the two-way interaction between a pair of factors is influenced by the third one ? Specifically here, I would like to test if the stimulus x strength interaction is influenced by the population factor. Should I then test for significance of this two-way interaction beforehand ? &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;My meditations have carried me far beyond what my very basic understanding of statistics allow me to comprehend.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance for your input,&lt;/p&gt;&#10;&#10;&lt;p&gt;Benjamin&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/7lc9g.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-03T18:40:11.933" FavoriteCount="0" Id="114222" LastActivityDate="2014-09-05T02:29:58.430" OwnerUserId="48159" PostTypeId="1" Score="3" Tags="&lt;anova&gt;&lt;mixed-effect&gt;" Title="The biological significance of ANOVA 3-way interaction" ViewCount="105" />
  <row Body="&lt;p&gt;So using 10-fold CV will split the data into 10 different sets of roughly the same size. The model is fit on 90% and the remaining 10% is used to estimate accuracy. This process continues &quot;round robin&quot; 9 more times.&lt;/p&gt;&#10;&#10;&lt;p&gt;The accuracy is the average of the 10 holdouts for each tuning value. For example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; set.seed(1)&#10;&amp;gt; train_control &amp;lt;- trainControl(method=&quot;cv&quot;, number=10,&#10;+                               savePredictions = TRUE) &#10;&amp;gt; output &amp;lt;- train(Species~., data=iris, trControl=train_control, method=&quot;rpart2&quot;)&#10;note: only 2 possible values of the max tree depth from the initial fit.&#10; Truncating the grid to 2 .&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;There is a sub-object that has the hold-out estimates of accuracy for each fold, and averaging these gets you the reported value (the ones shown here are for the optimal value of Cp):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; output$resample&#10;        Accuracy Kappa Resample&#10;    1  0.8666667   0.8   Fold01&#10;    2  0.9333333   0.9   Fold02&#10;    3  1.0000000   1.0   Fold03&#10;    4  0.9333333   0.9   Fold04&#10;    5  0.9333333   0.9   Fold05&#10;    6  0.8000000   0.7   Fold06&#10;    7  1.0000000   1.0   Fold07&#10;    8  1.0000000   1.0   Fold08&#10;    9  0.9333333   0.9   Fold09&#10;    10 0.9333333   0.9   Fold10&#10;    &amp;gt; mean(output$resample$Accuracy)&#10;[1] 0.9333333&#10;&amp;gt; getTrainPerf(output)&#10;  TrainAccuracy TrainKappa method&#10;1     0.9333333        0.9 rpart2&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;There is little value in getting predictions on the 90% each time. Since the same data is used to build the model, the predictions can be extremely optimistic (which is the motivation for using cross-validation in the first place). &lt;/p&gt;&#10;&#10;&lt;p&gt;If you want to see where the estimates in &lt;code&gt;output$resample&lt;/code&gt; were created:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; ## For the model associated with optimal Cp value, here is the predictions on the &#10;&amp;gt; ## first fold that was held-out&#10;&amp;gt; first_holdout &amp;lt;- subset(output$pred, Resample == &quot;Fold01&quot;)&#10;&amp;gt; head(first_holdout)&#10;        pred        obs rowIndex maxdepth Resample&#10;1     setosa     setosa        6        2   Fold01&#10;2     setosa     setosa       27        2   Fold01&#10;3     setosa     setosa       31        2   Fold01&#10;4     setosa     setosa       36        2   Fold01&#10;5     setosa     setosa       45        2   Fold01&#10;6 versicolor versicolor       67        2   Fold01&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;If we get the accuracy for this set:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; postResample(first_holdout$pred, first_holdout$obs)&#10; Accuracy     Kappa &#10;0.7666667 0.6500000 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Max&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-09-03T19:09:14.287" Id="114224" LastActivityDate="2014-09-03T19:09:14.287" OwnerUserId="3468" ParentId="114168" PostTypeId="2" Score="0" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;In normal multivariate regression you can leave out covariates (assuming they aren't correlated with any covariates you leave in) and the variance of covariates left out gets subsumed into the error term. &lt;/p&gt;&#10;&#10;&lt;p&gt;In survival analysis poisson models are used. In these models the variance is a function of the mean. What then happens to the variance of covariates left out of the model since there is no free parameter for estimating the error?&lt;/p&gt;&#10;&#10;&lt;p&gt;My, maybe naive, thought is that the variance estimate for poisson models is only correct if the model is fully specified. But that would mean that the variance for any model leaving out covariates is underestimated. Which, in turn, would mean that the p-values for any such would be very optimistic. But such models are very common in epidemiology and I can't believe they would all be this flawed. What am I missing?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-03T19:40:20.660" Id="114229" LastActivityDate="2014-09-03T22:02:13.167" OwnerUserId="9728" PostTypeId="1" Score="0" Tags="&lt;survival&gt;&lt;poisson-regression&gt;" Title="What happens to the variance of missing covariates in poisson models" ViewCount="22" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;This is a bit longish (I want to be thorough), but my actual question is a short one (in bold below).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;BACKGROUND:&lt;/strong&gt; Suppose I have a conditional &lt;strong&gt;logistic discrete time event history model&lt;/strong&gt; (aka logistic discrete time survival models aka logit hazard model) across $p$ possibly time-varying predictors $\mathbf{X} = \{X_{1t}, \dots, X_{pt}\}$ for $T$ time periods indexed by $t$ from $1, \dots, T$ something like:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\text{logit}\left(h_{t,\mathbf{X}}\right) = \alpha_{1}d_{1} + \dots + \alpha_{T}d_{t} + \mathbf{BX_{t}},$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Where $d_{1}, \dots, d_{T}$ are indicator variables of each time period, $\alpha_{1}, \dots, \alpha_{T}$ are the period-specific intercepts for the hazard function, and $\mathbf{BX}$ are the conditionals and their effects. This model implies that the hazard function is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$h_{t,\mathbf{X}} = \frac{e^{\alpha_{1}d_{1} + \dots + \alpha_{T}d_{T} + \mathbf{BX_{t}}}}{1 + e^{\alpha_{1}d_{1} + \dots + \alpha_{T}d_{T} + \mathbf{BX_{t}}}}= \frac{e^{\alpha_{t}d_{t} + \mathbf{BX_{t}}}}{1 + e^{\alpha_{t}d_{t} + \mathbf{BX_{t}}}}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;A success story:&lt;/strong&gt;&lt;br&gt;&#10;I would like to employ frequentist inference using confidence intervals around my estimated model quantities. So, using the &lt;a href=&quot;https://en.wikipedia.org/wiki/Delta_method&quot; rel=&quot;nofollow&quot;&gt;delta method&lt;/a&gt;, I can estimate the variance of $\hat{h}_{t,\mathbf{X}}$ as (see end of question for derivation):&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\sigma^{2}_{\hat{h}_{t}} = \left[\frac{e^{\hat{\alpha}_{t}+\hat{\mathbf{B}}\mathbf{X}_{t}}}{\left(1+e^{\hat{\alpha}_{t}+\hat{\mathbf{B}}\mathbf{X}_{t}}\right)^{2}}\right]^{2}\mathbf{Z}_{t}\mathbf{\Sigma}_{t}\mathbf{Z}_{t}^{\text{T}},$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Where the $p+1$ row vector $\mathbf{Z}_{t} = \left[d_{t},\mathbf{X}_{t}\right]$, and $\mathbf{\Sigma}_{t}$ is the variance-covariance matrix of $\mathbf{Z}_{t}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;When I run simulations of Wald-type confidence intervals ($\hat{h}_{t,\mathbf{X}} \pm t_{\text{CI}}\sigma^{2}_{\hat{h}_{t,\mathbf{X}}}$) using this variance estimator for unconditional models, for models conditioned on two variables, and for models conditioned on those variables plus an interaction (aside: also using different parameterizations of time, more about those at the very end), I get coverage probabilities very close to the nominal confidence level at each time period (i.e. CI of 0.9, 0.95, and 0.99). For example, here are the coverage probabilities across 9 models with a nominal 95% confidence level:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/rkzqB.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Woot! That looks swell.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;A failure story:&lt;/strong&gt;&lt;br&gt;&#10;The survivor function, $S_{t,\mathbf{X}}$ is also an important quantity in event history models:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\hat{S}_{t,\mathbf{X}} = \prod_{i=1}^{t}{\left(1-\hat{h}_i\right)} &#10;= \prod_{i=1}^{t}{\frac{1}{1+e^{\hat{\alpha}_{i}+\hat{\mathbf{B}}\mathbf{X}_{i}}}}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;However, when I go on to derive the variance of the survival function using the delta method to get (see end of question for derivation):&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\sigma^{2}_{\hat{S}_{t}} = \left[\sum_{i=1}^{t} { \mathbf{Z}_{i}{\mathbf{\Sigma}_{i}}\mathbf{Z}_{i}^{\mathrm{T}}&#10;\left(\frac{e^{\hat{\alpha}_{i}+\hat{\mathbf{B}}\mathbf{X}_{i}}}{1+e^{\hat{\alpha}_{i}+\hat{\mathbf{B}}\mathbf{X}_{i}}}\right)^{2}}\right] \left[\prod_{i=1}^{t}{\left(\frac{1}{1+e^{\hat{\alpha}_{i}+\hat{\mathbf{B}}\mathbf{X}_{i}}}\right)}\right]^{2}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;My coverage probabilities are badly biased. For example, here are the coverage probabilities across 9 models with a nominal 95% confidence level (YUCK!):&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/YZUrU.png&quot; alt=&quot;enter image description here&quot;&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;I have played around with multiple comparisons adjustments at each succeeding time period... but something fundamental is eluding me. I and a colleague have checked the simulations together, and implemented them in R and Stata so we don't think it is a programming error.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;QUESTION: Why am I not getting proper coverage probabilities on $\hat{S}_{t}$ for any but unconditional models with fully discrete parameterizations of time?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Brainstorms are welcome. &quot;You forgot to carry the 2&quot;-type ideas are welcome. Hints are welcome. &quot;Assumption of independence is inappropriate because...&quot; is welcome.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;br&gt;&lt;strong&gt;Derivation of $\sigma^{2}_{\hat{h}_{t}}$ using a first-order delta-method approximation:&lt;/strong&gt;&#10;$$\mathbf{V}_{t} = \left.\left[\begin{array}{cccc}\frac{{\partial}h_{t}}{{\partial}\alpha_{t}} &amp;amp; \frac{{\partial}h_{t}}{{\partial}\beta_{1}} &amp;amp; \cdots &amp;amp; \frac{{\partial}h_{t}}{{\partial}\beta_{p}}\end{array}\right]\right|_{\alpha_{t} = \hat{\alpha}_{t}, \mathbf{B} = \hat{\mathbf{B}}}\\&#10; \\&#10; =  \left[\begin{array}{ccc}\frac{e^{\left(\hat{\alpha}_{t}+\hat{\mathbf{B}}\mathbf{X}_{t}\right)}}{\left(1+e^{\left(\hat{\alpha}_{t}+\hat{\mathbf{B}}\mathbf{X}_{t}\right)}\right)^{2}} &amp;amp; \frac{X_{1t}e^{\left(\hat{\alpha}_{t}+\hat{\mathbf{B}}\mathbf{X}_{t}\right)}}{\left(1+e^{\left(\hat{\alpha}_{t}+\hat{\mathbf{B}}\mathbf{X}_{t}\right)}\right)^{2}} &amp;amp; \cdots &amp;amp; \frac{X_{pt}e^{\left(\hat{\alpha}_{t}+\hat{\mathbf{B}}\mathbf{X}_{t}\right)}}{\left(1+e^{\left(\hat{\alpha}_{t}+\hat{\mathbf{B}}\mathbf{X}_{t}\right)}\right)^{2}}\end{array}\right]\\&#10;\\&#10; = \frac{e^{\left(\hat{\alpha}_{t}+\hat{\mathbf{B}}\mathbf{X}_{t}\right)}}{\left(1+e^{\left(\hat{\alpha}_{t}+\hat{\mathbf{B}}\mathbf{X}_{t}\right)}\right)^{2}}\left[\begin{array}{cccc}1 &amp;amp; X_{1t} &amp;amp; \cdots &amp;amp; X_{pt}\end{array}\right]$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\sigma^{2}_{\hat{h}_{t,\mathbf{X}}} = \mathbf{V}_{t}\mathbf{\Sigma}_{t}\mathbf{V}_{t}^{\mathrm{T}} \nonumber\\&#10;= \left[\frac{e^{\left(\hat{\alpha}_{t}+\hat{\mathbf{B}}\mathbf{X}_{t}\right)}}{\left(1+e^{\left(\hat{\alpha}_{t}+\hat{\mathbf{B}}\mathbf{X}_{t}\right)}\right)^{2}}\right]^{2}\left[\begin{array}{cccc}1 &amp;amp; X_{1t} &amp;amp; \cdots &amp;amp; X_{pt}\end{array}\right]\mathbf{\Sigma}_{t}\left[\begin{array}{c}1 \\ X_{1t} \\ \vdots \\ X_{pt}\end{array}\right] \\&#10;\\&#10;= \left[\frac{e^{\hat{\alpha}_{t}+\hat{\mathbf{B}}\mathbf{X}_{t}}}{\left(1+e^{\hat{\alpha}_{t}+\hat{\mathbf{B}}\mathbf{X}_{t}}\right)^{2}}\right]^{2}\mathbf{Z}_{t}\mathbf{\Sigma}_{t}\mathbf{Z}_{t}^{\text{T}}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;br&gt;&lt;strong&gt;Derivation of $\sigma^{2}_{\hat{S}_{t},\mathbf{X}}$ using a first-order delta method approximation:&lt;/strong&gt;&lt;br&gt;&#10;We start by making $S_{t,\mathbf{X}}$ more tractable by taking the log and assuming (since $h_{t,\mathbf{X}}$ is conditioned on survival to $t$):&#10;$$\sigma^{2}_{\ln(\hat{S}_{t,\mathbf{X}})} = \sigma^{2}_{\sum_{i=1}^{t}{\ln\left(1-\hat{h}_{i,\mathbf{X}}\right)}}\\&#10;\\&#10;= \sum_{i=1}^{t}{\sigma^{2}_{\ln\left(1-\hat{h}_{i,\mathbf{X}}\right)}} \quad \mathrm{(by\phantom{0}independence)}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Then we get down:&#10;$$\sigma^{2}_{\hat{S}_{t,\mathbf{X}}} = \left[\sum_{i=1}^{t}{\sigma^{2}_{\hat{h}_{i,\mathbf{X}}}\left(\frac{1}{\hat{h}_{i,\mathbf{X}}-1}\right)^{2}}\right]\hat{S}_{t,\mathbf{X}}^{2} \\&#10;\\&#10;= \left[\sum_{i=1}^{t}{\mathbf{V}_{i}{\mathbf{\Sigma}_{i}}\mathbf{V}_{i}^{\mathrm{T}}\left(\frac{1}{\hat{h}_{i,\mathbf{X}}-1}\right)^{2}}\right]\hat{S}_{t,\mathbf{X}}^{2}\\&#10;\\&#10;= \left[\sum_{i=1}^{t}{\mathbf{V}_{i}{\mathbf{\Sigma}_{i}}\mathbf{V}_{i}^{\mathrm{T}}\left(1+e^{\hat{\alpha}_{i}+\hat{\mathbf{B}}\mathbf{X}_{i}}\right)^{2}}\right]\hat{S}_{t,\mathbf{X}}^{2}\\&#10;\\&#10;= \left[\sum_{i=1}^{t}{\left[\begin{array}{cccc} 1 &amp;amp; X_{1i} &amp;amp; \cdots &amp;amp; X_{pi}\end{array}\right]\mathbf{\Sigma}_{i}\left[\begin{array}{c}1 \\ X_{1i} \\ \vdots \\ X_{pi}\end{array}\right]\left(\frac{e^{\hat{\alpha}_{i}+\hat{\mathbf{B}}\mathbf{X}_{i}}}{1+e^{\hat{\alpha}_{i}+\hat{\mathbf{B}}\mathbf{X}_{i}}}\right)^{2}}\right] \times &#10;\left[\prod_{i=1}^{t}{\left(\frac{1}{1+e^{\hat{\alpha}_{i}+\hat{\mathbf{B}}\mathbf{X}_{i}}}\right)}\right]^{2}\\&#10;\\&#10;\sigma^{2}_{\hat{S}_{t,\mathbf{X}}} = \left[\sum_{i=1}^{t} { \mathbf{Z}_{i}{\mathbf{\Sigma}_{i}}\mathbf{Z}_{i}^{\mathrm{T}}&#10;\left(\frac{e^{\hat{\alpha}_{i}+\hat{\mathbf{B}}\mathbf{X}_{i}}}{1+e^{\hat{\alpha}_{i}+\hat{\mathbf{B}}\mathbf{X}_{i}}}\right)^{2}}\right] \left[\prod_{i=1}^{t}{\left(\frac{1}{1+e^{\hat{\alpha}_{i}+\hat{\mathbf{B}}\mathbf{X}_{i}}}\right)}\right]^{2}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;br&gt;&lt;strong&gt;Alternate parameterizations of time:&lt;/strong&gt;&lt;br&gt;&#10;The derivations and definitions above are given in terms of a &lt;strong&gt;fully discrete&lt;/strong&gt; paramaterization of time (i.e. the $\alpha_{t}d_{t}$ terms).&lt;/p&gt;&#10;&#10;&lt;p&gt;In the case of a &lt;strong&gt;constant effect of time&lt;/strong&gt;, the $\alpha_{t}d_{t}$ terms are replaced with a single $\beta_{0}$ term.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the case of a &lt;strong&gt;linear effect of time&lt;/strong&gt; the $\alpha_{t}d_{t}$ terms are replaced with the terms $\beta_{0} + \beta_{1}t$.&lt;/p&gt;&#10;&#10;&lt;p&gt;$\mathbf{Z}_{t}$ and $\mathbf{\Sigma}_{t}$ are redefined as appropriate in these cases.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-03T21:20:39.893" Id="114246" LastActivityDate="2014-12-10T19:54:43.267" LastEditDate="2014-12-10T19:54:43.267" LastEditorUserId="44269" OwnerUserId="44269" PostTypeId="1" Score="2" Tags="&lt;confidence-interval&gt;&lt;survival&gt;&lt;delta-method&gt;" Title="How to analytically estimate CIs on the survival function, $S_{t}$, in a logit hazard model" ViewCount="71" />
  <row AcceptedAnswerId="114284" AnswerCount="1" Body="&lt;p&gt;I need to sample a variable from a distribution that's like a binomial distribution except with a &quot;bias&quot;, I'm not sure what it may be called:&#10;$p(X=k)$ is proportional to $k.B(n,p)(k)$&#10;where $B(n,p)$ is the binomial distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any ideas on how this can be done?&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm working in python so if there's any &quot;direct&quot; way in numpy or other packages that would be extra useful...&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-09-03T22:20:23.680" Id="114255" LastActivityDate="2014-09-04T08:48:44.790" OwnerUserId="55202" PostTypeId="1" Score="1" Tags="&lt;sampling&gt;&lt;binomial&gt;&lt;python&gt;" Title="How to sample from &quot;biased&quot; binomial distribution (ideally in python/numpy)" ViewCount="96" />
  <row Body="&lt;p&gt;Many points to make.&lt;/p&gt;&#10;&#10;&lt;p&gt;1) you're fitting the zeroes between your actual data (it looks like the data can take only integer values), fitting zeroes where you can't have data is pulling your curve down.&lt;/p&gt;&#10;&#10;&lt;p&gt;2) you're dealing with a discrete distribution (in fact, insertion length sounds like a count of how many whatevers were inserted in whatever), so the relevant distribution with exponential-like decay is called the &lt;a href=&quot;http://en.wikipedia.org/wiki/Geometric_distribution&quot; rel=&quot;nofollow&quot;&gt;&lt;em&gt;geometric&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;3) Maximum likelihood estimation is easy in this case. $\hat{p}=1/\bar{x}$. (what's &lt;code&gt;mean(data[1,])&lt;/code&gt;?). From that estimate you can then obtain the fitted exponential curve.&lt;/p&gt;&#10;&#10;&lt;p&gt;4) Even if you do want to directly fit an exponential curve, the problem with nonlinear least squares (even if you do it right by only fitting where the data can occur) is that the counts in each of the bins are not equally variable (you're putting relatively too much weight on observations that are less certain). &lt;/p&gt;&#10;&#10;&lt;p&gt;One could use a GLM with log-link instead, and take proper account of the multinomial nature of the data. But it won't do any better than just fitting the geometric via maximum likelihood.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-09-04T00:30:55.453" Id="114267" LastActivityDate="2014-09-04T00:58:07.360" LastEditDate="2014-09-04T00:58:07.360" LastEditorUserId="805" OwnerUserId="805" ParentId="114266" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;To start with, &lt;code&gt;PHt&lt;/code&gt; is your dependent variable, not independent variable. In addition, your model 1 can simply be written as &lt;code&gt;aov(PHt~REGION*MANAGEMENT,data=data)&lt;/code&gt; because the &lt;code&gt;*&lt;/code&gt; symbol denotes both main effects and interaction. If you want interaction only, it should be &lt;code&gt;REGION:MANAGEMENT&lt;/code&gt;. See &lt;code&gt;?formula&lt;/code&gt; for more detail.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now I will try to answer your 3 questions in reverse order.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;A3:&lt;/strong&gt; &#10;The fact that the design is unbalanced really suggests you should be using mixed model (unless it is very close to being balanced, then perhaps you can stay with &lt;code&gt;aov&lt;/code&gt;) Google the R package &lt;code&gt;nlme&lt;/code&gt; for fitting mixed model.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;A2:&lt;/strong&gt;&#10;The MS for Residuals in each error stratum (&lt;code&gt;subject_f:REGION&lt;/code&gt;, then &lt;code&gt;subject_f:MANAGEMENT&lt;/code&gt;, then &lt;code&gt;subject_f:REGION:MANAGEMENT&lt;/code&gt;) is trying to estimate the variability attributed to that specific stratum. So the first MSResidual, 0.02334, is saying &lt;code&gt;subject_f:REGION&lt;/code&gt; adds $\sigma^2\approx 0.02334^2$ to the total variability in the data, while &lt;code&gt;subject_f:MANAGEMENT&lt;/code&gt; adds approximately $0.02325^2$ to the total variation in data. Since they are different factors that add to the total variability, it makes sense that they are different.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;A1:&lt;/strong&gt;&#10;I am actually not very sure what you are asking. I guess the surface answer is yes, if you want to include the interaction term in the &lt;code&gt;Error()&lt;/code&gt; term, then it is correct to 'include the interaction term in the &lt;code&gt;Error()&lt;/code&gt; term'. However, is this really what you want? It is not immediately clear to me that &lt;code&gt;MANAGEMENT&lt;/code&gt; should appear in the &lt;code&gt;Error()&lt;/code&gt; term. I am not saying it can't be the case, but are you sure that's what you want? Also, what is the interpretation of &lt;code&gt;subject/MANAGEMENT*REGION&lt;/code&gt;?&lt;/p&gt;&#10;&#10;&lt;p&gt;I personally would suggest that you consult your local statistical expert. But if you really must do it yourself, then perhaps &lt;a href=&quot;http://stat.bell-labs.com/NLME/UGuide.pdf&quot; rel=&quot;nofollow&quot;&gt;read up&lt;/a&gt; on mixed model? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-04T01:21:02.673" Id="114275" LastActivityDate="2014-09-04T01:48:24.733" LastEditDate="2014-09-04T01:48:24.733" LastEditorUserId="28309" OwnerUserId="28309" ParentId="113675" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Guilherme is on the money here. While the other responses are useful, please note that logistic regression (and all nonlinear regression like Poisson, for that matter) are fundamentally different than linear regression. There may be serious issues with the logit scaling factor when running the same analysis on six different data sets and then running that analysis on the combined data set. Changes in coefficients may have nothing to do with meaningful differences (even if statistically significant or substantively important).  They could have everything to do with unobserved heterogeneity across the samples. You absolutely have to test for that. Many (if not most) researchers in social and policy science fields ignore this. Guilherme gives the seminal articles on this that I recommend everyone look at. Peters suggestions are practical, but simply coding a dummy variable for the sample the data come from will not address this heterogeneity in the scaling factor. You can do that in linear regression and the heterogeneity shouldn't affect your coefficients, but here it may. &lt;/p&gt;&#10;&#10;&lt;p&gt;One other aspect to the effect of unobserved heterogeneity unique to logit vs. linear regression, is the effect of different regressors in each data set. If you don't have the same variables, or likely if they are measured differently, you have a form of omitted variable bias. Unlike with linear regression, an omitted variable orthogonal to your key regressor can still bias your estimate. As Cramer puts it:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Even with orthogonal regressors, then, omitted variables depress $\hat β$ towards zero, relatively to its value in the full equation. In other words, the $\hat β$ of discrete models vary inversely with the extent of unobserved heterogeneity. The practical consequence is that estimates from samples that differ in this respect are not directly comparable.&#10;  (&lt;a href=&quot;http://dare.uva.nl/document/2/96199&quot; rel=&quot;nofollow&quot;&gt;http://dare.uva.nl/document/2/96199&lt;/a&gt;)&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Cramer also points out though the coefficient estimates are biased downward when omitting a variable, the partial derivatives are not. This is fairly complicated and you should read the article for a more lucid explanation – the overall point is, don't exclusively look at the log-odds or odds ratios. Consider predicted probabilities and derivatives; see the margins command in Stata for more details. JD Long has a paper that goes into detail here. &lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, there are a number of papers you can Google for that discuss interaction terms in logit models. My understanding has been that take the logit coefficient on an interaction as a guide but not definitive, especially if you prefer to see the coefficients as exponentiated odds ratios. Looking at predicted probabilities and average marginal effect is better (again, look up documentation on Stata's margin command for logit, even if you use SPSS this will still be helpful). &lt;/p&gt;&#10;&#10;&lt;p&gt;I'm not familiar enough with SPSS to know how that package can deal with these issues, but I will say this: when you get into deeper statistical issues like this, it is an indication that it is time for you to move to a more flexible, sophisticated package like Stata or R. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-09-04T01:23:11.367" Id="114276" LastActivityDate="2014-09-04T01:45:56.837" LastEditDate="2014-09-04T01:45:56.837" LastEditorUserId="32036" OwnerUserId="42898" ParentId="8718" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;The definition of the canonical (or natural) parameter in the exponential family / generalized linear models is a precise mathematical one. It's not necessarily what might seem &quot;natural&quot; to casual inspection (though it often makes sense).&lt;/p&gt;&#10;&#10;&lt;p&gt;In particular, for models of the form&lt;/p&gt;&#10;&#10;&lt;p&gt;$$f(y | \theta, \phi) = h(y,\phi) \exp{\left(\frac{b(\theta)T(y) - A(\theta)}{d(\phi)} \right)}. \,$$&lt;/p&gt;&#10;&#10;&lt;p&gt;if the parameterization is chosen such that $b(\theta)=\theta$, then $\theta$ is the natural, or canonical parameter. When this paramaterization is used, there are some nice consequences which make it attractive (the natural parameter space is always convex, for example).&lt;/p&gt;&#10;&#10;&lt;p&gt;Further details are here:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Exponential_family&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Exponential_family&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Generalized_linear_model&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Generalized_linear_model&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-04T01:23:53.487" Id="114277" LastActivityDate="2014-09-04T01:59:11.793" LastEditDate="2014-09-04T01:59:11.793" LastEditorUserId="805" OwnerUserId="805" ParentId="114270" PostTypeId="2" Score="2" />
  
  
  <row AcceptedAnswerId="114338" AnswerCount="2" Body="&lt;p&gt;I am running generalised linear mixed effects models in R using the lme4 package. I am wondering if there are any post-hoc tests available for models built using the glmer function?&lt;/p&gt;&#10;&#10;&lt;p&gt;I know the package lmerTest has a function for post hoc testing lmer models (class merMod), but it won't work for objects of class glmerMod (method given in the &lt;a href=&quot;http://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf&quot; rel=&quot;nofollow&quot;&gt;lmerTest manual&lt;/a&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;My models have either a numeric or integer response variable, two fixed effects (both a factor with 2 levels), and two random effects (both a factor, one nested within the other). I have used either a Gamma (for numeric response) or poisson (for integer response) error family. I want to know which combinations of levels of the fixed effects are statistically different to other combinations.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Are there any options for post hoc testing this type of model?&lt;/li&gt;&#10;&lt;li&gt;If not, can anyone recommend another method for performing generalised linear mixed effects models in R that do allow for post hoc testing?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Edit--&lt;/p&gt;&#10;&#10;&lt;p&gt;The model I am running for ANCOVA analysis is:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;m1&amp;lt;-glmer(data=mydata,FLWR_MASS~BASE_MASS*F1TREAT*SO+&#10;    (1 |LINE/MATERNAL_ID),family=Gamma(link=log))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Where FLWR_MASS and BASE_MASS are numeric, and F1TREAT and SO are both factors each with 2 levels.&lt;/p&gt;&#10;&#10;&lt;p&gt;The code I am using for the post hoc testing of the slope is:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;testInteractions(m1, custom=list(F1TREAT='control', SO=c(1,-1),&#10;    slope='BASE_MASS', adjustment=&quot;none&quot;))&#10;testInteractions(m1, custom=list(F1TREAT='stress', SO=c(1,-1),&#10;    slope='BASE_MASS', adjustment=&quot;none&quot;))&#10;testInteractions(m1, custom=list(SO='s', F1TREAT=c(1,-1),&#10;    slope=&quot;BASE_MASS&quot;, adjustment=&quot;none&quot;))&#10;testInteractions(m1, custom=list(SO='o', F1TREAT=c(1,-1),&#10;    slope=&quot;BASE_MASS&quot;, adjustment=&quot;none&quot;)) &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;However, as I've mentioned I get the same output regardless of what I specify as the slope (even if it is a term not included in the model)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-04T04:44:14.650" FavoriteCount="1" Id="114291" LastActivityDate="2014-09-30T05:10:32.223" LastEditDate="2014-09-22T04:38:08.947" LastEditorUserId="55217" OwnerUserId="55217" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;post-hoc&gt;&lt;lme4&gt;" Title="Post hoc test for object of class glmerMod in R?" ViewCount="173" />
  
  <row Body="&lt;p&gt;Hey here is a contribution, after I read about it a bit. Probably a bit late for the person who asked, but maybe worth for someone else.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;For the mean case :&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;Consider the problem $argmin_x \sum_{i=1}^n (y_i - x)$&lt;/p&gt;&#10;&#10;&lt;p&gt;Introduce $f(x) = \sum_{i=1}^n(y_i - x)^2$&lt;/p&gt;&#10;&#10;&lt;p&gt;$f'(x)=0 \Leftrightarrow 2 \sum_{i=1}^n (y_i - x ) = 0$&lt;/p&gt;&#10;&#10;&lt;p&gt;$f'(x)=0\Leftrightarrow \sum_{i=1}^n y_i = \sum_{i=1}^n x$&lt;/p&gt;&#10;&#10;&lt;p&gt;$f'(x)=0\Leftrightarrow x = \frac{\sum_{i=1}^n}{n}$&lt;/p&gt;&#10;&#10;&lt;p&gt;As the function is convex, this is a minimum&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;For the median case&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Consider the problem $argmin_x \sum_{i=1}^n |y_i - x|$&lt;/p&gt;&#10;&#10;&lt;p&gt;Introduce $f(x) = \sum_{i=1}^n|y_i - x|$&lt;/p&gt;&#10;&#10;&lt;p&gt;$f'(x)=0 \Leftrightarrow  \sum_{i=1}^n sgn(y_i - x ) = 0$ &lt;/p&gt;&#10;&#10;&lt;p&gt;(where $sgn(x)$ is the sign of x : $sgn(x)=1$ if $x &amp;gt;0$ and $sgn(x)=-1$ if $x&amp;lt;0$)&lt;/p&gt;&#10;&#10;&lt;p&gt;$f'(x)=0\Leftrightarrow \# \{y_i / y_i &amp;gt;x \} -  \# \{y_i / y_i &amp;lt;x \} = 0  $ &lt;/p&gt;&#10;&#10;&lt;p&gt;(where $\#{}$ is the cardinal of the space, so in this discrete case, the number of elements in it)&lt;/p&gt;&#10;&#10;&lt;p&gt;$f'(x)=0\Leftrightarrow x$ is the median if n is odd (you have to refine a bit if it is even, but the principle is the same).&lt;/p&gt;&#10;&#10;&lt;p&gt;As the function is convex too, this is a minimum again.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-09-04T09:17:45.583" Id="114308" LastActivityDate="2014-09-04T11:08:20.517" LastEditDate="2014-09-04T11:08:20.517" LastEditorUserId="26008" OwnerUserId="26008" ParentId="7307" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;Could you please help me with such a problem:&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's assume I have multidimensional data that consists of 30 samples and 10 variables. This is a 30x10 matrix. I have three groups of equal size (10 samples per group) and the samples are paired. The question is to find variables that bring statistical significant results.&lt;/p&gt;&#10;&#10;&lt;p&gt;I can use Friedman test to find corresponding p-values. However, I don't know how to correctly use multiple comparisons. If I use post-hoc test to identify the group that significantly different from others, should I apply also, for example, Bonferroni correction considering the amount of variables I have?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-04T10:26:51.997" Id="114309" LastActivityDate="2014-09-04T10:26:51.997" OwnerUserId="43820" PostTypeId="1" Score="0" Tags="&lt;multiple-comparisons&gt;&lt;post-hoc&gt;&lt;bonferroni&gt;" Title="Multiple comparisons, multidimensional data, several groups" ViewCount="29" />
  
  <row AnswerCount="2" Body="&lt;p&gt;So, imagine having access to sufficient data (millions of datapoints for training and testing) of sufficient quality. Please ignore concept drift for now and assume the data static and does not change over time. Does it even make sense to use all of that data in terms of the quality of the model?&lt;/p&gt;&#10;&#10;&lt;p&gt;Brain and Webb (&lt;a href=&quot;http://www.csse.monash.edu.au/~webb/Files/BrainWebb99.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.csse.monash.edu.au/~webb/Files/BrainWebb99.pdf&lt;/a&gt;) have included some results on experimenting with different dataset sizes. Their tested algorithms converge to being somewhat stable after training with 16,000 or 32,000 datapoints. However, since we're living in the big data world we have access to data sets of millions of points, so the paper is somewhat relevant but hugely outdated.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there any know more recent research on the impact of dataset sizes on learning algorithms (Naive Bayes, Decision Trees, SVM, neural networks etc).&lt;/p&gt;&#10;&#10;&lt;p&gt;When does a learning algorithm converge to a certain stable model for which more data does not increase the quality anymore?&#10;Can it happen after 50,000 datapoints, or maybe after 200,000 or only after 1,000,000?&#10;Is there a rule of thumb?&#10;Or maybe there is no way for an algorithm to converge to a stable model, to a certain equilibrium?&#10;Why am I asking this? Imagine a system with limited storage and a huge amount of unique models (thousands of models with their own unique dataset) and no way of increasing the storage. So limiting the size of a dataset is important.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any thoughts or research on this?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-09-04T12:41:45.160" FavoriteCount="3" Id="114324" LastActivityDate="2014-11-19T05:48:57.957" OwnerUserId="55235" PostTypeId="1" Score="2" Tags="&lt;machine-learning&gt;&lt;large-data&gt;" Title="Does the dataset size influence a machine learning algorithm?" ViewCount="127" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm trying to replicate the analysis presented on the bottenada.se and described in the &lt;a href=&quot;http://github.com/MansMeg/Ada/blob/master/Demonstration/demo.md&quot; rel=&quot;nofollow&quot;&gt;ADA repository&lt;/a&gt;. Although the demo file is quite detailed, I didn't grasp where those values for the &quot;C0&quot; component come from. I would like to try other m0 combinations, but I couldn't understand how can I generate C0 values to fill in the model. The help file says:&lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;You need to define your prior for the election in two steps. First give your best estimates where you believe the election will end. Then you need to specify this as a covariance matrix. In Ada we specify our priors the following way (as of august 9).&quot;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; priors2014$m0 &amp;lt;- c(0.227, 0.0665, 0.0645, 0.0595, 0.353, 0.08, 0.08, 0.081)&#10;    priors2014$C0 &amp;lt;- diag(c(0.001024, 0.000144, 0.000144, 0.000144, 0.001024, 0.000625, &#10;    0.000625, 0.000625))&#10;colnames($)priors2014C0 &amp;lt;- rownames($)priors2014C0 &amp;lt;- names(priors2014$m0) &amp;lt;- c(&quot;M&quot;, &#10;    &quot;FP&quot;, &quot;C&quot;, &quot;KD&quot;, &quot;S&quot;, &quot;V&quot;, &quot;MP&quot;, &quot;SD&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="5" CreationDate="2014-09-04T12:55:02.813" Id="114326" LastActivityDate="2014-09-04T12:55:02.813" OwnerUserId="48599" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;bayes&gt;" Title="Specify a covariance matrix" ViewCount="27" />
  <row AnswerCount="1" Body="&lt;p&gt;I did a spectrometry measurement with the evaluation of the concentration of an element. For the element of interest I've two peaks (with normal distribution).&lt;/p&gt;&#10;&#10;&lt;p&gt;$ C_1 ± \sigma_1 $    where $ \sigma_1 = \sqrt{C_1} $ and number of counts &lt;em&gt;n&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;$ C_2 ± \sigma_2 $  where $ \sigma_2 = \sqrt{C_2} $ and number of counts &lt;em&gt;m&lt;/em&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;The concentration is evaluated by the ratio between counts $ C_i $ and mass $ m ± \sigma_m $ and time $t$ (with negligible measurement error).&lt;/p&gt;&#10;&#10;&lt;p&gt;$ A_i = C_i/(m*t) $  and the measurement uncertainty is $\sigma_{Ai} = f ( \sigma_i  ;\sigma_m) $ &lt;/p&gt;&#10;&#10;&lt;p&gt;At the end I quantify the ratio $ R=A_1/A_2 $ with $\sigma_R = f ( \sigma_{a1}  ;\sigma_{a2}) $&lt;/p&gt;&#10;&#10;&lt;p&gt;I measured the ratio &lt;strong&gt;R&lt;/strong&gt; for three different samples of soil (they were sampled in different places.)&lt;/p&gt;&#10;&#10;&lt;p&gt;soil &lt;em&gt;s1&lt;/em&gt; --&gt;  $ R_{s1} ± \sigma_{s1} $&lt;/p&gt;&#10;&#10;&lt;p&gt;soil &lt;em&gt;s2&lt;/em&gt; --&gt;  $ R_{s2} ± \sigma_{s2} $&lt;/p&gt;&#10;&#10;&lt;p&gt;soil &lt;em&gt;s3&lt;/em&gt; --&gt;  $ R_{s3} ± \sigma_{s3} $&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;question&lt;/strong&gt; I have to compare these three results to evaluate if they are  similar or different. How could I make this comparison? &lt;/p&gt;&#10;&#10;&lt;p&gt;How could I make the comparison between the soil &lt;em&gt;s1&lt;/em&gt; and the average(&lt;em&gt;s2&lt;/em&gt;;&lt;em&gt;s3&lt;/em&gt;)? one sample &lt;em&gt;t-test&lt;/em&gt;?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-09-04T13:20:28.440" Id="114329" LastActivityDate="2014-09-04T16:14:59.157" LastEditDate="2014-09-04T16:14:59.157" LastEditorUserId="22047" OwnerUserId="54887" PostTypeId="1" Score="0" Tags="&lt;hypothesis-testing&gt;&lt;multiple-comparisons&gt;" Title="How to compare concentration ratios" ViewCount="31" />
  <row AcceptedAnswerId="114358" AnswerCount="1" Body="&lt;p&gt;My question is simple: Is there any results regarding the distribution of log-likelihood function gradient? &#10;It may be asymptotic results as well.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-04T13:45:36.293" Id="114332" LastActivityDate="2014-09-04T16:26:54.950" OwnerUserId="9343" PostTypeId="1" Score="1" Tags="&lt;distributions&gt;&lt;likelihood&gt;&lt;gradient&gt;" Title="Distribution of log-likelihood gradient" ViewCount="32" />
  <row Body="&lt;p&gt;I think statistics should always focus on their use. In your example, the overall question to compare the safety of flying with driving is hardly useful: You cannot choose to drive from New York to Paris in France by car. In turn, you cannot choose to go to your next supermarket by plane.&lt;/p&gt;&#10;&#10;&lt;p&gt;But you can decide if you want to travel from Madrid to Paris by either plane or car. This question is much more useful. Also, comparisons between both become automatically fairer because now you compare where both are more comparable. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, you might find that the actual choice between both by the subjects in your sample might depend on factors like cost, travelling time and luggage. You may find that these factors could also be related to security preferences. You could then try to adjust for these confounding factors by some model if you want to compare only the technical security itself. Or you don't consider these factors as confounders because you are interested in the security of the transportation including the different usages in terms of safety.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the end, fair statistics is possible by isolating closely the factors involved from the factors you want to have a statement about.&lt;/p&gt;&#10;&#10;&lt;p&gt;Another requirement is to choose the measurable variable that represents best the point of interest: Is safety understood in terms of life hazards or in terms of injuries? There, it depends on what you are afraid of.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, there are questions that cannot be answered by statistical means due to lack of measurable variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the end, fair statistics is possible by isolating closely the factors involved from the factors you want to have a statement about.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-04T15:03:41.667" Id="114348" LastActivityDate="2014-09-05T05:14:16.153" LastEditDate="2014-09-05T05:14:16.153" LastEditorUserId="41422" OwnerUserId="28705" ParentId="114310" PostTypeId="2" Score="3" />
  <row AnswerCount="1" Body="&lt;p&gt;I am designing a randomised pretest-posttest trial for which I'm trying to do a sample size calculation. My brain is about to explode from confusion, so I hope someone can help me.&lt;/p&gt;&#10;&#10;&lt;p&gt;Just a little summary of what the trial will look like. We have 2 interventions which will be delivered in 3 arms: intervention 1 only, intervention 1+2, intervention 2 only. The outcome of the trial is a difference in stage of change, which is a 5-category outcome reflecting readiness to change. Each participant will be in one stage at start and the same or another stage at the end of the trial. A higher number of stage is better. However, a difference between stage 1 and stage 2 can not be considered equal to a difference between stage 3 and 4.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am not sure yet how the outcome will be presented, but I think it will look like this. I will categorise each participant as either improving (e.g. from stage 2 to 4), no effect (e.g. remaining in stage 3) or decreasing (e.g. moving from stage 4 to 1). Another option is presenting the distribution over the 5 stages (proportions).&lt;/p&gt;&#10;&#10;&lt;p&gt;Now this is more or less where I get lost. I need to do a sample size calculation and I just don't know how to go about this. I have found previous articles on both the distribution over the stages pre and post, as well as on numbers/proportions of participants in each of the three categories as I described above (increase, no effect, decrease) pre and post. &lt;/p&gt;&#10;&#10;&lt;p&gt;What type of sample size calculation should I use? And related, what statistical methods can (should) be used to analyse the data?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-09-04T16:13:30.897" Id="114355" LastActivityDate="2014-09-04T17:18:47.497" LastEditDate="2014-09-04T16:19:37.027" LastEditorUserId="7290" OwnerUserId="55170" PostTypeId="1" Score="1" Tags="&lt;repeated-measures&gt;&lt;ordinal&gt;&lt;power-analysis&gt;" Title="Power analysis for repeated measures design with ordinal response" ViewCount="41" />
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I am wondering how could I line plot mean with error bar of cross tabulated data in R package.&#10;My data looks like this&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Mean &#10;           Stage&#10;Treatment    breaker        green        pink             red          turning&#10;     cont    0.06779080    0.1964575    0.010335260    0.005861540    0.05828315&#10;     FRM     0.17211680    0.1853831    0.021794200    0.013183020    0.09467819&#10;     KNO3    0.24755945    0.2863050    0.007866763    0.002554247    0.06097948&#10;     LFR     0.08053258    0.1287083    0.008566730    0.000000000    0.07710268&#10;     salt    0.38564291    0.4419331    0.050551620    0.001115087    0.14508939&#10;&#10;Standard deviation&#10;         Stage&#10;Treatment    breaker        green        pink            red            turning&#10;     cont    0.01504616    0.14157770    0.003498454    0.0037076134    0.032316651&#10;     FRM     0.02710538    0.03422274    0.017164023    0.0143484928    0.058139975&#10;     KNO3    0.05372195    0.12865178    0.003552846    0.0006738978    0.001983925&#10;     LFR     0.04366554    0.05611974    0.009180117    0.0000000000    0.029396442&#10;     salt    0.03035888    0.12908778    0.028818513    0.0004681397    0.050566535&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="7" CreationDate="2014-09-04T21:30:14.040" Id="114388" LastActivityDate="2014-09-28T19:15:08.720" LastEditDate="2014-09-04T21:39:10.823" LastEditorUserId="54300" OwnerUserId="55138" PostTypeId="1" Score="1" Tags="&lt;data-visualization&gt;" Title="plotting line graph with error bar in R from cross tabulated data" ViewCount="132" />
  <row Body="&lt;p&gt;If you are comparing distributions by their shape a more relevant test would be the two-sample K-S test, particularly given the large number of observations. A K-S test is a comparison of the empirical CDF's (&lt;a href=&quot;http://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test&lt;/a&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;Two sided K-S tests are available in R through &quot;ks.test&quot;.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-09-04T21:32:18.253" Id="114389" LastActivityDate="2014-09-04T21:32:18.253" OwnerUserId="2339" ParentId="114254" PostTypeId="2" Score="2" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a dataset where one variable is a proportion with a restricted range (say 4%). I would like to use this variable to predict a death rate. Running a simple correlation I'm guessing would underestimate the relationship (?). I also have raw counts for both variables. What analysis do I do here? I'm stuck. Binomial regression? Below is an excerpt of my early attempts....&lt;/p&gt;&#10;&#10;&lt;p&gt;Firearm denial proportion and firearm suicide rate are positively correlated  (r = .25, p &amp;lt; .01). There is a range restriction problem with using denials as a proportion (R = 4.13), but that should only result in underestimating the correlation. &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-09-04T22:02:50.173" Id="114396" LastActivityDate="2014-09-04T22:02:50.173" OwnerUserId="55267" PostTypeId="1" Score="1" Tags="&lt;proportion&gt;" Title="How to analyze data with restricted range proportions" ViewCount="17" />
  
  <row AcceptedAnswerId="116420" AnswerCount="1" Body="&lt;p&gt;As part of reproducing a model I described partially in this &lt;a href=&quot;http://stackoverflow.com/questions/25654487/multivariate-normal-changes-value-of-variable-in-pymc&quot;&gt;question&lt;/a&gt; on Stack Overflow, I want to obtain a plot of a posterior distribution. The (spatial) model describes the selling price of some properties as a Bernoulli distribution depending on whether the property is expensive (1) or cheap (0). In equations:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$y_{i} \sim \text{Bernoulli}(p_{i})$$&#10;$$p_{i} \sim \text{logit}^{-1}(b_{0} + b_{1}\text{LivingArea}/1000 + b_{2}\text{Age} + w({\bf{s}}))$$&#10;$$w({\bf{s}}) \sim \text{MVN}({\bf{0}}, {\bf{\Sigma}}) $$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $y_{i}$ is the binary result 1 or 0, $p_{i}$ is the probability of being cheap or expensive, $w({\bf{s}})$ is a spatial random variable where $\bf{s}$  represents its position. All of this for each  $i = \{1, ..., 70\}$ because there are 70 properties in the dataset. $\bf{\Sigma}$ is a covariance matrix based on the geographical position of the data points. If you're curious about this model, the dataset can be found &lt;a href=&quot;http://pastebin.com/raw.php?i=41us4HVj&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;The plot I want to obtain is the following contour plot:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/XGaGp.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The figure is described as &quot;Image plot of the posterior median surface of the latent process $w({\bf{s}})$, binary spatial model&quot;. The book also says this:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Figure 5.8 shows the image plot with overlaid contour lines for the&#10;  posterior mean surface of the latent $w({\bf{s}})$ process.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;However, there are only 70 pairs of points in the dataset. I suppose that, in order to produce a contour plot, I need to estimate $w({\bf{s}})$ in 70*70 points. So, my question is: How do I produce this posterior median surface? So far I have samples of posterior distributions for all the parameters involved (using PyMC) and I know that I can predict $y^*$ at a new point using the posterior predictive distribution. However, I don't know how to predict values $w({\bf{s}})$ at a new point $s^*$. Maybe I'm wrong and the plot wasn't constructed by prediction but by interpolation.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;UPDATE&lt;/strong&gt;:&lt;/p&gt;&#10;&#10;&lt;p&gt;First, this is the median of the posterior distribution of $w({\bf{s}})$ at each location where there is a property. This is based on the MCMC trace for $w$. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/J3Fss.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;And this is the interpolation (with a contour plot) using a radial basis function:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/L0tGQ.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;(If you're interested in the code, let me know)&lt;/p&gt;&#10;&#10;&lt;p&gt;As you can see, there are significant differences in the plots. A couple of questions: &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;How can I know if these differences are explained by the interpolation procedure? &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Maybe, there are important variations in the posterior distribution of $w({\bf{s}})$ that I calculated and the one showed in the book. How much variation is acceptable between MCMC simulations? Even my own parameters change a bit depending on the sampling I use (Metropolis, Metropolis Adaptive.)&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Is there some Bayesian procedure to predict points $w(s)$ in order to generate a contour plot as I did using radial basis function?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="14" CreationDate="2014-09-05T04:31:44.060" Id="114412" LastActivityDate="2014-09-23T06:16:26.667" LastEditDate="2014-09-08T03:31:51.307" LastEditorUserId="2676" OwnerUserId="2676" PostTypeId="1" Score="7" Tags="&lt;prediction&gt;&lt;spatial&gt;&lt;posterior&gt;&lt;hierarchical-bayesian&gt;&lt;pymc&gt;" Title="Plotting a &quot;posterior median surface&quot;" ViewCount="142" />
  <row Body="&lt;p&gt;I wasn't able to actually test it (I mean, under R), but try&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;tapply( 1:length( NH4 ), as.factor( Day ), function( x ) { return( weighted.mean( NH4[ x ], Flow[ x ] ) ) } )&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="6" CreationDate="2014-09-05T08:42:14.007" Id="114421" LastActivityDate="2014-09-05T08:52:47.403" LastEditDate="2014-09-05T08:52:47.403" LastEditorUserId="6476" OwnerUserId="6476" ParentId="114418" PostTypeId="2" Score="0" />
  <row AnswerCount="1" Body="&lt;p&gt;The likelihood function of a lognormal distribution is:&lt;/p&gt;&#10;&#10;&lt;p&gt;$f(x; \mu, \sigma) \propto \prod_{i_1}^n \frac{1}{\sigma x_i} \exp \left ( - \frac{(\ln{x_i} - \mu)^2}{2 \sigma^2} \right )   $&lt;/p&gt;&#10;&#10;&lt;p&gt;and Jeffreys's Prior is:&lt;/p&gt;&#10;&#10;&lt;p&gt;$p(\mu,\sigma) \propto \frac{1}{\sigma^2}  $ &lt;/p&gt;&#10;&#10;&lt;p&gt;so combining the two gives:&lt;/p&gt;&#10;&#10;&lt;p&gt;$f(\mu,\sigma^2|x)= \prod_{i_1}^n \frac{1}{\sigma x_i} \exp \left ( - \frac{(\ln{x_i} - \mu)^2}{2 \sigma^2} \right ) \cdot \sigma^{-2} $&lt;/p&gt;&#10;&#10;&lt;p&gt;I know that the posterior density for $\sigma^2$ is inverse Gamma distributed, so I have to calculate &lt;/p&gt;&#10;&#10;&lt;p&gt;$f(\sigma^2|x) = \int f(\mu,\sigma^2|x) d\mu $&lt;/p&gt;&#10;&#10;&lt;p&gt;but I have no clue where to start here.&lt;/p&gt;&#10;&#10;&lt;p&gt;After Glen_b's comment I give it a shot:&lt;/p&gt;&#10;&#10;&lt;p&gt;$f(\mu,\sigma^2|x)= \prod_{i_1}^n \frac{1}{\sigma x_i} \exp \left ( - \frac{(\ln{x_i} - \mu)^2}{2 \sigma^2} \right ) \cdot \sigma^{-2} $&lt;/p&gt;&#10;&#10;&lt;p&gt;$= \sigma^{-n-2} \prod_{i=1}^n \frac{1}{x_i} \exp \left ( - \frac{1}{2\sigma^2} \sum_{i=1}^n (\ln x_i - \mu ) \right)  $&lt;/p&gt;&#10;&#10;&lt;p&gt;but I cannot see this going anywhere.&lt;/p&gt;&#10;&#10;&lt;p&gt;Another idea I got is to define $y_i=\ln(x_i)$, then $y$ is normal distributed.&#10;So&lt;/p&gt;&#10;&#10;&lt;p&gt;$f(\mu,\sigma^2 |y) = \left [ \prod_{i=1}^n \frac{1}{\sqrt{2 \pi}} \cdot \frac{1}{\sigma} \exp \left ( - \frac{1}{2  \sigma^2} (y_i - \mu)^2   \right )  \right ] \cdot \frac{1}{\sigma^2}$&lt;/p&gt;&#10;&#10;&lt;p&gt;$&#10;\propto \sigma^{-n-2} \cdot \exp \left ( - \frac{1}{2 \sigma^2} \sum_{i=1}^n (y_i - \bar y)^2  + n(\bar y - \mu)^2 \right ) &#10;$&#10;$&#10;= \sigma^{-n-2} \cdot \exp \left ( - \frac{1}{2 \sigma^2} ( (n-1)s^2 + n(\bar y - \mu)^2 ) \right )  &#10;$&#10;$&#10;= \sigma^{-n-2} \cdot \exp \left ( - \frac{1}{2 \sigma^2} ( (n-1)s^2 \right ) \exp \left (n(\bar y - \mu)^2 ) \right )  &#10;$&lt;/p&gt;&#10;&#10;&lt;p&gt;then integrate:&lt;/p&gt;&#10;&#10;&lt;p&gt;$&#10;\sigma^{-n-2} \cdot \exp \left ( - \frac{1}{2 \sigma^2} ( (n-1)s^2 \right ) \int \exp \left (  - \frac{1}{2 \sigma^2} n(\bar y - \mu)^2 ) \right ) d \mu&#10;$&lt;/p&gt;&#10;&#10;&lt;p&gt;by the method you suggested I get:&lt;/p&gt;&#10;&#10;&lt;p&gt;$&#10;\int \exp \left (  - \frac{1}{2 \sigma^2} n(\bar y - \mu)^2 ) \right ) d \mu = \sqrt{\frac{2\pi \sigma^2}{n}}&#10;$&lt;/p&gt;&#10;&#10;&lt;p&gt;So:&lt;/p&gt;&#10;&#10;&lt;p&gt;$&#10;\propto (\sigma^2)^{-(n+1)/2} \exp \left ( - \frac{1}{2 \sigma^2} ( (n-1)s^2 \right )&#10;$&lt;/p&gt;&#10;&#10;&lt;p&gt;which is indeed inverse Gamma distributed.&lt;/p&gt;&#10;&#10;&lt;p&gt;But I am unsure if this is correct, it's also the same result as I get for a normal likelihood.&lt;/p&gt;&#10;&#10;&lt;p&gt;I found this in the literature (without any further explanantion):&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/1lOV4.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-09-05T09:56:16.737" Id="114425" LastActivityDate="2014-09-06T07:55:02.383" LastEditDate="2014-09-06T07:55:02.383" LastEditorUserId="37541" OwnerUserId="37541" PostTypeId="1" Score="3" Tags="&lt;bayesian&gt;&lt;prior&gt;&lt;posterior&gt;&lt;conjugate-prior&gt;&lt;uninformative-prior&gt;" Title="Deriving the posterior density for a lognormal likelihood and Jeffreys's prior" ViewCount="323" />
  
  
  
  
  
  <row Body="&lt;p&gt;You might want to read this question I asked on Data Science before you start going crazy with the models: &lt;a href=&quot;http://datascience.stackexchange.com/questions/992/why-might-several-types-of-models-give-almost-identical-results&quot;&gt;http://datascience.stackexchange.com/questions/992/why-might-several-types-of-models-give-almost-identical-results&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-05T16:21:14.907" Id="114469" LastActivityDate="2014-09-05T16:21:14.907" OwnerUserId="46522" ParentId="114409" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;For a time series I wanted to plot separately the partial auto correlation. Below is the graph for a time series which shows PACF plot of the time series $x$ which I wanted to reproduce:  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/T53D6.png&quot; alt=&quot;acf_desired&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;This shows plot of two time series of two time series - one is a sequence of &#10;[-1,1] (variable s) and the other is just numeric numbers (x). The size of data = 256 points. X axis is the delay and Y axis is the PACF coefficients.&lt;br&gt;&#10;This is the plot of PACF that I got from the code:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/IFCGk.jpg&quot; alt=&quot;ACF which I got&quot;&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;This is my code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;N=256;&#10;x(1) = 0.1;&#10;% generate the time series&#10;for i =1 : N &#10;    x(i+1) = 4*x(i)*(1-x(i));&#10;end&#10;&#10;[Rx, lags] = xcorr(x, 'coeff');&#10;plot (Rx)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;where &lt;code&gt;data = x&lt;/code&gt; time series variable.&lt;br&gt;&#10;I have used the following command to plot plot &lt;code&gt;x&lt;/code&gt; and the graph shows the plot of two time series for diferent values of $x(1)$. Is this the correct way to do the plot? In Matlab there is a command &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;parcorr(x)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I even tried with this command, but the result is not any close to any of the two lines in the original plot the above picture.&#10;The axis are all different and appear weird. Can somebody please show how to plot PACF for time series?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-09-05T19:05:46.050" Id="114475" LastActivityDate="2014-09-05T20:21:35.993" LastEditDate="2014-09-05T20:21:35.993" LastEditorUserId="21160" OwnerUserId="21160" PostTypeId="1" Score="0" Tags="&lt;time-series&gt;&lt;matlab&gt;&lt;autocorrelation&gt;" Title="Matlab: Unable to plot partial autocorrelation plot" ViewCount="97" />
  <row Body="&lt;p&gt;&lt;strong&gt;A distribution with a linear PDF can be considered a special case of a (truncated) Pareto distribution, Beta distribution, or power distribution.&lt;/strong&gt;  Only particular values of the parameters in these distribution families will give a linear PDF, of course.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Among other things, such a distribution is a &lt;a href=&quot;http://en.wikipedia.org/wiki/Truncated_distribution&quot;&gt;truncated&lt;/a&gt; &lt;strong&gt;Generalized Pareto Distribution.&lt;/strong&gt;  The &lt;a href=&quot;http://en.wikipedia.org/wiki/Generalized_Pareto_distribution&quot;&gt;Wikipedia parameterization&lt;/a&gt; of the PDF (for the case where the endpoints $a$ and $b$ are finite, as they must be for a linear graph) can be expressed in terms of the basic function&lt;/p&gt;&#10;&#10;&lt;p&gt;$$f(x; \eta) = (1 - \eta x)^{1/\eta - 1}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;for $0\le x \le 1/\eta$.  (This can then be rescaled by $\sigma\ne 0$ and shifted by $\mu$ to obtain the most general form.  Here I have set $\eta = -\xi$ which will be a &lt;em&gt;positive&lt;/em&gt; number.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Evidently this function is linear in $x$ if and only if $1/\eta - 1=1$; that is, $\eta = 1/2$ (and so $\xi=-1/2$).  Its graph equals zero at the endpoint $x=1/\eta = 2$.  By &lt;em&gt;truncating&lt;/em&gt; it, though, we would obtain the general form stated in the question.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;The same thing can be obtained by truncating a generalized &lt;a href=&quot;http://en.wikipedia.org/wiki/Beta_distribution&quot;&gt;&lt;strong&gt;Beta Distribution&lt;/strong&gt;&lt;/a&gt;.  Its PDF is proportional to&lt;/p&gt;&#10;&#10;&lt;p&gt;$$x^{\alpha-1}(1-x)^{\beta-1}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;for $0\le x \le 1$, whence the Beta$(2,1)$ and Beta$(1,2)$ distributions are linear.  As with the Pareto$(\xi=-1/2)$ distribution, the graphs of these PDFs are zero at one endpoint.  The general linear PDF described in question is obtained in the same way via truncation, rescaling, and shifting.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Finally, &lt;a href=&quot;http://reference.wolfram.com/language/ref/PowerDistribution.html&quot;&gt;&lt;em&gt;Mathematica&lt;/em&gt;&lt;/a&gt; defines a &quot;&lt;strong&gt;power distribution&lt;/strong&gt;&quot; as one having a PDF proportional to &lt;/p&gt;&#10;&#10;&lt;p&gt;$$f(x; k, a) = x^{a-1}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;for $0 \le x \le 1/k$.  The case $a=2$ gives a linear PDF, identical to Beta$(2,1)$.  Rescaling it by $k$ and recentering it with a parameter $\mu$, and (once again) truncating it will yield the general PDF described in the question.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-09-05T22:44:31.553" Id="114491" LastActivityDate="2014-09-05T22:44:31.553" OwnerUserId="919" ParentId="114481" PostTypeId="2" Score="8" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Given a collection of series, &lt;em&gt;e.g.&lt;/em&gt; the nine depicted below,&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/vSFaZ.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;where each series has a rank associated with it, &lt;em&gt;e.g.&lt;/em&gt; magenta is #1, orange is #2, black is #3, and so on, &lt;strong&gt;I'd like to find a relationship between the &lt;em&gt;shape&lt;/em&gt; of a series and its rank. In other words, I want to know: Are there certain &lt;em&gt;characteristics&lt;/em&gt; of a series that correlates to it having a higher rank?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Of course, as stated, the problem is underspecified, since there can be as many &quot;characteristics&quot; as one can imagine—for examples we might describe series as being &quot;volatile,&quot; or having certain kinds of &quot;inflections,&quot; or &quot;upswings toward the end,&quot; etc. But, I'm thinking there must exist a general approach—if there's not a clear technique, then I'd be grateful for terminology that may lead me to a strategy. (Currently I don't know how to word this except as &quot;a correlation between a variable and a series.&quot;)&lt;/p&gt;&#10;&#10;&lt;p&gt;If I had to solve it on my own, I might explore a genetic algorithm whose cost function penalizes the complexity of a characteristic (&lt;em&gt;e.g.&lt;/em&gt; number of piecewise components, or degrees of polynomials or differentials, needed to describe the characteristic), but all this is unrefined speculation.&lt;/p&gt;&#10;&#10;&lt;p&gt;For a related example, here's the above expressed as normalized deltas (essentially a derivative):&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/lSFMh.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-06T00:38:53.597" Id="114498" LastActivityDate="2014-09-06T00:38:53.597" OwnerUserId="25034" PostTypeId="1" Score="0" Tags="&lt;correlation&gt;" Title="Is there a way to correlate a variable with the shape of a series?" ViewCount="12" />
  
  
  <row Body="&lt;p&gt;Use fisher's exact test instead which gives a more accurate p value than the chi squared for low sample populations hyper-geometric distributions (finite distribution parameters).  &lt;/p&gt;&#10;&#10;&lt;p&gt;This link below tell of HYPGEOMDIST which is a pretty powerful excel function that should get you started.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.real-statistics.com/binomial-and-related-distributions/hypergeometric-distribution/&quot; rel=&quot;nofollow&quot;&gt;http://www.real-statistics.com/binomial-and-related-distributions/hypergeometric-distribution/&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-06T04:41:20.157" Id="114507" LastActivityDate="2014-09-06T04:41:20.157" OwnerUserId="42878" ParentId="114506" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Some models (here: linear regression) have parameters $\beta$:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \hat{y} = \sum_{i\in\{1..p\}} \beta_i x_i + \beta_0 $$&lt;/p&gt;&#10;&#10;&lt;p&gt;For the same number of input features, more complex models (here: basis expansion to a quadratic model) have more parameters:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \hat{y} = \sum_{i,j\in\{1..p\} \atop i\le j} \beta_{ij} x_ix_j + \sum_{i\in\{1..p\}} \beta_{i} x_i + \beta_0 $$&lt;/p&gt;&#10;&#10;&lt;p&gt;In general, models with more parameters are more flexible (because there are more parameters to tune to fit the model to the data), but are more difficult to fit and thus more likely tend to overfit.  Regularization helps overcome these problems by reducing the degrees of freedom for tuning, thus reducing the complexity a bit:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \hat{y} = \sum_{i,j\in\{1..p\} \atop i\le j} \beta_{ij} x_ix_j + \sum_{i\in\{1..p\}} \beta_{i} x_i + \beta_0 \textrm{, with} \sum \beta^2 \textrm{ small} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;We can express the 'model complexity' or 'degrees of freedom for tuning' in terms of 'effective parameters': For the model that you investigate, find the parameter-based model (without regularization) that has the same degree of freedom for tuning.  The number of parameters for that model is the effective number of parameters for your model.&lt;/p&gt;&#10;&#10;&lt;p&gt;This generalization can also be extended further to models that are not based on parameters (e.g. k nearest neighbor). Then the degree of complexity is known as &lt;em&gt;Vapnik–Chervonenkis (VC) dimension&lt;/em&gt;.  Conveniently, for (non-regularized) linear regression, it is $p+1$, the actual number of parameters.&#10;&lt;em&gt;Elements of Statistical Learning&lt;/em&gt;, section 7.9, gives more information about that (and they seem to use the terms 'VC', 'effective number of parameters' and 'model complexity' more or less interchangably).&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2014-09-06T11:27:07.897" Id="114524" LastActivityDate="2014-09-06T11:27:07.897" OwnerUserId="54325" ParentId="114434" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I think you just continue with your analysis. For one thing, negative income is not impossible. If you lose money (e.g. on investments) and don't make any, that would be negative income. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-09-06T13:27:30.527" Id="114531" LastActivityDate="2014-09-06T13:27:30.527" OwnerUserId="686" ParentId="114529" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;I'm a little unclear - do you want to compare the beta for the directors with that for the non-directors? Or a smaller group of directors with the other directors? In this answer, I'm assuming the latter.&lt;/p&gt;&#10;&#10;&lt;p&gt;You should fit one model, not two. My suggestion would be to include two dummy variables in the model: one, call it $I_{nd}$ is 1 if a non-director and 0 otherwise, and the other, $I_{sg}$, is 1 if in the subgroup of directors and 0 otherwise. Thus, for directors NOT in the subgroup, both dummies are zero. Those directors then become the &quot;reference&quot; group in the model, and the regression coefficients for those indicators will then estimate comparisons between their respective groups and the directors not in the subgroup. And their $t$ statistics give you tests of significance for those comparisons.&lt;/p&gt;&#10;&#10;&lt;p&gt;PS - I wonder if your model is too simple though, in that perhaps the should be other predictors as well for such things as the type of company. If important contributors to stock returns are not controlled for, the results of your analysis could be very misleading. The statistics are right only if the model explains the important things that are going on.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-06T18:05:40.630" Id="114546" LastActivityDate="2014-09-06T18:05:40.630" OwnerUserId="52554" ParentId="114530" PostTypeId="2" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;Choosing positives sample is a relative straightforward task, but I'm having some problem on determine what should I use for the negative example.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm working on a SVM binary classificator, trying to learn whether a pair of usernames belongs to the same individual.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is obvious what to use as a positive example, I've a set of usernames pairs I know belong to the same person. I'm shuffling randomly those same username to obtain my negative example, but i'm not sure whether is a good choice or not.&lt;/p&gt;&#10;&#10;&lt;p&gt;Do you have any suggestion? Should I randomly create two string as fake usernames?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-06T20:34:53.087" Id="114555" LastActivityDate="2014-09-06T20:34:53.087" OwnerUserId="55344" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;classification&gt;&lt;svm&gt;&lt;train&gt;" Title="How to choose negative training sample for Classification problem" ViewCount="44" />
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Say we want to minimize the function $f^2({\bf{x}})$, under the constraint&#10;$g({\bf{x}})=0$.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;The classic solution (&lt;strong&gt;Method I&lt;/strong&gt;) is to introduce a Lagrange Multiplier, and solve:&#10;$$\frac{\partial f^2({\bf{x}})}{\partial x_i}+\lambda \frac{\partial g({\bf{x}})}{\partial x_i}=0$$&lt;/p&gt;&#10;&#10;&lt;p&gt;One could however use a different method (&lt;strong&gt;Method II&lt;/strong&gt;), where we redefine the cost function as:&#10;$$J\equiv f^2({\bf{x}})+\Lambda g^2({\bf{x}})$$&#10;and then we must solve:&#10;$$\frac{\partial f^2({\bf{x}})}{\partial x_i}+\Lambda \frac{\partial g^2({\bf{x}})}{\partial x_i}=0$$&#10;Here $\Lambda$ is a set constant, and is not to be solved for like $\lambda$. Solving leads to a solution that is dependent on $\Lambda$, and is equivalent to method I, when $\Lambda\to\infty$. I think the second method has an advantage when the data is noisy and therefore will not exactly satisfy the constraint, but the choice for $\Lambda$ needs to be made manually.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Does Method II have a name? is it commonly used? what are the main&#10;pros and cons, other than the ones mentioned?&lt;/strong&gt;&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-09-07T08:08:11.567" Id="114590" LastActivityDate="2014-09-07T08:08:11.567" OwnerUserId="21026" PostTypeId="1" Score="1" Tags="&lt;optimization&gt;&lt;noise&gt;" Title="Lagrange Multipliers in practice" ViewCount="39" />
  <row AnswerCount="0" Body="&lt;p&gt;I know that machine learning is a very popular tool in pharmacy. Are there any books that describe use of machine learning in pharmacy? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-07T12:02:26.137" Id="114595" LastActivityDate="2014-09-07T12:02:26.137" OwnerUserId="2407" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;pharmacy&gt;" Title="What book about use of machine learning in pharmacy would you recommend?" ViewCount="33" />
  <row Body="&lt;p&gt;What questions would you ask about the data?&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;How long did the hamster spend on the wheel last night?&lt;/li&gt;&#10;&lt;li&gt;Was it longer or shorter than the night before that?&lt;/li&gt;&#10;&lt;li&gt;Is there any day per week that the hamster runs longer on the wheel? Eg does a hamster have a week day or weekend day %^)?&lt;/li&gt;&#10;&lt;li&gt;How long does the hamster stay on the wheel in a continuous stretch, on average?&lt;/li&gt;&#10;&lt;li&gt;Does different type of food affect the time the hamster spends on the wheel?&lt;/li&gt;&#10;&lt;li&gt;Do my activities change the hamster's wheel behaviour?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;All of these would suggest different types of plots. Aggregate time on wheel by day, plot over days. &lt;/p&gt;&#10;&#10;&lt;p&gt;See &lt;a href=&quot;http://journal.r-project.org/archive/2013-1/hofmann-unwin-cook.pdf&quot;&gt;http://journal.r-project.org/archive/2013-1/hofmann-unwin-cook.pdf&lt;/a&gt; for how to plot data in different ways to explore different things, answer different questions. One plot is almost never enough.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2014-09-07T12:28:25.920" Id="114596" LastActivityDate="2014-09-07T12:28:25.920" OwnerUserId="55365" ParentId="114305" PostTypeId="2" Score="5" />
  
  
  
  
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;In his up-voted answer to &lt;em&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/14437/software-needed-to-scrape-data-from-graph/72974&quot;&gt;Software needed to scrape data from graph&lt;/a&gt;&lt;/em&gt;, &lt;a href=&quot;http://stats.stackexchange.com/users/25283/alexey-popkov&quot;&gt;Alexey&lt;/a&gt; writes (with respect to the need for data scraping off of raster graphs) &quot;But nowadays the good practice is to publish graphs in vector form.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;To rip off my comment to Alexey from that page: &lt;strong&gt;Is there a good reference for best practices around which vector format(s) to present graphs of data in?&lt;/strong&gt; For example, ought I use an &lt;a href=&quot;https://en.wikipedia.org/wiki/Encapsulated_PostScript&quot; rel=&quot;nofollow&quot;&gt;eps&lt;/a&gt; encapsulation of an &lt;a href=&quot;https://en.wikipedia.org/wiki/Scalable_Vector_Graphics&quot; rel=&quot;nofollow&quot;&gt;svg&lt;/a&gt; file in my &lt;a href=&quot;https://en.wikipedia.org/wiki/LaTeX&quot; rel=&quot;nofollow&quot;&gt;LaTeX&lt;/a&gt; manuscripts, or am I supposed to &lt;a href=&quot;http://stackoverflow.com/questions/2979097/best-way-to-draw-a-bar-chart-in-latex&quot;&gt;output graph in LaTeX directly&lt;/a&gt;? Or something else?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-07T20:45:27.273" FavoriteCount="1" Id="114636" LastActivityDate="2014-09-07T21:45:00.780" LastEditDate="2014-09-07T20:57:11.207" LastEditorUserId="44269" OwnerUserId="44269" PostTypeId="1" Score="3" Tags="&lt;data-visualization&gt;&lt;communication&gt;&lt;reproducible-research&gt;" Title="Standards on presenting graphs and charts in vector format?" ViewCount="63" />
  <row Body="&lt;p&gt;This is analogous to the $z$-test vs the $t$-test in the univariate case, where if the variance is known, the distribution of the test statistic is normal ($z$-test), and if it is estimated, the distribution of the test statistic is $t$ ($t$-test), with the $t$-test converging to the $z$-test with large n.&lt;/p&gt;&#10;&#10;&lt;p&gt;Same thing in linear regression, if the error variance is assumed known (or large $n$ with the asymptotic assumption), then wald test. You are correct that the p-value based on the $F$ is usually reported. &lt;/p&gt;&#10;&#10;&lt;p&gt;In addition, note that the wald chi-square test reduces to the $z$-test with one variable, and that the $F$-test reduces to the $t$-test.&lt;/p&gt;&#10;&#10;&lt;p&gt;A wrinkle here - another test that uses the chi-squared distribution is the likelihood ratio test, though I don't think I've seen it referred to as a wald test before.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2014-09-07T21:00:00.570" Id="114640" LastActivityDate="2014-09-07T21:00:00.570" OwnerUserId="4485" ParentId="114632" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Or make your data and code available using R, &lt;a href=&quot;http://ggplot2.org/&quot; rel=&quot;nofollow&quot;&gt;ggplot2&lt;/a&gt; and &lt;a href=&quot;http://yihui.name/knitr/&quot; rel=&quot;nofollow&quot;&gt;knitr&lt;/a&gt; so it is fully reproducible. And then you can use the gridSVG package to create SVG versions of the ggplot2 plots.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-09-07T21:02:45.170" Id="114641" LastActivityDate="2014-09-07T21:45:00.780" LastEditDate="2014-09-07T21:45:00.780" LastEditorUserId="55365" OwnerUserId="55365" ParentId="114636" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;Let me describe what I see as soon as I look at it:&lt;/p&gt;&#10;&#10;&lt;p&gt;If we're interested in the conditional distribution of $y$ (which if often where interest focuses if we see $x$ as IV and $y$ as DV), &#10;then for $x\leq 0.5$ the conditional distribution of $Y|x$ appears bimodal with an upper group (between about 70 and 125, with mean a bit below 100) and a lower group (between 0 and about 70, with mean around 30 or so). Within each modal group, the relationship with $x$ is nearly flat. (See red and blue lines below drawn roughly &#10;where I guess some rough sense of location to be)&lt;/p&gt;&#10;&#10;&lt;p&gt;Then by looking at where those two groups are more or less dense in $X$, we can go on to say more:&lt;/p&gt;&#10;&#10;&lt;p&gt;For $x&amp;gt;0.5$ the upper group disappears completely, which makes the overall mean of $x$ fall, and below about 0.2, the lower group is much less dense than above it, making the overall average higher. &lt;/p&gt;&#10;&#10;&lt;p&gt;Between these two effects, it induces an apparent negative (but nonlinear) relationship between the two, as $E(Y|X=x)$ seems to be decreasing against $x$ but with a broad, mostly flat region in the center. (See purple dashed line)&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/RxVem.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;No doubt it would be important to know what $Y$ and $X$ were, because then it might be clearer why the conditional distribution for $Y$ might be bimodal over much of its range (indeed, it might even become clear that there are indeed two groups, whose distributions in $X$ induce the apparent decreasing relationship in $Y|x$).&lt;/p&gt;&#10;&#10;&lt;p&gt;This what I saw based on purely &quot;by-eye&quot; inspection. With a bit of playing around in something like a basic image manipulation program (like the one I drew the lines with) we could start to figure out some more accurate numbers. If we digitize the data (which is pretty simple with decent tools, if sometimes a little tedious to get right), then we can undertake more sophisticated analyses of that sort of impression.&lt;/p&gt;&#10;&#10;&lt;p&gt;This kind of exploratory analysis can lead to some important questions (sometimes ones that surprise the person who has the data but has only shown a plot), but we must take some care over the extent to which our models are chosen by such inspections - if we apply models chosen on the basis of the appearance of a plot and then estimate those models on the same data, we'll tend to encounter the same problems we get when we use more formal model-selection and estimation on the same data. [This is not to deny the importance of exploratory analysis at all - it's just we must be careful of the consequences of doing it without regard to &lt;em&gt;how&lt;/em&gt; we go about it. ]&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Response to Russ' comments:&lt;/p&gt;&#10;&#10;&lt;p&gt;[later edit: To clarify -- I broadly agree with Russ' criticisms taken as a general precaution, and there's certainly some possibility I've seen more than is really there. I plan to come back and edit these into a more extensive commentary on spurious patterns we commonly identify by eye and ways we might start to avoid the worst of that. I believe I'll also be able to add some justification about why I think it's probably not just spurious in this specific case (e.g. via a regressogram or 0-order  kernel smooth, though of course, absent more data to test against, there's only so far that can go; for example, if our sample is unrepresentative, even resampling only gets us so far.]&lt;/p&gt;&#10;&#10;&lt;p&gt;I completely agree we have a tendency to see spurious patterns; it's a point I make frequently both here and elsewhere.&lt;/p&gt;&#10;&#10;&lt;p&gt;One thing I suggest, for example, when looking at residual plots or Q-Q plots is to generate many plots where the situation is known (both as things should be and where assumptions don't hold) to get a clear idea how much pattern should be ignored.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/111010/interpreting-qqplot-is-there-any-rule-of-thumb-to-decide-for-non-normality/111013#111013&quot;&gt;Here's an example&lt;/a&gt; where a Q-Q plot is placed among 24 others (which satisfy the assumptions), in order for us to see how unusual the plot is. This kind of exercise is important because it helps us avoid fooling ourselves by interpreting every little wiggle, most of which will be simple noise.&lt;/p&gt;&#10;&#10;&lt;p&gt;I often point out that if you can change an impression by covering a few points, we may be relying on an impression generated by nothing more than noise. &lt;/p&gt;&#10;&#10;&lt;p&gt;[However, when it's apparent from many points rather than few, it's harder to maintain that it's not there.]&lt;/p&gt;&#10;&#10;&lt;p&gt;The displays in whuber's answer supports my impression, the Gaussian blur plot seems to pick up the same tendency to bimodality in $Y$. &lt;/p&gt;&#10;&#10;&lt;p&gt;When we don't have more data to check, we can at least look at whether the impression tends to survive resampling (bootstrap the bivariate distribution and see if it's nearly always still present), or other manipulations where the impression shouldn't be apparent if it's simple noise.&lt;/p&gt;&#10;&#10;&lt;p&gt;1) Here's one way to see if the apparent bimodality is more than just skewness plus noise - does it show up in a kernel density estimate? Is it still visible if we plot kernel density estimates under a variety of transformations? Here I transform it toward greater symmetry, at 85% of default bandwidth (since we're trying to identify a relatively small mode, and the default bandwidth is not optimized for that task):&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/nbkf0.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The plots are $Y$, $\sqrt{Y}$ and $\log(Y)$. The vertical lines are at $68$, $\sqrt{68}$ and $\log(68)$. The bimodality is diminished, but still quite visible. Since it's very clear in the original KDE it seems to confirm it's there - and the second and third plots suggest its at least somewhat robust to transformation.&lt;/p&gt;&#10;&#10;&lt;p&gt;2) Here's another basic way to see if it's more than just &quot;noise&quot;:&lt;/p&gt;&#10;&#10;&lt;p&gt;Step 1: perform clustering on Y&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Ej1xd.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Step 2: Split into two groups on $X$, and cluster the two groups separately, and see if it's quite similar. If there's nothing going on the two halves shouldn't be expected to split all that much alike.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/GRbSQ.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The points with dots were clustered differently from the &quot;all in one set&quot; cluster in the previous plot. I'll do some more later, but it seems like perhaps there really might be a horizontal &quot;split&quot; near that position.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm going to try a regressogram or Nadaraya-Watson estimator (both being local estimates of the regression function, $E(Y|x)$). I haven't generated either yet, but we'll see how they go. I'd probably exclude the very ends where there's little data.&lt;/p&gt;&#10;&#10;&lt;p&gt;3) Edit: Here's the regressogram, for bins of width 0.1 (excluding the very ends, as I suggested earlier):&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/h6tt2.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;This is entirely consistent with the original impression I had of the plot; it doesn't prove my reasoning was correct, but my conclusions arrived at the same result the regressogram does. &lt;/p&gt;&#10;&#10;&lt;p&gt;If what I saw in the plot - and the resulting reasoning - was spurious, I probably should not have succeeded at discerning $E(Y|x)$ like this.&lt;/p&gt;&#10;&#10;&lt;p&gt;(Next thing to try would be a Nadayara-Watson estimator. Then I might see how it goes under resampling if I have time.)&lt;/p&gt;&#10;&#10;&lt;p&gt;4) Later edit:&lt;/p&gt;&#10;&#10;&lt;p&gt;Nadarya-Watson, Gaussian kernel, bandwidth 0.15:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/5SF0F.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Again, this is surprisingly consistent with my initial impression. Here's The NW estimators based on ten bootstrap resamples:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/4Y7KJ.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The broad pattern is there, though a couple of the resamples don't as clearly follow the description based on the whole of the data. We see that the case of the level of the left is less certain than on the right - the level of noise (partly from few observations, partly from the wide spread) is such that it's less easy to claim the mean is really higher at the left than at the center.&lt;/p&gt;&#10;&#10;&lt;p&gt;My overall impression is that I probably wasn't simply fooling myself, because the various aspects stand up moderately well to a variety of challenges (smoothing, transformation, splitting into subgroups, resampling) that would tend to obscure them if they were simply noise. On the other hand, the indications are that the effects, while broadly consistent with my initial impression, are relatively weak, and it may be too much to claim any real change in expectation moving from the left side to the center.&lt;/p&gt;&#10;" CommentCount="13" CreationDate="2014-09-08T00:11:26.777" Id="114654" LastActivityDate="2014-09-08T22:48:03.187" LastEditDate="2014-09-08T22:48:03.187" LastEditorUserId="805" OwnerUserId="805" ParentId="114610" PostTypeId="2" Score="10" />
  
  
  
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm running a Mixed effects model ANOVA with two fixed factors (condition, repetition) and one random factor (subject). Subsequently, a Tukey multiple comparisons test is performed.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now I'd like to plot the means and standard errors (SEMs) of the single conditions in a single error bar plot, and report the p values between the conditions. &lt;/p&gt;&#10;&#10;&lt;p&gt;The problem: while in the Tukey test, I got significant differences and non-overlapping SEMs between certain means, for my plotted real/observed data the SEM bars overlap.&lt;/p&gt;&#10;&#10;&lt;p&gt;This is now counterintuitive, since commonly you would assume that in the case of overlapping, the means are not significantly different.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;is the difference between estimated marginal means and observed means due to having a random factor in my model, or what is the reason for the discrepancy?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;how would you report the data? Would you still plot observed data with the p values and state that the p values are derived from the estimated model? Or would you plot estimated means and standard errors?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Thank you!&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT: I'm adding the multiple comparisons result for a sample case as well as the observed means and standard error plot in case this helps. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/iAy0X.jpg&quot; alt=&quot;Observed means and standard errors&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/hWuCy.jpg&quot; alt=&quot;Multiple comparisons result&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-08T13:38:45.207" Id="114701" LastActivityDate="2014-09-08T19:13:05.167" LastEditDate="2014-09-08T19:13:05.167" LastEditorUserId="54643" OwnerUserId="54643" PostTypeId="1" Score="1" Tags="&lt;anova&gt;&lt;mean&gt;&lt;standard-error&gt;&lt;post-hoc&gt;" Title="Standard error bars overlap but significance - estimated marginal means versus observed means" ViewCount="331" />
  
  <row AnswerCount="1" Body="&lt;p&gt;The typical difference-in-differences estimator (as fixed effects) fits a model of the form&#10;$$&#10;y_{it} = \alpha_i + \delta T_{it} + X_{it}'\beta + \epsilon_{it}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $T$ is some treatment that happens to $i$ at time $t$.  &lt;/p&gt;&#10;&#10;&lt;p&gt;The coefficient $\delta$ is identified from the jump between time periods when T goes from zero to one, essentially using as counterfactuals those that didn't get treated during that period, after controlling for unobservables that don't vary in time.&lt;/p&gt;&#10;&#10;&lt;p&gt;Normally the (panel) dataset starts with everyone un-treated, and ends with some remaining untreated while others get treated.  Alternatively, if everyone (eventually) gets treated, you can still include post-treatment data to improve statistical precision -- the $\delta$ is still identified from the time periods where some got treated and others didn't.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question:  is it legit to fit a model where one group starts treated, the other group starts untreated, and then the untreated group gets treated?  This is basically the mirror image of a situation in which one group stayed untreated and one group got treated -- we still have heterogeneity in some time periods.  Mathematically it seems identical -- the standard error components motivations seems to still apply.&lt;/p&gt;&#10;&#10;&lt;p&gt;Am I missing something?  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-08T14:42:07.637" Id="114708" LastActivityDate="2014-09-08T20:04:30.737" OwnerUserId="55417" PostTypeId="1" Score="4" Tags="&lt;econometrics&gt;&lt;panel-data&gt;&lt;fixed-effects-model&gt;&lt;observational-study&gt;&lt;difference-in-difference&gt;" Title="Difference-in-differences with no pre-treatment?" ViewCount="105" />
  <row Body="&lt;p&gt;Whenever I have a complicated model to fit, I usually just fit it directly in &lt;code&gt;rstan&lt;/code&gt; because it's great at fitting highly constrained coefficients, and because it's easy to include penalties and transformations of variables. This is true even when I'm not explicitly fitting a Bayesian model.&lt;/p&gt;&#10;&#10;&lt;p&gt;This is what I've worked up for your particular problem. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(rstan)&#10;&#10;set.seed(1880)&#10;&#10;N       &amp;lt;- 1500&#10;d       &amp;lt;- c(1/2, 2/pi, 2/3)&#10;x       &amp;lt;- c(2, 1, 3)&#10;limit   &amp;lt;- 5&#10;d%*%x &amp;lt;= limit&#10;&amp;gt; TRUE&#10;A       &amp;lt;- cbind(1, rnorm(N), rnorm(N))&#10;b.hat   &amp;lt;- A%*%x&#10;tau     &amp;lt;- 5&#10;wgt     &amp;lt;- rexp(N)&#10;Sigma   &amp;lt;- tau*wgt&#10;b       &amp;lt;- rnorm(N, mean=b.hat, sd=Sigma)&#10;&#10;constrained.reg &amp;lt;- &quot;&#10;    data{&#10;        int&amp;lt;lower=1&amp;gt;        N;&#10;        int&amp;lt;lower=1&amp;gt;        K;&#10;        vector&amp;lt;lower=0&amp;gt;[N]  wgt;&#10;        matrix[N,K]         A;&#10;        vector[N]       b;&#10;        vector[K]       d;&#10;        real            limit; // s.t. d*x&amp;lt;=limit&#10;    }&#10;parameters{&#10;    real&amp;lt;upper=limit&amp;gt;   c; // this is the largest possible value of x%*%d.&#10;    simplex[K]      sim_x;&#10;    real&amp;lt;lower=0&amp;gt;       tau;&#10;}&#10;transformed parameters {&#10;    vector[K]   x;&#10;    vector[N]   b_hat;&#10;    vector[N]   Sigma;&#10;&#10;    x       &amp;lt;- d .*sim_x /c;&#10;    b_hat   &amp;lt;- A*x;&#10;    Sigma   &amp;lt;- tau*wgt;&#10;}&#10;    model{&#10;        b ~ normal(b_hat, Sigma);&#10;        increment_log_prob(-2*log(tau)); // uniform prior on beta, noninformative prior on tau&#10;    }&#10;    generated quantities{&#10;        vector[N]   resid;&#10;        resid   &amp;lt;- (b_hat-b) ./Sigma;&#10;    }&#10;&quot;&#10;fake.data   &amp;lt;- list(N=N, A=A, K=3, b=b, wgt=wgt, d=d, limit=limit)&#10;&#10;fit.test    &amp;lt;- stan(model_code=constrained.reg, data=fake.data, iter=10)&#10;&#10;system.time(fit     &amp;lt;- stan(fit=fit.test, iter=1000, data=fake.data))&#10;print(fit, c(&quot;x&quot;, &quot;tau&quot;)); x&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I realized that I was being dense and that we can enforce the inequality by sampling a value &lt;em&gt;as large as&lt;/em&gt; the maximum permissible dot product result and then transforming appropriately.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;     mean se_mean   sd 2.5%  25%  50%  75% 97.5% n_eff Rhat&#10;x[1] 1.99       0 0.01 1.98 1.98 1.99 1.99  2.00  1645 1.00&#10;x[2] 0.99       0 0.01 0.97 0.98 0.99 0.99  1.00   624 1.00&#10;x[3] 3.00       0 0.01 2.98 2.99 3.00 3.01  3.02   945 1.00&#10;tau  4.82       0 0.09 4.62 4.76 4.82 4.88  5.00   558 1.01&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;These results look fine to me.&lt;/p&gt;&#10;" CommentCount="13" CreationDate="2014-09-08T15:20:51.117" Id="114713" LastActivityDate="2014-09-09T12:17:06.583" LastEditDate="2014-09-09T12:17:06.583" LastEditorUserId="22311" OwnerUserId="22311" ParentId="114692" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;I think you pretty much nailed it in your Edit. Generative model makes more restrictive assumption about the distribution of $x$.&lt;/p&gt;&#10;&#10;&lt;p&gt;From &lt;a href=&quot;http://www.google.co.uk/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;ved=0CCgQFjAA&amp;amp;url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fum%2Fpeople%2Fminka%2Fpapers%2Fbcrf%2Fqi-bayesian-crf-aistat05.pdf&amp;amp;ei=sMkNVPG5Ec_laI2cgsAN&amp;amp;usg=AFQjCNFytrsnk_GtH3kj9taAOZWfjsElFQ&amp;amp;bvm=bv.74649129,d.d2s&quot; rel=&quot;nofollow&quot;&gt;Minka&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;Unlike traditional generative random fields, CRFs only&#10;model the conditional distribution $p(t|x)$ and do not explicitly model the marginal $p(x)$. Note that the labels ${ti }$ are&#10;globally conditioned on the whole observation $x$ in CRFs.&#10;Thus, we do not assume that the observed data $x$ are conditionally independent as in a generative random field.&quot;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2014-09-08T15:28:58.277" Id="114716" LastActivityDate="2014-09-08T15:28:58.277" OwnerUserId="54974" ParentId="73015" PostTypeId="2" Score="2" />
  <row AnswerCount="0" Body="&lt;p&gt;Let's say you are trying to choose between two models. One has two significant fixed effects. The other includes only one of the two fixed effects from the aforementioned model but has a lower AIC (and the fixed effect is still significant). Which model should I choose?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2014-09-08T16:20:13.440" Id="114722" LastActivityDate="2014-09-08T16:20:13.440" OwnerUserId="55427" PostTypeId="1" Score="1" Tags="&lt;model-selection&gt;&lt;aic&gt;" Title="the role of AIC versus p-values in model selection" ViewCount="41" />
  <row Body="&lt;p&gt;For building models, West, Welsh, and Galecki (2014) propose 2 strategies: step-up and top-down. In the step-up strategy your first step is to find the best &quot;unconditional&quot; model: you start with including all statistically significant as well as theoretically relevant random effects but only the fixed intercept. Then test $H_0: \sigma^2_{random}=0$ using the likelihood test. If the $H_0$ cannot be rejected, remove the random effect from the model. Afterwards, you start including level 1 covariates and remove non-significant ones, and then level-2, and so on.&lt;/p&gt;&#10;&#10;&lt;p&gt;So in your case, you may want to find the best &quot;unconditional&quot; model by running&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;null.1_reml &amp;lt;- lmer(meas ~ 1 + (1 | animal), REML=TRUE)&#10;null.2_reml &amp;lt;- lmer(meas ~ 1 + (1 + location | animal), REML=TRUE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;You cannot simply use &lt;code&gt;anova(null.1_reml, null.2_reml)&lt;/code&gt; because the the test statistic is distributed as a mixture of $\chi^2_1$ and $\chi^2_2$ distributions with equal weights. So use this instead:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;0.5*(1 - pchisq(x, 1)) + 0.5*(1 - pchisq(x, 2))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;where &lt;code&gt;x&lt;/code&gt; is the difference in the log-likelihood value between the 2 models.&lt;/p&gt;&#10;&#10;&lt;p&gt;This tests&lt;/p&gt;&#10;&#10;&lt;p&gt;$&#10;H_0: D=&#10;\left(&#10;\begin{matrix}&#10;  \sigma^2_{int} &amp;amp; 0 \\&#10;  0 &amp;amp; 0&#10;\end{matrix}&#10;\right)&#10;$&lt;/p&gt;&#10;&#10;&lt;p&gt;$&#10;H_A: D=&#10;\left(&#10;\begin{matrix}&#10;  \sigma^2_{int} &amp;amp; \sigma_{int,location} \\&#10;  \sigma_{int,location} &amp;amp; \sigma^2_{location}&#10;\end{matrix}&#10;\right)&#10;$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\sigma_{int,location}$ is the covariance between the intercept and the location effect. If you want to test them individually, you can also fit a model with only the location effect and the intercept without their covariance. After finding the best unconditional model proceed to include fixed effects.&lt;/p&gt;&#10;&#10;&lt;p&gt;The top-down strategy recommends you first add fixed effects of as many covariates of interest as possible and remove non-significant ones to find the best mean model. Then include random effects.  &lt;/p&gt;&#10;&#10;&lt;h3&gt;Reference&lt;/h3&gt;&#10;&#10;&lt;p&gt;West, B.T., Welch, K.B. and Galecki, A.T. (with Contributions from Brenda W. Gillespie) (2014).  Linear Mixed Models: A Practical Guide using Statistical Software, Second Edition.  Chapman Hall / CRC Press: Boca Raton, FL.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2014-09-08T18:10:21.867" Id="114733" LastActivityDate="2014-09-08T18:10:21.867" OwnerUserId="53057" ParentId="114689" PostTypeId="2" Score="1" />
  
