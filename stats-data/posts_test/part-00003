  <row AcceptedAnswerId="48423" AnswerCount="1" Body="&lt;p&gt;I'm modeling the effect of a categorical predictor on a binary dependent variable using logistic regression. I'm comparing models with/without the predictor using a likelihood-ratio test.&lt;/p&gt;&#10;&#10;&lt;p&gt;Two categories of the predictor are associated with values of 1 only (no 0s) for the dependent variable. Regression coefficients for these categories (expressed as changes in log(odds) compared to a reference category) are very large and highly suspicious, as this reference category is always associated with response values of 1 (but for one case), and I would thus expect regression coefficients close to 0 for these two categories. Comparisons between the reference category and other categories having more balanced distribution of 1 and 0s matches what I'm expecting from visual inspection of the data. &#10;Removing cases associated with these two 'problematic' categories does not change the logLikelihood of the models, but because it changes the number of parameters it affects the results of the likelihood ratio test.&lt;/p&gt;&#10;&#10;&lt;p&gt;Models are fitted using the glm function with binomial family and logit link in R.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question therefore is: what model (or procedure) should I use to: &lt;/p&gt;&#10;&#10;&lt;p&gt;(1) test the global significance of the effect of the predictor on the dependent variable? Should I keep data from the 'problematic' categories in the model or not before conducted the likelihood ratio test?&lt;/p&gt;&#10;&#10;&lt;p&gt;(2) compare these two 'problematic' categories with others?&lt;/p&gt;&#10;&#10;&lt;p&gt;Any hint appreciated,&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-01-24T10:04:10.883" FavoriteCount="0" Id="48410" LastActivityDate="2013-02-23T18:59:05.077" OwnerUserId="11233" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;regression&gt;&lt;logistic&gt;" Title="How do factor categories with no variance influence logistic regression?" ViewCount="151" />
  <row AnswerCount="1" Body="&lt;p&gt;I come to you today because I face a huge problem that I cannot explain.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have run a multinomial logistic regression (using the mlogit package) on behavioral data. I prepare the data by doing&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    mlogit &amp;lt;- mlogit.data(Merge, choice = &quot;Choice&quot;, shape = &quot;long&quot;, alt.var = &quot;Comp&quot;, &#10;                          drop.index = TRUE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;on my Merge data.&lt;/p&gt;&#10;&#10;&lt;p&gt;which gives me the following:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;                Date     Time ActivityX ActivityY Temp Behavior Valley Age Month Year kid Individual Choice&#10;    1.F   01/05/2012 00:00:00        80        58   10        F  Fuorn   8     5 2012   Y         26   TRUE&#10;    1.R   01/05/2012 00:00:00        80        58   10        F  Fuorn   8     5 2012   Y         26  FALSE&#10;    1.M   01/05/2012 00:00:00        80        58   10        F  Fuorn   8     5 2012   Y         26  FALSE&#10;    1.RUN 01/05/2012 00:00:00        80        58   10        F  Fuorn   8     5 2012   Y         26  FALSE&#10;    2.F   01/05/2012 00:05:00        90        76   10        F  Fuorn   8     5 2012   Y         26   TRUE&#10;    2.R   01/05/2012 00:05:00        90        76   10        F  Fuorn   8     5 2012   Y         26  FALSE&#10;    2.M   01/05/2012 00:05:00        90        76   10        F  Fuorn   8     5 2012   Y         26  FALSE&#10;    2.RUN 01/05/2012 00:05:00        90        76   10        F  Fuorn   8     5 2012   Y         26  FALSE&#10;    3.F   01/05/2012 00:10:00        51        47   10        M  Fuorn   8     5 2012   Y         26  FALSE&#10;    3.R   01/05/2012 00:10:00        51        47   10        M  Fuorn   8     5 2012   Y         26  FALSE&#10;    3.M   01/05/2012 00:10:00        51        47   10        M  Fuorn   8     5 2012   Y         26   TRUE&#10;    3.RUN 01/05/2012 00:10:00        51        47   10        M  Fuorn   8     5 2012   Y         26  FALSE&#10;    4.F   01/05/2012 00:15:00         0         0   10        R  Fuorn   8     5 2012   Y         26  FALSE&#10;    4.R   01/05/2012 00:15:00         0         0   10        R  Fuorn   8     5 2012   Y         26   TRUE&#10;    4.M   01/05/2012 00:15:00         0         0   10        R  Fuorn   8     5 2012   Y         26  FALSE&#10;    4.RUN 01/05/2012 00:15:00         0         0   10        R  Fuorn   8     5 2012   Y         26  FALSE&#10;    5.F   01/05/2012 00:20:00         0         0    9        R  Fuorn   8     5 2012   Y         26  FALSE&#10;    5.R   01/05/2012 00:20:00         0         0    9        R  Fuorn   8     5 2012   Y         26   TRUE&#10;    5.M   01/05/2012 00:20:00         0         0    9        R  Fuorn   8     5 2012   Y         26  FALSE&#10;    5.RUN 01/05/2012 00:20:00         0         0    9        R  Fuorn   8     5 2012   Y         26  FALSE&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;then I ran my regression :&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;m1 &amp;lt;- mlogit(Choice ~ 1 |Temp + Valley + Age + kid + Month , mlogit)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and it gave me significant results :&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;                          Estimate  Std. Error  t-value  Pr(&amp;gt;|t|)    &#10;    M:(intercept)      -4.2153e-01  5.7533e-02  -7.3268 2.358e-13 ***&#10;    R:(intercept)       6.2325e-01  3.4958e-02  17.8284 &amp;lt; 2.2e-16 ***&#10;    RUN:(intercept)    -1.2275e+01  4.0526e-01 -30.2895 &amp;lt; 2.2e-16 ***&#10;    M:Temp              1.5371e-02  9.8680e-04  15.5764 &amp;lt; 2.2e-16 ***&#10;    R:Temp             -3.9871e-02  6.7926e-04 -58.6975 &amp;lt; 2.2e-16 ***&#10;    RUN:Temp           -4.4532e-02  6.8696e-03  -6.4825 9.023e-11 ***&#10;    M:ValleyTrupchun   -3.6154e-01  1.6362e-02 -22.0968 &amp;lt; 2.2e-16 ***&#10;    R:ValleyTrupchun   -4.0186e-02  9.7968e-03  -4.1020 4.096e-05 ***&#10;    RUN:ValleyTrupchun  1.2895e+00  8.5357e-02  15.1066 &amp;lt; 2.2e-16 ***&#10;    M:Age              -1.1026e-02  2.6902e-03  -4.0985 4.158e-05 ***&#10;    R:Age               1.9465e-02  1.6479e-03  11.8119 &amp;lt; 2.2e-16 ***&#10;    RUN:Age             5.5473e-02  1.6661e-02   3.3294 0.0008703 ***&#10;    M:kidY              6.0686e-02  2.2638e-02   2.6807 0.0073460 ** &#10;    R:kidY             -4.1638e-01  1.2391e-02 -33.6024 &amp;lt; 2.2e-16 ***&#10;    RUN:kidY            6.2311e-01  1.0410e-01   5.9854 2.158e-09 ***&#10;    M:Month            -2.0466e-01  8.4448e-03 -24.2346 &amp;lt; 2.2e-16 ***&#10;    R:Month             2.4148e-02  5.2317e-03   4.6157 3.917e-06 ***&#10;    RUN:Month           9.8715e-01  5.6209e-02  17.5622 &amp;lt; 2.2e-16 ***&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;those results were in line with what I expected to find in literature so I was quite happy.&lt;/p&gt;&#10;&#10;&lt;p&gt;My next step was to plot my results and here is when I have some trouble.&lt;/p&gt;&#10;&#10;&lt;p&gt;First of all when I plot my original data and compare it with the result of my regression I find some huge differences. For example, when I plot the %of time spend in a behavior (M for moving, F for feeding, R for resting and Run for running, in my regression F is the reference) in function of age, I find that the older an individual gets, the more they will rest and the more they will move, but the estimates I got from my regression shows that they should rest more (when they get older) but move less. So to summarize, my graph on the original data shows the opposite as what I got from the regression.&lt;/p&gt;&#10;&#10;&lt;p&gt;I don't know if it is normal, in the sense that I don't know if I can compare my original data to the result of my regression in a way that my regression shows the probability from switching to a behavior from an other each time my variable grows of one unit.&lt;/p&gt;&#10;&#10;&lt;p&gt;So I wanted to use the &lt;code&gt;predict()&lt;/code&gt; function but I don't know how to do that. I was hoping to get some help here.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-01-24T11:19:40.857" FavoriteCount="1" Id="48415" LastActivityDate="2014-08-06T08:37:29.017" LastEditDate="2013-06-03T22:15:45.630" LastEditorUserId="7290" OwnerUserId="17750" PostTypeId="1" Score="5" Tags="&lt;r&gt;&lt;multinomial&gt;" Title="predict() - multinomial logistic regression" ViewCount="736" />
  <row AnswerCount="0" Body="&lt;h1&gt;The problem&lt;/h1&gt;&#10;&#10;&lt;p&gt;I am trying to fit a calibration curve from known x and y with jags. I would then like to predict new xes, based on the measurement of new ys. For this, I am following Hamada et al. (2003), J Quality Technology 35:194-205. Since I have subject effect, I wanted to incorporate that in my calibration curve, but prediction has to be done for unknown subjects. I tried to follow Gelman et al. (2003): Bayesian Data Analysis, p.137, to expand the jags model accordingly. I am new to jags, and I wanted to know whether I am doing this properly. Any pointers to improve my analyses and predictions are very welcome!&lt;/p&gt;&#10;&#10;&lt;h1&gt;R code with simulated data&lt;/h1&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(&quot;rjags&quot;)&#10;library(&quot;lme4&quot;)&#10;&#10;# create data set&#10;sim.data &amp;lt;- data.frame(x = c(rep(100, 10), rep(200, 10), rep(300, 10), rep(400, 10)))&#10;sim.data$indid &amp;lt;- c(rep(LETTERS[1:10], 4))&#10;    sim.data$ranintr &amp;lt;- c(rep(rnorm(10, 0, 10), 4))&#10;sim.data$resid &amp;lt;- rnorm(40, 0, 30)&#10;&#10;a &amp;lt;- 200&#10;b &amp;lt;- 1&#10;&#10;sim.data$y &amp;lt;- a + b*sim.data$x + sim.data$ranintr + sim.data$resid&#10;&#10;# prepare simulated data for jags&#10;N1 &amp;lt;- length(data.sim$y)&#10;    x &amp;lt;- sim.data$x&#10;y &amp;lt;- sim.data$y&#10;    indid &amp;lt;- factor(sim.data$indid)&#10;K &amp;lt;- length(unique(indid))&#10;&#10;# new data for prediction&#10;N2 &amp;lt;- 2&#10;x2 &amp;lt;- c(NA, NA)&#10;y2 &amp;lt;- c(250, 350)&#10;&#10;jags &amp;lt;- jags.model('simModelCalibLinearHierarchical_predict.bug',&#10;                   data = list('x' = x, 'y' = y,&#10;                       'N1' = N1, 'K' = K, 'indid' = indid,&#10;                       'N2' = N2, 'x2' = x2, 'y2' = y2),&#10;                   n.chains = 2,&#10;                   n.adapt = 100)&#10;&#10;update(jags, 300000)&#10;&#10;samples &amp;lt;- coda.samples(jags, c('alpha0', 'beta', &#10;                                'sigma2', 'sigma2.a.indid', 'ICC', 'x2'), &#10;                        100000, thin = 100)&#10;&#10;fit.1 &amp;lt;- lmer(sim.data$y ~ sim.data$x + (sim.data$x | sim.data$indid))&#10;&#10;summary(samples[,c('alpha0', 'beta', 'x2[1]', 'x2[2]')])&#10;summary(fit.1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;h1&gt;BUGS code&lt;/h1&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;model{&#10;&#10;  # calibration&#10;  for(i in 1:N1){       # the calibration observations&#10;    y[i] ~ dnorm(mu[i], tau)&#10;    mu[i] &amp;lt;- alpha0 + alpha[indid[i]] + beta * x[i]&#10;  }&#10;  # K should be the number of unique subjects&#10;  for(i in 1:K){&#10;    alpha[i] ~ dnorm(0, tau.a.indid)&#10;  }&#10;&#10;  # prediction&#10;  for(i in 1:N2){       # the number of predictions&#10;    y2[i] ~ dnorm(mu2[i], tau)&#10;    mu2[i] &amp;lt;- alpha0 + alpha2[i] + beta * x2[i]&#10;    x2[i] ~ dnorm(mux2, taux2)&#10;    alpha2[i] ~ dnorm(mean.alpha, tau.a.indid)&#10;  }&#10;&#10;  # priors for calibration&#10;  tau ~ dgamma(0.01, 0.01)&#10;  alpha0 ~ dnorm(0, 0.0001)&#10;  beta ~ dnorm(0, 0.0001) &#10;  tau.a.indid ~ dgamma(0.01, 0.01)&#10;&#10;  # priors for predictions&#10;  mux2 ~ dnorm(300, 0.0001)  # prior chosen so that (0, 600) contains 99.97% &#10;                             # of its distribution; this reflects our knowledge &#10;                             # that values must be positive &#10;  taux2 ~ dgamma(0.01, 0.01)&#10;&#10;  # calculations&#10;  mean.alpha &amp;lt;- mean(alpha)&#10;  sigma2 &amp;lt;- 1 / tau&#10;  sigma2.a.indid &amp;lt;- 1/tau.a.indid&#10;  ICC &amp;lt;- sigma2.a.indid / (sigma2 + sigma2.a.indid)&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2013-01-24T12:50:08.820" Id="48420" LastActivityDate="2013-01-24T12:50:08.820" OwnerUserId="19892" PostTypeId="1" Score="0" Tags="&lt;bayesian&gt;&lt;multilevel-analysis&gt;&lt;jags&gt;&lt;calibration&gt;" Title="Multilevel calibration curve in jags" ViewCount="128" />
  <row AnswerCount="0" Body="&lt;p&gt;I am wondering how I could express the variance of log-odds into understandable terms.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example the variance in the log-odds of crime being reported to the police between neighbourhoods is 0.07 (0.01), and the mean log-odds is -0.65 (0.03). Just transforming this variance to probability given me 0.51. So if the probabilities have a variation of 0.51, the standard deviation is then 0.7, which would make the 95% coverage bound wider than 1, which is impossible for probabilities.&lt;/p&gt;&#10;&#10;&lt;p&gt;Another idea was to calculate the 95% coverage bounds of my log-odds:&lt;/p&gt;&#10;&#10;&lt;p&gt;$(-0.65) +/- [1.96\sqrt{0.07}] = [-1.19, -0.15]$&lt;/p&gt;&#10;&#10;&lt;p&gt;If I transform these to probabilities:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\exp(x)/[1+\exp(x)] = [0.23,0.46]$&lt;/p&gt;&#10;&#10;&lt;p&gt;So the chance of a crime being reported to the police can vary from 23% to 46%.&lt;/p&gt;&#10;&#10;&lt;p&gt;So the 95% range is actually only 0.23 or 23%.&lt;/p&gt;&#10;&#10;&lt;p&gt;Could someone explain me why this first calculation was wrong, and whether the second is correct? Would you have any other suggestions on how to make this variance more graspable?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-01-24T13:39:31.860" Id="48422" LastActivityDate="2013-01-24T15:18:09.753" LastEditDate="2013-01-24T15:18:09.753" LastEditorUserId="88" OwnerUserId="18334" PostTypeId="1" Score="1" Tags="&lt;logit&gt;" Title="Variance of log-odds probabilities" ViewCount="202" />
  
  <row AcceptedAnswerId="48449" AnswerCount="2" Body="&lt;p&gt;During studying the convergence assessment on Markov Chain Monte Carlo, I once read the following statement:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;A slowly converging sampler may be indistinguishable from one that will never converge(e.g., due to nonidentifiability)!&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;How to understand this statement? Thanks!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-24T15:21:12.427" Id="48429" LastActivityDate="2013-02-01T03:57:55.780" OwnerUserId="3125" PostTypeId="1" Score="2" Tags="&lt;bayesian&gt;&lt;mcmc&gt;" Title="Regarding the convergence assessment on Markov chain monte carlo (MCMC)" ViewCount="115" />
  <row AnswerCount="0" Body="&lt;p&gt;I am going to analyze some data for an intermittent operation using R. Let's say I operate a Xmas tree stand from Black Friday to Christmas Eve every year. Let's say I operate 150 different Xmas tree stands every year. Let's say I have data for revenue-per-day for these 150 Xmas tree stands for the last 6 years. I want to analyze this data to figure out how to maximize my profit.&lt;/p&gt;&#10;&#10;&lt;p&gt;Trends that I expect to exist include&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Daily trends in volume (higher volume on weekends, etc)&lt;/li&gt;&#10;&lt;li&gt;Trend of #   of days before Xmas (start low, max somewhere then decrease probably)&lt;/li&gt;&#10;&lt;li&gt;Which day of week Xmas is probably has an affect&lt;/li&gt;&#10;&lt;li&gt;Year over year (should show Great Depression of 2008, then recovery etc)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I want to know how many stands to operate, and if I would do better with half the stands but move them around to different locations on different days to capture max revenue with minimum costs.  For instance, I might find that for year when Xmas is on Wednesday, Location A should not be manned until 3 weeks before Xmas, should be manned on Thursday, Friday and Saturday for that week, and only on Tuesday and Saturday for the next 2 weeks. Bad example, but hopefully shows what I'm looking for.&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to try this in R. My limited understanding of time series is stumped when I have ~335 days between data sets which require a resolution of 1 day. Should I instead add dummy variables of day of week, days before Xmas, year and just try to fit on them?&lt;/p&gt;&#10;&#10;&lt;p&gt;Any insight would be appreciated.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-24T17:51:14.283" Id="48442" LastActivityDate="2013-09-09T14:36:56.770" LastEditDate="2013-09-09T14:36:56.770" LastEditorUserId="27581" OwnerUserId="20036" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;time-series&gt;" Title="Time series for Intermittent (1 month per year) data in R" ViewCount="91" />
  
  <row Body="&lt;p&gt;From the point of view of functional analysis and measure theory the $L^p$ type distances do not define measurable sets on spaces of functions (infinite dimensional spaces loose countable additive in the metric ball coverings). This firmly disqualifies any sort of measurable interpretation of the distances of choices 2 &amp;amp; 3.&lt;/p&gt;&#10;&#10;&lt;p&gt;Of course Kolomogorov, being much brighter than any of us posting, especially including myself, anticipated this. The clever bit is that while the distance in the KS test is of the $L^0$ variety, the uniform norm itself is not used to define the measurable sets. Rather the sets are part of a stochastic filtration on the differences between the distributions evaluated at the observed values; which is equivalent to the stopping time problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;In short the uniform norm distance of choice 1 is preferable because the test it implies is equivalent to the stopping time problem, which itself produces computationally tractable probabilities. Where as choices 2 &amp;amp; 3 cannot define measurable subsets of functions.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-24T21:31:39.063" Id="48468" LastActivityDate="2013-01-24T21:39:30.217" LastEditDate="2013-01-24T21:39:30.217" LastEditorUserId="20047" OwnerUserId="20047" ParentId="411" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Yes. In fact, you don't even need to know that $E[X]$ is finite: if you know that the $k$-th moment $E[X^k]$ is finite, then all lower moments must be finite.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can see this using Jensen's inequality, which says that for any convex function $\varphi$ and random variable $X$,&#10;$$\varphi(E[X]) \leq E[\varphi(X)].$$&#10;Now, suppose we know that $E[X^k]$ is finite, and we want to check whether $E[X^m]$ is finite where $m &amp;lt; k$. Let $\varphi(X) = X^{k/m}$. Since $k&amp;gt;m$, this is a convex function. Then by Jensen's inequality,&#10;$$(E[X^m])^{k/m} \leq E[(X^m)^{k/m}] = E[X^k].$$&#10;We know that $E[X^k]$ is finite, so $(E[X^m])^{k/m}$ must also be finite, and so $E[X^m]$ is finite.&lt;/p&gt;&#10;&#10;&lt;p&gt;In summary: given that the $k$-th moment is finite, all lesser moments must also be finite.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-01-25T02:12:29.300" Id="48480" LastActivityDate="2013-01-25T02:12:29.300" OwnerUserId="16297" ParentId="48477" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;In case anyone's interested, I have implemented computing this expectation in two ways&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;a href=&quot;https://github.com/mizzao/libmao/blob/master/src/main/java/net/andrewmao/models/discretechoice/OrderedNormalEM.java&quot; rel=&quot;nofollow&quot;&gt;via a multivariate normal integral&lt;/a&gt;, using Alan Genz' &lt;code&gt;MVNEXP&lt;/code&gt; FORTRAN library:&#10;(Check out the &lt;code&gt;conditionalExp&lt;/code&gt; method down the page)&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;https://github.com/mizzao/libmao/blob/master/src/main/java/net/andrewmao/models/discretechoice/NormalGibbsSampler.java&quot; rel=&quot;nofollow&quot;&gt;Via Gibbs sampling&lt;/a&gt;, using a truncated normal distribution for MCMC (&lt;a href=&quot;http://books.nips.cc/papers/files/nips25/NIPS2012_0077.pdf&quot; rel=&quot;nofollow&quot;&gt;also check out this paper&lt;/a&gt;)&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;In regards to @whuber's comment, the integration is NOT necessarily the faster approach; the Gibbs sampling is numerically simpler and can be sped up significantly by using fast (but less accurate) versions of the univariate normal. The multivariate normal integral is also tricky to use and has a tradeoff between accuracy and speed...just like the Gibbs sampler. &lt;/p&gt;&#10;&#10;&lt;p&gt;In preliminary testing, it's inconclusive which one is faster for a given accuracy.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-25T07:30:20.980" Id="48487" LastActivityDate="2013-01-25T07:30:20.980" OwnerUserId="13188" ParentId="48075" PostTypeId="2" Score="0" />
  
  <row AcceptedAnswerId="53113" AnswerCount="2" Body="&lt;p&gt;For a random variable $X\sim \text{Exp}(\lambda)$ ($\mathbb{E}[X] = \frac{1}{\lambda}$) I feel intuitively that $\mathbb{E}[X|X &amp;gt; x]$ should equal $x + \mathbb{E}[X]$ since by the memoryless property the distribution of $X|X &amp;gt; x$ is the same as that of $X$ but shifted to the right by $x$.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, I'm struggling to use the memoryless property to give a concrete proof. Any help is much appreciated.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-01-25T09:42:29.490" FavoriteCount="3" Id="48496" LastActivityDate="2015-03-07T22:54:51.680" OwnerUserId="20065" PostTypeId="1" Score="5" Tags="&lt;random-variable&gt;&lt;exponential&gt;&lt;conditional-expectation&gt;" Title="Conditional expectation of exponential random variable" ViewCount="2727" />
  <row AcceptedAnswerId="48503" AnswerCount="1" Body="&lt;p&gt;I have a dataset that contains data from $307$ subjects and nine variables for each subject. I would like to run a PCA. My problem is that I get a Kaiser-Meyer-Olkin (KMO) value of $0.06$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can it be due to the fact that within every subject the variables add up to $1$? Is there another way to test whether it is okay to run PCA on my data?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-25T10:37:52.123" Id="48497" LastActivityDate="2014-09-30T01:15:01.980" LastEditDate="2014-09-30T01:15:01.980" LastEditorUserId="3277" OwnerUserId="20068" PostTypeId="1" Score="2" Tags="&lt;pca&gt;" Title="Is it valid to perform PCA if Kaiser-Meyer-Olkin (KMO) index is very low?" ViewCount="287" />
  <row Body="&lt;p&gt;The demo version of AUTOBOX has a large number of test sets (&lt;a href=&quot;http://www.autobox.com&quot; rel=&quot;nofollow&quot;&gt;http://www.autobox.com&lt;/a&gt;) that could be useful to you. Rob Hyndman has amassed a large &lt;a href=&quot;http://robjhyndman.com/TSDL/&quot; rel=&quot;nofollow&quot;&gt;library of time series data&lt;/a&gt; that could also be useful.&lt;/p&gt;&#10;&#10;&lt;p&gt;EDITED 2/1/13&lt;/p&gt;&#10;&#10;&lt;p&gt;The federal reserve allows you to download 15,000 economic indicator data like housing starts, etc. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://research.stlouisfed.org/fred2/&quot; rel=&quot;nofollow&quot;&gt;http://research.stlouisfed.org/fred2/&lt;/a&gt; &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-25T11:37:47.103" Id="48502" LastActivityDate="2013-02-01T15:42:57.967" LastEditDate="2013-02-01T15:42:57.967" LastEditorUserId="3382" OwnerUserId="3382" ParentId="48500" PostTypeId="2" Score="4" />
  <row AnswerCount="1" Body="&lt;p&gt;It is known that odds ratios enjoy a certain symmetry. For example,  the odds ratio of outcome $Y$ is the inverse of the odds ratio of outcome $\neg Y$. Risk ratios, on the other hand, do not enjoy this symmetry. However, risk ratios have the property of collapsibility. So adjusting for a covariate that is not a confounder does not change the magnitude of the risk ratio. Consider the more formal definition of collapsibility:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;em&gt;&lt;strong&gt;Definition&lt;/em&gt;&lt;/strong&gt;. Let $g[P(x,y)]$ be any functional that measures the association between $Y$ and $X$ in the joint distribution $P(x,y)$. Then $g$ is collapsibile on a variable $Z$ if $$E_{z}g[P(x,y|z)] = g[P(x,y)]$$&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;So in the case of the risk ratio, $g[P(x,y)]$ would be the risk ratio?  What if we don't know $P(x,y)$? A risk ratio is the ratio of two incidence densities which doesn't seem to depend on any probability distribution.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-25T15:11:01.343" FavoriteCount="1" Id="48517" LastActivityDate="2013-09-04T00:16:03.383" OwnerUserId="19814" PostTypeId="1" Score="2" Tags="&lt;probability&gt;&lt;epidemiology&gt;" Title="Collapsibility: Odds Ratios versus Risk Ratios" ViewCount="715" />
  
  
  
  
  
  <row Body="&lt;p&gt;Don't re-invent the wheel!&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(MASS)&#10;M&amp;lt;-matrix(-0.9,2,2)&#10;diag(M)&amp;lt;-1&#10;X&amp;lt;-mvrnorm(200,rep(0,2),M,empirical=TRUE)&#10;cor(X)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2013-01-25T19:00:53.350" Id="48535" LastActivityDate="2013-01-25T19:06:14.213" LastEditDate="2013-01-25T19:06:14.213" LastEditorUserId="603" OwnerUserId="603" ParentId="48534" PostTypeId="2" Score="4" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Simple random sampling with replacement.&lt;/p&gt;&#10;&#10;&lt;p&gt;Population size = $ N $&lt;/p&gt;&#10;&#10;&lt;p&gt;Sample size = $ n $&lt;/p&gt;&#10;&#10;&lt;p&gt;I am interested in learning about the distribution of the random  variables&#10;$ X_0, X_1, ..., X_n $, where $ X_i  $  is the number of units who were chosen exactly a number $ i $ of times for the sample.&#10;Note that:&#10;$$&#10;\sum_{i=0}^n X_i = N&#10;$$&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-26T13:20:52.053" Id="48577" LastActivityDate="2013-01-26T13:20:52.053" OwnerUserId="20113" PostTypeId="1" Score="3" Tags="&lt;sampling&gt;" Title="Distribution of number of times units are chosen for a simple random sample" ViewCount="30" />
  <row AnswerCount="0" Body="&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;strong&gt;Possible Duplicate:&lt;/strong&gt;&lt;br&gt;&#10;  &lt;a href=&quot;http://stats.stackexchange.com/questions/48696/generalized-linear-mixed-model-in-r-with-repeated-measures&quot;&gt;Generalized Linear Mixed Model in R with repeated measures&lt;/a&gt;  &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&#10;&#10;&lt;p&gt;I am trying to investigate how four variables (var1=continuous, var2=factor, var3=factor, var4=continuous) influence the number of trials individuals approached (out of total nr of trials --&gt; binomial) across two conditions that differed in food availability (food availability 1 = 42 trials; food availability 8 = 35 trials) (n = 19 individuals). The response variable is binomial as it is the number of trials out of total number of trials. I am using the 'lmer' function of the lme4 package.&lt;/p&gt;&#10;&#10;&lt;p&gt;I thought the additive model I should run would be with random factor ID:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;glmer(cbind(appr_Y,appr_N) ~ Condition+Var1+Var2+Var3+Var4+(1|ID), data=dataset, &#10;      family=binomial)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;However, the result I get shows that Condition is super significant (p &amp;lt; 2e-16) while the other variables aren't, while exploring the data visually shows no difference in the response variable for Condition and the variables having strong effects.&lt;/p&gt;&#10;&#10;&lt;p&gt;Below a dummy representing the large data table:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Con ID  Var1  Var2  appr_Y  appr_N  Trial_total&#10;1   1   10      y   14      6       20&#10;1   2   4       y   10      10      20&#10;1   3   5       n   5       15      20&#10;1   4   32      n   18      2       20&#10;1   5   11      y   3       17      20&#10;2   1   10      y   20      5       25&#10;2   2   4       y   10      15      25&#10;2   3   5       n   24      1       25&#10;2   4   32      n   11      14      25  &#10;2   5   11      y   7       18      25&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;What am I doing wrong? &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;update&lt;/strong&gt;: I analysed the data with GenStat (which doesn't show AIC values) and the output is totally different. In GenStat it asks for the random factor (here ID) and the denominator (here Trial_total), which is different than putting in Appr_Y, Appr_N. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;update2&lt;/strong&gt;: The above dataset was just a dummy. I hereby provide the 'summary' of the model and the information about the dataset:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; summary(GLMM1)&#10;Generalized linear mixed model fit by the Laplace approximation &#10;Formula: cbind(Appr_Y, Appr_N) ~ Condition + Var1 + Var2 + Var3 + Var4 + (1 | ID) &#10;Data: dataset &#10;AIC   BIC logLik deviance&#10;102.1 113.5 -44.04    88.08&#10;Random effects:&#10;Groups Name        Variance Std.Dev.&#10;ID     (Intercept) 0.59495  0.77133 &#10;Number of obs: 38, groups: ID, 19&#10;&#10;Fixed effects:&#10;                  Estimate Std. Error z value Pr(&amp;gt;|z|)    &#10;(Intercept)       -2.43536    0.60237  -4.043 5.28e-05 ***&#10;Condition8         1.14942    0.12274   9.365  &amp;lt; 2e-16 ***&#10;Var1               0.04524    0.04002   1.130   0.2583    &#10;Var2Paired        -0.35299    0.47970  -0.736   0.4618    &#10;Var3no             0.55914    0.44095   1.268   0.2048    &#10;Var4               0.11996    0.06282   1.909   0.0562 .  &#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 &#10;&#10;Correlation of Fixed Effects:&#10;            (Intr) Cndt8- Var1 Var2P Var3no&#10;Cndtn8-strn -0.128                            &#10;Var1        -0.294  0.015                     &#10;Var2unp     -0.474 -0.015 -0.352              &#10;Var3no      -0.178  0.016 -0.310 -0.097       &#10;Var4        -0.664  0.021 -0.078  0.467 -0.134&#10;&amp;gt; str(dataset)&#10;'data.frame':   38 obs. of  9 variables:&#10;$ ID          : Factor w/ 19 levels &quot;39&quot;,&quot;40&quot;,&quot;41&quot;,..: 1 2 3 4 5 6 7 8 9 10 ...&#10;   $ Appr_Y      : num  3 12 0 7 27 6 12 1 5 17 ...&#10;$ Appr_N      : num  39 30 42 35 15 36 30 41 37 25 ...&#10;    $ Var2        : Factor w/ 2 levels &quot;paired&quot;,&quot;unpaired&quot;: 2 2 2 2 1 1 2 1 2 1 ...&#10;$ Var1        : num  2 16 19 18 13 11 14 1 8 9 ...&#10;    $ Var3        : Factor w/ 2 levels &quot;yes&quot;,&quot;no&quot;: 2 2 2 1 2 2 2 1 1 2 ...&#10;$ Var4        : num  2.6 6.87 2.4 1.1 4.32 ...&#10;    $ Condition   : Factor w/ 2 levels &quot;1&quot;,&quot;8&quot;: 1 1 1 1 1 1 1 1 1 1 ...&#10;$ n           : num  42 42 42 42 42 42 42 42 42 42 ...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Do I perhaps have to do something with weighing the data as trial nr is not the same across conditions? Or using Appr_Y, total nr of trials instead of Appr_Y, Appr_N ?&lt;/p&gt;&#10;" ClosedDate="2013-01-28T15:19:55.940" CommentCount="6" CreationDate="2013-01-26T14:19:35.213" FavoriteCount="2" Id="48582" LastActivityDate="2013-01-27T09:54:34.893" LastEditDate="2013-01-27T09:54:34.893" LastEditorUserId="20112" OwnerUserId="20112" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;lmer&gt;&lt;glmm&gt;&lt;glmer&gt;" Title="Generalized linear mixed model in R" ViewCount="126" />
  
  
  <row AcceptedAnswerId="48597" AnswerCount="2" Body="&lt;p&gt;What is the purpose of the link function as a component of the generalized linear model? Why do we need it?&lt;/p&gt;&#10;&#10;&lt;p&gt;Wikipedia states:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;It can be convenient to match the domain of the link function to the range of the distribution function's mean&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;What's the advantage of doing this?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-26T17:03:36.330" FavoriteCount="3" Id="48594" LastActivityDate="2014-04-09T23:17:20.040" LastEditDate="2013-01-27T10:11:56.353" LastEditorUserId="88" OwnerUserId="10749" PostTypeId="1" Score="3" Tags="&lt;regression&gt;&lt;generalized-linear-model&gt;&lt;link-function&gt;" Title="Purpose of the link function in generalized linear model" ViewCount="3304" />
  
  <row Body="&lt;p&gt;For those who might be interested in an answer, this comes from a much more knowledgeable source than me (PhD candidate in NLP):&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;When doing multiclass classification, precision and recall are really&#10;  only  properly defined for individual classes (you can average across&#10;  classes to  get a general scores for the entire system, but it's not&#10;  really that  useful; in my opinion, you're probably better off just&#10;  using overall  accuracy as your metric of performance). &lt;/p&gt;&#10;  &#10;  &lt;p&gt;For an individual class, the false positives are those instances which&#10;  were classified as that class, but in fact aren't, and the true&#10;  negatives are those instances which are not that  class, and were&#10;  indeed classified as not belonging to that class  (regardless of&#10;  whether they were correctly classified).&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="2" CreationDate="2013-01-26T21:36:13.937" Id="48605" LastActivityDate="2013-01-26T21:36:13.937" OwnerUserId="19809" ParentId="48036" PostTypeId="2" Score="6" />
  
  
  <row Body="&lt;p&gt;The distribution looks &lt;strong&gt;log-normal&lt;/strong&gt; to me.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can fit your data using two parameters: scale and location. These can be fitted in much the same way as a normal distribution using expectation maximisation.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Log-normal_distribution&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Log-normal_distribution&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-27T13:42:16.840" Id="48626" LastActivityDate="2013-01-27T13:42:16.840" OwnerUserId="8257" ParentId="1315" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;Another way to motivate the squared residuals is by making the often reasonable assumption that the residuals are Gaussian distributed. In other words, we assume that&#10;$$y = ax + b + \varepsilon$$&#10;for Gaussian noise $\varepsilon$. In this case, the log-likelihood of the parameters $a, b$ is given by&#10;$$\log p(y \mid x, a, b) = \log \mathcal{N}(y; ax + b, 1) = -\frac{1}{2} (y - [a + bx])^2 + \text{const},$$&#10;so that &lt;a href=&quot;http://en.wikipedia.org/wiki/Maximum_likelihood&quot;&gt;maximizing the likelihood&lt;/a&gt; amounts to minimizing the squared residuals.&lt;/p&gt;&#10;&#10;&lt;p&gt;If the noise $\varepsilon$ was &lt;a href=&quot;http://en.wikipedia.org/wiki/Laplace_distribution&quot;&gt;Laplace distributed&lt;/a&gt;, the absolute value of residuals would be more appropriate. But because of the &lt;a href=&quot;http://en.wikipedia.org/wiki/Central_limit_theorem&quot;&gt;central limit theorem&lt;/a&gt;, Gaussian noise is much more common.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-01-27T17:04:12.473" Id="48635" LastActivityDate="2013-01-27T17:38:11.877" LastEditDate="2013-01-27T17:38:11.877" LastEditorUserId="7733" OwnerUserId="7733" ParentId="48627" PostTypeId="2" Score="8" />
  <row AnswerCount="3" Body="&lt;p&gt;I have an assignment to forecast snow depth at a certain location at a specific date.  My approach to forecast this is to basically take previous years snowfall data and compare it to this years data so far.  I need to figure out which snowfall pattern over the last 50 years  is most correlated with this years pattern so far.  I have matlab at my disposal and I have all the time series data but I don't know the best way to measure correlation between a certain years data and the current years data statistically. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have thought about doing an absolute value difference between this years daily data and a given previous years, then averaging it to find the average difference.  Whichever one has the smallest average difference will be the year I choose to base the prediction off of.  I don't think this is the most accurate method though.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am not very knowledgeable in statistics and want to be able to show that I used a statistically accurate method to measure correlation when I turn in my report.  &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-01-27T18:48:49.737" Id="48639" LastActivityDate="2013-01-28T11:38:38.527" LastEditDate="2013-01-27T22:48:59.527" LastEditorUserId="930" OwnerUserId="20159" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;self-study&gt;&lt;forecasting&gt;" Title="Correlation between two sets of time series data" ViewCount="934" />
  <row Body="&lt;p&gt;A &lt;a href=&quot;http://stats.stackexchange.com/questions/29096/correlation-between-two-time-series&quot;&gt;cross-correlation&lt;/a&gt; will be a better approach than your average absolute difference although there are some obvious problems to be wary of - if the current year has exactly twice the snow every day of some reference year it is perfectly correlated for example (this could still be useful info of course).&lt;/p&gt;&#10;&#10;&lt;p&gt;Your general approach is not a common one for forecasting in this situation, although it will not deliver insane results.  Better ways would use the information in all 50 years efficiently, rather than just finding the &quot;best&quot; year and using that as a model.  You may wish to do some more reading on forecast methods - there is plenty of Matlab-specific material on this on the web - although it quickly gets very complex.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-27T19:11:57.213" Id="48642" LastActivityDate="2013-01-28T00:45:43.083" LastEditDate="2013-01-28T00:45:43.083" LastEditorUserId="7972" OwnerUserId="7972" ParentId="48639" PostTypeId="2" Score="1" />
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;&lt;strong&gt;Background:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I had a data set containing 212 observations with a lots of missing values. Most of the IVs and DVs are categorical (DVs are ordinal) in nature. There are 3 DVs and about 30 IVs. My intention was to run an ordinal logistic regression. A list-wise deletion keeps only 42 observations, so I decided to use hot deck imputation to fill in the missing values. I chose similar variables as the deck variables during the hot deck imputation (the deck variables should always be categorical and as far I know there should be a maximum of 5 deck variables). &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Here are my queries:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;1)&lt;/strong&gt; When I imputed via hot deck once, 169 of the observations were filled in completely. If I use these imputed values for another hot deck imputation, then all 212 observations will fill in completely. But I am not sure if it is valid to use the imputed values for a further imputation. Can anyone suggest?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;2)&lt;/strong&gt; Someone suggested me (from his experience) to use the 3 DVs as the background or deck variables for imputing all the DVs and all the IVs, because that will probably facilitate my regression results. May I know your comment about it? &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;3)&lt;/strong&gt; If I see almost all of the values (except from a very few) of a continuous IV are 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90 etc. then isn't it better to impute them via hot deck rather than via EM (as the hot deck will impute a variable with it's existing values only)?&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2013-01-28T06:54:29.183" Id="48668" LastActivityDate="2013-01-28T11:13:09.227" LastEditDate="2013-01-28T11:13:09.227" LastEditorUserId="88" OwnerUserId="12603" PostTypeId="1" Score="2" Tags="&lt;missing-data&gt;&lt;data-imputation&gt;&lt;multiple-imputation&gt;" Title="Hot deck imputation: validity of double imputation and selection of deck variables for a regression" ViewCount="431" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Given a document-term matrix $X$, where $$X(d, t) = \textit{occurrences of 't' in 'd'}$$, it's possible to compute it's Truncated Singular Value Decomposition:$$X_k = U_k \Sigma_k V_k^T$$&#10;Then, for a document profile $d$ (i.e. the vector of occurences in that document), it is possible to compute a 'predicted' document profile $\hat{d}$ that, according to the model, is more consistent, as follows:&#10;$$\hat{d} = d V_k V_k^T$$&#10;This is the LSA/LSI/SVD method.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Now, suppose that instead computing only $V_k$, I compute $C$ matrices $V_{k,c}, c = 0, 1, \dots ,C $, with the same dimensions of $V_k$ but different entries.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then, for a given document profile, I compute $C$ 'predicted' profiles, as:&#10;$$\hat{d}_c = d V_{k,c} V_{k,c}^T$$&#10;one for each $c = 0, 1, \dots ,C $. Among these, I choose as final predicted profile, the one which minimize the L-2 norm, w.r.t. the input profile:&#10;$$\hat{d} = \arg \min_c \| d-\hat{d}_c\|_2 $$.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm not totally persuaded by this algorithm; choosing the output with a L-2 norm w.r.t. the input seems to me to be source of overfitting. Can I have to your opinion, suggestion, critiques please?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you very much.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-28T21:43:16.160" Id="48724" LastActivityDate="2013-01-28T21:43:16.160" OwnerUserId="10669" PostTypeId="1" Score="1" Tags="&lt;information-retrieval&gt;&lt;lsa&gt;" Title="Is that overfitting?" ViewCount="94" />
  <row AnswerCount="0" Body="&lt;p&gt;What is the proper way to measure significance in an A/B test, when a visitor can potentially &quot;convert&quot; more than once?&lt;/p&gt;&#10;&#10;&lt;p&gt;I understand a chi-square test or z-test are most often used for A/B testing, but from what I understand that is only if it is categorical, like if a visitor purchased or not.&lt;/p&gt;&#10;&#10;&lt;p&gt;In my case, customers can purchase multiple times in a test period, and I want to make sure I'm using the right significance test for this.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;test A: 800,000 visitors, 50,000 purchases&#10;test B: 45,000 visitors, 2,700 purchases&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I saw a similar question here, but it was asking about revenue:&#10;&lt;a href=&quot;http://stats.stackexchange.com/questions/19507/ab-testing-other-factors-besides-conversion-rate&quot;&gt;AB Testing other factors besides conversion rate&lt;/a&gt;. In my case, should I be using a Wilcoxon test also?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-28T21:55:48.837" Id="48726" LastActivityDate="2013-01-28T21:55:48.837" OwnerUserId="16383" PostTypeId="1" Score="2" Tags="&lt;hypothesis-testing&gt;&lt;statistical-significance&gt;&lt;chi-squared&gt;&lt;wilcoxon&gt;" Title="significance test to use for multiple conversions in an A/B test" ViewCount="126" />
  
  
  <row Body="&lt;p&gt;I would suggest that the massive coefficients, and the correspondingly massive standard errors, would almost definitely be caused by quasi-complete or complete separation.  That is, for some combination of parameters, either everyone had the outcome or nobody had the outcome, and so the coefficient heads towards infinity (or negative infinity.)&lt;/p&gt;&#10;&#10;&lt;p&gt;This tends to happen especially when one specifies a lot of interaction terms, as the chances of having a combination of factors which results in some &quot;empty&quot; (no outcomes in cell, or everyone has outcomes) cells will increase.&lt;/p&gt;&#10;&#10;&lt;p&gt;See the following page for some further details and suggested strategies:&#10;&lt;a href=&quot;http://www.ats.ucla.edu/stat/mult_pkg/faq/general/complete_separation_logit_models.htm&quot; rel=&quot;nofollow&quot;&gt;http://www.ats.ucla.edu/stat/mult_pkg/faq/general/complete_separation_logit_models.htm&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;More generally, it means that you're probably trying to do &quot;too much&quot; with your model for the size of your dataset (particularly the number of outcomes observed).&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT: A couple of pragmatic suggestions&lt;/p&gt;&#10;&#10;&lt;p&gt;You might try (1) quick and simple: drop the interaction terms from your model, to see if that helps (whether this makes sense from a research question perspective is an entirely different issue); or (2) get R to make you a bi-i-i-i-g contingency table for (e.g. rows) the combinations described in the interactions by (e.g. columns) the outcome variable. You might be able to see some evidence of separation here.&lt;/p&gt;&#10;" CommentCount="11" CreationDate="2013-01-29T01:25:16.073" Id="48741" LastActivityDate="2013-02-03T21:35:02.900" LastEditDate="2013-02-03T21:35:02.900" LastEditorUserId="16974" OwnerUserId="16974" ParentId="48739" PostTypeId="2" Score="8" />
  <row AnswerCount="1" Body="&lt;p&gt;My logistic model &lt;a href=&quot;http://stats.stackexchange.com/q/48739/5509&quot;&gt;has been suspicious&lt;/a&gt; due to enormous coefficients, so I tried to do a crossvalidation, and also do a crossvalidation of simplified model, to confirm the fact that the original model is overspecified, as &lt;a href=&quot;http://stats.stackexchange.com/a/48741/5509&quot;&gt;James suggested&lt;/a&gt;. However, I don't know how to interpret the result (this is the model from the linked question):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; summary(m5)&#10;&#10;Call:&#10;glm(formula = cbind(ml, ad) ~ rok + obdobi + kraj + resid_usili2 + &#10;    rok:obdobi + rok:kraj + obdobi:kraj + kraj:resid_usili2 + &#10;    rok:obdobi:kraj, family = &quot;quasibinomial&quot;)&#10;[... see http://stats.stackexchange.com/q/48739/5509 for complete summary output ]&#10;&#10;&amp;gt; cv.glm(na.omit(data.frame(orel, resid_usili2)), m5, K = 10)&#10;$call&#10;cv.glm(data = na.omit(data.frame(orel, resid_usili2)), glmfit = m5, &#10;    K = 10)&#10;&#10;$K&#10;[1] 10&#10;&#10;$delta&#10;[1] 0.2415355 0.2151626&#10;&#10;$seed&#10;  [1]         403         271  1234892862 -1124595763  -489713400  1566924080   147612843&#10;  [8]  1879282918  -694084381  1171051622  2063023839 -1307030905  -477709428  1248673977&#10; [15]  -746898494   420363755  -890078828   460552896  -758793089  -913500073  -882355605&#10;[....]&#10;Warning message:&#10;glm.fit: algorithm did not converge&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I guess the delta is the mean fitting error, but how to interpret it? Is it a good or bad fit? BTW, the algorithm did not converge, maybe due to the enormous coefficients (?)&lt;/p&gt;&#10;&#10;&lt;p&gt;I tried a simplified model:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; summary(m)&#10;&#10;Call:&#10;glm(formula = cbind(ml, ad) ~ rok + obdobi + kraj, family = &quot;quasibinomial&quot;)&#10;&#10;Deviance Residuals: &#10;    Min       1Q   Median       3Q      Max  &#10;-2.7335  -1.2324  -0.1666   1.0866   3.1788  &#10;&#10;Coefficients:&#10;              Estimate Std. Error t value Pr(&amp;gt;|t|)    &#10;(Intercept) -107.60761   48.06535  -2.239 0.025335 *  &#10;rok            0.05381    0.02393   2.249 0.024683 *  &#10;obdobinehn    -0.26962    0.10372  -2.599 0.009441 ** &#10;krajJHC        0.68869    0.27617   2.494 0.012761 *  &#10;krajJHM       -0.26607    0.28647  -0.929 0.353169    &#10;krajLBK       -1.11305    0.55165  -2.018 0.043828 *  &#10;krajMSK       -0.61390    0.37252  -1.648 0.099593 .  &#10;krajOLK       -0.49704    0.32935  -1.509 0.131501    &#10;krajPAK       -1.18444    0.35090  -3.375 0.000758 ***&#10;krajPLK       -1.28668    0.44238  -2.909 0.003691 ** &#10;krajSTC        0.01872    0.27806   0.067 0.946322    &#10;krajULKV      -0.41950    0.61647  -0.680 0.496315    &#10;krajVYS       -1.17290    0.39733  -2.952 0.003213 ** &#10;krajZLK       -0.38170    0.36487  -1.046 0.295698    &#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 &#10;&#10;(Dispersion parameter for quasibinomial family taken to be 1.304775)&#10;&#10;    Null deviance: 2396.8  on 1343  degrees of freedom&#10;Residual deviance: 2198.6  on 1330  degrees of freedom&#10;AIC: NA&#10;&#10;Number of Fisher Scoring iterations: 4&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and it's crossvalidation:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; cv.glm(orel, m, K = 10)&#10;$call&#10;cv.glm(data = orel, glmfit = m, K = 10)&#10;&#10;$K&#10;[1] 10&#10;&#10;$delta&#10;[1] 0.2156313 0.2154078&#10;&#10;$seed&#10;  [1]         403         526   300751243  -244464717  1066448079  1971573706 -1154513152&#10;  [8]   634841816 -1521293072 -1040655077   505710009  -323431793 -1218609191  1060964279&#10; [15]  1349082996   -32847357 -1387496845   821178952  -971482876  1295018851  1380491861&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now it converged. But the delta seems more or less the same, despite of the fact that this model looks much more sane! I'm confused by the crossvalidation now... please give me a hint on how interpret it.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-01-29T09:42:29.297" Id="48766" LastActivityDate="2015-02-07T10:19:01.010" LastEditDate="2013-02-03T11:39:07.713" LastEditorUserId="5509" OwnerUserId="5509" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;regression&gt;&lt;logistic&gt;&lt;cross-validation&gt;&lt;cv.glm&gt;" Title="Intepretation of crossvalidation result - cv.glm()" ViewCount="2077" />
  
  <row Body="&lt;p&gt;As per ocram's answer, ML is biased for the estimation of variance components. But observe that the bias gets smaller for larger sample sizes. Hence in answer to your questions &quot;&lt;em&gt;...what are the advantages of REML vs ML ? Under what circumstances may REML be preferred over ML (or vice versa) when fitting a mixed effects model ?&lt;/em&gt;&quot;, for small sample sizes REML is preferred. However, likelihood ratio tests for REML require exactly the same fixed effects specification in both models. So, to compare models with different fixed effects (a common scenario) with an LR test, ML must be used.&lt;/p&gt;&#10;&#10;&lt;p&gt;REML takes account of the number of (fixed effects) parameters estimated, losing 1 degree of freedom for each. This is achieved by applying ML to the least squares residuals, which are independent of the fixed effects. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-01-29T10:56:50.867" Id="48770" LastActivityDate="2013-01-29T13:11:07.807" LastEditDate="2013-01-29T13:11:07.807" LastEditorUserId="5739" OwnerUserId="7486" ParentId="48671" PostTypeId="2" Score="13" />
  <row AnswerCount="0" Body="&lt;p&gt;If you have a one dimensional space, like you have one variable, and two sets of observations,  and want a measure of separation between the two sets of observations you can calculate a d prime like &lt;/p&gt;&#10;&#10;&lt;p&gt;$d=(\mu1-\mu2)/\sqrt{(s_1^2+s_2^2)/2}$&lt;/p&gt;&#10;&#10;&lt;p&gt;basically distance between means over the square root of sum of variances over two,&lt;br&gt;&#10;I am not sure, but I guess there is an assumption of normality and positive means here,&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is, what if you are in a multi dimensional (or at least two dimensional) space, so you have more than one variable that defines each observation? How do you calculate the 2D or ND d prime?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-01-29T11:24:41.230" Id="48772" LastActivityDate="2013-01-29T12:57:21.917" LastEditDate="2013-01-29T12:57:21.917" LastEditorUserId="3277" OwnerUserId="17909" PostTypeId="1" Score="0" Tags="&lt;distance&gt;" Title="Multidimensional d prime" ViewCount="55" />
  <row Body="&lt;p&gt;A couple of uses that spring to mind:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Spam detection: It may be stretching the definition, but spam and not-spam can be considered topics. You might also classify by emotion, trustworthiness, etc.&lt;/li&gt;&#10;&lt;li&gt;Navigation: Since long before computers existed, libraries have categorized books by topic. It makes them easier to find. The same goes for any document store.&lt;/li&gt;&#10;&lt;li&gt;Further processing. Automatic methods like machine learning often work much better if the domain on which they operate can be narrowed (for instance, machine translation might be tuned for news or for scientific articles).&lt;/li&gt;&#10;&lt;li&gt;Information retrieval: A search engine user might like to filter search results by topic (using a 'faceted search' pattern). It might also be used as a keyword in the inverse index representation of the document (that is, to enrich the flat text).&lt;/li&gt;&#10;&lt;li&gt;Suggested content: Track the topics for the content that a user visits and then suggest new content that matches this 'signature'. You can also use this signature as way to visualize user preferences to interested parties.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="4" CreationDate="2013-01-29T16:04:47.467" Id="48788" LastActivityDate="2013-01-29T16:04:47.467" OwnerUserId="20085" ParentId="48784" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;I am assuming that measures of both the exposure (leadership characteristics) and outcome (teacher self-efficacy) are ordinally scaled variables. For instance, teacher efficacy may be evaluated on a 4-point Likert scale. &lt;/p&gt;&#10;&#10;&lt;p&gt;With Pearson's chi-square test for contingency tables, the null hypothesis is stated that for any reported leadership level, the proportions of self-efficacy in teaching are exactly the same. We will reject the null hypothesis if any proportion of efficacy responses in any leadership evaluation level is different than any other efficacy response. This makes reporting the results of a significant analysis difficult. You might observe the &quot;worst&quot; leadership responders having bimodal proportions (50% least efficacious, 50% most efficacious) whereas &quot;best&quot; leadership responders are moderate (25% all efficacy). In this case, there is no difference in aggregate teaching levels (same mean, consistent median, just a little more diverse).&lt;/p&gt;&#10;&#10;&lt;p&gt;If we're interested in a &lt;em&gt;trend&lt;/em&gt; in that data, one option is to use linear regression with numerically coded contingency levels. This exploits the ordering to determine if there's a first order trend in the numeric data. If we find the inference on the slope parameter from a linear regression model is significant, then we can report the results of a significant analysis as, &quot;on average, individuals with better evaluations of leadership also self-reported better teaching efficacy (p value)&quot;. This is the same as doing an ANOVA. It also has more power because it uses ordering.&lt;/p&gt;&#10;&#10;&lt;p&gt;When I analyze this type of data, I like treating ordinal variables as continuous because it allows me to exploit their ordered nature to develop better statistical results.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-29T16:11:20.030" Id="48790" LastActivityDate="2013-01-29T16:11:20.030" OwnerUserId="8013" ParentId="48787" PostTypeId="2" Score="1" />
  
  
  
  <row Body="&lt;p&gt;Back in the late 1990s, I did my dissertation on collinearity. &lt;/p&gt;&#10;&#10;&lt;p&gt;My conclusion was that condition indexes were best.&lt;/p&gt;&#10;&#10;&lt;p&gt;The main reason was that, rather than look at &lt;em&gt;individual&lt;/em&gt; variables, it lets you look at &lt;em&gt;sets&lt;/em&gt; of variables. Since collinearity is a function of sets of variables, this is a good thing. &lt;/p&gt;&#10;&#10;&lt;p&gt;Also, the results of my Monte Carlo study showed better sensitivity to problematic collinearity, but I have long ago forgotten the details.&lt;/p&gt;&#10;&#10;&lt;p&gt;On the other hand, it is probably the hardest to explain. Lots of people know what $R^2$ is. Only a small subset of those people have heard of eigenvalues. However, when I have used condition indexes as a diagnostic tool, I have never been asked for an explanation. &lt;/p&gt;&#10;&#10;&lt;p&gt;For much more on this, check out books by David Belsley. Or, if you really want to, you can get my dissertation &lt;a href=&quot;http://fordham.bepress.com/dissertations/AAI9926897/&quot;&gt;Multicollinearity diagnostics for multiple regression: A Monte Carlo study&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2013-01-29T23:10:28.183" Id="48827" LastActivityDate="2013-01-29T23:10:28.183" OwnerUserId="686" ParentId="48822" PostTypeId="2" Score="7" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I have a fairly simple question about (multivariate) Kernel Density Estimators, but I somehow don't seem to find the answer anywhere: are these estimators supposed to specify a proper probability density function (integrating to 1.0)?&lt;/p&gt;&#10;&#10;&lt;p&gt;I am currently implementing a small software package including kernel density estimators, and in particular a product kernel density estimator for the multivariate case, using the formula found &lt;a href=&quot;http://research.cs.tamu.edu/prism/lectures/pr/pr_l7.pdf&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;, and Scott's rule of thumb for the bandwidth selection.&lt;/p&gt;&#10;&#10;&lt;p&gt;The estimator seems to get right the overall shape of the distribution, but the density values are clearly not classical probability densities, as their integration would yield a value far superior to 1. So I am wondering if I made an error in my implementation, or if these values are correct. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have a follow-up question in case the estimator is not supposed to yield regular probability densities: is there an easy way to convert the estimator function into a standard PDF (i.e. to normalise the function)?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-01-30T00:26:11.073" Id="48833" LastActivityDate="2013-03-01T16:30:24.270" OwnerUserId="12793" PostTypeId="1" Score="1" Tags="&lt;multivariate-analysis&gt;&lt;pdf&gt;&lt;kde&gt;" Title="Are Product KDEs proper density functions?" ViewCount="71" />
  
  <row AcceptedAnswerId="48940" AnswerCount="1" Body="&lt;p&gt;I am given the following transition matrix&lt;/p&gt;&#10;&#10;&lt;p&gt;$$P= \pmatrix{ 1-\alpha &amp;amp; \alpha \\ \beta &amp;amp; 1-\beta}, \ \alpha,\beta \in (0,1)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;with the states $S=\{1,2\}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to determine the stationary distribution $\pi$ of the Markov chain determined by a starting distribution and the transition matrix $P$.&lt;/p&gt;&#10;&#10;&lt;p&gt;We have the system:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\pi(1)=(1-\alpha)\cdot \pi(1) + \beta\cdot\pi(2)$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\pi(2)=\alpha\cdot \pi(1) + (1-\beta)\cdot\pi(2)$  &lt;/p&gt;&#10;&#10;&lt;p&gt;When I substitute for $\pi(2)$ from the first equation in the second, I get $0=0$.&#10;However, the answer should be:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\pi(1)=\frac{\beta}{\alpha+\beta},\pi(2)=\frac{\alpha}{\alpha+\beta}$$.&lt;/p&gt;&#10;&#10;&lt;p&gt;What am I doing wrong? Thank you for your time.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-01-30T01:19:36.223" FavoriteCount="1" Id="48836" LastActivityDate="2013-01-31T00:23:52.007" LastEditDate="2013-01-30T02:19:00.937" LastEditorUserId="10749" OwnerUserId="10749" PostTypeId="1" Score="3" Tags="&lt;stochastic-processes&gt;&lt;markov-process&gt;&lt;stationarity&gt;" Title="Stationary matrix given a transition matrix" ViewCount="163" />
  <row AcceptedAnswerId="48853" AnswerCount="1" Body="&lt;p&gt;I have a fitted GLM model: &lt;code&gt;m1=glm(y~x,family=poisson,data=data)&lt;/code&gt;.  I would like to use this fitted model to simulate new data but &lt;code&gt;simulate(m1,nsim=1)&lt;/code&gt; results only in y's for the original x-values used to fit the model.  Can the simulate function be used to generate y's from new x-values?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-01-30T05:16:18.770" Id="48848" LastActivityDate="2013-02-01T16:59:17.340" LastEditDate="2013-02-01T16:59:17.340" LastEditorUserId="12318" OwnerUserId="12318" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;generalized-linear-model&gt;&lt;poisson&gt;&lt;simulation&gt;" Title="Using fitted GLM model to simulate y's from new x-values" ViewCount="730" />
  
  <row Body="&lt;p&gt;Canonical correlation is not your tool, but possibly you don't mean that method because you also refer to 'correlation analysis' which is different.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the simplest case you have an instrument that measures satisfaction, e.g. a question in a survey, and you have 200 measures from employees and 400 from the customers.  You don't have missing data unless the answers are coupled somehow. As @F.Tusell suggests, if the survey was given to the customer who complained and also to the employee that dealt with the complaint then they would be analysed together.  As you describe it you just have data in two groups with different sizes.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the absence of anything coupling the two sets of responses you might be interested in &lt;em&gt;difference&lt;/em&gt; between these two satisfaction distributions.  One very simple but informative question might be: do customer and employee satisfaction levels differ on average?  This is the sort of thing answered by a t-test.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-30T11:53:55.193" Id="48873" LastActivityDate="2013-01-30T11:53:55.193" OwnerUserId="1739" ParentId="46743" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;(1) The base of the logarithm won't necessarily affect the structure of your model - changing the base is equivalent to using different units of measurement for the response. If the model is invariant to using different units of measurement then only the interpretation of coefficients changes when you transform.&lt;/p&gt;&#10;&#10;&lt;p&gt;(2) AICs are not comparable for models with different response variables: unless you can work out what the effect of a transformation will be on the likelihood it's best to follow the usual rule-of-thumb &amp;amp; not compare AICs of the same data-set with different transformations.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-01-30T12:32:45.743" Id="48875" LastActivityDate="2013-01-30T16:35:48.233" LastEditDate="2013-01-30T16:35:48.233" LastEditorUserId="17230" OwnerUserId="17230" ParentId="48872" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;You should be able to use KDR to estimate a Dirichlet type distribution, you just need to ensure you capture the constraints correctly.&lt;/p&gt;&#10;&#10;&lt;p&gt;First off, drop the dimensionality by 1.  As you correctly point out, in the 2D case, you have a beta distribution, hence there is only 1 degree of freedom.  In this case the correct thing to do would be to estimate the beta shaped pdf of one of the two variables.  In higher dimensions, once you have $n-1$ of your probabilities, you know the last one.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now you only have a boundary problem.  For the 2D case, this is just the edges at 0 and 1.  If you use the normal Kernel it will &quot;spill over&quot; beyond this which will have 2 effects&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;The support will be incorrect, if you truncate it you won't sum to zero, if you rescale you will over estimate the centre, underestimate the edges.&lt;/li&gt;&#10;&lt;li&gt;The pdf estimate at the edges will tend to be pulled towards the middle - there will be no values to the left of $0$ or to the right of $1$ to balance the kernel smooth.  I.e. imagine that the distribution rises to a point at 1, then the kernel will lower that point by averaging with values at 0.95, biasing the estimate at 1 downwards.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;The second of these is difficult (impossible?) to deal with in generality, although there have been many attempts.  Given that your reason for choosing this route was for simply implementation, we shall ignore those.  The first problem can be mitigated by simply using a kernel that truncates at 0 and 1.  One way of doing this is to use a beta distribution as your kernel, with quite large parameters.  This approximates a normal away from 0 and 1, but then &quot;bunches up&quot; when at the edges.&lt;/p&gt;&#10;&#10;&lt;p&gt;Moving on to higher dimensions, in 3D you have support on a right angled triangle from (0,0) - (0,1) - (1,0).  By analogy with the above, you could use a dirichlet kernel.  This behaves like a normal distribution in the middle of the triangle, but bunches up at the boundaries.  The same then clearly applies in higher dimensions.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-01-30T14:13:38.537" Id="48885" LastActivityDate="2013-01-30T14:13:38.537" OwnerUserId="19879" ParentId="48877" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;Consider &lt;em&gt;any&lt;/em&gt; location-scale family determined by a &quot;standard&quot; distribution $F$,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\Omega_F = \left\{F_{(\mu, \sigma)}: x \to F\left(\frac{x-\mu}{\sigma}\right) \mid \sigma \gt 0\right\}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Assuming $F$ differentiable we readily find that the PDFs are $\frac{1}{\sigma}f\left((x-\mu)/\sigma\right)dx$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Truncating these distributions to restrict their support between $a$ and $b$, $a \lt b$, means that the PDFs are replaced by &lt;/p&gt;&#10;&#10;&lt;p&gt;$$f_{(\mu, \sigma; a,b)}(x) = \frac{f\left(\frac{x-\mu}{\sigma}\right)dx}{\sigma C(\mu, \sigma, a, b)}, a \le x \le b$$&lt;/p&gt;&#10;&#10;&lt;p&gt;(and are zero for all other values of $x$) where $C(\mu, \sigma, a, b) = F_{(\mu,\sigma)}(b) - F_{(\mu,\sigma)}(a)$ is the normalizing factor needed to ensure that $f_{(\mu, \sigma; a, b)}$ integrates to unity.  (Note that $C$ is identically $1$ in the absence of truncation.)  The log likelihood for iid data $x_i$ therefore is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\Lambda(\mu, \sigma) = \sum_i \left[\log{f\left(\frac{x_i-\mu}{\sigma}\right)} - \log{\sigma}-\log{C(\mu, \sigma, a, b)}\right].$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Critical points (including any global minima) are found where either $\sigma=0$ (a special case I will ignore here) or the gradient vanishes.  Using subscripts to denote derivatives, we may formally compute the gradient and write the likelihood equations as&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\eqalign{&#10;0 &amp;amp;= \frac{\partial\Lambda}{\partial\mu} &amp;amp;= \sum_i \left[\frac{-f_\mu\left(\frac{x_i-\mu}{\sigma}\right)}{f\left(\frac{x_i-\mu}{\sigma}\right)} -\frac{C_\mu(\mu,\sigma,a,b)}{C(\mu,\sigma,a,b)}\right] \\&#10;0 &amp;amp;= \frac{\partial\Lambda}{\partial\sigma} &amp;amp;= \sum_i \left[\frac{-f_\sigma\left(\frac{x_i-\mu}{\sigma}\right)}{\sigma^2f\left(\frac{x_i-\mu}{\sigma}\right)} -\frac{1}{\sigma}-\frac{C_\sigma(\mu,\sigma,a,b)}{C(\mu,\sigma,a,b)}\right]&#10;}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Because $a$ and $b$ are fixed, drop them from the notation and write $nC_\mu(\mu, \sigma, a, b)/C(\mu, \sigma,a,b)$ as $A(\mu,\sigma)$ and $nC_\sigma(\mu, \sigma, a, b)/C(\mu, \sigma,a,b)$ as $B(\mu, \sigma)$.  (With no truncation, both functions would be identically zero.)  Separating the terms involving the data from the rest gives&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\eqalign{&#10;-A(\mu,\sigma) &amp;amp;=  \sum_i \frac{f_\mu\left(\frac{x_i-\mu}{\sigma}\right)}{f\left(\frac{x_i-\mu}{\sigma}\right)} \\&#10;-\sigma^2 B(\mu,\sigma) - n\sigma &amp;amp;= \sum_i \frac{f_\sigma\left(\frac{x_i-\mu}{\sigma}\right)}{f\left(\frac{x_i-\mu}{\sigma}\right)} &#10;}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;By comparing these to the no-truncation situation it is evident that&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;Any sufficient statistics for the original problem are sufficient for the truncated problem (because the right hand sides have not changed).&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Our ability to find closed-form solutions relies on the tractability of $A$ and $B$.  If these do not involve $\mu$ and $\sigma$ in simple ways, we cannot hope to obtain closed-form solutions in general.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;For the case of a normal family, $C(\mu,\sigma,a,b)$ of course is given by the cumulative normal PDF, which is a difference of error functions: there is no chance that a closed-form solution can be obtained in general.  However, there are only two sufficient statistics (the sample mean and variance will do) and the CDF is as smooth as can be, so numerical solutions will be relatively easy to obtain.&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2013-01-30T17:25:43.827" Id="48909" LastActivityDate="2013-01-31T15:25:42.390" LastEditDate="2013-01-31T15:25:42.390" LastEditorUserId="18218" OwnerUserId="919" ParentId="48897" PostTypeId="2" Score="10" />
  
  <row Body="&lt;p&gt;This is more a response to @PeterFlom's comment on my comment, but it is too big to fit in a comment (and does relate to the original question).&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is some R code to show a case where there are multiple lines that all give the same minimum MAD/SAD values.&lt;/p&gt;&#10;&#10;&lt;p&gt;The first part of the example is clearly contrived data to demonstrate, but the end includes more of a random element to demonstrate that the general concept will still hold in some more realistic cases.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x &amp;lt;- rep(1:10, each=2)&#10;y &amp;lt;- x/10 + 0:1&#10;plot(x,y)&#10;&#10;sad &amp;lt;- function(x,y,coef) { # mad is sad/n&#10;    yhat &amp;lt;- coef[1] + coef[2]*x&#10;    resid &amp;lt;- y - yhat&#10;    sum( abs( resid ) )&#10;}&#10;&#10;library(quantreg)&#10;fit0 &amp;lt;- rq( y~x )&#10;abline(fit0)&#10;&#10;fit1 &amp;lt;- lm( y~x, subset= c(1,20) )&#10;fit2 &amp;lt;- lm( y~x, subset= c(2,19) )&#10;fit3 &amp;lt;- lm( y~x, subset= c(2,20) )&#10;fit4 &amp;lt;- lm( y~x, subset= c(1,19) )&#10;&#10;fit5.coef &amp;lt;- c(0.5, 1/10)&#10;&#10;abline(fit1)&#10;abline(fit2)&#10;abline(fit3)&#10;abline(fit4)&#10;abline(fit5.coef)&#10;for (i in seq( -0.5, 0.5, by=0.1 ) ) {&#10;    abline( fit5.coef + c(i,0) )&#10;}&#10;&#10;tmp1 &amp;lt;- seq( coef(fit1)[1], coef(fit2)[1], len=10 )&#10;tmp2 &amp;lt;- seq( coef(fit1)[2], coef(fit2)[2], len=10 )&#10;&#10;for (i in seq_along(tmp1) ) {&#10;    abline( tmp1[i], tmp2[i] )&#10;}&#10;&#10;sad(x,y, coef(fit0))&#10;sad(x,y, coef(fit1))&#10;sad(x,y, coef(fit2))&#10;sad(x,y, coef(fit3))&#10;sad(x,y, coef(fit4))&#10;sad(x,y, fit5.coef )&#10;&#10;for (i in seq( -0.5, 0.5, by=0.1 ) ) {&#10;    print(sad(x,y, fit5.coef + c(i,0) ))&#10;}&#10;&#10;for (i in seq_along(tmp1) ) {&#10;    print(sad(x,y, c(tmp1[i], tmp2[i]) ) )&#10;}&#10;&#10;set.seed(1)&#10;y2 &amp;lt;- y + rnorm(20,0,0.25)&#10;plot(x,y2)&#10;fitnew &amp;lt;- rq(y2~x)  # note the still non-unique warning&#10;abline(fitnew)&#10;abline(coef(fitnew) + c(.1,0))&#10;abline(coef(fitnew) + c(0, 0.01) )&#10;sad( x,y2, coef(fitnew) )&#10;sad( x,y2, coef(fitnew)+c(.1,0))&#10;sad( x,y2, coef(fitnew)+c(0,0.01))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2013-01-30T19:11:15.033" Id="48917" LastActivityDate="2013-01-30T19:11:15.033" OwnerUserId="4505" ParentId="48627" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;I know of one way, there might be others.&lt;/p&gt;&#10;&#10;&lt;p&gt;The probabilistic formulation of PCA, pPCA, is actually a density model-it allows you to estimate $p(x)$. Thus, in a generative approach you can get a predictive distribution via Bayes rule: $p(c|x) \propto p(x|c) p(c)$. Thus, you can do one PCA for each handwritten digit. For a new digit, pick the class for which the reconstruction error of the corresponding PCA (which is proportional to the likelihood) is lowest.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-30T19:29:56.317" Id="48920" LastActivityDate="2013-01-30T19:29:56.317" OwnerUserId="2860" ParentId="48918" PostTypeId="2" Score="1" />
  <row AnswerCount="2" Body="&lt;p&gt;I'd like to use exponential smoothing to forecast the following data. &#10;The data is daily based. Because of some policy reasons, every $29^\text{th}$, $30^\text{th}$ and $31^\text{th}$ of each month, the data will drop to just hundreds.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Do I need to take out the two/three days at the end of each month since they are not following the same pattern with other days of each month? &lt;/li&gt;&#10;&lt;li&gt;If I use &lt;code&gt;Holtwinter()&lt;/code&gt; to do the triple exponential smoothing, R will show error message: &lt;code&gt;time series has no or less than 2 periods.&lt;/code&gt;&lt;/li&gt;&#10;&lt;li&gt;If I remove the two days at the end of each month, the time range would be &lt;code&gt;3/12/1998&lt;/code&gt;  to &lt;code&gt;3/28/1998&lt;/code&gt;, &lt;code&gt;4/1/1998&lt;/code&gt; to &lt;code&gt;4/28/1998&lt;/code&gt;, &lt;code&gt;5/1/1998&lt;/code&gt; to &lt;code&gt;5/28/1998&lt;/code&gt;. How can I run R since the data are not in the complete month period?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;pre&gt;&#10;18000   3/12/1998&#10;61000   3/13/1998&#10;59000   3/14/1998&#10;59000   3/15/1998&#10;66000   3/16/1998&#10;38000   3/17/1998&#10;37000   3/18/1998&#10;20000   3/19/1998&#10;72000   3/20/1998&#10;44000   3/21/1998&#10;37000   3/22/1998&#10;33000   3/23/1998&#10;28000   3/24/1998&#10;54000   3/25/1998&#10;24000   3/26/1998&#10;66000   3/27/1998&#10;52000   3/28/1998&#10;280     3/29/1998&#10;200     3/30/1998&#10;400     3/31/1998&#10;186000  4/1/1998&#10;31000   4/2/1998&#10;82000   4/3/1998&#10;39000   4/4/1998&#10;58000   4/5/1998&#10;26000   4/6/1998&#10;41000   4/7/1998&#10;37000   4/8/1998&#10;19000   4/9/1998&#10;65000   4/10/1998&#10;54000   4/11/1998&#10;55000   4/12/1998&#10;56000   4/13/1998&#10;40000   4/14/1998&#10;34000   4/15/1998&#10;27000   4/16/1998&#10;72000   4/17/1998&#10;56000   4/18/1998&#10;56000   4/19/1998&#10;60000   4/20/1998&#10;39000   4/21/1998&#10;43000   4/22/1998&#10;22000   4/23/1998&#10;63000   4/24/1998&#10;35000   4/25/1998&#10;36000   4/26/1998&#10;34000   4/27/1998&#10;43000   4/28/1998&#10;300     4/29/1998&#10;250     4/30/1998&#10;133000  5/1/1998&#10;28000   5/2/1998&#10;63000   5/3/1998&#10;65000   5/4/1998&#10;33000   5/5/1998&#10;29000   5/6/1998&#10;21000   5/7/1998&#10;75000   5/8/1998&#10;34000   5/9/1998&#10;77000   5/10/1998&#10;54000   5/11/1998&#10;32000   5/12/1998&#10;26000   5/13/1998&#10;19000   5/14/1998&#10;64000   5/15/1998&#10;54000   5/16/1998&#10;64000   5/17/1998&#10;58000   5/18/1998&#10;29000   5/19/1998&#10;29000   5/20/1998&#10;16000   5/21/1998&#10;62000   5/22/1998&#10;32000   5/23/1998&#10;38000   5/24/1998&#10;29000   5/25/1998&#10;38000   5/26/1998&#10;36000   5/27/1998&#10;34000   5/28/1998&#10;160     5/29/1998&#10;150     5/30/1998&#10;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2013-01-30T19:30:45.290" FavoriteCount="1" Id="48921" LastActivityDate="2013-08-31T18:39:54.273" LastEditDate="2013-08-31T18:39:54.273" LastEditorUserId="27581" OwnerUserId="20287" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;time-series&gt;&lt;forecasting&gt;&lt;exponential-smoothing&gt;" Title="Using exponential smoothing to forecast irregularly spaced data in R" ViewCount="1210" />
  <row Body="&lt;p&gt;There aren't a priori correct &amp;amp; incorrect models - it depends what you're modelling.  If your dependent variable is $Y=\log \frac{Q}{R}$ &amp;amp; you &lt;em&gt;don't&lt;/em&gt; include $R$ as a dependent variable, you're saying you &lt;em&gt;know&lt;/em&gt; the relation between $Q$ &amp;amp; $R$, effectively bundling $-\log R$ into the intercept term of the model for $\log Q$.  If you're not so sure, then by all means include it - perhaps as a $\beta_r \log R$ term.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-01-30T20:30:49.417" Id="48927" LastActivityDate="2013-01-30T20:30:49.417" OwnerUserId="17230" ParentId="48871" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;You could repeat the sampling process many times and calculate the confidence interval. This will give you an estimate where e.g. 95% of your sample medians fall.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-30T20:31:42.053" Id="48928" LastActivityDate="2013-01-30T20:31:42.053" OwnerUserId="12719" ParentId="48925" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;Just an empirical answer, I am sure someone else will be able to give you a more formal one.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(12345)&#10;&#10;# Generate a big dataset, log-normally distributed&#10;bigdata &amp;lt;- rlnorm(10e6, meanlog=log(25))&#10;&#10;# Now generate 500 different samples with &#10;# 10, 100, 1000, or 10000 elements in it&#10;# and compare the medians&#10;&#10;nelem &amp;lt;- c(10, 100, 1000, 10000)&#10;&#10;m &amp;lt;- matrix(NA, nrow=1000, ncol=length(nelem))&#10;par(mfrow=c(2,2))&#10;&#10;for (el in 1:length(nelem))&#10;  {&#10;  for (i in 1:500)&#10;    {&#10;    data &amp;lt;- sample(bigdata, nelem[el], replace=F)&#10;    m[i, el] &amp;lt;- median(data)&#10;    }&#10;&#10;  # Plot the histogram&#10;  hist(m, 100, col=&quot;black&quot;, freq=F, las=1, &#10;       main=paste(nelem[el],&quot;element sampled&quot;), xlab=&quot;Median&quot;)&#10;  # Plot the &quot;real&quot; median&#10;  abline(v=median(bigdata), col=&quot;red&quot;, lwd=2)&#10;  }&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This gives you the distribution of the medians from 500 trials of sampling:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/qxNyB.jpg&quot; alt=&quot;median histograms&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;As you can see, you get fairly good results already by sampling 1000 elements of the 10M from the original data.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-01-30T20:38:59.247" Id="48929" LastActivityDate="2013-01-30T20:38:59.247" OwnerUserId="582" ParentId="48925" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;When you are doing contrasts, you are doing a specific, stated linear combination of cell means within the context of the appropriate error term. As such, the concept of &quot;Type of SS&quot; is not meaningful with contrasts. Each contrast is essentially the first effect using a Type I SS. &quot;Type of SS&quot; has to do with what is partialled out or accounted for by the other terms. For contrasts, nothing is partialled out or accounted for. The contrast stands by itself.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-01-30T21:45:58.713" Id="48932" LastActivityDate="2013-01-30T21:45:58.713" OwnerUserId="20268" ParentId="4544" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a survival data set. I want to see the cluster specific variation, which we usually can do by using Cox proportional hazard model with frailty. But, the constant hazard ratio assumption of Cox PH may not be appropriate in some situations, in particular when the hazard rates of different individuals converge to the population mortality rate. In this situation, a proportional odds model may be more appropriate. But how can we perform a proportional odds model with random effect(frailty)? Any R package is available for this?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-01-31T04:26:17.503" Id="48944" LastActivityDate="2014-02-14T01:59:12.033" LastEditDate="2014-02-14T01:59:12.033" LastEditorUserId="88" OwnerUserId="17994" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;frailty&gt;&lt;odds&gt;&lt;ordered-logit&gt;" Title="Proportional odds model with random effects" ViewCount="260" />
  <row Body="&lt;p&gt;Let me translate this output into a more visual format. You've basically fit an equation that is $y = .0094922 + .5210616 \cdot x$ to the data (straight blue line and green points below). That's your prediction of $y$ for each value of $x$. &lt;/p&gt;&#10;&#10;&lt;p&gt;This model has an unadjusted $R^2$ of $0.2494$, which is reasonable for cross-sectional data like this. I am not sure what the covariance terms above are. They do not seem to correspond to anything I get with my software, so watch out for that.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can interpret the correlation as a measure of fit with this simple model. The correlation between your prediction and your actual data is about $0.4994$.&lt;/p&gt;&#10;&#10;&lt;p&gt;I don't think it helps to normalize this data (subtract the mean and divide by the standard deviation), unless it help with the interpretation somehow. It won't help the fit. I am also not sure if you had another transformation in mind above.&lt;/p&gt;&#10;&#10;&lt;p&gt;There does seems to be a strange nonlinear relationship, which is picked up by the lowess (a fancier type of regression, orange line). Since you have not told us anything about the data, it's hard to know whether this is something that is meaningful.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/BpMjK.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-01-31T06:35:52.823" Id="48949" LastActivityDate="2013-01-31T06:35:52.823" OwnerUserId="7071" ParentId="48882" PostTypeId="2" Score="3" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am currently working on a model for exchange rates, and I want to use an ARIMA-GARCH specification. More precisely, I work on the log-returns series. &lt;/p&gt;&#10;&#10;&lt;p&gt;First of all, I perform multiple KPSS and ADF tests to get the conclusion that the series is stationnary up to a trend. I then use the auto.arima function to select the best model based on AIC or BIC series. My problem arises when I want to check the model adequacy by looking at the residuals. Since I work with data from January 1999 to December 2012, I have a time series of 3428 observations. When I look at the p-values of the Ljung-Box test, I get the conclusion that the different lags are not significant up to lag 23, and become significant from this lag onwards. &lt;/p&gt;&#10;&#10;&lt;p&gt;Which conclusion should I make ? If I take the usual $\log(n)$ rule, I have to look around lag 8 and I thus have no problem. But sometimes in the literature, it is asked to look around $\sqrt(n)$ which is around lag 59, and I can not accept the model. Moreover, I want next to simulate trajectories for one year, that is, 250 points, and we can find advices saying it would be better to look around the lag corresponding to the projection horizon. Thus here, I would have a big problem, since I won't find a model which Ljung-Box p-values significative up to lag 250.&lt;/p&gt;&#10;&#10;&lt;p&gt;Would you have any advice for me ? Thanks in advance.&lt;/p&gt;&#10;&#10;&lt;p&gt;Ludovic&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-31T08:12:39.553" Id="48955" LastActivityDate="2013-01-31T08:12:39.553" OwnerUserId="17913" PostTypeId="1" Score="2" Tags="&lt;time-series&gt;&lt;arima&gt;&lt;garch&gt;" Title="ARIMA-GARCH model for exchange rates" ViewCount="509" />
  <row Body="&lt;p&gt;If your raw count data doesn't look like a Poisson distribution, then you are missing something. Perhaps the number of actions is dependent on the temperature, so on hot days people do fewer things. Then temperature variation over your study period would affect the distribution and make it non-Poisson.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, the number of actions each day could still be Poisson with a mean dependent on the temperature. If you have the temperature each day, then you can do a GLM, regressing number of actions as a Poisson variable, dependent on temperature. If that fits nicely, job done.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you don't have possible explanatory variables, then all you can say is &quot;something else is going on - the number of actions is not from independent Poisson samples&quot; - ie reject your null hypothesis.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are distribution-free tests that can compare paired observations by using rankings and so on. Typically they do large numbers of permutations and compute a test statistic...&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-31T08:52:49.560" Id="48960" LastActivityDate="2013-01-31T08:52:49.560" OwnerUserId="1549" ParentId="48956" PostTypeId="2" Score="5" />
  <row AcceptedAnswerId="49087" AnswerCount="1" Body="&lt;p&gt;My knowledge of statistics are small so I need a little bit of help. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have some data showing the relationship between age and walking speed. The numbers of walking speed are presented only as the 15th and the 50th percentile. &#10;For example:  Child(0-12 years): 1.07m/s (15th percentile)  1.33m/s (50th percentile). &lt;/p&gt;&#10;&#10;&lt;p&gt;I need to find how many percent is between 1.0m/s - 1.2m/s and 1.2m/s - 1.4 m/s etc. &#10;That is the likelihood of observing a child walking between 1.0 and 1.2 m/s etc. &lt;/p&gt;&#10;&#10;&lt;p&gt;Is that possible? Please help me out. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-01-31T12:24:33.080" Id="48973" LastActivityDate="2013-02-01T20:05:11.523" OwnerUserId="20315" PostTypeId="1" Score="3" Tags="&lt;normal-distribution&gt;&lt;quantiles&gt;" Title="Finding gaussian distribution given 15th and 50th percentile." ViewCount="747" />
  <row Body="&lt;p&gt;As there seems to be a misunderstanding about the statistical aspect of the procedures you used, here are some hints:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Your R model is actually a multiple linear regression with &lt;code&gt;geno_A&lt;/code&gt; and &lt;code&gt;geno_B&lt;/code&gt; treated as numeric variables, and it includes an interaction term, which is why you get four parameter estimates. I hope you really want &lt;code&gt;geno_A&lt;/code&gt; to be treated as numeric and not a categorical variable, otherwise you will have to dummy recode it.&lt;/li&gt;&#10;&lt;li&gt;Your Java code does not match the above model and there are two problems: first, you didn't include the interaction term, which is simply the product of &lt;code&gt;geno_A&lt;/code&gt; and &lt;code&gt;geno_B&lt;/code&gt; (see &lt;a href=&quot;http://stats.stackexchange.com/q/11645/930&quot;&gt;here&lt;/a&gt; for an illustration on how to code interaction between two variables); second, you are using &lt;code&gt;SimpleRegression&lt;/code&gt; but you should use &lt;code&gt;OLSMultipleLinearRegression&lt;/code&gt; instead.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Here is your code rewritten as &lt;a href=&quot;https://gist.github.com/4682926&quot;&gt;LinearRegression.java&lt;/a&gt; to fit the following two models: a simple additive model and a model with interaction. Its output agrees with R.&lt;/p&gt;&#10;&#10;&lt;h3&gt;Java&lt;/h3&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;% javac -cp commons-math3-3.1.1.jar LinearRegression.java &amp;amp;&amp;amp; java -cp commons-math3-3.1.1.jar:. LinearRegression&#10;First model: y = int + genoA + genoB&#10;Intercept: -0,032   beta1: -0,105   beta2: -0,605&#10;&#10;Second model: y = int + genoA + genoB + genoA:genoB&#10;Intercept: -0,008   beta1: -0,153   beta2: -0,677   beta2: 0,096&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;h3&gt;R&lt;/h3&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; lm(test_trait ~ geno_A + geno_B)&#10;&#10;Call:&#10;lm(formula = test_trait ~ geno_A + geno_B)&#10;&#10;Coefficients:&#10;(Intercept)       geno_A       geno_B  &#10;   -0.03233     -0.10479     -0.60492  &#10;&#10;&amp;gt; lm(test_trait ~ geno_A * geno_B)&#10;&#10;Call:&#10;lm(formula = test_trait ~ geno_A * geno_B)&#10;&#10;Coefficients:&#10;  (Intercept)         geno_A         geno_B  geno_A:geno_B  &#10;    -0.008235      -0.152979      -0.677208       0.096383  &#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2013-01-31T13:38:45.607" Id="48979" LastActivityDate="2013-01-31T13:38:45.607" OwnerUserId="930" ParentId="48854" PostTypeId="2" Score="5" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I have two time series that come from the same system. One is taken from the whole system, and the other is taken from, say, 10% of the system. The two time series have the same frequency. Is there a measure to know how similar these two time series are?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-31T20:26:59.000" Id="49008" LastActivityDate="2013-08-31T20:12:57.010" LastEditDate="2013-08-31T20:12:57.010" LastEditorUserId="27581" OwnerUserId="20336" PostTypeId="1" Score="0" Tags="&lt;time-series&gt;&lt;entropy&gt;&lt;mutual-information&gt;&lt;cross-correlation&gt;&lt;granger-causality&gt;" Title="Compare two time series" ViewCount="292" />
  <row AcceptedAnswerId="49032" AnswerCount="1" Body="&lt;p&gt;I have a bunch of web page response time data, and I'm recording dispersion stats--both variance and quartile--on an hourly basis.&lt;/p&gt;&#10;&#10;&lt;p&gt;Are there good ways to roll this up into larger time scales (e.g., daily, weekly), as opposed to having to reprocess the original dataset? I understand that in the initial hourly rollup I'm losing some information, so I may have to have estimated values for the daily, weekly etc rollups, and that's fine as long as the estimate is reasonably close.&lt;/p&gt;&#10;&#10;&lt;p&gt;One thought that I have is that in the hourly rollups I can model the distribution of data, and then aggregate the models themselves for the daily/weekly/etc rollups. I haven't actually done the work yet of determining appropriate models for that hourly data, but conceptually this seems like a plausible direction. (Just as an aside, are there distributions that are generally accepted as being reasonable models for web page response times?)&lt;/p&gt;&#10;&#10;&lt;p&gt;Another idea would be to have some pre-defined bins--e.g., [0, 0.5), [0.5, 1.0), [1.0, 2.0), etc., and store counts. Of course I could have more bins for better resolution.&lt;/p&gt;&#10;&#10;&lt;p&gt;Are there other approaches worth exploring?&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm not necessarily looking for theoretically perfect solutions here. Basically I'm putting together an operational dashboard for a bunch of web applications, and I need a practical way to allow operational staff to view dispersion statistics at different timescales.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-01-31T21:54:37.910" FavoriteCount="1" Id="49017" LastActivityDate="2013-05-02T02:59:05.500" LastEditDate="2013-01-31T22:35:02.917" LastEditorUserId="11284" OwnerUserId="11284" PostTypeId="1" Score="4" Tags="&lt;variance&gt;&lt;aggregation&gt;" Title="Options for aggregating dispersion data" ViewCount="162" />
  <row Body="&lt;p&gt;&lt;a href=&quot;http://tvtropes.org/pmwiki/pmwiki.php/Main/MathematiciansAnswer&quot; rel=&quot;nofollow&quot;&gt;Yes, you can&lt;/a&gt; compute the sample correlation matrix and see what the correlations are (in R this is as sample as &lt;code&gt;cor(X)&lt;/code&gt; where &lt;code&gt;X&lt;/code&gt; is your data matrix). But I'm not sure what you'll be able to conclude from these values; you'll need to define what you're looking for before you can properly test it.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, without any additional knowledge of what distribution your data comes from, one option would be to perform a $\chi^2$-test on each pair of columns to determine if they're independent. However, it's likely that independence is not really what you care about: with 10,000 rows, it's very likely that such a test will determine that two columns are not independent even if the relationship between them is quite weak.&lt;/p&gt;&#10;&#10;&lt;p&gt;Maybe you'd be better served by performing logistic regressions using each column to predict the others; the magnitude of the coefficients would then indicate how good a predictor each column is of every other. But it's hard to say what the best approach is without knowing more about your problem. A good place to start thinking about would be: if an oracle could tell you exactly what the relationships between your columns are, what would you do with the information?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-01-31T22:40:31.773" Id="49019" LastActivityDate="2013-01-31T22:40:31.773" OwnerUserId="2111" ParentId="48995" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;I am trying to compare incidence rate ratios (IRR).  This is what I have:&lt;/p&gt;&#10;&#10;&lt;p&gt;Group A (exposed vs. unexposed) IRR and &#10;Group B (exposed vs. unexposed) IRR&lt;/p&gt;&#10;&#10;&lt;p&gt;So how would I correctly test for the difference between the IRR from Group A and the IRR from Group B?  The question I would like to answer is whether the incremental differences observed in Group A are similar to those observed in Group B?  Please help!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-01-31T22:57:02.050" Id="49020" LastActivityDate="2013-02-01T09:15:20.567" OwnerUserId="20342" PostTypeId="1" Score="0" Tags="&lt;incidence-rate-ratio&gt;" Title="Comparing Incidence Rate Ratios" ViewCount="301" />
  <row Body="&lt;p&gt;As another option here, how about using a multinomial logistic regression (with the outcomes being trauma, [non-elective] caesarean, and no trauma?) &lt;/p&gt;&#10;&#10;&lt;p&gt;I'm not entirely sure if this approach will fully address the issue of bias, but one would get some measures of association about the associations between e.g. fetus size and having a non-elective caesarean section, and could see these side by side with the associations between the same exposure and trauma, and at the least make some qualitative comparisons about magnitudes of effect.&lt;/p&gt;&#10;&#10;&lt;p&gt;[This question was bumped by an answer edit, with an inactive original questioner, so I'm really just floating another idea here (plus I'm interested in this kind of question)]&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-01T01:54:09.523" Id="49033" LastActivityDate="2013-02-01T01:54:09.523" OwnerUserId="16974" ParentId="4807" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I know I'm very late to the party, but one thing that might help: You appear to be under the impression that there are three sources of randomness in your model: $S$, $E$, and $u = S - E$, where $u$ is the proxy measurement error. This is not correct. There are only &lt;strong&gt;two&lt;/strong&gt; sources of randomness in your model. They are (in a typical setup), $E$, your latent variable, and $u$, your proxy measurement error. &lt;/p&gt;&#10;&#10;&lt;p&gt;$S$ is a random variable, in the sense that it is equal to $E + u$, ie the sum of two random variables. However, because $S$ is uniquely determined by $E$ and $u$, it follows that $S$ is &lt;strong&gt;not a source of randomness&lt;/strong&gt;. So to discuss the dependency between $u$, $S$, and $E$ is not really meaningful, because at most only two of these three random variables can be a source of randomness in your model.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note, in a typical framework, a common assumption is that $u$ (proxy measurement error) is independent of $E$ (latent variable). Your framework appears odd in that someone has told you that your proxy measurement error is independent of $S$. We could, of course, set things up this way, by assuming that $u$ and $S$ are the sources of randomness and that $E$ is uniquely determined by the two of them. But, as I said, this is an odd way of doing things.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-02-01T02:30:43.373" Id="49036" LastActivityDate="2013-02-04T07:01:07.000" LastEditDate="2013-02-04T07:01:07.000" LastEditorUserId="16319" OwnerUserId="16319" ParentId="26734" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Think about temperature. If I predict that tomorrow it will be 30 degrees Celsius and it is actually 31, my &quot;percentage&quot; error is 100*1/31 = 3.2%. However, on the Fahrenheit scale, my prediction is 86 degrees, and it is actually 87.8, giving a &quot;percentage&quot; error of 100*1.8/87.8 = 2.05%.&lt;/p&gt;&#10;&#10;&lt;p&gt;If there is a meaningful zero, such as in measurements of length, then this problem doesn't occur. A percentage error using inches will give the same answer as a percentage error using centimetres.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-02-01T04:22:19.067" Id="49041" LastActivityDate="2013-02-01T04:22:19.067" OwnerUserId="159" ParentId="49030" PostTypeId="2" Score="6" />
  <row AcceptedAnswerId="49058" AnswerCount="2" Body="&lt;p&gt;&lt;strong&gt;My problem&lt;/strong&gt;: I recently met a statistician that informed me that splines are only useful for exploring data and are subjected to overfitting, thus not useful in prediction. He preferred exploring with simple polynomials... As I’m a big fan of splines, and this goes against my intuition I’m interested in finding out how valid these arguments are, and if there is a large group of &lt;em&gt;anti-spline-activists&lt;/em&gt; out there? &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Background&lt;/strong&gt;: I try to follow Frank Harrell, Regression Modelling Strategies (1), when I create my models. He argues that restricted cubic splines are a valid tool for exploring continuous variables. He also argues that the polynomials are poor at modelling certain relationships such as thresholds, logarithmic (2). For testing the linearity of the model he suggests an ANOVA test for the spline:&lt;/p&gt;&#10;&#10;&lt;p&gt;$H_0: \beta_2 = \beta_3 = … = \beta_{k-1} = 0 $&lt;/p&gt;&#10;&#10;&lt;p&gt;I’ve googled for overfitting with splines but not found that much useful (apart from general warnings about not using too many knots). In this forum there seems to be a preference for spline modelling, &lt;a href=&quot;http://stats.stackexchange.com/questions/47074/does-it-make-sense-to-cut-a-continuous-variable-to-intervals/47076#47076&quot;&gt;Kolassa&lt;/a&gt;, &lt;a href=&quot;http://stats.stackexchange.com/questions/47074/does-it-make-sense-to-cut-a-continuous-variable-to-intervals/47076#47076&quot;&gt;Harrell&lt;/a&gt;, &lt;a href=&quot;http://stats.stackexchange.com/questions/5015/what-are-the-advantages-disadvantages-of-using-splines-smoothed-splines-and&quot;&gt;gung&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;I found one blog post about polynomials, &lt;a href=&quot;http://www.portfolioprobe.com/2011/03/28/the-devil-of-overfitting/&quot;&gt;the devil of overfitting&lt;/a&gt; that talks about predicting polynomials. The post ends with these comments:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;To some extent the examples presented here are cheating — polynomial&#10;  regression is known to be highly non-robust.  Much better in practice&#10;  is to use splines rather than polynomials.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Now this prompted me to check how splines would perform in with the example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(rms)&#10;p4 &amp;lt;- poly(1:100, degree=4)&#10;true4 &amp;lt;- p4 %*% c(1,2,-6,9)&#10;days &amp;lt;- 1:70&#10;&#10;set.seed(7987)&#10;noise4 &amp;lt;- true4 + rnorm(100, sd=.5)&#10;reg.n4.4 &amp;lt;- lm(noise4[1:70] ~ poly(days, 4))&#10;reg.n4.4ns &amp;lt;- lm(noise4[1:70] ~ ns(days,4))&#10;dd &amp;lt;- datadist(noise4[1:70], days)&#10;options(&quot;datadist&quot; = &quot;dd&quot;)&#10;reg.n4.4rcs_ols &amp;lt;- ols(noise4[1:70] ~ rcs(days,5))&#10;&#10;plot(1:100, noise4)&#10;nd &amp;lt;- data.frame(days=1:100)&#10;lines(1:100, predict(reg.n4.4, newdata=nd), col=&quot;orange&quot;, lwd=3)&#10;lines(1:100, predict(reg.n4.4ns, newdata=nd), col=&quot;red&quot;, lwd=3)&#10;lines(1:100, predict(reg.n4.4rcs_ols, newdata=nd), col=&quot;darkblue&quot;, lwd=3)&#10;&#10;legend(&quot;top&quot;, fill=c(&quot;orange&quot;, &quot;red&quot;,&quot;darkblue&quot;), &#10;       legend=c(&quot;Poly&quot;, &quot;Natural splines&quot;, &quot;RCS - ols&quot;))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Gives the following image:&#10;&lt;img src=&quot;http://i.stack.imgur.com/q34Vc.png&quot; alt=&quot;A comparison of splines and polynomials&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In conclusion I have not found much that would convince me of reconsidering splines, what am I missing? &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;F. E. Harrell, Regression Modeling Strategies: With Applications to Linear Models, Logistic Regression, and Survival Analysis, Softcover reprint of hardcover 1st ed. 2001. Springer, 2010.&lt;/li&gt;&#10;&lt;li&gt;F. E. Harrell, K. L. Lee, and B. G. Pollock, “Regression Models in Clinical Studies: Determining Relationships Between Predictors and Response,” JNCI J Natl Cancer Inst, vol. 80, no. 15, pp. 1198–1202, Oct. 1988.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;h2&gt;Update&lt;/h2&gt;&#10;&#10;&lt;p&gt;The comments made me wonder what happens within the data span but with uncomfortable curves. In most of the situations I'm not going outside the data boundary, as the example above indicates. I'm not sure this qualifies as prediction... &lt;/p&gt;&#10;&#10;&lt;p&gt;Anyway here's an example where I create a more complex line that cannot be translated into a polynomial. Since most observations are in the center of the data I tried to simulate that as well:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(rms)&#10;cmplx_line &amp;lt;-  1:200/10&#10;cmplx_line &amp;lt;- cmplx_line + 0.05*(cmplx_line - quantile(cmplx_line, .7))^2&#10;cmplx_line &amp;lt;- cmplx_line - 0.06*(cmplx_line - quantile(cmplx_line, .3))^2&#10;center &amp;lt;- (length(cmplx_line)/4*2):(length(cmplx_line)/4*3)&#10;cmplx_line[center] &amp;lt;- cmplx_line[center] + &#10;    dnorm(6*(1:length(center)-length(center)/2)/length(center))*10&#10;&#10;ds &amp;lt;- data.frame(cmplx_line, x=1:200)&#10;&#10;days &amp;lt;- 1:140/2&#10;&#10;set.seed(1234)&#10;sample &amp;lt;- round(rnorm(600, mean=100, 60))&#10;sample &amp;lt;- sample[sample &amp;lt;= max(ds$x) &amp;amp; &#10;                         sample &amp;gt;= min(ds$x)]&#10;sample_ds &amp;lt;- ds[sample, ]&#10;&#10;sample_ds$noise4 &amp;lt;- sample_ds$cmplx_line + rnorm(nrow(sample_ds), sd=2)&#10;reg.n4.4 &amp;lt;- lm(noise4 ~ poly(x, 6), data=sample_ds)&#10;dd &amp;lt;- datadist(sample_ds)&#10;options(&quot;datadist&quot; = &quot;dd&quot;)&#10;reg.n4.4rcs_ols &amp;lt;- ols(noise4 ~ rcs(x, 7), data=sample_ds)&#10;AIC(reg.n4.4)&#10;&#10;plot(sample_ds$x, sample_ds$noise4, col=&quot;#AAAAAA&quot;)&#10;lines(x=ds$x, y=ds$cmplx_line, lwd=3, col=&quot;black&quot;, lty=4)&#10;&#10;nd &amp;lt;- data.frame(x=ds$x)&#10;    lines(ds$x, predict(reg.n4.4, newdata=ds), col=&quot;orange&quot;, lwd=3)&#10;lines(ds$x, predict(reg.n4.4rcs_ols, newdata=ds), col=&quot;lightblue&quot;, lwd=3)&#10;&#10;legend(&quot;bottomright&quot;, fill=c(&quot;black&quot;, &quot;orange&quot;,&quot;lightblue&quot;), &#10;       legend=c(&quot;True line&quot;, &quot;Poly&quot;, &quot;RCS - ols&quot;), inset=.05)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This gives the following plot:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ftXrr.png&quot; alt=&quot;A more complex non-polynomial line plot&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;h2&gt;Update 2&lt;/h2&gt;&#10;&#10;&lt;p&gt;Since this post I've published &lt;a href=&quot;http://informahealthcare.com/doi/full/10.3109/17453674.2014.916492&quot;&gt;an article&lt;/a&gt; that looks into non-linearity for age on a large dataset. The supplement compares different methods and I've written a blog post &lt;a href=&quot;http://gforge.se/2014/09/an-exercise-in-non-linear-modeling/&quot;&gt;about it&lt;/a&gt;. &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-02-01T09:36:38.343" FavoriteCount="12" Id="49052" LastActivityDate="2014-09-19T20:48:45.483" LastEditDate="2014-09-19T20:48:45.483" LastEditorUserId="5429" OwnerUserId="5429" PostTypeId="1" Score="25" Tags="&lt;regression&gt;&lt;splines&gt;" Title="Are splines overfitting the data?" ViewCount="2416" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have two questions:  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;First question:&lt;/strong&gt;  &lt;/p&gt;&#10;&#10;&lt;p&gt;I want to compute the weighted standard deviation with tri-cubic kernel. I am using lowess function in R to compute the weighted mean using tri-cubic kernel. But unfortunately the lowess function in R does not give the std (or I am not aware of it).&lt;br&gt;&#10;My question is, if I use&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;the mean values computed by R,   &lt;/li&gt;&#10;&lt;li&gt;compute the weight (using tri-cubic kernel) for each point with an span (alpha) of 0.01  &lt;/li&gt;&#10;&lt;li&gt;and put every thing in the formula &lt;a href=&quot;http://stats.stackexchange.com/questions/6534/how-do-i-calculate-a-weighted-standard-deviation-in-excel&quot;&gt;here&lt;/a&gt;,   &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Is the computed std correct or not? (for example, are the weights that I have computed exactly the same as the ones used by lowess to compute the mean?)&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Second question:&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;why the lowess values computed by R are so different with the ones computed in the smooth function in MATLAB?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks a lot for your help.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-01T10:39:55.160" Id="49057" LastActivityDate="2013-02-03T00:21:41.213" LastEditDate="2013-02-03T00:21:41.213" LastEditorUserId="88" OwnerUserId="2885" PostTypeId="1" Score="1" Tags="&lt;standard-deviation&gt;&lt;kernel&gt;&lt;smoothing&gt;" Title="Computing weighted standard deviation using lowess mean values" ViewCount="134" />
  <row Body="&lt;p&gt;Overfitting comes from allowing too large a class of models. This gets a bit tricky with models with continuous parameters (like splines and polynomials), but if you discretize the parameters into some number of distinct values, you'll see that increasing the number of knots/coefficients will increase the number of available models exponentially. For every dataset there is a spline and a polynomial that fits precisely, so long as you allow enough coefficients/knots. It may be that a spline with three knots overfits more than a polynomial with three coefficients, but that's hardly a fair comparison.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you have a low number of parameters, and a large dataset, you can be reasonably sure you're not overfitting. If you want to try higher numbers of parameters you can try cross validating within your test set to find the best number, or you can use a criterion like &lt;a href=&quot;http://en.wikipedia.org/wiki/Minimum_description_length&quot;&gt;Minimum Description Length&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt;: As requested in the comments, an example of how one would apply MDL. First you have to deal with the fact that your data is continuous, so it can't be represented in a finite code. For the sake of simplicity we'll segment the data space into boxes of side $\epsilon$ and instead of describing the data points, we'll describe the boxes that the data falls into. This means we lose some accuracy, but we can make $\epsilon$ arbitrarily small, so it doesn't matter much.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, the task is to describe the dataset as sucinctly as possible with the help of some polynomial. First we describe the polynomial. If it's an n-th order polynomial, we just need to store (n+1) coefficients. Again, we need to discretize these values. After that we need to store first the value $n$ in prefix-free coding (so we know when to stop reading) and then the $n+1$ parameter values. With this information a receiver of our code could restore the polynomial. Then we add the rest of the information required to store the dataset. For each datapoint we give the x-value, and then how many boxes up or down the data point lies off the polynomial. Both values we store in prefix-free coding so that short values require few bits, and we won't need delimiters between points. (You can shorten the code for the x-values by only storing the increments between values)&lt;/p&gt;&#10;&#10;&lt;p&gt;The fundamental point here is the tradeoff. If I choose a-order polynomial (like f(x) = 3.4), then the model is very simple to store, but for the y-values, I'm essentially storing the distance to the mean. More coefficients give me a better fitting polynomial (and thus shorter codes for the y values), but I have to spend more bits describing the model. The model that gives you the shortest code for your data is the best fit by the MDL criterion. &lt;/p&gt;&#10;&#10;&lt;p&gt;(Note that this is known as 'crude MDL', and there are some refinements you can make to solve various technical issues).&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-02-01T11:00:45.970" Id="49058" LastActivityDate="2013-02-08T10:16:23.570" LastEditDate="2013-02-08T10:16:23.570" LastEditorUserId="20085" OwnerUserId="20085" ParentId="49052" PostTypeId="2" Score="13" />
  
  <row Body="&lt;p&gt;Though I would have preferred to comment rather than answer, since I am no expert on non-parametric tests, I will attempt to at least help you find a suitable solution. &lt;/p&gt;&#10;&#10;&lt;p&gt;If I understand correctly, the 360 (6*30*2) data points per approach were generated not because of an interest in differences between case studies, trials or libraries (e.g., whether a certain approach works best for a particular case study), but simply to ensure that results are representative. It therefore appears to me that, since you seem to have used a complete block design, your main question (whether any of the approaches is better or worse) could perhaps be rephrased in terms of the &lt;a href=&quot;http://en.wikipedia.org/wiki/Friedman_test&quot; rel=&quot;nofollow&quot;&gt;Friedman test&lt;/a&gt;. Since post-hoc analysis for this test is available, it should then also prove possible to determine whether any approach is best (better than all the others).&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-02-01T14:17:26.590" Id="49071" LastActivityDate="2013-02-01T14:17:26.590" OwnerUserId="20273" ParentId="48514" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;What question are you trying to answer?  What do you want to learn from your data?  You should figure out what your question is, then decide on what statistics to compute in order to answer that question.&lt;/p&gt;&#10;&#10;&lt;p&gt;Computing statistics that you know how to compute, then asking what they mean is kind of like going to a doctor and saying that you found a bottle of aspirin and want to know what symtoms you should develope so that taking the aspirin will cure you.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-01T16:41:53.540" Id="49081" LastActivityDate="2013-02-01T16:41:53.540" OwnerUserId="4505" ParentId="49074" PostTypeId="2" Score="1" />
  <row AnswerCount="3" Body="&lt;p&gt;There has been some discussion about why differences exist between factor significance after an ANOVA and no significance in the pairwise comparisons, and it has been very helpful. However, there is no directions regarding how to proceed. For instance, in my case the overall P value given by a one-way ANOVA for the treatment factor was P=0.0127 but all of the treatments came out as not significantly different from each other following Tukey's LSD. Should I run a different post-hoc test in this case? Which one would you recommend?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you so much!&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-02-01T21:50:56.277" FavoriteCount="2" Id="49093" LastActivityDate="2015-02-27T10:30:51.297" OwnerUserId="20374" PostTypeId="1" Score="5" Tags="&lt;anova&gt;&lt;post-hoc&gt;&lt;tukey-hsd&gt;" Title="ANOVA results do not match post-hoc Tukey test, how to proceed?" ViewCount="3224" />
  <row Body="&lt;p&gt;It's possible that you could have no significant differences among all of the individual means with the most liberal of tests—even a planned comparison.  I was just tasking my students to develop simulated data with just this feature.&lt;/p&gt;&#10;&#10;&lt;p&gt;You do an ANOVA to test the pattern of your data.  If the test is passed then, in most situations, all you need to do is then describe that pattern.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-01T23:46:25.323" Id="49099" LastActivityDate="2013-02-04T23:52:31.120" LastEditDate="2013-02-04T23:52:31.120" LastEditorUserId="601" OwnerUserId="601" ParentId="49093" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;Variance Inflation Factor (VIF) quantifies the severity of multicollinearity in an ordinary least squares regression analysis. It provides an index that measures how much the variance (the square of the estimate's standard deviation) of an estimated regression coefficient is increased because of collinearity.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, the work of David Belsley suggests that condition indices are a better tool for this task. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-02T15:27:26.563" Id="49115" LastActivityDate="2013-03-18T09:57:34.497" LastEditDate="2013-03-18T09:57:34.497" LastEditorUserId="686" OwnerUserId="5509" PostTypeId="5" Score="0" />
  
  <row Body="&lt;p&gt;The same model is used by poker players to estimate the probability of finishing in each place in a tournament given the stack sizes. It is called the Independent Chip Model or ICM. You can download my program &lt;a href=&quot;http://www.icmexplorer.com&quot; rel=&quot;nofollow&quot;&gt;ICM Explorer&lt;/a&gt; which can calculate the finishing probabilities for up to $10$ balls/players.&lt;/p&gt;&#10;&#10;&lt;p&gt;Although there doesn't seem to be a simple expression for the probability of finishing in the $k$th place, it's actually quite easy to answer your question. The expected number of red balls you draw before the green ball is the sum of the probabilities that you draw each red ball before the green ball. That's the same as a &quot;last longer&quot; bet in a poker tournament. According to this model, you can ignore all of the other players: consider the first time that you draw the green ball or the $i$th red ball. The conditional probability that you draw the red ball of weight $r_i$ before the green ball of weight $g$ is $r_i/(r_i+g)$, so the expected number of red balls you draw before the green ball is &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\sum_i \frac{r_i}{r_i+g}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and the expected number of attempts necessary to find the green ball is $1$ more than this.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-02-02T16:54:54.240" Id="49118" LastActivityDate="2013-02-02T16:54:54.240" OwnerUserId="11981" ParentId="49114" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="49132" AnswerCount="2" Body="&lt;p&gt;I have never had problems with R crashing before.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am using the &lt;code&gt;mice&lt;/code&gt; package (mice 2.13) to perform multiple imputations. The code works fine on some subsets of the data, but when I run it on other subsets, R crashes (not immediately - after some time). From the output in R just before it crashes, I believe it is using the &lt;code&gt;2l.pan&lt;/code&gt; method of imputation (from the &lt;code&gt;pan&lt;/code&gt; package) I have run update.packages() already.&lt;/p&gt;&#10;&#10;&lt;p&gt;How can I diagnose this problem ? &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Problem signature:&#10;  Problem Event Name:   APPCRASH&#10;  Application Name: Rgui.exe&#10;  Application Version:  2.151.59607.0&#10;  Application Timestamp:    4fe47a63&#10;  Fault Module Name:    R.dll&#10;  Fault Module Version: 2.151.59607.0&#10;  Fault Module Timestamp:   4fe47a4e&#10;  Exception Code:   c0000005&#10;  Exception Offset: 0000000000032ec8&#10;  OS Version:   6.1.7601.2.1.0.256.4&#10;  Locale ID:    2057&#10;  Additional Information 1: 7782&#10;  Additional Information 2: 77823beb5887f451c3dd7ae4fe931995&#10;  Additional Information 3: 4491&#10;  Additional Information 4: 4491b41bf90894717964f5eef2cccd84&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I have managed to create a reproducible example, with data:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;require(foreign)&#10;require(mice)&#10;require(pan)&#10;&#10;dt.fail &amp;lt;- read.csv(&quot;http://goo.gl/pg8um&quot;)&#10;dt.fail$X &amp;lt;- NULL&#10;&#10;dt.fail$out &amp;lt;- as.factor(dt.fail$out )&#10;dt.fail$grp&amp;lt;- as.factor(dt.fail$grp)&#10;dt.fail$v1&amp;lt;- as.factor(dt.fail$v1)&#10;dt.fail$v2&amp;lt;- as.factor(dt.fail$v2)&#10;dt.fail$v3 &amp;lt;- as.factor(dt.fail$v3)&#10;dt.fail$v7&amp;lt;- as.factor(dt.fail$v7)&#10;dt.fail$v8 &amp;lt;- as.factor(dt.fail$v8)&#10;dt.fail$v9 &amp;lt;- as.factor(dt.fail$v9)&#10;dt.fail$v11 &amp;lt;- as.factor(dt.fail$v11)&#10;dt.fail$v12 &amp;lt;- as.factor(dt.fail$v12)&#10;&#10;&#10;PredMatrix &amp;lt;- quickpred(dt.fail)&#10;PredMatrix['CTP',] &amp;lt;- c(1,-2,0,0,0,0,0,0,0,0,1,0,1,1,0,2)&#10;&#10;&#10;&#10;impute = mice(&#10;data=dt.fail, &#10;    m = 1, &#10;    maxit = 1,&#10;    imputationMethod = c(&#10;    &quot;logreg&quot;,   # out&#10;    &quot;&quot;,      # grp   ----&amp;gt; cluster grouping factor&#10;    &quot;pmm&quot;,  # v1&#10;    &quot;polyreg&quot;,  # v2&#10;    &quot;logreg&quot;,   # v3&#10;    &quot;pmm&quot;,  # v4&#10;    &quot;logreg&quot;,   # v5&#10;    &quot;logreg&quot;,   # v6&#10;    &quot;polyreg&quot;,  # v7 ----&amp;gt; auxilliary&#10;    &quot;polyreg&quot;,  # v8 ----&amp;gt; auxilliary&#10;    &quot;polyreg&quot;,  # v9 ----&amp;gt; auxilliary&#10;    &quot;polyreg&quot;,  # v10 ----&amp;gt; auxilliary&#10;    &quot;&quot;,     # v11 ----&amp;gt; complete&#10;    &quot;&quot;,     # v12 ----&amp;gt; complete&#10;    &quot;2l.pan&quot;,   # CTP ----&amp;gt; multilevel imputation&#10;    &quot;&quot;),        # const ----&amp;gt; needed for multilevel impuitation&#10;&#10;predictorMatrix = PredMatrix, seed = 101&#10;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And for completeness, here is the predictor matrix I was using:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    .     out grp v1 v2 v3 v4 v5 v6 v7 v8 v9 v10 v11 v12 CTP const&#10;out     0   0  0  0  0  0  1  1  0  0  0   0   0   0   0     0&#10;grp     0   0  0  0  0  0  0  0  0  0  0   0   0   1   0     0&#10;v1      0   0  0  0  0  0  0  0  0  0  0   0   1   1   0     0&#10;v2      0   0  0  0  0  1  1  1  0  1  0   0   1   1   1     0&#10;v3      0   0  0  0  0  1  1  1  0  1  1   0   1   1   1     0&#10;v4      0   0  0  1  1  0  1  1  0  1  1   0   1   1   1     0&#10;v5      1   1  0  0  0  0  0  1  0  1  0   0   1   0   0     0&#10;v6      1   1  0  1  0  1  1  0  0  1  0   0   1   0   0     0&#10;v7      0   0  0  0  0  0  1  1  0  1  0   0   0   1   0     0&#10;v8      0   0  0  0  0  0  1  1  0  0  0   0   1   1   0     0&#10;v9      0   0  0  0  1  1  1  1  0  1  0   0   1   1   1     0&#10;v10     0   0  0  0  0  0  1  1  0  1  0   0   1   1   0     0&#10;v11     0   0  0  0  0  0  0  0  0  0  0   0   0   0   0     0&#10;v12     0   0  0  0  0  0  0  0  0  0  0   0   0   0   0     0&#10;CTP     1  -2  0  0  0  0  0  0  0  0  1   0   1   1   0     2&#10;const   0   0  0  0  0  0  0  0  0  0  0   0   0   0   0     0&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="6" CreationDate="2013-02-01T14:30:23.027" Id="49119" LastActivityDate="2013-04-10T12:28:35.007" LastEditDate="2013-04-10T12:28:35.007" LastEditorUserId="7486" OwnerDisplayName="Robert Long" OwnerUserId="7486" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;missing-data&gt;&lt;multiple-imputation&gt;&lt;mice&gt;" Title="Diagnosing why MICE is crashing R when attempting to impute multilevel data" ViewCount="222" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a large number of predictors that are hypothesized to be important in determining my binary outcome variable (&lt;a href=&quot;http://stats.stackexchange.com/q/43815/5264&quot;&gt;here's&lt;/a&gt; a bit more about my goal, the predictors and the outcome).  The problem is that considering the number of &quot;positive cases&quot; that I have, I have too many predictors (I have about 450 cases in total, about 100 positive cases, and more than 20 predictors.) &lt;/p&gt;&#10;&#10;&lt;p&gt;I'd like to use PCA to reduce my data, but many of my predictors are non-negative with a spike at zero and a long tail. For instance, the value for &quot;the number of cigarettes per day smoked in the past 30 days&quot; is zero for about 75% of the respondents and 99+ for a few. &lt;/p&gt;&#10;&#10;&lt;p&gt;I understand that there's nothing conceptually wrong with using PCA with such variables. But PCA finds the directions of maximum variance in data and I'm not sure that variance is the right measure of dispersion for this kind of data. (These variables also cause all sorts of errors and warnings to be generated when I use other data reduction software.) &lt;/p&gt;&#10;&#10;&lt;p&gt;I'd like to find a non-response-variable-driven way to deal with these variables. What I've tried already is to use $x \rightarrow log(x + 1)$ transformation followed by restricted cubic spline transforms (as implemented by $transcan$ function in Hmisc package in R), but they don't improve the situation much.  I've seen &lt;a href=&quot;http://onlinelibrary.wiley.com/doi/10.1002/sim.3864/abstract;jsessionid=8667AFD156D1359B5454B666A79C955E.d01t01&quot; rel=&quot;nofollow&quot;&gt;this paper&lt;/a&gt; by Royston et al. that explicitly deals with this problem, but their approach is response-variable driven, which is something I'd like to avoid.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'd love to hear everyone's thoughts, experiences, and suggestions. &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!   &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-02T20:24:56.097" Id="49134" LastActivityDate="2013-02-02T20:24:56.097" OwnerUserId="5264" PostTypeId="1" Score="1" Tags="&lt;pca&gt;&lt;data-transformation&gt;" Title="PCA with &quot;zero-spike&quot; variables" ViewCount="56" />
  <row Body="&lt;p&gt;You could bin the data into evenly spaced intervals, then compute the unbiased sample variances of subsets $\{X_{n+1}:X_n=x_1,X_{n-k}=x_2\}$. By the law of total variance, $$\mathrm{Var}[E(X_{n+1}|X_n,X_{n-k})|X_n] = \mathrm{Var}[X_{n+1}|X_n]-E(\mathrm{Var}[X_{n+1}|X_n])$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The LHS, if it is almost zero, provides evidence that the transition probabilities do not depend on $X_{n-k}$, though it is clearly a weaker statement: e.g., let $X_{n+1}\sim N(X_n,X_{n-1})$. Taking the expected value of both sides of the above equation, the RHS can be computed from the sample variances (i.e., replacing expected values with averages). If the expected value of the variance is zero then the variance is 0 almost always.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-03T04:14:09.880" Id="49154" LastActivityDate="2013-09-09T10:58:54.793" LastEditDate="2013-09-09T10:58:54.793" LastEditorUserId="27581" OwnerUserId="20414" ParentId="37386" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;The denominator can be written as: &lt;/p&gt;&#10;&#10;&lt;p&gt;$P(X=x)=\sum_{C}P(X=x|C=c_i)P(C=c_i)$ &lt;/p&gt;&#10;&#10;&lt;p&gt;and the sum is over all possible values of $C$. $P(X=x)$, as you can see, does not depend on $C$. With fixed $X$ (&lt;em&gt;i.e.&lt;/em&gt; with your fixed data) hence can be considered as a constant. &lt;/p&gt;&#10;&#10;&lt;p&gt;Thus $P(C=c_i|X=x) \propto P(X=x|C=c_i)P(C=c_i)$. &lt;/p&gt;&#10;&#10;&lt;p&gt;The right side of the above equation will be called the &lt;em&gt;&quot;unnormalized posterior density&quot;.&lt;/em&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-03T10:55:57.410" Id="49158" LastActivityDate="2013-02-03T10:55:57.410" OwnerUserId="12603" ParentId="49156" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;&quot;The Central Limit Theorem is about the journey and the Strong Law of Large Numbers is about the destination.&quot;  stats.SE user &lt;em&gt;cardinal&lt;/em&gt; in a comment on &lt;a href=&quot;http://stats.stackexchange.com/q/49123/6633&quot;&gt;this question&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2013-02-03T14:44:05.673" CreationDate="2013-02-03T14:44:05.673" Id="49164" LastActivityDate="2013-02-03T14:44:05.673" OwnerUserId="6633" ParentId="726" PostTypeId="2" Score="8" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;In Monte Carlo simulation, we often consider how well a sequence of generated points are. If I am correct, one aspect is &lt;a href=&quot;http://en.wikipedia.org/wiki/Statistical_randomness&quot; rel=&quot;nofollow&quot;&gt;statistical randomness&lt;/a&gt;:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;A numeric sequence is said to be statistically random when it contains no recognizable patterns or regularities; sequences such as the results of an ideal dice roll, or the digits of π exhibit statistical randomness.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I was wondering if a sequence is statistically random, then does it mean that &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;when the points in the sequence are viewed as realizations of a random variable, the random variable has a uniform distribution?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;when different points in the sequence are viewed as realizations of different random variables, the random variables are independent or even i.i.d.?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Does statistical randomness have anything to do with uniform distribution and independence?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks and regards!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-03T18:05:34.860" Id="49178" LastActivityDate="2013-02-03T18:05:34.860" OwnerUserId="1005" PostTypeId="1" Score="3" Tags="&lt;monte-carlo&gt;&lt;randomness&gt;" Title="Relation between statistical randomness, uniform distribution and independence" ViewCount="57" />
  
  <row AnswerCount="1" Body="&lt;p&gt;How to show what is going on if we drop significant variable in logit model ? Bias and heteroskedasticity should emerge. But what is the framework for showing such behaviours in econometric models ?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-03T19:27:26.793" Id="49187" LastActivityDate="2013-02-04T20:32:46.080" OwnerUserId="4908" PostTypeId="1" Score="-1" Tags="&lt;regression&gt;&lt;probability&gt;&lt;estimation&gt;&lt;logit&gt;" Title="How to show what is going on if we drop significiant variable in logit model?" ViewCount="111" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am having trouble with a series of system tests in which each test is assumed to be a normal approximations of binomial distribution. Each test sample, n, is a Bernoulli (&quot;success&quot;, &quot;failure&quot;) trial of the system.  Each system test has approximately 550 IID observations.  Each has a lifetime that may vary between 2-8 seconds, regardless of success or failure.  During this lifetime a success or failure will occur.  Therefore, for each system test an estimated probability of failure, $p$, is determined where $p = \rm{Pr}[X=0]$. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, since the test data in which the estimated probability of failure was determined is available, I wish to develop a CDF, F(t), that provides the probability of failure per unit time (0.1 seconds). More specifically, I wish to develop a graph to show when the failure occurred.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I have a few questions that I am having problems with:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;I would like to develop the CDF's, but I am not sure whether the lifetime differences of each sample varying from 2-8 seconds may bias the CDF curve.  This curve will provide the cumulative estimated probability of failure in 0.1 second increments.  &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Do the number of failures within each time interval affect the significance of the estimated probability of failure within that particular time interval?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;In addition I would like to develop two corresponding curves that illustrate the estimated probability of failure using a 95% confidence level.  How do I do this, and what assumptions of the distribution should I consider?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I realize that this may be a trivial problem, but I have doubt about some assumptions.  Since my work has recently highly relied on these CDF responses to make important decisions, I would like to gain more confidence on how to complete these distributions and provide the resources/references that support these statistical conclusions.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-04T06:17:22.610" Id="49213" LastActivityDate="2013-02-04T23:36:32.550" LastEditDate="2013-02-04T23:36:32.550" LastEditorUserId="805" OwnerUserId="20436" PostTypeId="1" Score="1" Tags="&lt;confidence-interval&gt;&lt;dataset&gt;&lt;binomial&gt;&lt;cdf&gt;" Title="CDF and confidence interval on non-parametric binomial data" ViewCount="100" />
  
  <row Body="&lt;p&gt;The &lt;a href=&quot;http://en.wikipedia.org/wiki/Normal_distribution&quot; rel=&quot;nofollow&quot;&gt;Wikipedia article&lt;/a&gt; on the normal distribution contains many reasons. I'll summarize a few of the more useful ones here - but I really do suggest having a read through that article:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;It is entirely characterized by two parameters that are easy to estimate&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;A sum of two jointly normal random variables is also normal&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Uncorrelated, jointly distributed, normal random variables are independent&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Normality assumptions frequently result in analytic (as opposed to numeric) solutions to many estimation problems&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;To this Wikipedia-based list I'll add one more:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Estimators are often consistent when normality is assumed, &lt;em&gt;even if the normality assumption is violated&lt;/em&gt;, see eg &lt;a href=&quot;http://en.wikipedia.org/wiki/Quasi-maximum_likelihood&quot; rel=&quot;nofollow&quot;&gt;quasi-maximum likelihood&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="2" CreationDate="2013-02-04T06:22:50.557" Id="49214" LastActivityDate="2013-02-04T22:19:32.227" LastEditDate="2013-02-04T22:19:32.227" LastEditorUserId="16319" OwnerUserId="16319" ParentId="49212" PostTypeId="2" Score="6" />
  
  <row Body="&lt;p&gt;Short answer: The number of iterations incorporates the burn in and does not incorporate thinning.&lt;/p&gt;&#10;&#10;&lt;p&gt;Less short answer: If you were to run a BUGS model through R2WinBUGS or R2OpenBUGS (or view a summary of WinBUGS output) with the arguments you stated:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; n.iter=5000, n.burnin=5000, n.thin=2&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;you would get an error message/no output. &lt;code&gt;n.iter&lt;/code&gt; refers to the total number of iterations including the burn in, hence all your iterations are burn in and are thrown away (or not included in the CODA output and any ACF plot in WinBUGS). &lt;/p&gt;&#10;&#10;&lt;p&gt;Thinning is treated differently (in relation to &lt;code&gt;n.iter&lt;/code&gt;). For example if you set your MCMC up with any of the following arguments:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; n.iter=6000, n.burnin=5000, n.thin=1&#10; n.iter=6000, n.burnin=5000, n.thin=5&#10; n.iter=6000, n.burnin=5000, n.thin=10&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;only 1000 iterations will be saved, i.e. all non-thinned simulations are discarded (in CODA output or any ACF plot in WinBUGS).&lt;/p&gt;&#10;&#10;&lt;p&gt;Not sure if this is the same for jags?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-04T12:24:09.833" Id="49230" LastActivityDate="2013-11-09T11:27:00.397" LastEditDate="2013-11-09T11:27:00.397" LastEditorUserId="13267" OwnerUserId="13267" ParentId="49209" PostTypeId="2" Score="2" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;What should one do, if the parameters for a logistic regression model are significant as well as the  model itself, but the test for the goodness of fit show that the model is bad (i.e. the P-values of deviance etc. are low)? &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-02-04T19:26:43.707" Id="49251" LastActivityDate="2013-02-05T00:05:34.513" LastEditDate="2013-02-04T19:31:39.760" LastEditorUserId="88" OwnerUserId="20455" PostTypeId="1" Score="1" Tags="&lt;logistic&gt;" Title="Significant logistic regression model but poor goodness of fit" ViewCount="607" />
  <row AcceptedAnswerId="49376" AnswerCount="2" Body="&lt;p&gt;I have two questions on using random forest (specifically randomForest in R) for missing value imputation (in the predictor space). &lt;/p&gt;&#10;&#10;&lt;p&gt;1) How does the imputation algorithm work - specifically how and why is the class label required for imputation? is the proximity matrix which serves to weight the average value to impute a missing value defined separately by class?&lt;/p&gt;&#10;&#10;&lt;p&gt;2) If the class label is needed to impute missing values - how can this be used to impute missing values for new data that you are trying to predict?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-05T01:23:36.110" FavoriteCount="2" Id="49270" LastActivityDate="2014-01-15T21:18:45.087" LastEditDate="2013-02-06T04:13:24.050" LastEditorUserId="812" OwnerUserId="2040" PostTypeId="1" Score="4" Tags="&lt;data-mining&gt;&lt;predictive-models&gt;&lt;missing-data&gt;&lt;random-forest&gt;&lt;data-imputation&gt;" Title="Imputation with Random Forests" ViewCount="1589" />
  <row Body="&lt;p&gt;I make an analogy: Solving statistical problems without context is like boxing while blindfolded. You might knock your opponent out but you might bash your hand on the ringpost.&lt;/p&gt;&#10;&#10;&lt;p&gt;I work mostly with medical and social science researchers. There seems to be a widespread feeling there that the proper model for research is &lt;/p&gt;&#10;&#10;&lt;p&gt;1) &lt;strong&gt;They&lt;/strong&gt; come up with an idea, gather data, write about it and &lt;strong&gt;then&lt;/strong&gt;&#10;2) They give it to &lt;strong&gt;us&lt;/strong&gt; to &quot;do the statistics&quot;. &lt;/p&gt;&#10;&#10;&lt;p&gt;So, I agree that we need to understand the issues; of course, we don't need as full an understanding of the research as the practitioner has. That is why I (and many other data-people) can work with people in different profession. But, the less we know about a subject, the more we need to interact with the professional to make sure that the results make sense.&lt;/p&gt;&#10;&#10;&lt;p&gt;One of the many things I like about what I do is that I get to learn a bit about a lot of different subjects. &lt;/p&gt;&#10;" CommentCount="1" CommunityOwnedDate="2013-02-05T12:31:10.620" CreationDate="2013-02-05T11:35:14.137" Id="49288" LastActivityDate="2013-02-05T11:35:14.137" OwnerUserId="686" ParentId="49287" PostTypeId="2" Score="6" />
  
  <row Body="&lt;p&gt;As others pointed out, you can't go further without any model assumption. Just assume the closest model based on your insight from the data and proceed with a formal statistical inference procedure, for example, possibly, the method of moments in your problem. Remember, all models are wrong, some are useful (George Box).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-05T17:02:14.923" Id="49319" LastActivityDate="2013-02-05T17:02:14.923" OwnerUserId="20492" ParentId="49309" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;What the other answers already suggest is to compare mean and variance.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;If mean equals variance then it could be Poisson&lt;/li&gt;&#10;&lt;li&gt;If mean is less than veriaance then it could be negative binomial&lt;/li&gt;&#10;&lt;li&gt;If mean is greater than variance then it could be binomial&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;And then there are the inflated and truncated families. So you have to know whether 0 has a special role and whether the distribution has finite support (like Binomial) or infinite (like Poisson and negative binomial). &lt;/p&gt;&#10;&#10;&lt;p&gt;In fact just knowing mean and variance is not much more than&#10;guessing. Do you know anything of the generation of the data (like number of car accidents or number school drop-outs)?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-02-05T17:09:35.410" Id="49321" LastActivityDate="2013-02-06T08:58:35.943" LastEditDate="2013-02-06T08:58:35.943" LastEditorUserId="12147" OwnerUserId="12147" ParentId="49309" PostTypeId="2" Score="3" />
  <row AnswerCount="0" Body="&lt;p&gt;&lt;strong&gt;Is there a formula that is able to determine whether the difference between two co-efficients of variation is significant or if not is there a specific heuristic used to determine significance (i.e. cv&amp;lt;0.05)?&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;I am curious of this because: I am trying to compare a data set with widely different means.  I want to see if the difference between co-efficient of variance between the two sets is significant in order to determine the importance of the behavior of a certain factor.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have already consulted this forum (as well as &lt;a href=&quot;http://listserv.uga.edu/cgi-bin/wa?A2=ind1212a&amp;amp;L=sas-l&amp;amp;H=1&amp;amp;P=1084/%22this%22&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;) but I have not found a clear answer.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any suggestions?&#10;Thank you in advance for your time.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-02-05T17:24:06.770" Id="49324" LastActivityDate="2013-02-06T12:37:28.983" LastEditDate="2013-02-06T12:37:28.983" LastEditorUserId="20494" OwnerUserId="20494" PostTypeId="1" Score="0" Tags="&lt;statistical-significance&gt;&lt;coefficient-of-variation&gt;" Title="Significance of Co-efficient of Variation" ViewCount="111" />
  
  
  <row Body="&lt;p&gt;Assuming the molecules are indistinguishable (a fairly standard assumption from any quantum treatment) then the set of states is simply the set of integers $\left\lbrace 0,1,\dots,N \right\rbrace$, representing the number of molecules in container A. Because at each epoch a single molecule must make a transition through the aperture, it follows that the transition probability function can be derived from the odds of choosing one molecule at random, whose only distinguishable feature is the container it is from; thus&#10;$$&#10;p\left(\left. X_{n+1} = k \right| X_n = m \right) = \left\lbrace &#10;\begin{array}{11}&#10;1 &amp;amp; m = N \text{ and } k = N - 1 \\&#10;1 &amp;amp; m = 0 \text{ and } k = 1 \\&#10;\frac{N - m}{N} &amp;amp; 0 &amp;lt; m &amp;lt; N \text{ and } k = m + 1 \\&#10;\frac{m}{N} &amp;amp; 0 &amp;lt; m &amp;lt; N \text{ and } k = m - 1 \\&#10;0 &amp;amp; \text{otherwise}&#10;\end{array}&#10;\right.&#10;$$&#10;This becomes more apparent by noting that $\frac{m}{N}$ is the probability of choosing a molecule from container A, and $\frac{N-m}{N}$ is the converse probability of choosing a molecule from container B &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-05T20:29:04.207" Id="49349" LastActivityDate="2013-02-05T20:29:04.207" OwnerUserId="20047" ParentId="48129" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;I have categorical data on Police Stations against Crime categories on a 2-way contingency table. I have an asymmetric row map and column map from correspondence analysis using XLSTAT. Which one of the two maps should be used to interpret the results?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-05T21:02:35.080" Id="49350" LastActivityDate="2013-04-06T23:55:46.477" OwnerUserId="20507" PostTypeId="1" Score="3" Tags="&lt;correspondence-analysis&gt;" Title="How to choose asymmetric biplots in correspondence analysis" ViewCount="141" />
  <row Body="&lt;p&gt;Whether or not it makes sense to use GLMs depends on the distribution of $y$. I'd be inclined to use a nonlinear least squares model for the whole thing. &lt;/p&gt;&#10;&#10;&lt;p&gt;So if your regression model is $a = Z\alpha+\nu$ where $Z$ are the predictors and $\alpha$ are the parameters in the regression model for $a$, and your model for $b$ is $b = f(x)+\epsilon$ but where $f(x)$ is restricted to be non-negative, you could write $f(x) = \exp(\psi(x))$ and fit a model like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;y = Z\alpha+\exp(\psi(x))+\eta&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\eta$ is the sum of the two individual noise terms. (If you &lt;em&gt;really&lt;/em&gt; intend that $y=a+b$ with no error at all, you have to do it differently; that's not really a stats problem as much as an approximation problem and you would probably want to look at infinity-norms then.)&lt;/p&gt;&#10;&#10;&lt;p&gt;If you put say a cubic regression spline in for $\psi$ that would be one easy way of getting some general smooth function in. That model could be fitted by nonlinear least squares. (Indeed, some algorithms can take advantage of the linearity of $a$ to simplify and speed up calculation.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Depending on what you assume about $y$ or $f$, there are other things you might do instead.&lt;/p&gt;&#10;&#10;&lt;p&gt;That doesn't really address the imputation issue yet. However, this sort of model framework can be inserted into something like your suggestion of using EM.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-02-05T21:24:49.953" Id="49353" LastActivityDate="2014-03-10T01:17:54.760" LastEditDate="2014-03-10T01:17:54.760" LastEditorUserId="805" OwnerUserId="805" ParentId="49337" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;You need to change &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;model &amp;lt;- lm(x ~ y)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;to&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;model &amp;lt;- lm(y ~ x).&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Currently your fitline is showing x as a function of y instead of verse-vica.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-06T01:14:16.540" Id="49368" LastActivityDate="2013-02-06T01:14:16.540" OwnerUserId="2669" ParentId="49367" PostTypeId="2" Score="8" />
  
  
  <row AcceptedAnswerId="50994" AnswerCount="1" Body="&lt;p&gt;I am not a statistician so apologies if my terminology is wrong&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a dataset of &gt;14 million p values derived from Fisher's exact testing on genome scale sequencing data.&lt;/p&gt;&#10;&#10;&lt;p&gt;Benjamini-Hochberg correction of these p-values turns out only ~1500 significant p-values so I have been attempting to use qvalue R package to relax the FDR and therefore gain a greater number of significant p-values.&lt;/p&gt;&#10;&#10;&lt;p&gt;Applying the qvalue package with default options returns a $\pi_0$ of 1&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;qobj&amp;lt;-qvalue(p.fish.unadj)&#10;qsummary(qobj)&#10;pi0:    1&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I presume this is a &quot;good thing&quot; and means that:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;I have good power in my dataset to detect &lt;em&gt;true null&lt;/em&gt; results and therefore &lt;em&gt;true alternative&lt;/em&gt; results also&lt;/li&gt;&#10;&lt;li&gt;$\pi_0=1$ is an approximation because my true alternatives i.e. significant tests is such a small number compared to the overall size of the dataset&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Am I right?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-02-06T11:29:42.380" Id="49391" LastActivityDate="2013-02-27T22:43:01.177" LastEditDate="2013-02-06T11:47:00.170" LastEditorUserId="686" OwnerUserId="20534" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;multiple-comparisons&gt;&lt;fdr&gt;" Title="True meaning of $\pi_0 = 1$ in false discovery rate" ViewCount="219" />
  <row Body="&lt;p&gt;The Jeffreys' minimum informative prior for this case is a Beta(1/2,1/2) (it has a U shape in [0,1], not flat). &lt;/p&gt;&#10;&#10;&lt;p&gt;The posterior distribution for p* is a Beta(1/2+a,1/2+b)&lt;/p&gt;&#10;&#10;&lt;p&gt;You can search in the wikipedia for:  &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Prior_probability&quot; rel=&quot;nofollow&quot;&gt;Prior_probability&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Beta_distribution&quot; rel=&quot;nofollow&quot;&gt;Beta_distribution&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Jeffreys_prior&quot; rel=&quot;nofollow&quot;&gt;Jeffreys_prior&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2013-02-06T12:32:43.967" Id="49393" LastActivityDate="2013-02-06T13:21:10.983" LastEditDate="2013-02-06T13:21:10.983" LastEditorUserId="7290" OwnerUserId="20538" ParentId="48910" PostTypeId="2" Score="0" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Are there any other distributions with similar properties to the family of &lt;a href=&quot;http://en.wikipedia.org/wiki/Stable_distributions&quot; rel=&quot;nofollow&quot;&gt;stable distributions&lt;/a&gt;? That is, $\alpha$-stable, normal tempered stable, classical tempered stable, etc. etc. where the distribution could exhibit the following properties: Heavy tailed, leptokurtotic, symmetric or asymmetric but can still retain some of the Gaussian properties? &lt;/p&gt;&#10;&#10;&lt;p&gt;For instance setting $\alpha = 2$ in any of the stable distributions will result in a normal distribution with no leptokurtotic or heavy tailed properties.&lt;/p&gt;&#10;&#10;&lt;p&gt;I know of the &lt;a href=&quot;http://en.wikipedia.org/wiki/Generalised_hyperbolic_distribution&quot; rel=&quot;nofollow&quot;&gt;generalized hyperbolic distribution&lt;/a&gt; family which is similar in features and also,  for a few special cases when the degrees of freedom are very high decays to the gaussian.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-02-06T16:42:58.697" Id="49413" LastActivityDate="2013-03-27T04:04:33.313" LastEditDate="2013-03-27T04:04:33.313" LastEditorUserId="9662" OwnerUserId="9662" PostTypeId="1" Score="2" Tags="&lt;probability&gt;&lt;distributions&gt;" Title="Distributions similar to the family of stable distributions" ViewCount="82" />
  
  <row Body="&lt;p&gt;Tools like the binomial assume equal probabilities because that greatly simplifys things, so you cannot simplify.  If you need an exact answer then you will need to look at every possibly way that your event (at least X events occure), calculate the probability of each possibility then sum those up.  There are tools that will generate every possible combination for you so that you can sum them up (the &lt;code&gt;combn&lt;/code&gt; function in R is one) so this is doable.&lt;/p&gt;&#10;&#10;&lt;p&gt;If the set of probabilities is fixed, so you don't need to recompute every time and can just precompute the chances, and you can live with a good approximation then I would suggest simulation.  Randomly generate an outcome with the given set of probabilities, repeat this a bunch of times (10,000 or even 100,000) and then just calculate how often you had X or more events.  This would take only a few lines in R (and probably be similar in other tools as well).&lt;/p&gt;&#10;&#10;&lt;p&gt;There is a similar question &lt;a href=&quot;http://stats.stackexchange.com/questions/49408/probability-distribution-for-varying-probabilities-in-r/49440#49440&quot;&gt;here&lt;/a&gt;, see my example there (using R) for the more exact calculation.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-02-06T20:30:29.393" Id="49435" LastActivityDate="2013-02-06T20:51:34.533" LastEditDate="2013-02-06T20:51:34.533" LastEditorUserId="4505" OwnerUserId="4505" ParentId="49431" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="49827" AnswerCount="2" Body="&lt;p&gt;I hope I am able to word my question clearly. Suppose I have a model below:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;glmer(Y~X + (1|subject), family=&quot;binomial&quot;, data=dat)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The intercept is the log odds of success for the reference level (level 1 of X), and the slopes indicate the log odds ratio of the other levels to the reference level. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;            Estimate Std. Error z value Pr(&amp;gt;|z|)    &#10;(Intercept)   2.1745     0.3517   6.183  6.3e-10 ***&#10;         X2  -0.5559     0.3276  -1.697   0.0897 .  &#10;         X3   0.2309     0.3634   0.635   0.5252    &#10;         X4  -4.8587     0.4155 -11.693  &amp;lt; 2e-16 ***&#10;         X5  -2.8946     0.3200  -9.045  &amp;lt; 2e-16 ***&#10;         X6  -3.6111     0.3387 -10.663  &amp;lt; 2e-16 ***&#10;         ---&#10;         Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This is all fine. However, what kinds of statistical tests can I use if in addition to this, I would also like to check whether each level (including the reference level)'s odds of success are significantly higher than 0? How would I be able to do this?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-06T20:49:15.803" Id="49439" LastActivityDate="2013-02-12T17:10:10.520" LastEditDate="2013-02-06T21:40:21.460" LastEditorUserId="7290" OwnerUserId="18615" PostTypeId="1" Score="2" Tags="&lt;logistic&gt;&lt;odds-ratio&gt;&lt;glmm&gt;&lt;glmer&gt;&lt;odds&gt;" Title="How to compare whether the odds of success of different levels of a predictor are different from 0" ViewCount="185" />
  
  
  <row Body="&lt;p&gt;Are the values always between 0 and 1?&lt;/p&gt;&#10;&#10;&lt;p&gt;If so you might consider a beta distribution and beta regression.&lt;/p&gt;&#10;&#10;&lt;p&gt;But make sure to think through the process that leads to your data.  You could also do a 0 and 1 inflated model (0 inflated models are common, you would probably need to extend to 1 inflated by your self).  The big difference is if those spikes represent large numbers of exact 0's and 1's or just values close to 0 and 1.&lt;/p&gt;&#10;&#10;&lt;p&gt;It may be best to consult with a local statistician (whith a non-disclosure agreement so that you can discuss the details of where the data come from) to work out the best approach.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-02-06T21:36:21.820" Id="49446" LastActivityDate="2013-02-06T21:36:21.820" OwnerUserId="4505" ParentId="49443" PostTypeId="2" Score="7" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Is the Kruskal-Wallis test valid when applied to a subset of data? Is there a more appropriate test for comparing the effects of two variables on non-parametric, incomplete data with different variances?&lt;/p&gt;&#10;&#10;&lt;p&gt;Super simple version&#10;I analyse my data using method A.&#10;A has 6 values.&#10;I have two variables L and P&#10;L has 3 values: L1, L2, L3.&#10;P has 4 values, P1-P4.&#10;I have data for 9 combinations L1P1, L1P2, L1P3, L1P4, L2P1-4 and L3P1.&#10;I have 10 samples for each combination.&#10;I am interested in the effect of P on L for a given value of L.  (is L1P1 different to L1P2-4? and is L2P1 different to L2P2-4?)&#10;I also want to compare L1-3.&#10;The distribution of values is non-normal and the variance differs between combinations.&#10;I then compare the results of A1-A6 (This is likely to be a qualitative assessment so I don't think I need a statistical test here.)&#10;My hypothesis is that A correlates quantitatively with the experimental data and predicts the correct value of P, furthermore the value of P will be constant for L1 and L2.&lt;/p&gt;&#10;&#10;&lt;p&gt;More detailed version. Including where I have got to with my understanding of stats.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have 4 proteins. (Not really, it is the same protein in different conformations but for the sake of clarity and to avoid jargon I have treated them as different in the explanation below)&#10;I have 3 ligands/drugs.&#10;I have 9 initial structures. 2 ligands combined with each protein + final ligand in one protein.&#10;I have conducted 90 simulations. 10 of each initial structure. Each simulation is stochastic/determinist/reproducible to binary accuracy (not random) consisting of 476 data points. But the initial conditions were randomised (initial velocity of each atom using a  seed value to randomise), therefore each run is reproducible within itself but different compared to other runs of the same initial structure.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have biological data (inhibition values) for my compounds. I want to know how well my simulations predict that data. There are several parameters that I have measured in each simulation. I want to compare how well those parameters predict the experimental data.&#10;One flaw in my experimental data is that it cannot distinguish between the proteins for the first two drugs.&#10;My simulations were trying to predict which protein is preferred.&#10;My parameters are of two types: single points and averages. Some of the parameters are single measurements taken during the simulation, lowest energy is one example. Other parameters are averages of a value throughout the simulation, e.g. average energy.&lt;/p&gt;&#10;&#10;&lt;p&gt;Ideally I believe I would use a  2-way anova to conduct the analysis. I have three problems with this approach. Firstly, the variance of the ten simulations of each initial structure differs. This differing variance is seen for all parameters I have measured (Bartlett's test for equal variances).&#10;Secondly, I have no reason to believe these should be normally distributed measurements, this is borne out by various normality tests (D'Agostino &amp;amp; Pearson omnibus normality test) I have used on the data; (the average value parameters are closer to a normal distribution than the single point parameters)&#10;Finally the software I am using (Graphpad) doesn’t like the fact I am missing data. It needs/wants the other three proteins for the final ligand.&lt;/p&gt;&#10;&#10;&lt;p&gt;For the first two reasons I cannot use a 1-way anova.&#10;So I have been using a Kruskal-Wallis test, however I have concerns with how I have been applying it. I have tabulated my data in 9 columns x10 rows and have conducted the test on everything.&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to test each of the drugs alone.  (each drug against its four proteins to determine the preferred protein)&#10;Then I want to compare drugs. (each drug in its preferred protein against the other drugs in their preferred protein)&lt;/p&gt;&#10;&#10;&lt;p&gt;My concern is that if I do a pairwise comparison (Dunn's Multiple Comparison Test)  for the first step, within the test spanning all initial combinations then the variance of the other drug and its proteins are affecting the results. If I perform the Kruskal-Wallis comparison on each drug separately then compare the 3 best I think I might be doing the equivalent of a multiple T-test and hence type 1 errors.&lt;/p&gt;&#10;&#10;&lt;p&gt;My co-supervisor has suggested that Bayesian might be able to answer these questions, but I have no experience at all with the technique and based on my reading I have no idea how to form the priori.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-07T01:23:16.383" Id="49464" LastActivityDate="2013-02-10T22:46:34.297" LastEditDate="2013-02-10T22:46:34.297" LastEditorUserId="20575" OwnerUserId="20575" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;&lt;nonparametric&gt;" Title="Appropriate analysis on non normal data with different variance" ViewCount="289" />
  <row AnswerCount="1" Body="&lt;p&gt;I have some data I need to analyze, and some prior knowledge I'd like to apply.  I have discrete time points, and noisy, continuous outputs $x(t)$ that I observe at those time points, and would like to infer the discrete state $y(t)$ that is most likely to correspond to $x(t)$.  If it matters, I know the distributions $x(y_1),\cdots,x(y_n)$, and that there are only six possible hidden states, and have lots of annotated data to train the model on.&lt;/p&gt;&#10;&#10;&lt;p&gt;Were it that simple, I think a hidden Markov model and the Viterbi algorithm would do the trick.  But I know that the output at time $t_i$ is affected not only by the hidden state at time $t_i$, but by the three previous hidden states, like so:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/VbGXt.png&quot; alt=&quot;link to picture of model&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I left the diagram undirected, since the correlations between successive hidden states are not causal, but I could live with rightward arrows all around if I have to.&lt;/p&gt;&#10;&#10;&lt;p&gt;Anyway, does anyone know what such a model might be called, and/or if it's even plausible to try to learn the parameters for it?  It seems simple enough that I feel like someone has got to have done it before, but I haven't been able to locate anything.  I think it's basically a linear-chain conditional random field, but those extra edges make me feel lost and confused.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;If it's any easier to parse/think about, here's an example where the current observed state depends on the past two, rather than four, hidden states:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/xMQ3L.png&quot; alt=&quot;Simplified model&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;This is sort of the minimal extension of a linear chain CRF, but I still can't find an example of something like it. Any help?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-02-07T01:56:11.870" FavoriteCount="2" Id="49466" LastActivityDate="2013-02-09T09:53:23.987" LastEditDate="2013-02-07T16:07:00.637" LastEditorUserId="20572" OwnerUserId="20572" PostTypeId="1" Score="6" Tags="&lt;time-series&gt;&lt;markov-process&gt;&lt;graphical-model&gt;&lt;resources&gt;" Title="Is there a standard name for probabilistic graphical models like this?" ViewCount="254" />
  
  <row AnswerCount="1" Body="&lt;p&gt;My dataset is sports data and the outcome variable is wins in a season and I am trying to see the affect of the characteristics of the players on the team on this outcome. My question is, is it easier to use interactions or construct a measure?&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to know if players attending a top 25 college affects wins when other teamate characteristics are considered. I have 3 possible structures, &lt;code&gt;G1&lt;/code&gt;: at least one captain (of the two) went a top 25 and some other or no other players on the team did, &lt;code&gt;G2&lt;/code&gt;: no captain attended a top 25 and other players on the team did, and &lt;code&gt;G3&lt;/code&gt;: no one on the team attended a top 25 college. I am really interested in comparing &lt;code&gt;G2&lt;/code&gt; to &lt;code&gt;G3&lt;/code&gt; given my outcome. &lt;/p&gt;&#10;&#10;&lt;p&gt;Is it best to create 3 variables &lt;code&gt;G1&lt;/code&gt;, &lt;code&gt;G2&lt;/code&gt;, and &lt;code&gt;G3&lt;/code&gt; and then run my regression with either &lt;code&gt;G2&lt;/code&gt; or &lt;code&gt;G3&lt;/code&gt; as the reference group and see the effect of the coefficient or to crate a dummy for the captains and then the non-captains and then create interactions? I want the most accurate approach, but I am not sure how to setup these interactions or if they will be too confusing to compare to a reference group.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-07T06:59:29.373" FavoriteCount="2" Id="49478" LastActivityDate="2015-02-26T15:06:51.800" LastEditDate="2013-02-07T14:24:29.360" LastEditorUserId="20093" OwnerUserId="20093" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;multivariate-analysis&gt;&lt;dataset&gt;&lt;interaction&gt;" Title="Constructing a measure versus using an interaction" ViewCount="113" />
  
  <row AcceptedAnswerId="49486" AnswerCount="2" Body="&lt;p&gt;This is probably going to sound extremely stupid, but it's part of a larger question, about sample distribution of means, that I'm having problems understanding, so please bear with me.&lt;/p&gt;&#10;&#10;&lt;p&gt;If I record the time between planes landing, and have 250 observations, are those 250 observations my &quot;population&quot;, or does the term &quot;population&quot; refer to all the values, i.e. if I had sat there till the end of time recording?&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm assuming its the latter but just want to double check.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-07T10:33:38.027" FavoriteCount="2" Id="49483" LastActivityDate="2013-02-07T11:22:20.337" LastEditDate="2013-02-07T11:14:08.997" LastEditorUserId="686" OwnerUserId="17858" PostTypeId="1" Score="1" Tags="&lt;population&gt;" Title="What does population actually refer to?" ViewCount="151" />
  
  <row AcceptedAnswerId="49492" AnswerCount="1" Body="&lt;p&gt;I am trying to check what is the probability that a new observation is anomalous or not?&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose I have the following set of observations:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x &amp;lt;- c(11,22,3,4,25,6,7,1,1,2,1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;where&#10;the sample mean of $x$ is 7.545455&#10;and the sample standard deviation is 8.489566&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose I observe a new value of $x$, 111, which is anomalous because its value is considerably more than the sample mean of the normal distribution. How can I check in R that it is anomalous?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-02-07T13:02:24.000" FavoriteCount="1" Id="49491" LastActivityDate="2013-02-07T14:36:26.923" LastEditDate="2013-02-07T14:36:26.923" LastEditorUserId="88" OwnerUserId="16016" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;standard-deviation&gt;&lt;mean&gt;&lt;simulation&gt;" Title="Normal distribution in R" ViewCount="129" />
  
  <row AcceptedAnswerId="56601" AnswerCount="2" Body="&lt;p&gt;I have a dataset I'm working on that has some co-variate shift between the training set and the test set.  I'm trying to build a predictive model to predict an outcome, using the training set.  So far my best model is a random forest.&lt;/p&gt;&#10;&#10;&lt;p&gt;How can I deal with the shifted distributions in the training vs. test set?  I've come across 2 possible solutions that I've been able to implement myself:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Remove the shifted variables.  This is sub-optimal, but helps prevent my model from over fitting the training set.&lt;/li&gt;&#10;&lt;li&gt;Use a logistic regression to predict whether a given observation is from the test set (after balancing the classes), predict &quot;test set probabilities&quot; for the training set, and then boostrap sample the training set, using the probabilities for sampling.  Then fit the final model on the new training set.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Both 1 and 2 are pretty easy to implement, but neither one satisfies me, as #1 omits variables that might be relevant, and #2 uses a logistic regression, when my final model is tree-based.  Furthermore, #2 takes a few paragraphs of custom code, and I worry that my implementation may not be correct.&lt;/p&gt;&#10;&#10;&lt;p&gt;What are the standard methods for dealing with covariate shift?  Are there any packages in R (or another language) that implement these methods?&lt;/p&gt;&#10;&#10;&lt;p&gt;/edit: It seems like &quot;kernel mean matching&quot; is another approach I could take.  I've found lots of academic papers on the subject, but no one seems to have published any code.  I'm going to try to implement this on my own, and will post the code as an answer to this question when I do.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-07T14:42:46.003" FavoriteCount="2" Id="49497" LastActivityDate="2013-04-19T15:03:03.817" LastEditDate="2013-02-17T19:04:31.673" LastEditorUserId="2817" OwnerUserId="2817" PostTypeId="1" Score="5" Tags="&lt;r&gt;" Title="Covariate shift correction: standard implementation in R?" ViewCount="317" />
  
  
  
  <row Body="&lt;p&gt;What you are trying to do (if I understand you correctly), is to model the probability of the stimulus being seen by a Weibull distribution... &lt;em&gt;but with the Weibull parameters depending on a covariate, i.e., the size of the stimulus&lt;/em&gt;. So you are really doing a so-called &quot;Weibull regression&quot;. The &lt;code&gt;weibreg()&lt;/code&gt; function in the &lt;code&gt;eha&lt;/code&gt; package &lt;a href=&quot;http://rss.acs.unt.edu/Rdoc/library/eha/html/weibreg.html&quot; rel=&quot;nofollow&quot;&gt;looks like it should do what you want&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-07T15:11:39.343" Id="49502" LastActivityDate="2013-02-07T15:16:55.060" LastEditDate="2013-02-07T15:16:55.060" LastEditorUserId="1352" OwnerUserId="1352" ParentId="49500" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;I'm not sure you can do much more with what you've got to work with.&lt;/p&gt;&#10;&#10;&lt;p&gt;I mean, you can lower bound $\gamma \geq \mu^2$, as&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;\mathbb{Var}(X_t) = \mathbb{E}(X_t^2)-\mathbb{E}(X_t)^2 = \gamma-\mu^2 \geq 0&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;then estimate $\mu$ from the arithmetic mean of your set of samples $S_T$ as usual. &lt;/p&gt;&#10;&#10;&lt;p&gt;If we have the situation where $Y_t$ is equal to $X_t$ plus some uncorrelated noise, then you can upper bound it too as&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;\mathbb{E}(Y_t^2) \geq \mathbb{E}(X_t^2).&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;To see why, look at Innuo's splendid comment on your original post, and note that the variance of $Z_t$ must be positive. If the covariance between it and $X_t$ is zero, then the inequality becomes pretty clear. If the correlation between $X_t$ and $Z_t$ is positive, then it still holds, albeit more weakly.&lt;/p&gt;&#10;&#10;&lt;p&gt;So we have $\mathbb{E}(Y_t^2) \geq \gamma \geq \mu^2$ for uncorrelated or positively correlated additive noise. I think that's all that can be said. If those bounds are sufficiently tight then maybe that will do? If not, then either $X_t$ or $Z_t$ has a big variance, and beyond taking more direct measurements somehow I'm not sure if you can tell which.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-02-07T15:15:15.393" Id="49503" LastActivityDate="2013-02-07T15:15:15.393" OwnerUserId="16765" ParentId="49037" PostTypeId="2" Score="1" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;We are conducting a study which has a pre-post design.  The unit of the study is the outpatient visit. In each visit, we study how patients and physicians interact. &lt;/p&gt;&#10;&#10;&lt;p&gt;Each physician will see between 3-6 patients and most of these patients (ie, except for dropouts) will be seen twice: once in the pre-phase, once in the post-phase. &lt;/p&gt;&#10;&#10;&lt;p&gt;Because the relation of patient to physician in any given visit is symmetric, it would seem to lead to two distinct ways to model the hierarchical structure. Starting from the root vertex of the (directed) tree:&lt;/p&gt;&#10;&#10;&lt;p&gt;A) study $\to$ physician $\to$ patient $\to$ pre/post&lt;/p&gt;&#10;&#10;&lt;p&gt;B) study $\to$ pre/post $\to$ physician $\to$ patient&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a scientific reason or stat. methodological issues to prefer one nesting over the other? What differences in results can we expect?&lt;/p&gt;&#10;&#10;&lt;p&gt;Any pointers to the literature are appreciated. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-07T18:24:29.773" Id="49522" LastActivityDate="2013-05-10T04:27:58.387" OwnerUserId="10880" PostTypeId="1" Score="1" Tags="&lt;multilevel-analysis&gt;&lt;nested&gt;" Title="Nesting / hierarchical modeling in a pre-post study" ViewCount="211" />
  <row Body="&lt;p&gt;I was missing the package &lt;a href=&quot;http://cran.r-project.org/web/packages/glmnet/index.html&quot; rel=&quot;nofollow&quot;&gt;&lt;code&gt;glmnet&lt;/code&gt;&lt;/a&gt;. The error message:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;&lt;pre&gt;&lt;code&gt;Error in { : &#10;  task 1 failed - &quot;arguments imply differing number of rows: 1264, 736&quot;&#10;In addition: There were 50 or more warnings (use warnings() to see the first&#10; 50)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;misled me to think there was problem with the input, not that I was missing a package.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-07T18:50:19.430" Id="49525" LastActivityDate="2014-07-30T18:31:42.030" LastEditDate="2014-07-30T18:31:42.030" LastEditorUserId="30802" OwnerUserId="2798" ParentId="49455" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;I am no chemist, but I imagine that future values of nitrogen oxide (your target variable) could in part be determined by nitrogen *di*oxide... but also vice versa, so your target variable and your &quot;exogenous&quot; variable influence &lt;em&gt;each other&lt;/em&gt; (so the &quot;exogenous&quot; variable would not be really exogenous any more). This might call for &lt;a href=&quot;http://en.wikipedia.org/wiki/Vector_autoregression&quot; rel=&quot;nofollow&quot;&gt;Vector Autoregression&lt;/a&gt; or similar models.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-07T20:27:01.147" Id="49535" LastActivityDate="2013-02-07T20:27:01.147" OwnerUserId="1352" ParentId="49519" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Yes. Of course, we are not talking about one-dimensional functions, but functions $\mathbb{R}^p \to \mathbb{R}$ to be maximized (viz., the likelihood), so this is slightly more advanced than the one-dimensional case.&lt;/p&gt;&#10;&#10;&lt;p&gt;Some facility with logarithms will definitely be helpful, since maximizing the logarithm of the likelihood is usually much easier than maximizing the likelihood itself.&lt;/p&gt;&#10;&#10;&lt;p&gt;Quite a lot more than simple MLE can be understood (information matrices etc.) if you can deal with second derivatives of $\mathbb{R}^p \to \mathbb{R}$ functions, i.e., the Hessian matrix.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-07T21:36:37.137" Id="49545" LastActivityDate="2013-02-07T21:36:37.137" OwnerUserId="1352" ParentId="49544" PostTypeId="2" Score="4" />
  
  
  
  
  <row Body="&lt;p&gt;When you report on your statistical analysis, yes, you should report the model you based your conclusions on, not drop components that are not statistically significant - those may still influence the effect of other regressors.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you are really asking whether you should drop the insignificant predictor and re-fit your model (called &quot;stepwise regression&quot;) - &lt;a href=&quot;http://stats.stackexchange.com/questions/3814/how-to-annoy-a-statistical-referee/3817#3817&quot;&gt;you should not do that if your goal is inference&lt;/a&gt;, but this may be defensible if your goal is prediction.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-08T15:14:51.823" Id="49588" LastActivityDate="2013-02-08T15:14:51.823" OwnerUserId="1352" ParentId="49574" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;(a) The purpose of using a kernel is to solve a nonlinear regression problem in this case. A good kernel will allow you to solve problems in a possibly infinite-dimensional feature space. &#10;But, using a linear kernel $K(\mathbf{x,y}) = \mathbf{x}^\top \mathbf{y}$ and doing the kernel ridge regression in the dual space is same as solving the problem in the primal space, i.e., it doesn't bring any advantage (it's just much slower as the number of sample grows as you observed).&lt;/p&gt;&#10;&#10;&lt;p&gt;(b) One of the most popular choices is the squared exponential kernel $K(x,y) = \exp(-\frac{\tau}{2} ||\mathbf{x}-\mathbf{y}||^2)$ which is &lt;em&gt;universal&lt;/em&gt; (see ref below). There are many many kernels, and each of them will induce different inner product (and hence metric) to your feature space.&lt;/p&gt;&#10;&#10;&lt;p&gt;(c) Straightforward implementation requires solving a linear equation of size $n$, so it's $O(n^3)$. There are many faster approximation methods such as Nyström approximation. This is an area of active research.&lt;/p&gt;&#10;&#10;&lt;p&gt;References:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Bharath Sriperumbudur, Kenji Fukumizu, and Gert Lanckriet. On the relation between universality, characteristic kernels and RKHS embedding of measures. Journal of Machine&#10;Learning Research, 9:773–780, 2010.&lt;/li&gt;&#10;&lt;li&gt;Bernhard Schlkopf, Alexander J. Smola. &lt;a href=&quot;http://amzn.to/YftHsQ&quot; rel=&quot;nofollow&quot;&gt;Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond&lt;/a&gt; 2002&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2013-02-08T15:35:39.510" Id="49590" LastActivityDate="2013-02-08T15:43:21.940" LastEditDate="2013-02-08T15:43:21.940" LastEditorUserId="8960" OwnerUserId="8960" ParentId="49557" PostTypeId="2" Score="3" />
  <row AnswerCount="1" Body="&lt;p&gt;I am working on a multiple regression model that will forecast the value of loans being granted within the current month.&lt;/p&gt;&#10;&#10;&lt;p&gt;The data points are broken down per day ( Jan will have 31 data points etc) and I will be updating the model every week.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have 4 independent variables driving the model.&#10;I am currently testing the model and trying to understand what is happening in particular to the $p$-values.&lt;/p&gt;&#10;&#10;&lt;p&gt;On Day 8, 2 of my Variables (loans and declines) have $p$-values of &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;loans   0.014030324&#10;declines    0.980464984&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;On day 15 when I run the regression I get these&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;loans   0.003114471&#10;declines    0.023498327&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I am just wondering why the &quot;declines&quot; $p$-value is now starting to show as a significant value. Is it because the model is working over more data points or is there something in the data that is suggesting that this variable is becoming more significant?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-02-08T15:46:17.717" FavoriteCount="1" Id="49592" LastActivityDate="2013-02-09T11:53:39.213" LastEditDate="2013-02-09T11:53:39.213" LastEditorUserId="3826" OwnerUserId="20641" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;multiple-regression&gt;&lt;p-value&gt;" Title="P-value for multiple regression changes as more points are added" ViewCount="460" />
  
  
  <row Body="&lt;p&gt;If you want to compare your three groups, just run your regression over them.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you want to study specifically:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;The effect of top25 captain(s) [0 or more]&lt;/li&gt;&#10;&lt;li&gt;The effect of top25 teammate(s) [0 or more]&lt;/li&gt;&#10;&lt;li&gt;The interaction between these two groups&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;You should study the complete design. In this case you will need a fourth group (no captains, no teammates from top25). The interaction could be hard to observe with small sample or small differences.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-02-08T16:38:03.320" Id="49599" LastActivityDate="2013-02-08T16:38:03.320" OwnerUserId="20591" ParentId="49478" PostTypeId="2" Score="0" />
  <row Body="&lt;h2&gt;Background&lt;/h2&gt;&#10;&#10;&lt;p&gt;The different fit indices tend to be sensitive to different forms of misspecification.  Looking at the formulae, where $T$ is the model statistic, $df$ the degrees of freedom, and subscripts indicate baseline versus target:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;TLI = \frac{(T_b / df_b) - (T_t / df_t)}{(T_b / df_b) - 1}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;SRMR = \sqrt{\left( \frac{2\Sigma_{i=1}^{p}\Sigma_{j=1}^{i} \left[ \left(\frac{s_{ij} - \hat{\sigma}_{ij}}{s_{ii}s_{jj}}\right)^2&#10;\right]}{p(p+1)} \right)}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;One of the keys here is that things like the TLI and CFI (not shown) incorporate the degrees of freedom, which means simpler models but that do fairly well will be preferred.  The SRMR does not.  There is no benefit from a more parsimonious model.&lt;/p&gt;&#10;&#10;&lt;p&gt;It is perhaps not surprising then that the different fit indices tend to be sensitive to different types of model misspecification.&lt;/p&gt;&#10;&#10;&lt;p&gt;A further hint is the squared term --- particular covariances misspecified will contribute much more (dropping off quadratically at the residual nears zero).&lt;/p&gt;&#10;&#10;&lt;h2&gt;Latent Growth Models&lt;/h2&gt;&#10;&#10;&lt;p&gt;Turning to latent growth models, the form is very specific and many parameters are fixed.  You have a piecewise model, but still you have many time points (15) and are only fitting two lines.  It is not at all surprising to me that there is some misspecification here.  The CFI/TLI are likely relatively good because of how parsimonious the model is.  That is a good sign, but the SRMR is disturbingly high.  It may not change your parameters of interest much, but I would definitely want to at least figure out what part of the model was misspecified.&lt;/p&gt;&#10;&#10;&lt;h2&gt;Suggestions&lt;/h2&gt;&#10;&#10;&lt;p&gt;The tools to determine and correct model misspecification are basically the same regardless of the problem.  That is perhaps an oversimplification, but not by much.&lt;/p&gt;&#10;&#10;&lt;p&gt;In your case, you do not have a measurement issue (that is, you do not need to examine whether there should be alternate factors or different groupings of the items per se); however, it may be unreasonable to assume linear growth, even piecewise linear growth.&lt;/p&gt;&#10;&#10;&lt;p&gt;Another common area of misspecification with growth models is of the error structure.  It is possible, perhaps even likely that residuals will be more highly correlated with near time points than with those farther away in time.  If there is some cyclical pattern to the assessments, those may also play a role (e.g., seasons, times of day, days of week, etc.).&lt;/p&gt;&#10;&#10;&lt;p&gt;Examine the standardized residual covariances --- which ones are high?  What happens if you add a residual covariance to account for that?  Consider relaxing the linear time constraint.  You could try quadratic time or freely estimate it.  You can try modification indices to see &quot;automated&quot; suggestions for how to improve your model.&lt;/p&gt;&#10;&#10;&lt;p&gt;If all of that is seeming too complex or variable.  Try simplifying your model.  Rather than doing a piecewise model, fit a model to just the first piece (ignore the second piece and leave it out for now).  Make sure that your growth models are solid for each of the pieces before combining models for all 15 time points.  The same approaches I described can be used with the individual pieces.  What happens if the individual pieces fit great, but combined they do not?  This suggests it is the relations between pieces that are being misspecified---what time points from the first or second are more likely highly related?  What is going on around the measures you may need to account for either in the functional form (linear etc.) or with residual covariances?  At each step you can use the residual covariance matrices, modification indices, your own theoretical judgement, examination of the raw correlation matrices, and data visualization to help get a handle on these things.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-02-08T19:34:35.667" Id="49610" LastActivityDate="2013-02-08T19:34:35.667" OwnerUserId="12521" ParentId="49103" PostTypeId="2" Score="2" />
  
  
  
  <row Body="&lt;p&gt;While I'm a bit late to the party, if you are thinking about anything sinusoidal, wavelet transforms are a good tool to have in your pocket also.  In theory, you can use wavelet transforms to decompose a sequence into various &quot;parts&quot; (e.g., waves of different shapes/frequencies, non-wave components such as trends, etc).  A specific form of wave transform that is used a ton is the Fourier transform, but there's a lot of work in this area.  I'd love to be able to recommend a current package, but I haven't done signal analysis work in quite a while.  I recall some Matlab packages supporting functionality on this vein, however.&lt;/p&gt;&#10;&#10;&lt;p&gt;Another direction to go if you're only trying to find trends in cyclic data is something like the Mann-Kendall Trend test.  It's used a lot for things like detecting changes in weather or water quality, which has strong seasonal influences.  It doesn't have the bells and whistles of some more advanced approaches, but since it's a veteran statistical test it is fairly easy to interpret and report.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-09T06:17:31.850" Id="49626" LastActivityDate="2013-02-09T06:17:31.850" OwnerUserId="20396" ParentId="3316" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;I think the answer is generally yes.  If you know more about a distribution then you should use that information.  For some distributions this will make very little difference, but for other it could be considerable.&lt;/p&gt;&#10;&#10;&lt;p&gt;As an example, consider the poisson distribution.  In this case the mean and the variance are both equal to the parameter $\lambda$ and the ML estimate of $\lambda$ is the sample mean.&lt;/p&gt;&#10;&#10;&lt;p&gt;The charts below show 100 simulations of estimating the variance by taking the mean or the sample variance.  The histogram labelled X1 is the using sample mean, and X2 is using the sample variance.  As you can see, both are unbiased but the mean is a much better estimate of $\lambda$ and hence a better estimate of he variance.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/qY8V3.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The R code for the above is here:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(ggplot2)&#10;library(reshape2)&#10;testpois = function(){&#10;  X = rpois(100, 4)&#10;  mu = mean(X)&#10;  v = var(X)&#10;  return(c(mu, v))&#10;}&#10;&#10;P = data.frame(t(replicate(100, testpois())))&#10;P = melt(P)&#10;&#10;ggplot(P, aes(x=value)) + geom_histogram(binwidth=.1, colour=&quot;black&quot;, fill=&quot;white&quot;) +&#10;  geom_vline(aes(xintercept=mean(value, na.rm=T)),   # Ignore NA values for mean&#10;             color=&quot;red&quot;, linetype=&quot;dashed&quot;, size=1) + facet_grid(variable~.)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;As to the question of bias, I wouldn't worry too much about your estimator being biased (in the example above it isn't, but that is just luck).  If unbiasedness is important to you you can always use Jackknife to try remove the bias.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-02-09T10:54:01.740" Id="49631" LastActivityDate="2014-02-07T08:58:08.177" LastEditDate="2014-02-07T08:58:08.177" LastEditorUserId="19879" OwnerUserId="19879" ParentId="49617" PostTypeId="2" Score="7" />
  <row Body="&lt;p&gt;You might start by looking at Elo rating systems used in Chess. (chess.com or letsplaychess.com have introductory articles about such systems, which began in chess but have spread to other contexts). Glickman has a lot of writing about other schemes, such as the Glicko system (Glickman's a math guy, so he likely has the technical depth you are looking for).  You want to pay particular attention to how such systems handle startup conditions, because in your sports situations you never really get out of startup (except perhaps by the end of the season in baseball). &lt;/p&gt;&#10;&#10;&lt;p&gt;There may be other answers here in the literature of paired comparisons. What you have isn't so much a time series as a sequence of paired comparisons.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-09T19:31:39.403" Id="49650" LastActivityDate="2013-02-09T19:31:39.403" OwnerUserId="3919" ParentId="49426" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;I think the solution is the concept of a compound Poisson distribution. The idea is a random sum &#10;$$&#10;S = \sum_{i=1}^N X_i&#10;$$ &#10;with $N$ Poisson distributed and $X_i$ and $iid$ sequence independent of $N$. When we restric to the case that $X_i=k$ always, then we can describe $k N$ for a real number $k$ and a Poisson distributed $N$. You get the pgf by&#10;$$&#10;E[s^{k N}] = E[(s^{k})^N] = G_N(s^{k}) = \exp(\lambda(s^k-1))&#10;$$&#10;For the sum $Z = k_1 N_1 + k_2 N_2$ you get&#10;$$&#10;G_Z(s) = \exp(\lambda_1(s^{k_1}-1) + \lambda_2(s^{k_2}-1)).&#10;$$ &#10;define $\lambda = \lambda_1 + \lambda_2$ then&#10;$$&#10;G_Z(s) = \exp(\lambda ( \frac{\lambda_1}{\lambda}(s^{k_1}-1)+ \frac{\lambda_2}{\lambda}(s^{k_1}-1)) = \exp(\lambda (\frac{\lambda_1}{\lambda}s^{k_1}+ \frac{\lambda_2}{\lambda}s^{k_1}-1)).&#10;$$&#10;The final interpretation is that the resulting rv is a compound Poisson distribution with intensity $\lambda = \lambda_1 + \lambda_2$ and distribution of the $X_i$ that take the value $k_1$ with probability $\lambda_1/\lambda$ and the value $k_2$ with $\lambda_2/\lambda$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Having proved that the distributions is compound Poisson we can either use Panjer recursion in the case that $k_1$ and $k_2$ are positive integers. Or we can easily derive the Fourier transform from the form of the pgf and get the distribution back by the inverse. Note that there is a point mass at $0$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Edit after a discussion:&lt;/p&gt;&#10;&#10;&lt;p&gt;I think the best you can do is MC. You could use the derivation that this is a compound Poisson distr. &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;sample N from $Pois(\lambda)$ (very efficient) &lt;/li&gt;&#10;&lt;li&gt;then for each $i=1,\ldots,N$ sample whether it is from $X_1$ or $X_2$ where&#10;the probability of the first is $\lambda_1/\lambda$. Do this by sampling  a Bernoulli rv with probability of success $\lambda_1/\lambda$. If it is $1$ then add $k_1$ to the sampled sum else add $k_2$. &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;You will have a sample of say 100 000 in seconds. &lt;/p&gt;&#10;&#10;&lt;p&gt;Alternatively you can sample the two summands in your inital representation separately ... this will be as quick.&lt;/p&gt;&#10;&#10;&lt;p&gt;Everything else (FFT) is complicated if the constant factor k1 and k2 are totally general.&lt;/p&gt;&#10;" CommentCount="13" CreationDate="2013-02-09T23:25:58.913" Id="49659" LastActivityDate="2013-02-17T21:02:17.403" LastEditDate="2013-02-17T21:02:17.403" LastEditorUserId="12147" OwnerUserId="12147" ParentId="49649" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;In statistical inference, the terms parametric, semi-parametric, and non-parametric refer to the assumptions necessary for inference to be valid. For instance, the T-test is a &lt;em&gt;non-parametric&lt;/em&gt; test for differences in means. This is because the asymptotic sampling distribution of the test-statistic has a normal distribution by the central limit theorem (so there are mild regularity constraints on our probability model for observed data, such as having finite variance). &lt;/p&gt;&#10;&#10;&lt;p&gt;A parametric test requires assumptions about the probability model for inference to be correct. As you mentioned, many of the GLMs use the mean-variance relationship determined by the link function (and it's relationship to data generating mechanisms for observed data) to improve  inference efficiency. Correct assumptions about the probability model for the observed data is a sufficient condition to get more efficient inference relative to non-parametric approaches. When robust standard errors are used (a special case of the GEE), this becomes a semi-parametric approach. The GEE requires only that the mean model is correct to have correct inference (the mean-variance relationship could have been misspecified, but asymptotically, the 95% confidence intervals for the sampling distribution of the test statistic will be correct).&lt;/p&gt;&#10;&#10;&lt;p&gt;The model you've written up there is an example of a semi-parametric modeling approach. The $m(T)$ can be estimated iteratively using the EM algorithm, or depending on the link function $G(\cdot)$ can be factored into a partial likelihood like with the Cox proportional hazards model.&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2013-02-09T23:47:24.920" Id="49660" LastActivityDate="2013-02-09T23:47:24.920" OwnerUserId="8013" ParentId="49647" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm trying to understand when credibility intervals are useful?&lt;/p&gt;&#10;&#10;&lt;p&gt;Are there examples of real world situations where credibility intervals are the better thing to use compared to confidence intervals? Note that by &quot;useful&quot;, I mean maximizing some concrete real-world objective (so not for instance trying to get posterior intervals for one's subjective beliefs which can be useful but not something I'm looking for)&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-02-10T02:58:39.170" FavoriteCount="2" Id="49671" LastActivityDate="2013-02-10T02:58:39.170" OwnerUserId="250" PostTypeId="1" Score="2" Tags="&lt;bayesian&gt;&lt;credible-interval&gt;" Title="Credibility Intervals" ViewCount="98" />
  <row AnswerCount="1" Body="&lt;p&gt;I'm trying to compare several sets of experiment data, by comparing means. I read there are several different tests such as &lt;em&gt;Each Pair, Student’s t&lt;/em&gt; and &lt;em&gt;All Pairs, Tukey HSD&lt;/em&gt;, which give different circles of different radius, an example shown below &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/QCKE1.gif&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;How are the circles defined? How do I calculate the radius? And is there a rule what test one should use for what kind of data?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-02-10T03:03:22.580" Id="49672" LastActivityDate="2013-10-28T02:39:50.397" LastEditDate="2013-02-10T19:53:22.873" LastEditorUserId="88" OwnerUserId="20684" PostTypeId="1" Score="1" Tags="&lt;mean&gt;&lt;students-t&gt;&lt;tukey-hsd&gt;&lt;jmp&gt;" Title="Comparing many means in JMP" ViewCount="185" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I want to  create a regression model to predict state crime rate.  There are two variables among 10 ( Vi= # of violent crimes per 100,000 population, Vi2 = # of violent crimes per 10,000 population) that are highly correlated, because Vi is 10 times Vi2, so I decided to use partial correlation.  After controlling for Vi2, the correlation of Vi with the dependent variable is reduced to less than 0.4.  Should O control for Vi2 in this way, or remove it altogether?  Should I remove Vi?&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, in general may I remove variables if their partial &lt;em&gt;r&lt;/em&gt; is less than 0.4 (indicating a weak relationship with the dependent variable) or do need to use an automated variable selection method to get a final model?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-02-10T06:02:39.170" Id="49676" LastActivityDate="2013-02-10T12:13:50.080" LastEditDate="2013-02-10T12:13:50.080" LastEditorUserId="88" OwnerUserId="20456" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;model-selection&gt;&lt;feature-selection&gt;&lt;partial-correlation&gt;&lt;statistical-control&gt;" Title="Partial correlation" ViewCount="156" />
  <row Body="&lt;p&gt;[EDITED in light of the comment]&lt;/p&gt;&#10;&#10;&lt;p&gt;I think there is a problem if you use CV results to select among multiple models. &lt;/p&gt;&#10;&#10;&lt;p&gt;CV allows you to use the entire dataset to train and test one model/method, while being able to have a reasonable idea of how well it will generalize. But if you're comparing multiple models, my instinct is that the model comparison uses up the extra level of train-test isolation that CV gives you, so the final result will not be a reasonable estimate of the chosen model's accuracy.&lt;/p&gt;&#10;&#10;&lt;p&gt;So I'd guess that if you create several models and choose one based on its CV, you're being overly-optimistic about what you've found. Another validation set would be needed to see how well the winner generalizes.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-02-10T17:16:41.360" Id="49694" LastActivityDate="2013-02-10T18:11:20.247" LastEditDate="2013-02-10T18:11:20.247" LastEditorUserId="1764" OwnerUserId="1764" ParentId="49692" PostTypeId="2" Score="5" />
  
  <row Body="&lt;p&gt;This is not a problem if the CV is &lt;strong&gt;nested&lt;/strong&gt;, i.e. all optimisations, feature selections and model selections, whether they themselves use CV or not, are wrapped in one big CV.&lt;/p&gt;&#10;&#10;&lt;p&gt;How does this compare to having an extra validation set? While the validation set is usually just a more or less randomly selected part of the whole data, it is simply an equivalent of one iteration of CV. To this end, it is actually a worse method because it can be easily be biased by (hopefully) luckily/unluckily selected or cherry-picked validation set.&lt;/p&gt;&#10;&#10;&lt;p&gt;The only exception to this are time-series and other data where the object order matters; but they require special treatment either way.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-10T20:31:49.547" Id="49698" LastActivityDate="2013-02-11T00:30:01.883" LastEditDate="2013-02-11T00:30:01.883" LastEditorUserId="6029" OwnerUserId="88" ParentId="49692" PostTypeId="2" Score="12" />
  <row AcceptedAnswerId="49727" AnswerCount="1" Body="&lt;p&gt;I have recently begun to read about bayesian statistics and I am playing around with the R2WinBUGS package. I'm trying to fit a logistic regression to the spam data (that can be found on the webpage of the elements of statistical learning) using R and WinBUGS. My approach was to first divide the data into 80% training and 20% testing sets. I can fit the model using the 80% set but I dont know how to write WinBugs code to predict on new observations (say the 20% test set) and I wonder if this approach to study model/classification precisicion make sense in a Bayesian Approach?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-11T03:15:52.637" FavoriteCount="2" Id="49714" LastActivityDate="2013-02-11T16:25:02.587" LastEditDate="2013-02-11T03:31:00.620" LastEditorUserId="11748" OwnerUserId="11748" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;regression&gt;&lt;logistic&gt;&lt;bayesian&gt;&lt;winbugs&gt;" Title="Logistic Regression - Bayesian Approach - Assessing Classification Precision" ViewCount="360" />
  <row AcceptedAnswerId="49723" AnswerCount="3" Body="&lt;p&gt;I am trying to analyse the following table:&lt;/p&gt;&#10;&#10;&lt;p&gt;5  5  5  5  4  5  5  5  5 &lt;/p&gt;&#10;&#10;&lt;p&gt;3  4  3  3  4  4  3  5  5 &lt;/p&gt;&#10;&#10;&lt;p&gt;Each number is a success score out of 5 attempts, so we can assume binomial (p, 5), for unknown p. Each column pertains to one person. The bottom row contains the &quot;pre&quot; test results and the top row is the &quot;post&quot; test results.&lt;/p&gt;&#10;&#10;&lt;p&gt;At a quick glance, every item in the top row is at least as large as the corresponding item in the bottom row, so by a quick sign test, the hypothesis that there is a difference pre and post should be significant.&lt;/p&gt;&#10;&#10;&lt;p&gt;If I do a paired t-test on these data (holding my nose, because each item is binomial (p,5)) I get a significant result. If I put the data into a 2 x 2 contingency table (pre post x right wrong) and ignore the matching, I get a significant result.&lt;/p&gt;&#10;&#10;&lt;p&gt;But when I do a contingency test of the table above, testing for independence conditional on the margins, the result is non-significant (p = 0.97). I used fisher.test in R, the Fisher exact test.&lt;/p&gt;&#10;&#10;&lt;p&gt;So what is the best way to test the hypothesis that there is a significant difference, pre and post, taking matching into account? And why does the analysis of independence not apply here?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-02-11T04:06:26.483" Id="49717" LastActivityDate="2013-02-11T10:33:03.373" LastEditDate="2013-02-11T08:51:21.583" LastEditorUserId="3277" OwnerUserId="14188" PostTypeId="1" Score="6" Tags="&lt;hypothesis-testing&gt;&lt;repeated-measures&gt;&lt;binomial&gt;&lt;contingency-tables&gt;&lt;paired-comparisons&gt;" Title="Matched binomial pairs" ViewCount="343" />
  
  <row AcceptedAnswerId="49721" AnswerCount="2" Body="&lt;p&gt;I wanted to find a variant of the perceptron which works for non-separable data, so I tried using $f(x)=\mathrm{\tanh}(x)$ instead of the hard threshold function and finding a $w$ that minimises the function $$\frac{1}{2} \sum_i (y_i - f(w \cdot x_i))^2$$&#10;where $y_i \in \{-1,1\}$ are the classes of the training examples and $x_i$ are the features. I minimised it by using &quot;batch&quot; gradient descent and it seems to work nicely and gives me what I want.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is: is this the same as &lt;strong&gt;Adaline&lt;/strong&gt;? I found a few references to Adaline, but I can't tell whether Adaline refers to the classifer itself, or also the algorithm which is used to train it, or even the physical machine which originally implemented that algorithm (by the way, I am equally confused about the use of &quot;perceptron&quot;). Am I justified in saying that I am using Adaline?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-11T05:34:27.147" Id="49719" LastActivityDate="2013-02-15T01:48:14.250" LastEditDate="2013-02-15T01:48:14.250" LastEditorUserId="13818" OwnerUserId="13818" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;classification&gt;&lt;neural-networks&gt;&lt;terminology&gt;" Title="What is the name of this perceptron-like classifier?" ViewCount="893" />
  
  <row Body="&lt;p&gt;I would recommend you to use Generalized Estimating Equations (GEE). This is analogous to Repeated-measures ANOVA but allows for non-continuous or non-normal response because, being a Generalized linear model, it adopts various link functions.&lt;/p&gt;&#10;&#10;&lt;p&gt;With your example, I'd use binomial distribution with logit link (albeit you might prefer probit). The input data was restructured from &quot;wide&quot; into &quot;long&quot; which appears this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;   id pre_post count&#10;    1   1       5&#10;    2   1       5&#10;    3   1       5&#10;    4   1       5&#10;    5   1       4&#10;    6   1       5&#10;    7   1       5&#10;    8   1       5&#10;    9   1       5&#10;    1   2       3&#10;    2   2       4&#10;    3   2       3&#10;    4   2       3&#10;    5   2       4&#10;    6   2       4&#10;    7   2       3&#10;    8   2       5&#10;    9   2       5&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The DV is count, how much a respondent scored out of 5 attempts. pre_post is the repeated-measures factor levels of which you want to compare. Id is respondent. I'll use SPSS to analyse it; the command is below.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;GENLIN count /*response variable: score&#10;  OF 5 /*out of 5 trials&#10;  BY pre_post&#10; /MODEL pre_post INTERCEPT=YES /*the only factor to test is pre_post&#10;  DISTRIBUTION=BINOMIAL LINK=LOGIT&#10; /REPEATED SUBJECT=id WITHINSUBJECT=pre_post /*id is respondent id, pre_post is the repeated measures factor&#10;  CORRTYPE=INDEPENDENT ADJUSTCORR=YES /*we'll assume Independent correlation structure.&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Here is an excerpt from the results:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/WVEg0.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;You see that is a significant difference between pre and post. Usual RM ANOVA (paired-samples t-test) gave close significance, .007.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-11T08:36:46.117" Id="49723" LastActivityDate="2013-02-11T08:45:18.800" LastEditDate="2013-02-11T08:45:18.800" LastEditorUserId="3277" OwnerUserId="3277" ParentId="49717" PostTypeId="2" Score="5" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to understand a) what relative validity is, and b) how to test it statistically. So far I have gathered, from sources such as &lt;a href=&quot;http://www.psychology.uiowa.edu/faculty/wasserman/glossary/relative%20validity.html&quot; rel=&quot;nofollow&quot;&gt;this&lt;/a&gt; that relative validity is a larger reliability in causation associated with certain stimuli, given a set of stimuli. It seems that some statisticians test relative validity using regression models, such as in &lt;a href=&quot;http://journals.lww.com/lww-medicalcare/Abstract/1996/03000/A_12_Item_Short_Form_Health_Survey__Construction.3.aspx&quot; rel=&quot;nofollow&quot;&gt;this article&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, in none of these sources it is explained &lt;em&gt;how&lt;/em&gt; regression (or any other statistical tool) is used to measure relative validity. Hence my question - how do you test relative validity statistically?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-02-11T15:27:32.097" Id="49743" LastActivityDate="2013-02-11T15:27:32.097" OwnerUserId="10598" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;validity&gt;" Title="How to test relative validity statistically?" ViewCount="175" />
  <row Body="&lt;p&gt;Ken Bollen and I &lt;a href=&quot;http://dx.doi.org/10.1177/0049124112442138&quot; rel=&quot;nofollow&quot;&gt;wrote about negative variance estimates&lt;/a&gt; (aka Heywood cases). You might want to take a look for some insights. For this huge model, God only knows how model misspecifications are going to show up, but in my experience, Heywood cases are typical outlets for the model to let the steam out when something is not fitting right.&lt;/p&gt;&#10;&#10;&lt;p&gt;That having said, I would try different diagnostics: first fit all of the submodels with 6 or so indicators, and see if there's anything wrong with them. In the CFA context, I would imagine that underidentification would arise only if some variables have zero coefficient/covariance with the factors they are supposed to measure. You should be able to catch that with the analysis of subscales.&lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, for the Likert scales with 4 categories, you really should use polychoric correlations (&lt;code&gt;polycor&lt;/code&gt; package). For one thing, the categorical nature of the data would yield the likelihood ratio tests unreliable (as if I would trust that 900 observations could give rise to 2166 independent degrees of freedom, anyway).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-02-11T17:32:17.327" Id="49759" LastActivityDate="2013-02-11T17:32:17.327" OwnerUserId="5739" ParentId="49607" PostTypeId="2" Score="2" />
  
  
  
  <row Body="&lt;p&gt;There are some details you've left out, and I am not sure what sort of answer you are looking for, but here goes... &lt;/p&gt;&#10;&#10;&lt;p&gt;Assuming you are performing something along the lines of a t-test, you are not looking for the probability of &quot;mu = 25,&quot; but instead answering the question &quot;is it reasonable to conclude that mu is some number other than (!=) 25?&quot;  So the idea that mu equals exactly 25 is not problematic in the sense that you were referring to in your question.  If you conduct the test, you will either conclude that you can reject the null assumption as unreasonable or not, or you will calculate a p-value which answers a different, conditional, question (&quot;what is the probability of seeing data as extreme as mine, in terms of its sample mean, if the data actually came from a distribution centered at 25?&quot;) which will allow you to draw a similar conclusion regarding the reasonableness of the assumed mu.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you are asking the different question: &quot;what is the true mean of this (given) data?&quot; then you could build a confidence interval.  There your concerns would be a consideration and handled in the interval's construction.&lt;/p&gt;&#10;&#10;&lt;p&gt;In classical/frequentist statistics, parameters like mu are treated as fixed, exact numbers; so asking &quot;what is the probability of mu being 25?&quot; is a sort of &quot;nonsense&quot; question in that philosophical context.  A Bayesian approach would allow you to formulate the question in that way, but given the context of the question I don't think that is what you are working on.  In that approach, there are ways to fix up a tiny interval around mu to get a probability.&lt;/p&gt;&#10;&#10;&lt;p&gt;You sound as though you are coming to an introductory stats book with a little extra background in probability already under your belt.  Sometimes that makes the overly simple books harder to follow.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-02-11T22:07:35.717" Id="49777" LastActivityDate="2013-02-11T22:07:35.717" OwnerUserId="10884" ParentId="49770" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;According to &lt;a href=&quot;http://www.jstatsoft.org/v32/i10/paper&quot; rel=&quot;nofollow&quot;&gt;package documentation&lt;/a&gt; there is a function called &lt;code&gt;plotvgam&lt;/code&gt; for plotting vglm results:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;fit2.ms &amp;lt;- vglm(mstatus ~ poly(age, 2) + foo(age) + age,&#10;+ family = multinomial(refLevel = 2), constraints = clist,&#10;+ data = nzmarital)&#10;par(mfrow = c(2, 2))&#10;plotvgam(fit2.ms, se = TRUE, scale = 12, lcol = mycol[1],&#10;+ scol = mycol[1], which.term = 1)&#10;plotvgam(fit2.ms, se = TRUE, scale = 12, lcol = mycol[2],&#10;+ scol = mycol[2], which.term = 2)&#10;plotvgam(fit2.ms, se = TRUE, scale = 12, lcol = mycol[3],&#10;+ scol = mycol[3], which.term = 3)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2013-02-11T22:41:09.720" Id="49783" LastActivityDate="2013-02-11T22:41:09.720" OwnerUserId="3376" ParentId="49757" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="49788" AnswerCount="2" Body="&lt;p&gt;The optimization task is to find the operator $F(x_1, x_2, ...,x_n) \rightarrow y$. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Under which conditions should NN provide better results than LS (in terms of mean square fit error)? &lt;/p&gt;&#10;&#10;&lt;p&gt;Specifically, I mean the two approaches in MATLAB (assuming column vectors):&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;NN&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;net = fitnet(num_hidden_layers);&lt;/code&gt;&lt;br&gt;&#10;&lt;code&gt;net = train(net, in_train', out_train');&lt;/code&gt;&lt;br&gt;&#10;&lt;code&gt;out_test = net(in_test')';&lt;/code&gt;  &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Least Squares: $y = w_0 + \sum_i{x_i*w_i}$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;w = in_train \ out_train;&lt;/code&gt;&lt;br&gt;&#10;&lt;code&gt;out_test = in_test * w;&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Taking into account that NN approach has thresholds as non-linear operators, I tried to add non-Gaussian noise,  and non-linear input - out artificial dependencies. But couldn't find conditions in which NN would be better than LS.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-11T22:53:17.197" Id="49785" LastActivityDate="2013-02-12T13:45:23.737" LastEditDate="2013-02-11T23:19:32.893" LastEditorUserId="7290" OwnerUserId="20745" PostTypeId="1" Score="3" Tags="&lt;matlab&gt;&lt;neural-networks&gt;&lt;least-squares&gt;" Title="When would a neural network outperform an OLS estimate?" ViewCount="186" />
  
  
  <row Body="&lt;p&gt;To my knowledge lmer is not having an &quot;easy&quot; way to address this. Also given that in most cases lmer makes heavy use of sparse matrices for Cholesky factorization I would find it unlikely that it allows for totally unstructured VCV's. &lt;/p&gt;&#10;&#10;&lt;p&gt;To your address your question on &quot;default structure&quot;: there is not a concept of default; depending on how you define your structure, you use that structure. Eg. using random effects like : $(1|RandEff_1)+(1|RandEff_2)$ where each random effect has 3 levels will result in unnested and independent random effects and a diagonal random effects VCV matrix of the form:&lt;/p&gt;&#10;&#10;&lt;p&gt;$R = \begin{bmatrix} &#10;\sigma_{RE1}^2 &amp;amp;  0&amp;amp; 0 &amp;amp; &amp;amp; 0 &amp;amp; 0  &amp;amp; 0\\   &#10;0 &amp;amp;  \sigma_{RE1}^2&amp;amp; 0 &amp;amp; &amp;amp; 0 &amp;amp; 0  &amp;amp; 0\\ &#10;0 &amp;amp;  0&amp;amp; \sigma_{RE1}^2 &amp;amp; &amp;amp; 0 &amp;amp; 0  &amp;amp; 0\\ &#10;0&amp;amp;  0&amp;amp; 0 &amp;amp; &amp;amp;  \sigma_{RE2}^2 &amp;amp; 0 &amp;amp; 0 \\&#10;0 &amp;amp;  0&amp;amp; 0 &amp;amp; &amp;amp; 0 &amp;amp;  \sigma_{RE2}^2 &amp;amp; 0\\&#10;0&amp;amp;  0&amp;amp; 0 &amp;amp; &amp;amp;  0&amp;amp; 0 &amp;amp; \sigma_{RE2}^2  \\\end{bmatrix}$&lt;/p&gt;&#10;&#10;&lt;p&gt;All is not lost with LME's though:&#10;You can specify these VCV matrix attributes &quot;easily&quot; is you are using the R-package MCMCglmm. Look at the &lt;a href=&quot;http://cran.r-project.org/web/packages/MCMCglmm/vignettes/CourseNotes.pdf&quot; rel=&quot;nofollow&quot;&gt;CourseNotes.pdf&lt;/a&gt;, p.70. In that page it does give some analogues on how lme4 random effects structure would be defined but as you'll see yourself, lmer is less flexible than MCMCglmm in this matter.&lt;/p&gt;&#10;&#10;&lt;p&gt;Half-way there is problem nlme's lme corStruct classes, eg. &lt;em&gt;corCompSymm&lt;/em&gt;, &lt;em&gt;corAR1&lt;/em&gt;, etc. etc. &lt;a href=&quot;http://stats.stackexchange.com/questions/5344/how-to-choose-nlme-or-lme4-r-library-for-mixed-effects-models&quot;&gt;Fabian's response&lt;/a&gt; in this tread gives some more concise examples for lme4-based VCV specification but as mentioned before they are not as explictly as those in MCMCglmm or nlme.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-02-12T01:57:04.723" Id="49793" LastActivityDate="2013-02-13T08:45:56.237" LastEditDate="2013-02-13T08:45:56.237" LastEditorUserId="11852" OwnerUserId="11852" ParentId="49775" PostTypeId="2" Score="3" />
  
  
  <row Body="&lt;p&gt;It doesn't matter so much that the Z score is often compared to a symmetrical normal distribution.  The key thing about your proposed approach is that it will give you a positive value when the partner has an &quot;above mean&quot; (common sense term = &quot;above average&quot;) number of repeat visits, pages per visit, or time per page.  So long as you are aware that this is what it is doing, it's not necessarily a bad approach.&lt;/p&gt;&#10;&#10;&lt;p&gt;You might want to consider alternative cut off points - for example, the &lt;em&gt;median&lt;/em&gt; of each of these variables is likely to be lower than the mean; if you used this as the cut-off point instead you would be getting the best half of partners against each criterion.  However, any cut-off point is arbitrary and its use depends on whether the results are practicable (ie does it give you a reasonable number of partners to use).&lt;/p&gt;&#10;&#10;&lt;p&gt;So the short answer to your question is - there is no problem with using these Z scores even when the underlying distribution is skewed.  Just be aware that a positive Z score means that partner has a value higher than the &lt;em&gt;mean&lt;/em&gt; for that variable, nothing else.  And the mean is susceptible to outliers ie a single partner with a squillion repeat visits will result in the mean being so high that only that partner makes your list.  So watch out for that problem and consider using another cut-off (median, or 75th percentile) instead.  Ultimately, the answer depends on your business drivers.&lt;/p&gt;&#10;&#10;&lt;p&gt;The next step up in analytical techniques is to find a single criterion against which to rank partners, which somehow takes into account all three of the variables you are interested in.  A common naive way to go about this is to take averages of standardised scores; more sophisticated alternatives are to use principal components analysis or factor analysis.  But this takes us away from the actual question.&lt;/p&gt;&#10;&#10;&lt;p&gt;My most important tip - use graphical techniques, particularly scatterplots showing two variables at a time with a point representing each partner; ideally with the more interesting points neatly labelled for you (the number one feature lack of Excel, unfortunately).  A &quot;scatterplot matrix&quot; is a handy technique if you have the software to do it easily.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-12T09:40:13.933" Id="49809" LastActivityDate="2013-02-12T09:45:47.203" LastEditDate="2013-02-12T09:45:47.203" LastEditorUserId="7972" OwnerUserId="7972" ParentId="49629" PostTypeId="2" Score="3" />
  <row AnswerCount="1" Body="&lt;p&gt;I have dataset with both linear and quadratic relationships for my response variable among individuals. My dataset includes individuals sampled from two populations (9 individuals from population A and 8 from population B). For each individual I have measured stable nitrogen-isotopes from 9 sequentially grown wing feathers (time series).&lt;/p&gt;&#10;&#10;&lt;p&gt;I have two hypotheses:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;their are differences in the mean isotope values across the wing for individuals between the two regions&lt;/li&gt;&#10;&lt;li&gt;their differences in the variation of the isotope values across the wing for individuals between the two regions&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I must admit I am a novice to mixed models. Originally I had falsely assumed that my data for each individual were 'linear'. Thanks to Ben Bolker I now know how to test these assumptions and have discover one individual from the 17 has a quadratic relationship. I had built the following GLMMs using the 'nlme' package in 'R' before discovering my error:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;model1 &amp;lt;- lme(Delta15N ~ factor(Population), method = &quot;REML&quot;, data = Data, random = ~ 1 | Individual, correlation = corAR1(form = ~ 1 | Individual))&#10;&#10;model2 &amp;lt;- lme(Delta15N ~ factor(Population)*Feather, method = &quot;REML&quot;, data = Data, random = ~ 1 | Individual, correlation = corAR1(form = ~ 1 | Individual))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Can you please suggest a reference or example code that I might use to correctly model my data?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-12T11:15:29.440" Id="49816" LastActivityDate="2013-02-12T11:36:36.667" LastEditDate="2013-02-12T11:22:11.293" LastEditorUserId="7928" OwnerUserId="7928" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;nonlinear-regression&gt;&lt;glmm&gt;" Title="Can I use a non-linear mixed model for data containing both linear and quadratic relationship?" ViewCount="191" />
  
  
  
  
  
  <row Body="" CommentCount="0" CreationDate="2013-02-12T22:41:20.277" Id="49878" LastActivityDate="2013-02-12T22:41:20.277" LastEditDate="2013-02-12T22:41:20.277" LastEditorUserId="-1" OwnerUserId="-1" PostTypeId="5" Score="0" />
  <row Body="&lt;p&gt;For finite mixture models, I would recommend McLachlan and Peel's &lt;em&gt;Finite Mixture Models&lt;/em&gt;, though it might be a bit dated and not particularly social sciencey.&lt;/p&gt;&#10;&#10;&lt;p&gt;For a quick intro to get the basic idea and play around in Stata, take a look at &lt;a href=&quot;http://www.soph.uab.edu/files/16/deb_slides.pdf&quot; rel=&quot;nofollow&quot;&gt;Partha Deb's slides&lt;/a&gt; and &lt;code&gt;fmm&lt;/code&gt; from SSC. Lots of economics examples here, but the halibut length with unobserved gender is a timeless classic.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-13T02:28:42.840" Id="49888" LastActivityDate="2013-02-13T03:02:30.767" LastEditDate="2013-02-13T03:02:30.767" LastEditorUserId="7071" OwnerUserId="7071" ParentId="49885" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;How about: teaching machines to learn&lt;/p&gt;&#10;&#10;&lt;p&gt;Recognise meaningful patterns in data : data mining&lt;/p&gt;&#10;&#10;&lt;p&gt;Predict outcome from known patterns : ML&lt;/p&gt;&#10;&#10;&lt;p&gt;Find new features to remap raw data : AI&lt;/p&gt;&#10;&#10;&lt;p&gt;This bird brain really needs simple definitions.&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2013-02-13T06:14:04.810" CreationDate="2013-02-13T06:14:04.810" Id="49892" LastActivityDate="2013-02-13T06:14:04.810" OwnerUserId="20794" ParentId="5026" PostTypeId="2" Score="0" />
  <row AnswerCount="1" Body="&lt;p&gt;I have a cross-sectional data set with about 8000 observations on child obesity (eg BMI). This data was collected in 8 countries and within schools (about 200 schools), i.e. observations are clustered within schools and countries. I am interested in how child characteristics (e.g. socio economic status) relate to child obesity. My approach was to estimate a pooled OLS and clustering at the school level and also including country dummy variables in the regressions. As I have many and reasonably large school clusters, I thought this would be a good approach. I have however been told that I should estimate a FE effect model. Is it possible to estimate a country fixed-effects model and cluster at the school level?  &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-02-13T09:29:41.733" Id="49898" LastActivityDate="2013-02-15T17:00:41.830" LastEditDate="2013-02-13T14:28:41.217" LastEditorUserId="5739" OwnerUserId="20796" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;multilevel-analysis&gt;&lt;panel-data&gt;&lt;clustered-standard-errors&gt;" Title="Clustered (multilevel) data and fixed effects" ViewCount="2301" />
  
  
  
  
  <row AcceptedAnswerId="50233" AnswerCount="2" Body="&lt;p&gt;I am building an interactive forecast tool (in python) as an aid to forecasting that is done in my organisation. To date the forecast process has been largely human driven, with forecasters assimilating the data in their natural neural networks and using their learned gut feel to make predictions. From a long term forecast verification and predictive modelling study I've done I've found what you might expect; different forecasters exhibit different biases, the effects of some predictors seem to be overstated and other important ones seem to be ignored and in general the forecast performance is mediocre compared with relatively simple empirical models.&lt;/p&gt;&#10;&#10;&lt;p&gt;The forecasts will continue to be manual, but I am trying to build a useful tool to provide the forecasters with a better quantification of the relative effects of predictors. There are also important effects such as seasonal influences that are often overlooked that I would like the tool to highlight to the user. I am expecting a degree of backlash and scepticism about the modelling process from some of the more 'experienced' forecasters (many of whom have little formal knowledge of statistics), so the communication is at least as important and the model performance itself in terms of achieving a measurable improvement in forecast accuracy.&lt;/p&gt;&#10;&#10;&lt;p&gt;The models I'm developing have a strong auto-regressive component that is at times modified significantly by events which show up as measured values in some predictors that are, during non-event times, close to zero. This accords with the mental model that forecasters use. The key part is being able to demonstrate which of the 'event' measurements are most influential in driving the prediction away from the auto-regressive value for any given forecast. I imaging the process in this way; the forecaster divines their best guess value, the model suggests a different one and the forecaster asks why. The model replies something like &quot;see here, this value of this predictor increases the forecast value in Summer. If it was Winter, it would move the other way. I know there are these other measurements, but they have much less effect than this one&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, imagine the model was a simple linear regression. One could imagine displaying the relative 'effect' of event based predictors by multiplying the value by the model co-efficient and displaying as a simple bar chart. All the bars from the different predictors add up to the total deviation from the AR value, and this succinctly and clearly shows the ones that are, in this instance, having a strong influence.&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem is that the process being forecast displays a high degree of non-linearity in the predictors, or at least, I have had much more success with black-box non-linear machine learning algorithms (random forest and GBM) than with GLMs for this data-set. Ideally I would like to be able to seamlessly change the model working 'under the hood' without the user experience changing, so I need some generic way of demonstrating in a simple fashion the importance of the different measurements without using some algorithm specific approach. My current approach will be to quasi-linearise the effects by setting all values to zero except for one predictor, record the predicted deviation and then repeat for all predictors, displaying the results in the bar chart mentioned above. In the presence of strong non-linearity, this may not work so well. Are there any known approaches for achieving in a clear way what I am trying to do here?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-02-14T01:50:18.927" FavoriteCount="2" Id="49959" LastActivityDate="2013-02-18T16:21:44.493" OwnerUserId="10354" PostTypeId="1" Score="6" Tags="&lt;data-visualization&gt;&lt;communication&gt;" Title="How could I visualise the importance of different inputs to the forecast for a black-box non-linear model?" ViewCount="175" />
  <row Body="&lt;p&gt;Check this out: &lt;a href=&quot;http://en.wikipedia.org/wiki/Fisher_information#Matrix_form&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Fisher_information#Matrix_form&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;From the definition, we have&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;  I_{ij} = \mathrm{E}_\theta \left[ \left(\partial_i \log f_{X\mid\Theta}(X\mid\theta)\right) \left(\partial_j \log f_{X\mid\Theta}(X\mid\theta)\right)\right] \, ,&#10;$$&#10;for $i,j=1,\dots,k$, in which $\partial_i=\partial /\partial \theta_i$. Your expression for $I_{ij}$ follows from this one under regularity conditions.&lt;/p&gt;&#10;&#10;&lt;p&gt;For a nonnull vector $u = (u_1,\dots,u_k)^\top\in\mathbb{R}^n$, it follows from the linearity of the expectation that&#10;$$&#10;  \sum_{i,j=1}^k u_i I_{ij} u_j = \sum_{i,j=1}^k \left( u_i \mathrm{E}_\theta \left[ \left(\partial_i \log f_{X\mid\Theta}(X\mid\theta)\right) \left(\partial_j \log f_{X\mid\Theta}(X\mid\theta)\right)\right] u_j \right) \\&#10;= \mathrm{E}_\theta \left[ \left(\sum_{i=1}^k u_i \partial_i \log f_{X\mid\Theta}(X\mid\theta)\right) \left(\sum_{j=1}^k u_j \partial_j \log f_{X\mid\Theta} (X\mid\theta)\right)\right] \\&#10;= \mathrm{E}_\theta \left[ \left(\sum_{i=1}^k u_i \partial_i \log f_{X\mid\Theta}(X\mid\theta)\right)^2 \right] \geq 0 \, .&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;If this component wise notation is too ugly, note that the Fisher Information matrix $H=(I_{ij})$ can be written as $H = \mathrm{E}_\theta\left[S S^\top\right]$, where the scores vector $S$ is defined as&#10;$$&#10;  S = \left( \partial_1 \log f_{X\mid\Theta}(X\mid\theta), \dots, \partial_k \log f_{X\mid\Theta}(X\mid\theta) \right)^\top \, .&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Hence, $u^\top H u = u^\top \mathrm{E}_\theta[S S^\top] u = \mathrm{E}_\theta[u^\top S S^\top u] = \mathrm{E}_\theta\left[\left( S^\top u \right)^2\right] \geq 0$.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-02-14T02:35:24.347" Id="49961" LastActivityDate="2013-04-28T18:30:08.323" LastEditDate="2013-04-28T18:30:08.323" LastEditorUserId="9394" OwnerUserId="9394" ParentId="49942" PostTypeId="2" Score="9" />
  
  
  <row Body="&lt;p&gt;Given the answers and comments on both sites, you're probably done with your homework. The following is just a clarification for future visitors.&lt;/p&gt;&#10;&#10;&lt;p&gt;This is your statistical model: you have random variables $Y_1,\dots,Y_n$, which are independent and identically distributed, with $Y_i\sim\mathrm{Beta}(\theta +1,1)$, for $\theta&amp;gt;-1$.&lt;/p&gt;&#10;&#10;&lt;p&gt;An unbiased estimator $\hat{\theta}_n=\hat{\theta}_n(Y_1,\dots,Y_n)$ of the parameter $\theta$, by definition, must satisfy $\mathrm{E}_\theta[\hat{\theta}_n]=\theta$, for every $\theta$. The original question is wrong, because the estimator obtained by applying the method of moments to this problem is not unbiased.&lt;/p&gt;&#10;&#10;&lt;p&gt;An estimator $\hat{\theta}_n$ of the parameter $\theta$ is (weakly) consistent if $\hat{\theta}_n \stackrel{P_\theta}{\longrightarrow} \theta$, for every $\theta$, and &lt;strong&gt;strongly&lt;/strong&gt; consistent if $\hat{\theta}_n \longrightarrow \theta$, a.s. $[P_\theta]$, for every $\theta$. In what follows, we compute the method of moments estimator of $\theta$ and prove that it is strongly consistent (which entails that it is weakly consistent).&lt;/p&gt;&#10;&#10;&lt;p&gt;To obtain the estimator, note that first population moment is $\mathrm{E}_\theta[Y_1] =(\theta+1)/(\theta+2)$, and the first sample moment is $\bar{Y}_n=(Y_1+\dots+Y_n)/n$. Equating both moments we find the estimator&#10;$$&#10;  \hat{\theta}_n = \frac{2\bar{Y}_n-1}{1-\bar{Y}_n} \, .&#10;$$&#10;Since the function $t\mapsto (2t-1)/(1-t)$ is continuous in the appropriate domain, by the Strong Law of Large Numbers we have&#10;$$&#10;  \hat{\theta}_n \longrightarrow \frac{2\mathrm{E}[Y_1]-1}{1-\mathrm{E}[Y_1]} = \theta \, ,&#10;$$&#10;almost surely $[P_\theta]$, for every $\theta$. Therefore, the method of moments estimator $\hat{\theta}_n$ is a consistent estimator of the parameter $\theta$.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-02-14T12:44:11.280" Id="49984" LastActivityDate="2013-02-15T00:27:23.103" LastEditDate="2013-02-15T00:27:23.103" LastEditorUserId="9394" OwnerUserId="9394" ParentId="49965" PostTypeId="2" Score="4" />
  <row AnswerCount="3" Body="&lt;p&gt;Likelihood methods have many desired properties. Sadly, local maxima in finite samples is not one of them. The fact a local maximum exists near the true parameter value is of no comfort if one converges to different local maxima. &lt;/p&gt;&#10;&#10;&lt;p&gt;I am looking for works reviewing this problem, demonstrating it, and offering solutions. &lt;/p&gt;&#10;" CommentCount="9" CreationDate="2013-02-14T14:17:22.030" FavoriteCount="1" Id="49987" LastActivityDate="2014-12-09T07:32:21.413" OwnerUserId="6961" PostTypeId="1" Score="4" Tags="&lt;maximum-likelihood&gt;&lt;method-of-moments&gt;" Title="Local maxima anomalies of likelihood methods" ViewCount="217" />
  
  <row Body="&lt;p&gt;I think you probably want a &lt;a href=&quot;http://en.wikipedia.org/wiki/Friedman_test&quot; rel=&quot;nofollow&quot;&gt;Friedman test&lt;/a&gt;.  This is designed for just this case.  It is a non-parametric equivalent of an ANOVA  for where the data are rankings.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can implement this in R using friedman.test() which comes with the base distribution.  However, you need some additional information not contained in your summary table.  You will need to present the data with 112 rows and 4 columns (one for each QC sample), with the cell being the rank from one to four of each sample for that day.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-14T18:26:48.287" Id="50000" LastActivityDate="2013-02-17T01:39:21.857" LastEditDate="2013-02-17T01:39:21.857" LastEditorUserId="7972" OwnerUserId="7972" ParentId="45218" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="50014" AnswerCount="1" Body="&lt;p&gt;I have a philosophical question regarding omitted variable bias.&lt;/p&gt;&#10;&#10;&lt;p&gt;We have the typical regression model (population model)&#10;$$&#10;Y= \beta_0 + \beta_1X_1 + ... + \beta_nX_n + \upsilon, &#10;$$&#10;where the samples are from $(Y,X_1,...,X_n)$, and then a bunch of conditions by which the OLS estimates behave quite well.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then we know that, if we omit one of the main variables, $X_k$, this might bias the estimates of $\beta_0, \beta_1, ..., \beta_{k-1}, \beta_{k+1}, ..., \beta_n$. This would affect, at least, the estimated effect of the rest of the variables on $Y$, and also the hypothesis tests about $beta_1,beta_2, ...$, as the predicted values are not reliable.&lt;/p&gt;&#10;&#10;&lt;p&gt;The thing is, we don't know which variables are in the true population model.  Instead, we have a bunch of candidates from which we should analyze and find out the most appropriate subset. This process of variable selection uses OLS estimates and hypothesis tests again. Based on that, we reject or include different variables. But since each candidate model is omitting relevant variables (you will never be able to find the true model), wouldn't these decisions be based on biased results? Why then, should we trust them?  &lt;/p&gt;&#10;&#10;&lt;p&gt;(I'm thinking of forward stepwise method, for instance, where you pick 1 variable then add the rest. You compare the models doing inference, and I'm thinking that omitted variables may be disturbing everything.)&lt;/p&gt;&#10;&#10;&lt;p&gt;I was never too worried about this topic until I began thinking of it, and I'm sure I'm wrong somewhere.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2013-02-14T20:05:00.363" FavoriteCount="1" Id="50007" LastActivityDate="2013-02-14T22:44:08.297" LastEditDate="2013-02-14T21:12:56.113" LastEditorUserId="88" OwnerUserId="20854" PostTypeId="1" Score="5" Tags="&lt;regression&gt;&lt;model-selection&gt;&lt;assumptions&gt;&lt;bias&gt;" Title="Omitted variable bias in linear regression" ViewCount="2127" />
  <row Body="&lt;p&gt;Is this much different from doing two local polynomials of degree 2, one for below the threshold and one for above with smooth at $K_i$ points? Here's an example with Stata:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;use votex // the election-spending data that comes with rd&#10;&#10;tw &#10;(scatter lne d, mcolor(gs10) msize(tiny)) &#10;(lpolyci lne d if d&amp;lt;0, bw(0.05) deg(2) n(100) fcolor(none)) &#10;(lpolyci lne d if d&amp;gt;=0, bw(0.05) deg(2) n(100) fcolor(none)), xline(0)  legend(off)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Alternatively, you can just save the lpoly smoothed values and standard errors as variables instead of using &lt;code&gt;twoway&lt;/code&gt;. Below $x$ is the bin, $s$ is the smoothed mean, $se$ is the standard error, and $ul$ and $ll$ are the upper and lower limits of the 95% Confidence Interval for the smoothed outcome. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lpoly lne d if d&amp;lt;0, bw(0.05) deg(2) n(100) gen(x0 s0) ci se(se0)&#10;lpoly lne d if d&amp;gt;=0, bw(0.05) deg(2) n(100) gen(x1 s1) ci se(se1)&#10;&#10;/* Get the 95% CIs */&#10;forvalues v=0/1 {&#10;    gen ul`v' = s`v' + 1.95*se`v' &#10;    gen ll`v' = s`v' - 1.95*se`v' &#10;};&#10;&#10;tw &#10;(line ul0 ll0 s0 x0, lcolor(blue blue blue) lpattern(dash dash solid)) &#10;(line ul1 ll1 s1 x1, lcolor(red red red) lpattern(dash dash solid)), legend(off)  &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;As you can see, the lines in the first plot are the same as in the second.&lt;/p&gt;&#10;" CommentCount="12" CreationDate="2013-02-14T20:28:59.337" Id="50008" LastActivityDate="2013-02-15T22:06:50.393" LastEditDate="2013-02-15T22:06:50.393" LastEditorUserId="7071" OwnerUserId="7071" ParentId="45184" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="50010" AnswerCount="1" Body="&lt;p&gt;I have a question regarding the interpretation of the whiskers of a boxplot. I have read the following: &quot;On the top and bottom of&#10;the rectangle, the “whiskers” show the range of 1.5 times the distance between the&#10;0.25- and 0.75-quantiles&quot;¨, but do not entirely understand what is meant with &quot;distance&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;It can't be that the probability mass is meant, since between the 0.25 and 0.75 quantile we obviously have always the same percentage of data.&#10;What then is exactly the idea?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-02-14T20:40:55.057" Id="50009" LastActivityDate="2013-02-14T22:30:13.557" LastEditDate="2013-02-14T21:01:57.730" LastEditorUserId="7290" OwnerUserId="20856" PostTypeId="1" Score="4" Tags="&lt;data-visualization&gt;&lt;boxplot&gt;" Title="Understanding the whiskers of a boxplot" ViewCount="119" />
  
  
  
  <row Body="&lt;p&gt;As Charlie suggested, using the weights does give means that &quot;match Match.&quot;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;weighted.mean(lalonde[rr$index.control,]$u75, rr$weights)&#10;weighted.mean(lalonde[rr$index.treated,]$u75, rr$weights)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2013-02-15T03:31:21.407" Id="50029" LastActivityDate="2013-02-15T03:31:21.407" OwnerUserId="8053" ParentId="50013" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;What does uninformative prior mean to you?&lt;/p&gt;&#10;&#10;&lt;p&gt;If you mean the Jeffreys prior, then it is $\beta \sim \textrm{Gamma}(0,0)$ as @Daniel points out.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you mean a flat prior (which isn't uninformative, although it gives the illusion of being uninformative), then it is simply $\beta \sim \textrm{Gamma}(1,0)$, which you can verify by looking at the pdf to be an improper flat prior.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, if there is a big difference between the priors, then you probably don't have enough data.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-15T04:08:15.430" Id="50031" LastActivityDate="2013-02-15T07:52:19.067" LastEditDate="2013-02-15T07:52:19.067" LastEditorUserId="858" OwnerUserId="858" ParentId="50016" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;Using $\text{Gamma}(a,b), \; a\approx b \approx 0 $ is uniform logarithmic-ally. This gives the non-informative prior, in terms of what your model want to do with the input data. &#10;As an example of this, you can see this: &#10;&lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume1/tipping01a/tipping01a.pdf&quot; rel=&quot;nofollow&quot;&gt;http://jmlr.csail.mit.edu/papers/volume1/tipping01a/tipping01a.pdf&lt;/a&gt;&#10;search for &quot;gamma&quot;. &#10;Also check this out: &lt;a href=&quot;http://www.stats.org.uk/priors/noninformative/YangBerger1998.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.stats.org.uk/priors/noninformative/YangBerger1998.pdf&lt;/a&gt; &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-02-15T06:11:11.783" Id="50037" LastActivityDate="2013-02-17T18:28:26.790" LastEditDate="2013-02-17T18:28:26.790" LastEditorUserId="17812" OwnerUserId="17812" ParentId="50016" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;I don't know if this would be computationally equivalent to gung's answer or not, but you could also use logistic regression to predict the probability of heads based on the IV of the dataset source.  Also, a $\chi^2$ test of independence or a Fisher's exact test could be used.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-15T06:28:34.577" Id="50038" LastActivityDate="2013-02-15T06:28:34.577" OwnerUserId="196" ParentId="50030" PostTypeId="2" Score="0" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I have total 19 banks with five years data, comprising 95 observations total. I have 5 independent variables and 1 dependent variable. Can i apply multiple regression analysis by taking all five years data at once or I have to apply multiple regression for each year separately? &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-02-15T11:07:56.970" Id="50054" LastActivityDate="2013-02-15T14:27:55.380" LastEditDate="2013-02-15T12:58:45.703" LastEditorUserId="88" OwnerUserId="17857" PostTypeId="1" Score="0" Tags="&lt;multiple-regression&gt;&lt;panel-data&gt;" Title="Can I apply multiple regression analysis on panel data?" ViewCount="620" />
  <row AnswerCount="0" Body="&lt;p&gt;I want to find the parameters of a model which specifies a set of classification probabilities, for say M classes. (I'll use the parameters in another model later.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Given a set of parameters $\theta$, I can calculate its implied classification probabilities, $\pi(\theta)$, using Monte Carlo integration - $\pi$ is a smooth vector function of $\theta$. I would keep changing the parameters until I get the $\theta(\mathbf{p})$ which generates the classification probabilities I'm looking for - fixed $\mathbf{p}$. (I'll set the seed to control the randomness.)&lt;/p&gt;&#10;&#10;&lt;p&gt;So, I want to use Nelder-Mead to minimise the &quot;distance&quot; between the $\mathbf{\pi(\theta)}$ and $\mathbf{p}$. I would like the distance to be Kullback-Leibler or Kolmogorov-Smirnoff as opposed to least squares for my fitness function.&lt;/p&gt;&#10;&#10;&lt;p&gt;Which fitness function will lead to the solution faster - KS or KL? Also, given that I can evaluate the function to be optimized, but not its derivatives, is Nelder-Mead my best option.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-15T14:26:40.323" FavoriteCount="1" Id="50062" LastActivityDate="2013-08-12T16:06:10.290" LastEditDate="2013-08-12T16:06:10.290" LastEditorUserId="17230" OwnerUserId="13351" PostTypeId="1" Score="4" Tags="&lt;classification&gt;&lt;optimization&gt;&lt;monte-carlo&gt;&lt;kullback-leibler&gt;" Title="Estimating parameters using Kullback-Leibler or Kolmogorov-Smirnoff via Nelder-Mead" ViewCount="133" />
  
  <row AcceptedAnswerId="50084" AnswerCount="1" Body="&lt;p&gt;Im looking for some pointers on how to perform a Monte Carlo simulation. Say im conducting a survey of how late flights are for a certain airline. I amass 1000 records, and plot a cumulative sum of these values. I want to determine how likely it is to have several flights grouped together that a decrease in the cumulative sum would exceed X%. Would i&lt;/p&gt;&#10;&#10;&lt;p&gt;1) calculate the mean and standard deviation of this sample, use these to generate multiple samples, and record the probability of such a decrease? &lt;/p&gt;&#10;&#10;&lt;p&gt;2) take the original sample, shuffle it, and then select values from it (without replacement), and record the probability of such a decrease?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-02-15T18:30:23.230" Id="50081" LastActivityDate="2013-02-15T19:46:41.107" OwnerUserId="17858" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;sampling&gt;&lt;monte-carlo&gt;" Title="Pointers on how to perform a Monte Carlo Analysis" ViewCount="90" />
  
  <row AcceptedAnswerId="50108" AnswerCount="2" Body="&lt;p&gt;Given U is an uniform random variable on [0,1] and F is a cdf with inverse $F^{-1}$. I want to prove that $F^{-1}(U)$ has distribution F. Is the following proof correct:&lt;/p&gt;&#10;&#10;&lt;p&gt;$F_{X}^{-1}(u) = x$ &amp;lt;=&gt; $F_{X}(x) = u$&lt;/p&gt;&#10;&#10;&lt;p&gt;Proof:&lt;/p&gt;&#10;&#10;&lt;p&gt;$F_{X}(x) = F_{U}(u)$ =&gt;&#10;$\int_{0}^{u} P_{u}(u)du = \int_{0}^{1} 1 du = u.$&lt;/p&gt;&#10;&#10;&lt;p&gt;Is this correct?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-02-16T00:25:37.250" Id="50100" LastActivityDate="2013-02-16T09:20:11.037" LastEditDate="2013-02-16T09:20:11.037" LastEditorUserId="20856" OwnerUserId="20856" PostTypeId="1" Score="0" Tags="&lt;simulation&gt;" Title="Simulation of Random Variables (Proof)" ViewCount="88" />
  <row AcceptedAnswerId="50117" AnswerCount="1" Body="&lt;p&gt;Sorry if this has been asked before but I've already done quite a bit of work here and I feel like I'm quite close to an answer. &lt;/p&gt;&#10;&#10;&lt;p&gt;I am interested in testing whether the PHP function &lt;a href=&quot;http://php.net/array_key_exists&quot; rel=&quot;nofollow&quot;&gt;array_key_exists&lt;/a&gt; depends on the size of the array. If it did, we should expect that the growth of the computation time for the function would depend on the size of the array. If it does not, then the computation time should be constant. In other words, I want to test the strength of a possible linear relationship. &lt;/p&gt;&#10;&#10;&lt;p&gt;I &lt;a href=&quot;http://pastie.org/pastes/6190227/text&quot; rel=&quot;nofollow&quot;&gt;measured&lt;/a&gt; the computation time for varying sizes of arrays. A few things I see, in ascending order of importance:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;There lots of points (almost half of them) where the time actually decreases as the size increases. This basically gives me grounds to accept the null hypothesis immediately, because &lt;em&gt;any&lt;/em&gt; linear relationship would entail a monotonically increasing function of the size. &lt;/li&gt;&#10;&lt;li&gt;Very low standard deviation in the time, indicating that the values are tightly clustered.&lt;/li&gt;&#10;&lt;li&gt;The slope of the simple linear regression is quite small.&lt;/li&gt;&#10;&lt;li&gt;Exceedingly low Pearson coefficient and its square has practically vanished. &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;If my null hypothesis is that the &lt;em&gt;real&lt;/em&gt; Pearson coefficient is zero, namely there is no relationship between the array size and computation time, how small does my &lt;em&gt;sample&lt;/em&gt; Pearson coefficient or its square need to be to reject it? What assumptions do I need to make (perhaps acceptable probabilities of Type I and Type II) to do so? &lt;/p&gt;&#10;&#10;&lt;p&gt;Is it better to formulate the null hypothesis from the Pearson coefficient or from the linear regression slope? Are they about the same as far as explanatory power in this circumstance?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-02-16T02:34:00.970" Id="50107" LastActivityDate="2013-02-17T06:17:26.550" OwnerUserId="9936" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;inference&gt;&lt;pearson&gt;" Title="Inference from linear regression slope and Pearson" ViewCount="128" />
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;A report says that 82% of British Columbians over the age of 25 are high school graduates. &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;So population proportion is 0.82&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;A survey of randomly selected residents of a certain city included 1290 who were over the age of 25, and 1012 of them were high school graduates. &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;1012/1290 is the sample proportion, and 1290 is the sample size.&lt;/p&gt;&#10;&#10;&lt;p&gt;But I guess you knew these!&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Is the city's result of 1012 unusually high, low, or neither?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Do a two sided test!&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;How should I approach this question? Any help and hints would be appreciated.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I bet you have already been shown &lt;em&gt;exactly&lt;/em&gt; how to do it.&lt;/p&gt;&#10;&#10;&lt;p&gt;start here:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;https://www.google.com/search?q=one+sample+proportions+test&quot; rel=&quot;nofollow&quot;&gt;https://www.google.com/search?q=one+sample+proportions+test&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;e.g. &lt;a href=&quot;http://www.statisticslectures.com/topics/onesamplezproportions/&quot; rel=&quot;nofollow&quot;&gt;this&lt;/a&gt; has a worked example&lt;/p&gt;&#10;&#10;&lt;p&gt;I'll come back later with more details if you need them.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-02-16T08:52:02.540" Id="50115" LastActivityDate="2013-02-16T08:52:02.540" OwnerUserId="805" ParentId="50112" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;Okay, so I came to this conclusion. Can anyone verify it please?&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Online gradient descent:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;\frac{\partial}{\partial w_{j,k}}J =\frac{\partial}{\partial w_{j,k}}\frac{1}{2}||XW-Y||_F^2+\lambda\sum_{i=1}^m||W_i-\frac{1}{m}\sum_{s=1}^mW_s||^2_2&#10;$$&#10;let $&#10; z_{i,k} = \sum_{j=1}^dx_{i,j,k}w_{j,k} - y_{i,k}&#10;$&#10;$$&#10;=\frac{\partial}{\partial w_{j,k}}\frac{1}{2}\left(\sqrt{\sum_{k=1}^{m}\sum_{i=1}^{n_{k}}|z_{i,k}|^2}\right)^2+\lambda\sum_{k=1}^m\left(\sqrt{\sum_{j=1}^d|w_{j,k}-\frac{1}{m}\sum_{s=1}^mw_{j,s}|^2}\right)^2&#10;$$&#10;$$&#10;=\frac{\partial}{\partial w_{j,k}}\frac{1}{2}{\sum_{k=1}^{m}\sum_{i=1}^{n_{k}}z_{i,k}^2}+\lambda\sum_{k=1}^m(\sum_{j=1}^d(w_{j,k}-\frac{1}{m}\sum_{s=1}^mw_{j,s})^2)&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;\frac{\partial}{\partial w_{j,k}}J= \sum_{i=1}^{n_{k}}((\sum_{j=1}^dx_{i,j,k}w_{j,k}-y_{i,k}) x_{i,j,k})+\lambda2(w_{j,k}-\frac{1}{m}\sum_{s=1}^mw_{j,s})(1-\frac{1}{m})&#10;$$&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Batch gradient descent&#10;$$&#10;\frac{\partial}{\partial W}J= (XW-Y)X + 2\lambda\sum_{i=1}^m(W_i-\frac{1}{m}\sum_{s=1}^mW_s)&#10;$$&lt;/p&gt;&#10;" CommentCount="11" CreationDate="2013-02-16T12:04:12.120" Id="50122" LastActivityDate="2013-02-18T18:41:02.963" LastEditDate="2013-02-18T18:41:02.963" LastEditorUserId="7255" OwnerUserId="7255" ParentId="50068" PostTypeId="2" Score="0" />
  
  
  <row Body="&lt;p&gt;@Gottfried Helms has given you a good answer.  If you're looking for a slightly more intuitively-accessible interpretation, the standard answer is this:  Imagine regressing A onto C and B onto C, and in both cases saving the residuals.  The partial correlation of A and B controlling for C is the correlation between those two sets of residuals.  In other words, it indexes the strength of the linear association between the portion of the variability in A and B that cannot be accounted for by recourse to variability in C.  This can be contrasted with the part (or semi-partial) correlation in which the residuals for one of A or B is correlated with the other full variable.  For an example of how it can be used, showing that the partial correlation between A and B controlling for C is zero can be part of an argument that the relationship between A and B is fully mediated by C (although this approach will only work in the simplest case, see &lt;a href=&quot;http://www.public.asu.edu/~davidpm/classes/psy536/Baron.pdf&quot; rel=&quot;nofollow&quot;&gt;Baron &amp;amp; Kenny (1986)&lt;/a&gt;, and &lt;a href=&quot;http://davidakenny.net/cm/mediate.htm&quot; rel=&quot;nofollow&quot;&gt;Kenny's mediation webpage&lt;/a&gt;).  If you want a little more information about these topics, I discuss it &lt;a href=&quot;http://stats.stackexchange.com/questions/21350/whats-the-order-of-correlation/21357#21357&quot;&gt;here&lt;/a&gt;, there's a decent &lt;a href=&quot;http://en.wikipedia.org/wiki/Partial_correlation&quot; rel=&quot;nofollow&quot;&gt;Wikipedia page&lt;/a&gt;, and I'm particularly fond of &lt;a href=&quot;http://luna.cas.usf.edu/~mbrannic/files/regression/Partial.html&quot; rel=&quot;nofollow&quot;&gt;this webpage&lt;/a&gt;.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-16T19:12:31.650" Id="50134" LastActivityDate="2013-02-16T21:47:00.020" LastEditDate="2013-02-16T21:47:00.020" LastEditorUserId="7290" OwnerUserId="7290" ParentId="50121" PostTypeId="2" Score="2" />
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to correlate hundreds of geoclimatic variables with genetic data.  When I do individual tests, I find several significant correlations.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then I went ahead and performed PCA on the variables and found that first 5 PCs explain most variance.  The correlations between the PCs and the genetic data reveals very few (as compared to when using individual variables) significant correlations.&lt;/p&gt;&#10;&#10;&lt;p&gt;So I am wondering which results are true. &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-02-16T20:01:27.520" Id="50137" LastActivityDate="2013-02-16T20:01:27.520" OwnerUserId="20909" PostTypeId="1" Score="2" Tags="&lt;correlation&gt;&lt;pca&gt;" Title="PCA or no PCA. Which results are true" ViewCount="82" />
  <row AnswerCount="0" Body="&lt;p&gt;My dependent variable - logincome - and one independent variable - age - are continuous. All other explanatory variables are categorical including BA_degree, race, occupation, region, homeownership.&lt;/p&gt;&#10;&#10;&lt;p&gt;When I run OLS in stata, it reveals very high levels of heteroscedasticity. Do you have any suggestions how to fix this problem? &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-02-16T20:19:13.017" Id="50138" LastActivityDate="2013-02-16T20:19:13.017" OwnerUserId="20912" PostTypeId="1" Score="1" Tags="&lt;categorical-data&gt;&lt;least-squares&gt;&lt;heteroscedasticity&gt;&lt;continuous-data&gt;" Title="High heteroscedasticity level" ViewCount="94" />
  
  <row Body="&lt;p&gt;Parametric does NOT mean &quot;Bayesian based&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is one definition of &quot;parametric statistics&quot;&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Parametric statistics is a branch of statistics that assumes data come&#10;  from a type of probability distribution and makes inferences about the parameters of the distribution&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;(From Wikipedia).&lt;/p&gt;&#10;&#10;&lt;p&gt;As Wikipedia goes on to note, most of the common, elementary statistics are parametric. For example, ordinary least squares regression is parametric. Loess regression is nonparametric.&lt;/p&gt;&#10;&#10;&lt;p&gt;Parametric statistics are usually easier to interpret and may be more powerful (in a statistical sense) but they are based on more assumptions than nonparametric statistics. They vary in their degree of robustness, but are usually less robust than nonparametric statistics.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, the equation derived from ordinary least squares regression is (in most cases, anyway) quite easy to understand. That from a regression involving splines is often much less clear and may require graphical representation to be understood well. &lt;/p&gt;&#10;&#10;&lt;p&gt;Bayesian statistics is something altogether different, having to do with using prior information. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-02-16T21:28:07.647" Id="50142" LastActivityDate="2013-02-16T21:28:07.647" OwnerUserId="686" ParentId="50141" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;My guess is that you only ran these simulations once.  If you run them a few times the results will vary.  You might get a smaller coefficient for the first one.  But in general the reason for this pattern is because the true correlation for the underlying population is 0 and your simulations are following the &lt;a href=&quot;http://en.wikipedia.org/wiki/Law_of_large_numbers&quot; rel=&quot;nofollow&quot;&gt;law of large numbers&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-02-17T08:47:20.583" Id="50173" LastActivityDate="2013-02-17T13:01:11.797" LastEditDate="2013-02-17T13:01:11.797" LastEditorUserId="601" OwnerUserId="601" ParentId="50171" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;&lt;strong&gt;Partial answer to question 1:&lt;/strong&gt; I have not really checked but I think that one simply obtains a conjugate family for model $(**)$ by considering the normal distributions on the within subspace (the range of the $X$ matrix) as prior distributions (conditionally to $\Sigma$) for the within parameter ${\boldsymbol \beta}$. Practically it suffices to project the posterior distributions of model $(*)$ on the within subspace, and that should be the Jeffreys prior of model $(**)$ when taking the Jeffreys prior for model $(*)$. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Question 1bis&lt;/strong&gt;: I am not entirely satisfied by the previous answer. What about the posterior distribution of $\Sigma$ ? (I will think later about this question because I'm too hungry now)&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Partial answer to question 1bis&lt;/strong&gt;: Indeed, the partial answer to question 1 seems to be wrong. The Jeffreys posterior for the covariance matrix the multinormal model is inverse-Wishart with a mean related to the frequentist estimate $\hat\Sigma$ of $\Sigma$ in model $(*)$.  Here we should have instead the variation around the fitted values of each response (see question 2bis), which are not the means of each response when there is a within-design.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Partial answer to question 2:&lt;/strong&gt; Since the within parameters are simply estimated by applying a linear projection to the estimated parameters of model $(*)$, the least-squares theory of model $(**)$ is inherited from the least-squares theory of model $(*)$. No ?&#10;And about $\hat\Sigma$ in model $(**)$ it should be $\frac{1}{n-1}({\boldsymbol y} - {\boldsymbol 1}_n \otimes X\hat{\boldsymbol \beta}){({\boldsymbol y} - {\boldsymbol 1}_n \otimes X\hat{\boldsymbol \beta})}'$ where $X \hat{\boldsymbol \beta} = P \hat{\boldsymbol \mu}^*$, $\hat{\boldsymbol \mu}^*$ is the estimate of ${\boldsymbol \mu}$ in model $(*)$, and $P$ is the projection on the range of $X$. In the example below, &lt;code&gt;gls()&lt;/code&gt; returns $\frac{1}{n}({\boldsymbol y} - {\boldsymbol 1}_n \otimes X\hat{\boldsymbol \beta}){({\boldsymbol y} - {\boldsymbol 1}_n \otimes X\hat{\boldsymbol \beta})}'$ as the maximum likelihood estimate of $\Sigma$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Below is a R code for the Bayesian analysis using the &lt;code&gt;bayesm&lt;/code&gt; package, with the timepoints example of my OP.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;################################&#10;####     SIMULATED DATA     ####&#10;################################&#10;library(mvtnorm)&#10;&#10;m &amp;lt;- 3 #  number of timepoints&#10;n &amp;lt;- 24 #  number of individuals&#10;timepoints &amp;lt;- c(1,2,3) &#10;dat &amp;lt;- data.frame(Subject=gl(n,m), timepoint=rep(timepoints,n))  &#10;&#10;# model parameters &#10;alpha &amp;lt;- 0  # within intercept&#10;beta &amp;lt;- 1   # within slope&#10;Mean &amp;lt;- alpha+beta*timepoints&#10;Sigma &amp;lt;- matrix(c(4,1,1,1,2,1,1,1,2), ncol=3)/2&#10;&#10;# simulated responses&#10;set.seed(666)&#10;dat$response &amp;lt;- c(t(rmvnorm(n=n, mean=Mean, sigma=Sigma)))&#10;&#10;# data in wide format&#10;wdat &amp;lt;- reshape(dat, direction=&quot;wide&quot;, v.names=&quot;response&quot;, idvar=&quot;Subject&quot;, timevar=&quot;timepoint&quot;)&#10;&#10;&#10;################################&#10;## BAYESIAN ANALYSIS          ##&#10;## WITH JEFFREYS PRIOR        ##&#10;## IGNORING THE WITHIN DESIGN ##&#10;################################&#10;library(bayesm)  &#10;&#10; ##### set data parameters #####&#10; Y &amp;lt;- as.matrix(wdat[c(&quot;response.1&quot;,&quot;response.2&quot;,&quot;response.3&quot;)]) &#10; X &amp;lt;- model.matrix(Y~1) # matrix of covariates&#10; p &amp;lt;- dim(X)[2] # number of covariates&#10; ##### (noninformative) prior parameters #####&#10; theta0 &amp;lt;- matrix(rep(0,p*m), ncol=m)&#10; A0 &amp;lt;- 0.0001*diag(p)&#10; nu0 &amp;lt;- 0&#10; Omega0 &amp;lt;- matrix(rep(0,m*m),ncol=m)&#10; ##### posterior simulations #####&#10; n.sims &amp;lt;- 10000 # number of simulations&#10; Beta.sims &amp;lt;- array(NA, dim=c(p,m,n.sims)) # for storing simulations of beta&#10; Sigma.sims &amp;lt;- array(NA, dim=c(m,m,n.sims)) # for storing simulations of Sigma&#10; for(sim in 1:n.sims){&#10;    reg &amp;lt;- rmultireg(Y=Y,X=X,Bbar=theta0,A=A0,nu=nu0,V=Omega0)&#10;    Beta.sims[,,sim] &amp;lt;- reg$B&#10;      Sigma.sims[,,sim] &amp;lt;- reg$Sigma&#10; }&#10;&#10;&amp;gt; # compare expected posterior means with true means:&#10;&amp;gt; rowMeans(Beta.sims, dims = 2)&#10;         [,1]     [,2]     [,3]&#10;[1,] 1.053459 1.956234 2.861015&#10;&amp;gt; Mean&#10;[1] 1 2 3&#10;&amp;gt; # theoretically these are also the frequentist estimates:&#10;&amp;gt; colMeans(Y)&#10;response.1 response.2 response.3 &#10;  1.045015   1.956583   2.858293 &#10;&#10;&amp;gt; # compare posterior median Sigma with true Sigma:&#10;&amp;gt; apply(Sigma.sims, c(1, 2), quantile, probs = 0.5)&#10;          [,1]      [,2]      [,3]&#10;[1,] 2.5455707 0.6033634 0.9018785&#10;[2,] 0.6033634 1.4969095 0.6485045&#10;[3,] 0.9018785 0.6485045 1.0620879&#10;&amp;gt; Sigma&#10;     [,1] [,2] [,3]&#10;[1,]  2.0  0.5  0.5&#10;[2,]  0.5  1.0  0.5&#10;[3,]  0.5  0.5  1.0&#10;&#10;##############################################&#10;## PROJECTIONS OF THE POSTERIOR SIMULATIONS ##&#10;## ON THE WITHIN-DESING SUBSPACE            ##&#10;##############################################&#10;&#10;# projection matrix:&#10;X &amp;lt;- model.matrix(~1+timepoints)&#10;P &amp;lt;- solve((t(X)%*%X))%*%t(X)&#10;rownames(P) &amp;lt;- c(&quot;alpha&quot;,&quot;beta&quot;)&#10;# projections of the posterior means are the posterior within parameters:&#10;sims &amp;lt;- P%*%Beta.sims[1,,]&#10;&#10;&amp;gt; # posterior summaries&#10;&amp;gt; summary(data.frame(alpha=sims[&quot;alpha&quot;,], beta=sims[&quot;beta&quot;,]))&#10;     alpha              beta       &#10; Min.   :-1.5637   Min.   :0.3387  &#10; 1st Qu.:-0.1171   1st Qu.:0.8134  &#10; Median : 0.1472   Median :0.9038  &#10; Mean   : 0.1493   Mean   :0.9038  &#10; 3rd Qu.: 0.4106   3rd Qu.:0.9953  &#10; Max.   : 2.0783   Max.   :1.5786&#10;&#10;&amp;gt; # frequentist estimates&#10;&amp;gt; P%*%colMeans(Y)&#10;           [,1]&#10;alpha 0.1400198&#10;beta  0.9066386&#10;&#10;# note: maximum likelihood estimates from gls() differ from &quot;frequentist estimates&quot; &#10;library(nlme)&#10;fit &amp;lt;-  gls(response ~ timepoint,  data=dat, method=&quot;ML&quot;,  &#10;  correlation=corSymm(form=  ~ timepoint | Subject),  &#10;    weights=varIdent(form = ~1 | timepoint))      &#10;&amp;gt; coef(fit)&#10;(Intercept)   timepoint &#10;  0.1430734   0.9054123 &#10;&#10;&amp;gt; ## the ML estimate of Sigma is the mean variation around the fitted values:&#10;&amp;gt; # calculate this variation &#10;&amp;gt; alpha.f &amp;lt;- (P%*%colMeans(Y))[&quot;alpha&quot;,]&#10;&amp;gt; beta.f &amp;lt;- (P%*%colMeans(Y))[&quot;beta&quot;,]&#10;&amp;gt; fitted.frequentist &amp;lt;- alpha.f + beta.f*timepoints&#10;&amp;gt; crossprod(Y-rep(1,n)%*%t(fitted.frequentist)) &#10;           response.1 response.2 response.3&#10;response.1   54.45126   13.18208   19.65210&#10;response.2   13.18208   32.16598   14.20923&#10;response.3   19.65210   14.20923   22.76616&#10;&amp;gt; # compare:&#10;&amp;gt; getVarCov(fit) * n&#10;Marginal variance covariance matrix&#10;       [,1]   [,2]   [,3]&#10;[1,] 54.451 13.182 19.652&#10;[2,] 13.182 32.166 14.209&#10;[3,] 19.652 14.209 22.766&#10;  Standard Deviations: 7.3791 5.6715 4.7714 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;h1&gt;Update&lt;/h1&gt;&#10;&#10;&lt;p&gt;After these thoughts, I finally &lt;em&gt;conjecture&lt;/em&gt; that the Jeffreys posterior is given by:&#10;\begin{align*}&#10;\Sigma \mid {\boldsymbol y}  &amp;amp; \sim  {\cal IW}_m(n, \Omega_n^{-1}) \\&#10;{\boldsymbol \mu} \mid \Sigma, {\boldsymbol y}&#10;&amp;amp; \sim {\cal N}_{n \times m}\left(X\hat{\boldsymbol \beta}, \Sigma \otimes J_n^{-1} \right)&#10;\end{align*}&#10;with&#10;\begin{align*}&#10;J_n &amp;amp; = {(1,1\ldots,1)}'(1,1\ldots,1) \\&#10;\Omega_n &amp;amp; =  ({\boldsymbol y} -{\boldsymbol 1}_n \otimes  X\hat{\boldsymbol \beta}){({\boldsymbol y} - {\boldsymbol 1}_n \otimes  X\hat{\boldsymbol \beta})}'&#10;\end{align*}&#10;And then the posterior distributions of the within parameters are obtained by applying the projection $P$ on the within subspace to ${\boldsymbol \mu}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;When there is no within design this is the Jeffreys posterior for the multinormal model.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now I don't have the courage to check my conjecture. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-17T13:52:43.217" Id="50178" LastActivityDate="2014-02-27T19:45:45.330" LastEditDate="2014-02-27T19:45:45.330" LastEditorUserId="8402" OwnerUserId="8402" ParentId="50086" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;Pearson correlation is for parametric testing and is more powerful than non-parametirc test. Thus, we opt to use transformation before any non-parametric procedures. Transform your data and get pearsons correlation. That's it.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-02-17T17:01:38.183" Id="50188" LastActivityDate="2013-02-17T17:01:38.183" OwnerUserId="20952" ParentId="22719" PostTypeId="2" Score="-2" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm trying to analyse data for my psych undergrad dissertation but struggling with which tests to use because my project is unlike anything I've had experience with previously (and also I don't have a mind for statistics). I have 200 cases of patients who've attended therapy, and the dates of every session they attended. I made extra variables for the number of days wait between each session but I want to know whether different diagnoses (e.g. depression, anxiety...) had an effect on how often patients came for therapy. The trouble is that the diagnostic groups have very different n in each (there are 5 main diagnostic groups, ranging from n = 109 to n = 3!). I'm not sure if there's anyway to do this? &lt;/p&gt;&#10;&#10;&lt;p&gt;Any advice much appreciated. Thanks!&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-02-17T18:11:59.540" FavoriteCount="0" Id="50193" LastActivityDate="2013-02-17T18:11:59.540" OwnerUserId="20955" PostTypeId="1" Score="2" Tags="&lt;time-series&gt;&lt;group-differences&gt;" Title="What test(s) for my data?" ViewCount="34" />
  <row Body="&lt;p&gt;Yes, this property was proved by Azzalini himself in his paper &quot;A class of distributions which includes the normal ones&quot;. You can find it there as &lt;strong&gt;Property D&lt;/strong&gt;, which is used to prove strong unimodality of the skew normal density.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-02-17T22:43:05.573" Id="50205" LastActivityDate="2013-02-17T22:43:05.573" OwnerUserId="20960" ParentId="50199" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;For the life of me I cannot find a way to solve this question. Any help would be appreciated!&lt;/p&gt;&#10;&#10;&lt;p&gt;From past experience, a professor knows that the test score of students taking a final examination is a random variable with mean 65. Suppose in addition the professor knows that the variance of a student's test score is equal to 30. How many students would have to take the examination so as to ensure, with probability at least 0.8, that the class average would be within 5 of 65?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-02-17T22:53:23.830" Id="50207" LastActivityDate="2013-07-19T06:26:37.530" OwnerUserId="20962" PostTypeId="1" Score="3" Tags="&lt;self-study&gt;" Title="Probability of average given mean and variance" ViewCount="352" />
  
  
  
  <row Body="&lt;p&gt;No such thing as a silly question.  The simple answer is yes, you are correct.&lt;/p&gt;&#10;&#10;&lt;p&gt;More nuanced answer - Y is negatively related to v2, after controlling for v1.  That is, for any given level of v1 and keeping v1 constant, an increase in v2 means a decrease in Y.  Note that the intepretation starts to get complex if v1 and v2 are correlated (positively or negatively).&lt;/p&gt;&#10;&#10;&lt;p&gt;Additional observations, not directly related to answering your question:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Terminology can be confusing.  &quot;General linear model&quot;, a term used by SPSS and in some other quarters, should not be confused with &quot;generalized linear model&quot;.  The &quot;general linear model&quot; refers to just an multivariable regression with a Normally distributed response and a combination of categorical and continuous explanatory variables.  A generalized linear model is a further generalization to allow the response to have a distribution other than Normal, so long as it is in the exponential family; and a non linear &quot;link&quot; to the linear predictor.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;You might want to read some of the wisdom on Cross-Validated and elsewhere about issues with building a model by step-wise selection of variables.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2013-02-18T09:44:21.247" Id="50224" LastActivityDate="2013-02-18T09:44:21.247" OwnerUserId="7972" ParentId="50212" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;Eliminating the n-part in both metrics we can express them as p-norms or &lt;a href=&quot;http://en.wikipedia.org/wiki/Lp_space#The_p-norm_in_finite_dimensions&quot; rel=&quot;nofollow&quot;&gt;$L^p$-norms&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;$\sqrt{n}*RMSE = 2-norm$&lt;/li&gt;&#10;&lt;li&gt;$n*MAE=1-norm$&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;So &lt;a href=&quot;http://en.wikipedia.org/wiki/Lp_space#Relations_between_p-norms&quot; rel=&quot;nofollow&quot;&gt;we can utilize&lt;/a&gt; that&lt;/p&gt;&#10;&#10;&lt;p&gt;$\|x\|_{p+a} \leq \|x\|_{p}$ for any vector x and real numbers p ≥ 1 and a ≥ 0.&lt;/p&gt;&#10;&#10;&lt;p&gt;So&lt;/p&gt;&#10;&#10;&lt;p&gt;$\sqrt{n}*0.8563=\sqrt{n} * RMSE&amp;lt;n*MAE$ &lt;/p&gt;&#10;&#10;&lt;p&gt;=&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;$MAE &amp;gt; \frac{0.8563}{\sqrt{n}} = \frac{0.8563}{\sqrt{1,408,789}}=0.0007214446$&lt;/p&gt;&#10;&#10;&lt;p&gt;which is the &lt;em&gt;lower bound&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Given that $E[X^2] \geq E[X]^2$ (many thanks @Innuo), setting $X=(|y_i-\hat{y}_i|)$ leads us to &lt;/p&gt;&#10;&#10;&lt;p&gt;$RMSE^2 \geq MAE^2$ =&gt; $RMSE \geq MAE$&lt;/p&gt;&#10;&#10;&lt;p&gt;So the RMSE is actually the &lt;em&gt;upper bound&lt;/em&gt; for MAE.&lt;/p&gt;&#10;&#10;&lt;p&gt;So my estimate for MAE is $[0.0007214446,0.8563]$ given RMSE=0.8563 &lt;strong&gt;and&lt;/strong&gt; the model which has been used to calculate the specific RMSE.&lt;/p&gt;&#10;&#10;&lt;p&gt;The general &lt;strong&gt;model-independent&lt;/strong&gt; boundaries of course remain [0,4], where the upper bound has been derived from the fact that $y_i,\hat{y}_i$ is an integer in the range from 1 to 5 (according to the &lt;a href=&quot;http://www.netflixprize.com//rules&quot; rel=&quot;nofollow&quot;&gt;rules of the netflix contest&lt;/a&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;with &lt;/p&gt;&#10;&#10;&lt;p&gt;$\frac{1}{n}*\sum_{i=1}^{n}|5-1|=\frac{n}{n}4=4$&lt;/p&gt;&#10;&#10;&lt;p&gt;assuming that the model is smart enough to not predict ratings out of range.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-02-18T14:50:08.430" Id="50237" LastActivityDate="2013-02-18T16:09:19.953" LastEditDate="2013-02-18T16:09:19.953" LastEditorUserId="264" OwnerUserId="264" ParentId="50231" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;There's a method called predict_probability which returns a distribution for the prediction&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-18T15:08:54.287" Id="50241" LastActivityDate="2013-02-18T15:08:54.287" OwnerUserId="20984" ParentId="32204" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Have you tried &lt;a href=&quot;http://scikit-learn.org/dev/modules/generated/sklearn.ensemble.RandomForestClassifier.html&quot; rel=&quot;nofollow&quot;&gt;scikit-learn&lt;/a&gt; module in python.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can &quot;computer_importance&quot; for the features of its randomForestClassifier&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-02-18T16:21:44.493" Id="50247" LastActivityDate="2013-02-18T16:21:44.493" OwnerUserId="20551" ParentId="49959" PostTypeId="2" Score="1" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to fit the Weibull distribution to a dataset in R. I have a dataset, &quot;loss&quot;, as a .csv file, and use &lt;code&gt;fitdistr(loss,&quot;invexp&quot;)&lt;/code&gt;. I received the message:  &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Failure infitdistr(loss, &quot;invweibull&quot;) : unsupported distribution&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;How can I fit the inverse Weibull Distribution to a dataset in R?&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Thank you! I am wonder both how to get this done in R and why I have an error when using a function fitdistr to make maximum-likelihood estimation. My error message is about function &quot;invweibull&quot; in R. The same problem I have for the frechet distribution. &#10;My dataset is the following: &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;336.893&#10;  2.468.001&#10;  572.651&#10;  426.581&#10;  501.593&#10;  433.167&#10;  708.431&#10;  736.761&#10;  1.405.323&#10;  2.161.790&#10;  492.029&#10;  1.429.564&#10;  585.885&#10;  563.030&#10;  408.345&#10;  4.840.762&#10;  308.791&#10;  1.201.796&#10;  1.463.367&#10;  849.336&#10;  510.141&#10;  282.088&#10;  648.475&#10;  699.729&#10;  849.677&#10;  606.299&#10;  380.988&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I am not sure if I can simply fit a Weibull to the inverse of the observations and say then that parameters are the same for inversed weibull. I use paket &quot;actuar&quot; and function &quot;invweibull&quot; and &quot;frechet&quot; and become a failure unsupported distribution. Simple question is, why the estimations can not be done for this dataset and may be somebody know how is it possible to fit inverse weibull and frechet distributions in R? Thank you very much for your respond to my question!&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-02-18T16:32:44.473" Id="50250" LastActivityDate="2013-02-19T18:42:21.733" LastEditDate="2013-02-19T18:42:21.733" LastEditorUserId="919" OwnerUserId="20988" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;distributions&gt;" Title="Fitting the inverse Weibull distribution to data in R" ViewCount="364" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to work out how to generate numbers from a bivariate distribution (any one but the normal distribution) while still being able to control the correlation between the two variables (let's call them $X$ and $Y$). When generating numbers from a normal distribution it is quite easy, as you just specify the covariance (correlation) in the covariance matrix. I have done this in the normal case and compared the power of three correlation tests for various values of rho, the true correlation. &lt;/p&gt;&#10;&#10;&lt;p&gt;Now I want to do this for some other bivariate distribution, but I don't know how to do it while still knowing the true correlation. One suggestion I was offered was to generate $Z$ and $e$, and then let $V = b*Z + e$, but I tried calculating the correlation when $Z$ and $e$ were both chi square and uniform, but could not figure anything out.&lt;/p&gt;&#10;" ClosedDate="2013-02-18T20:00:56.203" CommentCount="1" CreationDate="2013-02-18T19:27:34.537" Id="50267" LastActivityDate="2013-02-18T20:03:35.437" LastEditDate="2013-02-18T20:03:35.437" LastEditorUserId="7290" OwnerUserId="20995" PostTypeId="1" Score="2" Tags="&lt;probability&gt;&lt;correlation&gt;&lt;simulation&gt;&lt;random-generation&gt;" Title="How to generate a non-normal correlated bivariate distribution" ViewCount="31" />
  
  
  <row Body="&lt;p&gt;The predict command should generate variances and covariances? In which case just generate the correlations manually via the correlation formula.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-02-19T10:47:56.863" Id="50317" LastActivityDate="2013-02-19T10:47:56.863" OwnerUserId="21025" ParentId="45572" PostTypeId="2" Score="0" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm doing some research into factor analysis and I've hit that barrier where I don't know what search terms to use.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm trying to see if something is possible. Basically I have a data set with ~100 numerical variables. Each record also has a target variable which is a category. My ultimate goal is to perform knn on this data set. To reduce the dimensionality I ran PCA which gave a minor performance boost. &lt;/p&gt;&#10;&#10;&lt;p&gt;My understanding is that PCA reduces the data set to variables which capture the most variance in the multi-dimensional data set. But this method doesn't even consider the category that each record has.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there any method to reduce the number of dimensions, or identify key factors that specifically contribute to the target variable?&lt;/p&gt;&#10;&#10;&lt;p&gt;This may be poorly worded (or just an outright stupid question...), so let me know if it doesn't make sense and I'll try to clear it up.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-02-19T13:56:44.783" Id="50329" LastActivityDate="2013-02-19T14:40:01.823" LastEditDate="2013-02-19T14:40:01.823" LastEditorUserId="88" OwnerUserId="21031" PostTypeId="1" Score="2" Tags="&lt;pca&gt;&lt;categorical-data&gt;&lt;dimensionality-reduction&gt;" Title="Factor analysis for categorical target variable" ViewCount="327" />
  <row Body="&lt;p&gt;If $p_{min}$ is not smooth you might do a grid search over a representative sample of $o^v$ values where the number of samples selected is less than V (but still high enough to be sufficient) and are either evenly spaced across the scale or in density space. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-02-19T14:36:50.740" Id="50335" LastActivityDate="2013-02-19T14:36:50.740" OwnerUserId="196" ParentId="50306" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Does 'Coefficient of Variation' fit the bill?&lt;/p&gt;&#10;&#10;&lt;p&gt;%CV = SD/MEAN x 100%&lt;/p&gt;&#10;&#10;&lt;p&gt;Your two samples have a %CV of 0.79% and 9.85% respectively. Suppose your expectation was that the %CV would always be below 1%, then 9.85% could be used to trigger an automated action to search for errors. For example, if the large %CV is actually due to a single error, it is easy to identify as the square of its deviation is the largest of any sample.    &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-19T15:16:50.470" Id="50340" LastActivityDate="2013-02-19T15:16:50.470" OwnerUserId="21033" ParentId="35598" PostTypeId="2" Score="2" />
  
  
  
  <row Body="&lt;p&gt;I think the hint given for this problem is not very helpful. &lt;em&gt;Even&lt;/em&gt; if the joint distribution of the minimum and maximum of two independent $U(0,1)$ random variables has been solved as an example in class or in the textbook, teaching a student to rely on plugging-and-chugging from formulas instead of &lt;em&gt;thinking&lt;/em&gt; about&#10;the problem is very bad pedagogical practice, and even more so in this particular&#10;case because the general result is not too difficult to derive.&lt;/p&gt;&#10;&#10;&lt;p&gt;If $Z = \min(X,Y)$ and $W = \max(X,Y)$, then for $w &amp;gt; z$,&#10;$$\begin{align*}&#10;F_{Z,W}(z,w) &amp;amp;= P\{Z \leq z, W \leq w\}\\&#10;&amp;amp;= P\left[\{X \leq z, Y \leq w\} \cup \{X \leq w, Y \leq z\}\right]\\&#10;&amp;amp;= P\{X \leq z, Y \leq w\} + P\{X \leq w, Y \leq z\} - P\{X \leq z, Y \leq z\}\\&#10;&amp;amp;= F_{X,Y}(z, w) + F_{X, Y}(w,z) - F_{X,Y}(z,z)&#10;\end{align*}&#10;$$ &#10;while for $w &amp;lt; z$,&#10;$$\begin{align*}&#10;F_{Z,W}(z,w) &amp;amp;= P\{Z \leq z, W \leq w\} = P\{Z \leq w, W \leq w\}\\&#10;&amp;amp;= P\{X \leq w, Y \leq w\}\\&#10;&amp;amp;= F_{X,Y}(w,w).&#10;\end{align*}&#10;$$ &#10;Consequently, if $X$ and $Y$ are jointly continuous&#10;random variables, then &#10;$$f_{Z,W}(z,w) = \frac{\partial^2}{\partial z \partial w}F_{Z,W}(z,w) = &#10;\begin{cases}&#10;f_{X,Y}(z,w) + f_{X,Y}(w,z), &amp;amp; \text{if}~w &amp;gt; z,\\&#10;\\&#10;0, &amp;amp; \text{if}~w &amp;lt; z.&#10;\end{cases}&#10;$$&#10;One can even think of this end result geometrically. Consider the&#10;joint density $f_{X,Y}(x,y)$ as a solid (of volume $1$)&#10;sitting on the $x$-$y$ plane. Slice it with a vertical cut&#10;along the line $x=y$ and &lt;em&gt;flip over&lt;/em&gt; the part below the line $x=y$&#10;so that it sits on top of the part above the line $x=y$.&#10;The resulting solid is the joint density of the minimum&#10;and the maximum.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, if the solid is a rectangular parallelepiped&#10;whose base is the square with&#10;vertices $(1,1), (-1,1), (-1,-1), (1,-1)$, the slicing&#10;and flipping over gives a right triangular prism of twice the height&#10;as the parallelepiped whose base has vertices  $(1,1), (-1,1), (-1,-1)$.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;If only the marginal densities are desired and not the&#10;joint density, the solution is even easier for the case&#10;of iid $U(-1,1)$ random variables.  For $-1 \leq z \leq 1$,&#10;$$\begin{align}&#10;1-F_Z(z) = P\{Z &amp;gt; z\} &amp;amp;= P\{\min(X,Y) &amp;gt;z\}\\&#10;&amp;amp;= P\{X &amp;gt;z, Y &amp;gt; z\} = P\{X&amp;gt;z\}P\{Y&amp;gt;z\} = \left(\frac{1}{2}(1-z)\right)^2&#10;\end{align}$$&#10;giving, upon taking the derivative with respect to $z$ that&#10;$$f_Z(z) = \begin{cases}\frac{1-z}{2}, &amp;amp;-1 \leq z \leq 1,\\0, &amp;amp;\text{otherwise.}&#10;\end{cases}$$&#10;Similarly, for $-1 \leq z \leq 1$,&#10;$$\begin{align}&#10;F_W(z) = P\{W \leq w\} &amp;amp;= P\{\max(X,Y) \leq w\}\\&#10;&amp;amp;= P\{X \leq w, Y \leq w\} = P\{X\leq w\}P\{Y\leq w\} &#10;= \left(\frac{1}{2}(w-(-1))\right)^2&#10;\end{align}$$&#10;giving, upon taking the derivative with respect to $w$ that&#10;$$f_W(w) = \begin{cases}\frac{1+w}{2}, &amp;amp;-1 \leq w \leq 1,\\0, &amp;amp;\text{otherwise.}&#10;\end{cases}$$&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-02-20T13:25:22.193" Id="50391" LastActivityDate="2013-02-20T19:17:03.447" LastEditDate="2013-02-20T19:17:03.447" LastEditorUserId="6633" OwnerUserId="6633" ParentId="50376" PostTypeId="2" Score="5" />
  
  <row Body="&lt;p&gt;It is absolutely possible. Both &lt;a href=&quot;http://scikit-learn.org/stable/modules/ensemble.html&quot; rel=&quot;nofollow&quot;&gt;python&lt;/a&gt; and &lt;a href=&quot;http://cran.r-project.org/web/packages/randomForest/index.html&quot; rel=&quot;nofollow&quot;&gt;R&lt;/a&gt; have implementations of random forests for regression.  Perhaps your should change tools?&lt;/p&gt;&#10;&#10;&lt;p&gt;Or you can use &lt;a href=&quot;http://www.rforge.net/rJava/&quot; rel=&quot;nofollow&quot;&gt;rJava&lt;/a&gt; to run R from within Java.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-20T14:38:42.037" Id="50401" LastActivityDate="2013-02-20T14:38:42.037" OwnerUserId="2817" ParentId="50388" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;The catch with learning systems that is not usually mentioned in the books is that your variables should represent data coming from some stationary process or at least a wide sense stationary process (there are some systems trying to overcome this but I will not get into that now). In such cases the mean and variance of the data is assumed to be stable. For that reason mean and variance of the training and test sets should be equivalent. Therefore the initial transformation should work for both sets. If the data is not coming from such a process then you need to find a transformation that does the trick (but most of the time this is not as easy as it sounds).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-20T16:26:14.840" Id="50419" LastActivityDate="2013-02-20T16:26:14.840" OwnerUserId="20980" ParentId="50417" PostTypeId="2" Score="1" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I just got a list of 100 p-values, some of which are associated. For details, these 100 p-values could be separated into 7 groups, each group was calculated from a expression dataset. However, these datasets had 10%~60% overlap among each other, that is the pvalues inside a dataset were independent, but were not across the dataset, which bring the whole 100 p-values not independent.&lt;/p&gt;&#10;&#10;&lt;p&gt;So is there any solutions you know to directly and simply solve this? Some ref. pointed out to use the BH adjustment inside the R package, while choosing a loose criteria, e.g. 0.2, to define the significance level.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;tmp$V7 = p.adjust(tmp$V6,method=&quot;BY&quot;)&#10;tmpV6[1:5]&#10;[1] 0.040461040 0.001250268 0.037407030 0.009361665 0.006866937&#10;tmpV7[1:5]&#10;[1] 0.31423224 0.08659423 0.29570166 0.14456543 0.13611251&#10;tmp$V7 = p.adjust(tmp$V6,method=&quot;BH&quot;)&#10;tmp$V6[1:5]&#10;[1] 0.040461040 0.001250268 0.037407030 0.009361665 0.006866937&#10;tmp$V7[1:5]&#10;[1] 0.05252837 0.01447545 0.04943072 0.02416616 0.02275313&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Please take a look at this, &lt;code&gt;tmp$V6&lt;/code&gt; is the original p-value, while &lt;code&gt;tmp$V7&lt;/code&gt; is after correction.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, it seems that after BY correction, I get even worse result than BH (or fdr) method.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-20T19:25:42.527" Id="50432" LastActivityDate="2015-02-26T07:35:23.060" LastEditDate="2015-02-26T07:35:23.060" LastEditorUserId="56674" OwnerUserId="21069" PostTypeId="1" Score="3" Tags="&lt;multiple-comparisons&gt;&lt;p-value&gt;" Title="Method of p value correction for a set of dependent p values" ViewCount="461" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm a master's level statistician and have been doing logistic regression for a while. I'm helping a friend who is taking an advanced stats course and ran across some terms I'm not familiar with when working with LR. One of the LR homework problems is asking for the model's proportional by chance accuracy rate, full model's accuracy rate, and the model's sensitivity rate. I'm not sure how the sensitivity rate is different than the accuracy rate. Also, the prof wants a PDA model on the same data set to compare to the LR. I'm not sure what a PDA model is or is this a term for something that is usually referred to as something else.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-20T21:30:34.150" Id="50449" LastActivityDate="2013-02-20T21:30:34.150" OwnerUserId="17786" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;logistic&gt;&lt;terminology&gt;" Title="logistic regression sensitivity and other terms" ViewCount="63" />
  <row Body="&lt;p&gt;Why don't you treat every delivery as independent of the others, and use runs per ball instead of runs per over? If you said that the number of runs scored per ball in the first over has an expected value of $0.75$, for example, then after scoring $1$ off the first ball, there are $5$ balls left in the over, so their expected score off the over is $1+ 0.75*5 = 4.75$. Then their expected total has gone up by $0.25$ runs as well.&lt;/p&gt;&#10;&#10;&lt;p&gt;(Note: if you use the normal distribution, the number of runs scored (off a ball or an over) could be negative, and assuming that deliveries or overs are independent of one another is probably wrong. Modelling cricket is interesting, but I wouldn't bet real money on the outcome if you are using assumptions like these!)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-20T21:40:23.713" Id="50451" LastActivityDate="2013-02-21T00:18:47.857" LastEditDate="2013-02-21T00:18:47.857" LastEditorUserId="13818" OwnerUserId="13818" ParentId="50427" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;If you want to force a regression line to go through a single point, that can be done in a roundabout way. Let's say your point is $(x_i,y_i)$.  You just re-center your data with that point as the origin.  That is, you subtract $x_i$ from every $x$-value, and $y_i$ from every $y$-value.  Now the point  is at the origin of the coordinate plane.  Then you simply fit a regression line while suppressing the intercept (forcing the intercept to be (0,0).  Because this is a linear transformation, you can easily back-transform everything afterwards if you want to.  &lt;/p&gt;&#10;&#10;&lt;p&gt;If you want to force a line to go through two points in an X-Y plane, that's pretty easy to do also. Any two points can be fit with a line.  You can use the &lt;a href=&quot;http://en.wikipedia.org/wiki/Linear_equation&quot;&gt;point-slope formula&lt;/a&gt; to calculate your slope, and then use one of the points, the slope, and the &lt;a href=&quot;http://en.wikipedia.org/wiki/Linear_equation&quot;&gt;equation of a line&lt;/a&gt; to find the intercept.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Note that it may not be possible to fit a straight line through three points in a coordinate plane.  However, we can guarantee that they can be fit perfectly with a parabola (that is, using both $X$ and $X^2$).  There is algebra for this too, but as we move up, it might be easier to just fit a model with software by including &lt;em&gt;only&lt;/em&gt; those three (more) points in the dataset.  Similarly, you could get the straight line that best approximates those three points by fitting a model that has access &lt;em&gt;only&lt;/em&gt; to those three points.  &lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;I feel compelled to mention at this point, however, that this may not be a great thing to do (unless your theory provides very solid reasons for doing so).  You may also want to look into &lt;a href=&quot;http://en.wikipedia.org/wiki/Bayesian_linear_regression&quot;&gt;Bayesian regression&lt;/a&gt;, where you can allow your model to find the best combination of the information in your data, and some prior information (which you could use to strongly bias your intercept towards zero, for example, without quite forcing it).  &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-02-20T22:20:57.767" Id="50453" LastActivityDate="2013-02-20T22:20:57.767" OwnerUserId="7290" ParentId="50447" PostTypeId="2" Score="7" />
  <row Body="&lt;p&gt;Why would you use Poisson? If you have $n$ independent observations out of $N$ falling in $(a,b)$ with constant chance for each, wouldn't that be &lt;em&gt;binomial&lt;/em&gt; rather than Poisson?&lt;/p&gt;&#10;&#10;&lt;p&gt;You can think of transforming not $X$ but (assuming invertibility, with $g = f^{-1}$), transforming $(a,b)$;  the probability relates to the cdf of $X$ ($F(a') = F(g(a))$ and so on). The standard error of the binomial is $Np(1-p)$ where $p=F(b)-F(a)$ and $p' = F(b')-F(a')$ where the transformation of the endpoints works as above.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-02-21T00:45:45.633" Id="50473" LastActivityDate="2013-02-21T00:45:45.633" OwnerUserId="805" ParentId="50467" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="50478" AnswerCount="1" Body="&lt;p&gt;I have a dataset like&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;+--------+------+-------------------+&#10;| income | year |        use        |&#10;+--------+------+-------------------+&#10;|  46328 | 1989 | COMMERCIAL EXEMPT |&#10;|  75469 | 1998 | CONDOMINIUM       |&#10;|  49250 | 1950 | SINGLE FAMILY     |&#10;|  82354 | 2001 | SINGLE FAMILY     |&#10;|  88281 | 1985 | SHOP &amp;amp; HOUSE      |&#10;+--------+------+-------------------+&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I embed it into a LIBSVM format vector space&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;+1 1:46328 2:1989 3:1&#10;-1 1:75469 2:1998 4:1&#10;+1 1:49250 2:1950 5:1&#10;-1 1:82354 2:2001 5:1&#10;+1 1:88281 2:1985 6:1&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Feature indices:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;1 is &quot;income&quot;&lt;/li&gt;&#10;&lt;li&gt;2 is &quot;year&quot;&lt;/li&gt;&#10;&lt;li&gt;3 is &quot;use/COMMERCIAL EXEMPT&quot;&lt;/li&gt;&#10;&lt;li&gt;4 is &quot;use/CONDOMINIUM&quot;&lt;/li&gt;&#10;&lt;li&gt;5 is &quot;use/SINGLE FAMILY&quot;&lt;/li&gt;&#10;&lt;li&gt;6 is &quot;use/SHOP &amp;amp; HOUSE&quot;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Is it OK to train a support vector machine (SVM) with a mix of continuous (year, income) and categorical (use) data like this?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-02-21T00:56:54.157" FavoriteCount="3" Id="50474" LastActivityDate="2013-06-12T09:55:43.317" LastEditDate="2013-06-12T09:55:43.317" LastEditorUserId="21054" OwnerUserId="21076" PostTypeId="1" Score="8" Tags="&lt;categorical-data&gt;&lt;svm&gt;&lt;continuous-data&gt;" Title="Is it OK to mix categorical and continuous data for SVM (Support Vector Machines)?" ViewCount="2073" />
  <row Body="&lt;p&gt;Sofastats looks really well done, and it can import from OpenOffice files.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.sofastatistics.com/&quot; rel=&quot;nofollow&quot;&gt;http://www.sofastatistics.com/&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-21T04:22:09.270" Id="50488" LastActivityDate="2013-02-21T04:22:09.270" OwnerUserId="21091" ParentId="13623" PostTypeId="2" Score="0" />
  <row AnswerCount="2" Body="&lt;p&gt;I want to compare the distributions of $K$ contingency tables, where each table is $N \times M$.&#10;To make things clearer, let's work on $K=M=N=2$.&#10;Suppose I have a classical cases and controls test, where I have $2$ variables, treated or not, and cured or not.&#10;Suppose that besides testing whether the treatment is effective or not, I want to test whether the effect is different among groups, say males and females. Then I would have $2$ $2 \times 2$ tables, one for males, one for females. I want to test whether the distribution of the tables is independent of sex.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a standard test for this?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-21T05:19:46.017" Id="50490" LastActivityDate="2013-04-11T11:57:01.267" LastEditDate="2013-03-12T10:47:23.530" LastEditorUserId="805" OwnerUserId="5859" PostTypeId="1" Score="2" Tags="&lt;hypothesis-testing&gt;&lt;chi-squared&gt;&lt;contingency-tables&gt;" Title="Comparing contingency table distribution" ViewCount="196" />
  <row Body="&lt;p&gt;This is a good problem to view through a time series lense.  Effectively you are trying to forecast X deliveries ahead.  In the beginning you have no information other than past games, although there must be ways to determine which games are most relevant.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Each extra delivery you get is an actual observation in a time series which can use to refine your model.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-21T06:49:13.403" Id="50495" LastActivityDate="2013-02-21T06:49:13.403" OwnerUserId="7972" ParentId="50427" PostTypeId="2" Score="1" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am taking a business statistics course and I can't seem to figure out how to find the missing figures in the following problem; here's the question:&lt;/p&gt;&#10;&#10;&lt;p&gt;Using the data below, determine the trial central line and trial control limits for the Xbar chart. Subgroup size is 5. For the R chart, the trial central line is 0.11, the UCL is 0.23, and LCL is 0. Note not all of the subgroups are included in the data; however, the sums are correct. (5 subgroups)&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    X-Bar    R&#10;1   2.08    .05&#10;2   2.09    .24&#10;3   2.08    .10&#10;*   *       *&#10;*   *       *&#10;*   *       *&#10;25  2.09    .11&#10;    =52.0   =2.75&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="3" CreationDate="2013-02-21T14:14:11.667" Id="50520" LastActivityDate="2013-02-21T16:40:54.203" LastEditDate="2013-02-21T16:40:54.203" LastEditorUserId="919" OwnerUserId="21108" PostTypeId="1" Score="0" Tags="&lt;self-study&gt;&lt;descriptive-statistics&gt;&lt;control-chart&gt;" Title="Control chart Xbar and R missing values" ViewCount="342" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm modeling longitudinal patient level health records data which will have the following structure:&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{equation*}&#10;\text{Prob}(Y_{it}=1 \text{ at time } t \text{ and member } i)=\beta_{i0} +\beta_{i1}x_{i,t-1} + \beta_{i2}x_{i,t-2} + ... + \beta_{ik}x_{i,t-k} + \text{additional covariates for patient age, diagnoses, etc.} \end{equation*}&lt;/p&gt;&#10;&#10;&lt;p&gt;where Y is an inpatient hospitalization indicator variable for time period t and patient i, and the x's measure frequency of home nursing visits during prior time periods. &lt;/p&gt;&#10;&#10;&lt;p&gt;The predictor variables are significantly correlated at the patient level. I'm considering a logistic regression, however the strong correlation between the x's makes things a bit more complicated.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is a time series model appropriate here, or would a multilevel repeated measures model be better (say using proc mixed in SAS with a random intercept for each member)?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-02-21T17:02:14.183" Id="50541" LastActivityDate="2013-02-21T17:02:14.183" OwnerUserId="13634" PostTypeId="1" Score="2" Tags="&lt;time-series&gt;&lt;multilevel-analysis&gt;&lt;random-effects-model&gt;" Title="random effect or time series model?" ViewCount="193" />
  <row AnswerCount="0" Body="&lt;p&gt;I am facing a situation where I need to assess the predictive ability of a single (regression) model. I do not need to compare its performance with some other model, I just need to show that it predicts well or not. I have the Mean Square Error and Mean Absolute error for both on the training set and a separate test set. I also have the actual predictions so I can do some comparisons of the RMSE values with the mean predictions (or something similar) if needed. I understand that this probably depends also on the domain and how large an acceptable error is, but I cannot find any reference that discusses this topic in any level, every discussion on model assessment refers to comparison of different models.&#10;Any help would be greatly appreciated.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-02-21T17:26:06.703" FavoriteCount="1" Id="50548" LastActivityDate="2013-02-21T17:26:06.703" OwnerUserId="20780" PostTypeId="1" Score="1" Tags="&lt;predictive-models&gt;&lt;validation&gt;" Title="Assessment of a single predictive model" ViewCount="77" />
  <row AnswerCount="1" Body="&lt;p&gt;I've been trying to figure this one for weeks so your advise is very appreciated. &#10;I have log returns for the past 5 yrs on the following indexes: NYSE, Communication, Growth and Large Cap. I performed PCA on those and kept 2 ‘unobservable factors’ with loadings as follows:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;NYSE Index  0.2599870   0.6090115&#10;Communications  0.7952713   -0.5660120&#10;Growth          0.3918862   0.2710737&#10;Large Cap   0.3825905   0.4850304&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Orthogonal regression of NYSE returns on 2 factor scores gives me 2 betas equal to factor loadings (0.609 and 0.2599). Therefore betas for NYSE are:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;            NYSE       Index Communications Growth  Large Cap&#10;NYSE Index  0.316      0.337                0.309    0.359&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;respectively &lt;/p&gt;&#10;&#10;&lt;p&gt;To calculate delta R on NYSE in terms of the factors:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Return on NYSE= 0.316* R NYSE + 0.337* R Comm+0.309* R Growth +0.359 R Large Cap.&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;However this doesn’t actually work… What am I missing? How can I calculate Return on NYSE Index using PCA's common factors?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-02-21T18:11:01.310" Id="50556" LastActivityDate="2013-02-21T20:18:18.357" LastEditDate="2013-02-21T20:18:18.357" LastEditorUserId="19879" OwnerUserId="21119" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;time-series&gt;&lt;pca&gt;&lt;interpretation&gt;&lt;risk&gt;" Title="Calculating return from a factor model" ViewCount="70" />
  
  <row Body="&lt;p&gt;There is a blog post that tries to explain why: &lt;a href=&quot;http://www.portfolioprobe.com/2011/01/12/the-number-1-novice-quant-mistake/&quot; rel=&quot;nofollow&quot;&gt;http://www.portfolioprobe.com/2011/01/12/the-number-1-novice-quant-mistake/&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Basically using levels gives you spurious answers because there is no component of the data that is independent across observations.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-21T19:24:10.013" Id="50566" LastActivityDate="2013-02-21T19:24:10.013" OwnerUserId="21125" ParentId="50536" PostTypeId="2" Score="2" />
  <row AnswerCount="0" Body="&lt;p&gt;I have 52 weekly observations of count data that has some spikes and a definite trend (available &lt;a href=&quot;https://dl.dropbox.com/u/37488982/obs-per-week.csv&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;). &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;obs.pw &amp;lt;- read.csv(paste0(download.path,'obs-per-week.csv'))&#10;obs.pw &amp;lt;- obs.pw[ 2:dim(obs.pw)[2]]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The mean weekly values calculated as such (can't post images yet):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;plot(colMeans(obs.pw), xlab=&quot;Weeks&quot;, ylab=&quot;Mean Counts&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The weekly observations (8976 users per week) follow a zero-inflated negative binomial distribution. They are generally about 95% zeros. &lt;/p&gt;&#10;&#10;&lt;p&gt;The summary data for the column sums of this data:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;summary(colSums(obs.pw))&#10;Min. 1st Qu.  Median    Mean 3rd Qu.    Max. &#10;853    1424    1567    1607    1770    2374&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Although the spikes and trend information is interesting it is not what I am interested in exploring now. &lt;/p&gt;&#10;&#10;&lt;p&gt;I am &lt;strong&gt;trying to the segment the non-zero counts for users into different ranges based on how infrequent that observation is&lt;/strong&gt;. I want to transform each observed integer count into ordinal categorical variables. Example variables would be: low, medium or high. &lt;/p&gt;&#10;&#10;&lt;p&gt;A threshold is set for each category and the value is greater than or equal to the threshold the observation is assigned to that category (e.g. if(obs &gt;= 30) cat.obs &amp;lt;- medium). The categories would ideally divide different quantiles of the data.&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem with the spread of points and trend highlighted in the graph means that &lt;strong&gt;these thresholds are time dependent&lt;/strong&gt;. I am using these categorical variables as input to a Hidden Markov Model and want an observation to be just as rare on week 1 as week 50.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have tried dividing each individual observation by the mean observation count for that week but this line still seems to have a trend.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a way to scale these observations with the given mean data, or otherwise so that a threshold on day 1 for a low,med,high observation is roughly the same as for week 50?&lt;/p&gt;&#10;&#10;&lt;p&gt;Or is this the wrong approach entirely?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-21T20:11:38.883" Id="50571" LastActivityDate="2013-02-21T20:11:38.883" OwnerUserId="17188" PostTypeId="1" Score="1" Tags="&lt;hidden-markov-model&gt;&lt;discrete-data&gt;&lt;threshold&gt;" Title="Remove variance from temporal observations (input to HMM emission) by manipulating the individual observations" ViewCount="75" />
  
  
  <row Body="&lt;p&gt;The Theil-Sen estimator is essentially an estimator for the slope alone; the line has been constructed in a host of different ways.&lt;/p&gt;&#10;&#10;&lt;p&gt;In fact there are a large variety of ways to calculate the intercept.&lt;/p&gt;&#10;&#10;&lt;p&gt;You said:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;My understanding of the intercept calculation is that I first calculate the median slope, and then construct a line through every data point with this slope, find the intercept of every line, and then take the median intercept.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;That's one way that it's done.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are a couple of approaches that compute kinds of weighted-medians but based off that same approach (putting more weight on the points further apart in x-space).&lt;/p&gt;&#10;&#10;&lt;p&gt;Another common one (probably the most common) is to compute median($y-bx$). This is what Sen looked at, for example. &lt;/p&gt;&#10;&#10;&lt;p&gt;Another is to try to get an estimator with higher efficiency at the normal (akin to that of the slope estimator in typical situations) and similar breakdown point to the slope estimate (there's probably little point in having better breakdown at the expense of efficiency), such as using the Hodges-Lehmann estimator (median of pairwise averages) on $y-bx$. This has a kind of symmetry in the way the slopes and intercepts are defined ... and generally gives something very close to the LS line when the normal assumptions nearly hold, whereas the Sen-intercept can be - relatively speaking - quite different.&lt;/p&gt;&#10;&#10;&lt;p&gt;And some people just compute the mean.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are still other suggestions that have been looked at. There's really no 'one' intercept to go with the slope estimate.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.stat.ncsu.edu/information/library/mimeo.archive/ISMS_1987_1690R.pdf&quot;&gt;Dietz lists several possibilities&lt;/a&gt;, possibly even including all the ones I mentioned, but that's by no means exhaustive.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-02-22T00:53:58.443" Id="50589" LastActivityDate="2013-02-22T02:53:35.103" LastEditDate="2013-02-22T02:53:35.103" LastEditorUserId="805" OwnerUserId="805" ParentId="50587" PostTypeId="2" Score="7" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I collect numbers from generators that yield different ranges of whole numbers with an unknown distribution. I want to estimate the mean of the numbers outputted by this generator. I'm convinced the distributions are symmetric (more specifically, uniform) though, so I can just average the min and max after I have a decent sample size.&lt;br&gt;&#10;What would be the best way to test that the distribution of numbers is symmetric or uniform?&lt;/p&gt;&#10;&#10;&lt;p&gt;Things I've tried:&lt;br&gt;&#10;-Checked that the mean, median, and midpoint converge to the same values, but I can't quantify how close the values would have to be.&lt;br&gt;&#10;-Histogram, but my sample sizes (n&amp;lt;100 because I must manually collect) are too low for me to tell&lt;br&gt;&#10;-Chi-square test with the frequencies of each number in the range and testing them against the expected value for a uniform distribution, but I've read there are problems with this in other posts&lt;br&gt;&#10;-Pearson's skewness coefficient and comparing that with the standard error of skewness, but I think there are limits to this kind of approach&#10;-Comparing kurtosis of data with that of a uniform distribution&lt;/p&gt;&#10;&#10;&lt;p&gt;I've also read about the Kolmogorov-Smirnov tests and Shapiro-Wilk tests but these seem too complicated for such a simple seeming task..&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-02-22T05:19:06.743" Id="50603" LastActivityDate="2013-02-22T07:46:22.517" OwnerUserId="21138" PostTypeId="1" Score="4" Tags="&lt;distributions&gt;&lt;uniform&gt;&lt;skewness&gt;" Title="How do I test for a symmetric distribution?" ViewCount="610" />
  <row Body="&lt;p&gt;It depends what you want to do with your model later.&lt;/p&gt;&#10;&#10;&lt;p&gt;Joint models attempt to predict the whole distribution over $X$ and $y$. It has some useful properties:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Outlier detection. Samples very unlike your training samples can be identified as they'll have a low marginal probability. A conditional model won't necessarily be bale to tell you this.&lt;/li&gt;&#10;&lt;li&gt;Sometimes it's easier to optimise. If your model was a gaussian mixture model, say, there are well documented ways to fit it to the joint density you can just plug in (expectation maximisation, variational bayes), but things get more complicated it you want to train it conditionally. &lt;/li&gt;&#10;&lt;li&gt;&lt;strong&gt;Depending on the model&lt;/strong&gt;, training can potentially be parallelised by taking advantages of conditional independences, and you may also avoid the need to retrain it later if new data becomes available. E.G. if every marginal distribution $f(X|y)$ is parameterised separately, and you observe a new sample $(X=x_1,y=y_1)$, then the only marginal distribution you need to retrain is $f(X|y=y_1)$. The other marginal distributions $f(X|y=y_2), f(X|y=y_3), \ldots$ are unaffected. This property is less common with conditional models.&lt;/li&gt;&#10;&lt;li&gt;I recall reading a paper which indicated joint models have some other nice properties in cases where there's lots and lots of data, but cannot remember the exact claim, or find it in my big folder of interesting papers. If I find it later I'll put in a reference.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Conditional models however have some interesting properties too&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;They can work really well.&lt;/li&gt;&#10;&lt;li&gt;Some have had a lot of work put in finding sensible optimisation strategies (e.g. support vector machines) &lt;/li&gt;&#10;&lt;li&gt;The conditional distribution is very often `simpler' to model than the joint - to model the latter, you have to model the former &lt;em&gt;as well as&lt;/em&gt; modelling the marginal distribution. If you're only interested in getting accurate predictions of what value $y$ is for a given $X$, it can be more sensible to concentrate your model's capacity on representing this alone.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="1" CreationDate="2013-02-22T11:16:42.523" Id="50616" LastActivityDate="2013-02-22T11:16:42.523" OwnerUserId="16765" ParentId="50485" PostTypeId="2" Score="3" />
  
  
  
  <row Body="&lt;p&gt;We consider a mixed model with random slopes and random intercepts. Given that we have only one regressor, this model can be written as&#10;$$ y_{ij}= \beta_0 + \beta_1 x_{ij} + u_{0j}+u_{1j}x_{ij}+\epsilon_{ij},&#10;$$&#10;where $y_{ij}$ denotes the $i$-th observation of group $j$ of the response, and $x_{ij}$  and $\epsilon_{ij}$ the respective predictor and error term. &lt;/p&gt;&#10;&#10;&lt;p&gt;This model can be expressed in matrix notation as follows: &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\mathbf{Y}=\mathbf{X}\beta + \textbf{Zb} + \epsilon,$$&#10;which is equivalent to&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\mathbf{Y}= \begin{bmatrix}&#10;\mathbf{X} &amp;amp; \textbf{Z}&#10;\end{bmatrix} \begin{bmatrix}&#10;\beta\\ &#10;\mathbf{b}&#10;\end{bmatrix}+ \epsilon $$&lt;/p&gt;&#10;&#10;&lt;p&gt;Let us assume that we have $J$ groups, i.e. $j=1,\dots,J$ and let $n_j$ denote the number of observations in the $j$-th group. Partitioned for each group, we can write above formula as&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\begin{bmatrix}&#10; \mathbf{Y_1}&#10;\\ \mathbf{Y_2}&#10;\\ \vdots &#10;\\ \mathbf{Y_J}&#10;\end{bmatrix}=\begin{bmatrix}&#10;\mathbf{X_1} &amp;amp; \mathbf{Z_1} &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\ &#10;\mathbf{X_2} &amp;amp;  0 &amp;amp;  \mathbf{Z_2} &amp;amp; 0 &amp;amp; 0 \\ &#10;\vdots &amp;amp;  &amp;amp;  &amp;amp; \dots  &amp;amp;  \\ &#10;\mathbf{X_J} &amp;amp; 0 &amp;amp; 0  &amp;amp; 0 &amp;amp; \mathbf{Z_J}&#10;\end{bmatrix} \begin{bmatrix}&#10;\beta \\ &#10;b_1 \\ &#10;b_2 \\&#10;\vdots \\ &#10;b_J &#10;\end{bmatrix}+\begin{bmatrix}&#10;\epsilon_1 \\ &#10;\epsilon_2 \\ &#10;\vdots \\ &#10;\epsilon_J&#10;\end{bmatrix}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\mathbf{Y_j}$ is a $n_j \times 1 $ matrix containing all observations of the response for group $j$, $\mathbf{X_j}$ and $\mathbf{Z_j}$ are $n_j \times 2 $ design matrices in this case and $\epsilon_j$ is again a $n_j \times 1 $ matrix.&lt;/p&gt;&#10;&#10;&lt;p&gt;Writing them out, we have:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\mathbf{Y_j} = \begin{bmatrix}&#10;y_{1j}\\ &#10;y_{2j}\\ &#10;\vdots \\ &#10;y_{n_jj}&#10;\end{bmatrix},&#10;\mathbf{X_j}=\mathbf{Z_j}=\begin{bmatrix}&#10;1 &amp;amp; x_{1j} \\ &#10;1 &amp;amp; x_{2j} \\ &#10;\vdots &amp;amp; \vdots \\ &#10;1 &amp;amp; x_{n_jj}&#10;\end{bmatrix}$&#10;and&#10;$\epsilon_j = \begin{bmatrix}&#10;\epsilon_{1j}\\ &#10;\epsilon_{2j}\\ &#10;\vdots \\ &#10;\epsilon_{n_jj}&#10;\end{bmatrix}.$&lt;/p&gt;&#10;&#10;&lt;p&gt;The regression coefficient vectors then are &lt;/p&gt;&#10;&#10;&lt;p&gt;$\beta = \begin{pmatrix}&#10;\beta_0 \\ &#10;\beta_1&#10;\end{pmatrix}$,&#10;$b_j=\begin{pmatrix}&#10;u_{0j}\\ &#10;u_{1j}&#10;\end{pmatrix}$ &lt;/p&gt;&#10;&#10;&lt;p&gt;To see that the two model formulations are indeed equivalent, let us look at any of the groups  (let's say the $j$-th one).&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \mathbf{Y_j} = \mathbf{X_j} \beta + \mathbf{Z_j}b_j + \epsilon_j$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Applying above definitions, one can show that the $i$-th row of the resulting vector is just &#10;$$ y_{ij}= \beta_0 + \beta_1 x_{ij} + u_{0j}+u_{1j}x_{ij}+\epsilon_{ij},&#10;$$&#10;where $i$ ranges from $1$ to $n_j$.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-02-22T14:16:44.280" Id="50627" LastActivityDate="2013-02-22T14:47:24.390" LastEditDate="2013-02-22T14:47:24.390" LastEditorUserId="20457" OwnerUserId="20457" ParentId="49935" PostTypeId="2" Score="6" />
  <row AnswerCount="0" Body="&lt;p&gt;I want to predict an event based on transactional data. I'm only interested in predicting either future hazard or time-to-event. My understanding is that to predict in the future I have to use a fully-parameterized proportional hazard model with time-varying covariates. By &quot;fully parameterized&quot; I mean that I assume the underlying distribution of the event in time.&lt;/p&gt;&#10;&#10;&lt;p&gt;So the PH model has two parts: the baseline hazard (whose exact formula I assumed, so this is not a Cox model) and the effet of the covariates (&lt;code&gt;e^(linear combination of the covariates)&lt;/code&gt;). The value of the second part (the effect of the covariates) is varying in time until now; after that, I assume the value of the covariates stays constant. As for the first part, the baseline hazard, I know its function, so I can extrapolate the value of the baseline hazard in the future.&lt;/p&gt;&#10;&#10;&lt;p&gt;My first question: does it make sense to make predictions in the future in that manner with a PH model?&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, I'm interested in predicting not only the hazard but the time-to-event. For that I need to create an AFT model. My understanding is that even though theoretically possible it is computationally too intensive to create an AFT model with time-varying covariates. So I want to build an AFT model with the last slice in time of my transactional data set. But I want to take into account my whole history to get my parameters. The idea would be to obtain the survival function $S(t) = e^{(-h(t) \cdot t)}$ from the hazard function $h(t)$ of a fitted PH model. Isolate t in the formula and express it in function of $S(t)$ and $h(t)$: $T(t)=- \frac{ln(S(t))}{h(t)}$. Then I fix t at the last point in time, so the function doesn't depend on time but is using parameter estimates that were derived by taking time into account. It is important to note that I'm not actually fitting the AFT model. The reason why I'm trying to use the estimates obtained with the fully-parameterized PH model with time-varying covariates into a non-fitted AFT model is that I believe these betas are &quot;better&quot; since they were derived based on a larger data set that takes the time dimension into account.&lt;/p&gt;&#10;&#10;&lt;p&gt;My second question is: can I do that or is there a silly step in the above development?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance for your help and sorry for the long question!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-22T14:51:50.717" FavoriteCount="2" Id="50631" LastActivityDate="2014-01-18T10:02:14.633" LastEditDate="2014-01-18T10:02:14.633" LastEditorUserId="35842" OwnerUserId="21152" PostTypeId="1" Score="2" Tags="&lt;time-series&gt;&lt;survival&gt;&lt;cox-model&gt;&lt;hazard&gt;" Title="Survival analysis - Using parameter estimates of a fully-parameterized PH model into an AFT model" ViewCount="105" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have a combinatorics question. Say you have two sequences:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$X_{1},X_{2},X_{3},\ldots,X_{N_{1}}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and&lt;/p&gt;&#10;&#10;&lt;p&gt;$$Y_{1},Y_{2},Y_{3},\ldots,Y_{N_{2}}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;How can i pair up elements from the $X$'s with elements of the $Y$'s, such that if I pair up $Y_{1}$ with $X_{3}$, then I can only pair up $Y_{2}$ with $X_{n}$ such that $n$ is greater than the index of the $X$ paired up with the previous $Y$, in this case $n&amp;gt;3$.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-22T19:34:24.657" Id="50645" LastActivityDate="2013-09-10T16:08:46.013" LastEditDate="2013-09-10T16:08:46.013" LastEditorUserId="29617" OwnerUserId="8089" PostTypeId="1" Score="2" Tags="&lt;combinatorics&gt;" Title="Ordered combinations" ViewCount="42" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I have got a set of data and decided to do a Poisson regression so $log(\lambda)=X\beta$. &#10;I wonder if there is any test available for such purpose?&#10;The first thought is to use the Durbin-Watson test statistics to test the independence of data.&#10;My primary concern is that since the mean and variance is not the same for each observation so if the Durbin-Watson statistic is correct?&#10;If not, is there any suggestions?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-02-23T08:02:37.190" Id="50677" LastActivityDate="2013-02-23T23:14:21.710" OwnerUserId="20801" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;generalized-linear-model&gt;&lt;poisson&gt;" Title="testing independent of data assumption, before doing Poisson Regression" ViewCount="355" />
  <row Body="&lt;p&gt;I'm just going to throw out three more approaches, mostly for the reaction, since these methods are newer and I'm curious about them...&lt;/p&gt;&#10;&#10;&lt;p&gt;First, the &lt;strong&gt;MIC&lt;/strong&gt; (maximal information coefficient) introduced by Reshef et al. (2011) to determine nonlinear correlations. Described as 'a correlation for the 21st century' when it made a splash in the media.&lt;/p&gt;&#10;&#10;&lt;p&gt;Second, the distance correlation statistic, &lt;strong&gt;dCor&lt;/strong&gt;, a standardized Brownian covariance introduced by Szekely, Rizzo, and Bakirov (2007) and favoured by Simon &amp;amp; Tibshirani (2012) in their critique of the low power of MIC.&lt;/p&gt;&#10;&#10;&lt;p&gt;Third, the &lt;strong&gt;HHG&lt;/strong&gt; test of Heller, Heller and Gorfine (2012a, b), noted by them as an superior alternative to MIC with better power characteristics. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# Using @Jeromy Anglim's gist on github with the data&#10;library(RCurl)&#10;writeChar(con=&quot;data.txt&quot;, getURL(&quot;https://raw.github.com/gist/1320989/40be602c43b5f29d79af50bf2b63ba6c1a839807/data.txt&quot;, ssl.verifypeer = FALSE))&#10;&#10;# read downloaded data into R&#10;x &amp;lt;- read.table(&quot;data.txt&quot;)&#10;names(x) &amp;lt;- c(&quot;x&quot;, &quot;y&quot;)&#10;x$logx &amp;lt;- log(x$x)&#10;x$logy &amp;lt;- log(x$y)&#10;&#10;## maximal information coefficient on untransformed data&#10;library(minerva)&#10;with(x, mine(x, y))&#10;$MIC&#10;    [1] 0.4333141&#10;    # maximal information coefficient on log-log data&#10;    with(x, mine(logx, logy))&#10;    $MIC&#10;[1] 0.4333141&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;MIC&lt;/strong&gt;: No difference between untransformed and log transform data. Quite similar to the absolute value of the pearson on log-log data.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;## distance correlation statistic on untransformed data&#10;library(&quot;energy&quot;)&#10;with(x, dcor(x, y)) &#10;[1] 0.2352139&#10;# distance correlation statistic on log-log data&#10;with(x, dcor(logx, logy))&#10;[1] 0.3638021&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;dCor&lt;/strong&gt;: Lower values than MIC, and sensitive to log transform. Similar to the spearman correlation. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; ## HHG test&#10;download.file(&quot;http://www.math.tau.ac.il/~ruheller/Software/HHG2x2_0.1-1.tar.gz&quot;, &quot;HHG2x2_0.1-1.tar.gz&quot;)&#10;install.packages(&quot;HHG2x2_0.1-1.tar.gz&quot;, repos = NULL, type=&quot;source&quot;)&#10;library(HHG2x2)&#10;writeChar(con=&quot;myHHG.R&quot;, getURL(&quot;https://raw.github.com/andrewdyates/HHG_R/master/R/myHHG.R&quot;, ssl.verifypeer = FALSE))&#10;source(&quot;myHHG.R&quot;)&#10;&#10;xs &amp;lt;- x[sample(nrow(x), 50), ] # crashed with the full dataset...&#10;Dx = as.matrix(dist((xs[,1]),diag=TRUE,upper=TRUE))&#10;Dy = as.matrix(dist((xs[,2]),diag=TRUE,upper=TRUE))&#10;myHHG(Dx,Dy); pvHHG(Dx,Dy)&#10;$sum_chisquared&#10;[1] 8374.196&#10;&#10;$sum_lr&#10;[1] 3890.457&#10;&#10;$max_chisquared&#10;[1] 21.28747&#10;&#10;$max_lr&#10;[1] 10.80188&#10;&#10;$pv&#10;[1] 9.998e-05&#10;&#10;$output_monte&#10;[1] 10001&#10;&#10;$A_threshold&#10;[1] 2.985682&#10;&#10;$B_threshold&#10;&#10;[1] -4.553877&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;HHG&lt;/strong&gt;: Actually I'm not sure how to get a comparable distance metric out of the HHG...&lt;/p&gt;&#10;&#10;&lt;p&gt;References:&lt;/p&gt;&#10;&#10;&lt;p&gt;Gorfine, M., Heller, R., &amp;amp; Heller, Y. (2012a). Comment on “Detecting Novel Associations in Large Data Sets”. Preprint, available at the website &#10;&lt;a href=&quot;http://iew3.technion.ac.il/~gorfinm/files/science6.pdf&quot; rel=&quot;nofollow&quot;&gt;http://iew3.technion.ac.il/~gorfinm/files/science6.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Heller, R., Heller, Y., &amp;amp; Gorfine, M. (2012b). A consistent multivariate test of association based on ranks of distances. Biometrika, arXiv preprint arXiv:1201.3522.&lt;/p&gt;&#10;&#10;&lt;p&gt;Reshef, D. N., Y. A. Reshef, et al. (2011). &quot;Detecting Novel Associations in Large Data Sets.&quot; Science 334(6062): 1518-1524.&lt;/p&gt;&#10;&#10;&lt;p&gt;Simon, Noah and Robert Tibshirani (2012). Comment On Detecting Novel Associations In Large Data Sets By Reshef et al, Science Dec 16, 2011. &#10;www-stat.stanford.edu/~tibs/reshef/comment.pdf&lt;/p&gt;&#10;&#10;&lt;p&gt;Szekely, G.J., Rizzo, M.L., and Bakirov, N.K. (2007), Measuring and Testing Dependence by Correlation of Distances, Annals of Statistics, Vol. 35 No. 6, pp. 2769-2794. &#10;&lt;a href=&quot;http://dx.doi.org/10.1214/009053607000000505&quot; rel=&quot;nofollow&quot;&gt;http://dx.doi.org/10.1214/009053607000000505&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-23T09:55:15.953" Id="50680" LastActivityDate="2013-02-25T04:54:09.187" LastEditDate="2013-02-25T04:54:09.187" LastEditorUserId="7744" OwnerUserId="7744" ParentId="17618" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Odd as it seems, I think these participants &lt;em&gt;are&lt;/em&gt; matched pairs. This is similar to a study in which both eyes of various people are studied - in which case the right eye would match with the left eye. The idea is that the variables are matched, not necessarily that the subjects are. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-23T15:53:59.963" Id="50694" LastActivityDate="2013-02-23T15:53:59.963" OwnerUserId="686" ParentId="50686" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;For count dependent variables the usual choices are Poisson or negative binomial regression (usually the latter). If there are excess zeroes, there are various choices such as zero-inflated models and hurdle models. &lt;/p&gt;&#10;&#10;&lt;p&gt;Why are you using year as a categorical variable? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-23T17:13:45.217" Id="50701" LastActivityDate="2013-02-23T17:13:45.217" OwnerUserId="686" ParentId="50699" PostTypeId="2" Score="2" />
  <row Body="&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;If you mean to perform 180 statistical tests to compare two data points, then there is no suitable statistics. However, if the sampling is random, then your two data points are the available best guess of the two means you'd like to compare. You may simply perform a subtraction and see which one is higher.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;If you mean to compare the effect of two conditions applied to 180 different chemicals. For instance, you expose 180 chemicals to i) nothing and ii) high energy environment, either heat or radiation and then measure something. Then there are tests available.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;The difference lies between whether you only have n=1 in each of the two environments or n=180 in each of the two environments. Either way, you'll need to clarify the design in your question.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-23T17:42:50.480" Id="50702" LastActivityDate="2013-02-23T17:42:50.480" OwnerUserId="13047" ParentId="50696" PostTypeId="2" Score="1" />
  <row Body="Experimental design, or design of experiments (DOE), is the design of any information-gathering exercises where variation is present. In statistics, controlled experiments are usually implied, which involve an &quot;experimental group&quot; and a &quot;control group&quot;.
&#10;In the design of experiments, the experimenter is often interested in the effect of some process or intervention (the &quot;treatment&quot;) on some objects (the &quot;experimental units&quot;)." CommentCount="0" CreationDate="2013-02-23T21:59:44.533" Id="50711" LastActivityDate="2013-02-23T22:01:15.593" LastEditDate="2013-02-23T22:01:15.593" LastEditorUserId="20943" OwnerUserId="20943" PostTypeId="4" Score="0" />
  <row AcceptedAnswerId="50724" AnswerCount="1" Body="&lt;p&gt;Here is the scenario:&lt;/p&gt;&#10;&#10;&lt;p&gt;I am iteratively seeking a goal state by selecting from one of a set of sets of options.  The set of sets defines the possible routes to the goal state, and each inner set defines the probability I will succeed, and what the failure penalties and probabilities are.  A failure costs me a variable amount of score, which is also a given.  &lt;/p&gt;&#10;&#10;&lt;p&gt;My objective is not only to achieve the goal state, but to do so with minimum score.  &lt;/p&gt;&#10;&#10;&lt;p&gt;For example, say I can select from two groups:&lt;/p&gt;&#10;&#10;&lt;p&gt;In group 1, I have a 25% chance of success.  However, I have a 50% chance of failing and adding 5 to my score, and a 25% chance of failing and adding 10 to my score.&lt;/p&gt;&#10;&#10;&lt;p&gt;In group 2, I have a 50% chance of success.  However, I have a 10% chance of failing and adding 5 to my score, a 30% chance of failing and adding 15 to my score, and a 10% chance of failing and adding 25 to my score.&lt;/p&gt;&#10;&#10;&lt;p&gt;If I wish to succeed while minimizing my score, which of these two groups should I repeatedly select from?  &lt;/p&gt;&#10;&#10;&lt;p&gt;I'm looking for a general solution to this problem, where there may be an arbitrary number of groups, and each group has an arbitrary number of outcomes.  As in the example, for each group, their relative probabilities of success and failure are known, along with the points added for failing.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I &lt;em&gt;think&lt;/em&gt; I want to optimize for &quot;mean score for success&quot; in the group in order to minimize my overall score.  However, I'm a bit confused because it seems to me that if the distribution of possible final scores is flat enough, I may be better off accepting a higher mean score from my selected group.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-02-23T22:26:55.063" Id="50715" LastActivityDate="2013-02-24T03:55:20.467" OwnerUserId="10173" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;games&gt;" Title="How do I optimize this scenario to achieve the lowest score?" ViewCount="62" />
  
  <row AcceptedAnswerId="50742" AnswerCount="3" Body="&lt;p&gt;I'm looking for a quick (event if not so accurate) way to deal with over-sampled curves using R. Consider the following example in which &lt;code&gt;x&lt;/code&gt; contains 1000 values in the range [0, 1) and &lt;code&gt;y&lt;/code&gt; is some function of &lt;code&gt;x&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x &amp;lt;- sort(runif(1000))&#10;y &amp;lt;- sin(x)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I'm looking for a way to take only 100 points on this curve, preferably in equal intervals, without knowing the actual model that produces &lt;code&gt;y&lt;/code&gt;'s from &lt;code&gt;x&lt;/code&gt;'s. My naiive approach is to find indices of original &lt;code&gt;x&lt;/code&gt; values that are closest to the new ones:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x.new &amp;lt;- seq(0, 1, 0.01)&#10;y.new &amp;lt;- c()&#10;for(x.val in x.new){&#10;  ix &amp;lt;- which.min((x.val - x) ** 2) #closest value in x&#10;  y.new &amp;lt;- c(y.new, y[ix])&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Or, using a &lt;code&gt;sapply&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;find.closest.y &amp;lt;- function(v){&#10;  return(y[which.min((v - x)**2)])&#10;}&#10;&#10;y.new &amp;lt;- sapply(x.new, find.closest.y)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I'm OK with the accuracy of this method, but I suspect that there is a way to acheive this using functions from standard R libraries. Am I right? Is there a simpler method to downsample curves?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-02-24T07:14:03.940" FavoriteCount="0" Id="50729" LastActivityDate="2013-02-24T18:33:35.923" OwnerUserId="3184" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;sampling&gt;&lt;resampling&gt;" Title="Downsampling excessively sampled curves" ViewCount="442" />
  <row AcceptedAnswerId="50743" AnswerCount="2" Body="&lt;p&gt;I'm trying to show that if $X_n \rightarrow X $ in $r$-th mean, then $E|X_n|^r \rightarrow E|X|^r$. (Edit: should have said with $r \ge 1$)&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is whether the following steps are sufficient or not:&lt;/p&gt;&#10;&#10;&lt;p&gt;$[E|X_n - X|^r]^{\frac{1}{r}} \rightarrow 0$ as $n \rightarrow \infty$&lt;/p&gt;&#10;&#10;&lt;p&gt;Setting $r=1$&lt;/p&gt;&#10;&#10;&lt;p&gt;$E|X_n - X| \rightarrow 0$ as $n \rightarrow \infty$&lt;/p&gt;&#10;&#10;&lt;p&gt;And so:&lt;/p&gt;&#10;&#10;&lt;p&gt;$E|X_n| \rightarrow E|X|$ as $n \rightarrow \infty$&lt;/p&gt;&#10;&#10;&lt;p&gt;Hence &lt;/p&gt;&#10;&#10;&lt;p&gt;$E|X_n|^r \rightarrow E|X|^r$ as $n \rightarrow \infty$&lt;/p&gt;&#10;&#10;&lt;p&gt;Thoughts appreciated.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-24T12:31:43.163" Id="50737" LastActivityDate="2013-02-24T17:16:50.427" LastEditDate="2013-02-24T17:02:30.977" LastEditorUserId="16663" OwnerUserId="16663" PostTypeId="1" Score="0" Tags="&lt;self-study&gt;&lt;convergence&gt;" Title="Implication of convergence in rth mean" ViewCount="49" />
  
  
  
  <row Body="&lt;p&gt;In addition to what Sven Hohenstein said, the &lt;code&gt;mtcars&lt;/code&gt; data is &lt;strong&gt;not balanced&lt;/strong&gt;. Usually one uses &lt;code&gt;aov&lt;/code&gt; for lm with categorical data (which is just a wrapper for &lt;code&gt;lm&lt;/code&gt;) which specifically says on &lt;code&gt;?aov&lt;/code&gt;:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;aov is designed for balanced designs, and the results can be hard to&#10;  interpret without balance: beware that missing values in the&#10;  response(s) will likely lose the balance.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I think you can also see this on the weird correlations of the model matrix:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;mf &amp;lt;- model.matrix(mpg ~ cyl, data = mtcars)&#10;cor(mf)&#10;            (Intercept)       cyl6       cyl8&#10;(Intercept)           1         NA         NA&#10;cyl6                 NA  1.0000000 -0.4666667&#10;cyl8                 NA -0.4666667  1.0000000&#10;Warning message:&#10;In cor(mf) : the standard deviation is zero&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Hence, the standard errors obtained from &lt;code&gt;aov&lt;/code&gt; (or &lt;code&gt;lm&lt;/code&gt;) will likely be bogus (you can check this if you compare with &lt;code&gt;lme&lt;/code&gt; or &lt;code&gt;lmer&lt;/code&gt; standard errors.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-02-24T23:25:27.800" Id="50773" LastActivityDate="2013-02-24T23:25:27.800" OwnerUserId="442" ParentId="50623" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Those three factors you mention are IVs (predictors) not DVs (responses). &lt;/p&gt;&#10;&#10;&lt;p&gt;The DV &quot;Change detection accuracy&quot; (e.g. see fig 3) looks like a count variable (a sum of binary variables, if you like), will be both discrete and heteroskedastic, and may be skew as well - and strictly should probably be handled as a GLM. If the samples are large and the proportions don't vary too much it may not be a very serious problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;The DV &quot;Response time&quot; (e.g. see fig 4) is continuous, but is likely to be highly skewed and heteroskedastic (so again, likely not well suited for normal theory ANOVA). &lt;em&gt;Speeds&lt;/em&gt; (inverse of times) are often much more reasonable on normal assumptions (but even there you can get some degree of skewness or heteroskedasticity). How much of an issue the time variable is it's impossible to judge without looking at the data.&lt;/p&gt;&#10;&#10;&lt;p&gt;The DV &quot;Number of saccades&quot; is another count variable, and the earlier comments apply&lt;/p&gt;&#10;&#10;&lt;p&gt;The DV &quot;Saccade distance&quot; may be right skew and mildly heteroskedastic, but probably not as badly as the time variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;First fixation&quot; looks like another count variable, ...&lt;/p&gt;&#10;&#10;&lt;p&gt;.... and so on.&lt;/p&gt;&#10;&#10;&lt;p&gt;Some of these might be okay handled via ANOVA, I can't tell without the data.&lt;/p&gt;&#10;&#10;&lt;p&gt;Effect sizes and so on will be fine; the significance levels are what you have to worry about.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;rolando2 said:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Homo- or heteroskedasticity is a property of a solution (some would say of a &quot;model&quot;), not of a single variable.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I'm bringing this up here because I want to discuss it in some depth.&lt;/p&gt;&#10;&#10;&lt;p&gt;Heteroskedasticity is not a property of a single variable, but &lt;em&gt;we're not dealing with a single variable&lt;/em&gt;.       &lt;/p&gt;&#10;&#10;&lt;p&gt;We're discussing ANOVA: the existence of a model is already a given - in the sense that we're modeling relations among means of different groups - we have both y and x-variables (the x's will be factors in ANOVA of course).&lt;/p&gt;&#10;&#10;&lt;p&gt;And that's enough variables to have heteroskedasticity, if it's present. &lt;/p&gt;&#10;&#10;&lt;p&gt;Since count variables seem to nearly always show variability that relates to the mean in some way (in particular, among groups with very large fitted means compared to other groups, we also see larger variation around the mean), it's important to bring it up.&lt;/p&gt;&#10;&#10;&lt;p&gt;That this happens is to be expected, since counts are &lt;em&gt;bounded&lt;/em&gt;. They cannot go below 0. &lt;/p&gt;&#10;&#10;&lt;p&gt;Consider a simple one-way ANOVA with (for clarity) multiple count observations per group. Moving across groups, as the mean gets closer to 0, the variability of points about the mean must also tend to be smaller - they are less and less able to go far below the current mean (because of the bound) and so by necessity must not go too far above it (or the mean would tend to be larger than it is). &lt;/p&gt;&#10;&#10;&lt;p&gt;As a result, when there are count dependent variables, you nearly always find variance that's somewhat related to the mean in the data (not necessarily linearly) -- as long as there's variation in the mean among the groups, we would expect there to be accompanying changes in variances, simply because of the basic (and obvious) case that counts are bounded below.&lt;/p&gt;&#10;&#10;&lt;p&gt;Which is to say, in the situation in the question, if there's to be variation in the mean (&amp;amp; that expectation is why ANOVA was considered in the first place), then we have every ingredient needed for us to expect heteroskedasticity to be present - if the means differ between the groups, we should typically expect there to be changes in variance.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-02-25T00:10:37.153" Id="50776" LastActivityDate="2014-07-25T00:44:36.053" LastEditDate="2014-07-25T00:44:36.053" LastEditorUserId="805" OwnerUserId="805" ParentId="50770" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;For the standard deviation, you used the population standard deviation, I think the sample standard deviation would be more appropriate (divide by n-1 instead of n).&lt;/p&gt;&#10;&#10;&lt;p&gt;As for the last question, my understanding is that it asks, given the normal distribution with the parameters you have estimated, what are the probabilities of observing a weight : 1) 16 or less and 2) 46 or more&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-02-25T00:19:11.117" Id="50777" LastActivityDate="2013-02-25T00:19:11.117" OwnerUserId="21232" ParentId="50775" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;I think I've figured it out now. As stated in the comments, I think there is indeed a problem with the estimators.&#10;I found another &lt;a href=&quot;http://stats.stackexchange.com/a/12255/19731&quot;&gt;answer on using a linear estimator&lt;/a&gt; for a similar problem. If I didn't make any mistakes, the estimator for $\mu$ in my case should be $$\hat{\mu} =  \frac{ \sum_{i=1..n} z_i/(C+\Sigma_i)}{ \sum_{i=1..n} 1/(C+\Sigma_i)}$$ &#10;Since I don't actually know $C$, I guess I could use the estimate $\hat{C}$ based on the samples. Based on the Wikipedia Article on &lt;a href=&quot;http://en.wikipedia.org/wiki/Weighted_mean#Weighted_sample_variance&quot; rel=&quot;nofollow&quot;&gt;weighted mean&lt;/a&gt; and then subtracting the covariance influence from the $\Sigma_i$, I can estimate the covariance as&#10;$$\hat{C} = \left(\frac{\sum w_i}{(\sum w_i)^2 - \sum w_i^2} \sum w_i(z_i - \hat{\mu}) \right) - \frac{n}{\sum w_i}$$&#10;where $w_i = 1/\Sigma_i$.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-25T08:50:19.317" Id="50794" LastActivityDate="2013-02-25T08:50:19.317" OwnerUserId="19731" ParentId="50348" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;In terms of the Dirichlet prior, I believe it's saying that you have a set of $n$ variables which are all percentages/proportions between 0 and 1 and all add up to 1. (That is $x_1 \dots x_n$ where $0 \le x_i \le 1$ and $\sum x_i = 1$.) In the case of HMM's, that could be used to model the probability of transitioning to one of $n$ possible states, or the probability of emitting one of $n$ possible symbols.&lt;/p&gt;&#10;&#10;&lt;p&gt;The &lt;a href=&quot;http://en.wikipedia.org/wiki/Dirichlet_distribution&quot; rel=&quot;nofollow&quot;&gt;Dirichlet wikipedia page&lt;/a&gt; says it pretty well, especially the section entitled &quot;Conjugate to categorical/multinomial&quot;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-25T19:51:22.187" Id="50826" LastActivityDate="2013-02-25T20:49:57.270" LastEditDate="2013-02-25T20:49:57.270" LastEditorUserId="1764" OwnerUserId="1764" ParentId="50825" PostTypeId="2" Score="2" />
  
  <row AcceptedAnswerId="50855" AnswerCount="1" Body="&lt;p&gt;According to the wikipedia article on the lognormal distribution, the lognormal distribution is &quot;the maximum entropy probability distribution for a random variate $X$ for which the mean and variance of $\log(X)$  is fixed&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a not too complicated account of what this means and how this is derived?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-02-25T23:05:31.260" Id="50835" LastActivityDate="2013-02-26T09:49:26.433" LastEditDate="2013-02-26T09:48:21.527" LastEditorUserId="21279" OwnerUserId="20395" PostTypeId="1" Score="2" Tags="&lt;normal-distribution&gt;&lt;entropy&gt;&lt;lognormal&gt;" Title="Lognormal Distribution as Maximum Entropy Probability Distribution" ViewCount="279" />
  <row AcceptedAnswerId="50851" AnswerCount="1" Body="&lt;p&gt;I have some raw data that comes from a simulation. I take the sample points, and produce a moving average variable, and plot it against time. So imagine a signal that has peaks and valleys. The peaks represent periods of high activity, and the valleys represent periods of low activity.&lt;/p&gt;&#10;&#10;&lt;p&gt;What filter can I apply to this data to report the windows of high activity? We can assume that the windows do not overlap.&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT: The words &quot;what filter can I apply&quot; are not used correctly. I am not looking for a &quot;filter&quot;. I am looking for a technique to &lt;strong&gt;find the windows&lt;/strong&gt;. I start with the data, and all I want to do is find where the interesting windows of high activity are, so I can analyze them. You could say &quot;just look at them&quot;, but I'm dealing with many data sets so that doesn't scale.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-26T01:04:06.040" Id="50841" LastActivityDate="2013-02-27T20:17:36.830" LastEditDate="2013-02-27T18:28:18.457" LastEditorUserId="21341" OwnerDisplayName="user2098308" OwnerUserId="21341" PostTypeId="1" Score="3" Tags="&lt;signal-processing&gt;" Title="Mathematical technique to find activity windows in a moving average plot" ViewCount="78" />
  
  
  
  <row Body="&lt;p&gt;&lt;strong&gt;Edited based on clarification above:&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;Segmenting the signal into windows can be done easily in &lt;em&gt;Matlab&lt;/em&gt; for example, using the &lt;code&gt;buffer&lt;/code&gt; function to segment the signal into windows. The function &lt;code&gt;medfilt1&lt;/code&gt; will median filter the signal if you would like to smooth it prior to windowing and calculating the mean per window (epoch). &lt;/p&gt;&#10;&#10;&lt;p&gt;If you are interested in characterizing the activity in each window, then you can calculate summary statistics across all the samples in each window; such as mean, standard deviation or other more sophisticated measures. From your description, periods with high activity will have higher variance. This might manifest as higher mean amplitude values per window or larger standard deviation of the amplitude values per window.&#10;You would then store the indices of each 'high activity' window to allow further analysis.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-26T08:38:30.897" Id="50851" LastActivityDate="2013-02-27T20:17:36.830" LastEditDate="2013-02-27T20:17:36.830" LastEditorUserId="11030" OwnerUserId="11030" ParentId="50841" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="50862" AnswerCount="1" Body="&lt;p&gt;I have two alternative hierachical bayesian models that were designed to the describe the same process (from a high-level point-of-view). Both model provides&#10; comparable (but not identical) inferences on test data-sets and&#10; both exhibits satisfactory behaviours. So from a practical&#10; point-of-view, I have no reason to prefer one model over the&#10; other (while they rely on different views). One important&#10; difference between them consists in the number of auxiliary&#10; latent rv composing the model: one model is composed $N+1$ (scalar) latent rv whereas the other is composed of (scalar) $2N+1$ latent rvs($N$ being the nber of observed sample). Is there any reason to prefer a model with less auxiliary variables (regarless of the computational time)?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-26T12:46:43.840" FavoriteCount="1" Id="50861" LastActivityDate="2013-02-26T12:59:42.803" OwnerUserId="14346" PostTypeId="1" Score="1" Tags="&lt;bayesian&gt;&lt;modeling&gt;&lt;hierarchical-bayesian&gt;" Title="Is there any reason to prefer a bayesian model with few variables?" ViewCount="79" />
  
  <row Body="&lt;p&gt;I am looking forward to others answers here, but I have one strong suspicion what might be the cause. It is not at all clear in what way the p-value would have to be adjusted since that depends mainly on the way a requirement of the method is adjusted. Note that I don't see any need to restrict the discussion to violations of normality, all that I write here holds for requirements in general.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, some violations of the requirements actually lead to a test that is too conservative - meaning the p-value would actually have to be adjusted downwards. One example is a t-test (assuming equal variance) with imbalanced group sizes where the smaller group actually has a smaller variance than the large one. &lt;/p&gt;&#10;&#10;&lt;p&gt;Then violations can usually not be measured on a numeric scale. Non-normality might mean wide tails, skewness or even bimodality. I don't see any practical way to give a theoretical reason of how much a given p-value has to be adjusted to account for a given violation. I don't even see a set of general guidelines which works in most situations. The situation is complicated by the fact that p-values are already a weird non-linear transformation of effect sizes. So it boils down the fact that it is not really practicable. It should certainly be taken into account when interpreting the result.&lt;/p&gt;&#10;&#10;&lt;p&gt;This basically covered the practical point of view, but I have also an suspicion that there is a mathematical and theoretical tradition to prefer methods with strong and exact properties when used on data which fulfills the requirements to methods which tend to give more realistic results on real data but which are inferior on the theoretical perfect data. The first kind just lends themselves more to mathematical analysis.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-02-26T14:01:43.310" Id="50866" LastActivityDate="2013-02-26T14:01:43.310" OwnerUserId="10524" ParentId="50858" PostTypeId="2" Score="0" />
  
  
  <row AcceptedAnswerId="50900" AnswerCount="1" Body="&lt;p&gt;Let $f$ be a function such that:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$f~:~(x,~\theta)\in\mathbb{R}^{3}\times\mathbb{R}^{12} \rightarrow f(x,~\theta)\in\mathbb{R}^3$$&lt;/p&gt;&#10;&#10;&lt;p&gt;My observations $y$ are noisy values taken by the function $f(\cdot ,~\theta)$ for known values of $x$. I would like to estimate the conditional distribution of $\theta \in \mathbb{R}^{12}$ given the observations $y$. I assume the prior distribution of $\theta$ is uniform, and the likelihood of the observations given $\theta$ is Gaussian.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to have some insights about the estimation of $\theta$ with MCMC Metropolis-Hastings simulations. I use Gaussian proposals. To test the algorithm, I have created synthetic noiseless $y$ given a known $\theta$, and I try to use MCMC to find point-estimates really close to the true $\theta$. To make things harder, I start far from the true values, but even if I started &quot;close&quot; (in $\mathbb{R}^{12}$) to the solution, the convergence is not fast or obvious. I guess this is due to the quite complex non linear expression of $f$.&lt;/p&gt;&#10;&#10;&lt;p&gt;1) First of all, since $y\in\mathbb{R}^3$, would it be relevant to define separate Gaussian likelihood functions, one for each component of $y$, with potentially different error variances $\sigma_i^2$. Or is it better to just define one error variance $\sigma^2$ and jointly consider all the components of $y$ ?&lt;/p&gt;&#10;&#10;&lt;p&gt;2) Second, if I want to provide an estimate of $\sigma^2$ to the algorithm, I could just compute the empirical variance, which is roughly $\frac{1}{n-p}$ times the sum of square differences between $y$ and $f(x,~\theta_{init})$. However, I have noticed, by looking at the each simulated component of $\theta$ independently, that the chain does not mix well unless I give a much higher value to $\sigma^2$ (like omitting the factor $\frac{1}{n-p}$ where $n$ can be of the order of $10^5$ and $p=12$). Would this be due to the high dimensionality of the estimation problem? As I understand it, the higher $\sigma^2$, the higher the probability to accept a new move even if the move &quot;looks bad&quot;, and thus the higher the acceptance rate. So, to put my second question in other words, should I be patient and drastically increase the number of simulations, or should I just force the chain to sample more space more quickly by increasing $\sigma^2$? I would love to get a result in $10^5$ simulations, but I am afraid I will need a lot more because of the dimensionality and non linearity.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-02-26T20:36:42.500" Id="50889" LastActivityDate="2013-02-27T00:20:03.680" OwnerUserId="1351" PostTypeId="1" Score="1" Tags="&lt;estimation&gt;&lt;mcmc&gt;&lt;metropolis-hastings&gt;&lt;point-estimation&gt;&lt;rejection-sampling&gt;" Title="Estimated error variance $\sigma^2$ for MCMC estimation in a high-dimensional space" ViewCount="272" />
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;Were finalizing an RCT with two intervention groups (n=13, n=11). Both samples are evaluated pre vs post treatment for pain (VAS), and also against each other (group vs group). However, there are four subjects lost to post treatment in one of the groups (n13 --&gt; n=9).&#10;Which statistic model/test would be appropriate to use?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-02-27T10:08:16.943" Id="50926" LastActivityDate="2013-02-27T11:51:07.863" OwnerUserId="21326" PostTypeId="1" Score="2" Tags="&lt;time-series&gt;" Title="Which statistical test to use when sample size differ?" ViewCount="167" />
  <row Body="&lt;p&gt;We can refer to &lt;a href=&quot;http://research.microsoft.com/en-us/um/people/dmax/publications/dmart-final.pdf&quot; rel=&quot;nofollow&quot;&gt;this paper&lt;/a&gt;, and explications below sum up approach in this paper. &lt;/p&gt;&#10;&#10;&lt;p&gt;First, autoregressive models can be described as follows. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Model for time series&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Given a temporal sequence of vaiables, $Y=(Y_{1},...,Y_{T})$, a time series is a sequence of values for these variables, $y=(y_{1},...,y_{T})$. If $f(.|.,\theta)$ is a probability distribution or the model, we retict to models with form&lt;/p&gt;&#10;&#10;&lt;p&gt;$ p(y_{t}|y_{1},...,y_{t-1},\theta) = f(y_{t}|y_{t-p},...,y_{t-1},\theta)$&lt;/p&gt;&#10;&#10;&lt;p&gt;Model is probabilistic, stationary, and has p-Markov property.  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Autoregressive Tree Model&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;First, an &lt;em&gt;AR model&lt;/em&gt; is of the form&lt;/p&gt;&#10;&#10;&lt;p&gt;$f(y_{t}|y_{t-p},...,y_{t-1},\theta) = \mathit{N}( m + \sum_{j=1}^{p}b_{j}y_{t-j}, \sigma^{2}) $ &lt;/p&gt;&#10;&#10;&lt;p&gt;where $\mathit{N}(\mu,\theta)$ is normal distribution with obvious notation.&lt;/p&gt;&#10;&#10;&lt;p&gt;That is, at each time, probability for a value has mean 'autoregressively' dependent of the last p values for the series. &lt;/p&gt;&#10;&#10;&lt;p&gt;An &lt;em&gt;ART model&lt;/em&gt; is an AR model that is piecewise linear, and therefore can be represented as a tree. Each non leaf is a boolean formula, and each leaf is an AR model. &lt;/p&gt;&#10;&#10;&lt;p&gt;This is simple: branching along the tree operates depending on past values for the series. Each leaf is then an AR model for predicting the next time series value. &lt;/p&gt;&#10;&#10;&lt;p&gt;An AR model is a degenerated ART model, where there is one 'boolean' decision node, and one leaf AR model. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;ART model over AR model&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;ART models non-linearities in time series data&lt;/li&gt;&#10;&lt;li&gt;ART models periodicity in time series data&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;An alternative for ART are neural networks BUT they are difficult to interpret and/or expensive to learn.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-27T13:53:38.737" Id="50942" LastActivityDate="2013-02-27T21:01:20.423" LastEditDate="2013-02-27T21:01:20.423" LastEditorUserId="10812" OwnerUserId="10812" ParentId="50941" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;PCA calculates the eigenvalues that explain most of the variation across the data, in this case it would operate per feature vector and does not take account of class labels. &#10;LDA maximizes Fishers discriminant ratio (or Mahalaobis distance), i.e. it maximizes the distance between classes.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you define the feature vector for each observation (case) as the data at an instantaneous time point, then the temporal components of the data are not relevant. In this case you can apply PCA as pre-processing stage to each feature vector to reduce dimensionality prior to classification.&lt;/p&gt;&#10;&#10;&lt;p&gt;If however, you define each trial as a 10s epoch or segment around the point of interest, you could then calculate a summary statistic for each sensor across all time samples in the epoch. Each feature in your feature vector would then be a summary of the behaviour of each sensor over the 10s (e.g. mean amplitude across each 10s epoch). You could then apply PCA as pre-processing step to reduce the dimensionality of the feature vector from 306 to a more manageable number.&lt;/p&gt;&#10;&#10;&lt;p&gt;This second approach assumes that summary statistics calculated over each 10s epoch contains more information relevant to your problem than the instantaneous feature detailed above. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-02-27T16:07:23.173" Id="50958" LastActivityDate="2013-02-27T16:07:23.173" OwnerUserId="11030" ParentId="50932" PostTypeId="2" Score="3" />
  
  
  
  <row Body="&lt;p&gt;I think most people will agree that successive outcomes of soccer matches (of the same team?!) are not independent of each other. Clearly there are factors, such as injured players, making matches that are close in time dependent. &lt;/p&gt;&#10;&#10;&lt;p&gt;The exact nature of these invisible ties is nearly impossible to state correctly and in particular completely. People very involved in the sport are probably able to make more educated guesses about the outcome of coming events. &lt;/p&gt;&#10;&#10;&lt;p&gt;I don't think you can do anything to ensure independence between the results of soccer matches. Basically nothing on this earth is independent of anything else. True independence only occurs in idealized thought experiments. But the degree of correlation varies greatly.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can begin to control for the most influential factors (injuries, home play, change of players). But this list is infinite, although the effect of the factors on it decreases rapidly. So practically you can control for a large portion of dependencies. The remaining correlations will at some point become too small to detect - they will become &lt;em&gt;empirically&lt;/em&gt; zero, but they will never become &lt;em&gt;provably&lt;/em&gt; zero.&lt;/p&gt;&#10;&#10;&lt;p&gt;By the way, successive real world roulette outcomes are also dependent on each other. But real world roulette resembles its idealistic counterpart very closely. So without considering any factors, they are very, very independent of each other.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-27T20:47:57.610" Id="50979" LastActivityDate="2013-04-26T14:48:23.110" LastEditDate="2013-04-26T14:48:23.110" LastEditorUserId="3293" OwnerUserId="3293" ParentId="50977" PostTypeId="2" Score="9" />
  <row AcceptedAnswerId="51015" AnswerCount="1" Body="&lt;p&gt;I should use the bagging (&lt;em&gt;bootstrap aggregating&lt;/em&gt;) technique in order to train a random forest classifier. I read &lt;a href=&quot;http://en.wikipedia.org/wiki/Bootstrap_aggregating&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; the description of this learning technique, but I have not figured out how I initially organize the dataset.&lt;/p&gt;&#10;&#10;&lt;p&gt;Currently I first load all the positive examples and immediately after the negative ones. Moreover, positive examples are less than half of the negative ones, so by sampling from the dataset uniformly, the probability of obtaining a negative example is greater than that of obtaining a positive example.&lt;/p&gt;&#10;&#10;&lt;p&gt;How should I build the initial dataset?&#10;Should I shuffle the initial dataset containing positive and negative examples?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-27T21:45:23.853" Id="50987" LastActivityDate="2013-02-28T04:15:03.670" OwnerUserId="16937" PostTypeId="1" Score="2" Tags="&lt;machine-learning&gt;&lt;bootstrap&gt;&lt;random-forest&gt;&lt;cart&gt;&lt;bagging&gt;" Title="Building the dataset for Random Forest training procedure" ViewCount="205" />
  
  <row Body="&lt;p&gt;The point of the LASSO is that all variables &lt;em&gt;are&lt;/em&gt; in the model until they are subject to sufficient shrinkage that their coefficient would change sign (i.e. go through zero effect). Variables are only eliminated from the model, they do not enter, at progressively larger values of shrinkage. Algorithms exist that compute the entire path from least-squares solution to full shrinkage in an efficient manner. The plots often produced for these paths are counter-intuitive as the least squares solution is on the right and the path progresses to the left as more shrinkage is applied.&lt;/p&gt;&#10;&#10;&lt;p&gt;The terms in the model are all standardised and hence the (absolute) sizes of the coefficients at any stage in the LASSO path are an indication of the &quot;importance&quot; of each variable in terms of their effect on the response.&lt;/p&gt;&#10;&#10;&lt;p&gt;The LASSO is very different to forward selection; all variables are in the model and then shrinkage is applied to the coefficients which places a restriction on the cumulative size of the absolute values of the set of coefficients. As shrinkage is increased, the maximum size on the set coefficients is reduced. Variables contributing little (or nothing) to the fit to the response (i.e. of lesser importance) are shrunk more than important variables. However, the LASSO doesn't handle correlated variables so well, unlike ridge regression, and hence a procedure called the elastic net has been developed which combines elements of the two (LASSO and ridge penalties).&lt;/p&gt;&#10;&#10;&lt;p&gt;One of the advantages of the LASSO is that variable selection is part of the model fitting process. IIRC however, the LASSO estimates of the coefficients are subject to greater bias, but lower variance, than the least squares solutions. This trade-off may be a good thing in terms of prediction if the reduction in variance contribution to model error is larger than the additional bias contribution. But as the LASSO estimates are somewhat biased, that complicates their interpretation as one would in ordinary least squares.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-02-28T03:35:53.103" Id="51014" LastActivityDate="2013-02-28T04:09:09.343" LastEditDate="2013-02-28T04:09:09.343" LastEditorUserId="1390" OwnerUserId="1390" ParentId="51010" PostTypeId="2" Score="8" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I am simulating the effect of certain conditions on estimates obtained using OLS regression.  In running 100 replications I get 100 sets of standard errors for the regression coefficients.  How do I calculate the standard error of the estimates?  Is it simply the standard deviation of the estimated standard errors for a given coefficient divided by the square root of the number of replications?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-02-28T04:37:41.793" FavoriteCount="2" Id="51017" LastActivityDate="2014-02-25T04:40:00.153" OwnerUserId="21365" PostTypeId="1" Score="5" Tags="&lt;regression&gt;&lt;variance&gt;&lt;simulation&gt;" Title="Standard error for a statistic obtained via simulation" ViewCount="271" />
  
  <row Body="&lt;p&gt;Could you look at other corpuses of project values as a benchmark? I bet the US-gov, Worldbank, UN etc. must have some public project values information. &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-02-28T07:26:44.307" Id="51025" LastActivityDate="2013-02-28T07:26:44.307" OwnerUserId="21072" ParentId="51022" PostTypeId="2" Score="1" />
  
  <row AnswerCount="1" Body="&lt;p&gt;As the title suggest...I have a very basic question.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a case with the following data:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Universe: 18840 balls total&#10;red balls in the universe: 6680&#10;Sample: 382 balls total&#10;red balls in the sample: 160&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I would like to estimate if the percentage of red balls in my sample is significantly different from the percentage of reds in universe.&lt;/p&gt;&#10;&#10;&lt;p&gt;In your opinion is it more correct to utilize a chi-square test or an hypergeometric distribution?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-28T09:09:17.320" FavoriteCount="0" Id="51028" LastActivityDate="2013-03-12T10:46:48.037" LastEditDate="2013-03-12T10:46:48.037" LastEditorUserId="805" OwnerUserId="21372" PostTypeId="1" Score="4" Tags="&lt;chi-squared&gt;&lt;hypergeometric&gt;" Title="Which are differences between the hypergeometric distribution and chi-square distribution" ViewCount="709" />
  
  <row Body="&lt;p&gt;The inputs should be scaled to the so-called &quot;active range&quot; of the activation function, or, in other words, the area of the function curve where the derivative of the function is clearly non-zero. This is done for backpropagation to work properly, since it uses activation function derivatives, and &lt;code&gt;~ 0&lt;/code&gt; derivatives imply extremely small (insignificant) changes to NN weights (no learning). For sigmoid, the active range lies somewhere between &lt;code&gt;-sqrt(3)&lt;/code&gt; and &lt;code&gt;sqrt(3)&lt;/code&gt;. You may scale inputs to that range.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, yes: sigmoid will always output values in &lt;code&gt;(0;1)&lt;/code&gt;, because that's sigmoid's range. You will need to scale NN outputs to the necessary ranges.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-28T11:06:43.457" Id="51034" LastActivityDate="2013-02-28T11:11:55.590" LastEditDate="2013-02-28T11:11:55.590" LastEditorUserId="13128" OwnerUserId="13128" ParentId="51012" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Take care to note you're discussing two different statistics here.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's set up the sampling situation in detail first so we can be clear:&lt;/p&gt;&#10;&#10;&lt;p&gt;We have red balls and not-red balls (for simplicity I will call them all 'black', but they could be a mix of non-red colors - it's irrelevant to this set up since they all are simply categorized as not-red). &lt;/p&gt;&#10;&#10;&lt;p&gt;You have a population (your 'universe') of 18840 balls, 6680 red and 12160 black. You draw a random sample of 382 balls without replacement, and obtain 160 red and 222 black.&lt;/p&gt;&#10;&#10;&lt;p&gt;That is, your example data are like so:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;         Drawn    Not drawn    Total&#10;&#10;Red      160        6520        6680&#10;Black    222       11938       12160&#10;&#10;Total    382       18458       18840&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Looking at the number of reds drawn as a random variable, that has a &lt;a href=&quot;http://en.wikipedia.org/wiki/Hypergeometric_distribution#Application_and_example&quot; rel=&quot;nofollow&quot;&gt;hypergeometric distribution&lt;/a&gt; (though there formulated in terms of white and black balls drawn from an urn rather than red and black balls drawn from a universe).&lt;/p&gt;&#10;&#10;&lt;p&gt;[Conditioning on the margins gives the hypergeometric - this is also the situation used for &lt;a href=&quot;http://en.wikipedia.org/wiki/Fisher%27s_exact_test&quot; rel=&quot;nofollow&quot;&gt;Fisher's exact test&lt;/a&gt; based on the hypergeometric, and one of the situations for which the usual 2x2 &lt;a href=&quot;http://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test#Test_of_independence&quot; rel=&quot;nofollow&quot;&gt;chi-square test&lt;/a&gt; of association/test of independence applies. If you don't condition on both margins, you don't have a hypergeometric; but that's what you normally do in the specific balls-in-urns model you describe.]&lt;/p&gt;&#10;&#10;&lt;p&gt;If $O_{ij}$ is the observed count in cell $(i,j)$ in the above $2\times 2$ table, then your statistics are $O_{11}$ in the first case (assuming red is first) and $X^2 = \sum \sum {(O_{ij} - E_{ij})^2 \over E_{ij}}$ in the second. Both statistics are actually discrete, but you can approximate either by a continuous distribution - the first by a normal approximation, the second by a chi-square.&lt;/p&gt;&#10;&#10;&lt;p&gt;With random sampling, the distribution of the number of red balls in the sample ($O_{11}$) is hypergeometric - that is, given the usual assumptions it's exactly correct. &lt;/p&gt;&#10;&#10;&lt;p&gt;Given the universe details and the sample size, the usual 'chi-square' statistic, though discrete, will be quite well approximated by a chi-square distribution when the number of red balls in the sample is hypergeometric. It's &lt;em&gt;not&lt;/em&gt; exact, but it will be quite close in this case.&lt;/p&gt;&#10;" CommentCount="10" CreationDate="2013-02-28T11:13:30.080" Id="51035" LastActivityDate="2013-03-01T05:57:09.770" LastEditDate="2013-03-01T05:57:09.770" LastEditorUserId="805" OwnerUserId="805" ParentId="51028" PostTypeId="2" Score="5" />
  <row AnswerCount="1" Body="&lt;p&gt;I wanted to see the impact of small business support programs on small business growth. I regressed their growth (represented by the number of employees) on only those 5 kinds of support programs (as regressors). and I found R squared= 0.35, adjusted R squared= -0.03, some coefficients are positive while some others are negative, and all the p-values is &gt; the significance level (10%) (coefficients are insignificant). 1) May I conclude that the support programs have a positive but insignificant effect? 2) Should I include control variables to complete the model (so that r squared become &gt; 0.5).  &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-02-28T13:16:03.280" FavoriteCount="1" Id="51039" LastActivityDate="2013-07-29T00:42:38.827" OwnerUserId="21371" PostTypeId="1" Score="1" Tags="&lt;regression&gt;" Title="Regression with or without controlled variables?" ViewCount="171" />
  <row AnswerCount="0" Body="&lt;p&gt;I try to sample a set of parameters $(x_i)$ from a posterior&#10;$p((x_i)|(o_j))$ using a metropolis-hastings algorithm. &#10;The fact is that a simple independent random walk on each of the $x_i$ as&#10;proposal leads to poor convergence results. To alleviate this, I would like to reparametrise the $(x_i)$ as:&lt;/p&gt;&#10;&#10;&lt;p&gt;$(x^c_i),f$ with $\sum_i x^c_i=1$,&lt;/p&gt;&#10;&#10;&lt;p&gt;in order to design a proposal of the form&#10;$$&#10;p((x^c_i),f|(\tilde{x^c}_i),\tilde{f}) = p((x^c_i)|(\tilde{x^c}_i)) p(f|\tilde{f})&#10;$$&#10;for which I expect better convergence properties. My question concerns the way to&#10;design a $p((x^c_i)|(\tilde{x^c}_i))$ ensuring a tractable sampler; I did not manage to find any reference about that. Thanks.&lt;/p&gt;&#10;&#10;&lt;p&gt;ps: I am not very confident with the title of the post&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-02-28T13:32:50.803" Id="51040" LastActivityDate="2013-02-28T13:32:50.803" OwnerUserId="14346" PostTypeId="1" Score="0" Tags="&lt;mcmc&gt;&lt;random-generation&gt;" Title="proposal distribution on a manifold" ViewCount="58" />
  <row AcceptedAnswerId="51053" AnswerCount="1" Body="&lt;p&gt;I'm a little bit confused with precision recall. I read some papers about recommender systems, where in one paper they have a graphical representation and in other papers they don't (they just have the values for precision/recall). What is the advantage of a graphical representation?&lt;/p&gt;&#10;&#10;&lt;p&gt;Am I right, that for a graph you have to do something like a step function? e.g. you have a list of values &lt;code&gt;true,true,false,true,false&lt;/code&gt; and you begin with the first one, calculate precision recall and then draw that point into the graph. For the next point, you take the first and the second value &lt;code&gt;true,true&lt;/code&gt; into account, calculate precision recall, draw the point... and so on. Am I right that this is similar to drawing a ROC curve?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-28T14:25:22.313" FavoriteCount="1" Id="51042" LastActivityDate="2013-06-29T11:12:08.887" LastEditDate="2013-06-29T11:12:08.887" LastEditorUserId="22047" OwnerUserId="16537" PostTypeId="1" Score="2" Tags="&lt;data-visualization&gt;&lt;roc&gt;&lt;recommender-system&gt;&lt;precision-recall&gt;" Title="Precision - Recall: Graphical Representation" ViewCount="528" />
  <row Body="&lt;p&gt;I could be wrong, but I think you need to decide the relative importance of performance parameters based on domain knowledge. At least in my knowledge statistics offers you no insight into the relative weights. &lt;/p&gt;&#10;&#10;&lt;p&gt;e.g. Say you had to rank cars on the basis of gas-mileage and top-speed. Every persons mental model of car rating would give different weights to each of these two parameters. So I am doubtful you can get Statistics to solve this conundrum for you.&lt;/p&gt;&#10;&#10;&lt;p&gt;I agree with @BGreene that PCA might help you understand the dataset; though high variance by itself does not imply that a variable must be the most important in a rating context. &lt;/p&gt;&#10;&#10;&lt;p&gt;Again, I could be wrong. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-28T14:25:28.703" Id="51043" LastActivityDate="2013-02-28T14:25:28.703" OwnerUserId="21072" ParentId="51041" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;I would suggest a brief search on &quot;&lt;a href=&quot;https://www.google.com/search?q=linear+regression+model+diagnostics&amp;amp;ie=utf-8&amp;amp;oe=utf-8&amp;amp;aq=t&amp;amp;rls=org.mozilla%3aen-US%3aofficial&amp;amp;client=firefox-a&quot;&gt;linear regression model diagnostics&lt;/a&gt;&quot; as a start. But here are some that I would suggest you to check:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Make sure the assumptions are satisfactorily met&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;Use scatterplot or component plus residual plot to examine the linear relationship between the independent predictor(s) and the dependent variable.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Compose a plot with standardized residual versus predicted value and ensure there isn't extreme point with very high residual, and the spread of the residual is largely similar along the predicted value, as well as spreading largely equally above and below the mean of residual, zero.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;You can also change the y-axis to residual$^2$. This plot helps identifying unequal variance.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Re-examine the study design to ensure the assumption of independence is reasonable.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Retrieve the variance inflation factor (VIF) or tolerance statistics to examine possible collinearity.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Examine potential influential point(s)&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Check statistics such as Cook's D, DFits, or DF Beta to find out if a certain data point is drastically changing your regression results. You can find more &lt;a href=&quot;http://online.stat.psu.edu/online/development/stat501/14outliers/01outlier_intro.html&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Examine the change in $R^2$ and Adjusted $R^2$ statistics&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Being the ratio of regression sum of squares to total sum of squares, $R^2$ can tell you how many % of variability in your dependent variable are explained by the model.&lt;/li&gt;&#10;&lt;li&gt;Adjusted $R^2$ can be used to check if the extra sum of squares brought about my the additional predictor(s) is really worth the degrees of freedom they'll take.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Check necessary interaction&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;If there is a main independent predictor, before you make any interpretation of its independent effect, check if it is interacting with other independent variables. Interaction, if left unadjusted, can bias your estimate.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Apply your model to another data set and check its performance&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;You can also apply the regression formula to other separate data and see how well it predicts. Graph like scatter plot and statistics like % difference from the observed value can serve as a good start.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="1" CreationDate="2013-02-28T15:31:13.910" Id="51049" LastActivityDate="2013-02-28T16:56:12.120" LastEditDate="2013-02-28T16:56:12.120" LastEditorUserId="13047" OwnerUserId="13047" ParentId="51046" PostTypeId="2" Score="11" />
  
  <row AnswerCount="3" Body="&lt;p&gt;I am working on a packet generator that can generate packets of 50 different sizes and the packet sizes follow exponential distribution. Given a mean packet size how can I choose the 50 packet sizes from the continuous exponential distribution so that the resulting discrete distribution accurately follows the continuous one.Many thanks in advance.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-02-28T16:37:56.363" Id="51056" LastActivityDate="2013-05-04T07:59:39.233" OwnerUserId="21392" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;distributions&gt;&lt;exponential&gt;" Title="Discretizing Exponential distribution" ViewCount="316" />
  
  
  <row AcceptedAnswerId="51518" AnswerCount="1" Body="&lt;p&gt;Here is what I have :&lt;/p&gt;&#10;&#10;&lt;p&gt;A scaled training set, with labels.&lt;/p&gt;&#10;&#10;&lt;p&gt;Segmented images, from which I extract new vectors to classify.&lt;/p&gt;&#10;&#10;&lt;p&gt;My classifier is a KNN which would have obviously been trained using my training set.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, I wonder how I should scale those new vectors I just got. Is this correct to scale them on their own, or should I do something else ? I wonder for example if an outlier would have an effect on the scaling and subsequent classification...&lt;/p&gt;&#10;&#10;&lt;p&gt;[EDIT] adding an outlier (which I would like to detect using kNN algorithm) to the test datas does impact the scaling, so subsequent classification won't work properly. What should I do then ?&lt;/p&gt;&#10;&#10;&lt;p&gt;[EDIT 2] This is how I scale my data : &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/9tpnF.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Which in Scilab I translate to :&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;function dataout = scaledata(datain)&#10;&#10;dataout = zeros(size(datain,1),size(datain,2));&#10;&#10;for i=1:size(datain,2)&#10;    dataout(1:size(datain,1),i) = (datain(1:$,i) - min(datain(1:$,i))) / ...&#10;                                     (max(datain(1:$,i)) - min(datain(1:$,i)));&#10;end&#10;&#10;endfunction&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Thank you&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-02-28T20:26:30.860" Id="51074" LastActivityDate="2013-03-06T20:14:11.980" LastEditDate="2013-03-06T20:05:01.470" LastEditorUserId="8984" OwnerUserId="8984" PostTypeId="1" Score="0" Tags="&lt;classification&gt;&lt;k-nearest-neighbour&gt;&lt;multidimensional-scaling&gt;" Title="How to scale new datas when a training set already exists" ViewCount="181" />
  
  <row Body="&lt;p&gt;Let me expand on alternative solution proposed by @curious_cat. &lt;/p&gt;&#10;&#10;&lt;p&gt;$P_{ij}$ is the matrix of pitches &lt;/p&gt;&#10;&#10;&lt;p&gt;$L_{ij}$ is the matrix of sells&lt;/p&gt;&#10;&#10;&lt;p&gt;$S_{ij} = L_{ij}/P_{ij}$ is the matrix of success rates (elementwise division where it exists and 0 elsewhere)&lt;/p&gt;&#10;&#10;&lt;p&gt;As @curious_cat suggested, you want to approximate $S_{ij}$ by the outer product of two &lt;strong&gt;positive&lt;/strong&gt; vectors &lt;/p&gt;&#10;&#10;&lt;p&gt;$$S_{ij} \approx M_i \times A_j^T$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Least square minimization will lead to&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\min | S_{ij} - M_j \times A_i^T |_2$$ &#10;where $| \quad |_2$ is the Frobenius norm.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;BUT&lt;/strong&gt; you do not want to minimize for the entries in which $S_{ij}$ is not defined. So what you realy want is something like: &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \min |W_{ij} \odot (S_{ij} - M_j \times A_i^T)|_2$$ &#10;where $\odot$ is the elementwise multiplication. &lt;/p&gt;&#10;&#10;&lt;p&gt;1) At a first approximation, $w_{ij}$ is 0 where $p_{ij}$ is 0 and 1 elsewhere.&lt;/p&gt;&#10;&#10;&lt;p&gt;This is a &lt;strong&gt;weighted non-negative matrix factorisation&lt;/strong&gt; (or approximation) problem. Google should give some references to it.&lt;/p&gt;&#10;&#10;&lt;p&gt;2) Now, shooting from the hip, let us try to answer the point also made by  @curious_cat that you should trust more a success rate of 1000 sells over 2000 pitches than a 2 sells over 4 pitches. &lt;/p&gt;&#10;&#10;&lt;p&gt;The weight $w_{ij}$ need not to be uniformly 1 for the entries that are defined in $S_{ij}$. One can give it more weight to success rates with higher pitches. &lt;/p&gt;&#10;&#10;&lt;p&gt;My guess is to use $\sqrt{p_{ij}}$ as the weight. The intuition is that the confidence interval on the success rate is inversely proportional to $\sqrt{p_{ij}}$.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-02-28T21:42:52.293" Id="51080" LastActivityDate="2013-03-05T02:37:55.740" LastEditDate="2013-03-05T02:37:55.740" LastEditorUserId="8226" OwnerUserId="8226" ParentId="50632" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;1. You could introduce a third variable (the actual image). More generally, you can put all your independent variables - actual image, age group, sex into a model for predicted image (e.g. a logistic regression model) and test various hypotheses.&lt;/p&gt;&#10;&#10;&lt;p&gt;2. If you reject the null, it's reasonable to look at the source of the difference. e.g. if you conclude that men and women are different, you can (for example) simply point to the sign of a coefficient and say 'it was significant because the women did better'. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, &lt;a href=&quot;http://en.wikipedia.org/wiki/Simpson%27s_paradox&quot; rel=&quot;nofollow&quot;&gt;beware Simpson's paradox&lt;/a&gt;!&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-03-01T00:50:55.777" Id="51094" LastActivityDate="2013-03-01T00:50:55.777" OwnerUserId="805" ParentId="51089" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;1) It depends on the test statistic! You can sensibly get positive or negative test statistics, but the two I'd most likely use (two sample proportions test of $\pi_2-\pi_1$ OR chi-square) would both be positive.&lt;/p&gt;&#10;&#10;&lt;p&gt;2) There's not enough information - we don't have sample sizes! &lt;/p&gt;&#10;&#10;&lt;p&gt;[Well, in fact we &lt;em&gt;sort&lt;/em&gt; of can sometimes figure out a difference must be significant even without the sample sizes, just based on the counts that can give us the observed proportions:&lt;/p&gt;&#10;&#10;&lt;p&gt;i) What's the smallest sample size in which we could observe 45%? Assuming it's a rounded fraction, it looks like 5/11 &lt;/p&gt;&#10;&#10;&lt;p&gt;ii) what's the smallest sample size in which we could observe 53%? That looks like 8/15 is the smallest possible.&lt;/p&gt;&#10;&#10;&lt;p&gt;So if it's significant at the lowest possible sample size, and the next few 45%-vs-53% sample-sizes up (to allow for the fact that the &lt;em&gt;actual&lt;/em&gt; difference might get a little smaller and more than undo the gain in smaller standard errors), it will be significant at larger samples. But in this case it turns out it's not significant for 5/11 vs 8/15:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; prop.test(c(5,8),c(11,15))&#10;&#10;2-sample test for equality of proportions with&#10;continuity correction&#10;&#10;data:  c(5, 8) out of c(11, 15) &#10;X-squared = 0, df = 1, p-value = 1&#10;alternative hypothesis: two.sided &#10;95 percent confidence interval:&#10; -0.5452923  0.3877165 &#10;sample estimates:&#10;   prop 1    prop 2 &#10;0.4545455 0.5333333 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So that's no help - since it's not significant at the smallest sample sizes, we still can't tell for sure. I assume you have sample size information elsewhere that you didn't notice.]&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-03-01T04:43:12.463" Id="51106" LastActivityDate="2013-03-01T04:56:19.630" LastEditDate="2013-03-01T04:56:19.630" LastEditorUserId="805" OwnerUserId="805" ParentId="51103" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;If you want to control the probability of a Type I error you need to correct. The tests do seem to be independent from your description, so the correct method would be to use Šidák correction. This is an exact adjustment under the condition of independence. It is based on the fact that in this case&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\alpha = 1 - (1-\beta)^n$$&#10;where $\beta$ is your significance niveau for your individual tests, $\alpha$ your Familywise Error Rate (FWER) and n the number of the independent tests. Solving this leads to the adjustment&#10;$$&#10;\beta=1-(1-\alpha)^\frac{1}{n}. &#10;$$&#10;In your example this means with two tests you need to set your significance niveau to about $0.2532$ to have $\alpha=.05$. So not much better than Bonferroni in this case.&lt;/p&gt;&#10;&#10;&lt;p&gt;Much can be said about where and when the familywise control of Type I errors makes sense. In many cases Type I errors are introduced against by publication bias, but this is hard for an author to defend against. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-03-01T13:41:29.487" Id="51125" LastActivityDate="2013-03-01T13:41:29.487" OwnerUserId="10524" ParentId="51033" PostTypeId="2" Score="0" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Do machine learning algorithms like Boosted Regression Trees (in the R package (gbm)) follow the same statistical assumptions of not including correlated predictor variables in GLM? &lt;/p&gt;&#10;&#10;&lt;p&gt;i.e. If I have two correlated predictrs (rsq=.7) should I be including both into my BRT model? &lt;/p&gt;&#10;&#10;&lt;p&gt;Any input or thoughts on this question would be greatly appreciated. &lt;/p&gt;&#10;&#10;&lt;p&gt;Cheers, &lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-03-02T02:22:08.433" Id="51176" LastActivityDate="2013-03-02T03:06:20.500" LastEditDate="2013-03-02T03:06:20.500" LastEditorUserId="21421" OwnerUserId="21421" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;regression&gt;&lt;machine-learning&gt;&lt;generalized-linear-model&gt;&lt;gbm&gt;" Title="Machine Learning Algorithms vs. Linear Regression" ViewCount="270" />
  <row Body="&lt;p&gt;For the geographic distances, it will make sense to combine latitude and longitude into a single quantity.  A formula from this page should help:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/TSPFAQ.html&quot; rel=&quot;nofollow&quot;&gt;http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/TSPFAQ.html&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;where code is given to compute distance along the surface of the Earth, rounded to the nearest kilometer.  You will probably want to leave out the rounding.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then combine distance with the other quantities.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-02T09:34:12.273" Id="51183" LastActivityDate="2013-03-02T09:34:12.273" OwnerUserId="21470" ParentId="18913" PostTypeId="2" Score="0" />
  <row AnswerCount="1" Body="&lt;p&gt;The mean and standard deviation of a random sample of $n$ measurements equal &#10;33.9 and 3.3 respectively.&lt;/p&gt;&#10;&#10;&lt;p&gt;Construct a 95% confidence interval for $n = 100$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hence I am trying to calculate margin of error and then use that to derive the sample size and then to construct my diagram. But the information given seems not to be enough.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-03-03T05:29:39.133" Id="51218" LastActivityDate="2013-07-15T16:13:16.360" LastEditDate="2013-06-15T15:40:19.733" LastEditorUserId="6029" OwnerUserId="21491" PostTypeId="1" Score="0" Tags="&lt;self-study&gt;&lt;confidence-interval&gt;" Title="How to calculate the margin of error if the sample size is not given?" ViewCount="390" />
  
  <row Body="&lt;p&gt;No, don't use 0s, use the &lt;a href=&quot;http://en.wikipedia.org/wiki/Student%27s_t-test#One-sample_t-test&quot; rel=&quot;nofollow&quot;&gt;one sample t-test&lt;/a&gt; and test whether the mean differs from 0 (it does).&lt;br&gt;&#10;In &lt;code&gt;R&lt;/code&gt; it goes as follows:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x &amp;lt;- c(0.2245, 0.243, 0.2312, 0.1795, 0.1923, 0.17, 0.2025, 0.2059, &#10;0.2394, 0.205, 0.2201, 0.2261, 0.1817, 0.2143, 0.2126, 0.237, &#10;0.1984, 0.228, 0.2292, 0.2236, 0.2096, 0.2258, 0.2155)&#10;&amp;gt; t.test(x)&#10;&#10;        One Sample t-test&#10;&#10;data:  x &#10;t = 52.3, df = 22, p-value &amp;lt; 0.00000000000000022&#10;alternative hypothesis: true mean is not equal to 0 &#10;95 percent confidence interval:&#10; 0.2052 0.2222 &#10;sample estimates:&#10;mean of x &#10;   0.2137 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2013-03-03T16:28:23.793" Id="51245" LastActivityDate="2013-03-03T16:28:23.793" OwnerUserId="442" ParentId="51242" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Typically, there is a tradeoffs between precision and recall, so yes.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's a simple example. Imagine you have predicted probabilities from a logistic regression, and you are choosing a classification threshold.  A higher threshold will typically have better precision and worse recall than a low threshold.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-03-03T16:30:58.650" Id="51246" LastActivityDate="2013-03-03T16:30:58.650" OwnerUserId="2817" ParentId="51244" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="51254" AnswerCount="1" Body="&lt;p&gt;I want to see if the &lt;code&gt;proportion of 'I don't know'&lt;/code&gt; responses correlates with the progression through the survey. Generally we expect this proportion to increase through the survey due to a fatigue effect. However, the following chart suggests this effect is not present in my data: &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Y2HQs.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I would however like to back this up with a statistic. I figure I need a &lt;code&gt;correlation&lt;/code&gt; measure between the question order and proportion of DK. Does anyone know about such measure, and maybe how to produce it in &lt;code&gt;SPSS&lt;/code&gt; (R could also be an option)?&lt;/p&gt;&#10;&#10;&lt;p&gt;PS: I eventually also want to control for question and respondent characteristics.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-03-03T17:31:10.097" Id="51251" LastActivityDate="2013-03-03T18:36:00.963" LastEditDate="2013-03-03T18:36:00.963" LastEditorUserId="7972" OwnerUserId="18334" PostTypeId="1" Score="1" Tags="&lt;correlation&gt;&lt;spss&gt;&lt;survey&gt;&lt;proportion&gt;" Title="A statistic to describe a correlation with progression" ViewCount="82" />
  <row Body="&lt;p&gt;E. L. Lehmann addressed this question in an introduction to a reprint of Gosset's 1908 article in &lt;em&gt;Breakthroughs in Statistics, Volume II--Methodology and Distribution&lt;/em&gt; (Samuel Kotz &amp;amp; Norman L. Johnson, eds., 1992).&lt;/p&gt;&#10;&#10;&lt;p&gt;Lehmann first describes the state of the art in Gosset's time: it amounted to a &quot;z test&quot; where the estimated standard deviation was treated as if it were a constant.  Then he discusses Gosset's contribution:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;However, if the sample size $n$ is small, $S^2$ will be subject to considerable variation.  It was the effect of this variation that concerned Student, the pseudonym of W. S. Gosset... .  He pointed out that if the form of the distribution of the $X$'s is known, this variation can be taken into account, since for any given $n$ the distribution of $t$ is then determined exactly.  He proposed to work out this distribution for the case in which the $X$'s are normal.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;This in fact is what Gosset did, albeit without mathematical rigor: he derived some properties of the distribution of $t$ for the normal case, matched them to properties of known distributions, and correctly guessed its distribution--acknowledging that this was less than rigorous.  To support his guess, he conducted a Monte-Carlo simulation using samples of four from a dataset.&lt;/p&gt;&#10;&#10;&lt;p&gt;Gosset wrote pseudonymously because his employer (the Guinness brewery) apparently felt that this improved understanding of small-sample variation was a bit of an advantage in the business : it would have led to improved quality control procedures.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-03-03T21:55:59.600" Id="51265" LastActivityDate="2013-03-03T21:55:59.600" OwnerUserId="919" ParentId="51258" PostTypeId="2" Score="5" />
  <row AcceptedAnswerId="59854" AnswerCount="2" Body="&lt;p&gt;I am getting the impression that when people are referring to a 'deep belief' network that this is basically a neural network but very large. Is this correct or does a deep belief network also imply that the algorithm itself is different (ie, no feed forward neural net but perhaps something with feedback loops)? &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-03-04T04:18:42.890" FavoriteCount="8" Id="51273" LastActivityDate="2014-04-02T10:01:03.313" OwnerUserId="17935" PostTypeId="1" Score="19" Tags="&lt;machine-learning&gt;&lt;neural-networks&gt;&lt;deep-learning&gt;&lt;deep-belief-networks&gt;" Title="What is the difference between a neural network and a deep belief network?" ViewCount="6797" />
  <row AnswerCount="0" Body="&lt;p&gt;Consider that $P$ is the water pressure coming out of a valve $A$. Let $P_{dif}$ be the difference between the maximum and the minimum pressure of valve $A$: &lt;/p&gt;&#10;&#10;&lt;p&gt;$$P_{dif}≔P_{max}-P_{min}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, what I want to do is estimate $P_{dif}$. In order to do that, I take a number of water pressure samples from valve $A$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Let $S$ be a set of $3$ measured samples: &lt;/p&gt;&#10;&#10;&lt;p&gt;$$S = {5,7,1}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;That is, $S$ contains 3 random samples of $P$, therefore, by placing S in ascending order, I can estimate $P_{dif}$ like that:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\hat{P}_{dif} = S_{(n:3)}-S_{(n:1)} = 7-1 = 6$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Questions:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;What is the probability of exceeding this estimation ($\hat{P}_{dif}$). That is, what is the probability that the population parameter ($P_{dif}$) will exceed the estimation ($\hat{P}_{dif}$).&lt;/li&gt;&#10;&lt;li&gt;Reliability of the estimation ($\hat{P}_{dif}$): this refers to the probability that the estimation is wrong (it may be possible to find this by deriving the confidence interval of the sample range, but I don't know how).&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Any help towards this will be greatly appreciated.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-03-04T20:03:26.330" Id="51308" LastActivityDate="2013-03-04T20:03:26.330" OwnerUserId="21255" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;confidence-interval&gt;&lt;extreme-value&gt;" Title="Probability of exceedance and reliability of a sample range estimation" ViewCount="206" />
  
  
  <row Body="&lt;p&gt;See Proposition B here:&#10;&lt;a href=&quot;http://www.ssc.wisc.edu/~jkennan/research/DiscreteApprox.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.ssc.wisc.edu/~jkennan/research/DiscreteApprox.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;It shows that the best $n$-point discrete approximation, where &quot;best&quot; is defined relative to an $L^p$ metric (for any $p &amp;gt; 0$), is one that places equal density on each point.&lt;/p&gt;&#10;&#10;&lt;p&gt;So if $F$ is the exponential CDF you should choose your fifty support points $x_i^*$ such that $F(x_i^*) = \frac{2i-1}{2n}$ for $1 \le i \le 50$.  To do that you'll need a way to calculate the inverse exponential CDF, for example in Matlab you would use expinv().&lt;/p&gt;&#10;&#10;&lt;p&gt;Then assign each point equal mass, which will be $1/50$.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-05T03:56:20.337" Id="51338" LastActivityDate="2013-03-05T04:10:29.623" LastEditDate="2013-03-05T04:10:29.623" LastEditorUserId="21168" OwnerUserId="21168" ParentId="51056" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;I would like to create a weighted covariance matrix (say 5 variables) using 3 different time points where the weights come from a kernel function (can be normal, triangular, etc.) but I'm not understanding how it works. Basically I'm imagining&lt;/p&gt;&#10;&#10;&lt;p&gt;$$S(u,v)=\frac{w_1*cov_1(u,v)+w_2*cov_2(u,v)+w_3*cov_3(u,v)}{w_1+w_2+w_3},$$ &lt;/p&gt;&#10;&#10;&lt;p&gt;where the numbers denote the 3 time points. Does that make sense? &lt;/p&gt;&#10;&#10;&lt;p&gt;Any method of combining the covariance estimates is a good start -- I'm completely open to alternative ideas! The kernel weights came from a paper by Razavian et al &lt;a href=&quot;http://repository.cmu.edu/cgi/viewcontent.cgi?article=2059&amp;amp;context=compsci&quot; rel=&quot;nofollow&quot;&gt;&quot;Time-Varying Gaussian Graphical Models of Molecular Dynamics Data&quot;&lt;/a&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;Here is some sample data:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;cov1=matrix(c(175.28, 23.37, 12.76, 3.45, 5.94, 23.37, 90.29, 44.41, 42.83, 52.00, 12.76, 44.41, 29.94, 22.17, 22.39, 3.45, 42.83, 22.17, 31.95, 32.21, 5.94, 52.00, 22.39, 32.21, 58.22), 5, 5)&#10;cov2=matrix(c(207.29, 43.96, 45.03, 35.60, 27.97, 43.96, 115.52, 71.71, 80.95, 68.69, 45.03, 71.71, 58.70, 53.07, 40.71, 35.60, 80.95, 53.07, 67.84, 51.95, 27.97, 68.69, 40.71, 51.95, 59.26), 5, 5)&#10;cov3=matrix(c(165.67, 63.42, 37.82, 32.63, 43.37, 63.42, 121.70, 66.00, 74.47, 71.16, 37.82, 66.00, 47.34, 42.24, 34.40, 32.63, 74.47, 42.24, 55.98, 47.48, 43.37, 71.16, 34.40, 47.48, 75.61), 5, 5)       &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I've also plotted the normal, uniform, Epanechnikov, and triangular kernels but how do I know where in the plots do I get the &quot;weights&quot; for a specific covariance element? I've spent time reading about kernels and staring at these plots but... nothing. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;f.normal &amp;lt;- function(x) { dnorm(x); }&#10;f.uniform &amp;lt;- function(x) { ifelse(abs(x)&amp;lt;1, 0.5, 0); }&#10;f.epanechnikov &amp;lt;- function(x) { ifelse(abs(x)&amp;lt;1, (1-x^2)*3/4, 0); }&#10;f.triangle &amp;lt;- function(x) { ifelse(abs(x)&amp;lt;1, 1-abs(x), 0); }&#10;plot(c(-1,1),c(0,1.2),xlab=&quot;z&quot;, ylab=&quot;&quot;, main=&quot;Kernels&quot;, type=&quot;n&quot;)&#10;curve(f.normal(x), c(-1,1), add=T, lwd=2, lty=1, col=&quot;black&quot;)&#10;curve(f.uniform(x), c(-1,1), add=T, lwd=2, lty=2, col=&quot;blue&quot;)&#10;curve(f.epanechnikov(x), c(-1,1), add=T, lwd=2, lty=3, col=&quot;red&quot;)&#10;curve(f.triangle(x), c(-1,1), add=T, lwd=2, lty=4, col=&quot;green&quot;)&#10;legend(0.5,1.1,legend=c(&quot;Normal&quot;,&quot;Uniform&quot;,&quot;Epanech&quot;,&quot;Triangle&quot;), lwd=c(2,2,2,2), lty=c(1,2,3,4), col=c(&quot;black&quot;,&quot;blue&quot;,&quot;red&quot;,&quot;green&quot;),cex=0.8)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Any help please? Please be nice -- I really have been trying hard to understand what is going on. &lt;/p&gt;&#10;" CommentCount="7" CreationDate="2013-03-05T04:45:36.910" FavoriteCount="1" Id="51343" LastActivityDate="2015-02-20T06:15:11.177" LastEditDate="2013-06-08T14:43:22.323" LastEditorUserId="22047" OwnerUserId="21556" PostTypeId="1" Score="2" Tags="&lt;machine-learning&gt;&lt;covariance&gt;&lt;kernel&gt;" Title="Weighted covariance matrix using kernels" ViewCount="330" />
  
  <row Body="&lt;p&gt;Your problem is poorly specified as it's given, so forget the 50 packet size case - give us an example for how you'd do it with only 3 different packet sizes. That'll clarify your problem so we can make reasonable suggestions.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-05T07:01:03.080" Id="51346" LastActivityDate="2013-03-05T07:01:03.080" OwnerUserId="34" ParentId="51056" PostTypeId="2" Score="1" />
  
  
  
  <row Body="&lt;p&gt;I think that causality is about some underlying process for how something &quot;works&quot;.  Regression is much more to do with association and correlation.  So you need much more than significant coefficients to claim that X &quot;causes&quot; anything.  You need to be able to use your theory to make quantitative and/or qualitative predictions about what happens to Y and Z when X varies.  If you can't do this without using your data, then causality is out of reach.  The data can only tell you whether or not your predictions are accurate.  This means that to get a definitive answer on causality, you really need to be able to make &lt;em&gt;sharp&lt;/em&gt; predictions.&lt;/p&gt;&#10;&#10;&lt;p&gt;In terms of your specific questions, if X causes both Z and Y, then we can infer that Z and Y are both related to each other via the inference path $Y\to X \to Z$.  For example, if $Y\approx X\beta$ and $Z\approx X\alpha$ means that $Z\approx Y\frac{\alpha}{\beta}$.  This is the omitted variable problem.  As far as testing significance goes, I think you should really be going back to whatever theory you've obtained your causal hypothesis from and see ask those same three questions of your theory.  For example could the causal sequence go X causes Y and Z, but Y also causes Z.  Could Z or Y feedback onto X in a causal cycle?  Its all about testing the predictions made from your theory.  I think a legitimate test is one that has meaning in terms of your theory.&lt;/p&gt;&#10;&#10;&lt;p&gt;Forgive the assumption on my part, but your question is a bit vague/abstract, and it sounds more like you've found associations and/or relationships that &quot;make sense&quot; after looking at the estimates.  While this is a good way to validate your regression results, you need to give evidence of being able to predict the result from the theory without using the data.  Otherwise I think you are simply confusing association with causation.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-03-05T12:32:13.267" Id="51373" LastActivityDate="2013-03-05T12:32:13.267" OwnerUserId="2392" ParentId="51361" PostTypeId="2" Score="0" />
  
  
  
  <row Body="&lt;p&gt;Using a standard framework: your TIME main effect is about time collapsing over CONDITION. Your CONDITION variable is about condition collapsing over TIME.  Your interaction is simply stated as &quot;the effect of time depends on condition&quot; or &quot;the effect of condition depends on time&quot;.  You are making an accept/reject decision about the interaction, but a more nuanced (and appropriate) approach would be to look at the effect size.  If the effect size of your interaction really is 0, then you only have evidence that A) there were differences (regardless of condition) as a function of time and B) control was different from treatment.  A is probably acceptable for you.  It is B without a Condition x Time interaction that is bothering you.  B indicates that there are differences between treatment and control, but that the effect of time is the same for both.  Therefore, there may have been baseline differences between those in treatment and control.  You should check for this using a between subjects test using only the Initial Flexion data points.  However, a better solution is just to plot the means for all four cells of your design.  This plot will show both your main effects and your interaction (or lack thereof).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-05T18:18:36.340" Id="51402" LastActivityDate="2013-03-05T18:18:36.340" OwnerUserId="196" ParentId="51399" PostTypeId="2" Score="2" />
  
  
  
  <row Body="&lt;p&gt;It may be important for you to understand the different types of &lt;a href=&quot;http://en.wikipedia.org/wiki/Multilevel_model&quot; rel=&quot;nofollow&quot;&gt;multilevel modeling&lt;/a&gt; strategies out there before you resolve to this method. Your approach is not particularly common. In order to obtain a consistent and unbiased estimate of the slope of the slopes, you should use inverse variance weighting to account for slopes that are especially more variable. This is done easily in R by storing all your fits in a list of fits, then using the coef and vcov methods to extract the relevant information into an analyzable vector format.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-03-05T20:42:33.707" Id="51410" LastActivityDate="2013-03-05T20:42:33.707" OwnerUserId="8013" ParentId="51378" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="51414" AnswerCount="1" Body="&lt;p&gt;&lt;strong&gt;Preliminaries&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;For simplicity's sake assume we are dealing with a 2-dimensional dataset of examples $(x_i, y_i) \in \mathbb{R}^2$ which are split into a training (objects and their label known) and test set (only objects known).     &lt;/p&gt;&#10;&#10;&lt;p&gt;Prediction using &lt;em&gt;least squares&lt;/em&gt; can be represented using the following matrix procedure:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;From our training set find the optimal regressor constant&#10;$w = Y'X(X'X)^{-1}$&lt;/li&gt;&#10;&lt;li&gt;Apply the above to successive unlabeled examples from the test set to find their label $\hat{y} = w'x = Y'X(X'X)^{-1}x$&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;The Question&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;After you've finished part 1. of the above procedure how do you find the optimal linear least squares line? &lt;/p&gt;&#10;&#10;&lt;p&gt;If a linear line can be represented using $y = w'x + c$ how do you find the constant $c$? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-05T21:01:38.727" Id="51413" LastActivityDate="2014-12-27T21:21:57.677" OwnerUserId="17076" PostTypeId="1" Score="3" Tags="&lt;regression&gt;&lt;least-squares&gt;&lt;algorithms&gt;" Title="What happens to the constant in Least Squares" ViewCount="256" />
  
  <row Body="&lt;p&gt;If you look at the names in your kmeans object you will notice that there is a &quot;cluster&quot; object. This contains the class labels ordered the same as your input data. Here is a simple example that binds the cluster labels back to your data.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x &amp;lt;- data.frame(X=rnorm(100, sd=0.3), Y=rnorm(100, mean=1, sd=0.3))&#10;&#10;k &amp;lt;- kmeans(x, 2) &#10;names(k)&#10;x &amp;lt;- data.frame(x, K=k$cluster)&#10;&#10;# You can also directly return the clusters&#10;x &amp;lt;- data.frame(x, K=kmeans(x, 2)$cluster)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2013-03-05T22:43:31.640" Id="51419" LastActivityDate="2013-03-05T22:43:31.640" OwnerUserId="17198" ParentId="51418" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Craigslist Missed Connections? Seriously, lots of interesting data there, fairly easy to get at, and it seems under-explored. Here's &lt;a href=&quot;http://www.buzzfeed.com/mjs538/fascinating-map-of-the-most-frequently-cited-locations-where&quot; rel=&quot;nofollow&quot;&gt;one neat example&lt;/a&gt; to inspire. It's not quite as rich as the &lt;a href=&quot;http://blog.okcupid.com/&quot; rel=&quot;nofollow&quot;&gt;OK Trends&lt;/a&gt; data, but it's available. One example might be how well the demographics mirror the census data for your city.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-06T01:31:07.063" Id="51427" LastActivityDate="2013-03-06T01:31:07.063" OwnerUserId="7071" ParentId="51425" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;Let's say there are only two choices A and B. If you make decision A, then the naive person is 30 percent correct in their choice; if you make decision B, then the naive person is 20 correct in their choice. Historically let 60% times A be the right answer. You making decision A happens  $0.6 * 0.9 + 0.4 * 0.1$. Hence the other person being correct would be $(0.6 * 0.9 + 0.4 * 0.1)*0.3 + (1-0.6 * 0.9 - 0.4 * 0.1)*0.2$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then solve for x as before?&lt;/p&gt;&#10;&#10;&lt;p&gt;Just my 2 cents.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-06T07:58:19.677" Id="51445" LastActivityDate="2014-11-19T18:18:18.647" LastEditDate="2014-11-19T18:18:18.647" LastEditorUserId="22047" OwnerUserId="21072" ParentId="51390" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;I think the equation is correct.  v is computed from the two standard deviations but also from the sample sizes. The square root of v is the standard deviation of L. But the standard deviation of a computed parameter &lt;em&gt;is&lt;/em&gt; a standard error, just like the standard deviation of a sample mean &lt;em&gt;is&lt;/em&gt; the standard error of the mean (which is very different than the standard deviation of the data or the distribution itself). &lt;/p&gt;&#10;&#10;&lt;p&gt;When you look at a set of values, the standard deviation of those values is very different than the standard error of the mean of those values. But when you look at a estimated (via calculations) parameter, the &quot;standard deviation&quot; of that parameter and the &quot;standard error&quot; of that parameter are really the same. Or more precisely, the standard error is an estimate of the unknown standard deviation.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-03-06T14:20:11.637" Id="51472" LastActivityDate="2013-03-06T17:17:04.913" LastEditDate="2013-03-06T17:17:04.913" LastEditorUserId="25" OwnerUserId="25" ParentId="51450" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="51481" AnswerCount="3" Body="&lt;p&gt;I am looking to construct a predictive model where the outcome variable is binary and the input is time series. To make it more concrete, the model will predict if a customer churns (left the company; coded as 1 or 0) based on the amount they spent with the company in the prior 60 days. So, the data is one customer per row and the columns are an outcome factor (1 or 0) and 60 additional columns for the amount spent the in time t-1, t-2....t-60.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is some example data:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#create the data a series of length 60 and a class ID&#10;sc &amp;lt;- read.table(&quot;http://kdd.ics.uci.edu/databases/synthetic_control/synthetic_control.data&quot;, header=F, sep=&quot;&quot;)&#10;&#10;#binary class lable&#10;classId &amp;lt;- as.factor(c(rep(0,300), rep(1,300)))&#10;newSc &amp;lt;- data.frame(cbind(classId, sc))&#10;newSc$ID&amp;lt;-seq(1,600,1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The actual model may have many of these series for each customer, so I need to reduce the dimensionality of the data for the series, e.g. instead of using 60 values, I need to reduce this down to a handful. Of course, I can use the mean, min, max etc of the series but I have been reading about using Discrete Fourier Transform. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Questions:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Is the DFFT in &lt;a href=&quot;http://stat.ethz.ch/R-manual/R-patched/library/stats/html/fft.html&quot; rel=&quot;nofollow&quot;&gt;R&lt;/a&gt; a proper method to use for my purpose? Any information on how it works would be appreciated.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Assuming this R function is correct, how do you extract just the most meaningful coefficients to achieve dimensionality reduction?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;ADD:&#10;There seems to be a consensus that using DFFT for dimension reduction is not a wise choice, but it seems that in data mining, this function, DWT and SVD are all commonly used: &#10;&lt;a href=&quot;http://www.cs.gmu.edu/~jessica/BookChapterTSMining.pdf&quot; rel=&quot;nofollow&quot;&gt;Time Series Mining&lt;/a&gt; starting on page 20.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-03-06T15:31:02.723" FavoriteCount="5" Id="51475" LastActivityDate="2013-03-07T13:29:47.867" LastEditDate="2013-03-07T13:29:47.867" LastEditorUserId="88" OwnerUserId="2040" PostTypeId="1" Score="3" Tags="&lt;data-mining&gt;&lt;data-transformation&gt;&lt;dimensionality-reduction&gt;&lt;signal-processing&gt;" Title="Series dimensionality reduction for classification Input" ViewCount="408" />
  <row Body="&lt;p&gt;The linked paper will be somewhat enlightening, since it is interested in the more or less the same issue in another context.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://nips.cc/Conferences/2012/Program/event.php?ID=3217&quot; rel=&quot;nofollow&quot;&gt;http://nips.cc/Conferences/2012/Program/event.php?ID=3217&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-06T16:03:28.210" Id="51480" LastActivityDate="2013-03-06T16:03:28.210" OwnerUserId="12534" ParentId="50807" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;The best tool to resolve (multi-) collinearity is in my view the Cholesky-decomposition of the correlation/covariance matrix.  The following example discusses even the case of collinearity, where none of the bivariate correlations are &quot;extreme&quot;, because we have rank-reduction only over sets of more variables than only two.      &lt;/p&gt;&#10;&#10;&lt;p&gt;If the correlation-matrix, say &lt;strong&gt;R&lt;/strong&gt;, is positive definite, then all entries on the diagonal of the cholesky-factor, say &lt;strong&gt;L&lt;/strong&gt;, are non-zero (aka machine-epsilon). Btw, to use this tool for the collinearity-detection it must be implemented as to allow zero-eigenvalues, don't know, whether, for instance, you can use SPSS for this.&lt;br&gt;&#10;The number of on-zero entries in the diagonal indicate the actual rank of the correlation-matrix. And because of the triangular structure of the &lt;strong&gt;L&lt;/strong&gt;-matrix the variables &lt;em&gt;above&lt;/em&gt; the first occuring diagonal zero form a partial set of variables which is of reduced-rank. However, there may be some variables in that block, which do not belong to that set, so to find the crucial subset which contains &lt;strong&gt;only&lt;/strong&gt; the multicollinearity you do several recomputations of the cholesky-decomposition, where you reorder the variables such that you find the smallest possible subset, which shows rank-reduction - so this is an iterative procedure. &#10;(If needed, I'll show an example where I use my MatMate-program for the script, later).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;hr&gt;&#10;Here is an example using random-data on 5 variables, say $x_1$ to $x_5$ which I configured, such that the correlation matrix is positive semidefinite (up to machine precision) because I made $x_5 = 2 \cdot x_2 + \sqrt 2 \cdot x_4 $ (and after that normed to unit-variance) and thus that subset of three variables make a collinear subspace (more exactly: we should call it &quot;co-planar&quot; since they are linearly dependent only in a plane). &#10;here is the correlation-matrix&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;;MatMate-Listing vom:06.03.2013 17:43:23&#10;;============================================&#10;&#10;C=        x1        x2        x3        x4        x5&#10;x1        1.0000    -0.7506   0.2298    -0.8666   0.0952&#10;x2        -0.7506   1.0000    -0.2696   0.4569    0.5355&#10;x3        0.2298    -0.2696   1.0000    0.1890    -0.4407&#10;x4        -0.8666   0.4569    0.1890    1.0000    -0.5066&#10;x5        0.0952    0.5355    -0.4407   -0.5066   1.0000&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and here the cholesky-factor / loadingsmatrix:       &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;[20]     L = cholesky(C)&#10;&#10;L=        f1        f2        f3        f4        f5&#10;x1        1.0000     .         .         .         .    &#10;x2        -0.7506   0.6607     .         .         .    &#10;x3        0.2298    -0.1469   0.9621     .         .    &#10;x4        -0.8666   -0.2930   0.3587    0.1856     .    &#10;x5        0.0952    0.9186    -0.3406   -0.1762    .    &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;As we see, that only 4 of 5 diagonal elements are non-zero (above machine-epsilon) we know, that correlation matrix has rank 4 instead of 5 and we have collinearity. But we do not yet know, whether 4 variables are linnearly dependent or whether we have possibly a rank reduced subspace of smaller dimension. So we try iteratively the rotation to triangularity, where the order of the variables $x_1$ to $x_5$ is systematically altered to identify any possible smalles subset.       &lt;/p&gt;&#10;&#10;&lt;p&gt;For instance, we make the last item &quot;the first&quot; &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;[22] l1=rot(L,&quot;drei&quot;,5´1´2´3´4)&#10;L1=       f1        f2        f3        f4        f5&#10;x1        0.0952    0.9955     .         .         .    &#10;x2        0.5355    -0.8053   0.2545     .         .    &#10;x3        -0.4407   0.2730    0.7320    0.4421     .    &#10;x4        -0.5066   -0.8221   0.2598     .         .    &#10;x5        1.0000     .         .         .         .    &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and we see,. that rank-reduction is already occuring if we ignore variable 3, because the variables $x_1,x_2,x_4,x_5$ define already a 3-dimensional subspace (instead of a 4-dimensional one).         &lt;/p&gt;&#10;&#10;&lt;p&gt;Now we proceed altering the order for the cholesky-decomposition (actually I do this by a column rotation with a &quot;triangularity-criterion&quot;):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;[24] l1=rot(L,&quot;drei&quot;,5´4´1´2´3)&#10;L1=       f1        f2        f3        f4        f5&#10;x1        0.0952    -0.9492   0.3000     .         .    &#10;x2        0.5355    0.8445     .         .         .    &#10;x3        -0.4407   -0.0397   0.7803     .        -0.4421&#10;x4        -0.5066   0.8622     .         .         .    &#10;x5        1.0000     .         .         .         .    &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now we've nearly done: the subset of $x_2,x_4,x_5$ forms a reduced subspace and to see more, we put them at &quot;the top&quot; of the cholesky-process:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;[26] l1=rot(L,&quot;drei&quot;,5´4´2´1´3)&#10;L1=       f1        f2        f3        f4        f5&#10;x1        0.0952    -0.9492    .        0.3000     .    &#10;x2        0.5355    0.8445     .         .         .    &#10;x3        -0.4407   -0.0397    .        0.7803    0.4421&#10;x4        -0.5066   0.8622     .         .         .    &#10;x5        1.0000     .         .         .         .    &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and see, that $x_1$ has a component outside of that reduced space, and $x_3$ has a further component outside of the  rank 3 space, and are thus partly independent of that 2-dimensional subspace (which can thus be give the term &quot;co-planarity&quot;).&#10;We can now decide, which of the three variables $x_2,x_4$ or $x_5$ can be removed to overcome the multi-collinearity problem.       &lt;/p&gt;&#10;&#10;&lt;p&gt;If we would use some software which does not allow this flexible reordering &quot;inside&quot; the rotation-parameters/procedure, we would re-order the variables forming the correlation-matrix and would do the cholesky-decomposition to arrive at something like:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;[26] l1=cholesky(...) // something in your favorite software...&#10;L1=       f1        f2        f3        f4        f5&#10;------------------------------------------------------&#10;x5        1.0000     .         .         .         .   Co-planar subset  &#10;x2        0.5355    0.8445     .         .         .    &#10;x4        -0.5066   0.8622     .         .         .    &#10;------------------------------------------------------ &#10;x1        0.0952    -0.9492    .        0.3000     .      further linearly independent&#10;x3        -0.4407   -0.0397    .        0.7803    0.4421  variables &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;[update]: Note, that the candidates from which we would remove one, were not necessarily been recognized by the inspection of correlations in the correlation-matrix. There the highest correlation is 0.8666 between $x_1$ and $x_4$ - but $x_1$ does not contribute to the rank-deficiency! Furthermore, the correlations between $x_2,x_4,x_5$ are all in an &quot;acceptable&quot; range when one wants to apply some jackknife-estimate for the removal of high-correlations assuming multicollinearity - one would not look at them as the most natural candidates from the set of &lt;strong&gt;&lt;em&gt;bivariate&lt;/em&gt;&lt;/strong&gt; correlations only.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-06T16:43:16.110" Id="51486" LastActivityDate="2013-03-06T20:46:16.837" LastEditDate="2013-03-06T20:46:16.837" LastEditorUserId="1818" OwnerUserId="1818" ParentId="51473" PostTypeId="2" Score="2" />
  <row AnswerCount="0" Body="&lt;p&gt;I've got 2 tables, clicks and impressions.&lt;/p&gt;&#10;&#10;&lt;p&gt;The clicks table saves a user's clicks on an advertisement. It is something like:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;advertise id, user id, time of click&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The impressions table saves the impressions that a user made to an advertisement. It is like:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;advertise id, user id, time of impression&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;For ranking the ads, I am using the CTR:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;total clicks for the advertisement&lt;/code&gt; / &lt;code&gt;total impressions for the advertise&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;A few days ago, I noticed that usually the users that don't click after the second impression never click the advertisement. Do you know how I can use this data to rank the advertisements  per user?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-06T17:52:23.663" FavoriteCount="2" Id="51499" LastActivityDate="2014-03-06T00:06:14.710" LastEditDate="2014-03-06T00:06:14.710" LastEditorUserId="32036" OwnerUserId="21628" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;conditional-probability&gt;&lt;ranking&gt;" Title="How to get the probability of clicking an ad after the $n$th impression?" ViewCount="71" />
  <row Body="&lt;p&gt;NOT all the MCMC methods avoid the need for the normalising constant. However, many of them do (such as the Metropolis-Hastings algorithm), since the iteration process is based on the ratio $R(\theta_1,\theta_2)=\dfrac{\pi(\theta_1\vert x)}{\pi(\theta_2\vert x)}$, where&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\pi(\theta\vert x) = \dfrac{\pi(x\vert \theta)\pi(\theta)}{\int \pi(x\vert \theta)\pi(\theta) d\theta} = \dfrac{\pi(x\vert \theta)\pi(\theta)}{\pi(x)},$$&lt;/p&gt;&#10;&#10;&lt;p&gt;is the posterior distribution of $\theta$ given the sample $x$. Therefore, the normalising constant $\pi(x)$ in the denominator does not depend on $\theta$ and it cancels out when you calculate $R(\theta_1,\theta_2)$. This is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$R(\theta_1,\theta_2)= \dfrac{\pi(x\vert \theta_1)\pi(\theta_1)}{\pi(x\vert \theta_2)\pi(\theta_2)},$$&lt;/p&gt;&#10;&#10;&lt;p&gt;which does not involve the normalising constant, only the likelihood $\pi(x\vert \theta)$ and the prior $\pi(\theta)$.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-06T17:53:10.927" Id="51500" LastActivityDate="2013-03-06T17:59:21.153" LastEditDate="2013-03-06T17:59:21.153" LastEditorUserId="21631" OwnerUserId="21631" ParentId="51493" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;There may be a more authoritative answer for your specific case, but derived/transformed variables are used routinely in regressions.&lt;/p&gt;&#10;&#10;&lt;p&gt;Given that, I can see three issues in your case:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;How do you express your count? For example, it may be better to express strikes as a percentage of the days in the month that had strikes, rather than a count.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;How do you count strikes and how can they occur? Can people be on strike for an hour rather than a whole day? Do you count days from the beginning of a strike to the end, or only days they'd otherwise work? (E.g. if people don't normally work on weekends, do they count as strike days?)&lt;/p&gt;&#10;&#10;&lt;p&gt;(If you wish to do something that would require forecasting strike probabilities, this could also determine whether you need to account for a zero-inflation effect.)&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;If strikes can occur for multiple days, is a seven-day strike the same as seven one-day strikes or might it have greater or less effect?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;EDIT: Gelman mentions on page 69 of Gelman &amp;amp; Hill (2007) that, &quot;... sometimes several inputs can be averaged or summed to create a 'total score' that can be used as a single predictor in the model.&quot; I don't immediately have other references or any particularly for aggregate variables.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-06T19:28:11.627" Id="51511" LastActivityDate="2013-03-06T21:15:55.593" LastEditDate="2013-03-06T21:15:55.593" LastEditorUserId="1764" OwnerUserId="1764" ParentId="51509" PostTypeId="2" Score="4" />
  <row AcceptedAnswerId="51532" AnswerCount="2" Body="&lt;p&gt;Assume I have a data set as follows:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Total_Cost  Average_Cost  Quantity&#10;    12           6            2&#10;    21           7            3&#10;    28           7            4&#10;    30           6            5&#10;    36           6            6&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The linear relationship between Total_Cost and Quantity is nearly perfect, while between Average_Cost and Quantity is very poor. However, I saw many researches that basically use average cost as a response but not total cost.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question here is therefore: Are there any particular reasons for us to study the linear relationship between Average_Cost and whatever independent variables? (For instance, the use of Total_Cost as a response is way too straightforward.)&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-03-06T21:27:03.440" Id="51524" LastActivityDate="2013-03-06T22:04:22.297" LastEditDate="2013-03-06T21:50:31.387" LastEditorUserId="21071" OwnerUserId="21071" PostTypeId="1" Score="0" Tags="&lt;mathematical-statistics&gt;&lt;econometrics&gt;" Title="Why would we want to use average cost as response but not total cost?" ViewCount="90" />
  
  
  <row Body="&lt;p&gt;You may have discovered the fact that LDA models can suffer from &lt;em&gt;instability&lt;/em&gt;: slight changes in the training set lead to very different models. &lt;/p&gt;&#10;&#10;&lt;p&gt;There's a whole body of literature on the variance of resampling methods for validation. This may give you a good start.&lt;/p&gt;&#10;&#10;&lt;p&gt;See also &lt;a href=&quot;http://stats.stackexchange.com/a/31507/4598&quot;&gt;here for a related discussion&lt;/a&gt;.&#10;(I'm sure there is a literature list somewhere around here, but I cannot find it right now)&lt;/p&gt;&#10;&#10;&lt;p&gt;Anyways,&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Braga-Neto has been working a lot on this&lt;/li&gt;&#10;&lt;li&gt;There's a very nice technical report by Ron Kohavi&lt;/li&gt;&#10;&lt;li&gt;We also did a study about &lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0169743905000687&quot; rel=&quot;nofollow&quot;&gt;Variance reduction in estimating classification error using sparse datasets&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;And another one on &lt;a href=&quot;http://link.springer.com/article/10.1007%2Fs00216-007-1818-6&quot; rel=&quot;nofollow&quot;&gt;Assessing and improving the stability of chemometric models in small sample size situations&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="2" CreationDate="2013-03-06T22:09:14.787" Id="51533" LastActivityDate="2013-03-06T22:09:14.787" OwnerUserId="4598" ParentId="51488" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Taking the log of a predictor (independent) variable should have no effect, as CART is invariant to monotonic transformations of the predictors.&lt;/p&gt;&#10;&#10;&lt;p&gt;See this example where I fit a CART with Age (named &lt;code&gt;fit1&lt;/code&gt;) and log(Age) (named &lt;code&gt;fit2&lt;/code&gt;). The split points are the same for both trees (only that the split points in &lt;code&gt;fit2&lt;/code&gt; are on the log scale, e.g., $4.706 \approx \ln(111)$), the predictions and nodes are exactly the same. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;R&amp;gt; library(rpart)&#10;R&amp;gt; fit &amp;lt;- rpart(Kyphosis ~ Age + Number + Start, data=kyphosis)&#10;R&amp;gt; fit2 &amp;lt;- rpart(Kyphosis ~ log(Age) + Number + Start, data=kyphosis)&#10;R&amp;gt; fit&#10;n= 81 &#10;&#10;node), split, n, loss, yval, (yprob)&#10;      * denotes terminal node&#10;&#10; 1) root 81 17 absent (0.79012 0.20988)  &#10;   2) Start&amp;gt;=8.5 62  6 absent (0.90323 0.09677)  &#10;     4) Start&amp;gt;=14.5 29  0 absent (1.00000 0.00000) *&#10;     5) Start&amp;lt; 14.5 33  6 absent (0.81818 0.18182)  &#10;      10) Age&amp;lt; 55 12  0 absent (1.00000 0.00000) *&#10;      11) Age&amp;gt;=55 21  6 absent (0.71429 0.28571)  &#10;        22) Age&amp;gt;=111 14  2 absent (0.85714 0.14286) *&#10;        23) Age&amp;lt; 111 7  3 present (0.42857 0.57143) *&#10;   3) Start&amp;lt; 8.5 19  8 present (0.42105 0.57895) *&#10;R&amp;gt; fit2&#10;n= 81 &#10;&#10;node), split, n, loss, yval, (yprob)&#10;      * denotes terminal node&#10;&#10; 1) root 81 17 absent (0.79012 0.20988)  &#10;   2) Start&amp;gt;=8.5 62  6 absent (0.90323 0.09677)  &#10;     4) Start&amp;gt;=14.5 29  0 absent (1.00000 0.00000) *&#10;     5) Start&amp;lt; 14.5 33  6 absent (0.81818 0.18182)  &#10;      10) log(Age)&amp;lt; 4.005 12  0 absent (1.00000 0.00000) *&#10;      11) log(Age)&amp;gt;=4.005 21  6 absent (0.71429 0.28571)  &#10;        22) log(Age)&amp;gt;=4.706 14  2 absent (0.85714 0.14286) *&#10;        23) log(Age)&amp;lt; 4.706 7  3 present (0.42857 0.57143) *&#10;   3) Start&amp;lt; 8.5 19  8 present (0.42105 0.57895) *&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2013-03-06T22:09:38.910" Id="51534" LastActivityDate="2013-03-07T09:47:30.523" LastEditDate="2013-03-07T09:47:30.523" LastEditorUserId="8413" OwnerUserId="8413" ParentId="51523" PostTypeId="2" Score="8" />
  <row AnswerCount="1" Body="&lt;p&gt;I'm trying to use the ridge.cv() function in R. The &lt;a href=&quot;http://www.inside-r.org/packages/cran/parcor/docs/ridge.cv&quot; rel=&quot;nofollow&quot;&gt;documentation&lt;/a&gt; says that the input y is the &quot;vector of responses&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;What exactly does that mean?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-07T01:53:00.193" Id="51547" LastActivityDate="2013-03-07T01:56:31.537" OwnerUserId="21064" PostTypeId="1" Score="-2" Tags="&lt;r&gt;&lt;ridge-regression&gt;" Title="How to use ridge.cv in R?" ViewCount="120" />
  
  
  <row Body="&lt;p&gt;I thought I will provide some intuition, as the answer is already given. The second answer is incorrect, because a) seat 15 is impossible to be selected for rows 1-4, and b) row 5 onwards, it is more likely to be selected for a lower numbered row.&lt;/p&gt;&#10;&#10;&lt;p&gt;So given seat 15 is selected, the chance of a particular row to have been picked is definitely not equally likely.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-07T04:16:06.720" Id="51561" LastActivityDate="2013-03-07T04:16:06.720" OwnerUserId="994" ParentId="38870" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;As far as I know, there is no reason you can't use a difference score as an independent variable in a regression. It violates no assumptions. &lt;/p&gt;&#10;&#10;&lt;p&gt;Your second question is more complex. Your idea of using T1 measures as covariates is often done. People also sometimes use difference scores as a DV (as you probably know from your reading).&lt;/p&gt;&#10;&#10;&lt;p&gt;There are some problems with pre- post- testing when the variables are measured with error (as all psychological variables inevitably are). If I recall correctly, there are details in &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/1557983100&quot; rel=&quot;nofollow&quot;&gt;Collins and Horn&lt;/a&gt;. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-03-07T12:04:40.017" Id="51578" LastActivityDate="2013-03-07T12:04:40.017" OwnerUserId="686" ParentId="51564" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Let's say each user &lt;code&gt;u&lt;/code&gt; is represented by a (sparse) vector of features &lt;code&gt;f[u]&lt;/code&gt;;  each campaign &lt;code&gt;c&lt;/code&gt; is also represented by a vector features in the same space &lt;code&gt;f[c]&lt;/code&gt;. You could simply find a nearest neighbour (or several of those) by using &lt;a href=&quot;http://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm&quot; rel=&quot;nofollow&quot;&gt;kNN&lt;/a&gt; e.g. with Manhattan distance, or some other measure that suits your features better. Of course, this will not work nicely for a user for which you do not have data at all. &lt;/p&gt;&#10;&#10;&lt;p&gt;Another approach is to use &lt;a href=&quot;http://en.wikipedia.org/wiki/Collaborative_filtering&quot; rel=&quot;nofollow&quot;&gt;collaborative filtering&lt;/a&gt; i.e. showing a user stuff that similar users liked/viewed/clicked. In your settings you might prefer this one if you have at least some data about a user after registration.&lt;/p&gt;&#10;&#10;&lt;p&gt;Both these approaches have a scalable implementation e.g. in Apache Mahout.&lt;/p&gt;&#10;&#10;&lt;p&gt;I think the problem that you are referring to (&lt;em&gt;a brand new user&lt;/em&gt;) is a so-called &lt;em&gt;cold start problem&lt;/em&gt;, and I do not know whether it has an ultimate solution. Though there are methods that can tackle it, take a look at the survey (Methods and metrics for cold-start recommendations, Schein et al, 2002).&lt;/p&gt;&#10;&#10;&lt;p&gt;P.S. &#10;As for Infer.Net, it is a very cool toolbox which possibly can also solve your problem - authors created an opponent recommender system for XBox with it, called TrueSkill. But it actually would require some knowledge in Bayesian learning. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;update&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;If the number of campaigns is not too big, you could use naive bayes classification. Each campaign is a separate class &lt;code&gt;c_i&lt;/code&gt;. The training phase boils down to computing likelihoods for separate features (e.g. &lt;code&gt;p(country=usa|campaign=c_i)&lt;/code&gt; - probability that the country is &lt;code&gt;usa&lt;/code&gt; given that his campaign is &lt;code&gt;c_i&lt;/code&gt;), each can be a frequency of a certain value of a feature. One should also compute priors &lt;code&gt;p(campaign=c_i)&lt;/code&gt; - which shows how frequent campaign &lt;code&gt;c_i&lt;/code&gt; is. &lt;/p&gt;&#10;&#10;&lt;p&gt;Then in production for each &lt;code&gt;c_i&lt;/code&gt; you compute posterior probability &lt;code&gt;p(campaign=c|country=usa,viewed_item1=True,)&lt;/code&gt;, which is simply a product of prior and likelihoods. The campaign with the highest probability is assigned to the user. I would assume that one could use sparsity and do computations efficiently. Note that this approach assumes that all the features are independent.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-03-07T12:17:26.837" Id="51579" LastActivityDate="2013-03-07T17:35:57.217" LastEditDate="2013-03-07T17:35:57.217" LastEditorUserId="20409" OwnerUserId="20409" ParentId="51557" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;&lt;strong&gt;I have a question about a very basic classification problem.&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The objective is to discriminate between feature A and B. &#10;Where A is a specific biological sample, and B all others.&#10;And the goal is to classify new unknown samples.&lt;/p&gt;&#10;&#10;&lt;p&gt;To do so I have 12 variables with binary data, representing the presence or absence of certain traits.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example: 6 (3 known A and 3 known B) samples with 4 variables.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;       Feature   Var1. Var2. Var3. Var4.  &#10;Sample1    *A*       1     1     1     1  &#10;Sample2    *A*       1     0     1     1  &#10;Sample3    *A*       0     1     1     0  &#10;Sample4    *B*       1     0     0     0  &#10;Sample5    *B*       0     0     1     0  &#10;sample6    *B*       0     1     1     0  &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;What kind of classification algorithm can I use to predict a new unknown sample?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-03-07T14:55:43.560" Id="51588" LastActivityDate="2013-07-06T13:02:14.303" LastEditDate="2013-07-06T13:02:14.303" LastEditorUserId="6029" OwnerUserId="21677" PostTypeId="1" Score="2" Tags="&lt;classification&gt;&lt;binary&gt;" Title="Classification of biological samples with binary variables" ViewCount="72" />
  
  
  <row Body="&lt;p&gt;What you described is &lt;em&gt;stratification&lt;/em&gt;: you know before you sample that a given unit is a professor, or a student, or a law enforcement officer. If you know something about an observation unit beforehand, that's typically is (or can be) a stratification variable. Now, the &lt;em&gt;clusters&lt;/em&gt; would be units you would sample together for logistics reasons: you don't have a full list of students in your country, but you have a full list of universities, and you can sample may be 20 of these, and try to reach students or professors in these universities somehow (that's difficult, but I will leave these difficulties to you). Now, university is then a cluster. Within that cluster, you stratify your potential respondents into professors and students, and take samples of these independently. So you have multiple complex sample features:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;You stratified your population into academic vs. government&lt;/li&gt;&#10;&lt;li&gt;You took a cluster sample of universities in the academic stratum&lt;/li&gt;&#10;&lt;li&gt;You stratified each cluster into professors and students&lt;/li&gt;&#10;&lt;li&gt;You took samples of professors and students from their respective (within-cluster) strata&lt;/li&gt;&#10;&lt;li&gt;You stratified the government sample by agency (say you have police to deal with crimes, and education people to deal with universities)&lt;/li&gt;&#10;&lt;li&gt;You can further take a cluster sample of police departments within the enforcement agency&lt;/li&gt;&#10;&lt;li&gt;Finally, you get down to policemen by sampling within the cluster (police department)&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;There will be tricks along the way -- you would want to sample proportional to size when taking clusters, for the reasons you can find in any good sampling book.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-07T16:09:04.590" Id="51600" LastActivityDate="2013-03-07T16:09:04.590" OwnerUserId="5739" ParentId="51571" PostTypeId="2" Score="1" />
  
  
  <row Body="Clustered standard errors represent the version of the general sandwich variance estimator that correct for (potential) grouping of the observations, e.g., repeated measurements clustered within an individual, or individuals clustered within a hierarchy level (geographical region, educational institution, etc.)." CommentCount="0" CreationDate="2013-03-07T16:19:11.683" Id="51609" LastActivityDate="2013-03-07T17:45:44.727" LastEditDate="2013-03-07T17:45:44.727" LastEditorUserId="5739" OwnerUserId="5739" PostTypeId="4" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;Hello and thanks for this source about statistics. My Problem is about a situation where Kaplan Meier curves differ in meaning of a significant log rank test, but the hazard ratio crosses 1 when looking on the 95% CI.&#10;How to interpret this?&#10;Could the problem be too much censored data?&#10;I have some individuals with data for &gt; 3000 days and most with data for only about 500 days.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance an greetings from switzerland.&lt;/p&gt;&#10;&#10;&lt;p&gt;Martin&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-03-07T17:11:41.933" Id="51616" LastActivityDate="2013-03-07T17:11:41.933" OwnerUserId="21688" PostTypeId="1" Score="0" Tags="&lt;hazard&gt;&lt;kaplan-meier&gt;&lt;logrank&gt;" Title="KM curve: log rank test significant but HR crosses 1" ViewCount="188" />
  
  
  
  
  
  
  <row Body="&lt;p&gt;How good an estimate do you need?  The number of active Twitter users (for some definition of active) must be available from Twitter or online news articles.  What fraction drive to work?  Certainly not more than $100\%.$  Probably more than $10\%$  Now we are within a factor $3.$  Sounds flip, but maybe it is good enough.  What later estimates are you going to make (uptake rate of a product?  That won't be this good)  To do better, there must be data online about fractions of country populations that drive to work.  Twitter people might be somewhat more or less prone to drive to work, but that will get you closer if you can get the Twitter distribution by country.  I would guess that would get you to $\pm 30-50\%$.  To do better you might have to pay for a survey. Maybe that still wouldn't be better, but maybe it is more impressive.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-03-08T17:52:10.480" Id="51727" LastActivityDate="2013-03-08T17:52:10.480" OwnerDisplayName="Ross Millikan" OwnerUserId="21745" ParentId="51726" PostTypeId="2" Score="1" />
  
  
  
  <row AcceptedAnswerId="51775" AnswerCount="1" Body="&lt;p&gt;EDIT: I've modified my STAN code and it looks like I am getting numbers close to using R's &lt;code&gt;arima&lt;/code&gt;. The original code, now moved to the end, was incorrect.&lt;/p&gt;&#10;&#10;&lt;p&gt;I've been using STAN for simple things, but want to do a linear regression that has ARMA(1,1) noise. I've heard that this is basically equivalent to AR(1) + White Noise. I've made an attempt that gives reasonable numbers but I can't tell if I'm actually doing what I think I'm doing.&lt;/p&gt;&#10;&#10;&lt;p&gt;The reason I'm attempting to do this is that the data is a climate temperature time series, which is thus serially correlated, and so I believe OLS regression  -- used to calculate a trend -- will underestimate the uncertainty in the regression, which is reflected in the SE's of the slope and intercept coefficients. (And also in the residual error?)&lt;/p&gt;&#10;&#10;&lt;p&gt;My code, using &lt;code&gt;rstan&lt;/code&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;temps &amp;lt;-&#10;    structure(c(-0.59, -0.17, 0.05, -0.7, -0.27, -0.94, -0.69, -0.96, &#10;    -0.58, -0.35, -0.58, -0.54, -0.48, -1.41, -0.82, -0.73, -0.48, &#10;    -0.37, -0.07, -0.16, -0.58, -0.43, -0.16, -0.19, -0.81, -0.37, &#10;    -0.52, -0.55, -0.51, -0.85, -0.43, -0.72, -0.43, -0.63, 0.16, &#10;    -0.26, -0.14, -0.48, -0.61, -0.36, -0.05, 0.22, -0.34, -0.23, &#10;    -0.2, -0.18, 0.51, -0.2, 0.28, -0.53, -0.07, 0.09, 0.45, -0.27, &#10;    -0.12, -0.35, -0.21, 0.11, 0.37, 0.09, -0.18, 0.14, 0.35, -0.16, &#10;    0.62, 0.04, 0.32, -0.12, 0.41, 0.28, -0.4, -0.34, 0.2, 0.26, &#10;    -0.27, 0.44, -0.11, -0.15, 0.68, 0.24, 0.12, 0.16, 0.33, 0.12, &#10;    -0.04, -0.01, -0.22, -0.14, -0.26, -0.42, -0.01, -0.1, -0.57, &#10;    0.16, -0.31, 0.12, 0.06, -0.19, 0.1, -0.01, 0.16, 0.81, -0.13, &#10;    0.53, 0.31, 0.06, 0.34, 0.23, 0.57, 0.07, 0.41, 0.51, 0.52, 0.39, &#10;    0.34, 0.73, 0.3, 0.38, 0.66, 0.6, 0.35, 0.55, 0.98, 0.89, 0.67, &#10;    0.86, 0.6, 1.31, 0.2, 0.75, 0.72, 0.53, 0.48), .Tsp = c(1880, &#10;    2012, 1), class = &quot;ts&quot;)&#10;&#10;stan.code1 &amp;lt;-&#10;&quot;data&#10;    {&#10;    int&amp;lt;lower=1&amp;gt; N ;&#10;    real x[N] ;&#10;    real y[N] ;&#10;    }&#10;&#10;parameters&#10;    {&#10;    real alpha ;&#10;    real beta ;&#10;    real kappa ;&#10;&#10;    real&amp;lt;lower=0&amp;gt; sigma0 ;&#10;    }&#10;&#10;model&#10;    {&#10;    real sigma[N] ;&#10;&#10;    alpha ~ cauchy (0, 5) ;&#10;    beta  ~ cauchy (0, 5) ;&#10;&#10;    kappa ~ gamma (1.2, 1) ;&#10;    sigma0 ~ gamma (3, 1) ;&#10;    sigma[1] &amp;lt;- sigma0 ;&#10;&#10;    y[1] ~ normal (alpha + beta * x[1], sigma[1]) ;&#10;&#10;    for (n in 2:N)&#10;        {&#10;        sigma[n] &amp;lt;- sigma0 + kappa * sigma[n-1] ;&#10;&#10;        y[n] ~ normal (alpha + beta * x[n], sigma[n]) ;&#10;        }&#10;    }&quot;&#10;&#10;    stan.list1 &amp;lt;- list (N=length (temps), x=1880:2012, y=temps)&#10;&#10;    stan.model1 &amp;lt;- stan (model_code=stan.code1, model_name=&quot;GISS NH Jan&quot;, data=stan.list1, iter=15000, chain=4)&#10;&#10;    print (stan.model1, digits_summary=8)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The alpha, beta, and sigma appear to be reasonable if I use them to throw lines onto the data using &lt;code&gt;abline&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;QUESTIONS: &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Have I really done a linear regression with AR(1) noise plus white noise? That is ARMA(1,1) noise? I believe $\kappa={{1-\theta}\over{1-\phi}}$.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Given that it is done properly -- either I lucked out, or someone will post a correct version of the STAN code -- how would I use kappa in calculating the larger uncertainty interval around the regression?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;EDIT: Original code I used, with &lt;code&gt;rstan&lt;/code&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;temps &amp;lt;-&#10;structure(c(-0.59, -0.17, 0.05, -0.7, -0.27, -0.94, -0.69, -0.96, &#10;-0.58, -0.35, -0.58, -0.54, -0.48, -1.41, -0.82, -0.73, -0.48, &#10;-0.37, -0.07, -0.16, -0.58, -0.43, -0.16, -0.19, -0.81, -0.37, &#10;-0.52, -0.55, -0.51, -0.85, -0.43, -0.72, -0.43, -0.63, 0.16, &#10;-0.26, -0.14, -0.48, -0.61, -0.36, -0.05, 0.22, -0.34, -0.23, &#10;-0.2, -0.18, 0.51, -0.2, 0.28, -0.53, -0.07, 0.09, 0.45, -0.27, &#10;-0.12, -0.35, -0.21, 0.11, 0.37, 0.09, -0.18, 0.14, 0.35, -0.16, &#10;0.62, 0.04, 0.32, -0.12, 0.41, 0.28, -0.4, -0.34, 0.2, 0.26, &#10;-0.27, 0.44, -0.11, -0.15, 0.68, 0.24, 0.12, 0.16, 0.33, 0.12, &#10;-0.04, -0.01, -0.22, -0.14, -0.26, -0.42, -0.01, -0.1, -0.57, &#10;0.16, -0.31, 0.12, 0.06, -0.19, 0.1, -0.01, 0.16, 0.81, -0.13, &#10;0.53, 0.31, 0.06, 0.34, 0.23, 0.57, 0.07, 0.41, 0.51, 0.52, 0.39, &#10;0.34, 0.73, 0.3, 0.38, 0.66, 0.6, 0.35, 0.55, 0.98, 0.89, 0.67, &#10;0.86, 0.6, 1.31, 0.2, 0.75, 0.72, 0.53, 0.48), .Tsp = c(1880, &#10;2012, 1), class = &quot;ts&quot;)&#10;&#10;stan.code1 &amp;lt;-&#10;&quot;data&#10;    {&#10;    int&amp;lt;lower=1&amp;gt; N ;&#10;    real x[N] ;&#10;    real y[N] ;&#10;    }&#10;&#10;parameters&#10;    {&#10;    real alpha ;&#10;    real beta ;&#10;    real kappa ;&#10;&#10;    real&amp;lt;lower=0&amp;gt; sigma ;&#10;    }&#10;&#10;model&#10;    {&#10;    alpha ~ cauchy (0, 5) ;&#10;    beta  ~ cauchy (0, 5) ;&#10;    kappa ~ cauchy (0, 5) ;&#10;    sigma ~ gamma (3, 1) ;&#10;&#10;    y[1] ~ normal (alpha + beta * x[1], sigma) ;&#10;&#10;    for (n in 2:N)&#10;        {&#10;        y[n] ~ normal (alpha + beta * x[n] + kappa * y[n-1], sigma) ;&#10;        }&#10;    }&quot;&#10;&#10;stan.list1 &amp;lt;- list (N=length (temps), x=1880:2012, y=temps)&#10;&#10;stan.model1 &amp;lt;- stan (model_code=stan.code1, model_name=&quot;GISS NH Jan&quot;, data=stan.list1, iter=15000, chain=4)&#10;&#10;print (stan.model1, digits_summary=8)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2013-03-09T14:59:53.667" Id="51770" LastActivityDate="2013-03-09T21:44:34.110" LastEditDate="2013-03-09T21:44:34.110" LastEditorUserId="1764" OwnerUserId="1764" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;mcmc&gt;&lt;stan&gt;" Title="Using STAN (related to BUGS/JAGS) to do linear regression with with ARMA(1,1) noise?" ViewCount="636" />
  
  <row Body="&lt;p&gt;I implemented the &lt;a href=&quot;http://www.cs.wustl.edu/~jain/papers/ftp/psqr.pdf&quot; rel=&quot;nofollow&quot;&gt;P-Square Algorithm for Dynamic Calculation of Quantiles and Histograms without Storing Observations&lt;/a&gt; in a neat Python module I wrote called &lt;a href=&quot;https://bitbucket.org/scassidy/livestats&quot; rel=&quot;nofollow&quot;&gt;LiveStats&lt;/a&gt;. It should solve your problem quite effectively.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-03-09T16:33:02.540" Id="51776" LastActivityDate="2013-03-09T16:33:02.540" OwnerUserId="21774" ParentId="346" PostTypeId="2" Score="3" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I am currently assessing some results I have from a model I applied on a corpus of text data I have mined.&lt;/p&gt;&#10;&#10;&lt;p&gt;My problem is that my professor have told me to use a certain method, and I do not really know how to attack this problem the most sensible way.&lt;/p&gt;&#10;&#10;&lt;p&gt;The main idea is to assess if there is a relationship between months and/or years on my response variable. My data has the following nature:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;DV: Binary (Event or non event)&lt;/p&gt;&#10;  &#10;  &lt;p&gt;IV1: Month where event occurred/not occurred&lt;/p&gt;&#10;  &#10;  &lt;p&gt;IV2: Year where event occured/not occurred&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;I have a total of 431.000 observations&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;As it is now I have transformed my data so that I have count data for each time period instead of a binary DV. I also did a logit transformation on defined as ln(DV/(1-IV3)), in order to sort out the effect of activity.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;DV(Event): Events in a time period given IV1 and IV2&lt;/p&gt;&#10;  &#10;  &lt;p&gt;IV1(Year): Year 1995 to 2012&lt;/p&gt;&#10;  &#10;  &lt;p&gt;IV2(Month): Jan to Dec within IV1&lt;/p&gt;&#10;  &#10;  &lt;p&gt;IV3(Activity): Events + Non-events in time period given IV1 and IV2 (Neutralized)&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;I have a total of 176 observations&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Right now my model is defined as: &lt;code&gt;DV = b1*IV1 + b2*IV2 + error&lt;/code&gt;but I am struggling alot with the intuition of whether this make sense. I have been looking into poisson models and zero inflated models, but until now the most sensible I think is to do the logit transformation in order to neutralize activity and the look at year and month as factors in a normal linear regression, but then again I am thinking that I might as well use a logistic regression.&lt;/p&gt;&#10;&#10;&lt;p&gt;Does any of you know how to handle such a case?&lt;/p&gt;&#10;&#10;&lt;p&gt;I am using R, SPSS and rapidminer.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-09T19:10:53.127" Id="51781" LastActivityDate="2013-03-12T10:26:15.460" LastEditDate="2013-03-12T10:26:15.460" LastEditorUserId="805" OwnerUserId="17107" PostTypeId="1" Score="0" Tags="&lt;time-series&gt;&lt;count-data&gt;" Title="Which model for count data over time?" ViewCount="82" />
  <row AnswerCount="1" Body="&lt;p&gt;For an iid sample $X_1, \dots, X_n$, is the rank statistic distribution free? I.e. for each $k$ s.t $1 \leq k \leq n$, will the distribution of the rank of $X_k$ not depend on the distribution of $X_i$? &lt;/p&gt;&#10;&#10;&lt;p&gt;I think yes. Suppose $X_i$ has a cdf $F$ which admits an inverse $F^{-1}$, then $F^{-1}(X_1), \dots, F^{-1}(X_n)$ will still be iid but with the uniform distribution over $[0,1]$. Since $F$ is invertible, both $F$ and $F^{-1}$ are strictly increasing, so the rank of $X_k$ and the rank of $F^{-1}(X_k)$ will be the same. So we can assume the distribution of $X_i$ to be the uniform distribution over $[0,1]$, and thus the rank of $X_k$ doesn't depend on the distribution of $X_i$.&lt;/p&gt;&#10;&#10;&lt;p&gt;If $F$ is not invertible, the ties might change when going from $X_1, \dots, X_n$ to $F^{-1}(X_1), \dots, F^{-1}(X_n)$. We cannot say the distribution of the rank statistic doesn't depend on the distribution of $X_i$?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks and regards!&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-03-09T19:16:59.047" Id="51782" LastActivityDate="2013-03-09T19:52:51.353" LastEditDate="2013-03-09T19:26:46.840" LastEditorUserId="1005" OwnerUserId="1005" PostTypeId="1" Score="1" Tags="&lt;mathematical-statistics&gt;" Title="Is a rank statistic always distribution free?" ViewCount="112" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I have a short question about Monte Carlo integration with Metropolis sampling. I have a continuous state space, but only certain parts of this state space are valid. It is possible that the transition function can suggest a move to an invalid part of the state space, and the jump should be obviously rejected. However, I'm not sure whether this invalid sample should be counted; i.e. if N samples are taken and one is rejected from an invalid part of state space, should I divide by N-1 in the estimate.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-03-10T02:10:18.697" FavoriteCount="1" Id="51808" LastActivityDate="2013-04-09T03:46:59.540" OwnerUserId="21794" PostTypeId="1" Score="6" Tags="&lt;mcmc&gt;&lt;monte-carlo&gt;&lt;metropolis-hastings&gt;" Title="Metropolis Sampling and invalid states" ViewCount="68" />
  
  <row Body="&lt;p&gt;Let me start by defining the terms of the discussion as I see them.  A &lt;strong&gt;p-value&lt;/strong&gt; is the probability of getting a sample statistic (say, a sample mean) &lt;em&gt;as far as&lt;/em&gt;, or &lt;em&gt;further&lt;/em&gt; from some reference value than your sample statistic, &lt;em&gt;if&lt;/em&gt; the reference value were the true population parameter.  For example, a p-value answers the question: what is the probability of getting a sample mean IQ more than $|\bar x-100|$ points away from 100, if 100 is really the mean of the population from which your sample was drawn.  Now the issue is, how should that number be employed in making a statistical inference?  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Fisher&lt;/strong&gt; thought that the p-value could be interpreted as &lt;em&gt;a continuous measure of evidence against the null hypothesis&lt;/em&gt;.  There is no particular fixed value at which the results become 'significant'.  The way I usually try to get this across to people is to point out that, for all intents and purposes, p=.049 and p=.051 constitute an identical amount of evidence against the null hypothesis (cf. @Henrik's answer &lt;a href=&quot;http://stats.stackexchange.com/questions/726/famous-statistician-quotes/783#783&quot;&gt;here&lt;/a&gt;).  &lt;/p&gt;&#10;&#10;&lt;p&gt;On the other hand, &lt;strong&gt;Neyman &amp;amp; Pearson&lt;/strong&gt; thought you could use the p-value as part of &lt;em&gt;a formalized decision making process&lt;/em&gt;.  At the end of your investigation, you have to either reject the null hypothesis, or fail to reject the null hypothesis.  In addition, the null hypothesis could be either true or not true.  Thus, there are four theoretical possibilities (although in any given situation, there are just two): you could make a &lt;em&gt;correct decision&lt;/em&gt; (fail to reject a true--or reject a false--null hypothesis), or you could make a &lt;a href=&quot;http://en.wikipedia.org/wiki/Type_I_and_type_II_errors&quot;&gt;type I or type II error&lt;/a&gt; (by rejecting a true null, or failing to reject a false null hypothesis, respectively).  (Note that the p-value is not the same thing as the type I error rate, which I discuss &lt;a href=&quot;http://stats.stackexchange.com/questions/46856/interpretation-of-p-value-in-hypothesis-testing/46896#46896&quot;&gt;here&lt;/a&gt;.)  The p-value allows the process of deciding whether or not to reject the null hypothesis to be formalized.  Within the Neyman-Pearson framework, the process would work like this: there is a null hypothesis that people will believe by default in the absence of sufficient evidence to the contrary, and an alternative hypothesis that you believe may be true instead.  There are some long-run error rates that you will be willing to live with (note that there is no reason these have to be 5% and 20%).  Given these things, you design your study to differentiate between those two hypotheses while maintaining, at most, those error rates, by conducting a power analysis and conducting your study accordingly.  (Typically, this means having sufficient data.)  After your study is completed, you compare your p-value to $\alpha$ and reject the null hypothesis if $p&amp;lt;\alpha$; if it's not, you fail to reject the null hypothesis.  Either way, your study is complete and you have made your decision.  &lt;/p&gt;&#10;&#10;&lt;p&gt;The Fisherian and Neyman-Pearson approaches are &lt;em&gt;not the same&lt;/em&gt;.  The central contention of the Neyman-Pearson framework is that at the end of your study, you have to make a decision and walk away.  Allegedly, a researcher once approached Fisher with 'non-significant' results, asking him what he should do, and Fisher said, 'go get more data'.  &lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Personally, I find the elegant logic of the Neyman-Pearson approach very appealing.  But I don't think it's always appropriate.  To my mind, at least two conditions must be met before the Neyman-Pearson framework should be considered:  &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;There should be some specific alternative hypothesis (&lt;a href=&quot;http://en.wikipedia.org/wiki/Effect_size&quot;&gt;effect magnitude&lt;/a&gt;) that you care about for some reason.  (I don't care what the effect size is, what your reason is, whether it's well-founded or coherent, etc., only that you have one.)  &lt;/li&gt;&#10;&lt;li&gt;There should be some reason to suspect that the effect will be 'significant', if the alternative hypothesis is true.  (In practice, this will typically mean that you conducted a power analysis, and have enough data.)  &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;When these conditions aren't met, the p-value can still be interpreted in keeping with Fisher's ideas.  Moreover, it seems likely to me that most of the time these conditions are not met.  Here are some easy examples that come to mind, where tests are run, but the above conditions are not met:  &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;the omnibus ANOVA for a multiple regression model (it is possible to figure out how all the hypothesized non-zero slope parameters come together to create a &lt;a href=&quot;http://en.wikipedia.org/wiki/Noncentrality_parameter&quot;&gt;non-centrality parameter&lt;/a&gt; for the &lt;a href=&quot;http://en.wikipedia.org/wiki/Noncentral_F-distribution&quot;&gt;F distribution&lt;/a&gt;, but it isn't remotely intuitive, and I doubt anyone does it)  &lt;/li&gt;&#10;&lt;li&gt;the value of a &lt;a href=&quot;http://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test&quot;&gt;Shapiro-Wilk&lt;/a&gt; test of the normality of your residuals in a regression analysis (what magnitude of $W$ do you care about and why? how much power to you have to reject the null when that magnitude is correct?)  &lt;/li&gt;&#10;&lt;li&gt;the value of a test of homogeneity of variance (e.g., &lt;a href=&quot;http://en.wikipedia.org/wiki/Levene%27s_test&quot;&gt;Levene's test&lt;/a&gt;; same comments as above)  &lt;/li&gt;&#10;&lt;li&gt;any other tests to check assumptions, etc.  &lt;/li&gt;&#10;&lt;li&gt;t-tests of covariates other than the explanatory variable of primary interest in the study  &lt;/li&gt;&#10;&lt;li&gt;initial / exploratory research (e.g., pilot studies)  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="3" CreationDate="2013-03-10T05:29:43.217" Id="51823" LastActivityDate="2014-01-16T01:01:05.163" LastEditDate="2014-01-16T01:01:05.163" LastEditorUserId="7290" OwnerUserId="7290" ParentId="23142" PostTypeId="2" Score="27" />
  
  
  <row Body="&lt;p&gt;I see no reason why factor scores cannot be used this way. &lt;/p&gt;&#10;&#10;&lt;p&gt;As to whether you should run a factor analysis on the 10 tasks, I think it depends on what you are trying to do. If the 10 tasks are well-established and you think they are a good representation of cognitive ability, go for 10 separate analyses.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you want to try to find some different representation of cognitive ability, then FA may help.&lt;/p&gt;&#10;&#10;&lt;p&gt;As you probably know, &quot;cognitive ability&quot; is a topic on which there is little agreement. Is it one thing? Several things? A great many? Is there a &quot;g&quot; factor that corresponds to &quot;overall ability&quot;? Do you take a Gardnerian view that there are multiple intelligences that are only modestly (if at all) related? Or a Sternbergian view that there are several abilities that are somewhat related? Or some other view?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-03-10T13:06:43.090" Id="51838" LastActivityDate="2013-03-10T13:06:43.090" OwnerUserId="686" ParentId="51814" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I think the difference is that distribution free statistics use limit theorems to show that you can construct tests of the weak null hypothesis about some function of the distribution function that are asymptotically consistent and unbiased. I think there is some deficiency in the popular understanding of what constitutes &quot;nonparametric&quot; statistics, and that the statement of the null hypothesis eludes many applied statisticians.&lt;/p&gt;&#10;&#10;&lt;p&gt;For instance, if a 2-sample test has data drawn from a family of parametric probability models $X_1, X_2, \ldots, X_n \sim_{iid} \mathcal{F}_{\theta_1}$, $Y_1, Y_2, \ldots, Y_m \sim_{iid} \mathcal{F}_{\theta_2}$, then that parameter forms the obvious test of the weak null hypothesis: $\mathcal{H}_0: \theta_1 = \theta_2$. However, there might be a functional value which, independent of the distribution, requires an asymptotically correct test: so with two samples having no known class of probability models $X_1, X_2, \ldots, X_n \sim_{iid} \mathcal{F}$, $Y_1, Y_2, \ldots, Y_m \sim_{iid} \mathcal{G}$, you define a &quot;parameter&quot; for that distribution: $\theta_1 = f(\mathcal{F})$, $\theta_2 = f(\mathcal{G})$, and test the weak null hypothesis: $\mathcal{H}_0: \theta_1 = \theta_2$. In most cases, limit theorems such as the $\delta$-method, central limit theorem, etc. can show that there exists test statistics for which $n^k g \left( X, Y \right) \rightarrow_d \chi^2_p(\lambda)$ where the non-centrality parameter $\lambda$ is 0 only if the weak null hypothesis is true. There are usually some regularity conditions on the distributions $\mathcal{F}$, $\mathcal{G}$, but in general they can apply to a large array of probability models. The statistic $g()$ is what we call &lt;strong&gt;distribution free&lt;/strong&gt;. Take the T-test: for normal, exponential, negative binomial, and poisson data, the T-test is well powered to detect differences in the means between these distributions for any two-sample experiment, even when the mean does not parametrize that distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;non-parametric&lt;/strong&gt; statistics are traditionally used when researchers &lt;em&gt;wish&lt;/em&gt; to state their strong null hypothesis as $\mathcal{H_0}: \mathcal{F} = \mathcal{G}$. That is, if there is a difference in the 99.99th percentile in these data (and even if there is a symmetric difference in the 00.01th percentile), they would like a test that is calibrated to reject the null hypothesis. We don't wish to prespecify what we would believe to be a meaningful difference in the distribution for these data, whether it be a mean, median, or quantile based summary. This is stupid in my mind. Tests like this can be done with Kolmogorov-Smirnoff tests for the empirical distribution function or using non-parametric smoothed kernel density estimates. The required sample size to detect the egregious, symmetric quantile difference above is enormous, but it is powered to reject the strong null hypothesis in that case.&lt;/p&gt;&#10;&#10;&lt;p&gt;Rank based statistics like the log-rank, Wilcoxon, Mann Whitney U, etc. are &lt;em&gt;not&lt;/em&gt; tests of the strong null hypothesis, so in my mind they're not non-parametric.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2013-03-10T14:58:00.577" Id="51842" LastActivityDate="2013-03-11T19:21:57.343" LastEditDate="2013-03-11T19:21:57.343" LastEditorUserId="8013" OwnerUserId="8013" ParentId="51802" PostTypeId="2" Score="3" />
  
  <row AcceptedAnswerId="51863" AnswerCount="1" Body="&lt;p&gt;The company where I work did a survey with a complex sample. Originally, the sample was a 2-step stratified, but due to some problems, we lost one of them (so, let's consider that the sample is stratified only on 1 variable).&lt;/p&gt;&#10;&#10;&lt;p&gt;The biggest part of the questions is like &quot;What you think the company can do in respect of X?&quot; and then there is a list of options (some questions allow only 1 answer, some more than 1, others are Likert-type response options).&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is: Wow can I analyze this data? It would be nice if there were a test like Chi-square for complex sample (probably there exists one, but I don't know)&lt;/p&gt;&#10;&#10;&lt;p&gt;Any software is welcome, but I prefer R and SPSS.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-10T20:12:58.847" Id="51861" LastActivityDate="2013-03-10T21:01:11.263" LastEditDate="2013-03-10T21:01:11.263" LastEditorUserId="930" OwnerUserId="21821" PostTypeId="1" Score="1" Tags="&lt;chi-squared&gt;&lt;survey&gt;&lt;sample&gt;&lt;stratification&gt;" Title="Analysis of complex sample" ViewCount="92" />
  <row AcceptedAnswerId="91133" AnswerCount="4" Body="&lt;p&gt;I have an independent variable is a manipulation/predictor which is applied in increasing degrees of strength across groups. The independent variable alters the amount of variation in the dependent variable. &lt;/p&gt;&#10;&#10;&lt;p&gt;Because we are looking at variance, I cannot use Spearman's Rho to calculate correlation without a weird hack (detailed below).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;What is the best way to measure the correlation of variance?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In non-mathematical terms,  I am testing if increasing the use of a metaphor/mental model unifies responses.  If the mental model alters the way people think about a problem, then increasing the &quot;reminders&quot; of this mental model should unify the responses.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are 5 independent groups, 0-4, ranked according to the number of &quot;reminders&quot; in each one.  As the reminders are qualitatively different, I must use ranks.&lt;/p&gt;&#10;&#10;&lt;p&gt;If one examines the SD of each group according to rank, they line up almost perfectly:&lt;/p&gt;&#10;&#10;&lt;p&gt;SD - group&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;drop down&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;.639 - drop down menu&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;radio buttons&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;.604 - vertical&lt;/li&gt;&#10;&lt;li&gt;.484 - horizontal&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;slider&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;.5515 &lt;/li&gt;&#10;&lt;li&gt;.4504 - colored&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Group Averages&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;.639&lt;/li&gt;&#10;&lt;li&gt;.544&lt;/li&gt;&#10;&lt;li&gt;.501&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;That's a difference of ~15% and ~8%, albeit a very rough and indirect way of measuring change in variance.  However, Bartlett and Levene's tests confirm a statistically significant difference in the variance between the groups.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, neither Spearman nor Kendall look at variance directly, so a difference of -1 and +1 will negate each other using these measures.  If I combine the two extreme scores (0-1-2 -&gt; 0-1) then I get a Spearman's of .142 with a sig &amp;lt;.001.&lt;/p&gt;&#10;&#10;&lt;p&gt;While I think this is a defensible hack, it doesn't seem to match up with my ad-hoc analysis above.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-03-11T00:49:46.160" FavoriteCount="1" Id="51876" LastActivityDate="2014-10-23T20:57:33.727" LastEditDate="2014-03-23T23:20:18.943" LastEditorUserId="1689" OwnerUserId="1689" PostTypeId="1" Score="5" Tags="&lt;hypothesis-testing&gt;&lt;variance&gt;&lt;rank-correlation&gt;" Title="How to measure correlation between variance and rank?" ViewCount="474" />
  <row Body="&lt;p&gt;It sounds like you need a derivative free method (since it seems you can't supply a gradient for your objective function).  I guess your parameter space is not bounded since you are currently using Nelder-Mead.  I'm not sure what software you're using but you should check out &lt;a href=&quot;http://www.gerad.ca/nomad/Project/Home.html&quot; rel=&quot;nofollow&quot;&gt;NOMAD&lt;/a&gt; which is &quot;freely distributed under the GNU Lesser General Public License.&quot;  NOMAD handles nonlinear functions of integers and/or real variables.  In R the &lt;code&gt;crs&lt;/code&gt; package provides an interface, &lt;a href=&quot;http://www.inside-r.org/packages/cran/crs/docs/snomadr&quot; rel=&quot;nofollow&quot;&gt;see this&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-03-11T03:59:34.227" Id="51883" LastActivityDate="2013-03-11T03:59:34.227" OwnerUserId="21168" ParentId="27212" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;Certainly you can. The only onerous part is computing all the ranks, but that's still pretty easy with ten thousand observations.&lt;/p&gt;&#10;&#10;&lt;p&gt;The questionable part would be why you'd &lt;em&gt;want&lt;/em&gt; to test with 10000 pairs. Surely with almost any kind of non-artificial data, you're going to reject - few correlations are so small that they won't fall outside the limits for that. For example - at those sort of sample sizes you'll reject a correlation of around 0.02 as being significantly different from 0!&lt;/p&gt;&#10;&#10;&lt;p&gt;Surely the size of the correlation is more meaningful than whether you can reject it at such a big sample size?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-03-11T04:01:39.070" Id="51884" LastActivityDate="2013-03-11T04:09:40.367" LastEditDate="2013-03-11T04:09:40.367" LastEditorUserId="805" OwnerUserId="805" ParentId="51879" PostTypeId="2" Score="1" />
  
  
  
  <row Body="&lt;p&gt;The short answer is the &quot;usual&quot; way in the &lt;em&gt;social sciences&lt;/em&gt; is to have pre-defined discrete areas and provide summary statistics for samples within those areas aggregated up. Examples of pre-defined areas may be census geographies, zip-codes, or any other neighborhood unit you can dream up. &lt;/p&gt;&#10;&#10;&lt;p&gt;This is the usual because it is fairly rare to have surveys geocoded to the exact location or be able to utilize that information due to privacy reasons, so typically surveys can't be geolocated besides within a general area anyway. Even if you don't have pre-defined areas you could always create a uniform grid (i.e. quadrats) and provide summary statistics. If your sampling over the city diverges from uniform though this approach will not work as well, as you could have some quadrats with few (or zero) samples and some with many samples (in fact some people justify aggregation to larger units because of such concerns).&lt;/p&gt;&#10;&#10;&lt;p&gt;More novel is to utilize what van Ham &amp;amp; Manley (2012) refer to as bespoke neighborhoods. One essentially picks a point, and then some arbitrary buffer around that point, and then estimates whatever summary statistic one is interested in based on samples around that point. Related examples can be found in Lee et al. (2008), Li &amp;amp; Radke (2012) and Ratcliffe &amp;amp; Taniguchi (2008).&lt;/p&gt;&#10;&#10;&lt;p&gt;If you want more continuous measurements over the entire study space, you can utilize geo-statistical kriging to estimate a continuous surface (although this is also pretty novel for social science). Similar examples can be found in the social science literature under area-to-point kriging (which is a slightly different context). See &lt;a href=&quot;http://stats.stackexchange.com/a/15850/1036&quot;&gt;this other answer of mine&lt;/a&gt; for a list of scholarly examples. &lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;h1&gt;Citations&lt;/h1&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Lee, B. A., Reardon, S. F., Firebaugh, G., Farrell, C. R., Matthews, S. A., and O'Sullivan, D. (2008). Beyond the census tract: Patterns and determinants of racial segregation at multiple geographic scales. &lt;em&gt;American Sociological Review&lt;/em&gt;, &lt;a href=&quot;http://dx.doi.org/10.1177/000312240807300504&quot; rel=&quot;nofollow&quot;&gt;73(5):766-791&lt;/a&gt;. PDF &lt;a href=&quot;http://cepa.stanford.edu/sites/default/files/766.full_.pdf&quot; rel=&quot;nofollow&quot;&gt;Here&lt;/a&gt;.&lt;/li&gt;&#10;&lt;li&gt;Li, W. and Radke, J. D. (2012). Geospatial data integration and modeling for the investigation of urban neighborhood crime. &lt;em&gt;Annals of GIS&lt;/em&gt;, &lt;a href=&quot;http://dx.doi.org/10.1080/19475683.2012.691903&quot; rel=&quot;nofollow&quot;&gt;18(3):185-205&lt;/a&gt;.&lt;/li&gt;&#10;&lt;li&gt;Ratcliffe, J. H. and Taniguchi, T. A. (2008). Is crime high around drug-gang street corners?: Two spatial approaches to the relationship between gang set spaces and local crime levels. &lt;em&gt;Crime Patterns and Analysis&lt;/em&gt;, &lt;a href=&quot;http://www.eccajournal.org/V1N1S2008/Ratcliffe_and_Taniguchi.pdf&quot; rel=&quot;nofollow&quot;&gt;1(1):23-46&lt;/a&gt;. PDF at link&lt;/li&gt;&#10;&lt;li&gt;van Ham, M. and Manley, D. (2012). Neighbourhood effects research at a crossroads: Ten challenges for future research. &lt;em&gt;Environment and Planning A&lt;/em&gt;, &lt;a href=&quot;http://dx.doi.org/10.1068/a45439&quot; rel=&quot;nofollow&quot;&gt;44(12):2787-2793&lt;/a&gt;.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2013-03-11T05:49:34.720" Id="51889" LastActivityDate="2013-03-11T05:49:34.720" OwnerUserId="1036" ParentId="51736" PostTypeId="2" Score="4" />
  
  
  <row AcceptedAnswerId="51919" AnswerCount="2" Body="&lt;p&gt;Consider the following code and output:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;  par(mfrow=c(3,2))&#10;  # generate random data from weibull distribution&#10;  x = rweibull(20, 8, 2)&#10;  # Quantile-Quantile Plot for different distributions&#10;  qqPlot(x, &quot;log-normal&quot;)&#10;  qqPlot(x, &quot;normal&quot;)&#10;  qqPlot(x, &quot;exponential&quot;, DB = TRUE)&#10;  qqPlot(x, &quot;cauchy&quot;)&#10;  qqPlot(x, &quot;weibull&quot;)&#10;  qqPlot(x, &quot;logistic&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/fnXnH.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;It seems that that Q-Q plot for log-normal is almost the same as the Q-Q plot for weibull. How can we distinguish them? Also if the points are within the region defined by the two outer black lines, does that indicate that they follow the specified distribution?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-03-11T15:29:13.020" FavoriteCount="2" Id="51918" LastActivityDate="2014-08-13T14:05:25.983" LastEditDate="2014-08-13T14:05:25.983" LastEditorUserId="7290" OwnerUserId="19814" PostTypeId="1" Score="7" Tags="&lt;r&gt;&lt;data-visualization&gt;&lt;interpretation&gt;&lt;qq-plot&gt;" Title="Q-Q plot interpretation" ViewCount="2153" />
  <row AcceptedAnswerId="51936" AnswerCount="1" Body="&lt;p&gt;I am running a model selection analysis with a continuous dependent variable and a variety of continuous and categorical explanatory variables.  For two of my continuous explanatory variables I am fitting curvature terms as it looks like there is a quadratic relationship between them and the dependent variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;When I run the model selection analysis using MuMIn in R, I get a variety of models out, some of which contain only the quadratic term, and not the lower order associated linear term, in them.  In my head this seems mathematically incorrect - is the linear term not essential when fitting a higher order polynomial (unless that linear term = 0...)?&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there anyway to get around this issue other than carrying out the model selection by hand (pretty impossible for me since I am trying to fit 24 parameters)?  Can I tell R not to include any quadratic term in a model without its associated linear term?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-03-11T16:17:44.980" FavoriteCount="0" Id="51921" LastActivityDate="2013-03-11T19:01:16.577" LastEditDate="2013-03-11T16:23:05.847" LastEditorUserId="21030" OwnerUserId="21030" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;model-selection&gt;&lt;lm&gt;" Title="Curvature terms and model selection" ViewCount="193" />
  
  
  
  <row Body="&lt;p&gt;Your calculations are correct.&lt;/p&gt;&#10;&#10;&lt;p&gt;The next step I would do is to check if the value &lt;code&gt;z=1100&lt;/code&gt; is among the credible interval for your posterior distribution. For this you can check if it is within the 95% HDI (highest density interval) of your distribution. If it's not, than you can say that &lt;code&gt;z=1110&lt;/code&gt; is not a credible value for your posterior (analogous to frequentist reject hypothesis).&lt;/p&gt;&#10;&#10;&lt;p&gt;Doing &lt;code&gt;pnorm(q=1100,mean=1090,sd=sqrt(90),lower.tail=FALSE)&lt;/code&gt; in R yields &lt;code&gt;0.1459203&lt;/code&gt; which is bigger than &lt;code&gt;(1-0.95)/2=0.025&lt;/code&gt;, so we can say that value &lt;code&gt;z=1100&lt;/code&gt; is within the 95% HDI and therefore it is a credible value. (The accumulated distribution can be used this way to get the HDI because of the concavity and symmetry of the normal distribution).&lt;/p&gt;&#10;&#10;&lt;p&gt;After doing that, if it does make sense to your problem, you might also try to establish a ROPE (region of practical equivalence) and check if your the posterior 95% HDI is in entirely inside it (analogous to frequentist accept hypothesis, if such thing exist).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-12T02:55:51.637" Id="51969" LastActivityDate="2013-03-12T03:08:16.450" LastEditDate="2013-03-12T03:08:16.450" LastEditorUserId="16612" OwnerUserId="16612" ParentId="51948" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;@Marco: Look up the Literature on Optimal Control. Mostly in Electrical Engineering. This sounds so much like one of the Control System Design Problems.&lt;/p&gt;&#10;&#10;&lt;p&gt;If I may ask, what's the physical model behind your problem. Some context, if it is not confidential?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-12T05:06:05.437" Id="51973" LastActivityDate="2013-03-12T05:06:05.437" OwnerUserId="21072" ParentId="51956" PostTypeId="2" Score="1" />
  
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am running a hierarchical model in JAGS. It is a piecewise constant survival regression model.&lt;/p&gt;&#10;&#10;&lt;p&gt;So I have 4 sets of regression coefficients and baseline hazard rates.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to plot a mean curve and range using the mean and 95% highest density interval (HDI) of these parameters using the covariates from my data.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have output from CODA, and it provides  marginal highest density intervals for each parameter. What would be the correct way to get the joint HDI for all parameters?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-12T13:30:13.300" FavoriteCount="1" Id="51999" LastActivityDate="2013-03-21T02:50:30.330" LastEditDate="2013-03-21T02:50:30.330" LastEditorUserId="16612" OwnerUserId="18098" PostTypeId="1" Score="2" Tags="&lt;bayesian&gt;&lt;jags&gt;&lt;coda&gt;" Title="Coda output, joint quantile" ViewCount="53" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Our client software randomly selects a server to connect to. There are four separate addresses the software randomly chooses from. Two addresses are &quot;old&quot; and two are &quot;new&quot;. As many firms need to explicitly allow outbound access to IP addresses, there will be some firms (users of some firms) that cannot connect to the &quot;new&quot; addresses.&lt;/p&gt;&#10;&#10;&lt;p&gt;We record connections over time. Based on when the new addresses were introduced (June 2012), I'm interested in finding a means of indicating which firms (users of those firms), since June 2012 to now, are unable to connect to the new addresses.&lt;/p&gt;&#10;&#10;&lt;p&gt;(We have no way of explicitly determining this. I am wondering if we can infer it with a probability model of some sort.) &lt;/p&gt;&#10;&#10;&lt;p&gt;So far I have done the following:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Retrieved connection history of all individual users since June 2012&lt;/li&gt;&#10;&lt;li&gt;Done a count of the number of times each user has connected to one of the &quot;old&quot; addresses&lt;/li&gt;&#10;&lt;li&gt;Discarded any users from the list who have connected to a new address at least once (since this would suggest there is no problem connecting to new addresses)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;In the end, I have a list of users who have only connected to the old addresses, and how many times they have done so.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-12T15:49:17.717" Id="52011" LastActivityDate="2013-03-13T11:32:23.977" LastEditDate="2013-03-13T11:32:23.977" LastEditorUserId="21897" OwnerUserId="21897" PostTypeId="1" Score="2" Tags="&lt;probability&gt;&lt;randomization&gt;" Title="In a random selection of addresses, determine if users are unable to access one of the addresses" ViewCount="36" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have the (not necessarily veridical) impression that typical textbook independence tests of proportions are intended for use the proportions involved are not super small.&lt;/p&gt;&#10;&#10;&lt;p&gt;In my case n may be 10^8 but the count of successes may be 10^4.&lt;/p&gt;&#10;&#10;&lt;p&gt;N per group may also differ, which of course means the sample variances for sample proportions differ.&lt;/p&gt;&#10;&#10;&lt;p&gt;How can I test for the independence of two sample proportions when sample size is large and the sample proportions are very small?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-03-12T18:00:29.203" Id="52021" LastActivityDate="2013-03-12T21:31:43.043" LastEditDate="2013-03-12T21:31:43.043" LastEditorUserId="21466" OwnerUserId="21466" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;&lt;statistical-significance&gt;&lt;nonparametric&gt;&lt;independence&gt;&lt;proportion&gt;" Title="What test of independence of sample proportions is appropriate when n is very large and p is very small?" ViewCount="110" />
  
  
  <row Body="&lt;p&gt;The t-test is obviously a risky choice. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, in your example, the Wilcoxon also has problems. The Wilcoxon test assumes that the shapes of the distribution in both groups are the same, but there is a location shift. This is clearly not the case in your example. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you can transform the data so the shapes of the distributions in both groups are similar, you can use Wilcoxon. &lt;/p&gt;&#10;&#10;&lt;p&gt;If this can't be done, I'd recommend looking at a randomisation test in this example. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-13T07:44:56.860" Id="52062" LastActivityDate="2013-03-13T07:44:56.860" OwnerUserId="19822" ParentId="52047" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;For those who are looking for a batch method for bandwidth estimation, I would suggest the multivariate bandwith estimator from [1] -- the approximate calculation in C interfaced to Matlab can be obtained from the following link:&#10;&lt;a href=&quot;http://www.vicos.si/images/2/2e/KDE_bw_Matlab.zip&quot; rel=&quot;nofollow&quot;&gt;http://www.vicos.si/images/2/2e/KDE_bw_Matlab.zip&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;If you have large amounts of data (e.g., order of 1000), it might not make sense to apply the standard batch method. Instead you can use the online Kernel Density Estimator from [1], of which Matlab implementation can be obtained from this site:&#10;&lt;a href=&quot;http://www.vicos.si/Research/Multivariate_Online_Kernel_Density_Estimation&quot; rel=&quot;nofollow&quot;&gt;http://www.vicos.si/Research/Multivariate_Online_Kernel_Density_Estimation&lt;/a&gt;&#10;(Also some video examples of the estimation process are included)&lt;/p&gt;&#10;&#10;&lt;p&gt;[1] Multivariate Online Kernel Density Estimation with Gaussian Kernels&#10;Matej Kristan, Aleš Leonardis, and Danijel Skočaj, Pattern Recognition, 2011&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-13T10:48:28.857" Id="52077" LastActivityDate="2013-03-13T10:48:28.857" OwnerUserId="21934" ParentId="14143" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;&lt;strong&gt;There is no such thing as non-linear regression without model!&lt;/strong&gt; This is a conceptual issue and not a programming or Matlab problem. Nonlinear regression minimizes, by definition, the error between a model and your data, under a certain error metric, by searching the space of the model's parameters. &lt;/p&gt;&#10;&#10;&lt;p&gt;What you can do is fit and test a couple of models (e.g. Polynomials with increasing degree) with your data and evaluate which one fits best without over fitting. But you as the expert for your data have to chose which models you test. &lt;/p&gt;&#10;&#10;&lt;p&gt;Polynomial fitting in Matlab can be computed with &lt;code&gt;polyfit&lt;/code&gt;, by the way. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Regarding your questions&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The thing is I have used polyfit and got some coefficients say a,b,c,d. So, it is a nonlinear 4th order equation.(A)Should I then use this equation as a parameter when using nlfit which asks for a model?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;You don't need to call &lt;code&gt;nlfit&lt;/code&gt; after &lt;code&gt;polyfit&lt;/code&gt;. &lt;code&gt;polyfit&lt;/code&gt; does the complete fitting of a polynomial mode. The coefficients you get fully describe the polynomial that models your data. Look in the doc how you plot the polygon against your data, and how to evaluate the quality of the fit.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Will nlfit give better coefficients,what is the benefit of doing nlfit over polyfit?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;nlfit&lt;/code&gt; is a more general model fitting method. With &lt;code&gt;nlfit&lt;/code&gt; you can fit your own model, but you have to implement your model and provide it to &lt;code&gt;nlfit&lt;/code&gt;. &lt;code&gt;polyfit&lt;/code&gt; on the other hand only fits polynomial models. You can think of it as a highly specialized version of &lt;code&gt;nlfit&lt;/code&gt;, made to fit only one type of model (namely polynomials). &lt;code&gt;nlfit&lt;/code&gt; will not give you better coefficients if you feed it with a polynomial model.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;(B)How do I know that the obtained equation is following an AR model or MA model or ARMA model?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I'm not an expert in these models, but I think AR / MA / ARMA are different types of models, so &lt;code&gt;polyfit&lt;/code&gt; will not tell you anything about these.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-03-13T07:10:25.610" Id="52084" LastActivityDate="2013-03-14T08:23:58.647" LastEditDate="2013-03-14T08:23:58.647" LastEditorUserId="21939" OwnerDisplayName="DCS" OwnerUserId="21939" ParentId="52083" PostTypeId="2" Score="4" />
  <row AnswerCount="0" Body="&lt;p&gt;As we know probabilistic Hermite polynomials are orthogonal with respect to the weight function $\frac{1}{\sqrt{2 \pi}} e^{-x^2/2}$ (density of standard normal).&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a distribution which is a mixture of two Gaussians each having $(Mean, variance) =(1 ,\sigma^2)$   and $(-1,\sigma^2)$ respectively.&lt;/p&gt;&#10;&#10;&lt;p&gt;what are the changes we have to make ( In weight function and coefficients of Hermite polynomials) so that Hermite polynomials are orthogonal with respect to the mixture of two Gaussian distribution.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-03-13T12:39:05.590" FavoriteCount="1" Id="52087" LastActivityDate="2013-03-13T12:39:05.590" OwnerUserId="20228" PostTypeId="1" Score="2" Tags="&lt;mixture&gt;&lt;polynomial&gt;" Title="Orthogonality of Hermite Polynomials" ViewCount="134" />
  <row Body="&lt;p&gt;Disputes about normality with large N are often to do with tests of normality, not normality per se.  For larger sample sizes passing a test of normality, like Shapiro-Wilks is not required.  Consider the following in R.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;findNonNormal &amp;lt;- function(n = 5000){&#10;    p &amp;lt;- 1&#10;    while(p &amp;gt; 0.05) {&#10;        y &amp;lt;- rnorm(n)&#10;        p &amp;lt;- shapiro.test(y)$p.value&#10;        }&#10;    y&#10;    }&#10;&#10;y &amp;lt;- findNonNormal()&#10;hist(y)&#10;qqnorm(y)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The results show a remarkably normal distribution that the test says is not normal.  That's because the power of the test is so high with that N that it finds non normal distributions with very small deviations.  You could easily find similar results with the N's you mentioned.&lt;/p&gt;&#10;&#10;&lt;p&gt;Generally, passing an eyeball test of normality is all that's needed.  This eyeball test needs to be adjusted with N.  If you feel you cannot do the assessment just do some simulations with a similar N and see what typical data from a truly normal distribution look like.&lt;/p&gt;&#10;&#10;&lt;p&gt;If your data really are not normal don't do the parameteric tests.  But, contrary to your belief, a large N with reasonably normal distributions is when the power of a parametric test becomes most valuable.  It allows one to make estimates of the parameters in the population, and the better and larger the sample the more accurate those estimates will be.&lt;/p&gt;&#10;&#10;&lt;p&gt;Additionally, if you're looking at a t-test, for example, the distribution of the data is going to be bimodal with a strong effect.  It's because there are two means in the distribution.  So, the requirement is not that the data look normal but the residuals look normal.  This is true for your ANOVA as well.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-03-13T13:45:16.440" Id="52096" LastActivityDate="2013-03-13T14:04:37.930" LastEditDate="2013-03-13T14:04:37.930" LastEditorUserId="601" OwnerUserId="601" ParentId="52091" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;If the &lt;code&gt;kmeans&lt;/code&gt; function in base is an option, this will work:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# prepare data (from kmeans help)&#10;x &amp;lt;- rbind(matrix(rnorm(100, sd = 0.3), ncol = 2),&#10;           matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2))&#10;colnames(x) &amp;lt;- c(&quot;x&quot;, &quot;y&quot;)&#10;&#10;# calculate k-means&#10;(cl &amp;lt;- kmeans(x, 5, nstart = 25))&#10;&#10;# plot&#10;plot(x, col = cl$cluster)&#10;points(cl$centers, col = 1:5, pch = 8)&#10;&#10;# select points of interest&#10;# returns row number of selected point&#10;identify(x[,1], x[,2])&#10;# click away on the plot... &#10;# then press escape to get the points&#10;&#10;# find co-ords for selected point&#10;# in this case, point at row 16&#10;x[16,]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="9" CreationDate="2013-03-10T22:49:01.600" Id="52101" LastActivityDate="2013-03-10T22:49:01.600" OwnerDisplayName="Ben" OwnerUserId="7744" ParentId="51707" PostTypeId="2" Score="4" />
  <row AnswerCount="2" Body="&lt;p&gt;Lets say we have a dependent variable $Y$ with few categories and set of independent variables.  &lt;/p&gt;&#10;&#10;&lt;p&gt;What are the advantages of Multinomial logistic regression over set of binary logistic Regressions? By set of binary logistic regression I mean that for each category $y_{i} \in Y$ we build separate binary logistic regression model with target=1 when $Y=y_{i}$ and 0 otherwise.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-03-13T14:31:41.380" FavoriteCount="3" Id="52104" LastActivityDate="2014-06-18T12:20:57.087" LastEditDate="2013-03-13T15:12:47.640" LastEditorUserId="1643" OwnerUserId="1643" PostTypeId="1" Score="7" Tags="&lt;logistic&gt;&lt;categorical-data&gt;&lt;multinomial&gt;" Title="Multinomial logistic regression vs binary logistic regression" ViewCount="5178" />
  
  <row Body="&lt;p&gt;Major update, based on your comment.&lt;/p&gt;&#10;&#10;&lt;p&gt;The code should be something like when using &lt;code&gt;lme&lt;/code&gt; from &lt;code&gt;nlme&lt;/code&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lme.model &amp;lt;- lme(dv ~ season * treat, random = ~1|rep, data=data)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;where &lt;code&gt;rep&lt;/code&gt; is a factor assigning unique codes to each of your 10 independent study sites (i.e., 5 per treatment), &lt;code&gt;season&lt;/code&gt; is a factor indicating measurment quarter, and treatment is the treatment factor.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, this will not give you a real ANOVA but a mixed model with one random effect (&lt;code&gt;rep&lt;/code&gt;) and two fixed effects (&lt;code&gt;season&lt;/code&gt; and &lt;code&gt;between&lt;/code&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;To fit a real ANOVA (namely one with one between- and one within-subjects factor, a so called split-plot design) you could use package &lt;a href=&quot;http://cran.r-project.org/web/packages/afex/index.html&quot; rel=&quot;nofollow&quot;&gt;&lt;code&gt;afex&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;require(afex)&#10;anova &amp;lt;- ez.glm(&quot;rep&quot;, &quot;dv&quot;, data = data, within = &quot;season&quot;, between = &quot;treat&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;You could run this on each dv separately.&lt;/p&gt;&#10;&#10;&lt;p&gt;To analyze all dvs together you would need some multivariate analysis of which I am no expert.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Response prior to your comment:&lt;/p&gt;&#10;&#10;&lt;p&gt;If &lt;code&gt;treat&lt;/code&gt; is your unit of observation (of which having two seems to be quite low), then the following code would be correct:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lme &amp;lt;- lme(dv ~ season, random = ~1|treat, data=data)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;However, as said, having a repeated measures ANOVA with only two units of observation is pretty uncommon and seems a bad idea. If this is really your design (observed two treatments over several seasons), you are probably better off with other analyses, such as single-case analysis, e.g., &lt;a href=&quot;http://cran.r-project.org/web/packages/afex/index.html&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2013-03-13T16:12:28.650" Id="52118" LastActivityDate="2013-03-13T18:10:43.843" LastEditDate="2013-03-13T18:10:43.843" LastEditorUserId="442" OwnerUserId="442" ParentId="52093" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;+1 to @John.  Let me add a couple of supplementary points:  &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;The concern behind the normality assumption is whether or not you can trust the default p-values that software will output.  &lt;/li&gt;&#10;&lt;li&gt;If your residuals (although it may not be obvious, t-tests &amp;amp; ANOVAs are special cases of regression) are normally distributed, you can trust your p-values to be the actual probability of getting a sample statistic as far or further from the null value, if the null hypothesis is true, even if your sample size is &lt;a href=&quot;http://stats.stackexchange.com/questions/37993/is-there-a-minimum-sample-size-required-for-the-t-test-to-be-valid&quot;&gt;minimal&lt;/a&gt;.  &lt;/li&gt;&#10;&lt;li&gt;With a sufficiently large sample size, the &lt;a href=&quot;http://en.wikipedia.org/wiki/Central_limit_theorem&quot; rel=&quot;nofollow&quot;&gt;central limit theorem&lt;/a&gt; will lead the sampling distribution of your sample statistic to converge to approximate normality, and you will be able to trust your p-values.  &lt;/li&gt;&#10;&lt;li&gt;The required N for the CLT to cover you depends on how, and how much, your residuals are non-normal, with skew generally being more damaging than kurtosis in practice.  (Skewed data where the direction of the skew flips are especially damaging, but probably rare.)  &lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2013-03-13T17:26:28.310" Id="52125" LastActivityDate="2013-03-13T17:26:28.310" OwnerUserId="7290" ParentId="52091" PostTypeId="2" Score="2" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I calculated the 5 year survival of two populations keeping different variable constant each time and got percentages.&#10;I wanted to determine if these percentages were significantly different from each other.&#10;Could anyone please guide me with the same?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-13T19:29:53.823" Id="52138" LastActivityDate="2013-09-26T15:24:07.350" OwnerUserId="21964" PostTypeId="1" Score="1" Tags="&lt;statistical-significance&gt;&lt;spss&gt;&lt;survival&gt;&lt;biostatistics&gt;" Title="How to compare two survival percentages for two different populations?" ViewCount="125" />
  <row AnswerCount="0" Body="&lt;p&gt;In fda package, function smooth.basis is used to construct a functional data object by smoothing data using a roughness penalty.&#10;The usage can be found &lt;a href=&quot;http://www.inside-r.org/packages/cran/fda/docs/smooth.basis&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;smooth.basis(argvals=1:n, y, fdParobj, wtvec=NULL, fdnames=NULL,&#10;             covariates=NULL, method=&quot;chol&quot;, dfscale=1, returnMatrix=FALSE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In the documentation, it said &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;em&gt;&lt;strong&gt;argvals:&lt;/em&gt;&lt;/strong&gt; Argument values can even vary from one variable to another, in which&#10;  case they are input as an array with dimensions corresponding to&#10;  observation points, curves and variables, respectively. Note, however,&#10;  that the number of observation points per curve and per variable may&#10;  NOT vary. If it does, then curves and variables must be smoothed&#10;  individually rather than by a single call to this function.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;My question is how to set it up to use the function when we have subjects with different number of observations observed at different time series.&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, subject 1 has 10 observations at time week 1 to week 10, but subject 2 has only 5 observations at week 1.5, week 2.5, week 3.5, week 4.5, week 5.5.&lt;/p&gt;&#10;&#10;&lt;p&gt;Do I need to use the same basis system like b-spline to smooth two curves to make them comparable or consistent? Can anyone give me some examples to illustrate it?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-13T22:02:54.480" Id="52145" LastActivityDate="2013-03-14T08:10:51.387" LastEditDate="2013-03-14T08:10:51.387" LastEditorUserId="88" OwnerUserId="10723" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;programming&gt;" Title="Smoothing with fda package when time grids differ" ViewCount="105" />
  <row AnswerCount="1" Body="&lt;p&gt;How can I simulate a data for multiple linear regression (one IV and more then 2 IV) when we know the skewness and kurtosis of the data?&lt;/p&gt;&#10;&#10;&lt;p&gt;Another question is, how can we determine the type of distribution (ex: gamma dist) based on the skewness and kurtosis? We know that for normal distribution, the skewness and kurtosis are (0,0)...&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-03-14T02:35:52.263" Id="52156" LastActivityDate="2014-11-25T03:36:14.120" LastEditDate="2014-11-25T03:36:14.120" LastEditorUserId="805" OwnerUserId="13614" PostTypeId="1" Score="1" Tags="&lt;distributions&gt;&lt;multiple-regression&gt;&lt;simulation&gt;" Title="Simulating data for a multiple linear regression" ViewCount="153" />
  <row Body="&lt;p&gt;There is not enough information to decide whether it should be rejected.&lt;/p&gt;&#10;&#10;&lt;p&gt;For a two-tail p-value(0.01 in this case), half of that p-value is located at each tail(ie-.005 at each tail). &lt;/p&gt;&#10;&#10;&lt;p&gt;For a one-tail p-value, you would only consider one tail, resulting in a p-value of .005 or 0.995, depending on whether the observed proportion was greater than or less than 0.3.&lt;/p&gt;&#10;&#10;&lt;p&gt;Furthermore, Johns null hypothesis should be less than or equal, not just equal. This is because you never prove an alternative hypothesis correct, you prove the null hypothesis incorrect.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-03-14T03:05:01.963" Id="52159" LastActivityDate="2013-03-14T23:42:37.150" LastEditDate="2013-03-14T23:42:37.150" LastEditorUserId="21972" OwnerUserId="21972" ParentId="52154" PostTypeId="2" Score="3" />
  
  <row AcceptedAnswerId="52196" AnswerCount="1" Body="&lt;p&gt;I am doing SNP association study, and the estimated p-values for each SNP are plotted as a QQ-plot. The question is, can one interpret false positive hits from a QQ-plot in GWAS (Genome-Wide Association Study)?&lt;/p&gt;&#10;&#10;&lt;p&gt;From this article: &lt;a href=&quot;http://www.genomesunzipped.org/2010/07/how-to-read-a-genome-wide-association-study.php&quot; rel=&quot;nofollow&quot;&gt;http://www.genomesunzipped.org/2010/07/how-to-read-a-genome-wide-association-study.php&lt;/a&gt; , it says:&lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;...on the other hand, should show a solid line matching X=Y until it sharply curves at the end (representing the small number of true associations among thousands of unassociated SNPs)&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;Is this always true, or may False Positive also deviate sharply at the end of the curve? How to tell from a QQ-plot if a deviation is because of a true association or a false association?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-14T12:13:55.143" Id="52192" LastActivityDate="2013-03-14T13:17:48.030" LastEditDate="2013-03-14T12:26:14.553" LastEditorUserId="930" OwnerUserId="21840" PostTypeId="1" Score="2" Tags="&lt;genetics&gt;&lt;qq-plot&gt;" Title="How to interpret false positive from QQ-plot in genome-wide association studies?" ViewCount="2389" />
  <row Body="&lt;p&gt;Under the null hypothesis, your p-values should adhere to a uniform distribution (I'm ignoring some issues with dependencies here, as they are not really relevant to the discussion). This includes the false positives: just by coincidence, you should get a few extreme p-values, but since this is exactly what is in the expected distribution, this will not distort the QQ-plot: false positives are already accounted for in this type of QQ-plot.&lt;/p&gt;&#10;&#10;&lt;p&gt;On the other hand, if some SNPs were tested where the null hypothesis is not true, typically this should show up as a lower p-value than expected, and these will indeed distort the image.&lt;/p&gt;&#10;&#10;&lt;p&gt;What the author of the article you link seems to mean is: if you have wicked strong distortion, there are unexpectedly (as per the uniform distribution) many extreme p-values, so something is likely to be wrong.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, in short: a uniform QQ-plot of p-values (like this) should show relatively little digression from the straight line, and only in the extremely low p-values (showing you have more low p-values than if the null hypothesis were true everywhere).&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-03-14T13:17:48.030" Id="52196" LastActivityDate="2013-03-14T13:17:48.030" OwnerUserId="4257" ParentId="52192" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;You may try to visualize the idea by making a residual plot of the said regression:&#10;&lt;img src=&quot;http://i.stack.imgur.com/R3t8O.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The constant variance assumption is that the expected dependent variable conditioned on all independent variables is constant. If we, very roughly, slice up the fitted values into chunks, and calculate the variance of the residual within each, they should be roughly similar. From the above plot you can see that it's not always true for binary outcome (red fonts indicate the variance within each segment.)&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Not always&lt;/em&gt; because sometimes the situation is not that bad, especially if the independent variable is not very predictive (aka, there is a good deal of overlapping in the groups outcome = 1 and outcome = 0.) However, the normality assumption will still be violated.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-03-14T13:26:39.767" Id="52197" LastActivityDate="2013-03-14T13:26:39.767" OwnerUserId="13047" ParentId="52187" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="52227" AnswerCount="1" Body="&lt;p&gt;When studying the latent Dirichlet allocation, I am not very clear about some procedures in their deriving equations. Please refer to the attached figure, how to understand those two steps, marked as 1 and 2 in the figure.&#10;&lt;img src=&quot;http://i.stack.imgur.com/03JPU.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-14T15:34:30.987" Id="52213" LastActivityDate="2013-05-04T21:10:00.697" LastEditDate="2013-03-14T18:37:38.240" LastEditorUserId="88" OwnerUserId="3269" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;text-mining&gt;&lt;gibbs&gt;&lt;dirichlet-distribution&gt;&lt;topic-models&gt;" Title="Derivation of the posterior over topics in LDA" ViewCount="179" />
  <row Body="&lt;p&gt;+1 to @NickSabbe, for 'the plot just tells you that &quot;something is wrong&quot;', which is often the best way to use a qq-plot (as it can be difficult to understand how to interpret them).  It is possible to learn how to interpret a qq-plot by thinking about how to make one, however.  &lt;/p&gt;&#10;&#10;&lt;p&gt;You would start by sorting your data, then you would count your way up from the minimum value taking each as an equal percentage.  For example, if you had 20 data points, when you counted the first one (the minimum), you would say to yourself, 'I counted 5% of my data'.  You would follow this procedure until you got to the end, at which point you would have passed through 100% of your data.  These percentage values can then be compared to the same percentage values from the corresponding theoretical normal (i.e., the normal with the same mean and SD).  &lt;/p&gt;&#10;&#10;&lt;p&gt;When you go to plot these, you will discover that you have trouble with the last value, which is 100%, because when you've passed through 100% of a theoretical normal you are 'at' infinity.  This problem is dealt with by adding a small constant to the denominator at each point in your data before calculating the percentages.  A typical value would be to add 1 to the denominator; for example, you would call your 1st (of 20) data point 1/(20+1)=5%, and your last would be 20/(20+1)=95%.  &lt;em&gt;Now&lt;/em&gt; if you plot these points against a corresponding theoretical normal, you will have a &lt;a href=&quot;http://en.wikipedia.org/wiki/P%E2%80%93P_plot&quot; rel=&quot;nofollow&quot;&gt;pp-plot&lt;/a&gt; (for plotting probabilities against probabilities).  Such a plot would most likely show the deviations between your distribution and a normal in the center of the distribution.  This is because 68% of a normal distribution lies within +/- 1 SD, so pp-plots have excellent resolution there, and poor resolution elsewhere.  (For more on this point, it may help to read my answer here: &lt;a href=&quot;http://stats.stackexchange.com/a/100383/7290&quot;&gt;PP-plots vs. QQ-plots&lt;/a&gt;.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Often, we are most concerned about what is happening in the tails of our distribution.  To get better resolution &lt;em&gt;there&lt;/em&gt; (and thus worse resolution in the middle), we can construct a &lt;a href=&quot;http://en.wikipedia.org/wiki/Q%E2%80%93Q_plot&quot; rel=&quot;nofollow&quot;&gt;qq-plot&lt;/a&gt; instead.  We do this by taking our sets of probabilities and passing them through the inverse of the normal distribution's CDF (this is like reading the z-table in the back of a stats book backwards--you read in a probability and read out a z-score).  The result of this operation is two sets of &lt;em&gt;quantiles&lt;/em&gt;, which can be plotted against each other similarly.  &lt;/p&gt;&#10;&#10;&lt;p&gt;@whuber is right that the reference line is plotted afterwards (typically) by finding the best fitting line through the middle 50% of the points (i.e., from the first quartile to the third).  This is done to make the plot easier to read.  Using this line, you can interpret the plot as showing you whether the quantiles of your distribution progressively diverge from a true normal as you move into the tails.  (Note that the position of points further out from the center are not really independent of those closer in; so the fact that, in your specific histogram, the tails seem to come together after having the 'shoulders' differ does not mean that the quantiles are now the same again.)  &lt;/p&gt;&#10;&#10;&lt;p&gt;You can interpret a qq-plot analytically by considering the values read from the axes compare for a given plotted point.  If the data were well described by a normal distribution, the values should be about the same.  For example, take the extreme point at the very far left bottom corner: its $x$ value is somewhere past $-3$, but its $y$ value is only a little past $-.2$, so it is much further out than it 'should' be.  In general, a simple rubric to interpret a qq-plot is that if a given tail twists off counterclockwise from the reference line, there is &lt;em&gt;more&lt;/em&gt; data in that tail of your distribution than in a theoretical normal, and if a tail twists off clockwise there is &lt;em&gt;less&lt;/em&gt; data in that tail of your distribution than in a theoretical normal.  In other words:  &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;if both tails twist counterclockwise you have heavy tails (&lt;a href=&quot;http://en.wikipedia.org/wiki/Kurtosis#Terminology_and_examples&quot; rel=&quot;nofollow&quot;&gt;leptokurtosis&lt;/a&gt;),  &lt;/li&gt;&#10;&lt;li&gt;if both tails twist clockwise, you have light tails (platykurtosis), &lt;/li&gt;&#10;&lt;li&gt;if your right tail twists counterclockwise and your left tail twists clockwise, you have right skew  &lt;/li&gt;&#10;&lt;li&gt;if your left tail twists counterclockwise and your right tail twists clockwise, you have left skew  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="1" CreationDate="2013-03-14T17:04:09.297" Id="52221" LastActivityDate="2014-11-10T19:07:33.447" LastEditDate="2014-11-10T19:07:33.447" LastEditorUserId="22047" OwnerUserId="7290" ParentId="52212" PostTypeId="2" Score="5" />
  
  <row Body="&lt;p&gt;This sounds like you want a multiple regression model.  However the topic is much more than can be covered in a forum like this.  To do a proper regression model (and be sure that it is the correct model) really requires at least 2 courses in statistics (more recommended).  &lt;/p&gt;&#10;&#10;&lt;p&gt;So, you should either enroll in some statistics courses (or find the equivalent self-study) or consult with a local statistician who can make sure that you are doing the right thing and help you understand the results.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-03-14T19:37:50.357" Id="52236" LastActivityDate="2013-03-14T19:37:50.357" OwnerUserId="4505" ParentId="52230" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;For reducing your data points, you can use the &lt;a href=&quot;http://en.wikipedia.org/wiki/Ramer%E2%80%93Douglas%E2%80%93Peucker_algorithm&quot; rel=&quot;nofollow&quot;&gt;Ramer–Douglas–Peucker algorithm&lt;/a&gt; which is very easy to understand and implement. The sampled signal will be very similar to the original one.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-14T20:28:45.447" Id="52240" LastActivityDate="2013-03-14T20:28:45.447" OwnerUserId="22013" ParentId="980" PostTypeId="2" Score="0" />
  
  <row AcceptedAnswerId="52284" AnswerCount="1" Body="&lt;p&gt;I have a regression model that looks like this: $$Y = \beta_0+\beta_1X_1 + \beta_2X_2 + \beta_3X_3 +\beta_{12}X_1X_2+\beta_{13}X_1X_3+\beta_{123}X_1X_2X_3$$&lt;/p&gt;&#10;&#10;&lt;p&gt;...or in R notation: &lt;code&gt;y ~ x1 + x2 + x3 + x1:x2 + x1:x3 + x1:x2:x3&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's say $X_1$ and $X_2$ are categorical variables and $X_3$ is numeric. The complication is that $X_1$ has three levels $X_{1a}, X_{1b}, X_{1c}$ and instead of standard contrasts, I need to test: &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Whether the intercept for level $X_{1a}$ significantly differs from the average intercept for levels $X_{1b}$ and $X_{1c}$.&lt;/li&gt;&#10;&lt;li&gt;Whether the response of $X_2$ is significantly different between level $X_{1a}$ and the average of levels $X_{1b}$ and $X_{1c}$.&lt;/li&gt;&#10;&lt;li&gt;Whether the slope of $X_3$ is significantly different between level $X_{1a}$ and the average of levels $X_{1b}$ and $X_{1c}$.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Based on &lt;a href=&quot;http://stats.stackexchange.com/questions/32188/how-to-interpret-these-custom-contrasts&quot;&gt;this post&lt;/a&gt; it seems like the matrix I want is...&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; 2&#10;-1&#10;-1&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So I do &lt;code&gt;contrasts(mydata$x1)&amp;lt;-t(ginv(cbind(2,-1,-1)))&lt;/code&gt;. The estimate of $\beta_1$ changes, but so do the others. I can reproduce the new estimate of $beta_1$ by subtracting the predicted values of the $X_1b$ and $X_1c$ group means (when $X_3=0$ and $X_2$ is at its reference level) from twice the value of $X_1a$ at those levels. But I can't trust that I specified my contrast matrix correctly unless I can also similarly derive the other coefficients.&lt;/p&gt;&#10;&#10;&lt;p&gt;Does anybody have any advice for how to wrap my head around the relationship between cell means and contrasts? Thanks. Is there a standard name for this type of contrast?&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Aha! According to the &lt;a href=&quot;http://www.ats.ucla.edu/stat/r/library/contrast_coding.htm&quot; rel=&quot;nofollow&quot;&gt;link posted in Glen_b's answer&lt;/a&gt;, the bottom line is, you can convert ANY comparison of group means you want into an R-style contrast attribute as follows:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Make a square matrix. The rows represent the levels of your factor and the columns represent contrasts. Except the first one, which tells the model what the intercept should represent.&lt;/li&gt;&#10;&lt;li&gt;If you want your intercept to be the grand mean, fill the first column with all of the same non-zero value, doesn't matter what. If you want the intercept to be one of the level means, put a number in that row and fill the rest with zeros. If you want the intercept to be a mean of several levels, put numbers in those rows and zeros in the rest. If you want it to be a weighted mean, use different numbers, otherwise use the same number. &lt;em&gt;You can even put in negative values in the intercept column and that probably means something too, but it completely changes the other contrasts, so I have no idea what that's for&lt;/em&gt;&lt;/li&gt;&#10;&lt;li&gt;Fill in the rest of the columns with positive and negative values indicating what levels you want compared to what others. I forget why summing to zero is important, but adjust the values so that the columns do sum to zero.&lt;/li&gt;&#10;&lt;li&gt;Transpose the matrix using the &lt;code&gt;t()&lt;/code&gt; function.&lt;/li&gt;&#10;&lt;li&gt;Use &lt;code&gt;ginv()&lt;/code&gt; from the &lt;code&gt;MASS&lt;/code&gt; package or &lt;code&gt;solve()&lt;/code&gt; to get the inverse of the transposed matrix.&lt;/li&gt;&#10;&lt;li&gt;Drop the first column, e.g. &lt;code&gt;mycontrast&amp;lt;-mycontrast[,-1]&lt;/code&gt;. You now have a p x p-1 matrix, but the information you put in for your intercept was encoded in the matrix as a whole during step 5.&lt;/li&gt;&#10;&lt;li&gt;If you want labels in the summary output more pleasant to read than &lt;code&gt;lm()&lt;/code&gt; et al.'s default output, name your matrix's columns accordingly. The intercept will always automatically get named &lt;code&gt;(Intercept)&lt;/code&gt; however.&lt;/li&gt;&#10;&lt;li&gt;Make your matrix the new contrast for the factor in question, e.g. &lt;code&gt;contrasts(mydata$myfactor)&amp;lt;-mymatrix&lt;/code&gt;&lt;/li&gt;&#10;&lt;li&gt;Run &lt;code&gt;lm()&lt;/code&gt; (and probably many other functions that use formulas) as normal in standard R without having to load &lt;code&gt;glht&lt;/code&gt;, &lt;code&gt;doBy&lt;/code&gt;, or &lt;code&gt;contrasts&lt;/code&gt;.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Glen_b, thank you, and thank you UCLA Statistical Consulting Group. My applied stats prof spent several days handwaving on this topic, and I was still left with no idea how to actually write my own contrast matrix. And now, an hour of reading and playing with R, and I finally think I get it. Guess I should have applied to UCLA instead. Or University of StackExchange.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-14T22:30:02.800" FavoriteCount="3" Id="52252" LastActivityDate="2013-03-15T07:46:02.210" LastEditDate="2013-03-15T07:46:02.210" LastEditorUserId="4829" OwnerUserId="4829" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;contrasts&gt;" Title="How to specify a contrast matrix (in R) for the difference between one level and an average of the others?" ViewCount="1034" />
  
  <row Body="&lt;p&gt;I think you are missing something still in your understanding of the purpose of cross-validation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's get some terminology straight, generally when we say 'a model' we refer to a particular method for describing how some input data relates to what we are trying to predict. We don't generally refer to particular instances of that method as different models. So you might say 'I have a linear regression model' but you wouldn't call two different sets of the trained coefficients different models. At least not in the context of model selection.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, when you do K-fold cross validation, you are testing how well your model is able to get trained by some data and then predict data it hasn't seen. We use cross validation for this because if you train using all the data you have, you have non left for testing. You could do this once, say by using 80% of the data to train and 20% to test, but what if the 20% you happened to pick to test happens to contain a bunch of points that are particularly easy (or particularly hard) to predict? We will not have come up with the best estimate possible of the models ability to learn and predict.&lt;/p&gt;&#10;&#10;&lt;p&gt;We want to use all of the data. So to continue the above example of an 80/20 split, we would do 5-fold cross validation by training the model 5 times on 80% of the data and testing on 20%. We ensure that each data point ends up in the 20% test set exactly once. We've therefore used every data point we have to contribute to an understanding of how well our model performs the task of learning from some data and predicting some new data.&lt;/p&gt;&#10;&#10;&lt;p&gt;But the purpose of cross-validation is not to come up with our final model. We don't use these 5 instances of our trained model to do any real prediction. For that we want to use all the data we have to come up with the best model possible. The purpose of cross-validation is model checking, not model building.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, say we have two models, say a linear regression model and a neural network. How can we say which model is better? We can do K-fold cross-validation and see which one proves better at predicting the test set points. But once we have used cross-validation to select the better performing model, we train that model (whether it be the linear regression or the neural network) on all the data. We don't use the actual model instances we trained during cross-validation for our final predictive model.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that there is a technique called bootstrap aggregation (usually shortened to 'bagging') that does in a way use model instances produced in a way similar to cross-validation to build up an ensemble model, but that is an advanced technique beyond the scope of your question here.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-03-15T02:53:12.133" Id="52277" LastActivityDate="2013-03-15T02:53:12.133" OwnerUserId="10354" ParentId="52274" PostTypeId="2" Score="21" />
  
  <row AcceptedAnswerId="52306" AnswerCount="2" Body="&lt;p&gt;Assume that I have a log transformed model as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;Model 1: Y = a + b*ln(x) -&gt; Interpretation: A 1% increase in X is associated with an average b/100 units increase in Y.&lt;/p&gt;&#10;&#10;&lt;p&gt;If I add 1 to X to avoid having 0 values and get:&lt;/p&gt;&#10;&#10;&lt;p&gt;Model 2: Y = c + d*ln(x+1)&lt;/p&gt;&#10;&#10;&lt;p&gt;Should I interpret the model as &quot;a 1% increase in (X+1) is associated with an average d/100 units increase in Y?&quot; Or there are some better ways to interpret the model? Thanks.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-15T05:35:56.697" Id="52285" LastActivityDate="2013-03-15T11:46:02.897" OwnerUserId="21071" PostTypeId="1" Score="2" Tags="&lt;mathematical-statistics&gt;&lt;data-transformation&gt;" Title="How to interpret a log transformed (X+1)?" ViewCount="443" />
  <row Body="&lt;p&gt;Or, you could look at alternatives to quadratic curves. Your data almost certainly isn't generated by a parabolic process, it just happens to fit a parabola better than a straight line. But, what if you do a likelihood ratio test and find that a cubic or quartic curve fits even better? You can keep going and end up with a high order polynomial that fits your data really really well. And then you add or remove a few data points, and the fitted model is now a polynomial with completely different coefficients that fits &lt;em&gt;that&lt;/em&gt; data really really well.&lt;/p&gt;&#10;&#10;&lt;p&gt;I struggled for a long time with this exact problem. If it's longitudinal data, you might take the first derivative, $(Y_{t}-Y_{t-1})\over(X_{t}-X_{t-1})$. This will hopefully linearize the data and you can interpret the main effects in the model as being about initial rates of income change (make sure to center your X variable so that 0 falls on an age that is interpretable-- maybe earliest age in the sample, maybe the mean age in the sample) and the slopes as being about the rate at which this change accelerates or decelerates with age. The Z variable has the same interpretation as it normally would except like everything else, it's affect the &lt;em&gt;rate&lt;/em&gt; of change rather than the actual income.&lt;/p&gt;&#10;&#10;&lt;p&gt;If all the measurements are from different individuals, however, I'm not sure what to do. Maybe find a nonlinear model appropriate to the data? Easier said than done, though.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-15T08:29:24.823" Id="52292" LastActivityDate="2013-03-15T08:29:24.823" OwnerUserId="4829" ParentId="38019" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;You could also do a QQPlot to see if the two distribtuions are different from each other.  Here is the R Documentation on how to compare two datasets to see if they are distributed the same. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://stat.ethz.ch/R-manual/R-patched/library/stats/html/qqnorm.html&quot; rel=&quot;nofollow&quot;&gt;http://stat.ethz.ch/R-manual/R-patched/library/stats/html/qqnorm.html&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The plot will look as a straight line if the two distributions are the same.  Any deviation will be represented by a deviance from a straight line.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-03-15T12:15:00.410" Id="52311" LastActivityDate="2013-03-15T12:15:00.410" OwnerUserId="20381" ParentId="52305" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Let me throw in a few points in addition to &lt;a href=&quot;http://stats.stackexchange.com/a/52277/4598&quot;&gt;Bogdanovist's answer&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;As you say, you train $k$ different models. They differ in that 1/(k-1)th of the training data is exchanged against other cases. These models are sometimes called &lt;em&gt;surrogate models&lt;/em&gt; because the (average) performance measured for these models is taken as a surrogate of the performance of the model trained on all cases.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, there are some assumptions in this process. &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;Assumption 1: the surrogate models are equivalent to the &quot;whole data&quot; model.&lt;br&gt;&#10;It is quite common that this assumption breaks down, and the symptom is the well-known pessimistic bias of $k$-fold cross validation (or other resampling based validation schemes). The performance of the surrogate models is on average worse than the performance of the &quot;whole data&quot; model if the learning curve has still a positive slope (i.e. less training samples lead to worse models).&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Assumption 2 is a weaker version of assumption 1: even if the surrogate models are on average worse than the whole data model, we assume them to be equivalent to each other. This allows summarizing the test results for $k$ surrogate models as one average performance.&lt;br&gt;&#10;Model instability leads to the breakdown of this assumption: the true performance of models trained on $N \frac{k - 1}{k}$ training cases varies a lot. You can measure this by doing iterations/repetitions of the $k$-fold cross validation (new random assignments to the $k$ subsets) and looking at the variance (random differences) between the predictions of different surrogate models for the same case.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The finite numer of cases means the performance measurement will be subject to a random error (variance) due to the finite number of test cases. This source of variance is different (and thus adds to) from the model instablilty variance.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;The differences in the observed performance are due to these two sources of variance.&lt;/p&gt;&#10;&#10;&lt;p&gt;The &quot;selection&quot; you think about is a data set selection: selecting one of the surrogate models means selecting a subset of training samples and claiming that this subset of training samples leads to a superior model. While this may be truely the case, usually the &quot;superiority&quot; is spurious. In any case, as picking &quot;the best&quot; of the surrogate models is a data-driven optimization, you'd need to validate (measure performance) this picked model with new unknown data. The test set within this cross validation is not independent as it was used to select the surrogate model.&lt;/p&gt;&#10;&#10;&lt;p&gt;You may want to look at our paper, it is about classification where things are usually worse than for regression. However, it shows how these sources of variance and bias add up.&lt;br&gt;&#10;Beleites, C. and Neugebauer, U. and Bocklitz, T. and Krafft, C. and Popp, J.: Sample size planning for classification models. Anal Chim Acta, 2013, 760, 25-33.&lt;br&gt;&#10;&lt;a href=&quot;http://dx.doi.org/10.1016/j.aca.2012.11.007&quot;&gt;DOI: 10.1016/j.aca.2012.11.007&lt;/a&gt;&lt;br&gt;&#10;&lt;a href=&quot;http://arxiv.org/abs/1211.1323&quot;&gt;accepted manuscript on arXiv: 1211.1323&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-03-15T12:22:58.430" Id="52312" LastActivityDate="2013-03-15T12:22:58.430" OwnerUserId="4598" ParentId="52274" PostTypeId="2" Score="9" />
  <row AcceptedAnswerId="52322" AnswerCount="2" Body="&lt;p&gt;Imagine the following setup: You have 2 coins, coin A which is &lt;em&gt;guaranteed&lt;/em&gt; to be fair, and coin B which may or may not be fair. You are asked to do 100 coin flips, and your objective is to &lt;em&gt;maximize the number of heads&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Your prior information about coin B is that it was flipped 3 times and yielded 1 head. If your decision rule was simply based on comparing the expected probability of heads of the 2 coins, you would flip coin A 100 times and be done with it. This is true even when using reasonable Bayesian estimations (posterior means) of the probabilities, since you have no reason to believe that coin B yields more heads.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, what if coin B is actually biased in favor of heads? Surely the &quot;potential heads&quot; you give up by flipping coin B a couple of times (and therefore gaining information about its statistical properties) would be valuable in some sense and therefore would factor into your decision. How can this &quot;value of information&quot; be described mathematically?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; How do you construct an optimal decision rule mathematically in this scenario?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-03-15T13:38:58.483" FavoriteCount="7" Id="52319" LastActivityDate="2013-03-15T22:03:22.473" OwnerUserId="21154" PostTypeId="1" Score="11" Tags="&lt;bayesian&gt;&lt;decision-theory&gt;" Title="Coin flipping, decision processes and value of information" ViewCount="441" />
  <row AnswerCount="0" Body="&lt;p&gt;Could someone please explain me this reviewer comment:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;asymmetric distribution could affect Principal&#10;  Component Analysis results,  symmetry of distribution should be&#10;  tested. Authors should also indicate if outliers were observed and&#10;  consequently excluded because they could affect factors&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;My question: what does it mean asymmetry distribution could affect PCA? And also outliers, could they affect factors?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you in advance  &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-03-15T13:50:02.933" FavoriteCount="1" Id="52320" LastActivityDate="2013-03-15T14:03:52.380" LastEditDate="2013-03-15T14:03:52.380" LastEditorUserId="88" OwnerUserId="22046" PostTypeId="1" Score="1" Tags="&lt;pca&gt;" Title="Asymmetric distribution and PCA" ViewCount="66" />
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I was surprised to discover yesterday that various summary statistics can be calculated on dates. Googling has found lots on syntax required to calculate summary statistics on dates, but very little on how calculations are done. How are summary statistics (mean, sd etc) calculated on dates?&lt;/p&gt;&#10;&#10;&lt;p&gt;Example R code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;myDates &amp;lt;- as.Date(paste0(sample(1:28, 100, replace=T), '/', &#10;                          sample(1:12, 100, replace=T), '/', &#10;                          2013), &#10;                   '%d/%m/%Y')&#10;&#10;mean(myDates)&#10;median(myDates)&#10;sd(myDates)&#10;var(myDates)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="6" CreationDate="2013-03-15T19:32:28.973" Id="52365" LastActivityDate="2013-03-15T22:02:03.533" OwnerUserId="12492" PostTypeId="1" Score="0" Tags="&lt;summary-statistics&gt;" Title="How is mean date calculated?" ViewCount="297" />
  
  
  
  <row Body="&lt;p&gt;A proportion test as zeferino suggest, or also a $\chi^2$ test or the Fisher exact probability test. &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-03-15T21:33:31.093" Id="52376" LastActivityDate="2013-03-15T21:33:31.093" OwnerUserId="18592" ParentId="52373" PostTypeId="2" Score="0" />
  
  
  <row Body="&lt;p&gt;If $f$ is tractable you may be able to compute an exact small sample variance of the estimator. Alternatively, you can use simulation from $f$ to investigate properties of the estimator. You might like to investigate the parametric bootstrap.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;May I know how to compute the exact small sample variance? Compute the Fisher information??&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;The Fisher Information would not give the exact small sample variance. You know $f$, and you know your estimator for $\theta$, so in some cases you can compute the sampling distribution of $\hat{theta}$ under $f$.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Are there any references of computing the sampling distribution of $\hat{\theta}$ under $f$?   &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;For particular instances, sure, and the approaches for doing so are pretty standard (there are several different techniques that may be useful). For example (given in each case that the $X_i$ are independent), if $f$ is a gamma density and  $\hat{\theta}$ is the sample mean, then it will have a gamma density. If $f$ is uniform and $n$ is odd and $\hat{\theta}$ is a median, then it will have a beta density. This is simply a matter of being able to derive the &lt;a href=&quot;http://www.google.com/search?q=distribution+of+transformations+of+random+variables&quot; rel=&quot;nofollow&quot;&gt;distributions of functions of random variables&lt;/a&gt;; however, in practice you usually just rely on already known results, of which there are quite a few.&lt;/p&gt;&#10;&#10;&lt;p&gt;As mentioned, you can also simulate to any desired degree of accuracy.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-03-16T01:11:23.677" Id="52394" LastActivityDate="2013-03-16T05:20:32.120" LastEditDate="2013-03-16T05:20:32.120" LastEditorUserId="805" OwnerUserId="805" ParentId="52333" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;The last answer is the best one for your situation. The basic approach is that each check-box should be stored as a 0 (unchecked) or 1 (checked). If you have logic in the questionnaire so some people do not get asked the question, you can have 0 (exposed to question, but unchecked), 1 (checked) and missing/null (not exposed to question). &lt;/p&gt;&#10;&#10;&lt;p&gt;The analysis can be very easy - sum up the values in the column (ie count all the 1s) and divide by the number of responses (count all the 1s and 0s). That's the percentage that checked the box and where you can start.&lt;/p&gt;&#10;&#10;&lt;p&gt;In some situations, when you have a very wide range of possible answers and each respondent has responses on a small set of that range, it may be more efficient to store each record as a combination of the respondent id, the response type, and the value of the response. This helps you avoid having a table with hundreds or thousands of columns which can be unwieldy for storage purposes.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-16T01:59:22.667" Id="52397" LastActivityDate="2013-03-16T01:59:22.667" OwnerUserId="3331" ParentId="52064" PostTypeId="2" Score="3" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm currently studying Hidden Markov Models. There's a set of observations from which I need to determine the optimal number of states. After having found the maximum likelihood using Baum-Welch, I considered two model selection criteria for determining the optimal states. These are Minimum Description length (MDL) and Bayesian Inference criterion (BIC). However, with MDL, the number of states=2 whereas with BIC it's 4. Does this mean that MDL performs better than BIC?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-16T03:41:03.203" Id="52404" LastActivityDate="2014-04-17T01:20:07.900" LastEditDate="2013-11-12T01:11:00.280" LastEditorUserId="4862" OwnerUserId="17362" PostTypeId="1" Score="5" Tags="&lt;hidden-markov-model&gt;&lt;bic&gt;" Title="Comparison between MDL and BIC" ViewCount="275" />
  
  
  
  
  <row Body="&lt;p&gt;In &lt;b&gt;finite precision&lt;/b&gt;, cycling may appear.&lt;/p&gt;&#10;&#10;&lt;p&gt;Cycling is frequent in single precision, exceptional in double precision.&lt;/p&gt;&#10;&#10;&lt;p&gt;When close to a local minimum, the objective function may sometimes slightly increase due to round-off errors. This is often innocuous as the algorithm function decreases again and eventually reaches a local minimum. But occasionally, the algorithm steps on a previously visited assignment, and start cycling.&lt;/p&gt;&#10;&#10;&lt;p&gt;It is easy and safe to watch for cycles in real-world stopping criteria implementations.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-16T22:37:57.113" Id="52440" LastActivityDate="2013-03-16T23:57:51.583" LastEditDate="2013-03-16T23:57:51.583" LastEditorUserId="7290" OwnerUserId="22097" ParentId="10024" PostTypeId="2" Score="1" />
  
  <row AnswerCount="1" Body="&lt;p&gt;For my masters dissertation I am currently preparing a Survey Questionnaire. The dissertation main aim is to find the factors which have an impact on failure of IT related projects in SMEs. I have done some extensive reading in the respective area and shortlisted around 30 factors.&lt;/p&gt;&#10;&#10;&lt;p&gt;My current plan is to use each of these factors as a Likert item, with 5 options. Then the idea was to find the mean and the standard deviation for each of these item. Then from the survey result shortlist the 10 most selected factors with the highest mean.&lt;/p&gt;&#10;&#10;&lt;p&gt;But googling around especially on this site I have read that it is advisable not to calculate the mean of the likert item, mostly because they are ordinal. &lt;/p&gt;&#10;&#10;&lt;p&gt;Is likert approach the right one? if yes, are there methods (other then mean calculation) to find the 10  most selected factors from the survey?&lt;/p&gt;&#10;&#10;&lt;p&gt;Help on this would really be appreciated.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank You&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-17T12:18:02.900" Id="52471" LastActivityDate="2013-03-17T15:02:11.423" LastEditDate="2013-03-17T15:02:11.423" LastEditorUserId="88" OwnerUserId="22117" PostTypeId="1" Score="2" Tags="&lt;likert&gt;" Title="Survey questionnaire to find out the important factors?" ViewCount="253" />
  <row Body="&lt;p&gt;I'm surprised nobody suggested the obvious solution:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; #generate completely separated data&#10;library(robustbase)&#10;set.seed(123)  &#10;x&amp;lt;-rnorm(200)&#10;x[1:40]&amp;lt;-x[1:40]+10  &#10;x[41:80]&amp;lt;-x[41:80]-10&#10;Rob&amp;lt;-ltsReg(x~1,nsamp=&quot;best&quot;)&#10;#all the good guys&#10;which(Rob$raw.weights==1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now for the explanation: the &lt;a href=&quot;http://svitsrv25.epfl.ch/R-doc/library/robustbase/html/ltsReg.html&quot; rel=&quot;nofollow&quot;&gt;&lt;code&gt;ltsReg&lt;/code&gt;&lt;/a&gt; function &#10;in package &lt;a href=&quot;http://cran.r-project.org/web/packages/robustbase/index.html&quot; rel=&quot;nofollow&quot;&gt;&lt;code&gt;robustbase&lt;/code&gt;&lt;/a&gt;, when called with the option &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;nsamp=&quot;best&quot;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;yields the univariate (exact) MCD weights. &#10;(these are a n-vector 0-1 weights stored &#10;in the &lt;code&gt;$raw.weights&lt;/code&gt; object. The algorithm&#10; to identify them is the MCD estimator (1)).  &lt;/p&gt;&#10;&#10;&lt;p&gt;In a nutshell, these weights are 1 for the &#10;members of the subset of $h=\lceil(n+2)/2\rceil$ most &#10;concentrated observations. &lt;/p&gt;&#10;&#10;&lt;p&gt;In dimension one, it starts by sorting all the &#10;observations then computes the measure of all &#10;contiguous subsets of $h$ observations: denoting &#10;$x_{(i)}$ the $i^{th}$ entry of the vector of sorted &#10;observations, it computes the measure of&lt;br&gt;&#10;(e.g. $(x_{(1)},...,x_{(h+1)})$ then $(x_{(2)},...,x_{(h+2)})$ &#10;and so forth...) then retains the one with smaller &#10;measure.&lt;/p&gt;&#10;&#10;&lt;p&gt;This algorithm assumes that your group of interest numbers&#10; a strict majority of the original sample and that it&#10;has a symmetrical distribution (but there no&#10; no hypothesis on the distribution of the remaining &#10;$n-h$ observation).&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;(1) P.J. Rousseeuw (1984). Least median of squares regression, Journal of&#10;  the American Statistical Association.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="0" CreationDate="2013-03-17T13:43:55.053" Id="52478" LastActivityDate="2013-03-17T16:07:09.307" LastEditDate="2013-03-17T16:07:09.307" LastEditorUserId="603" OwnerUserId="603" ParentId="899" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="52528" AnswerCount="1" Body="&lt;p&gt;In Enders' 'Applied Econometric Time Series', I repeatedly stumbled upon the notion of the &quot;ordering of a VAR model&quot; and I am not sure I understand the concept right. As far as I understand it, the concept relates to the problem of recovering the true $\epsilon$ of the underlying structural equations. &lt;/p&gt;&#10;&#10;&lt;p&gt;Looking e.g. at the following VAR in standard form:&lt;/p&gt;&#10;&#10;&lt;p&gt;$y_t = a_{10} + a_{11}y_{t-1} + a_{12}z_{t-1} + e_{1t}$&lt;/p&gt;&#10;&#10;&lt;p&gt;$z_t = a_{20} + a_{21}y_{t-1} + a_{22}z_{t-1} + e_{2t}$&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem is that the two errors are actually composites of the underlying shocks $\epsilon_{yt}$ and $\epsilon_{zt}$ of the structural equations, which we do not know. Here, Enders mentions that &quot;(...) if the correlation coefficient between the $e_{1t}$ and $e_{2t}$ is low, the ordering is not likely to be important&quot;, and vice versa. &lt;/p&gt;&#10;&#10;&lt;p&gt;My problem is that I do not grasp the concept of ordering and how it would affect the identification problem of the disturbances in the structural model. Does ordering refer to the order of the independent variables in the VAR?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-03-17T14:07:21.513" FavoriteCount="1" Id="52481" LastActivityDate="2013-03-17T22:10:21.827" LastEditDate="2013-03-17T14:32:37.483" LastEditorUserId="20898" OwnerUserId="20898" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;var&gt;" Title="Ordering in VAR models" ViewCount="792" />
  <row Body="&lt;p&gt;After some thinking and experimentation my conclusions are the following:&lt;/p&gt;&#10;&#10;&lt;p&gt;1) The &quot;fat-tails&quot; are consequence of Jensen Inequality, given that the &#10;   second derivative of the Gaussian density is positive in the tails:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac{\partial^2 \phi(x)}{\partial x^2} = (-1 + x^2)e^{-0.5x^2} &amp;gt; 0$$&lt;/p&gt;&#10;&#10;&lt;p&gt;for $|x| &amp;gt; 1$.&lt;/p&gt;&#10;&#10;&lt;p&gt;2) The relative error I was using is not ideal because the ratio of&#10;   densities is quite unstable. Using the Kullback-Leibler&#10;   divergence makes much more sense to compare densities.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-17T14:09:01.600" Id="52483" LastActivityDate="2013-03-17T14:09:01.600" OwnerUserId="26105" ParentId="52420" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;you could run QQPlots for two distributions against each other to see if they're from the same distribtion.  If two datasets are, then the QQPlot will appear as a straight line.  Comparing each of the three distributions pairwise should let you know if any of the distributions differ.  You could also make density plots or histograms of each of the data sets and overlay them on top of each other.  QQPlot in R is in the stats package.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-03-17T14:56:45.573" Id="52491" LastActivityDate="2013-03-17T14:56:45.573" OwnerUserId="20381" ParentId="52489" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="52705" AnswerCount="2" Body="&lt;p&gt;I am using Weka 3.6 to do Association Rule mining.  In our data set, each transaction is a word, and each letter in the word is an item.  The rules that we are mining would be in the format of &lt;code&gt;{a set of letters} -&amp;gt; {another set of letters}&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;So far, I have formatted six transactions by representing the existence of a letter with &lt;code&gt;'1'&lt;/code&gt; and the absence of a letter with &lt;code&gt;'0'&lt;/code&gt;, but this is giving me some unwanted rules.  Specifically, I am getting rules where the absence of a letter implies the absence of another letter, e.g. &lt;code&gt;C='0' ==&amp;gt; U='0'&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;How do I filter out rules that represent the absence of items?&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;For what it's worth, here is my &lt;code&gt;.arff&lt;/code&gt; file:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;@relation transactions&#10;@attribute A {'1','0'}&#10;@attribute C {'1','0'}&#10;@attribute D {'1','0'}&#10;@attribute E {'1','0'}&#10;@attribute I {'1','0'}&#10;@attribute K {'1','0'}&#10;@attribute M {'1','0'}&#10;@attribute N {'1','0'}&#10;@attribute O {'1','0'}&#10;@attribute U {'1','0'}&#10;@attribute Y {'1','0'}&#10;@data&#10;'0','0','0','1','0','1','1','1','1','0','1' %monkey&#10;'0','0','1','1','0','1','0','1','1','0','1' %donkey&#10;'1','0','0','1','0','1','1','0','0','0','0' %make&#10;'0','1','0','0','0','1','1','0','0','1','1' %mucky&#10;'0','1','0','1','1','1','0','1','1','0','0' %conkie&#10;'1','0','0','0','0','1','1','1','1','0','1' %mankoy&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2013-03-17T16:25:31.167" FavoriteCount="1" Id="52498" LastActivityDate="2013-03-19T15:18:19.723" LastEditDate="2013-03-17T20:48:53.513" LastEditorUserId="21395" OwnerUserId="21395" PostTypeId="1" Score="2" Tags="&lt;data-mining&gt;&lt;weka&gt;&lt;association-rules&gt;" Title="Excluding false values with association rule mining in Weka" ViewCount="269" />
  <row AcceptedAnswerId="52511" AnswerCount="1" Body="&lt;p&gt;I am struggeling with this definition the minimum sum of squares for a Matrix $M$:&lt;/p&gt;&#10;&#10;&lt;p&gt;The minimum value of $f(β)$ where $β = (X′X)^{−1}X′y$ from the system of normal equations $X′Xβ = X′y$ is $$y′(I − X(X′X)^{−1}X′)y = y′My$$,&#10;with $M = (I − X(X′X)^{−1}X′)$ a symmetric, idempotent (T × T)-Matrix. &#10;The trace of $M$ is&#10;$\operatorname{tr(M)} = \operatorname{tr}(I)−\operatorname{tr}(X(X′X)^{−1}X′) = T −K $and is &#10;equal to the rank $r(M)$. &lt;/p&gt;&#10;&#10;&lt;p&gt;What is meant by the rank $M$(can this be the &quot;real&quot; rank of the Matrix $M$) and how to come to this conclusion?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-17T17:35:30.507" Id="52502" LastActivityDate="2013-03-17T23:40:03.190" LastEditDate="2013-03-17T23:40:03.190" LastEditorUserId="805" OwnerUserId="13561" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;multiple-regression&gt;&lt;mathematical-statistics&gt;" Title="The multiple regression model - The minimum sum of squares" ViewCount="61" />
  <row Body="&lt;p&gt;The trace of an idempotent matrix is always equal to it's rank.&lt;a href=&quot;http://en.wikipedia.org/wiki/Idempotent_matrix&quot; rel=&quot;nofollow&quot;&gt;1&lt;/a&gt; The rank of a matrix is the size of the largest linear independent basis of the matrix. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Idempotent_matrix&quot; rel=&quot;nofollow&quot;&gt;Idempotent Matrix&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-03-17T18:58:31.433" Id="52511" LastActivityDate="2013-03-17T18:58:31.433" OwnerUserId="9568" ParentId="52502" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="52518" AnswerCount="3" Body="&lt;p&gt;As far as I understand, R-Squared explains how well the model predicts the observation. Adjusted R-Squared is the one that takes into account more observations (or degrees of freedom). So, Adjusted R-squared predicts the model better? Then why is this less than R squared? It appears it should often be more. &lt;/p&gt;&#10;&#10;&lt;p&gt;I would appreciate if anyone can point out where I made wrong assumption.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-17T20:13:31.483" FavoriteCount="2" Id="52517" LastActivityDate="2015-02-14T23:06:15.747" LastEditDate="2013-07-02T14:52:38.117" LastEditorUserId="6029" OwnerUserId="22133" PostTypeId="1" Score="6" Tags="&lt;regression&gt;&lt;r-squared&gt;" Title="Why is adjusted R-squared less than R-squared if adjusted R-squared predicts the model better?" ViewCount="21064" />
  
  <row Body="&lt;p&gt;There are at least a couple options with &quot;check all that apply&quot; data.&lt;/p&gt;&#10;&#10;&lt;p&gt;First, you could analyze each body part as a separate dependent (in this case) possibly logistic regression (injured that part vs. did not) or possibly some form of count regression (number of times that part was injured). I suspect a zero-inflated negative binomial regression would be best here.  The equipment would be the independent variable. &lt;/p&gt;&#10;&#10;&lt;p&gt;Second you could analyze combinations of body parts as the dependent variable. This would depend on having a large enough N to make various combinations common enough to analyze, and it would require some judgement in picking combinations. You don't say how many body parts you list, but it looks like it would be at least 10. That makes for a LOT of potential combinations. You might wind up with things like &quot;Head and something else&quot; but you'd have to be careful to make the categories exhaustive and exclusive. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-03-17T22:24:35.160" Id="52532" LastActivityDate="2013-03-17T22:24:35.160" OwnerUserId="686" ParentId="52525" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;ADF is a parametric test and KPSS is a non-parametric test of unit root. That being said, the chosen lag order in the ADF should be such that residuals are white noise.  &lt;/p&gt;&#10;" CommentCount="7" CreationDate="2013-03-17T22:32:09.863" Id="52533" LastActivityDate="2013-03-17T22:32:09.863" OwnerUserId="14860" ParentId="52505" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;If you look at the code for &lt;code&gt;plot.lm&lt;/code&gt; (by typing &lt;code&gt;plot.lm&lt;/code&gt;), you see these snippets in there (the comments are mine; they're not in the original):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;r &amp;lt;- residuals(x)                                # &amp;lt;---  r contains residuals&#10;&#10;...&#10;&#10;if (any(show[2L:6L])) {&#10;    s &amp;lt;- if (inherits(x, &quot;rlm&quot;)) &#10;        x$s&#10;        else if (isGlm) &#10;            sqrt(summary(x)$dispersion)   &#10;    else sqrt(deviance(x)/df.residual(x))        #&amp;lt;---- value of s&#10;    hii &amp;lt;- lm.influence(x, do.coef = FALSE)$hat  #&amp;lt;---- value of hii&#10;&#10;...&#10;&#10;    r.w &amp;lt;- if (is.null(w)) &#10;        r                                        #&amp;lt;-- r.w  for unweighted regression&#10;    else sqrt(w) * r&#10;    rs &amp;lt;- dropInf(r.w/(s * sqrt(1 - hii)), hii)  # &amp;lt;-- std. residual in plots&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So - if you don't use weights - the code clearly defines its standardized residuals to be the internally studentized residuals defined here:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Studentized_residual#How_to_studentize&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Studentized_residual#How_to_studentize&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;which is to say:&lt;/p&gt;&#10;&#10;&lt;p&gt;$${\widehat{\varepsilon}_i\over \widehat{\sigma} \sqrt{1-h_{ii}\  }}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;(where $\widehat{\sigma}^2={1 \over n-m}\sum_{j=1}^n \widehat{\varepsilon}_j^{\,2}$, and $m$ is the column dimension of $X$).&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-03-17T22:56:02.870" Id="52535" LastActivityDate="2013-03-21T05:47:19.050" LastEditDate="2013-03-21T05:47:19.050" LastEditorUserId="805" OwnerUserId="805" ParentId="52522" PostTypeId="2" Score="3" />
  
  <row Body="" CommentCount="0" CreationDate="2013-03-17T23:55:08.747" Id="52546" LastActivityDate="2013-03-17T23:55:08.747" LastEditDate="2013-03-17T23:55:08.747" LastEditorUserId="-1" OwnerUserId="-1" PostTypeId="5" Score="0" />
  
  
  <row Body="&lt;p&gt;Yes.&lt;/p&gt;&#10;&#10;&lt;p&gt;And here is how you go about finding one.&lt;/p&gt;&#10;&#10;&lt;p&gt;For our purposes, convex means $F''(x)\ge 0$ and concave means $F''(x)\le 0$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Ok, so let $F$ be such a function. If we also assume monotonicity, we have $F'(x)\ge 0$, and $F$ is a cumulative distribution function. Therefore, the convex and concave conditions are $f'(x)\ge 0$ for $x\le c$ and $f'(x) \le 0$ for $x\ge c$ (where $f=F'$ is the pdf of $F$).&lt;/p&gt;&#10;&#10;&lt;p&gt;In other words, now we are looking for density function on $[0,1]$ that is increasing on $[0,c]$ and decreasing on $[c,1]$. We go to a table of probability distributions on $[0,1]$ (e.g., &lt;a href=&quot;http://en.wikipedia.org/wiki/List_of_probability_distributions&quot; rel=&quot;nofollow&quot;&gt;Wikipedia's list&lt;/a&gt;) and see that the cumulative distribution function of the &lt;a href=&quot;http://en.wikipedia.org/wiki/Beta_distribution&quot; rel=&quot;nofollow&quot;&gt;Beta distribution&lt;/a&gt; fits the bill.&lt;/p&gt;&#10;&#10;&lt;p&gt;Another approach would be to explicitly construct such a function (I believe a quartic would do the job).&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2013-03-18T08:44:49.183" Id="52576" LastActivityDate="2013-03-18T14:24:00.870" LastEditDate="2013-03-18T14:24:00.870" LastEditorUserId="2970" OwnerUserId="5234" ParentId="52571" PostTypeId="2" Score="6" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have a question on coding a Likert scale in SPSS.&lt;/p&gt;&#10;&#10;&lt;p&gt;Assuming we have a 7-point Likert scale, a &quot;well-being&quot; score may rank from 1 through 7.&lt;/p&gt;&#10;&#10;&lt;p&gt;While doing some reading on multi-variate analysis, I've come across a reference that suggests a &quot;deviation score&quot;.   That is, it's ranging from [-3, -2, -1, 0, 1, 2, 3].&lt;/p&gt;&#10;&#10;&lt;p&gt;For later analysis of survey results, which method (&quot;well-being score&quot; vs. &quot;deviation score&quot;) will serve me best?    Or are these &quot;apples and oranges&quot;?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks,&#10;EEH&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-18T14:01:15.983" FavoriteCount="1" Id="52589" LastActivityDate="2013-03-18T15:04:44.543" LastEditDate="2013-03-18T15:04:44.543" LastEditorUserId="88" OwnerUserId="22000" PostTypeId="1" Score="1" Tags="&lt;likert&gt;" Title="Likert scale coding" ViewCount="221" />
  <row Body="&lt;p&gt;Let the parameters be $\mu$ and $\sigma$, which are the expectation and standard deviation of the associated &lt;em&gt;normal&lt;/em&gt; distribution.  Then the expected value of the lognormal distribution is $m = \exp(\mu + \sigma^2/2)$ and its $0.95$ quantile is $q = \exp(\mu + z_{0.95} \sigma)$ where $z_{0.95}$ is the $0.95$ quantile of the standard Normal distribution (equal approximately to $1.645$).  Taking logarithms yields two simultaneous equations&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\log(m) = \mu + \sigma^2/2$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\log(q) = \mu + z_{0.95} \sigma.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;There are two solutions,&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\sigma = z\pm\sqrt{z^2+2 \log\left(\frac{m}{q}\right)};\quad \mu = \log(q) - z\sigma.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;($z_{0.95}$ is abbreviated $z$.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Only one of them gives a positive value of $\sigma$ unless $q \gt m$.  (Lognormal distributions can be so skewed that their means exceed high upper percentiles.)  For example, with $m=1$ and $q=2$, the two PDFs have graphs like these:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/izUrI.png&quot; alt=&quot;Plots&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;You will need to provide some additional criterion for selecting between these two solutions.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-18T15:50:39.307" Id="52600" LastActivityDate="2013-03-18T16:50:43.150" LastEditDate="2013-03-18T16:50:43.150" LastEditorUserId="919" OwnerUserId="919" ParentId="52592" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;No. The results depend on n. In the extreme case consider n = 2 or n = 3. This will not detect even large deviations from normality in your sample.&lt;/p&gt;&#10;&#10;&lt;p&gt;One easier option would be just adjust the significance threshold of your normality test, but in practice one should still follow these two steps:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Consider which deviations from normality would&#10;affect your analysis &lt;/li&gt;&#10;&lt;li&gt;Look for that&lt;/li&gt;&#10;&lt;li&gt;Decide if the magnitudes of the deviations (if any) is strong enough&#10;that it has to be taken into account&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;One option would be to just print QQ-Plots for each sample and have someone with experience look at them. If most of them are good and the bad ones really obvious this will take little time. Still another option would be to use methods which are robust to occasional outliers (but this really depends on your problem whether it's applicable). &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-18T15:58:47.017" Id="52602" LastActivityDate="2013-03-18T15:58:47.017" OwnerUserId="10524" ParentId="52598" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="52607" AnswerCount="1" Body="&lt;p&gt;I have a dataset with two classes, A and B, and an independent variable x. Class B is a minority class with only 1% of the observations, which tend to occur with small values of x. I am interested in the relative densities for the classes, which I would like to visualize in R.&lt;/p&gt;&#10;&#10;&lt;p&gt;In my imagination, I would use &lt;code&gt;plot(density(data[cl==&quot;A&quot;]$x)/density(data$x))&lt;/code&gt;, but R doesn't allow division of densities. How should I do this?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-03-18T15:59:34.293" Id="52603" LastActivityDate="2013-03-19T07:29:36.523" OwnerUserId="19765" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;data-visualization&gt;&lt;density&gt;" Title="Relative density plots in R" ViewCount="287" />
  <row Body="&lt;p&gt;Combining your own code with whuber's suggestions (but please bear in mind his warning), something like this might do the trick (untested):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;xmin&amp;lt;-min(data$x)&#10;xmax&amp;lt;-max(data$x)&#10;n&amp;lt;-1000&#10;Adens&amp;lt;-density(data[cl==&quot;A&quot;]$x), from=xmin, to=xmax, n=n)&#10;Totdens&amp;lt;-density(data$x, from=xmin, to=xmax, n=n)&#10;&#10;# Because each density by default has a total area of 1, rescale the y-values of Adens &#10;# such that its area represents the actual proportion of the A-class. Then, plot the&#10;# ratio between both densities.&#10;Aprop &amp;lt;- length(data[cl==&quot;A&quot;]$x)/length(data$x)&#10;plot(Adens$x, Aprop*Adens$y/Totdens$y, type=&quot;l&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Does that work for you?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-03-18T16:37:48.647" Id="52607" LastActivityDate="2013-03-19T07:29:36.523" LastEditDate="2013-03-19T07:29:36.523" LastEditorUserId="19765" OwnerUserId="4257" ParentId="52603" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Multivariate displays are tricky, especially with that number of variables. I have two suggestions.&lt;/p&gt;&#10;&#10;&lt;p&gt;If there are certain variables that are particularly important to the clustering, or substantively interesting, you can use a scatterplot matrix and display the bivariate relationships between your interesting variables. You could even used enhanced scatterplots (e.g. use shapes with size proportional to a third variable) to add in some more dimensionality&lt;/p&gt;&#10;&#10;&lt;p&gt;Alternatively, you could use a springplot which was developed for displaying high dimensional data that exhibits clustering. Note, I have never seen this in the literature I am familiar with, but I think it is a very interesting way of displaying multivariate data. The following citation is where the plot was originally proposed. &lt;/p&gt;&#10;&#10;&lt;p&gt;Hoffman,P.E. et al. (1997) DNA visual and analytic data mining. In the Proceedings of the IEEE Visualization. Phoenix, AZ, pp. 437-441.&lt;/p&gt;&#10;&#10;&lt;p&gt;And here is where I originally found mention of it.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://orange.biolab.si/docs/latest/widgets/rst/visualize/radviz/#hoffman1997&quot; rel=&quot;nofollow&quot;&gt;http://orange.biolab.si/docs/latest/widgets/rst/visualize/radviz/#hoffman1997&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, fair warning, I haven't been able to find an implementation of springplots outside of Orange. Then again, I haven't searched that hard! &lt;/p&gt;&#10;&#10;&lt;p&gt;I am assuming that your data is real valued and continuous, if it is discrete or non-interval, so on so forth, I don't think either plots would be helpful.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-18T21:18:16.263" Id="52633" LastActivityDate="2013-03-18T21:18:16.263" OwnerUserId="22190" ParentId="52625" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;Interesting problem.  &lt;/p&gt;&#10;&#10;&lt;p&gt;If we assume that each message exchanged between pairs of nodes contain summary information about every other exchange that has happened, for example, ID and status or ID and last known location.  &lt;/p&gt;&#10;&#10;&lt;p&gt;In that case, any single node will have access to information about other nodes, and the number of nodes that any node has information on will increase as a function of time.&lt;/p&gt;&#10;&#10;&lt;p&gt;The closest analogy to this is from statistical mechanics, where over time, the number of collisions between distinct particles increases, to the point where with probability 1, every node has some (perhaps out of date) information about every other node.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-18T22:57:22.460" Id="52640" LastActivityDate="2013-03-18T22:57:22.460" OwnerUserId="14640" ParentId="29810" PostTypeId="2" Score="1" />
  
  
  
  <row Body="&lt;p&gt;we use models with multiplicative interaction effects when relationship between independent variable and dependent variable are non-additive.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;My question is, Are all models with multiplicative interaction effects non-linear? and all models with additive interaction effects linear?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;The answer to such a question depends on what you mean when you say 'linear' and 'nonlinear', and what domain of models you're restricting yourself to.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Usually&lt;/em&gt; the terms 'linear' and 'nonlinear' in statistical models refers to linearity &lt;em&gt;in the parameters&lt;/em&gt;, not the variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;So for example, $y = \alpha x^2 +\epsilon$ is &lt;em&gt;linear&lt;/em&gt; in $\alpha$ though not in $x$, while $y = \exp(-\alpha) x +\epsilon$ is &lt;em&gt;non-linear&lt;/em&gt; in $\alpha$, though it is in $x$. In usual parlance, the first is a linear model and the second is not. However, in those cases at least both may be turned into models that are linear in both the parameters and the predictors - in the first case by the transformation $x^* = x^2$, giving a model that has a linear relationship between $y$ and $x^*$, and in the second case by the reparameterization $\alpha^* = \exp(-\alpha)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;As such a standard general linear model (regression-type model) with multiplicative interaction is &lt;em&gt;linear in the parameters&lt;/em&gt;, even though it's not linear in either predictor (IV). However, note that even in terms of the IVs, it is &lt;em&gt;conditionally linear&lt;/em&gt; - fix one of the IVs and the relationship is linear in the other.&lt;/p&gt;&#10;&#10;&lt;p&gt;[Minor mathematical aside: It should be noted that when we're taking about the relationships of $y$ and some $x$ being linear (in &lt;a href=&quot;http://en.wikipedia.org/wiki/Linearity&quot; rel=&quot;nofollow&quot;&gt;this&lt;/a&gt; sense rather than the 'makes a straight-line' sense), if we recognize we're using homogeneous co-ordinates in regression, it &lt;em&gt;is linear&lt;/em&gt;. I mention it because I have seen people with just enough mathematics to be dangerous object that 'linear regression is not linear'.]&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;all models with additive interaction effects linear?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;If I understand what you're even asking with 'additive interaction effects', there's really no such thing. If it's additive it's already in the main effects and there's nothing left over for some notional 'interaction'.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Also, With non-linearity, the effect of independent variable on dependent variable depends on the value of independent variable, &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Only if you think of 'effect' as inherently linear&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;in effect, independent variable somehow interacts with itself. &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;This way lies much confusion. Why not just think of there being a relationship that's described by some curve rather than by a straight line?&lt;/p&gt;&#10;&#10;&lt;p&gt;--&lt;/p&gt;&#10;&#10;&lt;p&gt;Edit to address followup questions:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;What do you mean when you say &quot;what domain of models you're restricting yourself to&quot;? &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;When you said &quot;all models with multiplicative interaction effects&quot; you presumably meant 'all models' in some class, such as regression models, or general linear models, or &lt;em&gt;generalized linear models&lt;/em&gt;, or ... the list could go on for some time. &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Thanks! for noting about linearity. For the longest time, even I thought being linear meant the relationship was a straight line.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Me too.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;This does clear some doubts, but raises a few questions. So, if we recognize we're using homogeneous co-ordinates in regression, it is linear. &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;In terms of $x$'s - the actual columns of the $X$-matrix in a regression - it's linear in that linked mathematical sense if you realize you're working with homogeneous co-ordinates.&lt;/p&gt;&#10;&#10;&lt;p&gt;A multiple linear regression is &lt;em&gt;already&lt;/em&gt; linear in the mean parameters (i.e. the $\beta$ vector, the parameters other than $\sigma^2$), without any such need to invoke homogeneous co-ordinates. That I was referring to the relationship with the $x$'s when I raised homogeneous co-ordinates was explicitly stated.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Also, Did you mean to say &quot;Only if you think of 'effect' as inherently nonlinear instead of linear?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Nope. The way you phrased the question I was responding to only makes sense if you take the word 'effect' to imply linearity, otherwise the whole notion of 'interaction with itself' seems to be utterly meaningless. How is one to interpret the phrase?&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;What I meant to ask was. I read somewhere that &quot;with non-linearity, the effect of X on Y depends on the value of X and X somehow interacts with itself&quot;. &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I regard the statement as an unhelpful attempt at analogy, and, as already explained, I think you should not think about it this way. Not everything that someone writes down is useful.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Does this mean that X interacts with itself(X)? or does it mean that X interact with other variables(X,W etc.) if any?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I'm not going to make any further attempt to interpret something that makes no sense without first having a great deal of clarification of its intent. I've suggested &lt;em&gt;a&lt;/em&gt; way to interpret it that makes at least a little sense, kind of. If you want to interpret it more generally, explaining what it means would be up to you - or the original author of it. As a general statement, it makes &lt;em&gt;no sense whatever&lt;/em&gt; to me. &lt;/p&gt;&#10;&#10;&lt;p&gt;I expect if you were to ask what, exactly, it means, you would receive an answer that contained a lot of hidden premises, and one of those premises would rely, directly or indirectly, on taking underlying meaning of 'effects' to be linear, when we have no good reason to do that.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2013-03-19T00:24:30.330" Id="52649" LastActivityDate="2013-03-19T23:46:22.790" LastEditDate="2013-03-19T23:46:22.790" LastEditorUserId="805" OwnerUserId="805" ParentId="52644" PostTypeId="2" Score="2" />
  <row AnswerCount="1" Body="&lt;p&gt;I am a newbie in mathematical statistics and haven't learnt any group theory before. My lecture notes are too brief. How can the Borel subsets on $\mathbb R$ satisfy A.2 and A.3 of a $\sigma$-algebra ? &lt;/p&gt;&#10;&#10;&lt;p&gt;A.2 is closure under complement. A.3 is under countable unions.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-03-19T01:07:14.160" Id="52651" LastActivityDate="2013-03-19T05:47:42.827" LastEditDate="2013-03-19T04:03:51.140" LastEditorUserId="2970" OwnerUserId="22202" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;self-study&gt;&lt;mathematical-statistics&gt;" Title="Why are the Borel subsets on $\mathbb R$ a $\sigma$-algebra?" ViewCount="124" />
  
  
  <row AcceptedAnswerId="53013" AnswerCount="1" Body="&lt;p&gt;Currently I am working on a bayesian model framework and have questions related to the philosophy of using such techniques of modeling.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;How do I know that the prior which I have captured from the experts is valid. There are parameters of the model which has captured a very wide range - say, 10% to 90%. It does not give me comfort, rather, it may show that the expert panel inputs missed out on the clear range. Is there any method out there which may allow me to check this? &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;We do not have enough data to work on, thus, the Bayesian framework. When can we say that Bayesian analysis is not required and the whole analysis/ model can be done using data. Is there any threshold on data availability/ philosophy where it indicates the transition from Bayesian to classical? (I understand that they are techniques from two different school of thought, so Bayesian is used in our case for lack of data)&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The conjugate prior method for a beta-beta-beta model gives &#10;alpha estimate for posterior = a1+a2-1&#10;beta estimate for posterior = b1+b2-1. &#10;the question which haunts me is, how will the effect of a larger data set be captured in the posterior parameters, if this model is being used post the required amount of data is available?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;It would be great if someone can answer my questions. If further clarification is required on my thoughts/ questions... please do let me know. Thanks!&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-03-19T08:42:42.427" Id="52672" LastActivityDate="2013-04-21T21:58:05.000" OwnerUserId="22162" PostTypeId="1" Score="0" Tags="&lt;bayesian&gt;" Title="Queries on the Bayesian method" ViewCount="100" />
  
  <row AnswerCount="1" Body="&lt;p&gt;What is the probability of rolling exactly two sixes in 6 rolls of a die?&lt;/p&gt;&#10;&#10;&lt;p&gt;Solution by the Binomial Probability formula is &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\binom{6}{2} \left(\frac{1}{6}\right)^2 \left(\frac{5}{6}\right)^4 = \frac{15 \times 5^4}{6^6} = \frac{3125}{15552} \approx 0.200939.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;But by basic probability understanding&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Probability = successful outcome/ total possible outcomes&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;So that way the probability should be $15/ (6^6) \approx 0.0003215.$&lt;/p&gt;&#10;&#10;&lt;p&gt;I know that the second calculation is not giving the right answer, but somehow am not convinced on why it is wrong as I don’t see the approach being wrong.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can someone help me understand why the second approach is wrong?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-19T15:46:01.893" Id="52709" LastActivityDate="2013-03-19T19:33:32.373" LastEditDate="2013-03-19T19:33:32.373" LastEditorUserId="88" OwnerUserId="22235" PostTypeId="1" Score="2" Tags="&lt;probability&gt;&lt;binomial&gt;" Title="Binomial probability function" ViewCount="50" />
  <row Body="&lt;p&gt;To get to 15, you just count the possible &quot;locations&quot; where the double six can occur (like: a six in the first place can be combined with a six on either 5 of the other positions etc). &lt;/p&gt;&#10;&#10;&lt;p&gt;However, you are forgetting that each set of locations (e.g. a six on the first two rolls) occurs more than once in you 6^6 rolls: once for each of the combinations of 4 non-six rolls of the other dice.&lt;/p&gt;&#10;&#10;&lt;p&gt;Something like:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;6 6 1 1 1 1&#10;6 6 1 1 1 2&#10;6 6 1 1 1 3&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;etc.&lt;/p&gt;&#10;&#10;&lt;p&gt;Of course, for each location (again, I mean 2 &quot;positions of the sixrolled dice), there are 5^4 possible non-six rolls for the remaining 4 rolls...&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-19T16:16:44.647" Id="52718" LastActivityDate="2013-03-19T16:16:44.647" OwnerUserId="4257" ParentId="52709" PostTypeId="2" Score="3" />
  <row AnswerCount="2" Body="&lt;p&gt;I have a dataset of M elements where every item is represented by a feature vector of length N where N is very large and only a small subset of N is bigger then zero for every item. So I have a sparse MxN  matrix and I want to cluster these M items.&lt;/p&gt;&#10;&#10;&lt;p&gt;What tools and algorithms do you advise to use? Any script or library in R or in other programming languages would be very useful.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-03-19T16:37:57.023" Id="52721" LastActivityDate="2013-03-20T09:38:23.260" LastEditDate="2013-03-19T19:38:10.813" LastEditorUserId="88" OwnerUserId="4001" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;clustering&gt;&lt;feature-selection&gt;&lt;sparse&gt;" Title="Sparse hyperspace clustering" ViewCount="142" />
  <row Body="&lt;p&gt;You want to compare the group that finished the treatment with the group that didn't (20 vs 11). You want to use a t-test to do that (not a paired t-test since your data isn't paired). That said, the sample size of 11 will determine the statistical power of the test, which will be very low. Only very significant differences in mean will be detected. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-19T17:35:23.520" Id="52726" LastActivityDate="2013-03-19T17:35:23.520" OwnerUserId="21972" ParentId="52614" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;Of course they are different, because the three interaction terms are different.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/7vnk4.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In your scheme one, all the data in your centered variable &quot;quant&quot; will turn to 0 if the subject is a male (int1).&lt;/p&gt;&#10;&#10;&lt;p&gt;In your scheme two, all the data in your centered variable &quot;quant&quot; will change sign if the subject is a male (int2).&lt;/p&gt;&#10;&#10;&lt;p&gt;In your scheme three, all the data in your centered variable &quot;quant&quot; will be as is if it is from a male, but double if it's from a female (int3).&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, a couple points for the upcoming discussion:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;int1, int2, and int3 are three different variables and they have different means and standard deviations. If you feed a third variable into a regression model, the coefficients of the two per-exisiting predictors' may change. This is extremely common for continuous predictors that are not 100% independent from each other.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Standardized coefficient is just the original coefficient adjusted by the ratio of SD(x) and SD(y): $Standardized\beta x_i = \beta x_i \frac{SD_{xi}}{SD_y}$&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Because of the above two points, the final standardized coefficients can change. The standardized coefficient of centeredmeanB changes because you add a different third variable into the model, causing its coefficient to change. Once it changes, the standardized beta also changes even the SD of centeredmenaB remains the same.&lt;/p&gt;&#10;&#10;&lt;p&gt;The changes in the interaction terms is easier to perceive: they are different variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you use generalized linear model to try this one more time, you should see that scheme number one is the closest to how interaction terms are usually tested. I'd suggest sticking to 1/0 coding for binary variables, the coefficients would make more sense, the intercept makes more sense, and the interaction calculated from it is likely to cause fewer mishaps.&lt;/p&gt;&#10;&#10;&lt;p&gt;A better advice, though, is to capitalize on the generalize linear model module in SPSS. Users can specify if the variable is categorical or continuous. That way no matter how you code your gender variable, the results will always be consistent.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-03-19T20:56:09.457" Id="52753" LastActivityDate="2013-03-19T21:09:26.053" LastEditDate="2013-03-19T21:09:26.053" LastEditorUserId="13047" OwnerUserId="13047" ParentId="52727" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;This is kind of similar to capture-recapture (or mark-and-recapture) sampling, but instead of assuming random selection and trying to estimate the population size (you know it already), you want to see if the recapture is consistent with resampling 'at random'. Your problem turns out to be easier.&lt;/p&gt;&#10;&#10;&lt;p&gt;So imagine you have an urn full of white balls. When you sample yours, you magically turn them black before putting them back. So now the urn has (otherwise identical) black and white balls. &lt;/p&gt;&#10;&#10;&lt;p&gt;Your friend draws his sample, and your interest is &quot;does he get 'too many' black balls?&quot;. If he's sampling at random, the number of black balls in his sample follows a &lt;em&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Hypergeometric_distribution&quot; rel=&quot;nofollow&quot;&gt;hypergeometric distribution&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can assess the probability of getting as many as he got, or more (i.e. a sample at least as unusual as his) by finding the upper tail probability from the hypergeometric. In large samples like yours, you could use a normal approximation (I'd be inclined to use a continuity correction).&lt;/p&gt;&#10;&#10;&lt;p&gt;To formally &lt;em&gt;test&lt;/em&gt; whether it could have happened by chance, you'd compare that upper tail probability with your favorite significance level.&lt;/p&gt;&#10;&#10;&lt;p&gt;This tells you the probability of a result at least as weird as he got &lt;em&gt;if he wasn't copying you&lt;/em&gt;. To compute the &lt;em&gt;actual&lt;/em&gt; probability you ask about:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&quot;Hmm... There are 10,000 'things' to choose from, and I assume (for simplicity!) that each one has an equal chance of being chosen. I've picked 900 'things'. My friend picked 1500 things... and he picked 890 things in common with me. How likely is it that my friend peeked at my 'picking' history, and copied me?&quot;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;- which flips the conditioning around - would require &lt;a href=&quot;http://en.wikipedia.org/wiki/Bayes%27_theorem&quot; rel=&quot;nofollow&quot;&gt;taking a Bayesian approach&lt;/a&gt;. Which means you need a prior probability that he copied you.&lt;/p&gt;&#10;&#10;&lt;p&gt;Using the numbers in your question, to look at the upper tail probabilities mentioned before, the numbers you give already start to look a little bit suspicious at 152 items in common:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; phyper(152,900,10000-900,1500,lower.tail=FALSE)&#10;[1] 0.04497392&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;quite suspicious at by 160:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; phyper(160,900,10000-900,1500,lower.tail=FALSE)&#10;[1] 0.007129847&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And you can certainly rule out random selection producing results like that well before it gets as high as 890.&lt;/p&gt;&#10;&#10;&lt;p&gt;That doesn't &lt;em&gt;automatically&lt;/em&gt; imply that your friend copied you - perhaps there's some &lt;em&gt;other&lt;/em&gt; source of nonrandomness. It just says 'this didn't just happen by chance'.&lt;/p&gt;&#10;&#10;&lt;p&gt;The give the numbers in a particular instance, the probability that 'the larger includes the smaller' is just an extreme-tail hypergeometric probability. This will generally be &lt;em&gt;very&lt;/em&gt; small, unless there are hardly any items in the smaller sample.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-19T22:27:30.393" Id="52762" LastActivityDate="2013-03-19T22:53:49.500" LastEditDate="2013-03-19T22:53:49.500" LastEditorUserId="805" OwnerUserId="805" ParentId="52756" PostTypeId="2" Score="1" />
  <row Body="&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;How to decide number of Hidden States (although HMM says we don't need to know, we just have to make a guess, even for making guess what should be the best criteria)&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;The number of hidden states is problem dependent. For example in speech recognition and synthesis, 3 and 5 states are commonly used. The reason for using these is that speech is a highly variable data. So the distribution at different instants of speech sounds (phonemes) varies with time and each state models the different distributions.&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Once define hidden states let say 5, then how to define initial probabilities for each hidden state and the transitional probabilities among each other...&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;An HMM can be defined by (A, B, pi), where A is a matrix of state transition probabilities, B is a vector of state emission probabilities and pi (a special member of A) is a vector of initial state distributions. The following steps are taken to estimate these parameters:&lt;/strong&gt;  &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;strong&gt;For the A and pi parameters, randomly initialise the HMM (between 0 and 1)&lt;/strong&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;strong&gt;Initialise the B parameter by uniformly segmenting the training data and estimating the global mean and variance. The B parameter deals with the mean and variances of each state&lt;/strong&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;strong&gt;Re-estimate and refine the parameters using the Baum-Welch algorithm. This is a variant of the well known Expectation-Maximation (EM) algorithm.&lt;/strong&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;References:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Rabiner, L. 1989. A tutorial on hidden Markov models and selected applications in speech recognition.&lt;/li&gt;&#10;&lt;li&gt;Baum, L.E., T. Petrie, G. Soules and N. Weiss. 1970. A Maximization Technique Occurring in the Statistical Analysis of Probabilistic Functions of Markov Chains. The Annals of Mathematical Statistics Vol. 41, No. 1, pp. 164-171.&lt;/li&gt;&#10;&lt;li&gt;Dempster, A. P., N. M. Laird, and D. B. Rubin. 1977. Maximum likeli- hood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society. Series B (Methodological) 39 (1), pp. 1-38.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="1" CreationDate="2013-03-19T22:54:44.817" Id="52767" LastActivityDate="2013-03-20T00:40:17.343" LastEditDate="2013-03-20T00:40:17.343" LastEditorUserId="7290" OwnerUserId="22256" ParentId="47846" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;The mean and standard deviation of the histogram method are quite close to the sample mean and sample standard deviation of the raw data. The KDE is way off.&lt;/p&gt;&#10;&#10;&lt;p&gt;I tried this in Matlab (because I do not know R) with the same kernel width and generated 5000 random points. My mean and standard deviation look good:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&#10;x = [11.62, 37.49, 43.61, -8.42, -24.9, -43.34, -8.19, 53.99, -1.44, 47.67, 33.92, -35.03, 31.12, -0.41, -9.78,-11.59,20.34, 25.9, 19.75, 36.44, -8.07, 5.71, 5.5, 18.79, 31.71, 24.02, 18.37, -0.99, 52.62, 31.56, 6.56, -10.78, 43.36,11.96, 0.47, 26.89, -8.73, 22.8, 16.48, 12.45, -10.06, 23.98, 11.06, -8.5, 4.01, 14.31, 18.98, -14.66, -26.47, 37.2,23.84, -7.18, 6.56, 18.44, 32.5, -4.92, 21.55, 22.56, 6.27, 31.73, 18.67, 5.25, 16.61, 31.69, -3.11, 30.47, 7.62,10.08,1.32, 37.58, 22.96, 33.36, 28.58, 21.04, -9.11, -11.89, -22.1, 28.68, 10.88, 4.91, 15.79, 5.49, -37, 26.46, 15.06,2.11,16]';&#10;&#10;kd = fitdist(x, 'kernel', 'width', 10);&#10;randdata = kd.random(1000,1);&#10;mean(randdata)&#10;std(randdata)&#10;&lt;/code&gt;&#10;&lt;/pre&gt;&#10;&#10;&lt;p&gt;which gives me&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&#10;ans =&#10;&#10;   10.9047&#10;&#10;&#10;ans =&#10;&#10;   23.0171&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I guess there a bug in your code.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-03-19T23:26:15.670" Id="52769" LastActivityDate="2013-03-19T23:26:15.670" OwnerUserId="22257" ParentId="52763" PostTypeId="2" Score="3" />
  
  
  <row Body="&lt;p&gt;A measure that is low when highly skewed raters agree is actually highly desirable.  Gwet's AC1 specifically assumes that chance agreement should be at most 50%, but if both raters vote +ve 90% of the time, Cohen and Fleiss/Scott says that chance agreement is 81% on the positives and 1% on the negatives for a total of 82% expected accuracy.&lt;/p&gt;&#10;&#10;&lt;p&gt;This is precisely the kind of bias that needs to be eliminated.  A contingency table of&lt;/p&gt;&#10;&#10;&lt;p&gt;81 9&lt;br&gt;&#10; 9 1&lt;/p&gt;&#10;&#10;&lt;p&gt;represents chance level performance.  Fleiss and Cohen Kappa and Correlation are 0 but AC1 is a misleading 89%.  We of course see the accuracy of 82% and also see Recall and Precision and F-measue of 90%, if we considered them in these terms...&lt;/p&gt;&#10;&#10;&lt;p&gt;Consider two raters, one of whom is a linguist who gives highly reliable part of speech ratings - noun versus verb say, and the other of whom is unbeknownst a computer program which is so hopeless it just guesses.&lt;/p&gt;&#10;&#10;&lt;p&gt;Since water is a noun 90% of the time, the linguist says noun 90% of the time and verb 10% of the time.&lt;/p&gt;&#10;&#10;&lt;p&gt;One form of guessing is to label words with their most frequent part of speech, another is to guess the different parts of speech with probability given by their frequency.  This latter &quot;prevalence-biased&quot; approach will be rated 0 by all Kappa and Correlation measures, as well as DeltaP, DeltaP', Informedness and Markedness (which are the regression coefficients which give one directional prediction information, and whose geometric mean is the Matthews Correlation). It corresponds to the table above.&lt;/p&gt;&#10;&#10;&lt;p&gt;The &quot;most frequent&quot; part of speech random tagger gives the following table for 100 words:&lt;/p&gt;&#10;&#10;&lt;p&gt;90 10&lt;br&gt;&#10; 0  0&lt;/p&gt;&#10;&#10;&lt;p&gt;That is it predicts correctly all 90 the linguist's nouns, but none of the 10 verbs.&lt;br&gt;&#10;All Kappas and Correlations, and Informedness, give this 0, but AC1 gives it a misleading 81%.&lt;/p&gt;&#10;&#10;&lt;p&gt;Informedness is giving the probability that the tagger is making an informed decision, that is what proportion of the time it is making an informed decision, and correctly returns no.&lt;/p&gt;&#10;&#10;&lt;p&gt;On the other hand, Markedness is estimating what proportion of the time the linguist is correctly marking the word, and it underestimates 40%.  If we considered this in terms of the precision and recall of the program, we have a Precision of 90% (we get the 10% wrong that are verbs), but since we only consider the nouns, we have a Recall of 100% (we get all of them as the computer always guesses noun).  But Inverse Recall is 0, and Inverse Precision is undefined as computer makes no -ve predictions (consider the inverse problem where verb is the +ve class, so computer is no always predicting -ve as the more prevalent class).&lt;/p&gt;&#10;&#10;&lt;p&gt;In the Dichotomous case (two classes) we have&lt;/p&gt;&#10;&#10;&lt;p&gt;Informedness = Recall + Inverse Recall - 1.&#10;Markedness = Precision + Inverse Precision - 1.&#10;Correlation = GeoMean (Informedness, Markedness).&lt;/p&gt;&#10;&#10;&lt;p&gt;Short answer - Correlation is best when there is nothing to choose between the raters, otherwise Informedness.  If you want to use Kappa and think both raters should have the same distribution use Fleiss, but normally you will want to allow them to have their own scales and use Cohen.  I don't know of any example where AC1 would give a more appropriate answer, but in general the unintuitive results come because of mismatches between the biases/prevalences of the two raters' class choices.  When bias=prevalence=0.5 all of the measures agree, when the measures disagree it is your assumptions that determine what is appropriate, and the guidelines I've given reflect the corresponding assumptions.&lt;/p&gt;&#10;&#10;&lt;p&gt;This Water example originated in...&lt;/p&gt;&#10;&#10;&lt;p&gt;Jim Entwisle and David M. W. Powers (1998), &quot;The Present Use of Statistics in the Evaluation of NLP Parsers&quot;, pp215-224, NeMLaP3/CoNLL98 Joint Conference, Sydney, January 1998.  -  should be cited for all Bookmaker theory/history purpose.&#10;&lt;a href=&quot;http://david.wardpowers.info/Research/AI/papers/199801a-CoNLL-USE.pdf&quot; rel=&quot;nofollow&quot;&gt;http://david.wardpowers.info/Research/AI/papers/199801a-CoNLL-USE.pdf&lt;/a&gt;&#10;&lt;a href=&quot;http://dl.dropbox.com/u/27743223/199801a-CoNLL-USE.pdf&quot; rel=&quot;nofollow&quot;&gt;http://dl.dropbox.com/u/27743223/199801a-CoNLL-USE.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Informedness and Markedness versus Kappa are explained in...&lt;/p&gt;&#10;&#10;&lt;p&gt;David M. W. Powers (2012). &quot;The Problem with Kappa&quot;. Conference of the European Chapter of the Association for Computational Linguistics (EACL2012) Joint ROBUS-UNSUP Workshop. - cite for work using Informedness or Kappa in an NLP/CL context.&#10;&lt;a href=&quot;http://aclweb.org/anthology-new/E/E12/E12-1035.pdf&quot; rel=&quot;nofollow&quot;&gt;http://aclweb.org/anthology-new/E/E12/E12-1035.pdf&lt;/a&gt;&#10;&lt;a href=&quot;http://dl.dropbox.com/u/27743223/201209-eacl2012-Kappa.pdf&quot; rel=&quot;nofollow&quot;&gt;http://dl.dropbox.com/u/27743223/201209-eacl2012-Kappa.pdf&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-03-20T11:29:15.477" Id="52808" LastActivityDate="2013-03-21T12:40:42.053" LastEditDate="2013-03-21T12:40:42.053" LastEditorUserId="21922" OwnerUserId="21922" ParentId="29717" PostTypeId="2" Score="3" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I have a logit model that comes up with a number between 0 and 1 for many cases, but how can we interprete this? &lt;/p&gt;&#10;&#10;&lt;p&gt;Lets take a case with a logit of 0.20&lt;/p&gt;&#10;&#10;&lt;p&gt;Can we assert that there is 20% probability that a case belongs to group B vs group A? &lt;/p&gt;&#10;&#10;&lt;p&gt;is that the correct way of interpreting the logit value?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-03-20T15:32:59.893" FavoriteCount="2" Id="52825" LastActivityDate="2013-11-15T01:04:38.427" LastEditDate="2013-03-22T11:28:53.583" LastEditorUserId="88" OwnerUserId="22289" PostTypeId="1" Score="5" Tags="&lt;regression&gt;&lt;logistic&gt;&lt;logit&gt;" Title="What does the logit value actually mean?" ViewCount="2810" />
  
  
  
  <row Body="&lt;p&gt;Could you maybe specify your model and give a screenshot of the output, then I could give you an detailed answer, but as a first try.... you may want to check out also the following examples on these websites:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.ats.ucla.edu/stat/stata/seminars/stata_logistic/default.htm&quot; rel=&quot;nofollow&quot;&gt;http://www.ats.ucla.edu/stat/stata/seminars/stata_logistic/default.htm&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.ats.ucla.edu/stat/stata/dae/logit.htm&quot; rel=&quot;nofollow&quot;&gt;http://www.ats.ucla.edu/stat/stata/dae/logit.htm&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.ats.ucla.edu/stat/stata/faq/oratio.htm&quot; rel=&quot;nofollow&quot;&gt;http://www.ats.ucla.edu/stat/stata/faq/oratio.htm&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.ats.ucla.edu/stat/mult_pkg/faq/general/odds_ratio.htm&quot; rel=&quot;nofollow&quot;&gt;http://www.ats.ucla.edu/stat/mult_pkg/faq/general/odds_ratio.htm&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;so if the coefficient is 0.2 it depends on the variable, I guess you have a dummy, which is e.g. 0 for group B and 1 for group A?&lt;/p&gt;&#10;&#10;&lt;p&gt;odds ratio is given by:&#10;$OR = e^b$&lt;/p&gt;&#10;&#10;&lt;p&gt;so in your case: $e^{70.20}$&lt;/p&gt;&#10;&#10;&lt;p&gt;This would be the odds ratio of your group variable corresponding to your reference group.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-03-20T15:59:10.067" Id="52832" LastActivityDate="2013-03-20T15:59:10.067" OwnerUserId="21998" ParentId="52825" PostTypeId="2" Score="0" />
  <row AnswerCount="1" Body="&lt;p&gt;I am analyzing pre- and post- intervention data for which I have 6 dependent variables for two groups (treatment, no treatment). I want to know if, controlling for pre scores from the 6 measures, post scores differ between the two groups. (Some DVs show significant correlations with each other.) &lt;/p&gt;&#10;&#10;&lt;p&gt;I want to know whether a MANCOVA is appropriate, with all 6 DVs and the corresponding 6 pre tests entered as covariates, or whether running a separate ANCOVA for each DV and the corresponding pre test as covariate is more appropriate. &lt;/p&gt;&#10;&#10;&lt;p&gt;Other ideas for analysis are welcome, thank you in advance.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-20T16:42:47.877" Id="52835" LastActivityDate="2014-08-14T01:26:35.203" LastEditDate="2014-08-14T01:26:35.203" LastEditorUserId="805" OwnerUserId="22293" PostTypeId="1" Score="1" Tags="&lt;ancova&gt;&lt;mancova&gt;" Title="MANCOVA or several ANCOVAS" ViewCount="163" />
  <row AcceptedAnswerId="52872" AnswerCount="1" Body="&lt;p&gt;I need to difference the series once to get a stationary series but then cannot run the tbats function because my differenced series has negative values. Does anyone know of any way of handling with this? I'm thinking of adding a constant but don't know whether I'm cheating or not. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-20T17:41:47.003" Id="52844" LastActivityDate="2013-03-21T00:19:18.857" OwnerUserId="22114" PostTypeId="1" Score="1" Tags="&lt;r&gt;" Title="TBATS requires positive data after differencing - forecast package R" ViewCount="77" />
  <row AcceptedAnswerId="52848" AnswerCount="1" Body="&lt;p&gt;Here is a statistics question which I have been thinking about while working with some of my data. I have a large dataset named &quot;bigbird&quot; (say about a billion rows) and I want to randomly sample a smaller dataset named &quot;smallbird&quot; from it using R. Now, I can easily do this with the following code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;smallbird&amp;lt;-bigbird[sample(1:nrow(bigbird),1000000,replace=FALSE),]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This is great and my theoretical model works to a certain degree. However, I am trying to finetune my model and I realize that I may have hit a theoretical concern as far as the random sampling is concerned. First, imagine that some of the variables inside bigbird are of similar form as follows:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;user,observation_no,year&#10;A, 5,1998&#10;B,7,2003&#10;A,6,1998&#10;D,1,2010&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Essentially, I have users, a observation number (which references a whole different set of variables) and the year in which they made a certain observation. I have 2 issues that need clarifying as follows:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;From looking at the overall dataset, it is evident that as my time period progresses (1998-2012), I have more observations and more new users in every year in an exponential fashion. That is, in 2012, there are many, many more new users in the dataset than in year 1998.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Similarly, as my time period progresses, it appears that many, many more observations are being made by users in a given year than in previous years. That is, in 2012, user A may have made 50 different observations as opposed to only 1 observation in year 1998.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I was looking for opinions, discussions and solutions to these 2 issues because I think (and &lt;strong&gt;please&lt;/strong&gt; correct me if I am wrong), that simple random sampling will not take care of these 2 issues. Thanks !&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-03-20T18:06:36.007" Id="52845" LastActivityDate="2013-07-19T18:29:27.770" OwnerUserId="10535" PostTypeId="1" Score="1" Tags="&lt;sampling&gt;&lt;large-data&gt;&lt;weighted-sampling&gt;" Title="Sampling small dataset from large dataset with reference to a given variable" ViewCount="77" />
  
  <row Body="&lt;p&gt;I second Nick Sabbe's harsh comment, and my short answer is, &lt;em&gt;It is not&lt;/em&gt;. I mean, it only is in the normal linear model. For absolutely any other sort of circumstances, the exact distribution is not a $\chi^2$. In many situations, you can hope that Wilks' theorem conditions are satisfied, and then &lt;strong&gt;asymptotically&lt;/strong&gt; the log-likelihood ratio test statistics converges in distribution to $\chi^2$. Limitations and violations of the conditions of Wilks' theorem are too numerous to disregard.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;The theorem assumes i.i.d. data $\Rightarrow$ expect issues with dependent data, such as time series or unequal probability survey samples (for which the likelihoods are poorly defined, anyway; the &quot;regular&quot; $\chi^2$ tests, such as independence tests in contingency tables, start behaving as a sum $\sum_k a_k v_k, v_k \sim \mbox{i.i.d.} \chi^2_1$ (&lt;a href=&quot;http://www.citeulike.org/user/ctacmo/article/1449501&quot;&gt;Rao &amp;amp; Scott&lt;/a&gt;). For i.i.d. data, $a_k=1$, and the sum becomes the $\chi^2$. But for non-independent data, this is no longer the case.&lt;/li&gt;&#10;&lt;li&gt;The theorem assumes the true parameter to be in the interior of the parameter space. If you have a Euclidean space to work with, that's not an issue. However, in some problems, the natural restrictions may arise, such as variance $\ge$ 0 or correlation between -1 and 1. If the true parameter is one the boundary, then the asymptotic distribution is a mixture of $\chi^2$ with different degrees of freedom, in the sense that the cdf of the test is the sum of such cdfs (&lt;a href=&quot;http://www.citeulike.org/user/ctacmo/article/1227041&quot;&gt;Andrews 2001&lt;/a&gt;, plus two or three more of his papers from the same period, with history going back to &lt;a href=&quot;http://www.citeulike.org/user/ctacmo/article/553241&quot;&gt;Chernoff 1954&lt;/a&gt;).&lt;/li&gt;&#10;&lt;li&gt;The theorem assumes that all the relevant derivatives are non-zero. This can be challenged with some nonlinear problems and/or parameterizations, and/or situations when a parameter is not identified under the null. Suppose you have a Gaussian mixture model, and your null is one component $N(\mu_0,\sigma^2_0)$ vs. the alternative of two distinct components $f N(\mu_1,\sigma_1^2) + (1-f) N(\mu_2,\sigma_2^2)$ with a mixing fraction $f$. The null is apparently nested in the alternative, but this can be expressed in a variety of ways: as $f=0$ (in which case the parameters $\mu_1,\sigma_1^2$ are not identified), $f=1$ (in which case $\mu_2, \sigma_2^2$ are not identified), or $\mu_1=\mu_2, \sigma_1=\sigma_2$ (in which case $f$ is not identified). Here, you can't even say how many degrees of freedom your test should have, as you have different number of restrictions depending on how you parameterize the nesting. See the work of Jiahua Chen on this, e.g. &lt;a href=&quot;http://www.citeulike.org/user/ctacmo/article/2989041&quot;&gt;CJS 2001&lt;/a&gt;.&lt;/li&gt;&#10;&lt;li&gt;The $\chi^2$ may work OK if the distribution has been correctly specified. But if it was not, the test will break down again. In the (largely neglected by statisticians) subarea of multivariate analysis known as structural equation covariance modeling, a multivariate normal distribution is often assumed, but even if the structure is correct, the test will misbehave if the distribution is different. &lt;a href=&quot;http://www.citeulike.org/user/ctacmo/article/582282&quot;&gt;Satorra and Bentler 1995&lt;/a&gt; show that the distribution will become $\sum_k a_k v_k, v_k \sim \mbox{i.i.d.} \chi^2_1$, the same story as with non-independent data in my point 1, but they've also demonstrated how the $a_k$s depend on the structure of the model and the fourth moments of the distribution.&lt;/li&gt;&#10;&lt;li&gt;For finite samples, in a large class of situations likelihood ratio is &lt;a href=&quot;http://www.jstor.org/stable/2333436&quot;&gt;Bartlett-correctible&lt;/a&gt;: while ${\rm Prob}[d(y) \le x]=F(x;\chi^2_d)[1+O(n^{-1})]$ for a sample of size $n$, and $F(x;\chi^2_d)$ being the distribution function of the $\chi^2_d$ distribution, for the regular likelihood problems you can find a constant $b$ such that ${\rm Prob}[d(y)/(1+b/n) \le x]=F(x;\chi^2_d)[1+O(n^{-2})]$, i.e., to a higher order of accuracy. So the $\chi^2$ approximation for finite samples can be improved (and arguably should be improved if you know how). The constant $b$ depends on the structure of the model, and sometimes on the auxiliary parameters, but if it can be consistently estimated, that works, too, in improving the order of coverage.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;For a review of these and similar esoteric issues in likelihood inference, see &lt;a href=&quot;http://www.citeulike.org/user/ctacmo/article/1318051&quot;&gt;Smith 1989&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-03-20T19:31:10.340" Id="52855" LastActivityDate="2013-03-20T19:31:10.340" OwnerUserId="5739" ParentId="52829" PostTypeId="2" Score="10" />
  
  
  
  <row AcceptedAnswerId="52885" AnswerCount="1" Body="&lt;p&gt;The &quot;classical&quot; $100(1-\alpha)\%$-confidence interval starts from the Student's t statistic $$t=\frac{\bar{x}-\mu}{s/\sqrt{n}}.$$&#10;Then, one obtains the desired result, e.g. $\bar{x}\pm t_{1-\alpha/2;n-1}\frac{s}{\sqrt{n}}$ by simple algebraic manipulation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's my problem. I am using an approximation for $\bar{x}$, which leads me to the following modified $t$ statistic: $$t'=\frac{a(\bar{x}-\mu)^2+(\bar{x}-\mu)+b}{s/\sqrt{n}}$$ where $a$ and $b$ are parameters depending on sample size, and some central moments. I would like to derive a CI for $\mu$ using $t'$, but the squared term gives me a little trouble. &lt;/p&gt;&#10;&#10;&lt;p&gt;Is it as simple as treating the enumerator as a quadratic equation in $\bar{x}-\mu$, solving it and moving terms around? That is, let $$a(\bar{x}-\mu)^2+(\bar{x}-\mu)+b=t' \frac{s}{\sqrt{n}},$$ find the roots $r_1, r_2$ and then write, e.g.,  $\bar{x}-\mu=r_1$, yielding $\mu = \bar{x}-r_1$ and thus the CI is $(\bar{x}-r_1) \pm t' (s n^{-1/2})$ ? If both roots are real, how do I decide on which to pick?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-21T01:11:42.157" Id="52875" LastActivityDate="2013-03-22T02:35:47.927" OwnerUserId="22312" PostTypeId="1" Score="1" Tags="&lt;confidence-interval&gt;&lt;estimation&gt;" Title="Creating a CI for the mean from an approximation of x-bar" ViewCount="78" />
  
  
  <row Body="&lt;p&gt;I did all of the questions from the first four chapters six years ago. Here's what I have:&lt;/p&gt;&#10;&#10;&lt;p&gt;$p(\mu,\theta)\propto\left|\frac{\partial \lambda}{\partial \mu}\right|p(\lambda,\theta)=\mu^{-1}.$&lt;/p&gt;&#10;&#10;&lt;p&gt;So&lt;/p&gt;&#10;&#10;&lt;p&gt;$\begin{array}{lrl}p\left(N,\theta\right) &amp;amp; = &amp;amp; \int_{0}^{\infty}p\left(\mu,N,\theta\right)\mathrm{d}\mu\\ &amp;amp; = &amp;amp; \int_{0}^{\infty}p\left(\mu,\theta\right)\Pr\left(N\,|\,\mu\right)\mathrm{d}\mu\\ &amp;amp; \propto &amp;amp; \int_{0}^{\infty}\mu^{-1}\left(\frac{\mu^{N}}{N!}e^{-\mu}\right)\mathrm{d}\mu\\ &amp;amp; = &amp;amp; \frac{\left(N-1\right)!}{N!}=N^{-1}\end{array}$&lt;/p&gt;&#10;&#10;&lt;p&gt;You don't need to be worried that $p(N,\theta)$ doesn't depend on $\theta$. This just means that the prior for $\theta$ is uniform on $[0,1]$, which is cool for a Bernoulli parameter.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-21T04:07:32.370" Id="52888" LastActivityDate="2013-03-21T04:07:32.370" OwnerUserId="9738" ParentId="52873" PostTypeId="2" Score="9" />
  <row Body="&lt;p&gt;1) The method of sampling the joint and then looking at the sampling distribution of $X$ from the pairs where $Y=y_i$ will give you samples from the conditional distribution ($p(X|Y=y_i)$), no problem there.&lt;/p&gt;&#10;&#10;&lt;p&gt;2) there are a number of ways of generating conditionals but you usually need to know (at least!) their joint probability function or some other things you don't appear to know (the conditioning in the opposite direction plus the marginals for example).&lt;/p&gt;&#10;&#10;&lt;p&gt;Please note that for discrete variables, we don't call the probability function a density; that term applies to continuous random variables.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-03-21T05:28:59.180" Id="52893" LastActivityDate="2013-03-21T05:28:59.180" OwnerUserId="805" ParentId="52887" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I want to see if there is a correlation (or any sort of relationship) between two time series I am working with.&lt;/p&gt;&#10;&#10;&lt;p&gt;One of them is a times series of temperature data, the other one is concentration of a substance. I am trying to see if there is a relationship between the concentration and temperature.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, temperature data series is not stationary (and there are gaps in it), and neither is the concentration series. The problem is that I don't have enough information on the temperature data series since I don't have data for even one period (one year). &lt;/p&gt;&#10;&#10;&lt;p&gt;What can I do?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-21T05:44:57.040" FavoriteCount="1" Id="52896" LastActivityDate="2013-03-21T10:05:54.100" LastEditDate="2013-03-21T10:05:54.100" LastEditorUserId="88" OwnerUserId="22301" PostTypeId="1" Score="1" Tags="&lt;cross-correlation&gt;" Title="Cross-correlation between two time series" ViewCount="218" />
  
  
  
  <row Body="&lt;p&gt;&lt;strong&gt;This problem describes &lt;em&gt;five&lt;/em&gt; situations, or states.&lt;/strong&gt;  Although it can be solved (somewhat painfully, in my view) using just two of them, as a good principle of &lt;em&gt;modeling&lt;/em&gt; we should start out by contemplating all five.&lt;/p&gt;&#10;&#10;&lt;p&gt;The states, and the transitions among them, are:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;$A$: &lt;strong&gt;Quiz $A$ is given.&lt;/strong&gt;  This transitions to $G$ (&quot;Good&quot;) when a majority passes it.  Presumably the chance that a majority passes is the same as the chance that one person passes it, $3/10$.  (I say &quot;presumably&quot; because this &quot;obvious&quot; deduction depends on implicit assumptions, such as that the chances of passing are independent among people, and without those assumptions the deduction can be false.)  Otherwise this transitions to $Z$ (&quot;Bad&quot;) with probability $1-3/10 = 7/10$.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;$B$: &lt;strong&gt;Quiz $B$ is given.&lt;/strong&gt;  This transitions to $G$ with probability $6/10$ and $Z$ with probability $1-6/10=4/10$.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;$C$&quot; &lt;strong&gt;Quiz $C$ is given.&lt;/strong&gt;  This transitions to $G$ with probability $9/10$ and $Z$ with probability $1-9/10=1/10$.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;$G$ (a &lt;strong&gt;&quot;good&quot; day&lt;/strong&gt;): This transitions to $A$, $B$, and $C$ with equal probabilities of $1/3$ each.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;$Z$ (a &lt;strong&gt;&quot;bad&quot; day&lt;/strong&gt;): This transitions to $A$ with probability $1$.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;The transition matrix $\mathbb{P}$ for $(A,B,C,G,Z)$ therefore is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\left(&#10;\begin{array}{ccccc}&#10; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \frac{3}{10} &amp;amp; \frac{7}{10} \\&#10; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \frac{3}{5} &amp;amp; \frac{2}{5} \\&#10; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \frac{9}{10} &amp;amp; \frac{1}{10} \\&#10; \frac{1}{3} &amp;amp; \frac{1}{3} &amp;amp; \frac{1}{3} &amp;amp; 0 &amp;amp; 0 \\&#10; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0&#10;\end{array}&#10;\right).$$&lt;/p&gt;&#10;&#10;&lt;p&gt;This is &lt;em&gt;bipartite&lt;/em&gt;: the transitions alternate between states in $\{A,B,C\}$ and states in $\{G,Z\}$.  By taking &lt;em&gt;two transitions&lt;/em&gt; at once--which models what happens in the course of a single day--we can study the evolution of the distribution in states $\{A,B,C\}$ and, separately, the evolution in states $\{G,Z\}$.  As expected, $\mathbb{P}^2$ decomposes into a direct sum.  One block is for the $\{A,B,C\}$ transitions,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\mathbb{P}_{ABC} = \left(&#10;\begin{array}{ccc}&#10; \frac{4}{5} &amp;amp; \frac{1}{10} &amp;amp; \frac{1}{10} \\&#10; \frac{3}{5} &amp;amp; \frac{1}{5} &amp;amp; \frac{1}{5} \\&#10; \frac{2}{5} &amp;amp; \frac{3}{10} &amp;amp; \frac{3}{10}&#10;\end{array}&#10;\right)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and the other is for the $\{G,Z\}$ transitions,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\mathbb{P}_{GZ} = \left(&#10;\begin{array}{cc}&#10; \frac{3}{5} &amp;amp; \frac{2}{5} \\&#10; \frac{3}{10} &amp;amp; \frac{7}{10}&#10;\end{array}&#10;\right).$$&lt;/p&gt;&#10;&#10;&lt;p&gt;From these matrices (in the usual purely mechanical ways) we deduce the stationary distributions $(5/7,1/7,1/7)$ for $(A,B,C)$ and $(3/7, 4/7)$ for $(G,Z)$: these are the (normalized-to-sum-to-unity) left eigenvectors with eigenvalues $1$.  From them we immediately read off the answers to question 1 ($5/7$, the $A$ component of the first eigenvector) and question 2 ($3/7$, the $G$ component of the second eigenvector).&lt;/p&gt;&#10;&#10;&lt;p&gt;These are &lt;em&gt;stationary&lt;/em&gt; distributions.  Let's track what actually happens when we start, say, in state $A$ (initially, quiz $A$ is given).  In this simulation, which used only the transition matrix $\mathbb{P}_{ABC}$, the majority failed $A$, so $A$ was given again; the majority failed again; finally the majority passed, and $B$ was randomly assigned; and so on.  I have plotted the results for the first $100$ days:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ULDQt.png&quot; alt=&quot;Point plot&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;After 100 days, quiz $A$ had been given $72$ times ($0.72$ of the total), quiz $B$ $17$ times ($0.17$), and quiz $C$ $12$ times ($0.12$).  Clearly these &lt;em&gt;realized frequencies&lt;/em&gt; do not equal the &lt;em&gt;limiting probabilities&lt;/em&gt; of $5/7 \approx 0.714$, $1/7 \approx 0.143$, and $1/7$, respectively, but they are &lt;em&gt;approximately&lt;/em&gt; the same.&lt;/p&gt;&#10;&#10;&lt;p&gt;After running a simulation for a million days(!), though, the counts happened to be $713446$, $143495$, and $143060$: each, as a fraction of the total, is considerably closer to the limiting probabilities.&lt;/p&gt;&#10;&#10;&lt;p&gt;Were we to re-run these simulations, the results would likely differ from the results I have obtained, and they would still differ from the limiting probabilities.  However, the differences in &lt;em&gt;proportions&lt;/em&gt; for the million-day simulation would be relatively small.  It is only in this &lt;em&gt;limiting,&lt;/em&gt; approximate sense that the stationary distributions of the Markov chain describe what really might happen as the chain evolves in the real world.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-21T15:23:24.083" Id="52926" LastActivityDate="2013-03-21T15:23:24.083" OwnerUserId="919" ParentId="52834" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;It is the reduced Chi-Square statistic, used for testing how 'useful' a model is for the data. You divide the chi-square statistic by the degrees of freedom to get a scaled measure of variance (it equals sum of squares divided by degrees of freedom). The ratio of two reduced Chi-Square statistics is the F statistic (used for testing variance between the two).&lt;/p&gt;&#10;&#10;&lt;p&gt;See this wikipedia article for information on interpreting the resulting value: &lt;a href=&quot;http://en.wikipedia.org/wiki/Goodness_of_fit&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Goodness_of_fit&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-03-21T17:54:11.167" Id="52936" LastActivityDate="2013-03-21T19:50:22.893" LastEditDate="2013-03-21T19:50:22.893" LastEditorUserId="21972" OwnerUserId="21972" ParentId="52935" PostTypeId="2" Score="1" />
  
  
  <row AcceptedAnswerId="52972" AnswerCount="2" Body="&lt;p&gt;I have this homework question I'm not 100% sure how to tackle.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a random vector with joint distribution function F(x,y) and am asked to find the marginal distribution function. &lt;/p&gt;&#10;&#10;&lt;p&gt;I think need to get to $fx(x,y)$, but I'm not sure how to do this.&lt;/p&gt;&#10;&#10;&lt;p&gt;My thinking is that I need to differentiate F(x,y) by both x and y,  to get f(x,y) and then integrate over y, but this gets me the marginal density for x.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any pointers would be greatly appreciated &lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT - Thanks Glen. I've switched to my proper user id. It's late here, I'll revisit tomorrow.&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT2 - to clarify, the first q in the homework starts at $F(x,y)$ and asks for marginal &lt;em&gt;probability&lt;/em&gt;. The second starts with $f(x,y)$ and asks for marginal &lt;em&gt;density&lt;/em&gt; so I'm assuming that in the first instance, they're after a pdf, and in the second, a cdf; to rule out suspicions of casual terminology use.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-03-22T03:31:07.503" FavoriteCount="1" Id="52971" LastActivityDate="2013-04-19T19:58:42.820" LastEditDate="2013-03-23T13:47:22.327" LastEditorUserId="22374" OwnerUserId="22374" PostTypeId="1" Score="0" Tags="&lt;self-study&gt;&lt;marginal&gt;&lt;multivariable&gt;" Title="How to get from joint distribution F(x,y) to f(x,y) to calculate the marginal distribution of X?" ViewCount="206" />
  <row Body="&lt;p&gt;To calculate the density of any conjugate prior see &lt;a href=&quot;http://mathoverflow.net/questions/63496/what-can-be-said-about-an-infinite-linear-chain-of-conjugate-prior-distributions/65203#65203&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, you don't need to evaluate the conjugate prior of the Dirichlet in order to perform Bayesian estimation of its parameters.  Just average the sufficient statistics of all the samples, which are the vectors of log-probabilities of the components of your observed categorical distribution parameters.  This average sufficient statistic are the expectation parameters of the maximum likelihood Dirichlet fitting the data $(\chi_i)_{i=1}^n$.  To go from expectation parameters to source parameters, say $(\alpha_i)_{i=1}^n$, you need to solve using numerical methods:&#10;\begin{align}&#10;\chi_i = \psi(\alpha_i) - \psi\left(\sum_j\alpha_j\right) \qquad \forall i&#10;\end{align}&#10;where $\psi$ is the digamma function.&lt;/p&gt;&#10;&#10;&lt;p&gt;To answer your first question, a mixture of Dirichlets is not Dirichlet because, for one thing, it can be multimodal.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-22T07:05:12.060" Id="52975" LastActivityDate="2013-03-22T07:22:49.300" LastEditDate="2013-03-22T07:22:49.300" LastEditorUserId="858" OwnerUserId="858" ParentId="50170" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;If we have a null hypothesis and subject it to Turning Point test, rank, runs portmant. etc. If the p value fall below 0.05 in some cases and not for others, do we then say that we reject the hypothesis?&lt;/p&gt;&#10;&#10;&lt;p&gt;I.e. what can we say about the WN hypothesis if our series passes some tests but fails at others?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-22T07:18:44.343" Id="52977" LastActivityDate="2013-05-22T06:06:21.440" OwnerUserId="22301" PostTypeId="1" Score="0" Tags="&lt;hypothesis-testing&gt;" Title="Concerning WN, IID tests" ViewCount="45" />
  <row AnswerCount="0" Body="&lt;p&gt;The normal distribution of MLE is start ${x_1,...,x_n}$ random sample from $\sim\mathcal{N}(\mu,\sigma^2)$ &lt;/p&gt;&#10;&#10;&lt;p&gt;This family of distributions has two parameters: $\theta = (\mu, \sigma)$, so we maximize the likelihood over both parameters simultaneously using the logarithm of the likelihood. I know this. &lt;/p&gt;&#10;&#10;&lt;p&gt;But what if  ${x_1,...,x_n}$ is a random sample from the normal distribution $\sim\mathcal{N}(\mu, \sigma_i^2)$ ?&lt;/p&gt;&#10;&#10;&lt;p&gt;This means the variance is not homoscedastic. Than what is the MLE of $(\mu, \sigma_i^2) $?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-03-22T08:02:12.767" Id="52978" LastActivityDate="2013-03-22T23:18:18.997" LastEditDate="2013-03-22T23:18:18.997" LastEditorUserId="805" OwnerUserId="22375" PostTypeId="1" Score="0" Tags="&lt;mathematical-statistics&gt;&lt;heteroscedasticity&gt;" Title="What is the MLE if the variance is heteroskedastic" ViewCount="119" />
  
  <row Body="&lt;p&gt;Alexandr,&lt;/p&gt;&#10;&#10;&lt;p&gt;You need to consider different strategies when building the model and trying to identify outliers.  It is an iterative process.  Let me just put out some concepts that you need to consider.  Differencing, ARMA model structure, parameter changes in the model(ie CHOW test), changes in seasonality, changes in trend and TSAY's focus of outliers, level shifts and variance change.  Use WLS when you find a variance change.&lt;/p&gt;&#10;&#10;&lt;p&gt;We have programmed it.  It took us years to perfect as the TSAY work omitted other things as I have listed above.&lt;/p&gt;&#10;&#10;&lt;p&gt;You could use this free piece of software if you are looking just for level shifts.  It does a pretty good job. &lt;a href=&quot;http://www.beringclimate.noaa.gov/regimes/&quot; rel=&quot;nofollow&quot;&gt;http://www.beringclimate.noaa.gov/regimes/&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-22T11:33:33.183" Id="52986" LastActivityDate="2013-03-22T11:33:33.183" OwnerUserId="3411" ParentId="52982" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;This is a further question to my original question, where I did not get an helpful answer (at leas not helpful for me) :&lt;a href=&quot;http://stats.stackexchange.com/questions/52688/methods-of-moments-for-t-distribution&quot;&gt;Methods of moments for t distribution&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to fit a t distribution to my data (data can be found here: &lt;a href=&quot;http://uploadeasy.net/upload/bo9j6.rar&quot; rel=&quot;nofollow&quot;&gt;http://uploadeasy.net/upload/bo9j6.rar&lt;/a&gt;). The probability density function is given by:&#10;\begin{align*}&#10;f(l|\nu ,\mu ,\beta) = \frac{\Gamma (\frac{\nu+1}{2})}{\Gamma (\frac{\nu}{2}) \sqrt{\pi \nu} \beta} \left(1+\frac{1}{\nu}\left(\frac{l - \mu}{\beta}\right)^2 \right)^{-\frac{1+\nu}{2}}&#10;\end{align*}&lt;/p&gt;&#10;&#10;&lt;p&gt;First of all I use ML with the following code (&lt;a href=&quot;http://stats.stackexchange.com/questions/52609/fitting-t-distribtution-to-financial-data&quot;&gt;Fitting t distribtution to financial data&lt;/a&gt;):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# fit t distribution&#10;library(MASS)&#10;&#10;fitdistr(alvsloss, &quot;t&quot;)&#10;&#10;# or&#10;&#10;# log-likelihood function&#10;loglik &amp;lt;-function(par){&#10;if(par[2]&amp;gt;0 &amp;amp; par[3]&amp;gt;0) return(-sum(log(dt((alvsloss-par[1])/par[2],df=par[3])/par[2])))&#10;else return(Inf)&#10;}&#10;&#10;# optimisation step&#10;optim(c(0,0.1,2.5),loglik)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I get the following output:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;   m               s               df     &#10;  -0.0004919768    0.0130128873    2.6340459185 &#10; ( 0.0003182568) ( 0.0003453702) ( 0.1620424078)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;$par&#10;[1] -0.0004451138  0.0129659465  2.6182237477&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;which is more or less the same, I guess differences are due to the precision of the numerical procedures.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now I want to do this with using the method of moments, I use the following part of a paper (&lt;a href=&quot;http://ideas.repec.org/a/eme/jrfpps/v7y2006i3p292-300.html&quot; rel=&quot;nofollow&quot;&gt;http://ideas.repec.org/a/eme/jrfpps/v7y2006i3p292-300.html&lt;/a&gt;):&lt;/p&gt;&#10;&#10;&lt;p&gt;mean:&#10;\begin{align*}&#10;\mu=E(l)&#10;\end{align*}&#10;variance&#10;\begin{align*}&#10;\sigma^2 = V(l)= E((l-\mu)^2)=\frac{\beta \nu}{\nu-2} , \nu&amp;gt;2&#10;\end{align*}&#10;excess kurtosis&#10;\begin{align*}&#10;\kappa=\frac{6}{\nu-4} , \nu &amp;gt; 4&#10;\end{align*}&lt;/p&gt;&#10;&#10;&lt;p&gt;my first questions:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Why are they using the excess kurtosis and not the third moment the skewness?&lt;/li&gt;&#10;&lt;li&gt;What values do I have to insert, is the following correct?:&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;mean:&#10;\begin{align*}&#10;\mu=E(l)=\bar{l}&#10;\end{align*}&#10;variance&#10;\begin{align*}&#10;\sigma^2 = V(l)= E((l-\mu)^2)=\frac{\beta \nu}{\nu-2} = \frac{1}{n}\sum_{i=1}^n (l_i-\bar{l})^2, \nu&amp;gt;2&#10;\end{align*}&#10;excess kurtosis&#10;\begin{align*}&#10;\kappa=\frac{6}{\nu-4} = \frac{1}{n} \sum_{i=0}^n \left(\frac{l_i-\bar{l}}{s}\right)^4-3, \nu &amp;gt; 4&#10;\end{align*}&lt;/p&gt;&#10;&#10;&lt;p&gt;this gives:&#10;\begin{align*}&#10;\hat{\mu}_{MM}=\bar{l}\\&#10;\hat{\nu}_{MM} = \frac{6}{\left(\frac{1}{n} \sum_{i=1}^n \left(\frac{l_i-\bar{l}}{s}\right)^4-3\right)} + 4\\&#10;\hat{\beta}_{MM} =  \left(\frac{1}{n}\sum_{i=1}^n (l_i-\bar{l})^2\right) * \frac{(\hat{\nu}-2)}{\hat{\nu}}&#10;\end{align*}&lt;/p&gt;&#10;&#10;&lt;p&gt;so I am using the sample mean, sample variance and sample excess kurtosis. Is this correct? &lt;/p&gt;&#10;&#10;&lt;p&gt;And my main question: The output of ML tells me (the column df), that $\nu$&amp;lt;4 ($\nu$ is the number of degrees of freedom, df), but in MM I need $\nu$ to be greater than 4 or? So what does this mean? Is MM not usable? Or does it not matter?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-03-22T12:19:18.207" FavoriteCount="1" Id="52989" LastActivityDate="2013-03-22T17:21:58.577" LastEditDate="2013-03-22T12:45:04.000" LastEditorUserId="21998" OwnerUserId="21998" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;estimation&gt;&lt;maximum-likelihood&gt;&lt;kurtosis&gt;&lt;method-of-moments&gt;" Title="t distribution method of moments" ViewCount="299" />
  
  
  
  
  <row Body="&lt;p&gt;It's impossible to read minds, so to locate any error in thinking let's help you work through as simple an example as possible using the most basic possible definitions and axioms of probability.&lt;/p&gt;&#10;&#10;&lt;p&gt;Consider a sample space $\Omega = \{au, bu, cu, av, bv, cv\}$ (&quot;$au$&quot; etc. are just names of six abstract things) where all subsets are considered measurable.  Define a probability $\mathbb{P}$ on $\Omega$ in terms of its values on the atoms via&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\mathbb{P}(au) = \mathbb{P}(cu) = p,\ \mathbb{P}(bu) = r-2p;\quad \mathbb{P}(av) = \mathbb{P}(cv) = q,\ \mathbb{P}(bv) = 1-r-2q$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $p,q,r$ are any numbers for which all six probabilities are positive.  For instance, we may take $p=1/6$, $q=1/8$, and $r=1/2$.  Because the sum of the six given probabilities is unity, this defines a valid probability measure.&lt;/p&gt;&#10;&#10;&lt;p&gt;Define the random variables $U$ and $X$ as &lt;/p&gt;&#10;&#10;&lt;p&gt;$$U(\omega)=-1, 0, 1$$&lt;/p&gt;&#10;&#10;&lt;p&gt;depending on whether the initial letter in the name of $\omega$ is $a$, $b$, or $c$ respectively; and &lt;/p&gt;&#10;&#10;&lt;p&gt;$$X(\omega) = 0,1$$&lt;/p&gt;&#10;&#10;&lt;p&gt;depending on whether the final letter in the name of $\omega$ is $u$ or $v$ respectively.  &lt;/p&gt;&#10;&#10;&lt;p&gt;This can be neatly summarized in a $3$ by $2$ table of probabilities, headed by values of $U$ and $X$, whose interpretation I trust is evident:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\begin{array}{r|cc}&#10;  &amp;amp; \text{X=0} &amp;amp; \text{X=1} \\&#10;\hline&#10; \text{U=-1} &amp;amp; p &amp;amp; q \\&#10; \text{U=0} &amp;amp; r-2 p &amp;amp; 1-r-2 q \\&#10; \text{U=1} &amp;amp; p &amp;amp; q&#10;\end{array}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;It is then easy to compute the following:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;$\mathbb{P}(X=0) = \mathbb{P}(\{au,bu,cu\}) = p + r-2p + p = r$ (sum the left column in the table).&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;$\mathbb{P}(U=-1) = \mathbb{P}(\{au,av\}) = p+q$ (sum the top row in the table).&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Independence&lt;/em&gt; means nothing other than probabilities multiply.&lt;/strong&gt;  This would imply, among other things, that &lt;/p&gt;&#10;&#10;&lt;p&gt;$$p = \mathbb{P}(au) = \mathbb{P}(X=0, U=-1) = \mathbb{P}(X=0)\mathbb{P}(U=-1) = r(p+q)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;(investigate the top left entry in the table).  But this is rarely the case; for instance, $1/6 \ne (1/2)(1/6 + 1/8)$.  Therefore &lt;strong&gt;$X$ and $U$ are not independent&lt;/strong&gt; (except for some special possible combinations of $p$, $q$, and $r$). However,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\mathbb{E}(U) = \mathbb{P}(U=-1)(-1) +  \mathbb{P}(U=0)(0) + \mathbb{P}(U=1)(1) = 0$$&lt;/p&gt;&#10;&#10;&lt;p&gt;That is, the expectation of $U$ is zero. (This should be obvious from the symmetry: the chance that $U=1$ balances the chance that $U=-1$ regardless of the value of $X$.)  However,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\mathbb{E}(U|X=0) = \mathbb{P}(U=-1, X=0)(-1) + \cdots + \mathbb{P}(U=1, X=0)(1) = 0,$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and similarly $$\mathbb{E}(U|X=1) = 0.$$  That is, the conditional expectation of $U$--which is a &lt;em&gt;function&lt;/em&gt;--has the constant value zero.  In symbols, we have found that &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\mathbb{E}(U|X) = 0 = \mathbb{E}(U)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and we have a simple, explicit example showing why &lt;strong&gt;constant conditional expectation does not imply independence.&lt;/strong&gt;&lt;/p&gt;&#10;" CommentCount="14" CreationDate="2013-03-22T16:42:55.033" Id="53012" LastActivityDate="2013-03-22T16:48:09.633" LastEditDate="2013-03-22T16:48:09.633" LastEditorUserId="919" OwnerUserId="919" ParentId="53009" PostTypeId="2" Score="5" />
  <row AcceptedAnswerId="60982" AnswerCount="2" Body="&lt;p&gt;I am looking at some data for the risk of mortality in patients undergoing treatment A vs treatment B and I am given the total number of patients in each treatment arm and the relative risk + confidence intervals of mortality. How would I go about finding the actual number of deaths in each treatment arm?&lt;/p&gt;&#10;&#10;&lt;p&gt;Here are the hard numbers:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Study 1&#10;Adjusted total # of patients for treatment A: 8366&#10;Adjusted total # of patients for treatment B: 10251&#10;Adjusted RR for mortality: 0.72 (0.66-0.78)&#10;&#10;Study 1&#10;Adjusted total # of patients for treatment A: 23113&#10;Adjusted total # of patients for treatment B: 26819&#10;Adjusted RR for mortality: 0.78 (0.72-0.83)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So I would like to compute the actual number of patients who died in each treatment arm for both studies 1 and 2.&lt;/p&gt;&#10;&#10;&lt;p&gt;Not sure if this helps but the paper states that:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;adjusted survival curves were estimated with the use of the&#10;  inverse-probability-weighting approach of Cole and Hernan. For each&#10;  treatment group, the survival curves adjusted with the use of inverse&#10;  probability weighting represent the expected rate of survival if &#10;  the  treatment of interest were applied to all study patients. Using&#10;  estimated rates of survival among patients undergoing A and among&#10;  those undergoing B, we calculated risk ratios at specific time&#10;  points and used bootstrap methods to obtain 95% confidence intervals.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I had a look at the Wikipedia article for RR (&lt;a href=&quot;http://en.wikipedia.org/wiki/Relative_risk&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Relative_risk&lt;/a&gt;) but its not the most helpful.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-03-22T17:01:30.233" FavoriteCount="1" Id="53014" LastActivityDate="2013-06-05T17:12:49.647" LastEditDate="2013-03-22T17:12:31.897" LastEditorUserId="12347" OwnerUserId="12347" PostTypeId="1" Score="2" Tags="&lt;statistical-significance&gt;&lt;confidence-interval&gt;&lt;standard-error&gt;&lt;epidemiology&gt;&lt;relative-risk&gt;" Title="Computing event rates given RR + CI and total sample size in each treatment group" ViewCount="126" />
  <row Body="&lt;p&gt;In general, you will do it the way you did in this case: you will compute the posterior using Bayes's Theorem. Unfortunately, in general, it won't be the case that the posterior has a nice identifiable simple form (your example is the nicest possible: the case of a &lt;a href=&quot;http://en.wikipedia.org/wiki/Conjugate_prior&quot; rel=&quot;nofollow&quot;&gt;conjugate prior&lt;/a&gt;). In general, you will have to use some Monte Carlo solution. The &lt;a href=&quot;http://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm&quot; rel=&quot;nofollow&quot;&gt;Metropolis-Hastings&lt;/a&gt; algorithm allows you to explore any posterior, in principle, but there are limitations: simple versions of the algorithm, such as Random Walk Metropolis, may not perform well if you have a multivariate posterior which exhibits extremely high correlations between its components. When you can sample from the full conditionals, Giggs Sampling (which can be seen as a particular case of Metropolis-Hastings) is probably the way to go. Check &lt;a href=&quot;http://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo&quot; rel=&quot;nofollow&quot;&gt;this&lt;/a&gt; for more information. Also, it may be the case that your problem needs an approximation technique such as &lt;a href=&quot;http://en.wikipedia.org/wiki/Approximate_Bayesian_computation&quot; rel=&quot;nofollow&quot;&gt;ABC&lt;/a&gt;, which is a hot topic nowadays.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-22T20:04:52.453" Id="53027" LastActivityDate="2013-03-22T22:06:46.367" LastEditDate="2013-03-22T22:06:46.367" LastEditorUserId="9394" OwnerUserId="9394" ParentId="53006" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;I refer here to a classical linear regression whose true representation is given by the equation: $y_i=x_i'\beta+u_i$,&lt;/p&gt;&#10;&#10;&lt;p&gt;where as usual $x_i$ is a $K\times 1$ vector of independent explanatory variables, $\beta$ is a  $K\times 1$ vector of parameters and $u_i$ the error term of the ith observation and by construction of the true representation $u_i$ is considered to be independent of $x_i$.&lt;/p&gt;&#10;&#10;&lt;p&gt;My first question on that is basic: If we assume independence between $x_i$ and $u_i$ does this imply $E[u_i^d|x_i]=E[u_i^d]$ for $d=1,2,..$ - so to speak, can we generalize the Law of iterated expectations consequently to all moments?&lt;/p&gt;&#10;&#10;&lt;p&gt;Secondly, what is the basis of many textbooks to claim that $x_i$ and $u_i$ are independent in the classical regression model? By independence, I don't mean uncorrelatedness, rather $f(u,x)=f(u)f(x)$. I am aware that the two key standard assumptions on which this small sample properties model is based on is strict exogenity $E[u_i|X]=0$, and normally distributed error terms $u_i|x_i \thicksim N(0, \sigma^2)$. Where does the claim of the independence between $x_i$ and $u_i$ originates from in textbooks? Is it implied by these two statements jointly together or is this a separate assumption? If the latter, why is it not mentioned exclusively in the set of assumptions for classical linear regression models which only assumes strict exogenity and normally distributed errors?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-03-22T20:10:38.770" Id="53028" LastActivityDate="2014-02-18T17:37:15.003" LastEditDate="2013-03-22T22:08:27.843" LastEditorUserId="22356" OwnerUserId="22356" PostTypeId="1" Score="1" Tags="&lt;regression&gt;" Title="Independence assumptions in the classical regression model and higher moments" ViewCount="204" />
  
  
  <row Body="&lt;p&gt;An &lt;em&gt;event&lt;/em&gt; is a measurable subset of a probability space. In order to say whether a given subset is measurable, one must have identified the sigma-algebra of measurable subsets with which one is working. &lt;/p&gt;&#10;&#10;&lt;p&gt;It is common in mathematical analysis to turn the real numbers into a measurable space by associating $\mathbb{R}$ with the Borel sigma-algebra -- the sigma-algebra generated by the open intervals in $\mathbb{R}$. In this standard case, the subset $(-\infty,t)$ is Borel-measurable for all $t$, per the argument of @Zen. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, while the Borel sigma-algebra is canonical, it is not obligatory. As a counter-example, one could turn $\mathbb{R}$ into a measurable space by equipping this set with the sigma-algebra consisting only of $\mathbb{R}$ and the empty set. In this alternative case, $(-\infty,t)$ is not a measurable subset of $\mathbb{R}$. &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-03-23T05:37:33.820" Id="53060" LastActivityDate="2013-03-24T19:23:06.837" LastEditDate="2013-03-24T19:23:06.837" LastEditorUserId="17286" OwnerUserId="17286" ParentId="53047" PostTypeId="2" Score="1" />
  
  
  
  
  <row AcceptedAnswerId="60599" AnswerCount="1" Body="&lt;p&gt;While I am using SVM, I train it with a train data and then I try to predict a sample if its label is -1 or +1. However, I see some confusion matrice for SVM like below. Mine are 2x2 matrice but their dimesions are larger, e.g. 15x15. Do they have more than one SVM? How people do such things? It is easy to write one diagonal but how they decide other values? In the figure below, for 2, it is easy to write 38 but how they write 2?&lt;/p&gt;&#10;&#10;&lt;p&gt;BTW, I am new to these topics.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/zsZn9.png&quot; alt=&quot;15x15 confusion matrix&quot;&gt;&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-03-23T19:09:38.567" FavoriteCount="1" Id="53093" LastActivityDate="2013-06-01T22:14:11.863" LastEditDate="2013-06-01T22:14:11.863" LastEditorUserId="17010" OwnerUserId="17010" PostTypeId="1" Score="2" Tags="&lt;classification&gt;&lt;svm&gt;" Title="SVM confusion matrix whose dimensions are more than two" ViewCount="736" />
  
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;$\ldots$ by the memoryless property the distribution of $X|X &amp;gt; x$ is the same as that of $X$ but shifted to the right by $x$.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Let $f_X(t)$ denote the probability density function (pdf) of $X$.  Then, the mathematical&#10;formulation for what you correctly&#10;state $-$ namely, &#10;the &lt;em&gt;conditional&lt;/em&gt; pdf of $X$ given that $\{X &amp;gt; x\}$ is the same as that of&#10;$X$ but shifted to the right by $x$ $-$ is that $f_{X \mid X &amp;gt; x}(t) = f_X(t-x)$.&#10;Hence, $E[X\mid X &amp;gt; x]$, the &lt;em&gt;expected value&lt;/em&gt; of $X$ given that $\{X &amp;gt; x\}$ is&#10;$$\begin{align}&#10;E[X\mid X &amp;gt; x] &amp;amp;= \int_{-\infty}^\infty t f_{X \mid X &amp;gt; x}(t)\,\mathrm dt\\&#10;&amp;amp;= \int_{-\infty}^\infty t f_X(t-x)\,\mathrm dt\\&#10;&amp;amp;= \int_{-\infty}^\infty (x+u) f_X(u)\,\mathrm du&#10;&amp;amp;\scriptstyle{\text{on substituting}~u = t-x}\\&#10;&amp;amp;= x + E[X].&#10;\end{align}$$&#10;Note that we have not explicitly used the density of $X$ in the calculation,&#10;and don't even need to integrate &lt;em&gt;explicitly&lt;/em&gt; if we simply remember that&#10;the area under a pdf is $1$ and also the definition of epected value.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-03-24T00:24:41.527" Id="53113" LastActivityDate="2013-03-24T02:41:02.307" LastEditDate="2013-03-24T02:41:02.307" LastEditorUserId="6633" OwnerUserId="6633" ParentId="48496" PostTypeId="2" Score="5" />
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I would like to use AMOS for SEM. According to the literature, either (1) worry is correlated with anxiety,  or (2) worry is characterized by anxiety. &lt;/p&gt;&#10;&#10;&lt;p&gt;Would you help me as to how I can draw one variable or the other, or can I not draw (1) or (2)?&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2013-03-24T06:06:55.963" Id="53124" LastActivityDate="2013-03-24T09:51:13.060" LastEditDate="2013-03-24T09:51:13.060" LastEditorUserId="3826" OwnerUserId="22439" PostTypeId="1" Score="0" Tags="&lt;sem&gt;&lt;amos&gt;" Title="When two variables are correlated, how can I draw two variables for SEM using AMOS?" ViewCount="90" />
  
  
  
  
  <row AnswerCount="2" Body="&lt;p&gt;If a time series has several runs of missing data is it best to impute the mean of the previous observations?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-03-24T15:57:43.440" Id="53146" LastActivityDate="2014-06-06T17:01:05.637" LastEditDate="2013-09-21T01:01:23.520" LastEditorUserId="24808" OwnerUserId="20884" PostTypeId="1" Score="2" Tags="&lt;time-series&gt;&lt;missing-data&gt;" Title="What to do for missing data in time series" ViewCount="826" />
  
  <row Body="&lt;p&gt;Is this a homework problem? I want to be helpful while not giving away the farm.&lt;/p&gt;&#10;&#10;&lt;p&gt;In this scenario, a &quot;1&quot; is a success (denoted $k$), and anything else is a failure. Assuming the dice are fair, probability of a success is 1/6 for six-sided dice, and your size is how many dice you're rolling (how many trials you're doing). I'm sure you've made it this far.&lt;/p&gt;&#10;&#10;&lt;p&gt;The key part of this is &quot;at least one.&quot; Using the 2d6 problem as an example, $\Pr(k=1|n=2, p=1/6)={2 \choose 1}(1/6)^1 (1-1/6)^{2-1}$=dbinom(1, size=2, prob=1/6). But this only tells us the probability of rolling a single 1. We also want to know the probability of 2 successes. This is given by $\Pr(k=2)$=dbinom(2, size=2, prob=1/6). Add the two scenarios to get the &quot;1 or more&quot; probability.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt; In the comments below: Tim points out that you can produce this result using pbinom(), the binomial cumulative distribution function, if you simply exclude the probability mass for zero successes.&lt;/p&gt;&#10;&#10;&lt;p&gt;The 3d6 scenario is just an extension of the 2d6 scenario. The final scenario is more complicated, because the value of $p$ is not the same for each trial. I'm not sure that I want to answer that unless I know if it's a homework problem or not.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-03-24T19:28:05.530" Id="53156" LastActivityDate="2013-03-25T16:34:42.040" LastEditDate="2013-03-25T16:34:42.040" LastEditorUserId="22311" OwnerUserId="22311" ParentId="53154" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;The most common tests for comparing means or medians of paired observations are (in no particular order):&lt;/p&gt;&#10;&#10;&lt;p&gt;i) paired t-test&lt;/p&gt;&#10;&#10;&lt;p&gt;ii) Wilcoxon signed rank test&lt;/p&gt;&#10;&#10;&lt;p&gt;iii) sign test&lt;/p&gt;&#10;&#10;&lt;p&gt;For multiple related observations, you &lt;em&gt;could&lt;/em&gt; treat the factor that relates the observations as a block and hence use ANOVA for a randomized complete block design in place of (i) and a Friedman test in place of (ii), though such things might be treated as repeated measures, depending on circumstances.&lt;/p&gt;&#10;&#10;&lt;p&gt;Regarding the paper, the situation is somewhat different from that title question in a couple of respects.&lt;/p&gt;&#10;&#10;&lt;p&gt;Not least, there are many tests!&lt;/p&gt;&#10;&#10;&lt;p&gt;Further, there are a number of items than are given numerical ratings and an importance ranking.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the analysis in the paper, they seem mostly to have used paired t-tests to compare the importance ranks of individual items. However those rankings can't independent across items (if I rate one item higher, other items must rank lower), meaning that the tests aren't independent. Indeed, the ratings wouldn't be independent either. (This of itself doesn't invalidate the individual tests, but they should probably be analyzed in a way that considers the multivariate structure of the problem.)&lt;/p&gt;&#10;&#10;&lt;p&gt;The main question would appear to be 'is a paired t-test suitable for this situation'?&lt;/p&gt;&#10;&#10;&lt;p&gt;Multiple testing aside, and ignoring the multivariate structure (with likely loss of power):&lt;/p&gt;&#10;&#10;&lt;p&gt;The underlying distributional requirement would be that the pair-differences are approximately normally distributed (with constant variance) under the null. We'd also require independence across subjects.&lt;/p&gt;&#10;&#10;&lt;p&gt;--&lt;/p&gt;&#10;&#10;&lt;p&gt;One final comment - they appear on first glance to be asserting 'no difference' on the basis of failure to achieve significance from a hypothesis test. The specific words are 'did not change'. Unless I missed something from my first quick skim, this is not a reasonable conclusion on the basis of the analysis.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-03-24T21:40:53.140" Id="53172" LastActivityDate="2014-10-17T21:43:39.257" LastEditDate="2014-10-17T21:43:39.257" LastEditorUserId="805" OwnerUserId="805" ParentId="53153" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;This looks like a variation on &quot;Principal Component Analysis&quot;.&#10;&lt;a href=&quot;http://mathworld.wolfram.com/PrincipalComponentAnalysis.html&quot; rel=&quot;nofollow&quot;&gt;http://mathworld.wolfram.com/PrincipalComponentAnalysis.html&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In structural analysis the eigenvalues of a system are used to look at linear deformations, places where superposition is still valid.  The method is called &quot;Modal Analysis&quot;.&#10;&lt;a href=&quot;http://macl.caeds.eng.uml.edu/macl-pa/modes/modal2.html&quot; rel=&quot;nofollow&quot;&gt;http://macl.caeds.eng.uml.edu/macl-pa/modes/modal2.html&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-24T23:07:53.923" Id="53181" LastActivityDate="2013-03-24T23:07:53.923" OwnerUserId="22452" ParentId="14229" PostTypeId="2" Score="1" />
  <row Body="&lt;h3&gt;Restatement of the question&lt;/h3&gt;&#10;&#10;&lt;p&gt;Within the set $\{1,2,\ldots, 370\}$ there are $37$ copies of each terminal digit, information we can represent as a vector $x = (x_0, x_1, \ldots, x_9) = (37, 37, \ldots, 37)$ whose subscripts denote the digits and whose values give their counts.  Consider sampling $k = 64$ of those numbers &lt;em&gt;without replacement&lt;/em&gt;, as suggested in the question.&lt;/p&gt;&#10;&#10;&lt;p&gt;Evidently $20$% of the time is $20$% of $64$, or $12.8$, lying between $12$ and $13$.  I will take the question to ask for the chance that &lt;em&gt;at least one&lt;/em&gt; of the nine digits occurs &lt;em&gt;$13$ or more times&lt;/em&gt; in the set of $k$ values.  To answer this, it's more convenient to compute the complementary chance that &lt;em&gt;no digit appears more than $m=12$ times.&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;h3&gt;Analysis&lt;/h3&gt;&#10;&#10;&lt;p&gt;The zero can appear anywhere from $0$ through $12$ times.  The chance that it appears exactly $i$ times is the chance that $i$ of the $x_0=37$ zeros were drawn and, independently, that the remaining $k-i = 64-i$ values drawn were among the $370 - 37 = 333$ non-zeros.  Because all $\binom{370}{64}$ draws are equally likely, this chance equals&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac{\binom{37}{i}\binom{370-37}{64-i}}{\binom{370}{64}}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;This must be multiplied by the chance that, when $64-i$ numbers are drawn from the $333$ ones, twos, ..., and nines, none appears more than $12$ times.  Summing these chances gives a formula.  To write it down, all we need do is give a name to the chances we have been discussing.  To this end,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$p(m, k, x)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;be the chance that when $k$ numbers are drawn (randomly and without replacement) from a set of values as counted by the entries in a vector $x$, none of those numbers will appear more than $m$ times in the sample.  We have seen that&lt;/p&gt;&#10;&#10;&lt;p&gt;$$p(m, k, x) = \sum_{i=0}^{\min(x_0, m, k)}\frac{\binom{x_0}{i}\binom{n-x_0}{k-i}}{\binom{n}{k}}p(m, k-i, (x_1, x_2, \ldots, x_s))$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $x = (x_0, x_1, \ldots, x_s)$ ($s+1 = 10$ is the number of distinct possible digits) and $n = x_0 + x_1 + \cdots + x_s = 370$ is the total number of digits to draw from.&lt;/p&gt;&#10;&#10;&lt;h3&gt;Computation&lt;/h3&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;An efficient solution is obtained with a dynamic program&lt;/strong&gt;.  It goes through at most $(s+1)(m+1)k$ steps and so can be very fast even for large problems.  (A dynamic program effectively performs a recursive calculation, storing intermediate results along the way to avoid recalculating them whenever they might be needed again.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is a &lt;em&gt;Mathematica&lt;/em&gt; implementation:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;p[m_, k_, x_] := &#10;  p[m, k, x] =  Module[{n = Total[x], x0 = First[x], y = Rest[x]},&#10;    Sum[(Binomial[x0, i] Binomial[n - x0, k - i]/Binomial[n, k]) p[m, k - i, y],&#10;     {i, 0, Min[x0, m, k]}]];&#10;p[m_, k_, {}] := p[m, k, {}] = 1;&#10;p[m_, k_, x_] /; Total[x] &amp;lt; k := 0&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;h3&gt;Example&lt;/h3&gt;&#10;&#10;&lt;p&gt;The answer to the question is obtained by the command&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;p[12, 64, ConstantArray[37, 10]]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;with the resulting output (after less than $0.1$ second)&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;$\frac{738854500118501526079209862453421360432359222031865764118385747121584}{773750633881045460211214653703944063084540878160228087585637765752449}$&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;(I have written it out in full to demonstrate that it's unlikely we will find any simple closed formula for the answer.)  This is approximately $95.49000$%, indicating &lt;strong&gt;the chance of observing $13$ or more of some digit is approximately $4.51000$%&lt;/strong&gt;. That answers the question.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the course of this calculation, we will have obtained values of $p(m,k,x)$ for all smaller values of $k$ and all suffixes of $x$, which are conveniently arranged in a two-way table.  I use rows for $x$, running from $(x_9)$ to $(x_8,x_9)$ down to the original $(x_0,x_1,\ldots, x_9)$, and columns for $k$ running from $12$ (the smallest value for which any conceivable calculation is needed) through $64$.  Here is a plot of the logarithms of $1-p(m,k,x)$ in such an arrangement, with lighter values corresponding to the largest logs:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/QTiEP.png&quot; alt=&quot;Array plot&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;A clearer picture can be obtained by using a third dimension and restricting the heights to the largest values (corresponding to all probabilities of $0.0001$ or greater):&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/h4LXB.png&quot; alt=&quot;3D plot&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;These are base-$10$ logs.  The surface has been reflected so that its front boundary corresponds to $s=10$ (all $10$ digits) and the leftmost point on that boundary corresponds to our answer (the common log of $0.0451$ is about $-1.3$).  Values along the frontmost row of this surface were computed in terms of values along the next row (where only nine digits have to be considered), which in turn were computed in terms of values along the row behind it (eight digits), and so on until all digits were used up.&lt;/p&gt;&#10;&#10;&lt;h3&gt;Checking&lt;/h3&gt;&#10;&#10;&lt;p&gt;A small simulation would help verify whether the preceding analysis and implementation are correct.  Let's repeat the experiment a million times and tally the results:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x = Mod[Range[370], 10] (* All 370 last digits *);&#10;sim = Table[(Tally@RandomSample[x, 64])[[All, 2]] // Max, {i, 10^6}]; &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This result--which can be expected to be accurate only to about $3.5$ decimal places (as we will see below)--takes considerably longer than computing the exact result ($6.5$ seconds &lt;em&gt;vs&lt;/em&gt; $0.08$ seconds), but gives more information.  For instance, it estimates the sampling distribution of the maximum digit count:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/fdA3F.png&quot; alt=&quot;Histogram&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;For instance, it is now evident that observing $16$ or more of any digit in this experiment is quite a rare event and that generally we should expect to see the maximum count range from $8$ through $13$ or so.  (In order to compute this distribution exactly, we would have to compute $p(m,k,x)$ separately for $m=0$ through $37 = \max(x)$; there is no overlap among the intermediate results.  That's a $5$ second calculation, taking about the same amount of time as this simulation.)&lt;/p&gt;&#10;&#10;&lt;p&gt;The proportion of trials with counts of $13$ or greater is $95.4785$%.  The standard error of this estimate is $\sqrt{0.954785(1-0.954785)/10^6}\approx 0.000208$.  The difference between this proportion and the probability computed by $p(m,k,x)$ is only $0.55$ standard errors, indicating that &lt;em&gt;the difference can be attributed to chance&lt;/em&gt;: this is evidence that the &lt;em&gt;Mathematica&lt;/em&gt; implementation of $p(m,k,x)$ is correct for these arguments.&lt;/p&gt;&#10;&#10;&lt;h3&gt;A simpler version of the problem&lt;/h3&gt;&#10;&#10;&lt;p&gt;The corresponding chances when sampling &lt;em&gt;with&lt;/em&gt; replacement are more easily computed. However, as another test of the &lt;em&gt;Mathematica&lt;/em&gt; implementation, let's estimate those chances by making the population larger and larger:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Table[p[12, 64, ConstantArray[10^i, 10]] // N, {i, 2, 9}]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Instead of using just $37$ of each final digit, this calculation uses $100$, then $1000$, ..., and finally $10^9$ of each final digit.  The output is&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;$\{0.923252,0.904265,0.902298,0.9021,0.902081,0.902079,0.902078,0.902078\}$&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Evidently a limiting value of $90.2078$% is being approached.  Its complement, $9.792\ldots$%, is the answer we should get in an analysis that assumes sampling with replacement.&lt;/p&gt;&#10;&#10;&lt;p&gt;Because the answer differs quite a bit from the previous one--it is twice as great--this is a situation where it would be important to be clear about whether the sampling is with or without replacement.  Intuitively, $13$ is a sufficiently large fraction of the $37$ copies of each digit available that by the time $12$ copies have been sampled, it is relatively more difficult to get the $13$th into the sample, thereby lowering the probability that $13$ or more copies will be observed (of any digit).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-24T23:38:57.653" Id="53185" LastActivityDate="2013-03-25T15:57:10.413" LastEditDate="2013-03-25T15:57:10.413" LastEditorUserId="919" OwnerUserId="919" ParentId="53140" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;Consider variations on the GINI score.  &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Gini_coefficient&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Gini_coefficient&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://mathworld.wolfram.com/GiniCoefficient.html&quot; rel=&quot;nofollow&quot;&gt;http://mathworld.wolfram.com/GiniCoefficient.html&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;It is normalized, and its output ranges from 0 to 1.  &lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT:&lt;/p&gt;&#10;&#10;&lt;p&gt;Why GINI is &quot;cool&quot; or at least potentially appropriate:&lt;/p&gt;&#10;&#10;&lt;p&gt;It is a measure of inequality or inequity.  It is used as a scale free measure to characterize the heterogeneity of scale-free networks, including infinite and random networks.  It is useful in building CART trees because it is the measure of splitting power of a particular data-split.&lt;/p&gt;&#10;&#10;&lt;p&gt;Because of its range:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;there is less roundoff errors.  Ranges far away from 1.0 tend to suffer numeric issues.&lt;/li&gt;&#10;&lt;li&gt;it is human readable, and more human accessible.  Humans have a more concrete grasp of ones of objects than they do of billions.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Because it is normalized:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;comparisons of scores are meaningful, a 0.9 in one country means the same level of relative non-uniformity as a 0.9 in any other country.&lt;/li&gt;&#10;&lt;li&gt;It is normalized against the Lorenz curve for perfect uniformity therefore the values are relevant indicators of the relationship of the distribution of values of interest to the Lorenz curve.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;References:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;[1] &lt;a href=&quot;http://data.worldbank.org/indicator/SI.POV.GINI&quot; rel=&quot;nofollow&quot;&gt;http://data.worldbank.org/indicator/SI.POV.GINI&lt;/a&gt; &lt;/li&gt;&#10;&lt;li&gt;[2] &lt;a href=&quot;http://research3.bus.wisc.edu/file.php/129/Papers/Gini27April2011.pdf&quot; rel=&quot;nofollow&quot;&gt;http://research3.bus.wisc.edu/file.php/129/Papers/Gini27April2011.pdf&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;[3] &lt;a href=&quot;http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#giniimp&quot; rel=&quot;nofollow&quot;&gt;http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#giniimp&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;[4] &lt;a href=&quot;http://www2.unine.ch/files/content/sites/imi/files/shared/documents/papers/Gini_index_fulltext.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www2.unine.ch/files/content/sites/imi/files/shared/documents/papers/Gini_index_fulltext.pdf&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="1" CreationDate="2013-03-24T23:49:45.900" Id="53187" LastActivityDate="2013-04-29T22:53:41.470" LastEditDate="2013-04-29T22:53:41.470" LastEditorUserId="22452" OwnerUserId="22452" ParentId="44320" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;I have a question about how to interpret some discrete data. Some are very easy, like the number of children in a household: that is a discrete and numerical, measured as a ratio variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;Other variables are not that easy, for example&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;age, measured in whole years: is that interval or ratio?&lt;/li&gt;&#10;&lt;li&gt;number of cigarettes smoked per day: is that interval, or ratio?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;The difference between interval and ratio is that ratio has an absolute zero (when the measured property is absent). In case of the age measured in whole years, an infant aged 2 months has zero (whole) years, but that does not mean the person doesn't have an age... so is that ratio?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-03-25T01:06:08.393" FavoriteCount="1" Id="53191" LastActivityDate="2013-04-03T16:01:00.890" OwnerUserId="22469" PostTypeId="1" Score="2" Tags="&lt;measurement&gt;" Title="interval or ratio?" ViewCount="1253" />
  
  
  <row Body="&lt;p&gt;How I would approach this problem:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;I would start with the data, and then for each point, count the number of neighbors within some range. &lt;/li&gt;&#10;&lt;li&gt;After doing this for both of the distributions, I would look at how the neighbor-count compares at the same points.  I might plot an empirical CDF of the differences in counts. &lt;/li&gt;&#10;&lt;li&gt;I might have to adjust the &quot;range&quot; within which to look some in order to determine whether or not this approach had valuable results.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="1" CreationDate="2013-03-25T05:13:35.690" Id="53209" LastActivityDate="2013-03-25T05:13:35.690" OwnerUserId="22452" ParentId="28870" PostTypeId="2" Score="0" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm studying computer science but am lost with statistical methods. An algorithm I've been studying needs a formal metric on the space of our data set.&lt;/p&gt;&#10;&#10;&lt;p&gt;Our data set can be described as `voting data'. That is, each data point is a vector containing +1 or -1 as its entries, representing a voter's  choices for/against a decision. (We still have to decide how for cases where a voter does not vote, maybe just put a zero on that specific entry.)&lt;/p&gt;&#10;&#10;&lt;p&gt;What kind of distance should I impose on my data for me to obtain a finite metric space on our data set? Using Euclidean distance might be overkill as our vectors do not contain too much geometric information. On the other hand, cosine similarity may not give me a finite metric space. Would the correlation metric (like the one used by &lt;em&gt;pdist&lt;/em&gt; in Matlab) suffice?&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2013-03-25T08:54:27.113" Id="53222" LastActivityDate="2013-03-25T08:54:27.113" OwnerUserId="22482" PostTypeId="1" Score="0" Tags="&lt;clustering&gt;&lt;metric&gt;" Title="metric on voting data" ViewCount="54" />
  <row AnswerCount="0" Body="&lt;p&gt;I am analyzing an environmental data set containing one response variable (R) and many explanatory variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;The explanatory variables are either factors (F1, F2, F3) or continuous variables (V1, V2, V3…).&lt;/p&gt;&#10;&#10;&lt;p&gt;I wanted to conduct a linear regression in the form lm (R~F1+F2+F3+V1+V2+V3+F1*F2+F2*F3…)&lt;/p&gt;&#10;&#10;&lt;p&gt;The matrix scatterplot did not show any collinearity issue between my explanatory variable, but from previous analysis (ANOVA) I know that the factors are significantly explaining variation in the continuous variables.&#10;(i.e. aov (V1~(F1+F2+F3)^3), all factors significant;&#10;(V2~(F1+F2+F3)^3), all factors significant….)&lt;/p&gt;&#10;&#10;&lt;p&gt;The vif in the linear regression model for each explanatory variable is &amp;lt;3.&lt;/p&gt;&#10;&#10;&lt;p&gt;So my questions are:&lt;/p&gt;&#10;&#10;&lt;p&gt;1) If an ANOVA analysis points out a relationship between subsets of explanatory variables are these to be considered collinear?&lt;/p&gt;&#10;&#10;&lt;p&gt;2) Should then I remove all the continuous variables from the analysis?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-03-25T12:56:10.700" Id="53231" LastActivityDate="2013-03-25T12:56:10.700" OwnerUserId="21238" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;anova&gt;&lt;multicollinearity&gt;" Title="Multicollinearity: is a matrix scatterplot enough to rule it out?" ViewCount="279" />
  <row Body="&lt;p&gt;There are $n_s = 3$ screws and $n_o = 5$ orientations. The probability for each orientation is $p_o = 1/n_o = 1/5$.&lt;/p&gt;&#10;&#10;&lt;p&gt;If the first screw is aligned, it can have any orientation. But the two other screws have to match the orientation of the first. Hence, the probability that the screws are all lined up is $$p_o^{n_s - 1} = \left(\frac{1}{5}\right)^2 = 1/25 = 0.04\text{.}$$&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-03-25T13:42:28.523" Id="53237" LastActivityDate="2013-03-25T14:09:51.700" LastEditDate="2013-03-25T14:09:51.700" LastEditorUserId="13680" OwnerUserId="13680" ParentId="53233" PostTypeId="2" Score="2" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I've performed an non linear regression with 3 variables and 5 parameters, using Wolfram Mathematica.&#10;Now I want to detect the outliers that are far from 2*(standard deviation) of my function.&#10;I've done a little software that investigate the outliers.&#10;But now I'm wondering if this technique has got a statistical basis.&#10;Can you help me to find this basis or some bibliography or if I can perform a non linear regression with mathematica bearing in mind the outliers?&#10;If you want I can post an example of my funcion.&#10;Thanks in advance.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-03-25T16:18:34.327" Id="53243" LastActivityDate="2013-03-26T14:10:16.773" OwnerUserId="22496" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;outliers&gt;&lt;nonlinear-regression&gt;" Title="Select outliers with standard deviation after a nonlinear regression" ViewCount="159" />
  
  <row Body="&lt;p&gt;You can check your answer with the user written function &lt;a href=&quot;http://ideas.repec.org/c/boc/bocode/s456703.html&quot; rel=&quot;nofollow&quot;&gt;mahapick&lt;/a&gt; in &lt;code&gt;stata.&lt;/code&gt; &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-03-25T16:50:00.663" Id="53251" LastActivityDate="2013-03-25T16:50:00.663" OwnerUserId="22500" ParentId="53102" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="53335" AnswerCount="1" Body="&lt;p&gt;Assume one variable $x$ has two states 0 and 1, $x$ changes between 0 and 1 following a continuous time Markov chain. The transition probability is represented as matrix $P$ and the time sojourning on a state follows exponential distribution with arrival ratio $\lambda$.&lt;/p&gt;&#10;&#10;&lt;p&gt;There is the other variable y which is independent with $x$. $y$ also changes between 0 and 1, and have the same $P$ and $\lambda$ with $x$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now the question is: will the new variable $z=x*y$ follow continuous time Markov chain? if yes, what will be the transition probability and $\lambda$. ($z$ only has two states: 0 and 1.)&lt;/p&gt;&#10;&#10;&lt;p&gt;I guess $z$ still follows continuous time Markov chain, but I have no idea how to prove? (maybe there has been a property stating this, which I don't know?) Could anyone give some ideas? Many thanks!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-03-25T21:27:34.137" FavoriteCount="1" Id="53274" LastActivityDate="2013-03-26T15:05:47.773" LastEditDate="2013-03-26T10:08:37.313" LastEditorUserId="22275" OwnerUserId="22275" PostTypeId="1" Score="3" Tags="&lt;markov-process&gt;" Title="the combination of two independent continuous time Markov chains" ViewCount="341" />
  
  
  
  
  
  
  
  <row Body="&lt;p&gt;If you can calculate the quantiles of a standardised distribution $F(\cdot;\theta)$, where $\theta \in \Theta$ is a shape parameter, then the quantile function of the corresponding distribution with location and scale parameters $(\mu,\sigma)$ is simply&lt;/p&gt;&#10;&#10;&lt;p&gt;$$Q(p;\mu,\sigma,\theta)=\mu+\sigma F^{-1}(p;\theta).$$&lt;/p&gt;&#10;&#10;&lt;p&gt;In R, this can implemented for the Student-$t$ as follows&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;qnst &amp;lt;- function(p,mu,sigma,nu) return(mu + sigma*qt(p,df=nu))&#10;&#10;# Quantile 0.25, mu = 10, sigma = 1, nu = 2&#10;qnst(0.25,10,1,2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2013-03-26T17:21:46.433" Id="53353" LastActivityDate="2013-03-26T17:27:02.367" LastEditDate="2013-03-26T17:27:02.367" LastEditorUserId="22547" OwnerUserId="22547" ParentId="53349" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;I want to use logistic regression to look at how a continuous variable that I have measured for a sample of participants (how long they looked at a product) is impacting a dichotomous variable (whether they bought it or not, Y or N). There are several products I am interested looking at, which means I will have to run separate LR's for each product, I believe. &lt;/p&gt;&#10;&#10;&lt;p&gt;I am mainly interested in comparing the relative impact that the IV has on purchases of each product. I thought that the most intuitive way to do this would be using the Odds Ratios. I have two questions about this.&lt;/p&gt;&#10;&#10;&lt;p&gt;1) Is it appropriate to compare the odds ratios between the models?&lt;/p&gt;&#10;&#10;&lt;p&gt;2) Will the model's predictive ability/performance be important to interpret if I am only interested in the odds ratios? What does a poorly performing model mean for the interpretation of the odds ratio?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for any help, it is very much appreciated.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-03-26T17:40:23.837" FavoriteCount="1" Id="53355" LastActivityDate="2013-03-28T04:08:17.937" OwnerUserId="22298" PostTypeId="1" Score="2" Tags="&lt;logistic&gt;" Title="Logistic regression model comparison" ViewCount="238" />
  
  
  
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm doing a two-way between-within ANOVA in SPSS. I have two groups with 9 subjects each (so total = 18), and 24 levels of one repeated measure.&lt;/p&gt;&#10;&#10;&lt;p&gt;I understand why Mauchly's test of Sphericity has no meaning when there are are only 2 levels of a repeated measures factor, but I notice (using General Linear Model.....repeated measures in SPSS) that Mauchly's test of Sphericity also appears to be undefined (or gives the useless output of Mauchly's W = '.0' , p = '.') when the number of levels of a repeated measure is equal to or greater than the number of cases (subjects). In these instances, even though Mauchly's statistic is not calculated, Greenhouse-Geisser, Huynh-Feldt, and Lower-Bound Epsilon values are calculated.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would be really happy if someone could provide some insight on why Mauchly's statistic is not calculated in these cases and what should be done to assess sphericity in the absence of Mauchly's statistic.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2013-03-27T02:46:16.943" Id="53393" LastActivityDate="2013-10-19T06:29:03.027" LastEditDate="2013-03-27T09:47:35.077" LastEditorUserId="3277" OwnerUserId="22570" PostTypeId="1" Score="1" Tags="&lt;anova&gt;&lt;spss&gt;&lt;repeated-measures&gt;&lt;linear-model&gt;" Title="Repeated measures ANOVA: Mauchly's test undefined" ViewCount="1836" />
  
  
  
  <row Body="&lt;p&gt;It sounds like your response (or &quot;dependent&quot;) variable is perception of improvement eg individuals' response to a statement &quot;leave management is better now than it was&quot; (agree/disagree etc).  Assuming this is the case, you first need to clarify that you don't really test the hypothesis of improvement of the system, only retrospective perceptions of it.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Putting that aside and given the data that you apparently have, it sounds like the technique you need is &lt;a href=&quot;http://en.wikipedia.org/wiki/Ordered_logit&quot; rel=&quot;nofollow&quot;&gt;ordinal regression&lt;/a&gt;.  A quick google suggests this is possible in SPSS (I'm not an SPSS user so I won't try to recommend any of the links).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-03-27T08:29:25.693" Id="53409" LastActivityDate="2013-03-27T08:29:25.693" OwnerUserId="7972" ParentId="53407" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="54448" AnswerCount="1" Body="&lt;p&gt;I have 3 categorical variables (CVa, CVb, CVc) all 0 or 1. Two continuous variables (IV1, IV2) are confounding my observational study. The multiple regression &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lm(DV ~ CVa + CVb + CVc + CVa:CVb + CVa:CVc + IV1 + IV2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;is showing great significance for CVa&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;              Estimate   Std. Error t value Pr(&amp;gt;|t|)&#10;(Intercept)  -1.414684   1.498886  -0.944  0.35233&#10;CVa1         -0.841076   0.256946  -3.273  0.00255 **&#10;CVb1         -0.413594   0.168753  -2.451  0.01990 * &#10;CVc1         -0.328669   0.183652  -1.790  0.08298 . &#10;IV1          -0.011768   0.006519  -1.805  0.08049 . &#10;IV2           0.487658   0.211015   2.311  0.02743 * &#10;CVa1:CVb1     0.321766   0.238869   1.347  0.18743   &#10;CVa1:CVc1     0.741290   0.259402   2.858  0.00744 **&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I thought that ANCOVA (between factor CVa) must also show significance, but&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;summary(aov(DV ~ CVa + CVb + CVc + CVa:CVb + CVa:CVc + IV1 + IV2))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;is not showing any significance for CVa&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;          Df Sum Sq Mean Sq F value  Pr(&amp;gt;F)   &#10;CVa        1  0.368  0.3681   3.093 0.08817 . &#10;CVb        1  0.427  0.4275   3.593 0.06709 . &#10;CVc        1  0.015  0.0148   0.125 0.72629   &#10;IV1        1  0.585  0.5849   4.916 0.03384 * &#10;IV2        1  0.693  0.6935   5.828 0.02166 * &#10;CVa:CVb    1  0.126  0.1262   1.061 0.31069   &#10;CVa:CVc    1  0.972  0.9716   8.166 0.00744 **&#10;Residuals 32  3.807  0.1190&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Am I doing ANOVA instead of ANCOVA? If yes, how do I control for IV1, IV2 to get that F-value they usually report in papers?&lt;/p&gt;&#10;&#10;&lt;p&gt;Just in case, &lt;code&gt;lsmeans(m2,pairwise ~ CVa * CVb)&lt;/code&gt; reports that main effect of CVa is significant when controlled for IV1, IV2&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;$`CVa:CVb pairwise differences`&#10;               estimate        SE df  t.ratio p.value&#10;0, 0 - 1, 0  0.47043119 0.1725208 32  2.72681 0.04807&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2013-03-27T12:37:51.073" Id="53432" LastActivityDate="2013-03-27T16:24:39.373" LastEditDate="2013-03-27T16:24:39.373" LastEditorUserId="7290" OwnerUserId="22587" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;multiple-regression&gt;&lt;ancova&gt;&lt;confounding&gt;" Title="Can ANCOVA disagree with multiple regression?" ViewCount="210" />
  
  
  
  
  
  
  
  
  
  
  <row AcceptedAnswerId="55255" AnswerCount="1" Body="&lt;p&gt;Assume I have two feature selection algorithms, A and B, which are developed based on SVM. I applied these two algorithms on the same dataset, a Liver Cancer dataset (400 features &amp;amp; 150 samples), and they selected two small subsets A(30 features) and B(50 features). The classifier used is the same and is a binary SVM. In order to compare which algorithm is better, I then applied 5-fold cross-validation on both subset A and subset B and obtained their ROC/AUC values by using the LIBSVM ROC tool. The error rate is calculated as the mis-classified rates(cancer/non-cancer). I would like to compare which subset can predict the class(labels) better. I have read several posts on this forum and done some googling. With much information, I am very confused now.If everyone with more knowledge can give me some directions, I sincerely appreciate. Here are my questions.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Is this kind of comparison even meaningful (i.e valid) ? &lt;/li&gt;&#10;&lt;li&gt;if not meaningful, what should be done to correct it ? Or this is still a open research question? if so, what strategy do people commonly use ?&lt;/li&gt;&#10;&lt;li&gt;If I applied both 5 CV(cross-validation) and LOOCV and the results are inconsistent, (i.e. for 5-CV, subset A has better AUC than B and for LOOCV, subset B has better AUC than A), does it mean that bot 5-CV and LOOCV are too sample sensitive and neither one is better ? Or LOOCV should always be a better measurement than 5 CV ?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Thank you very much for helps.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-03-28T01:44:37.880" Id="54507" LastActivityDate="2013-04-26T05:01:05.887" LastEditDate="2013-03-28T02:13:07.193" LastEditorUserId="12502" OwnerUserId="12502" PostTypeId="1" Score="3" Tags="&lt;svm&gt;&lt;cross-validation&gt;&lt;feature-selection&gt;&lt;computational-statistics&gt;" Title="Is it possible to compare two feature selections algorithms by cross-validations?" ViewCount="234" />
  <row AnswerCount="2" Body="&lt;p&gt;I have an explanatory variable, &lt;code&gt;close&lt;/code&gt;, which is the daily close price of a firm in the stock market.&lt;/p&gt;&#10;&#10;&lt;p&gt;The following summarizes this explanatory variable:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;                    quote at market close&#10;-------------------------------------------------------------&#10;      Percentiles      Smallest&#10; 1%          .49              0&#10; 5%          1.5              0&#10;10%         2.95          .0002       Obs             2261717&#10;25%         8.84          .0013       Sum of Wgt.     2261717&#10;&#10;50%        19.39                      Mean           44.81048&#10;                        Largest       Std. Dev.      1510.413&#10;75%        32.78       155411.3&#10;90%        57.02         155431       Variance        2281348&#10;95%        74.78         155978       Skewness       86.71337&#10;99%       126.81         155990       Kurtosis       7583.119&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Some graphs showing transformations (I use STATA):&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/MhaIV.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;As you can see, the data is very skweded. My problem is how can I deal with this issue? There is not just one firm above the 99% percentile. For example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; sum close if close &amp;gt; 126.81&#10;&#10;    Variable |       Obs        Mean    Std. Dev.       Min        Max&#10;-------------+--------------------------------------------------------&#10;       close |     22615    2047.126    14968.92     126.82     155990&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And even if I were to remove all data above the 99% percentile, why choose this as a border and not for example the 95% percentile?&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that doing a &lt;code&gt;drop if close &amp;gt; 126.81&lt;/code&gt; does seem to normalize the data when using a square root transformation:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/NAjpR.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;But again, my approach does not seem very scientific.&lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, are there any publications I could read that deal with the particular issue of transforming/normalizing skewed price data?&lt;/p&gt;&#10;&#10;&lt;p&gt;Edit: can anyone tell me why &lt;code&gt;ladder&lt;/code&gt; does not return anything?&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;. ladder close&#10;&#10;Transformation         formula               chi2(2)       P(chi2)&#10;------------------------------------------------------------------&#10;cubic                  close^3                    .            .&#10;square                 close^2                    .            .&#10;identity               close                      .            .&#10;square root            sqrt(close)                .            .&#10;log                    log(close)                 .            .&#10;1/(square root)        1/sqrt(close)              .            .&#10;inverse                1/close                    .            .&#10;1/square               1/(close^2)                .            .&#10;1/cubic                1/(close^3)                .            .&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="8" CreationDate="2013-03-28T08:47:28.010" Id="54514" LastActivityDate="2013-03-28T21:52:48.910" LastEditDate="2013-03-28T12:27:40.360" LastEditorUserId="3040" OwnerUserId="3040" PostTypeId="1" Score="1" Tags="&lt;outliers&gt;&lt;normalization&gt;&lt;skewness&gt;&lt;standardization&gt;" Title="How to deal with extreme but &quot;real&quot; data, classify as outliers or no?" ViewCount="2067" />
  <row AnswerCount="1" Body="&lt;p&gt;Let we have data of 3D (example angular velocity x, angular velocity y, angular velocity z).Can anyone explain/give example/how to differentiate between gaussian distribution and non-gaussian distribution of data since the difference is important for us to make decision whether to use Kalman filter or particle filter.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-28T08:49:15.403" Id="54515" LastActivityDate="2013-03-28T09:45:29.393" OwnerUserId="23618" PostTypeId="1" Score="0" Tags="&lt;distributions&gt;&lt;estimation&gt;" Title="Gaussian vs non-gaussian distribution" ViewCount="251" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I have got a problem about doing a classification. I have got around 50 datasets. Each of them has 15 features.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am trying to use these features to classify the 50 datasets to either 'Good' or 'Bad'. The ground truth labels of the 50 datasets are available so that a classical training and validation can be done.&lt;/p&gt;&#10;&#10;&lt;p&gt;As there are 15 features, the problem should be considered as a classification in high dimensions. My question is:&lt;/p&gt;&#10;&#10;&lt;p&gt;Should we always perform PCA before we run any generic classification algorithms, such as LDA, KNN or SVM? &lt;/p&gt;&#10;&#10;&lt;p&gt;I got someone's opinion that:&lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;PCA chooses the directions in which the variables have the most spread, not the dimensions that have the most relative distances between clustered subclasses.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;But for my understanding, in order to do a better classification, we need to find features that have large differences between two groups. For example, we can calculate the mean and standard deviation of a feature for 'Good' and 'Bad' separately, and we can see if there are large differences. If so, we choose this feature. Also, we need to find features that have got least correlation in between. If two features have a large positive correlation, we can just choose to use one of them. PCA is somehow picking up the dimension reduced features for us, given 15 features, it will give 2 or 3 principal components that can be classified better. Am I right? Or I am on the wrong course?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-03-28T14:11:01.977" FavoriteCount="2" Id="54547" LastActivityDate="2013-06-27T13:27:40.927" LastEditDate="2013-06-27T13:27:40.927" LastEditorUserId="22047" OwnerUserId="14826" PostTypeId="1" Score="1" Tags="&lt;classification&gt;&lt;pca&gt;" Title="Should PCA be performed before I do classification?" ViewCount="1202" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I am used to R, in which you can use factor(variable) to indicate a categorical variable. However, in scikit-learn, trying to pass a variable of strings causes the DecisionTreeClassifier to give an error.&lt;/p&gt;&#10;&#10;&lt;p&gt;How are you supposed to use a categorical variable in scikit-learn? Is the only option for encoding a multi-class variable to use dummy variables?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-28T18:30:49.883" FavoriteCount="2" Id="54577" LastActivityDate="2013-04-05T00:09:48.093" OwnerUserId="624" PostTypeId="1" Score="5" Tags="&lt;categorical-data&gt;&lt;cart&gt;&lt;scikit-learn&gt;" Title="How are categorical variables used when fitting a decision tree in scikit-learn?" ViewCount="1934" />
  <row AnswerCount="2" Body="&lt;p&gt;Based on readings with logistic regression, it appears that you could use this analysis to make predictions about categorical variables. Does logistic regression allow you to predict multiple dependent variables with one independent variable, or just one? For example, if I were looking at predicting the gender of someone based on their emotional intelligence scores, could I also predict gender and race at the same time? How does this work? &lt;/p&gt;&#10;&#10;&lt;h3&gt;Reference&lt;/h3&gt;&#10;&#10;&lt;p&gt;Stevens, J. P. (2009) &lt;em&gt;Applied Multivariate Statistics for the Social Sciences&lt;/em&gt; (5th Edition) New York: Routledge Academic.  Chapter3.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-28T19:43:06.370" Id="54581" LastActivityDate="2013-03-29T00:31:16.170" LastEditDate="2013-03-28T20:04:44.870" LastEditorUserId="919" OwnerUserId="23650" PostTypeId="1" Score="2" Tags="&lt;logistic&gt;&lt;multivariate-analysis&gt;" Title="Is there a multivariate version of logistic regression?" ViewCount="109" />
  
  
  <row Body="&lt;p&gt;As far as for the code, I think there is only one error which is the generation of random numbers according to Laplace distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;Replace &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;y = laplace(randn(1), b, x(i-1));&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;by (See &lt;a href=&quot;http://en.wikipedia.org/wiki/Laplace_distribution#Generating_random_variables_according_to_the_Laplace_distribution&quot; rel=&quot;nofollow&quot;&gt;the Laplace distribution wiki&lt;/a&gt; to learn how to generate random Laplace distributed numbers before seeing my answer.)&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;u = rand()-0.5;&#10;uu = b / sqrt(2); &#10;y = mu - uu * sign(u).* log(1- 2* abs(u));&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;One more thing, Laplace distribution is symmetric, so &lt;code&gt;laplace(x(i-1), b, y))=laplace(y, b, x(i-1))&lt;/code&gt;. Therefore, when calculating your &lt;code&gt;alpha&lt;/code&gt;, you can omit &lt;code&gt;laplace(x(i-1), b, y))/laplace(y, b, x(i-1))=1&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are a lot of books on MCMC approaches. But for a quick start, maybe &lt;a href=&quot;http://www.cs.princeton.edu/courses/archive/spr06/cos598C/papers/AndrieuFreitasDoucetJordan2003.pdf&quot; rel=&quot;nofollow&quot;&gt;this paper&lt;/a&gt; and &lt;a href=&quot;http://www.cc.gatech.edu/~lebanon/notes/metropolis.pdf&quot; rel=&quot;nofollow&quot;&gt;this note&lt;/a&gt; is good enough.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-03-28T20:16:50.430" Id="54587" LastActivityDate="2013-05-05T17:33:14.673" LastEditDate="2013-05-05T17:33:14.673" LastEditorUserId="5025" OwnerUserId="22287" ParentId="54578" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;You might consider reading this paper on using SEM with categorical outcomes.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1431551/&quot; rel=&quot;nofollow&quot;&gt;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1431551/&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-29T00:31:16.170" Id="54605" LastActivityDate="2013-03-29T00:31:16.170" OwnerUserId="21260" ParentId="54581" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;As Jeremy said, it shouldn't be a problem if your sample size is large enough, I should think.&lt;/p&gt;&#10;&#10;&lt;p&gt;I don't know much about it myself, but the Tobit model may be appropriate here?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-03-29T01:20:08.583" Id="54611" LastActivityDate="2013-03-29T01:20:08.583" OwnerUserId="21971" ParentId="54591" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;I'm not totally certain about this---I've never seen that particular notation before---but that quantity looks like a variation on &lt;a href=&quot;http://en.wikipedia.org/wiki/Deviance_%28statistics%29&quot; rel=&quot;nofollow&quot;&gt;Deviance&lt;/a&gt; or the &lt;a href=&quot;http://en.wikipedia.org/wiki/Deviance_information_criterion&quot; rel=&quot;nofollow&quot;&gt;Deviance Information Criterion (DIC)&lt;/a&gt;.  Assuming that's true, it's intended to be a goodness of fit measurement that takes &quot;parsimony&quot; into account, which makes it handy for model selection. &lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose we're trying to decide between several models. The simplistic approach would be to choose the model that minimizes the residuals (or maximizes the likelihood) of the data. However, that might lead us to choose a model that has tons of parameters and &lt;em&gt;overfits&lt;/em&gt; the data by fitting both the underlying data as well as the noise. &lt;/p&gt;&#10;&#10;&lt;p&gt;Deviance attempts to correct for this by comparing a model with a &quot;fully saturated&quot; model that has a parameters for all observations (I'm guessing that's what $\ln\mathfrak{L}_\mathrm{max}$ means). Models with high deviance &quot;easily&quot; fit the data, while models with small deviance are less likely to fit the data well by chance, so we usually prefer the model with the smaller deviance.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-03-29T03:47:55.073" Id="54620" LastActivityDate="2013-03-29T03:47:55.073" OwnerUserId="7250" ParentId="54613" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;Avoiding difficult formulas may be the simplest: just create two new vectors:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x2 &amp;lt;- ifelse(t&amp;lt;t1, x1, 0)&#10;x3 &amp;lt;- ifelse(t&amp;lt;t1, 0, x1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now you can simply fit &lt;code&gt;y ~ x2 + x3 -1&lt;/code&gt; (I'm using R formula notation here).&lt;/p&gt;&#10;&#10;&lt;p&gt;Different solutions are possible (this comes down to a different dummy coding scheme). The best may depend on the correctness of the matching assumptions, and of course on your goals for the research.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-03-29T13:31:28.947" Id="54647" LastActivityDate="2013-03-29T15:11:41.260" LastEditDate="2013-03-29T15:11:41.260" LastEditorUserId="805" OwnerUserId="4257" ParentId="54643" PostTypeId="2" Score="1" />
  <row AnswerCount="3" Body="&lt;p&gt;Consider tossing a fair coin and writing down the outcomes in a sequence of symbols. &quot;H&quot; stands for head and &quot;T&quot; for tail. Let A , B  and C  be the words HTHH , HHTH  and THHH  respectively. What is the probability of encountering each A, B and C as a subword in your sequence of outcomes before both others?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; My question is relateted to &lt;a href=&quot;http://stats.stackexchange.com/questions/12174/time-taken-to-hit-a-pattern-of-heads-and-tails-in-a-series-of-coin-tosses&quot;&gt;this question&lt;/a&gt;. While its answers intuitively explain why some patterns hit sooner than others, none of them include a way of calculating those probabilities. Which is what I'm asking.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit 2:&lt;/strong&gt; I have already recieved an answer. Having read the wikipedia articles, this question still stumps me. I'm starting a bounty on it.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-03-29T14:50:46.297" Id="54649" LastActivityDate="2013-04-25T21:00:29.650" LastEditDate="2013-04-19T05:32:33.583" LastEditorUserId="21346" OwnerUserId="21346" PostTypeId="1" Score="3" Tags="&lt;probability&gt;&lt;bernoulli-distribution&gt;" Title="The probability of getting one variation of consecutive outcomes in Bernoulli trials before another" ViewCount="240" />
  <row Body="&lt;p&gt;You can also try  &lt;a href=&quot;http://books.google.com/books?id=wgY1FLKogHwC&quot; rel=&quot;nofollow&quot;&gt;Using R for Data Management, Statistical Analysis, and Graphics by Horton, Kleinman (2010)&lt;/a&gt; and maybe some useful information can be found in &lt;a href=&quot;http://web.njit.edu/~wguo/Math447/Time%20Series%20Analysis%20and%20Its%20Application%20with%20R%20examples.pdf&quot; rel=&quot;nofollow&quot;&gt;Time Series Analysis and Its Application with R examples&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-29T14:57:10.800" Id="54650" LastActivityDate="2013-03-29T15:09:01.207" LastEditDate="2013-03-29T15:09:01.207" LastEditorUserId="15964" OwnerUserId="14443" ParentId="53258" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="54661" AnswerCount="3" Body="&lt;p&gt;I have a dataset that I want to fit according to &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\log(y) = a + b_1\log(x_1) + b_2\log(x_2) +\cdots + b_k\log(x_k).$$&lt;/p&gt;&#10;&#10;&lt;p&gt;My statistical package has options to do a linear regression and lognormal. I am not sure which one I should choose.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-03-29T15:09:14.857" Id="54651" LastActivityDate="2013-04-02T16:37:16.407" LastEditDate="2013-03-29T19:12:09.263" LastEditorUserId="919" OwnerUserId="23674" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;lognormal&gt;" Title="Are log-log models the same as lognormal models?" ViewCount="249" />
  
  <row AcceptedAnswerId="54680" AnswerCount="1" Body="&lt;p&gt;I have two multi variate normal distributions N1 and N2.&#10;Say two points p1 is from N1 and p2 is from N2.&#10;I want to get some statistical features from these two points. How can I do it?&lt;/p&gt;&#10;&#10;&lt;p&gt;I need a richer representation (not one number). Say my points are in a d dimensional space, in that case I need a d dimensional vector which represents the statistical difference between these two points.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-03-29T18:05:33.323" Id="54671" LastActivityDate="2013-03-29T21:46:06.597" LastEditDate="2013-03-29T21:46:06.597" LastEditorUserId="88" OwnerUserId="11409" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;probability&gt;&lt;distributions&gt;&lt;hypothesis-testing&gt;" Title="Comparing two points corresponding to two different normal distributions" ViewCount="64" />
  <row AnswerCount="2" Body="&lt;p&gt;I am reading &lt;a href=&quot;http://www.stanford.edu/~hastie/local.ftp/Springer/OLD//ESLII_print4.pdf&quot; rel=&quot;nofollow&quot;&gt;Elements of Statistical Learning&lt;/a&gt; and on page 12 (section 2.3) a linear model is notated as:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\widehat{Y} = X^{T} \widehat{\beta}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;...where $X^{T}$ is the transpose of a column vector of the predictors / independent variables / inputs. (It states earlier &quot;all vectors are assumed to be column vectors&quot; so wouldn't this make $X^{T}$ a row vector and $\widehat{\beta}$ a column vector?)&lt;/p&gt;&#10;&#10;&lt;p&gt;Included in $X$ is a &quot;$1$&quot; to be multiplied with the corresponding coefficient giving the (constant) intercept.&lt;/p&gt;&#10;&#10;&lt;p&gt;It goes on to say:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;In the $(p + 1)$-dimensional input–output space, $(X,\ \widehat{Y})$ represents a hyperplane. If the constant is included in $X$, then the hyperplane includes the origin&#10;  and is a subspace; if not, it is an affine set cutting the $Y$-axis at the point&#10;  $(0,\ \widehat{\beta_0})$.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Does &quot;$(X,\ \widehat{Y})$&quot; describe a vector formed by the concatenation of the predictors, the intercept's &quot;$1$&quot; and $\widehat{Y}$? And why does including a &quot;$1$&quot; in $X$ force the hyperplane to pass through the origin, surely that &quot;$1$&quot; is to be multiplied by $\widehat{\beta_0}$?&lt;/p&gt;&#10;&#10;&lt;p&gt;I am failing to understand the book; any help / advice / links to resources would be very much appreciated.&lt;/p&gt;&#10;" CommentCount="11" CreationDate="2013-03-29T20:24:58.987" Id="54687" LastActivityDate="2014-01-25T03:43:13.303" LastEditDate="2014-01-25T01:32:12.847" LastEditorUserId="7290" OwnerUserId="23681" PostTypeId="1" Score="8" Tags="&lt;regression&gt;&lt;references&gt;&lt;statistical-learning&gt;" Title="How can a vector of variables represent a hyperplane?" ViewCount="619" />
  <row AnswerCount="2" Body="&lt;p&gt;It's easy to calculate power for paired t-tests and ANOVA, but how do you calculate power for more complicated designs such as split plot designs or factorial designs?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-29T22:07:29.770" FavoriteCount="1" Id="54693" LastActivityDate="2013-10-23T08:55:18.413" LastEditDate="2013-04-02T22:50:29.340" LastEditorUserId="88" OwnerUserId="20884" PostTypeId="1" Score="4" Tags="&lt;experiment-design&gt;&lt;power-analysis&gt;" Title="Power for experimental design" ViewCount="212" />
  <row AnswerCount="0" Body="&lt;p&gt;Suppose I have a regression model, for example a logistic regression model, which provides a score between 0 and 1 reflecting whether or not that a student will pass a course given certain variables: &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Variables about that student's demographics / past academic performance&lt;/li&gt;&#10;&lt;li&gt;Variables describing the course, such as the course subject, whether it's a 100, 200, 300    level, etc. course, perhaps the previous term's pass rate for that course &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;and that the output of the logistic regression model is calibrated so that the response are, hopefully, reliable probabilities. &lt;/p&gt;&#10;&#10;&lt;p&gt;Taking a subset of the population, for example a department or a course, the expected pass rate for that subset should be the average of the probabilities output by the logistic for that subset, by linearity of expectation. &lt;/p&gt;&#10;&#10;&lt;p&gt;Is this a reasonable way of projecting pass rates for these subgroups, or should I expect these projected pass rates to be unreliable for some reason (e.g. the model is likely to be inaccurate for certain subpopulations even if the overall accuracy is quite good). &lt;/p&gt;&#10;&#10;&lt;p&gt;I've read some about multi-level modeling. Would that approach help to make these predictions for subpopulations be more reliable? &lt;/p&gt;&#10;&#10;&lt;p&gt;Also, I have historical pass rate data for each of the courses and departments, so I could possibly combine the projected pass rate based on the probabilities for each student in a course or department with some sort of projection of the pass rate based on the time-series of historical pass rates. Including this time-series information may help more on the department level where there are consistently large numbers of students. &lt;/p&gt;&#10;&#10;&lt;p&gt;Are there any other approaches you can recommend or flaws in the current approach you can point out? &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-03-30T00:06:15.617" Id="54701" LastActivityDate="2013-03-30T00:06:15.617" OwnerUserId="23701" PostTypeId="1" Score="3" Tags="&lt;logistic&gt;&lt;predictive-models&gt;&lt;multilevel-analysis&gt;" Title="Modeling pass rates for departments and courses within a school" ViewCount="46" />
  <row AnswerCount="1" Body="&lt;p&gt;An urn has half red and half white marbles. The probability of getting all red or all white in 4 draws is 12.5% and the probability of getting all white or all red in 7 draws is 1.56%. How does one calculate these percentages without knowing the actual numbers of red and white marbles in the urn? &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-03-30T00:31:10.953" Id="54702" LastActivityDate="2013-08-27T18:15:11.513" LastEditDate="2013-06-28T12:13:52.453" LastEditorUserId="22047" OwnerUserId="23702" PostTypeId="1" Score="1" Tags="&lt;probability&gt;" Title="Marbles in urn: probability of drawing homogeneous sample" ViewCount="86" />
  <row Body="&lt;p&gt;Logistic classifier is a linear classifier. It uses 'Least Square' to adjust the f(x) function, which classify the 2 class.&lt;br &gt; Instead of predict the class, the classifier predict the probability of class Y being 1 with knowing X.&lt;br&gt;&#10;In my problem, the Logistic regression model uses Newton algorithm instead of  'Least Square'. It builds a matrix to adjust the function. &lt;br&gt;&#10;To classifying 7 class, it needs to have 7 logistic function, where each logistic predict the probability of one class.&lt;br&gt;&#10;That's how I've got.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-30T04:44:13.167" Id="54718" LastActivityDate="2013-03-30T04:44:13.167" OwnerUserId="22103" ParentId="52453" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;I don't know which of the two ways to calculate the variance is to prefer but I can give you a third, practical and useful way to calculate confidence/credible intervals by using Bayesian estimation of Cohen's Kappa.&lt;/p&gt;&#10;&#10;&lt;p&gt;The &lt;a href=&quot;http://www.r-project.org/&quot; rel=&quot;nofollow&quot;&gt;R&lt;/a&gt; and &lt;a href=&quot;http://mcmc-jags.sourceforge.net/&quot; rel=&quot;nofollow&quot;&gt;JAGS&lt;/a&gt; code below generates MCMC samples from the posterior distribution of the credible values of Kappa given the data. &lt;/p&gt;&#10;&#10;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;library(rjags)&#10;library(coda)&#10;library(psych)&#10;&#10;# Creating some mock data&#10;rater1 &amp;lt;- c(1, 2, 3, 1, 1, 2, 1, 1, 3, 1, 2, 3, 3, 2, 3) &#10;rater2 &amp;lt;- c(1, 2, 2, 1, 2, 2, 3, 1, 3, 1, 2, 3, 2, 1, 1) &#10;agreement &amp;lt;- rater1 == rater2&#10;n_categories &amp;lt;- 3&#10;n_ratings &amp;lt;- 15&#10;&#10;# The JAGS model definition, should work in WinBugs with minimal modification&#10;kohen_model_string &amp;lt;- &quot;model {&#10;  kappa &amp;lt;- (p_agreement - chance_agreement) / (1 - chance_agreement)&#10;  chance_agreement &amp;lt;- sum(p1 * p2)&#10;&#10;  for(i in 1:n_ratings) {&#10;    rater1[i] ~ dcat(p1)&#10;    rater2[i] ~ dcat(p2)&#10;    agreement[i] ~ dbern(p_agreement)&#10;  }&#10;&#10;  # Uniform priors on all parameters&#10;  p1 ~ ddirch(alpha)&#10;  p2 ~ ddirch(alpha)&#10;  p_agreement ~ dbeta(1, 1)&#10;  for(cat_i in 1:n_categories) {&#10;    alpha[cat_i] &amp;lt;- 1&#10;  }&#10;}&quot;&#10;&#10;# Running the model&#10;kohen_model &amp;lt;- jags.model(file = textConnection(kohen_model_string),&#10;                 data = list(rater1 = rater1, rater2 = rater2,&#10;                   agreement = agreement, n_categories = n_categories,&#10;                   n_ratings = n_ratings),&#10;                 n.chains= 1, n.adapt= 1000)&#10;&#10;update(kohen_model, 10000)&#10;mcmc_samples &amp;lt;- coda.samples(kohen_model, variable.names=&quot;kappa&quot;, n.iter=20000)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The plot below shows a density plot of the MCMC samples from the posterior distribution of Kappa.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/6ecls.png&quot; alt=&quot;Posterior Kappa density&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Using the MCMC samples we can now use the median value as an estimate of Kappa and use the 2.5% and 97.5% quantiles as a 95 % confidence/credible interval. &lt;/p&gt;&#10;&#10;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;summary(mcmc_samples)$quantiles&#10;##      2.5%        25%        50%        75%      97.5% &#10;## 0.01688361 0.26103573 0.38753814 0.50757431 0.70288890 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Compare this with the &quot;classical&quot; estimates calculated according to Fleiss, Cohen and Everitt:&lt;/p&gt;&#10;&#10;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;cohen.kappa(cbind(rater1, rater2), alpha=0.05)&#10;##                  lower estimate upper&#10;## unweighted kappa  0.041     0.40  0.76&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Personally I would prefer the Bayesian confidence interval over the classical confidence interval, especially since I believe the Bayesian confidence interval have better small sample properties. A common concern people tend to have with Bayesian analyses is that you have to specify prior beliefs regarding the distributions of the parameters. Fortunately, in this case, it is easy to construct &quot;objective&quot; priors by simply putting uniform distributions over all the parameters. This should make the outcome of the Bayesian model very similar to a &quot;classical&quot; calculation of the Kappa coefficient.&lt;/p&gt;&#10;&#10;&lt;h2&gt;References&lt;/h2&gt;&#10;&#10;&lt;p&gt;Sanjib Basu, Mousumi Banerjee and Ananda Sen (2000). Bayesian Inference for Kappa from Single and Multiple Studies. &lt;em&gt;Biometrics&lt;/em&gt;, Vol. 56, No. 2 (Jun., 2000), pp. 577-582&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-30T16:53:50.967" Id="54737" LastActivityDate="2013-03-30T21:56:55.883" LastEditDate="2013-03-30T21:56:55.883" LastEditorUserId="6920" OwnerUserId="6920" ParentId="30604" PostTypeId="2" Score="4" />
  
  
  
  <row Body="&lt;p&gt;It depends on whether you are using dummy coding or effect coding or some other coding. In dummy coding, a positive $\beta$ for a categorical variable means that the predicted value of the dependent variable is higher by $\beta$ when the independent variable takes that category than when it takes the reference category&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-31T02:09:42.180" Id="54770" LastActivityDate="2013-03-31T02:09:42.180" OwnerUserId="686" ParentId="54769" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;To use a plot like contour() your vertical z axis is going to be a matrix so x and y need to be the points defining a grid over which to draw z.  So you want something more like:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# Changed data and model from yours to make it easier to check the axes are the right way &#10;# around later on:&#10;&#10;x1 &amp;lt;- rnorm(100)&#10;x2 &amp;lt;- rnorm(100,5,1)&#10;&#10;y &amp;lt;- 1 + x1*5 + x2*(-3) + rnorm(100) &#10;&#10;mod &amp;lt;- lm(y ~ x1 + x2)&#10;&#10;gridded &amp;lt;- data.frame(&#10;    x1=seq(from=min(x1), to=max(x1), length.out=100),&#10;    x2=seq(from=min(x2), to=max(x2), length.out=100))&#10;&#10;yhat &amp;lt;- predict(mod, newdata=expand.grid(gridded))&#10;&#10;image(gridded$x1, gridded$x2, matrix(yhat,nrow=100, byrow=FALSE))&#10;contour(gridded$x1, gridded$x2, matrix(yhat,nrow=100, byrow=FALSE), add=TRUE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/JdDSR.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-03-31T03:43:04.343" Id="54773" LastActivityDate="2013-03-31T03:43:04.343" OwnerUserId="7972" ParentId="54767" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Actually, @DilipSarwate explained &lt;em&gt;exactly&lt;/em&gt; how to do it in your previous question. &lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{align}&#10;M_Z(t) &amp;amp;= E[e^{sZ}] \\&#10;&amp;amp;=  \int_{-\infty}^\infty e^{tz}\, f_Z(z) dz \\&#10;&amp;amp;=  \int_{-\infty}^\infty e^{tz}\, [pf_X(z)+(1-p)f_Y(z)] dz &#10;\end{align}&lt;/p&gt;&#10;&#10;&lt;p&gt;and then use basic properties of  expectations / integrals&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-03-31T08:55:05.647" Id="54780" LastActivityDate="2013-03-31T09:35:26.660" LastEditDate="2013-03-31T09:35:26.660" LastEditorUserId="805" OwnerUserId="805" ParentId="54778" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I think it is time to start paying attention to the answers you receive:  &lt;a href=&quot;http://stats.stackexchange.com/a/54734&quot;&gt;http://stats.stackexchange.com/a/54734&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;It has been largely discussed the validity of using a measure of skewness in &lt;strong&gt;multimodal&lt;/strong&gt; distributions, &lt;strong&gt;since its interpretation becomes unclear&lt;/strong&gt;. This is the case of finite mixtures. If your mixture looks (or is) unimodal, then you can use this value to understand a bit how asymmetric it is.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;In addition, note that MLE is based on maximising the likelihood, not on matching moments.&lt;/p&gt;&#10;&#10;&lt;p&gt;Check the following example using a simulated sample from a skew-normal (the sample skewness and the skewness of the mixture are different). &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(sn)&#10;library(moments)&#10;library(mixtools)&#10;&#10;set.seed(4321)    &#10;samp &amp;lt;- rsn(1000,0,1,10)&#10;&#10;skewness(samp)&#10;&#10;mix&amp;lt;-normalmixEM(samp,k=2,fast=TRUE)&#10;&#10;# Sampling from a 2-gaussian mixture&#10;gaussmix &amp;lt;- function(n,m1,m2,s1,s2,alpha) {&#10;    I &amp;lt;- runif(n)&amp;lt;alpha&#10;    rnorm(n,mean=ifelse(I,m1,m2),sd=ifelse(I,s1,s2))&#10;}&#10;&#10;# A simulated sample&#10;samp2 &amp;lt;- gaussmix(100000,mix$mu[1],mix$mu[2],mix$sigma[1],mix$sigma[2],mix$lambda[1])&#10;&#10;skewness(samp2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The measure of skewness based on the third moment is driven by the tails, rather than the level of asymmetry of the distribution. For example, a Student-$t$ distribution with $\nu&amp;lt;3$ degrees of freedom has undefined skewness in spite of being clearly symmetric. For this reason, it is preferred to use a quantile-based measure of skewness. An example of this of this is the &lt;a href=&quot;http://www.jstor.org/stable/2684808&quot; rel=&quot;nofollow&quot;&gt;AG measure&lt;/a&gt; of skewness which can be defined for any unimodal distribution $F$ as&lt;/p&gt;&#10;&#10;&lt;p&gt;$$AG=1-2F(\mbox{mode}).$$&lt;/p&gt;&#10;&#10;&lt;p&gt;This can be estimated by using a nonparametric estimator of $F$, such as a kernel estimator as follows&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;rm(list=ls())&#10;library(sn)&#10;library(moments)&#10;library(mixtools)&#10;&#10;# AG measure of skewness: insert the data and the interval c(minim,maxim) where the mode is located&#10;&#10;AG &amp;lt;- function(data,minim,maxim){&#10;n = length(data)&#10;hb = (4*sqrt(var(data))^5/(3*n))^(1/5)&#10;&#10;     kn = function(x){&#10;     k = r = length(x)&#10;     for(i in 1:k) r[i] = mean(dnorm((x[i]-data)/hb))/hb&#10;     return(r)&#10;      } &#10;&#10;    mode = optimise(f = kn, interval = c(minim,maxim),maximum = TRUE)$maximum&#10;&#10;     KN = function(x){&#10;     k = r = length(x)&#10;    for(i in 1:k) r[i] = mean(pnorm((x[i]-data)/hb))&#10;    return(r)&#10;    } &#10;    return(1-2*KN(mode))&#10;}&#10;&#10;&#10;# Simulated sample&#10;set.seed(707)&#10;samp &amp;lt;- rsn(1000,0,1,10)&#10;&#10;# AG and moment-based skewness measure of the sample&#10;AG(samp,0,2)&#10;skewness(samp)&#10;&#10;mix&amp;lt;-normalmixEM(samp,k=2,fast=TRUE)&#10;&#10;# Sampling from a 2-gaussian mixture&#10;gaussmix &amp;lt;- function(n,m1,m2,s1,s2,alpha) {&#10;    I &amp;lt;- runif(n)&amp;lt;alpha&#10;    rnorm(n,mean=ifelse(I,m1,m2),sd=ifelse(I,s1,s2))&#10;}&#10;&#10;# A simulated sample&#10;mix&amp;lt;-normalmixEM(samp,k=2,fast=TRUE)&#10;samp2 &amp;lt;- gaussmix(100000,mix$mu[1],mix$mu[2],mix$sigma[1],mix$sigma[2],mix$lambda[1])&#10;&#10;# AG and moment-based skewness measure of the fitted mixture&#10;AG(samp2,0,2)&#10;skewness(samp2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;As you can see, the AG measure of skewness is similar for both distributions, as expected.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;The moral of the story is&lt;/strong&gt;: The moment-based measure of skewness is NOT the best choice.&lt;/p&gt;&#10;&#10;&lt;p&gt;The same happens if this measure is applied to your data:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(sn)&#10;library(moments)&#10;library(mixtools)&#10;&#10;# AG measure of skewness: insert the data and the interval c(minim,maxim) where the mode is located&#10;&#10;AG &amp;lt;- function(data,minim,maxim){&#10;n = length(data)&#10;hb = (4*sqrt(var(data))^5/(3*n))^(1/5)&#10;&#10;     kn = function(x){&#10;     k = r = length(x)&#10;     for(i in 1:k) r[i] = mean(dnorm((x[i]-data)/hb))/hb&#10;     return(r)&#10;      } &#10;&#10;    mode = optimise(f = kn, interval = c(minim,maxim),maximum = TRUE)$maximum&#10;&#10;     KN = function(x){&#10;     k = r = length(x)&#10;    for(i in 1:k) r[i] = mean(pnorm((x[i]-data)/hb))&#10;    return(r)&#10;    } &#10;    return(1-2*KN(mode))&#10;}&#10;&#10;&#10;&#10;# AG and moment-based skewness measure of the sample&#10;AG(dat,-.1,.1)&#10;skewness(dat)&#10;&#10;# Sampling from a 2-gaussian mixture&#10;gaussmix &amp;lt;- function(n,m1,m2,s1,s2,alpha) {&#10;    I &amp;lt;- runif(n)&amp;lt;alpha&#10;    rnorm(n,mean=ifelse(I,m1,m2),sd=ifelse(I,s1,s2))&#10;}&#10;&#10;# A simulated sample&#10;mix&amp;lt;-normalmixEM(dat,k=2,fast=TRUE)&#10;samp2 &amp;lt;- gaussmix(100000,mix$mu[1],mix$mu[2],mix$sigma[1],mix$sigma[2],mix$lambda[1])&#10;&#10;# AG and moment-based skewness measure of the fitted mixture&#10;AG(samp2,-.1,.1)&#10;skewness(samp2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="4" CreationDate="2013-03-31T11:57:31.217" Id="54784" LastActivityDate="2013-03-31T17:54:38.317" LastEditDate="2013-03-31T17:54:38.317" LastEditorUserId="23734" OwnerUserId="23734" ParentId="54781" PostTypeId="2" Score="5" />
  
  
  
  <row AcceptedAnswerId="54827" AnswerCount="1" Body="&lt;p&gt;I have fitted a 3 parameter (mean, sigma &amp;amp; tau) model to my data and have also computed the standard error for each of them. The statistic of interest for my data is the sum of mean and tau. My question is how do I estimate the total error for this new statistic? I know I can't simply add up the errors for mean and tau. Any help would be greatly appreciated. Thanks in advance.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-03-31T18:22:11.180" FavoriteCount="0" Id="54801" LastActivityDate="2013-04-01T03:30:15.280" OwnerUserId="23744" PostTypeId="1" Score="3" Tags="&lt;mathematical-statistics&gt;&lt;standard-error&gt;&lt;fitting&gt;" Title="Combining standard errors of fit parameters" ViewCount="86" />
  <row AcceptedAnswerId="54815" AnswerCount="2" Body="&lt;p&gt;I'm relatively new to statistics and I'm a little confused about which test I should be using for this problem. Let’s say I am collecting data about responses to a book under two different conditions (C1 and C2). In each condition, I am looking at several attributes about the book and asking for ratings using a 5 point Likert scale.&lt;/p&gt;&#10;&#10;&lt;p&gt;The example tables below show the responses for the “enjoyability” attribute of the book, out of sample size of, say, 22.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;C1&#10;Rating:        1   2   3   4   5&#10;Frequency:     2   1   2   9   8&#10;&#10;C2&#10;Rating:        1   2   3   4   5&#10;Frequency:     10  6   3   3   0&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I would like to compare the “enjoyability” ratings under each condition and test for significance (is the book significantly more enjoyable under C1?). Is Fisher’s exact test what I should be using? Is there another test which might be more suitable?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-03-31T23:03:32.343" FavoriteCount="1" Id="54814" LastActivityDate="2014-10-18T09:24:52.683" LastEditDate="2014-10-18T09:24:52.683" LastEditorUserId="805" OwnerUserId="23753" PostTypeId="1" Score="3" Tags="&lt;statistical-significance&gt;&lt;categorical-data&gt;&lt;fishersexact&gt;" Title="Should I use Fisher's exact test for this?" ViewCount="412" />
  <row AnswerCount="1" Body="&lt;p&gt;The following data are mandible lengths (mm) of 10 male and 10 female golden jackals Canis aureus in the collection at the British Natural History Museum. The two samples are independent.  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Males:   120 107 110 116 114 111 113 117 114 112  &#10;Females: 110 111 107 108 110 105 107 106 111 111&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Using a randomization test, assess whether or not there is evidence of sexual dimorphism (a difference in mean jaw length between the sexes). [Include your R code in your solution, together with an explanation of what each line of code does.]   &lt;/li&gt;&#10;&lt;li&gt;Carry out the Mann-Whitney test on these data. Is your result consistent with the result from your randomization test?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="7" CreationDate="2013-04-01T00:00:49.553" Id="54817" LastActivityDate="2013-04-01T01:31:11.870" LastEditDate="2013-04-01T01:31:11.870" LastEditorUserId="7290" OwnerUserId="23755" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;self-study&gt;&lt;nonparametric&gt;&lt;mann-whitney-u-test&gt;&lt;randomization&gt;" Title="Randomization Tests" ViewCount="190" />
  <row Body="&lt;p&gt;You should have a look at the R package &lt;code&gt;influence.ME&lt;/code&gt;. It allows you to compute measures of influential data for mixed effects models generated by &lt;code&gt;lme4&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;An example model:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(lme4)&#10;model &amp;lt;- lmer(mpg ~ disp + (1 | cyl), mtcars)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The function &lt;code&gt;influence&lt;/code&gt; is the basis for all further steps:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(influence.ME)&#10;infl &amp;lt;- influence(model, obs = TRUE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Calculate Cook's distance:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;cooks.distance(infl)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Plot Cook's distance:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;plot(infl, which = &quot;cook&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/gxD1K.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-04-01T03:03:21.617" Id="54825" LastActivityDate="2013-04-01T03:03:21.617" OwnerUserId="13680" ParentId="54818" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;If you can't derive Cov(mu, tau) you can bootstrap the statistic of interest, i.e. mu+tau (note that the standard bootstrap may not be valid for non-iid data)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-01T03:30:15.280" Id="54827" LastActivityDate="2013-04-01T03:30:15.280" OwnerUserId="23759" ParentId="54801" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;$\bar{X}_t = \frac{(t-1)}{t}\bar{X}_{t-1} + \frac{1}{t}X_t$ so I suppose you could say that if $\frac{1}{t}(X_t - \bar{X}_{t-1}) &amp;gt; \epsilon$, then update.  You can make similar formulas for more than one year. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-01T04:17:33.163" Id="54831" LastActivityDate="2013-04-01T04:17:33.163" OwnerUserId="23759" ParentId="48862" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;The validity of the BH procedure depends on the hypothesis tests being positively dependent. If you read their 2001 paper you would see that it is not necessary to be multivariate normal, they gave weak conditions in the paper:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Rosenbaum’s (1984) conditional (positive) association, is enough to&#10;  imply PRDS: $X$ is conditionally associated, if for any partition $(X1,$&#10;  $X2)$ of $X$, and any function $h(X1), X2$ given $h(X1)$ is positively&#10;  associated.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;If these seems like a reasonable assumption to make about your data, then just declare it as an assumption and try to come up with scenarios where it is and isn't met to clarify it to yourself.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-04-01T04:56:55.613" Id="54836" LastActivityDate="2013-04-01T04:56:55.613" OwnerUserId="23759" ParentId="39215" PostTypeId="2" Score="1" />
  <row Body="Stan is software for Bayesian estimation using the No-U-Turn sampling (NUTS) algorithm 
  
  
  
  <row Body="&lt;p&gt;I don't know much about glmnet and I may be on the wrong track, but do you know about the package glmulti? As far as I know it can handle missing observations, although I don't know exactly how it does it. It acts as wrapper for lm, glm, lmer etc, and gives you the most parsimonious candidate models based on your chosen model distribution and your information criterion. You can also select the &quot;genetic algorithm&quot; method which reduces computing time.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://cran.r-project.org/web/packages/glmulti/index.html&quot; rel=&quot;nofollow&quot;&gt;http://cran.r-project.org/web/packages/glmulti/index.html&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;AICcmodavg and MuMIn are also good for model selection, but they would be very very slow given the size of your dataset.&lt;/p&gt;&#10;&#10;&lt;p&gt;Sorry this should be a comment, but I don't have commenting privileges yet!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-01T07:11:56.517" Id="54850" LastActivityDate="2013-04-01T07:11:56.517" OwnerUserId="23665" ParentId="54821" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;I think an easier way to think of it is: If there is any variable $C$ $(0&amp;lt;P(C)&amp;lt;1)$ such that the occurrence of $C$ increases the probability of both $A$ and $B$, then $A$ and $B$ cannot be independent. In your example, you actually chose variables that you intuitively understand to be dependent, not independent. That is, the event that there is an earthquake and a giant stomping around aren't independent, since they both are more likely to occur when the floor is shaking. Here is another example: Let C be the event that it rains, and A be the event that you use an umbrella, and B the event that you wear rainboots. Clearly A and B are not independent because when C occurs, you are more likely to both wear galoshes and carry and umbrella. But if you lived in an area that never, ever rained, then A and B could potentially be independent--neither the umbrella nor galoshes are being used as rain gear, so perhaps you wear the galoshes in the garden and use the umbrella to catch fish. They are only able to be independent because they don't share a cause. &lt;/p&gt;&#10;&#10;&lt;p&gt;Here is a proof: Suppose $A$ and $B$ are independent and also conditionally independent given $C$. &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;$P(AB) = P(A)P(B) = P(A|C)P(B|C)P(C)^2$ since $A$ is independent of $B$&lt;/li&gt;&#10;&lt;li&gt;$P(AB) = P(AB|C)P(C) = P(A|C)P(B|C)P(C)$ since $A$ is cond. independent of $B$ given $C$.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;It follows from 1 and 2 that $P(C) = P(C)^2$ hence $P(C) = 0$ or $P(C) = 1$.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-04-01T08:58:46.717" Id="54857" LastActivityDate="2013-04-01T08:58:46.717" OwnerUserId="23759" ParentId="54849" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;&lt;a href=&quot;http://www.bbc.co.uk/science/horizon/2003/biblecode.shtml&quot; rel=&quot;nofollow&quot;&gt;BBC Horizon - The Bible Code&lt;/a&gt;. It shows, that whatever codes people found  in Bible, so far they didn't prove to be statistically significant.&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2013-04-01T10:32:39.423" CreationDate="2013-04-01T10:32:39.423" Id="54865" LastActivityDate="2013-04-01T10:32:39.423" OwnerUserId="10069" ParentId="10459" PostTypeId="2" Score="3" />
  
  
  
  
  
  <row Body="&lt;p&gt;There are tests of location difference that don't assume normality or which make other assumptions. &lt;/p&gt;&#10;&#10;&lt;p&gt;What exactly they test depends on which additional assumptions you make.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, in place of a two-sample t-test, there are classical nonparametric equivalents like the Wilcoxon-Mann-Whitney, robustified versions of t-tests (e.g. ones based on trimmed means and Winsorized variances) and there are resampling procedures (randomization and bootstrapping tests, for example).&lt;/p&gt;&#10;&#10;&lt;p&gt;With the WMW, what exactly is being tested depends on your additional assumptions; if you assume a location-shift alternative (that is, you assume that the only way for the null to be wrong is for the location of one groups to be shifted relative to the other), then &lt;em&gt;given that assumption&lt;/em&gt;, the test is a test of difference in means and also of medians and also of lower quartiles and ... . However, it is sensitive to other kinds of difference, and the alternative can be framed as generally as $P(X&amp;gt;Y) \neq \frac{1}{2}$ (e.g. see Conover's &lt;em&gt;Practical Nonparametric Statistics&lt;/em&gt; for that form).&lt;/p&gt;&#10;&#10;&lt;p&gt;A robustified test appears to be testing something different from a mean shift, but again, if under the alternative you have an 'identical distributions apart from location shift' assumption (hence identical in all respects under the null), it is also a test for difference in means.&lt;/p&gt;&#10;&#10;&lt;p&gt;Randomization and bootstrap tests can be more explicitly constructed to use a mean, of course.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you don't assume identical distributions under the null, things become more complex; the WMW can still carry meaning, for example, but understanding what you're doing becomes more complex. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-01T23:17:02.700" Id="54926" LastActivityDate="2013-04-01T23:17:02.700" OwnerUserId="805" ParentId="54913" PostTypeId="2" Score="4" />
  <row AcceptedAnswerId="54939" AnswerCount="1" Body="&lt;p&gt;I want to know if I need to use ARCH/GARCH model for my time series. How can I use &lt;code&gt;McLeod.Li.test&lt;/code&gt; in R to do that and how should I interpret the result? the description in R help was not clear for me because in the example of the R help they used difference of the log of the time series and I do not know why? Also, how should I interpret the resulted plot?&#10;R help example is like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;data(CREF)&#10;r.cref=diff(log(CREF))*100&#10;McLeod.Li.test(y=r.cref)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-02T01:58:55.627" Id="54937" LastActivityDate="2013-04-02T02:31:29.150" LastEditDate="2013-04-02T02:10:36.480" LastEditorUserId="17590" OwnerUserId="17590" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;time-series&gt;&lt;garch&gt;&lt;arch&gt;" Title="Interpretation of McLeod-Li test for ARCH effect" ViewCount="1356" />
  <row Body="&lt;p&gt;This answer is probably a bit late for you, but I think this falls under the category of repeatability. If anyone disagrees, feel free to comment!&lt;/p&gt;&#10;&#10;&lt;p&gt;Repeatability is the extent to which the identity of the observer can be used to predict the result, or in simple terms, the consistency with which one observer differs from another. &lt;/p&gt;&#10;&#10;&lt;p&gt;Repeatability can also be used to quantify how closely repeated measurements of an individual resemble each other, relative to measurements from another individual. (e.g. when measuring the weight of a bird at several occasions over a year, high repeatability means that the individuals consistently differ across the course of a year).&lt;/p&gt;&#10;&#10;&lt;p&gt;Repeatability is quantified by the intra-class correlation coefficient (ICC). In your case, the class is the observer. R can be calculated by ANOVA, or the best-practice method, linear mixed effects modelling (LMM).&lt;/p&gt;&#10;&#10;&lt;p&gt;If you have a low repeatability within observers (small R, nonsignificant p-value), you can say that the observers are not biasing the result.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you have high repeatability, you have a problem, although it is not so bad if the observer error is balanced across treatment groups (but still should be avoided if possible). This would perhaps be a good time to use a model in which the observer is specified as a random factor, so that adjustments can be made for the inter-observer differences.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is a very good review on the topic by Nakagawa and Schielzeth (2010): &lt;a href=&quot;http://onlinelibrary.wiley.com/doi/10.1111/j.1469-185X.2010.00141.x/abstract&quot; rel=&quot;nofollow&quot;&gt;http://onlinelibrary.wiley.com/doi/10.1111/j.1469-185X.2010.00141.x/abstract&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;And the related R package, with support for general and generalized linear mixed modelling:&#10;&lt;a href=&quot;http://rptr.r-forge.r-project.org/&quot; rel=&quot;nofollow&quot;&gt;http://rptr.r-forge.r-project.org/&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-02T05:25:00.653" Id="54946" LastActivityDate="2013-07-19T07:05:47.427" LastEditDate="2013-07-19T07:05:47.427" LastEditorUserId="23665" OwnerUserId="23665" ParentId="16337" PostTypeId="2" Score="3" />
  <row AnswerCount="0" Body="&lt;p&gt;I have $n$ realisations $s_1,\, \dots , s_n$ of random variables $S_1,\, \dots, S_n$ which are assumed to be i.i.d. with unknown distribution. These measure the time between events.&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to calculate the probability that the data can be modelled by a renewal process: $(X_t)_{t\geq0}$ where $X_t$ is the total number of jumps by time $t$.&lt;/p&gt;&#10;&#10;&lt;p&gt;In particular, I am using the elementary renewal equation $\lim_{t\rightarrow \infty} \frac{E[X_t]}{t} = \frac{1}{E[S_i]}$ and I want the probability that the observed values of the LHS and RHS agree for a given $(n, t)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Do I want to some how construct a confidence interval for the statistic $\frac{E[X_t]}{t}$ and test whether $\frac{1}{E[S_i]}$ lies in this interval?&lt;/p&gt;&#10;&#10;&lt;p&gt;I guess what I'm asking, is how can I calculate the error in the strong law of large numbers for a given $n$, I think?&lt;/p&gt;&#10;&#10;&lt;p&gt;Apologies for the confusing description, my statistics isn't the best!&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-04-02T08:34:21.173" Id="54947" LastActivityDate="2013-04-02T12:27:08.757" LastEditDate="2013-04-02T12:27:08.757" LastEditorUserId="21072" OwnerUserId="23814" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;&lt;stochastic-processes&gt;" Title="Renewal Process Hypothesis Test" ViewCount="62" />
  <row AcceptedAnswerId="64302" AnswerCount="2" Body="&lt;p&gt;Canonical correlation analysis (CCA) aims to maximize the usual Pearson product-moment correlation (i.e. linear correlation coefficient) of the linear combinations of the two data sets.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, consider the fact that this correlation coefficient only measures linear associations - this is the very reason why we also use, for example, Spearman-$\rho$ or Kendall-$\tau$ (rank)correlation coefficients which measure arbitrary monotone (not necessarily linear) connection between variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hence, I was thinking of the following: one limitation of CCA is that it only tries to capture linear association between the formed linear combinations due to its objective function. Wouldn't it be possible to extend CCA in some sense by maximizing, say, Spearman-$\rho$ instead of Pearson-$r$?&lt;/p&gt;&#10;&#10;&lt;p&gt;Would such procedure lead to anything statistically interpretable and meaningful? (Does it make sense - for example - to perform CCA on ranks...?) I am wondering if it would help when we are dealing with non-normal data...&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-04-02T10:37:48.550" FavoriteCount="3" Id="54951" LastActivityDate="2013-10-12T23:25:34.950" LastEditDate="2013-04-02T12:27:49.373" LastEditorUserId="21072" OwnerUserId="6476" PostTypeId="1" Score="7" Tags="&lt;multivariate-analysis&gt;&lt;data-transformation&gt;&lt;rank-correlation&gt;&lt;canonical-correlation&gt;" Title="Canonical correlation analysis with rank correlation" ViewCount="652" />
  
  
  
  <row Body="&lt;h3&gt;Comments:&lt;/h3&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Five samples is pretty low.  I prefer 98, or 300, or tens of thousands.&lt;/li&gt;&#10;&lt;li&gt;As I am understanding the question, it seems pretty simple to answer using Excel using brute methods.  &lt;/li&gt;&#10;&lt;li&gt;Really the number of required samples is governed by the function that you are trying to measure.  Metropolis-Hastings often takes tens of thousands to get &quot;its feet on the ground&quot;.  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;h3&gt;Answer:&lt;/h3&gt;&#10;&#10;&lt;p&gt;You can use bootstrap resampling to investigate the uncertainty in both the mean and the standard deviation for different sample counts.  This will use the data you have, not the date you wish you had, and give you an answer that is about as meaningful as possible.  So after you find say 300 estimates of the mean with uniform random sampling with replacement (bootstrapping) then you can perform EDA on those values by graphing the CDF and looking at both central tendency and variation.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I do not feel comfortable taking thousands of resamples when I only have 5 actual measurements.  Even 300 is a bit high, but it is at an boundary where results get to be clean and still remain informative.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-04-02T17:23:13.857" Id="54984" LastActivityDate="2013-04-02T17:23:13.857" OwnerUserId="22452" ParentId="54975" PostTypeId="2" Score="0" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have two waterbodies that were sampled for a single water quality variable over time. Both waterbodies were sampled in the fall of each year from 2009-2012. In each year of sampling, 5 replicate samples were taken (so I would have 20 samples in total for each waterbody). I want to compare the two waterbodies over time to see if the water quality variable in each waterbody follows a similar trend. I feel like this is similar to an ANCOVA problem where a person could test for equal slopes. But this problem is a bit different because instead of having two continuous time series, I have 5 replicates of samples for each waterbody in each year. Anyway I'm wondering if ANCOVA is not reasonable, what is the best method to proceed with? A colleague suggested planned linear contrasts might help but I don't understand how that would be applicable to comparing 2 different waterbodies.  &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-04-02T21:51:59.227" Id="55001" LastActivityDate="2013-04-02T21:51:59.227" OwnerUserId="22022" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;ancova&gt;&lt;contrasts&gt;&lt;replication&gt;" Title="Checking for differences in time trends" ViewCount="74" />
  <row Body="&lt;p&gt;If you have an exact linear relationship in your independent variables (a bit more common jargon than predictor variables) like $c=a+b$, then you cannot apply the regression purposefully. In other words, it is misspecified. A statistical software will usually come up with an error message here. Intuitively, there is no unique estimator as there is no room for variation, or in technical terms, you have an non-invertible X matrix and basically try to divide by 0.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you have a strong correlation between lets say a and c, then this is just strong multicollinearity. You can estimate the coefficients, but you need to take into account three things:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;the standard deviations will be very high and the t values very low. Basically two variables try to co-explain the one dependent thing.&lt;/li&gt;&#10;&lt;li&gt;the estimated coefficients will be very sensitive to outliers. So, data contamination is a big issue here as the coefficient can dramatically change with just one very small outlier.&lt;/li&gt;&#10;&lt;li&gt;Given that the data is not contaminated, you need to adjust your interpretation of the coefficient. In a multiple linear OLS regression, the coefficient indicates what happens to the dependent variable if all other variables are held constant. Take for example:&#10;$$&#10;\text{wage} = \text{constant} + \beta_1*\text{education} + \beta_2*\text{IQ} + u.&#10;$$&#10;(Here IQ and education are expected to be strongly correlated.)&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;So you may suddenly wonder that your coefficient $\beta_1$ turns out to be negative, while you expect it to be theoretically by all means positive. This may be due to the very strong correlation between wage and intelligence, as intelligence may have a greater effect on wages and $\beta_1$ is downward adjusted by this effect of intelligence on wage. So, this is correct estimated, but the interpretation is now different.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, if you take the change in the interpretation of the coefficients into account and your data correct, strong multicollinearity gives an unbiased predictor.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-04-02T23:11:14.120" Id="55006" LastActivityDate="2013-04-02T23:21:40.697" LastEditDate="2013-04-02T23:21:40.697" LastEditorUserId="7290" OwnerUserId="22356" ParentId="54990" PostTypeId="2" Score="5" />
  
  
  <row AcceptedAnswerId="55021" AnswerCount="1" Body="&lt;p&gt;Given records for a team in 2 semesters Fall and Spring. We want to check if there is a relationship between the semester &quot;Fall or Spring&quot; and result &quot;win or loss&quot;. Can chi-square test be used? Example of data&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Fall , Spring &#10;win , loss  &#10;win , loss&#10;win , loss&#10;win , win &#10;win, win&#10;loss, loss&#10;loss, win&#10;loss, loss&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Many thanks &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-03T01:58:24.963" Id="55020" LastActivityDate="2013-04-03T02:09:19.563" OwnerUserId="14602" PostTypeId="1" Score="0" Tags="&lt;statistical-significance&gt;&lt;chi-squared&gt;" Title="chi-square test to check relationshio between season and win/loss" ViewCount="153" />
  <row Body="&lt;p&gt;I think so. Make a 2x2 contingency table: Fall by rows, Spring by columns. Do the tabulations in each cell. The expected numbers, assuming independence, ought to be $.25N$, where $N$ is the number of pairs. Your critical $\chi^2$ should have 1 d.f.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2013-04-03T02:09:19.563" Id="55021" LastActivityDate="2013-04-03T02:09:19.563" OwnerUserId="22312" ParentId="55020" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="55024" AnswerCount="1" Body="&lt;p&gt;My stata regression currently has the female variable, but I want to test whether gender matters on another variable or not. How would I go about creating the male variable? Or do I even need it? Will dropping the female variable work as well?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-03T03:39:21.110" Id="55023" LastActivityDate="2013-04-03T03:42:20.303" OwnerUserId="23807" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;stata&gt;" Title="How to create a &quot;male&quot; variable in stata" ViewCount="342" />
  <row Body="&lt;p&gt;If you want a quantitative comparison (a number telling you \emph{how much} are the distributions different) you could also look at the &lt;a href=&quot;http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence&quot; rel=&quot;nofollow&quot;&gt;Kullback-Leibler Divergence&lt;/a&gt; between $p$ and $q$, i.e. $$D(p||q) = \sum_{x_i} \log \frac{p(x_i)}{q(x_i)} p(x_i)$$&#10;in the discrete case, measured in bits. A similar metric applies to the continuous case (check &lt;a href=&quot;http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence&quot; rel=&quot;nofollow&quot;&gt;Wikipedia&lt;/a&gt;). Please note that it is not symmetric, i.e. $D(p||q) \neq D(q||p)$.&#10;If the distributions are empirical, i.e. estimated using a histogram, then $D(p||q)$ will have a floor value determined by the estimator.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-03T08:39:23.443" Id="55035" LastActivityDate="2013-04-03T08:39:23.443" OwnerUserId="22309" ParentId="52305" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;A third option is to use a dummy coding as in (2) but to penalize differences in the coefficients of adjacent categories:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://cran.r-project.org/web/packages/ordPens/ordPens.pdf&quot; rel=&quot;nofollow&quot;&gt;http://cran.r-project.org/web/packages/ordPens/ordPens.pdf&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-03T11:05:01.580" Id="55043" LastActivityDate="2013-04-03T11:05:01.580" OwnerUserId="17230" ParentId="39031" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;You're on the right path with the line chart (as Peter Flom mentioned, AVOID PIE CHARTS, especially comparing two of them).  As for the line chart, consider the message you're trying to convey and use the chart's formatting to reinforce the message.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Here's an example where perhaps you want to emphasize the strongest importer at the beginning period in comparison with the strongest at the end.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/MvMjr.png&quot; alt=&quot;Fish Imports&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;By using pre-attentive attributes like color, you can direct the viewers attention to the information you want them to focus on.  Include a more descriptive title that summarizes your message and the line chart can really focus your reader.  All the other information is still present, so you don't lose context.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-03T12:47:54.987" Id="55046" LastActivityDate="2013-04-03T12:47:54.987" OwnerUserId="10217" ParentId="54483" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;Of course you also have to decide about the splitting ratios for (double) resampling...&lt;/p&gt;&#10;&#10;&lt;p&gt;However, resampling usually works for quite a wide range of splitting ratios, if you keep in mind&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;not to do a leave-one-out if that would reduce the number of possible distinct runs &lt;/li&gt;&#10;&lt;li&gt;leave enough training cases in the innermost training set so the traing algorithm has a decent chance to produce a useful model. &lt;/li&gt;&#10;&lt;li&gt;the more independent cases you have, the less important are these considerations.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;And what if you are working at larger scale data (but not big data) of  10000 &amp;lt; N &amp;lt; 1000000?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;What you can do if you are not sure wheher resampling is need is: resample a few times. Enough so you can measure whether the resampling was necessary. &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;check the stability of your predictions&lt;/li&gt;&#10;&lt;li&gt;check the stability of your model parameters&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;With these results, you can decide whether you should add more resampling iterations or whether things are fine as they are.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-03T12:55:54.400" Id="55047" LastActivityDate="2013-04-03T12:55:54.400" OwnerUserId="4598" ParentId="54900" PostTypeId="2" Score="2" />
  
  
  <row AcceptedAnswerId="55089" AnswerCount="1" Body="&lt;p&gt;How can I derive the posterior distribution of a likelihood function $N({\bf{t}}|\phi(x)^{T}{\bf{w}}, \beta^{-1})$ and a prior $N({\bf{w}}|{\bf{m_{0}}}, \beta^{-1}{\bf{S_{0}}})Gam(\beta, a_{0},b_{0})$? &lt;/p&gt;&#10;&#10;&lt;p&gt;I'd like to use something similar to the general relations for conditional distributions in the normal case:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$p(x|y) \propto p(y|x)p(x) = N(y|Ax+b, L^{-1})N(x|\mu, \Lambda^{-1})$$&lt;/p&gt;&#10;&#10;&lt;p&gt;which gives the result of:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$p(x|y) \propto N(x|\Sigma\{A^{T}L(y-b)+\Lambda \mu\}, \Sigma)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;However, this doesn't work with a normal-gamma distribution as a prior, and I'm not sure if there is such equivalent relation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any thoughts about how to approach this?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-03T20:27:59.043" Id="55086" LastActivityDate="2013-04-03T20:41:49.733" OwnerUserId="2676" PostTypeId="1" Score="0" Tags="&lt;normal-distribution&gt;&lt;gamma-distribution&gt;&lt;posterior&gt;" Title="Deriving posterior distribution with a normal likelihood and a normal-gamma prior" ViewCount="786" />
  
  
  <row Body="&lt;p&gt;I've taken a look at the book. It seems to me that the rationale for this &quot;prior sample size&quot; teminology is the following. We have the usual model with $X_1,\dots,X_n$ conditionaly independent and identically distributed, given $\Theta=\theta$, with distribution $X_1\mid\Theta=\theta\sim\mathrm{Ber}(\theta)$. Suppose that &lt;em&gt;a priori&lt;/em&gt; $\Theta\sim\mathrm{Beta}(a,b)$. The prior mean is just $\mathbb{E}[\Theta]=a/(a+b)=:\mu$. If $a$ and $b$ are integers bigger than $1$, one way to interpret this prior is to suppose that we started with a $\mathrm{U}[0,1]$ prior and observed $a-1$ successes and $b-1$ failures in a Bernoulli experiment. By Bayes's theorem, the &quot;posterior&quot; of $\Theta$ for this &lt;em&gt;gedanken&lt;/em&gt; experiment is exaclty $\mathrm{Beta}(a,b)$. Hence, we may suggestively define the &lt;strong&gt;prior sample size&lt;/strong&gt; $\nu:=a+b-2$. We know from the properties of the beta distribution that a bigger $\nu$ will give us a more concentrated distribution. Now, to answer your question, what you want to do seems impossible: the more data you observe, the smaller will be your posterior uncertainty about $\Theta$. The posterior of $\Theta$ is $\mathrm{Beta}(c,d)$, with $c=a+\sum_{i=1}^n x_i$, and $d=b+n-\sum_{i=1}^n x_i$. Hence, $c+d$ grows linearly with $n$.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-04-03T22:08:02.950" Id="55096" LastActivityDate="2013-04-03T22:25:31.757" LastEditDate="2013-04-03T22:25:31.757" LastEditorUserId="9394" OwnerUserId="9394" ParentId="54972" PostTypeId="2" Score="2" />
  
  
  
  <row AcceptedAnswerId="55133" AnswerCount="1" Body="&lt;p&gt;I am a bit confused with this. &lt;/p&gt;&#10;&#10;&lt;p&gt;Independence - The response variables are independent. I only have a single response variable so OK? Or observations are independent of each other? E.g Auto Correlated &lt;/p&gt;&#10;&#10;&lt;p&gt;Normality - The response variable is normally distributed. My response variable (Y) fails a number of normality tests, so not OK. Do I transform to meet this assumption?&lt;/p&gt;&#10;&#10;&lt;p&gt;Homoscedasticity. - Same variance. Not sure how to test this. Is this part of residual examination? The linear model i am proposing has 4/5 explanatory variables, how do i determine this?&lt;/p&gt;&#10;&#10;&lt;p&gt;Linearity - Straight line. Well when plotting Y and all X's separately, some are loosely linear. Should I be testing non-linear functions? How do I determine which to use?&lt;/p&gt;&#10;&#10;&lt;p&gt;I dont have a great deal of time, but I am not comfortable with the current model I am using. &lt;/p&gt;&#10;&#10;&lt;p&gt;Any help would be great, thank you &lt;/p&gt;&#10;" ClosedDate="2013-04-04T18:28:51.730" CommentCount="2" CreationDate="2013-04-04T10:10:42.880" Id="55131" LastActivityDate="2013-04-29T07:17:08.533" LastEditDate="2013-04-29T07:17:08.533" LastEditorUserId="19981" OwnerUserId="19981" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;linear-model&gt;" Title="Solved Assumptions of Linear Regression" ViewCount="208" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;The central limit theorem, as I understand it (engineer, non-statistician), says that the distribution comprised of means of some other reasonably behaved distributions converges to a normal distribution in the limit as sample size goes to infinity.&lt;/p&gt;&#10;&#10;&lt;p&gt;Convolution (1d) between scalar values is shown &lt;a href=&quot;http://mathworld.wolfram.com/Convolution.html&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;The mean can be considered an expectation operator.  Can the central limit theorem be expressed as &#10;N(mu,sig) = conv(scalar values, operator) &lt;/p&gt;&#10;&#10;&lt;p&gt;I am thinking to explore frequency domain deconvolution to learn about the nature of the distribution upon which the mean is being operated, but need help to determine if it can be expressed in these terms, and then to express it in a way that is useful.&lt;/p&gt;&#10;&#10;&lt;p&gt;If so please give complete details.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-04-04T15:48:50.580" Id="55145" LastActivityDate="2013-04-04T15:48:50.580" OwnerUserId="22452" PostTypeId="1" Score="1" Tags="&lt;mean&gt;&lt;central-limit-theorem&gt;&lt;fourier-transform&gt;&lt;integral&gt;" Title="Can the proof of the central limit theorem be expressed as the limit of a convolution with an operator" ViewCount="81" />
  <row AcceptedAnswerId="55160" AnswerCount="1" Body="&lt;p&gt;I'm wondering if there is a good way to calculate the clustering criterion based on BIC formula, for a k-means output in R? I'm a bit confused as to how to calculate that BIC so that I can compare it with other clustering models. Currently I'm using the stats package implementation of k-means.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-04-04T16:14:04.300" FavoriteCount="1" Id="55147" LastActivityDate="2013-04-06T05:59:07.897" LastEditDate="2013-04-06T05:59:07.897" LastEditorUserId="3277" OwnerUserId="23918" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;clustering&gt;&lt;k-means&gt;&lt;bic&gt;" Title="K-means &amp; BIC (to validate clusters) in R" ViewCount="1631" />
  
  <row Body="&lt;p&gt;The KS-Test and other tests such as Anderson Darling are used for continuous distributions.  For discrete distributions, you can use the Chi-Square goodness of fit test, which is based on comparing the #observed events vs. the number of expected based on the expected number for your distribution.  If the parameter is known for the Poisson distribution you would obviously use that, more likely you will estimate the parameter using MLE, which reduces the degrees of freedom in your Chi-sq test.  An example is here; you would just adapt it to your specific distribution:&#10;&lt;a href=&quot;http://www.stat.yale.edu/Courses/1997-98/101/chigf.htm&quot; rel=&quot;nofollow&quot;&gt;http://www.stat.yale.edu/Courses/1997-98/101/chigf.htm&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-04T16:31:25.853" Id="55149" LastActivityDate="2013-06-21T08:08:01.960" LastEditDate="2013-06-21T08:08:01.960" LastEditorUserId="17230" OwnerUserId="21340" ParentId="55148" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;This is explained (curiously in the context of the GH distribution) in the paper&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://edoc.hu-berlin.de/series/sfb-649-papers/2010-49/PDF/49.pdf&quot; rel=&quot;nofollow&quot;&gt;http://edoc.hu-berlin.de/series/sfb-649-papers/2010-49/PDF/49.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The tail behavior of the GH density is ‘semi-heavy’, i.e. the tails are lighter than&#10;  those of non-Gaussian stable laws and TSDs [Tempered Stable Distributions] with a relatively small truncation parameter (see Figure 1.4), but much heavier than Gaussian.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Although some financial people are very enthusiastic about using these distributions, it has been shown that Maximum Likelihood Estimation requires thousands of observations to be reliable in this family.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://projecteuclid.org/DPubS?verb=Display&amp;amp;version=1.0&amp;amp;service=UI&amp;amp;handle=euclid.bjps/1341320246&amp;amp;page=record&quot; rel=&quot;nofollow&quot;&gt;http://projecteuclid.org/DPubS?verb=Display&amp;amp;version=1.0&amp;amp;service=UI&amp;amp;handle=euclid.bjps/1341320246&amp;amp;page=record&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The term &lt;em&gt;financial data&lt;/em&gt; is too wide to be discussed in terms of a single family of distributions. However, in this context is common to find data sets containing &lt;em&gt;extreme values&lt;/em&gt; which makes necessary to employ distributions with heavier tails that those of the normal ones. Despite this, in vague words, these extreme observations are not too extreme. For this reason people employ this kind of &quot;semi-heavy tailed&quot; distributions.&lt;/p&gt;&#10;&#10;&lt;p&gt;Modelling is an art, and there is no theorem that tells you the right distribution. The idea is to understand the properties of the distributions in order to employ those whose properties match our intuition or the evidence about a phenomenon of interest.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-04T17:35:20.427" Id="55158" LastActivityDate="2013-04-04T17:46:03.890" LastEditDate="2013-04-04T17:46:03.890" LastEditorUserId="23911" OwnerUserId="23911" ParentId="55152" PostTypeId="2" Score="2" />
  <row AnswerCount="0" Body="&lt;p&gt;I have got a question that&lt;/p&gt;&#10;&#10;&lt;p&gt;What are the differences of the Student's T-Test, Kolmogorov-Smirnov Test (KS-Test), and the Mann-Whitney-Wilcoxon Test (MWW-Test)?&lt;/p&gt;&#10;&#10;&lt;p&gt;In the case of the T-test, the null hypothesis is μ1 = μ2, indicating that the mean of feature values for class 1 is the same as the mean of the feature values for class 2. In the case of the KS-test, the null hypothesis is cdf(1) = cdf(2), meaning that feature values from both classes have an identical cumulative distribution. Both tests determine if the observed differences are statistically significant and return a score representing the probability that the null hypothesis is true (From Levner 2005).&lt;/p&gt;&#10;&#10;&lt;p&gt;How about the MWW-Test? Are there any assumptions of the distribution of the data for these tests?&lt;/p&gt;&#10;&#10;&lt;p&gt;When we want to test if there is significant difference between paired variables, which one or ones we should use?&lt;/p&gt;&#10;&#10;&lt;p&gt;Please also provide reference papers/links when you answer the questions. Thanks very much.&lt;/p&gt;&#10;&#10;&lt;p&gt;A.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-04-04T17:35:55.090" Id="55159" LastActivityDate="2013-04-04T17:35:55.090" OwnerUserId="14826" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;" Title="What are the differences of the Student's T-Test, KS-Test and the MWW-Test?" ViewCount="118" />
  <row AcceptedAnswerId="77274" AnswerCount="1" Body="&lt;p&gt;There is an easy way to test the linearity hypothesis for a simple regression model $y \sim {\cal N}(\alpha+\beta x,\sigma^2)$: calling $H_0$ this model, perform a test against the ANOVA model $H_1$ obtained by considering the (numeric) covariate $x$ as a (qualitative) factor. Of course this requires to have several values of $y$ for each value of $x$. In R :&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;fit0 &amp;lt;- lm(y ~ x)&#10;fit1 &amp;lt;- lm(y ~ factor(x))&#10;anova(fit0, fit1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This is quite sensible. But I have seen somewhere a linearity test for an ANCOVA model and I'm puzzling over what it means. The R code, denoting by $x$ the numeric covariate and $A$ the qualitative factor, is like:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;fit0 &amp;lt;- lm(y ~ x + A + x:A)&#10;fit1 &amp;lt;- lm(y ~ x + A + x:A + factor(x))&#10;anova(fit0, fit1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;That means that $H_0$ is the classical ANCOVA model with interaction and $H_1$ is the bigger model obtained by adding the covariate $x$ considered as a factor.&lt;/p&gt;&#10;&#10;&lt;p&gt;Does it make sense for you ? I cannot figure out how to interpret model $H_1$.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-04-04T18:28:04.107" FavoriteCount="0" Id="55165" LastActivityDate="2013-11-21T18:51:08.390" LastEditDate="2013-04-04T20:48:43.503" LastEditorUserId="8402" OwnerUserId="8402" PostTypeId="1" Score="2" Tags="&lt;hypothesis-testing&gt;&lt;linear-model&gt;&lt;ancova&gt;" Title="Strange linearity test for ANCOVA" ViewCount="115" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a study with the following data and I would like to calculate if there is or isn't any statistically significant difference between the 4 groups:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Test  Patients    Negative  NoDimer PotNeg  Total&#10;W     317 (79.3)  12 (3.0)  194     19      31 (7.75)&#10;WS    271 (67.8)  11 (2.75) 155     15      26 (6.5)&#10;G     245 (61.3)  11 (2.75) 136     14      25 (6.25)&#10;GS    260 (65.0)  13 (3.25) 144     16      29 (7.25)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I have 4 decision rules (W..GS) and 400 patients. 2nd column shows how many patients have negative decision rule (ant the percentage). 3rd column shows how many patients had a Dimer test negative (out of 2nd column patients) and the percentage (out of all patients). &#10;4th colum shows how many of the patients in 2nd column did not have a Dimer test and the 5th column shows a possible number of patients with negative Dimer (out of 4th column) if they would have had the test done (determined using the proportion neg/pos from the patients that had it done). Last column is the sum of known and potential negative and proportion of all patients.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is it possible to determine from these data if there are significant differences between the 4 decision rules. What methods can I use for this and do I need to use more data?&#10;If there is a method an example (or pointer) in R would be greatly appreciated.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-04-04T21:46:44.023" Id="55189" LastActivityDate="2013-04-04T21:46:44.023" OwnerUserId="11210" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;statistical-significance&gt;" Title="How can I calculate if there is a statistical difference?" ViewCount="85" />
  
  
  <row Body="&lt;p&gt;You're asking for the bias of the p value. The first two assumptions you mention are for unbiasedness of your estimated coefficients. Heteroskedasticity will bias the estimates of the standard errors. Both affect your t statistic, which affects your p value. &lt;/p&gt;&#10;&#10;&lt;p&gt;The Wooldridge book gives a good overview of this topic.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-05T00:57:34.317" Id="55206" LastActivityDate="2013-04-05T00:57:34.317" OwnerUserId="21971" ParentId="55113" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;(a) $E[\bar{X}]$ = $E[\frac{X_1 + X_2 + X_3+\dots+X_n}{n}$]&lt;/p&gt;&#10;&#10;&lt;p&gt;= $\frac{1}{n}E[X_1+X_2+X_3+\dots + X_n]$ &lt;/p&gt;&#10;&#10;&lt;p&gt;=$\frac{1}{n}(\mu_1 + \mu_2 +\mu_3+\dots +\mu_n) = \frac{1}{n}(n\mu) = \mu$&lt;/p&gt;&#10;&#10;&lt;p&gt;maybe you could work on the the next part and see if you get stuck.  Here is the wikipedia about the central limit theorem that should help with the third question in part (a) and part (b)&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Central_limit_theorem&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Central_limit_theorem&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-05T02:16:07.187" Id="55208" LastActivityDate="2013-04-05T02:28:43.713" LastEditDate="2013-04-05T02:28:43.713" LastEditorUserId="20381" OwnerUserId="20381" ParentId="55207" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;@Ben Bolker answer the same question on &lt;a href=&quot;http://stackoverflow.com/&quot;&gt;Stack Overflow&lt;/a&gt; &lt;a href=&quot;http://stackoverflow.com/a/6701856/707145&quot;&gt;here&lt;/a&gt;. I'm reproducing his answer here:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;@Ben Bolker answer&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I haven't done it exactly the same way as you, but I think the answer should be OK.&lt;/p&gt;&#10;&#10;&lt;p&gt;Set up starting conditions&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;s0 &amp;lt;- list(b=0.1,c=0.1,d=0,Th=3)&#10;X &amp;lt;- read.table(&quot;rogersdat.txt&quot;,header=TRUE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Function for computed expected number eaten:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;predfun &amp;lt;- function(b,c,d,Th,Te,N0,debug=FALSE) {&#10;  a &amp;lt;- (d+b*N0)/(1+c*N0)&#10;  r &amp;lt;- N0 - (1/(a*Th))*lambertW(a*Th*N0*exp(a*(Th*N0-Te)))&#10;  if (debug) cat(mean(a),b,c,d,Th,mean(r),&quot;\n&quot;)&#10;  r&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Plot the data (just to make sure):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;with(X,plot(Ne~N0))&#10;## check starting value&#10;lines(1:100,with(c(s0,X),predfun(b,c,d,Th,Te=72,N0=1:100))) &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Fit:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(emdbook)&#10;n1 &amp;lt;- nls(Ne~predfun(b,c,d,Th,Te=72,N0),data=X,&#10;    lower=c(1e-6,1e-6,-Inf,1e-6),algorithm=&quot;port&quot;,&#10;    start=list(b=0.1,c=0.1,d=0.002,Th=3))&#10;summary(n1)&#10;&#10;Formula: Ne ~ predfun(b, c, d, Th, Te = 72, N0)&#10;&#10;Parameters:&#10;    Estimate Std. Error t value Pr(&amp;gt;|t|)   &#10;b  0.0004155  0.0008745   0.475  0.63591   &#10;c  0.0000010  0.0657237   0.000  0.99999   &#10;d  0.0008318  0.0067374   0.123  0.90203   &#10;Th 4.0639937  1.4665102   2.771  0.00686 **&#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 &#10;&#10;Residual standard error: 4.516 on 85 degrees of freedom&#10;&#10;Algorithm &quot;port&quot;, convergence message: relative convergence (4) &#10;&#10;confint.default(n1)&#10;&#10;          2.5 %      97.5 %&#10;b  -0.001298523 0.002129547&#10;c  -0.128815083 0.128817083&#10;d  -0.012373153 0.014036799&#10;Th  1.189686504 6.938300956&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Estimates match pretty well, confidence intervals don't (these types of confidence intervals are a little dicey on the boundary anyway ...)&lt;/p&gt;&#10;&#10;&lt;p&gt;I would actually suggest that maximum likelihood estimation with binomial errors would be a little better.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;pdat &amp;lt;- data.frame(N0=1:100,Ne=predict(n1,newdata=data.frame(N0=1:100)))&#10;with(pdat,lines(N0,Ne,col=2))&#10;library(ggplot2)&#10;ggplot(X,aes(x=N0,y=Ne))+stat_sum(aes(size=factor(..n..)),alpha=0.5)+theme_bw()+&#10;  geom_line(data=pdat,colour=&quot;red&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2013-04-05T07:59:55.707" Id="55221" LastActivityDate="2013-04-05T08:06:00.117" LastEditDate="2013-04-05T08:06:00.117" LastEditorUserId="3903" OwnerUserId="3903" ParentId="9895" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;If you are new to the scoring world, your first book should be by naeem siddiqi on credit scoring using SAS. If you have not taken the class go for it. The class main focus is the overall understanding of scoring and selling SAS enterprise miner for millions of dollars.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you need theory you need a categorical data analysis and Data mining class from a near by university. Even after taking these classes you will still need help. &lt;/p&gt;&#10;&#10;&lt;p&gt;currently the most popular techniques used are &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;logistic regression &lt;/li&gt;&#10;&lt;li&gt;neural networks&lt;/li&gt;&#10;&lt;li&gt;support vector machines and &lt;/li&gt;&#10;&lt;li&gt;random forests&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;clustering, discriminant analysis, factor analysis, principal components are a must as well. &lt;/p&gt;&#10;&#10;&lt;p&gt;Credit scoring by elizabeth mays will also give you a good overview. &lt;/p&gt;&#10;&#10;&lt;p&gt;I also took a credit risk modeling class by SAS institute, which helped me a little.&#10;It is a constant learning process and its never done. &lt;/p&gt;&#10;&#10;&lt;p&gt;Bayesian folks like their methods as well. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;i also forgot to mention. Logistic regression in the most popular technique out there and will always be the one that banks will continue to use. &#10;Other methods are very difficult to sell to the upper management people, unless your bank is willing to care less about understanding these methods and their focus remains risk taking and money making. &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-04-05T12:40:03.200" Id="55238" LastActivityDate="2013-04-05T14:04:30.027" LastEditDate="2013-04-05T14:04:30.027" LastEditorUserId="930" OwnerUserId="16789" ParentId="55237" PostTypeId="2" Score="4" />
  <row AcceptedAnswerId="55263" AnswerCount="2" Body="&lt;p&gt;I need to find the minimum of a function. Reading the docs at &lt;a href=&quot;http://docs.scipy.org/doc/scipy/reference/optimize.html&quot;&gt;http://docs.scipy.org/doc/scipy/reference/optimize.html&lt;/a&gt; I see that there are several algorithms that do the same thing, i.e. find the minimum. How do I know which one I should choose?&lt;/p&gt;&#10;&#10;&lt;p&gt;some of the algorithm listed&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Minimize a function using the downhill simplex algorithm.&lt;/li&gt;&#10;&lt;li&gt;Minimize a function using the BFGS algorithm.&lt;/li&gt;&#10;&lt;li&gt;Minimize a function with nonlinear conjugate gradient algorithm.&lt;/li&gt;&#10;&lt;li&gt;Minimize the function f using the Newton-CG method. &lt;/li&gt;&#10;&lt;li&gt;Minimize a function using modified Powell's method.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;My function is  linear. dimensionality is around 232750(this is how many different gradients I have to compute each time), it takes about 2 mins to compute the gradient and the cost once, so not cheap. I don't think I have constraints. it is deterministic and continuous.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-04-05T14:25:12.570" FavoriteCount="3" Id="55247" LastActivityDate="2013-04-06T11:45:24.727" LastEditDate="2013-04-06T11:45:24.727" LastEditorUserId="7255" OwnerUserId="7255" PostTypeId="1" Score="8" Tags="&lt;optimization&gt;" Title="How to choose the right optimization algorithm?" ViewCount="780" />
  
  <row Body="&lt;p&gt;One often overlooked problem with plotting/analyzing summary (e.g. mean) curves is how to deal with Phase Variation. In an extreme example, one could plot the mean of 2 sin curves that are 180 degrees out of phase. This would of course be a straight line, which clearly doesn't reflect anything interesting about the individual curves (expect perhaps that they are out of phase). &lt;/p&gt;&#10;&#10;&lt;p&gt;Thinking in terms of a latent growth curve model, estimated variances of the random growth factors do nothing to account for phase variation, which is one reason why overall model fit for latent growth curve models is often poor (and usually not even assessed in hierarchical linear models). &lt;/p&gt;&#10;&#10;&lt;p&gt;I know this isn't exactly what your question is about, but I'll go ahead and mention two ways to try and deal with this. The first is to expand a LGCM to a growth mixture model, where you can use latent group membership to try and capture differences in when the curves peak. Another idea is to use functional data analysis for curve registration. I can add more detail later if it would be helpful.&lt;/p&gt;&#10;&#10;&lt;p&gt;Regarding growth mixture models:&lt;/p&gt;&#10;&#10;&lt;p&gt;Your choice for the functional form with a latent growth curve model is somewhat limited. Depending on the number of time points available, you can do linear, polynomials, and linear splines. A less used option however is a &quot;freed-loading&quot; model, in which the factor loadings for a single &quot;slope&quot; factor are freely estimated, expect 2 of them, which are set to define the scale of the latent factor (this is nicely described in Bollen and Curran's book on LGCMs). You can estimate this model with as few as 5 time points. It comes in realy useful for non-monotonic, multi-modal curves. &lt;/p&gt;&#10;&#10;&lt;p&gt;One problem though is that this functional form is the same for every person...it assumed that they all peak and trough and the same points, and variation in the slope factor just describes how much they peak or trough relative to the mean. It does nothing for phase variation. By extending this to a mixture model, where the slope factor loadings are freely estimated for each latent class, you can have a model that captures any important differences in phase variation because they each get their own curves. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-04-05T15:15:11.893" Id="55253" LastActivityDate="2013-04-06T13:52:37.373" LastEditDate="2013-04-06T13:52:37.373" LastEditorUserId="16049" OwnerUserId="16049" ParentId="55018" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;From the linked abstract, it appears that &quot;explaining away&quot; is discussing a learning mechanism, a common way that humans reason, not a formal method of logic or probability. It's a human-like way of reasoning that's not formally correct, just as inductive reasoning is not formally correct (as opposed to deductive reasoning). So I think the formal logic and probability answers are very good, but not applicable. (Note that the abstract is in a Machine Intelligence context.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Your giants example is very good for this. We believe that earthquakes or giants can cause the ground to shake. But we also believe that giants do not exist -- or are extremely unlikely to exist. The ground shakes. We will not investigate whether a giant is walking around, but rather we'll inquire as to whether an earthquake happened. Hearing that an earthquake did in fact happen, we are even more convinced that earthquakes are an adequate explanation of shaking ground and that giants are even more certain not to exist or are at least even more highly unlikely to exist.&lt;/p&gt;&#10;&#10;&lt;p&gt;We would only accept that a giant caused the ground to shake only if: 1) we actually witnessed the giant and were willing to believe that we were not being fooled and that our previous assumption that giants were highly-unlikely or impossible was wrong, or 2) we could totally eliminate the possibility of an earthquake and also eliminate all possibilities D, E, F, G, ... that we previously had not thought of but that now seem more likely than a giant.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the giant case, it makes sense. This learning mechanism (an explanation we find likely becomes even more likely and causes other explanations to become less likely, each time that explanation works) is reasonable in general, but will burn us, too. For example, the ideas that the earth orbits the sun, or that ulcers are caused by bacteria had a hard time gaining traction because of &quot;explaining away&quot;, which in this case we'd call confirmation bias.&lt;/p&gt;&#10;&#10;&lt;p&gt;The fact that the abstract is in a Machine Intelligence setting also makes me thing this is discussing a learning mechanism commonly used by humans (and other animals, I imagine) that could benefit learning systems even though it can also be highly flawed. The AI community tried formal systems for years without getting closer to human-like intelligence and I believe that pragmatics has won out over formalism and &quot;explaining away&quot; is something that we do and thus that AI needs to do.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-05T15:41:56.737" Id="55257" LastActivityDate="2013-04-05T15:47:50.777" LastEditDate="2013-04-05T15:47:50.777" LastEditorUserId="1764" OwnerUserId="1764" ParentId="54849" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I am working on fitting distributions to financial data using different volatility models. The simplest case of a gaussian distribution I do understand:&#10;The data is $\mathcal{N}$$(\mu,\sigma^2)$ distributed. So at each day I use a certain amount of data to estimate the $\mu$ and the $\sigma$. If I apply the &quot;normal&quot; ML I get as solution the empirical mean and empirical standard deviation. There I implement a volatility model, by not using the classical formula for the standard deviation and the full data set, but e.g. by using an exponentially weighted moving average. So I calculate a value for $\sigma$ with a model for each day and plug this into the formula of my normal distribution, ok. But what is in case of a more complicated distribution? E.g. the generalized hyperbolic distribution. How can I implement a volatility model? I mean, how can I combine it with the distribution? If I use classical ML approach, this does not use a volatility model, but in case of the generalized hyperbolic distribution, there is no $\sigma$ in the parameters, so I cannot just plug it in?&lt;/p&gt;&#10;&#10;&lt;p&gt;This master thesis &lt;a href=&quot;http://www.math.chalmers.se/~palbin/mattiasviktor.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.math.chalmers.se/~palbin/mattiasviktor.pdf&lt;/a&gt; is doing it, but I do not understand how they do it? On page 48 they just say &quot;Over a 1300 day period, the parameters of the probability density functions&#10;are estimated every five days, using a 500 day window&quot; And on page 50 they give the generalized hyperbolic distribution which uses e.g. a GARCH-AR volatility model? &lt;/p&gt;&#10;&#10;&lt;p&gt;The only way I can imagine is, that I calculate a value for $\sigma$ at each day, at for each day I match the moments of the distributions. But this is not ML? On page 84 you can see a nice picture, for each day the hyperbolical distribution is combined with a volatility model.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am interested in doing this, since I want to calculate the VaR over a time period with fitting a distribution and using a time volatility model.&lt;/p&gt;&#10;&#10;&lt;p&gt;ok edit, so in more detail:&#10;I have financial data (&lt;a href=&quot;http://uploadeasy.net/upload/cdm3n.rar&quot; rel=&quot;nofollow&quot;&gt;http://uploadeasy.net/upload/cdm3n.rar&lt;/a&gt;). These are the losses (negative returns) of, I think the last 10 years of a company.&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to calculate the VaR using a distribution with a volatility model. So first of all, I assume a normal distribution: $\mathcal{N}(\mu,\sigma)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The VaR is nothing else, than the quantile: $VaR_\alpha=F^{-1}(\alpha)$&lt;/p&gt;&#10;&#10;&lt;p&gt;So I have a volatility model for my $\sigma$, I get for each day a value, so $\sigma_1,\sigma_2,...,\sigma_n$. For simplicity reasons I set $\mu$ to zero. So I have for each day a normal distribution with mean zero and a sigma which comes from the volatility model. Lets use a very simple volatility model, e.g. the (empirical) standard deviation of the last 10 days. So I use the first 10 days the calculate the volatility for the 11th day and so on. I plug this into my normaldistribution to compute the quantile. The R code would be e.g.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;volatility&amp;lt;-0&#10;quantile&amp;lt;-0&#10;for(i in 11:length(dat)){&#10;volatility[i]&amp;lt;-sd(dat[(i-10):(i-1)])&#10;}&#10;&#10;for(i in 1:length(dat)){&#10;quantile[i]&amp;lt;-qnorm(0.975,mean=0,sd=volatility[i])&#10;}&#10;# the first quantile value is the VaR for the 11th date&#10;&#10;#plot the volatility&#10;plot(c(1:length(volatility)),volatility,type=&quot;l&quot;)&#10;&#10;#add VaR&#10;lines(quantile,type=&quot;l&quot;,col=&quot;red&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;How can I do this in case of a non standardized t distribution with $\nu,\mu,\beta$. Or in case of a generalized hyperbolic distribution?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-04-05T16:46:17.710" Id="55262" LastActivityDate="2013-04-06T09:33:53.007" LastEditDate="2013-04-06T09:33:53.007" LastEditorUserId="21998" OwnerUserId="21998" PostTypeId="1" Score="1" Tags="&lt;distributions&gt;&lt;standard-deviation&gt;&lt;fitting&gt;&lt;volatility-forecasting&gt;" Title="Volatility model combined with different distributions?" ViewCount="140" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I have a multiple linear regression with a couple of independent variables in it. Most of them are significant at p&amp;lt;0.001. The model has an R² of 0.83. When I add more variables, the old and the new variables are all highly significant, but R² does not improve at all.  &lt;/p&gt;&#10;&#10;&lt;p&gt;What does that tell me?&lt;/p&gt;&#10;" CommentCount="10" CreationDate="2013-04-05T19:50:27.260" Id="55273" LastActivityDate="2013-04-17T01:55:18.870" LastEditDate="2013-04-05T20:32:39.430" LastEditorUserId="7290" OwnerUserId="23974" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;multiple-regression&gt;&lt;statistical-significance&gt;" Title="Highly significant coefficient does not increase R²" ViewCount="349" />
  <row Body="&lt;p&gt;Because one has $\boxed{T^2=F}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;To show that, you have to check that (with $N=mn$):&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;$SSW/(N-2)= S^2_p$ (the unbiaised estimate of $\sigma^2$)&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;$SSB = {(\bar X- \bar Y)}^2/(\frac{1}{n}+\frac{1}{m})$&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;To show the second point you only have to use : &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;the elementary equality $SSB=m{(\bar x - \bar{x\cdot y})}^2+n{(\bar y - \bar{x\cdot y})}^2$&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;the fact that the mean of the whole sample $x\cdot y=(x_1, \ldots, x_m, y_1, \ldots y_n)$ is the weighted mean $\frac{m \bar x + n \bar y}{m+n}$&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;some elementary but a little tiedous calculations to conclude&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Sorry for the strange notation $x\cdot y$ for the &quot;whole sample&quot;, this was my first idea and I'm in a hurry now.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-05T19:55:41.010" Id="55274" LastActivityDate="2013-04-05T19:55:41.010" OwnerUserId="8402" ParentId="55236" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;See &lt;a href=&quot;http://en.wikipedia.org/wiki/Logistic_regression&quot; rel=&quot;nofollow&quot;&gt;logistic regression&lt;/a&gt;. 'Death' as the response variable, 'Smoking' as a predictor - either nominal or score the levels as seem appropriate.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-05T21:24:04.143" Id="55281" LastActivityDate="2013-04-05T22:04:31.420" LastEditDate="2013-04-05T22:04:31.420" LastEditorUserId="17230" OwnerUserId="17230" ParentId="55279" PostTypeId="2" Score="2" />
  
  
  
  
  
  <row AcceptedAnswerId="55313" AnswerCount="2" Body="&lt;p&gt;I want to draw a random number from 1 to 100. If this number is even, I want to then draw a random number from the odd numbers between 1 and 100. If the first number is odd, I want to draw a random number from the even numbers between 1 and 100.&lt;/p&gt;&#10;&#10;&lt;p&gt;Are the following three strategies the same?&lt;/p&gt;&#10;&#10;&lt;p&gt;(1) Draw a random number from the urn. Sort all odd (or even) numbers out and then draw a random number from the remaining numbers.&lt;/p&gt;&#10;&#10;&lt;p&gt;(2) Draw a random number from the urn. Draw a second number from the urn and put it back until the second number is not odd (or even).&lt;/p&gt;&#10;&#10;&lt;p&gt;(3) Draw two numbers from the urn and put them back until not both are odd (or even).&lt;/p&gt;&#10;&#10;&lt;p&gt;As a real world example, imagine I want to draw a proband from a pool of probands. I then want to pair this person off with a second proband of the opposite sex. I want the sex of the first proband to be random (e.g. because of sequence effects), therefore I cannot draw first a man and then a woman, or vice versa. I could draw first a man before a woman, then a woman before a man, then again a man before a woman etc. But I want to avoid the predictable regularity of that.&lt;/p&gt;&#10;&#10;&lt;p&gt;Contrary to the notification that pops up when I enter the title for this question, this is not a subjective question asking for opinions or guesses. I am sure the &quot;relative randomness&quot; of the events can be calculated: I would guess that you'd have to calculate the probabilities to draw a specific number under all three conditions, and the best &quot;relative randomness&quot; will be achieved, where this number has the lowest probability. Unfortunately, I don't know how to calculate this. Therefore I ask here.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;I did not know the proper tags for this question. Please change them, if you feel others fit better.&lt;/em&gt;&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-04-06T05:01:35.773" Id="55301" LastActivityDate="2013-04-06T11:15:28.930" LastEditDate="2013-04-06T05:17:46.123" LastEditorUserId="14650" OwnerUserId="14650" PostTypeId="1" Score="2" Tags="&lt;probability&gt;&lt;randomness&gt;" Title="Best process for randomly drawing an odd and an even number" ViewCount="247" />
  
  
  
  <row Body="&lt;p&gt;I don't know if what your simulator is doing with tie percentages, but it's likely due to paired boards, 3 of a kind boards not helping the inferior starting hand if the board is shared.  For example.&lt;/p&gt;&#10;&#10;&lt;p&gt;Your hand:&#10;AA&lt;/p&gt;&#10;&#10;&lt;p&gt;My random hand:&#10;JJ&lt;/p&gt;&#10;&#10;&lt;p&gt;Board runs out 22347, we both make 2 pair.  In fact, any pair on the board or 3 of a kind 22237 doesn't help my hand, b/c I started the hand behind and we're sharing those cards.  If I get to have my own board, then making a second pair or a full house will help me.  In fact, sometimes I would be able to start with 2-7 and if my board was&lt;/p&gt;&#10;&#10;&lt;p&gt;44336, I would have made two pair and could beat your lone pair of aces, while if the board was shared, I'd have 2 pairs 4's and 3's, and you'd have Aces and fours and win the hand.  That's likely a great percentage of the difference.  Having your own board simply gives the inferior starting hand more chances to pull from behind.  Which is exactly why more &quot;bad beats&quot; are dealt in stud games then flop games.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-06T15:26:02.947" Id="55323" LastActivityDate="2013-04-06T15:26:02.947" OwnerUserId="20381" ParentId="55320" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;1) Why transform at all?  If you use a nonparametric test, normality is not assumed. &lt;/p&gt;&#10;&#10;&lt;p&gt;2) &quot;Statistically significantly similar&quot; is tricky; usually statistical significance applies to a difference (although there are also equivalence tests).&lt;/p&gt;&#10;&#10;&lt;p&gt;3) I am not sure what your first plot does... what are the two axes? &lt;/p&gt;&#10;&#10;&lt;p&gt;4) Are your QQPLOTs plots against a normal distribution? It looks like data 1 is truncated &lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT in response to comments:&#10;You do seem to want some kind of regression here. I would first look at a scatterplot of the two variables. Then I'd try an ordinary least square regression for starters, and look at the residuals. If there are problems with normality and homoscedasticity, you might need to do something else. &lt;/p&gt;&#10;&#10;&lt;p&gt;By the way, I still am not sure what that first plot is. &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-04-06T17:31:53.920" Id="55331" LastActivityDate="2013-04-06T22:16:19.773" LastEditDate="2013-04-06T22:16:19.773" LastEditorUserId="686" OwnerUserId="686" ParentId="55321" PostTypeId="2" Score="2" />
  
  <row AnswerCount="0" Body="&lt;p&gt;How would you deal with lists of varying lengths?  For example, if you asked for an &lt;em&gt;ordered list&lt;/em&gt; of your favourite movies without any constraints how would you value the weight of someone who submits 5 samples and another who submits 20 samples?&lt;/p&gt;&#10;&#10;&lt;p&gt;Aside from the linear valuations, crudely you could give each participant's first selection a score of n, 2nd = n-1, 3rd = n-2... nth=1/nth value (n being the longest list supplied) and build an aggregated score across the sampled population. But that feels far too crude for proper analysis. Is there a better weighting distribution with a tapered decay to suit a long tail?&lt;/p&gt;&#10;&#10;&lt;p&gt;Instinctively, the person who supplies a longer submission should weigh higher than a shallow submission. But I'm unsure of the social research techniques that might be suitable.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a statistical model that addresses this kind of analysis for open response lists?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-04-06T18:30:46.130" Id="55336" LastActivityDate="2013-04-06T18:30:46.130" OwnerUserId="23999" PostTypeId="1" Score="0" Tags="&lt;sampling&gt;&lt;social-science&gt;" Title="How would you approach weighting of open response lists in a statistical manner?" ViewCount="20" />
  <row Body="&lt;p&gt;What are you trying to do?&lt;/p&gt;&#10;&#10;&lt;p&gt;The &quot;Process documents from ...&quot; operators can calculate TF/IDF, Term Frequency, Term Occurences and binary term occurences for you if you select the appropriate option.&lt;/p&gt;&#10;&#10;&lt;p&gt;The result of &quot;Process documents from ...&quot; is a structure where each token (term) an attribute that has a value for each document. You can use the Loop Attributes operator to calculate something for each attribute. &lt;/p&gt;&#10;&#10;&lt;p&gt;For example, I used Loop Attributes to sum up the TF/IDF of each attribute (not that it makes sense but it illustrates what you can do).&lt;/p&gt;&#10;&#10;&lt;p&gt;I added the Loop Attributes after the Process Documents step and set the following options:&lt;/p&gt;&#10;&#10;&lt;p&gt;Attribute filter type: value_type&#10;Value type: real (as the TF/IDF vectors are real numbers)&lt;/p&gt;&#10;&#10;&lt;p&gt;Loop Attributes creates a macro that can be used to reference the attribute currently in the loop.&lt;/p&gt;&#10;&#10;&lt;p&gt;Inside Loop Attributes I added a Generate Aggregations operator with the following options:&lt;/p&gt;&#10;&#10;&lt;p&gt;Attribute name: sumatt_%{loop_attribute}&#10;Attribute filter type: single&#10;Attribute: %{loop_attribute}&lt;/p&gt;&#10;&#10;&lt;p&gt;The result is a sum attribute for each attribute (token).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-04-06T20:39:58.683" Id="55343" LastActivityDate="2013-04-06T20:39:58.683" OwnerUserId="7877" ParentId="55268" PostTypeId="2" Score="0" />
  
  <row AnswerCount="0" Body="&lt;pre&gt;&lt;code&gt;import sympy&#10;sympy.init_printing()&#10;x, y, mu, sigma, density1, density2 = sympy.symbols('x y mu sigma density1 density2')&#10;eq1 = sympy.Eq(density1, 1/(sympy.sqrt(2*sympy.pi)*sigma)&#10;                         *sympy.exp(-(x-mu)**2/(2*sigma**2)))           # normal &#10;eq2 = sympy.Eq(y, sympy.exp(x))                                         # substitution&#10;eq3 = sympy.Eq(density2, 1/(y*sympy.sqrt(2*sympy.pi)*sigma)&#10;                         *sympy.exp(-(sympy.ln(y)-mu)**2/(2*sigma**2))) # lognormal&#10;[eq1, eq2, eq3]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Output:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;⎡                           2                                          2⎤&#10;⎢                  -(-μ + x)                             -(-μ + log(y)) ⎥&#10;⎢                  ──────────                            ───────────────⎥&#10;⎢                        2                                        2     ⎥&#10;⎢             ___     2⋅σ                           ___        2⋅σ      ⎥&#10;⎢           ╲╱ 2 ⋅ℯ                 x             ╲╱ 2 ⋅ℯ               ⎥&#10;⎢density₁ = ─────────────────, y = ℯ , density₂ = ──────────────────────⎥&#10;⎢                   ___                                    ___          ⎥&#10;⎣               2⋅╲╱ π ⋅σ                              2⋅╲╱ π ⋅σ⋅y      ⎦&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;How can I tell SymPy take the first (normal) density, apply the x to y substitution and output the desired second (lognormal) density?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-04-06T22:35:53.947" Id="55353" LastActivityDate="2013-04-06T22:51:51.770" LastEditDate="2013-04-06T22:51:51.770" LastEditorUserId="14202" OwnerUserId="14202" PostTypeId="1" Score="0" Tags="&lt;normal-distribution&gt;&lt;lognormal&gt;" Title="How to calculate the log-normal density from the normal density using SymPy" ViewCount="86" />
  <row AcceptedAnswerId="55379" AnswerCount="2" Body="&lt;p&gt;In some real-life problems such as authentication, we only have training data for one label (x is authenticated) while the other label doesn't have any data or only few entries (x is an imposter).&lt;/p&gt;&#10;&#10;&lt;p&gt;What kind of changes should we do in order to adjust a classifier to deal with a label targeted for other/unknown entries?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-04-06T23:44:44.287" FavoriteCount="1" Id="55357" LastActivityDate="2013-04-07T10:59:40.320" LastEditDate="2013-04-07T02:22:19.163" LastEditorUserId="3826" OwnerUserId="24004" PostTypeId="1" Score="4" Tags="&lt;machine-learning&gt;&lt;classification&gt;" Title="Binary Classifier with training data for one label only" ViewCount="188" />
  
  <row AcceptedAnswerId="55368" AnswerCount="2" Body="&lt;p&gt;Can someone please explain the intuition behind mixed models in a nutshell? Whenever I read explanations, I get overwhelmed by notation and mathematical jargon. Can someone give me a simple example or motivation to help me understand the concept in a deeper way?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-04-07T03:52:44.653" FavoriteCount="5" Id="55364" LastActivityDate="2013-04-07T08:30:21.107" LastEditDate="2013-04-07T08:30:21.107" LastEditorUserId="930" OwnerUserId="20884" PostTypeId="1" Score="5" Tags="&lt;mixed-model&gt;" Title="Mixed model in simple english" ViewCount="237" />
  
  <row AcceptedAnswerId="55406" AnswerCount="1" Body="&lt;p&gt;I have a non-stationary series and a stationary differenced series, the differences have a (small) non-zero mean. I am wondering how we interpret that in terms of the original series. As the differences will revert to and move around the mean (-0.12) the original series will tend to 'drift' downwards? Is this what is meant by a stochastic drift (as opposed to a deterministic drift)?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-07T15:31:58.790" Id="55392" LastActivityDate="2013-04-07T20:14:49.777" LastEditDate="2013-04-07T20:14:49.777" LastEditorUserId="930" OwnerUserId="23774" PostTypeId="1" Score="2" Tags="&lt;time-series&gt;" Title="How to interpret a differenced series with non zero mean?" ViewCount="161" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I have two different time serie both length = 100 and I need to know what is the best test (non-parametric, if possible), that return how much these two series are same or similar shapes.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here are two examples: first one the two series are very similar shape, not matching perfectly but overall trends and tops and bottoms match.&#10;&lt;img src=&quot;http://i.stack.imgur.com/4GD6D.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Second example, these two serie are different both for tops, bottoms and trends&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/BoPqH.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks all&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; I am testing my series with Spearman's correlation coefficient, but I am not sure if this test can suits my need since Spearman test use a monotonic function, while my series are non-monotonic.&lt;/p&gt;&#10;&#10;&lt;p&gt;Am I correct?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-04-07T18:03:24.897" FavoriteCount="1" Id="55405" LastActivityDate="2014-06-04T08:41:17.103" LastEditDate="2014-06-04T08:41:17.103" LastEditorUserId="23964" OwnerUserId="23964" PostTypeId="1" Score="3" Tags="&lt;time-series&gt;&lt;java&gt;" Title="Looking for a test for shape comparison" ViewCount="252" />
  
  <row AnswerCount="0" Body="&lt;p&gt;In this &lt;a href=&quot;http://tamino.wordpress.com/2013/04/06/sampling-rate/&quot; rel=&quot;nofollow&quot;&gt;blog article&lt;/a&gt;&#10;an example is given of one can use the DFT to detect frequencies much higher than the sample rate.&#10;In the comments sections I asked how it was done, since DFT normally requires evenly sampled data. The author of the blog responded concisely with: &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;One can still define the DFT as proportional to&lt;/p&gt;&#10;  &#10;  &lt;p&gt;$$ X(\nu) = {1 \over N} \sum x_n e^{-i 2 \pi \nu t_n} $$ &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;How would one do that? I am asking for advice to do exactly that.&#10;I can replicate the example if I use the date compensated method of Ferraz-Mello &lt;a href=&quot;http://tamino.wordpress.com/2013/04/06/sampling-rate/&quot; rel=&quot;nofollow&quot;&gt;1&lt;/a&gt; but the blog author says that normally DFT will work using that hint.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://tamino.wordpress.com/2013/04/06/sampling-rate/&quot; rel=&quot;nofollow&quot;&gt;1&lt;/a&gt; &lt;a href=&quot;http://articles.adsabs.harvard.edu/cgi-bin/nph-iarticle_query?1981AJ.....86..619F&amp;amp;data_type=PDF_HIGH&amp;amp;whole_paper=YES&amp;amp;type=PRINTER&amp;amp;filetype=.pdf&quot; rel=&quot;nofollow&quot;&gt;http://articles.adsabs.harvard.edu/cgi-bin/nph-iarticle_query?1981AJ.....86..619F&amp;amp;data_type=PDF_HIGH&amp;amp;whole_paper=YES&amp;amp;type=PRINTER&amp;amp;filetype=.pdf&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-07T19:00:20.523" FavoriteCount="1" Id="55413" LastActivityDate="2014-10-31T18:25:55.620" LastEditDate="2014-10-31T18:25:55.620" LastEditorUserId="21149" OwnerUserId="21149" PostTypeId="1" Score="3" Tags="&lt;frequency&gt;&lt;fourier-transform&gt;" Title="Discrete Fourier Transform and uneven sampling" ViewCount="87" />
  <row Body="&lt;p&gt;A two-part regression may apply here as well - a logistic regression that predicts whether there are zero or 1+ counts, and a second Poisson or NB regression predicting the number of non-zero counts.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-07T20:48:00.017" Id="55427" LastActivityDate="2013-04-07T20:48:00.017" OwnerUserId="13634" ParentId="55424" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;To avoid having to consider the initial steps you can start with the position after four tosses at which point each of the possible states has a probability of $1/16$. &lt;/p&gt;&#10;&#10;&lt;p&gt;You can turn your question into a Markov process with three absorbing states out of sixteen, and the non-absorbing states having a 0.5 chance of going to one of two other states, making the question arithmetic.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Run the Markov process by taking the 133rd or higher power of the transition matrix, and you will find that the three absorbing states have probabilities of about &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;HHTH 0.307692308&lt;/li&gt;&#10;&lt;li&gt;HTHH 0.326923077&lt;/li&gt;&#10;&lt;li&gt;THHH 0.365384615&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="7" CreationDate="2013-04-08T00:51:36.150" Id="55436" LastActivityDate="2013-04-08T00:51:36.150" OwnerUserId="2958" ParentId="54649" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;Let me start by answering your question, then I will add some comments and suggestions.&lt;/p&gt;&#10;&#10;&lt;p&gt;First, I believe that when you say &quot;weight&quot; you actual mean &quot;input&quot;/ &quot;output&quot;. This is because you asked how to transform the time series to weights and how to transform output weights into a prediction. Neural network terminology uses &quot;weight&quot; to mean something else (pat's answer uses the term &quot;weight&quot; correctly).&lt;/p&gt;&#10;&#10;&lt;p&gt;This is what people usually suggest: If your time series looks like&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;X_1, X_2, ..., X_n, ...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;then you do the following:&lt;/p&gt;&#10;&#10;&lt;p&gt;Step 1: Decide how many observations you want to use to make a prediction.&lt;/p&gt;&#10;&#10;&lt;p&gt;Step 2: Decide how many steps forward you want to predict.&lt;/p&gt;&#10;&#10;&lt;p&gt;Both these choices are fixed for the NN.&lt;/p&gt;&#10;&#10;&lt;p&gt;For this example let's say you want to use the last 5 readings to make 2 predictions.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then you will &lt;/p&gt;&#10;&#10;&lt;p&gt;Step 3. Create a neural network with 5 input nodes and 2 output nodes. &lt;/p&gt;&#10;&#10;&lt;p&gt;Step 4. Create your training set with each element consisting of 5 sequential readings as input and the next two readings as output.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here are the first TWO elements of the training set:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Input = X_1, X_2, X_3, X_4, X_5&#10;Output = X_6, X_7&#10;&#10;Input = X_2, X_3, X_4, X_5, X_6&#10;Output = X_7, X_8&#10;&#10;etc.&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Hopefully that answers your question.&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Now for some &lt;a href=&quot;http://samosapedia.com/entries/2882/gyaan&quot; rel=&quot;nofollow&quot;&gt;gyan&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;If your data is noisy e.g. stock ticks, then my feeling is that this will be hard to train. I know I have had bad luck trying to train neural networks on noisy data.&lt;/p&gt;&#10;&#10;&lt;p&gt;So here is another strategy: &lt;/p&gt;&#10;&#10;&lt;p&gt;First model your time series using the ARIMA framework. This views a time series as&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Polynomial base +&#10;Cyclic component +&#10;Bounded randomness&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;(Take a look at the Weka example in pat's answer from this point of view.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Now my feeling (and I am still experimenting) is that the random component(s) are interfering with the training of the NN. So I want to avoid trying to predict them directly.&lt;/p&gt;&#10;&#10;&lt;p&gt;Picture your series data coming in. On every reading you feed it into your ARIMA black box, which figures out the underlying model and then spits out the ARIMA model parameters. So at time 0 you have a set of parameters, and then at time 1 you have an updated set of parameters, etc.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note: The ARIMA black box is slow.&lt;/p&gt;&#10;&#10;&lt;p&gt;Question 1: Would it be possible for a neural network to learn how these parameters change?&#10;  My feeling is that they will change slowly, so this may be doable. &lt;/p&gt;&#10;&#10;&lt;p&gt;Question 2: Could you train a different neural network to discern any patterns in the ARIMA error? I.e. If ARIMA predicts 5.4 and the actual next reading is 5.5, could you train a neural network to figure out that 0.1?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-08T09:27:52.883" Id="55460" LastActivityDate="2013-04-08T09:27:52.883" OwnerUserId="17541" ParentId="29519" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="55474" AnswerCount="1" Body="&lt;p&gt;Could someone give me a step-by-step example of time series prediction using ARIMAX or ARMAX model?  The example doesn't need to be long or complicated. It could be for example forecasting temperature with past data of only ten values (e.g [15, 16, 17, 15, ..., 12])&lt;/p&gt;&#10;&#10;&lt;p&gt;You don't have to calculate autocorrelation etc. step-by-step but I need it to be clear A) What are you doing and why, B) Which data you're using to get what etc.. &lt;/p&gt;&#10;&#10;&lt;p&gt;For example, I would like to see the procedure of finding AR-, MA-, and X- lags, possible integrations, finding the error terms $y_i - \widehat{y}_i = e_i$, solving the coefficients for AR-, MA- and X-parts and making the predictions. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-04-08T10:11:37.680" Id="55465" LastActivityDate="2013-06-21T04:49:31.637" LastEditDate="2013-06-21T04:49:31.637" LastEditorUserId="3826" OwnerUserId="18528" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;autoregressive&gt;" Title="Step-by-step example of predicting time series with ARIMAX or ARMAX model?" ViewCount="3440" />
  <row Body="&lt;p&gt;Ordinal logistic regression with age at death as the response and exercise regime as the predictor would save you having to code the age-ranges as a continuous variable - probably not a good idea with only five categories.&lt;/p&gt;&#10;&#10;&lt;p&gt;Plotting Kaplan-Meier survival curves &amp;amp; carrying out the log-rank test (an instance of survival analysis) would allow you to account for people who dropped out of the study (there must surely be some if you recruited them at 30 years old) or who are still alive.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-04-08T10:37:39.533" Id="55466" LastActivityDate="2013-04-08T11:29:26.827" LastEditDate="2013-04-08T11:29:26.827" LastEditorUserId="17230" OwnerUserId="17230" ParentId="55443" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;That actually depends on the particular solver you are using, and the data you work with. It depends on the data, because the resulting number of support vectors you get depends on the problem at hand. As a consequence, adding/removing certain features might simplify your problem, thus taking less time to train and resulting in fewer support vectors.&lt;/p&gt;&#10;&#10;&lt;p&gt;In any case, this question is answered in detail in the paper &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/papers/bottou_lin.pdf&quot; rel=&quot;nofollow&quot;&gt;Support Vector Machine Solvers&lt;/a&gt;, where a complexity of training a SVM is analyzed, and they also describe and compare different implementations.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hope that helps.&lt;/p&gt;&#10;&#10;&lt;p&gt;ON REPLY TO YOUR FIRST REPLY:&#10;Do you use &lt;a href=&quot;http://home.caltech.edu/~htlin/program/libsvm/&quot; rel=&quot;nofollow&quot;&gt;libsvm&lt;/a&gt;?. If so, you could try alpha seeding. LibSVM supports it. More details &lt;a href=&quot;http://trs-new.jpl.nasa.gov/dspace/bitstream/2014/15583/1/00-1286.pdf&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; and my reply to other question &lt;a href=&quot;http://stats.stackexchange.com/questions/30834/is-it-possible-to-append-training-data-to-existing-svm-models/51993#51993&quot;&gt;here&lt;/a&gt; for a quick intuitive description.&#10;Another approach migth be using &lt;a href=&quot;http://lingpipe-blog.com/2009/04/08/convergence-relative-sgd-pegasos-liblinear-svmlight-svmper/&quot; rel=&quot;nofollow&quot;&gt;stochastic gradient descent on SVM&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-04-08T10:50:54.450" Id="55470" LastActivityDate="2013-04-09T07:48:23.650" LastEditDate="2013-04-09T07:48:23.650" LastEditorUserId="17908" OwnerUserId="17908" ParentId="54775" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;From &lt;a href=&quot;http://en.wikipedia.org/wiki/Pearson_distribution#The_Pearson_type_III_distribution&quot; rel=&quot;nofollow&quot;&gt;wikipedia&lt;/a&gt;:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The Pearson type III distribution is a [shifted] gamma distribution or chi-squared distribution.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Then the answer is: no, it is not the same as the &quot;three-parameter Pearson 5 distribution&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;The Pearson type V distribution is a shifted inverse-gamma distribution. Its density is given in the following entry&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;http://www.mathwave.com/help/easyfit/html/analyses/distributions/pearson5.html&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The parameters of the Pearson type V distribution can be estimated as usual, using your favourite optimisation algorithm. &lt;/p&gt;&#10;&#10;&lt;p&gt;An example in R&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;rm(list=ls())&#10;library(MCMCpack)&#10;&#10;# Simulated data    &#10;set.seed(1000)&#10;n &amp;lt;- 250&#10;data &amp;lt;- rinvgamma(n, shape = 5, scale = 5) -1 &#10;hist(data)&#10;&#10;# log-likelihood&#10;ll &amp;lt;- function(par){&#10;if(par[3]&amp;lt;min(data) &amp;amp; par[1]&amp;gt;0 &amp;amp; par[2]&amp;gt;0) return( -sum(log(dinvgamma(data-par[3],par[1],par[2]))) )&#10;else return(Inf)&#10;}&#10;&#10;# Optimisation step&#10;optim(c(5,5,-1.5),ll)&#10;&#10;# MLE of (alpha,beta,mu), mu is the shift parameter&#10;&#10;optim(c(5,5,-1.5),ll)$par&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2013-04-08T13:39:38.677" Id="55481" LastActivityDate="2013-04-09T09:31:05.600" LastEditDate="2013-04-09T09:31:05.600" LastEditorUserId="24056" OwnerUserId="24056" ParentId="55479" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="55531" AnswerCount="2" Body="&lt;p&gt;I'm trying to find out the name of a distribution that is like negative binomial, only for finite population and without replacement.  Or like Hypergeometric distribution where the last event has to be a success.&lt;/p&gt;&#10;&#10;&lt;p&gt;That is:&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's say we have N balls in an urn, where W of them are white balls and B are black balls.  I want to know what are the chances of needing to draw exactly n balls so it is the first time I got k of the n drawn balls to be black (e.g: that the last ball drawn was black, and a total of k out of n are black).&lt;/p&gt;&#10;&#10;&lt;p&gt;I can't seem to find the name of such a distribution, so if you could also advise me on where to find names of distributions, that would also be lovely.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-08T16:47:19.320" Id="55493" LastActivityDate="2013-04-08T22:44:11.673" OwnerUserId="253" PostTypeId="1" Score="5" Tags="&lt;distributions&gt;&lt;negative-binomial&gt;&lt;hypergeometric&gt;" Title="What is an Hypergeometric distribution where the last event is success?" ViewCount="154" />
  
  <row Body="&lt;p&gt;First let's decide what form of stationarity you are asking about.  There are two types:&lt;/p&gt;&#10;&#10;&lt;p&gt;(1) strict stationarity:  All aspects of a time series behavior are not dependent on time.  i.e. for every m &amp;amp; n the distribtions of $\newcommand{\Cov}{\operatorname{Cov}}Z_t, Z_{t1}, \dots, Z_{t+m+n}$ are the same.&lt;/p&gt;&#10;&#10;&lt;p&gt;(2) weak stationarity (sometimes called covariance stationary): if $\mu, \sigma^2,\gamma$ are unchanged by shifts in time.&lt;/p&gt;&#10;&#10;&lt;p&gt;-strict stationarity and weak stationarity are equivalent for Gaussian processes, since a normal distribution is uniquely characterized by its first two moments.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's assume that we're talking about weak stationarity here because you seem to be asking about correlation.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Formulas for covariance:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\gamma = \Cov(Z_t,Z_{t+k})$ = $E[Z_t -\mu]E[Z_{t+k}-\mu]$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\rho = \frac{\Cov(Z_t,Z_{t+k})}{\sigma_{Z(t)}\sigma_{Z(t+k)}}$&lt;/p&gt;&#10;&#10;&lt;p&gt;So covariance and correlation are both functions of the mean and variance.  &lt;/p&gt;&#10;&#10;&lt;p&gt;So for example, if the series is consistently increasing over time, the sample mean and variance will grow with the size of the sample, and they will always underestimate the mean and variance in future periods. And if the mean and variance of a series are not well-defined, then neither are its correlations with other variables. For this reason you should also be cautious about trying to extrapolate regression models fitted to nonstationary data.&lt;/p&gt;&#10;&#10;&lt;p&gt;Most statistical forecasting methods are based on the assumption that the time series can be rendered approximately stationary (i.e., &quot;stationarized&quot;) through the use of mathematical transformations. A stationarized series is relatively easy to predict: you simply predict that its statistical properties will be the same in the future as they have been in the past!&lt;/p&gt;&#10;&#10;&lt;p&gt;I stole the last two paragraphs from this article from Duke University's website.  They described it better than I could have.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://people.duke.edu/~rnau/411diff.htm&quot; rel=&quot;nofollow&quot;&gt;http://people.duke.edu/~rnau/411diff.htm&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;There are multiple methods for making a time series stationary, which include detrending and first differencing.  You can find many descriptions of both on websites.  The Duke U link above explains first differencing.  Forecasts and means/variances can be made with the detrended model, and then the model can be converted back to the original.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-04-08T17:27:36.437" Id="55496" LastActivityDate="2013-04-08T20:58:35.857" LastEditDate="2013-04-08T20:58:35.857" LastEditorUserId="17230" OwnerUserId="20381" ParentId="55477" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;You're correct in that code needs to be written in a specific way to benefit from having multiple cores/processors. It's sometimes non-trivial to develop a parallel version of an algorithm that actually benefits from multiple cores, so it might be hard to get meaningful speedups in an algorithm you've developed yourself. That said, there are definitely packages around that have parallel versions of some machine learning algorithms.&lt;/p&gt;&#10;&#10;&lt;p&gt;Another use-case, which makes a lot of sense for machine learning applications, is to run multiple copies of single-core-friendly code. For example, if you're doing 10-fold cross validation, you could farm each fold out to a separate core and get the results (roughly) 10x as fast. Similarly, if you're doing a search for some parameters, you could run several copies, each starting from different locations, and then compare the results later.&lt;/p&gt;&#10;&#10;&lt;p&gt;This can be done by hand (just start 10 copies of the program), but a lot of packages (including WEKA, R, and Matlab) have some infrastructure to make it easier. &lt;/p&gt;&#10;&#10;&lt;p&gt;For WEKA, &lt;a href=&quot;http://forums.pentaho.com/showthread.php?69948-Use-of-multiple-cores&quot; rel=&quot;nofollow&quot;&gt;this thread&lt;/a&gt; has some suggestions, but the gist is that you can have one RemoteEngine client running per core and then assign the work that way. The GUI has some built-in multicore support too.&lt;/p&gt;&#10;&#10;&lt;p&gt;R has several options (see &lt;a href=&quot;http://stackoverflow.com/questions/4775098/r-with-a-multi-core-processor&quot;&gt;this StackOverflow thread&lt;/a&gt; and links therein). I haven't played with much personally, but I know a few people who are very happy with Revolutions.&lt;/p&gt;&#10;&#10;&lt;p&gt;Some Matlab functions have built-in support for multicore/multiprocessors (you'll have to look at the docs; it's a little bit random so far). The &lt;a href=&quot;http://www.mathworks.com/products/parallel-computing/&quot; rel=&quot;nofollow&quot;&gt;Parallel Computing toolbox&lt;/a&gt; has some nifty functionality whereby you can make a for-loop run in parallel. You basically do something like&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;matlabpool open;&#10;result = nan(N,1);&#10;parfor ii = 1:N&#10;  result(ii) = some_function(ii);&#10;end&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This automatically distributes calls to some_function across cores and collects the results for you. There are---of course---some caveats, but it's pretty close to that simple, and it gracefully degrades to running sequentially if run on a computer without the toolbox. I suspect that you could do something similar with the right packages in R, but I'm not sure.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-04-08T20:33:21.190" Id="55517" LastActivityDate="2013-04-08T20:33:21.190" OwnerUserId="7250" ParentId="55495" PostTypeId="2" Score="0" />
  
  
  <row Body="&lt;p&gt;First, note that $P(x)\propto e^{-bx}$ which, if $x$ were continuous, would be related to an exponential distribution. Then, what you can do is to simulate from a truncated exponential distribution  and take the &lt;code&gt;floor()&lt;/code&gt; (integer part) of the observations.&lt;/p&gt;&#10;&#10;&lt;p&gt;The cdf of a truncated exponential is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$F(x;n,b)= \dfrac{1-e^{-bx}}{1-e^{-bn}}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Then, if we make $F(x;n,b)=u$, we obtain that $x=-\dfrac{1}{b}\log[1-u(1-e^{-bn})]$. If $bn$ is large, then $e^{-bn}\approx 0$ which suggest to approximate $x\approx -\dfrac{1}{b}\log[1-u]$.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;rweirdp &amp;lt;- function(ns,n,b){&#10;u &amp;lt;- runif(ns)&#10;samp &amp;lt;- - log(1-u*(1-exp(-n*b)))/b&#10;return(floor(samp))&#10;}&#10;&#10;rweirdp(1000,10,1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="8" CreationDate="2013-04-08T23:36:52.227" Id="55535" LastActivityDate="2013-04-09T08:44:26.610" LastEditDate="2013-04-09T08:44:26.610" LastEditorUserId="24056" OwnerUserId="24056" ParentId="55533" PostTypeId="2" Score="4" />
  
  
  
  
  <row Body="&lt;p&gt;Yes, you can correlate the two factors; factors are (attempts to) derive the latent variables that are inherent in the answers to questions (in this case, your Likert items). Once you have these latent factors, they can be treated as interval level. &lt;/p&gt;&#10;&#10;&lt;p&gt;A different question is the proper way to do the factor analysis on Likert items. Since Likert items themselves are somewhere between ordinal and interval, some people say to not use Pearson correlations to do this. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-09T09:49:52.487" Id="55568" LastActivityDate="2013-04-09T09:49:52.487" OwnerUserId="686" ParentId="55561" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;What @whuber and I were saying to you in the comments are equivalent things. @whuber pointed out that the text you were reading makes points column vectors. I stuck to your own original notation where points are row vectors (this way of presenting is more common). When points are columns, thansposed (&quot;T&quot;, or just ' in my notation) multiplier is the right one; when they are rows, it is the left one. Instead of multiplying separate vectors, it's more convenient to multiply whole matrices. See it with your data (matrix A = your &quot;Ck&quot;):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;****** Points are rows, variables are columns [more common] ******&#10;A&#10;  1  2  3&#10;  4  5  6&#10;&#10;Column-centered A&#10;  -1.500000000  -1.500000000  -1.500000000&#10;   1.500000000   1.500000000   1.500000000&#10;&#10;A'A, the scatter matrix&#10;   4.500000000   4.500000000   4.500000000&#10;   4.500000000   4.500000000   4.500000000&#10;   4.500000000   4.500000000   4.500000000&#10;&#10;****** Points are columns, variables are rows [that's how in your book] ******    &#10;A&#10;  1  4&#10;  2  5&#10;  3  6&#10;&#10;Row-centered A&#10;  -1.500000000   1.500000000&#10;  -1.500000000   1.500000000&#10;  -1.500000000   1.500000000&#10;&#10;AA', the scatter matrix&#10;   4.500000000   4.500000000   4.500000000&#10;   4.500000000   4.500000000   4.500000000&#10;   4.500000000   4.500000000   4.500000000&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2013-04-09T12:28:28.913" Id="55576" LastActivityDate="2013-04-09T12:38:13.900" LastEditDate="2013-04-09T12:38:13.900" LastEditorUserId="3277" OwnerUserId="3277" ParentId="55480" PostTypeId="2" Score="4" />
  <row AnswerCount="2" Body="&lt;p&gt;We'd like to express how much better A performs when combined with B. Is it as simple as comparing the averages of the two groups?&lt;/p&gt;&#10;&#10;&lt;p&gt;When tested, widget A has a range of results from 36 to 43 with an AVG of 40(x).&#10;When widget B is tested together with product A, the results have a range of 27 to 34 and an average of 30(y).&lt;/p&gt;&#10;&#10;&lt;p&gt;If tested a lot, the tails might rarely overlap.&lt;/p&gt;&#10;&#10;&lt;p&gt;0 good&#10;44 bad. &lt;em&gt;very bad.&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; A)  0  .................|..|.||||||...|.  44&#10; AB) 0  ....|.|||||||.|..................  44&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;What's the best way to calculate and represent the data? Simple would be nice (i.e. &quot;B makes A 10% Better!&quot;)&lt;/p&gt;&#10;&#10;&lt;p&gt;Current thinking is:&#10;Percent Reduction =(x-y)/x&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-04-09T16:57:45.000" Id="55599" LastActivityDate="2013-04-10T03:03:26.463" LastEditDate="2013-04-10T00:19:02.730" LastEditorUserId="805" OwnerUserId="24114" PostTypeId="1" Score="0" Tags="&lt;t-test&gt;" Title="Percent reduction between two groups" ViewCount="116" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have one conceptual (but sort of vague) question regarding distribution fitting.&lt;/p&gt;&#10;&#10;&lt;p&gt;How many observations one would need to best fit any statistical distribution to given data. Like I am dealing with loss data and in one situation, I had only three observations still I could fit Normal distribution, which in reality I think is absurd. One more data and the scenario may change.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hence, is there any thumb rule for minimum number of observations in order to have reasonably stable distribution fitting?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanking in advance&lt;/p&gt;&#10;&#10;&lt;p&gt;Akshata&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-04-09T18:19:55.517" FavoriteCount="1" Id="55612" LastActivityDate="2013-04-09T19:29:28.523" OwnerUserId="24093" PostTypeId="1" Score="0" Tags="&lt;distributions&gt;" Title="Minimum no. of observations required for statistical distribution fitting!" ViewCount="2709" />
  <row Body="&lt;p&gt;Delta R2 is the change in R2 between two equations. Usually you see this come up when doing hierarchical regression with more than one step. For example, Step 1 R2 = .25 and Step 2 deltaR2 = .10. This would mean that Step 2 added .10 beyond the .25 of step 1, for a total of R2 = .35.&lt;/p&gt;&#10;&#10;&lt;p&gt;Edit: I should also mention that the deltaR2 will be associated with its own F value. This value will indicate whether the increase in R2 (deltaR2) is statistically significantly greater than no increase. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-09T19:16:12.663" Id="55616" LastActivityDate="2013-04-09T20:13:29.567" LastEditDate="2013-04-09T20:13:29.567" LastEditorUserId="3262" OwnerUserId="3262" ParentId="55615" PostTypeId="2" Score="3" />
  
  
  
  
  <row AcceptedAnswerId="55674" AnswerCount="2" Body="&lt;p&gt;I have a situation where we are calculating a customer life time value using some binary variables (ie have they purchased xyz widget?, etc.) and multiplying by a number that we believe approximates the value of these behaviors.  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;CLV = x1 * 500 + x2 * 300 + x3 * 100&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The problem comes when a colleague wants to do a logistic regression to determine which customers will have CLV's above a certain threshold (ie &lt;code&gt;if CLV &amp;gt;= 1000, y = 1, else y = 0&lt;/code&gt;)and use &lt;code&gt;x1&lt;/code&gt;,&lt;code&gt;x2&lt;/code&gt; and &lt;code&gt;x3&lt;/code&gt; as variables.  I'm pretty certain this is a no-no as it's basically leaking info to the model but I'm having a hard time explaining this to my colleague.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Has anyone dealt with this issue before or know of a good source to explain this to someone who is math literate and intelligent but just doesn't have a ton of experience with modeling?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-04-10T04:06:01.703" Id="55648" LastActivityDate="2013-04-10T09:27:28.110" OwnerUserId="7411" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;logistic&gt;&lt;experiment-design&gt;" Title="Explain overfitting / data leakage to a colleague" ViewCount="172" />
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to learn SVM Classifier using some amount of training data.&#10;and then I am predicting for another set (independent from training data)&lt;/p&gt;&#10;&#10;&lt;p&gt;I tried random C values from 0.000001 to 50000000 and the accuracy still remains the same.&lt;/p&gt;&#10;&#10;&lt;p&gt;I tried changing the training and test sets, it still is giving the same error&lt;/p&gt;&#10;&#10;&lt;p&gt;Kindly see the code here (Dont know if it makes sense), but everytime I change value of C and run it, nothing happens&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;input = svminput(600:1900,:);&#10;input = double(input);&#10;testd = svminput(1:500,:);&#10;testd = double(testd);&#10;hot2 = hot(600:1900);&#10;testlabel = hot(1:500);&#10;&#10;svmModel = trainSVM(input,hot2,5000000);// C value is 5000000 in this eg.&#10;prediction = classifySVM(svmModel,testd);&#10;sum(abs(prediction -testlabel'))/500&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="5" CreationDate="2013-04-10T06:23:59.953" Id="55660" LastActivityDate="2013-04-10T06:23:59.953" OwnerUserId="24135" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;svm&gt;&lt;parametric&gt;" Title="Increasing the value of C in SVM (LibSVM) is not changing the accuracy at all" ViewCount="336" />
  <row Body="&lt;p&gt;Maybe you can try &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lm(formula = Temp~Site*(Date + Year))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In this way you will have two interactions with the Site, and there will be no interaction between Date and year&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-10T07:40:12.720" Id="55666" LastActivityDate="2013-04-10T07:40:12.720" OwnerUserId="24091" ParentId="19060" PostTypeId="2" Score="0" />
  
  <row AcceptedAnswerId="55708" AnswerCount="1" Body="&lt;p&gt;Let's say I'm fitting data for example to ARMA(1,1)-model:&lt;/p&gt;&#10;&#10;&lt;p&gt;$x_t = \phi x_{t-1} + \epsilon_t + \theta \epsilon_{t-1}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now I estimate the parameters $\phi$ and $\theta$ and solve some values for them, e.g. &lt;/p&gt;&#10;&#10;&lt;p&gt;$x_t = 0.7 x_{t-1} + \epsilon_t + 0.8 \epsilon_{t-1}$ (I just made the numbers up)&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's say this the best model for my problem at hand and I begin forecasting. &lt;/p&gt;&#10;&#10;&lt;p&gt;So I make the forecast &lt;/p&gt;&#10;&#10;&lt;p&gt;$x_{t+1} = 0.7 x_{t} + \epsilon_{t+1} + 0.8 \epsilon_{t}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now where I get confused is with the future value of the error term $\epsilon_{t+1}$. How can I solve this value? Should I predict the residuals also or should I use the expected value of $\epsilon_{t+1}$, which should be 0. Hope my question is clear =) &lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you for any help =) &lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT:&lt;/p&gt;&#10;&#10;&lt;p&gt;About the value of $\epsilon_{t+1}$. Should I estimate the value of $\epsilon_{t+1}$ by assuming (as in literature normally is assumed) that the noise process $\epsilon_t$ is normally distributed $\epsilon_t$ ~ $iidN(0,\sigma_{\epsilon}^2)$ and then use estimation techniques (Least squares, Maximum likelihood, Yule-Walker) to estimate the value for noise process variance $\widehat{\sigma}_{\epsilon}^2$ and then just evaluate value for $\epsilon_{t+1}$ ~ $iidN(0,\widehat{\sigma}_{\epsilon}^2)$ from the estimated Gaussian distribution?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-04-10T10:55:56.047" FavoriteCount="1" Id="55701" LastActivityDate="2014-09-04T17:42:12.843" LastEditDate="2013-04-10T13:37:07.530" LastEditorUserId="18528" OwnerUserId="18528" PostTypeId="1" Score="0" Tags="&lt;time-series&gt;&lt;residuals&gt;" Title="Confusion about error term in ARMA-model when predicting future values" ViewCount="572" />
  
  
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;How do I go about inferring anything from a graph like this?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;You can a get a sense of the noise level and that there is a general trend. That will suggest fitting a line or other curve, possibly a &quot;non-parametric&quot; curve like a spline or loess.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Given the erratic Y values is there a common approach to somehow&#10;  normalizing these?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Most fitting methods are designed to handle random noise just fine. You should check that after each fit by looking at the residuals of the fit to look for non-randomness.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;It also doesn't seem like a situation where you'd just kinda draw a&#10;  line of best fit and go with that...&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;No, but it can be a useful starting point. Even if the trend is not purely linear, a line can give you an estimate of an rough trend.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-10T16:22:30.913" Id="55736" LastActivityDate="2013-04-10T16:22:30.913" OwnerUserId="1191" ParentId="55544" PostTypeId="2" Score="2" />
  
  
  
  
  <row AnswerCount="3" Body="&lt;blockquote&gt;&#10;  &lt;p&gt;$X_1$, $X_2$, $X_3$ are independent random variables, each with an exponential distribution, but with means of $2.0, 5.0, 10.0$ respectively. Let $Y$= the smallest or minimum value of these three random variables. Derive and identify the distribution of $Y$. (The distribution function may be useful). &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;How do I solve this question? Do I plug in each mean to the exponential distribution? I would appreciate it if someone could explain this to me, thanks.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-04-10T20:56:53.927" Id="55767" LastActivityDate="2014-02-22T17:53:54.047" LastEditDate="2013-08-26T01:51:32.337" LastEditorUserId="805" OwnerUserId="24173" PostTypeId="1" Score="2" Tags="&lt;probability&gt;&lt;self-study&gt;&lt;random-variable&gt;" Title="How to calculate the distribution of the minimum of multiple exponential variables?" ViewCount="621" />
  <row AcceptedAnswerId="55890" AnswerCount="1" Body="&lt;p&gt;I have some data resulting from a simulation that consists of several groups, each containing a single real datapoint and a variable number of matched controls. I take the rank of each real value within its distribution of controls, and normalize the ranks to between 0 and 1.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now I would like to test whether, across all groups, the ranks of my real datapoints are different to what I would expect by chance. However, I'm unsure about how to do this, since the distribution of possible normalized ranks across groups is discrete with uneven quantization (due to the variable number of control datapoints in each group).&lt;/p&gt;&#10;&#10;&lt;p&gt;I expect the CDF for the null distribution to look a bit like this:&#10;&lt;img src=&quot;http://i.stack.imgur.com/wQj7s.png&quot; alt=&quot;Simulated CDF for normalized ranks&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I have considered the possibility of bootstrapping a null distribution by drawing randomly from each set of possible normalized ranks, then doing a 2-sample K-S test on the real and null distributions. Is this the way to go, or would there be a more appropriate test?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; I've also posted a more complete description of the problem I'm trying to solve &lt;a href=&quot;http://stats.stackexchange.com/questions/55928/hypothesis-test-on-data-with-confounding-spatial-clustering&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-04-10T22:54:07.863" FavoriteCount="1" Id="55778" LastActivityDate="2013-04-12T15:02:07.770" LastEditDate="2013-04-12T15:02:07.770" LastEditorUserId="22156" OwnerUserId="22156" PostTypeId="1" Score="2" Tags="&lt;hypothesis-testing&gt;&lt;distributions&gt;&lt;discrete-data&gt;&lt;kolmogorov-smirnov&gt;&lt;ranks&gt;" Title="Equivalent to K-S test on discrete data with uneven quantization" ViewCount="142" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I am working on a problem in the online advertising space. I am trying to identify consumers similar to the set of consumers who have bought a product in the past (have 'converted'). If I can identify 'similar' users, I can deliver them targeted messaging with the goal of turning them into paying customers.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have approached this as a predictive modeling problem. I have been using KNN and logistic regression to determine major factors that influence the target variable ('being a converter'). &lt;/p&gt;&#10;&#10;&lt;p&gt;I feel that I am approaching this wrong, as in my set of 'non-converters' I have a set of people who are quite similar in every way to 'converters'. So if I built a perfect model it would not be helpful in detecting 'non-converters' who should be 'converters'. &lt;/p&gt;&#10;&#10;&lt;p&gt;My ultimate goal is to detect the non-converters who behave similarly to converters. Should my goal with the model be to predict converters with high accuracy, and argue that users classified as converters who were actually non-converters fall into the set of consumers I should be targeting? &lt;/p&gt;&#10;&#10;&lt;p&gt;Can someone help push me in the right direction? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-11T00:46:25.770" Id="55785" LastActivityDate="2013-04-11T02:14:39.373" LastEditDate="2013-04-11T02:14:39.373" LastEditorUserId="14741" OwnerUserId="24184" PostTypeId="1" Score="1" Tags="&lt;classification&gt;&lt;feature-selection&gt;&lt;basic-concepts&gt;&lt;similarities&gt;" Title="Finding similar users" ViewCount="45" />
  <row AcceptedAnswerId="55789" AnswerCount="1" Body="&lt;p&gt;I've encountered this question:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/LmgEf.jpg&quot; alt=&quot;http://i.stack.imgur.com/zlE1w.jpg&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;And got the answer here:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/W2jMY.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;However, what I don't quite understand is how are the two conditions derived from the question in the first place.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-04-11T01:24:54.110" Id="55788" LastActivityDate="2013-04-11T02:34:33.950" LastEditDate="2013-04-11T02:34:33.950" LastEditorUserId="7290" OwnerUserId="24136" PostTypeId="1" Score="0" Tags="&lt;probability&gt;&lt;self-study&gt;&lt;pdf&gt;" Title="Interpreting probability conditions from question" ViewCount="47" />
  <row Body="&lt;p&gt;If you set up the data in one long column with A and B as a new column, you then can run your regression model as a GLM with a continuous time variable and a nominal &quot;experiment&quot; variable (A, B).  The output of the ANOVA will give you the significance of the difference between the parameters.  &quot;intercept' is the common intercept and the &quot;experiment&quot; factor will reflect differences between the intercepts (actually overall means) between the experiments. the &quot;Time&quot; factor will be the common slope, and the interaction is the difference between the experiments with respect to the slope.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have to admit I cheat (?) and run the models separately first to get the two sets of parameters and their errors and then run the combined model to acquire the differences between the treatments (in your case A and B)...&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-04-11T02:58:22.503" Id="55790" LastActivityDate="2013-04-11T02:58:22.503" OwnerUserId="24187" ParentId="45528" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;There is a common phenomenon among many statisticians and non-statisticians called &lt;em&gt;leptokurtophobia&lt;/em&gt;:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The symptoms of leptokurtophobia are (1) routinely asking if your data are normally distributed and (2) transforming your data to make them appear to be less leptokurtic and more “mound shaped.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.qualitydigest.com/inside/twitter-ed/do-you-have-leptokurtophobia.html#&quot; rel=&quot;nofollow&quot;&gt;http://www.qualitydigest.com/inside/twitter-ed/do-you-have-leptokurtophobia.html#&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I think this guy took it to the next level.&lt;/p&gt;&#10;&#10;&lt;p&gt;In some cases people assume normallity of the observations only because it simplifies their calculations in spite of clear evidence of departures from this assumption.&lt;/p&gt;&#10;&#10;&lt;p&gt;Besides possible misconceptions or other errors in the modelling, the immediate implications are:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;They are inducing a bias by selecting people in order to obtain data where they can use the models they &quot;know&quot;. Ideally, statistics work the other way around: collecting data with a purpose in mind $\rightarrow$ statistical modelling.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The conclusions drawn have to be changed to something like: if we sample in such a way that we obtain a normal sample, then ... Which may not be that useful.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;There are many ways to modelling departures from normallity but specially non-statisticians are affraid of using them since they are typically more complicated.&lt;/p&gt;&#10;&#10;&lt;p&gt;Of course, there are cases where the Central Limit Theorem or some other (asymptotic) result make the assumption of normallity rather mild. However, we have to keep in mind that this is not always the case.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-04-11T07:58:30.720" Id="55804" LastActivityDate="2013-04-11T08:35:57.007" LastEditDate="2013-04-11T08:35:57.007" LastEditorUserId="24197" OwnerUserId="24197" ParentId="55803" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;It's OK to put a time term and auto-regressive terms in the same model. But, the model may not be meaningful when $\epsilon_t$ is non stationary. &lt;/p&gt;&#10;&#10;&lt;p&gt;If the series is stationary after removing deterministic part, one can add deterministic part in the time series analysis.&lt;/p&gt;&#10;&#10;&lt;p&gt;In your case if $y_t$ is stationary after removing the time trend, you can proceed with your model.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-11T11:26:16.780" Id="55818" LastActivityDate="2013-04-11T11:26:16.780" OwnerUserId="7788" ParentId="55809" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to identify new patterns after analyzing a number of URLs. So let's say, I am investigating the hypothetical website Yoohle.com and their URLs have the following structure. &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;domain = yoohle.com&lt;/li&gt;&#10;&lt;li&gt;q= search phrase&lt;/li&gt;&#10;&lt;li&gt;lan= language used&lt;/li&gt;&#10;&lt;li&gt;pr= partner_id&lt;/li&gt;&#10;&lt;li&gt;br= browser_id&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;so a sample url will look like this &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;www.yoohle.com/test_folder/test_page?q=hello+world&amp;amp;lan=en&amp;amp;pr=stackoverflow&amp;amp;br=chrome&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;If I am investigating the web traffic of this website and seeing abnormal increase month over month, I would like to find out what's causing this. In this example I can just parse out the URL and look at the pr= value since it will tell me if there is a new partnership (maybe stackoverflow is going to be powered by yoohle.com and that drives the increase etc.)&lt;/p&gt;&#10;&#10;&lt;p&gt;The question is, how can I build something robust that can compare 2 (or more) months and tell me exactly what's driving the increase. I want to get something like, &quot;we are seeing an increase and it is driven by the following pattern&quot;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;www.yoohle.com/test_folder/test_page%pr=stackoverflow%&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The tricky part is, you do not know anything about what the tokens mean unlike this example since I will not know what token stands for partner_id. Another issue is, if we look at token by token, this will be misleading because lan=en will also go up with a new partner assuming the users will still have English as the language.&lt;/p&gt;&#10;&#10;&lt;p&gt;My idea is to analyze the tokens by looking at all the combinations but it is very costly, (4! in this example and probably 10+! for other websites). Also analyzing tokens itself is not going to solve the problem since I still need to analyze the values of the tokens. &lt;/p&gt;&#10;&#10;&lt;p&gt;I tried k-means clustering, apriori algorithm did some research on URL/text mining but could not get what I want. Any ideas about how to approach building an algorithm will be beneficial. &lt;/p&gt;&#10;&#10;&lt;p&gt;Imagine that you are seeing realtime data, so we are talking about analyzing around 100K URLs in a given month.  &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-04-11T14:46:29.120" Id="55841" LastActivityDate="2013-04-11T14:46:29.120" OwnerUserId="24188" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;bayesian&gt;&lt;clustering&gt;&lt;modeling&gt;&lt;data-mining&gt;" Title="How to identify a new pattern in a URL with a machine learning algorithm (Text mining)" ViewCount="107" />
  <row Body="&lt;p&gt;You could use resampling and order statistics. From your dataset of 100 points, select say, 100000 samples of size 100 with replacement. Find the average class size in each case. Order the averages from smallest to largest. Remove the bottom 2500 (2.5%) and top 2500 (2.5%). Now your 95% confidence interval is the range from the lowest remaining average to the highest remaining average. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you need a quick back-of-the-envelope answer just order your 100 data points and remove the bottom 2 points and top 2. That should give you a rough 96% confidence interval.&lt;/p&gt;&#10;&#10;&lt;p&gt;You should also consider using a one-sided interval since you have a hard constraint on the upper end. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-11T14:48:29.983" Id="55842" LastActivityDate="2013-04-11T14:48:29.983" OwnerUserId="24073" ParentId="55598" PostTypeId="2" Score="2" />
  
  
  <row AcceptedAnswerId="55881" AnswerCount="1" Body="&lt;p&gt;In my paper, in the section methodology, I have a subsection called &lt;strong&gt;dependent variable&lt;/strong&gt;. But since I am performing a logistic regression, there seem to be 2 (or even 3) things that could be named the dependent variable:&lt;/p&gt;&#10;&#10;&lt;p&gt;First you have your &lt;code&gt;Y&lt;/code&gt;, which is a measured variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;Second there is the predicted&lt;code&gt;log-odds of the probability&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Third there is the predicted &lt;code&gt;probability&lt;/code&gt; (easily calculated from the log-odds).&lt;/p&gt;&#10;&#10;&lt;p&gt;I am wondering which one to describe in my paper in this section? I prefer not to talk about the log-odds, since it makes no sense to make things more complicated at that stage of the paper. I wanted to say the actual Y, cause that's what people are used to in linear regression, but it seems the Y is not &lt;em&gt;dependent&lt;/em&gt; of the independent variables, its dependent of the probability. Could I safely talk about the probability, without breaking any rules?&lt;/p&gt;&#10;&#10;&lt;p&gt;Does anyone have any experience with this? Maybe an example of a research paper/article.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-11T22:20:29.297" Id="55880" LastActivityDate="2013-04-11T22:23:51.100" OwnerUserId="18334" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;logistic&gt;&lt;reporting&gt;" Title="Research paper - section dependent variable for logistic regression" ViewCount="95" />
  
  
  
  <row Body="&lt;p&gt;A sufficiently large number of observations for an analysis may require changes in the way the data analysis proceeds, or in the way it is understood.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Some examples where the process may need to be adapted are:  &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;special strategies may be required if there are more data than can fit in a computer's memory,  &lt;/li&gt;&#10;&lt;li&gt;the analyst may need to pay attention to the computational efficiency of different optimization algorithms,  &lt;/li&gt;&#10;&lt;li&gt;consideration needs to be given to how to effectively visualize the data, if standard plots (e.g., a scatterplot) would just display a large black spot due to overlapping points.  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;A common example of a case where analysts conceptualize an aspect of the process differently concerns statistical significance.  With sufficient data, any difference, no matter how trivial, will be 'significant'.  This fact leads many analysts to view findings of significance differently than when smaller data sets are available.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-12T03:43:08.260" Id="55902" LastActivityDate="2013-07-21T03:50:05.603" LastEditDate="2013-07-21T03:50:05.603" LastEditorUserId="7290" OwnerUserId="7290" PostTypeId="5" Score="0" />
  <row AnswerCount="3" Body="&lt;p&gt;Why is it the case that when I run logistic regression with one categorical predictor, my regression is not significant whereas if I run the logistic regression with the same variable except it is continuous, the logistic regression automatically becomes significant?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-12T05:43:36.953" FavoriteCount="1" Id="55909" LastActivityDate="2013-10-08T12:52:30.117" LastEditDate="2013-07-25T15:20:10.110" LastEditorUserId="17230" OwnerUserId="20884" PostTypeId="1" Score="5" Tags="&lt;regression&gt;&lt;categorical-data&gt;&lt;continuous-data&gt;" Title="Logistic regression: categorical predictor vs. quantitative predictor" ViewCount="600" />
  
  <row AcceptedAnswerId="55919" AnswerCount="1" Body="&lt;p&gt;I have previously posted a question for the same &lt;a href=&quot;http://stats.stackexchange.com/questions/30664/suggestion-about-morphometric-analysis-in-r-using-lm-or-lme&quot;&gt;dataset&lt;/a&gt; but now I had somme issues with the models and I wanted to re-phrase my question.&lt;/p&gt;&#10;&#10;&lt;p&gt;My dataset contains 50 morphometric characters (which we reduced with factor analysis or pca to few common components) measured on the roe deer skull. I want a model for predicting skull dimensions from the absolute areas of the forest and plowland in habitats they live in. My first idea was to enter absolute areas as predictors but I was advised to use them as proportions and to add population abundance as a predictor in the model. &lt;/p&gt;&#10;&#10;&lt;p&gt;I know that GLM (with binomial error structure) is great for proportion data but I am unsure how to specify such a model in R. In fact I am also unsure how to enter proportions (I can calculate percentages, but am wondering if they must total to 1, 100%). &lt;/p&gt;&#10;&#10;&lt;p&gt;Any ideas?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;SAMPLE DATASET&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;   Factor1 population manage foraging height biome abundance area  forest plough &#10; -0.6033788 ADA_BEC   best    fields   low    agS    1500    73154  61154  12000&#10;  0.3250981 ADA_BEC   best    fields   low    agS    1500    73154  61154  12000&#10;  0.5577059 ADA_BEC   best    fields   low    agS    1500    73154  61154  12000&#10; -0.1596194 PM        good    plains   med    kgS    23980   856251 89499  579870&#10; -1.3089952 PM        good    plains   med    kgS    23980   856251 89499  579870&#10; -2.1693392 SP        poor    mount    high   hgS    2500    65872  38000  47098&#10; -0.9669080 SP        poor    mount    high   hgS    2500    65872  38000  47098&#10; -1.8857842 SP        poor    mount    high   hgS    2500    65872  38000  47098&#10;  0.7242678 DKN       best    fields   plain  agS    65908   989981 181133 12400&#10;  1.6815373 DKN       best    fields   plain  agS    65908   989981 181133 12400&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Area is the total area and forest is the forest and plough area (which don`t add up to the total area as there are meadowlands and urban areas, we have this data too). These areas are in expressed in Ha. Factor1 are factor axis scores and abundance is the total number although we have density (individual per area) also.   &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-12T08:15:16.990" FavoriteCount="2" Id="55916" LastActivityDate="2013-04-13T23:40:20.690" LastEditDate="2013-04-12T10:42:21.400" LastEditorUserId="6844" OwnerUserId="6844" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;generalized-linear-model&gt;" Title="Generalized LM or LM in ecological dataset" ViewCount="115" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I am having a problem using the &lt;code&gt;2l.norm&lt;/code&gt; method of multilevel imputation in &lt;code&gt;mice&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Unfortunately I cannot post a reproducible example because of the size of my data - when I reduce the size, the problem vanishes.&lt;/p&gt;&#10;&#10;&lt;p&gt;For a particular variable, &lt;code&gt;mice&lt;/code&gt; produces the following errors and warnings:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Error in chol.default(inv.sigma2[class] * X.SS[[class]] + inv.psi) : &#10;  the leading minor of order 1 is not positive definite&#10;In addition: Warning messages:&#10;1: In rgamma(n.class, n.g/2 + 1/(2 * theta), scale = 2 * theta/(ss *  :&#10;  NAs produced&#10;2: In rgamma(1, n.class/(2 * theta) + 1, scale = 2 * theta * H/n.class) :&#10;  NAs produced&#10;3: In rgamma(1, n.class/2 - 1, scale = 2/(n.class * (sigma2.0/H - log(sigma2.0) +  :&#10;  NAs produced&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;If I use the &lt;code&gt;2l.pan&lt;/code&gt;, &lt;code&gt;norm&lt;/code&gt; or &lt;code&gt;pmm&lt;/code&gt; methods, the problem does not occur.&lt;/p&gt;&#10;&#10;&lt;p&gt;The variable has the following distribution:&#10;&lt;img src=&quot;http://i.stack.imgur.com/brJBH.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's &#10;   50.0   117.0   136.0   136.7   155.0   249.0    3124 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Also, the class sizes have the following distribution:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. &#10;   3.00   50.00   80.00   88.52  111.00  350.00 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2013-04-12T11:59:32.393" Id="55925" LastActivityDate="2014-02-12T09:35:42.770" LastEditDate="2013-04-12T12:08:05.267" LastEditorUserId="7486" OwnerUserId="7486" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;missing-data&gt;&lt;multiple-imputation&gt;&lt;mice&gt;" Title="&quot;the leading minor of order 1 is not positive definite&quot; error using 2l.norm in mice" ViewCount="2474" />
  <row Body="&lt;p&gt;You might just want to calculate the confidence interval.&lt;/p&gt;&#10;&#10;&lt;p&gt;Zr has a nearly normal distribution with variance&lt;/p&gt;&#10;&#10;&lt;p&gt;sz² = 1 / n - 3.&lt;/p&gt;&#10;&#10;&lt;p&gt;Using these statistics we can construct a level C confidence interval for the population value &lt;/p&gt;&#10;&#10;&lt;p&gt;Zr +/- z* / Square Root(n - 3)&lt;/p&gt;&#10;&#10;&lt;p&gt;where z* is the critical value from the normal distribution such that the area between -z* and z* is equal to C. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-12T14:34:00.073" Id="55936" LastActivityDate="2013-04-12T14:34:00.073" OwnerUserId="21260" ParentId="55930" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;Regarding your first question, part 1:&lt;/p&gt;&#10;&#10;&lt;p&gt;Linear regression is &quot;just-identified&quot; in SEM. This is also called &quot;fully-saturated.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;A more simple example with 2 IVs and 1 DV gives:&lt;/p&gt;&#10;&#10;&lt;p&gt;3 variances and 3 covariances in the covariance matrix. This is your DF for SEM = 6. &lt;/p&gt;&#10;&#10;&lt;p&gt;Your regression includes 2 regression beta coefficients, 2 IV variances, 1 covariance between the IVs (you may or may not realize this is in the model, but it is), and 1 error variance = 6 parameters&lt;/p&gt;&#10;&#10;&lt;p&gt;6 DF = 6 Parameters&lt;/p&gt;&#10;&#10;&lt;p&gt;Unless constraints are made, regression models in SEM are always fully saturated and no assessment of model fit is possible. &lt;/p&gt;&#10;&#10;&lt;p&gt;Regarding Part 2:&lt;/p&gt;&#10;&#10;&lt;p&gt;I agree with Patrick that these are nested models and you can &quot;test&quot; the constraints with a chi2 test. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-12T16:17:10.187" Id="55948" LastActivityDate="2013-04-12T19:03:27.053" LastEditDate="2013-04-12T19:03:27.053" LastEditorUserId="16049" OwnerUserId="16049" ParentId="55378" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;In your case a single kernel model has $D+D(D-1)/2 = 465$ degrees of freedom, $30$ from the mean, and $435$ from the elements of the covariance matrix.  Thus you only have less than 2 samples per degree of freedom, and a two kernel model would have less than one data sample per degree of freedom.  A rule of thumb would be to require at least 3-5, and prefer $&amp;gt;10$, samples per degree of freedom for model estimation.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Your specific case is probably one of overfitting: the mixture model has more degrees of freedom, and thus can better fit the training data, including statistical fluctuations in it.  Sometimes this is referred to as &quot;fitting the noise&quot; or just &quot;overfitting&quot;.  The lower number of degrees of freedom in the single kernel model&#10;provided better generalization.&lt;/p&gt;&#10;&#10;&lt;p&gt;More generally, this is problem in model selection.&lt;/p&gt;&#10;&#10;&lt;p&gt;Globally, it's better to use a mixture of Gaussians when that kind of distribution can more accurately reflect the true distribution of your data.  Of course, you don't actually know the true distribution of the data.&lt;/p&gt;&#10;&#10;&lt;p&gt;In some cases you can identify whether a single kernel, or a mixture of kernels are required by qualitative data analysis.  Usually features like the &lt;a href=&quot;http://en.wikipedia.org/wiki/Bayesian_information_criterion&quot; rel=&quot;nofollow&quot;&gt;Bayesian Information Criterion&lt;/a&gt; (BIC) or &lt;a href=&quot;http://en.wikipedia.org/wiki/Akaike_information_criterion&quot; rel=&quot;nofollow&quot;&gt;Akaike Information Criterion&lt;/a&gt; are used to formalize the idea of balancing the goodness of fit to your limited data sample against the degrees of freedom that your model provides.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-04-12T16:22:04.507" Id="55950" LastActivityDate="2013-04-12T16:33:49.483" LastEditDate="2013-04-12T16:33:49.483" LastEditorUserId="16859" OwnerUserId="16859" ParentId="55933" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;Whenever you deal with huge amounts of data and you want to solve a supervised learning task with a feed-forward neural network, solutions based on backpropagation are much more feasible. The reason for this is, that for a complex neural network, the number of free parameters is very high. One industry project I am currently working on involves a feed-forward neural network with about 1000 inputs, two hidden layers @ 384 neurons each and 60 outputs. This leads to 1000*384 + 384*384 + 384*60 = 554496 weight parameters which are to be optimized. Using a GA approach here would be terribly slow. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-04-12T17:22:52.063" Id="55960" LastActivityDate="2013-04-12T17:22:52.063" OwnerUserId="22540" ParentId="55887" PostTypeId="2" Score="3" />
  <row AnswerCount="0" Body="&lt;p&gt;I am analyzing some tree physiology data (transpiration) in relation to a number of environmental variables (many of which are predictors such as temperature, PAR and vapour pressure deficit). &lt;/p&gt;&#10;&#10;&lt;p&gt;I have fine-scale (30 min intervals) data of these various measurements, and there are two objectives I am trying to achieve:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Use the various predictors (glm?) to see which among these explain the most amount of variation in transpiration. However, since there is clear autocorrelation at this scale (i.e., trans at time $t$ is highly correlated with trans at $t+1$ etc.), I am looking to use ARIMA models with regressors. &lt;/li&gt;&#10;&lt;li&gt;I would like to construct a final predictive ARIMA model that explains the highest variation in trans, from all the different candidate models.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;So far, I have noticed that ccf plots show -ve lags between trans and a number of variables (rightly so, e.g., as you expect temp at time $t$ to influence transpiration at $t+1$).&lt;/p&gt;&#10;&#10;&lt;p&gt;My questions are:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;How do you perform an ARIMA with transpiration as the response variable and several regressors? &lt;/li&gt;&#10;&lt;li&gt;How do you know which one of the regressors to leave out? Does this have to be done manually in R (as in, add each regressor to the model, and inspect the resulting AIC)? &lt;/li&gt;&#10;&lt;li&gt;Is &lt;code&gt;auto.arima&lt;/code&gt; the best way to determine the differencing term (etc.)?&#10;(E.g., &lt;code&gt;auto.arima(trans, xreg=temp+vpd+......)&lt;/code&gt;.)&lt;/li&gt;&#10;&lt;li&gt;How do you account for the lag between response variable at time $t$ and predictors at $t-1$?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="1" CreationDate="2013-04-12T17:27:55.817" FavoriteCount="2" Id="55961" LastActivityDate="2013-04-12T17:58:37.327" LastEditDate="2013-04-12T17:58:37.327" LastEditorUserId="7290" OwnerUserId="24291" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;multiple-regression&gt;&lt;arima&gt;" Title="How to do a multiple regression with ARIMA using R?" ViewCount="356" />
  <row Body="&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Fisher%27s_exact_test&quot; rel=&quot;nofollow&quot;&gt;Fisher's exact test&lt;/a&gt; is a hypothesis test for contingency tables.  It returns the exact probability of finding observed counts as far or further from independence, if the rows and columns were actually independent.  It is based on the &lt;a href=&quot;http://en.wikipedia.org/wiki/Hypergeometric_distribution&quot; rel=&quot;nofollow&quot;&gt;hypergeometric distribution&lt;/a&gt;.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Fisher's exact test may be contrasted with the chi-squared test for contingency tables, which compares the observed $\chi^2$ test statistic to the (continuous) &lt;a href=&quot;http://en.wikipedia.org/wiki/Chi-squared_distribution&quot; rel=&quot;nofollow&quot;&gt;chi-squared distribution&lt;/a&gt; to determine the p-value, instead of computing the p-value directly.  This strategy is computationally inexpensive.  The sampling distribution of the $\chi^2$ test statistic will match the theoretical chi-squared distribution asymptotically (i.e., with sufficiently large samples); with smaller sample sizes the match is only approximate, however.  Fisher's exact test is sometimes recommended when cells have small expected frequencies and thus the chi-squared test may not be appropriate.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-12T21:49:46.767" Id="55971" LastActivityDate="2013-07-22T05:26:11.743" LastEditDate="2013-07-22T05:26:11.743" LastEditorUserId="7290" OwnerUserId="7290" PostTypeId="5" Score="0" />
  
  
  
  
  
  
  
  
  
  
  
  <row AcceptedAnswerId="57400" AnswerCount="1" Body="&lt;p&gt;Since new to association rules need help for identifying the most frequent &#10;extra service ordered together with the products. And to derive the &#10;association rules, I have used &lt;code&gt;arules&lt;/code&gt; R package on past data.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have collected and shaped the dataset as like&#10;you can download sample dataset from link (&lt;a href=&quot;https://www.dropbox.com/s/4g5sqag9afsttmo/finaldf2.csv&quot; rel=&quot;nofollow&quot;&gt;https://www.dropbox.com/s/4g5sqag9afsttmo/finaldf2.csv&lt;/a&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;When I tried it with the following set of R codes, it can't calculate the &#10;association rules between product and services. But it calculates the rules between all &#10;services. &lt;/p&gt;&#10;&#10;&lt;p&gt;Can any one guide me why I am getting set of 0 rules for calculation of association rules of product. Am I doing something wrong to calculate the associations. Or &#10;My dataset should be different. &lt;/p&gt;&#10;&#10;&lt;p&gt;Here are the R codes generated for &lt;code&gt;arules&lt;/code&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# Loading the library&#10;library(arules)    &#10;&#10;# Loading the dataset &#10;product.transaction.set &amp;lt;- read.csv(&quot;finaldf2.csv&quot;)    &#10;&#10;# calculating the association with apriori algo&#10;rules &amp;lt;- apriori(product.transaction.set, parameter = list(support = 0.4,conf = 0.6,target = &quot;rules&quot;))&#10;&#10;## 1 - for product&#10;# Deriving the association rules for productname&#10;rules.sub.product &amp;lt;- subset(rules, subset = lhs %pin% &quot;LD&quot;) # output -&amp;gt; set of 0 rules     &#10;&#10;# for print the calculated association rules&#10;inspect(rules.sub.product) &#10;&#10;## 2- for add on services ()&#10;# Deriving the association rules for add on services&#10;rules.sub.services &amp;lt;- subset(rules, subset = lhs %pin% &quot;AO1&quot;) # output -&amp;gt; set of 1 rules &#10;&#10;# for print the calculated association rules&#10;inspect(rules.sub.services) &#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2013-04-13T20:40:09.267" FavoriteCount="1" Id="56034" LastActivityDate="2013-04-27T05:59:30.940" LastEditDate="2013-04-15T06:14:30.450" LastEditorUserId="16016" OwnerUserId="16016" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;association-rules&gt;" Title="Association analysis returns 0 useful rules" ViewCount="318" />
  <row AnswerCount="1" Body="&lt;p&gt;I've conducted a psychological experiment on the same subject, under two different condition. For each condition I've collected the number of correct and wrong answer for each stimulus (number of trials per stimulus = 10; number of stimulus = 15).&#10;From the data collected I fitted a psychometric curve (probit or logit curve), and now I want to compare the results.&lt;/p&gt;&#10;&#10;&lt;p&gt;Consider &lt;strong&gt;subject 1&lt;/strong&gt;: His answers are collected in the following matrices&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;**COND_1**&#10;    correct&#10;cnt   Y  N&#10;  1   0 10&#10;  2   0 10&#10;  3   0 10&#10;  4   0 10&#10;  5   0 10&#10;  6   0 10&#10;  7   1  9&#10;  8   4  6&#10;  9   5  5&#10;  10  7  3&#10;  11  8  2&#10;  12  7  3&#10;  13 10  0&#10;  14 10  0&#10;  15 10  0&#10;&#10;**COND_2**&#10;    correct&#10;cnt   Y  N&#10;  1   0 10&#10;  2   2  8&#10;  3   2  8&#10;  4   3  7&#10;  5   2  8&#10;  6   4  6&#10;  7   8  2&#10;  8  10  0&#10;  9   8  2&#10;  10 10  0&#10;  11  9  1&#10;  12 10  0&#10;  13 10  0&#10;  14 10  0&#10;  15 10  0&#10;&#10;COND_1.1 &amp;lt;- matrix(c(0, 2, 2, 3, 2, 4, 8, 10, 8, 10, 9, 10, 10, 10, 10, 10, 8, 8, 7, 8, 6, 2, 0, 2, 0, 1, 0, 0, 0, 0), byrow=F, ncol=2) &#10;COND_1.2 &amp;lt;- matrix(c(0, 0, 0, 0, 0, 0, 1, 4, 5, 7, 8, 7, 10, 10, 10, 10, 10, 10, 10, 10, 10, 9, 6, 5, 3, 2, 3, 0, 0, 0), byrow=F, ncol=2)&#10;&#10;cnt &amp;lt;- seq(from=0, to=1.4, by=0.1)&#10;&#10;ddprob.1.1 &amp;lt;- glm(COND_1.1 ~ cnt, family = binomial(link = &quot;probit&quot;)) &#10;ddprob.2.1 &amp;lt;- glm(COND_2.1 ~ cnt, family = binomial(link = &quot;probit&quot;))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/gDGhi.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Observing the plot, you can surely say that performance under condition 2 is better then the performance under condition 1.&#10;In fact if you compute some parameters like AUC, p25, p50, p75 (the value of the stimulus at which the number of correct answers are 25%, 50% and 75% each) [threshold parameters], you'll see that:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;COND_1      COND_2&#10;AUC    &amp;lt;    AUC&#10;p25    &amp;gt;    p25&#10;p50    &amp;gt;    p50&#10;p75    &amp;gt;    p75&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Now consider &lt;strong&gt;subject 2&lt;/strong&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;COND_1&#10;    correct&#10;cnt   Y  N&#10;  1   0 10&#10;  2   0 10&#10;  3   0 10&#10;  4   0 10&#10;  5   3  7&#10;  6   3  7&#10;  7   4  6&#10;  8   5  5&#10;  9   9  1&#10;  10  9  1&#10;  11 10  0&#10;  12 10  0&#10;  13 10  0&#10;  14 10  0&#10;  15 10  0&#10;&#10;COND_2&#10;    correct&#10;cnt   Y  N&#10;  1   0 10&#10;  2   1  9&#10;  3   0 10&#10;  4   4  6&#10;  5   2  8&#10;  6   6  4&#10;  7   4  6&#10;  8   7  3&#10;  9   4  6&#10;  10  7  3&#10;  11  7  3&#10;  12 10  0&#10;  13  9  1&#10;  14 10  0&#10;  15 10  0&#10;&#10;&#10;COND_1.2 &amp;lt;- matrix(c(0, 0, 0, 0, 3, 3, 4, 5, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 7, 7, 6, 5, 1, 1, 0, 0, 0, 0, 0), byrow=F, ncol=2)&#10;COND_2.2 &amp;lt;- matrix(c(0, 1, 0, 4, 2, 6, 4, 7, 4, 7, 7, 10, 9, 10, 10, 10, 9, 10, 6, 8, 4, 6, 3, 6, 3, 3, 0, 1, 0, 0), byrow=F, ncol=2)&#10;cnt &amp;lt;- seq(from=0, to=1.4, by=0.1)&#10;&#10;ddprob.1.1 &amp;lt;- glm(COND_1.2 ~ cnt, family = binomial(link = &quot;probit&quot;))&#10;ddprob.2.1 &amp;lt;- glm(COND_2.2 ~ cnt, family = binomial(link = &quot;probit&quot;))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The plotted curves are shown in the image below.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/7Qkct.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Can you say what performance is better? Under condition 1 or under condition 2?&#10;Extrapolating AUC, p25, p50, p75, you have:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;COND_1      COND_2&#10;AUC     &amp;gt;   AUC&#10;p25     &amp;gt;   p25&#10;p50     =   p50&#10;p75     &amp;lt;   p75&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So my question is: Is there a method to say that a generic curve (logit, probit or whatelse) is HIGHER then another?&#10;Is there a method to compare (a single number of &quot;performance&quot;) that describe the differences of the two curves? My example shows that AUC, p25, p50, p75 are not good parameters.&#10;I'd like to compute a single numeric parameter for each curves, to make a simple t-paired test, extrapolated from the distribution seen.&lt;/p&gt;&#10;" ClosedDate="2013-08-20T16:37:02.627" CommentCount="0" CreationDate="2013-04-13T15:22:41.017" Id="56048" LastActivityDate="2013-04-14T07:13:26.843" OwnerDisplayName="Tommaso" OwnerUserId="6153" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;curve-fitting&gt;" Title="Comparing two logit/probit curves with a single parameter" ViewCount="100" />
  
  
  
  <row Body="&lt;p&gt;Not as far as I know.  Which variable is dependent and which is independent depends on what the variables are. It can be determined (if at all) by arguments outside of statistics.&lt;/p&gt;&#10;&#10;&lt;p&gt;Sometimes, it's just that one &quot;makes sense&quot; and the other doesn't. If we have data on height and weight of human adults, it &quot;makes sense&quot; for weight to depend on height, but not the other way around.&lt;/p&gt;&#10;&#10;&lt;p&gt;Sometimes the temporal order of the variables determines which is which: Something that happens earlier cannot depend on something that happens later. For example, if the variables are &quot;who you voted for in 2012&quot; and sex, only one relationship makes any sense at all (I voted for Obama and therefore I'm female????)&lt;/p&gt;&#10;&#10;&lt;p&gt;Sometimes there is mutual causation. Does depression cause anxiety? Does anxiety cause depression? Probably both.&lt;/p&gt;&#10;&#10;&lt;p&gt;And sometimes there is no way to tell at all. Indeed, sometimes, there is no dependency in the relationship because, e.g. both variables depend on something else. &lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-04-14T12:14:13.503" Id="56076" LastActivityDate="2013-04-14T12:14:13.503" OwnerUserId="686" ParentId="56075" PostTypeId="2" Score="6" />
  
  
  <row AcceptedAnswerId="56097" AnswerCount="1" Body="&lt;p&gt;Imagine that we have mixture of two normal distributions with mixture parameter $\theta$:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$p(y_i|\theta) = \theta\phi(y_i;\mu_1, \sigma_1^2) + (1 - \theta)\phi(y_i; \mu_2, \sigma_2^2)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Assume that the ONLY unknown parameter is $\theta$. I want to get the MLE estimate of $\theta$. Here is what I do:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$L(\theta | data, \mu_1, \sigma_1^2, \mu_2, \sigma_2^2) = \prod_{i = 1}^{n} p(y_i|\theta) = \prod_{i = 1}^{n} \Big(\theta\phi(y_i;\mu_1, \sigma_1^2) + (1 - \theta)\phi(y_i; \mu_2, \sigma_2^2)\Big) \Rightarrow$$&#10;    $$L(\theta | data, \mu_1, \sigma_1^2, \mu_2, \sigma_2^2) \propto \prod_{i = 1}^{n}\Big[ \theta \Big(\phi_{i1} - \phi_{i2}\Big)\Big] = \theta^n \prod_{i = 1}^{n}\Big(\phi_{i1} - \phi_{i2}\Big) = \theta^nG(Y, param)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\phi_{i1}$ refers to the first normal density with $y_i$ plugged in and $\phi_{i2}$ refers to the second normal density. Note that there is no $\theta$ involved in G(Y, param).&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, in order to get the MLE, I take log and then derivative:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$l(\theta | data, \mu_1, \sigma_1^2, \mu_2, \sigma_2^2) = log(L(\theta | data, \mu_1, \sigma_1^2, \mu_2, \sigma_2^2)) \propto n log(\theta) + log(G) \Rightarrow $$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$l'(\theta | data, \mu_1, \sigma_1^2, \mu_2, \sigma_2^2) = \frac{n}{\theta}$$ &lt;/p&gt;&#10;&#10;&lt;p&gt;If my calculations above is right, then it's a very weird derivative and I don't know how to get the MLE estimate.&lt;/p&gt;&#10;&#10;&lt;p&gt;I appreciate if you could guide me on that.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-14T17:58:10.150" Id="56095" LastActivityDate="2013-04-14T18:23:09.470" OwnerUserId="8006" PostTypeId="1" Score="2" Tags="&lt;maximum-likelihood&gt;&lt;gaussian-mixture&gt;" Title="MLE of the mixture parameter in mixing two normal densities" ViewCount="217" />
  
  <row Body="&lt;p&gt;A binary logistic regression would work, for the same reason that you can use a regression when you want to compare two sample means.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, it may be more apparatus that is required. &lt;/p&gt;&#10;&#10;&lt;p&gt;With a small sample, one might consider a &lt;a href=&quot;http://en.wikipedia.org/wiki/Binomial_test&quot; rel=&quot;nofollow&quot;&gt;binomial test&lt;/a&gt;. In this case, your sample sizes are nice and big, so a straight out proportions test should be effectively indistinguishable from it and a little simpler to deal with.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, since you have three experimental runs it might be worth including experimental run as a covariate (even though if all is well it should not have a covariate effect). In that case, you &lt;em&gt;could&lt;/em&gt; use logistic regression to do teh comparison incorporating the experimental run variable, or you could look at a chi-square test to achieve basically the same thing (though if you want a one-tailed test, the first thing would be better).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-15T00:06:12.493" Id="56111" LastActivityDate="2013-04-15T00:06:12.493" OwnerUserId="805" ParentId="52373" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;It sounds like you've lost the information that would let you identify the inherent pairing in your data. This is unfortunate and will likely cost you power, but you can still do a test.&lt;/p&gt;&#10;&#10;&lt;p&gt;With the loss of pairing, you're in a two-sample situation.&lt;/p&gt;&#10;&#10;&lt;p&gt;You seem to suggest you're thinking of a Mann-Whitney test. That may well be suitable depending on the specific hypothesis you're interested in testing and the specific additional assumptions you may be willing to make. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you're willing to assume identical shapes apart from possible location shift, you can use a Mann-Whitney to test means, for example. If you want a more general location test, you don't necessarily need to assume identical shape. It sounds like your data are potentially fairly discrete - are you likely to have lots of ties?&lt;/p&gt;&#10;&#10;&lt;p&gt;Another possibility is - especially with such small samples - to consider a permutation test of whatever is quantity is of interest.&lt;/p&gt;&#10;" CommentCount="11" CreationDate="2013-04-15T01:08:29.677" Id="56115" LastActivityDate="2013-04-15T02:46:38.057" LastEditDate="2013-04-15T02:46:38.057" LastEditorUserId="805" OwnerUserId="805" ParentId="56104" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;I suggest &lt;a href=&quot;http://rattle.togaware.com/&quot; rel=&quot;nofollow&quot;&gt;Rattle&lt;/a&gt; which has random forest feature selection (and much more). It has nice GUI and very easy to use.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-04-15T03:21:01.173" Id="56119" LastActivityDate="2013-04-15T03:21:01.173" OwnerUserId="14741" ParentId="56092" PostTypeId="2" Score="3" />
  
  
  
  
  <row Body="&lt;p&gt;also look at&lt;/p&gt;&#10;&#10;&lt;p&gt;Mishra, SK (2009) &quot;A Note on the Ordinal Canonical Correlation Analysis of Two Sets of Ranking Scores&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1328319&quot; rel=&quot;nofollow&quot;&gt;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1328319&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-04-15T12:09:33.613" Id="56148" LastActivityDate="2013-04-15T12:09:33.613" OwnerUserId="24376" ParentId="54951" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Suppose we have three variables $X_1, \dots, X_3$. There are $5$ categories for each variable. In each of the categories, the variables can take at most $3$ values. Suppose we are interested in the frequency of values in each category across the $3$ variables. &lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose the data is the following: $X_1 = (1,2,3,3,2)$, $X_2 = (2,3,1,3,1)$ and $X_3 = (1,3,1,3,1)$. These represent the observed counts. To determine the correlations between each variable should we look at the counts? Or should we look at percentages? In other words, should we look at $X_1 = (1/3, 2/3, 1,1, 2/3) \dots$?&lt;/p&gt;&#10;" ClosedDate="2013-04-15T15:33:21.627" CommentCount="3" CreationDate="2013-04-15T14:53:10.490" Id="56162" LastActivityDate="2013-04-15T14:53:10.490" OwnerUserId="24380" PostTypeId="1" Score="0" Tags="&lt;correlation&gt;" Title="Correlation between numbers versus percentages" ViewCount="18" />
  <row AcceptedAnswerId="56176" AnswerCount="1" Body="&lt;p&gt;This is a general question, has probably been asked before (I searched and did not see similar), may have a simple answer, may be ridiculous, and is just to satisfy my own curiosity.&lt;/p&gt;&#10;&#10;&lt;p&gt;Say we build a linear model (or any model for that matter) on our training data, and then generate predictions from our test data and find that the model yields decent predictions as measured by say r^2.  We then test the assumptions of our model, find that it fails to meet necessary assumptions, and are either unable to manipulate our data meet the assumptions or  able to make our data to meet assumptions, but the new model built performs poorly compared to the invalid model.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any reason (besides that of a theoretical standpoint) not to use  the better-performing invalid model, so long as it keeps generating accurate predictions?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-15T16:51:46.160" FavoriteCount="1" Id="56172" LastActivityDate="2013-04-15T17:51:19.283" OwnerUserId="21900" PostTypeId="1" Score="0" Tags="&lt;predictive-models&gt;&lt;assumptions&gt;" Title="Model assumptions not met but model has predictive capabilities" ViewCount="66" />
  
  
  
  
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I am desperately looking for some statistical help with my data because I myself cannot transfer the theoretical stuff I lately read on residuals, chi square distributions, squared z-values ecetera to my problem. Therefore I would really appreciate somebody to help me on that:&lt;/p&gt;&#10;&#10;&lt;p&gt;I compared 2 distributions with 4 categories by using Fisher's exact test- the difference turned out to be significant. &lt;/p&gt;&#10;&#10;&lt;p&gt;Now I wanted to know which category is &quot;responsible&quot; for the difference. More specifically, I was interested in which of the 4 categories the observed values differed from the expected.&lt;/p&gt;&#10;&#10;&lt;p&gt;Therefore I calculated &quot;standardized residuals&quot; or &quot;squared z-values&quot; (if that is correct??), like this: (observed - expected) squared/ expected&lt;/p&gt;&#10;&#10;&lt;p&gt;category:     1; 2; 3;4&lt;/p&gt;&#10;&#10;&lt;p&gt;observed:     4;       7;       5;       56&lt;/p&gt;&#10;&#10;&lt;p&gt;expected:     1.4;      3.2;     4.6;     62.8&lt;/p&gt;&#10;&#10;&lt;p&gt;z-squared:    4.8;      4.5;     0.03;    0.73&lt;/p&gt;&#10;&#10;&lt;p&gt;Hence, from what I understand of this example the observed and expected values in category 3 and 4 are not &quot;that&quot; different, but well in category 1 and 2; But now what do the numbers exactly mean? Do they convey any information of contingency considering that the comparison deals with numbers of people in each category? &lt;/p&gt;&#10;&#10;&lt;p&gt;I would be very happy about any advice.&#10;Kind regards,&#10;Johanna&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-04-16T08:04:50.260" Id="56221" LastActivityDate="2013-04-16T10:24:44.117" OwnerUserId="24411" PostTypeId="1" Score="1" Tags="&lt;weighted-sampling&gt;&lt;fishersexact&gt;" Title="Post Fisher's exact test- how to weight within categorial differences?" ViewCount="249" />
  <row AcceptedAnswerId="56231" AnswerCount="2" Body="&lt;p&gt;I try to understand and visualize myself covariance matrix. Supposing I have a matrix &lt;code&gt;A = [ 2 3 4; 5 5 6 ]&lt;/code&gt;, how do I calculate its covariance matrix, and what is its practical meaning? (All I was able to understand by now is that on the diagonal of the covariance matrix, variances for particular variables are placed, and on the upper and lower fields correlations between those variables.)&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-04-16T09:11:00.700" Id="56227" LastActivityDate="2014-03-04T18:49:53.123" OwnerUserId="24412" PostTypeId="1" Score="-1" Tags="&lt;covariance&gt;&lt;matrix&gt;&lt;variance-covariance&gt;" Title="Covariance matrix explanation" ViewCount="667" />
  
  <row AcceptedAnswerId="56246" AnswerCount="1" Body="&lt;p&gt;Lets say there is a Markov chain whose transition matrix is defined as follows&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;P = \left( \begin{array}{cccc}&#10;   0.5 &amp;amp; 0.5  &amp;amp; 0 &amp;amp; 0  \\&#10;   0   &amp;amp; 0.25 &amp;amp; 0 &amp;amp; 0.75 \\&#10;   0   &amp;amp; 1    &amp;amp; 0 &amp;amp; 0    \\&#10;   0   &amp;amp; 1    &amp;amp; 0 &amp;amp; 0&#10; \end{array} \right)&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Now I've been told that communicating classes are those subsets of states communicate with each other, i.e. $P_{ij} &amp;gt; 0$ and $P_{ji} &amp;gt; 0$.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the example above 1 communicates only with itself, 2 and 4 communicate with each other, but &lt;strong&gt;state 3 doesn't communicate with any other state, even itself&lt;/strong&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;So the communicating classes are {1} and {2,4}. But is {3} a such a class? It seems odd for it not to be, but it doesn't seem to match the definition as I've seen it. Presumably the chain is still reducible?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; &lt;em&gt;Fixed an issue where initially the top row of P failed to sum to one&lt;/em&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-16T09:45:16.807" Id="56234" LastActivityDate="2013-04-16T11:31:04.510" LastEditDate="2013-04-16T11:31:04.510" LastEditorUserId="24413" OwnerUserId="24413" PostTypeId="1" Score="1" Tags="&lt;markov-process&gt;" Title="Reducible Markov Chain with a state that communicates with nothing" ViewCount="61" />
  <row AcceptedAnswerId="56243" AnswerCount="3" Body="&lt;p&gt;I am currently reading &lt;a href=&quot;https://developer.amazon.com/sdk/ab-testing/reference/ab-math.html&quot; rel=&quot;nofollow&quot;&gt;Math behind A/B testing written by Amazon&lt;/a&gt; and got stuck. At some point they say:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;To determine the 95% confidence interval on each side of conversion&#10;  rate, we multiply the standard error with the 95th percentile (one&#10;  tailed) of a standard normal distribution (a constant value equal to&#10;  1.65).&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Then they use that constant to calculate the confidence interval:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;range = conversion rate +- (1.65 x Standard Error)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I &lt;a href=&quot;http://answers.yahoo.com/question/index?qid=20081002234551AAFi6bU&quot; rel=&quot;nofollow&quot;&gt;read somewhere&lt;/a&gt; to get the aforementioned constant value from the following table:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.sjsu.edu/faculty/gerstman/StatPrimer/t-table.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.sjsu.edu/faculty/gerstman/StatPrimer/t-table.pdf&lt;/a&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/PiSUh.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem is that I can't see &lt;code&gt;1.65&lt;/code&gt; anywhere for &lt;code&gt;95%&lt;/code&gt; and the closest value is &lt;code&gt;1.960&lt;/code&gt;, hence my confusion.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Could someone explain me where the &lt;code&gt;1.65&lt;/code&gt; is coming from?&lt;/strong&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-16T09:50:10.170" FavoriteCount="1" Id="56236" LastActivityDate="2013-04-16T10:39:59.493" OwnerUserId="24414" PostTypeId="1" Score="1" Tags="&lt;confidence-interval&gt;&lt;standard-error&gt;&lt;ab-test&gt;&lt;confidence&gt;" Title="Need help understanding calculation about Confidence interval" ViewCount="8786" />
  
  <row AcceptedAnswerId="56273" AnswerCount="1" Body="&lt;p&gt;I am posting this question here, because I didn't get an answer from math-exchange... =) &lt;/p&gt;&#10;&#10;&lt;p&gt;I'm reading a time series analysis book and the formula for sample autocovariance is defined in the book as:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\widehat{\gamma}(h) = n^{-1}\displaystyle\sum_{t=1}^{n-h}(x_{t+h}-\bar{x})(x_t-\bar{x})$&lt;/p&gt;&#10;&#10;&lt;p&gt;with $\widehat{\gamma}(-h) = \widehat{\gamma}(h)\;$ for $\;h = 0,1, ..., n-1$. $\bar{x}$ is the mean.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can someone explain intuitively why we divide the sum by $n$ and not by $n-h$? The book explains that this is because the formula above is a non-negative definite function and so dividing by $n$ is preferred, but this isn't clear to me.  Can someone maybe prove this or show example or something. I'm a bit confused x) To me the intuitive thing at first would be to divide by $n-h$. Is this an unbiased or biased estimator of autocovariance?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you for any help =) &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-04-16T10:11:47.983" FavoriteCount="1" Id="56238" LastActivityDate="2013-06-10T16:50:37.557" LastEditDate="2013-04-16T10:32:46.590" LastEditorUserId="686" OwnerUserId="18528" PostTypeId="1" Score="3" Tags="&lt;time-series&gt;&lt;probability&gt;" Title="Question about sample autocovariance function" ViewCount="932" />
  
  
  
  
  <row AcceptedAnswerId="58016" AnswerCount="1" Body="&lt;p&gt;In the gls fit shown below, the estimates of the standard deviation for each level of X are apparently given by the product of (1.000000,  3.913972, 10.684698, 11.350910, 26.476561, 27.255072) times the residual standard error of 0.04896334. How does one estimate the standard error of these estimated standard deviations (or variances)?    &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; m&#10;    X     Y  F&#10;1   1  1.07  1&#10;2   1  1.01  1&#10;3   1  0.99  1&#10;4   1  1.09  1&#10;5   1  0.94  1&#10;6   1  1.00  1&#10;7   1  1.01  1&#10;8   1  0.98  1&#10;9   1  1.00  1&#10;10  1  1.03  1&#10;11  4  3.66  4&#10;12  4  3.75  4&#10;13  4  3.77  4&#10;14  4  3.92  4&#10;15  4  4.08  4&#10;16  4  3.99  4&#10;17  4  3.95  4&#10;18  4  4.10  4&#10;19  4  3.88  4&#10;20  4  4.04  4&#10;21 10 10.13 10&#10;22 10 10.20 10&#10;23 10  9.77 10&#10;24 10 10.28 10&#10;25 10  8.71 10&#10;26 10  9.79 10&#10;27 10  9.82 10&#10;28 10  9.85 10&#10;29 10 10.07 10&#10;30 10  9.63 10&#10;31 20 20.22 20&#10;32 20 19.46 20&#10;33 20 19.02 20&#10;34 20 20.06 20&#10;35 20 20.94 20&#10;36 20 19.92 20&#10;37 20 19.96 20&#10;38 20 20.04 20&#10;39 20 19.67 20&#10;40 20 19.96 20&#10;41 30 31.04 30&#10;42 30 31.40 30&#10;43 30 31.84 30&#10;44 30 30.77 30&#10;45 30 32.13 30&#10;46 30 31.17 30&#10;47 30 30.36 30&#10;48 30 29.95 30&#10;49 30 30.74 30&#10;50 30 30.67 30&#10;51 40 41.14 40&#10;52 40 40.29 40&#10;53 40 42.77 40&#10;54 40 38.36 40&#10;55 40 39.17 40&#10;56 40 39.61 40&#10;57 40 40.73 40&#10;58 40 39.42 40&#10;59 40 40.72 40&#10;60 40 40.24 40&#10;&amp;gt; Fit.gls &amp;lt;- gls(Y ~ X,weights=varIdent(form = ~ 1 | F),data=m)&#10;&amp;gt; summary(Fit.gls)&#10;Generalized least squares fit by REML&#10;  Model: Y ~ X &#10;  Data: m &#10;       AIC      BIC    logLik&#10;  78.96207 95.44562 -31.48104&#10;&#10;Variance function:&#10; Structure: Different standard deviations per stratum&#10; Formula: ~1 | F &#10; Parameter estimates:&#10;        1         4        10        20        30        40 &#10; 1.000000  3.913972 10.684698 11.350910 26.476561 27.255072 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2013-04-16T17:57:16.007" Id="56280" LastActivityDate="2013-05-03T11:33:04.817" OwnerUserId="17124" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;regression&gt;" Title="Standard Error of Standard Deviations Estimated with gls in R" ViewCount="176" />
  <row Body="&lt;p&gt;I think you may be mixing different issues in the same package. First things first. As @whuber has just pointed, it is not clear whether your DV is really ordinal, or only your IVs. That would configure very different scenarios, depending on such clarification. For instance, if your DV is also ordinal, you could (and maybe even should) go for a  Ordinal Logistic Regression, just as mentioned by @Scortchi. The skewed distribution of such a DV would be less than a problem in that framework.&lt;/p&gt;&#10;&#10;&lt;p&gt;Actually, being precise, the distribution of even a continuous DV is, by itself, less of a problem. What you should be more worried about is the distribution of the residuals of your whole model, not of the variables themselves. It is a common mistake to pay attention to the distribution of the variables not of the residuals.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, if your DV is continuous and the distribution of the residuals is heavily skewed, you should try first to transform your DV (log transformation being the standard, but in some cases you may need to power the DV or to square root it). If no transformation of the DV works, try also transforming any continuous IV. If you still have no good luck here, I would go for a robust regression, which allows for much less well behaved distribution of residuals. It is fairly usefull, for instance, when you have strong bell-shaped distribution of the residuals, i.e. heavy tails in both sides (a scenario that may be very hard to solve through transformation of variables). In R, it can be accomplished very well with lmrob function.&lt;/p&gt;&#10;&#10;&lt;p&gt;But anyway, at the end of your question, you mention that one third of the DV observations are equal zero. This case may also suggest that you may have a zero-inflated scenario and, thus, may need to look for models that can account for this kind of issue, such as the Zero-Inflated Regression models.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hope this helps, but any further insights depend on you better clarifying your variables and the specification of your models.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-16T18:24:45.127" Id="56286" LastActivityDate="2013-04-16T18:24:45.127" OwnerUserId="20211" ParentId="52695" PostTypeId="2" Score="2" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have a data sets like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;structure(list(Month = structure(c(14975, 15006, 15034, 15065, &#10;15095, 15126, 15156, 15187, 15218, 15248, 15279, 15309, 15340, &#10;15371, 15400, 15431, 15461, 15492, 15522, 15553, 15584, 15614, &#10;15645, 15675, 15706, 15737, 15765), class = &quot;Date&quot;), TotalLogins = c(284697404L, &#10;268944957L, 297847827L, 287150001L, 277779620L, 262275285L, 284271058L, &#10;294965702L, 285132804L, 238847338L, 287527433L, 314483537L, 324823553L, &#10;322896485L, 329044914L, 318228530L, 324395065L, 324988644L, 335464023L, &#10;336269471L, 324063033L, 349017727L, 347193478L, 355561387L, 373885187L, &#10;356774443L, 386372600L)), .Names = c(&quot;Month&quot;, &quot;TotalLogins&quot;), class = &quot;data.frame&quot;, row.names = c(11L, &#10;8L, 18L, 1L, 21L, 16L, 14L, 4L, 27L, 25L, 23L, 6L, 12L, 9L, 19L, &#10;2L, 22L, 17L, 15L, 5L, 28L, 26L, 24L, 7L, 13L, 10L, 20L))&#10;&#10;summary(lm(TotalLogins ~ Month, data=xx))&#10;&#10;Call:&#10;lm(formula = TotalLogins ~ Month, data = xx)&#10;&#10;Residuals:&#10;      Min        1Q    Median        3Q       Max &#10;-59901492  -4717787   1266082   9432626  26785529 &#10;&#10;Coefficients:&#10;              Estimate Std. Error t value Pr(&amp;gt;|t|)    &#10;(Intercept) -1.674e+09  2.121e+08  -7.892 3.01e-08 ***&#10;Month        1.294e+05  1.380e+04   9.377 1.16e-09 ***&#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1&#10;&#10;Residual standard error: 17020000 on 25 degrees of freedom&#10;Multiple R-squared:  0.7786,    Adjusted R-squared:  0.7697 &#10;F-statistic: 87.92 on 1 and 25 DF,  p-value: 1.157e-09&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I am trying to growth of the TotalLogins (coeffient). I dont see this value in the summary. How can I get growth of TotalLogins?&lt;/p&gt;&#10;&#10;&lt;p&gt;When I do this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lin&amp;lt;-lm(TotalLogins ~ Month, data=xx)&#10;coeff(lin)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;(Intercept)         Month &#10;-1673981283.6      129376.3 &lt;/p&gt;&#10;&#10;&lt;p&gt;It is a huge number, does not seem right. Any ideas?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-16T19:38:40.270" Id="56294" LastActivityDate="2013-04-16T20:03:25.870" OwnerUserId="16362" PostTypeId="1" Score="0" Tags="&lt;regression-coefficients&gt;" Title="determine the growth rate given a time series data" ViewCount="152" />
  
  <row AnswerCount="2" Body="&lt;p&gt;Suppose I have some dataset. I perform some regression on it. I have a separate test dataset. I test the regression on this set. Find the RMSE on the test data. How should I conclude that my learning algorithm has done well, I mean what properties of the data I should look at to conclude that the RMSE I have got is good for the data?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-16T21:03:02.497" FavoriteCount="1" Id="56302" LastActivityDate="2014-10-17T17:56:58.600" OwnerUserId="11469" PostTypeId="1" Score="8" Tags="&lt;regression&gt;&lt;error&gt;" Title="What are good RMSE values?" ViewCount="12620" />
  <row AcceptedAnswerId="56348" AnswerCount="6" Body="&lt;p&gt;I want to include time spent doing something (weeks breastfeeding, for example) as an independent variable in a linear model. However, some observations do not engage in the behavior at all. Coding them as 0 isn't really right, because 0 is qualitatively different from any value &gt;0 (i.e. women who don't breastfeed may be very different from women who do, even those who don't do it for very long). The best I can come up with is a set of dummies that categorizes the time spent, but this is a waste of precious information. Something like zero-inflated Poisson also seems like a possibility, but I can't exactly figure out what that would look like in this context. Does anyone have any suggestions?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-16T21:43:22.913" FavoriteCount="5" Id="56306" LastActivityDate="2013-04-19T12:28:38.353" OwnerUserId="16049" PostTypeId="1" Score="6" Tags="&lt;regression&gt;&lt;generalized-linear-model&gt;&lt;mixture&gt;" Title="Time spent in an activity as an independent variable" ViewCount="754" />
  <row Body="&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Tobit_model&quot; rel=&quot;nofollow&quot;&gt;Tobit model&lt;/a&gt; is what you want, I think.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-04-16T21:47:38.370" Id="56308" LastActivityDate="2013-04-16T21:47:38.370" OwnerUserId="10295" ParentId="56306" PostTypeId="2" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to regress (OLS) some time series on stock returns. I am not interested in regressing the returns of those time series on my stock returns, but I want to include information about the level (the higher the better) of these time series.&lt;/p&gt;&#10;&#10;&lt;p&gt;I read somewhere that the dependent variable has to be stationary in order to correctly regress it on my returns. Here's the approach I was thinking about: for series on the interval [-100;100] I use an indicator variable =1 for positive values and =-1 for negative values. Does it matter that I will have many 1's in a row, then many -1 in a row? It is by far not as erratic as for the returns. &lt;/p&gt;&#10;&#10;&lt;p&gt;Is my approach ok? Or how could I modify while keeping the same idea?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-04-16T22:58:29.477" Id="56312" LastActivityDate="2013-04-16T22:58:29.477" OwnerUserId="22048" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;least-squares&gt;&lt;stationarity&gt;" Title="ols regression on stationary series" ViewCount="47" />
  <row Body="&lt;p&gt;I don't get why you think you couldn't transform that DV to log-normality only because of the zeros. You can just add a constant to this variable and then log-tranform it. Usual numbers to do so are 1, 10 or the minimum value of your variable. It would therefore have the form of log(1+DV), log(10+DV), log(min(DV)+DV) or in a more general term, log(a+DV).&#10;In R you can use the log1p() function instead of log() to automatically apply a log(1+DV).&lt;/p&gt;&#10;&#10;&lt;p&gt;If it doesn't change the distribution of he residuals in your models, you can also try other transformations, such as DV^a (in which a represents any number, usually 2, 3 or 0.5) or even exp(DV).&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, another interesting option is to consider using robust regression, which can handle bad behaved distribugions.&lt;/p&gt;&#10;&#10;&lt;p&gt;For additional help, bringing us a densityplot() image of you residuals would be great.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-16T23:29:21.580" Id="56317" LastActivityDate="2013-04-16T23:34:36.443" LastEditDate="2013-04-16T23:34:36.443" LastEditorUserId="20211" OwnerUserId="20211" ParentId="56311" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;This topic is covered in depth in chapter 5 of Gelman's &lt;em&gt;Bayesian Data Analysis&lt;/em&gt;. I would highly recommend picking it up, as the example about rat tumors is basically identical to the problem you have outlined here.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;The broad outlines of your approach sound correct. It sounds like hierarchical Bayesian modeling is exactly what you are looking for. In this application, each player has his/her preferences for a game action drawn from a common beta distribution with parameters $(\alpha, \beta)$. These parameters themselves are drawn in turn from a &lt;em&gt;hyperprior&lt;/em&gt; and then updated to also include the data. So, treat each player as a different binomial trial with parameters drawn from this common beta distribution. One player's preference for a particular game action is a result of that player's disposition having been drawn from that common beta distribution.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Having different sample sizes for each opponent is not a problem. It just means that the $N$ parameter in your model will vary from player to player. Part of the point of hierarchical modeling is to incorporate the evidence from other, comparable experiments into the model and &quot;shrink&quot; the parameter estimates toward their common distribution.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;I do not think that I could adequately explain the estimation procedure here, as Gelman spends about ten pages and a number of graphs on the topic. But following the lead of the rat tumor example will surely set you down the correct path. The broad outlines are reparameterizing the model as $y=log(\alpha+\beta)$ and $x=log(\alpha)-log(\beta)$ so that the beta parameters are continuous on the whole real line. Gelman chose a noninformative hyperprior density $p(\alpha, \beta) \propto (\alpha+\beta)^{-5/2}$ which is essentially flat. Then he computed a grid of points $(x,y)$around the point estimate of maximum likelihood and established the posterior density at each point using standard Bayesian methods. The resulting posterior density for $(x, y)$ describes the uncertainty about the parameters $(\alpha, \beta)$; with a little algebra, you can move from one coordinate system to the other. Simple point estimates of $(\alpha, \beta)$ are not necessarily helpful, as they would not convey the uncertainty contained in the prior beta distribution. However, we can still summarize our results by conducting a number of draws $(\alpha, \beta)$ from the posterior and evaluating inferring parameter values for our different players given our data of their performance, and choose, say, the middle 95% of draws as a plausible range of values for the binomial probability parameter.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="3" CreationDate="2013-04-17T01:34:51.027" Id="56328" LastActivityDate="2013-04-17T13:15:05.433" LastEditDate="2013-04-17T13:15:05.433" LastEditorUserId="22311" OwnerUserId="22311" ParentId="56319" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;Regress &lt;code&gt;Y&lt;/code&gt; on &lt;code&gt;X&lt;/code&gt; and find &lt;code&gt;Rsquared&lt;/code&gt; and &lt;code&gt;Dubin-Watson statistic&lt;/code&gt;. If former is greater than later, then it indicates spurious correlation. If this is the case, you can proceed as follows: &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Check whether each variable has an unit root. If both variables have the unit root, then the correlation that you are saying may be spurious. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;If both don't have unit root, you can proceed with the usual OLS model. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;If both are unit root, you need to proceed for estimating whether two variables are cointegrated (move together in the long run). &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;If both have unit root and there is no cointegration, then use VAR model. &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Use standard text books on time series for details. For example, applied time series econometrics by &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0470505397&quot; rel=&quot;nofollow&quot;&gt;Walter Enders&lt;/a&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;I am not the user of &lt;code&gt;Python&lt;/code&gt; but looking at &lt;a href=&quot;http://stackoverflow.com/questions/12996021/packages-for-time-series-forecasting&quot;&gt;this&lt;/a&gt; post, it seems that &lt;code&gt;python&lt;/code&gt; lags &lt;code&gt;R&lt;/code&gt; in terms of time series analysis. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-17T02:36:04.370" Id="56336" LastActivityDate="2013-04-18T04:44:26.297" LastEditDate="2013-04-18T04:44:26.297" LastEditorUserId="14860" OwnerUserId="14860" ParentId="56230" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Here's an example of what a spineplot of the data would look like. I did this in Stata pretty quickly, but there's an &lt;a href=&quot;http://stat.ethz.ch/R-manual/R-patched/library/graphics/html/spineplot.html&quot;&gt;R implementation&lt;/a&gt;. I think in R it should be just: &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;spineplot(factor(often)~factor(importance))&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The spineplot actually seems to be the default if you give R categorical variables:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;plot(factor(often)~factor(importance))&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The fractional breakdown of the categories of often is shown for each category of importance. Stacked bars are drawn with vertical dimension showing fraction of often given the importance category. The horizontal dimension shows the fraction in each importance category. Thus the areas of tiles formed represent the frequencies, or more generally totals, for each cross-combination of importance and often.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/K5jkI.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="11" CreationDate="2013-04-17T02:52:04.947" Id="56338" LastActivityDate="2013-04-17T03:40:14.213" LastEditDate="2013-04-17T03:40:14.213" LastEditorUserId="7071" OwnerUserId="7071" ParentId="56322" PostTypeId="2" Score="14" />
  
  <row Body="&lt;p&gt;If the rate is a count in a given category divided by so many in total, you could do a binomial GLM; there you're in effect explicitly modelling a proportion...&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;glm( formula, family=binomial)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The default link is logit rather than log as for the Poisson, but for small proportions the difference is negligible.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you're looking at many categories, there are a number of extensions that might be appropriate, such as &lt;em&gt;multinomial logit choice models&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also see &lt;a href=&quot;http://en.wikipedia.org/wiki/Generalized_linear_model#Multinomial_regression&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Generalized_linear_model#Multinomial_regression&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-04-17T07:13:31.313" Id="56346" LastActivityDate="2013-04-17T23:08:39.593" LastEditDate="2013-04-17T23:08:39.593" LastEditorUserId="805" OwnerUserId="805" ParentId="56311" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;A spineplot (mosaic plot) works well for the example data here, but can be difficult to read or interpret if some combinations of categories are rare or don't exist. Naturally it's reasonable, and expected, that a low frequency is represented by a small tile, and zero by no tile at all, but the psychological difficulty can remain. It's also natural that people fond of spineplots choose examples which work well for their papers or presentations, but I've often produced examples that were too messy to use in public. Conversely, a spineplot does use the available space well. &lt;/p&gt;&#10;&#10;&lt;p&gt;Some implementations presuppose interactive graphics, so that the user can interrogate each tile to learn more about it. &lt;/p&gt;&#10;&#10;&lt;p&gt;An alternative which can also work quite well is a two-way bar chart (many other names exist). &lt;/p&gt;&#10;&#10;&lt;p&gt;See for example &lt;code&gt;tabplot&lt;/code&gt; within &lt;a href=&quot;http://www.surveydesign.com.au/tipsusergraphs.html&quot; rel=&quot;nofollow&quot;&gt;http://www.surveydesign.com.au/tipsusergraphs.html&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;For these data, one possible plot (prouced using &lt;code&gt;tabplot&lt;/code&gt; in Stata, but should be easy in any decent software) is &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/mfLhM.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The format means it is easy to relate individual bars to row and column identifiers and that you can annotate with frequencies, proportions or percents (don't do that if you think the result is too busy, naturally). &lt;/p&gt;&#10;&#10;&lt;p&gt;Some possibilities: &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;If one variable can be thought of a response to another as predictor, then it is worth thinking of plotting it on the vertical axis as usual. Here I think of &quot;importance&quot; as measuring an attitude, the question then being whether it affects behaviour (&quot;often&quot;). The causal issue is often more complicated even for these imaginary data, but the point remains. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Suggestion #1 is always to be trumped if the reverse works better, meaning, is easier to think about and interpret. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Percent or probability breakdowns often make sense. A plot of raw frequencies can be useful too. (Naturally, this plot lacks the virtue of mosaic plots of showing both kinds of information at once.) &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;You can of course try the (much more common) alternatives of grouped bar charts or stacked bar charts (or the still fairly uncommon grouped dot charts in the sense of W.S. Cleveland). In this case, I don't think they work as well, but sometimes they work better. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Some might want to colour different response categories differently. I've no objection, and if you want that you wouldn't take objections seriously any way. &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;The strategy of hybridising graph and table can be useful more generally, or indeed not what you want at all. An often repeated argument is that the separation of Figures and Tables was just a side-effect of the invention of printing and the division of labour it produced; it's once more unnecessary, just as it was to manuscript writers putting illustrations exactly how and where they liked. &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-04-17T11:08:11.627" Id="56354" LastActivityDate="2015-01-30T11:26:31.220" LastEditDate="2015-01-30T11:26:31.220" LastEditorUserId="22047" OwnerUserId="22047" ParentId="56322" PostTypeId="2" Score="8" />
  <row Body="&lt;p&gt;You can group your data and perform either a 2-proportion test or a Chi-square test. &lt;/p&gt;&#10;&#10;&lt;p&gt;Ho: Sales Rep knowledge has no impact on customer buying criteria&#10;Ha: Knowledgable Sales Rep has a positive impact on customer buying criteria&lt;/p&gt;&#10;&#10;&lt;p&gt;A simple grouping:&lt;/p&gt;&#10;&#10;&lt;p&gt;Divide your data into 2 groups (low sales rep knowledge and high sales rep knowledge).&#10;Divide your results into 2 groups (will likely buy and will not buy).&lt;/p&gt;&#10;&#10;&lt;p&gt;Once you have this grouped data then you can perform a 2-proportion test.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you have more than 2 groups, then a Chi-square test would be appropriate.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-04-17T11:46:43.230" Id="56356" LastActivityDate="2013-04-17T11:46:43.230" OwnerUserId="24467" ParentId="52794" PostTypeId="2" Score="-1" />
  <row Body="&lt;p&gt;I don't really see a way to estimate it very well from the variation in cross-section data. Everything I know about this literature uses panel data (which may still be bad, but at least you can include fixed effects to control for unobserved time-varying consumption factors) or an instrumental variables with panel data (which is better, but may not identify the parameter you care about here because responses to unexpected oil shocks are different from tax increases or they may be weak or not exogenous). &lt;/p&gt;&#10;&#10;&lt;p&gt;The problem is that price and quantity demanded are jointly determined, which induces a correlation between price and the error terms that will bias estimates towards zero in a naive regression of quantity on price. I guess you can say that any coefficient you estimate with OLS will be biased downward. This may be the problem you run into. Or you might be able to do something simple, find that it agrees with more sophisticated results, and dismiss the objections on those grounds.&lt;/p&gt;&#10;&#10;&lt;p&gt;Another strategy would be to take some elasticity estimates from the literature, and use those to see what happens. In the short-run, the demand is going to be very inelastic, so you can assume the full burden of the tax will fall on the consumer. Then you can use your data to show how consumption would change.&lt;/p&gt;&#10;&#10;&lt;p&gt;There's a nice blog summary of the literature and the econometric issues &lt;a href=&quot;http://modeledbehavior.com/2011/04/27/of-carbon-taxes-and-price-elasticities/&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;. In particular, there are links to two accessible papers. Hughes, Knittel, and Sperling  find short-run gas price elasticities of -0.034 to -0.077 in the first five years of this century, which means that a price increase of 1% will lower quantity demanded by between 0.034% and 0.077%. You can also try replicating their specification with your cross-section (using logged price and quantity). The Killian and Davis paper points out some issues with the first approach and find somewhat larger effects with the HKS method (-0.19% with state panel data) and much larger estimates using IV (-0.46% with stata panel data). You might use all of them in your simulations, and compare with your own estimates.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-17T16:59:07.883" Id="56384" LastActivityDate="2013-04-17T18:20:08.080" LastEditDate="2013-04-17T18:20:08.080" LastEditorUserId="7071" OwnerUserId="7071" ParentId="56342" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="56745" AnswerCount="1" Body="&lt;p&gt;[I asked this on StackOverflow and was told it would be a better fit here]&lt;/p&gt;&#10;&#10;&lt;p&gt;I am including a basic recommendation engine in a very small project for my final exam. I understand the code and the math but I am not too certain as to when I should update the values. &lt;/p&gt;&#10;&#10;&lt;p&gt;In this project it would be fairly simple to update in real time but in the real world would that not take an extreme amount of cpu time? Would it make more sense to run the algorithm on a schedule?&lt;/p&gt;&#10;&#10;&lt;p&gt;In the project I am using linq2entities, storing the purchases in a table that I use to calculate the correlation coefficient. I only have a couple of thousand users in the db but I am trying to get an idea of what would be more efficient in practice.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-17T17:14:26.350" Id="56386" LastActivityDate="2013-04-21T10:33:09.323" OwnerUserId="18281" PostTypeId="1" Score="1" Tags="&lt;recommender-system&gt;&lt;sql&gt;&lt;c#&gt;&lt;programming&gt;" Title="When should I update a recommendation engine?" ViewCount="80" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I create time series model via:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;model = sarima(data,1,0,1)&#10;sarima.for(data,100,1,0,1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The model diagnostics look good and indicate that the model is a good fit. However, when I try to forecast the data, all the forecasted values are the same. Why is this? &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Gpedv.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-17T23:02:26.720" FavoriteCount="1" Id="56418" LastActivityDate="2013-04-17T23:57:18.813" OwnerUserId="20884" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;time-series&gt;" Title="Time series forecasting" ViewCount="84" />
  <row AcceptedAnswerId="56426" AnswerCount="2" Body="&lt;p&gt;I am working on a machine learning experiment comparing the use of multiple different neural network classifiers by applying them on a large number of datasets, using stratified 10-fold cross-validation. I measure the performance as the average of the errors on the validation set (sometimes referred to as test set) of the 10-fold cross-validation procedure.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is, would it be ok to use this same validation set to do an early stopping of the training procedure? This early stopping would be performed by applying the trained model after each epoch to the validation set and measuring the performance, and if it declines for a number of successive learning epochs, the learning would be halted and we would take the epoch that produced the last good performance. This would be applied to all the different techniques, and across all the different datasets.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is this ok? Or is it statistically inaccurate?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-18T00:25:20.447" Id="56421" LastActivityDate="2014-12-03T22:23:44.093" LastEditDate="2013-04-18T09:31:05.583" LastEditorUserId="24494" OwnerUserId="24494" PostTypeId="1" Score="3" Tags="&lt;machine-learning&gt;&lt;cross-validation&gt;" Title="Is it ok to determine early stopping using the validation set in 10-fold cross-validation?" ViewCount="356" />
  
  
  <row Body="&lt;p&gt;&lt;em&gt;I'm not sure I understood your phrase &quot;not through randomisation&quot;.&#10;This answer assumes that you did not randomly assign participants to groups.&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;If there was no random assignment of participants to group you could call it quasi-experimental.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thus, control and treatment groups may differ anyway. The fact that you have pre and post measurement hopefully reduces this issue because to some extent you are adjusting for baseline differences. That said, it doesn't resolve the issue completely.&lt;/p&gt;&#10;&#10;&lt;p&gt;Assuming that the dependent variable is numeric, you could look at the &lt;a href=&quot;http://stats.stackexchange.com/questions/3466/best-practice-when-analysing-pre-post-treatment-control-designs&quot;&gt;suggestions on this question about how to analyse such a design&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-04-18T10:46:03.017" Id="56453" LastActivityDate="2013-04-18T13:04:40.663" LastEditDate="2013-04-18T13:04:40.663" LastEditorUserId="183" OwnerUserId="183" ParentId="56452" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;your time series seem to consist of two components: slow (and maybe seasonal) and fast. There are exponential smoothing family of models that might help (by decomposing your time series into its building blocks). &#10;Additionally, if there are certain exogenous factors that might influence your time series, you can use them as explanatory variables in explaining parts of variation.&#10;I hope it helps ...&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-18T11:07:01.270" Id="56455" LastActivityDate="2013-04-18T11:07:01.270" OwnerUserId="16645" ParentId="56451" PostTypeId="2" Score="2" />
  
  
  
  <row Body="&lt;p&gt;Image Doctor brings up a great point.  You need to classify your data first.  It sounds like you don't have much in the way of signal - do you really know what &quot;negative&quot; means?&lt;/p&gt;&#10;&#10;&lt;p&gt;Lets start by saying that you have enough trees, that you are randomly spreading around your information well, and that you are evaluating the most likely outcome reasonably well.  It is an assumption.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Bagging, or more correctly &quot;B-Agging&quot;, stands for Bootstrap Aggregation.  It gives you estimates of the variability of an estimate given only the data, and that your resample size is not too large.  If you resample too many times you falsely confound your information with the central limit theorem and results get strange.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the RandomForest documentation the &quot;margin&quot; function is described as &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;For margin, the margin of observations from the randomForest classiﬁer (or whatever &#10;  classiﬁer that produced the predicted probability matrix given to&#10;  margin). The margin of a data point is deﬁned as the proportion of&#10;  votes for the correct class minus maximum proportion of votes for the&#10;  other classes. Thus under majority votes, positive margin means&#10;  correct classiﬁcation, and vice versa.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;This sounds like a way to reverse the &quot;error rate&quot; or number of times the result is not given out of the total possible outcomes.  To me this sounds like a route to an estimate of the probability.&lt;/p&gt;&#10;&#10;&lt;p&gt;You might also consider &quot;oob.times&quot;, &quot;varImpPlot&quot;, and &quot;importance&quot;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-18T14:31:17.533" Id="56476" LastActivityDate="2013-04-18T14:31:17.533" OwnerUserId="22452" ParentId="56466" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;So much of this depends on the expected distribution and what your research question is.  As a rule of thumb, you should be wary of rules of thumb. If you know the expected distribution, run some simulations of different sizes and determine how often the sample simulations reflect the actual distribution.  This should give you some guidance as the final required sample size. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-04-18T14:35:50.643" Id="56477" LastActivityDate="2013-04-18T14:35:50.643" OwnerUserId="24341" ParentId="56465" PostTypeId="2" Score="12" />
  
  
  
  <row Body="&lt;p&gt;The RANSAC algorithm sounds like what you want. Basically, it assumes your data consists of a mix of inliers and outliers, and tries to identify the inliers by repeatedly sampling subsets of the data, fitting a model to it, then trying to fit every other data point to the model. &lt;a href=&quot;http://en.wikipedia.org/wiki/RANSAC&quot; rel=&quot;nofollow&quot;&gt;Here's the wikipedia article about it&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;In your case, you can just keep repeating the algorithm while saving the current best model that fits at least 40 points, so it won't guarantee you the absolute best correlation, but it should get close.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-18T16:50:05.383" Id="56497" LastActivityDate="2013-04-18T16:50:05.383" OwnerUserId="24517" ParentId="32063" PostTypeId="2" Score="2" />
  
  
  <row AcceptedAnswerId="64907" AnswerCount="1" Body="&lt;p&gt;I was reading the user's guide of SPSS on missing value analysis and I found the terms monotonic and non-monotonic pattern of missing values. The terms are not quite clear to me. They used &lt;code&gt;telco_missing.sav&lt;/code&gt; data file to show a graph from which they comment:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;If the data are monotone, then all missing cells and nonmissing cells&#10;  in the chart will be contiguous; that is, there will be no “islands”&#10;  of nonmissing cells in the lower right portion of the chart and no&#10;  “islands” of missing cells in the upper left portion of the chart.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I didn't either understand that. I hope someone will be kind enough to show me how a monotone and non-monotone patterns does look like and tell me how do I understand it a little more clearly.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-18T18:03:28.653" Id="56506" LastActivityDate="2013-07-19T23:14:04.547" OwnerUserId="12603" PostTypeId="1" Score="2" Tags="&lt;spss&gt;&lt;missing-data&gt;" Title="Monotonic and non-monotonic patterns of missing values: how do they look like?" ViewCount="1074" />
  <row AnswerCount="0" Body="&lt;p&gt;I would like to calculate the confidence interval for my autocorrelation function. Does this calculation differ in any way from the normal way of calculating confidence intervals? How would I calculate the CF for cross correlation between two time series?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you for any help =) &lt;/p&gt;&#10;" ClosedDate="2014-01-23T18:07:05.207" CommentCount="0" CreationDate="2013-04-18T18:30:24.683" Id="56508" LastActivityDate="2013-04-18T18:30:24.683" OwnerUserId="18528" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;autocorrelation&gt;&lt;cross-correlation&gt;" Title="How to calculate confidence interval for autocorrelation- and cross correlation functions?" ViewCount="174" />
  
  
  <row Body="&lt;p&gt;If we assume that your questionnaire measures one trait, then you could look for internal consistency of the questionnaire and whether it is actually unidimensional.  To check the internal consistency, you could use Chronbach's alpha. To determine if the questionnaire is indeed unidimensional, you could perform a confirmatory factor analysis and determine if the questions load on a single factor.  An CFA would also let you estimate how much of the variance of the questions is explained by a single factor.  In addition, you could perform a correlation matrix and review the correlations with a comparison to a theory of how the questions were expected to perform.  You should write your theory down a priori and then perform the matrix.  With the matrix you should be looking for convergent and divergent evidence of validity of the questionnaire.  If you create a total score, and you are measuring a unidimensional trait, then all questions should correlate positively with the total score.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hard to advise further without more information. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-18T20:19:33.600" Id="56523" LastActivityDate="2013-04-19T13:34:51.547" LastEditDate="2013-04-19T13:34:51.547" LastEditorUserId="24341" OwnerUserId="24341" ParentId="56510" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="56531" AnswerCount="1" Body="&lt;p&gt;Please see the model below (&lt;a href=&quot;http://i.stack.imgur.com/CspnD.png&quot; rel=&quot;nofollow&quot;&gt;link to bigger image&lt;/a&gt;). The independent variables are properties of 2500 companies from 32 countries, trying to explain companies' CSR (corporate social responsibility) score. &lt;/p&gt;&#10;&#10;&lt;p&gt;I am worried about the VIF scores of especially the &lt;code&gt;LAW_&lt;/code&gt;, &lt;code&gt;GCI_&lt;/code&gt; and &lt;code&gt;HOF_&lt;/code&gt; variables but I really need them all included in the model to connect it to the theoretical background the model is built upon. All variables are discrete numeric values, except the law &lt;code&gt;LAW_&lt;/code&gt; variables: they are dummies as to which legal system applies in the companies' country of origin (either english, french, german or scandinavian).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/CspnD.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Amongst other articles, I have read &lt;a href=&quot;http://www.thejuliagroup.com/blog/?p=1405&quot; rel=&quot;nofollow&quot;&gt;this article&lt;/a&gt; about dealing with collinearity. Often-suggested tips are removing the variable with highest VIF-score (in my model this would be &lt;code&gt;LAW_ENG&lt;/code&gt;). But then other VIF-scores increase as a result. I do not have the proper knowledge to see through what is going on here and how I can solve this problem. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have uploaded the corresponding data &lt;a href=&quot;https://www.dropbox.com/s/tw6vyz0sx89fnch/stackexchange.sav&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; (in SPSS &lt;code&gt;.sav&lt;/code&gt; format). I would really appreciate somebody with more experience having a quick look and tell me a way to solve the collinearity problem without taking out (any or too many) variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any help is greatly appreciated.&lt;/p&gt;&#10;&#10;&lt;p&gt;P.S. For reference, I am including a correlation table (&lt;a href=&quot;http://i.stack.imgur.com/Q4eiY.png&quot; rel=&quot;nofollow&quot;&gt;link to bigger image&lt;/a&gt;):&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Q4eiY.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-04-18T22:17:50.923" Id="56530" LastActivityDate="2013-09-21T22:28:33.217" LastEditDate="2013-04-18T23:42:48.537" LastEditorUserId="10776" OwnerUserId="10776" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;least-squares&gt;&lt;multicollinearity&gt;&lt;vif&gt;" Title="How to solve collinearity problems in OLS regression?" ViewCount="1458" />
  
  <row AnswerCount="1" Body="&lt;p&gt;If I have a logistic regression model with 3 predictors, $x_1$, $x_2$, $x_3$, and then I remove $x_3$ from my model (left with only $x_1$ and $x_2$), are those models nested?  And therefore I can use the likelihood ratio test ($\chi^2$ with 1 df in this case) to test if $x_3$ is necessary to keep in the model.  Or are these models not nested because they do not include the exact same variables?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-19T02:04:11.173" Id="56547" LastActivityDate="2013-04-19T03:46:24.547" LastEditDate="2013-04-19T02:28:52.587" LastEditorUserId="7290" OwnerUserId="24531" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;logistic&gt;&lt;nested&gt;" Title="Nested Models for Likelihood Test" ViewCount="86" />
  
  
  
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;From my reading, I understand that usually what you would do would be to: - take, say, 40 chunks of 100 replies as your sample, - for each sample, determine what percentage is written by bots - calculate the mean and standard deviation of the percentages of all 40 chunks - calculate the standard error for the sample, based on the standard deviation / sqrt of 40 (chunks) - calculate 95% significance of the mean percentage via mean of sample chunks +/- 1.96 x standard error&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;It's almost certainly not necessary to break it up into chunks.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;To what I understand, this can be done because we expect the data to be roughly normal.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Not when your proportion is very close to zero or 1. If the expected count of not-written-by-bots is no more than a handful, assuming normality isn't tenable.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;I can't really say that I am 99.999% confident that the % of bots for the population is 100% right?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;There's a commonly used approximate rule to get bounds on the proportion of &quot;successes&quot; when you observe only failures... sometimes called the &quot;&lt;a href=&quot;http://en.wikipedia.org/wiki/Rule_of_three_%28statistics%29&quot; rel=&quot;nofollow&quot;&gt;rule of 3&lt;/a&gt;&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;This rule says that if no successes occur in $n$ observations, then $[0, 3/n]$ is an approximate 95% confidence interval for the proportion of successes in the population.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are a number of ways of arriving at this rule, including using a Bayesian derivation of a credible interval.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you get all of your sample of 4000 being bot-posts, you would have a 95% interval for the population proportion of non-bot posts of about $[0, 0.00075]$, or between 99.925% and 100% bot-posts in the population.&lt;/p&gt;&#10;&#10;&lt;p&gt;To do something more sophisticated, I'd suggest considering Nick Sabbe's comment relating to a Bayesian approach. If you have any prior information about the proportion you can incorporate it (not that it will make much difference with a large sample size); you can also incorporate homogeneity of proportion in this framework relatively easily.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2013-04-19T07:22:39.277" Id="56566" LastActivityDate="2013-04-19T08:50:53.553" LastEditDate="2013-04-19T08:50:53.553" LastEditorUserId="805" OwnerUserId="805" ParentId="56562" PostTypeId="2" Score="3" />
  <row AnswerCount="1" Body="&lt;p&gt;I am trying to decompose and forecast a weekly time series which is believed to be affected by moving holidays (e.g. Chinese New Year, which happens in different weeks of a year). &#10;I would like create a regressor variable to reflect the holiday effect on the series. &#10;Is it correct to use the regressor variable as xreg in forecasting stl object / stlf?&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, I would like to know the difference between the following methods, and whether they are doing the job I wanted.&lt;/p&gt;&#10;&#10;&lt;p&gt;1) decompose using stl, then forecast the decomposed object, i.e.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;   model&amp;lt;-stl(tseries,&quot;periodic&quot;)&#10;   forecast&amp;lt;-forecast(model,h=10,method=&quot;arima&quot;,xreg=xreg,newxreg=newxreg)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;2) use stlf directly, i.e.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;forecast&amp;lt;-stlf(tseries,h=10,method=&quot;arima&quot;,xreg=xreg,newxreg=newxreg)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Thanks in advance&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-19T08:00:12.870" FavoriteCount="1" Id="56568" LastActivityDate="2013-07-19T12:29:29.433" LastEditDate="2013-06-19T11:42:19.650" LastEditorUserId="22047" OwnerUserId="24537" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;time-series&gt;&lt;forecasting&gt;" Title="Can I use xreg with stl decomposition to handle moving holiday?" ViewCount="172" />
  
  
  
  
  
  <row AcceptedAnswerId="56607" AnswerCount="2" Body="&lt;p&gt;As the title suggests, how should you set K in K-Nearest Neighbours?&lt;/p&gt;&#10;&#10;&lt;p&gt;Is it just a case of lower values of K are more susceptible to over-fitting and larger values of K are likely to give a more accurate reflection (less susceptible to noise).&lt;/p&gt;&#10;&#10;&lt;p&gt;Also the optimal value of K largely depends on the training set, but I was wondering whether there was a general 'technique' that is used?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-19T14:25:51.467" FavoriteCount="2" Id="56593" LastActivityDate="2013-04-19T19:46:25.547" LastEditDate="2013-04-19T14:34:22.567" LastEditorUserId="17503" OwnerUserId="17503" PostTypeId="1" Score="3" Tags="&lt;machine-learning&gt;&lt;k-nearest-neighbour&gt;" Title="In general how do you set K in K-NN?" ViewCount="100" />
  <row AcceptedAnswerId="56597" AnswerCount="2" Body="&lt;p&gt;I have a data set of x- and y-values, that I want make a linear fit on. Using &lt;code&gt;polyfit(x,y,1)&lt;/code&gt; I get the coefficients &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; for a linear fit &lt;code&gt;ax = b&lt;/code&gt; for this data, but I would also like to find the uncertainty or standard deviation for these coefficients. Does anyone know an easy way of doing this?&lt;/p&gt;&#10;&#10;&lt;p&gt;My Google-fu only gave me &lt;a href=&quot;http://www.mathworks.se/matlabcentral/newsreader/view_thread/103667&quot; rel=&quot;nofollow&quot;&gt;this&lt;/a&gt; result, and seeing as the last answer in that thread is a correction to first answer, I don't know if I should trust any of the answers. The answer from that thread is:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;[z,s]=polyfit(x,y,1);&#10;ste = sqrt(diag(inv(s.R)*inv(s.R')).*s.normr.^2./s.df);&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2013-04-19T09:55:16.180" FavoriteCount="1" Id="56596" LastActivityDate="2014-08-05T09:10:39.563" OwnerDisplayName="Filip Sund" OwnerUserId="24539" PostTypeId="1" Score="1" Tags="&lt;matlab&gt;&lt;curve-fitting&gt;" Title="Finding uncertainty in coefficients from polyfit in Matlab" ViewCount="7839" />
  <row Body="&lt;p&gt;Another option when you have data with a logical lower bound (such as 0, but could be other values) that you know the data will not go below and the regular kernel density estimate places values below that bound (or if you have an upper bound, or both) is to use logspline estimates.  The logspline package for R implements these and the functions have arguments for specifying the bounds so the estimate will go to the bound, but not beyond and still scale to 1.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are also methods (the &lt;code&gt;oldlogspline&lt;/code&gt; function) that will take into account interval censoring, so if those 0's are not exact 0's, but are rounded so that you know they represent values between 0 and some other number (a detection limit for example) then you can give that information to the fitting function.&lt;/p&gt;&#10;&#10;&lt;p&gt;If the extra 0's are true 0's (not rounded) then estimating the spike or point mass is the better approach, but can also be combined with logspline estimation.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-19T15:21:32.397" Id="56604" LastActivityDate="2013-04-19T15:21:32.397" OwnerUserId="4505" ParentId="6588" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm doing some clinical database research and in an effort to lessen the burden on our statistical staff, I started to look for different software solutions to get the analyses I need, and that's where BIDS (business intelligence development studio).  BIDS allows queries to be run against a SQL Server and store the results of those queries in a table or view.  That table or view is then consumed by BIDS and logistic and linear regressions as while as CART analyses can be done on them.&lt;br&gt;&#10;BIDS Version&lt;/p&gt;&#10;&#10;&lt;p&gt;The x axis that is cut off on the lift chart is 'overall population %'&#10;&lt;img src=&quot;http://i.stack.imgur.com/LbpXf.jpg&quot; alt=&quot;enter image description here&quot;&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;A mining accuracy chart of the CART&#10;&lt;img src=&quot;http://i.stack.imgur.com/Vm5Te.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm not quite sure how this works in other stats packages, but in BIDS you define a certain percentage of your data set to train the model and the rest of the set is compared against that model and the lift chart shows the improvement in identifying the outcome you desire vs. a guess.  I'm only vaguely familiar with CART analyses in the first place, and don't know the first thing about R, but this same analysis was done in R with very similar results.  The red portion of what looks like a health meter in a video game corresponds nearly identical to the analysis done in R.  However, there are no p values in the BIDS version.  Consider the node Max Total Poly Pharm &gt;=7 and &amp;lt; 13.&#10;BIDS shows me (not present in the picture)&#10;value         cases    probability&#10;not present   2133     91.89&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there any way to ascertain a p value from that.  And does R use any of it's cases as training data for the model?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-04-19T15:42:25.813" Id="56608" LastActivityDate="2013-04-22T18:07:35.123" OwnerUserId="20778" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;cart&gt;&lt;business-intelligence&gt;" Title="MS business intelligence development studio CART analysis" ViewCount="118" />
  <row Body="&lt;p&gt;The multiway chi-square test computes the marginal chances of each variable and compares the given chances to the products of the marginal chances: by definition, the variables are independent if and only if every one of the joint chances equals the product of the marginal chances.  Thus, if we convert this information into the format expected by your statistical software's contingency table independence test, all we need do is verify that the chi-squared statistic is zero.  This is sufficient because the chi-squared statistic is a sum of squares of residuals; when it is zero, every residual must be zero, whence independence holds.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Here is an example in &lt;code&gt;R&lt;/code&gt;.  It multiplies all the probabilities by a sufficiently large number to make them all integral, which is what a chi-squared test assumes it will be given (it &quot;thinks&quot; these are counts).&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;m = matrix(c(0, 0, 0, 1, 0, 0.039026946,&#10;0, 0, 0, 0, 0, 0.221152694,&#10;0, 0, 0, 0, 1, 0,&#10;0, 0, 0, 1, 1, 0,&#10;0, 0, 1, 0, 0, 0.081381771,&#10;0, 0, 1, 0, 1, 0,&#10;0, 0, 1, 1, 0, 0.014361489,&#10;0, 0, 1, 1, 1, 0,&#10;0, 1, 0, 0, 0, 0.12658807,&#10;0, 1, 0, 0, 1, 0,&#10;0, 1, 0, 1, 0, 0.022339071,&#10;0, 1, 0, 1, 1, 0,&#10;0, 1, 1, 0, 0, 0.046583025,&#10;0, 1, 1, 0, 1, 0,&#10;0, 1, 1, 1, 0, 0.008220534,&#10;0, 1, 1, 1, 1, 0,&#10;1, 0, 0, 0, 0, 0.174007266,&#10;1, 0, 0, 0, 1, 0,&#10;1, 0, 0, 1, 0, 0.030707165,&#10;1, 0, 0, 1, 1, 0,&#10;1, 0, 1, 0, 0, 0.064032769,&#10;1, 0, 1, 0, 1, 0,&#10;1, 0, 1, 1, 0, 0.0112999,&#10;1, 0, 1, 1, 1, 0,&#10;1, 1, 0, 0, 0, 0.09960197,&#10;1, 1, 0, 0, 1, 0,&#10;1, 1, 0, 1, 0, 0.017576818,&#10;1, 1, 0, 1, 1, 0,&#10;1, 1, 1, 0, 0, 0.036652435,&#10;1, 1, 1, 0, 1, 0,&#10;1, 1, 1, 1, 0, 0.006468077,&#10;1, 1, 1, 1, 1, 0), nrow=6)&#10;&#10;data &amp;lt;- data.frame(t(m))&#10;xt &amp;lt;- xtabs(~ X1+X2+X3+X4+X5, data=data)&#10;xt[1:32] &amp;lt;- data$X6 * 10^8&#10;    summary(xt)$statistic&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The output is zero, confirming independence.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-04-19T15:49:12.583" Id="56610" LastActivityDate="2013-04-19T15:49:12.583" OwnerUserId="919" ParentId="56605" PostTypeId="2" Score="3" />
  <row AnswerCount="0" Body="&lt;p&gt;In my experiment I compare the jump height of participants pre and post 2 separate interventions.&lt;/p&gt;&#10;&#10;&lt;p&gt;All participants completed baseline measures of jump height. Participants then underwent a training intervention followed by post test measurements. On a separate occasion the same participants completed a different intervention before post test measurements were taken. &lt;/p&gt;&#10;&#10;&lt;p&gt;I believe I should use paired t-tests to compare these groups as the same same participants were tested pre and post. However, I also want to compare the post test scores from intervention 1 with the post test scores from intervention 2 (to see if there were significant differences between the two interventions). &lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a test I should use to achieve this in one? Or do I use the paired t-tests for pre and post comparisons and a further test for the post-post comparison?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-19T16:41:34.593" Id="56616" LastActivityDate="2013-04-19T16:41:34.593" OwnerUserId="24557" PostTypeId="1" Score="1" Tags="&lt;spss&gt;&lt;t-test&gt;" Title="Which statistical test should i use? pre-post intervention comparisions and post-post comparisons" ViewCount="533" />
  <row Body="" CommentCount="0" CreationDate="2013-04-19T22:53:01.717" Id="56646" LastActivityDate="2013-04-19T22:53:01.717" LastEditDate="2013-04-19T22:53:01.717" LastEditorUserId="686" OwnerUserId="686" PostTypeId="5" Score="0" />
  
  <row Body="&lt;p&gt;Find PNet here: &lt;a href=&quot;http://sna.unimelb.edu.au/PNet&quot; rel=&quot;nofollow&quot;&gt;http://sna.unimelb.edu.au/PNet&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;This is Java based software for fitting exponential random graph models, now including a multilevel version.&lt;/p&gt;&#10;&#10;&lt;p&gt;Incidentally, RSiena does not fit ERGM models. The old R-independent SIENA software (which is no longer maintained) did, however.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-04-20T16:59:01.847" Id="56688" LastActivityDate="2013-04-20T16:59:01.847" OwnerUserId="24000" ParentId="56686" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="56706" AnswerCount="1" Body="&lt;p&gt;I have an experimental sample, size of about 1000 values​​. I need to generate a much larger sample for simulation. I can create a samples like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(ks)&#10;x&amp;lt;-rlnorm(1000)&#10;y&amp;lt;-rkde(fhat=kde(x=x, h=hpi(x)), n=10000, positive = TRUE)#&#10;z&amp;lt;-sample(x, 10000, replace = TRUE)&#10;&#10;par(mfrow=c(3,1))&#10;hist(x, freq=F, breaks=100,  col=&quot;red&quot;)&#10;hist(y, freq=F, breaks=100,  col=&quot;green&quot;)&#10;hist(z, freq=F, breaks=100,  col=&quot;blue&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;What fundamental limitations when using KDE or bootstrap? How else can I create such a sample?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-04-20T20:49:42.387" Id="56700" LastActivityDate="2013-04-20T21:57:31.850" OwnerUserId="24503" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;bootstrap&gt;&lt;random-generation&gt;&lt;kde&gt;" Title="Random sample using KDE or bootstrapping" ViewCount="118" />
  <row Body="&lt;p&gt;With the KDE you are smoothing your data to get guess what the distribution is. Then you are sampling from that distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;With bootstrapping, you are resampling from the data that you already have.&lt;/p&gt;&#10;&#10;&lt;p&gt;One difference between the two methods is that KDE can result in values that you did not observe in your original sample. In bootstrap this cannot happen. Whether this is good or bad depends on what you know about this distribution. If you think your sample already represents well the distribution, bootstrapping might be better.&lt;/p&gt;&#10;&#10;&lt;p&gt;A third approach might be to look at the histogram or use some prior knowledge about the distribution to fit some parametric distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;You have to understand though, that there are general limitations to what you are trying to do. In general, unless you have some prior beliefs about your distribution, you will not be able to &quot;discover&quot; a phenomenon that you did not observe in your sample by generating a larger sample artificially in this manner. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-04-20T21:57:31.850" Id="56706" LastActivityDate="2013-04-20T21:57:31.850" OwnerUserId="14434" ParentId="56700" PostTypeId="2" Score="1" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I hope this question is not off topic on CV:&lt;/p&gt;&#10;&#10;&lt;p&gt;I am currently fitting some different models (naive bayes, logistic regression...) in R which I up to now thought of as prototypes for a later Java implementation of the same models. &lt;/p&gt;&#10;&#10;&lt;p&gt;Now I came across multiple ways of executing R code from Java. &lt;/p&gt;&#10;&#10;&lt;p&gt;I am sure people were in a similar situation before, and I was wondering if you stick to Java libraries (or any other programming language) for solving statistical problems, or whether you execute R code from your programming environment? Since R is such a rich language for statistical problems, I do not see much motivation for using Java libraries for solving such tasks.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-04-21T05:18:53.313" Id="56729" LastActivityDate="2013-04-23T08:10:19.600" OwnerUserId="20563" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;logistic&gt;&lt;java&gt;" Title="Using R in Java Project" ViewCount="79" />
  
  
  <row Body="&lt;p&gt;$|X_{1,1} - \mu|$ cannot have a hypergeometric distribution in general because $\mu$ does not need to be an integer value and then $|X_{1,1} - \mu|$ would not be an integer. But conditionally on the margins, $X_{1,1}$ will have a hypergeometric distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you do it properly and fix the margins to known values, you can consider $X_{1,1}$ (or any other cell) to be your statistic. With the analogy of drawing $k$ balls from an urn containing $W$ white balls and $B$ black balls without replacement, $X_{1,1}$ can be interpreted as the number of white balls drawn, where $B$ is the sum of the first row, $W$ is the sum of the second row, $k$ is the sum of the first column.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-21T18:37:19.560" Id="56776" LastActivityDate="2013-04-21T18:37:19.560" OwnerUserId="10849" ParentId="56773" PostTypeId="2" Score="4" />
  <row AcceptedAnswerId="56882" AnswerCount="3" Body="&lt;p&gt;In R, if I call the &lt;code&gt;lm()&lt;/code&gt; function in the following way:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lm.1 = lm(response ~ var1 + var2 + var1 * var2)&#10;summary(lm.1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This gives me a linear model of the response variable with &lt;code&gt;var1&lt;/code&gt;, &lt;code&gt;var2&lt;/code&gt; and the interaction between them. However, how exactly do we numerically interpret the interaction term?&lt;/p&gt;&#10;&#10;&lt;p&gt;The documentation says this is the &quot;cross&quot; between &lt;code&gt;var1&lt;/code&gt; and &lt;code&gt;var2&lt;/code&gt;, but it didn't give an explanation of what exactly the &quot;cross&quot; is.&lt;/p&gt;&#10;&#10;&lt;p&gt;It would be helpful for me to know what exact numbers R is calculating to incorporate the interaction between the two variables. Thank you very much!&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-04-21T20:26:31.090" FavoriteCount="2" Id="56784" LastActivityDate="2013-04-22T17:35:00.253" LastEditDate="2013-04-21T22:08:05.617" LastEditorUserId="16974" OwnerUserId="24641" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;regression&gt;" Title="how to interpret the interaction term in lm formula in R?" ViewCount="1366" />
  <row Body="&lt;p&gt;Suppose you get point estimates of 4 for $x_1$, 2 for $x_2$ and 1.5 for the interaction. Then, the equation is saying that the &lt;code&gt;lm&lt;/code&gt; fit is&lt;/p&gt;&#10;&#10;&lt;p&gt;$y = 4x_1 + 2x_2 + 1.5x_1x_2$&lt;/p&gt;&#10;&#10;&lt;p&gt;Is that what you wanted? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-21T21:16:29.840" Id="56790" LastActivityDate="2013-04-21T21:16:29.840" OwnerUserId="686" ParentId="56784" PostTypeId="2" Score="1" />
  <row AnswerCount="3" Body="&lt;p&gt;Suppose we have a baseline exposure group and 2 other exposure groups for a case control study. Suppose the odds ratio for the first exposure is $1.5$ and the odds ratio for the second exposure is $1.8$. Does this mean that cases are $1.5$ times as likely to have exposure 1 than the controls? Also does this mean that cases are $1.8$ times as likely to have exposure 2 than the controls? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-21T22:45:57.763" FavoriteCount="2" Id="56793" LastActivityDate="2014-02-13T16:07:11.867" LastEditDate="2014-02-13T16:07:11.867" LastEditorUserId="17230" OwnerUserId="24643" PostTypeId="1" Score="1" Tags="&lt;epidemiology&gt;&lt;odds-ratio&gt;&lt;case-control-study&gt;" Title="How to interpret odds ratio?" ViewCount="1194" />
  
  
  <row Body="&lt;p&gt;Bootstrapping involves a random process, so there is uncertainty in your estimate of the confidence bounds. This will show up as these confidence bounds being rather ragged with only 100 replications. With 100 replications the lower bound will be based on the 2nd and 3rd smallest occurrences. So only a few &quot;weird&quot; samples can influence your estimate of the lower bound (and similarly the upper bound). If you take 20,000 replications the lower bound will be based on the 500th smallest occurrence, and thus tends to be a lot more stable.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-22T08:34:15.583" Id="56829" LastActivityDate="2013-08-20T13:37:14.497" LastEditDate="2013-08-20T13:37:14.497" LastEditorUserId="22047" OwnerUserId="23853" ParentId="56738" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;We know that as the number of independent variables increases, the coefficient of determination $R^2$ will increase but the adjusted $R^2$ may or may not increase. In the following question for the sake of simplicity I shall write only $R^2$ but it must be understood that the question applies to both $R^2$ and adjusted $R^2$. Further we shall make life easier by assuming all the conditions and assumptions of multiple regression are satisfied.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Consider a multiple regression where the the  dependent variable $y$ depends on at most $n$ independent variables $x_1, x_2, \ldots, x_n$. For a given $k$, $1 \le k \le n$, we find best $k$ variable liner fit for $y$. Let us denote the coefficient of determination of this best fit by $R_{max}^2(k)$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Similarly we find the worst possible $k$ variable fit and we denote its coefficient of determination by $R_{min}^2(k)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Trivially we have the following bounds&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;R_{min}^2(k) \ge R_{min}^2(1)&#10;$$&#10;and&#10;$$&#10;R_{max}^2(k) \le R_{max}^2(n) = R^2(n).&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is can we find improve and express the above bounds in terms of non trivial functions involving $n$, $k$, $R_{min}^2(1)$ and $R_{max}^2(n)$. Is any additional assumptions is required to obtain such non trivial bounds?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Motivation&lt;/strong&gt;: I am currently working on linear modeling where I have a large number of independent variables and I need a way to determine how small or large the coefficient of determination will be for a given $k$. Currently I am following a various algorithmic approaches and writing a programs that gives above bounds. This method is not much useful because despite using the best known algorithms such as leaps computation takes a lot of time as the number of variables increases. Therefore I want to see if a theoretical bound is possible.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;My progress so far&lt;/strong&gt;: Based on heuristic data I have generated using a computer program, I find that $R_{max}^2(k)$ approximately follows a logistic model&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;R_{max}^2(k) \approx \frac{R^2(n)}{1+ae^{-bk}}&#10;$$&#10;where $a$ and $b$ are some local constants which depends on the data being analyzed.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-04-22T08:41:16.570" FavoriteCount="2" Id="56831" LastActivityDate="2013-04-22T11:58:02.203" LastEditDate="2013-04-22T11:58:02.203" LastEditorUserId="183" OwnerUserId="21660" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;multiple-regression&gt;&lt;bounds&gt;" Title="Can we find bounds on R-squared?" ViewCount="179" />
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a slight confusion regarding &lt;strong&gt;seasonal&lt;/strong&gt; models and which polynomial to use for conducting unit root tests. &lt;/p&gt;&#10;&#10;&lt;p&gt;Given a model:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\phi(B)\Phi(B^s)\Delta^d\Delta^D_S X_t = \theta(B)\Theta(B^s)\epsilon    $&lt;/p&gt;&#10;&#10;&lt;p&gt;When I compute the roots for unit root tests, &lt;strong&gt;which polynomials do I calculate the roots of&lt;/strong&gt;?&lt;/p&gt;&#10;&#10;&lt;p&gt;Only the basic $AR(p)$ and $MA(q)$ compononents? That is, do I use just $\phi(B)$ and $\theta(B)$? &lt;/p&gt;&#10;&#10;&lt;p&gt;Or instead, do I also include the seasonal polynomials as well? &#10;ie, compute the roots by solving for $\phi(B)\Phi(B^s)$ and $\theta(B)\Theta(B^s)$ ?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-22T14:17:19.847" Id="56866" LastActivityDate="2013-04-22T14:17:19.847" OwnerUserId="20721" PostTypeId="1" Score="2" Tags="&lt;arima&gt;&lt;polynomial&gt;&lt;unit-root&gt;" Title="Unit root test for ARIMA models" ViewCount="81" />
  
  <row Body="&lt;p&gt;The standard way to write the prediction equation for your model is: &lt;/p&gt;&#10;&#10;&lt;p&gt;$\hat y = b_0 + b_1*x_1 + b_2*x_2 + b_{12} * x_1 *x_2$&lt;/p&gt;&#10;&#10;&lt;p&gt;But understanding the interaction is a little easier if we  factor this differently:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\hat y = (b_0 + b_2*x_2) + (b_1 + b_{12}*x_2) * x_1$&lt;/p&gt;&#10;&#10;&lt;p&gt;With this factoring we can see that for a given value of $x_2$ the y-intercept for $x_1$ is $b_0 + b_2*x_2$ and the slope on $x_1$ is $(b_1 + b_{12}*x_2)$.  So the relationship between $y$ and $x_1$ depends on $x_2$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Another way to understand this is by plotting the predicted lines between $y$ and $x_1$ for different values of $x_2$ (or the other way around).  The &lt;code&gt;Predict.Plot&lt;/code&gt; and &lt;code&gt;TkPredict&lt;/code&gt; functions in the TeachingDemos package for R were designed to help with these types of plots.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-22T17:35:00.253" Id="56882" LastActivityDate="2013-04-22T17:35:00.253" OwnerUserId="4505" ParentId="56784" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am facing the evaluation of a genetic programming algorithm. I am using the Proben1 cancer1 dataset to evaluate the models created by this algorithm. This dataset contains 699 samples, which is currently divided into 50% training, 25% validation and 25% test data. Many academic articles use k-fold validation for evaluation of the resulting models. &lt;/p&gt;&#10;&#10;&lt;p&gt;I do understand the creation of k models to reduce the variance in the error rating percentage. However, I do not understand why it would not be preferable to do x times the hold-out (k=2) method, where every time the data is randomly partitioned into training and test data. &lt;/p&gt;&#10;&#10;&lt;p&gt;The reason for my lack of understanding is that I think that models evaluated with a higher order k generalize less due to simple fact that they are trained with a higher percentage of the data. &lt;/p&gt;&#10;&#10;&lt;p&gt;Given my data set of 699 samples which of the following two methods would be preferable and why; &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;the division in training/validation/test and perhaps this repeated x&#10;times randomly assigning sample to each set with a test set of 25% &lt;/li&gt;&#10;&lt;li&gt;10-fold cross validation but thus with a test set of just 10%.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2013-04-22T23:27:15.770" FavoriteCount="1" Id="56906" LastActivityDate="2013-04-22T23:34:57.043" LastEditDate="2013-04-22T23:34:57.043" LastEditorUserId="24694" OwnerUserId="24694" PostTypeId="1" Score="3" Tags="&lt;machine-learning&gt;&lt;classification&gt;&lt;cross-validation&gt;" Title="k-fold cross validation vs k times hold-out validation" ViewCount="535" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm new to  inter rater reliability calculations.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have 5 developers and 6 raters. The raters ranked the first 3 of these developers based on some criteria like the following example:  &lt;/p&gt;&#10;&#10;&lt;p&gt;For example,&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;         dev1  dev2  dev3  dev4   dev5&#10;ranker1   1     2     3&#10;ranker2   3     1     2&#10;ranker3         3     1     2&#10;ranker4               1     3      2&#10;ranker5   2     1           2&#10;ranker6         3     1            2&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The ratings measure the &quot;level of expertise&quot;: &quot;1&quot; the best expertise, &quot;2&quot; less expertise, and &quot;3&quot; lowest expertise&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is what the best algorithm to calculate the  inter rater reliability among these raters?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-23T02:03:07.987" Id="56915" LastActivityDate="2013-04-23T04:38:33.050" LastEditDate="2013-04-23T04:38:33.050" LastEditorUserId="183" OwnerUserId="24696" PostTypeId="1" Score="5" Tags="&lt;inter-rater&gt;" Title="Calculating inter rater reliability where raters and ratees only partially overlap" ViewCount="141" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have several datasets in R+, each containing two training and test sets. For example the following &lt;a href=&quot;http://i.imgur.com/I1fx2kB.png&quot; rel=&quot;nofollow&quot;&gt;dataset&lt;/a&gt;. I want to train a classifier by using training data such that by applying the test data, I get some reasonable number of points as anomaly so that I can analysis the related situations.  The higher the value, the more abnormal it is. &lt;/p&gt;&#10;&#10;&lt;p&gt;By reasonable number I mean it to be less than P% in each 100 points (each day 100 points are generated, most of them should be considered normal and I want to analysis about P of the most abnormal ones). &lt;/p&gt;&#10;&#10;&lt;p&gt;I tried K-means with K=2. But as you see in the above link, anomaly cluster is selected by the outliers to be too high. So there would be no anomaly in test data.&lt;/p&gt;&#10;" ClosedDate="2013-04-23T14:38:28.853" CommentCount="0" CreationDate="2013-04-23T06:16:58.107" Id="56924" LastActivityDate="2013-04-23T09:54:39.120" OwnerUserId="24700" PostTypeId="1" Score="0" Tags="&lt;machine-learning&gt;&lt;clustering&gt;&lt;k-means&gt;" Title="Clustering a dataset to get the most abnormal data" ViewCount="78" />
  
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I have a repeated measures (time 1, time 2) experimental design and would like to run a confirmatory factor analysis on the scales I've used and to follow up with a cronbach's alpha. However I am unsure if I should run the analyses on time 1 or time 2 data. Any advise?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-04-23T14:37:37.797" Id="56967" LastActivityDate="2013-04-23T16:46:27.820" OwnerUserId="24723" PostTypeId="1" Score="2" Tags="&lt;repeated-measures&gt;&lt;factor-analysis&gt;" Title="Cronbach's Alpha and Factor Analysis for Repeated Measures Design" ViewCount="542" />
  <row AnswerCount="1" Body="&lt;p&gt;I am working on a project that involves computing similarity metrics between strings. I was would like to know whether it is possible to use hamming distance on strings with differnt length, and if possible, how to go about it. I step by step explanation would be grately appreciated.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-04-23T15:06:44.367" FavoriteCount="1" Id="56972" LastActivityDate="2013-04-23T15:13:05.870" OwnerUserId="24334" PostTypeId="1" Score="3" Tags="&lt;summary-statistics&gt;" Title="Hamming distance for strings with different length" ViewCount="1003" />
  
  
  <row Body="&lt;p&gt;@JeremyMiles is right.  First, there's a rule of thumb that the ANOVA is robust to heterogeneity of variance so long as the largest variance is not more than 4 times the smallest variance.  Furthermore, the general effect of heterogeneity of variance is to make the ANOVA less efficient.  That is, you would have lower power.  Since you have a significant effect anyway, there is less reason to be concerned here.  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Update:&lt;/em&gt;  &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;I demonstrate my point about lower efficiency / power here: &lt;a href=&quot;http://stats.stackexchange.com/a/94625/7290&quot;&gt;Efficiency of beta estimates with heteroscedasticity&lt;/a&gt;  &lt;/li&gt;&#10;&lt;li&gt;I have a comprehensive overview of strategies for dealing with problematic heteroscedasticity in one-way ANOVAs here: &lt;a href=&quot;http://stats.stackexchange.com/a/91881/7290&quot;&gt;Alternatives to one-way ANOVA for heteroscedastic data&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="2" CreationDate="2013-04-23T18:20:29.320" Id="56994" LastActivityDate="2014-07-09T18:56:48.663" LastEditDate="2014-07-09T18:56:48.663" LastEditorUserId="7290" OwnerUserId="7290" ParentId="56971" PostTypeId="2" Score="9" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am about to use this input for my data but i cant find under which packages does it work.&#10;I searched almost everywhere. &#10;m&amp;lt;-pdlglm(yy~pdl(zz,4,2)+sin(zz)+pdl(xx,4,4))&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-04-23T18:59:20.390" Id="56996" LastActivityDate="2013-04-23T18:59:20.390" OwnerUserId="24351" PostTypeId="1" Score="0" Tags="&lt;lags&gt;" Title="Polynomial distributed lag glms" ViewCount="77" />
  <row Body="&lt;p&gt;One assumption of the two sample t-test is that both samples come from a normal population. If you are convinced your samples both do not come from normal populations, you could use the Wilcoxon rank-sum test to test for difference in medians.&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2013-04-23T19:18:54.257" Id="56999" LastActivityDate="2013-04-23T19:18:54.257" OwnerUserId="21900" ParentId="56981" PostTypeId="2" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;Assuming the data structure given below,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;  \left[\begin{array}{l|l}&#10;  y &amp;amp; x \\ \hline \hline&#10;  y_{1} &amp;amp; x_{1} \\&#10;  y_{2} &amp;amp; x_{2} \\&#10;  \vdots &amp;amp; \vdots \\&#10;  y_{n} &amp;amp; x_{n} \\  &#10;  \end{array}\right]&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose we want to fit a Poisson regression model such that $y_{i} \sim \text{Pois}(\mu_{i})$ for $i=1,2\ldots,n$.&lt;/p&gt;&#10;&#10;&lt;p&gt;where: $\mu_{i} = e^{\beta_{0} + \beta_{1}x_{i}}$&lt;/p&gt;&#10;&#10;&lt;p&gt;The Fisher information can be found by:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ I(\beta) = \sum_{i=1}^{n}\mu_{i}x_{i}x_{i}^{T} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;Supposing we have the MLEs ($\hat{\beta}_{0}$ and $\hat{\beta}_{1}$) for $\beta_{0}$ and $\beta_{1}$, from the above, we should be able to find the Fisher information for both model parameters at the maximum:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ I(\hat{\beta}) = \begin{bmatrix} I_{11} &amp;amp; I_{12} \\ I_{21} &amp;amp; I_{22} \end{bmatrix} $$.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have the data and the resulting Fisher information matrix corresponding to the model above; however, when I try to code the Fisher information from the above manually, my result for the Fisher information differs from correct result.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have included the a snippet of R code. Perhaps somebody could tell me if the following code will achieve the Fisher information matrix based on the above model specification; or, more generally, if the model specification is correct.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;mu &amp;lt;- rep(0, n) # Initialize data structure for mu vector.&#10;    for (i in 1:n) {&#10;          # Assign values to mu vector.&#10;        mu[i] &amp;lt;- exp(beta[1]+(beta[2]*x[i,1:2])) &#10;    }&#10;&#10;  # Initialize Fisher information matrix.&#10;infor &amp;lt;- matrix(rep(0, 4), byrow=TRUE, nrow=2, ncol=2) &#10;for (i in 1:n) {&#10;    temp_matrix &amp;lt;- mu[i]*x[i,1:2]%*%t(x[i,1:2]) # Hold current index value.&#10;    infor &amp;lt;- infor + temp_matrix # Sum to Fisher information matrix.&#10;}&#10;&#10;print(infor)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Even though there is only one independent variable ($x$), I have augmented the matrix with a column of ones corresponding to the intercept parameter ($\beta_{0}$), as is usually the case with normal regression. Therefore, the independent (design) matrix is of the form:&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;$$ X = &#10;\begin{bmatrix} 1 &amp;amp; x_{1,2} \\ 1 &amp;amp; x_{2,2} \\ \vdots &amp;amp; \vdots \\ 1 &amp;amp; x_{n,2} &#10;\end{bmatrix} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;Hence each row &lt;code&gt;i&lt;/code&gt; iterates across &lt;code&gt;x[i,1:2]&lt;/code&gt;. Should this column be included?&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;In the preceding R code, it is assumed that data structures and values not initialized and/or assigned explicitly in the code have been previously initialized and/or assigned.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Furthermore, the code is designed to be explicit, not efficient.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="2" CreationDate="2013-04-23T21:30:14.977" Id="57005" LastActivityDate="2013-07-23T00:05:46.170" LastEditDate="2013-07-23T00:05:46.170" LastEditorUserId="7290" OwnerUserId="9171" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;self-study&gt;&lt;poisson&gt;&lt;poisson-regression&gt;&lt;fisher-information&gt;" Title="Fisher information for Poisson model" ViewCount="539" />
  <row Body="&lt;p&gt;This psycho-visual problem is the consequence of the mind making effective bisquare approximations when thinking about &quot;distance&quot;.  It is most often addressed by plotting the variable of interest, in this case the difference, as its own variable.  Personally I would put this into a subplot with y-axis label as &quot;difference between domestic and international&quot; or such.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-24T00:19:47.747" Id="57011" LastActivityDate="2013-04-24T00:19:47.747" OwnerUserId="22452" ParentId="57006" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;Random Forest is invariant to monotonic transformations of individual features.  Translations or per feature scalings will not change anything for the Random Forest.  SVM will probably do better if your features have roughly the same magnitude, unless you know apriori that some feature is much more important than others, in which case it's okay for it to have a larger magnitude.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-24T01:43:41.850" Id="57018" LastActivityDate="2013-04-24T01:43:41.850" OwnerUserId="4164" ParentId="57010" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;I am not a big fan of converting a continuous variable to multiple dummy variables. I guess the binning procedure is considered standard practice in score card development.&lt;/p&gt;&#10;&#10;&lt;p&gt;Regarding dummy variable insignificance:  When you add a dummy variable in regression, the omitted group act as reference group. The reference group is compared to other groups corresponding to the dummy variables. When variables have a nonlinear relationship (e.g. quadratic) with log odds, you may get some dummy variables that are insignificant (the group whose effect is near to the reference group). My suggestion to see the pattern of log-odds in each bin before merging. Either you can make fewer final bins depending one the pattern or change the reference group. I know it is bit abstract. But, I will not be able to go to specific without knowing the case. &lt;/p&gt;&#10;&#10;&lt;p&gt;You could also drop the insignificant variable. Doing it this way, you are merging the group associated with dropping dummy. It may not be appropriate if the merging of reference group and the dummy group (insignificant) doesn't make business sense.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-24T05:23:11.297" Id="57029" LastActivityDate="2013-05-24T08:33:54.543" LastEditDate="2013-05-24T08:33:54.543" LastEditorUserId="805" OwnerUserId="7788" ParentId="57028" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;This is the problem of &lt;a href=&quot;http://www.scholarpedia.org/article/Sampling_bias&quot; rel=&quot;nofollow&quot;&gt;limited sampling bias&lt;/a&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;The small sample estimates of the densities are noisy, and this variation induces spurious correlations between the variables which increase the estimated information value.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the discrete case this is a well studied problem. There are many techniques to correct, from the fully Bayesian (&lt;a href=&quot;http://nsb-entropy.sourceforge.net/&quot; rel=&quot;nofollow&quot;&gt;NSB&lt;/a&gt;), to simple corrections. The most basic (Miller-Madow) is to subtract $(R-1)(S-1) / 2N\ln2$ from the value. This is the difference in degrees of freedom between the two implicit models (full joint multinomial vs the product of independent marginals) - indeed with sufficient sampling $2N\ln(2)I$ is the likeilhood ratio test of indepenence (&lt;a href=&quot;http://en.wikipedia.org/wiki/G-test&quot; rel=&quot;nofollow&quot;&gt;G-test&lt;/a&gt;) which is $\chi^2$ distributed with $(R-1)(S-1)$ d.o.f. under the null hypothesis. With limited trials it can even be hard to estimate R and S reliably - an effective correction is to use a Bayesian counting procedure to estimate these (Panzeri-Treves or PT correction).&lt;/p&gt;&#10;&#10;&lt;p&gt;Some package implementing these techniques in Matlab include &lt;a href=&quot;http://www.ibtb.org/&quot; rel=&quot;nofollow&quot;&gt;infotoolbox&lt;/a&gt; and &lt;a href=&quot;http://neuroanalysis.org/neuroanalysis/goto.do?page=.repository.toolkit_home&quot; rel=&quot;nofollow&quot;&gt;Spike Train Analysis Toolkit&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;For the continuous case, estimators based on &lt;a href=&quot;http://arxiv.org/abs/cond-mat/0305641&quot; rel=&quot;nofollow&quot;&gt;nearest neighbour distances&lt;/a&gt; reduuce the problem.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-24T08:35:35.707" Id="57038" LastActivityDate="2013-04-24T08:41:17.823" LastEditDate="2013-04-24T08:41:17.823" LastEditorUserId="5407" OwnerUserId="5407" ParentId="35089" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;I don't really understand what variance you are talking about, is it a variance of variables over one measure ? a variance of a variable over multiple measures ?&lt;/p&gt;&#10;&#10;&lt;p&gt;The more a variable has variance the more it contains information. Think about it in variation: if a variable has little variance it does not evolve much around its mean and then it won't be usefull to learn from the variable. On the contrary, variables with lots of variance will be usefull.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-24T09:53:36.153" Id="57051" LastActivityDate="2013-04-24T09:53:36.153" OwnerUserId="21846" ParentId="57039" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;I am looking at whether Corporate Venture Capital-backed firms (1) perform better than Independent Venture capital - backed firms(0) in their POST-IPO performance. My assumption is that this relationship is either strengthened or weakened by the amount of Experience (continuous). &lt;/p&gt;&#10;&#10;&lt;p&gt;After creating a new variable for Corporate VC*Experience, I then put these values in a regression and it gives me a VIF of over 23.  &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-04-24T11:39:03.807" Id="57058" LastActivityDate="2013-04-24T11:53:26.100" OwnerUserId="24767" PostTypeId="1" Score="1" Tags="&lt;categorical-data&gt;&lt;multicollinearity&gt;" Title="How to eliminate high multicollinearity with a continuous moderating variable, and a categorical independent variable" ViewCount="371" />
  
  
  
  <row Body="&lt;p&gt;Model the actions of the players instead of the payoffs. That is, predict the probability that a player selects to cooperate at a particular round as a function of previous rounds (if the game is repeated in your setting) and your covariates. I think this makes more causal sense, as the players actually select the actions influenced by whatever, and the payoffs are just a deterministic function of the actions. Furthermore, this makes the output variables binary, which simplifies the analysis, as you do not have to think about the potentially difficult dependence between total payoffs.&lt;/p&gt;&#10;&#10;&lt;p&gt;I guess it is also probably fine to treat the strategies selected by each player as conditionally independent given the covariates&amp;amp;history, which makes the analysis just simple prediction of a binary variable. On the other hand, one could argue that unobserved variables might lead to dependence. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3435562/&quot; rel=&quot;nofollow&quot;&gt;Angel Sanchéz&lt;/a&gt; has applied logistic regression to modeling the probability of cooperating in Prisoner's dilemma. Their setting is probably somewhat different as it involves multiple players in a network, but you should still take a look to see if their approach can be modified to your setting.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-24T15:41:28.160" Id="57093" LastActivityDate="2013-04-24T15:41:28.160" OwnerUserId="24669" ParentId="50074" PostTypeId="2" Score="0" />
  
  
  <row Body="&lt;p&gt;Perhaps you can bound your variance. Suppose, for example, that you know your data must be in the range $[a,b]$. Then Popoviciu's inequality bounds your variance by $\sigma^2 \le (1/4)(b-a)^2$. Using the upper bound in the formulas you found will be a bit of overkill, but it should satisfy your requirements.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-24T17:38:47.987" Id="57108" LastActivityDate="2013-05-24T18:58:43.127" LastEditDate="2013-05-24T18:58:43.127" LastEditorUserId="5176" OwnerUserId="24073" ParentId="44189" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a dataset $X = \{x_1, ..., x_n\}$. I also have three algorithms, $A_1, A_2, A_3$, that each take a single data point as input and produce some measure of how well they performed. If I apply each of the algorithms to each of the data, I get a matrix of performance measures, $M$, where $M_{a,i}$ is the value of $A_a(x_i)$. This matrix is therefore $3 \times n$.&lt;/p&gt;&#10;&#10;&lt;p&gt;What statistical methods are most appropriate to analyse the differences in performance of these algorithms and the significance of any differences?&lt;/p&gt;&#10;&#10;&lt;p&gt;If it matters, the &quot;better&quot; algorithm is that which gives performance measures closer to zero. I'm interested in determining whether any algorithm is &lt;em&gt;significantly&lt;/em&gt; better than another.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-04-24T18:19:52.683" Id="57112" LastActivityDate="2013-04-24T19:40:15.153" LastEditDate="2013-04-24T19:40:15.153" LastEditorUserId="24789" OwnerUserId="24789" PostTypeId="1" Score="1" Tags="&lt;statistical-significance&gt;&lt;algorithms&gt;&lt;method-comparison&gt;" Title="Statistical analysis of multiple algorithms over a single dataset" ViewCount="43" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Are the values of &lt;code&gt;xcorr(x,y)&lt;/code&gt; in MATLAB correlation values or not? I'm asking this because in MATLAB &lt;code&gt;xcorr(x,y,'coeff')&lt;/code&gt; normalizes values. Is it normalizing covariance values to get correlations? &lt;/p&gt;&#10;&#10;&lt;p&gt;I'm confused if cross correlation values are necessarily between -1 and 1 like Pearson correlation values.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, I see from running an example that &lt;code&gt;xcorr(x,y,0,'coeff')&lt;/code&gt; != &lt;code&gt;corr(x,y)&lt;/code&gt;. Could someone explain this?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-04-24T18:38:58.130" FavoriteCount="1" Id="57115" LastActivityDate="2015-02-11T16:00:57.227" LastEditDate="2013-04-25T01:22:41.733" LastEditorUserId="805" OwnerUserId="24790" PostTypeId="1" Score="2" Tags="&lt;correlation&gt;&lt;matlab&gt;&lt;cross-correlation&gt;" Title="Cross Correlation (xcorr in MATLAB) vs Pearson Correlation (corr in MATLAB)" ViewCount="8638" />
  
  <row Body="&lt;p&gt;Interesting question. I believe that the answer depends on every single element of the problem: the true model, the estimators employed, ...&lt;/p&gt;&#10;&#10;&lt;p&gt;An extreme example is: let $x=(x_1,...,x_n)$ be an independent sample where $x_j\sim \text{Cauchy}(0,1)$ and define the estimators $\hat{\theta}_1=\text{mean}(x)$ and $\hat{\theta}_2=\text{median}(x)$. Now, suppose that you thought that those observations came from a Normal distribution $N(\mu,1)$ and that you are interested on estimating $\mu$. Under these assumptions $\hat{\theta}_1\stackrel{a.s.}{\rightarrow} \mu$ by the Law of large numbers and $\hat{\theta}_2\stackrel{a.s.}{\rightarrow} \mu$ by Glivenko-Cantelli Theorem.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, considering the true generating model, since the mean of the Cauchy distribution does not exist $\hat{\theta}_1$ behaves erratically (given that its limit does not exist) while $\hat{\theta}_2$ still converges to $0$ (since Glivenko-Cantelli still applies).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-04-24T21:33:00.183" Id="57131" LastActivityDate="2013-04-24T22:12:42.633" LastEditDate="2013-04-24T22:12:42.633" LastEditorUserId="24794" OwnerUserId="24794" ParentId="57109" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;One way to reconstruct axis scales is with--statistical analysis!&lt;/p&gt;&#10;&#10;&lt;p&gt;First capture the image in the finest detail possible and digitize reference marks as accurately as you reasonably can.  Here's an excerpt of my effort, which looks good to a small fraction of a pixel:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/04gbx.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Then create a table matching the digitized coordinates to the labels.  Here's a piece of that table:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Up4xi.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Because the x-coordinates are essentially constant, I'm confident there's not enough slanting in the image to mess things up.  Now we only need find a numerical relationship between the two.  For this, &lt;a href=&quot;http://formulize.nutonian.com/&quot; rel=&quot;nofollow&quot;&gt;Eureqa formulize&lt;/a&gt; does a wonderful job.  (It's a free download and takes almost no time to learn if you know anything about curve fitting.)  Here's half of the information shown while it's searching:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/zz59N.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;On the left the best solutions found so far are listed.  (By this time a third of a million generations in a genetic algorithm had been created, taking about ten minutes total.) &quot;Size&quot; is a measure of their complexity and &quot;fit&quot; measures the goodness of fit (in pixels).  Given that I have digitized to a small fraction of a pixel, but certainly not as good as a tenth (the image isn't that good), I would be content with the solutions of size 9 or 10 shown here.  The fit of solution 10 is shown at the right.  (Solution 9's fit veers away from the points at the lower right: such a systematic departure is an indication that the solution is not correct.)&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;As you can see, there isn't a real nice relationship here:&lt;/strong&gt; nothing like a linear transformation of an exponential or logarithm, for instance.  As a double-check I looked for a nice inverse relationship relating the labeled value to the pixel height.  Nothing any better is emerging (the software is running in the background--using all eight cores--as I write this).  I tentatively conclude that the vertical scale is almost as imaginative as the horizontal one, created by the stroke of an artist's brush and not by the pen of any scientist.  Regardless, I now have formulas that can be used to recreate (or even--at huge risk) extend the y-axis.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Here are the digitized points&lt;/strong&gt; for anyone who would like to investigate further.  Values for X and Y are in pixels within the image (transposed into separate lines in order to save screen space).  The precision of the digitizer itself was approximately 0.06 pixels, and I made an effort to digitize the y-values accurately and consistently (paying little attention to the x-values), but I wouldn't trust individual points to more than about 0.5 pixels, given that the linewidth in the image is typically two pixels.  Values of Z are inferred from the labels on the Y axis based on the supposition that the ticks subdivide each decade into equal intervals.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;X   84.127  84.269  84.127  84.269  84.269  84.198  84.340  84.198  84.340  84.269  84.411  84.411  84.269  84.411  84.340  84.482  84.482  84.198  84.056  84.624  84.127&#10;Y   407.016 383.463 359.484 335.647 311.243 287.264 270.522 253.637 236.894 220.010 203.126 190.072 176.167 162.830 149.776 136.297 125.088 114.305 102.954 91.887  80.678&#10;Z   100 98  96  94  92  90  88  86  84  82  80  78  76  74  72  70  68  66  64  62  60&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="7" CreationDate="2013-04-24T22:17:20.490" Id="57137" LastActivityDate="2013-04-25T02:34:33.090" LastEditDate="2013-04-25T02:34:33.090" LastEditorUserId="919" OwnerUserId="919" ParentId="57128" PostTypeId="2" Score="5" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have a set of data which contains answers to a questionnaire (questions about satisfaction rated 1 to 5) and some demographic information (sex, age). I can divide the data in two groups and I want to see if there is any statistical difference between those groups. I tried the Wilcoxon test, but I have many ties, so I'm not sure this is the test that I need. What would be the best test in this case?&lt;/p&gt;&#10;" ClosedDate="2013-06-25T19:35:44.077" CommentCount="8" CreationDate="2013-04-25T00:36:05.433" Id="57147" LastActivityDate="2013-06-25T16:41:57.680" LastEditDate="2013-06-25T16:41:57.680" LastEditorUserId="5739" OwnerUserId="11210" PostTypeId="1" Score="2" Tags="&lt;survey&gt;&lt;ordinal&gt;&lt;likert&gt;&lt;group-differences&gt;" Title="Best test to assess statistical difference between two groups on a set of 5-point questionnaire items" ViewCount="746" />
  <row AnswerCount="1" Body="&lt;p&gt;I am comparing two group means on two dependent variables (aggression and stress levels) using Mann-Whitney U test by SPSS. My question is that do I need to adjust the alpha level for this? &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-04-25T00:37:39.033" Id="57148" LastActivityDate="2013-05-25T07:06:33.877" LastEditDate="2013-04-25T01:18:34.500" LastEditorUserId="22468" OwnerUserId="24803" PostTypeId="1" Score="1" Tags="&lt;spss&gt;&lt;mann-whitney-u-test&gt;" Title="Does alpha need to be adjusted for running Mann-Whitney U test independently for two dependent variables?" ViewCount="241" />
  <row AcceptedAnswerId="57159" AnswerCount="1" Body="&lt;p&gt;This is probably a very naive question... I'd like to estimate &quot;adjusted&quot; or &quot;conditional&quot; means for a variable (i'm unsure of the correct terminology). My data are on cortisol levels (dependent variable) in rabbits (n=56). I have many measurements at different times of the day, over many months. I'd like to calculate mean weekly values of cortisol for each individual rabbit so these can be used as a predictor in another model for which I only have weekly data. Rather than calculate the means from the raw data, i'd like to control for the time of day the samples were taken (this can influence the measurement). I thought i'd regress time of day (in minutes from 00:00 each day) on cortisol level and then extract the fitted values and calculate the weekly mean for each rabbit from these. Would this give me the estimated mean for cortisol, while controlling for time of day? &lt;/p&gt;&#10;&#10;&lt;p&gt;I can't share my data, but i've created a similar mock up using the iris data set. Here I fit a model, extract the fitted values and then calculate &quot;adjusted&quot; means for each species while controlling for the predictor. Am I right in thinking the difference between these means and the ones for the raw data (below) reflect the adjustment made when controlling for the independent variable?&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;data(iris)&#10;&#10;fit &amp;lt;- lm(Sepal.Length ~ Petal.Length, data = iris)&#10;summary(fit)&#10;&#10;with(iris, plot(Sepal.Length ~ Petal.Length, col = as.numeric(Species), asp = 1))&#10;abline(coef(fit))&#10;&#10;iris$fitted &amp;lt;- fitted(fit)&#10;&#10;with(iris, aggregate(fitted, list(Species), mean))&#10;#      Group.1       x&#10;# 1     setosa  4.9044&#10;# 2 versicolor  6.0486&#10;# 3  virginica  6.5769&#10;&#10;with(iris, aggregate(Sepal.Length, list(Species), mean))&#10;#      Group.1      x&#10;# 1     setosa  5.006&#10;# 2 versicolor  5.936&#10;# 3  virginica  6.588&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2013-04-25T01:34:29.053" Id="57157" LastActivityDate="2013-04-25T02:08:59.447" OwnerUserId="14216" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;regression&gt;" Title="Calculate mean of one variable while controlling for another using regression" ViewCount="378" />
  <row Body="&lt;p&gt;As the variance of your simulated untruncated normal distribution increases the density gets flatter across the [a,b] interval.  At its limit it becomes almost a uniform distribution hence with a maximum standard deviation of $\sqrt{\frac{1}{12}(b-a)^2}$.  If the standard deviation of the population you are looking at is more than that, it must mean you have a bimodal distribution of some sort.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Even in that case (which means you don't have a truncated normal distribution) the maximum theoretical variance happens when $\mu$ is bang in the middle of $a$ and $b$ and the data is all at the extremes - half at the lower bound and half at the upper.  The maximum possible standard deviation from a bounded distribution of any sort is $\sqrt{.5(a-\mu)^2+.5(b-\mu)^2}$ which reduces to $b-\mu$.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would suggest you look at histograms and density line plots of your original data and question your starting assumption that comes from a truncated rounded Gaussian distribution.  I would also compare its actual standard deviation with the two theoretical limits above.  If it is less than the standard deviation of a uniform distribution you should be able to simulate a truncated normal distribution with that standard deviation, and you probably have a coding error.  If it is more than that amount, you need to find an alternative model.  &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-04-25T01:55:13.227" Id="57158" LastActivityDate="2013-04-25T01:55:13.227" OwnerUserId="7972" ParentId="57143" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;There is a simple geometric explanation.  Try the following example in R and recall that the first principal component maximizes variance.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(ggplot2)&#10;&#10;n &amp;lt;- 400&#10;z &amp;lt;- matrix(rnorm(n * 2), nrow = n, ncol = 2)&#10;y &amp;lt;- sample(c(-1,1), size = n, replace = TRUE)&#10;&#10;# PCA helps&#10;df.good &amp;lt;- data.frame(&#10;    y = as.factor(y), &#10;    x = z + tcrossprod(y, c(10, 0))&#10;)&#10;qplot(x.1, x.2, data = df.good, color = y) + coord_equal()&#10;&#10;# PCA hurts&#10;df.bad &amp;lt;- data.frame(&#10;    y = as.factor(y), &#10;    x = z %*% diag(c(10, 1), 2, 2) + tcrossprod(y, c(0, 8))&#10;)&#10;qplot(x.1, x.2, data = df.bad, color = y) + coord_equal()&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;PCA Helps&lt;/strong&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/XAEq7.png&quot; alt=&quot;PCA helps&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The direction of maximal variance is horizontal, and the classes are separated horizontally.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;PCA Hurts&lt;/strong&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/Y0Wqw.png&quot; alt=&quot;PCA hurts&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The direction of maximal variance is horizontal, but the classes are separated vertically&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-04-25T03:59:05.610" Id="57166" LastActivityDate="2013-04-25T03:59:05.610" OwnerUserId="1670" ParentId="52773" PostTypeId="2" Score="12" />
  <row AnswerCount="1" Body="&lt;p&gt;I iterated my 10-fold cross validation 100 times for several methods. Now I want to use a t-test to test if the results are significant. However, I'm not sure what the sample size is. &#10;Is the sample size the original amount of samples, or is it the original amount of samples x 100?&lt;/p&gt;&#10;&#10;&lt;h2&gt;edit:&lt;/h2&gt;&#10;&#10;&lt;p&gt;For university we need to classify 3 cancer types and give an estimation of how well our model will perform. We received a dataset with 100 samples. We split the data up into a training and test set using stratified sampling with a ratio of 0.3 and 0.7. The resulting training set consists of 69 samples, and the test set out of 31 samples.&lt;/p&gt;&#10;&#10;&lt;p&gt;We used repeated cross validation because of this paper:&#10;&lt;a href=&quot;http://www.cse.iitb.ac.in/~tarung/smt/papers_ppt/ency-cross-validation.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.cse.iitb.ac.in/~tarung/smt/papers_ppt/ency-cross-validation.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The repeated cross-validation is done on the same training set, but with the folds are randomly chosen every time, so they should be different every time.&lt;/p&gt;&#10;&#10;&lt;p&gt;The significance we want to test is if the accuracy of one model is significantly better than the accuracy of a different model. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-04-25T06:45:37.457" Id="57173" LastActivityDate="2014-07-10T18:49:57.050" LastEditDate="2013-04-25T08:28:06.880" LastEditorUserId="24144" OwnerUserId="24144" PostTypeId="1" Score="0" Tags="&lt;statistical-significance&gt;&lt;t-test&gt;&lt;cross-validation&gt;&lt;sample-size&gt;" Title="Repeated 100x 10-fold cross validation, what is the sample size when doing an significance test?" ViewCount="419" />
  
  
  <row AcceptedAnswerId="57294" AnswerCount="1" Body="&lt;p&gt;We have data from two sort tasks where the same items (N=87) were assigned to different types of group ('g' &amp;amp; 'd'). We want to compare the overlap between the assignments to the 'g' groups (N=13) to those of the 'd' groups (N=16). To give you an idea of the data I have created a truncated frequency table &lt;em&gt;(apologies for the table format - not sure how best to show it on here)&lt;/em&gt;: &lt;/p&gt;&#10;&#10;&lt;p&gt;In this example 9 items placed into group 'g4' were also placed into 'd1'. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;  | g1 | g2 | g3 | g4 | g5 |&#10;----------------------------&#10;d1|  1 |  0 |  0 |  9 |  0 |&#10;----------------------------&#10;d2|  0 |  0 |  0 |  0 |  4 |&#10;----------------------------&#10;d3|  0 |  7 |  4 |  0 |  2 |&#10;----------------------------&#10;d4|  0 |  0 |  8 |  0 |  4 |&#10;----------------------------&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The size of the 'g' groups differ and so the potential number of matching items assigned to the 'd' groups depends on the size of the 'g' group. For instance, g4 actually has 18 items assigned to it but only 9 match with d1, g3 has 3 items assigned and none match d1. So the frequencies alone are not particularly representative. &lt;/p&gt;&#10;&#10;&lt;p&gt;We wanted to have 1) a measure of the overlap between all of the 'g' groups with all of the 'd' groups taking into account the possible overlap, and 2). a global measure of the association between 'g' groupings and 'd' groupings. &lt;/p&gt;&#10;&#10;&lt;p&gt;I am assuming (please correct me if I am wrong) we can't use Chi-Squared as most of the cells are less than 5 and we can't use Fisher's exact test as it isn't a 2x2 table. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;My questions:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;1) Are there any other test we can perform to quantify the relationships between these assignments?&lt;/p&gt;&#10;&#10;&lt;p&gt;2) When I was playing with the data and ran a chi-squared test in R it gave me &lt;strong&gt;Pearson's Residual values&lt;/strong&gt; for each GxD combination - are these residual values OK to use independently of the Chi-Square test? --&lt;em&gt;They seem to represent what we need for our first requirement, i.e. providing an index of the overlap between groups taking into account group size - but I don't want to use these incorrectly.&lt;/em&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;3). I saw the option of computing p-values by Monte Carlo Simulation in the chisq.test function in R. Could this simulation be used to overcome the problem of cells &amp;lt; 5 with the data? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-25T14:44:43.917" Id="57228" LastActivityDate="2013-04-27T01:01:41.913" LastEditDate="2013-04-27T01:01:41.913" LastEditorUserId="805" OwnerUserId="6907" PostTypeId="1" Score="0" Tags="&lt;chi-squared&gt;&lt;association-measure&gt;&lt;fishersexact&gt;" Title="Quantifying the overlap in sort task results with cells &lt; 5" ViewCount="74" />
  <row AnswerCount="2" Body="&lt;p&gt;I want to calculate a quantile of a specific distribution. Therefore I need the cdf. My distribution is a standardized Student's-t distribution, this can be written as&#10;\begin{align*}&#10;f(l|\nu) =(\pi (\nu-2))^{-\frac{1}{2}}\Gamma \left(\frac{\nu}{2} \right)^{-1} \Gamma \left(\frac{\nu+1}{2} \right) \left(1+\frac{l^2}{\nu-2} \right)^{-\frac{1+\nu}{2}}&#10;\end{align*}&lt;/p&gt;&#10;&#10;&lt;p&gt;This can be implemented in R with:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;probabilityfunction&amp;lt;-function(x)(pinumber*(param-2))^(-1/2)*gamma(param&#10;/2)^(-1)*gamma((param+1)/2)*(1+l^2/(param-2))^(-(1+param)/2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Where pinumber is the value for pi and &lt;code&gt;param&lt;/code&gt; is the $\nu$. Lets say $\nu=5$. Then I can get the probability by just inserting a certain value for my &lt;code&gt;l&lt;/code&gt;. But I want to have the cumulative density, since I later want to compute the quantile. I thought about something like&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;cumsum(probabilityfunction(5))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;to give me the cumulative value up to 5.&lt;/p&gt;&#10;&#10;&lt;p&gt;But obviously this does not work. How can I get the cumulative probability and later on the quantile?&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT: OK, I found a first improvement:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;integrate(probabilityfunction,-Inf,2) &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;would be a good starting point, but how to do the other thing?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-04-25T15:01:06.130" Id="57231" LastActivityDate="2013-06-24T18:10:02.187" LastEditDate="2013-04-25T15:06:20.950" LastEditorUserId="21998" OwnerUserId="21998" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;pdf&gt;&lt;quantiles&gt;&lt;cdf&gt;" Title="Compute cdf and quantile of a specific distribution" ViewCount="267" />
  
  
  <row Body="&lt;p&gt;What sort of SVM do you use?. If you are using $\nu$-SVM, then you can find the answer in this &lt;a href=&quot;http://research.microsoft.com/pubs/67118/nips_nu.ps.gz&quot; rel=&quot;nofollow&quot;&gt;paper&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;The idea is the following: &quot;v is an upper bound on the fraction of margin errors, a lower bound on the fraction of support vectors, and both quantities approach v asymptotically&quot;. Additionally, this number cannot exceed the quantity 2*lmin/l, where l is the total number of SVs and lmin is the minimum between the number of positive and negative SVs (labels +/-1).&lt;/p&gt;&#10;&#10;&lt;p&gt;Notice that this means that for inbalanced problems, you will have to work with lower values of v, so you will tend to overfit data. Why? Because it forces you to use an eventually very small value of nu, that is, you allow very small number of errors during training, what leads to overfitting.&lt;/p&gt;&#10;&#10;&lt;p&gt;Oversampling is a way to dodge this problem. This intuition is also valid for C-SVM, though the meaning of the C parameter is different from that of the $\nu$ parameter. Still, it also acts as a parameter controlling the amount of regularization.&lt;/p&gt;&#10;&#10;&lt;p&gt;The idea is that C weights a term which can be understood as the number of errors your classifier does on the training data. If a class has a much higher density of samples, then the optimization algorithm tends to reduce the error by pushing the separating hyperplane to the minority class. The idea is that you decrease the total error by reducing the mistakes on the big class. Overall looks better, but as a result you have a higher error rate on the smaller class. Consider the extreme case when you have a ratio 97%-3%. More details about the problem and techniques to overcome it &lt;a href=&quot;http://www.cs.ox.ac.uk/people/vasile.palade/papers/Class-Imbalance-SVM.pdf&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hope this helps.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-04-25T17:34:50.663" Id="57250" LastActivityDate="2013-04-25T17:34:50.663" OwnerUserId="17908" ParentId="57230" PostTypeId="2" Score="1" />
  
  
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I am analyzing data using path analysis and I am hoping someone here can help.  &lt;/p&gt;&#10;&#10;&lt;p&gt;In my model, I am looking at predictors of a variable, $Y$. In the path model, $Y$ has 4 significant predictors lets call them $A$, $B$, $C$, and $X$ (path coefficient $X$ to $Y$ is $-.16$ which is significant). Of importance, $X$ is not significantly correlated with $Y$ when tested as a bivariate correlation ($r = .105$, ns). I feared that suppression may be present and so I ran the partial correlation controlling for $A$, $B$, and $C$. The partial correlation between $X$ and $Y$ was $-.209$, which is significant. &lt;/p&gt;&#10;&#10;&lt;p&gt;Does this rule out suppression and instead suggest incidental / accidental cancellation may be more likely the issue, or am I missing something? Are there other ways to determine if suppression is the issue?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-25T21:40:46.777" Id="57286" LastActivityDate="2014-11-23T03:26:07.597" LastEditDate="2014-01-26T05:25:15.047" LastEditorUserId="7290" OwnerUserId="24857" PostTypeId="1" Score="2" Tags="&lt;multiple-regression&gt;&lt;sem&gt;&lt;suppressor&gt;" Title="How to tell if my variable is a suppressor?" ViewCount="185" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I am trying to test if richness varies by treatment (Severity) over time (Block) using a repeated measures ANOVA in R. Any suggestions on how to do this correctly? I have tried, but get an error message stating I have an extra = in the formula:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;TWP.aov &amp;lt;- aov(Richness ~ Severity * Year + Error(Plot/Severity), data = TWP)&#10;summary(TWP.aov) &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Here is a subset of the data (called TWP which I read in as a csv file):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Block       Severity    Plot    Richness&#10;2003-2004   High        A       18&#10;2003-2004   High        B       24&#10;2003-2004   High        C       21&#10;2005-2006   High        A       28&#10;2005-2006   High        B       24&#10;2005-2006   High        C       20&#10;2007-2009   High        A       14&#10;2007-2009   High        B       27&#10;2007-2009   High        C       29&#10;2003-2004   Low         A       12&#10;2003-2004   Low         B       10&#10;2003-2004   Low         C       14&#10;2005-2006   Low         A       18&#10;2005-2006   Low         B       16&#10;2005-2006   Low         C       14&#10;2007-2009   Low         A       8&#10;2007-2009   Low         B       19&#10;2007-2009   Low         C       20&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;When I input the above subset I get the correct summary table: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Error: Plot&#10;          Df Sum Sq Mean Sq F value Pr(&amp;gt;F)&#10;Residuals  2  49.33   24.67        &#10;&#10;Error: Plot:Severity&#10;          Df Sum Sq Mean Sq F value Pr(&amp;gt;F)  &#10;Severity   1 304.22  304.22   85.56 0.0115 *&#10;Residuals  2   7.11    3.56                 &#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1&#10;&#10;Error: Within&#10;               Df Sum Sq Mean Sq F value Pr(&amp;gt;F)&#10;Block           2  43.00  21.500   0.745  0.505&#10;Severity:Block  2   1.44   0.722   0.025  0.975&#10;Residuals       8 230.89  28.861  &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;My actual data's summary table looks like this though:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Error: Plot&#10;               Df Sum Sq Mean Sq F value Pr(&amp;gt;F)  &#10;Severity        1    248  247.66   4.013 0.0479 *&#10;Block           2    493  246.67   3.997 0.0214 *&#10;Severity:Block  2    244  122.17   1.980 0.1436  &#10;Residuals      99   6110   61.71                 &#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;As of June 12, 2013: As an update, I still have not resolved the &quot;Error() model is singular&quot; issue with the aov function, so I have tried to run this in the nlme package using the following code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;TWP.lme &amp;lt;- lme(Richness ~Severity * Block, random = ~1|(Plot/Severity), data = TWP)&#10;    summary(TWP.lme)&#10;    anova(TWP.lme)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I do not get any error message, and I believe I have structured the error term correctly.  I have checked the results visually against the graphs of the data and all appears correct.&#10;Is there a way to check the results are correct? Why is there no error message regarding the model is singular?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-04-26T01:51:42.893" FavoriteCount="1" Id="57304" LastActivityDate="2013-11-04T02:47:55.117" LastEditDate="2013-11-04T02:47:55.117" LastEditorUserId="805" OwnerUserId="24869" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;anova&gt;&lt;repeated-measures&gt;" Title="Blocked repeated measures ANOVA in R" ViewCount="597" />
  <row AnswerCount="0" Body="&lt;p&gt;Is there any function in &lt;code&gt;R&lt;/code&gt; that can solve the problem like this example from the &lt;a href=&quot;http://support.sas.com/kb/22/800.html&quot; rel=&quot;nofollow&quot;&gt;SAS website&lt;/a&gt;:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Beginning in SAS 9.3, PROC FMM can be used as an alternative to the LOGISTIC and GENMOD procedures for fitting generalized linear models such as logistic and poisson models. You can fit the model in PROC FMM and use its RESTRICT statement to impose equality or inequality constraints on the model parameters.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;For example, in the following logistic model suppose you want to constrain the parameters for X1 and X2 to be equal.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; proc logistic;&#10;    model y = x1 x2 x3 x4;&#10;    run;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;  &#10;  &lt;p&gt;The following statements fit the model in PROC FMM and impose the restriction.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; proc fmm;&#10;    model y = x1 x2 x3 x4 / dist=binary link=logit;&#10;    restrict x1 1 x2 -1;&#10;    run;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;  &#10;  &lt;p&gt;To restrict the parameter on X1 to exceed that of X2, use the following RESTRICT statement.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; restrict x1 1 x2 -1 &amp;gt; 0;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="4" CreationDate="2013-04-26T03:54:11.633" FavoriteCount="0" Id="57312" LastActivityDate="2013-06-16T07:28:50.080" LastEditDate="2013-06-16T07:28:50.080" LastEditorUserId="21054" OwnerUserId="21599" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;logistic&gt;&lt;generalized-linear-model&gt;&lt;sas&gt;" Title="Restricting model parameters in logistic models in R" ViewCount="285" />
  
  
  
  
  <row Body="&lt;p&gt;If you just say &quot;loess&quot; people probably won't know what you mean. Perhaps &quot;locally weighted regression&quot; is better? The &lt;code&gt;R&lt;/code&gt; help description of &lt;code&gt;loess&lt;/code&gt; in &lt;code&gt;stats&lt;/code&gt; is &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Fit a polynomial surface determined by one or more numerical&#10;  predictors, using local fitting.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="1" CreationDate="2013-04-26T10:09:49.270" Id="57330" LastActivityDate="2013-04-26T10:09:49.270" OwnerUserId="686" ParentId="57325" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="57356" AnswerCount="1" Body="&lt;p&gt;What relation are between minimum contrast estimate and minimum distance estimate?&lt;/p&gt;&#10;&#10;&lt;p&gt;If I understand correctly, these two are different methods? or are they equivalent?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks and regards!&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Minimum distance estimate from &lt;a href=&quot;http://en.wikipedia.org/wiki/Minimum_distance_estimation&quot; rel=&quot;nofollow&quot;&gt;Wikipedia&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Let $\displaystyle X_1,\ldots,X_n$ be an independent and identically distributed (iid) random sample from a population with distribution $F(x;\theta)\colon \theta\in\Theta$ and $\Theta\subseteq\mathbb{R}^k (k\geq 1)$.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;Let $\displaystyle F_n(x)$ be the empirical distribution function based on the sample.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;Let $\hat{\theta}$ be an estimator for $\displaystyle \theta$. Then $F(x;\hat{\theta})$ is an estimator for $\displaystyle F(x;\theta)$.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;Let $d[\cdot,\cdot]$ be a functional returning some measure of &quot;distance&quot; between the two arguments. The functional $\displaystyle d$ is also called the criterion function.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;If there exists a $\hat{\theta}\in\Theta$ such that $d[F(x;\hat{\theta}),F_n(x)]=\inf\{d[F(x;\theta),F_n(x)]; \theta\in\Theta\},$ then $\hat{\theta}$ is called the minimum distance estimate of $\displaystyle \theta$.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Minimum contrast estimate from Bickel and Doksum's Mathematical Statistics Vol1&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/GDin4.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-26T13:10:38.663" Id="57338" LastActivityDate="2013-04-26T16:52:26.693" OwnerUserId="1005" PostTypeId="1" Score="1" Tags="&lt;estimation&gt;&lt;mathematical-statistics&gt;" Title="Relation between minimum contrast estimate and minimum distance estimate?" ViewCount="86" />
  
  <row Body="&lt;p&gt;The minimum distance estimator is equivalent to the minimum contrast estimator, provided you pick $d$ and and $\rho$ such that&#10;$$ d[F(x; \theta), \, F_n(x)] = \rho(X, \theta). $$&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-04-26T16:52:26.693" Id="57356" LastActivityDate="2013-04-26T16:52:26.693" OwnerUserId="24498" ParentId="57338" PostTypeId="2" Score="1" />
  
  
  
  <row AcceptedAnswerId="57385" AnswerCount="2" Body="&lt;p&gt;What functions qualify to be called measures of dispersion by statisticians?&lt;/p&gt;&#10;&#10;&lt;p&gt;Why there are so many of these?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-04-26T22:24:20.140" FavoriteCount="1" Id="57383" LastActivityDate="2013-04-27T10:46:03.823" LastEditDate="2013-04-27T10:46:03.823" LastEditorUserId="686" OwnerUserId="13991" PostTypeId="1" Score="2" Tags="&lt;mathematical-statistics&gt;" Title="What functions qualify to be called measures of dispersion by statistician?" ViewCount="125" />
  <row Body="&lt;p&gt;I'm afraid you've gotten confused. The &lt;a href=&quot;http://en.wikipedia.org/wiki/Standard_deviation&quot; rel=&quot;nofollow&quot;&gt;standard deviation&lt;/a&gt; of a random variable is defined according to a simple formula, namely:&#10;$$ \sigma_y = \sqrt{E[(Y - \mu_y)^2]}$$&#10;(This can be re-arranged in various ways, so you may see somewhat different formulae too--see the wikipedia page linked above).&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, for a normal distribution, it turns out that about 68% of the data (i.e., the 16th - 84th percentiles), lie within one standard deviation of the mean, but this in &lt;strong&gt;no way&lt;/strong&gt; implies that the standard deviation is, in general, connected to those percentiles.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you want to report standard deviations, calculate them with the formula above and go from there. &lt;/p&gt;&#10;&#10;&lt;p&gt;Edit: As @User603 points out, one could use percentiles to derive a (weak) &lt;em&gt;lower bound&lt;/em&gt; on a standard deviation using Chebyshev's Inequality. Chebyshev's inequality is usually paraphrased as saying that &quot;most&quot; values are near the mean. In particular, at least $100(1 - 1/(k^2))$ percent of the values are within $k$ standard deviations of the population mean, &lt;em&gt;regardless of the distribution&lt;/em&gt;. Note that these bounds are pretty loose--and they get even looser when you have sample means and standard deviations instead of population values--so this is of great theoretical interest, but rarely used to analyze data.&lt;/p&gt;&#10;&#10;&lt;p&gt;Therefore, I'd reiterate my suggestion that you just calculate the standard deviation directly. In numpy, it's just:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;numpy.std(y, ddof=1)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;(The ddof keyword argument gets you a less-biased estimate of the sample standard deviation; see the &lt;a href=&quot;http://docs.scipy.org/doc/numpy/reference/generated/numpy.std.html&quot; rel=&quot;nofollow&quot;&gt;numpy std docs&lt;/a&gt; for more details).&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-04-26T22:33:06.300" Id="57386" LastActivityDate="2013-04-27T04:57:32.020" LastEditDate="2013-04-27T04:57:32.020" LastEditorUserId="7250" OwnerUserId="7250" ParentId="57382" PostTypeId="2" Score="1" />
  
  <row AnswerCount="11" Body="&lt;blockquote&gt;&#10;  &lt;p&gt;&quot;Essentially, all models are wrong, but some are useful.&quot;&lt;/p&gt;&#10;  &#10;  &lt;p&gt;--- Box, George E. P.; Norman R. Draper (1987). Empirical Model-Building and Response Surfaces, p. 424, Wiley. ISBN 0471810339.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;What exactly is the meaning of the above phrase?  &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-04-27T08:39:58.983" FavoriteCount="10" Id="57407" LastActivityDate="2014-05-29T18:44:38.493" LastEditDate="2013-04-27T21:16:30.603" LastEditorUserId="556" OwnerUserId="13473" PostTypeId="1" Score="34" Tags="&lt;modeling&gt;" Title="What is the meaning of &quot;All models are wrong, but some are useful&quot;" ViewCount="9194" />
  <row Body="&lt;p&gt;First, the difference between significant and non-significant is not necessarily significant. So if after adding a covariate your interaction p-value changed from .04 to .06, this doesn't mean anything substantial other than you've bounced from one side of a binary  .05 decision threshold.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, the other possibility is that the covariate has substantially influenced the analysis. A typical motivation for including a covariate is that you want to control for its effect, and thereby adjust your estimates of main and interaction effects for the covariate. &lt;/p&gt;&#10;&#10;&lt;p&gt;I recommend that you produce a few plots of the cell mean both controlling and not controlling for the covariate to see what kind of difference inclusion of the covariates has made.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-04-27T15:03:51.553" Id="57433" LastActivityDate="2013-04-27T15:03:51.553" OwnerUserId="183" ParentId="57423" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;&quot;Taking a model too seriously is really just another way of not taking it seriously at all.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;By Andrew Gelman&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2013-04-27T15:58:23.957" CreationDate="2013-04-27T15:58:23.957" Id="57437" LastActivityDate="2013-04-27T15:58:23.957" OwnerUserId="24946" ParentId="726" PostTypeId="2" Score="5" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Do you know how i can get a code for estimating trend by Mitcherlich and logistic method in R?&lt;/p&gt;&#10;" ClosedDate="2013-04-28T07:16:48.060" CommentCount="5" CreationDate="2013-04-27T21:22:04.090" Id="57456" LastActivityDate="2013-04-28T04:47:14.770" LastEditDate="2013-04-27T21:25:37.123" LastEditorUserId="8077" OwnerUserId="24955" PostTypeId="1" Score="-1" Tags="&lt;r&gt;&lt;logistic&gt;&lt;trend&gt;&lt;code&gt;" Title="Methods For Estimating Trend in R" ViewCount="104" />
  
  
  
  <row Body="&lt;p&gt;From a visualization perspective, bar graphs are the last plot you should use.  Bar graphs should only be used for counts or percentage of totals.  To plot medians you should consider comparative boxplots, comparative histograms, or line-plots with medians and standard errors - something with distribution information included in the plot.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Comparing across categories could be very tough, even in the clothes example, some brands of the same item will hold price better than other brands.  You may have to stick with directly comparable products.&lt;/p&gt;&#10;&#10;&lt;p&gt;Just no bar graphs, please!&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-04-28T02:48:52.587" Id="57474" LastActivityDate="2013-04-29T12:07:41.790" LastEditDate="2013-04-29T12:07:41.790" LastEditorUserId="24341" OwnerUserId="24341" ParentId="57471" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;I believe your original question stems from being a bit uncertain about what PCA is doing. Principal Component Analysis allows you to identify the principal mode of variation in your sample. Those modes are emperically calculated as the eigenvectors of your sample's covariance matrix (the &quot;loadings&quot;). Subsequently those vectors serve as the new &quot;coordinate system&quot; of your sample as you project your original sample in the space they define (the &quot;scores&quot;). The proportion of variation associated with the $i$-th eigenvector/ mode of variation/ loading/ principal component equals $\frac{\lambda_i}{\Sigma_{k=1}^{p} \lambda_k}$ where $p$ is your sample's original dimensionality ($p =784$ in your case). [Remember because your covariance matrix is non-negative Definite you'll have no negative eigenvalues $\lambda$.]&#10;Now, by definition the eigenvectors are orthogonal to each other. That means that their respective projections are also orthogonal and where originally you had a big possibly correlated sample of variables, now you have a (hopefully significantly) smaller linearly independent sample (the &quot;scores&quot;).  &lt;/p&gt;&#10;&#10;&lt;p&gt;Practically with PCA you are using the projections of the PCs (the &quot;scores&quot;) as surrogate data for your original sample. You do all your analysis on the scores, and afterwards you reconstruct your original sample back using the PCs to find out out what happened on your original space (that's basically &lt;a href=&quot;https://en.wikipedia.org/wiki/Principal_component_regression&quot; rel=&quot;nofollow&quot;&gt;Principal Component Regression&lt;/a&gt;). Clearly, if you are able to meaningful interpreter your eigenvectors (&quot;loadings&quot;) then you are in an even better position: You can describe what happens to your sample in the mode of variation presented by that loading by doing inference on that loading directly and not care about reconstruction at all. :)&lt;/p&gt;&#10;&#10;&lt;p&gt;In general what do you &quot;after calculating the PCA&quot; depends on the target of your analysis. PCA just gives you a linearly independent sub-sample of your data that is the optimal under an RSS reconstruction criterion. You might use it for classification, or regression, or both, or as I mentioned you might want to recognise meaningful orthogonal modes of variations in your sample.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;A comment&lt;/em&gt; : I think the best &lt;em&gt;naive&lt;/em&gt; way to decide the number of components to retain is to base your estimate on some threshold of sample variation you would like to retain in your reduced dimensionality sample rather than just some arbitrary number eg. 3, 100, 200.&#10;As user4959 explained you can check that cumulative variation by checking the relevant field of the list under the &lt;code&gt;$loadings&lt;/code&gt;field in the list object produced by &lt;code&gt;princomp&lt;/code&gt;.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-04-28T03:28:07.933" Id="57475" LastActivityDate="2013-04-28T15:44:59.847" LastEditDate="2013-04-28T15:44:59.847" LastEditorUserId="11852" OwnerUserId="11852" ParentId="57467" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;I believe what you are getting at in your question concerns data truncation using a smaller number of principal components (PC). For such operations, I think the function &lt;code&gt;prcomp&lt;/code&gt;is more illustrative in that it is easier to visualize the matrix multiplication used in reconstruction. &lt;/p&gt;&#10;&#10;&lt;p&gt;First, give a synthetic dataset, &lt;code&gt;Xt&lt;/code&gt;, you perform the PCA (typically you would center samples in order to describe PC's relating to a covariance matrix:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#Generate data&#10;m=50&#10;n=100&#10;frac.gaps &amp;lt;- 0.5 # the fraction of data with NaNs&#10;N.S.ratio &amp;lt;- 0.25 # the Noise to Signal ratio for adding noise to data&#10;&#10;x &amp;lt;- (seq(m)*2*pi)/m&#10;t &amp;lt;- (seq(n)*2*pi)/n&#10;&#10;#True field&#10;Xt &amp;lt;- &#10; outer(sin(x), sin(t)) + &#10; outer(sin(2.1*x), sin(2.1*t)) + &#10; outer(sin(3.1*x), sin(3.1*t)) +&#10; outer(tanh(x), cos(t)) + &#10; outer(tanh(2*x), cos(2.1*t)) + &#10; outer(tanh(4*x), cos(0.1*t)) + &#10; outer(tanh(2.4*x), cos(1.1*t)) + &#10; tanh(outer(x, t, FUN=&quot;+&quot;)) + &#10; tanh(outer(x, 2*t, FUN=&quot;+&quot;))&#10;&#10;Xt &amp;lt;- t(Xt)&#10;&#10;#PCA&#10;res &amp;lt;- prcomp(Xt, center = TRUE, scale = FALSE)&#10;names(res)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In the results or &lt;code&gt;prcomp&lt;/code&gt;, you can see the PC's (&lt;code&gt;res$x&lt;/code&gt;), the eigenvalues (&lt;code&gt;res$sdev&lt;/code&gt;) giving information on the magnitude of each PC, and the loadings (&lt;code&gt;res$rotation&lt;/code&gt;). &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;res$sdev&#10;    length(res$sdev)&#10;res$rotation&#10;    dim(res$rotation)&#10;res$x&#10;    dim(res$x)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;By squaring the eigenvalues, you get the variance explained by each PC:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;plot(cumsum(res$sdev^2/sum(res$sdev^2))) #cumulative explained variance&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Finally, you can create a truncated version of your data by using only the leading (important) PCs:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;pc.use &amp;lt;- 3 # explains 93% of variance&#10;trunc &amp;lt;- res$x[,1:pc.use] %*% t(res$rotation[,1:pc.use])&#10;&#10;#and add the center (and re-scale) back to data&#10;if(res$scale != FALSE){&#10;     trunc &amp;lt;- scale(trunc, center = FALSE , scale=1/res$scale)&#10;    }&#10;    if(res$center != FALSE){&#10;    trunc &amp;lt;- scale(trunc, center = -1 * res$center, scale=FALSE)&#10;}&#10;dim(trunc); dim(Xt)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;You can see that the result is a slightly smoother data matrix, with small scale features filtered out:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;RAN &amp;lt;- range(cbind(Xt, trunc))&#10;BREAKS &amp;lt;- seq(RAN[1], RAN[2],,100)&#10;COLS &amp;lt;- rainbow(length(BREAKS)-1)&#10;par(mfcol=c(1,2), mar=c(1,1,2,1))&#10;image(Xt, main=&quot;Original matrix&quot;, xlab=&quot;&quot;, ylab=&quot;&quot;, xaxt=&quot;n&quot;, yaxt=&quot;n&quot;, breaks=BREAKS, col=COLS)&#10;box()&#10;image(trunc, main=&quot;Truncated matrix (3 PCs)&quot;, xlab=&quot;&quot;, ylab=&quot;&quot;, xaxt=&quot;n&quot;, yaxt=&quot;n&quot;, breaks=BREAKS, col=COLS)&#10;box()&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/M3XDu.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;And here is a very basic approach that you can do outside of the prcomp function:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#alternate approach&#10;Xt.cen &amp;lt;- scale(Xt, center=TRUE, scale=FALSE)&#10;C &amp;lt;- cov(Xt.cen, use=&quot;pair&quot;)&#10;E &amp;lt;- svd(C)&#10;A &amp;lt;- Xt.cen %*% E$u&#10;&#10;#To remove units from principal components (A)&#10;#function for the exponent of a matrix&#10;&quot;%^%&quot; &amp;lt;- function(S, power)&#10;     with(eigen(S), vectors %*% (values^power * t(vectors)))&#10;Asc &amp;lt;- A %*% (diag(E$d) %^% -0.5) # scaled principal components&#10;&#10;#Relationship between eigenvalues from both approaches&#10;plot(res$sdev^2, E$d) #PCA via a covariance matrix - the eigenvalues now hold variance, not stdev&#10;abline(0,1) # same results&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now, deciding which PCs to retain is a separate question - &lt;a href=&quot;http://stats.stackexchange.com/questions/33917/how-to-determine-significant-principal-components-using-bootstrapping-or-monte-c&quot;&gt;one that I was interested in a while back&lt;/a&gt;. Hope that helps.&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2013-04-28T05:41:54.530" Id="57478" LastActivityDate="2013-04-30T06:23:31.020" LastEditDate="2013-04-30T06:23:31.020" LastEditorUserId="10675" OwnerUserId="10675" ParentId="57467" PostTypeId="2" Score="12" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I am looking for a model to fit longitudinal data and opted for GLMM with a logit link.&#10;I tried to add random effects but when both a random slope and random intercept are in the model, this does not converge.&#10;Then I tried to include only one of them and both models converged but I am wondering if I can have a model with random slope without a random intercept. Does this conflicts with the hierarchy of the model? Or, since all subject start at the same point in my data (it's about roses and at day 0, they are all fresh), is it allowed?&lt;/p&gt;&#10;&#10;&lt;p&gt;The AIC value does drop when the random effect is a random intercept only but I am not sure if I can compare the AIC values.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you for any help!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-04-28T13:52:46.250" Id="57498" LastActivityDate="2013-04-28T14:33:53.287" LastEditDate="2013-04-28T14:33:53.287" LastEditorUserId="686" OwnerUserId="24975" PostTypeId="1" Score="1" Tags="&lt;random-effects-model&gt;&lt;glmm&gt;" Title="Random slope in a model without random intercept" ViewCount="323" />
  
  
  <row Body="&lt;p&gt;Regarding &lt;strong&gt;&quot;how do you get [the labels] from the data&quot;&lt;/strong&gt;:&lt;/p&gt;&#10;&#10;&lt;p&gt;That's up to you -- you can define them any way you like, but you have to think about what would make sense.&lt;/p&gt;&#10;&#10;&lt;p&gt;In this context, you could define the label to be +1 if today's close is higher than yesterday's close (i.e. the price rose as measured close-to-close), and -1 otherwise.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you did that, you could train your SVM to predict whether the stock market will rise or fall, i.e. whether tomorrow's label will be +1 or -1.&lt;/p&gt;&#10;&#10;&lt;p&gt;Your predictors for tomorrow's label could be, for example, today's label, today's volume, possibly some lagged values of the label and volume, the day of the week and the month of the year, to give a very simple example. I'm not saying this model will work well, but you can certainly play around with it.&lt;/p&gt;&#10;&#10;&lt;p&gt;Regarding &lt;strong&gt;&quot;trying to predict the stock price a few days out in the future&quot;&lt;/strong&gt;:&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem with what I described above is that, even if you manage to train a SVM that works well, it will only predict whether the market will rise or fall -- but not its exact value.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you wanted to predict the exact stock price -- i.e., a continuous variable -- I think you'd have to use something other than a SVM.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-28T18:11:39.227" Id="57516" LastActivityDate="2013-04-28T18:17:01.763" LastEditDate="2013-04-28T18:17:01.763" LastEditorUserId="9330" OwnerUserId="9330" ParentId="57153" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="57522" AnswerCount="1" Body="&lt;p&gt;I want to compute whether it is more probable that a patient has a disease or the contrary. If I am given the following information:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$P(\mathrm{disease})= 0.008$$&#10;$$P(+|\mathrm{disease})= 0.98$$&#10;$$P(-|¬\mathrm{disease})= 0.97$$&lt;/p&gt;&#10;&#10;&lt;p&gt;To find the answer would it be correct to do the following:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac{0.008 \times 0.98}{(0.008 \times 0.98)+(0.992 \times 0.03)}=0.2085$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and then calculate $1-0.2085$ to see which is larger?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-04-28T19:44:51.760" Id="57521" LastActivityDate="2013-08-29T10:35:30.770" LastEditDate="2013-08-29T10:35:30.770" LastEditorUserId="27581" OwnerUserId="17503" PostTypeId="1" Score="2" Tags="&lt;conditional-probability&gt;&lt;bayes&gt;" Title="Calculating whether a disease is probable using Bayes rule?" ViewCount="82" />
  <row AcceptedAnswerId="57527" AnswerCount="1" Body="&lt;p&gt;say we have a random sample of a random variable, but we are told that no information is available for values less than or equal to some number, perhaps k.  That is, we don't have the full sample.  We only get the values that are greater than k.  So, how do we account for this in the likelihood function?  Do we multiply by (1 - F(k)) or something like that?  Any help is greatly appreciated!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-28T20:55:52.483" Id="57525" LastActivityDate="2013-04-28T21:18:33.257" OwnerUserId="9495" PostTypeId="1" Score="2" Tags="&lt;maximum-likelihood&gt;" Title="MLE for distribution where some data not available" ViewCount="44" />
  
  <row Body="&lt;p&gt;Let me translate into statistician. So $B$ is a random variable where $B = \beta + \varepsilon$, with $\text{Var}(\varepsilon)$ = $\Sigma_B$, for $\Sigma_B$ known. An observation is taken, and the observed value of $B$ is $b$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Assuming $A$ is invertible, the solution of $Ax = b$ is $A^{-1}b$. Let $C = A^{-1}$ for the moment.&lt;/p&gt;&#10;&#10;&lt;p&gt;$\text{Var}(Cb) = C\,\text{Var}(b)\,C^\top = A^{-1} \Sigma_B (A^{-1})^\top$&lt;/p&gt;&#10;&#10;&lt;p&gt;If the two components of $b$ are independent, then $\Sigma_B$ is diagonal, with diagonal the squares of your $\sigma$'s. That variance-covariance matrix of $x$ is in general &lt;em&gt;not&lt;/em&gt; diagonal, meaning the values are correlated. The square roots of the diagonal elements of $ A^{-1} \Sigma_B (A^{-1})^\top$ are the standard deviations of the components of $x$.&lt;/p&gt;&#10;&#10;&lt;p&gt;This approach applies to more than two dimensions as well.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-04-28T23:37:28.723" Id="57533" LastActivityDate="2013-04-28T23:37:28.723" OwnerUserId="805" ParentId="57532" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;In general, there is no formula of the type you suggest. For example, take $x_1  = (1.1, 1, 0.9)$, $x_2 = -x_1$, $y_1 = (1.1, 0.8, 1.1)$, and $y_2 = -y_1$. Then $r_1 = r_2 = 0$, but $r$ is almost 1.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-29T07:20:10.153" Id="57544" LastActivityDate="2013-04-29T07:20:10.153" OwnerUserId="24498" ParentId="57540" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="57564" AnswerCount="1" Body="&lt;p&gt;Let $D$ be a positive random variable with $\mathrm{E}[D] &amp;lt; \infty$. Assume that there exist $y&amp;gt;0$ and $\epsilon&amp;gt;0$ such that: $P(D &amp;gt; \mathrm{E}[D]+y) &amp;gt; \epsilon$. Prove that there exist other $y'&amp;gt;0$, $\epsilon'&amp;gt;0$ such that: $P(D &amp;lt; \mathrm{E}[D]-y') &amp;gt; \epsilon'$.&lt;/p&gt;&#10;&#10;&lt;p&gt;In words, if there is some probability to be strictly above the expectation then there should be some probability to be strictly below the expectation.&lt;/p&gt;&#10;&#10;&lt;p&gt;I do not want to assume $V(D)$ is finite.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-29T11:07:26.897" Id="57560" LastActivityDate="2013-04-29T13:26:35.483" LastEditDate="2013-04-29T11:24:25.233" LastEditorUserId="8077" OwnerUserId="25007" PostTypeId="1" Score="0" Tags="&lt;probability&gt;" Title="Probability to be less the expectation" ViewCount="67" />
  <row Body="&lt;p&gt;You could go ahead and just add all these &quot;alternatives&quot; giving you a single vector. It tells you nothing about the interaction of those terms but if you are just looking into wether there was a structural break because of different investments it should suffice (as long as you have data for all alternatives, that is).  &lt;/p&gt;&#10;&#10;&lt;p&gt;Otherwise you'd probably have to use multiple dummies and interactions.  On the one hand it is a good idea to have one non-interaction dummy for your slope just because you never know what you missed. On the other hand you'll have a shitload of multilinearity in such a model.&lt;/p&gt;&#10;&#10;&lt;p&gt;It really depends on how you are approaching this. If you already have a concrete, single day for all those other variables then you can just go ahead and do an F/Chow test on that date for all simultaneously.&#10;If that is significant, you can start testing combinations. Once again, multicollinearity will make this a bitch.  &lt;/p&gt;&#10;&#10;&lt;p&gt;On the other hand if you do not know a predetermined date, you'll have to start with a QLR test anyway. I have written on this previously but it turns out to be an optimization problem.&lt;br&gt;&#10;I'd just write a program to test QLR test all dates and dummys and pick the combinations with the highest probability, then go from there.  &lt;/p&gt;&#10;&#10;&lt;p&gt;To answer your question: There is no easy way to test for the correct combination of dummys and dates other than to test around it. Technically if you are really having trouble finding any significance, you'll have to make sure you use modified critical values (Quandt likelihood ratio test  criticals) as well.&lt;/p&gt;&#10;&#10;&lt;p&gt;But all of this depends on the validity of your model. Because those series you mentioned might not behave very well anyways. From personal experience fitting a heavy dummied model to these kinds of price timeseries, it ain't gonna be pretty.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-29T11:23:57.310" Id="57563" LastActivityDate="2013-04-29T11:23:57.310" OwnerUserId="18459" ParentId="57548" PostTypeId="2" Score="1" />
  
  
  
  <row AnswerCount="2" Body="&lt;p&gt;My experiment had a 2x3 design with one covariate. If I analyse the results using an ANOVA, I get a strongly significant interaction between the two main factors (p&amp;lt;.001). If I add the covariate into the analysis and do an ANCOVA, the interaction between the two main factors is not significant (p=.252).&lt;/p&gt;&#10;&#10;&lt;p&gt;The covariate itself was not significant, nor did it have any significant interactions.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are similar questions on this website, but I was hoping to understand what does such a result pattern say about the covariate? How should I interpret the results?&lt;/p&gt;&#10;&#10;&lt;p&gt;Adding the covariate was the novel feature of my study and therefore it is important to me to understand this interaction properly.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is the covariate influencing the results?&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT:&lt;/p&gt;&#10;&#10;&lt;p&gt;I've been thinking about this all day and I guess the possible interpretation would be:&#10;When controlling for the covariate, the interaction of the main effects is no longer significant.&lt;/p&gt;&#10;&#10;&lt;p&gt;In which case I don't understand a couple of things:&lt;/p&gt;&#10;&#10;&lt;p&gt;a) Does that mean the covariate is responsible for the interaction?&lt;/p&gt;&#10;&#10;&lt;p&gt;b) Why is the interaction between FactorA*FactorB*Covariate not significant then?&lt;/p&gt;&#10;&#10;&lt;p&gt;c) Why isn't the covariate significantly interacting with anything at all, when its presence seems to be influencing the results?&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT 2:&lt;/p&gt;&#10;&#10;&lt;p&gt;Maybe I am doing something wrong, because it is not making much sense to me.&#10;Here are my results:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://imageshack.us/g/580/ancova1.png/&quot; rel=&quot;nofollow&quot;&gt;http://imageshack.us/g/580/ancova1.png/&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The last two pictures show when I run an ANOVA without the covariate (repeated measures, Factor 1 (2 levels). Factor 2 (3 levels).&lt;/p&gt;&#10;&#10;&lt;p&gt;The first three when I run an ANCOVA on the same factors, except I put in OptimismScore as a covariate.&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2013-04-29T15:00:22.243" Id="57585" LastActivityDate="2015-02-28T06:20:39.677" LastEditDate="2013-04-29T19:24:44.313" LastEditorUserId="25014" OwnerUserId="25014" PostTypeId="1" Score="4" Tags="&lt;anova&gt;&lt;interaction&gt;&lt;ancova&gt;" Title="2x3 ANOVA interaction no longer significant after including a covariate?" ViewCount="695" />
  
  <row Body="&lt;p&gt;I don't use SVM, but in Logistic Regression and ANN's I have sucessfully used k-fold with replicated training data (cut the folds, generate the proper training\test pairs, generate replicas of the less represented class only in training data), sometimes with noise. Biases were removed in the selection of the cutpoints.&lt;/p&gt;&#10;&#10;&lt;p&gt;A very elementary review to techniques available for unbalanced data if found on &lt;a href=&quot;http://www.ijcst.com/icaccbie11/sp1/krishnaveni.pdf&quot; rel=&quot;nofollow&quot;&gt;this paper&lt;/a&gt;. My favorite technique so far (PCA plus noise injection) is found in &lt;a href=&quot;http://scholar.google.com/scholar?q=%22An%20Approach%20for%20Learning%20from%20Small%20and%20Unbalanced%20Data%20Sets%20Using%20Gaussian%20Noise%20During%20Artificial%20Neural%20Network%20Training%22&amp;amp;btnG=&amp;amp;hl=en&amp;amp;as_sdt=0,5&quot; rel=&quot;nofollow&quot;&gt;this paper&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-29T15:07:29.753" Id="57586" LastActivityDate="2013-04-29T15:07:29.753" OwnerUserId="24175" ParentId="57539" PostTypeId="2" Score="1" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I have a the following time series &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;  Price      BrokerID 632 Behaviour  BrokerID 680 Behaviour ...BrokerID XYZ Behaviour&#10;&#10;  5.6          IP                       SP                   &#10;  5.7          BP                       IP&#10;  5.8          SP                       BP&#10;  5.83         IP                       SP&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;where &lt;code&gt;IP&lt;/code&gt; is idle position, &lt;code&gt;BP&lt;/code&gt; is buying position, and &lt;code&gt;SP&lt;/code&gt; is selling position. I want to use Broker behaviour as the known variable and price as the hidden variable and predict it using HMM. But my question is how to find the emission matrix between a character vector (broker behaviour) and price numeric vector? &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-04-29T17:03:08.587" FavoriteCount="1" Id="57602" LastActivityDate="2014-01-28T19:30:28.277" LastEditDate="2013-06-30T20:45:35.737" LastEditorUserId="88" OwnerUserId="24726" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;machine-learning&gt;&lt;hidden-markov-model&gt;" Title="Predicting high frequency finance time series with HMM" ViewCount="842" />
  <row AnswerCount="0" Body="&lt;p&gt;The following is a question from an exam paper on evaluating the performance of search engines. To this day I looked in my text book and literally close to 50 web pages and I can't&lt;br&gt;&#10;find one convincing argument for any of the cases. Can anyone help shed a light on this?&lt;/p&gt;&#10;&#10;&lt;p&gt;You have developed a new retrieval algorithm and&#10;want to evaluate its performance. To this end, you have crawled one billion&#10;webpages. The experiments take too long with your current infrastructure,&#10;so you randomly sample 10% of the data, run 100 queries on the sample&#10;and ask human subjects to assess the relevance of the top 100 results. After&#10;averaging, you observe the following mean recall and precision at diﬀerent&#10;ranks:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Rank:        1   2     3    4    5  ...  10   20  ...  50  ... 100&#10;Recall:    0.09 0.15 0.20 0.25 0.30 ... 0.50 0.70 ... 0.90 ... 1.00&#10;Precision: 0.90 0.75 0.67 0.63 0.60 ... 0.50 0.35 ... 0.18 ... 0.10&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Consider re-running the same experiment without sampling the data. Do&#10;you expect the following numbers to increase, decrease or stay the same:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;i. Recall at rank 10.&#10;ii. Precision at rank 10.&#10;iii. Precision at 50% recall.&#10;iv. Mean average precision.&#10;v. Area under the ROC curve&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2013-04-29T18:59:28.903" FavoriteCount="1" Id="57615" LastActivityDate="2013-04-29T18:59:28.903" OwnerUserId="23817" PostTypeId="1" Score="1" Tags="&lt;sample-size&gt;&lt;roc&gt;&lt;precision-recall&gt;&lt;out-of-sample&gt;&lt;average-precision&gt;" Title="Precision, Recall and area under ROC curve as sample size increases" ViewCount="235" />
  <row AnswerCount="0" Body="&lt;p&gt;I have two related questions regarding the computation of a non-parametric bootstrap confidence interval for the prediction error.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Setting:&lt;/strong&gt; I have a sample S from a data population P and a learner L, and I want to compute a 95% confidence interval for the 0.632 bootstrap estimator $\hat{\theta}_{.632bs}$ of the prediction error $\theta$ of the classifier C learned by learner L on sample S.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Q1&lt;/strong&gt; My first question is whether the following procedure, to compute the confidence interval with the percentile method, is correct. Specifically, the sampling of a test set $S_{test}$ in step 2 to evaluate my classifier on. I have read that, for every observation, you have to keep track of predictions from bootstrap samples not containing that observation. This seems to come down to the same thing according to me?  I compute the confidence interval with the percentile method as follows:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;I compute the resubstitution error $\theta_{resub}$ of the classifier C learned by L&#10;on S&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;I compute N (e.g. = 1000) bootstrap estimates $\hat{\theta}_i^*$ of the prediction error on S as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;For i = 1 : 1000 do:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Sample from S with replacement until I have a new sample $S_{train}$ the same size as S&lt;/li&gt;&#10;&lt;li&gt;From all the instances that were not sampled in step 1, I sample again with replacement until I have a sample $S_{test}$ with the same size as S&lt;/li&gt;&#10;&lt;li&gt;I learn a classifier on $S_{train}$ and evaluate it on $S_{test}$. The classifier performance on $S_{test}$ is my bootstrap estimate $\hat{\theta}_i^*$.&lt;/li&gt;&#10;&lt;/ul&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;For each sample i of the N bootstrap samples I compute the estimate $\hat{\theta}^*_{i,.632bs}$ as&#10;$\hat{\theta}^*_{i,.632bs} = 0.368 \cdot \theta_{resub} + 0.632 \cdot \hat{\theta}_i^*$&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;I sort the 1000 bootstrap estimates&lt;/li&gt;&#10;&lt;li&gt;I select the 25th bootstrap estimate as the lower bound of the confidence interval and the 975th estimate as the upper bound of the confidence interval.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Q2&lt;/strong&gt; My second question is how to compute the confidence interval for the 0.632 bootstrap estimator for the prediction error with the bias corrected accelerated (BCa) method. Namely, I find that I have a problem here: I need an estimate of a term $b$ for the bias correction, and for term $a$, the acceleration term (&lt;a href=&quot;http://www.tau.ac.il/~saharon/Boot/10.1.1.133.8405.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.tau.ac.il/~saharon/Boot/10.1.1.133.8405.pdf&lt;/a&gt; p1153-1154). &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;To compute $b$, I need an estimate of the prediction error $\hat{\theta}$ from the complete sample S. I am not sure how to do this. I see several possibilities: The average of $\hat{\theta}^*_{i,.632bs}$ over all $i$, the resubstitution error, a hold-out error, perform cross-validation on S...? &lt;/li&gt;&#10;&lt;li&gt;I have the same problem for computing $a$. Here, I need jackknife estimates that use all of the data from S except one instance $x_i$ ($S \setminus x_i$). What exactly do I compute here? Is this computation the same as computing the leave-one-out cross-validation estimator for the prediction error?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I have tried finding the answers in the literature, but I have difficulty understanding the papers I found, so I hope somebody can help me here. Thank you!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-29T19:17:06.643" FavoriteCount="0" Id="57617" LastActivityDate="2013-04-30T08:21:45.850" LastEditDate="2013-04-30T08:21:45.850" LastEditorUserId="25031" OwnerUserId="25031" PostTypeId="1" Score="2" Tags="&lt;classification&gt;&lt;confidence-interval&gt;&lt;nonparametric&gt;&lt;bootstrap&gt;" Title="Computing a bootstrap confidence interval for the prediction error with the percentile and the BCa method" ViewCount="263" />
  <row AnswerCount="2" Body="&lt;p&gt;I would like to compare two Simpson Indices from two different populations. I have calculated their variance, as it is done in the original paper by Simpson regarding measures of diversity and I have calculated a confidence interval for each of them using the formula:&lt;/p&gt;&#10;&#10;&lt;p&gt;$(S-2\sqrt{\text{var}},S+2\sqrt{\text{var}})$, &lt;/p&gt;&#10;&#10;&lt;p&gt;as suggested in a published paper.&lt;/p&gt;&#10;&#10;&lt;p&gt;What I would like to find is a p-value of the null hypothesis that the two indices are equal.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have read that someone can do a Welch t-test to compare them, but I haven't found any single paper or book with such an application. &lt;/p&gt;&#10;&#10;&lt;p&gt;My questions regarding this application are:&lt;/p&gt;&#10;&#10;&lt;p&gt;1) In Welch t-test the variances in the denominator are divided by $n_1$ and $n_2$ respectively, since it is the SE of the mean. I guess in this case and based on the formula for the CI we shouldn't divide by $n$ and have just the square root of the sum of variances. Correct?   &lt;/p&gt;&#10;&#10;&lt;p&gt;2) The degrees of freedom for a simple t-test are $n_1+n_2-2$, while for the Welch t-test is quite a complicated formula which gives a result close but not the same as $n_1+n_2-2$. Which one should be used?&lt;/p&gt;&#10;&#10;&lt;p&gt;3) By $n_1$ and $n_2$ above we mean the number of different categories in each population rather than the total number in each case. Correct?&lt;/p&gt;&#10;&#10;&lt;p&gt;I would be very grateful if someone can help me with these questions and even more if someone can provide some sort of documentation so I can justify my analysis. &lt;/p&gt;&#10;&#10;&lt;p&gt;Reference:&lt;/p&gt;&#10;&#10;&lt;p&gt;Simpson, E. H. (1949), Measurement of diversity. &lt;em&gt;Nature&lt;/em&gt;, &lt;strong&gt;163&lt;/strong&gt;, 688 (&lt;a href=&quot;http://people.wku.edu/charles.smith/biogeog/SIMP1949.pdf&quot; rel=&quot;nofollow&quot;&gt;pdf&lt;/a&gt;)&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2013-04-29T19:28:00.497" Id="57618" LastActivityDate="2013-08-30T00:04:44.533" LastEditDate="2013-04-30T23:21:22.650" LastEditorUserId="805" OwnerUserId="25032" PostTypeId="1" Score="2" Tags="&lt;t-test&gt;" Title="Comparison of two Simpson indices using t-test" ViewCount="354" />
  <row AnswerCount="0" Body="&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Pivotal_quantity&quot; rel=&quot;nofollow&quot;&gt;A pivotal quantity&lt;/a&gt; $Q(X, \theta)$ can be used to construct a confidence interval. I was wondering if it can be used to construct a test statistic and rejection region? In simpler cases involving a simple null hypothesis it is obvious how to use a pivot to construct a test statistic and rejection region. What about composite null hypotheses? Thanks and regards!&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;strong&gt;Pivotal quantities are fundamental to the construction of test statistics&lt;/strong&gt;, as they allow the statistic to not depend on parameters – for example, Student's t-statistic is for a normal distribution with unknown variance (and mean). &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="9" CreationDate="2013-04-29T22:21:48.830" Id="57636" LastActivityDate="2013-04-30T22:53:48.303" LastEditDate="2013-04-30T22:53:48.303" LastEditorUserId="805" OwnerUserId="1005" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;&lt;mathematical-statistics&gt;&lt;pivot&gt;" Title="Can pivot be used for testing" ViewCount="72" />
  <row Body="&lt;p&gt;Here's the Bayesian answer. How would the Bayesian estimate the population mean? Assuming a very noninformative prior (with precision zero, see comments), that's simply the average:&#10;$$\mu'=1/n\sum_{i=1}^n x_i\approx 6.7$$&#10;Since you know the population standard deviation, you know the population variance $Var=SD^2$, and you know the precision $\rho=1/Var=16$. The Bayesian learning rule for the precision is the sum of the precisions of your observations. You have 3, so $\rho'=3*16=48$, $Var=1/48$, $SD=\sqrt{1/48}\approx 0.144$.  So the Bayesian thinks the population mean is normally distributed with mean $\mu'$ and SD $0.144$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The question is: Given your estimate from the observations, how likely is it that the test value 6ppb or less can originate from $N(\mu',\rho')$? We check a standard normal table. Computing the Z score: $$Z=(6-\mu')/sd=-0.7/0.144\approx -4.86.$$&#10;The &lt;a href=&quot;http://en.wikipedia.org/wiki/Standard_normal_table#Cumulative_table&quot; rel=&quot;nofollow&quot;&gt;cumulative table&lt;/a&gt; actually only goes as far as $Z=3$, at which point $0.9987$ of the values are below $Z$, hence $1-0.9987$ of the values are below $Z=-3$ (by symmetry of the normal) and even fewer below $Z=-4.8$. Thus, you can reject the hypothesis that the estimated population mean is equal to the test value $6$ with your $\alpha$ level, since $1-0.9987&amp;lt;\alpha$.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-04-29T22:29:44.187" Id="57638" LastActivityDate="2013-04-30T10:09:50.373" LastEditDate="2013-04-30T10:09:50.373" LastEditorUserId="22543" OwnerUserId="22543" ParentId="57627" PostTypeId="2" Score="1" />
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;I would still like to have some measure of uncertainty for the pooled values. Any ideas of how to pool CVs? Or is this just not possible, at least conceptually?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;You almost did it in your question; you have rules for expectations of sums (covered in your question) and for variances of sums (the one you used is correct if you have independence, but if you don't there's still a rule for it).&lt;/p&gt;&#10;&#10;&lt;p&gt;What's the definition of CV?&lt;/p&gt;&#10;&#10;&lt;p&gt;You have already specified how to get the variance of the sum and the mean of the sum.&lt;/p&gt;&#10;&#10;&lt;p&gt;You need the &lt;em&gt;standard deviation&lt;/em&gt; and the mean. How will you get that from what you have?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-04-30T00:34:45.007" Id="57644" LastActivityDate="2013-04-30T00:34:45.007" OwnerUserId="805" ParentId="57581" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a binary variable that I'm investigating in SPSS, &lt;code&gt;inclination_to_dance&lt;/code&gt;. I have another linear variable as well, &lt;code&gt;no_of_beers&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;In SPSS, via the &lt;code&gt;Analyze&lt;/code&gt; menu, I'm able to run a binary logistic regression against these two that gives me some kind of output that tells me whether the impact of drinking on dancing is significant, as well as its coefficient.&lt;/p&gt;&#10;&#10;&lt;p&gt;What I &lt;em&gt;really&lt;/em&gt; want to know, though, is what the effect of drinking one additional beer will have on the probability that one will dance. Is there a way to investigate that probability in SPSS?&lt;/p&gt;&#10;" ClosedDate="2013-04-30T14:37:05.747" CommentCount="7" CreationDate="2013-04-30T06:50:08.227" FavoriteCount="1" Id="57662" LastActivityDate="2013-04-30T06:50:08.227" OwnerUserId="24701" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;probability&gt;&lt;logistic&gt;&lt;spss&gt;&lt;binary-data&gt;" Title="spss 20 for mac: how to get probabilities after running a binary logistic regression" ViewCount="22" />
  <row Body="&lt;p&gt;(Hoermann and Leydold 2003) provide one application of such a problem with a discussion of an algorithm. In this case we have points on a CDF and a PDF, and we want to write the CDF as an interpolating spline and use the fact that the PDF gives the first derivatives of the CDF. Moreover, it also includes the constraint that a CDF is never decreasing. In this case the purpose is to invert the CDF to get an approximation of the quartile function. This is pretty specific, but it may give you enough inspiration to solve your problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;Hoermann, Wolfgang and Leydold, Josef. (2003). Continuous random variate generation by fast numerical inversion.  &lt;em&gt;ACM Transactions on Modeling and Computer Simulation&lt;/em&gt;, 13(4): 347--362.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-04-30T07:44:31.450" Id="57666" LastActivityDate="2013-04-30T07:44:31.450" OwnerUserId="23853" ParentId="57595" PostTypeId="2" Score="1" />
  
  
  
  
  <row Body="&lt;p&gt;The Ljung-Box statistic is not significant calculated over lags one to six lag six simply because you have no particularly large autocorrelations until lags seven &amp;amp; eight.  Plot &amp;amp; examine the auto-correlation function of the residuals. There's some evidence for lack of fit, so perhaps you can come up with a better model:  higher autocorrelations seem to repeat at intervals of seven without dropping off, suggesting seasonal differencing with a period of seven might be worth a try; possibly a seasonal moving average term.  All the same, with about 150 observations your current model doesn't look awful.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-04-30T16:20:43.563" Id="57721" LastActivityDate="2013-04-30T16:20:43.563" OwnerUserId="17230" ParentId="57647" PostTypeId="2" Score="3" />
  
  
  <row Body="&lt;p&gt;Any library? Mathematica has it. Here's the code for an example plot of a Dirichlet CDF from the documentation:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Plot3D[CDF[DirichletDistribution[{1, 3, 2}], {x, y}], {x, 0, 1}, {y, 0, 1}]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2013-04-30T20:03:46.010" Id="57739" LastActivityDate="2013-04-30T20:03:46.010" OwnerUserId="12820" ParentId="57262" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;Do you mean something like this?:&lt;/p&gt;&#10;&#10;&lt;p&gt;in R:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;R &amp;lt;- matrix(c(1.0, 0.1, 0.1, &#10;            0.1, 1.0, 0.1, &#10;            0.1, 0.1, 1.0), nrow=3)&#10;N &amp;lt;- 100&#10;&#10;chi &amp;lt;- -2*log(det(R)^(N/2))&#10;df &amp;lt;- nrow(R)*(nrow(R)-1)/2&#10;p &amp;lt;- 1 - pchisq(chi, df)  &#10;chi&#10;p&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Or in Excel:&#10;Where the matrix is in cells C26:E28, and N is 100:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;=-2*(LN(MDETERM(C26:E28)^100/2))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And the above is in cell D31:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;=CHIDIST(D31,3)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;You can also use the sem package:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;require(sem)  &#10;rownames(R)  &amp;lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)&#10;colnames(R)  &amp;lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)&#10;mySem &amp;lt;- specifyModel()&#10;  a &amp;lt;-&amp;gt; a, va, NA&#10;  b &amp;lt;-&amp;gt; b, vb, NA&#10;  c &amp;lt;-&amp;gt; c, vc, NA&#10;&#10;semFit &amp;lt;- sem(mySem, S=R, N=100)  &#10;summary(semFit)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;(sem gives a very slightly different answer, because it multiplies by N-1).&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-04-30T20:15:17.303" Id="57742" LastActivityDate="2013-04-30T20:40:26.353" LastEditDate="2013-04-30T20:40:26.353" LastEditorUserId="17072" OwnerUserId="17072" ParentId="57684" PostTypeId="2" Score="0" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to replicate and hopefully improve on an analysis done in a study to find determinants of patient satisfaction after shoulder surgery.  Satisfaction is heavily skewed (with over 60% of patients responding 10)  The original study ran a multiple linear regression on satisfaction with a handful of predictors.  The residuals are just as skewed as the satisfaction variable itself, and no transformations I've tried have helped, probably because the variable is discrete (could argue ordinal or numerical).&lt;/p&gt;&#10;&#10;&lt;p&gt;My other thoughts have been to use ordinal regression on the full 10-level scale, or lump responses together into two or more groups for binary (say 10 vs not 10) or multinomial logistic regression.  The former seems to suffer from decreased interpretability and the later seems to lose information that might be important.&lt;/p&gt;&#10;&#10;&lt;p&gt;Am I missing any ideas/techniques?  How do I choose between these various evils?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance for any responses!&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-05-01T01:23:44.993" Id="57767" LastActivityDate="2013-05-01T01:23:44.993" OwnerUserId="24523" PostTypeId="1" Score="0" Tags="&lt;logistic&gt;&lt;multiple-regression&gt;&lt;ordinal&gt;&lt;residuals&gt;&lt;normality&gt;" Title="Modelling a skewed, 10-point Satisfaction variable" ViewCount="50" />
  
  
  <row Body="&lt;p&gt;It is common for systematic reviews to be looking at intervention vs. placebo or no additional intervention. The issue here relates more to risk of bias rather than the meta-analytic technique. There is no problem combining these two trial designs in one meta-analysis, but lack of blinding could be a source of statistical heterogeneity (if present in the analysis). It's also important to keep in mind that not all placebos are the same (e.g. injection vs. pill) and a lot of studies don't report adequately on the properties of placebos for the reviewer to be confident of it's use. In other words, just because a study is reported to be a double-blind, placebo-controlled trial doesn't mean that it was set up and conducted properly to prevent patients, personnel and outcome assessors to be blinded to intervention. Have a look at the Cochrane Handbook for Systematic Reviewers on more details guidance (cochrane-handbook.org).&lt;/p&gt;&#10;&#10;&lt;p&gt;Good luck.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-01T07:25:42.527" Id="57786" LastActivityDate="2013-05-01T07:25:42.527" OwnerUserId="24137" ParentId="56351" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I hope that you find this tutorial helpful (&lt;a href=&quot;http://www.biomedcentral.com/content/pdf/1471-2288-10-54.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.biomedcentral.com/content/pdf/1471-2288-10-54.pdf&lt;/a&gt;). It describes the use of a network meta-analysis for accounting for the lack of independence in multi-arm trials. The field is growing quickly and I expect to see a lot of new work on this area in the upcoming years.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-05-01T07:49:23.597" Id="57789" LastActivityDate="2013-05-01T07:49:23.597" OwnerUserId="24137" ParentId="56450" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;The multivariate normal central moment generating function is $\exp(t'St/2)$,&#10;from which it follows that the mixed fourth central moments are &#10;(in two different notations):&lt;/p&gt;&#10;&#10;&lt;p&gt;$m_{22} = m_{20} m_{02} + 2 m_{11}^2  $&lt;/p&gt;&#10;&#10;&lt;p&gt;$\quad\quad = s_{1122} = s_{11} s_{22} + 2 s_{12}^2 = s_{1}^2 s_{2}^2 (1 + 2 r_{12}^2)$&lt;/p&gt;&#10;&#10;&lt;p&gt;$m_{31} = 3 m_{20} m_{11} $&lt;/p&gt;&#10;&#10;&lt;p&gt;$\quad\quad = s_{1112} = 3 s_{11} s_{12} = 3 s_{1}^3 s_{2} r_{12}$&lt;/p&gt;&#10;&#10;&lt;p&gt;$m_{211} = m_{200} m_{011} + 2 m_{110} m_{101}  $&lt;/p&gt;&#10;&#10;&lt;p&gt;$\quad\quad\,\, = s_{1123} = s_{11} s_{23} + 2 s_{12} s_{13} = s_{1}^2 s_{2} s_{3} (r_{23} + 2 r_{12} r_{13})$&lt;/p&gt;&#10;&#10;&lt;p&gt;$m_{1111}\,\, = m_{1100} m_{0011} + m_{1010} m_{0101} + m_{1001} m_{0110}  $&lt;/p&gt;&#10;&#10;&lt;p&gt;$\quad\quad\quad = s_{1234} = s_{12} s_{34} + s_{13} s_{24} + s_{14} s_{23} = s_1 s_2 s_3 s_4 (r_{12} r_{34} + r_{13} r_{24} + r_{14} r_{23})$&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-05-01T07:59:02.013" Id="57791" LastActivityDate="2013-05-31T09:21:36.613" LastEditDate="2013-05-31T09:21:36.613" LastEditorUserId="805" OwnerUserId="20776" ParentId="57740" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="61019" AnswerCount="2" Body="&lt;p&gt;I'd like to wrap my head around this topic but learning from white-papers and tutorials is hard because there are many gaps which are usually filled in textbooks. &lt;/p&gt;&#10;&#10;&lt;p&gt;If it is important I have relatively strong mathematical background as I did my Ph.D. in applied mathematics (CFD to be more precise).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-01T09:45:21.703" FavoriteCount="4" Id="57797" LastActivityDate="2013-06-08T11:12:26.197" LastEditDate="2013-06-08T11:12:26.197" LastEditorUserId="22047" OwnerUserId="21104" PostTypeId="1" Score="4" Tags="&lt;bayesian&gt;&lt;nonparametric&gt;&lt;references&gt;&lt;nonparametric-bayes&gt;" Title="Introductory textbook on nonparametric Bayesian models?" ViewCount="434" />
  
  <row Body="&lt;p&gt;Most important is to figure out what the message you want to communicate is and try to find a graph whose message is close to that. &lt;/p&gt;&#10;&#10;&lt;p&gt;Some graphs that might spur some ideas:&lt;/p&gt;&#10;&#10;&lt;p&gt;Comparing bar charts (aka discrete histograms) shows the different distributions but perhaps misleads since score=0 has a higher bar for &quot;little better&quot; than &quot;lot better&quot;, though it's &lt;em&gt;relatively&lt;/em&gt; less common.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/zKVtN.png&quot; alt=&quot;comparing bar charts&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;A mosaic plot (aka spine plot) makes the population difference and the relative sizes clearer but it's harder to make out the distribution shape.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/SMfI7.png&quot; alt=&quot;mosaic plot&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Overlaid smooth distribution curves emphasizes the different distribution shapes but with a loss of detail.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ZWt29.png&quot; alt=&quot;compare densities&quot;&gt;&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-05-01T14:39:36.727" Id="57810" LastActivityDate="2013-05-01T14:39:36.727" OwnerUserId="1191" ParentId="57610" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;You could make the (very naive) assumption that the best features for SVM will also be the best ones for other classifiers as NB. Therefore you can (as you said) select the best features for NB. You can also include the similarity ones in this set.&lt;/p&gt;&#10;&#10;&lt;p&gt;Other (better) option is to apply feature selection weighting to see which features separate the classes better. In Text Classification, chi^2 is commonly applied and it will provide an idea (and a ranking) of how good the features are.&lt;/p&gt;&#10;&#10;&lt;p&gt;One question though, why do you want to select the features in advance in this fashion? I mean, apart from scalability issues, SVM is known for dealing well with very large number of features.&lt;/p&gt;&#10;&#10;&lt;p&gt;I think I have only partially answer your question, I hope it helps...&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-05-01T17:19:23.953" Id="57838" LastActivityDate="2013-05-01T17:19:23.953" OwnerUserId="24721" ParentId="57836" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="57846" AnswerCount="1" Body="&lt;p&gt;I have a data set that is repeated-measures ratings of four devices (two two-level variables) for 6 dimensions (ease, confidence, comfort, control, size and fit), with a two-level between-subjects variable. Like so: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ptcip / grp / fdback / dur / assem / accep / freq / t.vs.r / attrib / meas / d.rating&#10;1    RA   binary    short       &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt; &amp;lt;NA&amp;gt; rating       ease    9         2&#10;1    RA   binary    short       &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt; &amp;lt;NA&amp;gt; rating confidence    7         1&#10;1    RA   binary    short       &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt; &amp;lt;NA&amp;gt; rating    comfort    6         4&#10;1    RA   binary    short       &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt; &amp;lt;NA&amp;gt; rating    control    5         3&#10;1    RA   binary    short       &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt; &amp;lt;NA&amp;gt; rating       size    7        -1&#10;1    RA   binary    short       &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt; &amp;lt;NA&amp;gt; rating        fit    6         0&#10;1    RA   binary      med       &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt; &amp;lt;NA&amp;gt; rating       ease    9         6&#10;1    RA   binary      med       &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt; &amp;lt;NA&amp;gt; rating confidence    5         6&#10;1    RA   binary      med       &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt; &amp;lt;NA&amp;gt; rating    comfort    6         5&#10;1    RA   binary      med       &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt; &amp;lt;NA&amp;gt; rating    control    9        -1&#10;1    RA   binary      med       &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt; &amp;lt;NA&amp;gt; rating       size    2        -1&#10;1    RA   binary      med       &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt; &amp;lt;NA&amp;gt; rating        fit    8        -2&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I need to run separate ANOVAs on each of the dimensions (we know from other analyses that the dimensions are rated differently, so including it as a factor wouldn't be very informative). What's the most efficient way to write this--split each of the dimensions off into separate data frames, or specify aov to be run on data iff attrib == &quot;ease&quot;? Seems like the latter would be better, but I'm new to R.&lt;/p&gt;&#10;&#10;&lt;p&gt;The model I am currently specifying is&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;aov(d.ratings ~ grp*fdback*dur + Error(particip/(fdback*dur))+grp, PDdonly)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;but as that collapses over the six dimensions, it's not actually meaningful atm.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-01T18:47:22.337" FavoriteCount="1" Id="57844" LastActivityDate="2013-05-01T19:05:21.923" OwnerUserId="24671" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;anova&gt;" Title="Efficient way to run multiple ANOVAs on subsets of data" ViewCount="278" />
  <row AnswerCount="0" Body="&lt;p&gt;I came across this paper that uses link anomaly detection to predict trending topics, and I found it incredibly intriguing: The paper is &lt;em&gt;&lt;a href=&quot;http://arxiv.org/abs/1110.2899&quot; rel=&quot;nofollow&quot;&gt;&quot;Discovering Emerging Topics in Social Streams via Link Anomaly Detection&quot;&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would love to replicate it on a different data set, but I'm not familiar enough with the methods to know how to use them. Let's say I have a series of snapshots of network of nodes across a period of six months. The nodes have a long-tailed degree distribution, with most having only a few connections, but some having a great many. New nodes appear within this time period.&lt;/p&gt;&#10;&#10;&lt;p&gt;How could I implement sequentially discounted normalized maximum likelihood calculations used in the paper to detect anomalous links that I think might be precursors to a burst? Are there other methods that would be more appropriate?&lt;/p&gt;&#10;&#10;&lt;p&gt;I ask both theoretically and practically. If someone could point me to a way to implement this in python or R, that would be very helpful.&lt;/p&gt;&#10;&#10;&lt;p&gt;Anyone? I know you smart folks out there have some starting thoughts for an answer,&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-05-01T19:56:49.093" FavoriteCount="1" Id="57850" LastActivityDate="2013-05-02T21:18:44.283" LastEditDate="2013-05-02T21:18:44.283" LastEditorUserId="24230" OwnerUserId="24230" PostTypeId="1" Score="6" Tags="&lt;time-series&gt;&lt;machine-learning&gt;&lt;outliers&gt;&lt;python&gt;&lt;change-point&gt;" Title="Link Anomaly Detection in Temporal Network" ViewCount="327" />
  
  
  
  
  
  
  
  
  
  <row Body="&lt;p&gt;I'm not a statistician. Here's a stab anyways.&lt;/p&gt;&#10;&#10;&lt;p&gt;Basically, we generally don't &lt;em&gt;know&lt;/em&gt; the true value of some parameter. So we can't talk about how close we are to the truth (i.e. we don't know how accurate our measure is). Instead, we collect a bunch of data and make estimates of the truth. If those estimates vary a lot, then that's the definition of imprecision. If they vary little, then we have a precise estimate. But is it accurate? Who knows? We can only hope that we chose the best methods that lead us to an accurate assessment of the truth. If we have, then we presume we have an accurate measure, but we're only sure about how precise it is.&lt;/p&gt;&#10;&#10;&lt;p&gt;To clarify, certainly statisticians' idea of precision has nothing to do with how many digits we calculate a mean to. Instead, it's roughly how likely we would be to get a similar estimate if we resampled.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-05-02T11:23:29.900" Id="57899" LastActivityDate="2013-05-02T12:05:36.980" LastEditDate="2013-05-02T12:05:36.980" LastEditorUserId="24000" OwnerUserId="24000" ParentId="57884" PostTypeId="2" Score="1" />
  
  
  
  
  
  <row AcceptedAnswerId="58055" AnswerCount="1" Body="&lt;p&gt;Suppose I have $n$ observations $(\pmb{x}_i,y_i)$ and two subsets of indexes of $\{1:n\}$, $S_1$ and $S_2$, with $S_1\neq S_2$, $\#\{S_1\}\neq\#\{S_2\}$, and $\{S_1\cap S_2\}\neq\emptyset$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose I call $\hat{\beta}_1$ the vector of OLS-estimated coefficients of the &#10;regression $y_i\tilde{}\pmb{x}_i|i\in S_1$ and likewise for $\hat{\beta}_2$. &lt;/p&gt;&#10;&#10;&lt;p&gt;My question is  what is the right/best test for &lt;/p&gt;&#10;&#10;&lt;p&gt;$$H_0:\beta_1-\beta_2=0$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Some thoughts:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;My first inclination was for a Wald test, but how do we combine the two covariance matrices from the two regressions? Perhaps by pooling them?&lt;/li&gt;&#10;&lt;li&gt;Another idea I thought about was to use the &lt;a href=&quot;http://en.wikipedia.org/wiki/Bhattacharyya_distance&quot; rel=&quot;nofollow&quot;&gt;Bhattacharyya distance&lt;/a&gt; with $\beta_1$ against $\beta_2$ (together with their respective covariance matrices).&lt;/li&gt;&#10;&lt;li&gt;A third possibility would be to merge the two samples and add an interaction effect &lt;em&gt;on every coefficient&lt;/em&gt; with a dummy taking value 1 for all observations that are, say, members of $S_2$ only, but because of the overlap between, $S_1$ and $S_2$ i'm not sure this is really measuring what I want. &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Any thoughts?&lt;/p&gt;&#10;&#10;&lt;h1&gt;EDIT1:&lt;/h1&gt;&#10;&#10;&lt;p&gt;Here is a simple &lt;code&gt;R&lt;/code&gt; code to implement the solution proposed &#10;by Bill below:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(&quot;MASS&quot;)&#10;n&amp;lt;-200&#10;p&amp;lt;-5&#10;&#10;b&amp;lt;-rnorm(p)&#10;x&amp;lt;-mvrnorm(n,rep(0,p-1),diag(p-1))&#10;y&amp;lt;-cbind(1,x)%*%b+rnorm(n)&#10;L&amp;lt;-sample(1:200,100,replace=TRUE)&#10;L&amp;lt;-list(mod1=L[1:60],mod2=L[61:100])&#10;&#10;M1&amp;lt;-lm(y[L[[1]]]~x[L[[1]],])&#10;M2&amp;lt;-lm(y[L[[2]]]~x[L[[2]],])&#10;&#10;fx01&amp;lt;-function(M1,M2){&#10;    X1&amp;lt;-cbind(1,M1$model[,-1])&#10;     X2&amp;lt;-cbind(1,M2$model[,-1])&#10;    nt&amp;lt;-nrow(X1)+nrow(X2)&#10;    D1&amp;lt;-diag(nt)[1:nrow(X1),]   &#10;    D2&amp;lt;-diag(nt)[1:nrow(X2),]   &#10;    st&amp;lt;-crossprod(c(M1$resid,M2$resid))/(nt-2*ncol(X1))&#10;    P1&amp;lt;-solve(crossprod(X1))%*%crossprod(X1,D1)-solve(crossprod(X2))%*%crossprod(X2,D2)&#10;    P1&amp;lt;-tcrossprod(P1)*drop(st)&#10;    mahalanobis(M1$coef,M2$coef,P1)&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2013-05-02T17:23:10.333" Id="57940" LastActivityDate="2013-05-08T19:31:08.977" LastEditDate="2013-05-08T19:31:08.977" LastEditorUserId="603" OwnerUserId="603" PostTypeId="1" Score="4" Tags="&lt;regression&gt;&lt;hypothesis-testing&gt;" Title="Test for differences between parameters of models estimated from partially overlaping samples" ViewCount="102" />
  
  
  
  
  <row AcceptedAnswerId="68838" AnswerCount="2" Body="&lt;p&gt;I'm currently facing a problem in which some of my training examples belong to more than one class at the same time, say, sample $y_i$ pertains to class $A$ and $B$. I was thinking that a solution to it would be to consider that sample as two-fold, i.e., consider it as two samples, one for class $A$ and one for class $B$. However, my problem is that I'm performing a one-vs-all strategy, in which I think this solution may cause numerical errors (the feature matrix would have identical rows)!&lt;/p&gt;&#10;&#10;&lt;p&gt;Do you know of any reference to this kind of problem (or the technical name for it)?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-02T19:47:08.733" FavoriteCount="1" Id="57954" LastActivityDate="2013-08-31T08:12:39.960" LastEditDate="2013-08-31T08:12:39.960" LastEditorUserId="28903" OwnerUserId="9174" PostTypeId="1" Score="3" Tags="&lt;machine-learning&gt;&lt;supervised-learning&gt;" Title="How to handle samples that belong to multiple classes in supervised learning?" ViewCount="110" />
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I have a linear regression with multiple independent variables.  One of the variables is binary.  If the parameter estimates are positive or negative, does it share the same positive/negative relationship with the dependent variable as a continuous variable?  So would the binary variable with a 1 value have higher dependent variable value than those with a 0 value if it was a positive relationship?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-05-03T02:41:27.527" Id="57982" LastActivityDate="2014-06-15T15:54:19.543" LastEditDate="2013-05-03T16:52:48.897" LastEditorUserId="25185" OwnerUserId="25185" PostTypeId="1" Score="1" Tags="&lt;binary&gt;" Title="Positive/negative relationship for a binary variable in a linear regression" ViewCount="160" />
  
  <row Body="&lt;p&gt;summary: If you can get the number of widgets for each factory then you can just use the counts to do what you want, if you are stuck with the binned data you are probably out of luck. Details below.&lt;/p&gt;&#10;&#10;&lt;p&gt;The bins (10,000 to 20,000 and 20,000 to 40,000) are different widths which is going to make any sort of comparison or prediction very hard since you have will have to make some assumption about how the data are distributed within each bin. It seems like if you know how many factories are in each bin then you (or someone) must know how many widgets each factory produces otherwise how are you getting the numbers to put in each bin. If you have that data you could just use simple counts to answer the sort of questions you wanted, especially since then all widget production is covered, so you don't even have to worry about sample error. &lt;/p&gt;&#10;&#10;&lt;p&gt;Assuming that information is hidden from you as I said you would have make some assumptions about how factories are distributed within bins, if you assume they are evenly distributed then you could estimate how many factories would be greater or less than any specified number of widgets by just multiplying the number of factories by the proportion of the bin above (or below) the specified number of widgets. But this assumption will probably not work very well since your bins are wide relative to the total maximum number of widgets, and the break point is near the centre (to see why this is problem imagine a normal distribution, imagine splitting it into two bins with the break point in the middle, the two bins will be the same height, i.e. they will hide the fact that most factories widget production is near average).           &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-03T06:14:36.087" Id="57995" LastActivityDate="2013-05-04T00:44:12.710" LastEditDate="2013-05-04T00:44:12.710" LastEditorUserId="25193" OwnerUserId="25193" ParentId="57911" PostTypeId="2" Score="1" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm struggling with modeling some experimental data using the &lt;code&gt;lme4&lt;/code&gt; package in R, and would appreciate input.&lt;/p&gt;&#10;&#10;&lt;p&gt;My experimental design is as follows:  subjects entering the experiment answer a screener question, which is used to randomly assign them to a between-subjects condition (variable=&lt;code&gt;rank&lt;/code&gt;, which has 2-levels (0/1)). They then complete a distractor task, and make two choices (&lt;code&gt;choice&lt;/code&gt; being the within-subjects dependent variable).  At each choice, the stimuli are RANDOMLY ASSIGNED and crossed by two factors (&lt;code&gt;msg&lt;/code&gt; has 3-levels (&quot;norm&quot;, &quot;no norm&quot; and &quot;provincial&quot;), and &lt;code&gt;cost&lt;/code&gt; has 2-levels (0/1)).   &lt;strong&gt;Because participants were randomly assigned to both &lt;code&gt;cost&lt;/code&gt; and &lt;code&gt;msg&lt;/code&gt; at two points in time, theoretically they could have the same combination of &lt;code&gt;cost&lt;/code&gt; and &lt;code&gt;msg&lt;/code&gt; at both points in time.&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;My life would be easiest if I could use a wonderful package such as &lt;code&gt;ezANOVA&lt;/code&gt;, but my data won't allow me to do this because every individual doesn't have EVERY combination of the two within-subjects variables &lt;a href=&quot;http://stats.stackexchange.com/questions/57709/error-in-ezanova-with-balanced-dataset-with-no-missing-data&quot;&gt;see here&lt;/a&gt;.  So, I'm in the less-familiar territory of mixed models.&lt;/p&gt;&#10;&#10;&lt;p&gt;My hypothesis argues that there should be a three-way interaction between &lt;code&gt;rank&lt;/code&gt;, &lt;code&gt;msg&lt;/code&gt;, and &lt;code&gt;cost&lt;/code&gt;.  Thus, a simple version of my model might be:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;m1 &amp;lt;- glmer(choice ~ msg*cost*rank + (1|id), data=df, family=&quot;binomial&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;But, I've also seen it suggested &lt;a href=&quot;http://stats.stackexchange.com/questions/46321/how-to-deal-with-repeated-measurements-in-the-same-condition-of-a-factorial-expe&quot;&gt;on this site&lt;/a&gt; that my random effects should be modeled as follows in order to evaluate the interaction between the factor and subjects:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;m1 = lmer(choice ~ msg*cost*rank + (rank|id) + (cost|id), data=df)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The latter model has me straying into unfamilar territory, so I'd appreciate any advice about how to model this data in a way that is (1) simple, but (2) appropriate.&lt;/p&gt;&#10;&#10;&lt;p&gt;Sample data below:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; dput(df[1:700,2:6])&#10;structure(list(time = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, &#10;1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, &#10;1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, &#10;1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, &#10;1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, &#10;1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, &#10;1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, &#10;1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, &#10;1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, &#10;1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, &#10;1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, &#10;1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, &#10;1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, &#10;1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, &#10;1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, &#10;1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, &#10;1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, &#10;1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, &#10;1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, &#10;1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, &#10;1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, &#10;1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, &#10;1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, &#10;1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, &#10;1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, &#10;1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, &#10;1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, &#10;1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, &#10;1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, &#10;1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, &#10;1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, &#10;1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, &#10;1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, &#10;1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1), choice = c(1, 1, &#10;1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, &#10;1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, &#10;1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, &#10;0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, &#10;0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, &#10;1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, &#10;1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, &#10;1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, &#10;1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, &#10;1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, &#10;1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, &#10;1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, &#10;1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, &#10;1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, &#10;0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, &#10;1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, &#10;0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, &#10;1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, &#10;1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, &#10;1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, &#10;1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, &#10;0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, &#10;1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, &#10;1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, &#10;1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, &#10;0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, &#10;1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, &#10;1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, &#10;1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, &#10;1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, &#10;1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, &#10;1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, &#10;1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, &#10;0, 1, 0, 1, 0), msg = structure(c(3L, 1L, 1L, 2L, 3L, 1L, 3L, &#10;3L, 3L, 2L, 2L, 3L, 2L, 3L, 2L, 3L, 1L, 3L, 2L, 3L, 1L, 1L, 3L, &#10;1L, 2L, 3L, 3L, 2L, 2L, 1L, 1L, 1L, 1L, 3L, 2L, 3L, 3L, 2L, 3L, &#10;3L, 1L, 3L, 1L, 2L, 2L, 3L, 2L, 3L, 3L, 2L, 3L, 3L, 1L, 2L, 3L, &#10;3L, 1L, 2L, 3L, 2L, 3L, 3L, 1L, 1L, 3L, 1L, 3L, 2L, 1L, 3L, 2L, &#10;3L, 3L, 2L, 2L, 1L, 2L, 2L, 2L, 3L, 3L, 2L, 2L, 2L, 2L, 2L, 1L, &#10;3L, 2L, 3L, 2L, 3L, 3L, 3L, 1L, 1L, 3L, 1L, 2L, 2L, 3L, 2L, 3L, &#10;3L, 2L, 2L, 3L, 2L, 1L, 2L, 1L, 3L, 2L, 2L, 1L, 3L, 3L, 2L, 3L, &#10;3L, 3L, 2L, 2L, 1L, 2L, 3L, 2L, 2L, 2L, 3L, 3L, 3L, 1L, 2L, 2L, &#10;1L, 1L, 3L, 1L, 3L, 1L, 2L, 1L, 1L, 1L, 2L, 2L, 2L, 1L, 1L, 2L, &#10;2L, 1L, 2L, 1L, 1L, 3L, 3L, 1L, 2L, 2L, 1L, 1L, 2L, 1L, 1L, 3L, &#10;3L, 1L, 2L, 3L, 1L, 1L, 3L, 2L, 2L, 3L, 3L, 1L, 1L, 1L, 2L, 1L, &#10;2L, 3L, 3L, 2L, 1L, 2L, 3L, 1L, 2L, 2L, 1L, 3L, 3L, 1L, 1L, 1L, &#10;3L, 2L, 3L, 1L, 2L, 2L, 3L, 2L, 1L, 3L, 1L, 2L, 2L, 3L, 3L, 2L, &#10;1L, 3L, 3L, 1L, 1L, 2L, 2L, 2L, 1L, 2L, 3L, 2L, 2L, 1L, 1L, 1L, &#10;1L, 1L, 3L, 2L, 2L, 1L, 2L, 2L, 3L, 1L, 1L, 2L, 3L, 2L, 3L, 3L, &#10;3L, 3L, 1L, 3L, 2L, 1L, 2L, 3L, 1L, 1L, 2L, 3L, 3L, 2L, 1L, 1L, &#10;2L, 1L, 2L, 3L, 3L, 3L, 1L, 2L, 2L, 3L, 1L, 3L, 1L, 3L, 3L, 1L, &#10;1L, 3L, 1L, 3L, 1L, 1L, 3L, 1L, 1L, 2L, 2L, 3L, 2L, 3L, 2L, 3L, &#10;1L, 2L, 2L, 1L, 2L, 3L, 3L, 3L, 2L, 3L, 1L, 3L, 1L, 2L, 3L, 1L, &#10;1L, 1L, 1L, 1L, 1L, 2L, 1L, 1L, 1L, 3L, 2L, 3L, 2L, 1L, 3L, 3L, &#10;3L, 1L, 3L, 3L, 1L, 3L, 1L, 3L, 2L, 1L, 3L, 1L, 1L, 2L, 2L, 3L, &#10;1L, 3L, 3L, 3L, 1L, 3L, 1L, 3L, 2L, 1L, 2L, 1L, 1L, 2L, 3L, 3L, &#10;3L, 3L, 3L, 2L, 2L, 3L, 2L, 1L, 1L, 2L, 1L, 2L, 3L, 3L, 1L, 1L, &#10;1L, 3L, 3L, 1L, 1L, 3L, 1L, 2L, 3L, 1L, 1L, 3L, 2L, 1L, 3L, 3L, &#10;3L, 1L, 3L, 1L, 3L, 1L, 3L, 2L, 3L, 2L, 2L, 1L, 2L, 1L, 3L, 2L, &#10;2L, 3L, 1L, 2L, 1L, 3L, 3L, 3L, 3L, 2L, 2L, 3L, 3L, 3L, 1L, 1L, &#10;1L, 1L, 3L, 1L, 3L, 1L, 1L, 1L, 3L, 3L, 2L, 1L, 2L, 3L, 2L, 3L, &#10;2L, 3L, 1L, 3L, 1L, 3L, 1L, 1L, 3L, 1L, 3L, 2L, 3L, 2L, 1L, 1L, &#10;1L, 2L, 2L, 1L, 2L, 1L, 3L, 1L, 2L, 2L, 2L, 1L, 1L, 3L, 3L, 1L, &#10;1L, 3L, 1L, 3L, 3L, 3L, 3L, 2L, 1L, 1L, 3L, 2L, 1L, 3L, 2L, 2L, &#10;1L, 1L, 3L, 3L, 3L, 1L, 1L, 2L, 1L, 1L, 3L, 1L, 2L, 2L, 3L, 1L, &#10;2L, 2L, 3L, 2L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 1L, 2L, 3L, 1L, 3L, &#10;1L, 1L, 3L, 3L, 3L, 2L, 3L, 2L, 2L, 3L, 1L, 2L, 3L, 3L, 2L, 2L, &#10;1L, 2L, 3L, 2L, 1L, 2L, 3L, 3L, 2L, 2L, 2L, 3L, 1L, 2L, 3L, 2L, &#10;3L, 3L, 3L, 3L, 1L, 2L, 3L, 2L, 3L, 3L, 3L, 2L, 3L, 1L, 1L, 2L, &#10;2L, 3L, 3L, 1L, 1L, 3L, 1L, 1L, 1L, 2L, 3L, 2L, 3L, 1L, 2L, 3L, &#10;2L, 2L, 3L, 3L, 3L, 1L, 3L, 2L, 2L, 1L, 2L, 3L, 1L, 3L, 2L, 3L, &#10;1L, 2L, 3L, 2L, 1L, 2L, 3L, 3L, 3L, 3L, 2L, 3L, 2L, 3L, 3L, 2L, &#10;1L, 2L, 3L, 2L, 1L, 2L, 1L, 3L, 1L, 3L, 1L, 1L, 2L, 1L, 3L, 1L, &#10;2L, 2L, 3L, 2L, 2L, 3L, 3L, 1L, 3L, 1L, 3L, 2L, 1L, 3L, 1L, 2L, &#10;1L, 3L, 1L, 3L, 1L, 1L, 2L, 1L, 3L, 2L, 3L, 3L, 3L, 1L, 3L, 3L, &#10;2L, 2L, 1L, 2L, 2L, 2L, 2L, 3L, 1L, 1L, 1L, 2L, 3L, 2L, 3L, 1L, &#10;3L, 3L, 3L, 1L, 3L, 3L, 2L, 2L, 3L, 2L, 2L, 3L, 2L, 1L, 2L, 3L, &#10;2L, 2L, 1L, 3L, 2L), .Label = c(&quot;No norm&quot;, &quot;Norm&quot;, &quot;Provincial&quot;&#10;), class = &quot;factor&quot;), cost = structure(c(1L, 2L, 1L, 1L, &#10;1L, 1L, 1L, 1L, 1L, 2L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 1L, &#10;2L, 1L, 2L, 1L, 1L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 1L, 1L, &#10;2L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 2L, &#10;1L, 2L, 1L, 2L, 1L, 2L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 2L, 1L, 2L, &#10;2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 1L, 2L, 1L, 1L, 1L, &#10;1L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 1L, 1L, 1L, 2L, 2L, 1L, 1L, &#10;2L, 1L, 1L, 2L, 1L, 1L, 1L, 1L, 2L, 1L, 1L, 2L, 2L, 1L, 1L, 1L, &#10;1L, 1L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, &#10;2L, 1L, 1L, 2L, 2L, 2L, 1L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 1L, 1L, &#10;1L, 2L, 2L, 1L, 2L, 2L, 1L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 1L, 2L, &#10;2L, 2L, 2L, 1L, 2L, 1L, 1L, 1L, 2L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, &#10;1L, 1L, 1L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 1L, 2L, 1L, 2L, 1L, 2L, &#10;1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 1L, 2L, 2L, 2L, 1L, 2L, 2L, 1L, &#10;1L, 1L, 2L, 2L, 1L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 1L, 2L, 2L, 1L, &#10;2L, 1L, 1L, 2L, 1L, 2L, 1L, 2L, 2L, 2L, 1L, 2L, 1L, 2L, 1L, 1L, &#10;2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 1L, 2L, 2L, 1L, 2L, 1L, 1L, &#10;1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 1L, 1L, 1L, 1L, 2L, &#10;1L, 1L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 2L, 1L, 1L, 2L, 1L, 2L, 1L, &#10;1L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, &#10;1L, 1L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, &#10;1L, 1L, 2L, 1L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 1L, 1L, 2L, 1L, 1L, &#10;2L, 1L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 1L, 1L, 1L, 2L, &#10;1L, 2L, 1L, 2L, 1L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, &#10;1L, 1L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 2L, 1L, &#10;1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 1L, 2L, 1L, 2L, 2L, &#10;2L, 1L, 1L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 1L, 1L, 1L, 1L, 2L, 2L, &#10;1L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 2L, 1L, 1L, 1L, 2L, 2L, &#10;1L, 1L, 2L, 1L, 1L, 2L, 1L, 1L, 1L, 2L, 1L, 1L, 2L, 2L, 1L, 1L, &#10;2L, 2L, 1L, 1L, 1L, 2L, 2L, 1L, 2L, 1L, 1L, 2L, 1L, 1L, 2L, 2L, &#10;1L, 1L, 2L, 1L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, &#10;2L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 2L, 1L, &#10;1L, 1L, 1L, 2L, 1L, 1L, 1L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, &#10;2L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, &#10;2L, 1L, 1L, 1L, 2L, 1L, 1L, 1L, 2L, 1L, 1L, 2L, 1L, 1L, 2L, 1L, &#10;1L, 2L, 2L, 2L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, &#10;1L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 1L, 2L, 1L, &#10;2L, 1L, 2L, 1L, 1L, 2L, 1L, 1L, 1L, 2L, 2L, 1L, 2L, 1L, 2L, 1L, &#10;1L, 2L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 2L, &#10;2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 1L, 1L, 2L, 1L, 1L, 1L, 1L, &#10;2L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 2L, 1L, &#10;1L, 1L, 2L, 2L, 1L, 1L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, &#10;1L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 1L, &#10;2L, 2L, 2L, 1L, 2L, 2L, 2L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, &#10;1L, 2L, 1L, 2L, 1L, 2L, 2L, 1L), .Label = c(&quot;0&quot;, &quot;1&quot;), class = &quot;factor&quot;), &#10;    rank = structure(c(1L, 2L, 1L, 2L, 2L, 2L, 2L, 1L, 1L, &#10;    2L, 2L, 2L, 2L, 1L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 1L, 1L, &#10;    2L, 1L, 1L, 2L, 2L, 1L, 2L, 1L, 1L, 1L, 1L, 2L, 1L, 1L, 2L, &#10;    1L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, &#10;    1L, 2L, 1L, 1L, 2L, 1L, 2L, 2L, 1L, 1L, 1L, 2L, 2L, 1L, 2L, &#10;    1L, 2L, 1L, 1L, 2L, 1L, 1L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, &#10;    1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, &#10;    1L, 1L, 1L, 2L, 2L, 2L, 1L, 2L, 1L, 2L, 1L, 1L, 1L, 1L, 2L, &#10;    2L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, &#10;    2L, 2L, 1L, 2L, 2L, 2L, 2L, 1L, 2L, 1L, 1L, 1L, 2L, 2L, 2L, &#10;    2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 1L, 2L, 1L, 2L, 2L, 2L, &#10;    2L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, &#10;    2L, 1L, 1L, 1L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, &#10;    2L, 1L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, &#10;    2L, 1L, 2L, 2L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 1L, &#10;    2L, 1L, 1L, 1L, 2L, 2L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 1L, 1L, &#10;    1L, 2L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, &#10;    1L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 1L, 1L, 1L, 2L, 1L, 1L, 1L, &#10;    2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, &#10;    2L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 1L, 2L, 1L, 2L, 2L, 1L, &#10;    1L, 2L, 1L, 2L, 2L, 1L, 2L, 2L, 1L, 2L, 1L, 1L, 1L, 2L, 2L, &#10;    2L, 2L, 1L, 2L, 2L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 1L, 2L, 1L, &#10;    1L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, &#10;    1L, 2L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, &#10;    2L, 2L, 1L, 2L, 2L, 1L, 2L, 1L, 1L, 2L, 1L, 2L, 2L, 1L, 1L, &#10;    1L, 1L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 1L, 2L, 2L, 1L, &#10;    1L, 2L, 2L, 1L, 2L, 1L, 2L, 2L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, &#10;    2L, 1L, 1L, 2L, 1L, 2L, 2L, 1L, 2L, 2L, 1L, 2L, 1L, 2L, 1L, &#10;    2L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 1L, 2L, 1L, 1L, 1L, &#10;    2L, 1L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 1L, 1L, &#10;    1L, 2L, 2L, 2L, 2L, 1L, 2L, 1L, 1L, 1L, 1L, 2L, 2L, 1L, 2L, &#10;    2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 2L, 1L, 2L, 2L, &#10;    2L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 2L, 1L, 1L, 1L, &#10;    2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, &#10;    1L, 2L, 1L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 1L, 1L, &#10;    1L, 2L, 1L, 2L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 2L, 1L, &#10;    1L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 1L, 2L, 1L, 1L, 2L, 2L, 1L, &#10;    1L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 1L, 2L, &#10;    2L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 2L, 2L, 1L, 2L, &#10;    1L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 1L, 2L, &#10;    1L, 2L, 2L, 2L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 1L, 1L, 2L, 1L, &#10;    2L, 1L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 1L, 2L, 1L, 1L, 1L, 1L, &#10;    2L, 1L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, &#10;    1L, 2L, 1L, 1L, 1L, 2L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, &#10;    1L, 1L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 1L, 2L, 1L, &#10;    2L, 2L, 1L, 2L, 2L, 2L, 1L, 2L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, &#10;    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 1L, 1L, 1L, 1L, &#10;    1L), .Label = c(&quot;0&quot;, &quot;1&quot;), class = &quot;factor&quot;)), .Names = c(&quot;time&quot;, &#10;&quot;choice&quot;, &quot;msg&quot;, &quot;cost&quot;, &quot;rank&quot;), row.names = c(&quot;1.1&quot;, &#10;&quot;2.1&quot;, &quot;3.1&quot;, &quot;4.1&quot;, &quot;5.1&quot;, &quot;6.1&quot;, &quot;7.1&quot;, &quot;8.1&quot;, &quot;9.1&quot;, &quot;10.1&quot;, &#10;&quot;11.1&quot;, &quot;12.1&quot;, &quot;13.1&quot;, &quot;14.1&quot;, &quot;15.1&quot;, &quot;16.1&quot;, &quot;17.1&quot;, &quot;18.1&quot;, &#10;&quot;19.1&quot;, &quot;20.1&quot;, &quot;21.1&quot;, &quot;22.1&quot;, &quot;23.1&quot;, &quot;24.1&quot;, &quot;25.1&quot;, &quot;26.1&quot;, &#10;&quot;27.1&quot;, &quot;28.1&quot;, &quot;29.1&quot;, &quot;30.1&quot;, &quot;31.1&quot;, &quot;32.1&quot;, &quot;33.1&quot;, &quot;34.1&quot;, &#10;&quot;35.1&quot;, &quot;36.1&quot;, &quot;37.1&quot;, &quot;38.1&quot;, &quot;39.1&quot;, &quot;40.1&quot;, &quot;41.1&quot;, &quot;42.1&quot;, &#10;&quot;43.1&quot;, &quot;44.1&quot;, &quot;45.1&quot;, &quot;46.1&quot;, &quot;47.1&quot;, &quot;48.1&quot;, &quot;49.1&quot;, &quot;50.1&quot;, &#10;&quot;51.1&quot;, &quot;52.1&quot;, &quot;53.1&quot;, &quot;54.1&quot;, &quot;55.1&quot;, &quot;56.1&quot;, &quot;57.1&quot;, &quot;58.1&quot;, &#10;&quot;59.1&quot;, &quot;60.1&quot;, &quot;61.1&quot;, &quot;62.1&quot;, &quot;63.1&quot;, &quot;64.1&quot;, &quot;65.1&quot;, &quot;66.1&quot;, &#10;&quot;67.1&quot;, &quot;68.1&quot;, &quot;69.1&quot;, &quot;70.1&quot;, &quot;71.1&quot;, &quot;72.1&quot;, &quot;73.1&quot;, &quot;74.1&quot;, &#10;&quot;75.1&quot;, &quot;76.1&quot;, &quot;77.1&quot;, &quot;78.1&quot;, &quot;79.1&quot;, &quot;80.1&quot;, &quot;81.1&quot;, &quot;82.1&quot;, &#10;&quot;83.1&quot;, &quot;84.1&quot;, &quot;85.1&quot;, &quot;86.1&quot;, &quot;87.1&quot;, &quot;88.1&quot;, &quot;89.1&quot;, &quot;90.1&quot;, &#10;&quot;91.1&quot;, &quot;92.1&quot;, &quot;93.1&quot;, &quot;94.1&quot;, &quot;95.1&quot;, &quot;96.1&quot;, &quot;97.1&quot;, &quot;98.1&quot;, &#10;&quot;99.1&quot;, &quot;100.1&quot;, &quot;101.1&quot;, &quot;102.1&quot;, &quot;103.1&quot;, &quot;104.1&quot;, &quot;105.1&quot;, &#10;&quot;106.1&quot;, &quot;107.1&quot;, &quot;108.1&quot;, &quot;109.1&quot;, &quot;110.1&quot;, &quot;111.1&quot;, &quot;112.1&quot;, &#10;&quot;113.1&quot;, &quot;114.1&quot;, &quot;115.1&quot;, &quot;116.1&quot;, &quot;117.1&quot;, &quot;118.1&quot;, &quot;119.1&quot;, &#10;&quot;120.1&quot;, &quot;121.1&quot;, &quot;122.1&quot;, &quot;123.1&quot;, &quot;124.1&quot;, &quot;125.1&quot;, &quot;126.1&quot;, &#10;&quot;127.1&quot;, &quot;128.1&quot;, &quot;129.1&quot;, &quot;130.1&quot;, &quot;131.1&quot;, &quot;132.1&quot;, &quot;133.1&quot;, &#10;&quot;134.1&quot;, &quot;135.1&quot;, &quot;136.1&quot;, &quot;137.1&quot;, &quot;138.1&quot;, &quot;139.1&quot;, &quot;140.1&quot;, &#10;&quot;141.1&quot;, &quot;142.1&quot;, &quot;143.1&quot;, &quot;144.1&quot;, &quot;145.1&quot;, &quot;146.1&quot;, &quot;147.1&quot;, &#10;&quot;148.1&quot;, &quot;149.1&quot;, &quot;150.1&quot;, &quot;151.1&quot;, &quot;152.1&quot;, &quot;153.1&quot;, &quot;154.1&quot;, &#10;&quot;155.1&quot;, &quot;156.1&quot;, &quot;157.1&quot;, &quot;158.1&quot;, &quot;159.1&quot;, &quot;160.1&quot;, &quot;161.1&quot;, &#10;&quot;162.1&quot;, &quot;163.1&quot;, &quot;164.1&quot;, &quot;165.1&quot;, &quot;166.1&quot;, &quot;167.1&quot;, &quot;168.1&quot;, &#10;&quot;169.1&quot;, &quot;170.1&quot;, &quot;171.1&quot;, &quot;172.1&quot;, &quot;173.1&quot;, &quot;174.1&quot;, &quot;175.1&quot;, &#10;&quot;176.1&quot;, &quot;177.1&quot;, &quot;178.1&quot;, &quot;179.1&quot;, &quot;180.1&quot;, &quot;181.1&quot;, &quot;182.1&quot;, &#10;&quot;183.1&quot;, &quot;184.1&quot;, &quot;185.1&quot;, &quot;186.1&quot;, &quot;187.1&quot;, &quot;188.1&quot;, &quot;189.1&quot;, &#10;&quot;190.1&quot;, &quot;191.1&quot;, &quot;192.1&quot;, &quot;193.1&quot;, &quot;194.1&quot;, &quot;195.1&quot;, &quot;196.1&quot;, &#10;&quot;197.1&quot;, &quot;198.1&quot;, &quot;199.1&quot;, &quot;200.1&quot;, &quot;201.1&quot;, &quot;202.1&quot;, &quot;203.1&quot;, &#10;&quot;204.1&quot;, &quot;205.1&quot;, &quot;206.1&quot;, &quot;207.1&quot;, &quot;208.1&quot;, &quot;209.1&quot;, &quot;210.1&quot;, &#10;&quot;211.1&quot;, &quot;212.1&quot;, &quot;213.1&quot;, &quot;214.1&quot;, &quot;215.1&quot;, &quot;216.1&quot;, &quot;217.1&quot;, &#10;&quot;218.1&quot;, &quot;219.1&quot;, &quot;220.1&quot;, &quot;221.1&quot;, &quot;222.1&quot;, &quot;223.1&quot;, &quot;224.1&quot;, &#10;&quot;225.1&quot;, &quot;226.1&quot;, &quot;227.1&quot;, &quot;228.1&quot;, &quot;229.1&quot;, &quot;230.1&quot;, &quot;231.1&quot;, &#10;&quot;232.1&quot;, &quot;233.1&quot;, &quot;234.1&quot;, &quot;235.1&quot;, &quot;236.1&quot;, &quot;237.1&quot;, &quot;238.1&quot;, &#10;&quot;239.1&quot;, &quot;240.1&quot;, &quot;241.1&quot;, &quot;242.1&quot;, &quot;243.1&quot;, &quot;244.1&quot;, &quot;245.1&quot;, &#10;&quot;246.1&quot;, &quot;247.1&quot;, &quot;248.1&quot;, &quot;249.1&quot;, &quot;250.1&quot;, &quot;251.1&quot;, &quot;252.1&quot;, &#10;&quot;253.1&quot;, &quot;254.1&quot;, &quot;255.1&quot;, &quot;256.1&quot;, &quot;257.1&quot;, &quot;258.1&quot;, &quot;259.1&quot;, &#10;&quot;260.1&quot;, &quot;261.1&quot;, &quot;262.1&quot;, &quot;263.1&quot;, &quot;264.1&quot;, &quot;265.1&quot;, &quot;266.1&quot;, &#10;&quot;267.1&quot;, &quot;268.1&quot;, &quot;269.1&quot;, &quot;270.1&quot;, &quot;271.1&quot;, &quot;272.1&quot;, &quot;273.1&quot;, &#10;&quot;274.1&quot;, &quot;275.1&quot;, &quot;276.1&quot;, &quot;277.1&quot;, &quot;278.1&quot;, &quot;279.1&quot;, &quot;280.1&quot;, &#10;&quot;281.1&quot;, &quot;282.1&quot;, &quot;283.1&quot;, &quot;284.1&quot;, &quot;285.1&quot;, &quot;286.1&quot;, &quot;287.1&quot;, &#10;&quot;288.1&quot;, &quot;289.1&quot;, &quot;290.1&quot;, &quot;291.1&quot;, &quot;292.1&quot;, &quot;293.1&quot;, &quot;294.1&quot;, &#10;&quot;295.1&quot;, &quot;296.1&quot;, &quot;297.1&quot;, &quot;298.1&quot;, &quot;299.1&quot;, &quot;300.1&quot;, &quot;301.1&quot;, &#10;&quot;302.1&quot;, &quot;303.1&quot;, &quot;304.1&quot;, &quot;305.1&quot;, &quot;306.1&quot;, &quot;307.1&quot;, &quot;308.1&quot;, &#10;&quot;309.1&quot;, &quot;310.1&quot;, &quot;311.1&quot;, &quot;312.1&quot;, &quot;313.1&quot;, &quot;314.1&quot;, &quot;315.1&quot;, &#10;&quot;316.1&quot;, &quot;317.1&quot;, &quot;318.1&quot;, &quot;319.1&quot;, &quot;320.1&quot;, &quot;321.1&quot;, &quot;322.1&quot;, &#10;&quot;323.1&quot;, &quot;324.1&quot;, &quot;325.1&quot;, &quot;326.1&quot;, &quot;327.1&quot;, &quot;328.1&quot;, &quot;329.1&quot;, &#10;&quot;330.1&quot;, &quot;331.1&quot;, &quot;332.1&quot;, &quot;333.1&quot;, &quot;334.1&quot;, &quot;335.1&quot;, &quot;336.1&quot;, &#10;&quot;337.1&quot;, &quot;338.1&quot;, &quot;339.1&quot;, &quot;340.1&quot;, &quot;341.1&quot;, &quot;342.1&quot;, &quot;343.1&quot;, &#10;&quot;344.1&quot;, &quot;345.1&quot;, &quot;346.1&quot;, &quot;347.1&quot;, &quot;348.1&quot;, &quot;349.1&quot;, &quot;350.1&quot;, &#10;&quot;351.1&quot;, &quot;352.1&quot;, &quot;353.1&quot;, &quot;354.1&quot;, &quot;355.1&quot;, &quot;356.1&quot;, &quot;357.1&quot;, &#10;&quot;358.1&quot;, &quot;359.1&quot;, &quot;360.1&quot;, &quot;361.1&quot;, &quot;362.1&quot;, &quot;363.1&quot;, &quot;364.1&quot;, &#10;&quot;365.1&quot;, &quot;366.1&quot;, &quot;367.1&quot;, &quot;368.1&quot;, &quot;369.1&quot;, &quot;370.1&quot;, &quot;371.1&quot;, &#10;&quot;372.1&quot;, &quot;373.1&quot;, &quot;374.1&quot;, &quot;375.1&quot;, &quot;376.1&quot;, &quot;377.1&quot;, &quot;378.1&quot;, &#10;&quot;379.1&quot;, &quot;380.1&quot;, &quot;381.1&quot;, &quot;382.1&quot;, &quot;383.1&quot;, &quot;384.1&quot;, &quot;385.1&quot;, &#10;&quot;386.1&quot;, &quot;387.1&quot;, &quot;388.1&quot;, &quot;389.1&quot;, &quot;390.1&quot;, &quot;391.1&quot;, &quot;392.1&quot;, &#10;&quot;393.1&quot;, &quot;394.1&quot;, &quot;395.1&quot;, &quot;396.1&quot;, &quot;397.1&quot;, &quot;398.1&quot;, &quot;399.1&quot;, &#10;&quot;400.1&quot;, &quot;401.1&quot;, &quot;402.1&quot;, &quot;403.1&quot;, &quot;404.1&quot;, &quot;405.1&quot;, &quot;406.1&quot;, &#10;&quot;407.1&quot;, &quot;408.1&quot;, &quot;409.1&quot;, &quot;410.1&quot;, &quot;411.1&quot;, &quot;412.1&quot;, &quot;413.1&quot;, &#10;&quot;414.1&quot;, &quot;415.1&quot;, &quot;416.1&quot;, &quot;417.1&quot;, &quot;418.1&quot;, &quot;419.1&quot;, &quot;420.1&quot;, &#10;&quot;421.1&quot;, &quot;422.1&quot;, &quot;423.1&quot;, &quot;424.1&quot;, &quot;425.1&quot;, &quot;426.1&quot;, &quot;427.1&quot;, &#10;&quot;428.1&quot;, &quot;429.1&quot;, &quot;430.1&quot;, &quot;431.1&quot;, &quot;432.1&quot;, &quot;433.1&quot;, &quot;434.1&quot;, &#10;&quot;435.1&quot;, &quot;436.1&quot;, &quot;437.1&quot;, &quot;438.1&quot;, &quot;439.1&quot;, &quot;440.1&quot;, &quot;441.1&quot;, &#10;&quot;442.1&quot;, &quot;443.1&quot;, &quot;444.1&quot;, &quot;445.1&quot;, &quot;446.1&quot;, &quot;447.1&quot;, &quot;448.1&quot;, &#10;&quot;449.1&quot;, &quot;450.1&quot;, &quot;451.1&quot;, &quot;452.1&quot;, &quot;453.1&quot;, &quot;454.1&quot;, &quot;455.1&quot;, &#10;&quot;456.1&quot;, &quot;457.1&quot;, &quot;458.1&quot;, &quot;459.1&quot;, &quot;460.1&quot;, &quot;461.1&quot;, &quot;462.1&quot;, &#10;&quot;463.1&quot;, &quot;464.1&quot;, &quot;465.1&quot;, &quot;466.1&quot;, &quot;467.1&quot;, &quot;468.1&quot;, &quot;469.1&quot;, &#10;&quot;470.1&quot;, &quot;471.1&quot;, &quot;472.1&quot;, &quot;473.1&quot;, &quot;474.1&quot;, &quot;475.1&quot;, &quot;476.1&quot;, &#10;&quot;477.1&quot;, &quot;478.1&quot;, &quot;479.1&quot;, &quot;480.1&quot;, &quot;481.1&quot;, &quot;482.1&quot;, &quot;483.1&quot;, &#10;&quot;484.1&quot;, &quot;485.1&quot;, &quot;486.1&quot;, &quot;487.1&quot;, &quot;488.1&quot;, &quot;489.1&quot;, &quot;490.1&quot;, &#10;&quot;491.1&quot;, &quot;492.1&quot;, &quot;493.1&quot;, &quot;494.1&quot;, &quot;495.1&quot;, &quot;496.1&quot;, &quot;497.1&quot;, &#10;&quot;498.1&quot;, &quot;499.1&quot;, &quot;500.1&quot;, &quot;501.1&quot;, &quot;502.1&quot;, &quot;503.1&quot;, &quot;504.1&quot;, &#10;&quot;505.1&quot;, &quot;506.1&quot;, &quot;507.1&quot;, &quot;508.1&quot;, &quot;509.1&quot;, &quot;510.1&quot;, &quot;511.1&quot;, &#10;&quot;512.1&quot;, &quot;513.1&quot;, &quot;514.1&quot;, &quot;515.1&quot;, &quot;516.1&quot;, &quot;517.1&quot;, &quot;518.1&quot;, &#10;&quot;519.1&quot;, &quot;520.1&quot;, &quot;521.1&quot;, &quot;522.1&quot;, &quot;523.1&quot;, &quot;524.1&quot;, &quot;525.1&quot;, &#10;&quot;526.1&quot;, &quot;527.1&quot;, &quot;528.1&quot;, &quot;529.1&quot;, &quot;530.1&quot;, &quot;531.1&quot;, &quot;532.1&quot;, &#10;&quot;533.1&quot;, &quot;534.1&quot;, &quot;535.1&quot;, &quot;536.1&quot;, &quot;537.1&quot;, &quot;538.1&quot;, &quot;539.1&quot;, &#10;&quot;540.1&quot;, &quot;541.1&quot;, &quot;542.1&quot;, &quot;543.1&quot;, &quot;544.1&quot;, &quot;545.1&quot;, &quot;546.1&quot;, &#10;&quot;547.1&quot;, &quot;548.1&quot;, &quot;549.1&quot;, &quot;550.1&quot;, &quot;551.1&quot;, &quot;552.1&quot;, &quot;553.1&quot;, &#10;&quot;554.1&quot;, &quot;555.1&quot;, &quot;556.1&quot;, &quot;557.1&quot;, &quot;558.1&quot;, &quot;559.1&quot;, &quot;560.1&quot;, &#10;&quot;561.1&quot;, &quot;562.1&quot;, &quot;563.1&quot;, &quot;564.1&quot;, &quot;565.1&quot;, &quot;566.1&quot;, &quot;567.1&quot;, &#10;&quot;568.1&quot;, &quot;569.1&quot;, &quot;570.1&quot;, &quot;571.1&quot;, &quot;572.1&quot;, &quot;573.1&quot;, &quot;574.1&quot;, &#10;&quot;575.1&quot;, &quot;576.1&quot;, &quot;577.1&quot;, &quot;578.1&quot;, &quot;579.1&quot;, &quot;580.1&quot;, &quot;581.1&quot;, &#10;&quot;582.1&quot;, &quot;583.1&quot;, &quot;584.1&quot;, &quot;585.1&quot;, &quot;586.1&quot;, &quot;587.1&quot;, &quot;588.1&quot;, &#10;&quot;589.1&quot;, &quot;590.1&quot;, &quot;591.1&quot;, &quot;592.1&quot;, &quot;593.1&quot;, &quot;594.1&quot;, &quot;595.1&quot;, &#10;&quot;596.1&quot;, &quot;597.1&quot;, &quot;598.1&quot;, &quot;599.1&quot;, &quot;600.1&quot;, &quot;601.1&quot;, &quot;602.1&quot;, &#10;&quot;603.1&quot;, &quot;604.1&quot;, &quot;605.1&quot;, &quot;606.1&quot;, &quot;607.1&quot;, &quot;608.1&quot;, &quot;609.1&quot;, &#10;&quot;610.1&quot;, &quot;611.1&quot;, &quot;612.1&quot;, &quot;613.1&quot;, &quot;614.1&quot;, &quot;615.1&quot;, &quot;616.1&quot;, &#10;&quot;617.1&quot;, &quot;618.1&quot;, &quot;619.1&quot;, &quot;620.1&quot;, &quot;621.1&quot;, &quot;622.1&quot;, &quot;623.1&quot;, &#10;&quot;624.1&quot;, &quot;625.1&quot;, &quot;626.1&quot;, &quot;627.1&quot;, &quot;628.1&quot;, &quot;629.1&quot;, &quot;630.1&quot;, &#10;&quot;631.1&quot;, &quot;632.1&quot;, &quot;633.1&quot;, &quot;634.1&quot;, &quot;635.1&quot;, &quot;636.1&quot;, &quot;637.1&quot;, &#10;&quot;638.1&quot;, &quot;639.1&quot;, &quot;640.1&quot;, &quot;641.1&quot;, &quot;642.1&quot;, &quot;643.1&quot;, &quot;644.1&quot;, &#10;&quot;645.1&quot;, &quot;646.1&quot;, &quot;647.1&quot;, &quot;648.1&quot;, &quot;649.1&quot;, &quot;650.1&quot;, &quot;651.1&quot;, &#10;&quot;652.1&quot;, &quot;653.1&quot;, &quot;654.1&quot;, &quot;655.1&quot;, &quot;656.1&quot;, &quot;657.1&quot;, &quot;658.1&quot;, &#10;&quot;659.1&quot;, &quot;660.1&quot;, &quot;661.1&quot;, &quot;662.1&quot;, &quot;663.1&quot;, &quot;664.1&quot;, &quot;665.1&quot;, &#10;&quot;666.1&quot;, &quot;667.1&quot;, &quot;668.1&quot;, &quot;669.1&quot;, &quot;670.1&quot;, &quot;671.1&quot;, &quot;672.1&quot;, &#10;&quot;673.1&quot;, &quot;674.1&quot;, &quot;675.1&quot;, &quot;676.1&quot;, &quot;677.1&quot;, &quot;678.1&quot;, &quot;679.1&quot;, &#10;&quot;680.1&quot;, &quot;681.1&quot;, &quot;682.1&quot;, &quot;683.1&quot;, &quot;684.1&quot;, &quot;685.1&quot;, &quot;686.1&quot;, &#10;&quot;687.1&quot;, &quot;688.1&quot;, &quot;689.1&quot;, &quot;690.1&quot;, &quot;691.1&quot;, &quot;692.1&quot;, &quot;693.1&quot;, &#10;&quot;694.1&quot;, &quot;695.1&quot;, &quot;696.1&quot;, &quot;697.1&quot;, &quot;698.1&quot;, &quot;699.1&quot;, &quot;700.1&quot;&#10;), class = &quot;data.frame&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="6" CreationDate="2013-05-03T12:28:13.450" FavoriteCount="1" Id="58020" LastActivityDate="2013-05-05T16:55:14.697" OwnerUserId="12551" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;repeated-measures&gt;&lt;experiment-design&gt;&lt;multilevel-analysis&gt;" Title="Multilevel modeling of experimental data with repeated measures in the same condition of factorial experiment" ViewCount="241" />
  
  <row Body="&lt;p&gt;More information would help a lot, but there is a simple tentative answer from what you have told us. &lt;/p&gt;&#10;&#10;&lt;p&gt;In principle a ratio of lengths must be positive and is likely to be skew in distribution, but its logarithm will usually be (much?) more nearly symmetric in distribution. &lt;/p&gt;&#10;&#10;&lt;p&gt;PCA is just a transformation procedure, but putting in data that are approximately symmetric in distribution will often make patterns (much?) easier to see. &lt;/p&gt;&#10;&#10;&lt;p&gt;There is no difference statistically between log10 and ln. Go with what's easier to explain to your audience: do zoologists know about natural logarithms? &lt;/p&gt;&#10;&#10;&lt;p&gt;Angular transformation applies only to bounded measurements. If it's necessarily true that a length ratio must be within (0,1) -- i.e. it is anatomically inevitable that one bone or whatever must be shorter than another -- then angular is possible, but it may be that logit is better. &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-05-03T17:16:59.407" Id="58043" LastActivityDate="2013-05-03T17:16:59.407" OwnerUserId="22047" ParentId="58041" PostTypeId="2" Score="3" />
  <row AnswerCount="1" Body="&lt;p&gt;From Wald. Likelihood ratio, and lagrange multiplier tests in econometrics by Robert f. Engle:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;(Tests whose) size does not depend upon the particular value of $\theta \in$ null $\Theta_0$ are called similar tests. &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I don't quite understand the definition. Isn't the size of a test the sup of false positive rate over null $\Theta_0$? So it doesn't depend on a single $\theta \in \Theta_0$, but on $\Theta_0$?&lt;/p&gt;&#10;&#10;&lt;p&gt;Following provides more context from the source. Thanks and regards!&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/tc21f.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-05-03T19:18:55.363" Id="58053" LastActivityDate="2014-11-17T18:05:26.427" OwnerUserId="1005" PostTypeId="1" Score="2" Tags="&lt;hypothesis-testing&gt;" Title="What is a similar test?" ViewCount="147" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I am trying to see which distribution will best fit the data I am working on. The dataset is as following:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Site          Nausea      headache        Abdominal Distension&#10;1              17              5                   10&#10;2              12              8                   7&#10;.....&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So each site has the total # adverse events for each type/category and have equal # patients per site, say 60. If I were to analyse the data for multiple outcomes per site, the number of events per category given the category response rates can be assumed to be independently distributed. They can be modeled by a multinomial distribution with parameters $n=60$ and category response rates $p_{i1}, \ldots , p_{iC}$.The variability in the vectors of response counts is often higher than can be accommodated by the multinomial distribution. Therefore, individual variation in category response rates can be modeled by a Dirichlet distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;Just wondering if I am thinking through this correctly.&#10;If so, could someone share some thoughts on how this could be done in R?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-03T19:55:03.380" Id="58060" LastActivityDate="2015-03-05T17:03:24.933" LastEditDate="2014-01-08T23:00:30.747" LastEditorUserId="8013" OwnerUserId="12938" PostTypeId="1" Score="3" Tags="&lt;dirichlet-distribution&gt;" Title="Dirichlet multinomial for adverse event data" ViewCount="107" />
  <row Body="&lt;p&gt;Excel has a Microsoft provided Descriptive Statistics Tool in the &lt;strong&gt;Data Analysis Add-in&lt;/strong&gt;.  You'll need to activate it in your add-ins (varies based upon your version of Excel and you didn't mark which you use).&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's a link loading the Analysis Toolpak for: &lt;a href=&quot;http://office.microsoft.com/en-us/excel-help/load-the-analysis-toolpak-HP001127724.aspx&quot; rel=&quot;nofollow&quot;&gt;Excel 2003&lt;/a&gt;, &lt;a href=&quot;http://office.microsoft.com/en-us/excel-help/load-the-analysis-toolpak-HP010021569.aspx&quot; rel=&quot;nofollow&quot;&gt;Excel 2007&lt;/a&gt;, or &lt;a href=&quot;http://technet.microsoft.com/en-us/magazine/ff969363.aspx&quot; rel=&quot;nofollow&quot;&gt;Excel 2010&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In Excel 2010, once you've added it, you'll find Data Analysis in the Data Tab of the Ribbon.  Among the tools they provide is &lt;strong&gt;Descriptive Statistics&lt;/strong&gt;, which for any given range you provide will give you: &lt;em&gt;Mean, &#10;Standard Error,&#10;Median,&#10;Mode,&#10;Standard Deviation,&#10;Sample Variance,&#10;Kurtosis,&#10;Skewness,&#10;Range,&#10;Minimum,&#10;Maximum,&#10;Sum,&#10;Count&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;There's also quite a few other tools for you if you prefer Excel to a more specialized tool like R.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-03T20:17:44.590" Id="58063" LastActivityDate="2013-05-03T20:17:44.590" OwnerUserId="10217" ParentId="57991" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;For 1, for simplicity, just consider variable 5, difference.  For the other two, you would do the same thing.&lt;/p&gt;&#10;&#10;&lt;p&gt;Calculate the correlation (within each subject across elements) between centrality and difference.  That gives you one measure per subject (the correlation).  You can use whatever flavor of correlation floats your boat, but probably rank correlation is the right thing here.  Then take the mean correlation across subjects but within treatment/control.  You now have mean correlation in treatment and mean correlation in control.  If the mean is bigger for treatment, that says that people who are primed have a greater tendency to to assign higher difference to elements for which they assign higher centrality (and vice versa of course)&lt;/p&gt;&#10;&#10;&lt;p&gt;Now you just do a t-test for the difference in means between treatment and control.  If you are the kind of guy who gets tense about the fact that the data you are putting into the t-test is not normally distributed (you shouldn't be that kind of guy, but some people just are that kind of guy), then you will not like the t-test here.  Correlations have to be between -1 and 1, so they sure ain't normal.&lt;/p&gt;&#10;&#10;&lt;p&gt;But, there are other tests.  I like the Wilcoxon rank-sum test (also called Mann-Whitney U test), for example.  So, use that if you're that kind of guy.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-05-03T21:32:32.453" Id="58072" LastActivityDate="2013-05-03T21:32:32.453" OwnerUserId="25212" ParentId="57910" PostTypeId="2" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;I need to run an extended Cox model with a time-varying covariate in R: let’s call it the &quot;number of doses&quot; (&lt;code&gt;X&lt;/code&gt;).  I am interested in the hazard ratio associated with &lt;strong&gt;each level&lt;/strong&gt; of &lt;code&gt;X&lt;/code&gt;, ie. how an additional dose affects the likelihood of recovery. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;X&lt;/code&gt; is a discrete, positive integer. It can either remain constant or increase by 1 during each observation time.&#10;&lt;code&gt;status=1&lt;/code&gt; represents recovery and &lt;code&gt;d1&lt;/code&gt;,&lt;code&gt;d2&lt;/code&gt;,...&lt;code&gt;d6&lt;/code&gt; are the number of doses received (created for modeling purposes)&lt;/p&gt;&#10;&#10;&lt;p&gt;I am not sure how to set up the data. I’ve come up with two alternatives based on my reading and am attaching an excerpt of the data set-up and the code for each method, however I’m not sure which (if either) is correct for answering this question? They give fairly different results.&lt;/p&gt;&#10;&#10;&lt;p&gt;The data has been artificially parsed at each observation time (&lt;code&gt;stop&lt;/code&gt;):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    patient     start      stop doses d1 d2 d3 d4 d5 d6 status&#10;          1  0.000000  0.034000     1  1  0  0  0  0  0      0&#10;          1  0.034000 12.949399     2  1  1  0  0  0  0      0&#10;          1 12.949399 13.813242     3  1  1  1  0  0  0      0&#10;          1 13.813242 30.835070     4  1  1  1  1  0  0      0&#10;          2  0.000000  0.240000     1  1  0  0  0  0  0      0&#10;          2  0.240000  2.984179     2  1  1  0  0  0  0      0&#10;          2  2.984179  4.014723     3  1  1  1  0  0  0      0&#10;          2  4.014723  5.186506     4  1  1  1  1  0  0      0&#10;          2 20.869955 29.832999     4  1  1  1  1  0  0      0&#10;          2 29.832999 32.063887     5  1  1  1  1  1  0      0&#10;          2 32.063887 37.924743     6  1  1  1  1  1  1      1&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;METHOD 1: &lt;strong&gt;treat the number of doses as a factor&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    dt&amp;lt;-read.csv('data.csv',header=T)&#10;    surv&amp;lt;-Surv(start,stop,status,data=dt)&#10;    m&amp;lt;-coxph(surv ~ factor(doses),data=dt)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;METHOD 2: &lt;strong&gt;treat each dose as a binary variable&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    dt&amp;lt;-read.csv('data.csv',header=T)&#10;    surv&amp;lt;-Surv(start,stop,status,data=dt)&#10;    m&amp;lt;-coxph(surv ~ d1+d2+d3+d4+d5+d6, data=dt)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Does either method take into account that a patient had (&lt;code&gt;n-1&lt;/code&gt;) doses in the previous period?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-05-04T02:34:33.583" FavoriteCount="0" Id="58079" LastActivityDate="2014-01-19T02:49:59.787" LastEditDate="2014-01-19T02:49:59.787" LastEditorUserId="35842" OwnerUserId="25223" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;survival&gt;&lt;cox-model&gt;&lt;time-varying-covariate&gt;" Title="Extended Cox model with continuous time dependent covariate - how to structure data?" ViewCount="614" />
  
  
  <row Body="&lt;p&gt;There is simply no single “mathematically” correct way to derive a grade from several variables or dimensions. Without more information on the characteristics of your variables and what you are trying to achieve, it's difficult to provide any useful advice.&lt;/p&gt;&#10;&#10;&lt;p&gt;One important thing to note is that dividing by the maximum value is almost certainly a bad idea because it is very sensitive to outliers. Consider the following data set:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Energy      Load      Quality&#10;     3         1            1&#10;     1         3            1&#10;     1         1            3&#10;     1         1          100&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Intuitively, if you think that the three variables are expressed on the same scale and are equally good measures of whatever you are after, you would want the first three entries to have the same overall grade and the fourth one to be either much larger or flagged as a measurement error but in fact the third one will be noticeably smaller and the fourth one only slightly higher than the third or second. Apart from the fourth entry, differences in quality have almost no influence on the overall grade because of the one entry with an extremely large value. That's probably not a desirable feature.&lt;/p&gt;&#10;&#10;&lt;p&gt;The answers to &lt;a href=&quot;http://stats.stackexchange.com/questions/1112/how-to-represent-an-unbounded-variable-as-number-between-0-and-1&quot;&gt;How to represent an unbounded variable as number between 0 and 1&lt;/a&gt; also contain a lot of useful information regarding this problem (bringing whatever grade you come up with to the 0-1 range).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-05-04T10:12:25.377" Id="58091" LastActivityDate="2013-05-04T10:31:25.363" LastEditDate="2013-05-04T10:31:25.363" LastEditorUserId="6029" OwnerUserId="6029" ParentId="58089" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Since k-means is a &lt;em&gt;randomized&lt;/em&gt; algorithm, you may end up plotting differences in random initialization instead of changes in your data.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would recommend to look at deterministic algorithms. There are much more clever algorithms around than k-means these days...&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-05-04T10:17:07.623" Id="58093" LastActivityDate="2013-05-04T10:17:07.623" OwnerUserId="7828" ParentId="58071" PostTypeId="2" Score="1" />
  <row AnswerCount="2" Body="&lt;p&gt;I have designed a survey with multiple choice answers. Each question contains same set of answers &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Strongly agree &lt;/li&gt;&#10;&lt;li&gt;Agree &lt;/li&gt;&#10;&lt;li&gt;Disagree &lt;/li&gt;&#10;&lt;li&gt;Strongly disagree&lt;/li&gt;&#10;&lt;li&gt;Don't know&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;There are 25 questions and questions are grouped into 5 areas. I need to analyze each area.&lt;/p&gt;&#10;&#10;&lt;p&gt;What is the best approach to analyze the survey results which I collected from a number of people? Is it a good method to calculate the mean response from each area as I need area-wise analysis of questions? Can any one suggest a best option? I am not a statistician&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-05-04T10:24:24.123" Id="58094" LastActivityDate="2013-05-08T09:39:55.580" LastEditDate="2013-05-05T06:57:26.997" LastEditorUserId="25231" OwnerUserId="25231" PostTypeId="1" Score="1" Tags="&lt;categorical-data&gt;&lt;survey&gt;&lt;likert&gt;" Title="Best method to analyse survey results with multiple choice of answers" ViewCount="1970" />
  <row Body="&lt;p&gt;I see no need to answer the general question, as I think your post just arises from a misunderstanding of logarithms. &lt;/p&gt;&#10;&#10;&lt;p&gt;The logarithm of any positive number to any base is negative when that number is below 1. log10(0.01) is -2, for example. By eye the smallest logarithm on the graph is about -1.7, which would be about 0.02 metres. &lt;/p&gt;&#10;&#10;&lt;p&gt;I am not impressed by the graph. It is easy enough in decent software to use a log scale and also show values in the original units. As your question shows, that will be clearer to many statistics users. &lt;/p&gt;&#10;&#10;&lt;p&gt;[LATER] Matt Krause seems very likely to be right about miles, not metres. What meaning is there in saying that agriculture is 20 mm away? I suppose you could be standing at the farm gate.... &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-05-04T13:20:16.007" Id="58104" LastActivityDate="2013-05-04T13:25:33.230" LastEditDate="2013-05-04T13:25:33.230" LastEditorUserId="22047" OwnerUserId="22047" ParentId="58100" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;This is probably not the way most people would solve this, but I like it because it is completely general, not distribution specific, and basically a one-liner. In particular, if $s_1$ denotes the sample sum, then you seek $E[(\frac {s_1}{n} - p)^3]$, which is just the 1st Raw Moment of $(\frac {s_1}{n} - p)^3$:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://www.tri.org.au/se/rawmomenttorawBernoulli.png&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;where &lt;code&gt;RawMomentToRaw&lt;/code&gt; is a mathStatica function, and the $\mu_i$'s denote the raw moments of whatever distribution you happen to be working with (provided, of course, they exist). In the case of the Bernoulli, it happens that all the positive raw moments are $p$, so it simplifies to:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://www.tri.org.au/se/Bernoullisol.png&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-04T16:39:33.637" Id="58116" LastActivityDate="2013-05-04T16:39:33.637" OwnerUserId="24905" ParentId="58111" PostTypeId="2" Score="0" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;Alice's school is planning to take some students from her class on a field trip. Alice is really excited about it. There are a total of $S$ students in her class. But due to budget constraints, the school is planning to take only $N$ students for the trip. These students will be picked randomly. And each student has equal chance of being picked.&#10;Alice's friend circle has M students including her. Though she is excited about the field trip, she will enjoy it only if there are at least $K$ of her friends with her on the trip. She is wondering what are the chances of that happening. She needs your help. Tell her the probability that she will enjoy given that she goes on the trip.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-05-04T20:59:57.820" Id="58138" LastActivityDate="2013-06-04T04:45:16.303" LastEditDate="2013-06-04T04:45:16.303" LastEditorUserId="805" OwnerUserId="25251" PostTypeId="1" Score="0" Tags="&lt;self-study&gt;&lt;conditional-probability&gt;" Title="Probability Higher level" ViewCount="229" />
  <row Body="&lt;p&gt;These kinds of problems are usually solved by counting the number of &quot;good&quot; outcomes and dividing by the number of all possible outcomes.&lt;/p&gt;&#10;&#10;&lt;p&gt;How many ways are there to select $N$ of the $S$ students? How many ways are there that include selecting both Alice and any $K$ of her $M-1$ friends?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-05T00:55:27.927" Id="58147" LastActivityDate="2013-05-05T00:55:27.927" OwnerUserId="9964" ParentId="58138" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="58261" AnswerCount="2" Body="&lt;p&gt;Suppose that we have a two-column data set. One column consists of a hundred x=0 and a hundred x=1, whereas the other one consists of y's (1 or 0 response). Besides, suppose that the P(Y=1|X=0) = 0.001 and P(Y=1|X=1) = 0.05. When fitting a logistic model (glm(y ~ x)) to this data set, why are we having a quasi separation problem here?&lt;/p&gt;&#10;&#10;&lt;p&gt;My doubts are as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;Since P(Y=1|X=0) = 0.001, we have P(Y=0|X=0) = 0.999. On the other hand, P(Y=1|X=1) = 0.05, so P(Y=0|X=1) = 0.95. As a result, we would see Y=0 with a very high chance when x=0 or 1. Why does this situation imply a quasi separation problem?&lt;/p&gt;&#10;&#10;&lt;p&gt;Next, why log[(0.05/0.95)/(0.001/0.999)] = 4 is the theoretical coefficient of x in the model?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-05T06:03:30.930" Id="58152" LastActivityDate="2013-05-06T10:27:34.303" OwnerUserId="21071" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;logistic&gt;&lt;mathematical-statistics&gt;&lt;dataset&gt;&lt;conditional-probability&gt;" Title="How to detect a quasi separation problem for a data set?" ViewCount="155" />
  <row Body="&lt;p&gt;So the $z$-value in lmtest is simply the ratio of the estimate and its corresponding standard error. This is exactly what mpiktas described in the post that you linked (the one with 7 thumbs up). So, for example: $z_{ma5}=-0.05078/0.02079 = -2.443$. Then, the CDF of the standard normal distribution is used to calculate the two-sided p-value:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;2*pnorm(-2.443) = 0.0146&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;or if you use the other/upper tail of the distribution&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;2*(1-pnorm(2.443)) = 0.0146&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;which is the p-value provided by &lt;code&gt;lmtest&lt;/code&gt;. The solution of &lt;code&gt;lmtest&lt;/code&gt; is straightforward and well-known, so nothing wrong with that.&lt;/p&gt;&#10;&#10;&lt;p&gt;I honestly don't know why mpiktas &lt;em&gt;divides&lt;/em&gt; the value by 2 instead of multiplying it, which seems to be the correct way (&lt;strong&gt;EDIT:&lt;/strong&gt; it was a mistake. Fixed in the original answer of mpiktas).&lt;/p&gt;&#10;&#10;&lt;p&gt;Your caluclated p-values by the method of mpiktas are therefore 4 times smaller than the ones calculated by &lt;code&gt;lmtest&lt;/code&gt;: &lt;code&gt;4*0.003645037 = 0.0146&lt;/code&gt;.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-05-05T09:56:11.483" Id="58161" LastActivityDate="2013-05-08T11:12:17.783" LastEditDate="2013-05-08T11:12:17.783" LastEditorUserId="21054" OwnerUserId="21054" ParentId="58159" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="58169" AnswerCount="1" Body="&lt;p&gt;I'm doing a multilevel mixed model with three cross-level interactions in LMER. Do all three interactions necessarily need to be included in the same model or can I evaluate them in three separate models? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-05T12:01:56.793" Id="58166" LastActivityDate="2013-05-08T12:25:53.140" OwnerUserId="25260" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;modeling&gt;&lt;interaction&gt;" Title="interaction terms in separate models?" ViewCount="152" />
  <row Body="&lt;p&gt;cross correlation?  I don't know what language you're working in, but if it's R, try &lt;code&gt;ccf(x, y, na.action=na.exclude)&lt;/code&gt;.  Make sure that the missing time points have &lt;code&gt;NA&lt;/code&gt; s in there (instead of just making the series shorter by deleting missing or bad points, keep it the same length and use the NA).&lt;/p&gt;&#10;&#10;&lt;p&gt;ccf will show you the correlation between the two variables at different lags of x relative to y (where they give you x[t+lag] relative to y[t]).  Look for the lag where the acf is highest, e.g.&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#Simple case: lagged series, some missing obs, but mostly overlapping obs&#10;x &amp;lt;- rnorm(1000)&#10;y &amp;lt;- c(rep(NA,3), head(x, -3)*1.3)&#10;ccf(x,y, na.action=na.exclude)&#10;&#10;#More complicated: some missing obs, y sampled at half frequency&#10;x &amp;lt;- rnorm(1000)&#10;y &amp;lt;- c(rep(NA,3), head(x, -3)*1.3)&#10;m &amp;lt;- sample(1:1000, size=200)&#10;x[m] &amp;lt;- NA&#10;y[m] &amp;lt;- NA&#10;ccf(x,y, na.action=na.exclude)&#10;&#10;#More complicated, more revealing: it gets the lag wrong when fewer obs&#10;x &amp;lt;- rnorm(1000)&#10;y &amp;lt;- c(rep(NA,3), head(x, -3)*1.3)&#10;m &amp;lt;- sample(1:1000, size=500)&#10;x[m] &amp;lt;- NA&#10;y[m] &amp;lt;- NA&#10;ccf(x,y, na.action=na.exclude)&#10;&#10;&#10;#Most complicated: mild missingness, different sampling rates (but rates are multiples)&#10;x &amp;lt;- rnorm(1000)&#10;y &amp;lt;- c(rep(NA,3), head(x, -3)*10)&#10;mx &amp;lt;- (1:1000)%%1 == 0 &amp;amp; rbinom(n=1000, size=1, prob=0.01)==0 #test for multiple of 1 [basline sampling rate] &amp;amp; randomly introduce missingness&#10;my &amp;lt;- (1:1000)%%2 == 0 &amp;amp; rbinom(n=1000, size=1, prob=0.01)==0 #test for multiple of 2 [every other sample] &amp;amp; randomly introduce missingness&#10;#Note that for every other time y is observed, x is also observed, until you account for missingness, and then sometimes that won't hold true.&#10;x[!mx] &amp;lt;- NA&#10;y[!my] &amp;lt;- NA&#10;ccf(x,y, na.action=na.exclude) #this doesn't do so well, I think it's because neither time series is autocorrelated. Maybe if I used a more realistic time series it'd work, but I think that given glenb's criticism, I should stick to the conservative case&#10;&#10;&#10;#Terribly hopeless: there are no overlapping observations&#10;x &amp;lt;- rnorm(1000)&#10;y &amp;lt;- c(rep(NA,3), head(x, -3)*1.3)&#10;m &amp;lt;- (1:1000)%%2 == 0&#10;x[m] &amp;lt;- NA&#10;y[!m] &amp;lt;- NA&#10;ccf(x,y, na.action=na.exclude)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2013-05-05T12:04:51.237" Id="58168" LastActivityDate="2013-10-03T03:00:05.923" LastEditDate="2013-10-03T03:00:05.923" LastEditorUserId="25184" OwnerUserId="25184" ParentId="58106" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Well, as I see this is just given to you. You suppose it is known from somewhere: &quot;If the estimate is independently made in 25 parts, each with 50% error&quot; - it starts from here and just used as a condition in the following sentences.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-05T12:37:08.400" Id="58174" LastActivityDate="2013-05-05T12:37:08.400" OwnerUserId="14741" ParentId="58155" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;No, it definitely does not always give back a stationary series. For example if your true underlying model is given by an AR(2) : $Y_t = c + \theta_1 Y_{t-1} + \theta_2Y_{t-2} + \epsilon_t$, there is no reason whatsoever why $dY_t$ should &lt;em&gt;certainly&lt;/em&gt; be stationary. On the contrary most probably it won't be stationary and you would need to take the $d^2Y_t$ to expect something half sensible.&lt;/p&gt;&#10;&#10;&lt;p&gt;Check the following excellent link on &lt;a href=&quot;http://stats.stackexchange.com/questions/29121/intuitive-explanation-of-unit-root/29129#29129&quot;&gt;Intuitive explanation of unit root&lt;/a&gt; so you built up some familiarity what the unit-root that the Dickey Fuller test tries to detect actually entails.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-05-05T13:38:18.710" Id="58179" LastActivityDate="2013-05-05T13:38:18.710" OwnerUserId="11852" ParentId="58176" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Are your unlabelled observations similar to your labelled observations with regard to independant variables? If so, why dont you run your model keeping a hold-out sample of labelled data so that you may later measure accuracy. You can relate prediction success to the distance to the center of the label points.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-05-05T15:12:01.857" Id="58188" LastActivityDate="2013-05-05T15:12:01.857" OwnerUserId="24128" ParentId="57927" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;I will post an answer to a simpler case based on whuber's comment. If $Z \sim N(0,1)$ and we know $Z &amp;gt; a &amp;gt; 0,$  then the formula for for the conditional expectation of $1 \over Z$ is&lt;br&gt;&#10;$$E[{1 \over Z }| Z &amp;gt; a ] = {{E_1(a^2/2)} \over {{2^{3/2}\sqrt \pi}}  (1-\Phi(a))} \ ,$$  where $E_1(x)$ is defined in the link found in the first comment.  &lt;/p&gt;&#10;&#10;&lt;p&gt;It's not too hard to take it one step further and determine the expression when $Z$ is constrained to an interval, where again $a &amp;gt; 0.$ Here is what I found: $$E [{ 1 \over Z } | a &amp;lt; Z &amp;lt; b ] = { {E_1(a^2/2)}-{E_1(b^2/2)} \over {2^{3/2}  \sqrt \pi} \ [\Phi(b)-\Phi(a)]} \ .$$&lt;/p&gt;&#10;&#10;&lt;p&gt;And, continuing in this vein, let $Y \sim N(0,\sigma^2)$ and $d \gt c \gt 0.$ Then this further generalizes to $$E [{ 1 \over Y } | c &amp;lt; Y &amp;lt; d ] = { {E_1(c^2/2\sigma^2)}-{E_1(d^2/2\sigma^2)} \over {2^{3/2}  \sqrt \pi} \ [\Phi(d/\sigma)-\Phi(c/\sigma)]} \ .$$&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-05T19:48:25.347" Id="58206" LastActivityDate="2013-06-04T18:48:17.753" LastEditDate="2013-06-04T18:48:17.753" LastEditorUserId="24073" OwnerUserId="24073" ParentId="57863" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="58234" AnswerCount="1" Body="&lt;p&gt;From Wikipedia&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Fisher's factorization theorem or factorization criterion provides a convenient characterization of a sufficient statistic. If the probability density function is $ƒ_θ(x)$, then $T$ is sufficient for $θ$ if and only if nonnegative functions $g$ and $h$ can be found such that&#10;  $$&#10;    f_\theta(x)=h(x) \, g_\theta(T(x)), \,\!&#10;$$&#10;  i.e. the density $ƒ$ can be factored into a product such that one factor, $h$, does not depend on $θ$ and the other factor, which does depend on $θ$, depends on $x$ only through $T(x)$.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I was wondering if $\frac{g_\theta(t)}{c}$, where $c := \int g_\theta(t) dt$, is the pdf of $T(X)$ when the pdf of $X$ is $ƒ_θ(x)$?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks and regards!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-05T22:28:42.557" Id="58219" LastActivityDate="2013-05-06T02:15:49.010" LastEditDate="2013-05-06T01:50:30.657" LastEditorUserId="1005" OwnerUserId="1005" PostTypeId="1" Score="1" Tags="&lt;mathematical-statistics&gt;" Title="Does Fisher's factorization theorem provide the pdf of the sufficient statistic?" ViewCount="120" />
  
  
  <row Body="&lt;p&gt;The McLeod-Li test can be used as a test of independence and it can also be used to test the hypothesis of no ARCH. In fact, the McLeod-Li test can be used as a general misspecification test against nonlinearity. See &lt;a href=&quot;http://www.oxfordscholarship.com/view/10.1093/acprof%3aoso/9780199587148.001.0001/acprof-9780199587148&quot; rel=&quot;nofollow&quot;&gt;Teräsvirta, Tjøstheim and Granger (2010)&lt;/a&gt;, (pages 140-1 and 178-9).&lt;/p&gt;&#10;&#10;&lt;p&gt;You may find it useful to know that it is stated in &lt;a href=&quot;http://books.google.ie/books?id=5V7HL7MegEkC&amp;amp;pg=PA40&amp;amp;lpg=PA40&amp;amp;dq=mcleod%20li%20test%20null%20hypothesis%20iid&amp;amp;source=bl&amp;amp;ots=5BPFAL2iV8&amp;amp;sig=wY7kVwqfE9O8h4WKFU2akgMlEzY&amp;amp;hl=en&amp;amp;sa=X&amp;amp;ei=n_SGUZ_mNsWB7Qa_i4GABA&amp;amp;redir_esc=y#v=onepage&amp;amp;q=mcleod%20li%20test%20null%20hypothesis%20iid&amp;amp;f=false&quot; rel=&quot;nofollow&quot;&gt;Patterson, and Ashley (2000)&lt;/a&gt;, (page 40), that the null hypothesis is that the series is &lt;em&gt;i.i.d.&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I hope this information helps you, and rather than go into the particulars, I refer you to the aforementioned sources.&lt;/p&gt;&#10;&#10;&lt;p&gt;As a side note, it's worth mentioning that some tests can be used for a variety of purposes. For example, the BDS test can be used to test the &lt;em&gt;i.i.d&lt;/em&gt; hypothesis, but it can also be used to test for the presence of non-linearity. I think this is a similar situation to the usage of the McLeod-Li test.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-06T00:39:27.600" Id="58231" LastActivityDate="2013-05-06T00:39:27.600" OwnerUserId="24617" ParentId="44486" PostTypeId="2" Score="1" />
  <row AnswerCount="2" Body="&lt;p&gt;I have a question regarding the &lt;strong&gt;kmeans algorithm&lt;/strong&gt;. I know kmeans is a randomized algorithm, but how random is it and what results can I expect.  Suppose you have clustered a dataset into $4$ clusters, where each point has identity $1$, $2$, $3$, or $4$ (which tells you what cluster it belongs to). You then perform a second clustering on the same dataset with the same criteria.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Will all the points in a specific cluster say during the first clustering be in the same cluster the next time you apply the kmeans algorithm?&lt;/li&gt;&#10;&lt;li&gt;If not, will they most likely be in the same cluster and is there some measure for this likelihood?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;By some output I received in R, I believe that 1. does not hold as I get different cluster sizes for different runs on the same dataset.&lt;/p&gt;&#10;&#10;&lt;p&gt;All help is greatly appreciated!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-06T01:08:02.367" Id="58238" LastActivityDate="2013-05-06T07:36:00.367" LastEditDate="2013-05-06T02:55:59.700" LastEditorUserId="183" OwnerUserId="20015" PostTypeId="1" Score="4" Tags="&lt;clustering&gt;&lt;algorithms&gt;&lt;k-means&gt;" Title="How random are the results of the kmeans algorithm?" ViewCount="488" />
  
  <row Body="&lt;p&gt;The thing is that real data doesn't necessarily follow any particular distribution you can name ... and indeed it would be surprising if it did.&lt;/p&gt;&#10;&#10;&lt;p&gt;So while I could name a dozen possibilities, the actual process generating these observations probably won't be anything that I could suggest either. As sample size increases, you will likely be able to reject any well-known distribution. &lt;/p&gt;&#10;&#10;&lt;p&gt;Parametric distributions are often a useful fiction, not a perfect description.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's at least look at the log-data, first in a normal qqplot and then as a kernel density estimate to see how it appears:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/apil3.png&quot; alt=&quot;qqnorm log(x)&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;This has a clear suggestion of a peak near 6 and another about 12.3. The kernel density estimate of the log shows the same thing:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/aVgRo.png&quot; alt=&quot;kernel density estimate&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In both cases, the indication is that the distribution of the &lt;em&gt;log&lt;/em&gt; time is right skew, but it's not clearly unimodal. Clearly the main peak is somewhere around the 5 minute mark. It may be that there's a second small peak in the log-time density, that appears to be somewhere in the region of perhaps 60 hours. Perhaps there are two very qualitatively different &quot;types&quot; of repair, and your distribution is reflecting a mix of two types. Or just maybe once a repair hits a full day of work, it tends to just take a longer time (that is, rather than reflecting a peak at just over a week, it may reflect an anti-peak at just over a day - once you get longer than just under a day to repair, jobs tend to 'slow down').&lt;/p&gt;&#10;&#10;&lt;p&gt;Even the log of the log of the time is somewhat right skew. Let's look at a stronger transformation, where the second peak is quite clear - minus the inverse of the fourth root of time:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/btNBp.png&quot; alt=&quot;hist of -1/(x^0.25)&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The marked lines are at 5 minutes (blue) and 60 hours (dashed green); as you see, there's a peak just below 5 minutes and another somewhere above 60 hours. Note that the upper &quot;peak&quot; is out at about the 95th percentile and won't necessarily be close to a peak in the untransformed distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;There's also a suggestion of another dip around 7.5 minutes with a broad peak between 10 and 20 minutes, which might suggest a very slight tendency to 'round up' in that region (not that there's necessarily anything untoward going on; even if there's no dip/peak in inherent job time there, it could even be something as simple as a function of human ability to focus in one unbroken period for more than a few minutes.)&lt;/p&gt;&#10;&#10;&lt;p&gt;It looks to me like a two-component (two peak) or maybe three component mixture of right-skew distributions would describe the process reasonably well but would not be a perfect description.&lt;/p&gt;&#10;&#10;&lt;p&gt;The package &lt;code&gt;logspline&lt;/code&gt; seems to pick four peaks in log(time):&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/C2OKc.png&quot; alt=&quot;logpsine plot&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;with peaks near 30, 270, 900 and 270K seconds  (30s,4.5m,15m and 75h).&lt;/p&gt;&#10;&#10;&lt;p&gt;Using logspline with other transforms generally find 4 peaks but with slightly different centers (when translated to the original units); this is to be expected with transformations.&lt;/p&gt;&#10;" CommentCount="24" CreationDate="2013-05-06T01:46:08.143" Id="58241" LastActivityDate="2013-05-06T23:22:09.097" LastEditDate="2013-05-06T23:22:09.097" LastEditorUserId="805" OwnerUserId="805" ParentId="58220" PostTypeId="2" Score="20" />
  <row AnswerCount="0" Body="&lt;p&gt;I have daily revenue data from a small business with 6 locations. The business sells food products that range from roughly \$2.00 to \$9.00, mainly to professionals. They do over a million dollars a year in sales and are located in a major North American city (scattered around the downtown core / business district). The revenue data contains what is sold at each location on any given day. The daily revenue data spans the duration that a given location has been in operation. The longest being five years and the shortest being two years.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are some questions I wish to answer, for example:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;How sensitive are sales to macroeconomic events?&lt;/li&gt;&#10;&lt;li&gt;Do sales have seasonal patterns?&lt;/li&gt;&#10;&lt;li&gt;What factors/characteristics influence a given location's sales?&lt;/li&gt;&#10;&lt;li&gt;Do different locations exhibit the same patterns of sales growth?&lt;/li&gt;&#10;&lt;li&gt;What are the demand elasticities of different products?&lt;/li&gt;&#10;&lt;li&gt;What endogenously determined &quot;events&quot; have occurred that have resulted in an increase in sales? (A promotion, for example)&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Here are some ideas of how to answer the above questions:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;See if positive or negative macroeconomic demand shocks show up in the daily revenue time-series. Or, define some macroeconomic &quot;event&quot; (like a sharp decline in the stock market) and check for abnormal changes (decreases) in revenues.&lt;/li&gt;&#10;&lt;li&gt;Look for seasonal fluctuations around some trend line of sales.&lt;/li&gt;&#10;&lt;li&gt;Identify differences in characteristics between locations and run multiple variable regressions on the sales data. For instance, have a dummy variable for the presence of a government office within 100m of a given location, or a similarly branded competitor within 100m, et cetera. Then test the hypothesis that each variable has a coefficient of zero. (Note: possibly use this to predict the future revenue of a potential location)&lt;/li&gt;&#10;&lt;li&gt;Compare the &quot;shape&quot; of the growth in sales between different locations. &lt;/li&gt;&#10;&lt;li&gt;Determine if the sales of a given product change after a price increase. Do this for each increase in price, being careful to identify if other factors may have contributed to the change in demand.&lt;/li&gt;&#10;&lt;li&gt;Define some &quot;event&quot; and look for changes in revenue around this event.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Further, I suppose it would be helpful to &quot;clean&quot; the data depending on what I am using it for. I might find that one location had an initial rate of growth in income that was far higher than other locations, but that it opened just prior to a typically high-sales season and also a positive macroeconomic event. The same goes for an increase in sales after a promotion. I would not want to mix up endogenous and exogenous changes in revenue.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am hoping for some input on what meaningful conclusions I can actually draw given the data set. I've arrived at my ideas from brainstorming and I'm sure they are very flawed. I've just finished third year economics with some extra math and stats thrown in. I'm doing this because the data is available to me and I want to (try and) use some of the tools I've learned. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Questions:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;In terms of identifying differences between locations, is six locations too few? &lt;/li&gt;&#10;&lt;li&gt;In general, what statistical pitfalls will I run into? &lt;/li&gt;&#10;&lt;li&gt;Should I not do this because there is nothing meaningful to be found in this data? &lt;/li&gt;&#10;&lt;li&gt;Are there any statistical tests I &lt;em&gt;should&lt;/em&gt; do that will yield meaningful results (i.e any questions I can actually answer given the data set)? &lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="3" CreationDate="2013-05-06T02:46:17.147" Id="58245" LastActivityDate="2013-05-06T08:05:56.383" LastEditDate="2013-05-06T08:05:56.383" LastEditorUserId="183" OwnerUserId="25090" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;time-series&gt;&lt;multiple-regression&gt;&lt;predictive-models&gt;" Title="Statistical tests on the revenue data of a small business" ViewCount="149" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I would really appreciate any suggestions with the following data analysis issue.  Please read till the end as the problem at first may appear trivial, but after much researching, I assure you it is not.  The situation is a little complicated because I want to compare the ratios of means:&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, in one experiment I have collected data (Electric current levels, a continuous variable) from 7-8 cells (replicates) which express a particular ion channel gene (Gene1).  This gives me a mean current level for Gene1.  I have then measured current levels from a different set of 7-8 replicate cells which also express Gene1 PLUS an activator gene (the &quot;treatment&quot;).  Now I have a mean current value for Gene1+activator. &lt;/p&gt;&#10;&#10;&lt;p&gt;I repeat the two measurements for Gene2 (a different ion channel gene), again recording mean current values from 7-8 cell replicates for Gene2 alone and then for Gene2+activator. &lt;/p&gt;&#10;&#10;&lt;p&gt;The quantity I am interested in comparing is the %activation or fold activation caused by the activator for Gene1 versus Gene2.  So, I would obtain a ratio by dividing the mean current for Gene1+activator by the mean current for Gene1 alone.  This would give me the fold activation for Gene1.  I would compare this to a similar ratio obtained for Gene2. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have done some research on this and using &lt;a href=&quot;http://en.wikipedia.org/wiki/Fieller%27s_theorem&quot; rel=&quot;nofollow&quot;&gt;Fieller's intervals&lt;/a&gt; to compute the error bars or confidence intervals seems promising.  However, I don't know how to convert that to hypothesis test and get an appropriate p-value for the comparison of means being same/different. Furthermore, the best solution would also allow multiple comparisons and allow me to compare &quot;fold-activations&quot; for Gene1 and Gene2 and Gene3 and Gene4 at the same time.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Fieller's intervals seem like the perfect tool to compute 95% confidence intervals etc around each fold-activation but as we are submitting to a journal, they will insist on p-values for our comparisons.  As of now, I am reduced to insisting the lack of CI overlap clearly signals a significant difference but I know that lack of 95%CI overlap is an overly stringent comparison test which represents an alpha&amp;lt;0.05.  I would truly appreciate any suggestions to the appropriate comparison test (single or multiple comparison, either at this point will be helpful).  Thanks in advance. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-05-06T04:58:50.637" FavoriteCount="2" Id="58253" LastActivityDate="2013-05-06T06:05:34.533" LastEditDate="2013-05-06T06:05:34.533" LastEditorUserId="805" OwnerUserId="25281" PostTypeId="1" Score="3" Tags="&lt;hypothesis-testing&gt;&lt;statistical-significance&gt;&lt;confidence-interval&gt;&lt;p-value&gt;&lt;proportion&gt;" Title="Ratios of means - statistical comparison test using Fieller's theorem?" ViewCount="461" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I'm currently sifting through my copy of Analysis of Financial Time Series 2nd Edition by Ruey Tsay, and one of the sections involves fitting a MA model to certain data (data set is &lt;a href=&quot;http://faculty.chicagobooth.edu/ruey.tsay/teaching/fts/m-ew.dat&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;). Here's the fit with exact maximum likelihood according to the text, with certain insignificant parameters removed:&lt;/p&gt;&#10;&#10;&lt;p&gt;rt = 0.013 + a(t) + 0.181a(t−1) − 0.121a(t−3) + 0.122a(t−9)&lt;/p&gt;&#10;&#10;&lt;p&gt;σ(a) = 0.0724&lt;/p&gt;&#10;&#10;&lt;p&gt;However, when I try to fit it with R...&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; mew = read.table(&quot;m-ew.dat&quot;)&#10;&amp;gt; arima(mew,order = c(0,0,9),fixed = c(NA,0,NA,rep(0,5),NA,NA),method = &quot;ML&quot;)&#10;Call:&#10;arima(x = mew, order = c(0, 0, 9), fixed = c(NA, 0, NA, rep(0, 5), NA, NA), &#10;method = &quot;ML&quot;)&#10;&#10;Coefficients:&#10;        ma1  ma2      ma3  ma4  ma5  ma6  ma7  ma8     ma9  intercept&#10;      0.180    0  -0.1318    0    0    0    0    0  0.1373     0.0132&#10;s.e.  0.031    0   0.0362    0    0    0    0    0  0.0327     0.0029&#10;&#10;sigma^2 estimated as 0.005282:  log likelihood = 1039.1,  aic = -2068.21&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;As you can see, the ma1 coefficients are the same, but ma3 and ma9 are different, even with method = &quot;ML&quot;, i.e. maximum likelihood. Why is this?&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, from a practical standpoint, while ma2 and ma4-ma8 may be 0 (their 95% confidence intervals overlap with 0), removing them from the model raises the AIC, lowers the p-value with regards to the Ljung-Box test on the residuals, and also lowers the log-likelihood value. Is it even worth removing these parameters if such things happen?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-05-06T08:36:10.690" Id="58265" LastActivityDate="2013-05-06T13:24:15.260" LastEditDate="2013-05-06T11:06:45.947" LastEditorUserId="88" OwnerUserId="25291" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;time-series&gt;&lt;mathematical-statistics&gt;&lt;mathematics&gt;" Title="In R, coefficients of MA function are wrong?" ViewCount="252" />
  <row AcceptedAnswerId="58271" AnswerCount="2" Body="&lt;p&gt;I have a set of results of independent measurements of some physical quantity. As an example I give here real expermental data on methanol refractive index at 25 degrees Celsius published in scientific literature from 1960 to 2011:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;data={{1960, 1.32652}, {1961, 1.32662}, {1963, 1.32650}, {1963, &#10;  1.32750}, {1968, 1.32698}, {1968, 1.32890}, {1970, 1.32657}, {1970, &#10;  1.32660}, {1971, 1.3260}, {1971, 1.32610}, {1971, 1.32630}, {1971, &#10;  1.3350}, {1972, 1.32640}, {1972, 1.32661}, {1973, 1.32860}, {1975, &#10;  1.32515}, {1975, 1.32641}, {1976, 1.32663}, {1977, 1.32670}, {1980, &#10;  1.3250}, {1983, 1.32850}, {1985, 1.32653}, {1991, 1.32710}, {1995, &#10;  1.32621}, {1995, 1.32676}, {1996, 1.32601}, {1996, 1.32645}, {1996, &#10;  1.32715}, {1998, 1.32820}, {1999, 1.32730}, {1999, 1.32780}, {2001, &#10;  1.32634}, {2006, 1.32620}, {2011, 1.32667}};&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The first number is the year of first publication, the second is the published value. The authors do not always provide their own estimate for error of the published value and even when they do this, the published estimate is often not accurate.&lt;/p&gt;&#10;&#10;&lt;p&gt;It is easy to see that the distribution is asymmetrical (other datasets I work with are even more asymmetrical). Here is a density histogram with bins selected by hands:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;bins = {1.325, 1.3259, 1.3261, 1.3263, 1.3265, 1.3266, 1.3267, 1.3269,&#10;    1.3275, 1.328};&#10;Histogram[dataFirst[[All, 2]], {bins}, &quot;PDF&quot;]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/x4lne.png&quot; alt=&quot;histogram&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Because of asymmetric feature of the distribution I cannot use the mean as an estimate for the true value of methanol refractive index. &lt;/p&gt;&#10;&#10;&lt;p&gt;The fact is that the probability to get smaller value than the true value in general is not equal to the probability to get higher value. This means that the median in general is also unsuitable.&lt;/p&gt;&#10;&#10;&lt;p&gt;It may be assumed that each invidual measurement has normal distribution for the instrumental error of the measurement. Additional sources of error are impurities in methanol. The most common impurity is water which increases the refractive index, but authors usually dry methanol before measurement. Probably this is the reason why the density is higher for low values: other impurities such as some common organic solvents (benzene) and (possibly) diluted gases (methane, carbon oxide, carbon dioxide) lower the refractive index. From the other hand alcohols, aldehydes and ketones which may present in industrial methanol and some organic solvents (CCl4) increase the refractive index. So there at least two sources of error: one (instrumental) may be assumed normally distributed and even equal for the all measurements and the second (impurities) is probably asymmetrical (and may be even bimodal) and depends on how methanol was obtained and purified.&lt;/p&gt;&#10;&#10;&lt;p&gt;What is the best way to estimate the true value and its 95% confidence bands for the measured quantity in such cases? &lt;/p&gt;&#10;&#10;&lt;p&gt;P.S. References for relevant scientific papers will be appreciated.&lt;/p&gt;&#10;&#10;&lt;p&gt;P.S.2. The above code is for &lt;em&gt;Mathematica&lt;/em&gt; system.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-05-05T11:44:58.887" Id="58270" LastActivityDate="2013-05-06T11:05:48.523" OwnerDisplayName="Alexey Popkov" OwnerUserId="25283" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;mathematica&gt;" Title="How to estimate true value and 95% bands when distribution is asymmetrical?" ViewCount="95" />
  <row Body="&lt;p&gt;ARIMA models can easily be enhanced to include&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;incorporation of Pulse , level shifts , local time trends and seasonal pulses&lt;/li&gt;&#10;&lt;li&gt;lag and lead structures for user-specified causals&lt;/li&gt;&#10;&lt;li&gt;incorporation of variance changes&lt;/li&gt;&#10;&lt;li&gt;incorporation of parameter changes&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2013-05-06T13:11:05.423" Id="58281" LastActivityDate="2013-05-06T13:11:05.423" OwnerUserId="3382" ParentId="58249" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;Expanding on whuber's comment and my minor emendation of it, we have that for $z \geq 0$,&#10;$$P\{v_\max &amp;lt; z\} = [2\Phi(z/\sigma)-1]^N = [1-2Q(z/\sigma)]^N$$ where&#10;$\Phi(x)$ is the standard normal distribution function and $Q(x)=1-\Phi(x)$ is&#10;its complement. Now, since $1-2Q(z) &amp;lt; 1$ for $z &amp;gt; 0$, &lt;em&gt;if&lt;/em&gt; $z$ and $\sigma$&#10;were  &lt;em&gt;constants&lt;/em&gt;, that is, not functions of $N$, then as we raised a quantity&#10;less than $1$ to ever higher powers $N$, the result would tend to $0$ as $N \to \infty$.&#10;On the other hand, if $z/\sigma$ were such that $2Q(z/\sigma)=aN^{-1}$, then we&#10;could use the standard result&#10;$$\lim_{N\to \infty}\left(1-\frac{a}{N}\right)^N = e^{-a}$$&#10;to deduce that $P\{v_\max &amp;lt; z\}$ converges to a positive constant (smaller&#10;than $1$) as $N \to \infty$ and so  $P\{v_\max &amp;gt; z\}$ converges to a constant&#10;greater than $0$. This is not quite good enough; we need to have &#10;$2Q(z/\sigma)$ go to $0$ slightly&#10;faster than as $N^{-1}$.  What will work?  Well, a standard but weak&#10;bound on $Q(x)$ is $$Q(x) &amp;lt; \frac{1}{2}e^{-x^2/2} ~~ \text{for} ~x &amp;gt; 0$$&#10;and so if we choose $z = \sigma\sqrt{2\ln N}$, we get &#10;$2Q(z/\sigma) &amp;lt; e^{-\ln N} = N^{-1}$ which we just said is not a fast&#10;enough decay with increasing $N$, but if we use the&#10;&lt;a href=&quot;http://math.stackexchange.com/q/28751/15941&quot;&gt;tighter bound&lt;/a&gt;&#10;$$Q(x) &amp;lt; \frac{1}{x\sqrt{2\pi}}e^{-x^2/2},$$&#10;with the same choice $z = \sigma\sqrt{2\ln N}$,&#10;that extra $x$ in the denominator is giving us&#10;an $\sqrt{\ln N}$ faster decay that suffices to allow us&#10;to conclude that&#10;$$P\{v_\max &amp;gt; \sigma\sqrt{2\ln N}\,\} \to 0~~ \text{as}~ N \to \infty.$$&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-05-06T20:20:20.477" Id="58313" LastActivityDate="2013-05-06T20:20:20.477" OwnerUserId="6633" ParentId="58293" PostTypeId="2" Score="3" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have three data sets like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;data1: {A, A, B, C, D, ..}&#10;data2: {A, B, B, C, E, ...}&#10;data3: {A, C, D, D, E, ...}&lt;/p&gt;&#10;&#10;&lt;p&gt;How do I test if these three data sets are from the same distribution?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-07T04:58:10.780" Id="58333" LastActivityDate="2013-05-11T00:20:13.600" OwnerUserId="24993" PostTypeId="1" Score="0" Tags="&lt;hypothesis-testing&gt;&lt;distributions&gt;&lt;categorical-data&gt;" Title="What method can be used to test if three or more categorical sample data sets are from the same distribution?" ViewCount="80" />
  <row AnswerCount="0" Body="&lt;p&gt;The response was calculated as $\frac{Control-Observation}{Control}*100\%$. Raw values of $Control$ and $Observation$ are not available, I have only calculated values. Each value was measured twice. The scatterplot of two measurements is presented at the figure&#10;&lt;img src=&quot;http://i.stack.imgur.com/JYehn.png&quot; alt=&quot;Two measurements&quot;&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;As it can be seen, the error is heteroscedastic and depends on the response value. Note, that actually the chance that negative calculated response is really negative is rather smal. So there can be from  0 to 5 (rude score) subjects with really negative values thus all other negatives represent errors of experiment.&lt;/p&gt;&#10;&#10;&lt;p&gt;The goal of the model is rather predictivity than evaluation of predictors significance. When fitting model, I'll perform variable selection. I have read that logistic or probit regression are good to model percentages, but how to deal with repeated measurements and heteroscedasticity is unknown. Currently I'm going to find moving standard error of experiment and then use corresponding weights in loess (or, maybe, weighted SVM).&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;What method would be more appropriate in the described case? (non-linear flexible methods are preferred)&lt;/li&gt;&#10;&lt;li&gt;If my suggestion is OK, what weighted non-linear algorithms are available in R?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Upd1: Here is scale-location plot of model &lt;code&gt;response~subject&lt;/code&gt; that shows dependent variance of residuals&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/gTFcG.png&quot; alt=&quot;scale-location&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;It is provided just for clarification purposes. My aim is to explain exactly between-subject variability. Thus currently I'm fitting mean (for each subject) values of response with possible predictors, assigning weights to each mean value. I have about 600 possible covariates, though only small number of them (1-5) can be relevant. &#10;Here are residuals plots for single best-fit covariate with and without weighting.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/q9uNB.png&quot; alt=&quot;with weights&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/g2Lfa.png&quot; alt=&quot;without weights&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Both are awful. Should I continue this direction searching for complete list of relevant covariates?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-05-07T07:45:40.343" Id="58338" LastActivityDate="2013-05-10T08:42:48.937" LastEditDate="2013-05-10T08:42:48.937" LastEditorUserId="8165" OwnerUserId="8165" PostTypeId="1" Score="0" Tags="&lt;repeated-measures&gt;&lt;nonlinear-regression&gt;&lt;heteroscedasticity&gt;&lt;weighted-regression&gt;" Title="What method is better to model percentage response with each subject measured two times and heteroscedastic error?" ViewCount="155" />
  
  <row Body="&lt;p&gt;You could turn it into a regression problem and use the uncertainties as weights. That is, predict group (1 or 2?) from measurement in a regression. &lt;/p&gt;&#10;&#10;&lt;p&gt;But &lt;/p&gt;&#10;&#10;&lt;p&gt;The uncertainties are approximately constant, so it seems likely that nothing much will change by using them too. &lt;/p&gt;&#10;&#10;&lt;p&gt;You have a mild outlier at 10.5, which is complicating matters by reducing the difference between means. But if you can believe the uncertainties, that value is no more suspect than any others. &lt;/p&gt;&#10;&#10;&lt;p&gt;The t-test does not know that your alternative hypothesis is that two samples are drawn from different populations. All it knows about is comparing means, under certain assumptions. Rank-based tests are an alternative, but if you are interested in these data as measurements, they don't sound preferable for your goals. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-05-07T14:49:55.813" Id="58378" LastActivityDate="2013-05-07T14:49:55.813" OwnerUserId="22047" ParentId="58371" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I would like to use various parameters of remote sensing data (i.e. chorlophyl, sea-surface temperature, wind stress, etc.) as covariates in a species distribution model.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have sightings data, including sighting effort, collected from a number of scientific cruises over a two month period.&lt;/p&gt;&#10;&#10;&lt;p&gt;My idea was to average environmental data for the entire sighting period, treat all sightings as having been collected simultaneously, with the 2 month average of values used as the covariates.&lt;/p&gt;&#10;&#10;&lt;p&gt;Given the nature of remote-sensing data, there are numerous missing values due to clouds, etc., so if I were to average the data I would need to omit any cell that had no value for &gt;= 1 day of data. This results in a small fraction of the data, and just doesn't seem like a good way of doing things.&lt;/p&gt;&#10;&#10;&lt;p&gt;How can I obtain statistically consistent averaged estimates of parameters in the presence of missing data in remote sensor output?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt;&#10;Sighting effort is evaluated as the time taken before an observation is made. The more time taken for a sighting the higher the effort made to obtain the sighting. I could also use the distance covered before sighting rather than time.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-05-07T16:47:39.623" Id="58390" LastActivityDate="2013-05-07T17:18:50.363" LastEditDate="2013-05-07T17:18:50.363" LastEditorUserId="16938" OwnerUserId="16938" PostTypeId="1" Score="0" Tags="&lt;modeling&gt;&lt;missing-data&gt;" Title="Averaging gridded data with missing values" ViewCount="30" />
  <row AcceptedAnswerId="58400" AnswerCount="1" Body="&lt;p&gt;x is t-distributed;&#10;y is t-distributed.&lt;/p&gt;&#10;&#10;&lt;p&gt;How is x/y distributed? &lt;/p&gt;&#10;&#10;&lt;p&gt;Does it have a closed-form formula?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-05-07T17:20:37.197" Id="58398" LastActivityDate="2013-05-07T18:44:50.637" OwnerUserId="16360" PostTypeId="1" Score="1" Tags="&lt;distributions&gt;&lt;mathematical-statistics&gt;&lt;pdf&gt;&lt;t-distribution&gt;" Title="What is the distribution of the ratio of two t-distributed random variables?" ViewCount="89" />
  <row Body="&lt;p&gt;I'm going to cover how the p-value is calculated first, then how to work backwards from the p-value. &lt;/p&gt;&#10;&#10;&lt;p&gt;The t-statistic used to compute a p-value for a regression coefficient is $t= {\beta \over SE}$&lt;br&gt;&#10;Then the usually reported p-value is calculated from $|t|$ and a t-distribution with $n-k$ degrees of freedom, $n$ being sample size and $k$ the number of coefficients including intercept. Given $|t|$, as AdamO states, you won't be able to recover the sign of the coefficient, just its magnitude.&lt;/p&gt;&#10;&#10;&lt;p&gt;You could do this calculation in R with the &lt;code&gt;pt()&lt;/code&gt; function to calculate the probability (multiplying by 2 since it is a two sided test):&lt;br&gt;&#10;&lt;code&gt;2 * (1 - pt(beta/se, n-k))&lt;/code&gt;  &lt;/p&gt;&#10;&#10;&lt;p&gt;But here you already have the p-value, and need to work backwards. You can go from a probability to the t-statistic $|t|$ using the &lt;code&gt;qt()&lt;/code&gt; function (dividing p-value by 2 due to two sided):&lt;br&gt;&#10;&lt;code&gt;abs(qt(p / 2, n-k))&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;From there, calculating $|\beta|$ is straightforward.&lt;/p&gt;&#10;&#10;&lt;p&gt;Example with $\beta=0.72727$, $SE = 0.44998$, $n=6$, $k=3$&lt;br&gt;&#10;&lt;code&gt;&amp;gt; 2*(1-pt(0.72727/0.44998,df=3))&lt;/code&gt;&lt;br&gt;&#10;&lt;code&gt;[1] 0.2044641&lt;/code&gt;  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;&amp;gt; qt(0.2044641/2,3)&lt;/code&gt;&lt;br&gt;&#10;&lt;code&gt;[1] -1.616227&lt;/code&gt;&lt;br&gt;&#10;&lt;code&gt;&amp;gt; abs(qt(0.2044641/2,3))*.44998&lt;/code&gt;&lt;br&gt;&#10;&lt;code&gt;[1] 0.7272698&lt;/code&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-05-07T18:07:38.163" Id="58402" LastActivityDate="2013-05-07T18:13:26.347" LastEditDate="2013-05-07T18:13:26.347" LastEditorUserId="4485" OwnerUserId="4485" ParentId="58392" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;There's no need to make assumptions about the distribution of the predictors. If a predictor is heavily skewed towards higher values, you may need to transform it though. &lt;/p&gt;&#10;&#10;&lt;p&gt;Watch out for multicolinearity (hint: VIF). Always make sure your residuals are normally distributed, if they're not - do something. Transforms of the predictors can be worth trying out.&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT: Removed first line of post (wrong info). Check the later answers for information regarding standardization of the predictors.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-05-07T21:26:08.637" Id="58422" LastActivityDate="2013-05-07T21:54:33.120" LastEditDate="2013-05-07T21:54:33.120" LastEditorUserId="22163" OwnerUserId="22163" ParentId="58421" PostTypeId="2" Score="1" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I'm having a problem running a repeated-measures ANOVA in R using the &lt;code&gt;ezANOVA&lt;/code&gt; function.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have data from 18 subjects - each subject participated in 3 conditions, and data was collected in each subject/condition combination at 29 electrode sites (1566 total data points in a balanced design with no missing cells).&lt;/p&gt;&#10;&#10;&lt;p&gt;When I try to run the full ANOVA&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;model = ezANOVA(data, dv=voltage, wid=subject, within=.(electrode, channel))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I get the following error:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Error in lambda &gt; 0 : invalid comparison with complex values Error in&#10;  ezANOVA_main(data = data, dv = dv, wid = wid, within = within,  :&lt;br&gt;&#10;  The car::Anova() function used to compute results and assumption tests&#10;  seems to have failed. Most commonly this is because you have too few&#10;  subjects relative to the number of cells in the within-Ss design. It&#10;  is possible that trying the ANOVA again with &quot;type=1&quot; may yield&#10;  results (but definitely no assumption tests).&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Things work fine though if I include fewer levels of &lt;code&gt;electrode&lt;/code&gt; than I have subjects (18 or fewer), but if I add more it fails. Why is this a problem? I wouldn't think that having greater-than-n levels for a within-subjects factor would be a problem if it's a balanced fully repeated-measures design. (SPSS will compute things just fine). Using Type I SS works, but if I select this option I won't give me the sphericity corrected p-values that I need to report.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-08T00:28:01.590" Id="58435" LastActivityDate="2013-08-22T13:51:00.997" LastEditDate="2013-05-08T01:42:06.897" LastEditorUserId="7290" OwnerUserId="25360" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;anova&gt;&lt;repeated-measures&gt;" Title="Repeated-measures error in R ezANOVA using more levels than subjects (balanced design)" ViewCount="1788" />
  <row AcceptedAnswerId="58453" AnswerCount="1" Body="&lt;p&gt;Let's imagine we have one variable (factor.to.explain) that we want to explain by 10 other variables using 10 linear model (no interactions computed). We should correct for multiple testing. It is possible that one, some or several of them affects the factor.to.explain.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note: TP = True Positive, FN = False Negative, FP = False Positive, TN = True Negative&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;How does a Bonferroni correction affects sensitivity (= TP / TP + FN) and specificity (= TN / TN + FP) ?&lt;/li&gt;&#10;&lt;li&gt;How does other corrections for multiple comparisons affects sensitivity and specificity ?&lt;/li&gt;&#10;&lt;li&gt;Do sensitivity and specificity change if I do only 5 or all (10) my linear models ?&lt;/li&gt;&#10;&lt;li&gt;Is there a objective way of deciding what is the best sensitivity to specificity ratio ?*&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Specific question (not important if I don't get an answer):&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;What sensitivity and specificity ratio is usually aimed in ecology ?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="2" CreationDate="2013-05-08T07:08:34.217" Id="58444" LastActivityDate="2013-05-08T12:10:37.237" LastEditDate="2013-05-08T12:10:37.237" LastEditorUserId="6029" OwnerUserId="24097" PostTypeId="1" Score="2" Tags="&lt;multiple-regression&gt;&lt;multiple-comparisons&gt;" Title="Effect of corrections for multiple testing on sensitivity and specificity" ViewCount="172" />
  <row AnswerCount="0" Body="&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Hilbert_curve&quot; rel=&quot;nofollow&quot;&gt;Hilbert Curves (Wikipedia)&lt;/a&gt; are space-filling curves said to &quot;fairly well preserve locality&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Do you know any &lt;em&gt;theoretical&lt;/em&gt; results here, such as bounds that neighbors within a radius of $\varepsilon$ are preserved with probability $p$ or anything like that? Wikipedia also only has a &quot;citation needed&quot; flag there.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'd like to be able to give probabilistic bounds on the error when using Hilbert curves, for nearest neighbor search in $d$ dimensions on clustered data (i.e. not uniform on the data space). I believe I saw some results for $d=2$, but I'm interested in the more general case.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you for references to theoretical work on the quality of the hilbert curve.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-05-08T08:28:58.210" Id="58447" LastActivityDate="2013-05-08T13:45:43.277" LastEditDate="2013-05-08T13:45:43.277" LastEditorUserId="18215" OwnerUserId="18215" PostTypeId="1" Score="1" Tags="&lt;probability&gt;&lt;spatial&gt;&lt;distance&gt;&lt;curves&gt;" Title="Hilbert curves: bounds / probabilities of preserving neighbors" ViewCount="39" />
  
  
  <row Body="&lt;p&gt;If you want to determine how well the model can predict &lt;em&gt;unseen&lt;/em&gt; data you can use cross validation.&#10;In Matlab, you can use &lt;code&gt;glmfit&lt;/code&gt; to fit the logistic regression model and &lt;code&gt;glmval&lt;/code&gt; to test it.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is a sample of Matlab code that illustrates how to do it, where &lt;code&gt;X&lt;/code&gt; is the feature matrix and &lt;code&gt;Labels&lt;/code&gt; is the class label for each case, &lt;code&gt;num_shuffles&lt;/code&gt; is the number of repetitions of the cross-validation while &lt;code&gt;num_folds&lt;/code&gt; is the number of folds:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;for j = 1:num_shuffles&#10;    indices = crossvalind('Kfold',Labels,num_folds);&#10;    for i = 1:num_folds&#10;        test = (indices == i); train = ~test;&#10;        [b,dev,stats] = glmfit(X(train,:),Labels(train),'binomial','logit'); % Logistic regression&#10;        Fit(j,i) = glmval(b,X(test,:),'logit')';&#10;    end&#10;end&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;Fit&lt;/code&gt; is then the fitted logistic regression estimate for each test fold. Thresholding this will yield an estimate of the predicted class for each test case. Performance measures are then calculated by comparing the predicted class label against the actual class label. Averaging the performance measures across all folds and repetitions gives an estimate of the model performance on unseen data.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-08T10:49:25.310" Id="58463" LastActivityDate="2013-05-08T10:57:32.413" LastEditDate="2013-05-08T10:57:32.413" LastEditorUserId="11030" OwnerUserId="11030" ParentId="58449" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="58630" AnswerCount="1" Body="&lt;p&gt;When I estimate a logistic binomial regression with an allowed to vary effect on a second and a third level, the overall intercept represents the average log-odds for any level one unit in any higher level unit:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\text{logit}(\pi_{ijk}) = \beta_0 + \mu_{jk} + \mu_{k}$&lt;/p&gt;&#10;&#10;&lt;p&gt;Where $\beta_0$ is the fixed intercept and $\mu_{jk}$ and $\mu_{k}$ are the random cluster effects.&lt;/p&gt;&#10;&#10;&lt;p&gt;$\beta_0$ is estimated as -3.525 in this case (or 2.9% probability of success: $\pi_{ijk} = \frac{e^{\beta_{0}}}{1+e^{\beta_{0}}}$). &lt;/p&gt;&#10;&#10;&lt;p&gt;However, the model where no random effects are allowed (or the general mean):&lt;/p&gt;&#10;&#10;&lt;p&gt;$\text{logit}(\pi_{ijk}) = \beta_0$ &lt;/p&gt;&#10;&#10;&lt;p&gt;gives -2.932 log-odds or 5% probability of success. &lt;/p&gt;&#10;&#10;&lt;p&gt;Does the decrease of overall intercept mean there were more clusters with low probability than with high probability?&lt;/p&gt;&#10;&#10;&lt;p&gt;PS: I also switch from MQL to PQL when going to the random intercept model (since PQL is not possible for the fixed effect only model). It could have to do with that too.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-08T12:08:34.627" Id="58469" LastActivityDate="2013-05-10T07:44:37.537" LastEditDate="2013-05-08T14:15:39.393" LastEditorUserId="686" OwnerUserId="18334" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;mixed-model&gt;&lt;multilevel-analysis&gt;" Title="What does the change in intercept mean if you allow for clustering in your data?" ViewCount="76" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm working on a customer retention project that predicts the probability a customer is still subscribing to our services at time T. Unfortunately, we only have the most recent two years of customer data. Estimating the probability of survival at end of year 1 and 2 using Kaplan-Meier is straight-forward. But now we need to project the probability of survival at end of year 3, 4, and 5. Is it possible to do such a task using survival analysis or some other technique (preferably using R)?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-08T13:54:41.930" Id="58477" LastActivityDate="2013-05-08T13:54:41.930" OwnerUserId="17133" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;survival&gt;&lt;prediction&gt;&lt;kaplan-meier&gt;" Title="projecting future survival rate" ViewCount="76" />
  
  
  
  <row Body="&lt;p&gt;Leung, Mei, and Zhang have developed two tests for whether GWR is a better fit than OLS regression. &lt;a href=&quot;http://www.envplan.com/abstract.cgi?id=a3162&quot; rel=&quot;nofollow&quot;&gt;Their paper is here&lt;/a&gt;, but it's behind a paywall if you don't have academic access. As for variograms, etc. I know that Bivand, et al. cover tools and mechanisms &lt;a href=&quot;http://books.google.com/books?hl=en&amp;amp;lr=&amp;amp;id=4gg8yx_lcj0C&amp;amp;oi=fnd&amp;amp;pg=PR7&amp;amp;dq=applied%20spatial%20data%20analysis%20with%20R&amp;amp;ots=yzgG0o47bC&amp;amp;sig=4FybRPmHwajr0m9Ectm33qdlxgM#v=onepage&amp;amp;q=applied%20spatial%20data%20analysis%20with%20R&amp;amp;f=false&quot; rel=&quot;nofollow&quot;&gt;In their book&lt;/a&gt;. I know a pdf of this exists because I have it, but I forgot where I got it from.&lt;/p&gt;&#10;&#10;&lt;p&gt;As far as spatial stationarity in general, I'm a little bit skeptical; GWR is the only method I have looked at specifically, but it seems to give contradictory answers perhaps because of its susceptibility to collinearity. I don't know what your particular application is, but in home price hedonics there has been some movement to autoregressive models that incorporate heterogeneity sympathetically (like the spatial Durbin model).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-05-08T17:39:36.687" Id="58496" LastActivityDate="2013-05-08T17:39:36.687" OwnerUserId="10026" ParentId="57357" PostTypeId="2" Score="0" />
  
  
  <row Body="&lt;p&gt;When using a nonlinear kernel $\alpha_i \alpha_j \langle x^{(i)},x^{(j)}\rangle\neq  \langle \alpha_i x^{(i)},\alpha_j x^{(j)}\rangle $. Keep in mind that the support values ($\alpha$'s) are part of the &lt;em&gt;solution&lt;/em&gt; of the dual problem. &lt;/p&gt;&#10;&#10;&lt;p&gt;For nonlinear kernels, the training outcome of $\alpha_i \alpha_j \langle x^{(i)},x^{(j)}\rangle$ is completely different from $\langle \alpha_i x^{(i)},\alpha_j x^{(j)}\rangle$. In fact, for nonlinear kernels, the optimization problem you would get when using $\langle \alpha_i x^{(i)},\alpha_j x^{(j)}\rangle$ is no longer a QP and is probably intractable. There is really no reason for doing this.&lt;/p&gt;&#10;&#10;&lt;p&gt;Take an RBF kernel for example, with $\kappa(x^{(i)},x^{(j)})=e^{-\gamma \|x^{(i)}-x^{(j)}\|^2}$. In your idea, this becomes:&#10;$$\kappa(\alpha_i x^{(i)},\alpha_jx^{(j)})=e^{-\gamma \|\alpha_i x^{(i)}-\alpha_j x^{(j)}\|^2}.$$&#10;You can easily see that solving this to $\alpha$ (and possible $b$) is &lt;em&gt;much&lt;/em&gt; more complex!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-05-09T09:42:53.933" Id="58551" LastActivityDate="2013-05-09T09:58:09.277" LastEditDate="2013-05-09T09:58:09.277" LastEditorUserId="25433" OwnerUserId="25433" ParentId="58539" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;I am solving a classification problem using Random Forests. For that I have decided to use Python library scikit-learn. But I am new to both Random Forest algorithm and this tool. My data contains many factor variables. I googled for that and found out that it's not right to give numerical values to factor variables like we do in linear regression, as it will treat it as continuous variable and give wrong result. But I could not find anything about how to deal with factor variables in scikit-learn. Please tell me the options to use or point me to some document where I can get it.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-05-09T15:47:19.997" Id="58576" LastActivityDate="2013-06-18T08:53:11.073" OwnerUserId="25373" PostTypeId="1" Score="0" Tags="&lt;python&gt;&lt;random-forest&gt;&lt;scikit-learn&gt;" Title="What is the way to represent factor variables in scikit-learn while using Random Forests?" ViewCount="1654" />
  <row Body="&lt;p&gt;Basically, yes.  Your DGP is a little weird.  Normally, you would not both subscript the $Y_i^C$ and add and error.  I'm going to take away the subscript on the $Y^C$.  Observe:&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{align}&#10;Y_i &amp;amp;= Y^C \cdot exp(T_i\beta) \cdot \epsilon_i\\&#10;ln(Y_i) &amp;amp;= ln(Y^C) + T_i \beta + \epsilon_i&#10;\end{align}&lt;/p&gt;&#10;&#10;&lt;p&gt;That's just a regression equation (and, it's true since it is derived from the assumed true DGP by a legal step of algebra).  $T$ and $\epsilon$ are uncorrelated as long as the randomization was not broken.  That suffices to get $\hat{\beta}_{OLS}$ to be unbiased, consistent, and asymptotically normal (subject to some weak regularity conditions).  The only worry you have left is whether the usual OLS standard errors are right.  If it's reasonable in your application to assume that $\epsilon$ is homoskedastic and not serially correlated then the usual OLS standard errors are right.  That is, yes, just run a regression and use the default standard errors to make confidence intervals.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you think $\epsilon$ might be heteroskedastic, you can either use Huber-White (aka heteroskedasticity robust) standard errors or you can use case bootstrapping to calculate standard errors.  And you might as well just use BCa confidence intervals if you are going to bootstrap.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-09T16:17:15.007" Id="58579" LastActivityDate="2013-05-09T16:17:15.007" OwnerUserId="25212" ParentId="45895" PostTypeId="2" Score="1" />
  
  
  
  <row AcceptedAnswerId="58608" AnswerCount="1" Body="&lt;p&gt;Carlin (2005) points out that mixed effects models specifically for twin data can be simplified by calculating differences between paired clusters. This allows for modeling specifically the within cluster effects and cancelling out any between cluster effects.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://ije.oxfordjournals.org/content/34/5/1089.short&quot; rel=&quot;nofollow&quot;&gt;http://ije.oxfordjournals.org/content/34/5/1089.short&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The mixed model:&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{equation}&#10;E(Y_{ij} | X_{ij}) = \beta_0 + \beta_W (X_{ij} - \bar{X}_{i\cdot}) + \beta_B\bar{X}_{i\cdot}&#10;\end{equation}&lt;/p&gt;&#10;&#10;&lt;p&gt;Becomes&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{equation}&#10;E(D_i^{Y}) = \beta_W D_i^{X}&#10;\end{equation}&lt;/p&gt;&#10;&#10;&lt;p&gt;with $D_{i}^X = X_{i1} - X_{i2}$&lt;/p&gt;&#10;&#10;&lt;p&gt;The results of this analysis depends on how you code twins as 1 and 2. The author simply states, &quot;The oldest twin is chosen as the referent [i=1] twin&quot; without mentioning whether it is a global rule or a deliberate choice for this analysis. I think there must be some kind of causal framework aspect to this. It seems like, for some analyses (especially post-infancy), the choice of order could be different and this could give vastly different results. &lt;/p&gt;&#10;&#10;&lt;p&gt;How do you chose who is twin #1?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-09T21:02:12.510" Id="58604" LastActivityDate="2013-05-09T23:09:22.167" OwnerUserId="8013" PostTypeId="1" Score="2" Tags="&lt;causal-inference&gt;&lt;twin&gt;" Title="Choice of referent twin in twin difference model" ViewCount="30" />
  <row Body="&lt;p&gt;You can possibly start by looking at one of my answers here: &lt;br /&gt;&#10;&lt;a href=&quot;http://stats.stackexchange.com/questions/43779/non-linear-svm-classification-with-rbf-kernel/43806#43806&quot;&gt;Non-linear SVM classification with RBF kernel&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In that answer, I attempt to explain what a kernel function is attempting to do. Once you get a grasp of what it attempts to do, as a follow up, you can read my answer to a question on Quora :&#10;&lt;a href=&quot;https://www.quora.com/Machine-Learning/Why-does-the-RBF-radial-basis-function-kernel-map-into-infinite-dimensional-space/answer/Arun-Iyer-1&quot; rel=&quot;nofollow&quot;&gt;https://www.quora.com/Machine-Learning/Why-does-the-RBF-radial-basis-function-kernel-map-into-infinite-dimensional-space/answer/Arun-Iyer-1&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Reproducing the content of the answer on Quora, in case you don't have a Quora account. &lt;br /&gt;&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Question: Why does the RBF (radial basis function) kernel map into infinite dimensional space?&#10;  Answer : Consider the polynomial kernel of degree 2 defined by, $$k(x, y) =&#10; (x^Ty)^2$$ where $x, y \in \mathbb{R}^2$ and $x = (x_1, x_2), y = (y_1, y_2)$.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;Thereby, the kernel function can be written as, $$k(x, y) =&#10; (x_1y_1 + x_2y_2)^2 = x_{1}^2y_{1}^2 + 2x_1x_2y_1y_2 +&#10; x_{2}^2y_{2}^2$$ Now, let us try to come up with a feature map&#10;  $\Phi$ such that the kernel function can be written as&#10;  $k(x, y) = \Phi(x)^T\Phi(y)$.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;Consider the following feature map, $$\Phi(x) = (x_1^2,&#10; \sqrt{2}x_1x_2, x_2^2)$$ Basically, this feature map is mapping&#10;  the points in $\mathbb{R}^2$ to points in&#10;  $\mathbb{R}^3$. Also, notice that, $$\Phi(x)^T\Phi(y) =&#10; x_1^2y_1^2 + 2x_1x_2y_1y_2 + x_2^2y_2^2$$ which is essentially&#10;  our kernel function.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;This means that our kernel function is actually computing the&#10;  inner/dot product of points in $\mathbb{R}^3$. That is, it&#10;  is implicitly mapping our points from $\mathbb{R}^2$ to&#10;  $\mathbb{R}^3$.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;&lt;strong&gt;Exercise Question&lt;/strong&gt; : If your points are in $\mathbb{R}^n$, a&#10;  polynomial kernel of degree 2 will map implicitly map it to some&#10;  vector space F. What is the dimension of this vector space F? Hint:&#10;  Everything I did above is a clue.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;Now, coming to RBF.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;Let us consider the RBF kernel again for points in&#10;  $\mathbb{R}^2$. Then, the kernel can be written as&#10;  $$k(x, y) = \exp(-\|x - y\|^2) = \exp(- (x_1 - y_1)^2 - (x_2 - y_2)^2)$$ $$= \exp(- x_1^2 + 2x_1y_1 - y_1^2 - x_2^2 + 2x_2y_2 - y_2^2) $$ $$ = \exp(-\|x\|^2) \exp(-\|y\|^2) \exp(2x^Ty)$$ (assuming gamma = 1). Using the&#10;  taylor series you can write this as, $$k(x, y) = \exp(-\|x\|^2) \exp(-\|y\|^2) \sum_{n = 0}^{\infty} \frac{(2x^Ty)^n}{n!}$$ Now,&#10;  if we were to come up with a feature map $\Phi$ just like&#10;  we did for the polynomial kernel, you would realize that the feature&#10;  map would map every point in our $\mathbb{R}^2$ to an&#10;  infinite vector. Thus, RBF implicitly maps every point to an infinite&#10;  dimensional space.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;&lt;strong&gt;Exercise Question&lt;/strong&gt; : Get the first few vector elements of the feature&#10;  map for RBF for the above case?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Now, from the above answer, we can conclude something:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;It may be quite hard to predict in general what the mapping function&#10;$\Phi$ looks like for arbitrary kernel. Though, for some cases like&#10;polynomial and RBF we can see what it looks like. &lt;/li&gt;&#10;&lt;li&gt;Even when we know the mapping function, the exact effect that kernel will have on our set of points may be hard to predict. However, in certain cases we can say some things. For example, look at the $\Phi$ map given above for degree 2 polynomial kernel for $\mathbb{R}^2$. It looks like $\Phi(x) = (x_1^2,&#10; \sqrt{2}x_1x_2, x_2^2)$. From this we can determine that this map collapses diametrically opposite quadrants i.e first and third quadrant are mapped to same set of points and second and fourth quadrant are mapped to the same set of points. Therefore, this kernel allows us to solve XOR problem! In general, however, it might be harder to predict such behaviour for multidimensional spaces. And it gets harder in the case of RBF kernels.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2013-05-09T23:06:14.557" Id="58607" LastActivityDate="2013-05-09T23:06:14.557" OwnerUserId="10823" ParentId="58585" PostTypeId="2" Score="4" />
  <row AnswerCount="0" Body="&lt;p&gt;I've got some code that does a Dept of Defense secure erase of disks,&#10;and would like to know about a good sanity check to run after the fact.&lt;/p&gt;&#10;&#10;&lt;p&gt;Bottom line, the last write phase of a secure erase of a HDD is to write random&#10;data on every byte on the HDD, per the spec.  The official standard does not put any constraints on it beyond that.  So I use a decent cryptographic algorithm to write data ... but that is not an issue here.&lt;/p&gt;&#10;&#10;&lt;p&gt;What I would really like to determine is what a good sanity check is on a HDD that has been erased.  As the spec does not allow one to add any meaning to what is written on the disk, is there a way to assess the probability that the data is random?  &lt;/p&gt;&#10;&#10;&lt;p&gt;I read every byte on the surface and create 256 buckets for each possible byte value, and currently count the number of consecutive bytes, and also look at standard deviation to see if  any of the byte values are significantly different than others. With that in mind, what is a reasonable # of consecutive bytes for a HDD of X bytes to consider that results are 'inconclusive', or the data might have been tampered with? Same for std deviation, and is there a better model?   (Yes, I know somebody could still hide data all over the place if they wanted to, I'm just looking for a way to detect if the process may not have completed, or perhaps the very beginning or end of the disk were never randomized)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-10T03:22:03.133" Id="58618" LastActivityDate="2013-05-10T03:22:03.133" OwnerUserId="25472" PostTypeId="1" Score="4" Tags="&lt;randomness&gt;" Title="Sanity check to determine if 'random' data on disk drive may have been tampered with" ViewCount="89" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a data set $Y$, each element $y_i$ in $Y$ has the  covariate $\lambda_i$. Here, I want to use Poisson regression to fit a model: &lt;code&gt;glm(Y ~ $\lambda$, family = &quot;poisson&quot;, data = ..)&lt;/code&gt;. Then given a new observe $y_k$ and the covariate $\lambda_k$, suppose the new data follow the fitted model, how can I use R to calculate the probability of observing $y_k$ and $\lambda_k$.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-05-10T03:57:46.147" Id="58620" LastActivityDate="2013-05-10T14:07:37.593" LastEditDate="2013-05-10T14:07:37.593" LastEditorUserId="22062" OwnerUserId="22062" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;poisson&gt;" Title="How to calculate the predicted probability of poisson regression model" ViewCount="241" />
  
  
  <row AcceptedAnswerId="58631" AnswerCount="1" Body="&lt;p&gt;I want to see if a vendor is biased against one type of vehicle compared to another.&#10;I have two categories of vehicles: OWN and T/C.&#10;The vendor sells fuel to these vehicles. &lt;/p&gt;&#10;&#10;&lt;p&gt;The process is as follows:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Vehicle sends REQUEST to vendor.&lt;/li&gt;&#10;&lt;li&gt;Vendor may choose not to act on this request&lt;/li&gt;&#10;&lt;li&gt;If vendor chooses to act he will respond with an OFFER.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I have the number of REQUESTS (R_o / R_t) the supplier has received from each of the vehicle categories, and the number of OFFERS (O_o / O_t) he has responded with.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am basically trying to figure out if a given vendor is biased against a T/C vehicle compared to an OWN vehicle on the basis of categories. I am slightly familiar with the Chi-square dependence check, and it looks like that may be the best approach, but I wasn't confident enough in my own knowledge of it to implement it without consulting more knowledgeable people first. I hope you have time to help me out with this.&lt;/p&gt;&#10;&#10;&lt;p&gt;Chi-square calculation:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;     Observed:&#10;              T/C    OWN     Total&#10; No Offers    54     328      382&#10;Got Offers    22     230      252&#10;     Total    76     558      634&#10;&#10;      Expected:&#10;              T/C    OWN    Total&#10; No Offers    45.79  336.21  382&#10;Got Offers    30.21  221.79  252&#10;     Total    76     558     634&#10;&#10;&#10;            X^2-calc: (O-E)^2/E&#10;               1.45   0.20&#10;               2.26   0.30     X^2&#10;                               4.21&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2013-05-10T06:54:36.563" Id="58628" LastActivityDate="2013-05-13T08:51:07.123" LastEditDate="2013-05-11T04:00:26.103" LastEditorUserId="183" OwnerUserId="25474" PostTypeId="1" Score="3" Tags="&lt;chi-squared&gt;&lt;independence&gt;" Title="Optimal approach to independence check" ViewCount="104" />
  
  
  <row Body="&lt;p&gt;There'd be several things you could do, but it sounds like perhaps a Generalized Linear Mixed Model might be in order.&lt;/p&gt;&#10;&#10;&lt;p&gt;The probability that a part is declared a failure would have a fixed effect (the machine) and a random effect (the part). You're not so much  interested in the differences in the parts, only accounting for their unknown (common) effect, and the typical amount of variation they contribute. You are interested in identifying differences in the machines.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Generalized_linear_mixed_model&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Generalized_linear_mixed_model&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;There are various packages that can fit these.&lt;/p&gt;&#10;&#10;&lt;p&gt;If I've understood the situation right, you wouldn't expect any kind of machine-part interaction, and if I remember how glmms work, you could do it this way in R, using the &lt;code&gt;lme4&lt;/code&gt; package:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;testing.glmm1 &amp;lt;- lmer(TestResult ~ Machine + (1|Part), family=&quot;binomial&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;where Machine and Part are factors (the first fixed, the second random), and TestResult is the 0/1 Fail/Pass response on each test on each machine. You'd then (firstly, at least) look for whether there's a Machine effect, since you're interested in detecting whether the alternative (that some machines differ in their underlying Pass/Fail rates) is the case.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you haven't used GLMs it might pay to learn something about those first.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-10T10:34:23.653" Id="58637" LastActivityDate="2013-05-10T10:34:23.653" OwnerUserId="805" ParentId="58595" PostTypeId="2" Score="1" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm using a daily time series of sales data that contains about 2 years of daily data points. Based on some of the online-tutorials / examples I tried to identify the seasonality in the data. It seems that there is a weekly, monthly and probably a yearly periodicity / seasonality.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, there are paydays, particularly on 1st payday of the month effect that lasts for few days during the week. There are also some specific Holiday effects, clearly identifiable by noting the observations.&lt;/p&gt;&#10;&#10;&lt;p&gt;Equipped with some of these observations, I tried the following:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;ARIMA (with &lt;code&gt;Arima&lt;/code&gt; and &lt;code&gt;auto.arima&lt;/code&gt; from R-forecast package), using regressor (and other default values needed in the function).  The regressor I created is basically a matrix of 0/1 values:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;11 month (n-1) variables&lt;/li&gt;&#10;&lt;li&gt;12 holiday variables&lt;/li&gt;&#10;&lt;li&gt;Could not figure out the payday part...since it's little more complicated effect than I thought. The payday effect works differently, depending on the weekday of the 1st of month.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I used 7 (i.e., weekly frequency) to model the time series. I tried the test - forecasting 7 days at a time. The results are reasonable: average accuracy for a forecast of 11 weeks comes to weekly avg RMSE to 5%.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;TBATS model (from R-forecast package) - using multiple seasonality (7, 30.4375, 365.25) and obviously no regressor. The accuracy is surprisingly better than the ARIMA model at weekly avg RMSE 3.5% .&lt;/p&gt;&#10;&#10;&lt;p&gt;In this case, the model without ARMA errors perform slightly better. Now If I apply the coefficients for just the Holiday Effects from the ARIMA model described in #1, to the results of the TBATS model the weekly avg RMSE improves to 2.95%&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Now without having much background or knowledge on the underlying theories of these models, I'm in a dilemma whether this TBATS approach is even a valid one. Even though it's improving the RMSE significantly in the 11 weeks test, I'm wondering whether it can sustain this accuracy in the future. Or even if applying Holiday effects from ARIMA to the TBATS result is justifiable. Any thoughts from any / all the contributors will be highly appreciated. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;https://s3.amazonaws.com/CKI-FILE-SHARE/TS+Test+Data.txt&quot;&gt;Link for Test Data&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Note: Do &quot;Save Link As&quot;, to download the file.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-05-10T15:31:40.723" FavoriteCount="5" Id="58657" LastActivityDate="2014-09-12T19:48:11.593" LastEditDate="2013-05-10T17:04:01.780" LastEditorUserId="25488" OwnerUserId="25488" PostTypeId="1" Score="6" Tags="&lt;r&gt;&lt;time-series&gt;&lt;forecasting&gt;&lt;arima&gt;" Title="Time Series Forecasting with Daily Data: ARIMA with regressor" ViewCount="2513" />
  
  <row Body="&lt;p&gt;Thanks for the help.  @whuber pointed me to a post &lt;a href=&quot;http://stats.stackexchange.com/questions/14483/intuitive-explanation-for-density-of-transformed-variable/14490#14490&quot;&gt;discussing changing variables&lt;/a&gt;, which lead me to read &lt;a href=&quot;http://en.wikipedia.org/wiki/Probability_density_function#Dependent_variables_and_change_of_variables&quot; rel=&quot;nofollow&quot;&gt;a bit on wikipedia&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;a change in variables is defined as:&#10;$f_Y(y) = \left| \frac{d}{dy} (g^{-1}(y)) \right| \cdot f_X(g^{-1}(y))$&lt;/p&gt;&#10;&#10;&lt;p&gt;For the example above:&lt;/p&gt;&#10;&#10;&lt;p&gt;$Y=2 r sin(Θ/2)$&lt;/p&gt;&#10;&#10;&lt;p&gt;$Θ=2 arcsin{\frac{Y}{2r}}$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\frac{dΘ}{dY}=\frac{2}{r\sqrt{4-\frac{Y^2}{r^2}}}$,  $f(θ)=1/2π$&lt;/p&gt;&#10;&#10;&lt;p&gt;so $f_Y(y)=\frac{1}{r\pi\sqrt{4-\frac{Y^2}{r^2}}}$&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-10T19:53:02.473" Id="58674" LastActivityDate="2013-05-10T19:53:02.473" OwnerUserId="25420" ParentId="58532" PostTypeId="2" Score="0" />
  <row AnswerCount="0" Body="&lt;p&gt;I &lt;a href=&quot;http://stats.stackexchange.com/questions/58532/can-i-derive-the-pdf-fc-from-a-spatial-function-defined-as-cx&quot;&gt;now understand how to conduct a change of variables&lt;/a&gt; for a marginal PDF.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, given two functions that define parameter's spatially: $C_A(x)$ and $C_B(x)$, is it possible to construct the Joint PDF, $f(C_A,C_B)$?&lt;/p&gt;&#10;&#10;&lt;p&gt;Note: if C_A and C_B are independent, I understand that $f(C_A,C_B)=f(C_A)f(C_B)$.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-05-10T20:05:36.413" FavoriteCount="1" Id="58675" LastActivityDate="2013-05-10T20:05:36.413" OwnerUserId="25420" PostTypeId="1" Score="0" Tags="&lt;correlation&gt;&lt;multivariate-analysis&gt;&lt;pdf&gt;&lt;bivariate&gt;" Title="Joint PDF change of variables" ViewCount="59" />
  
  <row Body="&lt;p&gt;I think when you allow for unknown dispersion, the GLM is no longer a maximum likelihood technique, but maximizes a &lt;a href=&quot;http://en.wikipedia.org/wiki/Quasi-likelihood&quot; rel=&quot;nofollow&quot;&gt;&quot;quasi&quot; likelihood&lt;/a&gt;. Because of that, the deviance is fixed by using the sample dispersion as the model dispersion (as a consequence of maximizing quasi likelihood). By treating the dispersion as a parameter in a quasilikelihood, the family of e.g. quasibinomial likelihoods are equivalent to the binomial likelihood up to a proportional constant. Maximizing quasilikelihood treats this constant like a nuisance parameter.&lt;/p&gt;&#10;&#10;&lt;p&gt;Think of it like this: when the probability model for the underlying GLM is correct then the sample deviance will have an expected value of 1 (as confirmed by your formulation and limiting distribution statement above). But random variation in that data will show that the probability model does not always perfectly fit such data.&lt;/p&gt;&#10;&#10;&lt;p&gt;When the sample deviance is egregiously different from such, this is indication that the working probability model is not a good probability framework for the observed data. This doesn't mean that the inference on parameters is incorrect. In fact, by using scaled deviance, you can account for this over or under dispersion in the working GLM and get correct inference on the parameters. This is the GLM obtained from maximum quasilikelihood. &lt;/p&gt;&#10;&#10;&lt;p&gt;I recommend looking at Alan Agresti's example of horseshoe crabs and quasipoisson and quasibinomial models in Categorical Data Analysis 2nd ed for further clarification.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-05-10T20:53:13.000" Id="58681" LastActivityDate="2013-05-14T19:39:25.553" LastEditDate="2013-05-14T19:39:25.553" LastEditorUserId="8013" OwnerUserId="8013" ParentId="58679" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="61345" AnswerCount="2" Body="&lt;p&gt;L2 regularized logistic regression differs with L2 regularized support vector machine with their loss function. Are there more deep differences for these two models? I tried several data sets, and found L2 regularized logistic regression are always better than L2 regulared support vector machine. Are there any references/researches for discussion their differences in terms of classification performance?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-10T21:23:57.417" FavoriteCount="2" Id="58684" LastActivityDate="2013-06-10T13:42:55.983" LastEditDate="2013-06-10T13:42:55.983" LastEditorUserId="88" OwnerUserId="3269" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;classification&gt;&lt;svm&gt;&lt;libsvm&gt;" Title="Regularized logistic regression and support vector machine" ViewCount="684" />
  
  <row AnswerCount="0" Body="&lt;p&gt;There is a question &lt;a href=&quot;http://stats.stackexchange.com/questions/5450/what-if-interaction-wipes-out-my-direct-effects-in-regression&quot;&gt;What if interaction wipes out my direct effects in regression?&lt;/a&gt;. An answer was given that the true main effects are in the model without the interaction.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have the opposite situation. Main effects are not significant in step 1. A main effect is significant in step 2 after adding the interaction, but the interaction is not significant.&#10;Can I rightfully assume this is not a true main effect?  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;DV: guilt&#10;Step 1: Moral conviction (MC) + Speaking out (SO) &#10;Step 2: MC + SO + MCxSO&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="5" CreationDate="2013-05-11T01:25:08.913" Id="58692" LastActivityDate="2013-05-11T03:51:50.073" LastEditDate="2013-05-11T03:51:50.073" LastEditorUserId="183" OwnerUserId="25502" PostTypeId="1" Score="1" Tags="&lt;multiple-regression&gt;&lt;interaction&gt;" Title="Is it a true main effect if it is significant only when interaction is included?" ViewCount="192" />
  <row Body="&lt;p&gt;If your data are balanced, the sum contrasts (see &lt;code&gt;contr.sum&lt;/code&gt; under &lt;code&gt;?contrasts&lt;/code&gt;) are explicitly the differences you're asking about, so you may want to look at testing those.&lt;/p&gt;&#10;&#10;&lt;p&gt;More generally, one problem is that &quot;the mean of all races&quot; includes the present race you're comparing it to, so you lose independence, which can make things more complex. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, note that if the present race has the same mean as the average of all the races apart from itself it has the same mean as all the races including itself. One way to achieve that would be to look at the last helmert contrast with the race you want to compare to the others set as the last category  (see &lt;code&gt;contr.helmert&lt;/code&gt;); that would require reordering the factor each time you wanted to test a race though.&lt;/p&gt;&#10;&#10;&lt;p&gt;You might also find the discussion of deviation coding &lt;a href=&quot;http://www.ats.ucla.edu/stat/r/library/contrast_coding.htm#DEVIATION&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; relevant.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-05-11T09:04:37.703" Id="58702" LastActivityDate="2013-05-11T09:04:37.703" OwnerUserId="805" ParentId="58700" PostTypeId="2" Score="2" />
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm trying to fit a Johnson Unbounded distribution to a set of financial data with kurtosis and skewness, and also outliers. I started using Maximum Likelihood Estimators (MLE) but one outlier has too much influence on the estimated parameters.&lt;/p&gt;&#10;&#10;&lt;p&gt;Please can you point me to more robust method to estimate the parameters, one where outliers don't have too much influence.&lt;/p&gt;&#10;&#10;&lt;p&gt;I found a &lt;a href=&quot;http://www.bobwheeler.com/statistics/Papers/JohnsonCurves.PDF&quot; rel=&quot;nofollow&quot;&gt;quantile estimator for the distribution parameters&lt;/a&gt;. Is this method more robust than MLE?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-05-11T15:35:23.223" Id="58719" LastActivityDate="2013-05-12T05:11:56.507" OwnerUserId="6103" PostTypeId="1" Score="1" Tags="&lt;distributions&gt;&lt;curve-fitting&gt;&lt;fitting&gt;" Title="Robust distribution fitting?" ViewCount="201" />
  <row Body="&lt;p&gt;I am inclined to say you are having heteroskedastically consistent SE within clusters by design (ie. the assumptions you are doing when using &lt;code&gt;lmer&lt;/code&gt;). That is because if you assumed otherwise you would have a saturated model. &lt;/p&gt;&#10;&#10;&lt;p&gt;In a saturated model you would practically assume separate error terms $\epsilon$ (some for cluster specific variance and one for a &quot;sample-wide&quot; error) and you would face unidentifiability:&#10;$y = X\beta + Z\gamma + \epsilon_{cluster_1} + \cdots + \epsilon_{cluster_i} + \epsilon_{whole sample}$ (not cool)&lt;/p&gt;&#10;&#10;&lt;p&gt;Assuming no &quot;sample-wide&quot; error is not feasible using the current &lt;code&gt;lmer&lt;/code&gt; implementation. &lt;code&gt;lmer&lt;/code&gt;'s fitting procedure does assume you are able to work with the relative precision matrix $\Omega$ (&lt;a href=&quot;http://www.stat.wisc.edu/~bates/reports/MixedComp.pdf&quot; rel=&quot;nofollow&quot;&gt;Bates and DebRoy 2003&lt;/a&gt; offer a more detailed analysis of the matter.)&lt;/p&gt;&#10;&#10;&lt;p&gt;(I am not 100% sure about my interpretation of your question, so I am open to discussion on the matter.) &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-05-11T20:35:17.953" Id="58743" LastActivityDate="2013-05-11T20:35:17.953" OwnerUserId="11852" ParentId="58724" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I think that the solution might depend on how you want to use the data. But I'll make a simple suggestion.&lt;/p&gt;&#10;&#10;&lt;p&gt;Perhaps the most simple approach would be to use a covariate to remove the seasonality.  Do you have a time series of a strong driver?  E.g., air temperature might correlate very well with sea surface temperature.  Regress water temperature against air temperature, and check to see if the residuals contain the appropriate pattern (or lack thereof). If this approach works for surface temperature, maybe it'll work equally well for subsurface temperatures. Obviously this method will not work for certain applications.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-11T20:35:29.277" Id="58744" LastActivityDate="2013-05-11T20:35:29.277" OwnerUserId="25184" ParentId="58708" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;After performing a factor analysis on a set of variables, I have one variable that loads equally on two factors.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;What should I do with this variable that loads equally on two factors?&lt;/li&gt;&#10;&lt;li&gt;Should I remove this variable from the factor analysis, and rerun the factor analysis?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="2" CreationDate="2013-05-12T01:35:05.157" Id="58757" LastActivityDate="2013-05-12T11:36:17.887" LastEditDate="2013-05-12T01:57:48.853" LastEditorUserId="183" OwnerUserId="25529" PostTypeId="1" Score="1" Tags="&lt;factor-analysis&gt;" Title="What to do with a variable that loads equally on two factors in a factor analysis?" ViewCount="2975" />
  <row AnswerCount="1" Body="&lt;p&gt;I am having a hard time to understand how one derives the jackknife bias for the variance and mean. &lt;/p&gt;&#10;&#10;&lt;p&gt;1) Why do we need an inflation factor of $(n-1)$ when calculating the jackknife bias of the mean?&lt;/p&gt;&#10;&#10;&lt;p&gt;2) How does one derive the jackknife bias for the variance? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-12T02:32:16.170" Id="58759" LastActivityDate="2014-04-19T18:41:26.450" LastEditDate="2014-04-19T18:41:26.450" LastEditorUserId="17908" OwnerUserId="25503" PostTypeId="1" Score="3" Tags="&lt;self-study&gt;&lt;nonparametric&gt;&lt;resampling&gt;&lt;jackknife&gt;" Title="How to derive jackknife bias for variance and mean" ViewCount="109" />
  <row Body="&lt;p&gt;Proportion classified correctly is an improper scoring rule, i.e., it is optimized by a bogus model.  I would use the quadratic proper scoring rule known as the Brier score, or the concordance probability (area under ROC curve in the binary $Y$ case).  Random forest works better than SVM in your case.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-12T02:46:56.910" Id="58761" LastActivityDate="2013-05-12T02:46:56.910" OwnerUserId="4253" ParentId="58756" PostTypeId="2" Score="5" />
  <row AnswerCount="0" Body="&lt;p&gt;I read some articles where theory of rough sets is also considered as data mining algorithm. Hovewer, I have not found so far any example when this theory may be useful in solving data mining problems. I am also wondering that there is no R package for rough sets manipulation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Could you give me some examples or situations, when rough sets are the most appropriate technique for analysis?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you in advance.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-05-12T10:23:59.700" Id="58771" LastActivityDate="2013-05-12T10:23:59.700" OwnerUserId="14730" PostTypeId="1" Score="0" Tags="&lt;data-mining&gt;" Title="How can rough sets be applied in data mining?" ViewCount="52" />
  <row Body="&lt;p&gt;Yes, there are fast ways to do those calculations.&lt;/p&gt;&#10;&#10;&lt;p&gt;if you look at &lt;code&gt;?influence.measures&lt;/code&gt; several of the calculations there get you most of the way there.&lt;/p&gt;&#10;&#10;&lt;p&gt;Starting from either the &lt;code&gt;dffits&lt;/code&gt; or &lt;code&gt;rstudent&lt;/code&gt; functions, you ought to be able to back out the $\hat y_{(i)}$ values.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, the dffits function should implement &lt;a href=&quot;http://en.wikipedia.org/wiki/DFFITS&quot; rel=&quot;nofollow&quot;&gt;this&lt;/a&gt; and the other components are pretty easily found.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you investigate the content of the function &lt;code&gt;lm.influence&lt;/code&gt; you might be able to do it a shorter way.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-05-12T11:47:28.343" Id="58776" LastActivityDate="2013-05-12T11:47:28.343" OwnerUserId="805" ParentId="58775" PostTypeId="2" Score="2" />
  
  
  
  <row AcceptedAnswerId="58798" AnswerCount="1" Body="&lt;p&gt;I am currently trying to build a model using a data set that has large gap between data points. When I look for the correlation I clearly see a negative regression line. But I am worried about the gap that exist between the poins. &lt;/p&gt;&#10;&#10;&lt;p&gt;I build a simple linear model though this has high R squared I don't think simple linear regression is the best model to that fits the data. This looks like it has a negative exponential behavior. I thought to post here to get some expert thoughts on what I should do when you deal with the data that has a large gap between points and does this data has a linear relationship or strong non linear relationship?&lt;/p&gt;&#10;&#10;&lt;p&gt;Data Set: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;   density  co2&#10;1     20.4 38.8&#10;2     27.4 31.5&#10;3    106.2 10.6&#10;4     80.4 16.1&#10;5    141.3  7.7&#10;6    130.9  8.3&#10;7    121.7  8.5&#10;8    106.5 11.1&#10;9    130.5  8.6&#10;10   101.1 11.1&#10;11   123.9  9.8&#10;12   144.2  7.8&#10;13    29.5 31.8&#10;14    30.8 31.6&#10;15    26.5 34.0&#10;16    35.7 28.9&#10;17    30.0 28.8&#10;18   106.2 10.5&#10;19    97.0 12.3&#10;20    90.1 13.2&#10;21   106.7 11.4&#10;22    99.3 11.2&#10;23   107.2 10.3&#10;24   109.1 11.4&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Plot:&#10;&lt;img src=&quot;http://i.stack.imgur.com/MWjgD.png&quot; alt=&quot;Simple Linear Regression Plot&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Summary of Linear Model:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Coefficients:&#10;            Estimate Std. Error t value Pr(&amp;gt;|t|)    &#10;(Intercept) 38.12948    1.21768   31.31  &amp;lt; 2e-16 ***&#10;density     -0.24247    0.01261  -19.22 3.04e-15 ***&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In addition if I transfer both density and co2 as log transform variables, then I see following behavior. Since data is missing at the middle its really hard to stick to a log transformed model or the base model. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/pPUZN.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2013-05-12T17:25:42.897" Id="58793" LastActivityDate="2013-05-13T06:21:41.013" LastEditDate="2013-05-12T17:38:50.917" LastEditorUserId="24853" OwnerUserId="24853" PostTypeId="1" Score="4" Tags="&lt;regression&gt;&lt;lognormal&gt;" Title="Model Building: Missing Data or Large Gap between data points" ViewCount="253" />
  
  
  <row Body="&lt;p&gt;Such issues are common in various areas requiring judgement.&lt;/p&gt;&#10;&#10;&lt;p&gt;The chapter on &quot;&lt;a href=&quot;http://robjhyndman.com/forecasting/contents/chap10/&quot; rel=&quot;nofollow&quot;&gt;Judgemental forecasting and adjustments&lt;/a&gt;&quot; in Makridakis, Wheelwright and Hyndman &lt;em&gt;Forecasting: Methods and applications&lt;/em&gt; has similar stories of expert judgement under-performing sometimes even very simple systems.&lt;/p&gt;&#10;&#10;&lt;p&gt;There's a paper (Dawes et al (1989) &quot;Clinical vs Actuarial Judgement&quot; &lt;em&gt;Science&lt;/em&gt;, Vol 243, No 4899, p1668-74) about the failures of expert judgement in the medical area against what it calls 'actuarial' methods - basically fairly simple statistical models.&lt;/p&gt;&#10;&#10;&lt;p&gt;On the other hand there's a paper in the actuarial literature about the 'noisiness' and inconsistency of expert judgement in a particular problem in &lt;em&gt;that&lt;/em&gt; area where expert judgement is often regarded by its practitioners as of paramount importance.&lt;/p&gt;&#10;&#10;&lt;p&gt;Makridakis et al discuss failures of expert judgement in many areas, as they relate to forecasting, and contains quite a bit of valuable advice.&lt;/p&gt;&#10;&#10;&lt;p&gt;And so on it goes. &lt;a href=&quot;http://en.wikipedia.org/wiki/List_of_cognitive_biases&quot; rel=&quot;nofollow&quot;&gt;Cognitive biases&lt;/a&gt; abound and human experts suffer from them along with everyone else.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-13T04:31:22.917" Id="58820" LastActivityDate="2013-06-20T01:28:37.700" LastEditDate="2013-06-20T01:28:37.700" LastEditorUserId="805" OwnerUserId="805" ParentId="23375" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;This is a rather broad question, but I will try to give in informal reply.&#10;Generally, these statements establish the fact that the exponential family is a &quot;well behaved&quot; parametric family of distributions. &lt;/p&gt;&#10;&#10;&lt;p&gt;The emphasis in the first is in that the exponential family satisfies the regularity conditions needed so that a uniformly most powerful test exist. This means there is a test statistic and a rejection region, that have a type I error rate no greater than $\alpha$, and no other statistic nor rejection region have more power than these.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now to the second. The Cramer-Rao bound, or the &quot;information bound&quot; states that any unbiased estimator, &lt;strong&gt;cannot&lt;/strong&gt; be arbitrarily precise, i.e., it's variance is bounded from below. So the novelty in the second statement is that, in those cases where the MLE is actually unbiased, it is also efficient in the sense it has the minimal possible variance. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-05-13T06:25:29.207" Id="58824" LastActivityDate="2013-05-13T06:25:29.207" OwnerUserId="6961" ParentId="58646" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="58851" AnswerCount="1" Body="&lt;p&gt;I have the following data,&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;These are the x and y values to find correlation &#10;x=115778,171235,1&#10;y_a=31920,49327,0&#10;y_b=83858,121908,1&#10;cor_a= 0.99947573036   // These are the correlation values using numpy (x,y_a)&#10;cor_b= 0.999915675755   // (x,y_b)&#10;test_statistic_a= 30.870014264  // test statistic value using formula&#10;test_statistic_b= 76.9983294567&#10;d_f=1  // degrees of freedom &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;How to find the p_value for the above data and test the hypothesis? using rpy2&lt;/p&gt;&#10;" ClosedDate="2013-05-13T11:49:00.927" CommentCount="4" CreationDate="2013-05-13T07:33:53.840" Id="58826" LastActivityDate="2013-05-13T11:45:57.437" LastEditDate="2013-05-13T08:40:02.367" LastEditorUserId="25566" OwnerUserId="25566" PostTypeId="1" Score="-1" Tags="&lt;r&gt;&lt;python&gt;" Title="p value for the test statistic in python using rtool" ViewCount="303" />
  <row Body="&lt;p&gt;glm  requires that all variables be numeric.  Typically, you convert categorical variables to numeric variables using &lt;a href=&quot;http://en.wikipedia.org/wiki/Dummy_variable_%28statistics%29&quot; rel=&quot;nofollow&quot;&gt;dummy variables&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-05-13T14:04:20.027" Id="58862" LastActivityDate="2013-05-13T14:47:03.080" LastEditDate="2013-05-13T14:47:03.080" LastEditorUserId="2817" OwnerUserId="2817" ParentId="58861" PostTypeId="2" Score="0" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have an empirical estimate of a ROC curve, that is, a plot of the sensitivity versus 1-specificity over all possible cut-off values of the marker.&#10;Based on an empirical ROC curve, I would like to determine the optimal cut-off point that represents a better trade-off between sensitivity and specificity. I have read that the Youden Index can be used in that purpose.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is an example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;oneMinusSpecificity &amp;lt;- c(1.00000000, &#10;                         0.636363636,&#10;                         0.436363636,&#10;                         0.315151515,&#10;                         0.163636364,&#10;                         0.096969697,&#10;                         0.072727273,&#10;                         0.006060606,&#10;                         0.000000000)&#10;sensitivity &amp;lt;- c(1.00000000,&#10;                 0.91566265,&#10;                 0.77108434,&#10;                 0.66265060,&#10;                 0.39759036,&#10;                 0.33734940,&#10;                 0.28915663,&#10;                 0.07228916,&#10;                 0.00000000)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;which results in the following ROC curve. The vertical line represents the Youden index = largest distance between &quot;1 - specificity&quot; and &quot;sensitivity&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/FpnOk.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I find&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; youden &amp;lt;- max(sensitivity - oneMinusSpecificity)&#10;&amp;gt; youden&#10;[1] 0.3474991&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;How can I calculate the optimal cut-off using this information?&lt;/strong&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-05-13T15:03:09.997" FavoriteCount="2" Id="58868" LastActivityDate="2013-05-13T15:09:32.833" OwnerUserId="7064" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;roc&gt;" Title="Computing by hand the optimal threshold value for a biomarker using the Youden Index" ViewCount="582" />
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a $n\times n$ transition matrix where each value represents a node (of the tree) to node transition probability. I want to measure of possibility of occurrence of a tree according to the transition matrix. How can I do that?&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2013-05-13T19:14:50.573" Id="58905" LastActivityDate="2013-05-14T07:58:19.090" LastEditDate="2013-05-14T07:58:19.090" LastEditorUserId="805" OwnerUserId="22542" PostTypeId="1" Score="0" Tags="&lt;markov-process&gt;" Title="Measuring the possibility of occurrence of tree according to a transition matrix" ViewCount="34" />
  
  
  <row AcceptedAnswerId="58933" AnswerCount="2" Body="&lt;p&gt;I want to compare two percentages on a single categorical variable from one sample.&#10;For example, in my data set, a variable can take on two values (i.e. A and B). Of 586 instances, 87.4% is in Category A (512 of 586) and 12.6% is in Category B (74 of 586). &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Can I say 'my participants tend to be more concerned about Category A than B'? &lt;/li&gt;&#10;&lt;li&gt;Is a statistical test required to make such a claim? If so What statistical test is required?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2013-05-13T23:30:43.250" FavoriteCount="1" Id="58930" LastActivityDate="2013-05-14T08:11:15.807" LastEditDate="2013-05-14T01:34:48.480" LastEditorUserId="183" OwnerUserId="25612" PostTypeId="1" Score="3" Tags="&lt;statistical-significance&gt;" Title="How to compare percentages for two categories from one sample?" ViewCount="11323" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Say I am performing an analysis looking at a particular health measure. I am interested in the difference in that measure between patients and controls and wether or not the difference is different from 0. There have been studies in the past looking at my same research question and health measure but in different samples of patients.&lt;/p&gt;&#10;&#10;&lt;p&gt;In my Bayesian analysis I would build up a prior distribution based on the previous studies incorporating the mean difference and standard error. &lt;/p&gt;&#10;&#10;&lt;p&gt;Please forgive me if this a newbie question as I am newly learning Bayesian stats, but &lt;strong&gt;in what ways would the results from my Bayesian analysis be different from the results I would obtain from using an inverse variance weighted meta-analysis to combine the mean difference estimates from the prior studies with my current data&lt;/strong&gt;?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-05-14T02:56:29.983" FavoriteCount="2" Id="58939" LastActivityDate="2013-05-14T02:56:29.983" OwnerUserId="16286" PostTypeId="1" Score="6" Tags="&lt;bayesian&gt;&lt;meta-analysis&gt;&lt;case-control-study&gt;" Title="What is the difference between a frequentist approach with meta-analysis and a Bayesian approach?" ViewCount="219" />
  <row AnswerCount="0" Body="&lt;p&gt;I found a new use of a drug already in use. The results are as follows.&lt;/p&gt;&#10;&#10;&lt;p&gt;Serail No. of Patient-- Attacks while on Placebo--  Attacks while on drug   --Difference in the no. of attacks&lt;/p&gt;&#10;&#10;&lt;p&gt;1-   23-    2-  21&lt;/p&gt;&#10;&#10;&lt;p&gt;2-  17- 1-  16&lt;/p&gt;&#10;&#10;&lt;p&gt;3-  13- 3-  10&lt;/p&gt;&#10;&#10;&lt;p&gt;4-  26- 0-  26&lt;/p&gt;&#10;&#10;&lt;p&gt;5-  16- 0-  16&lt;/p&gt;&#10;&#10;&lt;p&gt;6-  11- 1-  10&lt;/p&gt;&#10;&#10;&lt;p&gt;7-  22- 3-  19&lt;/p&gt;&#10;&#10;&lt;p&gt;8-  15- 1-  14&lt;/p&gt;&#10;&#10;&lt;p&gt;9-  19- 2-  17&lt;/p&gt;&#10;&#10;&lt;p&gt;10- 16- 1-  15&lt;/p&gt;&#10;&#10;&lt;p&gt;11- 22- 1-  21&lt;/p&gt;&#10;&#10;&lt;p&gt;12- 27- 0-  27&lt;/p&gt;&#10;&#10;&lt;p&gt;13- 31- 3-  28&lt;/p&gt;&#10;&#10;&lt;p&gt;14- 24- 2-  22&lt;/p&gt;&#10;&#10;&lt;p&gt;15- 22- 1-  21&lt;/p&gt;&#10;&#10;&lt;p&gt;16- 19- 2-  17&lt;/p&gt;&#10;&#10;&lt;p&gt;17- 16- 1-  15&lt;/p&gt;&#10;&#10;&lt;p&gt;18- 25- 3-  22&lt;/p&gt;&#10;&#10;&lt;p&gt;19- 19- 1-  18&lt;/p&gt;&#10;&#10;&lt;p&gt;20- 21- 2-  19&lt;/p&gt;&#10;&#10;&lt;p&gt;21- 6-  6-  0&lt;/p&gt;&#10;&#10;&lt;p&gt;22- 28- 3-  25&lt;/p&gt;&#10;&#10;&lt;p&gt;23- 19- 1-  18&lt;/p&gt;&#10;&#10;&lt;p&gt;24- 21- 2-  19&lt;/p&gt;&#10;&#10;&lt;p&gt;25- 27- 1-  26&lt;/p&gt;&#10;&#10;&lt;p&gt;26- 20- 1-  19&lt;/p&gt;&#10;&#10;&lt;p&gt;27- 17- 1-  16&lt;/p&gt;&#10;&#10;&lt;p&gt;28- 25- 1-  24&lt;/p&gt;&#10;&#10;&lt;p&gt;29- 18- 1-  17&lt;/p&gt;&#10;&#10;&lt;p&gt;30- 19- 1-  18&lt;/p&gt;&#10;&#10;&lt;p&gt;31- 5-  5-  0&lt;/p&gt;&#10;&#10;&lt;p&gt;32- 17- 1-  16&lt;/p&gt;&#10;&#10;&lt;p&gt;33- 28- 1-  27&lt;/p&gt;&#10;&#10;&lt;p&gt;34- 4-  4-  0&lt;/p&gt;&#10;&#10;&lt;p&gt;I am sorry I am not able to format the table correctly. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have very basic knowledge of SPSS. Kindly advise how to go about.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-05-14T05:28:37.433" Id="58947" LastActivityDate="2013-05-14T14:18:50.907" LastEditDate="2013-05-14T14:18:50.907" LastEditorUserId="2956" OwnerUserId="2956" PostTypeId="1" Score="0" Tags="&lt;spss&gt;&lt;observational-study&gt;&lt;crossover-study&gt;&lt;double-blind&gt;" Title="A randomized, double-blind crossover (AB/BA) observational study of new use of an FDA approved drug" ViewCount="252" />
  
  <row Body="&lt;p&gt;It's possible to get high point biserial correlation even with 27 $1$'s and a $0$. Indeed, you can get as high as 1, so it's not that:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;  y &amp;lt;- c(0,rep(1,27))&#10;  x &amp;lt;- y&#10;  cor(x,y)&#10;[1] 1&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;-- and making x continuously distributed doesn't substantively alter that conclusion:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;  y &amp;lt;- c(0,rep(1,27))&#10;  x=c(rnorm(1,0),rnorm(27,100))&#10;  cor(x,y)&#10;[1] 0.9987537&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This corresponds to a $t$ statistic of 102.036 on 26 d.f.&lt;/p&gt;&#10;&#10;&lt;p&gt;On the other hand, see the advice in the wikipedia page on the &lt;a href=&quot;http://en.wikipedia.org/wiki/Point-biserial_correlation_coefficient&quot; rel=&quot;nofollow&quot;&gt;point-biserial&lt;/a&gt;:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;One disadvantage of the point biserial coefficient is that the further the distribution of Y is from 50/50, the more constrained will be the range of values which the coefficient can take. If X can be assumed to be normally distributed, a better descriptive index is given by the biserial coefficient&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Since we saw it's possible to get a high correlation, you might wonder what the restriction can be. I assume that they mean &quot;if we restrict $x$ to be normally distributed&quot; -- because then it can be limited:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; x &amp;lt;- sort(rnorm(28,100))&#10; (rpb &amp;lt;- cor(x,y))&#10;[1] 0.4682661&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Much smaller. But still potentially significant:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; pt(-rpb*sqrt(26/(1-rpb^2)),26)*2&#10;[1] 0.01196686&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;A thousand simulations suggest that when the smallest x goes with the lone zero, the correlation is mostly between 0.3 and 0.5, and about 60% of the values are significant at the 5% level.&lt;/p&gt;&#10;&#10;&lt;p&gt;So if the continuous variable is restricted to have a normal margin, even &lt;em&gt;then&lt;/em&gt; the correlation - while typically a lot smaller than what it otherwise possible, can still be significant.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-14T11:53:48.397" Id="58970" LastActivityDate="2013-05-14T12:33:22.327" LastEditDate="2013-05-14T12:33:22.327" LastEditorUserId="805" OwnerUserId="805" ParentId="58967" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;In my data the dependent variable ‘vocabulary uptake’ is measured as scores ranging from 0-10 and the independent variable is yielded from a ‘reading intervention’ with 4 different reading conditions (glossed, no glosses, etc.). There are about 80 participants in each of the four groups. The data is not normally distributed (I tried to transform the data). Given the fact that there is a natural limit (0), this is probably not surprising. I want to find out what the best way is to determine whether these groups are different or not.&lt;/p&gt;&#10;&#10;&lt;p&gt;My questions: &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;I am sure that the independent is a ‘nominal’ variable, but I am in doubt about the dependent – I cannot decide whether to treat it as ‘ratio’ data or as ‘ordinal’. The latter would make sense, as the scores can be ranked, but ‘discrete ratio’ makes sense as the values are measured, have numerical value, the difference between, e.g. 1 and 2 is the same as between 3 and 4, and a score of 0 means that there is none of that variable. ‘Ordinal’ data would be easy, but if I categorize the data as ‘discrete ratio’, my problem is that – to my knowledge – the standard nonparametric tests only work with continuous data. What can I do?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;What test + post-hoc test could I use to find out if there are significant differences between the groups? When working with a nominal dep. and a discrete ratio data indep. variable I would violate the assumptions for most nonparametric standard tests, which assume dichotomous independents and continuous/ordinal dep. variables (Mann-Whitney U, Wilcoxon) or categorical data for the independent variable and continuous/ordinal data for the independent variable (Kruskal-Wallis, Friedman’s test). Also there is a problem with tied values. &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I am working with SPSS.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for any suggestions! Your help would be GREATLY appreciated!&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-05-14T13:05:16.583" Id="58979" LastActivityDate="2014-07-21T07:24:49.343" LastEditDate="2013-05-15T09:12:52.853" LastEditorUserId="25630" OwnerUserId="25630" PostTypeId="1" Score="2" Tags="&lt;nonparametric&gt;&lt;group-differences&gt;" Title="How to test group difference between nominal and ratio data with nonparametric tests?" ViewCount="539" />
  
  <row AcceptedAnswerId="91787" AnswerCount="4" Body="&lt;p&gt;I write this question with reference to an example on p138-142 of the following document: &lt;a href=&quot;ftp://ftp.software.ibm.com/software/analytics/spss/documentation/amos/20.0/en/Manuals/IBM_SPSS_Amos_User_Guide.pdf&quot; rel=&quot;nofollow&quot;&gt;ftp://ftp.software.ibm.com/software/analytics/spss/documentation/amos/20.0/en/Manuals/IBM_SPSS_Amos_User_Guide.pdf&lt;/a&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;Here are illustrative figures and a table:&#10;&lt;img src=&quot;http://i.imgur.com/Rmayocw.png&quot; alt=&quot;CFA example&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I understand that the latent variable has no natural metric and that setting a factor loading to 1 is done to fix this problem. However, there are a number of things I don't (fully) understand:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;How does fixing a factor loading to 1 fix this indeterminacy of scale problem?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Why fix to 1, rather than some other number?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;I understand that by fixing one of the factor-&gt;indicator regression weights to 1 we thus make all the other regression weights for that factor relative to it. But what happens if we set a particular factor loading to 1 but then it turns out that the higher scores on the factor predict lower scores on the observed variable in question? After we've initially set the factor loading to 1 can we get to a negative understandardized regression weight, or to a negative standardized regression weight?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;In this context I've seen factor loadings referred to both as regression coefficients and as covariances. Are both these definitions fully correct?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Why did we need to fix spatial-&gt;visperc and verbal-paragrap both to 1? What would have happened if we just fixed one of those paths to 1?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Looking at the standardized coefficient, how can it be that the unstandardized coefficient for wordmean &gt; sentence &gt; paragrap, but looking at the standardized coefficients paragrap &gt; wordmean &gt; sentence. I thought that by fixing paragrap to 1 initially all other variables loaded on the factor were made to be relative to paragrap.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I'll also add in a question which I would imagine has a related answer: why to we fix the regression coefficient for the unique terms (e.g. err_v-&gt;visperc) to 1? What would it mean for err_v to have a coefficient of 1 in predicting visperc?&lt;/p&gt;&#10;&#10;&lt;p&gt;I'd greatly welcome responses even if they do not address all the questions.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-05-14T15:42:18.303" FavoriteCount="4" Id="58999" LastActivityDate="2014-03-29T10:32:13.387" LastEditDate="2013-05-15T08:50:09.440" LastEditorUserId="9162" OwnerUserId="9162" PostTypeId="1" Score="4" Tags="&lt;factor-analysis&gt;&lt;confirmatory-factor&gt;" Title="Why set weights to 1 in confirmatory factor analysis?" ViewCount="1120" />
  
  <row Body="&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;I may be misunderstanding the phrase &quot;indeterminancy of scale&quot;, but I believe it is set to one for identifiability. (That is, the number of unknowns in this system of equations should not exceed the number of equations.) Without setting one of the links to one, there are too many unknowns. Is that the same thing as indeterminancy of scale?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;In most SEM applications, you are working with covariance matrices, not the raw data. There is an alternative algorithm that uses the original data, called PLS (Partial Least Squares), which might shed some additional light on things for you.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="1" CreationDate="2013-05-14T17:11:47.580" Id="59005" LastActivityDate="2013-05-14T17:47:24.233" LastEditDate="2013-05-14T17:47:24.233" LastEditorUserId="1764" OwnerUserId="1764" ParentId="58999" PostTypeId="2" Score="6" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I've set up code to give me a graphical depiction of AIC vs BIC parsimony over various degrees of polynomial models.  On the rare occassion AIC does not match BIC trends, which parsimonious model would you select and why?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-14T18:43:19.203" Id="59019" LastActivityDate="2013-05-14T21:05:56.663" OwnerUserId="21409" PostTypeId="1" Score="2" Tags="&lt;aic&gt;&lt;bic&gt;" Title="AIC, BIC parsimony" ViewCount="236" />
  <row Body="&lt;p&gt;Is something like this what you're looking for? &lt;/p&gt;&#10;&#10;&lt;p&gt;In the example below, I've tentatively identified an AR(3) model, but I'm sure you can do a thorough analysis of the data to identify the appropriate model. I just want to get the idea across!&lt;/p&gt;&#10;&#10;&lt;p&gt;Note also that I used the predict() function to generate point forecasts. To create the interval forecasts I used the formula: point forecast $\pm$ 1.28 (standard error). That is, an 80% confidence interval. Change this to 1.96 for a 95% confidence interval.&lt;/p&gt;&#10;&#10;&lt;p&gt;To create more or fewer forecasts, change the value of n.ahead=12.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# Plot the data&#10;plot(dataframe$LOGINS, type=&quot;l&quot;)&#10;# Fit an AR(3) model&#10;fit &amp;lt;- arima(x=dataframe$LOGINS, order=c(3,0,0))&#10;# Get point forecasts (12 of them)&#10;forecasts &amp;lt;- predict(fit, n.ahead=12)&#10;# Concatenate LOGINS and the point forecasts (for plotting purposes)&#10;series &amp;lt;- c(dataframe$LOGINS, forecasts$pred)&#10;# Plot LOGINS &amp;amp; the point forecasts&#10;plot(series, type=&quot;l&quot;)&#10;# Add to the plot the upper 80% forecast C.I.&#10;lines(forecasts$pred+1.28*forecasts$se)&#10;# Add to the plot the lower 80% forecast C.I.&#10;lines(forecasts$pred-1.28*forecasts$se)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;You should get something that looks similar to this:&#10;&lt;img src=&quot;http://i.stack.imgur.com/zhoNq.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Obviously, you can play around with the plot and make it look the way you want it to.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let us know if this helps. If it doesn't, tell us why and we'll try to find a solution.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-05-14T19:34:17.067" Id="59024" LastActivityDate="2013-05-23T02:03:04.380" LastEditDate="2013-05-23T02:03:04.380" LastEditorUserId="24617" OwnerUserId="24617" ParentId="59018" PostTypeId="2" Score="6" />
  
  
  
  <row AnswerCount="2" Body="&lt;p&gt;How to incorporate costs (into logit model) of false positive, false negative, true positive, true negative responses, if they are different costs ? Is it possible to do that on the level of likelihood function ?&lt;/p&gt;&#10;&#10;&lt;p&gt;Edition :&#10;I see know that likeligooe function could be quite easily modified to incorporate costs, but then likelihood function become discontinuous :&lt;/p&gt;&#10;&#10;&lt;p&gt;y_i == 1 oraz f(x_i*B) &gt; 0.5 cost = cost11&lt;br&gt;&#10;y_i == 1 oraz f(x_i*B) &amp;lt; 0.5 cost = cost10&lt;br&gt;&#10;y_i == 0 oraz f(x_i*B) &gt; 0.5 cost = cost01&lt;br&gt;&#10;y_i == 0 oraz f(x_i*B) &amp;lt; 0.5 cost = cost00&lt;/p&gt;&#10;&#10;&lt;p&gt;non-modified likelihood function :&lt;br&gt;&#10;f(x_i*B)^y_i * ( 1 - f(x_i*B) ) ^ (1 - y_i)  &lt;/p&gt;&#10;&#10;&lt;p&gt;modified likelihood function :  &lt;/p&gt;&#10;&#10;&lt;p&gt;(cost11*positive(f(x_i*B)-0.5) + cost10*negative(f(x_i*B)-0.5) ) ^ y_i&lt;br&gt;&#10;*&lt;br&gt;&#10;(cost00*negative(f(x_i*B)-0.5) + cost01*positive(f(x_i*B)-0.5) ) ^ (1 - y_i)&lt;/p&gt;&#10;&#10;&lt;p&gt;where :&#10;positive(x) = 1 if x &gt; 0&lt;br&gt;&#10;positive(x) = 0 if x &amp;lt; 0  &lt;/p&gt;&#10;&#10;&lt;p&gt;negative(x) = 1 if x &gt; 0&lt;br&gt;&#10;negative(x) = 0 if x &amp;lt; 0  &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-05-15T08:19:18.807" FavoriteCount="1" Id="59079" LastActivityDate="2013-05-15T17:49:15.790" LastEditDate="2013-05-15T17:49:15.790" LastEditorUserId="4908" OwnerUserId="4908" PostTypeId="1" Score="6" Tags="&lt;maximum-likelihood&gt;&lt;contingency-tables&gt;&lt;logit&gt;&lt;likelihood-function&gt;" Title="How to incorporate costs (into logit model) of false positive, false negative, true positive, true negative if they are different costs?" ViewCount="364" />
  <row Body="&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/viDrY.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Body must be at least 30 characters; you entered 0.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-05-15T08:52:09.897" Id="59084" LastActivityDate="2013-05-15T08:52:09.897" OwnerUserId="21846" ParentId="59072" PostTypeId="2" Score="2" />
  <row AnswerCount="0" Body="&lt;p&gt;I have been looking at unit root testing. Specifically 2 tests:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;strong&gt;The ADF test.&lt;/strong&gt;&#10;The ADF (augmented Dickey Fuller) test has the null hypothesis that &quot;the time series has a unit root&quot; (meaning that the time series is not weakly stationary)  &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;strong&gt;The KPSS test.&lt;/strong&gt;&#10;The KPSS is a (Lagrange Multiplier type) test which has the null hypothesis that &quot;the time series is weakly stationary&quot; (so no unit root here).&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;What I am looking for when applying those tests is to say whether a process is stationary, so I can do things with it. Somebody told me that because of how the null hypothesis was specified, on average one test would give more stationary processes than the other. &lt;/p&gt;&#10;&#10;&lt;p&gt;Could anybody explain to me why ?&lt;/p&gt;&#10;&#10;&lt;p&gt;This is what I think: because the ADF test has as $H_0$ &quot;the process has a unit root&quot;, what I can say is &quot;With a given confidence, say 99%, I cannot reject the fact that this process has a unit root&quot;; because of this I can be pretty sure that a process is not stationary, but this is not the same that saying that a process is stationary, so the ADF test should &quot;keep&quot; more processes as stationary than the KPSS test ...&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-15T09:28:08.177" Id="59087" LastActivityDate="2013-05-15T17:33:32.603" LastEditDate="2013-05-15T17:33:32.603" LastEditorUserId="17230" OwnerUserId="19890" PostTypeId="1" Score="2" Tags="&lt;hypothesis-testing&gt;&lt;unit-root&gt;" Title="About 2 unit root tests and null hypothesis" ViewCount="113" />
  <row Body="&lt;p&gt;You did not specify your variables, if they are binary or something else. I think you talk about binary variables. There also exist multinomial versions of the probit and logit model.&lt;/p&gt;&#10;&#10;&lt;p&gt;In general, you can use the complete trinity of test approaches, i.e.&lt;/p&gt;&#10;&#10;&lt;p&gt;Likelihood-Ratio-test&lt;/p&gt;&#10;&#10;&lt;p&gt;LM-Test&lt;/p&gt;&#10;&#10;&lt;p&gt;Wald-Test&lt;/p&gt;&#10;&#10;&lt;p&gt;Each test uses different test-statistics. The standard approach would be to take one of the three tests. All three can be used to do joint tests.&lt;/p&gt;&#10;&#10;&lt;p&gt;The LR test uses the differnce of the log-likelihood of a restricted and the unrestricted model. So the restricted model is the model, in which the specified coefficients are set to zero. The unrestricted is the &quot;normal&quot; model. The Wald test has the advantage, that only the unrestriced model is estimated. It basically asks, if the restriction is nearly satisfied if it is evaluated at the unrestriced MLE. In case of the Lagrange-Multiplier test only the restricted model has to be estimated. The restricted ML estimator is used to calculate the score of the unrestricted model. This score will be usually not zero, so this discrepancy is the basis of the LR test. The LM-Test can in your context also be used to test for heteroscedasticity.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-15T09:37:28.550" Id="59090" LastActivityDate="2013-05-15T09:47:27.990" LastEditDate="2013-05-15T09:47:27.990" LastEditorUserId="25675" OwnerUserId="25675" ParentId="59085" PostTypeId="2" Score="9" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I am using a Johansen procedure to test for cointegration a vectorial 4-dim vector (timeserie). &lt;/p&gt;&#10;&#10;&lt;p&gt;First I tested for differential stationarity of each individual vector, all of those have a unit root so we are good there. &lt;/p&gt;&#10;&#10;&lt;p&gt;Then I have to find the optimal number of lags as to test for cointegration using Johansen, we assume that the underlying process is a &lt;code&gt;VAR&lt;/code&gt; process. And that's where the problem occur, I usually find &lt;code&gt;K=1&lt;/code&gt; (&lt;code&gt;K&lt;/code&gt; minimizes several information criterias).&lt;/p&gt;&#10;&#10;&lt;p&gt;I use the package &lt;code&gt;urca&lt;/code&gt; and the function &lt;code&gt;ca.jo&lt;/code&gt; to run the &lt;code&gt;trace&lt;/code&gt; and &lt;code&gt;eigen&lt;/code&gt; tests, but &lt;code&gt;ca.jo&lt;/code&gt; only allows &lt;code&gt;K&amp;gt;1&lt;/code&gt; (which makes sense because of the model).&lt;/p&gt;&#10;&#10;&lt;p&gt;What does this &lt;code&gt;K=1&lt;/code&gt; means ? I add that If I take &lt;code&gt;K=2&lt;/code&gt; both Johansen tests comes back significant and the process indeed looks cointegrated...&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-05-15T10:41:16.593" Id="59100" LastActivityDate="2014-01-16T06:34:10.207" OwnerUserId="19890" PostTypeId="1" Score="0" Tags="&lt;hypothesis-testing&gt;&lt;cointegration&gt;&lt;var&gt;" Title="Cointegration of a VAR(1) process" ViewCount="153" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a list experiment with N=150. The control group was given 3 factors and the treatment group was given 4 factors, with random assignment to these groups. Each group had to report how many factors apply to them. I've done 18 batches of this experiment. In most of them, the mean for the treatment group is higher than the mean for the control group or if it is the other way around, the difference is not significant.BUT in two of the batches, the mean for the treatment group is lower than the mean for the control group and that difference is significant...The questions have been pre-tested and seemed to work well. So why these results in the 2 off-batches? A million thanks in advance for any advice and suggestions, TP&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-05-15T11:59:16.170" Id="59106" LastActivityDate="2013-05-15T12:48:46.113" LastEditDate="2013-05-15T12:48:46.113" LastEditorUserId="88" OwnerUserId="25681" PostTypeId="1" Score="0" Tags="&lt;mean&gt;" Title="Control mean lower than treatment mean in a list experiment -- why?" ViewCount="37" />
  <row AnswerCount="2" Body="&lt;p&gt;I have 2 level  Design (DOE) with 4 factors (A,B,C,D). I've already calculated the estimates for each main effect and all the interaction effects.&lt;/p&gt;&#10;&#10;&lt;p&gt;How can I construct the normal probability plot to see which effects are significant? I have looked at many packages in R for Design of Experiment but cannot find a package that produces the plot.&lt;/p&gt;&#10;&#10;&lt;p&gt;How can I construct the plot with all the estimates I have by code in R&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-05-15T13:00:34.487" Id="59109" LastActivityDate="2014-06-03T21:13:22.120" LastEditDate="2013-05-15T16:53:15.340" LastEditorUserId="10217" OwnerUserId="24999" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;data-visualization&gt;&lt;categorical-data&gt;&lt;treatment-effect&gt;" Title="Plot normal probability for effect estimates in factorial design in R" ViewCount="686" />
  
  
  <row Body="&lt;p&gt;I was writing my own weighted average algorithm yesterday and it applies to what you’re looking to do. Your problem is what you're calling returns_rate and the logic behind it. For these examples we have a new field called weighted_score or you could call it weighted_returns if you wanted. The point is that its weighted.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;id | name | count | returns | weight | weighted_score&#10;-----------------------------------------------------&#10;1     A      150      10       0.2         2&#10;2     B      250      50       0.3         15&#10;3     C      350      60       0.4         24&#10;4     D      550      100      0.4         40&#10;5     E      150      80       0.2         16&#10;6     F      50       10       0.1         1&#10;-----------------------------------------------------&#10;sums                           1.6         98&#10;weighted_average                           61.3&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Logic:&lt;/strong&gt;  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;weighted_score = returns*weight&#10;weighted_average = sum(weighted_scores)/sum(weights)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I see that you have some logic to determine the weight however your upper limit is 40%. There really should be 0% through 100% in there. You can also calculate the weight on the fly for any given data set by finding the highest “count” and dividing the “count” in each row by that highest number and this gives you the appropriate 0% - 100% weights.&lt;br&gt;&#10;Example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;id | name | count | returns | weight | weighted_score&#10;-----------------------------------------------------&#10;1     A     150      10       0.2727         2.7&#10;2     B     250      50       0.4545         22.7&#10;3     C     350      60       0.6364         38.2&#10;4     D     550      100      1.0000         100&#10;5     E     150      80       0.2727         21.8&#10;6     F     50       10       0.0909         0.9&#10;-----------------------------------------------------&#10;sums                          2.7273         186.3636&#10;weighted_average                             68.3&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Logic:&lt;/strong&gt;  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;weight = count/550&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Another simpler option that doesn’t require so much processing trying to figure out the appropriate weight is to just create a static variable to use as your ceiling. Any &lt;em&gt;count&lt;/em&gt; lower than the ceiling will be weighted with lower importance and anything above the ceiling is weighted at 100%.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;id | name | count | returns | weight | weighted_score&#10;-----------------------------------------------------&#10;1     A     150      10       0.4983         2.7&#10;2     B     250      50       0.8306         22.7&#10;3     C     350      60       1.0000         38.2&#10;4     D     550      100      1.0000         100&#10;5     E     150      80       0.4983         21.8&#10;6     F     50       10       0.1661         0.9&#10;-----------------------------------------------------&#10;sums                          3.9934         248.0399&#10;weighted_average                             62.1&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Logic:&lt;/strong&gt;  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;if(count &amp;lt; 301) weight = count/301&#10;if(count &amp;gt; 301) weight = count/count&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2013-05-15T15:02:48.843" Id="59130" LastActivityDate="2013-05-15T15:02:48.843" OwnerUserId="25690" ParentId="58419" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;If X1, X2, and X3 are i.i.d. Gamma(a) then {X1,X2,X3}/(X1+X2+X3) will be Dirichlet(a,a,a). &lt;/p&gt;&#10;&#10;&lt;p&gt;If a&gt;1 then the mode will be 1/3. The peak will be sharper for larger values of a.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-05-15T16:20:33.110" Id="59136" LastActivityDate="2013-05-17T02:40:39.827" LastEditDate="2013-05-17T02:40:39.827" LastEditorUserId="603" OwnerUserId="20776" ParentId="59096" PostTypeId="2" Score="3" />
  <row AnswerCount="1" Body="&lt;p&gt;While reading some articles about High frequency statistics in financial markets, more precisely wile reading about microstructure noise, I was confronted by some signature plots simulated graphics. &lt;/p&gt;&#10;&#10;&lt;p&gt;It seems me to be a basic concept in statistics or, at least, just a well known term. However for a lack of culture on general statistics, I don't understand what it means (so neither what it illustrates) and I could not find this definition anywhere on the web. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-05-15T16:30:04.153" FavoriteCount="1" Id="59138" LastActivityDate="2013-06-03T04:38:29.837" LastEditDate="2013-05-15T16:58:00.810" LastEditorUserId="7290" OwnerUserId="18279" PostTypeId="1" Score="0" Tags="&lt;data-visualization&gt;" Title="Signature plots definition" ViewCount="282" />
  <row AnswerCount="0" Body="&lt;p&gt;Suppose there is a sample of varying size $n\in \mathbb N$, each sample point taking values in $\mathbb R$, and a statistic $T$. If I am correct, a statistic can accept arbitrary sample size. What are some standard ways to represent/write the statistic $T$ with varying sample size $n$?  For example,&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;We may write the statistic $T$ as a mapping from $\mathbb R^n$ to $\mathbb R$ for each different $n$&#10;value.  How can we make sure that, the different mapings for&#10;different $n$ values are from the same statistic $T$ not from&#10;another different one?&lt;/li&gt;&#10;&lt;li&gt;Can we write $T$ as a single mapping for all $n$ values? How is the statistic $T$ represented as a mapping with varying number of arguments (here arguments are sample points in the sample) then?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Thanks and regards!&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-05-16T00:11:26.863" Id="59163" LastActivityDate="2013-05-16T00:24:52.347" LastEditDate="2013-05-16T00:24:52.347" LastEditorUserId="1005" OwnerUserId="1005" PostTypeId="1" Score="0" Tags="&lt;mathematical-statistics&gt;" Title="How is a statistic represented as a mapping with varying sample size?" ViewCount="49" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have run a series of mixed linear regressions in Stata, some with inverse-square-root ($1/\sqrt{x}$) transformations and others with square root ($\sqrt{x}$) transformations.  &lt;/p&gt;&#10;&#10;&lt;p&gt;How do I calculate untransformed confidence intervals from the transformed results??&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-05-16T06:34:20.867" Id="59177" LastActivityDate="2013-05-17T01:23:20.153" LastEditDate="2013-05-16T07:33:27.673" LastEditorUserId="805" OwnerUserId="25719" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;mixed-model&gt;&lt;confidence-interval&gt;&lt;data-transformation&gt;&lt;stata&gt;" Title="How to calculate confidence intervals of $1/\sqrt{x}$-transformed data after running a mixed linear regression in stata?" ViewCount="597" />
  
  
  <row AcceptedAnswerId="59189" AnswerCount="1" Body="&lt;p&gt;I have various datasets I need to analyse regarding soil properties, all in the same fashion, with one fixed effect (which is a position along a transect, indicating different land uses). Now my main level of replication is across different transects, which will obviously have some form of random variance associated with them, and so I want to account for this in my statistical analyses. &lt;/p&gt;&#10;&#10;&lt;p&gt;So, in &lt;code&gt;lme4&lt;/code&gt; I specified a mixed model to this specification&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;model &amp;lt;- lmer(variable.of.interest ~ transect.position + (1|transect))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now, when I analyse the above model against a model without the transect position term, I get exactly the results I was expecting, and then plugging the above model into &lt;code&gt;anova()&lt;/code&gt;, I get the F values, d.f. etc. that I need.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, I can't figure out how to say, for my overall report, that the random effect of transect does not make any difference to the overall analyses (i.e. I can't get a p-value, F value, d.f. etc.).&lt;/p&gt;&#10;&#10;&lt;p&gt;Help?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-05-16T09:01:43.117" Id="59184" LastActivityDate="2013-05-16T10:17:51.007" LastEditDate="2013-05-16T09:42:38.317" LastEditorUserId="805" OwnerUserId="25722" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;anova&gt;&lt;statistical-significance&gt;&lt;mixed-model&gt;&lt;random-effects-model&gt;" Title="Simple Mixed Model with 1 Fixed and 1 Random Effect" ViewCount="121" />
  <row Body="&lt;p&gt;What comes first here? Understanding the data or being committed to using a particular technique? Why precisely do you think the PCA is going to help? &lt;/p&gt;&#10;&#10;&lt;p&gt;Perhaps it is beyond your control, or too late any way, but the questionnaire sounds badly designed. If a rating of &quot;perceived usefulness&quot; is needed, why not ask something like &quot;on a scale of 1 to 5, please rate how useful this thing is&quot;? (Similar comments for other scales.) &lt;/p&gt;&#10;&#10;&lt;p&gt;More positively, I would &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;pool the data for two countries, and then see how the people from each country compare on the scales defined by the PCs. The different sample sizes don't seem especially worrying here. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;do a PCA for Netherlands, then see (as above). &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;do a PCA for Bulgaria, then see (as above). &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;see which of #1 to #3 was most illuminating. &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I wouldn't try anything more complicated. What you have in mind sounds like an analysis where you would have a hard time disentangling side-effects of what you did from something revealing the structure of the data. &lt;/p&gt;&#10;&#10;&lt;p&gt;I am calling what you did PCA, on the philosophy of calling a spade a spade (you may need a different idiom in your first language). &lt;/p&gt;&#10;" CommentCount="7" CreationDate="2013-05-16T09:59:26.617" Id="59188" LastActivityDate="2013-05-16T09:59:26.617" OwnerUserId="22047" ParentId="59185" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;It's not clear to me exactly what you want, but if we plot these points (e.g. in &lt;code&gt;R&lt;/code&gt;) &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x &amp;lt;- c(0.025, 0.099, 0.22, 0.39, 0.62)&#10;y &amp;lt;- c(0.66, 0.72, 0.78, 0.75, 0.83)&#10;&#10;plot(x,y,type = 'p')&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;we can add a least squares fit line&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;abline(lsfit(x,y))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;which (as least square lines often do) looks pretty good, intuitively.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/juSRo.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-05-16T10:46:32.837" Id="59193" LastActivityDate="2013-05-16T10:46:32.837" OwnerUserId="686" ParentId="59190" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;It's probably not quite what you have in mind, but Dan Navarro has written a textbook for his own undergraduate statistics subject in psychology. I have read it all, but it seems to use the approach you mention on occasion. It uses a fair bit of R and generally seems to be more sophisticated than your average intro psych stats textbook. The PDF of the chapters are available online for free.&lt;/p&gt;&#10;&#10;&lt;p&gt;Daniel Navarro: &lt;a href=&quot;http://health.adelaide.edu.au/psychology/ccs/teaching/lsr/&quot; rel=&quot;nofollow&quot;&gt;Learning Statistics with R&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-16T11:45:25.350" Id="59199" LastActivityDate="2013-05-16T11:45:25.350" OwnerUserId="183" ParentId="59187" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;A generalization of the Wilcoxon-Mann-Whitney test is the proportional odds ordinal logistic model, which accepts covariates in addition to the group variable you are mainly testing.  Note that the prop. odds model does not need more than one observations at each unique value of $Y$ in order to work well.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-05-16T13:49:04.800" Id="59210" LastActivityDate="2013-05-16T13:49:04.800" OwnerUserId="4253" ParentId="59207" PostTypeId="2" Score="8" />
  <row Body="&lt;p&gt;$\alpha$ and $\beta$ are related. I'll try to illustrate the point with a diagnostic test. Let's say that you have a diagnostic test that measures the level of a blood marker. It is known that people having a certain disease have lower levels of this marker compared to healthy people. It is immediately clear that you have to decide a cutoff value, below which a person is classified as &quot;sick&quot; whereas people with values above this cutoff are thought to be healthy. It is very likely, though, that the distribution of the bloodmarker varies considerably even &lt;em&gt;within&lt;/em&gt; sick and healthy people. Some healthy persons might have very low blood marker levels, even though they are perfectly healthy. And some sick people have high levels of the blood marker even though they have the disease.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are four possibilites that can occur:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;a sick person is correctly identified as sick (true positive = TP)&lt;/li&gt;&#10;&lt;li&gt;a sick person is falsely classified as healthy (false negative = FN)&lt;/li&gt;&#10;&lt;li&gt;a healthy person is correctly identified as healthy (true negative =&#10;TN)&lt;/li&gt;&#10;&lt;li&gt;a healthy person is falsely classified as sick (false positive =&#10;FP)&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;These possibilities can be illustrated with a &lt;a href=&quot;http://en.wikipedia.org/wiki/Sensitivity_and_specificity&quot;&gt;2x2 table&lt;/a&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;               Sick Healthy&#10;Test positive   TP     FP&#10;Test negative   FN     TN&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;$\alpha$ denotes the false positive rate, which is $\alpha = FP/(FP + TN)$. $\beta$ is the false negative rate, which is $\beta = FN/(TP + FN)$. I wrote a simply &lt;code&gt;R&lt;/code&gt; script to illustrate the situation graphically.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;alphabeta &amp;lt;- function(mean.sick=100, sd.sick=10, mean.healthy=130, sd.healthy=10, cutoff=120, n=10000, side=&quot;below&quot;, do.plot=TRUE) {&#10;&#10;  popsick &amp;lt;- rnorm(n, mean=mean.sick, sd=sd.sick)&#10;  pophealthy &amp;lt;- rnorm(n, mean=mean.healthy, sd=sd.healthy)&#10;&#10;  if ( side == &quot;below&quot; ) {&#10;&#10;    truepos &amp;lt;- length(popsick[popsick &amp;lt;= cutoff])&#10;    falsepos &amp;lt;- length(pophealthy[pophealthy &amp;lt;= cutoff])&#10;    trueneg &amp;lt;- length(pophealthy[pophealthy &amp;gt; cutoff])&#10;    falseneg &amp;lt;- length(popsick[popsick &amp;gt; cutoff])&#10;&#10;  } else if ( side == &quot;above&quot; ) {&#10;&#10;    truepos &amp;lt;- length(popsick[popsick &amp;gt;= cutoff])&#10;    falsepos &amp;lt;- length(pophealthy[pophealthy &amp;gt;= cutoff])&#10;    trueneg &amp;lt;- length(pophealthy[pophealthy &amp;lt; cutoff])&#10;    falseneg &amp;lt;- length(popsick[popsick &amp;lt; cutoff])&#10;&#10;  }&#10;&#10;  twotable &amp;lt;- matrix(c(truepos, falsepos, falseneg, trueneg), 2, 2, byrow=T)&#10;  rownames(twotable) &amp;lt;- c(&quot;Test positive&quot;, &quot;Test negative&quot;)&#10;  colnames(twotable) &amp;lt;- c(&quot;Sick&quot;, &quot;Healthy&quot;)&#10;&#10;  spec &amp;lt;- twotable[2,2]/(twotable[2,2] + twotable[1,2])&#10;  alpha &amp;lt;- 1 - spec&#10;  sens &amp;lt;- pow &amp;lt;- twotable[1,1]/(twotable[1,1] + twotable[2,1])&#10;  beta &amp;lt;- 1 - sens&#10;&#10;  pos.pred &amp;lt;- twotable[1,1]/(twotable[1,1] + twotable[1,2])&#10;  neg.pred &amp;lt;- twotable[2,2]/(twotable[2,2] + twotable[2,1])&#10;&#10;&#10;  if ( do.plot == TRUE ) {&#10;&#10;    dsick &amp;lt;- density(popsick)&#10;    dhealthy &amp;lt;- density(pophealthy)&#10;&#10;    par(mar=c(5.5, 4, 0.5, 0.5))&#10;    plot(range(c(dsick$x, dhealthy$x)), range(c(c(dsick$y, dhealthy$y))), type = &quot;n&quot;, xlab=&quot;&quot;, ylab=&quot;&quot;, axes=FALSE)&#10;    box()&#10;    axis(1, at=mean(pophealthy), lab=substitute(mu[H[0]]~paste(&quot;=&quot;,m, sep=&quot;&quot;), list(m=mean.healthy)), cex.axis=1.5,tck=0.02)&#10;    axis(1, at=mean(popsick), lab=substitute(mu[H[1]]~paste(&quot;=&quot;,m, sep=&quot;&quot;), list(m=mean.sick)), cex.axis=1.5, tck=0.02)                                        &#10;    axis(1, at=cutoff, lab=substitute(italic(paste(&quot;Cutoff=&quot;,coff, sep=&quot;&quot;)), list(coff=cutoff)), pos=-0.004, tick=FALSE, cex.axis=1.25)&#10;    lines(dhealthy, col = &quot;steelblue&quot;, lwd=2)&#10;&#10;    if ( side == &quot;below&quot; ) {&#10;      polygon(c(cutoff, dhealthy$x[dhealthy$x&amp;lt;=cutoff], cutoff), c(0, dhealthy$y[dhealthy$x&amp;lt;=cutoff],0), col = &quot;grey65&quot;)&#10;    } else if ( side == &quot;above&quot; ) {&#10;      polygon(c(cutoff, dhealthy$x[dhealthy$x&amp;gt;=cutoff], cutoff), c(0, dhealthy$y[dhealthy$x&amp;gt;=cutoff],0), col = &quot;grey65&quot;)&#10;    }&#10;&#10;    lines(dsick, col = &quot;red&quot;, lwd=2)&#10;&#10;    if ( side == &quot;below&quot; ) {&#10;      polygon(c(cutoff,dsick$x[dsick$x&amp;gt;cutoff],cutoff),c(0,dsick$y[dsick$x&amp;gt;cutoff],0) , col=&quot;grey90&quot;)&#10;    } else if ( side == &quot;above&quot; ) {&#10;      polygon(c(cutoff,dsick$x[dsick$x&amp;lt;=cutoff],cutoff),c(0,dsick$y[dsick$x&amp;lt;=cutoff],0) , col=&quot;grey90&quot;)&#10;    }&#10;&#10;    legend(&quot;topleft&quot;,&#10;           legend=(c(as.expression(substitute(alpha~paste(&quot;=&quot;, a), list(a=round(alpha,3)))), &#10;                     as.expression(substitute(beta~paste(&quot;=&quot;, b), list(b=round(beta,3)))))), fill=c(&quot;grey65&quot;, &quot;grey90&quot;), cex=1.2, bty=&quot;n&quot;)&#10;    abline(v=mean(popsick), lty=3)&#10;    abline(v=mean(pophealthy), lty=3)&#10;    abline(v=cutoff, lty=1, lwd=1.5)&#10;    abline(h=0)&#10;&#10;  }&#10;&#10;  #list(specificity=spec, sensitivity=sens, alpha=alpha, beta=beta, power=pow, positiv.predictive=pos.pred, negative.predictive=neg.pred)&#10;&#10;  c(alpha, beta)&#10;&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Let's look at an example. We assume that the mean level of the blood marker among the sick people is 100 with a standard deviation of 10. Among the healthy people, the mean blood level is 140 with a standard deviation of 15. The clinician sets the cutoff at 120.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;alphabeta(mean.sick=100, sd.sick=10, mean.healthy=140, sd.healthy=15, cutoff=120, n=100000, do.plot=TRUE, side=&quot;below&quot;)&#10;&#10;              Sick Healthy&#10;Test positive 9764     901&#10;Test negative  236    9099&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/va42o.png&quot; alt=&quot;Beta and alpha with a cutoff of 120&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;You see that the shaded areas are in a relation with each other. In this case, $\alpha = 901/(901+ 9099) \approx 0.09$ and $\beta = 236/(236 + 9764)\approx 0.024$. But what happens if the clinician had set the cutoff differently? Let's set it a bit lower, to 105 and see what happens.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;              Sick Healthy&#10;Test positive 6909      90&#10;Test negative 3091    9910&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Kr1sw.png&quot; alt=&quot;Cutoff 105&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Our $\alpha$ is very low now because almost no healthy people are diagnosed as sick. But our $\beta$ has increased, because sick people with a high blood marker level are now falsely classified as healthy.&lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, let us look how $\alpha$ and $\beta$ change for different cutoffs:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;cutoffs &amp;lt;- seq(0, 200, by=0.1)&#10;cutoff.grid &amp;lt;- expand.grid(cutoffs)&#10;&#10;plot.frame &amp;lt;- apply(cutoff.grid, MARGIN=1, FUN=alphabeta, mean.sick=100, sd.sick=10, mean.healthy=140, sd.healthy=15, n=100000, do.plot=FALSE, side=&quot;below&quot;)&#10;&#10;plot(plot.frame[1,]~cutoffs, type=&quot;l&quot;, las=1, xlab=&quot;Cutoff value&quot;, ylab=&quot;Alpha/Beta&quot;, lwd=2, cex.axis=1.5, cex.lab=1.2)&#10;lines(plot.frame[2,]~cutoffs, col=&quot;steelblue&quot;, lty=2, lwd=2)&#10;legend(&quot;topleft&quot;, legend=c(expression(alpha), expression(beta)), lwd=c(2,2),lty=c(1,2), col=c(&quot;black&quot;, &quot;steelblue&quot;), bty=&quot;n&quot;, cex=1.2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/LLWDR.png&quot; alt=&quot;Plot of alpha and beta with different cutoff values&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;You can immediately see that the ratio of $\alpha$ and $\beta$ is not constant. What is also very important is the effect size. In this case, this would be the difference of the means of the blood marker levels among sick and healthy people. The greater the difference, the easier the two groups can be separated by a cutoff:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/pnSG5.png&quot; alt=&quot;Perfect cutoff&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Here we have a &quot;perfect&quot; test in the sense that the cutoff of 150 discriminates the sick from the healthy.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Bonferroni adjustements&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Bonferroni adjustments reduces the $\alpha$ error but &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1112991/&quot;&gt;inflate the type II error ($\beta$)&lt;/a&gt;. This means that the error of making a false negative decision is increased while false positives are minimized. That's why the Bonferroni adjustment is often called conservative. In the graphs above, note how the $\beta$ increased when we lowered the cutoff from 120 to 105: it increased from $0.02$ to $0.31$. At the same time, $\alpha$ decreased from $0.09$ to $0.01$.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-05-16T16:51:02.383" Id="59221" LastActivityDate="2013-05-16T20:55:20.640" LastEditDate="2013-05-16T20:55:20.640" LastEditorUserId="21054" OwnerUserId="21054" ParentId="59202" PostTypeId="2" Score="9" />
  <row AnswerCount="0" Body="&lt;p&gt;I am working with several hundred 3d point clouds generated using a 3d scanner and would like to be able to compare their shapes using something like a procrustes analysis. Instead of manually defining individual landmarks to compare however, I was hoping to compare the entire point cloud of each scan to all of the other scans to measure similarity of each point cloud to the rest of the dataset. Is this possible, and if so how would I go about it? I would like to use R if possible to do this analysis, though I do have access to other tools if need be.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-05-16T18:18:02.017" Id="59228" LastActivityDate="2014-05-30T14:58:41.840" LastEditDate="2013-05-16T21:03:57.987" LastEditorUserId="3277" OwnerUserId="25745" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;classification&gt;&lt;rotation&gt;" Title="Procrustes Analysis of 3d point cloud without defined landmarks" ViewCount="321" />
  <row Body="&lt;p&gt;You should not be using Cronbach's alpha in your case. I don't think you should be using &lt;em&gt;any&lt;/em&gt; measure of internal reliability because your variables are not intended to form a scale. &lt;/p&gt;&#10;&#10;&lt;p&gt;And, if your professor really said that the first thing you do, in all cases, is Cronbach's alpha, then I would drop the class.  That is like a cooking class where the teacher says &quot;First, turn on your oven&quot; even if the recipe involves no baking or broiling. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-05-16T18:18:38.263" Id="59229" LastActivityDate="2013-05-16T18:18:38.263" OwnerUserId="686" ParentId="59223" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;I think &quot;&lt;a href=&quot;http://melodi.ee.washington.edu/people/bilmes/mypapers/em.pdf&quot; rel=&quot;nofollow&quot;&gt;A Gentle Tutorial of the EM Algorithm&#10;and its Application to Parameter&#10;Estimation for Gaussian Mixture and&#10;Hidden Markov Models&lt;/a&gt;&quot; is exactly what you're looking for. Particularly the third section: Finding Maximum Likelihood Mixture Densities Parameters via EM. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-05-16T20:30:22.247" Id="59238" LastActivityDate="2013-05-16T20:30:22.247" OwnerUserId="8242" ParentId="59237" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;I have been confused by two separate questions (Stock &amp;amp; Watson - introduction to econometrics ch.3), using different values for standard errors.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;The first:&lt;/strong&gt; In a survey of 400 voters, 215 respond to vote for the incumbent, 185 for the challenger. Let p denote the fraction of all likely voters who preferred the incumbent at the time of the survey and $\hat p$ be the fraction of survey respondents that prefer the incumbent.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Now for the variance it is given by $\hat p(1-\hat p)/n$ and when calculating the $SE(\hat p)$ we have to take the square root of the variance to get $0.0249$, and I am fine with this.&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;The second question:&lt;/strong&gt; In a given population 11% of voters are African American. A survey using a random sample of 600 landline telephone numbers finds 8% African Americans. Is there evidence the survey is biased?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Now when calculating the t-statistic we use the null hypothesis with $p=0.11$, but it then states that $se(\hat p)=\hat p(1-\hat p)/n$.&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Why do we no longer have to take the square root of the value above to find the standard error? I imagine it must be to do with knowing the population variance?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-05-16T20:40:17.893" Id="59239" LastActivityDate="2013-05-16T22:47:36.467" LastEditDate="2013-05-16T22:10:09.950" LastEditorUserId="25753" OwnerUserId="25753" PostTypeId="1" Score="4" Tags="&lt;standard-error&gt;&lt;sample&gt;&lt;population&gt;" Title="Population standard deviation for a binomial(?) distribution?" ViewCount="268" />
  <row AcceptedAnswerId="59256" AnswerCount="1" Body="&lt;p&gt;I am currently designing an experiment to evaluate 4 iterations of a system. As it is rather difficult for a human to judge the output of the system by assigning it a score or several scores, I plan on giving the following task:&lt;/p&gt;&#10;&#10;&lt;p&gt;Give the human evaluator the outputs of two systems on the same input and ask him to decide which system's output is better in certain regards.&lt;/p&gt;&#10;&#10;&lt;p&gt;My final desired result is to have a ranked list of system iterations and/or pairwise comparisons stating which system iteration is better.&lt;/p&gt;&#10;&#10;&lt;p&gt;How should I proceed with the experimental design? What significance test should I use?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: It seems that sign test could be appropriate to test whether one system is truly better than the other.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Update2&lt;/strong&gt;: Does obtaining the ranking of 4 system iterations necessitate performing &lt;em&gt;m&lt;/em&gt; trials for each pair or could transitivity be used, as in if A &gt; B and B &gt; C, then A &gt; C, to reduce the number of required trials?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-05-16T21:30:27.087" FavoriteCount="0" Id="59242" LastActivityDate="2013-05-18T02:17:53.673" LastEditDate="2013-05-17T09:57:00.073" LastEditorUserId="25755" OwnerUserId="25755" PostTypeId="1" Score="4" Tags="&lt;statistical-significance&gt;&lt;experiment-design&gt;" Title="How to rank order performance of four systems using aggregates of pairwise assessments of system performance?" ViewCount="65" />
  <row AcceptedAnswerId="59278" AnswerCount="1" Body="&lt;p&gt;I'm a med student, conducting a retrospective analysis of weight/growth disturbances during chemo treatment.&lt;br&gt;&#10;&lt;strong&gt;I wonder, if I should:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;assume, that growth is a variable normally distributed across the population&lt;/li&gt;&#10;&lt;li&gt;test for (or visually assess) the normality once (when?)&lt;/li&gt;&#10;&lt;li&gt;test for (or visually assess) normality separately, for each sample/each test performed&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Why is that a problem for me:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;there are many samples and subsamples (gender, treatment protocol, time point of measurment)&lt;/li&gt;&#10;&lt;li&gt;samples have different means (supposedly effect of the treatment or small sample size)&lt;/li&gt;&#10;&lt;li&gt;sample sizes are different (ranging ~5-60)&lt;/li&gt;&#10;&lt;li&gt;treatment can hypothetically change the distribution of the variable (however I doubt it, since I've already generated some histograms and sometimes (bigger sample size) values look normally distributed)&lt;/li&gt;&#10;&lt;li&gt;each analysis will exclue different cases (database is not complete, I must perform pairwise deletion)&lt;/li&gt;&#10;&lt;li&gt;tests for normality are useless with small samples?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Here are some examples.&lt;/strong&gt;: &lt;a href=&quot;http://imgur.com/a/zb2hM&quot; rel=&quot;nofollow&quot;&gt;http://imgur.com/a/zb2hM&lt;/a&gt;&lt;br&gt;&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;X axis is an SDS for weight(it must compared indirectly, since patients are at different age; population's SD and mean are known for the specified age)&lt;/li&gt;&#10;&lt;li&gt;Samples differ with sample size, case's gender, treatment etc.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;h3&gt;Questions&lt;/h3&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Should I test for normality or assume normality?&lt;/li&gt;&#10;&lt;li&gt;Should I test for normality separately in each sample or just once?&lt;/li&gt;&#10;&lt;li&gt;How should I test for normality? &lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="3" CreationDate="2013-05-16T23:19:40.153" FavoriteCount="1" Id="59248" LastActivityDate="2013-05-21T08:08:34.597" LastEditDate="2013-05-17T02:16:10.023" LastEditorUserId="183" OwnerUserId="25757" PostTypeId="1" Score="4" Tags="&lt;normal-distribution&gt;&lt;sample-size&gt;&lt;normality&gt;&lt;histogram&gt;" Title="How to test for normality of growth disturbances in chemo treatment?" ViewCount="256" />
  <row Body="&lt;p&gt;It sounds like you need a decent basic statistics text that covers at least basic location tests, simple regression and multiple regression.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Std. Error,t value and Pr. &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;i) &lt;code&gt;Std. Error&lt;/code&gt; is the standard deviation of the sampling distribution of the estimate of the coefficient under the standard regression assumptions. Such standard deviations are called &lt;em&gt;standard errors&lt;/em&gt; of the corresponding quantity (the coefficient estimate in this case).&lt;/p&gt;&#10;&#10;&lt;p&gt;In the case of simple regression, it's usually denoted $s_{\hat \beta}$, as here:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Simple_linear_regression#Normality_assumption&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Simple_linear_regression#Normality_assumption&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;also see&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Proofs_involving_ordinary_least_squares&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Proofs_involving_ordinary_least_squares&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;For multiple regression, it's a little more complicated, but if you don't know what these things are it's probably best to understand them in the context of simple regression first.&lt;/p&gt;&#10;&#10;&lt;p&gt;ii) &lt;code&gt;t value&lt;/code&gt; is the value of the t-statistic for testing whether the corresponding regression coefficient is different from 0.&lt;/p&gt;&#10;&#10;&lt;p&gt;The formula for computing it is given at the first link above.&lt;/p&gt;&#10;&#10;&lt;p&gt;iii) &lt;code&gt;Pr.&lt;/code&gt; is the &lt;a href=&quot;http://en.wikipedia.org/wiki/P-value&quot; rel=&quot;nofollow&quot;&gt;p-value&lt;/a&gt; for the hypothesis test for which the t value is the test statistic. It tells you the probability of a test statistic at least as unusual as the one you obtained, &lt;em&gt;if the null hypothesis were true&lt;/em&gt;. In this case, the null hypothesis is that the true coefficient is zero; if that probability is low, it's suggesting that it would be rare to get a result as unusual as this if the coefficient were really zero.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Most importantly, which variables should I look at to ascertain on whether a model is giving me good prediction data?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;What do you mean by 'good prediction data'? Can you make it clearer what you're asking?&lt;/p&gt;&#10;&#10;&lt;p&gt;The square root of the &lt;code&gt;Residual standard error&lt;/code&gt;, which is usually called $s$, represents the standard deviation of the residuals.  It's a measure of how close the fit is to the points.&lt;/p&gt;&#10;&#10;&lt;p&gt;The &lt;code&gt;Multiple R-squared&lt;/code&gt;, also called the &lt;a href=&quot;http://en.wikipedia.org/wiki/Coefficient_of_determination&quot; rel=&quot;nofollow&quot;&gt;coefficient of determination&lt;/a&gt; is the proportion of the variance in the data that's explained by the model. The more variables you add - even if they don't help - the larger this will be. The &lt;a href=&quot;http://en.wikipedia.org/wiki/Coefficient_of_determination#Adjusted_R2&quot; rel=&quot;nofollow&quot;&gt;&lt;code&gt;Adjusted&lt;/code&gt; one&lt;/a&gt; reduces that to account for the number of variables in the model.&lt;/p&gt;&#10;&#10;&lt;p&gt;The &lt;a href=&quot;http://en.wikipedia.org/wiki/F-test#Regression_problems&quot; rel=&quot;nofollow&quot;&gt;$F$ statistic&lt;/a&gt; on the last line is telling you whether the regression as a whole is performing 'better than random' - any set of random predictors will have some relationship with the response. This is used for a test of whether the model outperforms 'random noise' as a predictor. The p-value in the last row is the p-value for that test.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Where do the data come from? Is this in some package?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-17T00:27:34.157" Id="59251" LastActivityDate="2013-05-17T00:44:00.860" LastEditDate="2013-05-17T00:44:00.860" LastEditorUserId="805" OwnerUserId="805" ParentId="59250" PostTypeId="2" Score="4" />
  <row AcceptedAnswerId="59288" AnswerCount="2" Body="&lt;p&gt;From &lt;a href=&quot;http://core.ecu.edu/psyc/wuenschk/stathelp/PowerAnalysis_Overview.doc&quot; rel=&quot;nofollow&quot;&gt;a note&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;strong&gt;A Priori Power Analysis.&lt;/strong&gt;  This is an important part of planning research.  You determine how many cases you will need to have a good chance of detecting an effect of a specified size with the desired amount of power.  See my document Estimating the Sample Size Necessary to Have Enough Power for required number of cases to have 80% for common designs.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;&lt;strong&gt;A Posteriori Power Analysis.&lt;/strong&gt; Also know as “post hoc” power analysis.  Here you find how much power you would have if you had a specified number of cases.  Is it “a posteriori” only in the sense that you provide the number of number of cases, as if you had already conducted the research. Like “a priori” power analysis, it is best used in the planning of research – for example, I am planning on obtaining data on 100 cases, and I want to know whether or not would give me adequate power.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;&lt;strong&gt;Retrospective Power Analysis.&lt;/strong&gt;  Also known as “observed power.”  There are several types, but basically this involves answering the following question:  “If I were to repeat this research, using the same methods and the same number of cases, and if the size of the effect in the population was exactly the same as it was in the present sample, what would be the probability that I would obtain significant results?”  Many have demonstrated that this question is foolish, that the answer tells us nothing of value, and that it has led to much mischief.  See this discussion from Edstat-L.  I also recommend that you read Hoenig and Heisey (The American Statistician, 2001, 55, 19-24).  A few key points:&lt;/p&gt;&#10;  &#10;  &lt;ul&gt;&#10;  &lt;li&gt;Some stat packs (SPSS) give you “observed power” even though it is useless.&lt;/li&gt;&#10;  &lt;li&gt;“Observed power” is perfectly correlated with the value of p – that is, it provides absolutely no new information that you did not already have.&lt;/li&gt;&#10;  &lt;li&gt;It is useless to conduct a power analysis AFTER the research has been completed.  What you should be doing is calculating confidence intervals for effect sizes.&lt;/li&gt;&#10;  &lt;/ul&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I was confused what differences and relation are between Retrospective Power Analysis and A Posteriori power analysis?  I think they both have a given sample size and try to estimate the power?&lt;/p&gt;&#10;&#10;&lt;p&gt;What does &quot;the size of the effect in the population was exactly the same as it was in the present sample&quot; mean?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks and regards!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-17T05:09:00.913" Id="59262" LastActivityDate="2013-05-18T20:41:30.220" LastEditDate="2013-05-17T15:04:17.197" LastEditorUserId="88" OwnerUserId="1005" PostTypeId="1" Score="3" Tags="&lt;power-analysis&gt;" Title="Differences and relation between retrospective power analysis and a posteriori power analysis?" ViewCount="878" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Given two weighted Gaussians with arbitrary weight, mean and variance, what is the parameters of the minimum enclosing Gaussian? The mean and variance should be chosen such that the weight is minimum such as to provide a tight upper bound on the positive differences between values of the two approximated initial Gaussians and the &quot;enclosing&quot; Gaussian. The maximum relative error is determined by a given approximation bound epsilon &gt; 0.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/63MK4.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Figure: given the two blue kernels (weight = 1, mean = -1, var = 2) and (weight = 2, mean = 2, var = 1), a solution for epsilon = 0.001 could look like the red Gaussian (weight ~= 5.2). How to calculate its parameters?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-05-17T09:19:46.520" FavoriteCount="1" Id="59282" LastActivityDate="2013-05-18T20:27:06.753" LastEditDate="2013-05-18T20:27:06.753" LastEditorUserId="25775" OwnerUserId="25775" PostTypeId="1" Score="0" Tags="&lt;normal-distribution&gt;&lt;approximation&gt;&lt;minimum&gt;" Title="Minimum enclosing Gaussian" ViewCount="99" />
  <row Body="&lt;p&gt;Assume for simplicity that your model is defined by only one parameter $\theta$. The power is the function $\theta \mapsto \Pr(\text{reject } H_0 \mid \theta)$, which depends on the sample size $n$.&lt;/p&gt;&#10;&#10;&lt;p&gt;In &lt;strong&gt;Retrospective Power Analysis&lt;/strong&gt;, you only plug in your estimate $\theta$: you look at the value $\Pr(\text{reject } H_0 \mid \hat\theta)$ at the power function at $\theta=\hat\theta$, with the same sample size $n$. It answers the question: &lt;em&gt;&quot;what would be the probability that I would obtain significant results if $\theta$ were $\hat\theta$&quot;&lt;/em&gt; ? As said in your text this question is rather useless because there is a one-to-one correspondance between the $p$-value and the restrospective power $\Pr(\text{reject } H_0 \mid \hat\theta)$.&lt;br&gt;&#10;For instance consider a binomial experiment with proportion parameter $\theta \in [0,1]$ and the hypothesis $H_0\colon\{\theta=0\}$. Obviously the power increases when $\theta$ increases. And obviously the $p$-value decreases when $\hat\theta$ increases. Consequently the lower $p$-value, the higher RP (retrospective power). A couple of years ago I wrote a R code for the case of Fisher tests in classical Gaussian linear models. &lt;a href=&quot;http://forums.cirad.fr/logiciel-R/viewtopic.php?t=2452&quot; rel=&quot;nofollow&quot;&gt;It is here.&lt;/a&gt; There's a code using simulations for the one-way ANOVA example and a code for the general model provding an exact calculation of RP in function of the $p$-value and the design parameters. I called my function &lt;code&gt;PAP()&lt;/code&gt; because &quot;Puissance a posteriori&quot; is the French translation of RP and PAP is also an acronym for &quot;Power Approach Paradox&quot;. The cause of the decreasing correspondence between $p$ and RP for Gaussian linear models is intuitively the same as for the binomial experiment: if $\theta$ is &quot;far from $H_0$&quot; then the power at $\theta$ is high, and if $\hat\theta$ is &quot;far from $H_0$&quot; then the $p$-value is small. Theoretically this is a consequence of the fact that the noncentral Fisher distributions are stochastically increasing in the noncentrality parameter (see &lt;a href=&quot;http://stats.stackexchange.com/questions/58107/conditional-expectation-of-r-squared&quot;&gt;this discussion&lt;/a&gt; about noncentral $F$ distributions in Gaussian linear models). In fact here the noncentrality parameter plays the role of $\theta$ (is it the so-called &lt;em&gt;effect size&lt;/em&gt; ? I don't know).&lt;br&gt;&#10;I claimed &quot;RS is rather useless because of the correspondence with $p$&quot; because this decreasing correspondence with $p$ means that &lt;em&gt;having a high RP is equivalent to having a small $p$, and vice-versa&lt;/em&gt;. But the more serious problem is the misinterpretation of RP;  for instance, I have found such claims in the literature:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;$H_0$ is not rejected and RP is high, so the decision of the test is significant.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;$H_0$ is not rejected, it is not surprising because RP is low. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;$H_0$ is rejected (so the decision is significant) and RP is high, so the decision is even more significant.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Respectively replace &quot;RP is high&quot; and &quot;RP is low&quot; with &quot;$p$ is low&quot; and &quot;$p$ is high&quot; in the three claims above and you will see that they are either useless, wrong, or puzzling.&lt;br&gt;&#10;From a more &quot;philosophical&quot; perspective, RP is useless because why would we mind about the probability that rejection of $H_0$ occurs once the experiment is done ?&lt;br&gt;&#10;See also here a funny but clever &lt;a href=&quot;http://www.onetrickwords.com/retro.htm&quot; rel=&quot;nofollow&quot;&gt;retrospective power online calculator&lt;/a&gt; ;-)&lt;/p&gt;&#10;&#10;&lt;p&gt;The paragraph &lt;strong&gt;A Posteriori Power Analysis&lt;/strong&gt; says nothing about the choice of $\theta$, but it emphasizes the main difference with the retrospective power: here the goal is to use the information issued from your first experiment to evaluate the power of a future experiment, focusing on the sample size. A sensible approach to evaluate this power is to consider your estimate $\hat\theta$ as a &quot;guess&quot; of the true $\theta$ and also to &lt;em&gt;consider the uncertainty about this estimate&lt;/em&gt;. There is a natural way to do so in Bayesian statistics, namely the &lt;em&gt;predictive power&lt;/em&gt;, which consists to average the possible values of $\Pr(\text{reject } H_0 \mid \theta)$ for various values of $\theta$, according to some distribution (the posterior distribution in Bayesian terms) representing the knowledge and the uncertainty about $\theta$ resulting from your first experiment. In the frequentist framework you could consider the values of the power evaluated at the bounds of your confidence interval about $\theta$.&lt;/p&gt;&#10;" CommentCount="16" CreationDate="2013-05-17T10:14:58.797" Id="59288" LastActivityDate="2013-05-18T20:41:30.220" LastEditDate="2013-05-18T20:41:30.220" LastEditorUserId="8402" OwnerUserId="8402" ParentId="59262" PostTypeId="2" Score="5" />
  <row AcceptedAnswerId="59317" AnswerCount="1" Body="&lt;p&gt;I have two groups of people, a healthy population which has carriers of disease and non-carriers, and I have a sick population.  I know the frequency of genetic variants in both groups.  I want to know if I can say anything about the variants with any statistical certainty and how I would do that.  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Variant      Frequency in Healthy Population    Frequency in Sick Population&#10;X            30                                 25&#10;Y            5                                  700&#10;Z            600                                600&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Each group (Healthy and Sick) has 1000 people in it.  Can I say anything about each variant?  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-17T15:04:15.700" FavoriteCount="0" Id="59308" LastActivityDate="2013-05-18T10:48:22.183" LastEditDate="2013-05-18T10:48:22.183" LastEditorUserId="17230" OwnerUserId="25786" PostTypeId="1" Score="3" Tags="&lt;genetics&gt;&lt;biostatistics&gt;" Title="Detecting variants of unknown significance" ViewCount="52" />
  <row AnswerCount="2" Body="&lt;p&gt;I am measuring time of a computer operation. The operation should run roughly same time each time I measure it. How many times should I measure it to get good average and standard deviation?&lt;/p&gt;&#10;&#10;&lt;p&gt;originally posted here: &lt;a href=&quot;http://physics.stackexchange.com/questions/64917/how-many-measurements-should-be-done&quot;&gt;http://physics.stackexchange.com/questions/64917/how-many-measurements-should-be-done&lt;/a&gt; , but I think that this forum is more appropriate for my question&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-05-17T17:03:54.627" Id="59318" LastActivityDate="2013-05-18T03:01:31.767" OwnerUserId="25789" PostTypeId="1" Score="1" Tags="&lt;measurement&gt;" Title="How many measurements should be done?" ViewCount="48" />
  <row AnswerCount="1" Body="&lt;p&gt;I am trying to determine the factor structure of a set of 84 items. Exploratory factor analysis using varimax rotation was conducted to estimate the underlying factor structure for the sample data. Two rounds of exploratory factor analysis were conducted. Items with loading below the generally accepted 0.60 threshold and those that highly cross-loaded on more than one factor were rejected. However, the overall results of the exploratory factor analysis shows that items for two different constructs load on a same factor. &lt;/p&gt;&#10;&#10;&lt;h3&gt;Questions&lt;/h3&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;strong&gt;How should I interpret the fact that items for two constructs load on the one factor?&lt;/strong&gt; &lt;/li&gt;&#10;&lt;li&gt;&lt;strong&gt;Is this a problem?&lt;/strong&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;strong&gt;If yes, what could I do?&lt;/strong&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="2" CreationDate="2013-05-17T19:40:11.037" FavoriteCount="0" Id="59330" LastActivityDate="2013-05-18T03:16:05.683" LastEditDate="2013-05-18T02:42:09.340" LastEditorUserId="183" OwnerUserId="25795" PostTypeId="1" Score="2" Tags="&lt;factor-analysis&gt;&lt;scales&gt;" Title="Items from two constructs load on one factor in factor analysis" ViewCount="796" />
  <row AnswerCount="0" Body="&lt;p&gt;I have started to use the &lt;code&gt;GENLIN&lt;/code&gt; procedure in SPSS more than any of the specific dialogues, but I don't understand the Scale parameter or why it has the effects it does on the regression results.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's an SPSS example: &lt;em&gt;code block 1&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Normally, if I wanted to see if &lt;em&gt;set&lt;/em&gt; has an effect on the linear parameters $b_0$ and $b_1$, I'd could just do linear regression with the interaction term &lt;em&gt;set1X&lt;/em&gt;: &lt;em&gt;code block 2&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The results are obvious: &lt;em&gt;set&lt;/em&gt; affects slope but not the intercept of my model (as I'd expect since I generated these sample data that way). But if I use &lt;code&gt;GENLIN&lt;/code&gt; instead of &lt;code&gt;REGRESSION&lt;/code&gt;, my sig. values and 95%CIs for the parameters are different: &lt;em&gt;code block 3&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;This Scale parameter (regressed by &lt;code&gt;GENLIN&lt;/code&gt; as 1.127 with a SE of 0.3563) seems to make the difference. If I change the scale to &lt;code&gt;PEARSON&lt;/code&gt; (for Pearson's chi sq.), &lt;code&gt;DEVIANCE&lt;/code&gt; (?), or a constant like 1, I get all different answers.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Summary&lt;/strong&gt;: what is Scale in Generalized Linear Models (&lt;code&gt;GENLIN&lt;/code&gt; in SPSS) and how should I handle it? Why doesn't OLS Linear regression use such a parameter? How do I know how to assign scale?&lt;/p&gt;&#10;&#10;&lt;h2&gt;Code block 1&lt;/h2&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;data list list /X set Y.&#10;begin data.&#10;1   1   7.0&#10;2   1   7.8&#10;3   1   12.4&#10;4   1   14.8&#10;5   1   19.0&#10;6   1   22.7&#10;7   1   24.4&#10;8   1   25.5&#10;9   1   29.5&#10;10  1   31.0&#10;1   2   7.9&#10;2   2   12.7&#10;3   2   14.3&#10;4   2   20.1&#10;5   2   20.8&#10;6   2   26.5&#10;7   2   30.9&#10;8   2   35.8&#10;9   2   38.0&#10;10  2   43.7&#10;end data.&#10;dataset name exampleData WINDOW=front.&#10;variable level X (scale) Y (scale) set (nominal).&#10;&#10;compute set1 = (set=1).&#10;compute set1X = set1*X.&#10;EXECUTE.&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;h2&gt;Code block 2&lt;/h2&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;REGRESSION&#10;  /MISSING LISTWISE&#10;  /STATISTICS COEFF OUTS CI(95) R ANOVA CHANGE&#10;  /CRITERIA=PIN(.05) POUT(.10)&#10;  /NOORIGIN &#10;  /DEPENDENT Y&#10;  /METHOD=FORWARD X set1 set1X.&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;h2&gt;Code block 3&lt;/h2&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;* Generalized Linear Models.&#10;GENLIN Y BY set (ORDER=ASCENDING) WITH X&#10;  /MODEL X set*X INTERCEPT=YES&#10; DISTRIBUTION=NORMAL LINK=IDENTITY&#10;  /CRITERIA SCALE=MLE COVB=MODEL PCONVERGE=1E-006(ABSOLUTE) SINGULAR=1E-012 ANALYSISTYPE=3(WALD) &#10;    CILEVEL=95 CITYPE=WALD LIKELIHOOD=FULL&#10;  /MISSING CLASSMISSING=EXCLUDE&#10;  /PRINT CPS DESCRIPTIVES MODELINFO FIT SUMMARY SOLUTION.&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2013-05-17T20:22:53.510" Id="59333" LastActivityDate="2013-05-17T20:22:53.510" OwnerUserId="8807" PostTypeId="1" Score="3" Tags="&lt;spss&gt;&lt;generalized-linear-model&gt;" Title="What does the Scale parameter mean in linear regression?" ViewCount="476" />
  <row Body="&lt;p&gt;I know this isn't necessarily what you are looking for, but I suggested to the &quot;improvement request&quot; folks at my work that they ask for variation on the bean plot as option for variability plots in one of MatLab's competitors.  We are a relatively big customer so a useful proportion of our requests actually do get implemented in the product.  It sometimes takes a year or three before it is rolled out.&lt;/p&gt;&#10;&#10;&lt;p&gt;I see the bean plot as having good value for EDA in my work environment.  If their competitors have it and their competitors customers are using it for good value then there is incentive for MathWorks to implement it in MatLab.  Eventually there is good incentive, it takes time.&lt;/p&gt;&#10;&#10;&lt;p&gt;That is how to make it happen natively inside MatLab.  That is not how to hack it to work with existing MatLab methods.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-17T22:15:23.127" Id="59341" LastActivityDate="2013-05-17T22:15:23.127" OwnerUserId="22452" ParentId="30965" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;The second approach based on a correlation won't tell you anything about the direction of the trend.  At its base it is the same as the first approach - fits a simple linear regression - but it is giving you one of the least useful bits of output from fitting that model.  So of the two, the first approach, based on linear regression, is definitely the best.  &lt;/p&gt;&#10;&#10;&lt;p&gt;However, it will still be highly flawed.  &lt;/p&gt;&#10;&#10;&lt;p&gt;First, the linear model seems unlikely to be appropriate.  What happens when the heart rate starts low, goes high, and returns to a low point?  Your regression will show a horizontal line.&lt;/p&gt;&#10;&#10;&lt;p&gt;Second, even if were, ordinary least squares as interpreted in a usual regression situation (with standard F statistics and standard errors of coefficient estimates etc) gives misleading estimates with time series data.  This is because classic regression modelling inference assumes the observations are independent and identically distributed, which is obviously not the case - each observation in a short time period such as this adds only limited extra information to what you got from the previous observation/s.&lt;/p&gt;&#10;&#10;&lt;p&gt;A recommendation on a better approach would depend more on what your underlying interest is in the trend or tendency.  A locally weighted scatterplot smoother to describe the trend may  be a possible solution if it is a matter of describing the trend in each time window - or it may not, depending on what you're interested in.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-18T04:39:28.400" Id="59357" LastActivityDate="2013-05-18T04:39:28.400" OwnerUserId="7972" ParentId="59355" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;Heart rates vary in a cyclic pattern that is driven by the respiratory rate. Inspiration causes decreased filling of the left atrium and the heart rate increase to maintain cardiac output. You need to detrend the respiratory influence.&lt;/p&gt;&#10;&#10;&lt;p&gt;Because the instantaneous heart rate is just the inverse of the RR interval, you do not need to wait for 15 or 20 seconds to calculate a rate. You can just use 1/RR-interval. Then you can calculate a probability that the HR is increasing by noticing when the RR interval has been below the detrended mean value for more than 8 RR intervals. (That's a simple binomial calculation. The probability that 8 successive intervals will be below the mean (actually the median) value is just 0.5^5 = 0.0315. You might be able to enhance this by taking into account how far below the median the RR intervals really were, but then you would also need to be careful how you interpreted the influence of PVCs with their much shorter RR-intervals. But if 3 successive RR intervals were below 0.0066 seconds (at a rate above 150/min) you would have a strong signal that the person had either &quot;V-tach&quot; or had paroxysmal atrial tachycardia.&lt;/p&gt;&#10;&#10;&lt;p&gt;(I'm only a masters-level statistician, epidemiologist actually, but am also a physician so I might know a bit about the domain-specific issues).)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-18T06:00:09.647" Id="59362" LastActivityDate="2013-05-18T06:07:45.600" LastEditDate="2013-05-18T06:07:45.600" LastEditorUserId="2129" OwnerUserId="2129" ParentId="59355" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;I'm cheating, but mathematica finds another expression for the sum on the right (involving the &lt;a href=&quot;http://en.wikipedia.org/wiki/Hypergeometric_function&quot; rel=&quot;nofollow&quot;&gt;Hypergeometric function&lt;/a&gt;):&#10;$$ &#10;\sum_{k=0}^y \left[\binom{y}{k}\right]^2p^k(1-p)^{n-k}=(1-p)^{n}\cdot \text{Hypergeometric}~_{2}F_{1}\left(-y, -y, 1, \frac{p}{1-p}\right)&#10;$$&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-05-18T07:07:24.717" Id="59364" LastActivityDate="2013-05-19T07:22:04.617" LastEditDate="2013-05-19T07:22:04.617" LastEditorUserId="21054" OwnerUserId="21054" ParentId="59328" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;i) First, a recommendation: &lt;/p&gt;&#10;&#10;&lt;p&gt;Use &lt;code&gt;pchisq( -2*sum(log(p-values)), df, lower.tail=FALSE)&lt;/code&gt; instead of &lt;code&gt;1- ...&lt;/code&gt; - you're likely to end up with &lt;a href=&quot;http://en.wikipedia.org/wiki/Loss_of_significance&quot; rel=&quot;nofollow&quot;&gt;more accuracy&lt;/a&gt; for small p-values. To see that they're sometimes going to give different results, try this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; x=70;c(1-pchisq(x,1),pchisq(x,1,lower.tail=FALSE))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;ii) Yes, it's one-sided. Small values of the chi-square statistic indicate that the component p-values tend to be large (that is, a lack of evidence against the overall null). Imagine you were doing a t-test and the sample means were &lt;em&gt;really, really close together&lt;/em&gt;... i.e. $|t|$ was unusually small. Would you reject the null hypothesis that they were equal because they were unusually close together?&lt;/p&gt;&#10;&#10;&lt;p&gt;Clearly not. You might conclude something else was wrong (like one of your assumptions could be faulty, or you used a &lt;em&gt;really&lt;/em&gt; bad test, or your calculation might be wrong, or someone fiddled the data, or ...) - but you wouldn't conclude the means were &lt;em&gt;different&lt;/em&gt; because they were surprisingly close!&lt;/p&gt;&#10;&#10;&lt;p&gt;Indeed - what would you do in that situation:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; t.test(x,y,var.equal=TRUE)&#10;&#10;    Two Sample t-test&#10;&#10;data:  x and y&#10;t = 1e-04, df = 18, p-value = 0.9999&#10;alternative hypothesis: true difference in means is not equal to 0&#10;95 percent confidence interval:&#10; -0.7213824  0.7214315&#10;sample estimates:&#10; mean of x  mean of y &#10;-0.2161466 -0.2161711 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So there's a two sample t-test with $p$ &lt;em&gt;really&lt;/em&gt; close to 1 (~0.999944). What do you conclude?&lt;/p&gt;&#10;&#10;&lt;p&gt;So now, with a goodness of fit, what kinds of things might a p-value really close to 1 tell you?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-05-18T08:02:59.460" Id="59365" LastActivityDate="2013-05-19T01:36:44.090" LastEditDate="2013-05-19T01:36:44.090" LastEditorUserId="805" OwnerUserId="805" ParentId="59360" PostTypeId="2" Score="3" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have this confusion related to minimization of gaussian likelihood function. The negative of the log likelihood of gaussian distribution is &lt;/p&gt;&#10;&#10;&lt;p&gt;$-\log \det(Q) + \text{tr}(SQ) + \lambda||Q||_{1}$ where $Q$ is the precision matrix to be estimated&lt;/p&gt;&#10;&#10;&lt;p&gt;I am following this &lt;a href=&quot;http://arxiv.org/pdf/1111.5479v2.pdf&quot; rel=&quot;nofollow&quot;&gt;paper&lt;/a&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;It's been mentioned that &lt;/p&gt;&#10;&#10;&lt;p&gt;Using sub-gradient notation, we can write the optimality conditions (aka “normal equations”) for a solution to the above equation as&lt;/p&gt;&#10;&#10;&lt;p&gt;$-Q^{-1} + S + \lambda\Gamma = 0$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\Gamma_{jk} = \text{sign}(Q_{jk})$ if $Q_{jk} \neq 0$&#10;and&lt;/p&gt;&#10;&#10;&lt;p&gt;$\Gamma_{jk} \in [-1, 1]$ if $Q_{jk} = 0$&lt;/p&gt;&#10;&#10;&lt;p&gt;I didn't get how come &lt;/p&gt;&#10;&#10;&lt;p&gt;$\Gamma_{jk} \in [-1, 1]$ if $Q_{jk} = 0$&lt;/p&gt;&#10;&#10;&lt;p&gt;Can anyone please explain it should be 0 isn't it. I didn't get how the interval $[-1, 1]$ came&lt;/p&gt;&#10;&#10;&lt;p&gt;Suggestions?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-18T15:40:09.217" Id="59387" LastActivityDate="2013-05-22T15:09:17.453" LastEditDate="2013-05-22T14:28:44.627" LastEditorUserId="8539" OwnerUserId="12329" PostTypeId="1" Score="0" Tags="&lt;normal-distribution&gt;&lt;optimization&gt;&lt;covariance&gt;" Title="Confusion related to minimization of a gaussian likelihood function" ViewCount="60" />
  <row Body="&lt;pre&gt;&lt;code&gt;########################################################################################&#10;#These values for n, m, x, y,a, and b are just randomly chosen for illustration.  These values will come from your data&#10;n = 100&#10;m = 100&#10;x = rbinom(n,1,.7)&#10;y = rbinom(m,1,.3)&#10;a = .2&#10;b = .1&#10;&#10;########################################################################################&#10;#R Code &#10;#You need to pick values for your priors, i.e., a = ? and b = ?&#10;&#10;#Set B to be any large number you want&#10;B = 1000&#10;diff = rep(NA,B)&#10;&#10;for(i in 1:B){&#10;&#10;#Obtain posterior samples from x and y&#10;posterior.x.samples = rbeta(1000,a+sum(x),b+n-sum(x))&#10;posterior.y.samples = rbeta(1000,a+sum(y),b+m-sum(y))&#10;&#10;#Calculate posterior expectations&#10;Ex = mean(posterior.x.samples)&#10;Ey = mean(posterior.y.samples)&#10;&#10;#Take the difference in posterior means&#10;diff[i] = Ex-Ey&#10;&#10;}&#10;&#10;hist(diff,xlab=&quot;Difference in Posterior Expecttaions&quot;,prob=T,&#10;     main=&quot;Histogram&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2013-05-18T16:22:06.157" Id="59390" LastActivityDate="2013-05-18T16:22:06.157" OwnerDisplayName="user25658" ParentId="59388" PostTypeId="2" Score="1" />
  
  <row AnswerCount="1" Body="&lt;p&gt;The question:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Find a 99% confidence interval for the difference in mean oxygen&#10;  consumption of an algae that is in 100% concentrated seawater with&#10;  algae that is not in 100% seawater (i.e., 50% or 75%).&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Is this the correct approach?&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;If we let $X_1$ be the random variable corresponding to the mean&#10;  oxygen consumption for algae in 100% seawater, $X_2$ for 50% and $X_3$&#10;  for 75% then I want to find the standard error of $X_1+\frac{X_2+X_3}{2}$.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;If so, I know how to proceed from there. Otherwise, I don't know how to start this.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-05-18T19:39:21.410" Id="59405" LastActivityDate="2014-04-21T23:06:22.320" LastEditDate="2013-05-18T21:36:23.557" LastEditorUserId="7290" OwnerUserId="25823" PostTypeId="1" Score="0" Tags="&lt;self-study&gt;&lt;confidence-interval&gt;" Title="Confidence interval for the difference between $X_1$ and $X_2$ or $X_3$" ViewCount="61" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I have this set of data&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x       y  &#10;0.5 306.3&#10;1   622.3&#10;2   1230.1&#10;3   2017.7&#10;4   2589&#10;5   3175.4&#10;6   3751.9&#10;9   5585.7&#10;12  7388.8&#10;15  9218.8&#10;20  12042.8&#10;25  15056.5&#10;30  18081.2&#10;45  27095.3&#10;60  35609.2&#10;80  47263.1&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;When I plot x vs y with R I obtain an almost linear plot, when I plot them with Libre Office Calc I obtain a parabolic plot. &lt;/p&gt;&#10;&#10;&lt;p&gt;How is that possible? &lt;/p&gt;&#10;&#10;&lt;p&gt;Which one is the right one?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-05-19T09:11:17.527" Id="59421" LastActivityDate="2014-12-01T04:12:11.280" LastEditDate="2013-05-19T10:12:25.933" LastEditorUserId="805" OwnerUserId="25392" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;data-visualization&gt;&lt;calc&gt;" Title="different results using R or Libre Office calc" ViewCount="409" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have to compare the slopes of 2 regression lines with R. The 2 regressions are made with the same parameters in 2 different locations.&lt;/p&gt;&#10;&#10;&lt;p&gt;I did my regressions with the function lm(). Now I have the results but I don't know how to compare them..&lt;/p&gt;&#10;&#10;&lt;p&gt;I tried with student test or ANOVA but it requires more than the 2 slopes.&lt;/p&gt;&#10;&#10;&lt;p&gt;I looked for an answer during something like 2 hours but didn't find, so I ask the question here : How should I do my test? &lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you in advance,&#10;b.raoul&lt;/p&gt;&#10;" ClosedDate="2014-07-19T02:39:51.883" CommentCount="5" CreationDate="2013-05-19T12:21:33.753" Id="59434" LastActivityDate="2013-05-19T12:21:33.753" OwnerUserId="25840" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;regression&gt;" Title="How to compare 2 regression slopes with R?" ViewCount="1186" />
  
  <row AnswerCount="0" Body="&lt;p&gt;Is there a handy plot for comparing the variance-covariance matrices of two (or perhaps more) groups?  An alternative to looking at lots of marginal plots, especially in the multivariate Normal case?&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2013-05-19T13:31:45.480" Id="59446" LastActivityDate="2013-05-19T13:31:45.480" OwnerUserId="17230" PostTypeId="1" Score="3" Tags="&lt;data-visualization&gt;&lt;multivariate-analysis&gt;" Title="Diagnostic plot for assessing homogeneity of variance-covariance matrices" ViewCount="328" />
  <row Body="&lt;p&gt;Have you considered building a model from the class of ARIMA models? They're quite good at modelling periodic data.&lt;/p&gt;&#10;&#10;&lt;p&gt;Using an ARIMA(2,0,2) or equivalently an ARMA(2,2) with none of the coefficients constrained to zero, I was able to make an improvement on your fit. See the figure below. The raw data is in blue, the predicted values are in red.&lt;/p&gt;&#10;&#10;&lt;p&gt;I can return to this answer later (I'm under a time constraint at the moment) and perhaps include some python code for you. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/M7QZQ.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-19T14:18:55.773" Id="59448" LastActivityDate="2013-05-19T14:18:55.773" OwnerUserId="24617" ParentId="59423" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;The conditions on $F$ are&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;$F(0)=0$&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;$F(\overline{\alpha}) = 1$&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;$F'(x) \gt 0$ for all $x \in (0, \overline{\alpha})$&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;$\int_0^{\overline{\alpha}} x F'(x) dx = 1$.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;$F'$ is differentiable.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;These are all &lt;em&gt;nonlocal&lt;/em&gt; in the sense that sufficiently small (smooth) perturbations of $F$ within small neighborhoods of any finite discrete set of points in $(0, \overline{\alpha})$ can be found which preserve them all. By applying such a local perturbation we can modify $F'$ within a narrow interval to be as large as we want without changing any of the conditions.&lt;/p&gt;&#10;&#10;&lt;p&gt;The figures below may help in following this argument.&lt;/p&gt;&#10;&#10;&lt;p&gt;I claim there exists at least one $x_0 \in (0, \overline{\alpha})$ for which $(N-1)(1-\delta+\delta F(x_0)) - \delta x_0 F'(x_0) \gt 0$.  For if not, a simple comparison shows that $F(0)$ must be less than the value at $0$ of the (unique) solution to the first-order ordinary differential equation for $F$,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$L[F](x) = (N-1)(1-\delta+\delta F(x)) - \delta x F'(x) = 0, \quad F(\overline{\alpha}) = 1.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;This equation can explicitly be solved and its value at $0$ found to equal $1 - 1/\delta \lt 0$, whence $F(0) \lt 0$, violating the first condition.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's perturb $F$ to $\widehat{F}$. The following argument needs only routine constructions to be made rigorous: mix in tiny amounts of a distribution supported in an arbitrarily small neighborhood of $x_0$ and another distribution supported in a neighborhood of an $x_1$ on the other side of $1$ from $x_0$ in such a way that the expectation of $F$ remains unchanged. By making the first of these mixed-in distributions have sufficiently small support, we can cause it to increase $F'$ at $x_0$ by any desired amount while changing $F(x_0)$ arbitrarily little. In this fashion we can cause $L[\widehat{F}](x_0)$ to range from $L[F](x_0)$ down to $-\infty$ in a continuous manner.  Do this so that $L[\widehat{F}](x_0) = 0$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Therefore, there exist &lt;em&gt;no&lt;/em&gt; restrictions whatsoever on any of the moments of $F$.  That explains why you had such difficulty obtaining any!&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/912YX.png&quot; alt=&quot;Figures&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In these figures, the plots associated with the original $F$ are shown in blue and those associated with the perturbed CDF $\widehat{F}$ are shown in red.  As $\varepsilon$ shrinks, the two spikes in the PDF and $L[F]$ grow longer (vertically).  Here, I noticed that $L[F](3/2) \gt 0$, and so perturbed $F$ near $3/2$ and near a counterbalancing value $x_1 = 1/2$.  This creates two tiny &lt;em&gt;apparent&lt;/em&gt; jumps around $3/2$ and $1/2$ in $F$, but upon closer inspection they are smooth--just steep.  Their steepness makes $L[F]$ small.  Because the jumps can be made arbitrarily steep, the spikes in $L[F]$ can be extended below zero, creating a zero-crossing: that solves the problem.  (Just for fun I have used an $F$ that fails to be twice differentiable at $0$: $F'$ diverges there.  The construction still goes through.)&lt;/p&gt;&#10;&#10;&lt;p&gt;The point is that we can &lt;em&gt;always&lt;/em&gt; create such spikes (and give them arbitrarily small area, thereby changing $F$ by arbitrarily small amounts), so to obtain a zero-crossing it's enough to show that $L[F]$ must have some neighborhood in which it is positive. But if it is not, $F$ will fail to be a CDF: consistently negative values of $L[F]$ mean that $F'$ is too large, on average, and so if $F$ ends up with a limiting value of $1$ at the right--as it must in order to be a CDF--also it &lt;em&gt;must&lt;/em&gt; have a negative value at $0$, which is not allowable.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2013-05-19T17:17:57.050" Id="59466" LastActivityDate="2013-05-19T20:14:18.850" LastEditDate="2013-05-19T20:14:18.850" LastEditorUserId="919" OwnerUserId="919" ParentId="59450" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to learn the difference between the three approaches and their applications.&lt;/p&gt;&#10;&#10;&lt;p&gt;a) As I understand,&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;AIC = -LL+K &#10;&#10;BIC = -LL+(K*logN)/2&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Unless I am missing something, shouldn't the K that minimizes the AIC minimize BIC as well since N is constant. &lt;/p&gt;&#10;&#10;&lt;p&gt;I looked at this &lt;a href=&quot;http://stats.stackexchange.com/questions/577/is-there-any-reason-to-prefer-the-aic-or-bic-over-the-other/767#767&quot;&gt;thread&lt;/a&gt; but couldn't find a satisfactory answer. &lt;/p&gt;&#10;&#10;&lt;p&gt;b) According to Witten's book on Data Mining (pg 267) the definition of MDL for evaluating the quality of network is the same as BIC. Is there a difference between BIC and MDL?&lt;/p&gt;&#10;&#10;&lt;p&gt;c) What are the different approaches to compute MDL? I am looking for its application in Clustering, Time Series Analysis (ARIMA and Regime Switching) and Attribute Selection. While almost all commonly used packages in R report AIC and BIC, I couldn't find any that implements MDL and I wanted to see if I can write it myself.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-05-19T17:18:04.683" Id="59467" LastActivityDate="2013-05-19T17:18:04.683" OwnerUserId="25180" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;time-series&gt;&lt;data-mining&gt;&lt;aic&gt;&lt;bic&gt;" Title="AIC vs BIC vs MDL" ViewCount="263" />
  
  
  
  
  
  
  <row Body="&lt;p&gt;There is a huge amount of literature on biclustering of gene expression data. Obviously &lt;em&gt;various&lt;/em&gt; stuff has been tried. It's just that the simpler models (assuming independency etc.) are usually more &lt;em&gt;efficient&lt;/em&gt;, i.e. run in reasonable time instead of days if you have a lot of data.&lt;/p&gt;&#10;&#10;&lt;p&gt;Either way, please look up the &lt;em&gt;latest literature&lt;/em&gt; yourself. Any answer we give here will be outdated in half a year by newer publications on this matter.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-20T10:48:22.180" Id="59504" LastActivityDate="2013-05-20T10:48:22.180" OwnerUserId="7828" ParentId="59426" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;Q2. The likelihood ratio's a sensible enough test statistic but (a) the Neyman-Pearson Lemma doesn't apply to composite hypotheses, so the LRT won't necessarily be most powerful; &amp;amp; (b) Wilks' Theorem only applies to nested hypotheses, so unless one family is a special case of the other (e.g. exponential/Weibull, Poisson/negative binomial) you don't know the distribution of the likelihood ratio under the null, even asymptotically.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-05-20T12:00:52.423" Id="59508" LastActivityDate="2013-05-20T12:00:52.423" OwnerUserId="17230" ParentId="59489" PostTypeId="2" Score="5" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;&lt;em&gt;Most of this is background, skip to the end if you already know enough about Dirichlet process mixtures&lt;/em&gt;. Suppose I am modeling some data as coming from a mixture of Dirichlet processes, i.e. let $F \sim \mathcal D(\alpha H)$ and conditional on $F$ assume $$Y_i \stackrel {iid}{\sim} \int f(y | \theta) F(d\theta).$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Here $\alpha &amp;gt; 0$ and $\alpha H$ is the prior base measure. It turns out that if for each observation $Y_i$, if I know the associated latent $\theta_i$, the likelihood of $\alpha$ in this model is $$L(\alpha | t) \propto \frac{\alpha^t\Gamma(\alpha)}{\Gamma(\alpha + n)}$$ where $t$ is the number of distinct values of $\theta_i$ (the random measure $F$ is discrete almost surely). &lt;a href=&quot;http://www.cs.berkeley.edu/~jordan/courses/281B-spring04/readings/escobar-west.pdf&quot;&gt;Escobar and West&lt;/a&gt; develop the following scheme for sampling $\alpha$ using a Gamma prior; first, they write $$&#10;\pi(\alpha | t) &#10;\propto \pi(\alpha) \frac{\alpha^t\Gamma(\alpha)}{\Gamma(\alpha + n)} &#10;\propto \pi(\alpha)\alpha^{t - 1}(\alpha + n){B(\alpha + 1, n)} &#10;\\= \pi(\alpha)\alpha^{t - 1} (\alpha + n) &#10;    \int_0^1 x^\alpha(1 - x)^{n - 1} \ dx,$$&#10;where $B(\cdot, \cdot)$ is the beta function. Then then note that if we introduce a latent parameter $X \sim \mbox{Beta}(\alpha + 1, n )$ then the likelihood has the form of a mixture of Gamma distributions and use this to write down a Gibbs sampler. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Now my question.&lt;/strong&gt; Why can't we just write &#10;$$&#10;L(\alpha | t) \propto \frac{\alpha^t \Gamma(\alpha)}{\Gamma(\alpha + n)}&#10;= \frac{\alpha^t \Gamma(n)\Gamma(\alpha)}{\Gamma(\alpha + n)\Gamma(n)}&#10;= \alpha^t B(\alpha, n) \Gamma(n)&#10;\\ \propto \alpha^t \int_0 ^ 1 x^{\alpha - 1} (1 - x)^{n - 1} \ dx,&#10;$$&#10;and instead of using a mixture of Gamma distributions use a single Gamma distribution? If we introduce $X \sim \mbox{Beta}(\alpha, n)$ shouldn't I be able to do the same thing but without needing to use the mixture?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit for more details More Details:&lt;/strong&gt; To fill in some gaps, the argument in Escobar and West is that, letting $\alpha$ have a Gamma distribution with shape $a$ and mean $a / b$, $$\pi(\alpha | t) \propto \alpha^{a + t - 2} (\alpha + n) e^{-b\alpha} \int_0 ^ 1 x^{\alpha} (1 - x)^{n - 1} \ dx$$ and so we can introduce a latent $X$ so that $$\pi(\alpha, x | t) \propto  \alpha^{a + t - 2} (\alpha + n) e^{-b\alpha}x^{\alpha}(1 - x)^{n - 1}.$$ The full conditionals are a $\mbox{Beta}(\alpha + 1, n)$ distribution for $X$ and a mixture of a $\mathcal G(a + t, b - \log(x))$ and a $\mathcal G(a + t - 1, b - \log(x))$ for $\alpha$. &lt;/p&gt;&#10;&#10;&lt;p&gt;By the same argument, I got the same result but with $\mbox{Beta}(\alpha, n)$ for $X$ and $\mathcal G(a + t, b - \log(x))$ for $\alpha$. This seems easier to me; why don't they just do that?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-20T19:15:37.910" FavoriteCount="1" Id="59548" LastActivityDate="2014-07-23T13:24:53.997" LastEditDate="2013-06-03T17:40:35.097" LastEditorUserId="5339" OwnerUserId="5339" PostTypeId="1" Score="6" Tags="&lt;bayesian&gt;&lt;nonparametric-bayes&gt;" Title="Putting a prior on the concentration parameter in a Dirichlet process" ViewCount="283" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I would like to simulate a multiple linear regression model using R. &lt;/p&gt;&#10;&#10;&lt;p&gt;If I have the skewness and kurtosis for the residuals, how can I do that? &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-05-21T04:59:24.757" Id="59584" LastActivityDate="2013-05-21T14:50:30.610" LastEditDate="2013-05-21T07:24:35.957" LastEditorUserId="3826" OwnerUserId="13614" PostTypeId="1" Score="1" Tags="&lt;multiple-regression&gt;&lt;simulation&gt;&lt;skewness&gt;&lt;kurtosis&gt;" Title="Simulating multiple linear regression" ViewCount="220" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I developed a conditional logit model in Stata. The model is good and the variables are highly significant.&#10;Then I do mfx predicted (PU0) to determine the marginal effects of variables $\frac{dy}{dx}$. The problem is when I get the table of the marginal effects of variables  in Stata: the significance of the variables becomes very small and insignificant. Why and therefore I can  comment or not the marginal effects?&lt;/p&gt;&#10;&#10;&lt;p&gt;Example coefficients of smoker 0.6666 and the significance P value 0.000 but when I put mfx compute to have $\frac{dy}{dx}$ smoker marginal effect is 1.2 and significance is $p= 0.521$. &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-05-21T08:19:26.483" Id="59596" LastActivityDate="2014-05-04T17:34:36.230" LastEditDate="2014-05-04T17:34:36.230" LastEditorUserId="26338" OwnerUserId="25910" PostTypeId="1" Score="2" Tags="&lt;statistical-significance&gt;&lt;stata&gt;&lt;logit&gt;&lt;marginal&gt;&lt;effects&gt;" Title="Marginal effect clogit not significant" ViewCount="393" />
  
  <row Body="&lt;p&gt;Similar to Stijn's comment, there isn't sufficient information to make a conclusive judgment.  There could be fraud (in either of the papers, not just the first), or there could have been a Type I / Type II error in one of the papers.&lt;/p&gt;&#10;&#10;&lt;p&gt;Out of interest if the two studies were identical and independent, so had identical power functions which factored, would it make sense to compare (ruling out fraud) the scenario for $D=0$:&lt;/p&gt;&#10;&#10;&lt;p&gt;$P(A=false positive, B=true negative)=\alpha(1 -\alpha)$&lt;/p&gt;&#10;&#10;&lt;p&gt;Against $D&amp;gt;0$&lt;/p&gt;&#10;&#10;&lt;p&gt;$P(A=true positive, B=false negative)=(1 -\beta)\beta$&lt;/p&gt;&#10;&#10;&lt;p&gt;And going with the more likely scenario?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-05-21T09:21:53.070" Id="59600" LastActivityDate="2013-05-21T09:21:53.070" OwnerUserId="16663" ParentId="59595" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;If the weather is a stationary process (debatable) then your long term prediction under some sort of ARIMA model (with seasonality) is likely going to be the process mean (with a large but bounded standard error). The interpretation of which is you should expect the average but anything might happen :) - useful!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-21T10:42:41.753" Id="59605" LastActivityDate="2013-05-21T10:42:41.753" OwnerUserId="16663" ParentId="54987" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="59628" AnswerCount="2" Body="&lt;p&gt;I have been struggling with this question all day, reading everything I could find, but still do not have a clear answer.&lt;/p&gt;&#10;&#10;&lt;p&gt;A part of my research was that: 30 students tried a new educational instrument (virtual patient). Before the interaction with the patient they filled in 18 Likert items, from 1 to 5 (1=highly disagree, 5=highly agree) about their expectations about the instrument, i.e. &quot;I am going to be deeply concentrated during the exercise&quot;. After the exercise, they filled in 18 similar Likert items, again from 1 to 5, about the actual experience, i.e. &quot;During the exercise, i was deeply concentrated&quot;. &lt;/p&gt;&#10;&#10;&lt;p&gt;An interesting aspect for me, is to examine weather there is a significant difference between the answers before and after the exercise, looking at each likert item separately.&lt;/p&gt;&#10;&#10;&lt;p&gt;What test should I use? If I get it right, t-test would not be the case here. Or am I wrong? What about Wilcoxon signed rank-test? Should I present and compare means or medians? &lt;/p&gt;&#10;&#10;&lt;p&gt;As you can probably understand I am by no means an expert in statistics, and I appreciate deeply any kind of help.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-05-21T14:04:20.377" Id="59625" LastActivityDate="2014-05-06T19:39:53.113" OwnerUserId="24710" PostTypeId="1" Score="2" Tags="&lt;statistical-significance&gt;&lt;likert&gt;" Title="How to compare Likert scale items before and after an intervention?" ViewCount="3328" />
  <row AnswerCount="0" Body="&lt;p&gt;The problem I am having is that I have a distribution that I want to remain the same shape, but the mean and variance altered. When I use &lt;code&gt;scale()&lt;/code&gt; in R, it can change to a standard normal, but I am having trouble to get other distributions (as when you change the mean, it also affects the standard deviation and vice versa).&lt;/p&gt;&#10;" ClosedDate="2013-05-21T16:15:27.717" CommentCount="2" CreationDate="2013-05-21T14:45:49.373" Id="59631" LastActivityDate="2013-05-21T14:57:44.110" OwnerUserId="16175" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;data-transformation&gt;" Title="How to reshape a variable to, say, mean = 5, sd = 10?" ViewCount="54" />
  <row AnswerCount="0" Body="&lt;p&gt;In statistical inference, it's common that investigators will report the results from Wald tests for simple univariate inference or likelihood ratio tests for nested models. However, in regression modeling, under standard assumptions about the normality of errors, the F-test is exact and equally powerful in finite samples relative to the other tests. It's commonly included in statistical output, yet it's not favored for inference.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can someone explain why the F-test is not used?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-05-21T16:50:22.313" Id="59646" LastActivityDate="2013-05-21T18:35:01.567" LastEditDate="2013-05-21T18:24:50.180" LastEditorUserId="8013" OwnerUserId="25926" PostTypeId="1" Score="1" Tags="&lt;regression&gt;" Title="What is the difference between the F-test and the Wald / Likelihood Ratio / Score tests of linear models?" ViewCount="294" />
  <row AcceptedAnswerId="59650" AnswerCount="2" Body="&lt;p&gt;So I have some data of which I'm looking at two different metrics, the first being &quot;how sick a person is (higher = sicker)&quot;, and the second being &quot;the presence of adverse events&quot;, and I've run 20 experiments:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;table=&#10;  Columns 1 through 11&#10;&#10;     1     0     1     1     0     0     1     1     2     0     2&#10;     0     0     1     1     1     1     0     0     1     0     0&#10;&#10;  Columns 12 through 20&#10;&#10;     2     1     2     2     2     2     2     0     1&#10;     0     0     1     0     0     0     1     0     0&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The correlation coefficient between these two data sets is -0.0066, which to me implies there is a fair amount of independence. I guess the question is now, is there any way to expose that certainty.&lt;/p&gt;&#10;&#10;&lt;p&gt;I understand that the t-statistic can be calculated from that, and equals&#10;-0.0281, which suggests a p-value (dof = 2) of 0.980134 (two-tailed) or 0.490067 (one tailed).&lt;/p&gt;&#10;&#10;&lt;p&gt;Now I may be stating the obvious, but does the high p-value imply a high probability of independence?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-05-21T16:55:31.750" Id="59647" LastActivityDate="2013-05-21T17:42:40.067" LastEditDate="2013-05-21T17:42:40.067" LastEditorUserId="17230" OwnerUserId="11551" PostTypeId="1" Score="0" Tags="&lt;correlation&gt;&lt;p-value&gt;" Title="Correlation vs measure of independence" ViewCount="76" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have some values from different repetitions of different experiments and I need to plot cumulative distribution function. You can see a fraction of my file below. can you help me how can i do that.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Experiment Replication Obs&#10;General0 0 1.294602161803 &#10;General0 0 1.094633445207 &#10;General0 0 0.844958280328 &#10;General0 0 0.599093485481 &#10;General0 0 0.362884584822 &#10;General0 0 0.132874339382 &#10;General0 0 0.006860913445 &#10;General0 0 0.010738332566 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;" ClosedDate="2013-05-21T18:22:39.210" CommentCount="4" CreationDate="2013-05-21T17:35:03.497" Id="59651" LastActivityDate="2013-05-21T17:35:03.497" OwnerUserId="25207" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;perl&gt;" Title="cumulative distribution function plotting" ViewCount="41" />
  
  <row AnswerCount="0" Body="&lt;p&gt;In general, decision trees such as CART represent row-wise (or record-wise) partitions of training data. I wonder if there is any recursive method partitioning data in column-wise (predictor-wise) as I believe it would be useful when you have a huge number of homogeneous predictors. Any hint or reference will be appreciated.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-21T18:33:27.310" Id="59655" LastActivityDate="2013-05-21T18:33:27.310" OwnerUserId="5597" PostTypeId="1" Score="1" Tags="&lt;classification&gt;&lt;cart&gt;" Title="Decision trees partitioning predictors?" ViewCount="33" />
  <row AnswerCount="1" Body="&lt;p&gt;I am using Mallow's Distance (normalized Earth Mover's Distance) to characterize the similarity between two histograms. This is working very well, but I would like to identify a specific cut-off where the distance represents a statistically significant value. Effectively what I would like to do is perform a hypothesis test regarding the similarity or non-similarity of the histograms at a certain level of significance.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I don't seem to be having much luck finding an explanation of how to determine the associated p-value. &lt;/p&gt;&#10;&#10;&lt;p&gt;Any/all pointers to an explanation of how to determine the p-value would be greatly appreciated.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-21T18:37:22.813" Id="59656" LastActivityDate="2013-05-23T03:37:09.283" OwnerUserId="3591" PostTypeId="1" Score="2" Tags="&lt;hypothesis-testing&gt;&lt;histogram&gt;" Title="Mallows distance hypothesis test: p-value =?" ViewCount="263" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have two variables, one continuous and one categorical which I currently both use as predictor variables in a logistic regression model. &lt;/p&gt;&#10;&#10;&lt;p&gt;Their relationship is shown in the following plot with the x axis showing the different categories of the categorical variable and the y axis showing the values of the continuous predictor variable. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/uENIg.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Being quite new to statistics in general, I wonder if one can make general statements as to whether one should only use the categorical, only the continuous variable or whether in some situations it still makes sense to use both variables even if they are clearly correlated as in the plot above.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-05-21T19:19:45.793" Id="59658" LastActivityDate="2013-05-21T19:27:08.490" LastEditDate="2013-05-21T19:27:08.490" LastEditorUserId="8013" OwnerUserId="20563" PostTypeId="1" Score="2" Tags="&lt;correlation&gt;&lt;multivariate-analysis&gt;" Title="Categorical vs Continuous Variable" ViewCount="130" />
  
  <row Body="&lt;p&gt;Something like that? Dmax occurs at the value &lt;code&gt;max.at&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(12345)&#10;&#10;x &amp;lt;- rnorm(10000, 5, 5)&#10;y &amp;lt;- rnorm(10000, 7, 6.5)&#10;&#10;# remove any missings from the data&#10;&#10;x &amp;lt;- x[!is.na(x)]&#10;y &amp;lt;- y[!is.na(y)]&#10;&#10;ecdf.x &amp;lt;- ecdf(x)&#10;ecdf.y &amp;lt;- ecdf(y)&#10;&#10;plot(ecdf.x, xlim=c(min(c(x,y)), max(c(x,y))), verticals=T, cex.lab=1.2, cex.axis=1.3,&#10;     las=1, col=&quot;skyblue4&quot;, lwd=2, main=&quot;&quot;)&#10;&#10;plot(ecdf.y, verticals=T, add=T, do.points=FALSE, cex.lab=1.2,&#10;     cex.axis=1.3, col=&quot;red&quot;, lwd=2)&#10;&#10;n.x &amp;lt;- length(x)&#10;n.y &amp;lt;- length(y)&#10;&#10;n &amp;lt;- n.x * n.y/(n.x + n.y)&#10;w &amp;lt;- c(x, y)&#10;&#10;z &amp;lt;- cumsum(ifelse(order(w) &amp;lt;= n.x, 1/n.x, -1/n.y))&#10;&#10;max(abs(z)) # Dmax&#10;[1] 0.1664&#10;&#10;ks.test(x,y)$statistic # the same&#10;     D &#10;0.1664&#10;&#10;max.at &amp;lt;- sort(w)[which(abs(z) == max(abs(z)))]&#10;[1] 9.082877&#10;&#10;# Draw vertical line&#10;&#10;abline(v=max.at, lty=2)&#10;&#10;lines(abs(z)~sort(w), col=&quot;purple&quot;, lwd=2)&#10;&#10;legend(&quot;topleft&quot;, legend=c(&quot;x&quot;, &quot;y&quot;, &quot;|Distance|&quot;), col=c(&quot;skyblue4&quot;, &quot;red&quot;, &quot;purple&quot;), lwd=c(2,2,2), bty=&quot;n&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/58YPS.png&quot; alt=&quot;Dmax visualization&quot;&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-05-21T19:44:26.610" Id="59663" LastActivityDate="2013-06-08T19:11:06.480" LastEditDate="2013-06-08T19:11:06.480" LastEditorUserId="21054" OwnerUserId="21054" ParentId="59654" PostTypeId="2" Score="4" />
  
  
  <row Body="&lt;p&gt;Communicating Classes for this matrix would be: {1}, {2}, {3}, {4,5}.&lt;/p&gt;&#10;&#10;&lt;p&gt;State 4 and 5 communicate with each other directly, therefore they constitute the same communicating class -- they are an equivalent class . The rest of the states do not have two way direct communication. For example you may access state 1 from 4 but not state 4 from 1, therefore states 4 and 1 are not in the same communication class.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Disclaimer: So I just started learning this topic myself -- I feel your pain of having a poor lecturer -- and thus the above is a result of novice understanding.&lt;/em&gt; &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-22T01:39:55.680" Id="59686" LastActivityDate="2013-05-22T03:51:53.430" LastEditDate="2013-05-22T03:51:53.430" LastEditorUserId="25947" OwnerUserId="25947" ParentId="58042" PostTypeId="2" Score="-1" />
  
  
  <row AcceptedAnswerId="59730" AnswerCount="2" Body="&lt;p&gt;I am trying to understand the way MA(q) models work.&#10;For this purpose I have created a simple data set with only&#10;three values. I then adapted a MA(1) model to it. The results&#10;are shown below:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x&amp;lt;-c(2,5,3)&#10;m&amp;lt;-arima(x,order=c(0,0,1))&#10;&#10;Series: x &#10;ARIMA(0,0,1) with non-zero mean &#10;&#10;Coefficients:&#10;          ma1  intercept&#10;      -1.0000     3.5000&#10;s.e.   0.8165     0.3163&#10;&#10;sigma^2 estimated as 0.5:  log likelihood=-3.91&#10;AIC=13.82   AICc=-10.18   BIC=11.12&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;While the MA(1) model looks like this: &#10;$$X_t = c +a_t - \theta*a_{t-1}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and $a_t$ is White Noise.&lt;/p&gt;&#10;&#10;&lt;p&gt;What I cant figure out is how to get the fitted values:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(forecast)&#10;fitted(m)&#10;Time Series:&#10;Start = 1 &#10;End = 3 &#10;Frequency = 1 &#10;[1] 3.060660 4.387627 3.000000&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I tried different ways, but I cant find out how the fitted values (&lt;code&gt;3.060660&lt;/code&gt;, &lt;code&gt;4.387627&lt;/code&gt; and &lt;code&gt;3.000000&lt;/code&gt;) are calculated.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would be very thankful for an answer!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-22T12:25:46.087" FavoriteCount="2" Id="59712" LastActivityDate="2013-05-22T17:10:29.767" LastEditDate="2013-05-22T12:46:27.713" LastEditorUserId="13680" OwnerUserId="25944" PostTypeId="1" Score="5" Tags="&lt;r&gt;&lt;time-series&gt;" Title="The way an MA(q) model works" ViewCount="263" />
  <row AnswerCount="2" Body="&lt;p&gt;With respect to hypothesis testing, estimating samples sizes is done through&#10;power, and it is intuitive that increasing the same size increases&#10;the precision of estimated effects. But what about prediction for both&#10;classification and regression? What aspects of the prediction problem&#10;are influenced by sample size other than estimating the generalization error or&#10;RMSE for regression. &lt;/p&gt;&#10;&#10;&lt;p&gt;In sum, properties that contribute to power in the hypothesis-testing setting differ from those that those that enable successful prediction through penalized regression/data mining/algorithmic modeling. How does sample size influence the success of these techniques?&lt;/p&gt;&#10;&#10;&lt;p&gt;One paper that describes this idea is &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2697341/&quot;&gt;this one&lt;/a&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;Can anyone provide references for their comments? Thanks.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-05-22T13:34:46.787" FavoriteCount="1" Id="59715" LastActivityDate="2014-12-18T14:33:19.507" LastEditDate="2013-05-22T15:09:12.340" LastEditorUserId="12548" OwnerUserId="12548" PostTypeId="1" Score="6" Tags="&lt;classification&gt;&lt;sample-size&gt;&lt;prediction&gt;" Title="Sample size with respect to prediction in classification and regression" ViewCount="597" />
  <row AnswerCount="1" Body="&lt;p&gt;I am performing a temperature preference test in mice. Mice are placed into a chamber with one half of a chamber having a cold floor and the other half having a warm floor. Mice spend a specific total time in the chamber (3 min) and can select the cold or warm part of the chamber. I calculate the time (in percent of the total time) spent on each side for each subject (each mouse). The results look like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Mouse   Warm%   Cold%&#10;    1     85%     15%&#10;    2     80%     20%&#10;    3     87%     13%&#10;    4     84%     16%&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;How do I calculate the confidence interval for the mean cold time percentage of the population from which this group of mice was selected?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;If I perform the same test with a second group of mice which received some experimental treatment, which statistical test to use to check for a difference between two groups of mice?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2013-05-22T15:44:22.823" Id="59725" LastActivityDate="2013-05-22T16:35:03.723" LastEditDate="2013-05-22T16:10:48.920" LastEditorUserId="6029" OwnerUserId="20279" PostTypeId="1" Score="3" Tags="&lt;confidence-interval&gt;&lt;proportion&gt;" Title="Statistical tests when each variable in a sample is a percentage" ViewCount="105" />
  
  <row AcceptedAnswerId="59763" AnswerCount="1" Body="&lt;p&gt;I have calculated a Durbin-Watson test and got as far as &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\eqalign{&#10;d&amp;amp;=2.207551844, \\ &#10;dL&amp;amp;= 1.6164, \\&#10;dU&amp;amp;= 1.7896.&#10;}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to test&lt;/p&gt;&#10;&#10;&lt;p&gt;$$H_0 \gt 0,\ H_1 \le 0.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;However, I do not really know if I can reject $H_0$. What would happen if $H_0 \lt 0, H_1 \ge 0$?&lt;/p&gt;&#10;&#10;&lt;p&gt;Please give me a hint on the interpretation of such a test!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-05-22T18:35:58.120" Id="59757" LastActivityDate="2013-05-22T20:49:24.290" LastEditDate="2013-05-22T19:57:00.560" LastEditorUserId="88" OwnerUserId="13561" PostTypeId="1" Score="3" Tags="&lt;hypothesis-testing&gt;&lt;self-study&gt;&lt;statistical-significance&gt;&lt;autocorrelation&gt;" Title="Interpretation of a Durbin-Watson test?" ViewCount="3017" />
  
  <row Body="&lt;p&gt;I found this &lt;a href=&quot;http://andrewgelman.com/wp-content/uploads/2012/03/tarpey.pdf&quot; rel=&quot;nofollow&quot;&gt;2009 JSA talk&lt;/a&gt; by Thad Tarpey to provide a useful explanation and commentary on the Box passage. He argues that if we regard models as approximations to the truth, we could just as easily call all models right.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here’s the abstract:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Students of statistics are often introduced to George Box’s famous&#10;  quote: “all models are wrong, some are useful.” In this talk I argue&#10;  that this quote, although useful, is wrong. A different and more&#10;  positive perspective is to acknowledge that a model is simply a means&#10;  of extracting information of interest from data. The truth is&#10;  infinitely complex and a model is merely an approximation to the&#10;  truth. If the approximation is poor or misleading, then the model is&#10;  useless. In this talk I give examples of correct models that are not&#10;  true models. I illustrate how the notion of a “wrong” model can lead&#10;  to wrong conclusions.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="0" CreationDate="2013-05-22T23:30:12.570" Id="59778" LastActivityDate="2013-05-22T23:38:10.077" LastEditDate="2013-05-22T23:38:10.077" LastEditorUserId="7071" OwnerUserId="7071" ParentId="57407" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="59786" AnswerCount="1" Body="&lt;p&gt;I have a dataset which is statistics from a web discussion forum. I'm looking at the distribution of the number of replies a topic is expected to have. In particular, I've created a dataset which has a list of topic reply counts, and then the count of topics which have that number of replies.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&quot;num_replies&quot;,&quot;count&quot;&#10;0,627568&#10;1,156371&#10;2,151670&#10;3,79094&#10;4,59473&#10;5,39895&#10;6,30947&#10;7,23329&#10;8,18726&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;If I plot the dataset on a log-log plot, I get what is basically a straight line:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/W16yN.png&quot; alt=&quot;Data plotted on log-log scale&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;(This is a &lt;a href=&quot;http://en.wikipedia.org/wiki/Zipf%27s_law&quot;&gt;Zipfian distribution&lt;/a&gt;). Wikipedia tells me that straight lines on log-log plots imply a function that can be modelled by a monomial of the form $y = ax^k$. And in fact I've eyeballed such a function:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lines(data$num_replies, 480000 * data$num_replies ^ -1.62, col=&quot;green&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/u8eeD.png&quot; alt=&quot;Eyeballed model&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;My eyeballs obviously aren't as accurate as R. So how can I get R to fit the parameters of this model for me more accurately? I tried polynomial regression, but I don't think that R tries to fit the exponent as a parameter - what is the proper name for the model I want?&lt;/p&gt;&#10;&#10;&lt;p&gt;Edit: Thanks for the answers everyone. As suggested, I've now fit a linear model against the logs of the input data, using this recipe:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;data &amp;lt;- read.csv(file=&quot;result.txt&quot;)&#10;&#10;# Avoid taking the log of zero:&#10;data$num_replies = data$num_replies + 1&#10;&#10;plot(data$num_replies, data$count, log=&quot;xy&quot;, cex=0.8)&#10;&#10;# Fit just the first 100 points in the series:&#10;model &amp;lt;- lm(log(data$count[1:100]) ~ log(data$num_replies[1:100]))&#10;&#10;points(data$num_replies, round(exp(coef(model)[1] + coef(model)[2] * log(data$num_replies))), &#10;       col=&quot;red&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The result is this, showing the model in red:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/JudrC.png&quot; alt=&quot;Fitted model&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;That looks like a good approximation for my purposes.&lt;/p&gt;&#10;&#10;&lt;p&gt;If I then use this Zipfian model (alpha = 1.703164) along with a random number generator to generate the same total number of topics (1400930) as the original measured dataset contained (using &lt;a href=&quot;http://coderepos.org/share/browser/lang/cplusplus/boost-supplement/trunk/boost_supplement/random/zipf_distribution.hpp&quot;&gt;this C code I found on the web&lt;/a&gt;), the result looks like:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/soakm.png&quot; alt=&quot;Random number generated results&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Measured points are in black, randomly generated ones according to the model are in red.&lt;/p&gt;&#10;&#10;&lt;p&gt;I think this shows that the simple variance created by randomly generating these 1400930 points is a good explanation for the shape of the original graph.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you're interested in playing with the raw data yourself, I have &lt;a href=&quot;http://s3.chickensmoothie.com/misc/research/topic_reply_counts_for_1400930_forum_topics.txt.zip&quot;&gt;posted it here&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="10" CreationDate="2013-05-23T01:31:18.773" FavoriteCount="9" Id="59784" LastActivityDate="2013-10-19T02:07:14.520" LastEditDate="2013-05-24T07:39:56.547" LastEditorUserId="25993" OwnerUserId="25993" PostTypeId="1" Score="17" Tags="&lt;r&gt;&lt;regression&gt;&lt;nonlinear-regression&gt;" Title="Regression for a model of form $y=ax^k$?" ViewCount="1098" />
  <row Body="&lt;p&gt;@GregSnow is right that this change doesn't really matter.  Let me add a few details to extend that.  What you are talking about is sometimes called &lt;em&gt;cell means coding&lt;/em&gt;, whereas the default coding scheme is called &lt;em&gt;reference cell coding&lt;/em&gt;.  Note that there are &lt;a href=&quot;http://www.ats.ucla.edu/stat/r/library/contrast_coding.htm&quot; rel=&quot;nofollow&quot;&gt;many possible valid coding schemes&lt;/a&gt;.  If you have a categorical variable with only two levels, then the t-test of the beta that comes by default in the regression output with any statistical software is more meaningful when you had used reference cell coding.  &lt;/p&gt;&#10;&#10;&lt;p&gt;On the other hand, when you have a categorical variable with multiple categories, which coding scheme you use is just a matter of taste.  To get a predicted value, you will have to solve the regression equation for $\hat y$ at the relevant spot in the covariate space either way.  To test if the categorical variable is related to the response, you will need to use an F change test (discussed &lt;a href=&quot;http://stats.stackexchange.com/questions/33059/testing-for-moderation-with-continuous-vs-categorical-moderators#33090&quot;&gt;here&lt;/a&gt;) either way, etc.  &lt;/p&gt;&#10;&#10;&lt;p&gt;To address your specific questions more concretely: if you use cell means coding instead of reference cell coding, the F-statistic will be the same; the t-statistics will change in that they will now test whether your cell means are zero, as @GregSnow explains; and the inclusion of continuous covariates will be the same.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-23T01:34:48.957" Id="59785" LastActivityDate="2013-05-23T01:46:51.947" LastEditDate="2013-05-23T01:46:51.947" LastEditorUserId="7290" OwnerUserId="7290" ParentId="59777" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;Well, it's also linear in the predictors. &lt;/p&gt;&#10;&#10;&lt;p&gt;For example, if you fit a quadratic you might say 'see, not linear!'... but it is! If $x_1 = x$ and $x_2 = x^2$, and you regress on $x_1$ and $x_2$, it's certainly linear in $(1,x_1,x_2)$. It's linear in the predictors you gave it.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you regress on $x_1 = \sin(\pi x)$ and $x_2 = \cos(\pi x)$... well, it's still linear in $(1,x_1,x_2)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;and so on.&lt;/p&gt;&#10;&#10;&lt;p&gt;By judicious choices of your $x$'s you can use it to fit curves, but it's still linear in what you give it.&lt;/p&gt;&#10;&#10;&lt;p&gt;Even a local polynomial (kernel-type) fit is actually linear in the predictors. You can write the whole thing as one large linear model.&lt;/p&gt;&#10;&#10;&lt;p&gt;If $y = X\beta+e$, $X\beta$ is clearly linear in either $X$ or $\beta$.&lt;/p&gt;&#10;&#10;&lt;p&gt;But yes, the linear-in-the-parameters is what the 'linear' in linear regression 'means'.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is it at least partly misleading that the elementary presentations are always drawing straight line relationships when regression can fit curves? Perhaps, but you pretty much have to start with lines.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-05-23T04:02:43.693" Id="59791" LastActivityDate="2013-05-23T05:18:48.017" LastEditDate="2013-05-23T05:18:48.017" LastEditorUserId="805" OwnerUserId="805" ParentId="59782" PostTypeId="2" Score="5" />
  <row AcceptedAnswerId="59804" AnswerCount="1" Body="&lt;p&gt;I have a set of observations. When I do an OLS I get a $\beta$. After I remove some values I also get a $\beta$. Now I want to test if the properties of two betas are the same? Please give me a hint how to do that?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-05-23T08:36:58.450" Id="59802" LastActivityDate="2013-08-28T12:32:54.847" LastEditDate="2013-08-28T12:32:54.847" LastEditorUserId="17230" OwnerUserId="13561" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;hypothesis-testing&gt;&lt;statistical-significance&gt;&lt;least-squares&gt;" Title="Test on the properties of two betas?" ViewCount="50" />
  <row Body="&lt;p&gt;Instead of the logistic function, I would suggest using the &lt;strong&gt;Normal cumulative distribution function&lt;/strong&gt; $ ncdf()$ in the following manner:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;$N$ = sample size (e.g. 100)&lt;/li&gt;&#10;&lt;li&gt;$\bar p$ = sample proportion (e.g. 165/300 = 55%)&lt;/li&gt;&#10;&lt;li&gt;$\alpha(N, \bar p) = ncdf ( \frac{\bar p-.5}{\sqrt{\bar p (1-\bar p)/N}} )$&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;The function $\alpha(N, \bar p)$ has the properties you are looking for: &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;It has the sample size $N$ and the sample proportion $\bar p$ as parameters&lt;/li&gt;&#10;&lt;li&gt;Its range is between 0 and 1 (it's a probability)&lt;/li&gt;&#10;&lt;li&gt;It is &quot;odd&quot; $\alpha(N, 1- \bar p) = 1 - \alpha(N, \bar p)$, or equivalently, $\alpha(N, \bar p) + \alpha(N, 1- \bar p) = 1$&lt;/li&gt;&#10;&lt;li&gt;Take an numerical example: $N$=100 and $\bar p$=55%, the probability is 84%&lt;/li&gt;&#10;&lt;li&gt;With $N$=300 and $\bar p$=55%, the probability is 96% (increases with the sample size)&lt;/li&gt;&#10;&lt;li&gt;With $N$=100 and $\bar p$=60%, the probability is 98% (increases with poll results deviating from 50%)&lt;/li&gt;&#10;&lt;li&gt;With $\bar p$ = 50%, the probability is 50%, whatever the sample size&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Explanation and disclaimer&lt;/em&gt;:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;I cannot give any statistical justification for the formula (maybe there is one, maybe there is none)&lt;/li&gt;&#10;&lt;li&gt;It was inspired to me by the textbook formula $\bar p +/- z * \sqrt { \bar p (1-\bar p)/N)}$, where $\alpha = ncdf(z)$ often used in the computation of confidence intervals&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2013-05-23T12:07:13.350" Id="59820" LastActivityDate="2013-05-24T19:38:44.860" LastEditDate="2013-05-24T19:38:44.860" LastEditorUserId="2451" OwnerUserId="2451" ParentId="41441" PostTypeId="2" Score="0" />
  
  
  <row Body="&lt;p&gt;Have you tried visualizing your data? If it doesn't cluster visually, the algorithms will likely not find anything either.&lt;/p&gt;&#10;&#10;&lt;p&gt;Maybe you could improve your question by just adding a plot of say, 100-500 such lines, maybe labeled ones (small streets, motorways in different colors?).&lt;/p&gt;&#10;&#10;&lt;p&gt;See: cluster analysis won't do magic. It needs guideance in choosing the algorithms, distance functions, data normalization and algorithm parameters carefully. We will not be able to give you a short answer like &quot;use k-means&quot; without knowing your data. And even then, you will have to try out a &lt;em&gt;lot&lt;/em&gt; of things.&lt;/p&gt;&#10;&#10;&lt;p&gt;But the first thing to do clearly is to visualize your data. Because also after running clustering, you &lt;em&gt;will&lt;/em&gt; want to visualize it to see if it somewhat worked.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-23T12:39:04.003" Id="59824" LastActivityDate="2013-05-23T12:39:04.003" OwnerUserId="18215" ParentId="59809" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;Please look here:&#10;&lt;a href=&quot;http://support.sas.com/kb/6/590.html&quot; rel=&quot;nofollow&quot;&gt;http://support.sas.com/kb/6/590.html&lt;/a&gt;&#10;when I mark an area top-down and copy it works. But when I mark an area bottom-up and copy it dosn't. The reason is that I normally set the curser after the last character of the last line I want to copy.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-23T14:07:51.017" Id="59835" LastActivityDate="2013-05-23T14:07:51.017" OwnerUserId="26010" ParentId="498" PostTypeId="2" Score="0" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Let $X_1,...,X_N$ be independent normal random variables. $X_i$ is normal with mean $\mu_i$ and standard deviation $\sigma_i$. Let $x_i$ be a single random sample from $X_i$.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Input:&lt;/strong&gt; We get all $x_i$'s and all $\sigma_i$'s, but we don't get the $\mu_i$'s.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Question 1&lt;/strong&gt; Estimate the histogram of the $\mu_i$'s.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Question 2&lt;/strong&gt; Assume the means $\mu_i$ are independently drawn from a distribution $\mathcal{M}$. Generate an estimate of $\mathcal{M}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note: Question 1 was answered below by @soakley but the solution didn't help my application, so I added Question 2.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that the goal is not to estimate the $\mu_i$'s individually, but rather to get a good estimate for the histogram, or the distribution, of all the $\mu_i$'s together. This estimate should hopefully be better than what we'd get by simply mixing the Gaussians around the $x_i$'s. A maximum-likelihood estimate should work, but I don't know how to produce it.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Warm up question:&lt;/strong&gt; A simpler question is the above when all the $\sigma_i$'s are the same. This is easy: see answer at the bottom.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;More details:&lt;/strong&gt;&#10;I need a method that I can program and will run in reasonable time. So exponential-time algorithms will not be sufficient.&lt;/p&gt;&#10;&#10;&lt;p&gt;In my input, $N$ is around 5000, the standard deviations are mostly between 5 and 50, and the means are mostly between 0 and 40.&lt;/p&gt;&#10;&#10;&lt;p&gt;So far I've tried the naive solution of drawing a Gaussian around each $x_i$ and mixing all these Gaussians. The results don't look at all like the correct distribution of $\mu_i$'s. For example, imagine all $X_i$'s are standard normal RVs. Then my naive method would guess that the $\mu_i$'s lie in a pretty wide Gaussian around 0. However, given the samples $x_i$ and the standard deviations $\sigma_i$, a clever algorithm could clearly see that the best guess is that all $\mu_i$'s are equal to zero. Therefore, it should be possible to do much better than my naive algorithm, possibly with some clever use of Fourier Decomposition.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Answer to warm-up question:&lt;/strong&gt;&#10;Question 1 doesn't have a good solution; to get a good solution, one would need to assume a posterior distribution. As for question 2: when all standard deviations are the same, then to get an estimate for the distribution $\mathcal{M}$ we simply need to de-convolve the distribution of $x_i$'s with a Gaussian. I don't see how to generalize this to the case of differing $\sigma_i$'s, though.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Motivation:&lt;/strong&gt;&#10;I am a poker player playing a very swingy poker game (Pot Limit Omaha). We want to find out if the rake is too high, by figuring out the &quot;true winrates&quot; of the player pool. &lt;a href=&quot;http://forumserver.twoplustwo.com/153/high-stakes-pl-omaha/no-money-plo-everyone-raked-1315665/&quot; rel=&quot;nofollow&quot;&gt;We have as data&lt;/a&gt; the winrates of all players in the player pool over a whole year (these are $x_i$), and the standard deviations of their winnings (this is $\sigma_i$) and we want to estimate the distribution of their &quot;true winrates&quot; (the $\mu_i$'s) in order to figure out if enough players are winning. This translates to the above problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Ongoing Research&lt;/strong&gt;&#10;I just found a &lt;a href=&quot;http://arxiv.org/abs/0905.2979&quot; rel=&quot;nofollow&quot;&gt;paper of Bovy et al&lt;/a&gt; which seems to address a generalization of my question and suggests an algorithm. It seems pretty relevant. I'll read it and report any findings here.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-05-23T15:22:48.433" FavoriteCount="1" Id="59845" LastActivityDate="2013-05-25T23:17:41.990" LastEditDate="2013-05-25T19:21:23.673" LastEditorUserId="26017" OwnerUserId="26017" PostTypeId="1" Score="4" Tags="&lt;hypothesis-testing&gt;&lt;normal-distribution&gt;&lt;model-selection&gt;&lt;maximum-likelihood&gt;&lt;fourier-transform&gt;" Title="Given samples from multiple normal RVs, how do we recover the histogram of their means?" ViewCount="109" />
  <row AcceptedAnswerId="59876" AnswerCount="1" Body="&lt;p&gt;I have a HLM model with significant variance in the level 1 intercepts across groups but no significant variance in the level 1 slopes across groups and find significant cross-level moderation effects. Does it make sense to interpret these or are random slopes a necessary condition for probing cross-level interaction effects?  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-23T15:59:35.637" FavoriteCount="2" Id="59850" LastActivityDate="2013-05-24T19:52:33.510" OwnerUserId="25260" PostTypeId="1" Score="4" Tags="&lt;multilevel-analysis&gt;" Title="Can I probe cross-level interactions without random slope in a hierarchical linear model?" ViewCount="1650" />
  
  
  
  <row Body="&lt;p&gt;Hopefully someone can give you a better answer, but I think the main point is this: the probabilities associated to each cluster decay, on average, exponentially for the Dirichlet process. Consider the stick breaking construction, where we let $\beta'_k \sim \mbox{Beta}(1, \alpha)$ and $\beta_k = \beta'_k \prod_{j &amp;lt; k} (1 - \beta'_j)$ where $\beta_k$ is the probability of landing in cluster $k$. Pitman-Yor processes have similar stick breaking constructions. &lt;/p&gt;&#10;&#10;&lt;p&gt;What happens to the probability of landing in cluster $k$ as $k \to \infty$? For the Dirichlet process this goes down exponentially fast - as the table I'm asking about becomes more and more obscure (say) my probability of ending up there goes down exponentially. If $\alpha = 1$, for example, the class probabilities might go down like $\frac 1 2$, $\frac 1 4$, $\frac 1 8$, .... (this would occur if each $\beta'_k$ were equal to its expectation). For the Pitman-Yor process, however, the probability does not go down exponentially, but instead like a power law. If we interpret the tables as being topics in a topic model, and we know a priori that distributions of topics have power law tails, then we would like the probability of me ending up at obscure latent topics to go down like a power law, so the Pitman-Yor might be more appropriate.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-23T17:46:16.230" Id="59863" LastActivityDate="2013-05-23T17:46:16.230" OwnerUserId="5339" ParentId="59859" PostTypeId="2" Score="1" />
  
  
  <row AcceptedAnswerId="59883" AnswerCount="1" Body="&lt;p&gt;I have  wood density data for a number of tree species given a tree's state of decay.  I am presented with mean density and standard errors (SE).  I am NOT provided with sample sizes.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, a decay class 1 fir tree has a mean density of 0.305 with a standard error of 0.024.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am using R.  I would like to generate a density value for a given decay class 1 fir tree similarly to using &lt;code&gt;rnorm(n, mean, standard deviation)&lt;/code&gt; when pulling from a normal distribution. &lt;/p&gt;&#10;&#10;&lt;p&gt;In this case, is it appropriate to substitute the SE for SD, such as &lt;code&gt;density&amp;lt;-rnorm(1, mean=0.305, sd=0.024)&lt;/code&gt;?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-05-23T20:07:57.467" Id="59880" LastActivityDate="2013-05-23T21:38:53.307" LastEditDate="2013-05-23T21:38:53.307" LastEditorUserId="7290" OwnerUserId="26036" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;normal-distribution&gt;" Title="Is it appropriate to use SE in place of SD in rnorm()?" ViewCount="105" />
  
  
  <row Body="&lt;p&gt;In addition to @gung's answer, I'll try to provide an example of what the &lt;code&gt;anova&lt;/code&gt; function actually tests. I hope this enables you to &lt;strong&gt;decide what tests are appropriate for the hypotheses you are interested in testing.&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's assume that you have an outcome $y$ and 3 predictor variables: $x_{1}$, $x_{2}$, and $x_{3}$. Now, if your logistic regression model would be &lt;code&gt;my.mod &amp;lt;- glm(y~x1+x2+x3, family=&quot;binomial&quot;)&lt;/code&gt;. When you run &lt;code&gt;anova(my.mod, test=&quot;Chisq&quot;)&lt;/code&gt;, the function compares the following models in sequential order:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;code&gt;glm(y~1, family=&quot;binomial&quot;)&lt;/code&gt; vs. &lt;code&gt;glm(y~x1, family=&quot;binomial&quot;)&lt;/code&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;code&gt;glm(y~x1, family=&quot;binomial&quot;)&lt;/code&gt; vs. &lt;code&gt;glm(y~x1+x2, family=&quot;binomial&quot;)&lt;/code&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;code&gt;glm(y~x1+x2, family=&quot;binomial&quot;)&lt;/code&gt; vs. &lt;code&gt;glm(y~x1+x2+x3, family=&quot;binomial&quot;)&lt;/code&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;So it &lt;strong&gt;sequentially&lt;/strong&gt; compares the smaller model with the next more complex model by adding one variable in each step. Each of those comparisons is done via a &lt;a href=&quot;http://stats.stackexchange.com/questions/59085/how-to-test-for-simultaneous-equality-of-choosen-coefficients-in-logit-or-probit/59093#59093&quot;&gt;likelihood ratio test&lt;/a&gt; (LR test; see example below). To my knowledge, these hypotheses are rarely of interest, but this has to be decided by you.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is an example in &lt;code&gt;R&lt;/code&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;mydata &amp;lt;- read.csv(&quot;http://www.ats.ucla.edu/stat/data/binary.csv&quot;)&#10;&#10;mydata$rank &amp;lt;- factor(mydata$rank)&#10;&#10;my.mod &amp;lt;- glm(admit ~ gre + gpa + rank, data = mydata, family = &quot;binomial&quot;)&#10;&#10;summary(my.mod)&#10;&#10;Coefficients:&#10;             Estimate Std. Error z value Pr(&amp;gt;|z|)    &#10;(Intercept) -3.989979   1.139951  -3.500 0.000465 ***&#10;gre          0.002264   0.001094   2.070 0.038465 *  &#10;gpa          0.804038   0.331819   2.423 0.015388 *  &#10;rank2       -0.675443   0.316490  -2.134 0.032829 *  &#10;rank3       -1.340204   0.345306  -3.881 0.000104 ***&#10;rank4       -1.551464   0.417832  -3.713 0.000205 ***&#10;   ---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 &#10;&#10;# The sequential analysis&#10;anova(my.mod, test=&quot;Chisq&quot;)&#10;&#10;Terms added sequentially (first to last)    &#10;&#10;     Df Deviance Resid. Df Resid. Dev  Pr(&amp;gt;Chi)    &#10;NULL                   399     499.98              &#10;gre   1  13.9204       398     486.06 0.0001907 ***&#10;gpa   1   5.7122       397     480.34 0.0168478 *  &#10;rank  3  21.8265       394     458.52 7.088e-05 ***&#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1&#10;&#10;# We can make the comparisons by hand (adding a variable in each step)&#10;&#10;         # model only the intercept&#10;mod1 &amp;lt;- glm(admit ~ 1, data = mydata, family = &quot;binomial&quot;) &#10;         # model with intercept + gre&#10;mod2 &amp;lt;- glm(admit ~ gre, data = mydata, family = &quot;binomial&quot;) &#10;         # model with intercept + gre + gpa&#10;mod3 &amp;lt;- glm(admit ~ gre + gpa, data = mydata, family = &quot;binomial&quot;) &#10;         # model containing all variables (full model)&#10;mod4 &amp;lt;- glm(admit ~ gre + gpa + rank, data = mydata, family = &quot;binomial&quot;) &#10;&#10;anova(mod1, mod2, test=&quot;Chisq&quot;)&#10;&#10;Model 1: admit ~ 1&#10;Model 2: admit ~ gre&#10;  Resid. Df Resid. Dev Df Deviance  Pr(&amp;gt;Chi)    &#10;1       399     499.98                          &#10;2       398     486.06  1    13.92 0.0001907 ***&#10;&#10;anova(mod2, mod3, test=&quot;Chisq&quot;)&#10;&#10;Model 1: admit ~ gre&#10;Model 2: admit ~ gre + gpa&#10;  Resid. Df Resid. Dev Df Deviance Pr(&amp;gt;Chi)  &#10;1       398     486.06                       &#10;2       397     480.34  1   5.7122  0.01685 *&#10;&#10;anova(mod3, mod4, test=&quot;Chisq&quot;)&#10;&#10;Model 1: admit ~ gre + gpa&#10;Model 2: admit ~ gre + gpa + rank&#10;  Resid. Df Resid. Dev Df Deviance  Pr(&amp;gt;Chi)    &#10;1       397     480.34                          &#10;2       394     458.52  3   21.826 7.088e-05 ***&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;The $p$-values in the output of &lt;code&gt;summary(my.mod)&lt;/code&gt; are &lt;a href=&quot;http://stats.stackexchange.com/questions/56066/wald-test-in-regression-ols-and-glms-t-vs-z-distribution&quot;&gt;Wald tests&lt;/a&gt; which test the following hypotheses (note that they're interchangeable and the &lt;strong&gt;order of the tests does not matter&lt;/strong&gt;):&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;For coefficient of &lt;code&gt;x1&lt;/code&gt;: &lt;code&gt;glm(y~x2+x3, family=&quot;binomial&quot;)&lt;/code&gt; vs.&#10;&lt;code&gt;glm(y~x1+x2+x3, family=&quot;binomial&quot;)&lt;/code&gt;&lt;/li&gt;&#10;&lt;li&gt;For coefficient of &lt;code&gt;x2&lt;/code&gt;: &lt;code&gt;glm(y~x1+x3, family=&quot;binomial&quot;)&lt;/code&gt; vs. &lt;code&gt;glm(y~x1+x2+x3, family=&quot;binomial&quot;)&lt;/code&gt;&lt;/li&gt;&#10;&lt;li&gt;For coefficient of &lt;code&gt;x3&lt;/code&gt;: &lt;code&gt;glm(y~x1+x2, family=&quot;binomial&quot;)&lt;/code&gt; vs. &lt;code&gt;glm(y~x1+x2+x3, family=&quot;binomial&quot;)&lt;/code&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;So each coefficient against the full model containing all coefficients. Wald tests are an approximation of the likelihood ratio test. We could also do the likelihood ratio tests (LR test). Here is how:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;mod1.2 &amp;lt;- glm(admit ~ gre + gpa, data = mydata, family = &quot;binomial&quot;)&#10;mod2.2 &amp;lt;- glm(admit ~ gre + rank, data = mydata, family = &quot;binomial&quot;)&#10;mod3.2 &amp;lt;- glm(admit ~ gpa + rank, data = mydata, family = &quot;binomial&quot;)&#10;&#10;anova(mod1.2, my.mod, test=&quot;LRT&quot;) # joint LR test for rank&#10;&#10;Model 1: admit ~ gre + gpa&#10;Model 2: admit ~ gre + gpa + rank&#10;  Resid. Df Resid. Dev Df Deviance  Pr(&amp;gt;Chi)    &#10;1       397     480.34                          &#10;2       394     458.52  3   21.826 7.088e-05 ***&#10;&#10;anova(mod2.2, my.mod, test=&quot;LRT&quot;) # LR test for gpa&#10;&#10;Model 1: admit ~ gre + rank&#10;Model 2: admit ~ gre + gpa + rank&#10;  Resid. Df Resid. Dev Df Deviance Pr(&amp;gt;Chi)  &#10;1       395     464.53                       &#10;2       394     458.52  1   6.0143  0.01419 *&#10;&#10;anova(mod3.2, my.mod, test=&quot;LRT&quot;) # LR test for gre&#10;&#10;Model 1: admit ~ gpa + rank&#10;Model 2: admit ~ gre + gpa + rank&#10;  Resid. Df Resid. Dev Df Deviance Pr(&amp;gt;Chi)  &#10;1       395     462.88                       &#10;2       394     458.52  1   4.3578  0.03684 *&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The $p$-values from the likelihood ratio tests are very similar to those obtained by the Wald tests by &lt;code&gt;summary(my.mod)&lt;/code&gt; above.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note: The third model comparison for &lt;code&gt;rank&lt;/code&gt; of &lt;code&gt;anova(my.mod, test=&quot;Chisq&quot;)&lt;/code&gt; is the same as the comparison for &lt;code&gt;rank&lt;/code&gt; in the example below (&lt;code&gt;anova(mod1.2, my.mod, test=&quot;Chisq&quot;)&lt;/code&gt;). Each time, the $p$-value is the same, $7.088\cdot 10^{-5}$. It is each time the comparison between the model without &lt;code&gt;rank&lt;/code&gt; vs. the model containing it.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2013-05-23T21:09:45.263" Id="59886" LastActivityDate="2013-05-23T22:25:54.473" LastEditDate="2013-05-23T22:25:54.473" LastEditorUserId="21054" OwnerUserId="21054" ParentId="59879" PostTypeId="2" Score="14" />
  <row Body="&lt;p&gt;In Stata forums, you are asked to explain where user-written programs come from; that seems a good convention for CV too. As Pier Luigi should already know, &lt;code&gt;switchr&lt;/code&gt; can be installed by typing in Stata &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ssc install switchr&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;You are looking at the syntax statement from the help, but later in the help there is much more detail and examples of main and regime equations. Scroll down to the end to see. (A general tip with unfamiliar commands is to look at the examples first.) &lt;/p&gt;&#10;&#10;&lt;p&gt;Correct spelling is Markov. &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-05-23T22:54:47.430" Id="59894" LastActivityDate="2013-05-28T18:48:38.703" LastEditDate="2013-05-28T18:48:38.703" LastEditorUserId="22047" OwnerUserId="22047" ParentId="59887" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;&lt;strong&gt;Stata&lt;/strong&gt; and &lt;strong&gt;RATS&lt;/strong&gt; are two other pieces of software that are popular for time-series analysis. &lt;strong&gt;OxMetrics&lt;/strong&gt; also seems to be a popular choice. &lt;/p&gt;&#10;&#10;&lt;p&gt;Although I'm an R user, I must say that I am quite a fan of Stata for time-series analysis. The Stata [TS] manual is rather good. I prefer R, though, because it's more flexible. Of course, there is the Mata language in Stata, but I can't say I'm a fan of it.&lt;/p&gt;&#10;&#10;&lt;p&gt;I haven't used MATLAB, so I won't give an answer to the first question.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-24T01:01:47.083" Id="59903" LastActivityDate="2014-03-03T09:05:23.280" LastEditDate="2014-03-03T09:05:23.280" LastEditorUserId="22047" OwnerUserId="24617" ParentId="16583" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;You have correctly identified that your problem is one of convolution of hypergeometrics.&lt;/p&gt;&#10;&#10;&lt;p&gt;Your two simplest choices appear to be either:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Direct numerical convolution of the probability functions. This is not difficult and given the probability function is on a grid, it can even be done in Excel fairly easily.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;You could perhaps try an approximation to the hypergeometric, such as perhaps a normal approximation; if we're not too far in the tail for the sum it should be reasonable.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;So let's try them:&lt;/p&gt;&#10;&#10;&lt;p&gt;1. &lt;strong&gt;Direct convolution&lt;/strong&gt;:&lt;/p&gt;&#10;&#10;&lt;p&gt;(Calculation details omitted - I did it in R using the &lt;code&gt;convolve&lt;/code&gt; function, though it could be done with &lt;code&gt;fft&lt;/code&gt; and its inverse or &lt;code&gt;filter&lt;/code&gt; as well, but they're all a bit fiddly. In this case it's probably easier to just do it directly as a calculation on the vector of probabilities.)&lt;/p&gt;&#10;&#10;&lt;p&gt;This is what the probability function of the convolution looks like:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/gp0Cl.png&quot; alt=&quot;convolution of 5 hypergeometrics&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The probability of getting $\geq 20$ on that p.f. is &lt;strong&gt;0.009385&lt;/strong&gt;. This is 'exact' to the numerical accuracy of the convolution (which should be to more than the number of figures I quoted, so the quoted figures should be accurate).&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's the same graph with the p.f. of the hypergeometric and the convolution of 5 of them computed in Excel, in this case by repeated use of the &lt;code&gt;SUMPRODUCT&lt;/code&gt; function. The answers are of course the same:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/bCDCn.png&quot; alt=&quot;conv. of 5 hypergeoms. from Excel&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;2. &lt;strong&gt;Normal approximation&lt;/strong&gt;. Since it turns out (as we see from 1.) that we're quite far out in the tail, this won't be very accurate. But let's go ahead and try it:&lt;/p&gt;&#10;&#10;&lt;p&gt;From &lt;a href=&quot;http://en.wikipedia.org/wiki/Hypergeometric_distribution&quot; rel=&quot;nofollow&quot;&gt;Wikipedia&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;For a single set of 20 draws ($N=80, K=20, n=10$, and $k$ is the number of 'hits' (black marbles you get).&lt;/p&gt;&#10;&#10;&lt;p&gt;Your mean for $k$ is: $n {K\over N} = 10 \times 20/80 = 2.5$ as you stated.&lt;/p&gt;&#10;&#10;&lt;p&gt;Your variance for $k$ is: $n{K\over N}{(N-K)\over N}{N-n\over N-1}$ $= 2.5 \cdot \frac{3}{4} \cdot \frac{70}{79} = 1.6614$  (s.d. 1.289)&lt;/p&gt;&#10;&#10;&lt;p&gt;Hence $S=Y_1+Y_2+\ldots +Y_5 \quad \dot{\sim}\,\, N(12.5,2.8822^2)$&lt;/p&gt;&#10;&#10;&lt;p&gt;With continuity correction:&lt;/p&gt;&#10;&#10;&lt;p&gt;$P(S\geq 19.5) \approx P(Z\geq (19.5-12.5)/2.8822) = P(Z\geq 2.4287) = 0.0076$&lt;/p&gt;&#10;&#10;&lt;p&gt;The z-value cutoff itself here is enough to hint to us that the approximation may not be very accurate.&lt;/p&gt;&#10;&#10;&lt;p&gt;(While not terrible accurate, the uncorrected version is significantly less accurate in this case, having about half the required probability.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Given that the numerical convolution isn't all that much harder than this inaccurate approximation, in this case, it's worth the additional effort.&lt;/p&gt;&#10;&#10;&lt;p&gt;---&lt;/p&gt;&#10;&#10;&lt;p&gt;An alternative approximation that is often used is the binomial approximation to the hypergeometric; the usual one would take $n=10$ and $p=0.25$. In this case, that's quite convenient because the convolution of them will also be binomial (with $n=50$ and $p=0.25$); however, it tends to be inaccurate when you sample more than about 5-10% of the population, and in this situation it's a lot more than that, so we would anticipate that it won't be very good. Further, the usual form overestimates the variance (since it ignores the 'without replacement' aspect). In this case it yields a probability about 50% too high.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, as &lt;a href=&quot;http://www.jstor.org/discover/10.2307/2681878&quot; rel=&quot;nofollow&quot;&gt;Brunk et al.&lt;/a&gt;$^{(1)}$ point out, since several of the hypergeometric parameters can be interchanged without changing the probabilities, there are actually four quite natural binomial approximations to the hypergeometric, and they discuss another reference that says the one with the smallest &quot;n&quot; for the binomial is generally the best approximation. Brunk et al. give a detailed analysis which broadly seems to back this simple advice up.&lt;/p&gt;&#10;&#10;&lt;p&gt;In their notation, the total population is $N$, there are $M$ items&#10;in the population with the characteristic of interest and there are $n$&#10;in the sample (i.e. my $K$ above is their $M$ and my $k$ is their $x$), then:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$H(x;N,n,M) = \binom{M}{x}\binom{N-M}{n-x}/\binom{N}{n}\,,$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $0 \leq x \leq \min(M,n)$ and the usual binomial approximating &#10;probability is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$B(x;n,M/N) = \binom{n}{x}(M/N)^r\, (1-M/N)^{n-r}\,,$$&lt;/p&gt;&#10;&#10;&lt;p&gt;for $0 \leq x \leq n$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Since in the hypergeometric, $n$ and $M$ can be interchanged, as can&#10;the roles items-of-interest and items-not-of-interest,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$H(x;N,n,M)=H(x;N,M,n)=H(n-x;N,N-M,n)=H(M-x;N,N-n,M)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;leading to four binomial approximations to those probabilities:&lt;/p&gt;&#10;&#10;&lt;p&gt;$B(x;n,M/N)$&lt;br&gt;&#10;$B(x;M,n/N)$&lt;br&gt;&#10;$B(n-x;N-M,n/N)$&lt;br&gt;&#10;$B(M-x;N-n,M/N)$  &lt;/p&gt;&#10;&#10;&lt;p&gt;In our situation, $N=80$, $M=20$ and $n=10$, so the four &quot;sample sizes&quot; are minimized by taking the first approximation, the one we already considered; this suggests we likely can't do better with the usual forms of binomial approximation.&lt;/p&gt;&#10;&#10;&lt;p&gt;--&lt;/p&gt;&#10;&#10;&lt;p&gt;An alternative possibility for a binomial approximation, since the probabilities are essentially zero near the maximum is to forget sticking to the given $n$ and instead choose $n$ and $p$ so as to approximately match the first two moments:&lt;/p&gt;&#10;&#10;&lt;p&gt;$np = 2.5$&lt;/p&gt;&#10;&#10;&lt;p&gt;$np(1-p)=1.6614$&lt;/p&gt;&#10;&#10;&lt;p&gt;Dividing the second equation by the first, $1-p = 0.66456$ or $p = 0.33544$, implying $n\approx 7.453$. If we seek to approximate the individual components of the convolution, the possibilities are to take $n = 7$ or $n=8$, with implied variances $1.607$ and $1.719$ (and of those we might anticipate $n=8$ to do slightly better since it gives more of a 'right tail'), but we could also consider rounding only to the nearest $0.2$ ($n=7.4$) so that we have integer $n$ once we have added the five components, yielding these approximations after the convolution:&lt;/p&gt;&#10;&#10;&lt;p&gt;(i) $n=35, p = 2.5/7$  &lt;/p&gt;&#10;&#10;&lt;p&gt;(ii) $n=37, p = 2.5/7.4$  &lt;/p&gt;&#10;&#10;&lt;p&gt;(iii) $n=40, p = 2.5/8$  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; 1-pbinom(19,35,2.5/7)&#10;[1] 0.007825608&#10;&amp;gt; 1-pbinom(19,37,2.5/7.4)&#10;[1] 0.008837474&#10;&amp;gt; 1-pbinom(19,40,2.5/8)&#10;[1] 0.01022883&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;With the smallest percentage error on the middle one, though the $n=8$ one is also &quot;close&quot;. Nevertheless, the accuracy is not particularly good, though much better than our other approximations.&lt;/p&gt;&#10;&#10;&lt;p&gt;(1): Brunk, H. D., J. E. Holstein and F. Williams (1968),&lt;br&gt;&#10;A Comparison of Binomial Approximations to the Hypergeometric Distribution&lt;br&gt;&#10;&lt;em&gt;The American Statistician&lt;/em&gt;, Vol. 22, No. 1 (Feb.), pp. 24-26  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-24T01:37:37.610" Id="59908" LastActivityDate="2013-05-25T03:38:05.800" LastEditDate="2013-05-25T03:38:05.800" LastEditorUserId="805" OwnerUserId="805" ParentId="59898" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;I want to quickly summarize my comments for your convenience. From what I've seen of your data, they seem compatible with a log-normal distribution with a mean and standard deviation &lt;strong&gt;on the log scale&lt;/strong&gt; of $\mu=-0.19$ and $\sigma=1.458$, respectively. The density plot of your log-transformed data is not perfectly symmetrical, it has a small &lt;a href=&quot;http://en.wikipedia.org/wiki/Skewness&quot; rel=&quot;nofollow&quot;&gt;negative skew&lt;/a&gt;. &quot;On the log scale&quot; means that the mean and standard deviation given are those corresponding to the log-transformed data  - which should follow a normal distribution then. The mean on your &lt;strong&gt;original scale&lt;/strong&gt; would be $\exp(\mu + \sigma^{2}/2)$ and the standard deviation $\sqrt{\left[\exp(\sigma^{2})-1 \right]\cdot \exp(2\mu + \sigma^{2})}$, or numerically: $2.39$ and $6.50$. The mode (the peak of your distribution) on the original scale would be $\exp(\mu - \sigma^{2})=0.099$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The probability that a message exceeds a size of $a$ can be calculated as follows:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;On the &lt;strong&gt;log scale&lt;/strong&gt; using the CDF of the normal distribution: &lt;code&gt;pnorm(log(a), mean=-0.19019347, sd=1.45795269, lower.tail=FALSE)&lt;/code&gt;&lt;/li&gt;&#10;&lt;li&gt;On the &lt;strong&gt;original scale&lt;/strong&gt; using the CDF of the log-normal distribution: &#10;&lt;code&gt;plnorm(a, meanlog=-0.19019347, sdlog=1.45795269, lower.tail=FALSE)&lt;/code&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="2" CreationDate="2013-05-24T13:23:50.663" Id="59942" LastActivityDate="2013-05-24T13:30:21.250" LastEditDate="2013-05-24T13:30:21.250" LastEditorUserId="21054" OwnerUserId="21054" ParentId="59848" PostTypeId="2" Score="1" />
  <row AnswerCount="2" Body="&lt;p&gt;I have two time series of daily data. One is &lt;code&gt;sign-ups&lt;/code&gt; and the other &lt;code&gt;terminations&lt;/code&gt; of subscriptions. I'd like to predict the latter using the information contained in both variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;Looking at the graph of these series it's obvious that terminations are correlated with multiples of the sign-ups the months before. That is, a spike in sign-ups on May 10th, will lead to an increase in terminations in June 10th, July 10th and August 10th and so on, although the effect wears off.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm hoping to get a hint as to which models I might employ to model this specific problem. Any advice would be much appreciated..&lt;/p&gt;&#10;&#10;&lt;p&gt;So far, I've been thinking a VAR model, but I'm not sure how to include the monthly effect - use a really high order of lags or add a seasonal component somehow?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-24T14:17:17.653" FavoriteCount="3" Id="59945" LastActivityDate="2014-03-15T18:18:22.670" LastEditDate="2013-05-24T15:07:32.420" LastEditorUserId="7290" OwnerUserId="8455" PostTypeId="1" Score="9" Tags="&lt;r&gt;&lt;time-series&gt;&lt;seasonality&gt;&lt;var&gt;" Title="How to model month to month effects in daily time series data?" ViewCount="337" />
  <row Body="&lt;p&gt;Consider a linear estimator $\mathbf{\hat{y}} = \mathbf{X\theta}$ fitted with linear regression $\mathbf{\theta} = (\mathbf{X^\top X})^{-1}\mathbf{X}^\top \mathbf{y}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;If $\mathbf{C}_y = \mathrm{diag}(\sigma_1^2,\sigma_2^2,\ldots,\sigma_m^2)$ is the covariance for the observations $\mathbf{y}$, the covariance for $\mathbf{\theta}$ is given by (see the lemma):&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;\mathbf{C}_\theta = (\mathbf{X^\top X})^{-1}\mathbf{X^\top C}_y \mathbf{X}(\mathbf{X^\top X})^{-1}&#10;$$&lt;/p&gt;&#10;&#10;&lt;h2&gt;Lemma&lt;/h2&gt;&#10;&#10;&lt;p&gt;The covariance of a linear mapping $\mathbf{y} = \mathbf{Ax} + \mathbf{b}$ is $\mathbf{C}_y = \mathbf{AC}_x\mathbf{A^\top}$ with $\mathbf{C}_x$ the covariance for $\mathbf{x}$.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-05-24T15:01:19.477" Id="59949" LastActivityDate="2013-05-24T15:01:19.477" OwnerUserId="25969" ParentId="57064" PostTypeId="2" Score="1" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I've seen this question answered &lt;a href=&quot;http://stats.stackexchange.com/questions/51275/what-is-the-difference-in-what-aic-and-c-statistic-auc-actually-measure-for-mo&quot;&gt;here&lt;/a&gt; but I do not understand the answer. Harrell recommends using deviance based measures. David Hand (referenced in the thread) says that that the AUC is inappropriate to compare different models, because it uses different misclassifications costs. I don't see how this would be the case when comparing lasso and elastic net, given that they are both trained on the same set of predictors.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-24T18:43:41.997" FavoriteCount="1" Id="59967" LastActivityDate="2013-05-24T18:43:41.997" OwnerUserId="13624" PostTypeId="1" Score="1" Tags="&lt;classification&gt;&lt;roc&gt;&lt;auc&gt;" Title="Using AUC to compare logistic lasso and elastic net" ViewCount="132" />
  
  <row Body="&lt;p&gt;There are several main advantages, in no particular order&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;SAS has a large installed base and a long track record&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I'm purposefully avoiding use of pejorative terms like &quot;legacy&quot; or &quot;habit&quot;  Many companies have been using SAS for 30 or 40 years, and they have millions of lines of working code. In addition, there are all of the benefits of a stable code base with millions of user days in an area where small errors can be critical.   This is the same reason that Unix flavors are still popular even though Unix is over 40 years old and obsolete in some ways.  Finally, there is a large community of experienced SAS professionals who are used to solving business problems&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;SAS is well suited to heterogeneous, complex data and operating environments&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Companies have lots of different data sources, based in different types of systems, as well as in many cases, multiple operating environments.  R has only very recently gotten some extremely basic capabilities to deal with more than can be kept in memory.  Compare this with SAS's ability to support native, optimized, in-database processing for terradata, to cite just one example.  In most real world situations, the hardest part of analytics is dealing with the data and operating environment. (need to run your Windows developed model scoring code on the mainframe?  With SAS, no problem.  With R, you are out of luck.)  R doesn't solve any of those problems. &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;The user doesn't have to worry about being &quot;on their own&quot;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;A SAS user can be reasonably certain that every code module has been tested by qualified people. It is not necessary to devote time and effort to learning the provenance of the code, or independently validating it.  Furthermore, if issues of any kind are encountered, robust assistance (from something as basic as documentation to something as comprehensive as detailed exploring unexpected results or behavior of a sophisticated method) the user can pick up the phone and get help.  &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;It's &quot;good enough&quot;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;The language turns off some people because it is different than modern languages for general programming.  Having said that, the language is high level, powerful, expressive, and comprehensive.   In short, once you learn it, it gets the job done.   For companies, the elegance of the solution isn't much of a selling point. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-05-24T22:05:39.970" Id="59976" LastActivityDate="2013-05-24T22:05:39.970" OwnerUserId="26087" ParentId="33780" PostTypeId="2" Score="5" />
  
  <row AcceptedAnswerId="60126" AnswerCount="3" Body="&lt;p&gt;We are at a position in a game where there is a decision to be made.  As there are relatively few game positions and possible moves, we have collected statistics from previous occasions the game was played and we found ourselves with the identical game position.&lt;/p&gt;&#10;&#10;&lt;p&gt;For each move, we have recorded if selection eventually led to a win or a loss.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Move     Win    Loss&#10; A        3      5&#10; B        2      4&#10; C        0      0&#10; D        0      4&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;My goal is to randomly select a move while proportionally favouring those moves more likely to win.  So for each of the possible moves, I would like to calculate the probability that that move is the most-winning one.  Let's call these PA, PB, PC and PD.  These should sum to 1.&lt;/p&gt;&#10;&#10;&lt;p&gt;Give the example data above, my intuition says:-&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Move A should be the most favoured, and so PA is the largest of the values.&lt;/li&gt;&#10;&lt;li&gt;Move C has no data.  It could be anything between always a loss and always a win.  Might need to be treated as a special case as there is nothing from which to calculate?&lt;/li&gt;&#10;&lt;li&gt;Move D appears poor so far.  Perhaps this is just sampling luck, and over time we would fnid ourselves with 12 wins and 4 losses.  There is still some chance it is really the most winning move.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Now I get a stuck.  It's seems as though I should calculate the win ratio for each of the moves and apply some certainty-factor based on the total number of times that option has been selected to come up with distribution representing the likely win ratio for the underlying population.  How would I combine these overlapping distributions and reduce them down to probabilities for each move?  I'm not sure how I would combine all of these numbers to reach PA, PB, PC and PD.  This seems a reminicent of an ANOVA?&lt;/p&gt;&#10;&#10;&lt;p&gt;In the actual problem there can be between 1 and 7 moves available.  I'm assuming any answer can be generalised up to more possible moves.  If it makes any difference, the game is not circular; having made a move we can never return to the same position within the game.  It could only be reached again in a new game.  There are, however, multiple ways to reach the same game position from the start of the game.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-05-25T02:52:42.787" Id="59986" LastActivityDate="2014-04-13T16:39:40.050" LastEditDate="2013-05-25T05:43:16.970" LastEditorUserId="26093" OwnerUserId="26093" PostTypeId="1" Score="2" Tags="&lt;probability&gt;&lt;binary-data&gt;&lt;group-differences&gt;&lt;multiarmed-bandit&gt;" Title="How can I calculate the relative probability of each move resulting in a win, given historic win/loss data for the possible moves?" ViewCount="155" />
  
  <row AcceptedAnswerId="60055" AnswerCount="1" Body="&lt;p&gt;From &lt;a href=&quot;https://en.wikipedia.org/wiki/Multiple_comparisons#Post-hoc_testing_of_ANOVAs&quot; rel=&quot;nofollow&quot;&gt;Wikipedia&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;strong&gt;Post-hoc testing of ANOVAs&lt;/strong&gt;&lt;/p&gt;&#10;  &#10;  &lt;p&gt;Multiple comparison procedures are commonly used in an analysis of&#10;  variance after obtaining a significant omnibus test result, like the&#10;  ANOVA F-test. The significant ANOVA result suggests rejecting the&#10;  global null hypothesis H0 that the means are the same across the&#10;  groups being compared. Multiple comparison procedures are then used to&#10;  determine which means differ. In a one-way ANOVA involving K group&#10;  means, there are K(K − 1)/2 pairwise comparisons.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;A number of methods have been proposed for this problem, some of which&#10;  are:&lt;/p&gt;&#10;  &#10;  &lt;p&gt;&lt;strong&gt;Single-step procedures&lt;/strong&gt;&lt;/p&gt;&#10;  &#10;  &lt;ul&gt;&#10;  &lt;li&gt;Tukey–Kramer method (Tukey's HSD) (1951)&lt;/li&gt;&#10;  &lt;li&gt;Scheffe method (1953)&lt;/li&gt;&#10;  &lt;/ul&gt;&#10;  &#10;  &lt;p&gt;&lt;strong&gt;Multi-step procedures based on Studentized range statistic&lt;/strong&gt;&lt;/p&gt;&#10;  &#10;  &lt;ul&gt;&#10;  &lt;li&gt;Duncan's new multiple range test (1955)&lt;/li&gt;&#10;  &lt;li&gt;&lt;p&gt;The Nemenyi test is similar to Tukey's range test in ANOVA.&lt;/p&gt;&lt;/li&gt;&#10;  &lt;li&gt;&lt;p&gt;The Bonferroni–Dunn test allows comparisons, controlling the familywise error rate.[vague]&lt;/p&gt;&lt;/li&gt;&#10;  &lt;li&gt;Student Newman-Keuls post-hoc analysis&lt;/li&gt;&#10;  &lt;li&gt;Dunnett's test (1955) for comparison of number of treatments to a single control group.&lt;/li&gt;&#10;  &lt;/ul&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I was wondering what &quot;single-step&quot; and &quot;multi-step&quot; mean here?&lt;/p&gt;&#10;&#10;&lt;p&gt;Are they both for pairwise comparisons for every two groups, right?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks and regards!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-25T10:33:33.420" Id="59999" LastActivityDate="2013-05-26T12:14:21.023" LastEditDate="2013-05-25T13:17:13.697" LastEditorUserId="1005" OwnerUserId="1005" PostTypeId="1" Score="2" Tags="&lt;hypothesis-testing&gt;&lt;anova&gt;&lt;multiple-comparisons&gt;" Title="What do &quot;single-step&quot; and &quot;multi-step&quot; mean in post-hoc testing of ANOVAs?" ViewCount="265" />
  <row Body="&lt;p&gt;Set amount of cases equal to the number of valid observations in your dataset to get the correct t-statistics. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-25T11:11:57.493" Id="60002" LastActivityDate="2014-04-21T04:02:45.193" LastEditDate="2014-04-21T04:02:45.193" LastEditorUserId="7290" OwnerUserId="26109" ParentId="59342" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;The paper suggested by learner above is a good suggestion of how this can be accomplished.  I would also suggest using the leave-one-out cross-validation predictions to fit the sigmoid function, rather than the actual output on the training examples as extra insurance against over-fitting the sigmoid.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, if you are interested in more than binary classification, then you might be better off using kernel logistic regression instead of the SVM, where the output is an estimate of a-posteriori probability of class membership, and so will be in the range [0-1].  However the training criterion used for KLR takes the who range of probability into account rather than concentrating on the decision boundary.&lt;/p&gt;&#10;&#10;&lt;p&gt;Prof. Vapnik quite rightly says that if you want to solve a problem, you ought to choose a method that solves it directly, rather than solving a more general problem and then simplifying the solution (as the former approach concentrates on what is actually important).  I would argue that you should also avoid solving a more simple problem and then post-processing it to approximate the solution to a more general problem - which is essentially what we are doing when fitting a sigmoid to the output of an SVM.  If we want probabilities, we should estimate them directly using KLR; if we want binary classification, we should use and SVM, rather than thresholding the probabilities provided by KLR.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-25T13:49:33.770" Id="60008" LastActivityDate="2013-05-25T13:49:33.770" OwnerUserId="887" ParentId="55068" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="60308" AnswerCount="1" Body="&lt;p&gt;I have seen several ways to write (and calculate and interpret) a survivor function in discrete time survival analysis and I wonder which is correct or if they both are, but the interpretation and/or setup of the problem is different and I am missing it.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is an example.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Customers open an account at a bank.&lt;/li&gt;&#10;&lt;li&gt;Record the month of opening and month of close and the discrete random variable T is in the set {0,1..,23}. After 23, there is right censoring. T denotes the number of month boundaries crossed between open and close. For example:&lt;/li&gt;&#10;&lt;li&gt;A customer that opens in January 2012 and closes in January 2012 is labeled as 0.&lt;/li&gt;&#10;&lt;li&gt;A customer that opens in January 2012 and closes in Feb 2012 is&#10;labeled as 1&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;An interval is of the form [a,a+1) where a=0,..,A.&lt;/p&gt;&#10;&#10;&lt;p&gt;The following hazards, h(t), are calculated.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, a customer opens and closes in the same month with probability 0.019194.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;&#10;What is the proper way to construct the survivor function? &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;I am seeing that some sources set S(t) as the probability that the event&#10;    will occur AFTER the period t: S(t) = &lt;strong&gt;Pr(T&gt;t)&lt;/strong&gt;. This is the yellow column below. It is calculated as S(t) = $\prod_{t=0}^{t}(1-h(t))$&lt;/li&gt;&#10;&lt;li&gt;Others will set the first period S(0) = 1 and then continue. This seems to be saying S(t) = &lt;strong&gt;Pr(T&gt;=t&lt;/strong&gt;). It is calculated as S(t) = $\prod_{t=0}^{t}(1-h(t-1))$&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;In the continuous case, I guess it doesn't matter between  Pr(T&gt;t) and Pr(T&gt;=t), but in the discrete case it does.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/8KDxw.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-05-26T02:12:34.447" FavoriteCount="2" Id="60036" LastActivityDate="2013-05-29T03:24:38.197" OwnerUserId="2040" PostTypeId="1" Score="3" Tags="&lt;survival&gt;&lt;discrete-data&gt;&lt;hazard&gt;" Title="Discrete Time Survival Analysis - Correct Way to Write Survival Function" ViewCount="395" />
  
  
  
  
  <row Body="&lt;p&gt;This is not an answer.  It is a community wiki that people may edit as they look for the answer.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.psycho.uni-duesseldorf.de/abteilungen/aap/gpower3/&quot; rel=&quot;nofollow&quot;&gt;G*power 3&lt;/a&gt; can perform (approximations) of these analyses (per &lt;a href=&quot;http://www.ats.ucla.edu/stat/gpower/indeppropor1.htm&quot; rel=&quot;nofollow&quot;&gt;this site&lt;/a&gt;).  The &lt;a href=&quot;http://www.psycho.uni-duesseldorf.de/abteilungen/aap/gpower3/download-and-register/Dokumente/GPower3-BRM-Paper.pdf&quot; rel=&quot;nofollow&quot;&gt;canonical reference for that software&lt;/a&gt; provides a reference for performing (at least some) of these types of power analyses as Cohen, 1988 chapter 6 (and 7) as does &lt;a href=&quot;http://www.ats.ucla.edu/stat/sas/dae/proportionpow.htm&quot; rel=&quot;nofollow&quot;&gt;this example&lt;/a&gt; using SAS.  The exact equations/procedures may be available from that source.  However, the approximations appear to break down at small probabilities.&lt;/p&gt;&#10;" CommentCount="3" CommunityOwnedDate="2013-05-26T16:15:28.053" CreationDate="2013-05-26T16:15:28.053" Id="60060" LastActivityDate="2013-06-14T03:09:22.517" LastEditDate="2013-06-14T03:09:22.517" LastEditorUserId="196" OwnerUserId="196" ParentId="59683" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;As you said, $L_{kk} = 0$ because you want to minimize the probability of misclassifying the sample. Then you want to minimize the area (where $p$ is the measure) of $\Omega-R_{k}$. The decision boundaries of your classifier define $R_{k}$. I hope that is clear.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, why $p(x,C_{j})$?. First, $p(x) = \sum_{i}p(x,C_{i})$. Now, let us assume that the $R_{i}'s$ form a partition of the space (i.e. $R_{i} \cap R_{j} = \emptyset$ if $i \neq j$ and $\cup_{k} R_{k} = \Omega$). Second, we know that $L_{kk} = 0$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The probability of misclassification for each class, is the probability of assigning the sample to any other class, i.e.&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\sum_{j}\int_{R_{j}}L_{kj}p(x)dx$$&lt;/p&gt;&#10;&#10;&lt;p&gt;If you substitute the above expressions and sum over k, you get your first expression.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-26T16:52:21.393" Id="60064" LastActivityDate="2013-05-26T16:52:21.393" OwnerUserId="17908" ParentId="50783" PostTypeId="2" Score="2" />
  
  
  
  
  <row AnswerCount="2" Body="&lt;p&gt;A short question concerning the constructing of a 'missing-dummy', which I will consequently add to my logistic regression. This missing-dummy gives value '1' to the cases where data concerning a specific variable is missing and value '0' to the cases which have data.&lt;/p&gt;&#10;&#10;&lt;p&gt;I've used this command in SPSS, but in some way the missing values are not recognized.&#10;( recode v204 (-2 -1=1)(else=0) into authomis ).&lt;/p&gt;&#10;&#10;&lt;p&gt;A cross-tabulation of v204 and authomis shows that the missing-dummy only has a '0'-category, regardless the fact that there are over 400 cases (on a total of 40.000) which have a missing on variable v204. Has this to do with how the missings are marked in variable v204? &#10;I can't think of any other reason. Hopefully you can. Thanks in advance.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-05-27T15:35:40.730" Id="60124" LastActivityDate="2013-09-25T13:01:00.957" LastEditDate="2013-08-26T08:58:08.783" LastEditorUserId="22047" OwnerUserId="26005" PostTypeId="1" Score="0" Tags="&lt;logistic&gt;&lt;spss&gt;" Title="Constructing a missing-dummy; why doesn't SPSS recognize the 'missings'?" ViewCount="168" />
  <row AcceptedAnswerId="60146" AnswerCount="1" Body="&lt;ol&gt;&#10;&lt;li&gt;Suppose a simple linear regression is carried out to investigate the relationship between salary and years employed, Y and X respectively.  Given some set of data points where should one begin to carry out the regression analysis and why?  Should I begin by calculating the mean of X and Y and then developing b0 and b1?  The first step surely is to do this to create the line, right?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="4" CreationDate="2013-05-27T17:44:14.847" Id="60135" LastActivityDate="2013-05-27T19:47:11.420" OwnerUserId="26091" PostTypeId="1" Score="0" Tags="&lt;regression&gt;" Title="First Steps, Linear Regression" ViewCount="43" />
  <row AcceptedAnswerId="60144" AnswerCount="1" Body="&lt;p&gt;This is probably a relatively simple question, but are b0 and b1 independent in a simple linear regression model?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-05-27T18:31:28.207" Id="60140" LastActivityDate="2013-05-27T19:37:51.973" OwnerUserId="26091" PostTypeId="1" Score="2" Tags="&lt;regression&gt;" Title="Independence in Linear Regression" ViewCount="54" />
  <row Body="&lt;p&gt;Good question. I suggest you take a look at the recent thesis of Shahab Jolani. He discusses various ways to combine the response model and the substantive model. See&#10;&lt;a href=&quot;http://igitur-archive.library.uu.nl/dissertations/2012-1120-200602/Jolani.pdf&quot; rel=&quot;nofollow&quot;&gt;http://igitur-archive.library.uu.nl/dissertations/2012-1120-200602/Jolani.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In general, the results are good (i.e. unbiased, appropriate coverage), and do not critically depend on the form of the parametrization chosen (chapter 2). Also, it is possible to put incomplete Y into the nonresponse model, thus opening up a way to deal with MNAR data by a MICE-like algorithm (chapter 4).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-05-27T19:50:26.773" Id="60148" LastActivityDate="2013-05-27T19:50:26.773" OwnerUserId="18128" ParentId="60114" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;Questionary data is special, and many methods designed for continuous variables will not work well.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, as you will likely not have millions of questionnaires, you can probably just use hierarchical clustering (try different linkages, maybe Ward) and a similarity coefficient that handles questionnaire data well; maybe try Gower's dissimilarity index first.&lt;/p&gt;&#10;&#10;&lt;p&gt;Unless you need a very scalable solution, hierarchical clustering is quite simple to implement yourself (so you can use your favorite similarity!). Plus, the dendrograms are a quite convenient visualization for small data sets.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-05-27T21:51:11.977" Id="60158" LastActivityDate="2013-05-27T21:51:11.977" OwnerUserId="7828" ParentId="60112" PostTypeId="2" Score="1" />
  
  
  
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;So an F-test for regression tests &quot;the utility of the model&quot;, &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Well, &lt;em&gt;no&lt;/em&gt;, it really doesn't. A rejection doesn't imply the model is actually useful &lt;em&gt;at all&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;or some such carefully worded statement.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;If you mean that 'there's &lt;em&gt;a&lt;/em&gt; carefully worded statement that applies to the F-test', this is a trivially true, but essentially useless statement.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Crossvalidation basically does the same thing. &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;No, it doesn't do the same thing as the F-test, not even approximately.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;What is the best way to express what an F-test tells you about the predictive power of your model?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Perhaps something along the lines of &quot;Unless you have an unusual definition of predictive power, the F-test may not directly tell you much about it.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;Specifically, the summary of the result of the &lt;em&gt;test&lt;/em&gt; would be &quot;$H_0$ was rejected&quot; or &quot;$H_0$ was not rejected&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;But let's take a step back and consider something slightly more informative, like the $p-value$. That doesn't really tell you about predictive value (which to me wouldn't tend to change much with $n$, once the sample size was large enough to have reasonable parameter estimates, utterly unlike the p-value.).&lt;/p&gt;&#10;&#10;&lt;p&gt;If you define predictive power as say $R^2$ (which to me would seem bizarre), then the $F$ &lt;em&gt;statistic&lt;/em&gt; is related to it (for fixed model and sample size, at least), but we're now getting some distance from the &lt;em&gt;test&lt;/em&gt; and still not all that close to &lt;em&gt;predictive power&lt;/em&gt; yet. And if we start doing variable selection or something, $R^2$ is getting further and further from carrying any information about actual predictive power.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Would it be weird to see an analysis of data where someone used crossvalidation and quoted the p-value for the F-test to asses its predictive ability?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Well, weird because the F-test doesn't really address that question.&lt;/p&gt;&#10;&#10;&lt;p&gt;It seems to me a better question for you might be &quot;What does an F-test actually do?&quot; and a similar question about cross-validation, and maybe resolve exactly what 'predictive power' might usefully mean.&lt;/p&gt;&#10;&#10;&lt;p&gt;Once you define more carefully what particular thing you're doing cross-validation &lt;em&gt;on&lt;/em&gt;, and exactly what the circumstances and assumptions are, it would be possible to make more useful/interesting statements.&lt;/p&gt;&#10;&#10;&lt;p&gt;--&lt;/p&gt;&#10;&#10;&lt;p&gt;You might find this a good starting point for basic discussion of cross-validation from a statistical point of view:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://robjhyndman.com/hyndsight/crossvalidation/&quot; rel=&quot;nofollow&quot;&gt;http://robjhyndman.com/hyndsight/crossvalidation/&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;You might notice there's hardly a mention of hypothesis tests at all, except a couple of sentences of precaution about circumstances in which they don't make much sense.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-05-28T07:14:45.257" Id="60195" LastActivityDate="2013-05-28T07:19:52.087" LastEditDate="2013-05-28T07:19:52.087" LastEditorUserId="805" OwnerUserId="805" ParentId="60193" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;This is a classic case of simultaneous inference used for selective inference (see [1]). I will explain.&lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;Selective (marginal) inference&quot; is when you want to make inference on a data-driven subset of parameters. In the ANOVA case, the subset would be several contrasts.&#10;&quot;Simultaneous inference&quot; is when you want to make inference on a particular vector of parameters. &#10;Naturally, simultaneous entails selective, as the joint truthfulness of a vector, entails the truthfulness of each of its (selected) elements. &lt;/p&gt;&#10;&#10;&lt;p&gt;Sheffe's view of the problem is the following: since he does not know a-priori the contrast the researcher will study, he will offer &lt;strong&gt;simultaneous&lt;/strong&gt; control over &lt;strong&gt;all possible&lt;/strong&gt; contrasts. In this case, no matter what contrast the researcher chooses, data driven or not, he is already controlling for it. &lt;/p&gt;&#10;&#10;&lt;p&gt;In conclusion: if using Scheffe's method for inference or CI, there is no problem in inferring on selected parameters. The problem is that for a particular parameter/contrast, it is excessively conservative (i.e., low powered).&lt;/p&gt;&#10;&#10;&lt;p&gt;[1] Cox, D. R. 1965. “A Remark on Multiple Comparison Methods.” Technometrics 7 (2): 223–224. doi:10.1080/00401706.1965.10490250.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-28T07:41:14.133" Id="60196" LastActivityDate="2013-05-28T11:43:03.860" LastEditDate="2013-05-28T11:43:03.860" LastEditorUserId="17230" OwnerUserId="6961" ParentId="60175" PostTypeId="2" Score="4" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I am beginner in forecasting, especially forecasting with R and I am really willing to improve my knowledge. &lt;/p&gt;&#10;&#10;&lt;p&gt;Recently, I started practicing electricity consumption time series forecasting. &lt;/p&gt;&#10;&#10;&lt;p&gt;The first barrier I faced is the choice of out of sample data for assessing the forecast accuracy of the forecast model i will be using (regression with ARIMA errors). &lt;/p&gt;&#10;&#10;&lt;p&gt;I have data for 147 months and I want to forecast the next 24 months, for the period June 2013 to January 2015. Furthermore, I have read in &lt;a href=&quot;http://stats.stackexchange.com/users/159/rob-hyndman&quot;&gt;@RobHyndman&lt;/a&gt;'s online text book  &lt;/p&gt;&#10;&#10;&lt;p&gt;Hyn­d­man, R.J. and Athana­sopou­los, G. (2013),&lt;br&gt;&#10;&lt;em&gt;Forecasting: principles and practice&lt;/em&gt;,&lt;br&gt;&#10;(accessed 28 May 2013),  &lt;a href=&quot;http://otexts.com/fpp/2/5/&quot; rel=&quot;nofollow&quot;&gt;section 2.5&lt;/a&gt;&lt;br&gt;&#10;under '&lt;strong&gt;Training and test sets&lt;/strong&gt;', that:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;size of the test set is typ­i­cally about 20% of the total sam­ple, although this value depends on how long the sam­ple is and how far ahead you want to fore­cast&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;If I divide the dataset with 20% of it being the out-of-sample data, the forecast model applied to the in-sample data is not quite accurate, since I guess it fails to capture the recent trend, (which began in the middle of the last year), of decreased electricity consumption due to a significantly raised electricity tariff. &lt;/p&gt;&#10;&#10;&lt;p&gt;What do I do about this?&lt;/p&gt;&#10;&#10;&lt;p&gt;Can you possibly give me instruction on what would be considered appropriate size of the out of sample data. I also tried with 7 months of out-of-sample data, but I am afraid that there might be an overfitting issue. Is that right?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-05-28T08:38:08.567" Id="60200" LastActivityDate="2014-04-30T06:51:26.360" LastEditDate="2013-05-28T23:10:30.720" LastEditorUserId="805" OwnerUserId="25925" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;forecasting&gt;&lt;out-of-sample&gt;" Title="Choosing the right size of an out of sample data" ViewCount="170" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I have a range of readings from a test (for example:0.796, 0.109, 0.11, 0.11, 0.109, 0.109, 0.109, 1.78). I want to remove the extraneous values of these so that I get a more realistic view on the more accurate test result range. If I take the average (1.78) or the median (1.09) it doesn't give me that picture. I am not able to figure out how I could mathematically have a way to achieve this, because I don't want these extraneous values to skew my results.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-05-28T13:39:52.247" Id="60231" LastActivityDate="2013-05-28T13:59:51.847" OwnerUserId="26217" PostTypeId="1" Score="0" Tags="&lt;average&gt;" Title="Exclude extraneous values in a range" ViewCount="31" />
  <row Body="&lt;p&gt;&lt;code&gt;TraMineR&lt;/code&gt; does not (yet) compute &lt;code&gt;Duration weighted Optimal Matching Algorithm&lt;/code&gt;. Unfortunatly, this distance measure does not garantee the triangular inequality (in French, see chapter 2 of &lt;a href=&quot;http://archive-ouverte.unige.ch/vital/access/manager/Repository/unige:22054&quot; rel=&quot;nofollow&quot;&gt;http://archive-ouverte.unige.ch/vital/access/manager/Repository/unige:22054&lt;/a&gt;). Depending on the distance analysis procedure (clustering) you plan to use, this may be problematic. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-05-28T14:16:12.177" Id="60237" LastActivityDate="2013-05-28T14:16:12.177" OwnerUserId="13555" ParentId="60229" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="60250" AnswerCount="3" Body="&lt;p&gt;I'm trying to get intuition for each of the main functions in actuarial science (specifically for the Cox Proportional Hazards Model).  Here's what I have so far:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;$f(x)$: starting at the start time, the probability distribution of when you will die.&lt;/li&gt;&#10;&lt;li&gt;$F(x)$: just the cumulative distribution.  At time $T$, what % of the population will be dead?&lt;/li&gt;&#10;&lt;li&gt;$S(x)$: $1-F(x)$.  At time $T$, what % of the population will be alive?&lt;/li&gt;&#10;&lt;li&gt;$h(x)$: hazard function.  At a given time $T$, of the people still alive, this can be used to estimate how many people will die in the next time interval, or if interval-&gt;0, 'instantaneous' death probability.&lt;/li&gt;&#10;&lt;li&gt;$H(x)$: cumulative hazard.  No idea.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;What's the idea behind combining hazard values, especially when they are continuous?  If we use a discrete example with death rates across four seasons, and the hazard function is as follows:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Starting at Spring, everyone is alive, and 20% will die&lt;/li&gt;&#10;&lt;li&gt;Now in Summer, of those remaining, 50% will die&lt;/li&gt;&#10;&lt;li&gt;Now in Fall, of those remaining, 75% will die&lt;/li&gt;&#10;&lt;li&gt;Final season is Winter.  Of those remaining, 100% will die&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Then the cumulative hazard is 20%, 70%, 145%, 245%??  What does that mean, and why is this useful?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-05-28T14:25:30.040" FavoriteCount="3" Id="60238" LastActivityDate="2015-02-18T13:45:30.637" LastEditDate="2013-07-22T12:15:16.160" LastEditorUserId="17230" OwnerUserId="26220" PostTypeId="1" Score="7" Tags="&lt;probability&gt;&lt;survival&gt;&lt;hazard&gt;" Title="Intuition for cumulative hazard function (survival analysis)" ViewCount="3343" />
  <row AcceptedAnswerId="60255" AnswerCount="1" Body="&lt;p&gt;How does SAS calculate t-values?  &lt;/p&gt;&#10;&#10;&lt;p&gt;For example, say we have two variables: intercept (which is always present) and temperature in a model that is regressing temperature on mortality in a certain region.&lt;/p&gt;&#10;&#10;&lt;p&gt;t Value&lt;/p&gt;&#10;&#10;&lt;p&gt;-1.39 (Intercept)&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;(B)  (Temperature)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;How might one calculate (B)?  I understand that symbolically if temperature is taken to be $b_1$ then we have $\frac{b_1- \beta_1}{s({b_1})}$.  I assume they are testing here whether or not $\beta = 0$ so we can make the numerator just $b_1$. Where might one look in a typical SAS output to get this data?  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-28T15:11:15.973" Id="60244" LastActivityDate="2013-05-28T16:52:10.823" OwnerUserId="26091" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;t-test&gt;&lt;sas&gt;" Title="Finding Data in SAS Output for Linear Regression" ViewCount="199" />
  
  
  <row Body="&lt;p&gt;For White standard errors clustered by group with the plm frame work try&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;coeftest(model.plm,vcov=vcovHC(model.plm,type=&quot;HC0&quot;,cluster=&quot;group&quot;))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;where &lt;code&gt;model.plm&lt;/code&gt; is a plm model.&lt;/p&gt;&#10;&#10;&lt;p&gt;See also this link&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.inside-r.org/packages/cran/plm/docs/vcovHC&quot;&gt;http://www.inside-r.org/packages/cran/plm/docs/vcovHC&lt;/a&gt; or the plm package documentation&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;For two-way clustering (e.g. group and time) see the following link:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://people.su.se/~ma/clustering.pdf&quot;&gt;http://people.su.se/~ma/clustering.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Clustering and other information, espeically for Stata, can be found here:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.kellogg.northwestern.edu/faculty/petersen/htm/papers/se/se_programming.htm&quot;&gt;http://www.kellogg.northwestern.edu/faculty/petersen/htm/papers/se/se_programming.htm&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-28T18:25:53.907" Id="60262" LastActivityDate="2013-06-30T22:56:16.687" LastEditDate="2013-06-30T22:56:16.687" LastEditorUserId="22047" OwnerUserId="12053" ParentId="10017" PostTypeId="2" Score="10" />
  
  <row AcceptedAnswerId="60517" AnswerCount="1" Body="&lt;p&gt;I'm working on a classifier to sort out transportation modes based on certain attributes of an activity. I use a nearest neighbor algorithm like this :&lt;/p&gt;&#10;&#10;&lt;p&gt;2 example sets of training data (let's assume they are the only ones I have)&lt;/p&gt;&#10;&#10;&lt;p&gt;This set represents a car ride:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Attribute  |  AvgSpeed   TopSpeed   GyroValue &#10;---------------------------------------------&#10;Value      |  40         80         20&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This set represents a walk:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Attribute  |  AvgSpeed   TopSpeed   GyroValue &#10;---------------------------------------------&#10;Value      |  5          8          90&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;AvgSpeed, TopSpeed and GyroValue have different weights (e.g. 0.3, 0.3, 0.4)&lt;/p&gt;&#10;&#10;&lt;p&gt;If I feed the program something like &lt;code&gt;A=25, B=60, C=40&lt;/code&gt;, it should be able to predict a car ride.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm currently using a k-neighbors classification system for this (more specifically the KNeighborsClassifier algorithm with scikit-learn in python). Would there be a stronger case for another type of ML algorithm here? I'm not convinced this is the best fit for my data, and this is not exactly my field of expertise.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-28T20:23:45.243" Id="60270" LastActivityDate="2013-05-31T11:55:40.473" OwnerUserId="26230" PostTypeId="1" Score="2" Tags="&lt;machine-learning&gt;&lt;algorithms&gt;&lt;k-nearest-neighbour&gt;" Title="Categorizing transportation modes with k-nearest neighbor?" ViewCount="70" />
  <row AnswerCount="3" Body="&lt;p&gt;I am hydrologist and I am looking for a solution for the following problem:&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose that I have $k$ regions each yielding a sample of the same variable (e.g., annual peak flow) drawn from different sites within the given region  (each sample $i=1,..K$ has a record length $n_i$).&lt;/p&gt;&#10;&#10;&lt;p&gt;The homogeneity test (e.g., $K$ sample Anderson-Darling test) answers the question: is the region homogeneous? As a null hypothesis test, there are only two possibilities of answer: i.e., either the region is homogeneous ($F_1=F_2=...=F_k=F$) or it is heterogeneous. This binary answer is not flexible and do not allow me to compare homogeneity among several regions to point out the most homogeneous of them. &lt;/p&gt;&#10;&#10;&lt;p&gt;So I am looking for a measure that overcomes this limitation. By analogy, this measure would be the same as the Akaike Information Criterion (AIC) for linear models. Indeed, the AIC compares different models in order to determine which one is the better to fit to data. Thus, this measure would compare different region and measure of how much sampling sites are alike within regions.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-28T22:23:09.343" FavoriteCount="1" Id="60280" LastActivityDate="2013-10-26T05:39:09.273" LastEditDate="2013-07-28T04:42:24.840" LastEditorUserId="183" OwnerUserId="26236" PostTypeId="1" Score="2" Tags="&lt;heteroscedasticity&gt;" Title="Quantifying homogeneity" ViewCount="86" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I would like to calculate the MLE estimates of the mean and variance of a Weibull distributional assumption, from a given sample. Now, there are two parameter and three parameter weibull distributions. How would I know, which one would be a better choice? I can see that a two parameter distribution in general can account more for dispersion although. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-05-28T23:21:03.453" Id="60288" LastActivityDate="2013-05-28T23:21:03.453" OwnerUserId="26238" PostTypeId="1" Score="2" Tags="&lt;hypothesis-testing&gt;&lt;estimation&gt;&lt;mathematical-statistics&gt;&lt;maximum-likelihood&gt;&lt;weibull&gt;" Title="MLE: Two parameter or three parameter Weibull?" ViewCount="144" />
  
  
  <row Body="&lt;p&gt;There are several options.&lt;/p&gt;&#10;&#10;&lt;p&gt;(i) You could do a two-sample test of binomial proportions / two sample proportions test.&lt;/p&gt;&#10;&#10;&lt;p&gt;With your sample size, the normal approximation should be okay, though - you don't necessarily have to worry about the binomial part.&lt;/p&gt;&#10;&#10;&lt;p&gt;(ii) You could do a chi-square test of independence (which also tests equality of proportion); this is basically equivalent to the first option if your test is two-tailed, or similarly, you could do a $G^2$ test.&lt;/p&gt;&#10;&#10;&lt;p&gt;(iii) You &lt;em&gt;might&lt;/em&gt; do a Fisher test, I guess. &lt;/p&gt;&#10;&#10;&lt;p&gt;(You could do something more complicated like a logistic regression but I don't see the need here.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Depending somewhat on your area, the 2x2 chi-square test is probably the most likely to be familiar to other people looking at it. If you want a one tailed test, the two sample proportions test is the way to go.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2013-05-29T01:26:11.397" Id="60304" LastActivityDate="2013-05-29T01:34:54.613" LastEditDate="2013-05-29T01:34:54.613" LastEditorUserId="805" OwnerUserId="805" ParentId="60246" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/34616/difference-between-regression-analysis-and-analysis-of-variance/34618#34618&quot;&gt;This answer&lt;/a&gt; that I posted earlier is somewhat relevant, but this question is somewhat different.&lt;/p&gt;&#10;&#10;&lt;p&gt;You might want to think about the differences and similarities between the following linear models:&#10;$$&#10;\begin{bmatrix} Y_1 \\ \vdots \\ Y_n \end{bmatrix} = \begin{bmatrix} 1 &amp;amp; x_1 \\ 1 &amp;amp; x_2 \\ 1 &amp;amp; x_3 \\ \vdots &amp;amp; \vdots \\ 1 &amp;amp; x_n \end{bmatrix} \begin{bmatrix} \alpha_0 \\ \alpha_1 \end{bmatrix} +  \begin{bmatrix} \varepsilon_1 \\ \vdots \\ \vdots \\ \varepsilon_n \end{bmatrix}&#10;$$&#10;$$&#10;\begin{bmatrix} Y_1 \\ \vdots \\ Y_n \end{bmatrix} = \begin{bmatrix} 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\ \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; &amp;amp; \vdots  \\ 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\ \hline 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\ \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; &amp;amp; \vdots \\ 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\  \hline 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 0 \\  \vdots &amp;amp; &amp;amp; &amp;amp; &amp;amp; \vdots \\ \vdots &amp;amp; &amp;amp; &amp;amp; &amp;amp; \vdots \end{bmatrix} \begin{bmatrix} \alpha_0 \\ \vdots \\ \alpha_k \end{bmatrix} +  \begin{bmatrix} \varepsilon_1 \\ \vdots \\ \vdots \\ \varepsilon_n \end{bmatrix}&#10;$$&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-29T02:21:39.997" Id="60307" LastActivityDate="2013-05-29T02:21:39.997" OwnerUserId="5176" ParentId="59047" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Kernel_density_estimation&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Kernel_density_estimation&lt;/a&gt; gives a good introduction. &lt;/p&gt;&#10;&#10;&lt;p&gt;Your first question is about &lt;code&gt;the trend in a population&lt;/code&gt;. The main sense of &quot;trend&quot; in statistics is that of an overall change in the mean or level of a variable with some other variable, most commonly time, but occasionally space or something else. In ordinary language &quot;trend&quot; is often used much more loosely to refer to any kind of pattern or relationship, but I'd advise against that in statistical discussions. Kernel density estimation just gives you a picture of the distribution of one or more variables, which is different from a trend as statistical people would use that word. &lt;/p&gt;&#10;&#10;&lt;p&gt;You may want to revise your question after some reading, but my main point is that this is a well-documented and standard technique with resources easily available to you. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-29T05:52:20.453" Id="60319" LastActivityDate="2013-05-29T05:52:20.453" OwnerUserId="22047" ParentId="60318" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;I cannot answer 2 and 3; the fact that there is near-perfect relationship among slope ant intercept coefficients suggest that the variance of both may be quite large and also account for the discontinuities you observe in 2. But one would need to look at your data and code.&lt;/p&gt;&#10;&#10;&lt;p&gt;Your question 1, on the other hand, is easy. You have to realize that, even if you assume the parameters are fixed, the Kalman filter gives a sequence of estimates with observations up to a certain time point t. If you were to fit ordinary regressions with samples from 1 to t for different t, you would also obtain different estimates with each.&lt;/p&gt;&#10;&#10;&lt;p&gt;What is true is that, except for numerical problems that may arise (and for the possible effect of the prior), the final estimate of the state would coincide with the estimate obtained from the ordinary regression with the same sample.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2013-05-29T07:14:56.200" Id="60321" LastActivityDate="2013-05-29T07:14:56.200" OwnerUserId="892" ParentId="59952" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;You can also have a look at &lt;a href=&quot;http://cran.r-project.org/web/packages/FSelector/FSelector.pdf&quot;&gt;FSelector&lt;/a&gt;, &lt;a href=&quot;http://cran.r-project.org/web/packages/varSelRF/varSelRF.pdf&quot;&gt;varSelRF&lt;/a&gt;.&#10;FSelector contains multiple functions for feature selection based for example on the chi square test, on the information theory (entropy, mutual information, gain ratio,...), on the correlation between feature, consistency etc... varSelRF is a useful package for feature selection using random forests with backwards variable elimination and with importance spectrum.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-05-29T07:37:19.097" Id="60324" LastActivityDate="2013-05-29T08:28:07.370" LastEditDate="2013-05-29T08:28:07.370" LastEditorUserId="24728" OwnerUserId="24728" ParentId="56092" PostTypeId="2" Score="8" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have an algorithm with 3 parameters and sum of these parameters is equal to one; $a_1+a_2+a_3=1$ and each of them must be between $0$ and $1$. I want to find the optimum point for this parameters. The equation is a linear combination of these three parameters, $y=a_1 g_1+a_2 g_2+a_3 g_3$ and $g_1$, $g_2$ and $g_3$ are some known constants. If I use &lt;a href=&quot;http://en.wikipedia.org/wiki/Cross-validation_%28statistics%29&quot; rel=&quot;nofollow&quot;&gt;k fold cross validation&lt;/a&gt; to find the optimum point the space of search is very large because the dataset is large. &lt;/p&gt;&#10;&#10;&lt;p&gt;Is there any test method for such cases? If I want to search the optimum point for example by k-fold and by taking two of them constant and change the other, the space become large. Any suggestion? Is there better method for this?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks.&lt;/p&gt;&#10;" CommentCount="17" CreationDate="2013-05-29T10:12:33.630" Id="60332" LastActivityDate="2013-05-29T13:23:29.850" LastEditDate="2013-05-29T11:21:18.903" LastEditorUserId="805" OwnerUserId="24972" PostTypeId="1" Score="0" Tags="&lt;cross-validation&gt;&lt;optimization&gt;&lt;parameterization&gt;" Title="Finding optimum point of parameters" ViewCount="65" />
  
  
  <row Body="&lt;p&gt;I am assuming that we can just replace $v_1, v_2, c$ with a Bernoulli random variable $B$ with probability $p$, which we use to accept events since $v_1$ and $v_2$ are almost surely not sampled at the same time.&lt;/p&gt;&#10;&#10;&lt;p&gt;The superposition of $N_3$ and $N_4$ are a Poisson process and are equal to $N_5$ by the colouring and superposition theorems.&lt;/p&gt;&#10;&#10;&lt;p&gt;Imagine a single Poisson process with rate $T = \lambda_1 + \lambda_2$.  Colour each event red with probability $\frac{p\lambda_1}{T}$, green with probability $\frac{p\lambda_2}{T}$, and blue otherwise.  Then, the red events are $N_3$ and the green events are $N_4$.  This construction matches yours, yet in this one it is clear that $N_3$ and $N_4$ are independent (contrary to your statement).&lt;/p&gt;&#10;&#10;&lt;p&gt;The superposition of independent Poisson processes is a Poisson process.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2013-05-29T17:38:20.537" Id="60365" LastActivityDate="2013-05-29T21:04:54.687" LastEditDate="2013-05-29T21:04:54.687" LastEditorUserId="858" OwnerUserId="858" ParentId="60355" PostTypeId="2" Score="1" />
  
  <row AnswerCount="2" Body="&lt;p&gt;first post, so go easy...&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm probably being a bit simplistic here, but from what I understand the Bonferroni post hoc test is simply when we adjust the alpha to account for inflated error when several post hoc tests are conducted. You divide the alpha by number of tests. So what exactly does SPSS do when we click the button for Bonferroni? Shouldn't the dividing of alpha be done by us in interpreting the result, e.g. if we do 10 post hoc tests our alpha criterion should be .005 so if the p value is .012 then it is not significant. But SPSS hasn't done anything there, we have just changed our interpretation. So what exactly is it adjusting?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-05-29T21:23:05.073" FavoriteCount="1" Id="60383" LastActivityDate="2014-11-27T04:41:43.833" OwnerUserId="26279" PostTypeId="1" Score="1" Tags="&lt;spss&gt;&lt;post-hoc&gt;&lt;bonferroni&gt;" Title="Bonferroni adjustment in SPSS - what does it do?" ViewCount="14518" />
  <row Body="&lt;p&gt;According to IBM, they compute the confidence intervals for the difference between two means $\bar{x}_{i}$ and $\bar{x}_{j}$ out of $k$ total levels to be compared with Bonferroni correction as:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\bar{x}_{i}-\bar{x}_{j} \gt s_{pp} \sqrt{\frac{1}{2} \left( \frac{1}{n_i} + \frac{1}{n_j}\right)} \sqrt{2F_{1-\alpha^{\prime},1,n-1}}$&lt;/p&gt;&#10;&#10;&lt;p&gt;where the term $\alpha^{\prime}=\frac{2\alpha}{k \left( k-1 \right)}$ is the adjusted false positive rate based on $\alpha$ as an overall false positive rate, group sample sizes $n_i$ and $n_j$, and the square root of the mean square error $s_{pp}$. &lt;/p&gt;&#10;&#10;&lt;p&gt;So basically solve for $\alpha$ and you get a new $p$ value. In the case of 4 groups with pairwise comparisons, the new $p$ values would be the unadjusted ones multiplied by $\frac{4\times3}{2}=6$&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-29T22:58:14.047" Id="60389" LastActivityDate="2013-05-29T22:58:14.047" OwnerUserId="8807" ParentId="60383" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Too long for a comment.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;But I do not think v1 and v2 can be replaced with a Bernoulli random variable. Specially, v1 and v2 be denoted as two dependent log-normal varialbles with correlation ρ. In other words, we can describe v1 and v2 as a multivariate log-normal variable. In this time, how to answer those two questions?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I think you're mistaken; they can be. &lt;/p&gt;&#10;&#10;&lt;p&gt;The only way that $v_1$ and $v_2$ affect anything is through the condition that they're less than $c$. Let $b_1$ and $b_2$ take the value 1 when their corresponding $v_i&amp;lt;c$. You now have that $b_1$ and $b_2$ are dependent Bernoullis. What information relevant to the question &lt;em&gt;other&lt;/em&gt; than the information in the pair $(b_1,b_2)$ is there in $(v_1,v_2)$? If there is no additional information of relevance to the question, then $(b_1,b_2)$ clearly &lt;em&gt;can&lt;/em&gt; replace $(v_1,v_2)$.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-05-30T01:25:59.503" Id="60400" LastActivityDate="2013-05-30T01:25:59.503" OwnerUserId="805" ParentId="60355" PostTypeId="2" Score="2" />
  <row AnswerCount="0" Body="&lt;p&gt;I have a certain confusion. I refer to this &lt;a href=&quot;http://arxiv.org/pdf/0903.5463v3.pdf&quot; rel=&quot;nofollow&quot;&gt;paper&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's say I have $p$ variables $x_1, x_2, \dots, x_p$ which follow a multivariate Gaussian distribution. Now suppose I have $N$ examples or samples from this distribution. However, some of the values in each example are missing.&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose I want to impute the values.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's say using only the available examples I calculate the $\mu$ and $\Sigma$ vector which are the mean and covariances (initial estimate). Let $K = \Sigma^{-1}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now I found this formula&lt;/p&gt;&#10;&#10;&lt;p&gt;$E[x_{ij}|x_{obs,i}, \mu,K]$ = $x_{ij}$ if $x_{ij}$ is observed&lt;/p&gt;&#10;&#10;&lt;p&gt;$=c_j$ if $x_{ij}$ is not observed&lt;/p&gt;&#10;&#10;&lt;p&gt;where $c = \mu-(K_{mis,mis})^{-1}*K_{mis,obs}(x_{obs,i}-\mu_{obs})$&lt;/p&gt;&#10;&#10;&lt;p&gt;$E[x_{ij}x_{ij'}|x_{obs,i},\mu,K]$ = $(K_{mis,mis})^{-1}_{jj'} + c_jc_{j'}$ if both $x_{ij}$ and $x_{ij'}$ are unobserved.&lt;/p&gt;&#10;&#10;&lt;p&gt;However,&#10;$E[x_{ij}x_{ij'}|x_{obs,i},\mu,K]$ = $x_{ij}x_{ij'}$ if both $x_{ij}$ and $x_{ij'}$ are observed.&lt;/p&gt;&#10;&#10;&lt;p&gt;I didn't get how the term $(K_{mis,mis})^{-1}_{jj'}$ disappears if either $x_{ij}$ or $x_{ij'}$ is observed. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-30T03:36:58.393" Id="60404" LastActivityDate="2014-03-01T04:44:06.460" LastEditDate="2013-06-29T18:23:30.200" LastEditorUserId="22047" OwnerUserId="12329" PostTypeId="1" Score="2" Tags="&lt;normal-distribution&gt;&lt;data-imputation&gt;&lt;conditional-expectation&gt;" Title="Confusion related to conditional Gaussian distribution" ViewCount="95" />
  <row Body="&lt;p&gt;Regarding the first part of your question. I believe it is more correct to say that $D$ is an indicator for assignment of treatment. Thus the assumption is that the choice, whether an individual gets treated or not, is not correlated to possible outcomes. &lt;/p&gt;&#10;&#10;&lt;p&gt;The problem is the possible selection into treatment. It may be that treatment is assigned (or there is self-selection into) to those who are going to benefit most from it. For instance, suppose there is some training that improves academic achievement and you want to measure its impact. However, students are not assigned randomly for this course, but it is chosen mostly by those who have excellent computer literacy. In this case, if you estimate treatment effect of this particular training, you compare the outcomes of participants (mostly computer literate) to non-participants (mostly not computer literate). Thus the result would be biased as the outcome is partly dependent on selection of individuals to be treated. Selection bias should be evident if assignment to treatment correlates with outcome. &lt;/p&gt;&#10;&#10;&lt;p&gt;If there is such non-random assignment to treatment and you know that the assignment depends only on characteristic $X$ (in this case computer literacy), you make an assumption that after controlling for $X$ both the treated and non-treated groups are equivalent in their remaining characteristics, except that some of them got treated and others not. So the difference between outcomes of the treated and non-treated can be attributed only to the fact of being treated, not that the individuals in groups were different from the beginning. Conditional on $X$ you assume that assignment to treatment is random, so it cannot correlate with possible outcomes. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-30T06:00:43.313" Id="60412" LastActivityDate="2013-05-30T06:00:43.313" OwnerUserId="26080" ParentId="60336" PostTypeId="2" Score="1" />
  <row AnswerCount="2" Body="&lt;p&gt;Here are some results for ANN and KNN on abalone data set using Weka:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Result for ANN &#10;Correctly Classified Instances  3183 76.203 % &#10;Incorrectly Classified Instances 994 23.797 % &#10;Mean absolute error      0.214 &#10;Root mean squared error  0.3349 &#10;Relative absolute error 58.6486 %&#10;&#10;Result for KNN &#10;Correctly Classified Instances  3211 76.8734 % &#10;Incorrectly Classified Instances 966 23.1266 % &#10;Mean absolute error      0.2142 &#10;Root mean squared error  0.3361 &#10;Relative absolute error 58.7113 %&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;KNN has high accuracy but ANN has low errors. So which of the two algorithms should I say is better? &lt;/p&gt;&#10;&#10;&lt;p&gt;Which is the more preferable criterion, accuracy or error?&lt;/p&gt;&#10;&#10;&lt;p&gt;What I understood was that error should decrease with high accuracy, but the results here are opposite. Why is this so?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-05-30T06:51:30.460" FavoriteCount="1" Id="60418" LastActivityDate="2014-01-28T13:48:45.447" LastEditDate="2013-05-30T08:09:26.107" LastEditorUserId="805" OwnerUserId="26293" PostTypeId="1" Score="1" Tags="&lt;classification&gt;&lt;weka&gt;" Title="What is the best criterion for performance evaluation in classification algorithms?" ViewCount="1716" />
  <row AnswerCount="0" Body="&lt;p&gt;Small question as a follow up from this topic:&#10;&lt;a href=&quot;http://stats.stackexchange.com/questions/27748/relationship-between-two-time-series-arima&quot;&gt;Relationship between two time series: ARIMA&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I am trying to reproduce the above through R. In the first steps, IrishStat applies AR(1) on doubly differenced x and single differenced y. However, after applying this filter in R, we obtain the following:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; ar1_12 &amp;lt;- arima(df1$x, order=c(1,2,0))&#10;&amp;gt; ar2_12 &amp;lt;- arima(df1$y, order=c(1,1,0))&#10;&amp;gt; ar1_12&#10;&#10;Call:&#10;arima(x = df1$x, order = c(1, 2, 0))&#10;&#10;Coefficients:&#10;ar1&#10;-0.3761&#10;s.e. 0.1306&#10;&#10;sigma^2 estimated as 0.3199: log likelihood = -42.53, aic = 89.06&#10;&amp;gt; ar2_12&#10;&#10;Call:&#10;arima(x = df1$y, order = c(1, 1, 0))&#10;&#10;Coefficients:&#10;ar1&#10;-0.4815&#10;s.e. 0.1213&#10;&#10;sigma^2 estimated as 0.0328: log likelihood = 14.64, aic = -25.29&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;What can we conclude from this? Could you provide maybe some elaboration on the thought process regarding the cross-correlative structure of these two arima filters? Why is it that both autoregressive factors are -.383 in the example by IrishStat?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-05-30T08:39:09.830" Id="60427" LastActivityDate="2013-05-30T08:39:09.830" OwnerUserId="26296" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;&lt;arima&gt;&lt;autocorrelation&gt;" Title="Applying ARIMA in R" ViewCount="78" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I am doing a regression analysis using dummy variables, but I need to include weights. I am using data from an opinion poll, and I am trying to make sure that the sample is regionally representative. So I would like to apply weights for responses from certain regions. How do I do this with dummy variables? Can I simply apply to weight to the dummy before putting it into the regression? Any guidance would be appreciated. thanks&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-05-30T13:30:30.020" Id="60445" LastActivityDate="2013-05-30T14:03:46.803" LastEditDate="2013-05-30T14:03:46.803" LastEditorUserId="6029" OwnerUserId="26306" PostTypeId="1" Score="1" Tags="&lt;regression&gt;" Title="Using weights on dummy variables in R" ViewCount="72" />
  
  <row Body="&lt;p&gt;I decided to move my comment to an answer and discuss it&lt;/p&gt;&#10;&#10;&lt;p&gt;To expand on my points a little:&lt;/p&gt;&#10;&#10;&lt;p&gt;Your thought that the way you're calculating $R^2$ isn't sensible is right.&lt;/p&gt;&#10;&#10;&lt;p&gt;A high correlation between residuals and arbitrary fitted values doesn't automatically imply a good fit. Indeed, forget nonlinear regression, and consider linear models.&lt;/p&gt;&#10;&#10;&lt;p&gt;Imagine you have two linear models. For the first model, the Flying Spaghetti Monster comes to you in a dream, touches you with his noodly appendage and &lt;em&gt;tells you the true parameter values&lt;/em&gt;:&lt;/p&gt;&#10;&#10;&lt;p&gt;$y_i = 9 + 2\,x_i + e_i$, (and that the errors are independent and identically distributed $N(0,\sigma^2)$)&lt;/p&gt;&#10;&#10;&lt;p&gt;The next day, as you're telling a friend over a pint about your experience, a passing homeopathy salesman suggests that instead&lt;/p&gt;&#10;&#10;&lt;p&gt;$y_i = -1000 + 7.3\times 10^{-14}\,x_i + e_i$&lt;/p&gt;&#10;&#10;&lt;p&gt;Imagine that on looking at the data, the dream-values appear to be about right (least squares estimates come out very close to them), and that $\sigma^2$ is estimated to be really tiny, so that the residuals from both the dream-values and the LS fit are minuscule.&lt;/p&gt;&#10;&#10;&lt;p&gt;Further, the correlation is almost 1.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, what's the correlation of the data with the fitted values from the salesman-in-the-pub's model?&lt;/p&gt;&#10;&#10;&lt;p&gt;It should be low, right? The model is completely wrong! Its predictions are no better than the mean.&lt;/p&gt;&#10;&#10;&lt;p&gt;... in fact, its correlation is &lt;em&gt;exactly the same&lt;/em&gt;; almost 1.&lt;/p&gt;&#10;&#10;&lt;p&gt;If that was what $R^2$ was, it would be useless as a way of comparing models.&lt;/p&gt;&#10;&#10;&lt;p&gt;This sounds like it might provide a useful object lesson in exactly why they shouldn't be using that definition of $R^2$ to compare those models. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, since they have different numbers of parameters, an unadorned residual sum of squares isn't exactly a fair comparison either. &lt;/p&gt;&#10;&#10;&lt;p&gt;Even a more correct $R^2$ would not necessarily be a good way of comparing nonlinear models; in the nonlinear realm there's often no good reason to consider a constant-mean model, the 'null' situation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Indeed, even when comparing two &lt;em&gt;linear&lt;/em&gt; models, $R^2$ is not necessarily the best way to do it, for example, for similar reasons that the (to my mind) marginally more sensible comparison of residual sum of squares I mentioned earlier should be avoided with models with different numbers of parameters.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;I thought that the squared correlation between observed Y and fitted Y is in fact the standard $R^2$ in OLS. &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Well, yes, it's true that $R^2$ for a simple ordinary-least-squares model, is the square of the correlation between observed and predicted, but the ability to interpret it the way you want to interpret it is conditioned on it &lt;em&gt;being the result of OLS&lt;/em&gt;. If you assert parameter values, for example, &lt;em&gt;you don't change the correlation&lt;/em&gt;, but you lose its interpretation as a measure of fit at all.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Coding your example seems to confirm this and also shows that the homeopathy salesman has a lower R2. &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;The computation in R isn't accurate for the salesman because of round-off error and accumulated numerical error; this is a mathematically exact relationship that we have to take care over doing numerically.&lt;/p&gt;&#10;&#10;&lt;p&gt;Observe what happens:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x &amp;lt;- runif(100,0,10) &#10;y &amp;lt;- 9 + 2*x + rnorm(100,0,.005) &#10;cor(y,x)&#10;cor(y,9+2*x)&#10;cor(y,-1000+ 7.3e-4*x)&#10;cor(y,-1000+ 7.3e-7*x)&#10;cor(y,-1000+ 7.3e-10*x)&#10;cor(y,-1000+ 7.3e-14*x)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="11" CreationDate="2013-05-30T15:22:04.133" Id="60449" LastActivityDate="2013-05-30T17:41:53.187" LastEditDate="2013-05-30T17:41:53.187" LastEditorUserId="805" OwnerUserId="805" ParentId="60403" PostTypeId="2" Score="5" />
  
  
  <row Body="&lt;p&gt;Consider the following setup. We have a $p$-dimensional parameter vector $\theta$ that specifies the model completely and a maximum-likelihood estimator $\hat{\theta}$. The Fisher information in $\theta$ is denoted $I(\theta)$. &#10;What is usually referred to as the &lt;em&gt;Wald statistic&lt;/em&gt; is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$(\hat{\theta} - \theta)^T I(\hat{\theta}) (\hat{\theta} - \theta)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $I(\hat{\theta})$ is the Fisher information evaluated in the maximum-likelihood estimator. Under regularity conditions the Wald statistic follows &#10;asymptotically a $\chi^2$-distribution with $p$-degrees of freedom when $\theta$ is the true parameter. The Wald statistic can be used to test a simple hypothesis $H_0 : \theta = \theta_0$ on the entire parameter vector. &lt;/p&gt;&#10;&#10;&lt;p&gt;With $\Sigma(\theta) = I(\theta)^{-1}$ the inverse Fisher information the Wald test statistic of the hypothesis $H_0 : \theta_1 = \theta_{0,1}$ is &#10;$$\frac{(\hat{\theta}_1 - \theta_{0,1})^2}{\Sigma(\hat{\theta})_{ii}}.$$&#10;Its asymptotic distribution is a $\chi^2$-distribution with 1 degrees of freedom.&lt;/p&gt;&#10;&#10;&lt;p&gt;For the normal model where $\theta = (\mu, \sigma^2)$ is the vector of the mean and the variance parameters, the Wald test statistic of testing if $\mu = \mu_0$ is &#10;$$\frac{n(\hat{\mu} - \mu_0)^2}{\hat{\sigma}^2}$$&#10;with $n$ the sample size.&#10;Here $\hat{\sigma}^2$ is the maximum-likelihood estimator of $\sigma^2$ (where you divide by $n$). The $t$-test statistic is &#10;$$\frac{n(\hat{\mu} - \mu_0)}{s}$$&#10;where $s^2$ is the unbiased estimator of the variance (where you divide by the $n-1$). The Wald test statistic is almost but not exactly equal to the square of the $t$-test statistic, but they are asymptotically equivalent when $n \to \infty$. The squared $t$-test statistic has an exact $F(1, n-1)$-distribution, which converges to the $\chi^2$-distribution with 1 degrees of freedom for $n \to \infty$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The same story holds regarding the $F$-test in one-way ANOVA. &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-05-30T17:24:19.303" Id="60459" LastActivityDate="2013-05-30T17:24:19.303" OwnerUserId="4376" ParentId="60438" PostTypeId="2" Score="8" />
  
  
  <row Body="&lt;p&gt;@NRH gave a good theoretical answer, here is one that intends to be simpler, more intuitive.&lt;/p&gt;&#10;&#10;&lt;p&gt;There is the formal Wald test (described in the answer by NRH), but we also refer to tests that look at the difference between an estimated parameter and its hypothesized value relative to the variation estimated at the estimated parameter as a Wald style test.  So the t-test as we usually use it is a Wald Style test even if it is slightly different from the exact Wald test (a difference of $n$ vs. $n-1$ inside a square root).  We could even design a Wald style test based on an estimated median minus the hypothesized median divided by a function of the IQR, but I don't know what distribution it would follow, it would be better to use a bootstrap, permutation, or simulated distribution for this test rather than depending on chi-square asymptotics.  The F-test for ANOVA fits the general pattern as well, the numerator can be thought of as measuring the difference of the means from an overall mean and the denominator is a measure of the variation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also note that if you Square a random variable that follows a t distribution then it will follow an F distribution with 1 df for the numerator and the denominator df will be those from the t distribution.  Also note that an F distribution with infinite denominator df is a chi-square distribution.  So that means that both the t-statistic (squared) and the F statistic are asymptotically chi-squared just like the Wald statistic.  We just use the more exact distribution in practice. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-30T21:03:54.387" Id="60473" LastActivityDate="2013-05-30T21:03:54.387" OwnerUserId="4505" ParentId="60438" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;They are not &quot;mean&quot; squares if you take &quot;mean&quot; to only be the sum divided by the number of things summed.  But there are different kinds of &quot;mean&quot;s and for mean squares we divide the sum by the number of &lt;em&gt;independent&lt;/em&gt; things summed.&lt;/p&gt;&#10;&#10;&lt;p&gt;We compute the squares (that we will sum) by subtracting a mean from the observations, then squaring.  If we knew the true population mean(s) and subtracted those true means from the observations then the resulting squares would be independent and it would be appropriate to divide by $k$ and $N$.  But if we knew the true means then we would not need to compute mean squares anyways.  Instead we are subtracting a mean that was computed from the same observations that we are subtracting it from, so the squares are not fully independent.  The degrees of freedom represent the amount of independent information available in the squares being summed, that is why we divide by the d.f.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-30T21:46:54.740" Id="60479" LastActivityDate="2013-05-30T21:46:54.740" OwnerUserId="4505" ParentId="60444" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;This comes from the fact that $\newcommand{\Var}{\operatorname{Var}}\newcommand{\Cov}{\operatorname{Cov}}\Var(X+Y) = \Var(X) + \Var(Y) + \Cov(X,Y)$ and for a constant $a$, $\Var( a X ) = a^2 \Var(X)$.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Since we are assuming that the individual observations are independent the $\Cov(X,Y)$ term is $0$ and since we assume that the observations are identically distributed all the variances are $\sigma^2$.  So &lt;/p&gt;&#10;&#10;&lt;p&gt;$\Var( \frac{1}{n} \sum X_i ) = \frac{1}{n^2} \sum \Var(X_i) = \frac{1}{n^2} \times \sum \sigma^2= \frac{n}{n^2} \sigma^2 = \frac{\sigma^2}{n}$&lt;/p&gt;&#10;&#10;&lt;p&gt;And when we take the square root of that (because it is harder to think on the variance scale) we get $\frac{\sigma}{\sqrt{n}}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;More intuitively, think of 2 statistics classes: in the first the teacher assigns each of the students to draw a sample of size 10 from a set of tiles with numbers on them (the teacher knows the true mean of this population, but the students don't) and compute the mean of their sample.  The second teacher assigns each of his/her students to take samples of size 100 from the same set of tiles and compute the mean.  Would you expect every sample mean to exactly match the population mean? or to vary about it?  Would you expect the spread of the sample means to be the same in both classes? or would the 2nd class tend to be closer to the population?  That's why it makes sense to divide by a function of the sample size.  The square root means we have a law of diminishing returns, to halve the standard error you need to quadruple the sample size.&lt;/p&gt;&#10;&#10;&lt;p&gt;As for the name, the full name is &quot;The estimated standard deviation of the sampling distribution of x-bar&quot;; it only takes saying that a few times before you appreciate having a shortened form.  I don't know who first substituted &quot;error&quot; for &quot;deviation&quot; this way, but it stuck.  The standard deviation measures variability of individual observations; the standard error measures variability in estimates of parameters (based on observations).&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-05-31T00:03:00.853" Id="60486" LastActivityDate="2013-05-31T21:27:30.393" LastEditDate="2013-05-31T21:27:30.393" LastEditorUserId="17230" OwnerUserId="4505" ParentId="60484" PostTypeId="2" Score="8" />
  
  
  <row Body="&lt;p&gt;Your &lt;code&gt;qx1&lt;/code&gt; is just the same as &lt;code&gt;(x1-mu1)/sigma1&lt;/code&gt;, as you can check, and the same for &lt;code&gt;qx2&lt;/code&gt;. The &lt;code&gt;dmvnorm&lt;/code&gt; function evaluated at $(x,y)$ calculates &#10;$$\frac{1}{2\pi\sqrt{1-\rho^2}\sigma_1\sigma_2}\exp\left( \frac{-1}{2(1-\rho^2)}\left[\frac{(x-\mu_1)^2}{\sigma_1^2} + \frac{(y-\mu_2)^2}{\sigma_2^2} - \frac{2\rho (x-\mu_1)(y-\mu_2)}{\sigma_1\sigma_2}\right]\right)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;but your function calculates (the log of)&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac{1}{2\pi\sqrt{1-\rho^2}\sigma_1\sigma_2}\exp\left( \frac{-1}{2(1-\rho^2)}\left[\frac{(\frac{(x-\mu_1)}{\sigma_1}-\mu_1)^2}{\sigma_1^2} + \frac{(\frac{(y-\mu_2)}{\sigma_2}-\mu_2)^2}{\sigma_2^2} - \frac{2\rho (\frac{x-\mu_1}{\sigma_1}-\mu_1)(\frac{y-\mu_2}{\sigma_2}-\mu_2)}{\sigma_1\sigma_2}\right]\right)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;times&#10;$$\frac{1}{\sqrt{2\pi}}\exp^{(\frac{x-\mu_1}{\sigma_1})^2} \frac{1}{\sqrt{2\pi}}\exp^{(\frac{y-\mu_2}{\sigma_2})^2} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;which is not the same. I think your intention was to define&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;    loglike_fun &amp;lt;- function(x1, x2, mu1, mu2, sigma1, sigma2, rho) &#10;sum(dmvnorm(cbind(x1,x2), c(0,0), matrix(c(1, rho, rho, 1), ncol=2), log=T))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and then it does what you want.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x &amp;lt;- rmvnorm(100000, c(mu, mu), matrix(c(sigma, rho, rho, sigma),ncol=2))&#10;&amp;gt; loglike_fun(x[,1], x[,2], mu1 = 0, mu2 = 0, sigma1 = 1, sigma2 = 1, rho = 0.75)&#10;[1] -242503.6&#10;&amp;gt; loglike_fun(x[,1], x[,2],mu1 = 0, mu2 = 0, sigma1 = 1.7, sigma2 = 1.7, rho = 0.9)&#10;[1] -271719.2&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Basically, you accidentally &quot;standardized&quot; &lt;code&gt;x1&lt;/code&gt; and &lt;code&gt;x2&lt;/code&gt; twice.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-05-31T01:24:41.440" Id="60491" LastActivityDate="2013-05-31T01:24:41.440" OwnerUserId="13818" ParentId="59932" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="60504" AnswerCount="2" Body="&lt;p&gt;I want to assume that the sea surface temperature of the Baltic Sea is the same year after year, and then describe that with a function / linear model. The idea I had was to just input year as a decimal number (or num_months/12) and get out what the temperature should be about that time. Throwing it into lm() function in R, it doesn't recognize sinusoidal data so it just produces a straight line. So I put the sin() function within a I() bracket and tried a few values to manually fit the function, and that gets close to what I want. But the sea is warming up faster in the summer and then cooling off slower in the fall... So the model is wrong the first year, then gets more correct after a couple of years, and then in the future I guess it becomes more and more wrong again. &lt;/p&gt;&#10;&#10;&lt;p&gt;How can I get R to estimate the model for me, so I don't have to guess numbers myself? The key here is that I want it to produce the same values year after year, not just be correct for one year. If I knew more about math, maybe I could guesstimate it as something like a Poisson or Gaussian instead of sin(), but I don't know how to do that either. Any help to get closer to a good answer would be greatly appreciated.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is the data I use, and the code to show results so far:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# SST from Bradtke et al 2010&#10;ToY &amp;lt;- c(1/12,2/12,3/12,4/12,5/12,6/12,7/12,8/12,9/12,10/12,11/12,12/12,13/12,14/12,15/12,16/12,17/12,18/12,19/12,20/12,21/12,22/12,23/12,24/12,25/12,26/12,27/12,28/12,29/12,30/12,31/12,32/12,33/12,34/12,35/12,36/12,37/12,38/12,39/12,40/12,41/12,42/12,43/12,44/12,45/12,46/12,47/12,48/12)&#10;Degrees &amp;lt;- c(3,2,2.2,4,7.6,13,16,16.1,14,10.1,7,4.5,3,2,2.2,4,7.6,13,16,16.1,14,10.1,7,4.5,3,2,2.2,4,7.6,13,16,16.1,14,10.1,7,4.5,3,2,2.2,4,7.6,13,16,16.1,14,10.1,7,4.5)&#10;SST &amp;lt;- data.frame(ToY, Degrees)&#10;SSTlm &amp;lt;- lm(SST$Degrees ~ I(sin(pi*2.07*SST$ToY)))&#10;summary(SSTlm)&#10;plot(SST,xlim=c(0,4),ylim=c(0,17))&#10;par(new=T)&#10;plot(data.frame(ToY=SST$ToY,Degrees=8.4418-6.9431*sin(2.07*pi*SST$ToY)),type=&quot;l&quot;,xlim=c(0,4),ylim=c(0,17))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2013-05-31T06:17:29.223" FavoriteCount="9" Id="60500" LastActivityDate="2013-06-14T09:11:03.663" OwnerUserId="26330" PostTypeId="1" Score="13" Tags="&lt;r&gt;&lt;regression&gt;&lt;time-series&gt;&lt;lm&gt;" Title="How to find a good fit for semi-sinusoidal model in R?" ViewCount="4536" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Given an (infinite) data population from which you repeatedly draw samples of a fixed size. On each sample you learn a classifier which you then evaluate by computing the prediction error on a large independent test set. &#10;The prediction error is defined as the average over all instances in the test set of the zero-one loss function $\mathcal{L}(y,\hat{y})$. (The zero-one loss function $\mathcal{L}(y,\hat{y})=0$ if the predicted label $\hat{y}$ of an instance equals the true label of the instance y, and is 1 otherwise.)&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is whether the central limit theorem holds in this situation, so that the distribution of the prediction error over the samples approximates a normal distribution, given a sufficient number of samples?&lt;/p&gt;&#10;&#10;&lt;p&gt;I know that the theorem holds for a sufficiently large number of &lt;em&gt;independent&lt;/em&gt; random variables. So I am in doubt because the individual predictions for one test set originate from the same classifier, and the classifiers vary over the samples. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-31T10:43:03.227" Id="60510" LastActivityDate="2013-05-31T10:43:03.227" OwnerUserId="25031" PostTypeId="1" Score="1" Tags="&lt;classification&gt;&lt;normal-distribution&gt;&lt;independence&gt;&lt;central-limit-theorem&gt;" Title="Does the central limit theorem hold for the prediction error over different samples?" ViewCount="49" />
  <row AcceptedAnswerId="60519" AnswerCount="1" Body="&lt;p&gt;When running an OLS regression with a squared term,&#10;$$&#10;y = a + b_1(X) + b_2(X^2) + e&#10;$$&#10;I know that the partial effect of $X$ is $b_1 + 2b_2(\bar X)$ to get the overall effect of $X$ on $Y$ evaluated at the mean. Using&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;reg y x c.x#c.x&#10;margins, dydx(*) atmeans&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;does this in Stata and it also includes a 95% CI for the overall effect. My question now is: how does &lt;code&gt;margins&lt;/code&gt; calculate the standard error and the 95% CI of the overall effect of $X$ at the mean?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-05-31T11:45:45.470" Id="60515" LastActivityDate="2013-05-31T12:49:33.680" LastEditDate="2013-05-31T12:16:48.370" LastEditorUserId="7290" OwnerUserId="26338" PostTypeId="1" Score="3" Tags="&lt;confidence-interval&gt;&lt;stata&gt;" Title="Confidence Interval for squared term evaluated at the mean in Stata" ViewCount="3117" />
  
  <row Body="&lt;p&gt;I've never been a fan of Type III SS for ANOVA's so this is a biased recommendation.&lt;/p&gt;&#10;&#10;&lt;p&gt;I believe you should select Type II in this case.  In the Type I ANOVA order matters.  So, whether you include iV1, or iV2 first makes a difference because the first (e.g. iV1) is compared to a model with just an intercept while the second is compared to a model with an intercept and the first.  Try out switching the order they're in and you'll see the difference in main effect outcomes because your predictors are correlated.&lt;/p&gt;&#10;&#10;&lt;p&gt;The Type III gets around this by assessing each predictor, including the interaction against a model including everything but that predictor.  That sounds like a good idea until you try to consider what an interaction is without one of the main effects included.  You're comparing the predictor to what is essentially a nonsensical model (I think one of your references sort of goes into this).  (Furthermore, my recollection is that Type III is especially sensitive to missing cells (thus Type IV I believe).)&lt;/p&gt;&#10;&#10;&lt;p&gt;Type II gets around the order issue in Type I and compares sensible models (unlike Type III).  Main effects are tested with all other main effects in the model but not the interaction.  Thus each main effect is easily interpreted as the unique contribution of that predictor.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Note, all of the SS types discussed would get the same interaction effect in your case because it's assessed with all of the main effects included in each case.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-05-31T14:08:05.710" Id="60531" LastActivityDate="2013-05-31T14:08:05.710" OwnerUserId="601" ParentId="60362" PostTypeId="2" Score="4" />
  <row AnswerCount="0" Body="&lt;p&gt;Here's my situation: Binary classification and I've got a training set of roughly 250k samples and 10 features, and a validation set of roughly 100k with the same number of features. I'm fitting GBDT to the data with subsampling, so BGBDT I guess? &lt;/p&gt;&#10;&#10;&lt;p&gt;Anyhow I took the training set and split it 80/20 and did a grid-search over the parameter space (using cross-validation) on the 80% and then fit a new model to the 20% using the paramaters obtained from the grid-search and used log loss to determine the error. The 20% are held-out and are trained separately from the 80%. Ok fine. I'm getting good results wrt log loss on the held-out model for the 20%, and I'm monitoring the out of bag estimate of the deviance during the tree building process which is decreasing throughout the run.&lt;/p&gt;&#10;&#10;&lt;p&gt;Overall I'm training roughly 150 trees, subsampling at 0.25, with a learning rate of 0.1, and maximum depth of 10. The results from training the held-out samples this way give good results... if there was a problem with overfitting I would expect that the held-out samples wouldn't perform well. But when I retrain the model on the entire training set and then predict on the validation set my results are very poor. One thing to note, the training and validation sets are uniformly sampled from the same distribution and the classes are balanced. &lt;/p&gt;&#10;&#10;&lt;p&gt;Suggestions? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-05-31T15:20:19.023" Id="60536" LastActivityDate="2013-05-31T15:40:13.053" LastEditDate="2013-05-31T15:40:13.053" LastEditorUserId="26348" OwnerUserId="26348" PostTypeId="1" Score="1" Tags="&lt;cart&gt;&lt;regularization&gt;&lt;boosting&gt;" Title="GBDT and model building: How am I overfitting?" ViewCount="165" />
  
  
  <row AcceptedAnswerId="60598" AnswerCount="3" Body="&lt;p&gt;I have a regularized logistic regression model using scikit-learn and would like to share it with others, however the data it is trained on is confidential and must remain protected. The model uses bag-of-words style features to automatically classify texts describing injuries, and would be useful for a broad variety of injury surveillance tasks.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is it possible to share a fully trained model like this without revealing potentially protected information, such as the words that occur in the texts it is trained on? If so, how would I do this, and how strong would the confidentiality protection be?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-06-01T04:19:24.697" Id="60580" LastActivityDate="2013-06-01T18:16:12.173" OwnerUserId="11770" PostTypeId="1" Score="5" Tags="&lt;machine-learning&gt;&lt;logistic&gt;&lt;scikit-learn&gt;" Title="Sharing a model trained on confidential data" ViewCount="111" />
  <row Body="&lt;p&gt;The source of the quotation would do no harm, but a guess at what is meant is possible. &lt;/p&gt;&#10;&#10;&lt;p&gt;We might now say &lt;em&gt;binned&lt;/em&gt; or &lt;em&gt;classed&lt;/em&gt;, the second term being older than the first, but the first seemingly becoming more common than the first. &lt;/p&gt;&#10;&#10;&lt;p&gt;Something like a distribution with classes starting at -3, -2, -1, 0, 1, 2, 3, or -0.5, -0.4, ..., 0.4, 0.5, or whatever the values were, seems implied. &lt;/p&gt;&#10;&#10;&lt;p&gt;Reporting entire data sets was uncommon in the 18th century and still rare now except for very small data sets. &lt;/p&gt;&#10;&#10;&lt;p&gt;In short, groups here means bins or classes. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-01T09:05:16.563" Id="60586" LastActivityDate="2013-06-01T09:05:16.563" OwnerUserId="22047" ParentId="60576" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;Based on only the context we can get from the URL name, they are most likely using 15 SVMs, each of which tries to classify one class &quot;versus&quot; the rest (one-vs-all). Thus you need 15, and thus 15x15 confusion matrix. &lt;/p&gt;&#10;&#10;&lt;p&gt;You make a good point - more than one of these might declare a new point as their own. What then? A simple way is to use the hyperplane that is output as the model for the SVM and compare the distance to the plane in all +1 class labelings and accept the class for which the distance from the point to the plane is the greatest (ie, was most confidently classified). &lt;/p&gt;&#10;&#10;&lt;p&gt;The off-trace (incorrect) labelings the classifier makes are still done in the same way, ie, &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;C_ij = count(we declared i, but was actually j)&lt;/code&gt; &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-01T16:22:59.187" Id="60599" LastActivityDate="2013-06-01T16:22:59.187" OwnerUserId="7362" ParentId="53093" PostTypeId="2" Score="1" />
  
  
  
  <row Body="&lt;p&gt;The short answer to your question is that rank$(S) \le n - 1$. So if $p &amp;gt; n$, then $S$ is singular.&lt;/p&gt;&#10;&#10;&lt;p&gt;For a more detailed answer, recall that &lt;a href=&quot;https://en.wikipedia.org/wiki/Sample_mean_and_sample_covariance#Sample_covariance&quot; rel=&quot;nofollow&quot;&gt;the (unbiased) sample covariance matrix&lt;/a&gt; can be written as&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;S = \sum_{i=1}^n (x_i - \bar{x})(x_i - \bar{x})^T.&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Effectively, we are summing $n$ matrices, each having a rank of 1. Assuming the observations are linearly independent, in some sense each observation $x_i$ contributes 1 to rank$(S)$, and a 1 is subtracted from the rank (if $p &amp;gt; n$) because we center each observation by $\bar{x}$. However, if &lt;a href=&quot;http://en.wikipedia.org/wiki/Multicollinearity&quot; rel=&quot;nofollow&quot;&gt;multicollinearity&lt;/a&gt; is present in the observations, then rank$(S)$ may be reduced, which explains why the rank might be less than $n - 1$.&lt;/p&gt;&#10;&#10;&lt;p&gt;A large amount of work has gone into studying this problem. For instance, a colleague of mine and I wrote &lt;a href=&quot;http://www.tandfonline.com/doi/abs/10.1080/00949655.2011.625946#.UaqtJGRKn2U&quot; rel=&quot;nofollow&quot;&gt;a paper&lt;/a&gt; on this same topic, where we were interested in determining how to proceed if $S$ is singular when applied to &lt;a href=&quot;http://en.wikipedia.org/wiki/Linear_discriminant_analysis&quot; rel=&quot;nofollow&quot;&gt;linear discriminant analysis&lt;/a&gt; in the $p &amp;gt;&amp;gt; n$ setting.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-06-02T02:39:26.560" Id="60629" LastActivityDate="2013-06-02T02:39:26.560" OwnerUserId="7620" ParentId="60622" PostTypeId="2" Score="2" />
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;Is it mathematically correct to say that these scores follow a &quot;truncated normal distribution&quot; rather than a traditional normal distribution? &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;The scores are not truncated normal either.&lt;/p&gt;&#10;&#10;&lt;p&gt;The variable is bounded, but I'm not sure truncation is really the right term for this situation, since it's not like the observations that would otherwise score above the highest mark are &lt;em&gt;lost&lt;/em&gt;. If anything you might envisage it as something nearer winsorization (but perhaps, given the possibility of someone capable of scoring very highly on a much harder test nevertheless making one mistake, even that's an imperfect description).&lt;/p&gt;&#10;&#10;&lt;p&gt;Even if we had exactly the right model for what happens at the bounds, they're still not &amp;lt;&lt;em&gt;whateverhappensattheendpoints&lt;/em&gt;&gt;-ized normal either. (I'll say &quot;truncated&quot; for this, to indicate that I'm talking about the thing you're calling truncation.) &lt;/p&gt;&#10;&#10;&lt;p&gt;They're discrete, for example. But they're also not the same as a normal that has been &quot;truncated&quot; and discretized. &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Does this truncation preclude one from being able to use a t-test to compare two groups?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Not necessarily. In many cases, the t-test will have characteristics that are not badly affected by the non-normality.&lt;/p&gt;&#10;&#10;&lt;p&gt;If the data look &lt;em&gt;approximately&lt;/em&gt; normal, you're probably fine. The less normal they look, the larger the sample size you need before the test has close to the anticipated properties.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-02T02:46:53.687" Id="60630" LastActivityDate="2013-06-02T07:27:00.820" LastEditDate="2013-06-02T07:27:00.820" LastEditorUserId="805" OwnerUserId="805" ParentId="60627" PostTypeId="2" Score="4" />
  <row AnswerCount="0" Body="&lt;p&gt;I am very new to stats, so I apologize for this naive question.&lt;/p&gt;&#10;&#10;&lt;p&gt;Say I have the following data&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; Yi      X1 X2 X3&#10;(5,17)    A  C  E  &#10;(8,10)    A  C  F&#10;(1,2)     A  D  E&#10;(2,4)     A  D  F&#10;(6,18)    B  C  E&#10;(9,11)    B  C  F&#10;(2,3)     B  D  E&#10;(5,9)     B  D  F&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;where the $Y_i$'s are binomial response variables ($y_1=(5,17)$ means 5 heads during 17 coin tosses) and $X_1$, $X_2$, and $X_3$ are binary explanatory variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am thinking about modelling this data with a GLM utilizing the logit link function. The goal is, of course, to understand the effect of $X_1$, $X_2$, and $X_3$ on $Y$.&lt;/p&gt;&#10;&#10;&lt;p&gt;What I would like to know is whether the fact that each value of &lt;code&gt;y&lt;/code&gt; is completely specified by its associated factors (&lt;code&gt;x1&lt;/code&gt;, &lt;code&gt;x2&lt;/code&gt;, &lt;code&gt;x3&lt;/code&gt;) would make the fit &amp;amp; subsequent inference problematic? &lt;/p&gt;&#10;&#10;&lt;p&gt;Based on what I've read, to fit such model, one needs to create one dummy variable for each level of each explanatory variable, effectively making the number of variables the same as the number of observed responses $y_i$.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2013-06-02T05:11:43.557" Id="60637" LastActivityDate="2013-06-02T23:12:01.833" LastEditDate="2013-06-02T23:12:01.833" LastEditorUserId="805" OwnerUserId="26390" PostTypeId="1" Score="1" Tags="&lt;generalized-linear-model&gt;&lt;binary-data&gt;" Title="generalized linear models, explanatory variables completely determine response" ViewCount="84" />
  <row AnswerCount="2" Body="&lt;p&gt;I'm wondering if I can call the following procedure “cross validation”.&lt;/p&gt;&#10;&#10;&lt;p&gt;I extracted &lt;em&gt;k&lt;/em&gt; independent sets of data with comparable size from the same population. One of them was used for model development / training and all other sets were used to validate this particular model. So there is no “rotation” where each of the &lt;em&gt;k&lt;/em&gt; data sets take turns to act as the training set. So is this a “cross-validation”?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-02T05:26:18.307" Id="60638" LastActivityDate="2013-10-01T00:24:36.137" LastEditDate="2013-07-02T14:43:53.327" LastEditorUserId="6029" OwnerUserId="22109" PostTypeId="1" Score="1" Tags="&lt;cross-validation&gt;&lt;terminology&gt;" Title="Cross-validation with multiple validation samples" ViewCount="77" />
  <row AnswerCount="2" Body="&lt;p&gt;I would like your opinion on how to analyze a questionnaire on risk assessment in construction projects. For each question, concerning a specific risk for the project, there are two answers: An estimate for the probability of occurrence for the risk (on the scale from 1 to 5) and one for the estimated impact of the risk to the project (also in the range 1-5).&lt;/p&gt;&#10;&#10;&lt;p&gt;A common way to combine these two values is by multiplying them. Then, according to a chosen (by the organization) &quot;Probability/Impact&quot; matrix, I could determine which combinations of probability and impact result in a risk’s being classified as &quot;high risk&quot;, &quot;moderate risk&quot;, or &quot;low risk&quot;. I could then use these qualitative responses to analyze my data. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, I do not want to lose the information about a risk's probability because a risk classified as &quot;high&quot; should be treated differently, if this arises from high probability instead of high impact. The impact cannot be changed but the probability might be reduced by taking appropriate measures. What would the form of the data matrix be, if I want to keep a qualitative and a quantitative response for each question?. What methods do you suggest for the analysis?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-02T07:17:20.967" Id="60640" LastActivityDate="2014-01-30T06:41:58.623" LastEditDate="2013-06-02T15:22:29.127" LastEditorUserId="183" OwnerUserId="339" PostTypeId="1" Score="3" Tags="&lt;multivariate-analysis&gt;&lt;data-transformation&gt;&lt;survey&gt;" Title="How to analyse a risk assessment questionnaire where each risk is rated for both probability and impact?" ViewCount="153" />
  <row Body="&lt;p&gt;Only the nature of your data and your question of interest can tell you which of these regressions are best for your situation. So there are no tests that will tell you which one of these methods is the best for you. (Click on the links of the regression methods below to see some worked examples in SPSS.)&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;If you have a &lt;strong&gt;binary outcome&lt;/strong&gt; (e.g. death/alive, sick/healthy, 1/0),&#10;then &lt;a href=&quot;http://core.ecu.edu/psyc/wuenschk/MV/Multreg/Logistic-SPSS.pdf&quot; rel=&quot;nofollow&quot;&gt;logistic regression&lt;/a&gt; is appropriate.&lt;/li&gt;&#10;&lt;li&gt;If your outcomes are discrete &lt;strong&gt;counts,&lt;/strong&gt; then &lt;a href=&quot;http://www.ats.ucla.edu/stat/spss/dae/poissonreg.htm&quot; rel=&quot;nofollow&quot;&gt;Poisson regression&lt;/a&gt; or &lt;a href=&quot;http://www.ats.ucla.edu/stat/spss/dae/neg_binom.htm&quot; rel=&quot;nofollow&quot;&gt;negative binomial regression&lt;/a&gt; can be used.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Remember that the &lt;a href=&quot;http://en.wikipedia.org/wiki/Poisson_distribution&quot; rel=&quot;nofollow&quot;&gt;Poisson distribution&lt;/a&gt; assumes that the mean and variance are the same. Sometimes, your data show extra variation that is greater than the mean. This situation is called &lt;a href=&quot;http://en.wikipedia.org/wiki/Overdispersion&quot; rel=&quot;nofollow&quot;&gt;overdispersion&lt;/a&gt; and negative binomial regression is more flexible in that regard than Poisson regression (you could still use Poisson regression in that case but the standard errors could be biased). The &lt;a href=&quot;http://en.wikipedia.org/wiki/Negative_binomial_distribution#Overdispersed_Poisson&quot; rel=&quot;nofollow&quot;&gt;negative binomial distribution&lt;/a&gt; has one parameter more than the Poisson regression that adjusts the variance independently from the mean. In fact, the Poisson distribution is a special case of the negative binomial distribution.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-02T09:44:08.410" Id="60644" LastActivityDate="2013-06-02T15:38:41.857" LastEditDate="2013-06-02T15:38:41.857" LastEditorUserId="22047" OwnerUserId="21054" ParentId="60643" PostTypeId="2" Score="6" />
  
  <row Body="&lt;p&gt;With N = 65 you should certainly not have 11 independent variables. Even if the sample is perfectly evenly divided (32 and 33) then the rule of thumb would indicate no more than 3 independent variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;What should you do? You shouldn't report these results as is. You should run simpler models unless you can get more data. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-06-02T13:13:27.493" Id="60657" LastActivityDate="2013-06-02T13:13:27.493" OwnerUserId="686" ParentId="60655" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;Most supervised data mining algorithms follow these three steps:  &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;The &lt;strong&gt;training set&lt;/strong&gt; is used to build the model. This contains a set of data that has preclassified target and predictor variables.&lt;/li&gt;&#10;&lt;li&gt;Typically a hold-out dataset or &lt;strong&gt;test set&lt;/strong&gt; is used to evaluate how well the model does with data outside the training set. The test set contains the preclassified results data but they are not used when the test set data is run through the model until the end, when the preclassified data are compared against the model results. The model is adjusted to minimize error on the test set.&lt;/li&gt;&#10;&lt;li&gt;Another hold-out dataset or &lt;strong&gt;validation set&lt;/strong&gt; is used to evaluate the adjusted model in step #2 where, again, the validation set data is run against the adjusted model and results compared to the unused preclassified data.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2013-06-02T14:01:40.643" Id="60662" LastActivityDate="2014-04-26T22:27:50.083" LastEditDate="2014-04-26T22:27:50.083" LastEditorUserId="17717" OwnerUserId="26402" ParentId="19048" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;A Gaussian Mixture (&lt;a href=&quot;http://www.mathworks.com/help/stats/gmdistribution.fit.html&quot; rel=&quot;nofollow&quot;&gt;1&lt;/a&gt;,&lt;a href=&quot;http://www.cs.toronto.edu/~fleet/courses/2503/fall11/Handouts/mixtureModel.pdf&quot; rel=&quot;nofollow&quot;&gt;2&lt;/a&gt;,&lt;a href=&quot;http://www.autonlab.org/tutorials/gmm.html&quot; rel=&quot;nofollow&quot;&gt;3&lt;/a&gt;) might be an acceptable, &quot;textbook&quot;, fit for your first plot.&lt;/p&gt;&#10;&#10;&lt;p&gt;The second plot looks like hyperbolic transformed material.  When I see the second plot I think $ y=\frac {1}{x}$.   Could you try transforming the second using $ y_{2} = \frac{1}{y}$ and show what that graph looks like?&lt;/p&gt;&#10;&#10;&lt;p&gt;Glens has a good point: If you are trying to do the speaking for the data, you can always accomplish that goal, you can always &quot;win&quot;, but it most often causes you to lose.  Be careful to let the data do the talking.&lt;/p&gt;&#10;&#10;&lt;p&gt;One question behind the question is: what is your motivation.  The human brain is an amazing computer able to find signal when the signal-to-noise ratio is stunningly bad.  We see patterns where they are highly obscured.  The human heart is deceitful above all things - our emotions can mislead us ten thousand times a day.  We see patterns that aren't there and we write them into the data and thereby guarantee that our &quot;solution&quot; doesn't actually work.  Being able to tell the difference between a highly obscured pattern and a pattern that isn't there takes mental clarity of vision into the emotional state.  Hard stuff.  Sorry for the philosophy lesson.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-06-02T14:31:55.143" Id="60663" LastActivityDate="2013-06-02T14:31:55.143" OwnerUserId="22452" ParentId="60660" PostTypeId="2" Score="0" />
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;is the &quot;naive&quot; method provably less accurate than others such as maximum-likelihood?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Under the assumptions of the model, then basically, yes, the naive approach is less accurate, at least in some senses, via considerations such as the &lt;a href=&quot;http://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound&quot; rel=&quot;nofollow&quot;&gt;Cramer-Rao lower bound&lt;/a&gt; and the &lt;a href=&quot;http://en.wikipedia.org/wiki/Rao%E2%80%93Blackwell_theorem&quot; rel=&quot;nofollow&quot;&gt;Rao-Blackwell theorem&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are a variety of concepts and theorems you may need to investigate to get a reasonably complete picture, but there's a number of senses in which MLEs are often 'good', at least asymptotically.&lt;/p&gt;&#10;&#10;&lt;p&gt;That said, in the case of AR models, there are much simpler approaches which are asymptotically equivalent to ML (such as conditioning on the first $p$ obervations in an AR(p)), so in that case there may not be much to choose between them.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-06-02T15:27:05.257" Id="60665" LastActivityDate="2013-06-02T15:35:42.493" LastEditDate="2013-06-02T15:35:42.493" LastEditorUserId="805" OwnerUserId="805" ParentId="60664" PostTypeId="2" Score="2" />
  <row AnswerCount="0" Body="&lt;p&gt;I was looking at CrossValidate archives as well as r-archives and crantastic...for a package that has a robust approach to generalized additive models. I found two packages &quot;robustgam&quot; and &quot;rgam&quot; but their implemented functions &#10;cover only binomial and Poisson distributions (pls correct me if I am wrong).&lt;/p&gt;&#10;&#10;&lt;p&gt;I would greatly appreciate if anyone could share with us other R-packages or robust approaches of general additive modeling that might have a better performance with  small data sets ($n&amp;lt;100$ records or 50 -100 records).&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-06-02T15:39:31.220" Id="60667" LastActivityDate="2013-06-02T15:52:46.673" LastEditDate="2013-06-02T15:52:46.673" LastEditorUserId="22047" OwnerUserId="22478" PostTypeId="1" Score="0" Tags="&lt;normal-distribution&gt;&lt;robust&gt;&lt;gam&gt;" Title="Robust GAM that covers Gaussian distribution" ViewCount="86" />
  <row AnswerCount="0" Body="&lt;p&gt;I'm trying to find a statistical test that can help me compare the means of two dependent groups. &lt;/p&gt;&#10;&#10;&lt;p&gt;My data:  &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;parameter values from white matter region in an in vivo rat brain  &lt;/li&gt;&#10;&lt;li&gt;parameter values from white matter region in the same rat brain, bur after the tissue has been fixated.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I cannot align / register the two brains to each other, so the region I have outlined in vivo is not exactly the same as the region I have outlined in the fixated tissue.&lt;/p&gt;&#10;&#10;&lt;p&gt;My wish:  &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;I would like to compare the means of the two white matter regions. &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;My suggestion:  &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;A paired t-test, but as I know that each pixel value in data set 1 does not exactly correspond to the pixel value in data set 2, the paired t-test is not exactly right to do. &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Is there a test of dependent but NOT paired data?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-06-02T16:22:15.480" Id="60668" LastActivityDate="2013-06-02T18:10:02.633" LastEditDate="2013-06-02T18:10:02.633" LastEditorUserId="7290" OwnerUserId="26405" PostTypeId="1" Score="2" Tags="&lt;hypothesis-testing&gt;&lt;statistical-significance&gt;&lt;t-test&gt;&lt;mean&gt;&lt;dependence&gt;" Title="Comparison of dependent but unpaired data sets" ViewCount="145" />
  <row AcceptedAnswerId="68148" AnswerCount="1" Body="&lt;p&gt;I'm trying to conduct a Student's t-test for a table of values while trying to follow the explanation and details found on &lt;a href=&quot;http://www.austincc.edu/biocr/1406/labm/ex1/prelab_1_11.htm&quot; rel=&quot;nofollow&quot;&gt;this website&lt;/a&gt;. I understand that if the p-value is&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&amp;lt;.01 then it's really significant&lt;/li&gt;&#10;&lt;li&gt;&gt;.05 it's not significant&lt;/li&gt;&#10;&lt;li&gt;in between then we need more data&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;But on that page they seem to accept their null hypothesis no matter what the p-value is. So I'm really not understanding now when to accept or reject the null hypothesis. &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;When you do you accept or reject the null-hypothesis?&lt;/li&gt;&#10;&lt;li&gt;Is it true that you are never supposed to accept your null hypothesis, but rather reject or fail to reject the null?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="6" CreationDate="2013-06-02T16:52:08.493" FavoriteCount="3" Id="60670" LastActivityDate="2015-02-19T14:23:24.990" LastEditDate="2013-08-23T02:42:33.503" LastEditorUserId="805" OwnerUserId="26407" PostTypeId="1" Score="6" Tags="&lt;hypothesis-testing&gt;&lt;t-test&gt;&lt;mean&gt;&lt;p-value&gt;" Title="&quot;Accept null hypothesis&quot; or &quot;fail to reject the null hypothesis&quot;?" ViewCount="18028" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I wanted to create a predictive model of mortality after patients had undergone a surgical procedure. But I also wanted to avoid doing what most researchers do by first performing univariate analysis then using the variables that are found to be significant to perform multivariate analysis using some sort of step-wise feature selection. So I used &lt;code&gt;glmnet&lt;/code&gt; to perform feature selection, and found about 20 of the initial 80 variables to be significant. I then used some of these variables (as supported by literature) to create a statistical model to predict mortality using the &lt;code&gt;glm&lt;/code&gt; function in R. I think the model does fairly well as it has a ROC of 0.8. However when I use the &lt;code&gt;summary&lt;/code&gt; function I notice that of the 15 variables that I am using, 5 of them do not have significant p-values. But if I remove these variables, in my mind the model would not make sense (since the literature supports their use), and in addition the ROC decreases to about 0.75. &lt;/p&gt;&#10;&#10;&lt;p&gt;Given this situation, how does one go about when analyzing these variables? It seems that they are useful and necessary (as they aid in the discrimination of patients who will die or live when having a procedure performed on them) but do not have a significant p-value.&lt;/p&gt;&#10;&#10;&lt;p&gt;Forgive me for being light on code, as I wanted more of a 10,000 feet overview of this rather than to get into the nitty-gritty from the get-go. As always, I appreciate the help!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-03T03:09:17.743" Id="60692" LastActivityDate="2013-06-03T06:26:47.960" OwnerUserId="9244" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;logistic&gt;&lt;modeling&gt;&lt;feature-selection&gt;&lt;glmnet&gt;" Title="Not all Features Selected by GLMNET Considered Signficant by GLM (Logistic Regression)" ViewCount="598" />
  
  <row Body="&lt;p&gt;It is straightforward: You can apply standard MI combining rules - but effects of variables which are not supported throughout imputed datasets will be less pronounced. For example, if a variable is not selected in a specific imputed dataset its estimate (incl. variance) is zero and this has to be reflected in the estimates used when using multiple imputation. You can consider bootstrapping to construct confidence intervals to incorporate model selection uncertainty, have a look at this recent publication which addresses all questions:&#10;&lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S016794731300073X&quot; rel=&quot;nofollow&quot;&gt;http://www.sciencedirect.com/science/article/pii/S016794731300073X&lt;/a&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;I would avoid using pragmatic approaches such as selecting a variable if it is selected in m/2 datasets or sth similar, because inference is not clear and more complicated than it looks at first glance.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-03T07:10:47.733" Id="60708" LastActivityDate="2013-06-03T07:10:47.733" OwnerUserId="26430" ParentId="46719" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;I believe there exists no &lt;em&gt;optimum&lt;/em&gt; initial value. As stated by user10525, the value β=0 works well and is the default choice for &lt;code&gt;glm&lt;/code&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;In order to check what's going on with &lt;code&gt;glm&lt;/code&gt; I would follow the basic steps:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Try to change the number of iterations in the Newton-Raphson algorithm in &lt;code&gt;glm&lt;/code&gt; adding &lt;code&gt;control=glm.control(maxit=Y)&lt;/code&gt;, where &lt;code&gt;Y&lt;/code&gt; is the number of desired iterations. I would even begin with &lt;code&gt;Y=1&lt;/code&gt; to check stability of the algorithm at β=0.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Adding  &lt;code&gt;start=c(a,b,c,...)&lt;/code&gt; you can change the initial value in the regression. Note that the length of &lt;code&gt;start&lt;/code&gt; must be equal to $p+1$, where $p$ denotes the number of covariates in your logistic regression. &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Analyse the stability of the regression for more choices of the above parameters; you could probably find at least a range of initial values corresponding to &quot;convergent&quot; logistic regressions for a given number of iterations.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Andrew Gelman discusses a nice example of divergence of &lt;code&gt;glm&lt;/code&gt; in presence of &quot;bad&quot; initial value choices in his blog:&#10;&lt;a href=&quot;http://andrewgelman.com/2011/05/04/whassup_with_gl/&quot; rel=&quot;nofollow&quot;&gt;http://andrewgelman.com/2011/05/04/whassup_with_gl/&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Please note that &lt;code&gt;glm&lt;/code&gt; &quot;explodes&quot; in presence of complete separation as the MLE does not exist: this is also something to check.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I hope this can be applied to your case. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-06-03T10:12:35.820" Id="60726" LastActivityDate="2013-06-03T11:34:40.243" LastEditDate="2013-06-03T11:34:40.243" LastEditorUserId="22047" OwnerUserId="26435" ParentId="33857" PostTypeId="2" Score="4" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;Suppose $X_1, X_2, \, ... \, , X_n$ are a simple random sample from a Normal$(\mu,\sigma^2)$ distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm interested in doing the following hypothesis test:&#10;$$&#10;H_0: | \mu| \le c \\&#10;H_1: |\mu| &amp;gt; c,&#10;$$&#10;for a given constant $c &amp;gt; 0$.&lt;/p&gt;&#10;&#10;&lt;p&gt;I was thinking of performing two one-sided $t$-tests (TOST) in an analogous way to the usual bioequivalence testing situation, where the null and is $|\mu| \ge c$ instead, but I don't know if this makes sense or is correct.&lt;/p&gt;&#10;&#10;&lt;p&gt;My idea is to perform the one-sided tests&#10;$$&#10;H_{01} : \mu \le c \\&#10;H_{11} : \mu  &amp;gt; c&#10;$$&#10;and&#10;$$&#10;H_{02} : \mu \ge -c \\&#10;H_{12} : \mu  &amp;lt; -c,&#10;$$&#10;and reject the global null hypothesis if one of the $p$-values&#10;is smaller than a significance level $\alpha$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance!&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I've been thinking a little while about this, and I think the approach I proposed does not have significance level $\alpha$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose that the true value of $\mu$ is $\mu_0$ and $\sigma^2$ is known.&lt;/p&gt;&#10;&#10;&lt;p&gt;The probability of rejecting the null in the first test is &#10;$$&#10;\mathbb{P}_{\mu_0}(\mathrm{Rej.H}_{01}) = 1 - \Phi \left(z_{1-\alpha} + \frac{c-\mu_0}{\sigma/\sqrt{n}} \right),&#10;$$&#10;where $\Phi$ if the standard cdf of the Normal distribution, and $z_{1-\alpha}$ is a value such that $\Phi(z_{1-\alpha}) = 1-\alpha$.&lt;/p&gt;&#10;&#10;&lt;p&gt;If $\mu_0 = c$, $\mathbb{P}_{\mu_0}(\mathrm{Rej.H}_{01}) = \alpha$. Then, if $\mu_0 &amp;gt; c$, $\mathbb{P}_{\mu_0}(\mathrm{Rej.H}_{01}) &amp;gt; \alpha$. Alternatively, if $\mu_0 &amp;lt; c$, $\mathbb{P}_{\mu_0}(\mathrm{Rej.H}_{01}) &amp;lt;\alpha$. &lt;/p&gt;&#10;&#10;&lt;p&gt;The probability of rejecting the null in the second test is&#10;$$&#10;\mathbb{P}_{\mu_0}(\mathrm{Rej.H}_{02}) = \Phi \left(- z_{1-\alpha} - \frac{\mu_0 + c}{\sigma/\sqrt{n}} \right).&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Again, if $\mu_0 = -c$ we have $\mathbb{P}_{\mu_0}(\mathrm{Rej.H}_{02}) = \alpha$. Similarly, if $\mu_0 &amp;gt; -c$, $\mathbb{P}_{\mu_0}(\mathrm{Rej.H}_{02}) &amp;lt; \alpha$. Finally, if $\mu_0 &amp;lt; -c$, $\mathbb{P}_{\mu_0}(\mathrm{Rej.H}_{02}) &amp;gt; \alpha$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Since the rejection regions of the two tests are disjoint, the probability of rejecting $H_0$ is:&#10;$$&#10;\mathbb{P}_{\mu_0}(\mathrm{Rej.H}_0) = 1 - \Phi \left(z_{1-\alpha} + \frac{c-\mu_0}{\sigma/\sqrt{n}} \right) + \Phi \left(- z_{1-\alpha} - \frac{\mu_0 + c}{\sigma/\sqrt{n}} \right)&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;So, if $\mu \in [-c,c]$, $2\alpha$ is an upper bound of the probability of rejecting the (global) null hypothesis. Therefore, the approach I proposed was too liberal.&lt;/p&gt;&#10;&#10;&lt;p&gt;If I'm not wrong, we can achieve a significance level of $\alpha$ by doing the same two tests and rejecting the null if the $p$-value of one of them is less than $\alpha/2$. A similar argument holds when the variance is unknown and we need to apply the $t$-test. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-06-03T11:24:49.913" FavoriteCount="3" Id="60735" LastActivityDate="2014-05-04T15:04:13.553" LastEditDate="2014-05-04T15:04:13.553" LastEditorUserId="44269" OwnerUserId="26441" PostTypeId="1" Score="10" Tags="&lt;hypothesis-testing&gt;&lt;biostatistics&gt;&lt;equivalence&gt;&lt;tost&gt;" Title="Null hypothesis of equivalence" ViewCount="330" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am comparing two different cache hit algorithms using four different training and testing sets. The training sets are related: 1 is a subset of 2 is a subset of 3 is a subset of 4. The same goes for the test sets. The result is two different matrices with cache hit ratios between 0 and 1, where higher values are better:&lt;/p&gt;&#10;&#10;&lt;p&gt;Algorithm A&lt;/p&gt;&#10;&#10;&lt;pre&gt;                  test set 1:  test set 2:  test set 3:  test set 4:&#10;training set 1:   0..1         0..1         0..1         0..1&#10;training set 2:   0..1         0..1         0..1         0..1&#10;training set 3:   0..1         0..1         0..1         0..1&#10;training set 4:   0..1         0..1         0..1         0..1&#10;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Algorithm B:&lt;/p&gt;&#10;&#10;&lt;pre&gt;                  test set 1:  test set 2:  test set 3:  test set 4:&#10;training set 1:   0..1         0..1         0..1         0..1&#10;training set 2:   0..1         0..1         0..1         0..1&#10;training set 3:   0..1         0..1         0..1         0..1&#10;training set 4:   0..1         0..1         0..1         0..1&#10;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I now want to determine the best algorithm. How can this be done? I have limited understanding of statistical concepts. I don't know the distribution of the data.&lt;/p&gt;&#10;&#10;&lt;p&gt;I thought about using a dependent sample t-test for each pair of cells in A and B, so that B is seen as the &quot;treatment&quot; and we then decide if the &quot;treatment&quot; was benefitial or not. &lt;/p&gt;&#10;&#10;&lt;p&gt;H0: There is no significant difference between the algorithms&lt;br&gt;&#10;H1: There is a significant difference between the algorithms&lt;/p&gt;&#10;&#10;&lt;p&gt;Is this method even remotely valid? Any help much appreciated!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-03T12:40:52.210" Id="60745" LastActivityDate="2013-06-03T12:40:52.210" OwnerUserId="26445" PostTypeId="1" Score="2" Tags="&lt;hypothesis-testing&gt;&lt;statistical-significance&gt;&lt;algorithms&gt;" Title="Statistically comparing two different cache hit algorithms" ViewCount="61" />
  <row AcceptedAnswerId="60759" AnswerCount="1" Body="&lt;p&gt;I understand that Multidimensional scaling (MDS) is same as doing Principal Components analysis (PCA) if Euclidean distance is used, this is known as Metric MDS. But I came across this in a book that &quot;it has been shown (Chatfield and Collins 1980) that the eigenvalues of $XX^T$ (unnormalised outer product matrix) are equal to the eigenvalues $X^TX$ (unnormalised inner product matrix) and eigenvectors of $XX^T$ and $X^TX$ are related by a linear transformation. &quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;Note, that X denotes a matrix of data, with $n$ features (rows) and $m$ instances (columns).&lt;/p&gt;&#10;&#10;&lt;p&gt;Now I am unable to get this Chattield and Collins book anywhere, and I can understand that the eigenvalues are equal. But how are the eigenvectors of PCA and metric MDS related to each other ?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-06-03T14:09:53.250" FavoriteCount="1" Id="60753" LastActivityDate="2013-11-04T20:05:36.413" LastEditDate="2013-06-03T14:21:06.977" LastEditorUserId="26450" OwnerUserId="26450" PostTypeId="1" Score="3" Tags="&lt;pca&gt;&lt;multidimensional-scaling&gt;" Title="MDS and PCA eigenvalues and eigenvectors" ViewCount="361" />
  
  
  <row AcceptedAnswerId="60798" AnswerCount="1" Body="&lt;p&gt;I'm working with a large data set (confidential, so I can't share too much), and came to the conclusion a negative binomial regression would be necessary. I've never done a glm regression before, and I can't find any clear information about what the assumptions are. Are they the same for MLR? &lt;/p&gt;&#10;&#10;&lt;p&gt;Can I transform the variables the same way (I've already discovered transforming the dependent variable is a bad call since it needs to be a natural number)? I already determined that the negative binomial distribution would help with the over-dispersion in my data (variance is around 2000, the mean is 48).&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for the help!!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-03T16:31:51.660" FavoriteCount="10" Id="60777" LastActivityDate="2015-02-07T19:22:13.163" LastEditDate="2014-03-30T04:07:31.953" LastEditorUserId="805" OwnerDisplayName="Carly" OwnerUserId="26493" PostTypeId="1" Score="15" Tags="&lt;regression&gt;&lt;generalized-linear-model&gt;&lt;data-transformation&gt;&lt;assumptions&gt;&lt;negative-binomial&gt;" Title="What are the assumptions of negative binomial regression?" ViewCount="4063" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I would like to know more details about R issues mentioned in &lt;a href=&quot;http://www.stat.pitt.edu/stoffer/tsa3/Rissues.htm&quot; rel=&quot;nofollow&quot;&gt;Time Series Analysis and Its Applications: With R Examples&lt;/a&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;For e.g. the first problem still exists in R version 3.0.1&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# generate an AR(1) with mean 50&#10;set.seed(66)      # so you can reproduce these results&#10;x = arima.sim(list(order=c(1,0,0), ar=.9), n=100) + 50   &#10;mean(x)  &#10;  [1] 50.60668   # the sample mean is close&#10;arima(x, order = c(1, 0, 0))  &#10;  Coefficients:&#10;           ar1  intercept  &amp;lt;--  here's the problem&#10;        0.8971    50.6304  &amp;lt;--  or here, one of these has to change&#10;  s.e.  0.0409     0.8365&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;*direct copy from the webpage&lt;/p&gt;&#10;&#10;&lt;p&gt;Theoretical model of above simulation is:&#10;$X_t = 5 + 0.9 X_{t-1} + Z_t$, where $\{ Z_t\} $ is the white noise&lt;/p&gt;&#10;&#10;&lt;p&gt;Is R following particular model convention or is R wrong?  If it is wrong, it is still in R core package and I wonder why it isn't changed.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, it would be very helpful if I get some hint on other five issues.&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2013-06-04T02:01:02.713" FavoriteCount="3" Id="60803" LastActivityDate="2013-06-21T07:37:10.517" LastEditDate="2013-06-04T03:09:09.000" LastEditorUserId="26058" OwnerUserId="26058" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;time-series&gt;" Title="R Time Series Issues" ViewCount="310" />
  <row Body="&lt;p&gt;I've decided to try the following approach:&lt;/p&gt;&#10;&#10;&lt;p&gt;Each combination of 2 tags will have a weight value. Each 2-tag combination (using the tags appearing in the photo) will have a weight value calculated by the following formula:&lt;/p&gt;&#10;&#10;&lt;p&gt;(P(tag1 &amp;amp; tag2 | object exists in a photo)) ^ 2 - P(tag1 &amp;amp; tag2 | object does not exist in a photo) * P(tag1 &amp;amp; tag2)&lt;/p&gt;&#10;&#10;&lt;p&gt;The formula is somewhat experimental, but the first (squared) term represents the probability of finding the two tags in a photo containing the object, and the subtracted part has two multipliers, each representing a value that would work against the object appearing in the photo.&lt;/p&gt;&#10;&#10;&lt;p&gt;The sum of each of these weights is taken, and the result represents a &quot;rank&quot; score for the photo.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-04T04:14:45.607" Id="60808" LastActivityDate="2013-06-04T04:14:45.607" OwnerUserId="26428" ParentId="60703" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="60821" AnswerCount="2" Body="&lt;p&gt;I am having trouble interpreting the z values for categorical variables in logistic regression. In the example below I have a categorical variable with 3 classes and according to the z value, CLASS2 might be relevant while the others are not. &lt;/p&gt;&#10;&#10;&lt;p&gt;But now what does this mean?&lt;/p&gt;&#10;&#10;&lt;p&gt;That I could merge the other classes to one? &lt;br&gt;&#10;That the whole variable might not be a good predictor?&lt;/p&gt;&#10;&#10;&lt;p&gt;This is just an example and the actual z values here are not from a real problem, I just have difficulties about their interpretation.  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;           Estimate    Std. Error  z value Pr(&amp;gt;|z|)    &#10;CLASS0     6.069e-02  1.564e-01   0.388   0.6979    &#10;CLASS1     1.734e-01  2.630e-01   0.659   0.5098    &#10;CLASS2     1.597e+00  6.354e-01   2.514   0.0119 *  &#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2013-06-04T07:21:51.857" FavoriteCount="5" Id="60817" LastActivityDate="2014-11-09T15:54:12.313" LastEditDate="2013-06-04T07:59:47.607" LastEditorUserId="22047" OwnerUserId="20563" PostTypeId="1" Score="6" Tags="&lt;r&gt;&lt;logistic&gt;&lt;feature-selection&gt;" Title="Significance of categorical predictor in logistic regression" ViewCount="4217" />
  <row Body="&lt;p&gt;The $z$-value is just the test-statistic for a statistical test, so if you have trouble interpreting it your first step is to find out what the null hypothesis is. The null hypothesis for the test for CLASS0 is that its coefficient is 0. The coefficient for CLASS0 is the difference in log(odds) between CLASS0 and the reference class (CLASS3?) is zero, or equivalently, that the ratio of the odds for CLASS0 and the reference class is 1. In other words that there is no difference in the odds of success between CLASS0 and the reference class.&lt;/p&gt;&#10;&#10;&lt;p&gt;So does a non-significant coefficient mean you can merge categories? No. First, non-significant means that we cannot reject the hypothesis that there is no difference, but that does not mean that no such differences exist. An absence of evidence is not the same thing as evidence of absence. Second, merging categories, especially the reference category, changes the interpretation of all other coefficients. Whether or not that makes sense depends on what those different classes stand for. &lt;/p&gt;&#10;&#10;&lt;p&gt;Does that mean that the entire categorical variable is a &quot;bad&quot; (non-significant) predictor? No, for that you would need to perform a simultaneous test for all CLASS terms. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-04T07:51:25.150" Id="60822" LastActivityDate="2013-06-04T07:58:59.807" LastEditDate="2013-06-04T07:58:59.807" LastEditorUserId="22047" OwnerUserId="23853" ParentId="60817" PostTypeId="2" Score="5" />
  
  
  <row Body="&lt;p&gt;Ivan,&lt;/p&gt;&#10;&#10;&lt;p&gt;No.  I will point you to the work of Tsay www.unc.edu/~jbhill/tsay.pdf‎&lt;/p&gt;&#10;&#10;&lt;p&gt;You might not have removed the seasonality properly.  Did you look for seasonal pulses?  &lt;/p&gt;&#10;&#10;&lt;p&gt;As for your outliers, there are outliers, there are level shifts, there are changes in trend as well as seasonal pulses.  You need to run a tournament of models with different approaches where you might need to look for the outliers first and then seasonality.   There could also be changes in parameters and variance that need to be tested for. Consider many many options.....post one data set here and perhaps we can do a deep dive?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-04T12:07:08.147" Id="60838" LastActivityDate="2013-06-04T12:07:08.147" OwnerUserId="3411" ParentId="60716" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;I have 150 properties that may occur in a population of 10000 people. Individual people may have none, one or a couple of these properties. The properties are not mutually exclusive and have different frequencies in the population.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to answer two questions here:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Are particular sets of these properties associated, that is do they occur together in a person more often than can be expected by chance?&#10;The method should not only analyse for pair-wise co-occurrence of two properties, but also identify sets of properties that occur together.&#10;Ideally I'd like to have a probability measure like a p-value for a given set of properties telling me the likelihood of observing this combination of properties just by chance.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Given a sample of e.g. 50 out of the 10000 people I'd like to know whether the co-occurrence of properties observed in the sample is significantly different from a random sample of people from the population.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;How would I best address this in statistical / mathematical terms? Are there any tools you could recommend for the calculation?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-06-04T13:38:04.720" Id="60845" LastActivityDate="2015-02-12T00:21:36.547" LastEditDate="2013-06-04T14:05:30.567" LastEditorUserId="7290" OwnerUserId="26486" PostTypeId="1" Score="5" Tags="&lt;correlation&gt;&lt;clustering&gt;&lt;multivariate-analysis&gt;&lt;combination&gt;" Title="Co-occurrence of properties in a population" ViewCount="93" />
  <row AnswerCount="0" Body="&lt;p&gt;I recently asked a friend what an &quot;acceptable&quot; standard error is for a coefficient in conditional logistic regression, and I got the logical, &quot;it depends&quot; response.&lt;/p&gt;&#10;&#10;&lt;p&gt;Knowing that every model or study is different, what is the &quot;rule of thumb&quot; for minimum standard error for coefficients in logistic regression?&lt;/p&gt;&#10;&#10;&lt;p&gt;I was once told 0.05... but haven't been able to find similar statements on the Google machine.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-06-04T14:05:00.533" Id="60848" LastActivityDate="2013-06-04T14:05:00.533" OwnerUserId="23758" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;logistic&gt;&lt;standard-error&gt;" Title="Standard Error In Logistic Regression" ViewCount="226" />
  
  <row AnswerCount="5" Body="&lt;p&gt;What is the best way to show a relationship between:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;continuous and discrete variable,&lt;/li&gt;&#10;&lt;li&gt;two discrete variables ?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;So far I have used scatter plots to look at the relationship between continuous variables. However in case of discrete variables data points are cumulated at certain intervals. Thus  the line of best fit might be biased.  &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-06-04T15:01:41.857" FavoriteCount="5" Id="60856" LastActivityDate="2014-01-19T09:25:29.463" LastEditDate="2013-06-04T16:40:33.217" LastEditorUserId="7290" OwnerUserId="25814" PostTypeId="1" Score="11" Tags="&lt;data-visualization&gt;&lt;categorical-data&gt;&lt;random-variable&gt;" Title="What is the best way to visualize relationship between discrete and continuous variables?" ViewCount="4878" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I want to compare the &lt;strong&gt;delays&lt;/strong&gt; of packets belonging to some kind of traffic to the delays of packets belonging to some other (different and larger) traffic, all generated from the same machine.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to see if the &lt;strong&gt;distributions&lt;/strong&gt; of the two groups of delays &lt;strong&gt;are the same&lt;/strong&gt;. Samples for the first group are between &lt;strong&gt;size 20&lt;/strong&gt; and &lt;strong&gt;2 000&lt;/strong&gt; (bust most of the time between 20 and 75), while the comparison group is always of the order of &lt;strong&gt;a few thousands&lt;/strong&gt; (up to 15 000). &lt;/p&gt;&#10;&#10;&lt;p&gt;When the first group has &lt;strong&gt;more than 250&lt;/strong&gt; values, I just (randomly) &lt;strong&gt;subsample&lt;/strong&gt; it in smaller groups of size 50, since it I noticed that it really takes nothing for large samples to get rejected with Kolmogorov-Smirnov. If the &lt;strong&gt;majority&lt;/strong&gt; of subsamples gets accepted by the test, the original sample also does.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, due to the precision of timestamping, there are lots of duplicate delays (i.e. &lt;strong&gt;ties&lt;/strong&gt;) in my observations. For instance, out of &lt;strong&gt;15000&lt;/strong&gt; delays in my second group, I have &lt;strong&gt;only 1000 unique values&lt;/strong&gt;, with a mean of 0.7 milliseconds and a standard deviation of around 7 milliseconds. 200 values appear up to 10 times; 90 values between 11 and 100; 30 between 101 and 300; 4 between 301 and 500.&lt;/p&gt;&#10;&#10;&lt;p&gt;What should I take into account in order to &lt;strong&gt;choose a non-parametric test&lt;/strong&gt; that best suits my case?&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, is my subsampling correct? &lt;/p&gt;&#10;&#10;&lt;p&gt;I've been using a significance level of 0.01, but I'm getting &lt;strong&gt;20% of false positives&lt;/strong&gt;, of which roughly half greater, half smaller than the second group (after the two-sided test, I always used the two one-sided ones just to check).&lt;/p&gt;&#10;&#10;&lt;p&gt;Any ideas?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-04T15:45:55.773" Id="60862" LastActivityDate="2013-06-06T19:55:45.797" LastEditDate="2013-06-06T19:55:45.797" LastEditorUserId="12287" OwnerUserId="12287" PostTypeId="1" Score="1" Tags="&lt;distributions&gt;&lt;nonparametric&gt;&lt;chi-squared&gt;&lt;kolmogorov-smirnov&gt;" Title="Choice of a non-parametric method" ViewCount="90" />
  
  
  
  <row AnswerCount="3" Body="&lt;p&gt;Suppose we say that the prevalence rate of some disease is $1$ in $50$. Does this make sense? Or should be convert this into a $ \%$? That is, should we say that the prevalence rate is $2 \%$? &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-06-05T02:53:38.063" Id="60907" LastActivityDate="2013-06-05T17:32:16.550" LastEditDate="2013-06-05T14:21:35.163" LastEditorUserId="7290" OwnerUserId="26511" PostTypeId="1" Score="3" Tags="&lt;epidemiology&gt;" Title="What would be a suitable way to present a prevalence rate?" ViewCount="151" />
  <row Body="&lt;p&gt;This is definitely too long for a comment and suggests some potential answers, but I hope to expand on it once you address the issues.&lt;/p&gt;&#10;&#10;&lt;p&gt;...&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;a histogram $y_1,…,y_m$&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Are the categories always on a lattice, and these are actually observations from a discrete distribution on a lattice (so we have ordering and equispacing)?&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;I am looking for a statistical test which tells me whether the hypothesis is true.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;No test will tell you whether a distributional hypothesis is true.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are a variety of possible tests for the goodness of fit of a hypothesized distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there any parameter estimation involved here? &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;I am not looking for a parameter test.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Do you mean a parametric test?&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;What is the solution for the hypothesis X1,…,Xn∼Geo(p)?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Is $p$ known? What do the first few $Y$'s look like here?&lt;/p&gt;&#10;&#10;&lt;p&gt;---&lt;/p&gt;&#10;&#10;&lt;p&gt;A commonly used approach would be to perform a chi-square for this kind of goodness of fit, but this is usually not a good idea because it ignores the ordering (losing power to uninteresting alternatives). If I've understood everything correctly, I'd suggest &lt;em&gt;either&lt;/em&gt; an Anderson-Darling (but modified to allow for the discreteness - which will probably require simulation of the null distribution), or a Smooth Test (see the discussion and links &lt;a href=&quot;http://stats.stackexchange.com/a/60906/805&quot;&gt;here&lt;/a&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;For tests &lt;em&gt;other&lt;/em&gt; than Smooth Tests, see:&lt;/p&gt;&#10;&#10;&lt;p&gt;D'Agostino, R. B. and Stephens, M. A. (1986).&lt;br&gt;&#10;&lt;em&gt;Goodness-of-Fit Techniques.&lt;/em&gt;&lt;br&gt;&#10;New York: Marcel Dekker.  &lt;/p&gt;&#10;&#10;&lt;p&gt;If you have parameter estimation, things become more complex, but can be dealt with.&lt;/p&gt;&#10;&#10;&lt;p&gt;---&lt;/p&gt;&#10;&#10;&lt;p&gt;In respect of your previously mentioned Geometric distribution example, a smooth test for the geometric is covered in&lt;/p&gt;&#10;&#10;&lt;p&gt;Best, D.J. and Rayner, J.C.W. (2003).&lt;br&gt;&#10;&quot;Tests of fit for the geometric distribution.&quot;&lt;br&gt;&#10;&lt;em&gt;Communications in Statistics - Simulation and Computation&lt;/em&gt;,&lt;br&gt;&#10;32 (4), 1065-1078.  &lt;/p&gt;&#10;&#10;&lt;p&gt;It's also in Section 8.4 of &lt;/p&gt;&#10;&#10;&lt;p&gt;Rayner J. C. W., O. Thas, D. J. Best. (2009),&lt;br&gt;&#10;&lt;em&gt;Smooth Tests of Goodness of Fit: Using R&lt;/em&gt;, 2nd Edition.&lt;br&gt;&#10;ISBN: 978-0-470-82442-9&lt;br&gt;&#10;&lt;a href=&quot;http://biomath.ugent.be/~othas/smooth2/Home.html&quot; rel=&quot;nofollow&quot;&gt;http://biomath.ugent.be/~othas/smooth2/Home.html&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;R packages that do smooth tests include &lt;a href=&quot;http://cran.r-project.org/web/packages/ddst/ddst.pdf&quot; rel=&quot;nofollow&quot;&gt;ddst&lt;/a&gt; and &lt;a href=&quot;http://biomath.ugent.be/~othas/smooth2/R-Package.html&quot; rel=&quot;nofollow&quot;&gt;smoothtest&lt;/a&gt;. The package ddst doesn't do the geometric case (though it does do the exponential distribution). I haven't checked what smoothtest contains, but since it does with the book linked just above, I expect it may implement the geometric one there. I don't know whether smoothtest runs under the most recent version of R, (but older versions of R can be set up).&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-06-05T08:00:16.750" Id="60926" LastActivityDate="2013-06-05T15:48:10.967" LastEditDate="2013-06-05T15:48:10.967" LastEditorUserId="805" OwnerUserId="805" ParentId="60924" PostTypeId="2" Score="3" />
  <row AnswerCount="1" Body="&lt;p&gt;I need to compute the Somers' D for a contingency table (choosing between R|C and C|R), but I can't find any function to do this. I know there is the &lt;code&gt;somers2&lt;/code&gt; function in the &lt;code&gt;Hmisc&lt;/code&gt;package, but it only works for 2x2 tables. &lt;/p&gt;&#10;&#10;&lt;p&gt;Does anyone know how to do this? &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt;:&lt;/p&gt;&#10;&#10;&lt;p&gt;I think I've found a solution, but I'm not sure if it's right. In &lt;a href=&quot;http://stackoverflow.com/questions/2557863/measures-of-association-in-r-kendalls-tau-b-and-tau-c&quot;&gt;this post at StackOverflow&lt;/a&gt;, &lt;strong&gt;doug&lt;/strong&gt; posted a way to find the concordant and discordant pairs, where &quot;t&quot; is a table:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# number of concordant pairs &#10;P = function(t) {   &#10;  r_ndx = row(t)&#10;  c_ndx = col(t)&#10; sum(t * mapply(function(r, c){sum(t[(r_ndx &amp;gt; r) &amp;amp; (c_ndx &amp;gt; c)])},&#10;  r = r_ndx, c = c_ndx))}&#10;&#10;# number of discordant pairs&#10;Q = function(t) {&#10;  r_ndx = row(t)&#10;  c_ndx = col(t)&#10;  sum(t * mapply( function(r, c){&#10;      sum(t[(r_ndx &amp;gt; r) &amp;amp; (c_ndx &amp;lt; c)])&#10;  },&#10;    r = r_ndx, c = c_ndx) )&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And if I'm right, this code can be slightly modified to find the ties (for instance, in the rows):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;TIES = function(t) {&#10;  r_ndx = row(t)&#10;  c_ndx = col(t)&#10;  sum(t * mapply( function(r, c){&#10;      sum(t[(r_ndx == r)])&#10;  },&#10;    r = r_ndx, c = c_ndx) )&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So a function to find the Somers' D would be this one: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;somersD = function(table, dependent=&quot;rows&quot;, digits=2){&#10;&#10;# concordant pairs&#10;C = function(t) {   &#10;  r_ndx = row(t)&#10;  c_ndx = col(t)&#10;  sum(t * mapply(function(r, c){sum(t[(r_ndx &amp;gt; r) &amp;amp; (c_ndx &amp;gt; c)])},&#10;                 r = r_ndx, c = c_ndx))&#10;}&#10;&#10;# discordant pais&#10;D = function(t) {&#10;  r_ndx = row(t)&#10;  c_ndx = col(t)&#10;  sum(t * mapply( function(r, c){&#10;    sum(t[(r_ndx &amp;gt; r) &amp;amp; (c_ndx &amp;lt; c)])&#10;  },&#10;                  r = r_ndx, c = c_ndx) )&#10;}&#10;&#10;# ties in the dependent variable&#10;if (dependent==&quot;rows&quot;){&#10;  E = function(t) {&#10;    r_ndx = row(t)&#10;    c_ndx = col(t)&#10;    sum(t * mapply( function(r, c){&#10;      sum(t[(r_ndx == r)])&#10;    },&#10;                    r = r_ndx, c = c_ndx) )&#10;  }&#10;} else if (dependent==&quot;cols&quot;){&#10;  E = function(t) {&#10;    r_ndx = row(t)&#10;    c_ndx = col(t)&#10;    sum(t * mapply( function(r, c){&#10;      sum(t[(c_ndx == c)])&#10;    },&#10;                    r = r_ndx, c = c_ndx) )&#10;  }&#10;} else { warning(&quot;'dependent' argument must be 'rows' or 'cols'&quot;) }&#10;&#10;c = C(table)&#10;d = D(table)&#10;e = E(table)&#10;&#10;somers = (c-d)/(c+d+e)&#10;&#10;print(paste(&quot;Somers' D: &quot;, round(somers, digits=digits)))&#10;&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The problem is that I don't know if I'm doing the right thing. So would be appreciated if anyone could confirm or correct this!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-05T08:18:02.817" Id="60927" LastActivityDate="2013-06-09T14:04:10.000" LastEditDate="2013-06-09T14:04:10.000" LastEditorUserId="24819" OwnerUserId="24819" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;self-study&gt;&lt;contingency-tables&gt;" Title="Somers' D (contingency table) in R" ViewCount="549" />
  
  <row Body="&lt;p&gt;Started as a comment, grew too long...&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;it's not 'sampled' at regular intervals, so I think it doesn't qualify as a Time Series data &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;This is an erroneous conclusion - it's certainly time series. A time series may be irregularly sampled, it just tends to require different from the usual approaches when it is. &lt;/p&gt;&#10;&#10;&lt;p&gt;This problem appears to be related to stochastic problems like dam levels (water is generally used at a fairly stable rate over time, sometimes increasing or decreasing more or less quickly, while at other times its fairly stable), while dam levels tend to only increase rapidly (essentially in jumps), as rainfall occurs. The paper usage and replenishment patterns may be somewhat similar (though the amount ordered may tend to be much more stable and in much rounder numbers than rainfall amounts, and to occur whenever the level gets low).&lt;/p&gt;&#10;&#10;&lt;p&gt;It's also related to insurance company capital (but kind of reversed) - aside initial capital, money from premiums (net operating costs) and investments comes in fairly steadily (sometimes more or less), while insurance policy payments tend to be made in relatively large amounts.&lt;/p&gt;&#10;&#10;&lt;p&gt;Both of those things have been modelled, and may provide a little insight for this problem.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-05T08:47:39.173" Id="60931" LastActivityDate="2013-06-05T08:47:39.173" OwnerUserId="805" ParentId="18459" PostTypeId="2" Score="2" />
  
  <row AnswerCount="2" Body="&lt;p&gt;My dataset is made of a label, $y_{t}$, which is the dependent variable, and about 20 columns of independent numeric variables, $X_{t}$, $t=1,2,...,T$.&lt;/p&gt;&#10;&#10;&lt;p&gt;These samples are time series and my goal is to classify $y_{t}$ according to $X_{t}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The dependent variable can get just two labels: &quot;$0$&quot; or &quot;$1$&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Probability of belonging to &quot;$0$&quot; or &quot;$1$&quot; label is not needed, although it could bring further value to the analysis.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to know which one of the following methods is the best one for my case and why (eventually how should I set methods and parameters if needed):&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;strong&gt;Support Vector Machines&lt;/strong&gt;: which kernel should I use (linear, polynomial, radial basis, sigmoid)?&lt;/li&gt;&#10;&lt;li&gt;&lt;strong&gt;Neural Networks&lt;/strong&gt;: how many layer and nodes should I set?&lt;/li&gt;&#10;&lt;li&gt;&lt;strong&gt;Random Forests&lt;/strong&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;strong&gt;Non parametric model applied to binary outcome&lt;/strong&gt; (this provides probabilities of belonging to each class)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;What can you suggest me?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-05T10:09:57.683" FavoriteCount="1" Id="60939" LastActivityDate="2013-06-05T14:23:43.393" OwnerUserId="14674" PostTypeId="1" Score="4" Tags="&lt;time-series&gt;&lt;classification&gt;&lt;svm&gt;&lt;neural-networks&gt;&lt;random-forest&gt;" Title="Classification in time series: SVMs, Neural Networks, Random Forests or non parametric models" ViewCount="2674" />
  
  <row Body="&lt;p&gt;The figure shows the distribution of estimated slope parameters over all models, not just those which were significantly different from zero. The spike at zero represents all the models where the slope was deemed insignificant, and so a zero-slope model was used. The point is to demonstrate that the variable-selection procedure leads to estimates of $\beta$ which are either zero (and thus far too low) or extremely large (because the larger estimates are &quot;more significant&quot;).&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-06-05T11:11:07.053" Id="60944" LastActivityDate="2013-06-05T11:11:07.053" OwnerUserId="9975" ParentId="60930" PostTypeId="2" Score="3" />
  <row Body="&lt;ol&gt;&#10;&lt;li&gt;You have to formulate the hypothesis before you do any tests. The&#10;hypothesis follows from your research question. In the case of the $t$-test, one possible two-sided hypothesis is the one you have given, i.e. that the difference between the two means equals some number. In most cases, $w=0$ and the $t$-test tests the null hypothesis that the two means are equal vs. that the two means are different. In the function &lt;code&gt;t.test&lt;/code&gt; you can specify $w$ with the option &lt;code&gt;mu&lt;/code&gt;. In the example that you provided, you've set $w=0$ so the null hypothesis is that the two means are equal. The output from &lt;code&gt;t.test&lt;/code&gt; even states the alternative hypothesis: &lt;code&gt;alternative hypothesis: true difference in means is not equal to 0&lt;/code&gt;. To specify a one-sided alternative hypothesis, use the &lt;code&gt;alternative&lt;/code&gt; option of &lt;code&gt;t.test&lt;/code&gt;. This may be &lt;code&gt;&quot;two.sided&quot;&lt;/code&gt; for a two-sided alternative: the difference of means could be smaller &lt;em&gt;or&lt;/em&gt; greater), &lt;code&gt;&quot;greater&quot;&lt;/code&gt;: the difference between the means is greater than the value that you specified with &lt;code&gt;mu&lt;/code&gt;, &lt;code&gt;&quot;smaller&quot;&lt;/code&gt; for the one-sided hypothesis that the difference of the means is smaller.&lt;/li&gt;&#10;&lt;li&gt;The significance test is given by the output of &lt;code&gt;t.test&lt;/code&gt; in &lt;code&gt;R&lt;/code&gt;. It provides the $t$-value , the degrees of freedom and the corresponding $p$-value. In your case, it is not surprising that the $p$-value is not significant ($p&amp;gt;0.05$) because you generated both samples from a normal distribution with equal mean.&lt;/li&gt;&#10;&lt;li&gt;The output of &lt;code&gt;t.test&lt;/code&gt; provides all necessary information but if you want to get the critical $t$-value manually, you can use the &lt;code&gt;qt&lt;/code&gt; function in &lt;code&gt;R&lt;/code&gt;. In your example this would be ($\alpha=0.05, df=2\cdot 10^{7}$): &lt;code&gt;qt(1-0.5*0.05, df=2e7)&lt;/code&gt;. This gives the critical $t$-value of $1.96$. The $t$-value from your test is much lower and thus, you have not enough evidence to reject the null hypothesis that the two means are equal. To get the $p$-value for a two-sided hypothesis in &lt;code&gt;R&lt;/code&gt;, you could type: &lt;code&gt;2*pt(-abs(t),df=n-1)&lt;/code&gt;. In your case $t=0.5014$ and thus: &lt;code&gt;2*pt(-abs(0.5014), df=2e7)&lt;/code&gt; which yields $0.616$.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="3" CreationDate="2013-06-05T12:25:04.427" Id="60951" LastActivityDate="2013-06-05T12:37:41.040" LastEditDate="2013-06-05T12:37:41.040" LastEditorUserId="21054" OwnerUserId="21054" ParentId="60946" PostTypeId="2" Score="2" />
  
  <row AcceptedAnswerId="60960" AnswerCount="1" Body="&lt;p&gt;it happened to me that in a logistic regression in R with &lt;code&gt;glm&lt;/code&gt; the Fisher scoring iterations in the output are less than the iterations selected with the argument &lt;code&gt;control=glm.control(maxit=25)&lt;/code&gt; in &lt;code&gt;glm&lt;/code&gt; itself.&lt;/p&gt;&#10;&#10;&lt;p&gt;I see this as the effect of divergence in the iteratively reweighted least&#10;squares algorithm behind &lt;code&gt;glm&lt;/code&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;My question is: under which criteria does &lt;code&gt;glm&lt;/code&gt; stop the iterations and provides with a partial output? I was thinking about something like &quot;when the new coefficients-old coefficients &amp;lt; epsilon, then STOP&quot;. Is this the case? If not, what does make &lt;code&gt;glm&lt;/code&gt; stop?&#10;Thanks,&#10;Avitus&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-06-05T13:58:59.630" FavoriteCount="1" Id="60958" LastActivityDate="2013-06-05T14:21:05.147" OwnerUserId="26435" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;regression&gt;&lt;logistic&gt;&lt;scoring&gt;" Title="Logistic regression: Fisher's scoring iterations do not match the selected iterations in glm" ViewCount="1561" />
  
  <row Body="&lt;p&gt;From the information you give, it could be several different study types. If the prevalence of ADHD is collected at a single time point, then this is a cross-sectional study, restricted to the population of individuals born in 1985. If the prevalence is collected over time, then this could be considered a cohort study. If the diagnoses were recorded for research purposes over time starting some time in the past, then this could be a prospective cohort. If you are now going back to medical records to ascertain diagnoses made in the past, then this is a retrospective cohort study. &lt;/p&gt;&#10;&#10;&lt;p&gt;You don't need to have exposed and unexposed groups to do a cohort study, but you do need to have a follow-up time component. Obviously to look at the effect of exposure on outcome you do need exposed and unexposed. As described above, this is a closed cohort (no one else can ever be born in 1985). You could compare the prevalence in this cohort to that in a cohort of, say, individuals born in 1995, or to the whole population currently. Or you could look within the birth cohort to assess the effects of other exposures, say, birth weight, on ADHD diagnosis.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-06-05T16:23:26.893" Id="60973" LastActivityDate="2013-06-05T16:23:26.893" OwnerUserId="18563" ParentId="60968" PostTypeId="2" Score="2" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have data in the form of timestamp,lat,long which is gps data for users. &#10;I'm new to data mining and want to understand how can I start clustering these data to understand more about it.&lt;/p&gt;&#10;&#10;&lt;p&gt;Should I like build a matrix of one trajectory v/s the other based on some distance metric and then apply some clustering algorithm on it?&lt;/p&gt;&#10;&#10;&lt;p&gt;Data will be a trajectory for each user.&lt;/p&gt;&#10;&#10;&lt;p&gt;Each user will have a sequence of points in the form of (timestamp,lat,long) starting at point A to point B. I want to cluster the trajectories.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-06-05T19:02:58.630" Id="60998" LastActivityDate="2013-06-07T08:54:09.943" LastEditDate="2013-06-05T20:32:36.090" LastEditorUserId="3899" OwnerUserId="3899" PostTypeId="1" Score="1" Tags="&lt;clustering&gt;&lt;data-mining&gt;&lt;gis&gt;" Title="Clustering spatio-temporal data?" ViewCount="75" />
  <row AnswerCount="0" Body="&lt;p&gt;My data (about 20-30 data points) seems to be following a quadratic pattern, and it's quite plausible that they influence each other:&lt;/p&gt;&#10;&#10;&lt;p&gt;For $X &amp;lt; 16$, the influence (direction of &quot;Granger-causality&quot;) seems to go from $Y$ to $X$, while for $X &amp;gt; 16$ the direction of causality seems to be reversed.&lt;/p&gt;&#10;&#10;&lt;p&gt;In other (interpretative) words: Two effects may be present, where one of them dominates up until $X &amp;lt; 16$, while the other takes over beyond that point.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is it fine just to quote the $R^2$-value(s) and the $p$-value(s) for &lt;em&gt;either&lt;/em&gt; $Y$ on $X$ &lt;em&gt;or&lt;/em&gt; $X$ on $Y$, or must some alarm bells go off if the two variables influence one another (i.e., the dependent variable effectively becoming the independent variable, and vice versa, for half the data-set)?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-06-05T20:25:43.237" FavoriteCount="1" Id="61003" LastActivityDate="2013-06-05T20:38:43.690" LastEditDate="2013-06-05T20:38:43.690" LastEditorUserId="26535" OwnerUserId="26535" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;correlation&gt;" Title="What to watch out for when regressing Y on X when not only Y influences X but also X influences Y?" ViewCount="85" />
  <row AcceptedAnswerId="61035" AnswerCount="1" Body="&lt;p&gt;First timer here. I'm reviewing a section of Casella and Berger on inverting a hypothesis test to get a Confidence Interval (CI), and came across a proof which I believe is missing a tiny piece. Here's the statement of the theorem verbatim from CB:&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Theorem:&lt;/strong&gt; For each $\theta_0 \in \Theta$, let $A(\theta_0)$ be the acceptance region of a level $\alpha$ test of $H_0: \theta = \theta_0$. For each $\boldsymbol{x} \in \mathcal{X}$, define a set $C(\boldsymbol{x})$ in the parameter space by&#10;$$C(\boldsymbol{x}) = \{\theta_0: \boldsymbol{x} \in A(\theta_0)\}$$&#10;then the random set $C(\boldsymbol{X})$ is a $1-\alpha$ confidence set.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; We want to show that the confidence coefficient is equal to $1-\alpha$. Fix some $\theta_0 \in \Theta$ and define $R(\theta_0)$ as the rejection region of the aforementioned level $\alpha$ test of $H_0: \theta = \theta_0$. Then:&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{align*}&#10;P_{\theta_0}(\theta_0 \in C(\boldsymbol{X})) &amp;amp;= P_{\theta_0}(\boldsymbol{X} \in A(\theta_0))\\&#10;&amp;amp;= 1 - P_{\theta_0}(\boldsymbol{X} \in R(\theta_0))\\&#10;&amp;amp;\geq 1-\alpha&#10;\end{align*}&#10;Where the last step follows from the fact that the hypothesis test is level $\alpha$. This is true for any $\theta_0 \in \Theta$, so thus $C(\boldsymbol{X})$ is a $1-\alpha$ confidence set.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Qualm:&lt;/strong&gt; My problem with this proof is that we have shown that all the coverage probabilities are $\geq 1-\alpha$, but we have not shown that the confidence coefficient is &lt;strong&gt;equal&lt;/strong&gt; to $1-\alpha$ (confidence coefficient = infimum of the coverage probabilities). More precisely, the the definition of a $1-\alpha$ confidence set $C(\boldsymbol{X})$ is:&#10;$$\inf_{\theta} P_{\theta}(\theta \in C(\boldsymbol{X})) = 1-\alpha$$&#10;It appears to me that the conclusion of the proof only shows that&#10;$$\inf_{\theta} P_{\theta}(\theta \in C(\boldsymbol{X})) \geq 1-\alpha$$&#10;I suppose this is a &quot;better&quot; conclusion, but still.&lt;/p&gt;&#10;&#10;&lt;p&gt;This is a very nitpicky detail. Nevertheless I'm curious what I'm missing. CB don't mention anything else in their presentation, so I figure I'm just having a brain freeze. Any pointers?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;ANSWERED:&lt;/strong&gt; After some deliberation with MansT below, we found it to be a typo in Casella and Berger. The correct theorem statement should read:&#10;$$\textrm{...then the random set } C(\boldsymbol{X}) \textrm{ has confidence coefficient } \geq 1-\alpha$$&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-06-05T20:35:16.390" Id="61006" LastActivityDate="2013-06-06T17:45:10.297" LastEditDate="2013-06-06T17:45:10.297" LastEditorUserId="26551" OwnerUserId="26551" PostTypeId="1" Score="2" Tags="&lt;hypothesis-testing&gt;&lt;self-study&gt;&lt;confidence-interval&gt;&lt;mathematical-statistics&gt;" Title="Inverting a hypothesis test: nitpicky detail" ViewCount="133" />
  <row Body="&lt;p&gt;&lt;strong&gt;The solution is straightforward in Cartesian coordinates and impossibly messy in spherical coordinates.&lt;/strong&gt;  I will describe it for a cone that fits within a hemisphere; larger cones can be treated in a similar manner.&lt;/p&gt;&#10;&#10;&lt;p&gt;Choose units in which the sphere has unit radius.  After a suitable rotation by some orthogonal matrix $\mathbb{Q}$, the cone will be vertical and projects onto a circle of radius $\rho$ in the xy plane. In cylindrical coordinates $(r, \theta, z)$, the points will have a uniform distribution in $\theta$ between $0$ and $2\pi$ (by cylindrical symmetry), a uniform distribution in $z$ between $\sqrt{1-\rho^2}$ and $1$, and necessarily $r = \sqrt{1-z^2}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Because $z$ has a uniform distribution, its variance is &lt;/p&gt;&#10;&#10;&lt;p&gt;$$(1 - (1-\rho^2))/12 = \rho^2/12.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The distribution of $r$ is deduced from that of $z$ by taking the Jacobian; its pdf is &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac{r}{\sqrt{1-r^2} (1 - \sqrt{1-\rho^2})}, \quad 0 \le r \le \sqrt{1-\rho^2}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;By axial symmetry the expectations of $x$ and $y$ are both zero, whence their variances are the expectations of their squares.  By the reflection symmetry $x\to y$, $y\to x$, these expectations are equal.  The sum of those expectations is the expectation of $x^2+y^2 = r^2$, which can be computed as&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\int_0^{\rho } \frac{ r^2 \, rdr}{\left(1-\sqrt{1-\rho ^2}\right) \sqrt{1-r^2}} = \frac{2- \left(2+\rho ^2\right)\sqrt{1-\rho ^2} }{3(1-\sqrt{1-\rho ^2})}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Half of this quantity therefore is the common variance of $x$ and $y$.  (As a quick check, the limiting values of the $x$ and $y$ covariances at $\rho\to 1$ are $1/3$.  This is correct, because the variance of $x$ for the upper hemisphere equals the variance of $x$ for the sphere, which clearly is $1/3$.)&lt;/p&gt;&#10;&#10;&lt;p&gt;The symmetries $x\to -x$ and $y\to -y$ immediately imply the covariances among $x$, $y$, and $z$ are all $0$.  We have thereby obtained the full covariance matrix $\mathbb{D}$.  Applying the original rotation gives the original covariance matrix as $\mathbb{Q'DQ}$.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-05T22:18:28.453" Id="61012" LastActivityDate="2013-06-05T22:18:28.453" OwnerUserId="919" ParentId="58302" PostTypeId="2" Score="3" />
  <row AnswerCount="0" Body="&lt;p&gt;I need help in calculating the goodness of fit for a Poisson model. The online literature on it is quite scarce, so if you have any useful articles and links I would appreciate it very much.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is how the table looks like from SPSS. The outcome variable is a count of patents a company has.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://img827.imageshack.us/img827/1655/screenshot20130606at124.png&quot; alt=&quot;model&quot;&gt;&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2013-06-05T22:50:04.240" Id="61015" LastActivityDate="2013-06-06T03:36:19.423" LastEditDate="2013-06-06T03:36:19.423" LastEditorUserId="25739" OwnerUserId="25739" PostTypeId="1" Score="0" Tags="&lt;regression&gt;&lt;poisson&gt;&lt;goodness-of-fit&gt;" Title="Testing the goodness of fit for a Poisson model" ViewCount="247" />
  
  
  <row Body="&lt;p&gt;Yes, this is usually the case with non-centered interactions. A quick look at what happens to the correlation of two independent variables and their &quot;interaction&quot;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;a = rnorm(10000,20,2)&#10;b = rnorm(10000,10,2)&#10;cor(a,b)&#10;cor(a,a*b)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And then when you center them:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;c = a - 20&#10;d = b - 10&#10;cor(c,c*d)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Incidentally, the same can happen with including polynomial terms (i.e., $X,~X^2,~...$) without first centering. &lt;/p&gt;&#10;&#10;&lt;p&gt;So you can give that a shot with your pair.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;As to why centering helps - but let's go back to the definition of covariance&lt;br&gt;&#10;$$&#10;\begin{aligned}&#10;\text{Cov}(X,XY) &amp;amp;= E[(X-E(X))(XY-E(XY))]  \\&#10; &amp;amp;= E[(X-\mu_x)(XY-\mu_{xy})]  \\&#10; &amp;amp;= E[X^2Y-X\mu_{xy}-XY\mu_x+\mu_x\mu_{xy}]  \\&#10; &amp;amp;= E[X^2Y]-E[X]\mu_{xy}-E[XY]\mu_x+\mu_x\mu_{xy}  \\&#10;\end{aligned}&#10;$$  &lt;/p&gt;&#10;&#10;&lt;p&gt;Even given independence of X and Y&lt;br&gt;&#10;$$&#10;\begin{aligned}&#10; &amp;amp;= E[X^2]E[Y]-\mu_x\mu_x\mu_y-\mu_x\mu_y\mu_x+\mu_x\mu_x\mu_y  \\&#10; &amp;amp;= (\sigma_x^2+\mu_x^2)\mu_y-\mu_x^2\mu_y  \\&#10; &amp;amp;= \sigma_x^2\mu_y  \\&#10;\end{aligned}&#10;$$  &lt;/p&gt;&#10;&#10;&lt;p&gt;This doesn't related directly to your regression problem, since you probably don't have completely independent $X$ and $Y$, and since correlation between two explanatory variables doesn't always result in multicollinearity issues in regression. But it does show how an interaction between two non-centered independent variables causes correlation to show up, and that correlation &lt;em&gt;could&lt;/em&gt; cause multicollinearity issues.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Intuitively to me, having non-centered variables interact simply means that when $X$ is big, then $XY$ is also going to be bigger on an absolute scale irrespective of $Y$, and so $X$ and $XY$ will end up correlated, and similarly for $Y$.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2013-06-06T00:49:31.960" Id="61022" LastActivityDate="2013-06-08T16:45:32.863" LastEditDate="2013-06-08T16:45:32.863" LastEditorUserId="7290" OwnerUserId="4485" ParentId="60476" PostTypeId="2" Score="6" />
  <row AcceptedAnswerId="61027" AnswerCount="2" Body="&lt;p&gt;&lt;strong&gt;Background:&lt;/strong&gt; I read one article where authors report Pearson correlation 0.754 from sample size 878. Resulting p-value for correlation test is &quot;two star&quot; significant (i.e. p &amp;lt; 0.01). &#10;However, I think that with such a large sample size, corresponding p-value should be less than 0.001 (i.e. three star significant). &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;strong&gt;Can p-values for this test be computed just from Pearson correlation coefficient and sample size?&lt;/strong&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;strong&gt;If yes, how can this be done in R?&lt;/strong&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="1" CreationDate="2013-06-06T06:02:16.183" FavoriteCount="1" Id="61026" LastActivityDate="2014-02-07T08:37:37.780" LastEditDate="2013-06-06T08:16:52.397" LastEditorUserId="183" OwnerUserId="14730" PostTypeId="1" Score="8" Tags="&lt;hypothesis-testing&gt;&lt;correlation&gt;&lt;p-value&gt;&lt;fraud&gt;" Title="Can p-values for Pearson's correlation test be computed just from correlation coefficient and sample size?" ViewCount="1724" />
  <row Body="&lt;p&gt;Yes, it can be done, if you use Fisher's R-to-z transformation. Other methods (e.g. bootstrap) can have some advantages but require the original data. In R (&lt;em&gt;r&lt;/em&gt; is the sample correlation coefficient, &lt;em&gt;n&lt;/em&gt; is the number of observations):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;z &amp;lt;- 0.5 * log((1+r)/(1-r))&#10;zse &amp;lt;- 1/sqrt(n-3)&#10;min(pnorm(z, sd=zse), pnorm(z, lower.tail=F, sd=zse))*2&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;See also &lt;a href=&quot;http://gaellaurans.blogspot.nl/2009/09/computing-confidence-interval-for.html&quot; rel=&quot;nofollow&quot;&gt;this post on my blog&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;That said, whether it is .01 or .001 doesn't matter that much. As you said, this is mostly a function of sample size and you already know that the sample size is large. The logical conclusion is that you probably don't even need a test at all (especially not a test of the so-called ‘nil’ hypothesis that the correlation is 0). With &lt;em&gt;N&lt;/em&gt; = 878, you can be quite confident in the precision of the estimate and focus on interpreting it directly (i.e. is .75 large in your field?).&lt;/p&gt;&#10;&#10;&lt;p&gt;Formally however, when you do a statistical test in the Neyman-Pearson framework, you need to specify the error level in advance. So, if the results of the test really matter and the study was planned with .01 as the threshold, it only makes sense to report &lt;em&gt;p&lt;/em&gt; &amp;lt; .01 and you should not opportunistically make it &lt;em&gt;p&lt;/em&gt; &amp;lt; .001 based on the obtained &lt;em&gt;p&lt;/em&gt; value. This type of undisclosed flexibility is even one of the main reasons behind criticism of little stars and more generally of the way null-hypothesis significance testing is practiced in social science.&lt;/p&gt;&#10;&#10;&lt;p&gt;See also Meehl, P.E. (1978). Theoretical risks and tabular asterisks: Sir Karl, Sir Ronald, and the slow progress of soft psychology. &lt;em&gt;Journal of Consulting and Clinical Psychology,&lt;/em&gt; 46 (4), 806-834. (The title contains a reference to these “stars” but the content is a much broader discussion of the role of significance testing.)&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2013-06-06T06:25:23.923" Id="61027" LastActivityDate="2013-06-06T12:50:14.933" LastEditDate="2013-06-06T12:50:14.933" LastEditorUserId="22047" OwnerUserId="6029" ParentId="61026" PostTypeId="2" Score="11" />
  
  
  
  <row AcceptedAnswerId="61063" AnswerCount="3" Body="&lt;p&gt;Are studies in epidemiology all observational and not controlled? My impression comes from &lt;a href=&quot;http://en.wikipedia.org/wiki/Epidemiology#Types_of_studies&quot; rel=&quot;nofollow&quot;&gt;Wikipedia&lt;/a&gt;, which lists:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Case Series&lt;/li&gt;&#10;&lt;li&gt;Case Control Studies&lt;/li&gt;&#10;&lt;li&gt;Cohort Studies&lt;/li&gt;&#10;&lt;li&gt;Outbreak Investigations&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;and no others.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-06-06T13:09:01.040" FavoriteCount="1" Id="61045" LastActivityDate="2013-06-16T07:50:17.210" LastEditDate="2013-06-16T07:50:17.210" LastEditorUserId="3826" OwnerUserId="1005" PostTypeId="1" Score="4" Tags="&lt;experiment-design&gt;&lt;epidemiology&gt;" Title="Are studies in epidemiology all observational?" ViewCount="313" />
  <row Body="&lt;p&gt;I'm pretty sure that &quot;plausible&quot; is used with its standard, non-technical meaning here, and refers to the mechanism by which your data is generated.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, if you have data on temperatures in your area and data on your electricity consumption, it's plausible that warmer temperatures will lead to more air-conditioning use and hence higher electricity use.&lt;/p&gt;&#10;&#10;&lt;p&gt;On the other hand, if you have data on the number of cookies sold in your area, I can't think of a plausible mechanism by which this would cause you to use more electricity in your home.&lt;/p&gt;&#10;&#10;&lt;p&gt;It may be that there is a plausible mechanism but you don't currently know enough to clearly see what it is. For example, you may see that various sea surface temperatures in the Pacific Ocean are correlated with drought conditions in Texas, but not know of the atmospheric mechanism that might underly this. Two hundred years ago, people may have been skeptical of any relationship over such a long distance, but today it's accepted and is plausible even without a deep understanding of meteorology. (To a meteorologist, it's extremely plausible.)&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT: In answer to your comment, &quot;plausible&quot; means &quot;believable&quot; or &quot;reasonable&quot;, so a plausible explanation &quot;makes sense&quot;. It's weaker than &quot;proven&quot;, but stronger than &quot;speculative&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you do have a &quot;plausible mechanism&quot;, it gives some amount of confidence that you can speak about causality. But it doesn't guarantee that you're right. If you do not have a plausible mechanism, then there is suspicion that you've merely found correlation and not causation. Not (currently) having a plausible mechanism doesn't guarantee that there can be none, as the caveat in your quote notes.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, just because someone says that XYZ is a &quot;plausible mechanism&quot; doesn't mean that knowledgeable people would agree. &quot;Plausible&quot; needs to be interpreted within a context. Plausible to the conspiracy theorist is not the same as plausible to a layman, which is not the same as plausible to a scholar in the field. (Which is where you may have gotten your connotation: someone saying something like, &quot;it &lt;em&gt;may&lt;/em&gt; be plausible that ...&quot; meaning that there's a very slight, speculative possibility, but that's a verbal nuance.)&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-06-06T13:55:04.640" Id="61049" LastActivityDate="2013-06-06T14:21:34.597" LastEditDate="2013-06-06T14:21:34.597" LastEditorUserId="1764" OwnerUserId="1764" ParentId="61046" PostTypeId="2" Score="2" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I am comparing two different models (multiple linear model). &lt;/p&gt;&#10;&#10;&lt;p&gt;Basically the explanatory variables remain the same while response variable are different.&lt;/p&gt;&#10;&#10;&lt;p&gt;$Model\ 1  - Y_1 = X_1+X_2+X_3$&lt;/p&gt;&#10;&#10;&lt;p&gt;$Model\ 2  - Y_2 = X_1+X_2+X_3$&lt;/p&gt;&#10;&#10;&lt;p&gt;I have results from R for both models. &lt;/p&gt;&#10;&#10;&lt;p&gt;I am just wondering if two of these results eg $F$-statistics, $p$-value, $R^2$, RSS are comparable. So can I say model 1 is better because p-value is smaller. etc ?&lt;/p&gt;&#10;&#10;&lt;p&gt;From what I learned from class, I only compared models which had the same response variable and different explanatory variables.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-06-06T14:55:19.527" Id="61058" LastActivityDate="2015-02-15T11:46:38.837" LastEditDate="2013-08-05T18:35:07.953" LastEditorUserId="22047" OwnerUserId="26528" PostTypeId="1" Score="3" Tags="&lt;regression&gt;&lt;multiple-regression&gt;&lt;modeling&gt;&lt;t-test&gt;&lt;f-test&gt;" Title="Regression model F test on different response variables?" ViewCount="224" />
  
  <row Body="&lt;p&gt;If you want to compare the distribution shapes, you might use Kolmogorov-Smirnov. I didn't quite understand: that if one sample size is 20 and the other one is thousands? Please be clear in your question. Explain your study more clearly please. What is timestamping? And what are these duplicates? What are your study goals? What are your variables? How many groups do you have?&lt;/p&gt;&#10;&#10;&lt;p&gt;Is your case like comparing packet delay of for example TCP with UDP? So here what are duplicates, what group has thousands of data? What group(s) has (have) only 20 data? By 20 data you mean 20 packets? Or 20 attempts of connection, each with thousands of data? etc...&lt;/p&gt;&#10;&#10;&lt;p&gt;The point is that if your group has thousands of observations, you should adjust your alpha because at a usual alpha = 0.05, a high number of observations will cause a false positive result with the slightest differences in the two distributions.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-06-06T16:46:29.800" Id="61072" LastActivityDate="2013-06-06T16:46:29.800" OwnerUserId="20556" ParentId="60862" PostTypeId="2" Score="1" />
  
  
  
  <row Body="&lt;p&gt;Method 1&lt;br/&gt;&#10;You are right. Cross-sectional analysis of this kind would mean to do the regression for a particular year for which you will only have 24 observations, so your estimates will be less precise. Otherwise there is nothing wrong with it but if you have the &quot;luxury&quot; to exploit also the time dimension of your data this might help (see below).&lt;/p&gt;&#10;&#10;&lt;p&gt;Method 2&lt;br/&gt;&#10;You could use treat your data as pooled cross-sections and use your 150 but you should not &quot;ignore&quot; the years. In this case you will also need year dummies and cluster your standard errors on the country ID variable. The latter point is suggested by Cameron &amp;amp; Trivedi (2009) in order to correct the standard errors for serial correlation. You said correctly that some countries are sampled in multiple years, hence for country i the error in year t will be correlated with the error in year t-1.&lt;/p&gt;&#10;&#10;&lt;p&gt;The better alternative would be to declare your data an unbalanced panel and do a fixed effects estimation. This type of regression absorbs all unobserved time-invariant country characteristics into a constant such that these are not in the error term anymore. This is particularly useful if you suspect some of your explanatory variables to be correlated with such time-invariant characteristics. If you want a refresher or basic reference for fixed effects methods you can find one &lt;a href=&quot;http://faculty.arts.ubc.ca/tlemieux/econ594/lecture4.pdf&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.&lt;br/&gt;&#10;The problem with fixed effects is that if you are interested in the effect of a time-invariant variable, you need to employ some tricks in order to do that. Otherwise it will just be absorbed in the fixed effect as well.&lt;/p&gt;&#10;&#10;&lt;p&gt;Method 3&lt;br/&gt;&#10;Of the three offered alternatives this seems to be the least preferrable one. Suppose you have two countries and three time periods. You observe GDP of country A in year 1 and 2, and GDP of country B in year 3. You want to construct a cross-section for year 3, so now suppose that in year 1 there was a deep recession that affected both countries. Averaging country A such that it appears in the final period (the one you want to use as cross-section) might give you a much lower value for GDP that you would have obtained, had you observed GDP in country A also in year 3.&lt;br/&gt;&#10;Actually, I haven't seen anyone using such averages to construct cross-sections but then I'm not a macro-guy either. Perhaps someone else knows more about your proposed method 3 but intuitively it seems less practical than the previous two. Still I hope this helps you.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-06-06T21:37:37.970" Id="61105" LastActivityDate="2013-06-06T21:37:37.970" OwnerUserId="26338" ParentId="61088" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;My model is as follows:&#10;$$&#10;\begin{aligned}&#10;y&amp;amp;=a_0 + a_1x_1 + a_2x_2  \\&#10;x_2 &amp;amp;= b_0 + b_1x_1 + b_2z&#10;\end{aligned}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm only interested in the effect of $x_2$ on $y$. More precisely, I want to see the combined effect of:  &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;$x_2$ directly, and &lt;/li&gt;&#10;&lt;li&gt;$x_2$ implicitly (through $x_1$)&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;After estimating the coefficients, I have the following procedure:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;solve equation 2 for $x_1$ &lt;/li&gt;&#10;&lt;li&gt;substitute the resulting term in equation 1.&lt;/li&gt;&#10;&lt;li&gt;look at the &quot;new&quot; $x_2$ coefficient: $(a_1/b_1 + a_2)$&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;The model comes from theory and there's no possibility to include other variables as instruments.&lt;/p&gt;&#10;&#10;&lt;p&gt;I tried multiple equations OLS with HAC standard errors and got some good results, but I'm not at all convinced that this is the right way.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-06-06T21:56:59.327" FavoriteCount="1" Id="61106" LastActivityDate="2013-06-07T04:02:48.337" LastEditDate="2013-06-06T22:20:45.357" LastEditorUserId="7290" OwnerUserId="26594" PostTypeId="1" Score="2" Tags="&lt;estimation&gt;&lt;multiple-regression&gt;&lt;endogeneity&gt;" Title="Estimation of simultaneous equations model" ViewCount="107" />
  
  
  <row Body="&lt;pre&gt;&lt;code&gt;require(imputation)&#10;x = matrix(rnorm(100),10,10)&#10;x.missing = x &amp;gt; 1&#10;x[x.missing] = NA&#10;y &amp;lt;- kNNImpute(x, 3)&#10;&#10;attributes(y)&#10;&#10;$names&#10;[1] &quot;x&quot;              &quot;missing.matrix&quot;&#10;&#10;y$x&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;&gt; x (original matrix)&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;             [,1]        [,2]       [,3]       [,4]        [,5]        [,6]        [,7]&#10; [1,]  0.38515909  0.52661156  0.6164138  0.3095225  0.55909716 -1.16543168 -0.70714440&#10; [2,] -0.39222402 -1.29703536  0.4429824 -1.3950116          NA -0.46841443 -0.57563472&#10; [3,] -2.04467869 -0.52022405         NA  0.7219057 -0.93573417 -1.51490638  0.62356689&#10; [4,] -1.08684345  0.63083074         NA  0.5603603  0.48583414          NA -0.69447183&#10; [5,]  0.30116921  0.25127476 -0.2132160         NA -1.63484823 -0.58266488  0.34432576&#10; [6,]  0.82152305 -0.12900915 -1.8498997  0.8012059          NA -0.14987133 -1.11232289&#10; [7,]  0.27912763 -0.68923032 -0.2355762 -0.2541675 -0.14181344 -0.08519797  0.13061823&#10; [8,]  0.06653984 -0.87521539 -0.0980306 -0.4350224  0.05021324 -1.66963624 -0.09204772&#10; [9,]  0.12687240 -0.62717646 -0.1258722         NA -0.86913445  0.68365036          NA&#10;[10,]  0.56680502  0.03318012  0.1411861  0.6573134 -0.14747073          NA -1.37949278&#10;             [,8]        [,9]       [,10]&#10; [1,] -2.67066748          NA -0.64370528&#10; [2,] -1.26864936 -1.95692064  0.28917897&#10; [3,] -0.27816124 -0.20332695 -1.29456054&#10; [4,] -1.10917662 -0.59598910 -0.32475962&#10; [5,] -0.15448822  0.71667444 -1.60827152&#10; [6,] -0.66691445  0.05396037  0.04074923&#10; [7,]  0.05644956  0.99416556 -0.77808427&#10; [8,] -0.32294266          NA -2.50933697&#10; [9,] -0.67226044          NA          NA&#10;[10,] -0.84866945 -0.54318570          NA&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;&gt; y$x (imputed matrix)&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;            [,1]        [,2]        [,3]        [,4]        [,5]        [,6]        [,7]&#10; [1,]  0.38515909  0.52661156  0.61641378  0.30952251  0.55909716 -1.16543168 -0.70714440&#10; [2,] -0.39222402 -1.29703536  0.44298237 -1.39501160 -0.22157531 -0.46841443 -0.57563472&#10; [3,] -2.04467869 -0.52022405  0.08298882  0.72190573 -0.93573417 -1.51490638  0.62356689&#10; [4,] -1.08684345  0.63083074 -0.66707695  0.56036034  0.48583414 -0.98956026 -0.69447183&#10; [5,]  0.30116921  0.25127476 -0.21321600 -0.02480909 -1.63484823 -0.58266488  0.34432576&#10; [6,]  0.82152305 -0.12900915 -1.84989965  0.80120592 -0.76323053 -0.14987133 -1.11232289&#10; [7,]  0.27912763 -0.68923032 -0.23557619 -0.25416751 -0.14181344 -0.08519797  0.13061823&#10; [8,]  0.06653984 -0.87521539 -0.09803060 -0.43502238  0.05021324 -1.66963624 -0.09204772&#10; [9,]  0.12687240 -0.62717646 -0.12587221  0.00000000 -0.86913445  0.68365036  0.00000000&#10;[10,]  0.56680502  0.03318012  0.14118610  0.65731337 -0.14747073  0.00000000 -1.37949278&#10;             [,8]        [,9]       [,10]&#10; [1,] -2.67066748  0.04286260 -0.64370528&#10; [2,] -1.26864936 -1.95692064  0.28917897&#10; [3,] -0.27816124 -0.20332695 -1.29456054&#10; [4,] -1.10917662 -0.59598910 -0.32475962&#10; [5,] -0.15448822  0.71667444 -1.60827152&#10; [6,] -0.66691445  0.05396037  0.04074923&#10; [7,]  0.05644956  0.99416556 -0.77808427&#10; [8,] -0.32294266  0.00000000 -2.50933697&#10; [9,] -0.67226044  0.00000000  0.00000000&#10;[10,] -0.84866945 -0.54318570  0.00000000&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;It's imputed the values that it can.  Those that can't be imputed are set to zero.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-06-07T01:58:57.780" Id="61115" LastActivityDate="2013-06-07T13:53:56.160" LastEditDate="2013-06-07T13:53:56.160" LastEditorUserId="20381" OwnerUserId="20381" ParentId="61110" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;First the curve you have drawn is not the bell you're looking for. Your &quot;bell&quot; should be more like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/UywSB.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Your histogram-drawn-as-a-bar-chart (yikes! Excel encourages terrible things) looks reasonably close to that.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, histograms are &lt;em&gt;not a very good way to check for normality of residuals&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;As discussed &lt;a href=&quot;http://stats.stackexchange.com/a/51753/805&quot;&gt;here&lt;/a&gt;, on occasions - and depending on your choices for where the histogram bars go, the &lt;em&gt;same set of values&lt;/em&gt; might look as different as these:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/dfxUJ.png&quot; alt=&quot;Skew vs bell&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Just to repeat - that's two different histograms of the &lt;em&gt;same&lt;/em&gt; numbers. Kernel density estimates and better still, QQ plots (at least once you learn how to read them) are significantly more informative. If you must use histograms, use plenty of bins and do more than one.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-07T04:54:40.893" Id="61123" LastActivityDate="2013-06-07T04:54:40.893" OwnerUserId="805" ParentId="58923" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;&lt;strong&gt;Two medical interventions A and B were compared within randomised controlled trial:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;differences in progression free survival (PFS, i.e. how long&#10;patient    lived with controlled disease, from entering study    to progression) significant with p&amp;lt;0,001,&lt;/li&gt;&#10;&lt;li&gt;differences in overall survival (OS, i.e how long patient lived, from entering study    to death) non-significant with p=0,12.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;We would like to model/predict PFS and OS beyond time of RCT. Which model should be chosen:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;joint modelling of OS (we don't allow differences between A and B, single OS function for two interventions), separate modelling of PFS (two different functions for A and B),&lt;/li&gt;&#10;&lt;li&gt;separate modelling of OS and PFS (four different functions for A and B, two for PFS, two for OS).&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Models are build by analysing goodness-of-fit (AIC/BIC, visual inspection and comparison with external data sources) to Kaplan-Meier curves. We consider using Weibull, log-normal, Gompertz, log-logistic and Gamma distributions.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Please note that English is not my first language.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-06-07T08:57:13.057" Id="61135" LastActivityDate="2013-06-07T08:57:13.057" OwnerUserId="26604" PostTypeId="1" Score="2" Tags="&lt;statistical-significance&gt;&lt;modeling&gt;" Title="Using non-significant data for modelling/predictions" ViewCount="31" />
  
  <row AcceptedAnswerId="61421" AnswerCount="4" Body="&lt;p&gt;I am a biology student. We do many Enzyme Linked Immunosorbent Assay (ELISA) experiments and Bradford detection. A 4-parametric logistic regression (&lt;a href=&quot;http://www.miraibio.com/blog/2010/08/the-4-parameter-logistic-4pl-nonlinear-regression-model/&quot; rel=&quot;nofollow&quot;&gt;reference&lt;/a&gt;) is often used for regression these data following this function:&#10;$$&#10;F(x) = \left(\frac{A-D}{1+(x/C)^B}\right) + D &#10;$$&#10;How can I do this in &lt;code&gt;R&lt;/code&gt;? I want to get the $A$, $B$, $C$ and $D$ values and plot the curve.&lt;/p&gt;&#10;&#10;&lt;p&gt;PS. If I have some data, how can I use the calculated function $F(x)$ to get the value? I mean how do I go from &quot;data -&gt; F(x) -&gt; value&quot;?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-07T09:45:37.763" Id="61144" LastActivityDate="2013-12-05T21:59:45.470" LastEditDate="2013-06-07T15:24:25.503" LastEditorUserId="7290" OwnerUserId="26342" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;regression&gt;&lt;logistic&gt;&lt;biostatistics&gt;&lt;bioinformatics&gt;" Title="How to do 4-parametric regression for ELISA data in R" ViewCount="2303" />
  <row AnswerCount="1" Body="&lt;p&gt;On seeing the following table, I concluded there is a lot of sparseness in the 'very unsafe' category. My first reaction was to merge it with the 'unsafe' category, leaving me with 3 categories. But another option is to just delete the category. What are the considerations to make before choosing one option? Also, I am wondering if it is ok to keep 2 degrees of safe, and only 1 degree of unsafe. It would, to me, not make much sense when discussing the odds. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/vSNvO.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;edit in response to Nick Cox:&lt;/p&gt;&#10;&#10;&lt;p&gt;I assume I should remove the category since the conclusions seem out of line with theoretical expectations. For example, it is assumed that those living in more urban areas experience greater levels of fear. And indeed, the odds of feeling very safe (vs safe) are:&#10;countryside &gt; small city &gt; big city. The odds of feeling safe (vs unsafe) have the same order. But then for unsafe (vs very unsafe) it becomes countryside &gt; big city &gt; small city, which is out of line with theory and the former trend.&lt;/p&gt;&#10;&#10;&lt;p&gt;I could say that, although urbanisation brings with greater feelings of unsafety, these feeling take on less severe degrees when initiated urbanisation proceeds to higher levels. But of course, I don't want to report such conclusions when these are due to data sparseness rather than social realities. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-06-07T10:07:45.247" Id="61145" LastActivityDate="2013-06-07T11:06:48.983" LastEditDate="2013-06-07T11:06:48.983" LastEditorUserId="18334" OwnerUserId="18334" PostTypeId="1" Score="1" Tags="&lt;categorical-data&gt;" Title="Merge categories or drop one?" ViewCount="53" />
  
  <row AcceptedAnswerId="61161" AnswerCount="2" Body="&lt;p&gt;I have four samples $x_1, x_2$ and  $y_1, y_2$ with $n_{x1} \neq n_{x2} \neq n_{y1} \neq n_{y2} $. I calculated, using a Wilcoxon rank sum test, that $x_1$ is significantly different to $x_2$ and $y_1$ significantly different to $y_2$. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, I would like to test whether the difference $x_1 - x_2$ differs significantly to $y_1 - y_2$ but I have no idea how do to that given the unequal sample sizes.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any ideas or suggestions would be really appreciated.   &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-07T10:43:02.363" Id="61150" LastActivityDate="2013-06-08T02:50:19.750" LastEditDate="2013-06-07T11:10:27.433" LastEditorUserId="26609" OwnerUserId="26609" PostTypeId="1" Score="0" Tags="&lt;group-differences&gt;&lt;wilcoxon&gt;" Title="Comparison of differences between pairs of samples of unequal size" ViewCount="794" />
  
  <row Body="&lt;p&gt;I'm the creator of bayes-scala toolbox you are referring to. Last year I implemented the EM in discrete bayesian network for learning from incomplete data (including latent variables), that looks like the use case you are asking about.&lt;/p&gt;&#10;&#10;&lt;p&gt;Some tutorial for a sprinkler bayesian network is &lt;a href=&quot;https://github.com/danielkorzekwa/bayes-scala#getting-started---learning-parameters-with-expectation-maximisation-in-bayesian-networks-from-incomplete-data--1&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;And, &quot;Learning Dynamic Bayesian Networks with latent variables&quot; is given &lt;a href=&quot;https://github.com/danielkorzekwa/bayes-scala#getting-started---learning-parameters-with-expectation-maximisation-in-unrolled-dynamic-bayesian-networks-from-incomplete-data--1&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-06-07T11:13:05.100" Id="61156" LastActivityDate="2013-11-05T11:05:34.767" LastEditDate="2013-11-05T11:05:34.767" LastEditorUserId="28740" OwnerUserId="26613" ParentId="61108" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;A couple of other packages to check out might be &lt;code&gt;ri&lt;/code&gt; (randomization inference), &lt;code&gt;MatchIt&lt;/code&gt; (provides various matching algorithms), and &lt;code&gt;experiment&lt;/code&gt; (some basic ATE and LATE methods).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-08T11:32:48.940" Id="61213" LastActivityDate="2013-06-08T11:32:48.940" OwnerUserId="25138" ParentId="61185" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="61243" AnswerCount="1" Body="&lt;p&gt;I've gone through Hidden Markov models (HMM) for the past few months. However there are a few things that are confusing.&lt;/p&gt;&#10;&#10;&lt;p&gt;The set up is simple: I have to model some human gestures such as walking, jumping, and falling. The observed data have been obtained via an accelerometer while the person was doing the movements.&lt;/p&gt;&#10;&#10;&lt;p&gt;I trained theses observations using the famous Baum-Welch algorithm to get the parameters of an HMM for some states. Further, using the Forward and Backward procedures, the likelihood of the observation sequences given the model (i.e., the parameters) were found.&lt;/p&gt;&#10;&#10;&lt;p&gt;Using a model selection criteria such as Akaike information criterion (AIC), I got the optimum states that represented the data: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;a)Walking: 2 states&#10;b)jumping: 2 states&#10;c)Falling: 4 states&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;All these HMMs are then stored in a directory.&lt;/p&gt;&#10;&#10;&lt;p&gt;Lastly, Viterbi decoding is used to get the most likely sequence of hidden states that produced the data.&lt;/p&gt;&#10;&#10;&lt;p&gt;My questions are:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Suppose I performed the experiment again and I just get the data without knowing what kind of movement has been done. After getting the data trained, I got 2 states. How will the machine differentiate which kind of movement has been done, especially if walking and jumping are represented by 2 states?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Suppose the person has performed a different kind of gesture, e.g., sliding, what is the expected output after training? Will the machine be able to detect that or generate a false negative result?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="3" CreationDate="2013-06-08T14:05:31.393" FavoriteCount="2" Id="61221" LastActivityDate="2013-06-09T01:31:39.780" LastEditDate="2013-06-09T01:31:39.780" LastEditorUserId="3826" OwnerUserId="17362" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;data-mining&gt;&lt;markov-process&gt;&lt;hidden-markov-model&gt;&lt;pattern-recognition&gt;" Title="Confusion about hidden Markov model" ViewCount="244" />
  <row Body="&lt;p&gt;John Fox's book &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/141297514X&quot; rel=&quot;nofollow&quot;&gt;An R companion to applied regression&lt;/a&gt; is an excellent ressource on applied regression modelling with &lt;code&gt;R&lt;/code&gt;. The package &lt;code&gt;car&lt;/code&gt; which I use throughout in this answer is the accompanying package. The book also &lt;a href=&quot;http://socserv.socsci.mcmaster.ca/jfox/Books/Companion/index.html&quot; rel=&quot;nofollow&quot;&gt;has as website&lt;/a&gt; with additional chapters.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;h2&gt;Transforming the response (aka dependent variable, outcome)&lt;/h2&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Power_transform#Box.E2.80.93Cox_transformation&quot; rel=&quot;nofollow&quot;&gt;Box-Cox transformations&lt;/a&gt; offer a possible way for choosing a transformation of the response. After fitting your regression model containing untransformed variables with the &lt;code&gt;R&lt;/code&gt; function &lt;code&gt;lm&lt;/code&gt;, you can use the function &lt;code&gt;boxCox&lt;/code&gt; from the &lt;a href=&quot;http://cran.r-project.org/web/packages/car/index.html&quot; rel=&quot;nofollow&quot;&gt;&lt;code&gt;car&lt;/code&gt; package&lt;/a&gt; to estimate $\lambda$ (i.e. the power parameter) by maximum likelihood. &lt;strong&gt;Because your dependent variable isn't strictly positive, Box-Cox transformations will not work&lt;/strong&gt; and you have to specify the option &lt;code&gt;family=&quot;yjPower&quot;&lt;/code&gt; to use the &lt;a href=&quot;http://www.stat.umn.edu/arc/yjpower.pdf&quot; rel=&quot;nofollow&quot;&gt;Yeo-Johnson transformations&lt;/a&gt; (see the original paper &lt;a href=&quot;http://www.jstor.org/discover/10.2307/2673623?uid=3737760&amp;amp;uid=2&amp;amp;uid=4&amp;amp;sid=21102084279913&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; and this &lt;a href=&quot;http://stats.stackexchange.com/questions/60202/r-code-for-yeo-johnson-transformation&quot;&gt;related post&lt;/a&gt;):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;boxCox(my.regression.model, family=&quot;yjPower&quot;, plotit = TRUE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This produces a plot like the following one:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/kXn68.png&quot; alt=&quot;Box-Cox lambdaplot&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The best estimate of $\lambda$ is the value that maximizes the profile likelhod which in this example is about 0.2. Usually, the estimate of $\lambda$ is rounded to a familiar value that is still within the 95%-confidence interval, such as -1, -1/2, 0, 1/3, 1/2, 1 or 2.&lt;/p&gt;&#10;&#10;&lt;p&gt;To transform your dependent variable now, use the function &lt;code&gt;yjPower&lt;/code&gt; from the &lt;code&gt;car&lt;/code&gt; package: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;depvar.transformed &amp;lt;- yjPower(my.dependent.variable, lambda)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In the function, the &lt;code&gt;lambda&lt;/code&gt; should be the rounded $\lambda$ you have found before using &lt;code&gt;boxCox&lt;/code&gt;. Then fit the regression again with the transformed dependent variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Important:&lt;/strong&gt; Rather than just log-transform the dependent variable, you should consider to fit a GLM with a log-link. Here are some references that provide further information: &lt;a href=&quot;http://blog.stata.com/2011/08/22/use-poisson-rather-than-regress-tell-a-friend/%24&quot; rel=&quot;nofollow&quot;&gt;first&lt;/a&gt;, &lt;a href=&quot;http://andrewgelman.com/2006/04/10/log_transformat/&quot; rel=&quot;nofollow&quot;&gt;second&lt;/a&gt;, &lt;a href=&quot;http://stats.stackexchange.com/questions/43930/choosing-between-lm-and-glm-for-a-log-transformed-response-variable&quot;&gt;third&lt;/a&gt;. To do this in &lt;code&gt;R&lt;/code&gt;, use &lt;code&gt;glm&lt;/code&gt;: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;glm.mod &amp;lt;- glm(y~x1+x2, family=gaussian(link=&quot;log&quot;))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;where &lt;code&gt;y&lt;/code&gt; is your dependent variable and &lt;code&gt;x1&lt;/code&gt;, &lt;code&gt;x2&lt;/code&gt; etc. are your independent variables.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;h2&gt;Transformations of predictors&lt;/h2&gt;&#10;&#10;&lt;p&gt;Transformations of &lt;strong&gt;strictly positive predictors&lt;/strong&gt; can be estimated by maximum likelihood after the transformation of the dependent variable. To do so, use the function &lt;code&gt;boxTidwell&lt;/code&gt; from the &lt;code&gt;car&lt;/code&gt; package (for the original paper see &lt;a href=&quot;http://www.jstor.org/stable/1266288&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;). Use it like that: &lt;code&gt;boxTidwell(y~x1+x2, other.x=~x3+x4)&lt;/code&gt;. The important thing here is that option &lt;code&gt;other.x&lt;/code&gt; indicates the terms of the regression that are &lt;em&gt;not&lt;/em&gt; to be transformed. This would be &lt;strong&gt;all your categorical variables.&lt;/strong&gt; The function produces an output of the following form:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;boxTidwell(prestige ~ income + education, other.x=~ type + poly(women, 2), data=Prestige)&#10;&#10;          Score Statistic   p-value MLE of lambda&#10;income          -4.482406 0.0000074    -0.3476283&#10;education        0.216991 0.8282154     1.2538274&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In that case, the score test suggests that the variable &lt;code&gt;income&lt;/code&gt; should be transformed. The maximum likelihood estimates of $\lambda$ for &lt;code&gt;income&lt;/code&gt; is -0.348. This could be rounded to -0.5 which is analogous to the transformation $\text{income}_{new}=1/\sqrt{\text{income}_{old}}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Another very interesting post on the site about the transformation of the independent variables is &lt;a href=&quot;http://stats.stackexchange.com/questions/35711/box-cox-like-transformation-for-independent-variables&quot;&gt;this one&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;h2&gt;Disadvantages of transformations&lt;/h2&gt;&#10;&#10;&lt;p&gt;While log-transformed dependent and/or independent variables can be &lt;a href=&quot;http://stats.stackexchange.com/questions/18480/interpretation-of-log-transformed-predictor&quot;&gt;interpreted relatively easy&lt;/a&gt;, the interpretation of other, more complicated transformations is less intuitive (for me at least). How would you, for example, interpret the regression coefficients after the dependent variables has been transformed by $1/\sqrt{y}$? There are quite a few posts on this site that deal exactly with that question: &lt;a href=&quot;http://stats.stackexchange.com/questions/38750/how-to-interpret-regression-coefficients-when-outcome-variable-was-transformed-t&quot;&gt;first&lt;/a&gt;, &lt;a href=&quot;http://stats.stackexchange.com/questions/2142/linear-regression-effect-sizes-when-using-transformed-variables/2148#2148&quot;&gt;second&lt;/a&gt;, &lt;a href=&quot;http://stats.stackexchange.com/questions/27067/back-transformation-of-regression-coefficients&quot;&gt;third&lt;/a&gt;, &lt;a href=&quot;http://stats.stackexchange.com/questions/35982/how-to-interpret-regression-coefficients-when-response-was-transformed-by-the-4t&quot;&gt;fourth&lt;/a&gt;. If you use the $\lambda$ from Box-Cox directly, without rounding (e.g. $\lambda$=-0.382), it is even more difficult to interpret the regression coefficients.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;h2&gt;Modelling nonlinear relationships&lt;/h2&gt;&#10;&#10;&lt;p&gt;Two quite flexible methods to fit nonlinear relationships are &lt;a href=&quot;http://www.stata.com/meeting/2sweden/lambert_fpma.pdf&quot; rel=&quot;nofollow&quot;&gt;fractional polynomials&lt;/a&gt; and &lt;a href=&quot;http://data.princeton.edu/eco572/smoothing2.html&quot; rel=&quot;nofollow&quot;&gt;splines&lt;/a&gt;. These three papers offer a very good introduction to both methods: &lt;a href=&quot;http://ije.oxfordjournals.org/content/28/5/964.abstract&quot; rel=&quot;nofollow&quot;&gt;First&lt;/a&gt;, &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/7548341&quot; rel=&quot;nofollow&quot;&gt;second&lt;/a&gt; and &lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0167947305001623&quot; rel=&quot;nofollow&quot;&gt;third&lt;/a&gt;. There is also a whole &lt;a href=&quot;http://www.amazon.de/Multivariable-Model-Regression-Polynomials-Probability/dp/0470028424/ref=sr_1_2?ie=UTF8&amp;amp;qid=1370346695&amp;amp;sr=8-2&amp;amp;keywords=sauerbrei&quot; rel=&quot;nofollow&quot;&gt;book&lt;/a&gt; about fractional polynomials and &lt;code&gt;R&lt;/code&gt;. The &lt;code&gt;R&lt;/code&gt; &lt;a href=&quot;http://cran.r-project.org/web/packages/mfp/&quot; rel=&quot;nofollow&quot;&gt;package &lt;code&gt;mfp&lt;/code&gt;&lt;/a&gt; implements multivariable fractional polynomials. &lt;a href=&quot;http://www.biometrie.uni-heidelberg.de/statmeth-ag/veranstaltungen/magdeburg07/talks/sauerbrei.pdf&quot; rel=&quot;nofollow&quot;&gt;This presentation&lt;/a&gt; might be informative regarding fractional polynomials. To fit splines, you can use the function &lt;code&gt;gam&lt;/code&gt; (generalized additive models, see &lt;a href=&quot;http://www3.nd.edu/~mclark19/learn/GAMS.pdf&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; for an excellent introduction with &lt;code&gt;R&lt;/code&gt;) from the &lt;a href=&quot;http://cran.r-project.org/web/packages/mgcv/index.html&quot; rel=&quot;nofollow&quot;&gt;package &lt;code&gt;mgcv&lt;/code&gt;&lt;/a&gt; or the functions &lt;code&gt;ns&lt;/code&gt; (natural cubic splines) and &lt;code&gt;bs&lt;/code&gt; (cubic B-splines) from the package &lt;code&gt;splines&lt;/code&gt; (see &lt;a href=&quot;http://data.princeton.edu/R/linearModels.html&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; for an example of the usage of these functions). Using &lt;code&gt;gam&lt;/code&gt; you can specify which predictors you want to fit using splines using the &lt;code&gt;s()&lt;/code&gt; function:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;my.gam &amp;lt;- gam(y~s(x1) + x2, family=gaussian())&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;here, &lt;code&gt;x1&lt;/code&gt; would be fitted using a spline and &lt;code&gt;x2&lt;/code&gt; linearly as in a normal linear regression. Inside &lt;code&gt;gam&lt;/code&gt; you can specify the distribution family and the link function as in &lt;code&gt;glm&lt;/code&gt;. So to fit a model with a log-link function, you can specify the option &lt;code&gt;family=gaussian(link=&quot;log&quot;)&lt;/code&gt; in &lt;code&gt;gam&lt;/code&gt; as in &lt;code&gt;glm&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Have a look at &lt;a href=&quot;http://stats.stackexchange.com/questions/60828/how-to-test-a-curvilinear-relationship-in-a-logistic-regression/60835#60835&quot;&gt;this post&lt;/a&gt; from the site.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2013-06-08T15:33:03.727" Id="61226" LastActivityDate="2013-07-04T17:32:30.577" LastEditDate="2013-07-04T17:32:30.577" LastEditorUserId="21054" OwnerUserId="21054" ParentId="61217" PostTypeId="2" Score="14" />
  <row AnswerCount="1" Body="&lt;p&gt;When I Google for&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&quot;fisher&quot; &quot;fiducial&quot;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;...I sure get a lot of hits, but all the ones I've followed are utterly beyond my comprehension.&lt;/p&gt;&#10;&#10;&lt;p&gt;All these hits do seem to have one thing in common: they are all written for dyed-in-the-wool statisticians, people thoroughly steeped in the theory, practice, history, and lore of statistics.  (Hence, none of these accounts bothers to explain or even illustrate what Fisher meant by &quot;fiducial&quot; without resorting to oceans of jargon and/or passing the buck to some classic or other of the mathematical statistics literature.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Well, I don't belong to the select intended audience that could benefit for what I've found on the subject, and maybe this explains why every one of my attempts to understand what Fisher meant by &quot;fiducial&quot; has crashed against a wall of incomprehensible gibberish.&lt;/p&gt;&#10;&#10;&lt;p&gt;Does anyone know of an attempt to explain &lt;em&gt;to someone who is not a professional statistician&lt;/em&gt; what Fisher meant by &quot;fiducial&quot;?&lt;/p&gt;&#10;&#10;&lt;p&gt;P.S. I realize that Fisher was a bit of a moving target when it came to pinning down what he meant by &quot;fiducial&quot;, but I figure the term must have some &quot;constant core&quot; of meaning, otherwise it could not function (as it clearly does) as terminology that is generally understood within the field.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-06-08T16:49:31.880" FavoriteCount="3" Id="61231" LastActivityDate="2014-07-28T00:23:37.350" LastEditDate="2013-07-21T18:26:17.267" LastEditorUserId="7290" OwnerUserId="4769" PostTypeId="1" Score="12" Tags="&lt;bayesian&gt;&lt;inference&gt;&lt;terminology&gt;&lt;fiducial&gt;&lt;ronald-fisher&gt;" Title="What does &quot;fiducial&quot; mean (in the context of statistics)?" ViewCount="250" />
  <row AcceptedAnswerId="61244" AnswerCount="1" Body="&lt;p&gt;I am reading a statistical procedure trying to figure out and understand what's going on.  &lt;/p&gt;&#10;&#10;&lt;p&gt;The statement says &quot;Compute the posterior on $\mu$.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;Does this mean compute $p(\mu)$?   Does this mean &quot;Compute the posterior probability of $\mu$.&quot; (see how &quot;of&quot; replaces &quot;on&quot;)?&lt;/p&gt;&#10;&#10;&lt;p&gt;Immediately before in the reading, $\mu$ is allowed to take on multiple values.&lt;/p&gt;&#10;&#10;&lt;p&gt;Maybe there is a statistical wording/vocabulary convention I'm unaware of that would make this make more sense to me?&lt;/p&gt;&#10;&#10;&lt;p&gt;This is not critical, but I am just trying to understand better. If anyone could help me or point me to a resource that would be great.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-08T19:20:11.967" Id="61237" LastActivityDate="2013-06-08T20:41:38.747" LastEditDate="2013-06-08T20:41:38.747" LastEditorUserId="7290" OwnerUserId="26651" PostTypeId="1" Score="2" Tags="&lt;probability&gt;&lt;self-study&gt;&lt;bayesian&gt;&lt;posterior&gt;&lt;resources&gt;" Title="Statistics wording question" ViewCount="80" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm looking for theoretical upper bounds of classification accuracy. Please let me know if you are familiar with results like the following. &lt;strong&gt;The setup below is a general one, but please share results you know that apply in particular settings as well. Thanks!&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $Y$ be the binary variable to classify and $X = (X_1,...,X_K)$ be $K$ explanatory variables. We know the joint distribution (in population) of $(y,X) \sim \mu$ and want to build a model of $\hat{y} = f(X)$. Then for the class of XXX functions $f()$, there will be an upper bound to the maximum classification accuracy: $P(\hat{y} = y) \le g(\mu(y,X))$? &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-06-08T21:53:00.813" Id="61247" LastActivityDate="2013-06-12T14:13:42.263" LastEditDate="2013-06-12T14:13:42.263" LastEditorUserId="88" OwnerUserId="26657" PostTypeId="1" Score="1" Tags="&lt;classification&gt;&lt;bounds&gt;" Title="Theoretical upper bounds of classification accuracy?" ViewCount="81" />
  <row AnswerCount="1" Body="&lt;p&gt;For an assignment I am asked to test whether different items (all dichotomous) can actually measure one concept. The items: &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Please tell me whether or not you think it should be possible for a  pregnant woman to obtain a legal abortion . . . &#10;A. If the woman's own health is seriously endangered by the pregnancy? &#10;B. If there is a strong chance of serious defect in the baby? &#10;C. If she became pregnant as the result of a rape?&#10;D. If the family has a very low income and cannot afford any more children? &#10;E. If she is not married   and does not want to marry the man?&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The results of my &lt;strong&gt;latent-class analysis&lt;/strong&gt; are that:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;The model with two and three classes do not have an acceptable model fit:&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;blockquote&gt;&#10;&lt;pre&gt;&lt;code&gt;2-class: X² = 492.485; L² = 337.1439; df = 20; p = .000; AIC = 297.1439; BIC = 195.7409&#10;3-class: X² = 170.096; L² = 26.4617; df = 14; p = .0226; AIC = -1.5383; BIC = -72.5204&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;There is a clear pattern in the estimated classes. One group agreeing on all items X1, one agreeing on only the medical/ethical reasons (health, defect and rape) , one never agreeing X3 &lt;em&gt;(exept sometimes for the health item, I think because this is a balance between killing the baby or killing the mother, so both answers are not pro-life).&lt;/em&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/3s6WK.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;X. are the three classes, the first horizontal line of numbers are the proportions of the sample in each class, and then the probabilities of those classes to say yes (=1) or no (=2) to the different item. &lt;/p&gt;&#10;&#10;&lt;p&gt;I was wondering if the simple answer is: the items can not measure one construct because the model does not have an acceptable fit.&lt;/p&gt;&#10;&#10;&lt;p&gt;Or is the structure of the latent classes more informative to answer this question? I know that putting constraints on the model could lead to an acceptable model fit, so in that way my criterium to base my statement on seems flawed. (something like: they can not measure one construct, but when we put constraint on the model, they can)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-09T00:34:33.807" Id="61250" LastActivityDate="2013-06-09T16:34:46.810" LastEditDate="2013-06-09T16:34:46.810" LastEditorUserId="18334" OwnerUserId="18334" PostTypeId="1" Score="2" Tags="&lt;self-study&gt;&lt;categorical-data&gt;&lt;latent-class&gt;" Title="Can the items measure one underlying construct?" ViewCount="56" />
  <row Body="&lt;p&gt;There is more than one F-measure around in the sense that it is computed on different data.&lt;/p&gt;&#10;&#10;&lt;p&gt;For evaluating cluster analysis, it seems to be most common to compute the F-measure on &lt;strong&gt;pairs of objects&lt;/strong&gt;, and on the complete data set, not on single clusters.&lt;/p&gt;&#10;&#10;&lt;p&gt;See for example: &lt;a href=&quot;http://stackoverflow.com/questions/12725263/computing-f-measure-for-clustering&quot;&gt;http://stackoverflow.com/questions/12725263/computing-f-measure-for-clustering&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-06-09T10:30:04.597" Id="61271" LastActivityDate="2013-06-09T10:30:04.597" OwnerUserId="7828" ParentId="61248" PostTypeId="2" Score="0" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I'm conducting the analysis on readmission rate for a hospital unit between 2009 and 2012. I have a list of &quot;historical&quot; data where patients did not have the intervention. I then have a different set of patients who did get the intervention.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is, is it enough to consider the historical dataset as a comparison group in the analysis? Ideally, I'd want a similar (non-equivalent) comparison group who did not get the intervention and followed after initiation of the intervention, but I don't believe this is available (from speaking with the data managers). Also having multiple baseline groups would increase my confidence in the intervention effecting the change in readmission. If I don't have this data, is it still ok to use the single baseline group?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for your help.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-06-09T19:05:22.207" Id="61301" LastActivityDate="2013-06-10T12:09:50.207" LastEditDate="2013-06-10T12:09:50.207" LastEditorUserId="21919" OwnerUserId="21919" PostTypeId="1" Score="1" Tags="&lt;time-series&gt;" Title="Interrupted time series control group" ViewCount="59" />
  <row Body="&lt;p&gt;Apologize in advance for a less-than-definitive answer, but hopefully this answer will be helpful.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the totally general case, (i.e. arbitrary Y,X) I would conjecture that no such bound exists, except for the trivial answer of perfect classification. &lt;/p&gt;&#10;&#10;&lt;p&gt;The reason I make this conjecture is the similar issues addressed by the &lt;a href=&quot;http://en.wikipedia.org/wiki/No_free_lunch_in_search_and_optimization&quot; rel=&quot;nofollow&quot;&gt;no free lunch theorem&lt;/a&gt; .&lt;/p&gt;&#10;&#10;&lt;p&gt;I think your question is more likely to be about a specific instance (i.e. with a specific, known data set)&lt;/p&gt;&#10;&#10;&lt;p&gt;In this case, I can suggest an approach... although this approach may or may not be useful in practice.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am assuming that the values of X are all finite and discrete, or can be made so without loss of discrimination power.&lt;/p&gt;&#10;&#10;&lt;p&gt;Consider the set of all your observations, partitioned into a grid consisting of X(k,l) where l is the levels of each variable Xk.&lt;/p&gt;&#10;&#10;&lt;p&gt;Take the sum of the minority values of y (the binary value you are predicting) in each cell, and divide by n.&lt;/p&gt;&#10;&#10;&lt;p&gt;Since, in a given cell the best possible prediction is the ratio of majority/minority observations in that cell.  No better prediction is possible for that cell, because, by construction, the cell comprises of a unique combinations of X values.   No additional information is available from X to discriminate further.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-06-09T20:07:19.250" Id="61304" LastActivityDate="2013-06-09T20:07:19.250" OwnerUserId="26087" ParentId="61247" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;When it says &quot;follows directly from the definition&quot;, it means directly from the definition of $\langle -,- \rangle$, not directly from the definition of $\Phi$. Equation 2.24 on page 33 is a definition (that's why it uses the := notation instead of just an equals sign.) The definition says that if $f = \sum_i \alpha_i k(.,x_i)$ and $g = \sum_j \beta_j k(.,y_j)$ then $\langle f,g\rangle$ is &lt;em&gt;defined to be&lt;/em&gt; $\sum_{i,j} \alpha_i \beta_j k(x_i, y_j)$. In particular, if $f=k(.,x)$ and $g=k(.,x')$ then the definiton in Equation 2.24 says that&#10;$$\langle f, g \rangle = k(x,x')$$&#10;(think of one $\alpha_i$ and one $\beta_j$ being $1$ and the rest $0$). This is Equation 2.30.&lt;/p&gt;&#10;&#10;&lt;p&gt;Equation 2.29 is similar but more general. Suppose we have some $f = \sum_j \beta_j k(.,x_j)$. Then&#10;$$\langle k(,.x), f \rangle = \langle k(.,x), \sum\beta_j k(.,x_j) \rangle$$ &#10;and again Definition 2.24 says that this equals&#10;$$\sum \beta_j k(x, x_j)$$&#10;&lt;em&gt;by definition&lt;/em&gt;. But this is just $f(x)$, so that gives (2.29).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-10T02:01:49.800" Id="61318" LastActivityDate="2013-06-10T02:01:49.800" OwnerUserId="13818" ParentId="61201" PostTypeId="2" Score="3" />
  
  
  
  <row AcceptedAnswerId="61375" AnswerCount="1" Body="&lt;p&gt;I plotted density function in R and under the plot is number of bandwith. What does this number mean?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-06-10T18:17:10.667" Id="61374" LastActivityDate="2013-06-10T18:24:37.653" LastEditDate="2013-06-10T18:20:20.957" LastEditorUserId="8077" OwnerUserId="26710" PostTypeId="1" Score="3" Tags="&lt;r&gt;" Title="What does bandwidth mean?" ViewCount="715" />
  <row Body="The act of generating a sequence of numbers or symbols randomly, or (more often) pseudo-randomly; i.e., with lack of any predictability or pattern." CommentCount="0" CreationDate="2013-06-10T20:26:41.917" Id="61386" LastActivityDate="2013-06-11T01:10:07.557" LastEditDate="2013-06-11T01:10:07.557" LastEditorUserId="805" OwnerUserId="22468" PostTypeId="4" Score="0" />
  
  <row AnswerCount="1" Body="&lt;p&gt;My study has 6 continuous predictor variables and a dichotomous criterion variable.  The IRB wants me to provide a power analysis, which I take to mean how I decided on number of participants to recruit.  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;How do I use a power analysis to compute minimum number of participants needed?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;All of the calculators (including G*Power 3) require only one predictor variable, so I'm lost.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-06-11T01:46:34.967" Id="61412" LastActivityDate="2013-06-11T07:29:31.353" LastEditDate="2013-06-11T07:29:31.353" LastEditorUserId="183" OwnerUserId="26717" PostTypeId="1" Score="2" Tags="&lt;logistic&gt;&lt;sample-size&gt;&lt;power&gt;" Title="Power analysis for minimum sample size for 6 continuous predictors and 1 dichotomous criterion" ViewCount="168" />
  
  
  
  <row Body="&lt;p&gt;Not necessarily. &lt;/p&gt;&#10;&#10;&lt;p&gt;Or, perhaps a better answer is, &quot;it depends on what you mean by causality&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;It is presumed, in the latent factor model, that the individual items measure the latent factor; that's really the point. So, on, e.g. the MMPI, the different questions are supposed to be measuring aspects of personality, and the purpose of factor analysis is to uncover those latent factors. &lt;/p&gt;&#10;&#10;&lt;p&gt;But does this mean that the personality factors &quot;cause&quot; the answers to the question?&lt;/p&gt;&#10;&#10;&lt;p&gt;The answer is philosophy, not statistics. &lt;/p&gt;&#10;&#10;&lt;p&gt;(&lt;em&gt;StasK's edits:&lt;/em&gt;) To put it differently, there is nothing magic about latent variable models that immediately let you jump to the causality conclusions. It's just an extension of regression analysis to unobserved variables, that's all. If you trust regression with observational data to draw causal conclusions, you can likewise trust latent variable models. This may be discipline-specific, as some disciplines (biostat) only accept randomized experiments as the golden standard for establishing causality, while others (education research) may be happy with something less golden and less standard.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-11T11:59:16.547" Id="61437" LastActivityDate="2013-06-11T15:48:06.807" LastEditDate="2013-06-11T15:48:06.807" LastEditorUserId="5739" OwnerUserId="686" ParentId="61436" PostTypeId="2" Score="7" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I want to be able to generate a covariance matrix of dimensions $D$ x $D$, such that certain specified cells of this matrix contain a fixed predetermined values (at least approximately).&lt;/p&gt;&#10;&#10;&lt;p&gt;For e.g. For matrix, &#10;$S$ = $$&#10;\begin{matrix}&#10;a_{11} &amp;amp;  a_{12}  &amp;amp; \ldots &amp;amp; a_{1D}\\&#10;a_{21}  &amp;amp;  a_{22} &amp;amp; \ldots &amp;amp; a_{2D}\\&#10;\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\&#10;a_{D1}  &amp;amp;   a_{D2}       &amp;amp;\ldots &amp;amp; a_{DD}&#10;\end{matrix}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to make sure that certain $a_{i,j}$ 's have a predetermined value. &lt;/p&gt;&#10;&#10;&lt;p&gt;(If I were generating a covariance matrix without this constraint, I would just use a Wishart Random Generator. In Matlab, it would be something like - &#10;&lt;code&gt;W = wishrnd([1 0.5; 0.5 3],30)/30&lt;/code&gt;)&lt;/p&gt;&#10;&#10;&lt;p&gt;One way I can think of this problem is that different cells of the covariance matrix have different degrees of freedom. So that the cells with fixed values can be assumed to have infinite degrees of freedom and the rest as some finite number.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2013-06-11T14:41:12.773" FavoriteCount="0" Id="61451" LastActivityDate="2013-06-11T22:43:12.333" LastEditDate="2013-06-11T18:29:33.200" LastEditorUserId="12786" OwnerUserId="12786" PostTypeId="1" Score="7" Tags="&lt;correlation&gt;&lt;covariance&gt;&lt;random-generation&gt;&lt;wishart&gt;" Title="Generate covariance matrix with fixed values in certain cells" ViewCount="160" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Suppose I have two sets of Pearson correlation coefficients -- call them set A &amp;amp; set B, and they are of the same size. How do I systematically compare the correlations in A against B? E.g., I want to test the hypothesis that A is less than B -- something similar to a two-sample t-test, but the problem here is the two samples are two samples of correlation coefficients.&lt;/p&gt;&#10;&#10;&lt;p&gt;I've done some research and found out that there is Fisher's z-transform for this purpose. But it only tests the difference between one correlation $\rho_1$ against another $\rho_2$. I couldn't find a way to systematically test the difference between to sets of correlations. Is there such a way? Or I can only compare each pair of correlations using Fisher's method and somehow derive the difference, if any?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-06-11T15:20:07.900" FavoriteCount="2" Id="61456" LastActivityDate="2013-06-11T15:20:07.900" OwnerUserId="23783" PostTypeId="1" Score="1" Tags="&lt;hypothesis-testing&gt;&lt;correlation&gt;&lt;fisher&gt;&lt;pearson&gt;" Title="Test of Difference between 2 Sets of Pearson Correlations" ViewCount="159" />
  
  
  
  
  <row Body="&lt;p&gt;An example of your syntax would be helpful. &lt;/p&gt;&#10;&#10;&lt;p&gt;GLMs require both a family (or what I call a variance family) and a link function to define the Fisher Scoring algorithm that solves for your parameter estimates. With Poisson variance and log link, this is Poisson regression or regression, attained with the argument &lt;code&gt;family=poisson&lt;/code&gt; to &lt;code&gt;glm&lt;/code&gt;. However, use the following argument &lt;code&gt;family=binomial(link=&quot;log&quot;)&lt;/code&gt; and you get relative risk regression. &lt;/p&gt;&#10;&#10;&lt;p&gt;Most families in the &lt;code&gt;R&lt;/code&gt; GLM function allow you to specify &lt;code&gt;link=&quot;log&quot;&lt;/code&gt; as an optional argument to the family object (e.g. &lt;code&gt;gaussian&lt;/code&gt;, &lt;code&gt;gamma&lt;/code&gt;, &lt;code&gt;poisson&lt;/code&gt;). Irregular GLMs, especially ones for which the range of the link function is bigger than that of the fitted mean in the variance, have bizarre constrictions imposed on the parameter space which Fisher Scoring cannot accommodate.&lt;/p&gt;&#10;&#10;&lt;p&gt;Using the &lt;code&gt;traceback()&lt;/code&gt; function is always useful with errors like these. You can also find the iteration where the algorithm diverges by specifying &lt;code&gt;glm.control=list(maxit=1)&lt;/code&gt; for a 1 step estimator, &lt;code&gt;glm.control=list(maxit=2)&lt;/code&gt; for a 2 step, and so on and so forth. Plotting your $\beta^{(i)}$ estimates for the (i)-th iteration will help you see what's happening before the Hessian becomes singular, Fisher Scoring diverges, and R explodes.&lt;/p&gt;&#10;&#10;&lt;p&gt;Your problem with starting values may be because you're supplying the means of the response variables when the variables are &lt;em&gt;contrasts&lt;/em&gt; between the &lt;em&gt;logs&lt;/em&gt; of the means. Hence ratios of logs of averages would be a better starting place. Personally, if this were such an issue, I'd fit a regular GLM to make sure the algorithm isn't universally divergent and use the parameter estimates for that model to start another.&lt;/p&gt;&#10;&#10;&lt;p&gt;For instance: Feeding forward logistic regression estimates (ORs) to obtain risk ratios (RRs)&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;## retrospective incidence of something nasty&#10;data &amp;lt;- data.frame(cases=rpois(10, 10), controls=rpois(10, 1000), age=factor(seq(10), labels=c('0-10', '10-20', '20-30', '30-40', '40-50', '50-60', '60-70', '70-80', '80-90', '90-100')))&#10;&#10;## logistic regression&#10;fit &amp;lt;- glm(cbind(cases, controls) ~ age, data=data, family=binomial)&#10;&#10;## relative risk regression&#10;fit2 &amp;lt;- glm(cbind(cases, controls) ~ age, data=data, family=binomial(link='log'), start=coef(fit))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="11" CreationDate="2013-06-11T20:02:02.097" Id="61482" LastActivityDate="2013-06-11T20:02:02.097" OwnerUserId="8013" ParentId="61480" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Here you can find a simple correlation clustering algorithm.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Correlation_clustering#Algorithms&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Correlation_clustering#Algorithms&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Even though the algorithm is very easy to implement, if you want, I can give you the python implementation.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-06-11T20:48:09.030" Id="61490" LastActivityDate="2013-06-11T20:48:09.030" OwnerUserId="26762" ParentId="57332" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;I think the difference primarily is a philosophical one when choosing Rasch/1PL models (the emphases on what measurement means is slightly different in that literature, and hence researchers try their best to obtain these special items), and an empirical/design one when deciding between using 2PL and 3PL models. &lt;/p&gt;&#10;&#10;&lt;p&gt;Since the slopes are all equal in 1PL models determining a persons location amounts to finding the optimal location where respondents have a P = 0.5 chance of answering correctly by simply choosing items with the best intercepts to get an estimate of $\theta$, whereas in 2- and 3PL models it's slightly more complicated due to the unequal slopes and lower bound parameters for guessing. As a consequence, 2-3PL models often require more advanced adaptive item selection procedures such as the Kullback–Leibler/Fisher information to select the next best item for honing in on $\theta$.   &lt;/p&gt;&#10;&#10;&lt;p&gt;Speaking purely from a design perspective if the adaptive testing items contain a finite number of responses then the 3PL seems like the better option, but if it's more of a fill in the blank style answer (e.g., 2 + 3 = &lt;em&gt;_&lt;/em&gt;_.) then the 1PL and 2PL models would, at least theoretically, be more reasonable. &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-06-11T20:58:25.713" Id="61492" LastActivityDate="2013-06-11T20:58:25.713" OwnerUserId="18152" ParentId="61347" PostTypeId="2" Score="3" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm new to Empirical Likelihood Estimation.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I'm trying to find an example of how to find the empirical likelihood estimate of a univariate mean $\mu$ using the &lt;code&gt;emplik&lt;/code&gt; software package in &lt;code&gt;R&lt;/code&gt;.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Any help or reference is greatly appreciated.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-12T01:05:45.880" Id="61507" LastActivityDate="2013-08-15T18:54:25.537" LastEditDate="2013-06-12T08:19:32.100" LastEditorUserId="21054" OwnerUserId="26771" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;likelihood&gt;&lt;empirical&gt;" Title="Empirical Likelihood estimation in R" ViewCount="246" />
  
  <row Body="&lt;p&gt;I think the problem is that you are generating the weights at random, uncorrelated with the y value.  In a real weighted regression the points with lower variance will have higher weights.  Since the true relationship is mean and variance of 0 that means that points furthest from 0 would be consistent with higher variances and therefore lower weights, but you don't given them lower weights, they get random weights which could be high or low giving some more extreme values than expected.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you do the simulation more realistically by generating a set of weights, then generate Y with variances based on the weights, then analyze (you could use the same set of x's, or randomly generate the x's as well), I would expect the t-values to behave more properly.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is a quick example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;tstats &amp;lt;- replicate(1000, { x &amp;lt;- rnorm(N); w &amp;lt;- abs(rnorm(100,1)); &#10;    y &amp;lt;- rnorm(100, 0, sqrt(1/w));&#10;    coef(summary(lm(y~x, weights=w)))[2,3]})&#10;mean(abs(tstats)&amp;gt;2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I saw just under 5% as expected.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-06-12T02:47:31.493" Id="61511" LastActivityDate="2013-06-12T02:47:31.493" OwnerUserId="4505" ParentId="61469" PostTypeId="2" Score="6" />
  <row AnswerCount="1" Body="&lt;p&gt;I have a 70K x 30 dataset and I want to build a regression model on it. Right now, I am running a bunch of algorithms via Weka tool with cross-validation and I compare the RMSE values reported by Weka in order to decide which model works better.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, after I experiment with Multi layer perceptron, Linear Regression and a bunch of tree-related algorithms, the best performance I got was K-NN algorithm. Since this algorithm is very naive and instance based, I am not sure if just comparing RMSE is the right way. &lt;/p&gt;&#10;&#10;&lt;p&gt;When experimenting a Regression model, what kind of process should I follow? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-12T03:27:31.010" Id="61513" LastActivityDate="2013-06-12T03:59:53.413" LastEditDate="2013-06-12T03:54:35.007" LastEditorUserId="7290" OwnerUserId="26774" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;machine-learning&gt;" Title="Is it reasonable to compare a regression model with machine learing algorithms using RMSE?" ViewCount="183" />
  <row Body="&lt;p&gt;I would suggest exploring some common data transformations such as square-root and log transforms. Examine the normality of each and remember which ones you have used when interpreting your coefficients. In terms of outliers, there are quite a few approaches to how to handle them. One thing you can always try when there are a few outliers is run the regression with and without them...and compare the results. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-12T05:28:08.747" Id="61519" LastActivityDate="2013-06-12T05:28:08.747" OwnerUserId="21260" ParentId="61514" PostTypeId="2" Score="0" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I'd like to know if there is a method (in R) to classify curves based mainly on their appearance ie one cluster will represent a specific form of curves.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-06-12T11:53:56.367" Id="61541" LastActivityDate="2013-06-12T12:56:51.850" OwnerUserId="26791" PostTypeId="1" Score="-2" Tags="&lt;clustering&gt;&lt;curves&gt;" Title="Unsupervised classification of curves (in R)" ViewCount="109" />
  <row Body="&lt;p&gt;Thirst of all I would recommend to look for a better method, or at least a method with more in-depth description, since the &quot;Distributed Markov chain Monte Carlo&quot; from the paper that you are refering doesn't seem to be clearly stated. The advantages and disadvantages are not well explored. There is a method, that showed up in arxiv quite recently called &quot;&lt;a href=&quot;http://arxiv.org/pdf/1306.0063v1.pdf&quot; rel=&quot;nofollow&quot;&gt;Wormhole Hamiltonian Monte Carlo&lt;/a&gt;&quot;, I would recommend to check it.&lt;/p&gt;&#10;&#10;&lt;p&gt;Going back to the paper that you gave reference,  the remote proposal $R_{i}(\theta_{i})$ is very vaguely discribed. In the application part it is described as &quot;maximum likelihood Gaussian over the preceeding t/2 samples&quot;. Maybe this means that you average the last t/2 values of the $i^{th}$ chain? A bit hard to guess with the poor description given in the reference.&lt;/p&gt;&#10;&#10;&lt;p&gt;[UPDATE:] Interaction between several chains and the application of this idea to sample from posterior distribution can be found in parallel MCMC methods, for example &lt;a href=&quot;http://arxiv.org/pdf/1210.7477v1.pdf&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;. However, running several chains and forcing them to interact may not fit for the multimodal posterior: for example, if there is a very pronounced region where most of the posterior distribution is concentrated the interaction of the chains may even worsten things by sticking to that specific region and not exploring other, less pronounced, regions/modes. So, I would strongly recommend to look for MCMC specifically designed for multimodal problems. And if you want to create another/new method, then after you know what is available in the &quot;market&quot;, you can create more efficient method.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-06-12T13:13:51.123" Id="61545" LastActivityDate="2013-06-14T09:02:21.123" LastEditDate="2013-06-14T09:02:21.123" LastEditorUserId="9343" OwnerUserId="9343" ParentId="61169" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have been trying to implement a Bayesian inference procedure from scratch for a specific problem, but I have implemented the procedure, and it doesn't seem to work. &lt;/p&gt;&#10;&#10;&lt;p&gt;Since, I can't just post the code online and ask community to debug my code, I was wondering if someone could provide with a broader checklist when going about coding up a Bayesian inference procedure. (regardless of language) &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT: Specifics of the problem&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I am trying to implement the procedure described in Section 5 of &lt;a href=&quot;http://uai.sis.pitt.edu/papers/11/p736-wilson.pdf&quot; rel=&quot;nofollow&quot;&gt;this paper&lt;/a&gt; on &lt;strong&gt;MATLAB&lt;/strong&gt; . Briefly put, the procedure I've implemented is - &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;I have 3 zero mean variables (i.e., $D = 3$ time series) for $500$ timepoints. I'm using initial $N = 350$ data points as training sample.&lt;/li&gt;&#10;&lt;li&gt;The covariance function I'm using is a squared exponential kernel with 1 hyperparameter - characteristic length scale $l$. I'm assuming it to be the same for all 3 timeseries.&lt;/li&gt;&#10;&lt;li&gt;I'm keeping degrees of freedom constant, $\nu = D + 1$.&lt;/li&gt;&#10;&lt;li&gt;$L$, the lower Cholesky decomposition of the scale matrix $V$ is computed as the $D \times D$ covariance matrix of the $N \times D$ training dataset.&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The sampling procedure essentially involves 2 steps (using Gibbs sampling)&lt;/p&gt;&#10;&#10;&lt;p&gt;5.1 Sample $u$ ($N \times D \times \nu$) dimensional vector, assuming Gaussian process prior (as defined in equation 19 of the paper). I've assumed a Gaussian likelihood function (as defined in equation 24). For this I'm using &lt;a href=&quot;http://homepages.inf.ed.ac.uk/imurray2/pub/10ess/elliptical_slice.m&quot; rel=&quot;nofollow&quot;&gt;Elliptical Slice Sampling&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;5.2 Sample GP hyperparameter $l$, using a lognormal prior (assumption, $mean=1.5$, $var = 1$). I've used &lt;a href=&quot;http://homepages.inf.ed.ac.uk/imurray2/teaching/09mlss/slice_sample.m&quot; rel=&quot;nofollow&quot;&gt;slice sampling for this&lt;/a&gt; with posterior as product of GP prior(eq. 19) and lognormal density.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I let this Gibbs sampler run for $10000$ iterations ($5000$ burn-in). But convergence plot of $u$ doesn't seem to converge. &lt;/p&gt;&#10;&#10;&lt;p&gt;I also tried this with smaller $N$ (~ $50$) and increased no. of iterations but didn't work.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-06-12T18:47:03.700" Id="61580" LastActivityDate="2013-07-23T16:59:20.833" LastEditDate="2013-07-23T16:59:20.833" LastEditorUserId="12786" OwnerUserId="12786" PostTypeId="1" Score="2" Tags="&lt;bayesian&gt;&lt;inference&gt;&lt;nonparametric-bayes&gt;" Title="Points to keep in mind while implementing a nonparametric bayesian inference procedure from scratch" ViewCount="117" />
  <row Body="&lt;p&gt;In a basic mixed effects model, &lt;/p&gt;&#10;&#10;&lt;p&gt;$$E[Y_{it}|X_{it}] = \alpha + \sigma_i^2 + \beta X_{it}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;clusters having just one observation contribute influence to both the estimated variance of the random effect and the slope of the fixed effect. This is because the random intercept is never actually estimated. While some numerical solvers produce estimates for random intercepts, they are actually post-hoc statistics calculated after joint estimation of the random effect variance and the fixed effect slope. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you fit mixed effects models with unbalanced designs, it's important to verify the normality of these estimates (this can be a strong and influential assumption when there are a small number of clusters). As an example, suppose I run a health care clinic and we're verifying the management of AIDS in subjects on antiretroviral therapies, such as effivirenz. If I combine prevalent cases at baseline &lt;em&gt;and&lt;/em&gt; incident cases during follow-up, my analysis is now sensitive to the distribution of incidence. For instance, suppose 70% of my cases were diagnosed two years ago, and have had successful management of disease while 30% of my cases are incident and have high viral loads before starting therapy. I now have an uneven bimodal distribution of random intercepts (viral load at &quot;visit 1&quot;) and my fixed effect is biased toward the null (when it's actually suggestive that it's effective in managing disease).&lt;/p&gt;&#10;&#10;&lt;p&gt;A GEE on the other hand makes no assumption about the distribution of random effects and is consistent for the population averaged effect estimate: $\beta_M$ (M for marginal) rather than $\beta_C$ (C for conditional). These models are related to one another, but on average $\beta_M \leq \beta_C$ yet tests of inference about $\beta_M$ can often be of higher power. &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2013-06-12T20:28:13.057" Id="61593" LastActivityDate="2013-06-12T20:28:13.057" OwnerUserId="8013" ParentId="61569" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;The biggest question here is whether the class of probability models defined by $f(\beta)$, $\beta \in \Omega_{\beta}$ is nested within the prob models $g(\beta, \theta)$, $\left\{ \beta, \theta \right\} \in \Omega_\beta \times \Omega_\theta$ such as in the case of $\mbox{Exp}(\theta)$ distributions with gamma or Weibull probability models.&lt;/p&gt;&#10;&#10;&lt;p&gt;If that's the case, you can use maximum likelihood to test the joint hypothesis for the maximum restricted likelihood where $\Omega_\theta$ is projected into the zero set. The negative two log likelihood ratio of the restricted and full MLEs has an asymptotic $\chi^2_1$ distribution when $\theta = 0$.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-06-12T21:48:10.887" Id="61597" LastActivityDate="2013-06-12T22:14:15.550" LastEditDate="2013-06-12T22:14:15.550" LastEditorUserId="22047" OwnerUserId="8013" ParentId="61588" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a 10,000 dimensioned dataset where all attributes are numeric values. I would like to select the best e.g. 50 attributes out of 10,000 so that I can run regression algorithms on it.&lt;/p&gt;&#10;&#10;&lt;p&gt;I've tried Weka's PCA and CfsSubsetEval algorithms, however they were not capable of handling that much dimension (algorithms never terminated in both case).&lt;/p&gt;&#10;&#10;&lt;p&gt;What kind of attribute selection algorithms (preferably with already implemented tools) exist in machine learning literature for such high dimensions?&lt;/p&gt;&#10;&#10;&lt;p&gt;Summary of what I am doing is: I have a log data where all attributes are numeric (10K attributes in total and 100,000 samples in total) and I want to predict the bandwidth out of it. So I want to select best attributes which can play a role in predicting this bandwidth attribute.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-13T00:00:16.930" Id="61607" LastActivityDate="2013-06-13T00:00:16.930" OwnerUserId="26619" PostTypeId="1" Score="1" Tags="&lt;regression&gt;&lt;machine-learning&gt;&lt;dimensionality-reduction&gt;" Title="What kind of attribute selection methods exist for high-dimensional regression data?" ViewCount="52" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Say I have $K$ independent normals, $X_i \sim \mathcal{N}(\mu_i, \sigma_i)$ for $i = 1,...,K$. How can I form an unbiased estimator of $\max_i \mu_i$ using $X_i$'s? &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-06-13T02:10:27.287" Id="61611" LastActivityDate="2013-06-19T14:10:16.947" LastEditDate="2013-06-19T14:10:16.947" LastEditorUserId="26657" OwnerUserId="26657" PostTypeId="1" Score="3" Tags="&lt;distributions&gt;&lt;extreme-value&gt;&lt;estimators&gt;" Title="Forming an unbiased estimator of the maximum of several parameters, given independent estimators of each parameter?" ViewCount="57" />
  <row AnswerCount="2" Body="&lt;p&gt;What are known upper bounds on how often the Euclidean norm of a uniformly chosen&#10;element of $\:\{-n,~-(n-1),~...,~n-1,~n\}^d\:$ will be larger than a given threshold?&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm mainly interested in bounds that converge exponentially to zero when $n$ is much less than $d$.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-06-13T02:34:18.163" FavoriteCount="1" Id="61612" LastActivityDate="2015-02-22T16:30:28.717" LastEditDate="2013-06-13T23:34:33.830" LastEditorUserId="26826" OwnerUserId="26826" PostTypeId="1" Score="10" Tags="&lt;uniform&gt;&lt;extreme-value&gt;&lt;bounds&gt;" Title="Tail bounds on Euclidean norm for uniform distribution on $\{-n,-(n-1),...,n-1,n\}^d$" ViewCount="194" />
  <row AnswerCount="2" Body="&lt;p&gt;I've just finished an animal experiment. I compared 1 control group and 1 experimental group, the only difference between the two is type of diet. For statistical analysis I used the independent groups t-test, and the result showed no significant differences between the two groups. However, the data shows the tendency that the experimental group has more benefit in all variables measured. So, what should I say about my data? All data are normally distributed.  &lt;/p&gt;&#10;&#10;&lt;p&gt;My supervisor said that maybe because I used very small sample (each group n=8) that I could not find any significant differences. He suggested me to do some &quot;probability test&quot; or something to extrapolate my data (unfortunately, I don't have any clue what he was talking about).  &lt;/p&gt;&#10;&#10;&lt;p&gt;So, is there any statistical analysis that I can use like what my supervisor told me to do?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2013-06-13T04:46:26.887" Id="61616" LastActivityDate="2013-11-10T17:01:01.183" LastEditDate="2013-06-13T05:13:29.993" LastEditorUserId="7290" OwnerUserId="26827" PostTypeId="1" Score="1" Tags="&lt;t-test&gt;&lt;power&gt;" Title="T-test shows no differences, but the experiment group shows tendency more benefit in all variables measured than control group" ViewCount="159" />
  
  
  <row AcceptedAnswerId="61632" AnswerCount="1" Body="&lt;p&gt;First of all, thank you for the great forum!&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a question concerning Generalized Linear Models (GLM).&#10;My dependent variable (DV) is continuous and not normal. So I log transformed it (still not normal but improved it).&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to relate the DV with two categorical variables and one continuous covariable. For this I want to conduct a GLM (I am using SPSS) but I am unsure how to decide on the distribution and function to choose.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have conducted Levene's nonparametric test and I have homogeneity of variances so I am inclined to use the normal distribution.&#10;I have read that for linear regression the data does not need to be normal, the residuals do. So, I have printed the standardized Pearson residuals and predicted values for linear predictor from each GLM individually (GLM normal identity function and normal log function). I have conducted normality tests (histogram and Shapiro-Wilk) and plotted residuals against predicted values (to check for randomness and variance) for both individually. Residuals from identity function are not normal but residuals from log function are normal. I am inclined to choose normal with log link function because the Pearson residuals are normally distributed.&lt;/p&gt;&#10;&#10;&lt;p&gt;So my questions are:&lt;br&gt;&#10;- Can I use GLM normal distribution with LOG link function on a DV that has already been log transformed?&lt;br&gt;&#10;- Is the variance homogeneity test sufficient to justify using normal distribution?&lt;br&gt;&#10;- Is the residual checking procedure correct to justify choosing the link function  model?  &lt;/p&gt;&#10;&#10;&lt;p&gt;Image of the DV distribution on the left and residuals from the GLM normal with log link function on the right.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/4BZnk.png&quot; alt=&quot;DV distribution on the left and residuals from the GLM normal on right&quot;&gt;&lt;/p&gt;&#10;" CommentCount="11" CreationDate="2013-06-13T08:13:16.053" Id="61626" LastActivityDate="2013-09-09T06:33:08.807" LastEditDate="2013-09-09T06:33:08.807" LastEditorUserId="805" OwnerUserId="26746" PostTypeId="1" Score="5" Tags="&lt;normal-distribution&gt;&lt;generalized-linear-model&gt;&lt;data-transformation&gt;&lt;residuals&gt;&lt;histogram&gt;" Title="I log transformed my dependent variable, can I use GLM normal distribution with LOG link function?" ViewCount="1915" />
  
  <row Body="&lt;p&gt;Fortunately they are the same. All we need to prove is $E_{\theta} \left[ \frac{\partial}{\partial \theta} \log f(x|\theta) \right] = 0$, so here we go.&lt;/p&gt;&#10;&#10;&lt;p&gt;First note that&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\int_{\mathbb{R}} \frac{\partial}{\partial \theta}\log(f(x|\theta)) \cdot f(x|\theta) dx = \int_{\mathbb{R}} \frac{\partial f(x|\theta)}{\partial \theta}dx. $$&lt;/p&gt;&#10;&#10;&lt;p&gt;Under the conditions of &lt;a href=&quot;http://en.wikipedia.org/wiki/Gottfried_Leibniz&quot; rel=&quot;nofollow&quot;&gt;Leibniz&lt;/a&gt;'s &lt;a href=&quot;http://en.wikipedia.org/wiki/Leibniz_integral_rule#Measure_theory_statement&quot; rel=&quot;nofollow&quot;&gt;integral rule&lt;/a&gt; you can rewrite the above&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \frac{\partial}{\partial \theta} \int_{\mathbb{R}} f(x|\theta)dx  = \frac{\partial}{\partial \theta} 1 = 0.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;So, except pathological cases where the conditions mentioned above do not hold, the expected value of the &lt;a href=&quot;http://en.wikipedia.org/wiki/Score_%28statistics%29&quot; rel=&quot;nofollow&quot;&gt;score&lt;/a&gt; is 0, and the covariance is the same as the expected value of the product.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-06-13T09:29:26.273" Id="61636" LastActivityDate="2013-06-13T20:00:56.577" LastEditDate="2013-06-13T20:00:56.577" LastEditorUserId="10849" OwnerUserId="10849" ParentId="61625" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;If you oversample the training data to change the relative class frequencies in the training set, you are implicitly telling the classifier to expect the validation set to have those same class frequencies.  As the class frequencies influence the decision boundary, if the validation set class frequencies are different, then the classification boundary will be wrong as a result and performance will be sub-optimal.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now it is true that for some classifiers, having imbalanced classes can cause it to perform badly, and re-sampling the data can help to correct this.  However if you oversample the minority class too much, you will over-correct and performance will be suboptimal because the the difference in class frequencies between the training and validation sets is more than is needed to correct the imbalance problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;My recipe then would be to use cross-validation to choose the degree of oversampling that is required to &lt;em&gt;just&lt;/em&gt; compensate for the &quot;class imbalance problem&quot; experienced by your classifier, but no more.  Note however this leads to an additional parameter that needs to be tuned, which leads to greater computational expense, and a higher likelihood of over-fitting in model selection (if the total number of tunable parameters is non-negligible). &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-06-13T11:03:44.367" Id="61643" LastActivityDate="2013-06-13T11:03:44.367" OwnerUserId="887" ParentId="61622" PostTypeId="2" Score="2" />
  <row AnswerCount="1" Body="&lt;p&gt;The general problem is that I have an calculated variable that represents agreement across parts. The variable is a summation in the form (part/count of parts)^2 for possible parts. The summation variable is between 0 and 1. How would I go about testing for significant differences between calculated part agreement for two study conditions?&lt;/p&gt;&#10;&#10;&lt;p&gt;The specific situation involves gestures that study participants used to accomplish a particular task. Here, agreement means the number of times a particular gesture was used over the total number of gestures for each task (squared as per above)--summed for all gestures per task. I would like to calculate whether or not agreement differs significantly per gesture across two groups (study compares two conditions) and am unsure how to approach this problem.   &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-13T11:06:07.940" Id="61644" LastActivityDate="2013-06-13T11:14:25.377" OwnerUserId="24725" PostTypeId="1" Score="1" Tags="&lt;statistical-significance&gt;" Title="Statistical significance between groups of a summation variable" ViewCount="65" />
  <row AnswerCount="0" Body="&lt;p&gt;I have survival data (coming from a labor market) looking like this&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;   gender  nation          profession year age event   survtime&#10;1       F Austria  blue-/white-collar 1997  30  TRUE   707 days&#10;2       M Austria          freelancer 2001  26  TRUE  1765 days&#10;3       F Austria marginal employment 2006  57  TRUE    57 days&#10;4       F Austria  blue-/white-collar 2011  33  TRUE   465 days&#10;5       F Austria  blue-/white-collar 1997  22  TRUE    35 days&#10;6       M Germany  blue-/white-collar 1996  38  TRUE   301 days&#10;7       M Austria  blue-/white-collar 2007  24 FALSE  2251 days&#10;8       F Germany  blue-/white-collar 2011  23  TRUE   172 days&#10;9       M Austria  blue-/white-collar 2004  35  TRUE   249 days&#10;10      M Austria          freelancer 2000  32  TRUE   366 days&#10;11      M Germany  blue-/white-collar 2008  22  TRUE    64 days&#10;12      F Austria  blue-/white-collar 1998  21  TRUE    56 days&#10;13      M Austria  blue-/white-collar 2012  36 FALSE   200 days&#10;14      F Austria  blue-/white-collar 2000  47  TRUE   841 days&#10;15      F Austria  blue-/white-collar 2008  27  TRUE   117 days&#10;16      F   other  blue-/white-collar 1971  24  TRUE 11324 days&#10;17      M Austria          freelancer 2001  34  TRUE   456 days&#10;18      M Austria  blue-/white-collar 2007  25  TRUE    31 days&#10;19      F Austria  blue-/white-collar 1992  25  TRUE  2010 days&#10;20      M   other  blue-/white-collar 2003  24  TRUE   347 days&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;(get it with &lt;code&gt;df &amp;lt;- source(&quot;http://pastebin.com/raw.php?i=WjQUatKy&quot;)$value&lt;/code&gt;)&lt;/p&gt;&#10;&#10;&lt;p&gt;I would expect the &lt;code&gt;year&lt;/code&gt;, which describes the year at which a labor contract started, to be an ancillary variable, meaning that I think that &lt;code&gt;survtime&lt;/code&gt; is changing with the economic conditions (like when there is a recession, survtime is maybe shorter).&lt;/p&gt;&#10;&#10;&lt;p&gt;How can I account for that in an extended Cox proportional hazard model? I use R and so far I have&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;df &amp;lt;- source(&quot;http://pastebin.com/raw.php?i=WjQUatKy&quot;)$value&#10;s &amp;lt;- Surv(as.numeric(df$survtime), df$event)&#10;cox &amp;lt;- coxph(s ~ gender + nation + strata(profession) + age + year, data=df)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;which however doesn't take this into account.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-13T11:53:43.527" Id="61650" LastActivityDate="2013-06-13T12:25:20.780" LastEditDate="2013-06-13T12:25:20.780" LastEditorUserId="6029" OwnerUserId="8595" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;survival&gt;&lt;cox-model&gt;" Title="Extended Cox-model with ancillary variable" ViewCount="84" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;From a Physics equation I have the following model:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$W=\beta_0+\beta_1Z$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$\beta_0$, $\beta_1$ are fixed values for which I want to find a $1-\alpha$ confidence region.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have $(z_1,w_1),(z_2,w_2),\ldots,(z_n,w_n)$ observations.&lt;/p&gt;&#10;&#10;&lt;p&gt;I get these observations from evaluating the function $f: \mathbf{R}^{l+k+2}\rightarrow \mathbf{R}^2$.&lt;/p&gt;&#10;&#10;&lt;p&gt;$$f\left(X_1,\ldots,X_l,Y_{1i},\ldots, Y_{ki},P_{1i},P_{2i}\right)=\left(f_1\left(\ldots\right),f_2\left(\ldots\right)\right)=(Z_i,W_i),$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $X_j\sim\mathcal{N}(\mu_j,\sigma^2)$ $Y_{ij}\sim\mathcal{N}(\theta_{ij},\sigma^2)$ $P_{ik}\sim\mathcal{N}(p_{ik},\frac{p_{ik}}{1000})$; $\sigma^2$ is known, but not $\mu_j,\theta_{ji},p_{ji}.$&lt;/p&gt;&#10;&#10;&lt;p&gt;$X,Y,P$ are measures from an electronic instrument.&lt;/p&gt;&#10;&#10;&lt;p&gt;The equation $w_i=\beta_0+\beta_1z_i$ holds when $\left(z_i,w_i\right)=f\left(\mu_1,\ldots,\mu_l,\theta_{1i},\ldots,\theta{ki},p_{1i},p_{2i}\right)$ ($\bf{not}$ when $\left(z_i,w_i\right)=\mathbf{E}(f(\ldots))$&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that there are both common $\left(X_j\right)$ and not common variables $\left(Y_{ji} , P_{ki}\right)$ for all observations. Still It happens that $Y_{ji}=Y_{st}$ and $P_{1i}=P_{2t}$ for some $j$ $i$ $s$ and $t$. (Meaning that variables $Y$ and $P$ are used for more than one observation, but not all)&lt;/p&gt;&#10;&#10;&lt;p&gt;$f_1$ and $f_2$ are rational functions. Both numerators and denominators are quadratic functions when restricted to one variable. They are well defined.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, &lt;strong&gt;what can i do to get confidence regions for $(\beta_0,\beta_1)$?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;A regression model and Maximum Likelihood techniques are tempting.&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{equation}&#10;\begin{pmatrix} w_1 \\ \vdots \\ w_n \end{pmatrix}&#10;=&#10;\begin{pmatrix} 1 &amp;amp; z_1 \\ \vdots \\ 1 &amp;amp; z_n \end{pmatrix}&#10;\begin{pmatrix} \beta_0 \\ \beta_1 \end{pmatrix}&#10;+&#10;\begin{pmatrix} \epsilon_1 \\ \vdots \\ \epsilon_n \end{pmatrix}&#10;\end{equation}&lt;/p&gt;&#10;&#10;&lt;p&gt;$\bf{\epsilon}=\bf{W-\beta_1Z-\beta_0}$   (Is this right?)&lt;/p&gt;&#10;&#10;&lt;p&gt;As $(Z,W)$ has a complicated joint distribution, so does $\bf{\epsilon}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;I can figure out what to do when each $(Z_i,W_i)\sim\mathcal{N_2}((z_i,w_i),\sum)$. But this is not the case.&lt;/p&gt;&#10;&#10;&lt;p&gt;I understand that there may not be an easy answer for these questions, but if anyone can give some general tips or point me to the right literature i can work from there.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for reading.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2013-06-13T14:05:29.453" Id="61668" LastActivityDate="2013-06-13T14:11:13.530" LastEditDate="2013-06-13T14:11:13.530" LastEditorUserId="930" OwnerUserId="21521" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;confidence-interval&gt;&lt;multivariate-analysis&gt;&lt;measurement-error&gt;" Title="Regression model where errors are not normally distributed" ViewCount="113" />
  <row Body="&lt;p&gt;Relevant Formula&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://upload.wikimedia.org/math/3/a/9/3a995705c12638ca89e78499b63fb568.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Source: &lt;a href=&quot;http://en.wikipedia.org/wiki/Standard_deviation&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Standard_deviation&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;For pay-off of 2, -1 and probabilities of 50%/50%.&lt;/p&gt;&#10;&#10;&lt;p&gt;Expected value = 0.50*2+.50*-1 = .50&lt;/p&gt;&#10;&#10;&lt;p&gt;Variance = 0.50*(2-.50)^2 + 0.50*(-1-.50)^2 = 2.25&lt;/p&gt;&#10;&#10;&lt;p&gt;SD = 2.25^(1/2) = 1.5&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;For pay-off of 3, -1 and probabilities of 50%/50%.&lt;/p&gt;&#10;&#10;&lt;p&gt;Expected value = 0.50*3+.50*-1 = 1&lt;/p&gt;&#10;&#10;&lt;p&gt;Variance = 0.50*(3-1)^2 + 0.50*(-1-1)^2 = 4&lt;/p&gt;&#10;&#10;&lt;p&gt;SD = 4^(1/2) = 2&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-13T15:23:23.713" Id="61674" LastActivityDate="2013-08-12T17:45:15.817" LastEditDate="2013-08-12T17:45:15.817" LastEditorUserId="582" OwnerUserId="26814" ParentId="61575" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;By considering the log of the time series, i.e. &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \{\log{S(t)}\}_{t=0}^{t=T}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;we have &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \log{S(0)} + (\mu - \delta^2/2)t + \delta w(t) $$ &lt;/p&gt;&#10;&#10;&lt;p&gt;( $w(0) = 0$ ). Taking first differences of this series, $\log{S_{t_i}} - \log{S_{t_{i-1}} }$, gives a new series:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ (\mu - \delta^2/2)( t_i - t_{i-1} ) + \delta( w(t_i) - w(t_{i-1} ) )$$&lt;/p&gt;&#10;&#10;&lt;p&gt;This new series is independent and Normally distributed, with mean $(\mu - \delta^2/2)( t_i - t_{i-1} )$ and standard deviation $\delta\sqrt{ t_i - t_{i-1} }$. One can use MLE to find the &quot;best&quot; estimators of the two unknowns. &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2013-06-13T18:00:18.960" Id="61683" LastActivityDate="2013-06-13T18:00:18.960" OwnerUserId="11867" ParentId="61682" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;If these means are percentages, you don't need the full stream. You can do a t-test with just proportion and N. There are lots of online calculators for this, or you can use &lt;code&gt;prop.test&lt;/code&gt; in &lt;code&gt;R&lt;/code&gt;. With full stream data you can use &lt;code&gt;t.test&lt;/code&gt; in &lt;code&gt;R&lt;/code&gt;. Or, you can make a table of successes/failures and use &lt;code&gt;chisq.test&lt;/code&gt;.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-06-13T18:12:39.657" Id="61686" LastActivityDate="2013-06-13T18:12:39.657" OwnerUserId="686" ParentId="61684" PostTypeId="2" Score="1" />
  <row AnswerCount="3" Body="&lt;p&gt;I just thought I'd ask this question to see if there is a method or reasoning of proving a hypothesis test inversely, whilst logic remains the same.&lt;/p&gt;&#10;&#10;&lt;p&gt;I will use the following example, which I recently got confused over:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Question: Test on a 5% level of significance if the model is&#10;  significant. State the null hypothesis, alternative hypothesis, the&#10;  p-value and conclusion.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;The correct answer: &#10;&lt;ul&gt;&#10;&lt;li&gt;H0(null): Model is not significant&lt;/li&gt;&#10;&lt;li&gt;H1(alt) Model is significant&lt;/li&gt;&#10;&lt;li&gt;p-value = 0.4 &gt; alpha (0.05)&lt;/li&gt;&#10;&lt;li&gt;Do not reject H0&lt;/li&gt;&#10;&lt;li&gt;model is not significant&lt;/li&gt;&#10;&lt;/ul&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I understand the logic of this answer, but I alternatively wrote an answer like this:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;My answer: &#10;&lt;ul&gt;&#10;&lt;li&gt;H0(null): Model is significant&lt;/li&gt;&#10;&lt;li&gt;H1(alt) Model is not significant&lt;/li&gt;&#10;&lt;li&gt;p-value = 0.4 &gt; alpha (0.05)&lt;/li&gt;&#10;&lt;li&gt;Reject H0&lt;/li&gt;&#10;&lt;li&gt;model is not significant&lt;/li&gt;&#10;&lt;/ul&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;After reviewing my error, I understand that we cannot reject H0 if the p-value is greater than the 5% level of significance, and this is where my question comes in.&lt;/p&gt;&#10;&#10;&lt;p&gt;How would it be possible to reject MY H0? Would I use an alternate value?&lt;/p&gt;&#10;&#10;&lt;p&gt;My own suggested answer to this (based on the 5% level of significance) is that if I have inverted everything, then I would invert my 5% level of significance and say that:&lt;/p&gt;&#10;&#10;&lt;p&gt;p-value = 0.4 &amp;lt; 0.95&#10;Therefore, reject H0 on a 5% level of significance.&lt;/p&gt;&#10;&#10;&lt;p&gt;My reasoning is deduced from the fact that (based on the 5% level of significance) it must hold that the p-value be greater than 0.95 to accept the null hypothesis.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can anyone confirm or reject this?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-06-13T18:16:58.630" FavoriteCount="1" Id="61688" LastActivityDate="2013-06-13T22:59:43.663" OwnerUserId="26867" PostTypeId="1" Score="3" Tags="&lt;hypothesis-testing&gt;&lt;p-value&gt;" Title="What would be the logical inverse of a hypothesis test? (If any)" ViewCount="516" />
  
  
  <row Body="&lt;p&gt;If you want to estimate the error of the model and its corresponding variability when predicting new observations, after step 3. After fitting all the trees. Here the model that is being validated is the whole ensemble of weak learners. But naturally you could tune the hyperparameters using CV too. For example the optimal number of boosted trees. In the package 'dismo' the function gbm.step does exactly this. Example of usage:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;brtTuning&amp;lt;- gbm.step(data=yourData,&#10;     gbm.x = 1:18,&#10;     gbm.y = 19,&#10;     family = &quot;gaussian&quot;,&#10;     tree.complexity = 5,&#10;     learning.rate = 0.005,&#10;     bag.fraction = 0.5)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;If you want to tune and then validate I beleive you need to do nested cross-validation.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-13T19:48:35.997" Id="61694" LastActivityDate="2013-06-13T19:48:35.997" OwnerUserId="11748" ParentId="52239" PostTypeId="2" Score="1" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I've seen a &lt;a href=&quot;https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;ved=0CDEQFjAA&amp;amp;url=http://repositorium.uni-osnabrueck.de/bitstream/urn%3anbn%3ade%3agbv:700-2008112111/2/E-Diss839_thesis.pdf&amp;amp;ei=RCm6UYW9CIeCjAKunIGYDw&amp;amp;usg=AFQjCNHTNmf0AAetC8N4hgJFErKcKoGM0g&amp;amp;sig2=5vQasbHtMzSBqB0UDAoJJg&amp;amp;bvm=bv.47883778,d.cGE&quot; rel=&quot;nofollow&quot;&gt;single paper&lt;/a&gt; on the topic of adapting fully recurrent networks to a reinforcement learning setting, but according to google scholar its had no citations and no code has been released implementing the algorithm it describes. Before I go out and roll my own implementation of this algorithm, I just wanted to check that a less obscure algorithm wasn't out there (hopefully with an open-source implementation).&lt;/p&gt;&#10;&#10;&lt;p&gt;If there isn't a more noteworthy algorithm of this sort, then a secondary question would be why not? Is the some research showing that recurrent architectures don't buy you anything when used for reinforcement learning? Perhaps extending temporal differencing to such an architecture results in intractable computational complexity?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-13T20:31:16.453" Id="61701" LastActivityDate="2013-06-13T20:31:16.453" OwnerUserId="10320" PostTypeId="1" Score="1" Tags="&lt;machine-learning&gt;&lt;neural-networks&gt;&lt;reinforcement-learning&gt;" Title="Any implementations of fully recurrent neural networks applied to reinforcement learning?" ViewCount="85" />
  
  <row Body="&lt;p&gt;A scatterplot and a 2d-histogram are two ways of presenting the same information. The second is basically taking the first and binning it to get an idea of the local density.&lt;/p&gt;&#10;&#10;&lt;p&gt;As such, you could fit a mixture to either kind of information (you don't fit it directly to the display, but to the data); if you fit to binned data rather than raw data obviously you'd lose some information, but it would still be quite possible to do so.&lt;/p&gt;&#10;&#10;&lt;p&gt;Since you ask which you &lt;em&gt;should&lt;/em&gt; do, it appears you have the ability to choose. Where feasible, unbinned data would give you more information than binned data, but other considerations can impact the choice.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-13T22:08:37.260" Id="61709" LastActivityDate="2013-06-16T05:36:06.850" LastEditDate="2013-06-16T05:36:06.850" LastEditorUserId="805" OwnerUserId="805" ParentId="61647" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;Your previous question related to &lt;em&gt;counts&lt;/em&gt;. A proportions test would be relevant to that.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now you're talking about &quot;&lt;em&gt;percentage of analyte&lt;/em&gt;&quot;. That doesn't sound like count data to me, but continuous &lt;em&gt;compositional data&lt;/em&gt;. The difference is critical.&lt;/p&gt;&#10;&#10;&lt;p&gt;You should clarify which kind of data this is.&lt;/p&gt;&#10;&#10;&lt;p&gt;If those means are continuous proportions rather than counts divided by counts, then you &lt;em&gt;shouldn't&lt;/em&gt; use proportions tests or chi-square or anything else designed for counts.&lt;/p&gt;&#10;&#10;&lt;p&gt;On the other hand, t-tests assume normality and constant variance; the first might not matter too much (though one sample is very small), and it's possible to adjust somewhat for the second. However, compositional data is usually modelled as beta (or Dirichlet, when there's more than one components of interest and more than two components in total). &lt;/p&gt;&#10;&#10;&lt;p&gt;Even worse, you seem to suggest you only have means and the total number of observations. Without standard deviations or the original data you can't compute a t-statistic.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-13T22:47:34.797" Id="61712" LastActivityDate="2013-06-13T22:47:34.797" OwnerUserId="805" ParentId="61684" PostTypeId="2" Score="0" />
  
  
  
  
  
  
  <row AcceptedAnswerId="61816" AnswerCount="2" Body="&lt;p&gt;I have two test groups that conducted an online task measuring response times (avg, avg(congruent), avg(incongruent)). I expected one group to be faster than the other but it turned out exactly the opposite way (significant). So now I'm trying to find one or more factors that are responsible for this unexpected outcome. &lt;/p&gt;&#10;&#10;&lt;p&gt;My test data looks like this:&#10;participants conducted a Simon task and I measured their response times for each trial. In SPSS I have 3 variables, one for their average response times over all trials, one with their average response time for the trials that were congruent (place on screen and direction match), one with their average response time for the trials that were incongruent (place on screen and direction do not match).&lt;/p&gt;&#10;&#10;&lt;p&gt;My groups are monolingual and multilingual.&lt;/p&gt;&#10;&#10;&lt;p&gt;Literature suggests that multilinguals would score either higher averages overall, or higher averages for just the incongruent trials. My outcomes show that the monolinguals have a higher overall average and that the difference is the same between congruent and incongruent trials for both groups.&lt;/p&gt;&#10;&#10;&lt;p&gt;For each group I know the following variables: &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;age&lt;/li&gt;&#10;&lt;li&gt;education level&lt;/li&gt;&#10;&lt;li&gt;accuracy%&lt;/li&gt;&#10;&lt;li&gt;sex&lt;/li&gt;&#10;&lt;li&gt;lurking variables (yes|no)&lt;/li&gt;&#10;&lt;li&gt;average response times (total, congruent trials, incongruent trials)&lt;/li&gt;&#10;&lt;li&gt;first language&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Each variable in itself seems to be significant, but none of the variables seem to have a significant influence in combination with the original groups.&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to ensure that my test was not faulty and verify the validity of the test method. Maybe one other factor I didn't account for in my hypothesis is causing these results. It would either help me substantiate that my test is correct and that the results are valid, or provide a new angle for future research.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm not sure what test to use. For my first comparisons I used Spearman's correlation. I was told once that I may not use partial correlations when using Spearman's correlation. &#10;Can anyone help me how to proceed from here?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2013-06-14T17:58:46.143" Id="61779" LastActivityDate="2013-06-15T10:10:16.437" LastEditDate="2013-06-15T09:41:41.363" LastEditorUserId="26907" OwnerUserId="26907" PostTypeId="1" Score="2" Tags="&lt;correlation&gt;&lt;spss&gt;&lt;partial&gt;" Title="Is partial correlation possible after Spearman's correlation?" ViewCount="624" />
  
  <row Body="&lt;p&gt;It's not true for all tests in statistics. Some non-parametric tests have reverse decisions.&lt;/p&gt;&#10;&#10;&lt;p&gt;Usually it is said that if the calculated value of a test statistic is greater than the tabulated value then we fail to reject $H_0$.&lt;/p&gt;&#10;&#10;&lt;p&gt;It is better to have a decision on the basis of the p-value. If the p-value is less than the chosen level of significance then we fail to reject $H_0$.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2013-06-15T04:01:29.323" Id="61800" LastActivityDate="2013-06-15T08:21:11.950" LastEditDate="2013-06-15T08:21:11.950" LastEditorUserId="22047" OwnerUserId="26919" ParentId="61028" PostTypeId="2" Score="0" />
  
