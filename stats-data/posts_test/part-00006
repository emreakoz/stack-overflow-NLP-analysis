  <row AcceptedAnswerId="1201" AnswerCount="1" Body="&lt;p&gt;Is there a good, modern treatment covering the various methods of multivariate interpolation, including which methodologies are typically best for particular types of problems? I'm interested in a solid statistical treatment including error estimates under various model assumptions.&lt;/p&gt;&#10;&#10;&lt;p&gt;An example:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Inverse_distance_weighting&quot; rel=&quot;nofollow&quot;&gt;Shepard's method&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Say we're sampling from a multivariate normal distribution with unknown parameters. What can we say about the standard error of the interpolated estimates?&lt;/p&gt;&#10;&#10;&lt;p&gt;I was hoping for a pointer to a general survey addressing similar questions for the various types of multivariate interpolations in common use. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-07-19T19:18:30.810" FavoriteCount="1" Id="11" LastActivityDate="2010-08-03T21:50:09.007" LastEditDate="2010-07-28T07:58:52.320" LastEditorUserId="34" OwnerUserId="34" PostTypeId="1" Score="2" Tags="&lt;multivariable&gt;&lt;interpolation&gt;" Title="Multivariate Interpolation Approaches" ViewCount="231" />
  <row Body="&lt;p&gt;John Cook gives some interesting recommendations. Basically, get percentiles/quantiles (not means or obscure scale parameters!) from the experts, and fit them with the appropriate distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.johndcook.com/blog/2010/01/31/parameters-from-percentiles/&quot;&gt;http://www.johndcook.com/blog/2010/01/31/parameters-from-percentiles/&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-07-19T19:19:46.160" Id="15" LastActivityDate="2010-07-19T19:19:46.160" OwnerUserId="6" ParentId="1" PostTypeId="2" Score="14" />
  
  <row Body="&lt;p&gt;Contingency table (chi-square). Also Logistic Regression is your friend - use dummy variables. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-07-19T19:28:15.640" Id="29" LastActivityDate="2010-07-19T19:28:15.640" OwnerUserId="36" ParentId="17" PostTypeId="2" Score="5" />
  
  
  <row AcceptedAnswerId="111" AnswerCount="4" Body="&lt;p&gt;What algorithms are used in modern and good-quality random number generators? &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-07-19T19:32:47.750" FavoriteCount="2" Id="40" LastActivityDate="2014-01-28T07:46:08.953" LastEditDate="2010-08-25T14:13:48.740" LastEditorUserId="919" OwnerUserId="69" PostTypeId="1" Score="8" Tags="&lt;algorithms&gt;&lt;random-variable&gt;&lt;random-generation&gt;" Title="Pseudo-random number generation algorithms" ViewCount="520" />
  <row Body="&lt;p&gt;The &lt;a href=&quot;http://en.wikipedia.org/wiki/Mersenne_twister&quot; rel=&quot;nofollow&quot;&gt;Mersenne Twister&lt;/a&gt; is one I've come across and used before now.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-07-19T19:34:44.033" Id="45" LastActivityDate="2010-07-19T19:34:44.033" OwnerUserId="55" ParentId="40" PostTypeId="2" Score="5" />
  <row AcceptedAnswerId="268" AnswerCount="4" Body="&lt;p&gt;I have a dataset of 130k internet users characterized by 4 variables describing users' number of sessions, locations visited, avg data download and session time aggregated from four months of activity.&lt;/p&gt;&#10;&#10;&lt;p&gt;Dataset is very heavy-tailed. For example third of users logged only once during four months, whereas six users had more than 1000 sessions.&lt;/p&gt;&#10;&#10;&lt;p&gt;I wanted to come up with a simple classification of users, preferably with indication of the most appropriate number of clusters.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there anything you could recomend as a soultion?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-07-19T19:36:12.140" FavoriteCount="3" Id="47" LastActivityDate="2010-11-23T18:59:58.170" LastEditDate="2010-10-08T16:05:59.363" LastEditorUserId="8" OwnerUserId="22" PostTypeId="1" Score="7" Tags="&lt;clustering&gt;&lt;large-data&gt;" Title="Clustering of large, heavy-tailed dataset" ViewCount="324" />
  
  <row Body="&lt;p&gt;From &lt;a href=&quot;http://en.wikipedia.org/wiki/Random_variable&quot; rel=&quot;nofollow&quot;&gt;Wikipedia&lt;/a&gt;:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;In mathematics (especially probability&#10;  theory and statistics), a random&#10;  variable (or stochastic variable) is&#10;  (in general) a measurable function &#10;  that maps a probability space into a&#10;  measurable space. Random variables&#10;  mapping all possible outcomes of an&#10;  event into the real numbers are&#10;  frequently studied in elementary&#10;  statistics and used in the sciences to&#10;  make predictions based on data&#10;  obtained from scientific experiments.&#10;  In addition to scientific&#10;  applications, random variables were&#10;  developed for the analysis of games of&#10;  chance and stochastic  events. The&#10;  utility of random variables comes from&#10;  their ability to capture only the&#10;  mathematical properties necessary to&#10;  answer probabilistic  questions.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;From &lt;a href=&quot;http://cnx.org/content/m13418/latest/&quot; rel=&quot;nofollow&quot;&gt;cnx.org&lt;/a&gt;:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;A random variable is a function, which assigns unique numerical values to all possible&#10;  outcomes of a random experiment under fixed conditions. A random variable is not a &#10;  variable but rather a function that maps events to numbers.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="4" CreationDate="2010-07-19T19:42:34.670" Id="57" LastActivityDate="2010-07-19T19:47:54.623" LastEditDate="2010-07-19T19:47:54.623" LastEditorUserId="69" OwnerUserId="69" ParentId="50" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;In such a discussion, I always recall the famous Ken Thompson quote &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;When in doubt, use brute force.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;In this case, machine learning is a salvation when the assumptions are hard to catch; or at least it is much better than guessing them wrong. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-07-19T19:51:34.287" Id="74" LastActivityDate="2010-07-19T19:51:34.287" OwnerUserId="88" ParentId="6" PostTypeId="2" Score="29" />
  
  <row Body="&lt;p&gt;Unlike a regular variable, a random variable may not be substituted for a single, unchanging value.  Rather &lt;strong&gt;statistical properties&lt;/strong&gt; such as the &lt;strong&gt;distribution&lt;/strong&gt; of the random variable may be stated.  The distribution is a function that provides the probability the variable will take on a given value, or fall within a range given certain parameters such as the mean or standard deviation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Random variables may be classified as &lt;em&gt;discrete&lt;/em&gt; if the distribution describes values from a countable set, such as the integers.  The other classification for a random variable is &lt;em&gt;continuous&lt;/em&gt; and is used if the distribution covers values from an uncountable set such as the real numbers.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2010-07-19T20:08:37.010" Id="86" LastActivityDate="2010-07-20T17:46:55.840" LastEditDate="2010-07-20T17:46:55.840" LastEditorUserId="13" OwnerUserId="13" ParentId="50" PostTypeId="2" Score="8" />
  <row AnswerCount="1" Body="&lt;p&gt;We're trying to use a Gaussian process to model h(t) -- the hazard function -- for a very small initial population, and then fit that using the available data.  While this gives us nice plots for credible sets for h(t) and so on, it unfortunately is also just pushing the inference problem from h(t) to the covariance function of our process.  Perhaps predictably, we have several reasonable and equally defensible guesses for this that all produce different result.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Has anyone run across any good approaches for addressing such a problem?  Gaussian-process related or otherwise?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2010-07-19T20:17:07.643" FavoriteCount="3" Id="93" LastActivityDate="2010-09-16T12:33:50.390" LastEditDate="2010-09-16T12:33:50.390" LastEditorUserId="88" OwnerUserId="61" PostTypeId="1" Score="5" Tags="&lt;nonparametric&gt;&lt;survival&gt;&lt;hazard&gt;" Title="Robust nonparametric estimation of hazard/survival functions based on low count data" ViewCount="419" />
  
  
  
  
  
  
  
  
  <row Body="&lt;p&gt;There are a number of statistical tests (known as &quot;unit root tests&quot;) for dealing with this problem. The most popular is probably the &quot;Augmented Dickey-Fuller&quot; (ADF) test, although the Phillips-Perron (PP) test and the KPSS test are also widely used. &lt;/p&gt;&#10;&#10;&lt;p&gt;Both the ADF and PP tests are based on a null hypothesis of a unit root (i.e., an I(1) series). The KPSS test is based on a null hypothesis of stationarity (i.e., an I(0) series). Consequently, the KPSS test can give quite different results from the ADF or PP tests.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-07-19T23:32:30.337" Id="171" LastActivityDate="2010-07-19T23:32:30.337" OwnerUserId="159" ParentId="161" PostTypeId="2" Score="8" />
  
  <row Body="&lt;p&gt;For a univariate KDE, you are better off using something other than Silverman's rule which is based on a normal approximation. One excellent approach is the Sheather-Jones method, easily implemented in R; for example,&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;plot(density(precip, bw=&quot;SJ&quot;))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The situation for multivariate KDE is not so well studied, and the tools are not so mature. Rather than a bandwidth, you need a bandwidth matrix. To simplify the problem, most people assume a diagonal matrix, although this may not lead to the best results. The &lt;a href=&quot;http://cran.r-project.org/web/packages/ks/&quot;&gt;ks package in R&lt;/a&gt; provides some very useful tools including allowing a full (not necessarily diagonal) bandwidth matrix.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-07-19T23:59:29.487" Id="179" LastActivityDate="2010-07-19T23:59:29.487" OwnerUserId="159" ParentId="168" PostTypeId="2" Score="14" />
  <row AcceptedAnswerId="518" AnswerCount="4" Body="&lt;p&gt;I need to analyze the 100k MovieLens dataset for clustering with two algorithms of my choice, between the likes of k-means, agnes, diana, dbscan, and several others. What tools (like Rattle, or Weka) would be best suited to help me make some simple clustering analysis over this dataset?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-07-20T00:20:51.767" Id="183" LastActivityDate="2013-07-15T11:25:42.467" OwnerUserId="166" PostTypeId="1" Score="3" Tags="&lt;clustering&gt;" Title="What tools could be used for applying clustering algorithms on MovieLens?" ViewCount="835" />
  
  <row Body="&lt;p&gt;If you want to know that a pattern is meaningful, you need to show what it actually &lt;em&gt;means&lt;/em&gt;. Statistical tests do not do this. Unless your data can be said to be in some sense &quot;complete&quot;, inferences draw from the data will always be provisional.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can increase your &lt;em&gt;confidence&lt;/em&gt; in the validity of a pattern by testing against more and more out of sample data, but that doesn't protect you from it turning out to be an artefact. The broader your range of out of sample data -- eg, in terms of how it is acquired and what sort of systematic confounding factors might exist within it -- the better the validation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Ideally, though, you need to go beyond identifying patterns and come up with a persuasive theoretical framework that &lt;em&gt;explains&lt;/em&gt; the patterns you've found, and then test &lt;em&gt;that&lt;/em&gt; by other, independent means. (This is called &quot;science&quot;.)&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2010-07-20T02:48:45.177" Id="200" LastActivityDate="2012-08-20T10:05:15.320" LastEditDate="2012-08-20T10:05:15.320" LastEditorUserId="174" OwnerUserId="174" ParentId="194" PostTypeId="2" Score="14" />
  
  <row Body="&lt;p&gt;First, we need to understand what is a markov chain. Consider the following &lt;a href=&quot;http://en.wikipedia.org/wiki/Examples_of_Markov_chains#A_very_simple_weather_model&quot;&gt;weather&lt;/a&gt; example from Wikipedia. Suppose that weather on any given day can be classified into two states only: sunny and rainy. Based on past experience, we know the following:&lt;/p&gt;&#10;&#10;&lt;p&gt;Probability(Next day is sunny | Given today is rainy ) = 0.50&lt;/p&gt;&#10;&#10;&lt;p&gt;Since, the next day's weather is either sunny or rainy it follows that:&lt;/p&gt;&#10;&#10;&lt;p&gt;Probability(Next day is Rainy | Given today is rainy ) = 0.50 &lt;/p&gt;&#10;&#10;&lt;p&gt;Similarly, let:&lt;/p&gt;&#10;&#10;&lt;p&gt;Probability(Next day is rainy | Given today is sunny ) = 0.10&lt;/p&gt;&#10;&#10;&lt;p&gt;Therefore, it follows that:&lt;/p&gt;&#10;&#10;&lt;p&gt;Probability(Next day is sunny | Given today is sunny ) = 0.90&lt;/p&gt;&#10;&#10;&lt;p&gt;The above four numbers can be compactly represented as a transition matrix which represents the probabilities of the weather moving from one state to another state as follows:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;         S   R&#10;P = S [ 0.9 0.1&#10;    R   0.5 0.5]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;We might ask several questions whose answers follow:&lt;/p&gt;&#10;&#10;&lt;p&gt;Q1: If the weather is sunny today then what is the weather likely to be tomorrow?&lt;/p&gt;&#10;&#10;&lt;p&gt;A1: Since, we do not know what is going to happen for sure, the best we can say is that there is a 90% chance that it is likely to be sunny and 10% that it will be rainy. &lt;/p&gt;&#10;&#10;&lt;p&gt;Q2: What about two days from today?&lt;/p&gt;&#10;&#10;&lt;p&gt;A2: One day prediction: 90% sunny, 10% rainy. Therefore, two days from now:&lt;/p&gt;&#10;&#10;&lt;p&gt;First day it can be sunny and the next day also it can be sunny. Chances of this happening are: 0.9  0.9. &lt;/p&gt;&#10;&#10;&lt;p&gt;Or&lt;/p&gt;&#10;&#10;&lt;p&gt;First day it can be rainy and second day it can be sunny. Chances of this happening are: 0.1 * 0.5&lt;/p&gt;&#10;&#10;&lt;p&gt;Therefore, the probability that the weather will be sunny in two days is:&lt;/p&gt;&#10;&#10;&lt;p&gt;Prob(Sunny two days from now) = 0.9  0.9 + 0.1  0.5 = 0.81 + 0.05 = 0.86 &lt;/p&gt;&#10;&#10;&lt;p&gt;Similarly, the probability that it will be rainy is:&lt;/p&gt;&#10;&#10;&lt;p&gt;Prob(Rainy two days from now) = 0.1 * 0.5 + 0.9 0.1 = 0.05 + 0.09 = 0.14&lt;/p&gt;&#10;&#10;&lt;p&gt;If you keep forecasting weather like this you will notice that eventually the nth day forecast where n is very large (say 30) settles to the following 'equilibrium' probabilities:&lt;/p&gt;&#10;&#10;&lt;p&gt;Prob(Sunny) = 0.833&#10;Prob(Rainy) = 0.167&lt;/p&gt;&#10;&#10;&lt;p&gt;In other words, your forecast for the nth day and the n+1th day remain the same. In addition, you can also check that the 'equilibrium' probabilities do not depend on the weather today. You would get the same forecast for the weather if you start of by assuming that the weather today is sunny or rainy.&lt;/p&gt;&#10;&#10;&lt;p&gt;The above example will only work if the state transition probabilities satisfy several conditions which I will not discuss here. But, notice the following features of this 'nice' markov chain (nice = transition probabilities satisfy conditions):&lt;/p&gt;&#10;&#10;&lt;p&gt;Irrespective of the initial starting state we will eventually reach an equilibrium probability distribution of states.&lt;/p&gt;&#10;&#10;&lt;p&gt;Markov Chain Monte Carlo exploits the above feature as follows: &lt;/p&gt;&#10;&#10;&lt;p&gt;We want to generate random draws from a target distribution. We then identify a way to construct a 'nice' markov chain such that its equilibrium probability distribution is our target distribution. &lt;/p&gt;&#10;&#10;&lt;p&gt;If we can construct such a chain then we arbitrarily start from some point and iterate the markov chain many times (like how we forecasted the weather n times). Eventually, the draws we generate would appear as if they are coming from our target distribution. &lt;/p&gt;&#10;&#10;&lt;p&gt;We then approximate the quantities of interest (e.g. mean) by taking the sample average of the draws after discarding a few initial draws which is the monte carlo component.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are several ways to construct 'nice' markov chains (e.g., gibbs sampler, Metropolis-Hastings algorithm).&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2010-08-16T06:37:30.433" CreationDate="2010-07-20T04:00:14.387" Id="207" LastActivityDate="2010-07-21T16:00:43.450" LastEditDate="2010-07-21T16:00:43.450" LastEditorDisplayName="user28" OwnerDisplayName="user28" ParentId="165" PostTypeId="2" Score="61" />
  
  <row Body="&lt;p&gt;Discrete data can only take particular values. There may potentially be an infinite number of those values, but each is distinct and there's no grey area in between. Discrete data can be numeric -- like numbers of apples -- but it can also be categorical -- like red or blue, or male or female, or good or bad.&lt;/p&gt;&#10;&#10;&lt;p&gt;Continuous data are not restricted to defined separate values, but can occupy any value over a continuous range. Between any two continuous data values there may be an infinite number of others. Continuous data are always essentially numeric.&lt;/p&gt;&#10;&#10;&lt;p&gt;It sometimes makes sense to treat numeric data that is properly of one type as being of the other. For example, something like &lt;em&gt;height&lt;/em&gt; is continuous, but often we don't really care too much about tiny differences and instead group heights into a number of discrete &lt;strong&gt;bins&lt;/strong&gt;. Conversely, if we're counting large amounts of some discrete entity -- grains of rice, or termites, or pennies in the economy -- we may choose not to think of 2,000,006 and 2,000,008 as crucially different values but instead as nearby points on an approximate continuum.&lt;/p&gt;&#10;&#10;&lt;p&gt;It can also sometimes be useful to treat numeric data as categorical, eg: underweight, normal, obese. This is usually just another kind of binning.&lt;/p&gt;&#10;&#10;&lt;p&gt;It seldom makes sense to consider categorical data as continuous.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2010-07-20T04:16:52.663" Id="209" LastActivityDate="2010-07-20T05:25:22.477" LastEditDate="2010-07-20T05:25:22.477" LastEditorUserId="174" OwnerUserId="174" ParentId="206" PostTypeId="2" Score="20" />
  <row AcceptedAnswerId="5001" AnswerCount="3" Body="&lt;p&gt;I have 2 ASR (Automatic Speech Recognition) models, providing me with text transcriptions for my testdata. The error measure I use is Word Error Rate.&lt;/p&gt;&#10;&#10;&lt;p&gt;What methods do I have to test for statistical significance of my new results?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;An example:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I have an experiment with 10 speaker, all having 100 (the same) sentences, total 900 words per speaker. Method A has an WER (word error rate) of 19.0%, Method B 18.5%.&lt;/p&gt;&#10;&#10;&lt;p&gt;How do I test whether Method B is significantly better?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-07-20T04:54:20.793" Id="212" LastActivityDate="2010-11-29T18:25:11.713" LastEditDate="2010-07-21T06:19:29.143" LastEditorUserId="190" OwnerUserId="190" PostTypeId="1" Score="3" Tags="&lt;statistical-significance&gt;" Title="What method to use to test Statistical Significance of ASR results" ViewCount="461" />
  <row AcceptedAnswerId="217" AnswerCount="3" Body="&lt;p&gt;What are some good visualization libraries for online use? Are they easy to use and is there good documentation?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-07-20T05:04:40.840" FavoriteCount="7" Id="216" LastActivityDate="2010-07-20T08:45:07.490" OwnerUserId="191" PostTypeId="1" Score="9" Tags="&lt;data-visualization&gt;&lt;library&gt;&lt;protovis&gt;" Title="Web visualization libraries" ViewCount="505" />
  <row AnswerCount="3" Body="&lt;p&gt;I have a friend who is an MD and wants to refresh his Statistics. So is there any recommended resource online (or offline) ? He did stats ~20 years ago.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2010-07-20T05:54:15.017" FavoriteCount="5" Id="223" LastActivityDate="2012-02-01T18:51:11.727" LastEditDate="2012-02-01T18:51:11.727" LastEditorUserId="4872" OwnerUserId="79" PostTypeId="1" Score="5" Tags="&lt;books&gt;" Title="Intro to statistics for an MD?" ViewCount="297" />
  <row Body="&lt;p&gt;&lt;a href=&quot;http://raphaeljs.com/&quot; rel=&quot;nofollow&quot;&gt;RaphaelJS&lt;/a&gt; can do some pretty amazing stuff and it just got some major backing from &lt;a href=&quot;http://www.sencha.com/&quot; rel=&quot;nofollow&quot;&gt;Sencha&lt;/a&gt; (formerly ExtJS).  Raphael is pretty smart about browsers by using a VML backend for Internet Explorer and SVG for everything else.  However, the library is pretty low-level.  Fortunately, the author has started another project, &lt;a href=&quot;http://g.raphaeljs.com/&quot; rel=&quot;nofollow&quot;&gt;gRaphael&lt;/a&gt;, that focuses on drawing charts and graphs.&lt;/p&gt;&#10;&#10;&lt;p&gt;The &lt;a href=&quot;http://simile.mit.edu/&quot; rel=&quot;nofollow&quot;&gt;MIT SIMILE Project&lt;/a&gt; also has some interesting JavaScript libraries:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://www.simile-widgets.org/timeplot/&quot; rel=&quot;nofollow&quot;&gt;Timeplot&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://www.simile-widgets.org/timeline/&quot; rel=&quot;nofollow&quot;&gt;Timeline&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;There is also a project to port Processing to JavaScript: &lt;a href=&quot;http://processingjs.org/&quot; rel=&quot;nofollow&quot;&gt;ProcessingJS&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://jmol.sourceforge.net/&quot; rel=&quot;nofollow&quot;&gt;Jmol&lt;/a&gt; is a Java applet for viewing chemical structures, but it is used as the display engine for 3D graphics in the &lt;a href=&quot;http://www.sagemath.org/&quot; rel=&quot;nofollow&quot;&gt;SAGE&lt;/a&gt; system, which has a completely browser-based GUI.&lt;/p&gt;&#10;&#10;&lt;p&gt;And for an open source alternative to Google Maps, there is the excellent &lt;a href=&quot;http://www.openlayers.org&quot; rel=&quot;nofollow&quot;&gt;OpenLayers&lt;/a&gt; JavaScript library which powers the frontend of the equally excellent &lt;a href=&quot;http://www.openstreetmap.org/&quot; rel=&quot;nofollow&quot;&gt;OpenStreetMap&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-07-20T07:17:58.993" Id="235" LastActivityDate="2010-07-20T08:45:07.490" LastEditDate="2010-07-20T08:45:07.490" LastEditorUserId="13" OwnerUserId="13" ParentId="216" PostTypeId="2" Score="4" />
  
  
  <row Body="&lt;p&gt;ggobi and the R links to Ggobi are really rather good for this.   There are simpler visualisations (iPlots is very nice, also interactive, as mentioned).&lt;/p&gt;&#10;&#10;&lt;p&gt;But it depends whether you are doing something more specialised.   For example TreeView lets you visualise the kind of cluster dendrograms you get out of microarrays.&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2010-07-20T08:38:38.223" CreationDate="2010-07-20T08:38:38.223" Id="248" LastActivityDate="2010-07-20T08:38:38.223" OwnerUserId="211" ParentId="196" PostTypeId="2" Score="4" />
  
  
  <row Body="&lt;p&gt;As you mentioned sorting would be &lt;code&gt;O(n·log n)&lt;/code&gt; for a window of length &lt;code&gt;n&lt;/code&gt;. Doing this moving adds another &lt;code&gt;l=vectorlength&lt;/code&gt; making the total cost &lt;code&gt;O(l·n·log n)&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;The simplest way to push this is by keeping an ordered list of the last n elements in memory when moving from one window to the next one. As removing/inserting one element from/into an ordered list are both &lt;code&gt;O(n)&lt;/code&gt; this would result in costs of &lt;code&gt;O(l·n)&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Pseudocode:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;l = length(input)&#10;aidvector = sort(input(1:n))&#10;output(i) = aid(n/2)&#10;for i = n+1:l&#10;    remove input(i-n) from aidvector&#10;    sort aid(n) into aidvector&#10;    output(i) = aid(n/2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2010-07-20T10:23:56.707" Id="266" LastActivityDate="2010-07-20T10:23:56.707" OwnerUserId="128" ParentId="134" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;The population is the whole set of values, or individuals, you are interested in. The sample is a subset of the population, and is the set of values you actually use in your estimation.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, for example, if you want to know the average height of the residents of China, that is your population, ie, the population of China. The thing is, this is quite large a number, and you wouldn't be able to get data for everyone there. So you draw a sample, that is, you get some observations, or the height of some of the people in China (a subset of the population, the sample) and do your inference based on that. &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2010-07-20T11:21:59.493" Id="274" LastActivityDate="2010-07-20T11:21:59.493" OwnerUserId="90" ParentId="269" PostTypeId="2" Score="10" />
  <row Body="&lt;p&gt;In case of Poisson it is bad, since counts are counts -- their unit is an unity. On the other hand, if you'd use some advanced software like R, its Poisson handling functions will be aware of such large numbers and would use some numerical tricks to handle them.&lt;/p&gt;&#10;&#10;&lt;p&gt;Obviously I agree that normal approximation is another good approach.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-07-20T11:29:53.070" Id="275" LastActivityDate="2010-07-20T12:26:55.800" LastEditDate="2010-07-20T12:26:55.800" LastEditorUserId="88" OwnerUserId="88" ParentId="270" PostTypeId="2" Score="5" />
  <row AnswerCount="3" Body="&lt;p&gt;When a non-hierarchical cluster analysis is carried out, the order of observations in the data file determine the clustering results, especially if the data set is small (i.e, 5000 observations). To deal with this problem I usually performed a random reorder of data observations. My problem is that if I replicate the analysis n times, the results obtained are different and sometimes these differences are great. &lt;/p&gt;&#10;&#10;&lt;p&gt;How can I deal with this problem? Maybe I could run the analysis several times and after consider that one observation belong to the group in which more times was assigned. Has someone a better approach to this problem?&lt;/p&gt;&#10;&#10;&lt;p&gt;Manuel Ramon&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-07-20T11:49:27.543" FavoriteCount="1" Id="278" LastActivityDate="2010-09-17T20:36:37.643" LastEditDate="2010-09-17T20:36:37.643" LastEditorUserId="88" OwnerUserId="221" PostTypeId="1" Score="6" Tags="&lt;clustering&gt;" Title="How to deal with the effect of the order of observations in a non hierarchical cluster analysis?" ViewCount="128" />
  
  
  <row AcceptedAnswerId="445" AnswerCount="7" Body="&lt;p&gt;I know of Cameron and Trivedi's Microeconometrics Using Stata. &lt;/p&gt;&#10;&#10;&lt;p&gt;What are other good texts for learning Stata? &lt;/p&gt;&#10;" CommentCount="1" CommunityOwnedDate="2011-11-16T20:17:12.253" CreationDate="2010-07-20T12:33:30.577" FavoriteCount="4" Id="290" LastActivityDate="2011-11-17T02:52:41.260" LastEditDate="2011-11-16T20:27:35.007" LastEditorUserId="919" OwnerUserId="189" PostTypeId="1" Score="7" Tags="&lt;books&gt;&lt;stata&gt;&lt;big-list&gt;" Title="Resources for learning Stata" ViewCount="940" />
  <row AcceptedAnswerId="318" AnswerCount="1" Body="&lt;p&gt;I'm a physics graduate who ended up doing infosec so most of the statistics I ever learned is useful for thermodynamics. I'm currently trying to think of a model for working out how many of a population of computers are infected with viruses, though I assume the maths works out the same way for real-world diseases so references in or answers relevant to that field would be welcome too.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's what I've come up with so far:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;assume I know the total population of computers, N.&lt;/li&gt;&#10;&lt;li&gt;I know the fraction D of computers that have virus-detection software (i.e. the amount of the population that is being screened)&lt;/li&gt;&#10;&lt;li&gt;I know the fraction I of computers that have detection software &lt;em&gt;that has reported an infection&lt;/em&gt;&lt;/li&gt;&#10;&lt;li&gt;I don't know, but can find out or estimate, the probability of Type I and II errors in the detection software.&lt;/li&gt;&#10;&lt;li&gt;I don't (yet) care about the time evolution of the population.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;So where do I go from here? Would you model infection as a binomial distribution with probability like (I given D), or as a Poisson? Or is the distribution different?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-07-20T15:03:40.450" Id="312" LastActivityDate="2010-07-20T15:23:53.653" OwnerDisplayName="user209" PostTypeId="1" Score="4" Tags="&lt;distributions&gt;&lt;modeling&gt;&lt;poisson&gt;&lt;binomial&gt;&lt;disease&gt;" Title="What approach could be used for modelling virus infections?" ViewCount="121" />
  
  <row Body="&lt;p&gt;No amount of verbal explanation or calculations really helped me to understand &lt;em&gt;at a gut level&lt;/em&gt; what p-values were, but it really snapped into focus for me once I took a course that involved simulation.  That gave me the ability to actually &lt;em&gt;see&lt;/em&gt; data generated by the null hypothesis and to plot the means/etc. of simulated samples, then look at where my sample's statistic fell on that distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;I think the key advantage to this is that it lets students forget about the math and the test statistic distributions for a minute and focus on the concepts at hand.  Granted, it required that I learn &lt;em&gt;how&lt;/em&gt; to simulate that stuff, which will cause problems for an entirely different set of students.  But it worked for me, and I've used simulation countless times to help explain statistics to others with great success (e.g., &quot;This is what your data looks like; this is what a Poisson distribution looks like overlaid.  Are you SURE you want to do a Poisson regression?&quot;).&lt;/p&gt;&#10;&#10;&lt;p&gt;This doesn't exactly answer the questions you posed, but for me, at least, it made them trivial.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-07-20T15:33:42.533" Id="319" LastActivityDate="2010-07-20T15:33:42.533" OwnerUserId="71" ParentId="31" PostTypeId="2" Score="18" />
  <row AcceptedAnswerId="329" AnswerCount="9" Body="&lt;p&gt;I realize that the statistical analysis of financial data is a huge topic, but that is exactly why it is necessary for me to ask my question as I try to break into the world of financial analysis.&lt;/p&gt;&#10;&#10;&lt;p&gt;As at this point I know next to nothing about the subject, the results of my google searches  are overwhelming.  Many of the matches advocate learning specialized tools or the R programming language. While I will learn these when they are necessary, I'm first interested in books, articles or any other resources that explain modern methods of statistical analysis specifically for financial data.  I assume there are a number of different wildly varied methods for analyzing data, so ideally I'm seeking an overview of the various methods that are practically applicable.  I'd like something that utilizes real world examples that a beginner is capable of grasping but that aren't overly simplistic.&lt;/p&gt;&#10;&#10;&lt;p&gt;What are some good resources for learning bout the statistical analysis of financial data?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2010-07-20T16:27:08.763" FavoriteCount="15" Id="328" LastActivityDate="2012-08-27T15:59:23.840" LastEditDate="2010-09-17T20:53:00.430" LastEditorUserId="88" OwnerUserId="75" PostTypeId="1" Score="5" Tags="&lt;books&gt;&lt;finance&gt;" Title="Resources for learning about the Statistical Analysis of Financial Data" ViewCount="2590" />
  
  
  
  
  <row Body="&lt;p&gt;A male cat and a female cat are penned up in a steel chamber, along with enough food and water for 70 days.  &lt;/p&gt;&#10;&#10;&lt;p&gt;A Frequentist would say the average gestation period for &lt;a href=&quot;http://en.wikipedia.org/wiki/Cat#Reproduction&quot; rel=&quot;nofollow&quot;&gt;felines&lt;/a&gt; is 66 days, the female was in heat when the cats were penned up, and once in heat she will mate repeatedly for 4 to 7 days.  Since there were likely many acts of propagation and enough subsequent time for gestation, the odds are, when the box is opened on day 70, there's a litter of newborn kittens.&lt;/p&gt;&#10;&#10;&lt;p&gt;A Bayesian would say, I heard some serious Marvin Gaye coming from the box on day 1 and then this morning I heard many kitten-like sounds coming from the box.  So without knowing much about cat reproduction, the odds are, when the box is opened on day 70, there's a litter of newborn kittens. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-07-20T19:54:13.473" Id="348" LastActivityDate="2010-07-20T22:52:03.620" LastEditDate="2010-07-20T22:52:03.620" LastEditorUserId="24" OwnerUserId="24" ParentId="22" PostTypeId="2" Score="-5" />
  
  <row AcceptedAnswerId="376" AnswerCount="4" Body="&lt;p&gt;Why do we seek to minimize &lt;code&gt;x^2&lt;/code&gt; instead of minimizing &lt;code&gt;|x|^1.95&lt;/code&gt; or &lt;code&gt;|x|^2.05&lt;/code&gt;.&#10;Are there reasons why the number should be exactly two or is it simply a convention that has the advantage of simplifying the math?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-07-20T22:26:26.083" FavoriteCount="5" Id="354" LastActivityDate="2013-01-09T21:19:25.140" LastEditDate="2010-11-28T12:04:24.103" LastEditorUserId="930" OwnerUserId="3807" PostTypeId="1" Score="9" Tags="&lt;standard-deviation&gt;&lt;least-squares&gt;" Title="Bias towards natural numbers in the case of least squares" ViewCount="687" />
  <row Body="&lt;p&gt;We try to minimize the variance that is left within descriptors. Why variance? Read &lt;a href=&quot;http://stats.stackexchange.com/questions/118/standard-deviation-why-square-the-difference-instead-of-taking-the-absolute-val&quot; rel=&quot;nofollow&quot;&gt;this question&lt;/a&gt;; this also comes together with the (mostly silent) assumption that errors are normally distributed.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Extension:&lt;/strong&gt;&lt;br&gt;&#10;Two additional arguments:  &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;For variances, we have this nice &quot;law&quot; that the sum of variances is equal to the variance of sum, for uncorrelated samples. If we assume that the error is not correlated with the case, minimizing residual of squares will work straightforward to maximizing explained variance, what is maybe a not-so-good but still popular quality measure.  &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;If we assume normality of an error, least squares error estimator is a maximal likelihood one.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="4" CreationDate="2010-07-20T23:21:05.400" Id="358" LastActivityDate="2010-07-21T09:31:29.867" LastEditDate="2010-07-21T09:31:29.867" LastEditorUserId="88" OwnerUserId="88" ParentId="354" PostTypeId="2" Score="6" />
  
  <row AcceptedAnswerId="74954" AnswerCount="2" Body="&lt;p&gt;What is the difference between the Shapiro-Wilk test of normality and the Kolmogorov-Smirnov test of normality?  When will results from these two methods differ?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-07-21T00:24:35.500" FavoriteCount="8" Id="362" LastActivityDate="2014-06-15T03:58:03.080" LastEditDate="2014-06-15T03:58:03.080" LastEditorUserId="805" OwnerUserId="196" PostTypeId="1" Score="15" Tags="&lt;distributions&gt;&lt;statistical-significance&gt;&lt;normality&gt;&lt;kolmogorov-smirnov&gt;" Title="What is the difference between the Shapiro-Wilk test of normality and the Kolmogorov-Smirnov test of normality?" ViewCount="31789" />
  <row AnswerCount="19" Body="&lt;p&gt;If you could go back in time and tell yourself to read a specific book at the beginning of your career as a statistician, which book would it be?&lt;/p&gt;&#10;" CommentCount="2" CommunityOwnedDate="2010-07-21T00:54:29.463" CreationDate="2010-07-21T00:44:08.597" FavoriteCount="73" Id="363" LastActivityDate="2012-02-14T19:22:31.310" OwnerUserId="74" PostTypeId="1" Score="55" Tags="&lt;books&gt;" Title="What is the single most influential book every statistician should read?" ViewCount="6101" />
  <row Body="&lt;p&gt;Suppose that the text has N words and that you require that an ASR should correctly predict at least 95% of words in the text. You currently have the observed error rate for the two methods. You can perform two type of tests.&lt;/p&gt;&#10;&#10;&lt;p&gt;Test 1: Do the ASR models meet your criteria of 95% prediction?&lt;/p&gt;&#10;&#10;&lt;p&gt;Test 2: Are the two ASR models equally good in speech recognition?&lt;/p&gt;&#10;&#10;&lt;p&gt;You could make different type of assumptions regarding the data generating mechanism for your ASR models. The simplest, although a bit naive, would assume that word detection of each word in the text is an iid bernoulli variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;Under the above assumption you could do a test of proportions where you check if the error rate for each model is consistent with a true error rate of 5% (test 1) or a test of difference in proportions where you check if the error rates between the two models is the same (test 2). &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-07-21T03:01:00.020" Id="368" LastActivityDate="2010-07-21T03:01:00.020" OwnerDisplayName="user28" ParentId="212" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="378" AnswerCount="12" Body="&lt;p&gt;From Wikipedia :&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Suppose you're on a game show, and&#10;  you're given the choice of three&#10;  doors: Behind one door is a car;&#10;  behind the others, goats. You pick a&#10;  door, say No. 1, and the host, who&#10;  knows what's behind the doors, opens&#10;  another door, say No. 3, which has a&#10;  goat. He then says to you, &quot;Do you&#10;  want to pick door No. 2?&quot; Is it to&#10;  your advantage to switch your choice?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;The answer is, of course, yes - but it's incredibly un-inituitive. What misunderstanding do most people have about probability that leads to us scratching our heads -- or better put; what general rule can we take away from this puzzle to better train our intuition in the future?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-07-21T04:30:50.663" FavoriteCount="12" Id="373" LastActivityDate="2013-06-12T23:06:43.350" LastEditDate="2013-06-12T23:06:43.350" LastEditorUserId="22468" OwnerUserId="252" PostTypeId="1" Score="26" Tags="&lt;probability&gt;&lt;puzzle&gt;" Title="The Monty Hall Problem - where does our intuition fail us?" ViewCount="1710" />
  
  <row Body="&lt;p&gt;You cannot prove, because it is impossible; you can only check if there are no any embarrassing autocorrelations or distribution disturbances, and indeed &lt;a href=&quot;http://en.wikipedia.org/wiki/Diehard_tests&quot; rel=&quot;nofollow&quot;&gt;Diehard&lt;/a&gt; is a standard for it. This is for statistics/physics, cryptographers will also mainly check (among other things) how hard is it to fit the generator to the data to obtain the future values.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-07-21T06:15:36.623" Id="380" LastActivityDate="2010-07-21T06:15:36.623" OwnerUserId="88" ParentId="30" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;On the math/foundations side: Harald Cramér's &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0691005478&quot; rel=&quot;nofollow&quot;&gt;Mathematical Methods of Statistics&lt;/a&gt;. &lt;/p&gt;&#10;" CommentCount="1" CommunityOwnedDate="2010-07-21T07:15:54.857" CreationDate="2010-07-21T07:15:54.857" Id="388" LastActivityDate="2010-07-21T07:15:54.857" OwnerUserId="251" ParentId="363" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;Take a look at the sample galleries for three popular visualization libraries:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://matplotlib.sourceforge.net/gallery.html&quot;&gt;matplotlib gallery&lt;/a&gt; (Python)&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://addictedtor.free.fr/graphiques/&quot;&gt;R graph gallery&lt;/a&gt; (R) -- (also see &lt;a href=&quot;http://had.co.nz/ggplot2/&quot;&gt;ggplot2&lt;/a&gt;, scroll down to reference)&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://prefuse.org/gallery/&quot;&gt;prefuse visualization gallery&lt;/a&gt; (Java)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;For the first two, you can even view the associated source code -- the simple stuff is simple, not many lines of code.  The prefuse case will have the requisite Java boilerplate code.  All three support a number of backends/devices/renderers (pdf, ps, png, etc).  All three are clearly capable of high quality graphics.&lt;/p&gt;&#10;&#10;&lt;p&gt;I think it pretty much boils down to which language are you most comfortable working in.  Go with that.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-07-21T08:19:36.577" Id="392" LastActivityDate="2010-07-21T08:19:36.577" OwnerUserId="251" ParentId="257" PostTypeId="2" Score="5" />
  
  
  <row Body="&lt;p&gt;This problem you are asking about is known as text mining!&lt;/p&gt;&#10;&#10;&lt;p&gt;There are a few things you need to consider. For example in your question you mentioned using keywords in titles. One may ask &quot;why not the text in the article rather than just the title?&quot; which brings me to the first consideration: What data do you limit yourself to?&lt;/p&gt;&#10;&#10;&lt;p&gt;Secondly, as the previous answer suggests, using frequencies is a great start. To take the analysis further you may start looking at what words occur frequently together! For example, the word 'happy' may occur very frequently... however if always accompanied by a 'not' your conclusions would be very different!&lt;/p&gt;&#10;&#10;&lt;p&gt;There is a very nice Australian piece of software I have used in the past called &lt;strong&gt;Leximancer&lt;/strong&gt;. I would advise anybody interested in text mining to have a look at their site and the examples they have... from memory one of which analysed speeches by 2 U.S.  presidential candidates. It makes for some very interesting reading!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-07-21T12:22:21.570" Id="401" LastActivityDate="2012-05-26T04:00:14.573" LastEditDate="2012-05-26T04:00:14.573" LastEditorUserId="5505" OwnerUserId="256" ParentId="369" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;I think you should take a look to the similar post from mathoverflow at &lt;a href=&quot;http://mathoverflow.net/questions/31655/statistics-for-mathematicians/31665#31665&quot; rel=&quot;nofollow&quot;&gt;http://mathoverflow.net/questions/31655/statistics-for-mathematicians/31665#31665&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;My answer to this post was Asymptotic statistics from Van der Vaart &lt;a href=&quot;http://www.cambridge.org/catalogue/catalogue.asp?isbn=9780521784504&quot; rel=&quot;nofollow&quot;&gt;http://www.cambridge.org/catalogue/catalogue.asp?isbn=9780521784504&lt;/a&gt;. &lt;/p&gt;&#10;" CommentCount="1" CommunityOwnedDate="2010-07-21T13:53:13.193" CreationDate="2010-07-21T13:53:13.193" Id="415" LastActivityDate="2010-07-21T13:53:13.193" OwnerUserId="223" ParentId="414" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;I agree that students find this problem very difficult. The typical response I get is that after you've been shown a goat there's a 50:50 chance of getting the car so why does it matter? Students seem to divorce their first choice from the decision they're now being asked to make i.e. they view these two actions as independent. I then remind them that they were twice as likely to have chosen the wrong door initially hence why they're better off switching. &lt;/p&gt;&#10;&#10;&lt;p&gt;In recent years I've started actually playing the game in glass and it helps students to understand the problem much better. I use three cardboard toilet roll &quot;middles&quot; and in two of them are paper clips and in the third is a £5 note. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-07-21T14:43:31.513" Id="419" LastActivityDate="2010-07-21T14:43:31.513" OwnerUserId="215" ParentId="373" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;That'll depend very much on their background, but I found &lt;a href=&quot;http://www.amazon.co.uk/Statistics-Nutshell-Desktop-Reference-OReilly/dp/0596510497&quot; rel=&quot;nofollow&quot;&gt;&quot;Statistics in a Nutshell&quot;&lt;/a&gt; to be pretty good.&lt;/p&gt;&#10;" CommentCount="1" CommunityOwnedDate="2010-07-21T15:09:11.543" CreationDate="2010-07-21T15:09:11.543" Id="422" LastActivityDate="2012-08-03T10:07:04.063" LastEditDate="2012-08-03T10:07:04.063" LastEditorDisplayName="user10525" OwnerUserId="247" ParentId="421" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;The UCLA resource listed by Stephen Turner (above) are excellent if you just want to apply methods you're already familiar with using Stata.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you're looking for textbooks which teach you statistics/econometrics while using Stata then these are solid recommendations (but it depends at what level you're looking at):&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Introductory Methods&lt;/strong&gt;&#10;An Introduction to Modern Econometrics Using Stata by Chris Baum&#10;Introduction to Econometrics by Chris Dougherty&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Advanced/Specialised Methods&lt;/strong&gt;&#10;Multilevel and Longitudinal Modeling Using Stata by Rabe-Hesketh and Skrondal&#10;Regression Models for Categorical Dependent Variables Using Stata by Long and Freese&lt;/p&gt;&#10;" CommentCount="2" CommunityOwnedDate="2011-11-16T20:17:12.253" CreationDate="2010-07-21T17:13:33.117" Id="445" LastActivityDate="2010-07-21T17:13:33.117" OwnerUserId="215" ParentId="290" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;If this is about your &quot;Qnotifier&quot; I think that you should plot the threshold line in some darker gray so it is distinguishable but not disturbing. Then I would color the part of the  plot that reaches over the threshold in some alarmistic hue, like red. &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2010-07-21T20:34:43.147" Id="464" LastActivityDate="2010-07-21T20:34:43.147" OwnerUserId="88" ParentId="459" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;Not Statistics specific, but a good resource is:  &lt;a href=&quot;http://www.reddit.com/r/mathbooks&quot;&gt;http://www.reddit.com/r/mathbooks&lt;/a&gt;&#10;Also, George Cain at Georgia Tech maintains a list of freely available maths texts that includes some statistical texts.  &lt;a href=&quot;http://people.math.gatech.edu/~cain/textbooks/onlinebooks.html&quot;&gt;http://people.math.gatech.edu/~cain/textbooks/onlinebooks.html&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2010-07-21T20:53:54.800" CreationDate="2010-07-21T20:53:54.800" Id="466" LastActivityDate="2010-07-21T20:53:54.800" OwnerUserId="115" ParentId="170" PostTypeId="2" Score="6" />
  
  <row Body="&lt;p&gt;For getting into stochastic processes and SDEs, Tom Kurtz's &lt;a href=&quot;http://www.math.wisc.edu/~kurtz/m735.htm&quot;&gt;lecture notes&lt;/a&gt; are hard to beat.  It starts with a decent review of probability and some convergence results, and then dives right into continuous time stochastic processes in fairly clear, comprehensible language.  In general it's one of the best books on the topic -- free or otherwise -- I've found.&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2010-07-21T23:00:34.700" CreationDate="2010-07-21T23:00:34.700" Id="472" LastActivityDate="2010-07-21T23:00:34.700" OwnerUserId="61" ParentId="170" PostTypeId="2" Score="6" />
  
  
  <row Body="&lt;p&gt;It also depends on where you wan't to publish your plots. You'll save yourself a lot of trouble by consulting the guide for authors before making any plots for a journal. &lt;/p&gt;&#10;&#10;&lt;p&gt;Also save the plots in a format that is easy to modify or save the code you have used to create them. Chances are that you need to make corrections. &lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2011-06-01T03:42:00.250" CreationDate="2010-07-22T11:05:55.197" Id="489" LastActivityDate="2010-07-22T11:05:55.197" OwnerUserId="214" ParentId="396" PostTypeId="2" Score="3" />
  <row AnswerCount="6" Body="&lt;p&gt;I am proposing to try and find a trend in some very noisy long term data. The data is basically weekly measurements of something which moved about 5mm over a period of about 8 months. The data is to 1mm accuracey and is very noisy regularly changing +/-1 or 2mm in a week. We only have the data to the nearest mm. &lt;/p&gt;&#10;&#10;&lt;p&gt;We plan to use some basic signal processing with a fast fourier transform to separate out the noise from the raw data. The basic assumption is if we mirror our data set and add it to the end of our existing data set we can create a full wavelength of the data and therefore our data will show up in a fast fourier transform and we can hopefully then separate it out. &lt;/p&gt;&#10;&#10;&lt;p&gt;Given that this sounds a little dubious to me, is this a method worth purusing or is the method of mirroring and appending our data set somehow fundamentally flawed? We are looking at other approaches such as using a low pass filter as well. &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2010-07-22T11:31:16.777" FavoriteCount="2" Id="492" LastActivityDate="2013-03-13T11:29:16.817" LastEditDate="2012-06-07T20:31:39.863" LastEditorUserId="919" OwnerUserId="210" PostTypeId="1" Score="10" Tags="&lt;time-series&gt;&lt;data-mining&gt;&lt;signal-processing&gt;&lt;trend&gt;" Title="Dubious use of signal processing principles to identify a trend" ViewCount="552" />
  
  <row AnswerCount="5" Body="&lt;p&gt;Sometimes, I just want to do a copy &amp;amp; paste from the output window in SAS. I can highlight text with a mouse-drag, but only SOMETIMES does that get copied to the clipboard. It doesn't matter if I use &quot;CTRL-C&quot; or right click -&gt; copy, or edit -&gt; copy&lt;/p&gt;&#10;&#10;&lt;p&gt;Any other SAS users experience this, and do you know a workaround/option/technique that can fix it? &lt;/p&gt;&#10;&#10;&lt;p&gt;Sometimes, I can fix it by clicking in another window, and coming back to the output window, but sometimes I just have to save the output window as a .lst and get the text from another editor. &lt;/p&gt;&#10;" CommentCount="11" CreationDate="2010-07-22T11:58:21.923" Id="498" LastActivityDate="2013-05-23T14:07:51.017" OwnerUserId="62" PostTypeId="1" Score="2" Tags="&lt;sas&gt;&lt;pc-sas&gt;" Title="In PC SAS, how do you copy &amp; paste from the output window?" ViewCount="2736" />
  
  <row Body="&lt;p&gt;1) A good demonstration of how &quot;random&quot; needs to be defined in order to work out probability of certain events:&lt;/p&gt;&#10;&#10;&lt;p&gt;What is the chance that a random line drawn across a circle will be longer than the radius?&lt;/p&gt;&#10;&#10;&lt;p&gt;The question totally depends how you draw your line. Possibilities which you can describe in a real-world way for a circle drawn on the ground might include:&lt;/p&gt;&#10;&#10;&lt;p&gt;Draw two random points inside the circle and draw a line through those. (See where two flies / stones fall...)&lt;/p&gt;&#10;&#10;&lt;p&gt;Choose a fixed point on the circumference, then a random one elsewhere in the circle and join those. (In effect this is laying a stick across the circle at a variable angle through a given point and a random one e.g. where a stone falls.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Draw a diameter. Randomly choose a point along it and draw a perpendicular through that. (Roll a stick along in a straight line so it rests across the circle.)&lt;/p&gt;&#10;&#10;&lt;p&gt;It is relatively easy to show someone who can do some geometry (but not necessarily stats) the answer to the question can vary quite widely (from about 2/3 to about 0.866 or so).&lt;/p&gt;&#10;&#10;&lt;p&gt;2) A reverse-engineered coin-toss: toss it (say) ten times and write down the result. Work out the probability of this exact sequence $\left(\frac{1}{2^{10}}\right)$. A tiny chance, but you just saw it happen with your own eyes!... Every sequence &lt;em&gt;might&lt;/em&gt; come up, including ten heads in a row, but it is hard for lay people to get their head round it. As an encore, try to convince them they have just as good a chance of winning the lottery with the numbers 1 through 6 as any other combination.&lt;/p&gt;&#10;&#10;&lt;p&gt;3) Explaining why medical diagnosis may seem really flawed. A test for disease foo which is 99.9% accurate at identifying those who have it but .1% false-positively diagnoses those who don't really have it may seem to be wrong really so often when the prevalence of the disease is really low (e.g. 1 in 1000) but many patients are tested for it. &lt;/p&gt;&#10;&#10;&lt;p&gt;This is one that is best explained with real numbers - imagine 1 million people are tested, so 1000 have the disease, 999 are correctly identified, but 0.1% of 999,000 is 999 who are told they have it but don't. So half those who are told they have it actually do not, despite the high level of accuracy (99.9%) and low level of false positives (0.1%). A second (ideally different) test will then separate these groups out. &lt;/p&gt;&#10;&#10;&lt;p&gt;[Incidentally, I chose the numbers because they are easy to work with, of course they do not have to add up to 100% as the accuracy / false positive rates are independent factors in the test.]&lt;/p&gt;&#10;" CommentCount="1" CommunityOwnedDate="2012-08-21T15:17:25.720" CreationDate="2010-07-22T15:49:54.270" Id="515" LastActivityDate="2013-10-23T15:29:05.390" LastEditDate="2013-10-23T15:29:05.390" LastEditorUserId="17230" OwnerUserId="270" ParentId="155" PostTypeId="2" Score="8" />
  
  
  <row AcceptedAnswerId="4183" AnswerCount="8" Body="&lt;p&gt;In answering &lt;a href=&quot;http://stats.stackexchange.com/questions/206/discrete-and-continuous&quot; rel=&quot;nofollow&quot;&gt;this question on discrete and continuous data&lt;/a&gt; I glibly asserted that it rarely makes sense to treat categorical data as continuous.&lt;/p&gt;&#10;&#10;&lt;p&gt;On the face of it that seems self-evident, but intuition is often a poor guide for statistics, or at least mine is. So now I'm wondering: is it true? Or are there established analyses for which a transform from categorical data to some continuum is actually useful? Would it make a difference if the data were ordinal?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-07-23T06:17:10.517" FavoriteCount="16" Id="539" LastActivityDate="2013-11-02T11:31:09.097" OwnerUserId="174" PostTypeId="1" Score="37" Tags="&lt;categorical-data&gt;&lt;continuous-data&gt;&lt;data-transformation&gt;" Title="Does it ever make sense to treat categorical data as continuous?" ViewCount="11844" />
  
  <row Body="&lt;p&gt;The most spectacular example is the &lt;a href=&quot;http://www.netflixprize.com/&quot; rel=&quot;nofollow&quot;&gt;Netflix challenge&lt;/a&gt;, which made really boosted blending popularity.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-07-23T17:02:28.887" Id="565" LastActivityDate="2010-07-23T17:02:28.887" OwnerUserId="88" ParentId="562" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;If you have no way of knowing the true concentration, the simplest approach would be a correlation.  A step beyond that might be to conduct a simple regression predicting the outcome on method 2 using method 1 (or vice versa). If the methods are identical the intercept should be 0; if the intercept is greater or less than 0 it would indicate the bias of one method relative to another.  The unstandardized slope should be near 1 if the methods on average produce results that are identical (after controlling for an upward or downward bias in the intercept).  The error in the unstandardized slope might serve as an index of the extent to which the two methods agree.  &lt;/p&gt;&#10;&#10;&lt;p&gt;It seems to me that the difficulty with statistical methods here that you are seeking to affirm what is typically posed as a null hypothesis, that is, that there are no differences between the methods.  This isn't a death blow for using statistical methods so long as you don't need a p value and you can quantify what you mean by &quot;equivalent&quot; and can decide how much deviation the two methods can have from one another before you no longer consider them equivalent.  In the regression approach I detailed above, you might consider the methods equivalent if confidence interval around the slope estimate included 1 and the CI around the intercept included 0.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-07-23T21:18:52.370" Id="578" LastActivityDate="2010-07-23T21:18:52.370" OwnerUserId="196" ParentId="527" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Indeed the only difference is that BIC is AIC extended to take number of objects (samples) into account. I would say that while both are quite weak (in comparison to for instance cross-validation) it is better to use AIC, than more people will be familiar with the abbreviation -- indeed I have never seen a paper or a program where BIC would be used (still I admit that I'm biased to problems where such criteria simply don't work).&lt;/p&gt;&#10;&#10;&lt;p&gt;Edit: AIC and BIC are equivalent to cross-validation provided two important assumptions -- when they are defined, so when the model is a maximum likelihood one and when you are only interested in model performance on a training data. In case of collapsing some data into some kind of consensus they are perfectly ok.&lt;br&gt;&#10;In case of making a prediction machine for some real-world problem the first is false, since your training set represent only a scrap of information about the problem you are dealing with, so you just can't optimize your model; the second is false, because you expect that your model will handle the new data for which you can't even expect that the training set will be representative. &#10;And to this end CV was invented; to simulate the behavior of the model when confronted with an independent data. In case of model selection, CV gives you not only the quality approximate, but also quality approximation distribution, so it has this great advantage that it can say &quot;I don't know, whatever the new data will come, either of them can be better.&quot;  &lt;/p&gt;&#10;" CommentCount="11" CreationDate="2010-07-23T21:23:18.427" Id="579" LastActivityDate="2014-04-11T11:31:10.767" LastEditDate="2014-04-11T11:31:10.767" LastEditorUserId="17230" OwnerUserId="88" ParentId="577" PostTypeId="2" Score="6" />
  
  <row Body="&lt;p&gt;I recall some discussion on this in the past; I'm not aware of any implementation of Maxwell &amp;amp; Delaney's approach, although it shouldn't be too difficult to do.  Have a look at &quot;&lt;a href=&quot;http://gribblelab.org/2009/03/09/repeated-measures-anova-using-r/&quot; rel=&quot;nofollow&quot;&gt;Repeated Measures ANOVA using R&lt;/a&gt;&quot; which also shows one method of addressing the sphericity issue in &lt;a href=&quot;http://en.wikipedia.org/wiki/Tukey%27s_range_test&quot; rel=&quot;nofollow&quot;&gt;Tukey's HSD&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;You might also find &lt;a href=&quot;http://www.r-statistics.com/2010/02/post-hoc-analysis-for-friedmans-test-r-code/&quot; rel=&quot;nofollow&quot;&gt;this description of Friedman's test&lt;/a&gt; of interest.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-07-24T12:09:08.743" Id="593" LastActivityDate="2010-07-24T12:09:08.743" OwnerUserId="5" ParentId="575" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;&lt;strong&gt;As a summary&lt;/strong&gt;, my answer is : if you have an explicit expression or can figure out some how what your distance is measuring (what &quot;differences&quot; it gives weigth to), then you can say what it is better for. An other complementary way to analyse and compare such test is the minimax theory. &lt;/p&gt;&#10;&#10;&lt;p&gt;At the end some test will be good for some alternatives and some for others. For a given set of alternatives it is sometime possible to show if your test has optimal property in the worst case: this is the minimax theory. &lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Some details&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;Hence You can tell about the properties of two different test by regarding the set of alternative for which they are minimax (if such alternative exist) i.e. (using the word of Donoho and Jin) by comparing their &quot;optimal detection boudary&quot; &lt;a href=&quot;http://projecteuclid.org/DPubS?service=UI&amp;amp;version=1.0&amp;amp;verb=Display&amp;amp;handle=euclid.aos/1085408492&quot; rel=&quot;nofollow&quot;&gt;http://projecteuclid.org/DPubS?service=UI&amp;amp;version=1.0&amp;amp;verb=Display&amp;amp;handle=euclid.aos/1085408492&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let me go distance by distance:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;KS distance is obtained calculating supremum of difference between empirical cdf and cdf. Being a suppremum it will be highly sensitive to local alternatives (local change in the cdf) but not with global change (at least using L2 distance between cdf would be less local (Am I openning open door ?)). However, the most important thing is that is uses the cdf. This implies an asymetry: you give more importance to the changes in the tail of your distribution.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Wassertein metric  (what you meant by Kantorovitch Rubinstein ? )  &lt;a href=&quot;http://en.wikipedia.org/wiki/Wasserstein_metric&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Wasserstein_metric&lt;/a&gt; is ubiquitous and hence hard to compare. &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;For the particular case of W2 it has been uses in &lt;a href=&quot;http://projecteuclid.org/DPubS?service=UI&amp;amp;version=1.0&amp;amp;verb=Display&amp;amp;handle=euclid.aos/1017938923&quot; rel=&quot;nofollow&quot;&gt;http://projecteuclid.org/DPubS?service=UI&amp;amp;version=1.0&amp;amp;verb=Display&amp;amp;handle=euclid.aos/1017938923&lt;/a&gt;  and it is related to the L2 distance to inverse of cdf. My understanding is that it gives even more weight to the tails but I think you should read the paper to know more about it. &lt;/li&gt;&#10;&lt;li&gt;For the case of the L1 distance between density function it will highly depend on how you estimate your dentity function from the data... but otherwise it seems to be a &quot;balanced test&quot; not giving importance to tails. &lt;/li&gt;&#10;&lt;/ul&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;To recall and extend the comment I made which complete the answer: &lt;/p&gt;&#10;&#10;&lt;p&gt;I know you did not meant to be exhaustive but you could add Anderson darling statistic (see &lt;a href=&quot;http://en.wikipedia.org/wiki/Anderson%E2%80%93Darling_test&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Anderson%E2%80%93Darling_test&lt;/a&gt;). This made me remind of a paper fromo Jager and Wellner (see &lt;a href=&quot;http://projecteuclid.org/DPubS?service=UI&amp;amp;version=1.0&amp;amp;verb=Display&amp;amp;handle=euclid.aos/1194461721&quot; rel=&quot;nofollow&quot;&gt;http://projecteuclid.org/DPubS?service=UI&amp;amp;version=1.0&amp;amp;verb=Display&amp;amp;handle=euclid.aos/1194461721&lt;/a&gt;) which extands/generalises Anderson darling statistic (and include in particular higher criticism of Tukey). Higher criticism was already shown to be minimax for a wide range of alternatives and the same is done by Jager and Wellner for their extention. I don't think that minimax property has been shown for Kolmogorov test. Anyway, understanding for which type of alternative your test is minimax helps you to know where is its strength, so you should read the paper above.. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-07-25T07:18:44.263" Id="609" LastActivityDate="2010-08-05T05:35:45.267" LastEditDate="2010-08-05T05:35:45.267" LastEditorUserId="223" OwnerUserId="223" ParentId="411" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;It sounds dodgy to me as the trend estimate will be biased near the point where you splice on the false data. An alternative approach is a nonparametric regression smoother such as loess or splines.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-07-25T09:13:59.517" Id="610" LastActivityDate="2010-07-25T09:13:59.517" OwnerUserId="159" ParentId="492" PostTypeId="2" Score="9" />
  
  <row AcceptedAnswerId="625" AnswerCount="1" Body="&lt;p&gt;In engineering, we usually have Handbooks that pretty much dictate the state of the practice. These books are usually devoid of theory and focus on the applied methodology. Is there a forecasting Handbook out there? that solely focuses on the technique and not the background?&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2010-07-26T15:00:23.580" CreationDate="2010-07-26T11:53:11.027" FavoriteCount="2" Id="624" LastActivityDate="2010-09-30T21:25:14.480" LastEditDate="2010-09-30T21:25:14.480" LastEditorUserId="930" OwnerUserId="59" PostTypeId="1" Score="3" Tags="&lt;forecasting&gt;" Title="Forecasting handbooks" ViewCount="241" />
  
  
  <row Body="&lt;p&gt;&lt;img src=&quot;http://bp1.blogger.com/_x7QjiZypFj0/Rp9dcGTsBKI/AAAAAAAAAA0/VWwfWDv6nzM/s400/Outlier.jpg&quot; alt=&quot;Image at bp1.blogger.com.&quot;&gt;&lt;/p&gt;&#10;" CommentCount="2" CommunityOwnedDate="2010-07-26T19:51:45.370" CreationDate="2010-07-26T19:51:45.370" Id="656" LastActivityDate="2010-08-11T08:50:54.893" LastEditDate="2010-08-11T08:50:54.893" LastEditorUserId="509" OwnerUserId="25" ParentId="423" PostTypeId="2" Score="64" />
  <row Body="&lt;p&gt;The principal components of a data matrix are the eigenvector-eigenvalue pairs of its variance-covariance matrix.  In essence, they are the decorrelated pieces of the variance.  Each one is a linear combination of the variables for an observation -- suppose you measure w, x, y,z on each of a bunch of subjects.  Your first PC might work out to be something like&lt;/p&gt;&#10;&#10;&lt;p&gt;0.5w + 4x + 5y - 1.5z&lt;/p&gt;&#10;&#10;&lt;p&gt;The loadings (eigenvectors) here are (0.5, 4, 5, -1.5).  The score (eigenvalue) for each observation is the resulting value when you substitute in the observed (w, x, y, z) and compute the total.&lt;/p&gt;&#10;&#10;&lt;p&gt;This comes in handy when you project things onto their principal components (for, say, outlier detection) because you just plot the scores on each like you would any other data.  This can reveal a lot about your data if much of the variance is correlated (== in the first few PCs).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-07-26T19:58:28.347" Id="659" LastActivityDate="2010-07-26T19:58:28.347" OwnerUserId="317" ParentId="222" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;Probability is a pure science (math), statistics is about data. They are connected since probability forms some kind of fundament for statistics, providing basic ideas.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-07-26T20:18:46.037" Id="667" LastActivityDate="2010-07-26T20:18:46.037" OwnerUserId="88" ParentId="665" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;Probability is about quantifying uncertainty whereas statistics is explaining the variation in some measure of interest (e.g., why do income levels vary?) that we observe in the real world. &lt;/p&gt;&#10;&#10;&lt;p&gt;We explain the variation by using some observable factors (e.g., gender, education level, age etc for the income example). However, since we cannot possibly take into account all possible factors that affect income, we leave any unexplained variation to random errors (which is where quantifying uncertainty comes in).&lt;/p&gt;&#10;&#10;&lt;p&gt;Since, we attribute &quot;Variation = Effect of Observable Factors + Effect of Random Errors&quot; we need the tools provided by probability to account for the effect of random errors on the variation that we observe.&lt;/p&gt;&#10;&#10;&lt;p&gt;Some examples follow:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Quantifying Uncertainty&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Example 1: You roll a 6-sided dice. What is the probability of obtaining a 1?&lt;/p&gt;&#10;&#10;&lt;p&gt;Example 2: What is the probability that the annual income of an adult person selected at random from the United States is less than $40,000?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Explaining Variation&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Example 1: We observe that the annual income of a person varies. What factors explain the variation in a person's income? &lt;/p&gt;&#10;&#10;&lt;p&gt;Clearly, we cannot account for all factors. Thus, we attribute a person's income to some observable factors (e.g, education level, gender, age etc) and leave any remaining variation to uncertainty (or in the language of statistics: to random errors). &lt;/p&gt;&#10;&#10;&lt;p&gt;Example 2: We observe that some consumers choose Tide most of the time they buy a detergent whereas some other consumers choose detergent brand xyz. What explains the variation in choice? We attribute the variation in choices to some observable factors such as price, brand name etc and leave any unexplained variation to random errors (or uncertainty).&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-07-26T20:45:36.923" Id="674" LastActivityDate="2010-07-26T21:01:25.187" LastEditDate="2010-07-26T21:01:25.187" LastEditorDisplayName="user28" OwnerDisplayName="user28" ParentId="665" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;The &lt;strong&gt;probability&lt;/strong&gt; of an event is its long-run relative frequency. So it's basically telling you the &lt;strong&gt;&lt;em&gt;chance&lt;/em&gt;&lt;/strong&gt; of, for example, getting a 'head' on the next flip of a coin, or getting a '3' on the next roll of a die.&lt;/p&gt;&#10;&#10;&lt;p&gt;A &lt;strong&gt;statistic&lt;/strong&gt; is any numerical measure computed from a sample of the population. For example, the sample mean. We use this as a statistic which estimates the population mean, which is a parameter. So basically it's giving you some kind of &lt;strong&gt;&lt;em&gt;summary&lt;/em&gt;&lt;/strong&gt; of a sample.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;em&gt;You can only get a statistic from a&#10;sample, otherwise if you compute a&#10;numerical measure on a population, it&#10;is called a population parameter.&lt;/em&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2010-07-26T21:00:00.617" Id="678" LastActivityDate="2010-07-26T21:05:06.097" LastEditDate="2010-07-26T21:05:06.097" LastEditorUserId="81" OwnerUserId="81" ParentId="665" PostTypeId="2" Score="3" />
  
  
  
  <row Body="&lt;p&gt;Because squares can allow use of many other mathematical operations or functions more easily than absolute values.&lt;/p&gt;&#10;&#10;&lt;p&gt;Example: squares can be integrated, differentiated, can be used in trigonometric, logarithmic and other functions, with ease.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-07-27T00:24:09.637" Id="701" LastActivityDate="2010-07-27T00:24:09.637" OwnerUserId="369" ParentId="118" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;All three are used when dealing with nuisance parameters in the completely specified likelihood function.  &lt;/p&gt;&#10;&#10;&lt;p&gt;The marginal likelihood  is the primary method to eliminate nuisance parameters in theory.  It's a true likelihood function (i.e. it's proportional to the (marginal) probability of the observed data).&lt;/p&gt;&#10;&#10;&lt;p&gt;The partial likelihood is not a true likelihood in general.  However, in some cases it can be treated as a likelihood for asymptotic inference.  For example in Cox proportional hazards models, where it originated, we're interested in the observed rankings in the data (T1 &gt; T2 &gt; ..) without specifying the baseline hazard.  Efron showed that the partial likelihood loses little to no information for a variety of hazard functions.&lt;/p&gt;&#10;&#10;&lt;p&gt;The profile likelihood is convenient when we have a multidimensional likelihood function and a single parameter of interest.  It's specified by replacing the nuisance S by its MLE at each fixed T (the parameter of interest), i.e. L(T) = L(T, S(T)).  This can work well in practice, though there is a potential bias in the MLE obtained in this way; the marginal likelihood corrects for this bias.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-07-27T00:47:25.470" Id="703" LastActivityDate="2010-07-27T00:47:25.470" OwnerUserId="251" ParentId="622" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;This is a very interesting question. Suppose that we have a 2 dimensional covariance matrix (very unrealistic example for SEM but please bear with me). Then you can plot the iso-contours for the observed covariance matrix vis-a-vis the estimated covariance matrix to get a sense of model fit.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, in reality you will a high-dimensional covariance matrix. In such a situation, you could probably do several 2 dimensional plots taking 2 variables at a time. Not the ideal solution but perhaps may help to some extent.  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;A slightly better method is to perform &lt;a href=&quot;http://en.wikipedia.org/wiki/Principal_component_analysis&quot; rel=&quot;nofollow&quot;&gt;Principal Component Analysis (PCA)&lt;/a&gt; on the observed covariance matrix. Save the projection matrix from the PCA analysis on the observed covariance matrix. Use this projection matrix to transform the estimated covariance matrix. &lt;/p&gt;&#10;&#10;&lt;p&gt;We then plot iso-contours for the two highest variances of the rotated observed covariance matrix vis-a-vis the estimated covariance matrix. Depending on how many plots we want to do we can take the second and the third highest variances etc. We start from the highest variances as we want to explain as much variation in our data as possible. &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2010-07-27T01:07:48.393" Id="705" LastActivityDate="2010-07-27T02:14:37.583" LastEditDate="2010-07-27T02:14:37.583" LastEditorDisplayName="user28" OwnerDisplayName="user28" ParentId="570" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;Yet another reason (in addition to the excellent ones above) comes from Fisher himself, who showed that the standard deviation is more &quot;efficient&quot; than the absolute deviation. Here, efficient has to do with how much a statistic will fluctuate in value on different samplings from a population. If your population is normally distributed, the standard deviation of various samples from that population will, on average, tend to give you values that are pretty similar to each other, whereas the absolute deviation will give you numbers that spread out a bit more. Now, obviously this is in ideal circumstances, but this reason convinced a lot of people (along with the math being cleaner), so most people worked with standard deviations.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-07-27T01:51:15.673" Id="706" LastActivityDate="2010-07-27T01:51:15.673" OwnerUserId="378" ParentId="118" PostTypeId="2" Score="10" />
  <row Body="&lt;p&gt;Let me give you a very very intuitional insight.  Suppose you are tossing a coin 10 times and you get 8 heads and 2 tails. The question that would come to your mind is whether this coin is biased towards heads or not. &lt;/p&gt;&#10;&#10;&lt;p&gt;Now if you go by conventional definitions or the frequentist approach of probability you might say that the coin is unbiased and this is an exceptional occurrence. Hence you would conclude that the possibility of getting a head next toss is also 50%. &lt;/p&gt;&#10;&#10;&lt;p&gt;But suppose you are a Bayesian.  You would actually think that since you have got exceptionally high number of heads, the coin has a bias towards the head side. There are methods to calculate this possible bias. You would calculate them and then when you toss the coin next time, you would definitely call a heads.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, Bayesian probability is about the belief that you develop based on the data you observe. I hope that was simple enough.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-07-27T02:31:38.587" Id="708" LastActivityDate="2011-02-02T19:11:03.597" LastEditDate="2011-02-02T19:11:03.597" LastEditorUserId="509" OwnerUserId="25692" ParentId="672" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;The &lt;a href=&quot;http://www.ats.ucla.edu/stat/&quot; rel=&quot;nofollow&quot;&gt;UCLA Statistical Computing&lt;/a&gt; site has a number of examples in various languages (SAS, R, etc).  In particular, see the following pages (look among the links titled logistic regression, categorical data analysis and generalized linear models):&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://www.ats.ucla.edu/stat/dae/&quot; rel=&quot;nofollow&quot;&gt;Data Analysis Examples&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://www.ats.ucla.edu/stat/examples/default.htm&quot; rel=&quot;nofollow&quot;&gt;Textbook Examples&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2010-07-27T05:40:44.950" Id="721" LastActivityDate="2010-07-27T05:40:44.950" OwnerUserId="251" ParentId="39" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;The &lt;a href=&quot;http://sourceforge.net/projects/anony-toolkit/&quot;&gt;Cornell Anonymization Tookit&lt;/a&gt; is open source.  Their &lt;a href=&quot;http://www.cs.cornell.edu/bigreddata/privacy/&quot;&gt;research page&lt;/a&gt; has links to associated publications.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-07-27T06:01:10.010" Id="724" LastActivityDate="2011-04-13T10:35:33.393" LastEditDate="2011-04-13T10:35:33.393" LastEditorUserId="930" OwnerUserId="251" ParentId="712" PostTypeId="2" Score="9" />
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;All we know about the world teaches us that the effects of A and B are always different---in some decimal place---for any A and B. Thus asking &quot;are the effects different?&quot; is foolish.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Tukey (again but this one is my favorite)&lt;/p&gt;&#10;" CommentCount="2" CommunityOwnedDate="2010-07-27T06:26:04.483" CreationDate="2010-07-27T06:26:04.483" Id="728" LastActivityDate="2010-07-27T06:42:07.557" LastEditDate="2010-07-27T06:42:07.557" LastEditorUserId="159" OwnerUserId="223" ParentId="726" PostTypeId="2" Score="37" />
  
  <row Body="&lt;p&gt;I don't know about famous, but the following is one of my favourites:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Conducting data analysis is like&#10;  drinking a fine wine. It is important&#10;  to swirl and sniff the wine, to unpack&#10;  the complex bouquet and to appreciate&#10;  the experience. Gulping the wine&#10;  doesn’t work.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;-Daniel B. Wright (2003), see &lt;a href=&quot;http://www.baomee.info/pdf/MakeFriends/1.pdf&quot;&gt;PDF of Article&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Reference&lt;/strong&gt;:&#10;Wright, D. B. (2003). Making friends with your data: Improving how statistics are conducted and reported1. British Journal of Educational Psychology, 73(1), 123-136.&lt;/p&gt;&#10;" CommentCount="1" CommunityOwnedDate="2010-07-27T09:44:12.930" CreationDate="2010-07-27T09:44:12.930" Id="755" LastActivityDate="2013-02-25T00:40:34.507" LastEditDate="2013-02-25T00:40:34.507" LastEditorUserId="183" OwnerUserId="183" ParentId="726" PostTypeId="2" Score="36" />
  
  <row Body="&lt;p&gt;One major benefit of mixed-effects models is that they don't assume independence amongst observations, and there can be a correlated observations within a unit or cluster.&lt;/p&gt;&#10;&#10;&lt;p&gt;This is covered concisely in &quot;Modern Applied Statistics with S&quot; (MASS) in the first section of chapter 10 on &quot;Random and Mixed Effects&quot;.  V&amp;amp;R walk through an example with gasoline data comparing ANOVA and lme in that section, so it's a good overview.  The R function to be used in &lt;code&gt;lme&lt;/code&gt; in the &lt;code&gt;nlme&lt;/code&gt; package.  &lt;/p&gt;&#10;&#10;&lt;p&gt;The model formulation is based on Laird and Ware (1982), so you can refer to that as a primary source although it's certainly not good for an introduction.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Laird, N.M. and Ware, J.H. (1982) &quot;Random-Effects Models for Longitudinal Data&quot;, Biometrics, 38, 963–974.&lt;/li&gt;&#10;&lt;li&gt;Venables, W.N. and Ripley, B.D. (2002) &quot;&lt;a href=&quot;http://www.stats.ox.ac.uk/pub/MASS4/&quot;&gt;Modern Applied Statistics with S&lt;/a&gt;&quot;, 4th Edition, Springer-Verlag.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;You can also have a look at the &lt;a href=&quot;http://cran.r-project.org/doc/contrib/Fox-Companion/appendix-mixed-models.pdf&quot;&gt;&quot;Linear Mixed Models&quot;&lt;/a&gt; (PDF) appendix to John Fox's &quot;An R and S-PLUS Companion to Applied Regression&quot;.  And &lt;a href=&quot;http://idiom.ucsd.edu/~rlevy/lign251/fall2007/lecture_14.pdf&quot;&gt;this lecture by Roger Levy&lt;/a&gt; (PDF) discusses mixed effects models w.r.t. a multivariate normal distribution.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-07-27T13:07:38.643" Id="772" LastActivityDate="2010-07-27T13:07:38.643" OwnerUserId="5" ParentId="764" PostTypeId="2" Score="11" />
  
  <row Body="&lt;p&gt;Operations Research (OR), sometimes called &quot;Management Science&quot;, consists of three main topics, Optimization, Stochastic Processes, Process and Production Methodologies. &lt;/p&gt;&#10;&#10;&lt;p&gt;OR uses statistical analysis in many contexts (for example discrete event simulations) but they should not be considered the same, additionally one of the main topics in OR is optimization (linear, and nonlinear) which can make it more clear why these two fields should be considered different &lt;/p&gt;&#10;&#10;&lt;p&gt;There is &lt;a href=&quot;http://www.or-exchange.com/questions&quot;&gt;another exchange website for OR&lt;/a&gt; if you are interested &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-07-27T13:55:18.623" Id="780" LastActivityDate="2010-07-27T19:10:25.243" LastEditDate="2010-07-27T19:10:25.243" LastEditorUserId="172" OwnerUserId="172" ParentId="775" PostTypeId="2" Score="9" />
  
  
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;The subjectivist (i.e. Bayesian)&#10;  states his judgements, whereas the&#10;  objectivist sweeps them under the&#10;  carpet by calling assumptions&#10;  knowledge, and he basks in the&#10;  glorious objectivity of science.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I.J. Good&lt;/p&gt;&#10;" CommentCount="2" CommunityOwnedDate="2010-07-27T15:05:38.800" CreationDate="2010-07-27T15:05:38.800" Id="794" LastActivityDate="2010-07-27T15:05:38.800" LastEditorUserId="7620" OwnerDisplayName="John A. Ramey" OwnerUserId="7620" ParentId="726" PostTypeId="2" Score="29" />
  <row Body="&lt;p&gt;The main advantage of LME for analysing accuracy data is the ability to account for a series of random effects. In psychology experiments, researchers usually aggregate items and/or participants. Not only are people different from each other, but items also differ (some words might be more distinctive or memorable, for instance). Ignoring these sources of variability usually leads to underestimations of accuracy (for instance lower d' values). Although the participant aggregation issue can somehow be dealt with  individual estimation, the item effects are still there, and are commonly larger than participant effects. LME not only allows you to tackle both random effects simultaneously, but also to add specificy additional predictor variables (age, education level, word length, etc.) to them.&lt;/p&gt;&#10;&#10;&lt;p&gt;A really good reference for LMEs, especially focused in the fields of linguistics and experimental psychology, is &#10;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0521709180&quot; rel=&quot;nofollow&quot;&gt;Analyzing Linguistic Data: A Practical Introduction to Statistics using R&lt;/a&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;cheers&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-07-27T15:26:34.537" Id="799" LastActivityDate="2010-07-27T15:26:34.537" OwnerUserId="447" ParentId="764" PostTypeId="2" Score="7" />
  
  
  
  
  
  <row Body="&lt;p&gt;If it's on Linux, then the most straight-forward is &lt;a href=&quot;http://cran.r-project.org/web/packages/multicore/index.html&quot;&gt;&lt;strong&gt;multicore&lt;/strong&gt;&lt;/a&gt;.  Beyond that, I suggest having a look at &lt;a href=&quot;http://www.stats.uwo.ca/faculty/yu/Rmpi/&quot;&gt;MPI&lt;/a&gt; (especially with the &lt;a href=&quot;http://cran.r-project.org/web/packages/snow/index.html&quot;&gt;&lt;strong&gt;snow&lt;/strong&gt;&lt;/a&gt; package).&lt;/p&gt;&#10;&#10;&lt;p&gt;More generally, have a look at:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;The &lt;a href=&quot;http://cran.r-project.org/web/views/HighPerformanceComputing.html&quot;&gt;High-Performance Computing view&lt;/a&gt; on CRAN.&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://cran.r-project.org/web/views/HighPerformanceComputing.html&quot;&gt;&quot;State of the Art in Parallel Computing with R&quot;&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Lastly, I recommend using the &lt;a href=&quot;http://cran.r-project.org/web/packages/foreach/index.html&quot;&gt;foreach&lt;/a&gt; package to abstract away the parallel backend in your code.  That will make it more useful in the long run.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2010-07-27T17:06:30.563" Id="827" LastActivityDate="2010-07-27T17:06:30.563" OwnerUserId="5" ParentId="825" PostTypeId="2" Score="13" />
  <row Body="&lt;p&gt;If you use Stata, you can use the -clt- command that creates graphs of sampling distributions, see&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.ats.ucla.edu/stat/stata/ado/teach/clt.htm&quot; rel=&quot;nofollow&quot;&gt;http://www.ats.ucla.edu/stat/stata/ado/teach/clt.htm&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="1" CommunityOwnedDate="2011-06-02T11:09:11.620" CreationDate="2010-07-27T17:37:35.300" Id="832" LastActivityDate="2010-07-27T17:37:35.300" OwnerDisplayName="Michael Mitchell" ParentId="643" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="844" AnswerCount="5" Body="&lt;p&gt;I have $N$ paired observations ($X_i$, $Y_i$) drawn from a common unknown distribution, which has finite first and second moments, and is symmetric around the mean.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $\sigma_X$ the standard deviation of $X$ (unconditional on $Y$), and $\sigma_Y$ the same for Y. I would like to test the hypothesis  &lt;/p&gt;&#10;&#10;&lt;p&gt;$H_0$: $\sigma_X = \sigma_Y$&lt;/p&gt;&#10;&#10;&lt;p&gt;$H_1$: $\sigma_X \neq \sigma_Y$&lt;/p&gt;&#10;&#10;&lt;p&gt;Does anyone know of such a test? I can assume in first analysis that the distribution is normal, although the general case is more interesting. I am looking for a closed-form solution. Bootstrap is always a last resort.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-07-27T18:38:02.297" FavoriteCount="2" Id="841" LastActivityDate="2011-03-13T20:31:13.040" LastEditDate="2011-02-10T07:50:44.253" LastEditorUserId="223" OwnerUserId="30" PostTypeId="1" Score="14" Tags="&lt;distributions&gt;&lt;hypothesis-testing&gt;&lt;standard-deviation&gt;&lt;normal-distribution&gt;" Title="Comparing the variance of paired observations" ViewCount="1223" />
  
  
  <row Body="&lt;p&gt;I'm not sure why this doesn't work with &lt;code&gt;lrm&lt;/code&gt;.  However, R does Logistic Regression just fine with its own internal functions.  See GLM.  Here's your model, working...&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;summary(glm(am~1, data = mtcars, family=binomial(link=logit)))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So, unless you need something that &lt;code&gt;lrm()&lt;/code&gt; from design provides, then use GLM with the binomial logit link.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-07-27T20:33:48.787" Id="858" LastActivityDate="2013-08-09T00:48:47.660" LastEditDate="2013-08-09T00:48:47.660" LastEditorUserId="7290" OwnerUserId="485" ParentId="837" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Just to expand on Rob's answer a bit, suppose that we want to know the cumulative distribution function (CDF) of the highest value of $N$ independent draws from a standard normal distribution, $X_1, ..., X_N$. Call this highest value $Y_1$, the first order statistic. Then the CDF is:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \begin{align*}P(Y_1 &amp;lt; x) &amp;amp;= P(\max(X_1, ..., X_N) &amp;lt; x) \\
&#10;\\
  <row AcceptedAnswerId="934" AnswerCount="5" Body="&lt;p&gt;The 'fundamental' idea of statistics for estimating parameters is &lt;a href=&quot;http://en.wikipedia.org/wiki/Maximum_likelihood&quot;&gt;maximum likelihood&lt;/a&gt;. I am wondering what is the corresponding idea in machine learning.&lt;/p&gt;&#10;&#10;&lt;p&gt;Qn 1. Would it be fair to say that the 'fundamental' idea in machine learning for estimating parameters is: 'Loss Functions'&lt;/p&gt;&#10;&#10;&lt;p&gt;[Note: It is my impression that machine learning algorithms often optimize a loss function and hence the above question.]&lt;/p&gt;&#10;&#10;&lt;p&gt;Qn 2: Is there any literature that attempts to bridge the gap between statistics and machine learning?&lt;/p&gt;&#10;&#10;&lt;p&gt;[Note: Perhaps, by way of relating loss functions to maximum likelihood. (e.g., OLS is equivalent to maximum likelihood for normally distributed errors etc)]&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2010-07-28T11:31:59.857" FavoriteCount="4" Id="886" LastActivityDate="2010-08-04T09:07:44.193" OwnerDisplayName="user28" PostTypeId="1" Score="9" Tags="&lt;machine-learning&gt;&lt;maximum-likelihood&gt;&lt;loss-functions&gt;" Title="What is the 'fundamental' idea of machine learning for estimating parameters?" ViewCount="1927" />
  <row AcceptedAnswerId="905" AnswerCount="1" Body="&lt;p&gt;What is the difference between offline and &lt;a href=&quot;http://en.wikipedia.org/wiki/Online_machine_learning&quot;&gt;online learning&lt;/a&gt;?  Is it just a matter of learning over the entire dataset (offline) vs. learning incrementally (one instance at a time)?  What are examples of algorithms used in both?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-07-28T13:32:32.843" FavoriteCount="4" Id="897" LastActivityDate="2014-08-21T13:48:17.960" LastEditDate="2010-08-07T17:50:51.657" LastEditorUserId="88" OwnerUserId="284" PostTypeId="1" Score="8" Tags="&lt;machine-learning&gt;" Title="Online vs offline learning?" ViewCount="3175" />
  
  <row Body="&lt;p&gt;What you are looking for is called regression; there are a lot of methods you can do it, both statistical and machine learning ones. If you want to find f, you must use statistics; in that case you must first assume that f is of some form, like f:y=a*x+b and then use some regression method to fit the parameters.&lt;br&gt;&#10;The plot suggests there are a lot of outliers (elements that does not follow f(x)); you may need robust regression to get rid of them.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-07-28T16:21:41.483" Id="914" LastActivityDate="2010-07-28T16:31:04.797" LastEditDate="2010-07-28T16:31:04.797" LastEditorUserId="88" OwnerUserId="88" ParentId="913" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;Like drknexus said, for a logistic regression, your outcome measure needs to be 0 and 1. I'd go back and recode your outcome as 0 (didn't like it), or 1 (did like it). Then, abandon excel and load the data into R (it's really not as intimidating as it looks).  Your regression will look something like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;glm(Liked ~ Visually.Stunning + Exhilarating + Artistic + Sporty, family = binomial, data = data)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The regression will return betas for each feature in terms of log-odds. So, for every 1 point increase in &lt;code&gt;Artistic&lt;/code&gt;, for instance, you'll have a value for how much that increases or decreases the log-odds of your enjoyment. Most of the betas will be positive, unless you &lt;em&gt;dislike&lt;/em&gt; sporty games or something.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, you'll have to ask yourself some interesting questions. The assumption of the model is that the values on each of these scores affect your enjoyment &lt;em&gt;independently&lt;/em&gt;, which probably isn't true! A game that is very Visually.Stunning and Exhilarating is probably way better than you would expect given those component parts. And it's probably the case that if a game gets scores of 1 on all features except Sporty, which gets a 4, that high Sporty score is worth less than if the other scores were higher.&lt;/p&gt;&#10;&#10;&lt;p&gt;That is, many or all of your features probably &lt;em&gt;interact&lt;/em&gt;. To fit an accurate model, then, you'll want to add in these interactions. That formula would look like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;glm(Liked ~ Visually.Stunning * Exhilarating * Artistic * Sporty, family = binomial, data = data)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now, there are two points of difficulty here. First, you need to have more data to fit a good model with this many interactions than the pure independence model. Second, you risk overfitting, which means that the model will very accurately describe the original data, but will be less good at making accurate predictions for future data.&lt;/p&gt;&#10;&#10;&lt;p&gt;Needless to say, some people spend all day fitting and refitting models like this one. &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2010-07-28T17:01:12.903" Id="917" LastActivityDate="2010-07-28T17:01:12.903" OwnerUserId="287" ParentId="868" PostTypeId="2" Score="5" />
  
  
  <row AcceptedAnswerId="932" AnswerCount="3" Body="&lt;p&gt;This one is bothering me for a while, and a great dispute was held around it. In psychology (as well as in other social sciences), we deal with different ways of dealing with numbers :-) i.e. &lt;strong&gt;the levels of measurement&lt;/strong&gt;. It's also common practice in psychology to standardize some questionnaire, hence transform the data into percentile scores (in order to assess a respondent's position within the representative sample).&lt;/p&gt;&#10;&#10;&lt;p&gt;Long story short, if you have a variable that holds the data expressed in percentile scores, how should you treat it? As an ordinal, interval, or even ratio variable?!&lt;/p&gt;&#10;&#10;&lt;p&gt;It's not ratio, cause there no real 0 (0&lt;sup&gt;th&lt;/sup&gt; percentile doesn't imply absence of measured property, but the variable's smallest value). I advocate the view that percentile scores are ordinal, since P&lt;sub&gt;70&lt;/sub&gt; - P&lt;sub&gt;50&lt;/sub&gt; is not equal to P&lt;sub&gt;50&lt;/sub&gt; - P&lt;sub&gt;30&lt;/sub&gt;, while the other side says it's interval. &lt;/p&gt;&#10;&#10;&lt;p&gt;Please gentlemen, cut the cord. Ordinal or interval?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2010-07-28T17:57:51.697" FavoriteCount="1" Id="928" LastActivityDate="2014-08-21T19:29:36.970" LastEditDate="2010-08-07T17:48:50.100" LastEditorUserId="88" OwnerUserId="1356" PostTypeId="1" Score="5" Tags="&lt;measurement&gt;" Title="Measurement level of percentile scores" ViewCount="2644" />
  <row AnswerCount="1" Body="&lt;p&gt;How comprehensive is the following book - What interpretations are missing?&lt;/p&gt;&#10;&#10;&lt;p&gt;Interpretations of Probability, Andrei Khrennikov, 2009, de Gruyter, ISBN 978-3-11-020748-4 &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.degruyter.com/cont/fb/ma/detailEn.cfm?isbn=9783110207484&amp;amp;sel=pi&quot; rel=&quot;nofollow&quot;&gt;http://www.degruyter.com/cont/fb/ma/detailEn.cfm?isbn=9783110207484&amp;amp;sel=pi&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Contents:http://www.degruyter.com/files/pdf/9783110207484Contents.pdf&lt;/p&gt;&#10;" ClosedDate="2010-07-29T23:43:25.867" CommentCount="4" CreationDate="2010-07-28T17:59:42.600" FavoriteCount="1" Id="929" LastActivityDate="2010-07-28T19:14:49.470" OwnerUserId="560" PostTypeId="1" Score="0" Tags="&lt;probability&gt;" Title="Probability Interpretations" ViewCount="200" />
  
  
  <row AcceptedAnswerId="947" AnswerCount="3" Body="&lt;p&gt;When I type a left paren or any quote in the R console, it automatically creates a matching one to the right of my cursor. I guess the idea is that I can just type the expression I want inside without having to worry about matching, but I find it annoying, and would rather just type it myself. How can I disable this feature?&lt;/p&gt;&#10;&#10;&lt;p&gt;I am using R 2.8.0 on OSX 10.5.8.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2010-07-28T21:45:46.273" Id="944" LastActivityDate="2010-07-29T02:11:34.563" OwnerDisplayName="anonymous" PostTypeId="1" Score="1" Tags="&lt;r&gt;" Title="How can I get R to stop autocompleting my quotes/parens?" ViewCount="170" />
  <row AcceptedAnswerId="963" AnswerCount="2" Body="&lt;p&gt;New to the site.  I am just getting started with R, and want to replicate a feature that is available in SPSS.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Simply, I build a &quot;Custom Table&quot; in SPSS with a single categorical variable in the column and many continous/scale variables in the rows (no interactions, just stacked on top of each other).  &lt;/p&gt;&#10;&#10;&lt;p&gt;The table reports the means and valid N's for each column (summary statistics are in the rows), and select the option to generate significance tests for column means (each column against the others) using alpha .05 and adjust for unequal variances.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is my question.&lt;/p&gt;&#10;&#10;&lt;p&gt;How can I replicate this in R?  What is my best option to build this table  and what tests are available that will get me to the same spot?  Since I am getting used to R, I am still trying to navigate around what is available.&lt;/p&gt;&#10;&#10;&lt;p&gt;Many thanks in advance!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-07-28T21:50:29.093" FavoriteCount="1" Id="946" LastActivityDate="2010-07-29T08:22:43.180" OwnerUserId="569" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;mean&gt;&lt;statistical-significance&gt;&lt;spss&gt;" Title="Column Means Significance Tests in R" ViewCount="737" />
  
  <row Body="&lt;p&gt;Wavelets are useful to detect singularities in a signal (see for example the paper &lt;a href=&quot;http://www.math.univ-toulouse.fr/~bigot/Site/Publications_files/Spectrometry.pdf&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; (see figure 3 for an illustration) and the references mentioned in this paper.  I guess singularities can sometimes be an anomaly? &lt;/p&gt;&#10;&#10;&lt;p&gt;The idea here is that the Continuous wavelet transform &lt;strong&gt;(CWT)&lt;/strong&gt; has maxima lines that propagates along frequencies, i.e. the longer the line is, the higher is the singularity. See Figure 3 in the paper to see what I mean! note that there is free Matlab code related to that paper, it should be &lt;a href=&quot;http://www.math.univ-toulouse.fr/~bigot/Site/Software.html&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Additionally, I can give you some heuristics detailing &lt;strong&gt;why&lt;/strong&gt; the DISCRETE (preceding example is about the continuous one) wavelet transform (&lt;strong&gt;DWT&lt;/strong&gt;) &lt;strong&gt;is interesting for a statistician&lt;/strong&gt; (excuse non-exhaustivity) :   &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;There is a wide class of (realistic (Besov space)) signals that are transformed into a &lt;strong&gt;sparse&lt;/strong&gt; sequence by the wavelet transform. (&lt;strong&gt;compression property&lt;/strong&gt;)&lt;/li&gt;&#10;&lt;li&gt;A wide class of (quasi-stationary) processes that are transformed into a sequence with almost uncorrelated features (&lt;strong&gt;decorrelation property&lt;/strong&gt;)&lt;/li&gt;&#10;&lt;li&gt;Wavelet coefficients contain information that is localized in &lt;strong&gt;time and in frequency&lt;/strong&gt; (at different scales). (multi-scale property)&lt;/li&gt;&#10;&lt;li&gt;Wavelet coefficients of a signal &lt;strong&gt;concentrate on its singularities&lt;/strong&gt;. &lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="1" CreationDate="2010-07-29T07:10:39.967" Id="962" LastActivityDate="2014-06-06T08:57:07.343" LastEditDate="2014-06-06T08:57:07.343" LastEditorUserId="13465" OwnerUserId="223" ParentId="942" PostTypeId="2" Score="11" />
  <row Body="&lt;p&gt;As I read in help for the &lt;code&gt;t.test&lt;/code&gt;, it is only applicable for the 2-sample tests. If you want to perform it to every combination of the columns of a matrix A, taken 2 at a time, you could do something like this (for the moment, I can't recall a better way)&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;apply(combn(1:dim(A)[2],2),2,function(x) t.test(A[,x[1]],A[,x[2]])) &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;or, if you just want the p.values&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;t(apply(combn(1:dim(A)[2],2),2,function(x) c(x[1],x[2],(t.test(A[,x[1]],A[,x[2]]))$p.value)))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2010-07-29T08:15:59.907" Id="963" LastActivityDate="2010-07-29T08:22:43.180" LastEditDate="2010-07-29T08:22:43.180" LastEditorUserId="339" OwnerUserId="339" ParentId="946" PostTypeId="2" Score="3" />
  
  <row AcceptedAnswerId="978" AnswerCount="3" Body="&lt;p&gt;When we are monitoring movements of structures we normally install monitoring points onto the structure before we do any work which might cause movement. This gives us chance to take a few readings before we start doing the work to 'baseline' the readings. &lt;/p&gt;&#10;&#10;&lt;p&gt;Quite often the data is quite variable (the variations in the reading can easily be between 10 and 20% of the fianl movement). The measurements are also often affected by the environment in which they are taken so one set of measurements taken on one project may not have the same accuracy as measurements on another project. &lt;/p&gt;&#10;&#10;&lt;p&gt;Is there any statisitcal method, or rule of thumb that can be applied to say how many baseline readings need to be taken to give a certain accuracy before the first reading is taken? Are there any rules of humb that can be applied to this situation? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-07-29T13:43:53.010" FavoriteCount="1" Id="977" LastActivityDate="2010-07-30T15:37:06.190" OwnerUserId="210" PostTypeId="1" Score="2" Tags="&lt;variance&gt;&lt;measurement&gt;" Title="How many measurements are needed to 'baseline' a measurement?" ViewCount="414" />
  <row Body="&lt;p&gt;I think you should look at &lt;a href=&quot;http://en.wikipedia.org/wiki/Statistical_power&quot; rel=&quot;nofollow&quot;&gt;power calculations&lt;/a&gt;. These are often used to decide the sample size of survey or clinical trial. Taken from wikipedia:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;A priori power analysis is conducted&#10;  prior to the research study, and is&#10;  typically used to determine an&#10;  appropriate sample size to achieve&#10;  adequate power.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="0" CreationDate="2010-07-29T13:51:07.953" Id="978" LastActivityDate="2010-07-29T13:51:07.953" OwnerUserId="8" ParentId="977" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;&lt;em&gt;Collaborative Statistics&lt;/em&gt; is CC BY: &lt;a href=&quot;http://cnx.org/content/col10522/latest/&quot; rel=&quot;nofollow&quot;&gt;http://cnx.org/content/col10522/latest/&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2010-07-29T16:02:18.907" CreationDate="2010-07-29T16:02:18.907" Id="995" LastActivityDate="2010-07-29T16:02:18.907" OwnerDisplayName="Nicole" ParentId="614" PostTypeId="2" Score="3" />
  
  
  <row Body="&lt;p&gt;In general, the standard deviation of a continous uniform distribution is (max - min) / sqrt(12).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-07-29T21:54:24.130" Id="1014" LastActivityDate="2010-07-29T21:54:24.130" OwnerUserId="614" ParentId="1012" PostTypeId="2" Score="8" />
  
  
  <row Body="&lt;p&gt;Calculating averages leads to a different dataset than simply reducing the number of data points.&#10;If one heartbeat per minute is much faster than the other heart beats you will lose the signal through your smoothing process.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you summary 125-125-0-125-125 as 100 than the story that the data tells is different through your smoothing.&lt;/p&gt;&#10;&#10;&lt;p&gt;Sometimes the heart even skips beats and I believe that's an event that interesting for however wants to look at plotted heart rate data.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would therefore propose that you calculate the distance between two points with a formula like &lt;code&gt;d=sqrt((time1-time2)^2 + (bpm1-bpm2))&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;You set a minimum distance in your program.&#10;Then you iterate through your data and after every point you delete all following points for which d is smaller than your minimum distance.&lt;/p&gt;&#10;&#10;&lt;p&gt;As the unit of time and bpm isn't the same you might want to think about how you can find a way to scale the units meaningfully. To do this task right you should speak to the doctors who in the end have to interpret your graphs and ask them what information they consider to be essential.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-07-29T23:45:46.407" Id="1025" LastActivityDate="2010-07-31T02:09:30.503" LastEditDate="2010-07-31T02:09:30.503" LastEditorUserId="3807" OwnerUserId="3807" ParentId="980" PostTypeId="2" Score="4" />
  
  
  
  <row AcceptedAnswerId="1065" AnswerCount="2" Body="&lt;p&gt;My stats has been self taught, but a lot of material I read point to a dataset having mean 0 and standard deviation of 1.&lt;/p&gt;&#10;&#10;&lt;p&gt;If that is the case then:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Why is mean 0 and SD 1 a nice property to have?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Why does a random variable drawn from this sample equal 0.5?  The chance of drawing 0.001 is the same as 0.5 so this should be flat distribution...&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;When people talk about Z Scores what do they actually mean here?   &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CommunityOwnedDate="2010-07-31T14:29:30.753" CreationDate="2010-07-31T14:29:15.543" FavoriteCount="2" Id="1063" LastActivityDate="2012-08-20T17:02:53.953" LastEditDate="2012-08-20T04:54:05.637" LastEditorUserId="2116" OwnerUserId="353" PostTypeId="1" Score="5" Tags="&lt;probability&gt;" Title="Why are mean 0 and standard deviation 1 distributions always used?" ViewCount="16124" />
  
  <row Body="&lt;p&gt;I second Rob's comment. An increasingly prefered alternative is to include all your variables and shrink them towards 0. See Tibshirani, R. (1996). Regression shrinkage and selection via the lasso.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www-stat.stanford.edu/~tibs/lasso/lasso.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www-stat.stanford.edu/~tibs/lasso/lasso.pdf&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2010-07-31T21:22:55.277" Id="1072" LastActivityDate="2010-07-31T21:22:55.277" OwnerUserId="603" ParentId="1016" PostTypeId="2" Score="4" />
  <row AnswerCount="1" Body="&lt;p&gt;I am trying to assess the significance of the obtained MI matrix. The initial input was a array of 3000 genes by 45 timepoints. MI was computed resulting in a array of 3600 by 3600.  I am thus comparing my results to a shuffled matrix with the same dimensions. I permutate the columns 100 times, thus have 100 results for each element in the matrix. At this stage shall I take the mean for each value in the cell and then overall mean of the matrix MI values to estimate the threshold cutoff? Is taking the mean plus 3SD sensible? Ideally comparison of probability density function between my model and the random should show large discrepancy.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-08-01T11:48:48.670" FavoriteCount="2" Id="1082" LastActivityDate="2010-08-01T18:41:27.523" OwnerDisplayName="CLOCK" PostTypeId="1" Score="3" Tags="&lt;correlation&gt;" Title="How to define the significance threshold for mutual information in terms of probability of that value occurring in surrogate set?" ViewCount="722" />
  
  <row Body="&lt;p&gt;I will give an itemized answer. Can provide more citations on demand, although this is not really controversial.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Statistics is &lt;strong&gt;not&lt;/strong&gt; all about&#10;maximizing (log)-likelihood. That's&#10;anathema to principled bayesians who&#10;just update their posteriors or&#10;propagate their beliefs through an&#10;appropriate model.&lt;/li&gt;&#10;&lt;li&gt;A lot of statistics &lt;strong&gt;is&lt;/strong&gt; about loss&#10;minimization. And so is a lot of&#10;Machine Learning. Empirical loss&#10;minimization has a different meaning&#10;in ML. For a clear, narrative view,&#10;check out Vapnik's &quot;The nature of&#10;statistical learning&quot;&lt;/li&gt;&#10;&lt;li&gt;Machine Learning is &lt;strong&gt;not&lt;/strong&gt; all about&#10;loss minimization. First, because&#10;there are a lot of bayesians in ML;&#10;second, because a number of&#10;applications in ML have to do with&#10;temporal learning and approximate DP.&#10;Sure, there is an objective function,&#10;but it has a very different meaning&#10;than in &quot;statistical&quot; learning.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I don't think there is a gap between the fields, just many different approaches, all overlapping to some degree. I don't feel the need to make them into systematic disciplines with well-defined differences and similarities, and given the speed at which they evolve, I think it's a doomed enterprise anyway.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-08-02T05:16:40.183" Id="1100" LastActivityDate="2010-08-02T05:16:40.183" OwnerUserId="30" ParentId="886" PostTypeId="2" Score="8" />
  
  
  
  
  <row Body="&lt;p&gt;The geometric approach is to consider the least squares projection of $Y$ onto the subspace spanned by $X$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Say you have a model:&lt;/p&gt;&#10;&#10;&lt;p&gt;$E[Y | X] = \beta_{1} X_{1} + \beta_{2} X_{2}$&lt;/p&gt;&#10;&#10;&lt;p&gt;Our estimation space is the plane determined by the vectors $X_{1}$ and $X_{2}$ and the problem is to find coordinates corresponding to $(\beta_{1}, \beta_{2})$ which will describe the vector $\hat{Y}$, a least squares projection of $Y$ on to that plane.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now suppose $X_{1} = 2 X_{2}$, i.e. they're collinear.  Then, the subspace determined by $X_{1}$ and $X_{2}$ is just a line and we have only one degree of freedom.  So we can't determine two values $\beta_{1}$ and $\beta_{2}$ as we were asked.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-08-02T23:26:02.567" Id="1151" LastActivityDate="2013-09-21T22:18:13.130" LastEditDate="2013-09-21T22:18:13.130" LastEditorUserId="17230" OwnerUserId="251" ParentId="1149" PostTypeId="2" Score="10" />
  <row Body="&lt;p&gt;If two regressors are perfectly correlated, their coefficients will be impossible to calculate; it's helpful to consider why they would be difficult to interpret &lt;em&gt;if we could calculate them&lt;/em&gt;.  In fact, this explains why it's difficult to interpret variables that are not perfectly correlated but that are also not truly independent.&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose that our dependent variable is the daily supply of fish in New York, and our independent variables include one for whether it rains on that day and one for the amount of bait purchased on that day.  What we don't realize when we collect our data is that every time it rains, fishermen purchase no bait, and every time it doesn't, they purchase a constant amount of bait.  So Bait and Rain are perfectly correlated, and when we run our regression, we can't calculate their coefficients.  In reality, Bait and Rain are probably not perfectly correlated, but we wouldn't want to include them both as regressors without somehow cleaning them of their endogeneity.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-08-03T02:20:32.477" Id="1155" LastActivityDate="2010-08-03T02:20:32.477" OwnerUserId="672" ParentId="1149" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;No, you can't compute a CI for the difference that way I'm afraid, for the same reason you can't use whether the CIs overlap to judge the statistical significance of the difference. See, for example, &lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;On Judging the Significance of Differences by Examining the Overlap Between Confidence Intervals&quot;&#10;Nathaniel Schenker, Jane F Gentleman. The American Statistician. August 1, 2001, 55(3): 182-186. doi:10.1198/000313001317097960. &#10;&lt;a href=&quot;http://pubs.amstat.org/doi/abs/10.1198/000313001317097960&quot; rel=&quot;nofollow&quot;&gt;http://pubs.amstat.org/doi/abs/10.1198/000313001317097960&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;or:&lt;/p&gt;&#10;&#10;&lt;p&gt;Overlapping confidence intervals or standard error intervals: What do they mean in terms of statistical significance?&#10;Mark E. Payton, Matthew H. Greenstone, and Nathaniel Schenker. Journal of Insect Science 2003; 3: 34. &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC524673/&quot; rel=&quot;nofollow&quot;&gt;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC524673/&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The correct procedure requires you also know the sample sizes of both groups. You can then back-compute the two standard deviations from the CIs and use those to conduct a standard two-sample t-test, or to calculate a standard error of the difference and hence a CI for the difference.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2010-08-03T12:30:00.740" Id="1171" LastActivityDate="2010-08-03T12:30:00.740" OwnerUserId="449" ParentId="1169" PostTypeId="2" Score="10" />
  
  <row Body="&lt;p&gt;I would suggest that it's a lag in teaching. Most people either learn statistics at collage or University. If statistics is not you first degree and instead did a mathematics or computer science degree then you probably only cover the fundamental statistics modules: &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Probability&lt;/li&gt;&#10;&lt;li&gt;Hypothesis testing&lt;/li&gt;&#10;&lt;li&gt;Regression&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;This means that when faced with a problem we try and use what we know to solve the problem. &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Data isn't Normal - take logs.&lt;/li&gt;&#10;&lt;li&gt;Data has annoying outliers - remove them.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Unless you stumble across something else, then it's difficult to do something better. It's really hard using Google to find something if you don't know what it's called!&lt;/p&gt;&#10;&#10;&lt;p&gt;I think with all techniques, it will take awhile before the newer techniques filter down. How long did it take standard hypothesis test to be part of a standard statistics curriculum?&lt;/p&gt;&#10;&#10;&lt;p&gt;BTW, with a statistics degree there will still be a lag in teaching - just a shorter one!&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2010-08-03T17:03:58.947" Id="1185" LastActivityDate="2010-08-03T17:03:58.947" OwnerUserId="8" ParentId="1164" PostTypeId="2" Score="22" />
  <row Body="&lt;p&gt;This thesis may be a starting place: &lt;a href=&quot;https://scholarbank.nus.edu.sg/bitstream/handle/10635/14229/GranelF.pdf?sequence=1&quot; rel=&quot;nofollow&quot;&gt;A Comparative Analysis of Index Decomposition Methods&lt;/a&gt;, by Frédéric Granel.  It should serve as a basic introduction to IDA and the Laspeyre index, but it does not include the Divisa index or any code implementing the methods.  &lt;/p&gt;&#10;&#10;&lt;p&gt;For more detail, including the Divisa index, you might try &lt;a href=&quot;http://www.sciencedirect.com/science?_ob=ArticleURL&amp;amp;_udi=B6V2W-4WR2C4X-3&amp;amp;_user=10&amp;amp;_coverDate=11%2F30%2F2009&amp;amp;_rdoc=1&amp;amp;_fmt=high&amp;amp;_orig=search&amp;amp;_sort=d&amp;amp;_docanchor=&amp;amp;view=c&amp;amp;_searchStrId=1420130047&amp;amp;_rerunOrigin=google&amp;amp;_acct=C000050221&amp;amp;_version=1&amp;amp;_urlVersion=0&amp;amp;_userid=10&amp;amp;md5=5872e15ee97331e2e332f10a8cefb041&quot; rel=&quot;nofollow&quot;&gt;Properties and linkages of some index decomposition analysis methods&lt;/a&gt;. As for implementations in R, there seems to be no package for IDA specifically, but &lt;a href=&quot;http://cran.r-project.org/web/packages/micEcon/micEcon.pdf&quot; rel=&quot;nofollow&quot;&gt;micEcon&lt;/a&gt; and &lt;a href=&quot;http://cran.r-project.org/web/packages/micEconAids/micEconAids.pdf.&quot; rel=&quot;nofollow&quot;&gt;micEconAids&lt;/a&gt; have parts of it, by way of demand analysis.&lt;/p&gt;&#10;&#10;&lt;p&gt;Best of luck.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-08-03T17:24:56.913" Id="1187" LastActivityDate="2010-08-03T17:30:07.610" LastEditDate="2010-08-03T17:30:07.610" LastEditorUserId="39" OwnerUserId="39" ParentId="1184" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Are you entirely sure that they're using the normal distribution directly?  It's very common to use transformed responses to model success rates, but this involves passing through a link function to move from a Gaussian random variable to a value in [0,1].  A commonly used transform is the probit one, which is just the inverse Gaussian CDF. (So you'd end up with something like $\Phi^{-1}(p) = X\beta + \sigma$, where $\Phi$ is the Gaussian CDF).&lt;/p&gt;&#10;&#10;&lt;p&gt;If you're actually using a normal distribution directly to model a result in [0,1], then it strikes me that the variance would have to be so small -- especially for p near 0 or 1 -- that you'd nearly always overfit the model.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-08-03T19:26:55.673" Id="1188" LastActivityDate="2010-08-03T19:26:55.673" OwnerUserId="61" ParentId="1123" PostTypeId="2" Score="1" />
  
  
  
  <row Body="&lt;p&gt;One practical issue that arises here is variable selection in modelling. A variable can be an important explanatory variable (e.g., is statistically significant) but may not be useful for predictive purposes (i.e., its inclusion in the model leads to worse predictive accuracy). I see this mistake almost every day in published papers.&lt;/p&gt;&#10;&#10;&lt;p&gt;Another difference is in the distinction between principal components analysis and factor analysis. PCA is often used in prediction, but is not so useful for explanation. FA involves the additional step of rotation which is done to improve interpretation (and hence explanation). There is a &lt;a href=&quot;http://blog.bzst.com/2010/08/pca-debate.html&quot;&gt;nice post today on Galit Shmueli's blog about this&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Update: a third case arises in time series when a variable may be an important explanatory variable but it just isn't available for the future. For example, home loans may be strongly related to GDP but that isn't much use for predicting future home loans unless we also have good predictions of GDP.&lt;/p&gt;&#10;" CommentCount="11" CommunityOwnedDate="2010-08-04T22:36:49.690" CreationDate="2010-08-03T23:36:08.593" Id="1206" LastActivityDate="2010-08-04T05:17:43.387" LastEditDate="2010-08-04T05:17:43.387" LastEditorUserId="159" OwnerUserId="159" ParentId="1194" PostTypeId="2" Score="9" />
  <row Body="&lt;p&gt;It's like that old joke. When asked for directions the philosopher said &quot;Well, if I wanted to go there, I wouldn't start from here ...&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;While I think each &quot;culture&quot; should be open to learning from the other, they have different ways of looking at the world.&lt;/p&gt;&#10;&#10;&lt;p&gt;I think the problem with learning statistics through studying machine learning algorithms is that, whilst ML algorithms start with statistical concepts, statistics doesn't start with algorithms, but probability models.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-08-04T02:05:29.027" Id="1211" LastActivityDate="2010-08-04T02:05:29.027" OwnerUserId="521" ParentId="770" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="1273" AnswerCount="5" Body="&lt;p&gt;I am using a control chart to try to work on some infection data, and will raise an alert if the infection is considered &quot;out of control&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Problems arrive when I come to a set of data where most of the time points have zero infection, with only a few occasions of one to two infections, but these already exceed the control limit of the chart, and raise an alert.&lt;/p&gt;&#10;&#10;&lt;p&gt;How should I work on the control chart if the data set is having very few positive infection counts?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-08-04T12:05:30.000" FavoriteCount="1" Id="1228" LastActivityDate="2010-09-16T06:59:23.943" LastEditDate="2010-09-16T06:59:23.943" LastEditorUserId="88" OwnerUserId="588" PostTypeId="1" Score="8" Tags="&lt;data-visualization&gt;&lt;control-chart&gt;" Title="How to interpret a control chart containing a majority of zero values?" ViewCount="304" />
  <row Body="&lt;p&gt;The problem is definitely &lt;em&gt;hard&lt;/em&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;Mechanical rules like the +/- &lt;em&gt;N1&lt;/em&gt; times standard deviations, or +/ &lt;em&gt;N2&lt;/em&gt; times MAD,  or +/- &lt;em&gt;N3&lt;/em&gt; IQR or ... &lt;em&gt;will&lt;/em&gt; fail because there are always some series that are different as for example:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;fixings like interbank rate may be constant for some time and then jump all of a sudden&lt;/li&gt;&#10;&lt;li&gt;similarly for &lt;em&gt;e.g.&lt;/em&gt; certain foreign exchanges coming off a peg&lt;/li&gt;&#10;&lt;li&gt;certain instrument are implicitly spreads; these may be near zero for periods and all of a sudden jump manifold&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Been there, done that, ... in a previous job.  You could try to bracket each series using arbitrage relations ships (&lt;em&gt;e.g.&lt;/em&gt; assuming USD/EUR and EUR/JPY are presumed good, you can work out bands around what USD/JPY should be; likewise for derivatives off an underlying etc pp.&lt;/p&gt;&#10;&#10;&lt;p&gt;Commercial data vendors expand some effort on this, and those of use who are clients of theirs know ... it still does not exclude errors.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2010-08-04T15:51:53.803" Id="1245" LastActivityDate="2010-08-04T15:51:53.803" OwnerUserId="334" ParentId="1223" PostTypeId="2" Score="9" />
  <row Body="&lt;p&gt;The shortest answer is that traditional tests of signal detection only give you a single point on the ROC (receiver operating characteristic) while the curve allows you to see responses through a range of values.  It's possible that the criteria and d' shift as one shifts throughout the curve.  It's like the difference between a t-test generated by selecting two classes of predictor variables and two regression lines generated by looking at parametric manipulations of each predictor variable.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-08-04T15:56:39.150" Id="1247" LastActivityDate="2010-08-04T15:56:39.150" OwnerUserId="601" ParentId="1241" PostTypeId="2" Score="6" />
  <row AcceptedAnswerId="2338" AnswerCount="4" Body="&lt;p&gt;The following question is one of those holy grails for me for some time now, I hope someone might be able to offer a good advice.&lt;/p&gt;&#10;&#10;&lt;p&gt;I wish to perform a non-parametric repeated measures multiway anova using R.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have been doing some online searching and reading for some time, and so far was able to find solutions for only some of the cases: friedman test for one way nonparametric repeated measures anova, ordinal regression with {car} Anova function for multi way nonparametric anova, and so on.  The partial solutions is NOT what I am looking for in this question thread.  I have summarized my findings so far in a post I published some time ago (titled: &lt;a href=&quot;http://www.r-statistics.com/2010/04/repeated-measures-anova-with-r-tutorials/&quot;&gt;Repeated measures ANOVA with R (functions and tutorials)&lt;/a&gt;, in case it would help anyone) &lt;/p&gt;&#10;&#10;&lt;p&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;If what I read online is true, this task might be achieved using a mixed Ordinal Regression model (a.k.a: Proportional Odds Model).&lt;/p&gt;&#10;&#10;&lt;p&gt;I found two packages that seems relevant, but couldn't find any vignette on the subject:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://cran.r-project.org/web/packages/repolr/&quot;&gt;http://cran.r-project.org/web/packages/repolr/&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://cran.r-project.org/web/packages/ordinal/&quot;&gt;http://cran.r-project.org/web/packages/ordinal/&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;So being new to the subject matter, I was hoping for some directions from people here.&lt;/p&gt;&#10;&#10;&lt;p&gt;Are there any tutorials/suggested-reading on the subject?  Even better, can someone suggest a simple example code for how to run and analyse this in R (e.g: &quot;non-parametric repeated measures multiway anova&quot;) ?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for any help,&#10;Tal&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2010-08-04T20:01:07.787" FavoriteCount="9" Id="1266" LastActivityDate="2013-07-08T12:19:21.943" OwnerUserId="253" PostTypeId="1" Score="10" Tags="&lt;r&gt;&lt;nonparametric&gt;&lt;anova&gt;" Title="A non-parametric repeated-measures multi-way Anova in R?" ViewCount="8096" />
  <row Body="&lt;p&gt;Filling with zero is bad. Try filling with resampling using observations from the past.  &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-08-04T21:10:02.473" Id="1269" LastActivityDate="2010-08-04T21:10:02.473" OwnerUserId="223" ParentId="1268" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;The trivial and non-helpful answer is &quot;Yes, downsample your 5-minute data to 60-minute data.&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;More practically, without throwing out 90% of your data, the answer is generally &quot;No, unless you get extremely lucky with sampling your five-minute data.&quot;  You should get an answer that's &lt;em&gt;close&lt;/em&gt; (and under most noise models I suspect they'll be equal in expectation) just by rescaling your smooth by a factor of 12, but any source of randomness in your data is going to cause some difference in the two curves on a point-by-point basis.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-08-04T23:36:10.817" Id="1276" LastActivityDate="2010-08-04T23:36:10.817" OwnerUserId="61" ParentId="1270" PostTypeId="2" Score="4" />
  
  
  <row Body="&lt;p&gt;sources:&lt;/p&gt;&#10;&#10;&lt;p&gt;1) statistics upto ANOVA&#10;2) probability upto Central limit theorem&#10;3) Basic programming and data analysis skills&#10;4) Familiarity with business economics&#10;5) Financial mathematics&lt;/p&gt;&#10;&#10;&lt;p&gt;This is a verrry basic smattering of things that she needs to read.&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2010-08-05T14:32:53.243" CreationDate="2010-08-05T13:04:27.533" Id="1302" LastActivityDate="2010-08-05T13:04:27.533" OwnerDisplayName="Jatin Khanna" ParentId="1293" PostTypeId="2" Score="1" />
  <row AnswerCount="3" Body="&lt;p&gt;In the traditional birthday paradox the question is &quot;what are the chances that two or more people in a group of n people share a birthday&quot;.  I'm stuck on a problem which is an extension of this.&lt;/p&gt;&#10;&#10;&lt;p&gt;Instead of knowing the probability that two people share a birthday I need to extend the question to know what is the probability that x or more people share a birthday.  With x=2 you can do this by calculating the probability that no two people share a birthday and subtract that from 1, but I don't think I can extend this logic to larger numbers of x.&lt;/p&gt;&#10;&#10;&lt;p&gt;To further complicate this I also need a solution which will work for very large numbers for n (millions) and x (thousands).&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2010-08-05T14:00:07.723" FavoriteCount="9" Id="1308" LastActivityDate="2011-04-29T00:26:29.467" LastEditDate="2011-04-29T00:26:29.467" LastEditorUserId="3911" OwnerUserId="765" PostTypeId="1" Score="17" Tags="&lt;probability&gt;&lt;bioinformatics&gt;" Title="Extending the birthday paradox to more than 2 people" ViewCount="5303" />
  <row Body="&lt;p&gt;It is always possible to solve this problem with a monte-carlo solution, although that's far from the most efficient.  Here's a simple example of the 2 person problem in R (from &lt;a href=&quot;http://www.meetup.com/nyhackr/calendar/10251302/?from=list&amp;amp;offset=0&quot; rel=&quot;nofollow&quot;&gt;a presentation I gave last year&lt;/a&gt;; I used this as an example of inefficient code), which could be easily adjusted to account for more than 2:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;birthday.paradox &amp;lt;- function(n.people, n.trials) {&#10;    matches &amp;lt;- 0&#10;    for (trial in 1:n.trials) {&#10;        birthdays &amp;lt;- cbind(as.matrix(1:365), rep(0, 365))&#10;        for (person in 1:n.people) {&#10;            day &amp;lt;- sample(1:365, 1, replace = TRUE)&#10;            if (birthdays[birthdays[, 1] == day, 2] == 1) {&#10;                matches &amp;lt;- matches + 1&#10;                break&#10;            }&#10;            birthdays[birthdays[, 1] == day, 2] &amp;lt;- 1&#10;        }&#10;        birthdays &amp;lt;- NULL&#10;    }&#10;    print(paste(&quot;Probability of birthday matches = &quot;, matches/n.trials))&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="3" CreationDate="2010-08-05T14:10:26.907" Id="1309" LastActivityDate="2010-08-05T14:54:11.593" LastEditDate="2010-08-05T14:54:11.593" LastEditorUserId="5" OwnerUserId="5" ParentId="1308" PostTypeId="2" Score="2" />
  
  
  
  
  <row Body="&lt;p&gt;&quot;New methods always look better than old ones. Neural nets are better than logistic regression, support vector machines are better than neural nets, etc.&quot; - Brad Efron&lt;/p&gt;&#10;" CommentCount="2" CommunityOwnedDate="2010-08-06T01:29:56.867" CreationDate="2010-08-06T01:29:56.867" Id="1334" LastActivityDate="2010-08-06T01:29:56.867" OwnerUserId="521" ParentId="726" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;You may want to define what you want more clearly (to yourself, if not here). If what you're looking for is the most statistically significant stationary period contained in your noisy data, there's essentially two routes to take:&lt;/p&gt;&#10;&#10;&lt;p&gt;1) compute a robust autocorrelation estimate, and take the maximum coefficient&lt;br&gt;&#10;2) compute a robust power spectral density estimate, and take the maximum of the spectrum&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem with #2 is that for any noisy time series, you will get a large amount of power in low frequencies, making it difficult to distinguish. There are some techniques for resolving this problem (i.e. pre-whiten, then estimate the PSD), but if the true period from your data is long enough, automatic detection will be iffy.&lt;/p&gt;&#10;&#10;&lt;p&gt;Your best bet is probably to implement a robust autocorrelation routine such as can be found in chapter 8.6, 8.7 in &lt;em&gt;Robust Statistics - Theory and Methods&lt;/em&gt; by Maronna, Martin and Yohai. Searching Google for &quot;robust durbin-levinson&quot; will also yield some results.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you're just looking for a simple answer, I'm not sure that one exists. Period detection in time series can be complicated, and asking for an automated routine that can perform magic may be too much. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-08-06T02:48:09.630" Id="1339" LastActivityDate="2010-08-06T02:48:09.630" OwnerUserId="781" ParentId="1207" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;I sometimes make the x-axis time and plot both scalar variables on the y-axis.&#10;When the two scalar variables are on a different metric, I rescale one or both of the scalar variables so they can be displayed on the same plot.&#10;I use things like colour and shape to discriminate the two scalar variables.&#10;I've often used &lt;code&gt;xyplot&lt;/code&gt; from &lt;code&gt;lattice&lt;/code&gt; for this purpose. &lt;/p&gt;&#10;&#10;&lt;p&gt;Here's an example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;require(lattice)&#10;xyplot(dv1 + dv2  ~ iv, data = x, col = c(&quot;black&quot;, &quot;red&quot;))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2010-08-06T03:22:13.263" Id="1343" LastActivityDate="2010-08-06T03:22:13.263" OwnerUserId="183" ParentId="1321" PostTypeId="2" Score="6" />
  
  <row AnswerCount="3" Body="&lt;p&gt;I am trying to compare it to Euclidean distance and Pearson correlation&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-08-06T10:48:52.543" FavoriteCount="3" Id="1357" LastActivityDate="2011-04-29T00:26:49.170" LastEditDate="2011-04-29T00:26:49.170" LastEditorUserId="3911" OwnerDisplayName="CLOCK" PostTypeId="1" Score="1" Tags="&lt;correlation&gt;&lt;mutual-information&gt;" Title="Is mutual information invariant to scaling, i.e. multiplying all elements by a nonzero constant?" ViewCount="672" />
  
  <row Body="&lt;p&gt;One passed by Gary Ramseyer:&lt;/p&gt;&#10;&#10;&lt;p&gt;Statistics play an important role in genetics. For instance, statistics prove that numbers of offspring is an inherited trait. If your parent didn't have any kids, odds are you won't either.&lt;/p&gt;&#10;" CommentCount="1" CommunityOwnedDate="2010-08-06T13:52:50.747" CreationDate="2010-08-06T13:52:50.747" Id="1367" LastActivityDate="2010-08-06T13:52:50.747" OwnerUserId="634" ParentId="1337" PostTypeId="2" Score="50" />
  <row Body="&lt;p&gt;George Burns said that &quot;If you live to be one hundred, you've got it made. Very few people die past that age.&quot;&lt;/p&gt;&#10;" CommentCount="1" CommunityOwnedDate="2010-08-06T14:12:21.147" CreationDate="2010-08-06T14:12:21.147" Id="1371" LastActivityDate="2010-08-06T14:12:21.147" OwnerUserId="666" ParentId="1337" PostTypeId="2" Score="63" />
  
  
  
  
  
  
  
  
  
  <row Body="&lt;p&gt;I assume you have continuous data. &lt;/p&gt;&#10;&#10;&lt;p&gt;If the data include &lt;strong&gt;zeros&lt;/strong&gt; this means you have a spike on zero which may be due to some particular aspect of your data. It appears for example in wind energy, wind below 2 m/s produce zero power (it is called cut in) and wind over (something around) 25 m/s also produce zero power (for security reason, it is called cut off).  While the distribution of produced wind energy seems continuous there is a spike in zero. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;My solution:&lt;/strong&gt;  In this case, I suggest to treat the zeros separately by working with a mixture of the spike in zero and the model you planned to use for the part of the distribution that is continuous (wrt Lebesgue). &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-08-09T14:05:50.187" Id="1445" LastActivityDate="2013-05-28T21:09:50.753" LastEditDate="2013-05-28T21:09:50.753" LastEditorUserId="22047" OwnerUserId="223" ParentId="1444" PostTypeId="2" Score="6" />
  <row Body="&lt;blockquote&gt;&#10;  &lt;p&gt;My greatest concern was what to call&#10;  it. I thought of calling it&#10;  'information,' but the word was overly&#10;  used, so I decided to call it&#10;  'uncertainty.' When I discussed it&#10;  with John von Neumann, he had a better&#10;  idea. Von Neumann told me, 'You should&#10;  call it entropy, for two reasons. In&#10;  the first place your uncertainty&#10;  function has been used in statistical&#10;  mechanics under that name, so it&#10;  already has a name. In the second&#10;  place, and more important, no one&#10;  really knows what entropy really is,&#10;  so in a debate you will always have&#10;  the advantage.'&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Claude Elwood Shannon&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2010-08-09T15:36:26.553" CreationDate="2010-08-09T15:36:26.553" Id="1449" LastActivityDate="2010-08-09T15:36:26.553" OwnerUserId="223" ParentId="726" PostTypeId="2" Score="30" />
  
  <row Body="&lt;p&gt;You could use a &lt;a href=&quot;http://en.wikipedia.org/wiki/Mixture_model&quot; rel=&quot;nofollow&quot;&gt;mixture model&lt;/a&gt; to separate out the components. The data generating process  can be represented as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;Let:&lt;/p&gt;&#10;&#10;&lt;p&gt;$z_i$: be the type (1 or 2) for the $i^{th}$ observation,&lt;/p&gt;&#10;&#10;&lt;p&gt;$y_i$ be the $i^{th}$ observation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then you have:&lt;/p&gt;&#10;&#10;&lt;p&gt;$f(y_i|z_i=1) \sim N(0,1)$&lt;/p&gt;&#10;&#10;&lt;p&gt;$f(y_i|z_i=2) \sim N(m,v)$&lt;/p&gt;&#10;&#10;&lt;p&gt;$P(z_i=1) = \pi$ and&lt;/p&gt;&#10;&#10;&lt;p&gt;$P(z_i=2) = 1-\pi$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The likelihood function is given by:&lt;/p&gt;&#10;&#10;&lt;p&gt;$L(m,v,\pi|-) = \sum_{y_i} \pi f(y_i|z_i=1) + (1-\pi) f(y_i|z_i=2)$ &lt;/p&gt;&#10;&#10;&lt;p&gt;You can then use either &lt;a href=&quot;http://en.wikipedia.org/wiki/Mixture_model#Common_approaches_for_estimation_in_mixture_models&quot; rel=&quot;nofollow&quot;&gt;EM or MCMC&lt;/a&gt; to estimate for the model parameters.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-08-09T18:03:31.257" Id="1457" LastActivityDate="2010-08-09T18:36:17.977" LastEditDate="2010-08-09T18:36:17.977" LastEditorDisplayName="user28" OwnerDisplayName="user28" ParentId="1454" PostTypeId="2" Score="1" />
  
  
  
  <row Body="&lt;p&gt;If you want something quick and dirty why not use the square root? &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-08-10T20:48:11.953" Id="1514" LastActivityDate="2010-08-10T20:48:11.953" OwnerUserId="856" ParentId="1444" PostTypeId="2" Score="7" />
  
  <row Body="&lt;p&gt;The &lt;em&gt;reason&lt;/em&gt; that we calculate standard deviation instead of absolute error is that we are &lt;strong&gt;assuming error to be normally distributed&lt;/strong&gt;.  It's a part of the model.&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose you were measuring very small lengths with a ruler, then standard deviation is a bad metric for error because you know you will never accidentally measure a negative length.  A better metric would be one to help fit a Gamma distribution to your measurements:&lt;/p&gt;&#10;&#10;&lt;p&gt;$E(\log(x)) - \log(E(x))$&lt;/p&gt;&#10;&#10;&lt;p&gt;Like the standard deviation, this is also non-negative and differentiable, but it is a better error statistic for this problem.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-08-10T22:34:01.363" Id="1516" LastActivityDate="2014-05-14T11:47:31.440" LastEditDate="2014-05-14T11:47:31.440" LastEditorUserId="858" OwnerUserId="858" ParentId="118" PostTypeId="2" Score="23" />
  
  <row AcceptedAnswerId="4850" AnswerCount="6" Body="&lt;p&gt;Say I have eaten hamburgers every Tuesday for years. You could say that I eat hamburgers 14% of the time, or that the probability of me eating a hamburger in a given week is 14%.&lt;/p&gt;&#10;&#10;&lt;p&gt;What are the main differences between probabilities and proportions?&lt;/p&gt;&#10;&#10;&lt;p&gt;Is a probability an expected proportion?&lt;/p&gt;&#10;&#10;&lt;p&gt;Are probabilities uncertain and proportions are guaranteed?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2010-08-11T07:24:02.763" FavoriteCount="3" Id="1525" LastActivityDate="2013-12-10T15:01:22.863" LastEditDate="2010-08-16T19:03:53.470" LastEditorUserId="74" OwnerUserId="74" PostTypeId="1" Score="11" Tags="&lt;probability&gt;" Title="What's the difference between a probability and a proportion?" ViewCount="16719" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I've been asked to give some advice for some clinicians who are comparing two different methods of blood pressure measurement.  I suggested to them that we should proceed with a two-one-sided-test technique to determine equivalence of the two techniques.&lt;/p&gt;&#10;&#10;&lt;p&gt;Unfortunately I have now learned that the clinicians have multiple measurements of blood pressure by each of the two methods and that the blood pressure can be quite variable within each patient during the period of observation (they are theatre cases).&lt;/p&gt;&#10;&#10;&lt;p&gt;Is it possible to use some multiple regression technique to perform an equivalence test?  Can I simply use confidence intervals to determine variability between the two techniques whilst accounting for inter-patient variability if I use the patients as factors in the regression model?&lt;/p&gt;&#10;&#10;&lt;p&gt;Sorry it's an amateur question, but despite the Masters degree, I still feel like quite the amateur!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-08-11T12:12:35.573" Id="1534" LastActivityDate="2010-08-11T14:05:25.087" LastEditDate="2010-08-11T13:09:20.933" LastEditorUserId="5" OwnerUserId="867" PostTypeId="1" Score="2" Tags="&lt;multiple-comparisons&gt;&lt;equivalence&gt;" Title="Best method for comparing multiple ranging measures" ViewCount="118" />
  <row AcceptedAnswerId="1539" AnswerCount="2" Body="&lt;p&gt;The Kolmogorov–Smirnov distribution is known from the &lt;a href=&quot;http://en.wikipedia.org/wiki/Kolmogorov_Smirnov&quot;&gt;Kolmogorov–Smirnov test&lt;/a&gt;. However, it is also the distribution of the supremum of the Brownian bridge.&lt;/p&gt;&#10;&#10;&lt;p&gt;Since this is far from obvious (to me), I would like to ask you for an intuitive explanation of this coincidence. References are also welcome.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-08-11T13:15:24.397" FavoriteCount="2" Id="1538" LastActivityDate="2011-04-29T00:23:29.750" LastEditDate="2011-04-29T00:23:29.750" LastEditorUserId="3911" OwnerUserId="650" PostTypeId="1" Score="14" Tags="&lt;distributions&gt;&lt;hypothesis-testing&gt;&lt;mathematical-statistics&gt;&lt;stochastic-processes&gt;" Title="Why does the supremum of the Brownian bridge have the Kolmogorov–Smirnov distribution?" ViewCount="1159" />
  
  <row Body="&lt;p&gt;The difference between statistics and data mining is largely a historical one, since they came from different traditions: statistics and computer science.  Data mining grew in parallel out of work in the area of artificial intelligence and statistics.&lt;/p&gt;&#10;&#10;&lt;p&gt;Section 1.4 from &lt;a href=&quot;http://www.cs.waikato.ac.nz/~ml/weka/book.html&quot;&gt;Witten &amp;amp; Frank&lt;/a&gt; summarizes my viewpoint so I'm going to quote it at length:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;What's the difference between machine&#10;  learning and statistics?  Cynics,&#10;  looking wryly at the explosion of&#10;  commercial interest (and hype) in this&#10;  area, equate data mining to statistics&#10;  plus marketing.  In truth, you should&#10;  not look for a dividing line between&#10;  machine learning and statistics&#10;  because there is a continuum--and a&#10;  multidimensional one at that--of data&#10;  analysis techniques.  Some derive from&#10;  the skills taught in standard&#10;  statistics courses, and others are&#10;  more closely associated with the kind&#10;  of machine learning that has arisen&#10;  out of computer science. &#10;  Historically, the two sides have had&#10;  rather different traditions.  If&#10;  forced to point to a single difference&#10;  of emphasis, it might be that&#10;  statistics has been more concerned&#10;  with testing hypotheses, whereas&#10;  machine learning has been more&#10;  concerned with formulating the process&#10;  of generalization as a search through&#10;  possible hypotheses...&lt;/p&gt;&#10;  &#10;  &lt;p&gt;In the past,&#10;  very similar methods have developed in&#10;  parallel in machine learning and&#10;  statistics...&lt;/p&gt;&#10;  &#10;  &lt;p&gt;But now the two&#10;  perspectives have converged.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;N.B.1 IMO, data mining and machine learning are &lt;em&gt;very&lt;/em&gt; closely related terms.  In one sense, machine learning techniques are used in data mining.  I regularly see these terms as interchangeable, and in so far as they are different, they usually go together.  I would suggest looking through &lt;a href=&quot;http://stats.stackexchange.com/questions/6/the-two-cultures-statistics-vs-machine-learning&quot;&gt;&quot;The Two Cultures&quot; paper&lt;/a&gt; as well as the other threads from my original question.&lt;/p&gt;&#10;&#10;&lt;p&gt;N.B.2 The term &quot;data mining&quot; can have a negative connotation when used colloquially to mean letting some algorithm loose on the data without any conceptual understanding.  The sense is that data mining will lead to spurious results and over-fitting.  I typically avoid using the term when talking to non-experts as a result, and instead use machine learning or statistical learning as a synonym.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2010-08-11T15:20:38.757" Id="1551" LastActivityDate="2010-08-11T15:42:21.273" LastEditDate="2010-08-11T15:42:21.273" LastEditorUserId="5" OwnerUserId="5" ParentId="1521" PostTypeId="2" Score="8" />
  
  <row AnswerCount="4" Body="&lt;p&gt;I have a question on subject chi-square test for independence.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have, for example, two events A and B. If chi square test is not passed: is A dependent on B (A|B) or B on A (B|A)? Or does be valid both? (A|B and B|A).&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you in advance.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-08-11T21:49:08.720" FavoriteCount="2" Id="1562" LastActivityDate="2010-10-30T14:56:29.207" LastEditDate="2010-08-11T23:36:13.873" LastEditorUserId="159" OwnerDisplayName="sewa373" PostTypeId="1" Score="4" Tags="&lt;statistical-significance&gt;&lt;chi-squared&gt;" Title="What dependence is implied by a chi square test for independence?" ViewCount="945" />
  <row Body="&lt;p&gt;Naive Bayes is usually the starting point for text classification, here's an &lt;a href=&quot;http://www.drdobbs.com/184406064;jsessionid=A35OB1KFVVLTTQE1GHPCKH4ATMY32JVN&quot; rel=&quot;nofollow&quot;&gt;article&lt;/a&gt; from Dr. Dobbs on how to implement one. It's also often the ending point for text classification because it's so efficient and parallelizes well, SpamAssassin and POPFile use it.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-08-11T22:10:15.810" Id="1563" LastActivityDate="2010-08-11T22:17:23.683" LastEditDate="2010-08-11T22:17:23.683" LastEditorUserId="511" OwnerUserId="511" ParentId="124" PostTypeId="2" Score="0" />
  <row AnswerCount="3" Body="&lt;p&gt;Could you inform me please, how can I calculate conditioned probability of several events?&lt;/p&gt;&#10;&#10;&lt;p&gt;for example:&lt;/p&gt;&#10;&#10;&lt;p&gt;P (A | B, C, D) - ?&lt;/p&gt;&#10;&#10;&lt;p&gt;I know, that:&lt;/p&gt;&#10;&#10;&lt;p&gt;P (A | B) = P (A intersection B) / P (B)&lt;/p&gt;&#10;&#10;&lt;p&gt;But, unfortunately, I can't find any formula if an event A depends on several variables. Thanks in advance.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-08-11T22:14:39.753" FavoriteCount="3" Id="1564" LastActivityDate="2011-04-29T00:24:17.987" LastEditDate="2011-04-29T00:24:17.987" LastEditorUserId="3911" OwnerDisplayName="sewa373" PostTypeId="1" Score="6" Tags="&lt;conditional-probability&gt;" Title="How can I calculate the conditional probability of several events?" ViewCount="6921" />
  <row Body="&lt;p&gt;&lt;strong&gt;Principal component analysis&lt;/strong&gt; involves extracting linear composites of observed variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Factor analysis&lt;/strong&gt; is based on a formal model predicting observed variables from theoretical latent factors.&lt;/p&gt;&#10;&#10;&lt;p&gt;In psychology these two techniques are often applied in the construction of multi-scale tests&#10; to determine which items load on which scales.&#10;They typically yield similar substantive conclusions (for a discussion see Comrey (1988) Factor-Analytic Methods of Scale Development in Personality and Clinical Psychology).&#10;This helps to explain why some statistics packages seem to bundle them together.&#10;I have also seen situations where &quot;principal component analysis&quot; is incorrectly labelled &quot;factor analysis&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;In terms of a &lt;strong&gt;simple rule of thumb&lt;/strong&gt;, I'd suggest that you:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Run factor analysis if you  assume or wish to test a theoretical model of latent factors causing observed variables.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Run principal component analysis If you want to simply reduce your correlated observed variables to a smaller set of important independent composite variables.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="3" CreationDate="2010-08-12T04:44:12.307" Id="1579" LastActivityDate="2013-06-25T00:20:29.910" LastEditDate="2013-06-25T00:20:29.910" LastEditorUserId="22047" OwnerUserId="183" ParentId="1576" PostTypeId="2" Score="65" />
  <row AcceptedAnswerId="1582" AnswerCount="3" Body="&lt;p&gt;Imagine&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;You run a linear regression with four numeric predictors (IV1, ..., IV4)&lt;/li&gt;&#10;&lt;li&gt;When only IV1 is included as a predictor the standardised beta is &lt;code&gt;+.20&lt;/code&gt;&lt;/li&gt;&#10;&lt;li&gt;When you also include IV2 to IV4 the sign of the standardised regression coefficient of IV1 flips to &lt;code&gt;-.25&lt;/code&gt; (i.e., it's become negative).&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;This gives rise to a few questions:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;With regards to terminology, do you call this a &quot;suppressor effect&quot;?&lt;/li&gt;&#10;&lt;li&gt;What strategies would you use to explain and understand this effect?&lt;/li&gt;&#10;&lt;li&gt;Do you have any examples of such effects in practice and how did you explain and understand these effects?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="2" CreationDate="2010-08-12T05:03:22.717" FavoriteCount="9" Id="1580" LastActivityDate="2013-11-25T11:40:27.387" LastEditDate="2013-11-25T11:40:27.387" LastEditorUserId="22047" OwnerUserId="183" PostTypeId="1" Score="12" Tags="&lt;regression&gt;&lt;predictor&gt;" Title="Regression coefficients that flip sign after including other predictors" ViewCount="9568" />
  
  <row Body="&lt;p&gt;Ridge regression is a form of shrinkage. See &lt;a href=&quot;http://www.jstor.org/pss/1268284&quot;&gt;Draper &amp;amp; Van Nostrand (1979)&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Shrinkage has also proved useful in estimating seasonal factors for time series. See &lt;a href=&quot;http://www.forecasters.org/ijf/journal-issue/273/article/5847&quot;&gt;Miller and Williams (IJF, 2003)&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-08-12T07:27:47.233" Id="1585" LastActivityDate="2010-08-12T07:27:47.233" OwnerUserId="159" ParentId="1557" PostTypeId="2" Score="9" />
  
  
  <row AcceptedAnswerId="1596" AnswerCount="1" Body="&lt;p&gt;If one has an r value of 0.60, can one state that an increase in one variable is 60% likely to mean an increase in the other variable?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-08-12T08:50:26.350" Id="1590" LastActivityDate="2011-09-21T23:22:24.123" LastEditDate="2011-09-21T23:22:24.123" LastEditorUserId="183" OwnerUserId="888" PostTypeId="1" Score="0" Tags="&lt;correlation&gt;" Title="Does an r value of 0.60 mean that an increase in one variable is 60% likely to mean an increase in the other variable?" ViewCount="798" />
  
  <row AcceptedAnswerId="1609" AnswerCount="1" Body="&lt;p&gt;Can someone provide me with a book or online reference on how to construct smoothing splines with cross-validation? I have a programming and undergraduate level mathematics background.  I would also appreciate an overview of whether this is smoothing technique is a good one for smoothing data and whether there are any disadvantages of which a non-statistician needs to be aware.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-08-12T15:16:55.543" Id="1604" LastActivityDate="2010-08-13T02:46:22.033" LastEditDate="2010-08-12T20:57:22.143" LastEditorUserId="847" OwnerUserId="847" PostTypeId="1" Score="4" Tags="&lt;cross-validation&gt;&lt;smoothing&gt;&lt;splines&gt;" Title="Constructing smoothing splines with cross-validation" ViewCount="442" />
  <row AcceptedAnswerId="1616" AnswerCount="25" Body="&lt;p&gt;I'm not a statistician by education, I'm a software engineer. Yet statistics comes up a lot. In fact, questions specifically about Type I and Type II error are coming up a lot in the course of my studying for the Certified Software Development Associate exam (mathematics and statistics are 10% of the exam). I'm having trouble always coming up with the right definitions for Type I and Type II error - although I'm memorizing them now (and can remember them most of the time), I really don't want to freeze up on this exam trying to remember what the difference is.&lt;/p&gt;&#10;&#10;&lt;p&gt;I know that Type I Error is a false positive, or when you reject the null hypothesis and it's actually true and a Type II error is a false negative, or when you accept the null hypothesis and it's actually false.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there an easy way to remember what the difference is, such as a mnemonic? How do professional statisticians do it - is it just something that they know from using or discussing it often?&lt;/p&gt;&#10;&#10;&lt;p&gt;(Side Note: This question can probably use some better tags. One that I wanted to create was &quot;terminology&quot;, but I don't have enough reputation to do it. If someone could add that, it would be great. Thanks.)&lt;/p&gt;&#10;" CommentCount="11" CreationDate="2010-08-12T19:55:02.167" FavoriteCount="27" Id="1610" LastActivityDate="2015-01-23T14:16:17.547" LastEditDate="2012-05-15T11:34:07.880" LastEditorDisplayName="user28" LastEditorUserId="686" OwnerUserId="110" PostTypeId="1" Score="47" Tags="&lt;terminology&gt;&lt;type-i-errors&gt;&lt;type-ii-errors&gt;" Title="Is there a way to remember the definitions of Type I and Type II Errors?" ViewCount="13986" />
  
  
  
  
  
  <row Body="&lt;p&gt;It's hard to ignore the wealth of statistical packages available in R/CRAN.  That said, I spend a &lt;em&gt;lot&lt;/em&gt; of time in Python land and would never dissuade anyone from having as much fun as I do.  :)  Here are some libraries/links you might find useful for statistical work.  &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://scipy.org&quot;&gt;NumPy/Scipy&lt;/a&gt; You probably know about these already.  But let me point out the &lt;a href=&quot;http://scipy.org/Cookbook&quot;&gt;Cookbook&lt;/a&gt; where you can read about many statistical facilities already available and the &lt;a href=&quot;http://www.scipy.org/Numpy_Example_List&quot;&gt;Example List&lt;/a&gt; which is a great reference for functions (including data manipulation and other operations).  Another handy reference is John Cook's &lt;a href=&quot;http://www.johndcook.com/distributions_scipy.html&quot;&gt;Distributions in Scipy&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://code.google.com/p/pandas/&quot;&gt;pandas&lt;/a&gt; This is a really nice library for working with statistical data -- tabular data, time series, panel data.  Includes many builtin functions for data summaries, grouping/aggregation, pivoting.  Also has a statistics/econometrics library.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://pypi.python.org/pypi/la&quot;&gt;larry&lt;/a&gt;  Labeled array that plays nice with NumPy.  Provides statistical functions not present in NumPy and good for data manipulation.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://code.google.com/p/python-statlib/&quot;&gt;python-statlib&lt;/a&gt; A fairly recent effort which combined a number of scattered statistics libraries.  Useful for basic and descriptive statistics if you're not using NumPy or pandas.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://statsmodels.sourceforge.net/&quot;&gt;statsmodels&lt;/a&gt; Statistical modeling: Linear models, GLMs, among others.  &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://scikits.appspot.com/scikits&quot;&gt;scikits&lt;/a&gt;  Statistical and scientific computing packages -- notably smoothing, optimization and machine learning.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://pypi.python.org/pypi/pymc/&quot;&gt;PyMC&lt;/a&gt; For your Bayesian/MCMC/hierarchical modeling needs. Highly recommended.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://www.pymix.org/pymix/index.php?n=PyMix.Home&quot;&gt;PyMix&lt;/a&gt; Mixture models.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;If speed becomes a problem, consider &lt;a href=&quot;http://deeplearning.net/software/theano/&quot;&gt;Theano&lt;/a&gt; -- used with good success by the deep learning people.&lt;/p&gt;&#10;&#10;&lt;p&gt;There's plenty of other stuff out there, but this is what I find the most useful along the lines you mentioned.&lt;/p&gt;&#10;" CommentCount="6" CommunityOwnedDate="2010-08-27T15:07:18.913" CreationDate="2010-08-13T05:30:09.033" Id="1632" LastActivityDate="2012-11-29T20:58:04.477" LastEditDate="2012-11-29T20:58:04.477" LastEditorUserId="930" OwnerUserId="251" ParentId="1595" PostTypeId="2" Score="189" />
  <row Body="&lt;p&gt;You could reject the idea entirely.&lt;/p&gt;&#10;&#10;&lt;p&gt;Some authors (Andrew Gelman is one) are shifting to discussing Type S (sign) and Type M (magnitude) errors.  You can infer the wrong effect direction (e.g., you believe the treatment group does better but actually does worse) or the wrong magnitude (e.g., you find a massive effect where there is only a tiny, or essentially no effect, or vice versa).&lt;/p&gt;&#10;&#10;&lt;p&gt;See more at &lt;a href=&quot;http://www.stat.columbia.edu/~cook/movabletype/archives/2004/12/type_1_type_2_t.html&quot;&gt;Gelman's blog&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-08-13T12:22:25.313" Id="1642" LastActivityDate="2010-08-13T12:22:25.313" OwnerUserId="702" ParentId="1610" PostTypeId="2" Score="9" />
  
  <row AcceptedAnswerId="1648" AnswerCount="5" Body="&lt;p&gt;So far, I've been using the Shapiro-Wilk statistic in order to test normality assumptions in small samples. &lt;/p&gt;&#10;&#10;&lt;p&gt;Could you please recommend another technique?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-08-13T12:42:30.220" FavoriteCount="11" Id="1645" LastActivityDate="2015-02-22T12:37:27.600" LastEditDate="2015-02-22T12:35:34.830" LastEditorUserId="22047" OwnerUserId="1356" PostTypeId="1" Score="16" Tags="&lt;hypothesis-testing&gt;&lt;goodness-of-fit&gt;&lt;normality&gt;&lt;small-sample&gt;" Title="Appropriate normality tests for small samples" ViewCount="7339" />
  <row Body="&lt;p&gt;You can structure the model along the following lines. Let,&lt;/p&gt;&#10;&#10;&lt;p&gt;$j = 1, 2$ be the two groups and&lt;/p&gt;&#10;&#10;&lt;p&gt;$i$ index the individuals in the two groups. &lt;/p&gt;&#10;&#10;&lt;p&gt;Then your model is:&lt;/p&gt;&#10;&#10;&lt;p&gt;$y_{ij} \sim N(\mu_j,\sigma_j^2)$  $\forall \ i, j$ &lt;/p&gt;&#10;&#10;&lt;p&gt;$\sigma_j^2 \sim IG(v,1)$  $\forall \ j$&lt;/p&gt;&#10;&#10;&lt;p&gt;(Note: $IG(.)$ is the inverse gamma distribution.)&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Priors&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;$\mu_j \sim N(\bar{\mu},\sigma_{\mu}^2)$&lt;/p&gt;&#10;&#10;&lt;p&gt;$v \sim IG(\bar{v},1)$ is the prior for $v$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The above structure will let you shrink the error variances ($\sigma_j^2$) appropriately. You can then evaluate whether the within group variability is different by looking at the credible intervals associated with the group variabilities.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-08-13T14:17:15.793" Id="1650" LastActivityDate="2010-08-13T14:17:15.793" OwnerDisplayName="user28" ParentId="1649" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;John Tukey systematically discusses transformations in his book on EDA.  In addition to the Box-Cox family (affinely scaled power transformations) he defines a family of &quot;folded&quot; transformations for proportions (essentially powers of x/(1-x)) and &quot;started&quot; counts (adding a positive offset to counted data before transforming them).  The folded transformations, which essentially generalize the logit, are especially useful for test scores.&lt;/p&gt;&#10;&#10;&lt;p&gt;In a completely different vein, Johnson &amp;amp; Kotz in their books on distributions offer many transformations intended to convert test statistics to approximate normality (or to some other target distribution), such as the cube-root transformation for chi-square.  This material is a great source of ideas for useful transformations when you anticipate your data will follow some specific distribution.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-08-13T15:48:29.310" Id="1655" LastActivityDate="2010-08-13T15:48:29.310" OwnerUserId="919" ParentId="1601" PostTypeId="2" Score="6" />
  
  
  <row Body="&lt;p&gt;When I took the Engineering Statistics course I mentioned in the question, the assigned textbook wasn't very helpful. Instead, I used &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0495107573&quot; rel=&quot;nofollow&quot;&gt;Probability and Statistics for Engineers and Scientists - Anthony Hayter&lt;/a&gt; to get through the course. It didn't cover everything in the same order and depth of the course, but it was sufficient to get me through the material and get a passing grade.&lt;/p&gt;&#10;&#10;&lt;p&gt;Topics covered include probability theory, random variables, discrete and continuous probability distributions, normal distributions, descriptive statistics, statistical estimation and sampling distributions, population means, discrete data analysis, ANOVA, linear regression, nonlinear regression, multifactor experimental design and analysis, nonparametric statistical analysis, quality control methods, and reliability analysis. Unfortunately, the course only covered the first 11 chapters and occasionally in more depth then this book went into.&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2010-08-14T00:58:21.737" CreationDate="2010-08-14T00:58:21.737" Id="1670" LastActivityDate="2010-08-14T00:58:21.737" OwnerUserId="110" ParentId="1667" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/1584885459&quot; rel=&quot;nofollow&quot;&gt;Statistical Computing with R - Maria L. Rizzo&lt;/a&gt; covers a lot of the topics in Probability and Statistics for Computer Scientists - basic probability and statistics, random variables, Bayesian statistics, Markov chains, visualization of multivariate data, Monte Carlo methods, Permutation tests, probability density estimation, and numerical methods.&lt;/p&gt;&#10;&#10;&lt;p&gt;The equations and formulas used are presented both as mathematical formulas as well as in R code. I would say that a basic knowledge of probability, statistics, calculus, and maybe discrete mathematics would be advisable for anyone who wants to read this book. A programming background would also be helpful, but there are some references for the R language, operators, and syntax.&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2010-08-14T01:16:16.037" CreationDate="2010-08-14T01:16:16.037" Id="1673" LastActivityDate="2010-08-14T01:16:16.037" OwnerUserId="110" ParentId="1668" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;What the p-value doesn't tell you is how likely it is that the null hypothesis is true.  Under the conventional (Fisher) significance testing framework we first compute the likelihood of observing the data assuming the null hypothesis is true, this is the p-value.  It seems intuitively reasonable then to assume the null hypothesis is probably false if the data are sufficiently unlikely to be observed under the null hypothesis.  This is entirely reasonable.  Statisticians tranditionally use a threshold and &quot;reject the null hypothesis at the 95% significance level&quot; if (1 - p) &gt; 0.95; however this is just a convention that has proven reasonable in practice - it doesn't mean that there is less than 5% probability that the null hypothesis is false (and therefore a 95% probability that the alternative hypothesis is true).  One reason that we can't say this is that we have not looked at the alternative hypothesis yet.&lt;/p&gt;&#10;&#10;&lt;p&gt;Imaging a function f() that maps the p-value onto the probability that the alternative hypothesis is true.  It would be reasonable to assert that this function is strictly decreasing (such that the more likely the observations under the null hypothesis, the less likely the alternative hypothesis is true), and that it gives values between 0 and 1 (as it gives an estimate of probability).  However, that is all that we know about f(), so while there is a relationship between p and the probability that the alternative hypothesis is true, it is uncalibrated.  This means we cannot use the p-value to make quantitative statements about the plausibility of the nulll and alternatve hypotheses.&lt;/p&gt;&#10;&#10;&lt;p&gt;Caveat lector: It isn't really within the frequentist framework to speak of the probability that a hypothesis is true, as it isn't a random variable - it is either true or it isn't.  So where I have talked of the probability of the truth of a hypothesis I have implicitly moved to a Bayesian interpretation.  It is incorrect to mix Bayesian and frequentist, however there is always a temptation to do so as what we really want is an quantative indication of the relative plausibility/probability of the hypotheses.  But this is not what the p-value provides.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-08-14T07:52:35.467" Id="1682" LastActivityDate="2010-08-14T07:52:35.467" OwnerUserId="887" ParentId="31" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;In a narrow sense, R Core has a recommendation: the &quot;recommended&quot; packages.&lt;/p&gt;&#10;&#10;&lt;p&gt;Everything else depends on your data analysis tasks at hand, and I'd recommend the &lt;a href=&quot;http://cran.r-project.org/web/views&quot;&gt;Task Views&lt;/a&gt; at CRAN.&lt;/p&gt;&#10;" CommentCount="1" CommunityOwnedDate="2010-08-14T13:06:45.767" CreationDate="2010-08-14T13:06:45.767" Id="1686" LastActivityDate="2010-08-14T13:06:45.767" OwnerUserId="334" ParentId="1676" PostTypeId="2" Score="10" />
  
  <row Body="&lt;p&gt;Sir Austin Bradford Hill's President's Address to the Royal Society of Medicine (&lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1898525/?tool=pubmed&quot;&gt;The Environment and Disease: Association or Causation?&lt;/a&gt;) explains nine criteria which help to judge whether there is a causal relationship between two correlated or associated variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;They are:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Strength of the association&lt;/li&gt;&#10;&lt;li&gt;Consistency: &quot;has it been repeatedly&#10;observed by different persons, in&#10;different places, cirumstances and&#10;times?&quot; &lt;/li&gt;&#10;&lt;li&gt;Specificity  &lt;/li&gt;&#10;&lt;li&gt;Temporality: &quot;which is the cart and&#10;which is the horse?&quot; - the cause&#10;must precede the effect &lt;/li&gt;&#10;&lt;li&gt;Biological gradient (dose-response&#10;curve) - in what way does the&#10;magnitude of the effect depended&#10;upon the magnitude of the (suspected) causal variable? &lt;/li&gt;&#10;&lt;li&gt;Plausibility - is there a likely&#10;explanation for causation?  &lt;/li&gt;&#10;&lt;li&gt;Coherance - would causation&#10;contradict other established facts?&lt;/li&gt;&#10;&lt;li&gt;Experiment - does experimental&#10;manipulation of the (suspected)&#10;causal variable affect the&#10;(suspected) dependent variable &lt;/li&gt;&#10;&lt;li&gt;Analogy - have we encountered&#10;similar causal relationships in the&#10;past?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2010-08-14T21:19:58.003" Id="1694" LastActivityDate="2010-08-14T21:19:58.003" OwnerUserId="942" ParentId="534" PostTypeId="2" Score="8" />
  
  <row AcceptedAnswerId="1706" AnswerCount="1" Body="&lt;p&gt;I am trying to solve for an efficient portfolio in R. How do I translate my constraints for a tangency point for 2 risky asset portfolio, and a given risk free rate to R solve.QP function? So basically I have the following equations:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;w = weight of the first risky asset&#10;R1 = mean return of the first risky asset&#10;R2 = mean return of the second risky asset&#10;sd1 = sdev of first risky asset&#10;sd2 = sdev of second risky asset&#10;corr = correlation between two risky assets&#10;rf = risk free rate&#10;Return of portfolio, R = R2*(1-w)+R1*w&#10;Standard Dev of portfolio, SD = sqrt((sd1*w)^2+(sd2*(1-w))^2+2*w*(1-w)*corr*sd1*sd2)&#10;&#10;Now I need to maximize R-rf while minimizing SD (that is maximize my sharpe). &#10;Let sigma be covariance matrix. So my function to minimize is W^T*sigma*W where W is&#10;the weights vector. Now simulataneously I need to maximize the excess return (R-rf)&#10;and W^T*1=1. I don't know how to express that in the constraints function.&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I am confused how to express these constraints as expected by &lt;a href=&quot;http://pbil.univ-lyon1.fr/library/quadprog/html/solve.QP.html&quot; rel=&quot;nofollow&quot;&gt;http://pbil.univ-lyon1.fr/library/quadprog/html/solve.QP.html&lt;/a&gt; . If you could also point me to a solved derivation of the final formula, that would be helpful as well as I am unable to get to the final formula.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2010-08-15T03:10:05.873" Id="1699" LastActivityDate="2010-09-30T21:19:46.560" LastEditDate="2010-09-30T21:19:46.560" LastEditorUserId="930" OwnerUserId="862" PostTypeId="1" Score="5" Tags="&lt;r&gt;&lt;finance&gt;&lt;maximum&gt;" Title="Tangency portfolio in R" ViewCount="1000" />
  
  <row AcceptedAnswerId="1717" AnswerCount="4" Body="&lt;p&gt;I have collected positional data. To visualize the data, I'd like to draw a 'typical' outcome of an experiment. &lt;/p&gt;&#10;&#10;&lt;p&gt;The data comes from a few hundred experiments, where I identify a variable number of objects at different positions relative to the origin in 2D. Thus, I can calculate the average number of objects, as well as estimate the empirical distribution of the objects. A plot of the 'typical' outcome would then have the average (or possibly mode) number of objects, say, 5. What I'm not sure about is where to position these 5 objects.&lt;/p&gt;&#10;&#10;&lt;p&gt;To simplify the problem, assume that the data follows a 2D normal distribution. If I were just to randomly draw 5 points from the distribution, I might get one point at [3,3], which would be a very rare outcome, and would thus not reflect the 'typical', or 'average' outcome. However, just drawing 5 points at [0,0] would also not make sense - even though [0,0] is the average position of the objects, 5 overlapping points are not an 'average' outcome of the process, either.&lt;/p&gt;&#10;&#10;&lt;p&gt;In other words, how can I get a 'likely' draw from a distribution?&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;It looks like I should mention &lt;em&gt;why&lt;/em&gt; I don't want to use the usual methods (like a 2D smoothed histogram, or plotting all the many points) to look at the 2D distribution. &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;The objects (which are vesicles (i.e. little spheres) inside cells) vary in number, size and position (distribution of the distance from the cell center, amount of clustering). I would like to display all these features in one graph. Since there are several hundred cells containing many vesicles each, it is not very useful to combine them all in a single plot. I am well aware that I could use a multipanel graph showing the distributions of all parameters, but this would be a lot less intuitive. &lt;/li&gt;&#10;&lt;li&gt;I would like to show a 'typical' cell that shows all the salient features that characterize a specific phenotype. This way, if I want to image a particular phenotype in a mixed population, I know what kind of cell I'm looking for.&lt;/li&gt;&#10;&lt;li&gt;I think such a plot would be a cool way to display a lot of information at once, and I just want to try.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Maybe it would be clearer If I said that I want to simulate a likely experimental result based on my measurements?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2010-08-15T14:30:23.597" Id="1709" LastActivityDate="2010-08-16T10:40:58.773" LastEditDate="2010-08-15T20:04:03.057" LastEditorUserId="198" OwnerUserId="198" PostTypeId="1" Score="4" Tags="&lt;distributions&gt;&lt;data-visualization&gt;" Title="How to draw a probable outcome from a distribution?" ViewCount="980" />
  <row Body="&lt;p&gt;I've recently been to a conference and one of the speakers gave this very interesting example (although the point was to illustrate something else):&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;Americans and English eat a lot of fat food. There is a high rate of cardiovascular diseases in US and UK.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;French eat a lot of fat food, but they have a low(er) rate of cardiovascular diseases.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Americans and English drink a lot of alcohol. There is a high rate of cardiovascular diseases in US and UK.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Italians drink a lot of alcohol but, again, they have a low(er) rate of cardiovascular diseases.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;The conclusion? Eat and drink what you want.&#10;And you have a higher chance of getting a heart attack if you speak English!&lt;/p&gt;&#10;" CommentCount="1" CommunityOwnedDate="2010-08-16T13:01:42.613" CreationDate="2010-08-15T15:33:31.153" Id="1710" LastActivityDate="2010-08-15T15:33:31.153" OwnerUserId="582" ParentId="36" PostTypeId="2" Score="3" />
  
  
  <row Body="&lt;p&gt;I would go straight to &lt;a href=&quot;http://videolectures.net/Top/#o=top&amp;amp;t=vl&quot; rel=&quot;nofollow&quot;&gt;VideoLectures.net&lt;/a&gt;. This is by far the best source--whether free or paid--i have found for very-high quality (both w/r/t the video quality and w/r/t the presentation content) video lectures and tutorials on statistics, forecasting, and machine learning. The target audience for these video lectures ranges from beginner (some lectures are specifically tagged as &quot;tutorials&quot;) to expert; most of them seem to be somewhere in the middle. &lt;/p&gt;&#10;&#10;&lt;p&gt;All of the lectures and tutorials are taught to highly experienced professionals and academics, and in many instances, the lecturer is the leading authority on the topic he/she is lecturing on. The site is also 100% free.&lt;/p&gt;&#10;&#10;&lt;p&gt;The one disadvantage is that you cannot download the lectures and store them in e.g., itunes; however, nearly every lectures has a set of slides which you can download (or, conveniently, you can view them online as you watch the presentation).&lt;/p&gt;&#10;&#10;&lt;p&gt;YouTube might have more, but even if you search Y/T through a specific channel, i am sure the signal-to-noise ratio is far higher--on VideoLectures.net, every lecture i've viewed has been outstanding and if you scan the viewer reviews, you'll find that's the consensus opinion towards the entire collection.&lt;/p&gt;&#10;&#10;&lt;p&gt;A few that i've watched and that i can recommend highly:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;em&gt;Basics of Probability and Statistics&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;em&gt;Introduction to Machine Learning&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;em&gt;Gaussian Process Basics&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;em&gt;Graphical Models&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;em&gt;k-Nearest Neighbor Models&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2010-08-16T07:09:25.387" Id="1728" LastActivityDate="2010-08-16T07:14:37.997" LastEditDate="2010-08-16T07:14:37.997" LastEditorUserId="438" OwnerUserId="438" ParentId="1719" PostTypeId="2" Score="3" />
  
  
  
  <row Body="&lt;p&gt;If I understand your question correctly you want to compute the quantiles for the &quot;No of failures before the first success&quot; given that $p=\frac{1}{104}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The distribution you should be looking at is the &lt;a href=&quot;http://en.wikipedia.org/wiki/Negative_binomial_distribution&quot; rel=&quot;nofollow&quot;&gt;negative binomial distribution&lt;/a&gt;. The wiki discusses the negative binomial as:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;In probability theory and statistics, the negative binomial distribution is a discrete probability distribution of the number of successes in a sequence of Bernoulli trials before a specified (non-random) number r of failures occurs.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Just invert the interpretation of success and failures with a setting of r=1 would accomplish what you want. The distribution with r=1 is also called the &lt;a href=&quot;http://en.wikipedia.org/wiki/Geometric_distribution&quot; rel=&quot;nofollow&quot;&gt;geometric distribution&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;You could then use the discrete distribution to compute the quantiles.&lt;/p&gt;&#10;&#10;&lt;p&gt;PS: I do not know R.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-08-16T17:43:49.463" Id="1751" LastActivityDate="2010-08-16T17:43:49.463" OwnerDisplayName="user28" ParentId="1749" PostTypeId="2" Score="3" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I am taking the time to learn how to analyze networks and want to test if there are differences between two networks over time.  Since I am new to R and networks in general, I am hoping to get some help how to compare and analyze network graphs.&lt;/p&gt;&#10;&#10;&lt;p&gt;Simply, my dataset will contain information on flows between two geographic units (think Census migration data).  I want to test if the flows, generally, are different with time. Since I am just starting out, I am not even sure if I am phrasing my question correctly.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have created a few basic graphs and generated some very basic &quot;summary statistics&quot; on a graph in isolation in R before, so I understand how to get up and running, but I am not really sure where to go from here.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-08-16T23:36:20.887" FavoriteCount="2" Id="1757" LastActivityDate="2010-08-17T07:51:41.177" LastEditDate="2010-08-17T07:37:41.443" LastEditorUserId="8" OwnerUserId="569" PostTypeId="1" Score="6" Tags="&lt;r&gt;&lt;networks&gt;" Title="Significant Difference between two network graphs" ViewCount="505" />
  <row Body="&lt;p&gt;I doubt that you can get an analytical solution to this problem. About the only step that is doable is the probability that the average of the first three velocities is under some cutoff and it is &lt;em&gt;not&lt;/em&gt; (probability the one velocity is under the cutoff)^3 as you stated. If $V_i \sim N (m, s^2)$, then $\bar{V}_3 \sim N (m, s^2/3)$, so the probability that $\bar{V}_3$ is less than $k$ standard deviations above the mean is $P(\bar{V}_3 &amp;lt; m + k s ) = \Phi(k/\sqrt{3})$ where $\Phi$ is the normal distribution function.  For example, for $k=0$: $P(\bar{V}_3 &amp;lt; m) = 0.5$, for $k=1$: $P(\bar{V}_3 &amp;lt; m + s) = 0.718$, etc.  &lt;/p&gt;&#10;&#10;&lt;p&gt;The calculation involving the next three rounds is complicated by two things: it should somehow be conditional on the fact that the average was too low already, and then should only use the three highest values. While it might work out to be some triple integral, I don't think you want to use such a formula. After that the fact that you have a choice whether the bullet should be weighed or fired also probably depends on the previous values, and things get even more complicated depending on your strategy.&lt;/p&gt;&#10;&#10;&lt;p&gt;Even the simple multiplication of a random bullet weight and one random velocity is not as innocuous as it seems: the product will not be normal (or any other &quot;regular&quot; distribution). Its standard deviation can be approximated via standard &lt;a href=&quot;http://en.wikipedia.org/wiki/Propagation_of_uncertainty#Example_formulas&quot; rel=&quot;nofollow&quot;&gt;error propagation&lt;/a&gt;, but calculating probabilities of falling below/above some cutoff is not straightforward.&lt;/p&gt;&#10;&#10;&lt;p&gt;In summary, I think you should write a little simulation program - you will get better answers faster. Note that for the chronometer you will have to define more exactly what does +/-4% mean.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2010-08-17T03:09:29.367" Id="1762" LastActivityDate="2010-08-17T03:36:03.013" LastEditDate="2010-08-17T03:36:03.013" LastEditorUserId="279" OwnerUserId="279" ParentId="1687" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Maybe it can be classified as lattice, but I'll try; plot all the bars scaled to the highest in one panel and put another panel showing zoom on lower ones. I used this technique once in case of a scatterplot, and the result was quite nice.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-08-17T06:22:29.067" Id="1765" LastActivityDate="2010-08-17T06:22:29.067" OwnerUserId="88" ParentId="1764" PostTypeId="2" Score="8" />
  <row Body="&lt;p&gt;You can calculate SD for each person and its standard error (for example using bootstrap). Then you can use &lt;strong&gt;rmeta&lt;/strong&gt; package to do analysis. I think you should use some transformation of SD for example log (or maybe better log of variance). &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-08-17T06:26:59.823" Id="1766" LastActivityDate="2010-08-17T06:26:59.823" OwnerUserId="419" ParentId="1649" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;You can try gamlss.cens package. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-08-17T12:27:00.557" Id="1779" LastActivityDate="2010-08-17T12:27:00.557" OwnerUserId="419" ParentId="1651" PostTypeId="2" Score="2" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I am fitting a GLM model (in R), and would like to get an estimation of the variability of the coefficients estimated by the model.&lt;/p&gt;&#10;&#10;&lt;p&gt;If I understand it correctly the method to use in such a case is bootstraping (not, cross validation).&lt;/p&gt;&#10;&#10;&lt;p&gt;Am I correct that an easy way to do this is by using the boot command from the boot package, then output the coefficients at each simulation, and at the end calculate their var?  Or is there something I might be missing? &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-08-17T15:56:39.067" FavoriteCount="2" Id="1787" LastActivityDate="2010-08-18T16:38:48.443" OwnerUserId="253" PostTypeId="1" Score="5" Tags="&lt;r&gt;&lt;confidence-interval&gt;&lt;variance&gt;&lt;generalized-linear-model&gt;&lt;bootstrap&gt;" Title="Using bootstrap for glm coefficients variance estimation (in R)" ViewCount="1324" />
  <row Body="&lt;p&gt;so you have a document x keyword matrix which basically represents a bipartite graph (or two-mode network depending on your cultural background) with edges between documents and tags. If you're not interested in individual documents - as I understand you -, you can create a network of keywords by counting the number of cooccurrences between each keyword. Simply plotting this graph might already give you a neat idea of what this data looks like. You can further tweak the visualization if you, e.g., scale the size of the keywords by the number of total occurrences, or (in case you have a lot of keywords) introduce a minimum number of total occurrences for a keyword to appear in the first place. &lt;/p&gt;&#10;&#10;&lt;p&gt;As a tool, I can only recommend &lt;a href=&quot;http://www.graphviz.org/&quot;&gt;GraphViz&lt;/a&gt; which allows you to specify graphs like&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;keyword1 -- keyword2&#10;keyword1 -- keyword3&#10;keyword1[label=&quot;statistics&quot;, fontsize=...]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and &quot;compile&quot; them into pngs, pdfs, whatever, yielding very nice results (particularly if you play a bit with the font settings).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-08-17T16:27:30.400" Id="1789" LastActivityDate="2010-08-17T16:27:30.400" OwnerUserId="979" ParentId="1780" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;From my point of view, when there are two explanatory variables and both have just two levels, we have the famous two-by-two contingency table. Fisher’s exact test can take such a matrix as its sole argument. Alternatively you can use Pearson’s chi-squared test. &lt;/p&gt;&#10;&#10;&lt;p&gt;If your null hypothesis is not the 25:25:25:25 distribution across the four categories (i.e. say it's 9:3:3:1), you'll have to calculate the expected frequencies explicitly.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then perform the chi-squared test (in R) like that:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;chisq.test(observed,p=c(9,3,3,1),rescale.p=TRUE)&#10;# rescale.p is needed because the probabilities do not sum to 1.0&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2010-08-17T18:31:59.927" Id="1794" LastActivityDate="2010-08-17T18:31:59.927" OwnerUserId="339" ParentId="1661" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;A useful approach when the variable is used as an independent factor in regression is to replace it by two variables: one is a binary indicator of whether it is zero and the other is the value of the original variable or a re-expression of it, such as its logarithm.  This technique is discussed in Hosmer &amp;amp; Lemeshow's book on logistic regression (and in other places, I'm sure).  Truncated probability plots of the positive part of the original variable are useful for identifying an appropriate re-expression.&lt;/p&gt;&#10;&#10;&lt;p&gt;When the variable is the dependent one in a linear model, censored regression (like Tobit) can be useful, again obviating the need to produce a started logarithm.  This technique is common among econometricians.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-08-17T18:48:40.177" Id="1795" LastActivityDate="2010-08-17T18:48:40.177" OwnerUserId="919" ParentId="1444" PostTypeId="2" Score="22" />
  
  <row Body="&lt;p&gt;Maybe you could measure the average difference between two runs of the same algorithm to the average difference between two runs from different algorithms. Doesn't solve the problem of how to measure that difference, but might be a more tractable problem. And the individual values of the time series would feed into the difference calculation instead of having to be treated as individual datapoints to be evaluated against each other (I also don't think that the particular difference at the nth step is what you really want to make statements about).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;&#10;Concerning details - well which features of the time series are you interested in, beyond the final error? I guess you actually got three different questions to solve: &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;What constitues similarity for you, ie what do you mean when you say you don't believe the two methods are different?&lt;/li&gt;&#10;&lt;li&gt;How do you quantify it - can be answered after 1, and &lt;/li&gt;&#10;&lt;li&gt;How can you test for significant differences between your two methods?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;All I was saying in the first post was that the answer to (1) probably doesn't consider the individual differences at each of the 1000 generations. And that I'd advise coming up with a scalar value for either each time series or at least similarity between time series. Only then you get to the actual statistics question (which I know least about of all three points, but I was advised to use a paired t-test in a similar question I just asked, when having a scalar value per element).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-08-18T12:00:36.353" Id="1824" LastActivityDate="2010-08-18T14:09:05.340" LastEditDate="2010-08-18T14:09:05.340" LastEditorUserId="979" OwnerUserId="979" ParentId="1813" PostTypeId="2" Score="4" />
  <row AcceptedAnswerId="1830" AnswerCount="5" Body="&lt;p&gt;How would you describe &lt;a href=&quot;http://en.wikipedia.org/wiki/Cross-validation_%28statistics%29&quot;&gt;&lt;strong&gt;cross-validation&lt;/strong&gt;&lt;/a&gt; to someone without a data analysis background?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-08-18T13:11:19.190" FavoriteCount="11" Id="1826" LastActivityDate="2010-08-19T23:49:54.203" OwnerUserId="5" PostTypeId="1" Score="28" Tags="&lt;cross-validation&gt;" Title="Cross-Validation in plain english?" ViewCount="3361" />
  
  <row Body="&lt;p&gt;The wiki link: &lt;a href=&quot;http://en.wikipedia.org/wiki/Linear_regression#Estimation_methods&quot; rel=&quot;nofollow&quot;&gt;Estimation Methods for Linear Regression&lt;/a&gt; gives a fairly comprehensive list of estimation methods including OLS and the contexts in which alternative estimation methods are   used.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-08-18T14:01:06.170" Id="1833" LastActivityDate="2013-03-26T15:47:07.683" LastEditDate="2013-03-26T15:47:07.683" LastEditorUserId="603" OwnerDisplayName="user28" ParentId="1829" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;The only further comment I would make is that the approach need not be Bayesian and the model need not be a mixed or random effects model. &lt;/p&gt;&#10;&#10;&lt;p&gt;In the simplest case if you had two series in x the mean model may be:&lt;/p&gt;&#10;&#10;&lt;p&gt;y = b01 + I*b02 + b11*x + I.b12*x&lt;/p&gt;&#10;&#10;&lt;p&gt;Where I indicates a sample from the 2nd series. An omnibus F-test can be used to determine whether the additional parameters are required to maintain distinct series (Ho: b02 = b12 = 0). &#10;&lt;a href=&quot;http://en.wikipedia.org/wiki/F_test&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/F_test&lt;/a&gt;&#10;This can be extended to more series, but it soon becomes more efficient to use a mixed or random effects model. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-08-18T14:13:25.737" Id="1835" LastActivityDate="2010-08-18T14:13:25.737" OwnerUserId="521" ParentId="1822" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="1843" AnswerCount="2" Body="&lt;p&gt;Greetings,&lt;/p&gt;&#10;&#10;&lt;p&gt;Currently I'm doing the following in R:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;require(zoo)&#10;data &amp;lt;- read.csv(file=&quot;summary.csv&quot;,sep=&quot;,&quot;,head=TRUE)&#10;cum  = zoo(data$dcomp, as.Date(data$date))&#10;data = zoo(data$compressed, as.Date(data$date))&#10;data &amp;lt;- aggregate(data, identity, tail, 1)&#10;cum  &amp;lt;- aggregate(cum, identity, sum, 1)&#10;days = seq(start(data), end(data), &quot;day&quot;)&#10;data2 = na.locf(merge(data, zoo(,days)))&#10;&#10;plot(data2,xlab='',ylab='compressed bytes',col=rgb(0.18,0.34,0.55))&#10;lines(cum,type=&quot;h&quot;,col=rgb(0,0.5,0))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Snip of summary.csv:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;date,revision,file,lines,nclass,nattr,nrel,bytes,compressed,diff,dcomp&#10;2007-07-25,16,model.xml,96,11,22,5,4035,991,0,0&#10;2007-07-27,17,model.xml,115,16,26,6,4740,1056,53,777&#10;2007-08-09,18,model.xml,106,16,26,7,4966,1136,47,761&#10;2007-08-10,19,model.xml,106,16,26,7,4968,1150,4,202&#10;2007-09-06,81,model.xml,111,16,26,7,5110,1167,13,258&#10;...&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The last two lines plot the information I need, and the result resembles the following:&#10;&lt;img src=&quot;http://i.stack.imgur.com/ech9l.png&quot; alt=&quot;alt text&quot;&gt;&#10;Blue line is the entropy in bytes of the artifact I'm interested. Green lines represent the entropy of the changes.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, in this graph, it works well because there isn't a huge difference in scales. But I have other graphs where the green lines become so small one cannot see.&lt;/p&gt;&#10;&#10;&lt;p&gt;The solution I was looking for, involved two things:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;To move the green vertical lines to a second graph, just below the first one, with its own y axis, but shared x axis.&lt;/li&gt;&#10;&lt;li&gt;To provide it a logarithmic scale, since I'm more interested in the &quot;magnitude&quot;, than in the specific values.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Thanks in advance!&lt;/p&gt;&#10;&#10;&lt;p&gt;P.S. If someone can also tell me how could I put &quot;minor ticks&quot; in the x scale referring to the months, I appreciate :-) If these are too much questions for a single post, I can divide them further.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-08-18T14:55:55.133" FavoriteCount="4" Id="1838" LastActivityDate="2013-11-16T01:50:25.777" LastEditDate="2010-08-18T15:39:27.513" LastEditorUserId="990" OwnerUserId="990" PostTypeId="1" Score="8" Tags="&lt;r&gt;&lt;time-series&gt;&lt;data-visualization&gt;&lt;entropy&gt;" Title="How do I vertically stack two graphs with the same x scale, but a different y scale in R?" ViewCount="4856" />
  <row Body="&lt;p&gt;I tend to think of 'least squares' as a criterion for defining the best fitting regression line (i.e., that which makes the sum of 'squared' residuals 'least') and the 'algorithm' in this context as the set of steps used to determine the regression coefficients that satisfy that criterion. This distinction suggests that it is possible to have  different algorithms that would satisfy the same criterion. &lt;/p&gt;&#10;&#10;&lt;p&gt;I'd be curious to know whether others make this distinction and what terminology they use.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-08-18T14:57:00.733" Id="1839" LastActivityDate="2010-08-18T14:57:00.733" OwnerUserId="183" ParentId="1829" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="2053" AnswerCount="4" Body="&lt;p&gt;One way to summarize the comparison of two survival curves is to compute the hazard ratio (HR). There are (at least) two methods to compute this value. &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Logrank method. As part of the Kaplan-Meier calculations, compute the number of observed events (deaths, usually) in each group ($Oa$, and $Ob$), and the number of expected events assuming a null hypothesis of no difference in survival ($Ea$ and $Eb$). The hazard ratio then is:&#10;$$&#10;    HR= \frac{(Oa/Ea)}{(Ob/Eb)}&#10;$$&lt;/li&gt;&#10;&lt;li&gt;Mantel-Haenszel method. First compute V, which is the sum of the hypergeometric variances at each time point. Then compute the hazard ratio as:&#10;$$&#10; HR= \exp\left(\frac{(Oa-Ea)}{V}\right)&#10;$$&#10;I got both these equations from chapter 3 of  Machin, Cheung and Parmar, &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0470870400&quot; rel=&quot;nofollow&quot;&gt;Survival Analysis&lt;/a&gt;. That book states that the two methods usually give very similar methods, and indeed that is the case with the example in the book. &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Someone sent me an example where the two methods differ by a factor of three. In this particular example, it is obvious that the logrank estimate is sensible, and the Mantel-Haenszel estimate is far off. My question is if anyone has any general advice for when it is best to choose the logrank estimate of the hazard ratio, and when it is best to choose the Mantel-Haenszel estimate? Does it have to do with sample size? Number of ties? Ratio of sample sizes?&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2010-08-18T15:47:15.627" FavoriteCount="5" Id="1844" LastActivityDate="2013-05-25T15:42:51.127" LastEditDate="2013-05-25T15:42:51.127" LastEditorUserId="7290" OwnerUserId="25" PostTypeId="1" Score="13" Tags="&lt;survival&gt;&lt;hazard&gt;" Title="What are the pros and cons of using the logrank vs. the Mantel-Haenszel method for computing the Hazard Ratio in survival analysis?" ViewCount="3897" />
  <row Body="&lt;p&gt;I can assure you that RF would work in that case and its importance measure would be pretty insightful (because there will be no large tail of misleading unimportant attributes like in standard (n &amp;lt;&amp;lt; p)s). I can't recall now any paper dealing with similar problem, but I'll look for it.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2010-08-18T21:28:17.517" Id="1858" LastActivityDate="2010-08-18T21:28:17.517" OwnerUserId="88" ParentId="1856" PostTypeId="2" Score="2" />
  
  <row AcceptedAnswerId="1871" AnswerCount="3" Body="&lt;p&gt;I ran a within subjects repeated measures experiment, where the independent variable had 3 levels. The dependent variable is a measure of correctness and is recorded as either correct / incorrect. Time taken to provide an answer was also recorded.&lt;/p&gt;&#10;&#10;&lt;p&gt;A within subjects repeated measures ANOVA is used to establish whether there is significant differences in correctness (DV) between the 3 levels of the IV, there is significant. Now, I'd like to analyze whether there is significant differences in the time taken to provide the answers when the answers are 1) correct, and 2) incorrect.&lt;/p&gt;&#10;&#10;&lt;p&gt;My problem is: Across the levels there are different numbers of correct / incorrect answers, e.g. level 1 has 67 correct answers, level 2 has 30, level 3 has 25. &lt;/p&gt;&#10;&#10;&lt;p&gt;How can I compare the time take taken for all correct answers across the 3 levels? I think this means its unbalanced? Can I do 3 one way ANOVAS to do a pairwise comparison, while adjusting p downwards to account for each comparison?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-08-19T00:50:20.120" FavoriteCount="3" Id="1863" LastActivityDate="2010-08-21T07:21:06.203" OwnerUserId="993" PostTypeId="1" Score="5" Tags="&lt;variance&gt;&lt;repeated-measures&gt;&lt;unbalanced-classes&gt;&lt;within-subjects&gt;" Title="Might be an unbalanced within subjects repeated measures?" ViewCount="753" />
  <row AcceptedAnswerId="1918" AnswerCount="2" Body="&lt;p&gt;Following to the recent questions we had &lt;a href=&quot;http://stats.stackexchange.com/questions/1818/how-to-determine-the-sample-size-needed-for-repeated-measurement-anova/1823#1823&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;I was hopping to know if anyone had come across or can share &lt;strong&gt;R code for performing a custom power analysis based on simulation for a linear model?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Later I would obviously like to extend it to more complex models, but lm seems to right place to start. Thanks.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-08-19T02:10:15.867" FavoriteCount="5" Id="1866" LastActivityDate="2010-08-19T17:13:01.420" OwnerUserId="253" PostTypeId="1" Score="11" Tags="&lt;r&gt;&lt;power&gt;&lt;power-analysis&gt;" Title="How to simulate a custom power analysis of an lm model (using R)" ViewCount="1884" />
  <row Body="&lt;p&gt;You really need to figure out what is the question that you are trying to answer- or what question is management most interested in. Then you can select the survey questions that are most relevant to your problem. &lt;/p&gt;&#10;&#10;&lt;p&gt;Without knowing anything about your problem or dataset, here are some generic solutions:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Visually represent the answers as clusters. My favorite is by either using dendrograms or just plotting on an xy axis (Google &quot;cluster analysis r&quot; and go to the first result by statmethods.net)&lt;/li&gt;&#10;&lt;li&gt;Rank the questions from greatest to least &quot;daily or more frequently&quot; responses. This is an example that may not exactly work for you but perhaps it will inspire you &lt;a href=&quot;http://www.programmingr.com/content/building-scoring-and-ranking-systems-r&quot; rel=&quot;nofollow&quot;&gt;http://www.programmingr.com/content/building-scoring-and-ranking-systems-r&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;Crosstabs: if for example, you have a question &quot;How often do you come in late for work?&quot; and &quot;How often do you use Facebook?,&quot; by crosstabbing the two questions you can find out the percentage of people who rarely do both, or who do both everyday.(Google &quot;r frequency crosstabs&quot; or go to the aforementioned statmethods.net)&lt;/li&gt;&#10;&lt;li&gt;Correlograms. I don't have any experience with these but I saw it also on the statmethods.net website. Basically you find which questions have the highest correlation and then create a table. You may find this useful although it looks kind of &quot;busy.&quot;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="1" CreationDate="2010-08-19T03:15:34.427" Id="1869" LastActivityDate="2010-08-19T03:21:26.100" LastEditDate="2010-08-19T03:21:26.100" LastEditorUserId="995" OwnerUserId="995" ParentId="1862" PostTypeId="2" Score="9" />
  
  
  <row Body="&lt;p&gt;To my understanding, Hedges's g is a somewhat more accurate version of Cohen's d (with pooled SD) in that we add a correction factor for small sample. Both measures generally agree when the homoscedasticity assumption is not violated, but we may found situations where this is not the case, see e.g. McGrath &amp;amp; Meyer, &lt;em&gt;Psychological Methods&lt;/em&gt; 2006, &lt;strong&gt;11(4)&lt;/strong&gt;: 386-401 (&lt;a href=&quot;http://www.bobmcgrath.org/Pubs/When_effect_sizes_disagree.pdf&quot;&gt;pdf&lt;/a&gt;). Other papers are listed at the end of my reply.&lt;/p&gt;&#10;&#10;&lt;p&gt;I generally found that in almost every psychological or biomedical studies, this is the Cohen's d that is reported; this probably stands from the well-known rule of thumb for interpreting its magnitude (Cohen, 1988). I don't know about any recent paper considering Hedges's g (or Cliff delta as a non-parametric alternative). Bruce Thompson has a &lt;a href=&quot;http://people.cehd.tamu.edu/~bthompson/apaeffec.htm&quot;&gt;revised version&lt;/a&gt; of the APA section on effect size.&lt;/p&gt;&#10;&#10;&lt;p&gt;Googling about Monte Carlo studies around effect size measures, I found this paper which might be interesting (I only read the abstract and the simulation setup): &lt;a href=&quot;http://www.coedu.usf.edu/main/departments/me/documents/cohen.pdf&quot;&gt;Robust Confidence Intervals for Effect Sizes: A Comparative Study of Cohen’s d and Cliff’s Delta Under Non-normality and Heterogeneous Variances&lt;/a&gt; (pdf).&lt;/p&gt;&#10;&#10;&lt;p&gt;About your 2nd comment, the &lt;code&gt;MBESS&lt;/code&gt; R package includes various utilities for ES calculation (e.g., &lt;code&gt;smd&lt;/code&gt; and related functions).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Other references&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Zakzanis, K.K. (2001). Statistics to tell the truth, the whole truth, and nothing but the truth: Formulae, illustrative numerical examples, and heuristic interpretation of effect size analyses for neuropsychological researchers. &lt;em&gt;Archives of Clinical Neuropsychology&lt;/em&gt;, 16(7), 653-667. (&lt;a href=&quot;http://www.sciencedirect.com/science?_ob=ArticleURL&amp;amp;_udi=B6VDJ-43TFJ8B-4&amp;amp;_user=10&amp;amp;_coverDate=10%2F31%2F2001&amp;amp;_rdoc=1&amp;amp;_fmt=high&amp;amp;_orig=search&amp;amp;_sort=d&amp;amp;_docanchor=&amp;amp;view=c&amp;amp;_searchStrId=1435354249&amp;amp;_rerunOrigin=google&amp;amp;_acct=C000050221&amp;amp;_version=1&amp;amp;_urlVersion=0&amp;amp;_userid=10&amp;amp;md5=12f831e4b738bda3a217d40d6cfd329f&quot;&gt;pdf&lt;/a&gt;)&lt;/li&gt;&#10;&lt;li&gt;Durlak, J.A. (2009). How to Select, Calculate, and Interpret Effect Sizes. &lt;em&gt;Journal of Pediatric Psychology&lt;/em&gt; (&lt;a href=&quot;http://jpepsy.oxfordjournals.org/cgi/content/full/jsp004v1&quot;&gt;pdf&lt;/a&gt;)&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="1" CreationDate="2010-08-19T08:50:44.340" Id="1885" LastActivityDate="2010-08-19T08:50:44.340" OwnerUserId="930" ParentId="1850" PostTypeId="2" Score="5" />
  
  <row Body="&lt;p&gt;As gd047 mentioned, the standard way of measuring variability is to use the &lt;a href=&quot;http://en.wikipedia.org/wiki/Variance&quot; rel=&quot;nofollow&quot;&gt;variance&lt;/a&gt;. So your pseudo-code will be:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;vnew = vector of length subbenchmarks&#10;for s in subbenchmarks:&#10;  vnew[i] = variance(s)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now the problem is, even if  you don't change your code, &lt;code&gt;vnew&lt;/code&gt; will be different for each run - there is noise. To determine if a change is significant, we need to perform a &lt;a href=&quot;http://en.wikipedia.org/wiki/Statistical_hypothesis_testing&quot; rel=&quot;nofollow&quot;&gt;hypothesis test&lt;/a&gt;, i.e. can the change be explained as random variation or is likely that something has changed. A quick and dirty rule would be:&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{equation}&#10;Y_i = \sqrt{n/2} (\frac{vnew_i}{vold_i} -1) \sim N(0,1)&#10;\end{equation}&lt;/p&gt;&#10;&#10;&lt;p&gt;This means any values of $Y_i &amp;lt; -1.96$ (at a 5% significance level) can be considered significant, i.e. an improvement. However, I would probably increase this to -3 or -4. This would test for improvement in &lt;strong&gt;individual&lt;/strong&gt; benchmarks. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you want to combine all your benchmarks into a single test, then let&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{equation}&#10;\bar Y = \frac{1}{n} \sum Y_i&#10;\end{equation}&lt;/p&gt;&#10;&#10;&lt;p&gt;So &lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{equation}&#10;\sqrt{n} \bar Y \sim N(0, 1)&#10;\end{equation}&lt;/p&gt;&#10;&#10;&lt;p&gt;Hence, an appropriate test would be to consider values of $\bar Y &amp;lt; 1.96$ to indicate an improvement. &lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;If the benchmarks aren't Normal, then I would try working with log(benchmarks). It also depends on what you want to do. I read your question as &quot;You would like a good rule of thumb&quot;. In this case, taking logs is probably OK.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Further details of the mathematical reasoning are found at Section 3.2 of this &lt;a href=&quot;http://dsmts.googlecode.com/files/dsmts-userguide31.pdf&quot; rel=&quot;nofollow&quot;&gt;document&lt;/a&gt;.&lt;/li&gt;&#10;&lt;li&gt;I've made a approximation by assuming that v_old represents the true underlying variance.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="1" CreationDate="2010-08-19T12:29:30.023" Id="1897" LastActivityDate="2010-08-19T12:53:59.380" LastEditDate="2010-08-19T12:53:59.380" LastEditorUserId="8" OwnerUserId="8" ParentId="1895" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;John Tukey's truly strange idea: exploratory data analysis.&#10;&lt;a href=&quot;http://en.wikipedia.org/wiki/Exploratory_data_analysis&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Exploratory_data_analysis&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2010-08-19T13:01:46.790" CreationDate="2010-08-19T13:01:46.790" Id="1900" LastActivityDate="2010-08-19T13:01:46.790" OwnerUserId="521" ParentId="1883" PostTypeId="2" Score="7" />
  <row Body="&lt;p&gt;The &lt;a href=&quot;http://cran.r-project.org/web/packages/igraph/index.html&quot;&gt;igraph&lt;/a&gt; library implements some algorithms for community structure based on Newman's optimization of modularity.  You can consult the &lt;a href=&quot;http://cran.r-project.org/web/packages/igraph/igraph.pdf&quot;&gt;reference manual&lt;/a&gt; for details and citations.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-08-19T16:59:14.233" Id="1917" LastActivityDate="2010-08-19T16:59:14.233" OwnerUserId="251" ParentId="1915" PostTypeId="2" Score="6" />
  
  <row AnswerCount="7" Body="&lt;p&gt;If so, what?&#10;If not, why not?&lt;/p&gt;&#10;&#10;&lt;p&gt;For a sample on the line, the median minimizes the total absolute deviation. It would seem natural to extend the definition to R2, etc., but I've never seen it. But then, I've been out in left field for a long time.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-08-19T19:36:01.337" FavoriteCount="3" Id="1927" LastActivityDate="2015-02-11T14:37:28.947" LastEditDate="2010-08-20T08:25:44.230" LastEditorUserId="8" OwnerUserId="1011" PostTypeId="1" Score="22" Tags="&lt;multivariable&gt;&lt;median&gt;" Title="Is there an accepted definition for the median of a sample on the plane, or higher ordered spaces?" ViewCount="1269" />
  
  
  <row Body="&lt;p&gt;The Wikipedia entry on Bootstrapping is actually very good:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29&quot;&gt;http://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The most common reason bootstrapping is applied is when the form of the underlying distribution from which a sample is taken is unknown. Traditionally statisticians assume a normal distribution (for very good reasons related to the central limit theorem), but statistics (such as the standard deviation, confidence intervals, power calculations etc) estimated via normal distribution theory are only strictly valid if the underlying population distribution is normal.&lt;/p&gt;&#10;&#10;&lt;p&gt;By repeatedly re-sampling the sample itself, bootstrapping enables estimates that are distribution independent. Traditionally each &quot;resample&quot; of the original sample randomly selects the same number of observations as in the original sample. However these are selected with replacement. If the sample has N observations, each bootstrap resample will have N observations, with many of the original sample repeated and many excluded. &lt;/p&gt;&#10;&#10;&lt;p&gt;The parameter of interest (eg. odds ratio etc) can then be estimated from each bootstrapped sample. Repeating the bootstrap say 1000 times allows an estimate of the &quot;median&quot; and 95% confidence interval on the statistic (eg odds ratio) by selecting the 2.5th, 50th and 97.5th percentile.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-08-20T00:35:28.433" Id="1939" LastActivityDate="2010-08-20T00:35:28.433" OwnerUserId="521" ParentId="1935" PostTypeId="2" Score="7" />
  <row Body="&lt;p&gt;Bootstrap is essentially a simulation of repeating experiment; let's say you have a box with balls an want to obtain an average size of a ball -- so you draw some of them, measure and take a mean. Now you want to repeat it to get the distribution, for instance to get a standard deviation -- but you found out that someone stole the box.&lt;br&gt;&#10;What can be done now is to use what you have -- this one series of measurements. The idea is to put the balls to the new box and simulate the original experiment by drawing the same number of balls with replacement -- both to have same sample size and some variability. Now this can be replicated many times to get a series of means which can be finally used to approximate the mean distribution. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-08-20T11:16:27.923" Id="1950" LastActivityDate="2010-08-20T11:16:27.923" OwnerUserId="88" ParentId="1935" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;There are actually several more methods and the choice often depends on whether you are most interested in looking for early differences, later differences or - as for the log-rank test &amp;amp; the Mantel-Haenszel test - give equal weight to all time points.&lt;/p&gt;&#10;&#10;&lt;p&gt;To the question at hand. The log-rank test is in fact a form of the Mantel-Haenszel test applied to survival data. The Mantel-Haenszel test is usually used to test for independence in stratified contingency tables.&lt;/p&gt;&#10;&#10;&lt;p&gt;If we try to apply the MH test to survival data, we can start by assuming that events at each failure time are independent. We then stratify by failure time. We use the MH methods for by making each failure time a strata. Not surprisingly they often give the same result.&lt;/p&gt;&#10;&#10;&lt;p&gt;The exception occcurs when more than one event occurs simultaneously - multiple deaths at exactly the same time point. I can't remember how the treatment then differs. I think the log-rank test averages over the possible orderings of the tied failure times. &lt;/p&gt;&#10;&#10;&lt;p&gt;So the log-rank test is the MH test for survival data and can deal with ties. I've never used the MH test for survival data.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-08-20T13:54:04.360" Id="1954" LastActivityDate="2010-08-20T13:54:04.360" OwnerUserId="521" ParentId="1844" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;Numerical experiments (for 2 &amp;lt;= n &amp;lt;= 4000 and all values of beta) indicate the estimate n^2 - (n Ln(n))*beta exceeds the &lt;em&gt;logarithm&lt;/em&gt; of the expectation by an amount on the order of beta*Exp(-n).  The error appears to increase monotonically in beta for each n.  This should provide some useful clues about how to proceed (for those with the time and interest).  In particular, an upper bound for the expectation exists of the form Exp(n^2 - (n Ln(n))*beta + C*Exp(-n)*beta) with C &amp;lt;&amp;lt; 1. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;After staring at the summation expression for the expectation, it became evident where the nLn(n)*beta term comes from: break each binomial coefficient Comb(n,k) into its sum Comb(n-1,k) + Comb(n-1,k-1) and write Exp(k^2) = Exp((k-1)^2)*Exp(2k-1).  This decomposes the expectation e(n,beta) into the sum of two parts, one of which looks like the expectation e(n-1,beta) and the other of which is messy (because each term is multiplied by Exp(2k-1)) but can be bounded above by replacing all those exponential terms by their obvious upper bound Exp(2n-1).  (This is not too bad, because the last term with the highest exponent strongly dominates the entire sum.)  This gives a recursive inequality for the expectation,&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;e(n,beta) &amp;lt;= (n^-beta * Exp(2n-1) + 1 - n^-beta) * e(n-1,beta)&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Doing this n times creates a polynomial whose highest term is in fact Exp(n^2)*n^(-n*beta), with the remaining terms decreasing fairly rapidly.  At this point any reasonable bound on the remainder will produce an improved bound for the expectation essentially of the form suggested by the numerical experiments.  At this point you have to decide how hard you want to work to obtain a tighter upper bound; the numerical experiments suggest this additional work is not going to pay off unless you're interested in the smallest values of n.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-08-20T20:37:23.543" Id="1967" LastActivityDate="2010-08-20T21:38:08.627" LastEditDate="2010-08-20T21:38:08.627" LastEditorUserId="919" OwnerUserId="919" ParentId="1249" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;If you have more than two levels, you can use (M)CA:&#10;&lt;a href=&quot;http://en.wikipedia.org/wiki/Correspondence_analysis&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Correspondence_analysis&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-08-20T22:45:24.010" Id="1975" LastActivityDate="2010-08-21T10:48:49.690" LastEditDate="2010-08-21T10:48:49.690" LastEditorUserId="603" OwnerUserId="603" ParentId="1961" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;I wrote a post a while back on &lt;a href=&quot;http://jeromyanglim.blogspot.com/2009/12/meta-analysis-tips-resources-and.html&quot;&gt;getting started with meta analysis&lt;/a&gt; with:&#10;(a) tips on getting started,&#10;(b) links to online introductory texts, &#10;and (c) links to free software for meta analysis.&lt;/p&gt;&#10;&#10;&lt;p&gt;Specifically, you might want to read &lt;a href=&quot;http://www.stat-help.com/meta.pdf&quot;&gt;James DeCoster's notes&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2010-08-21T04:42:39.203" CreationDate="2010-08-21T04:42:39.203" Id="1979" LastActivityDate="2010-08-21T04:42:39.203" OwnerUserId="183" ParentId="1963" PostTypeId="2" Score="7" />
  
  <row Body="&lt;p&gt;We wrote a paper explaining how to use R/Bioconductor when analysing microarray data. The paper was written in Sweave and all the code used to generate the graphs is included as supplementary material.&lt;/p&gt;&#10;&#10;&lt;p&gt;Gillespie, C. S., Lei, G., Boys, R. J., Greenall, A. J., Wilkinson, D. J., 2010. &lt;a href=&quot;http://www.mas.ncl.ac.uk/~ncsg3/microarray/&quot;&gt;Analysing yeast time course microarray data using BioConductor: a case study using yeast2 Affymetrix arrays&lt;/a&gt; BMC Research Notes,  3:81.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-08-21T21:59:56.010" Id="1993" LastActivityDate="2010-09-02T09:45:08.890" LastEditDate="2010-09-02T09:45:08.890" LastEditorUserId="8" OwnerUserId="8" ParentId="1980" PostTypeId="2" Score="7" />
  
  <row Body="&lt;p&gt;When the structure of your data is naturally hierarchical or nested, multilevel modeling is a good candidate.  More generally, it's one method to model interactions.&lt;/p&gt;&#10;&#10;&lt;p&gt;A natural example is when your data is from an organized structure such as country, state, districts, where you want to examine effects at those levels.  Another example where you can fit such a structure is is longitudinal analysis, where you have repeated measurements from many subjects over time (e.g. some biological response to a drug dose).  One level of your model assumes a group mean response for all subjects over time.  Another level of your model then allows for perturbations (random effects) from the group mean, to model individual differences.  &lt;/p&gt;&#10;&#10;&lt;p&gt;A popular and good book to start with is Gelman's &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/052168689X&quot;&gt;&lt;em&gt;Data Analysis Using Regression and Multilevel/Hierachical Models&lt;/em&gt;&lt;/a&gt;.  &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2010-08-22T00:40:31.490" Id="1997" LastActivityDate="2010-08-22T00:40:31.490" OwnerUserId="251" ParentId="1995" PostTypeId="2" Score="14" />
  <row Body="&lt;p&gt;It is as problematic as the degree of correlation.  &lt;/p&gt;&#10;&#10;&lt;p&gt;The irony is that you wouldn't bother controlling if there weren't some expected correlation with one of the variables.  And, if you expect your independent variable to affect your dependent then it's necessarily somewhat correlated with both.  However, if it's highly correlated them perhaps you shouldn't be controlling for it since it's tantamount to controlling out the actual independent or dependent variable.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-08-22T02:10:22.197" Id="2001" LastActivityDate="2010-08-22T02:10:22.197" OwnerUserId="601" ParentId="1998" PostTypeId="2" Score="4" />
  <row AcceptedAnswerId="2019" AnswerCount="2" Body="&lt;p&gt;Let $X_1$, $X_2, \dots, X_n$ be identically distributed random variables with given mean, and let $N$ be a random variable with $N \ge 0$ that is independent of $X_1$, $X_2, \dots, X_n$. If $Y=X_1+X_2+...X_n$; how do we find $\mathrm{E}(Y|N=n)$? With $D$ being the domain of $y$, I know the conditional expectation is defined as&lt;/p&gt;&#10;&#10;&lt;p&gt;$\mathrm{E}(Y|N=n) = \sum\limits_{y \in D} y \cdot  \mathrm{P}(Y=y|N=n) = \sum\limits_{y \in D} y \cdot \dfrac{P(Y=y,N=n)}{P(N=n)}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am not sure how to count/compute $P(Y=y,N=n)$. How can it be computed?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-08-22T04:51:46.790" Id="2010" LastActivityDate="2014-12-03T02:46:55.200" LastEditDate="2014-12-03T02:46:55.200" LastEditorUserId="38160" OwnerUserId="862" PostTypeId="1" Score="0" Tags="&lt;probability&gt;&lt;conditional-probability&gt;&lt;conditional-expectation&gt;" Title="Conditional expectation given number of outcomes" ViewCount="176" />
  <row Body="&lt;p&gt;&lt;a href=&quot;http://rattle.togaware.com/&quot; rel=&quot;nofollow&quot;&gt;Rattle&lt;/a&gt; is a data mining GUI that provides a front end to a wide range of R packages.&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2010-08-22T10:07:23.333" CreationDate="2010-08-22T10:07:23.333" Id="2017" LastActivityDate="2010-08-22T10:07:23.333" OwnerUserId="183" ParentId="2007" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;&lt;a href=&quot;http://www.dice.ucl.ac.be/esann/&quot; rel=&quot;nofollow&quot;&gt;European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2010-08-23T14:53:02.577" CreationDate="2010-08-23T14:53:02.577" Id="2048" LastActivityDate="2010-08-23T14:53:02.577" OwnerUserId="976" ParentId="1908" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;You can find candidates for &quot;outliers&quot; among the support points of the minimum volume bounding ellipsoid.  (&lt;a href=&quot;http://www.stat.ucl.ac.be/ISdidactique/Rhelp/library/cluster/html/ellipsoidhull.html&quot; rel=&quot;nofollow&quot;&gt;Efficient algorithms&lt;/a&gt; to find these points in fairly high dimensions, both exactly and approximately, were invented in a spate of papers in the 1970's because this problem is intimately connected with a question in experimental design.)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-08-23T22:41:05.283" Id="2058" LastActivityDate="2012-02-17T19:04:13.153" LastEditDate="2012-02-17T19:04:13.153" LastEditorUserId="919" OwnerUserId="919" ParentId="213" PostTypeId="2" Score="10" />
  <row AnswerCount="1" Body="&lt;p&gt;It seems that MLE (via EM) is widely used in machine learning / statistics to learn the parameters of a mixture of Gaussians. I'm assuming we're given random samples from the mixture. &lt;/p&gt;&#10;&#10;&lt;p&gt;My question is: Are there any proven &lt;em&gt;quantitative&lt;/em&gt; bounds on the error in terms of the number of samples (and perhaps the parameters of the Gaussian)?&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, what is the runtime required to estimate the parameters up to a certain error?&lt;/p&gt;&#10;&#10;&lt;p&gt;Ideally, these bounds would not assume that we start in a local neighborhood of the optimal solution or any such thing. (If EM is not the method of choice and there is a better way of doing it, please point this out, as well.)&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-08-23T23:21:15.567" FavoriteCount="1" Id="2059" LastActivityDate="2011-03-27T16:01:57.583" LastEditDate="2011-03-27T16:01:57.583" LastEditorUserId="919" OwnerUserId="1072" PostTypeId="1" Score="6" Tags="&lt;estimation&gt;&lt;normal-distribution&gt;&lt;em-algorithm&gt;&lt;mixture&gt;" Title="Learning parameters of a mixture of Gaussian using MLE" ViewCount="447" />
  
  <row Body="&lt;p&gt;Another possible interpretation is that you are confusing &quot;normal&quot; and &quot;uniform&quot;.  A uniform variate will have a mean of 0.5 and be evenly distributed on [0, 1).  If this interpretation is correct, your code simplifies to&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;return ranf()&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2010-08-25T13:36:22.150" Id="2102" LastActivityDate="2010-08-25T13:36:22.150" OwnerUserId="919" ParentId="2093" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;You also might want to consider that perhaps you &lt;em&gt;don't&lt;/em&gt; want a normally distributed variable on [0,1) but rather a random variable with a bell-shaped distribution naturally restricted to the [0,1) interval. If so, consider the &lt;a href=&quot;http://en.wikipedia.org/wiki/Beta_distribution&quot; rel=&quot;nofollow&quot;&gt;Beta distribution&lt;/a&gt; or the &lt;a href=&quot;http://www.johndcook.com/blog/2009/11/24/kumaraswamy-distribution/&quot; rel=&quot;nofollow&quot;&gt;Kumaraswamy distribution&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-08-25T14:43:24.377" Id="2105" LastActivityDate="2010-08-25T14:43:24.377" OwnerUserId="279" ParentId="2093" PostTypeId="2" Score="1" />
  <row AnswerCount="3" Body="&lt;p&gt;For the selection of predictors in multivariate linear regression with $p$ suitable predictors, what methods are available to find an 'optimal' subset of the predictors without explicitly testing all $2^p$ subsets? In 'Applied Survival Analysis,' Hosmer &amp;amp; Lemeshow make reference to Kuk's method, but I cannot find the original paper. Can anyone describe this method, or, even better, a more modern technique? One may assume normally distributed errors.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2010-08-25T18:15:55.980" FavoriteCount="4" Id="2111" LastActivityDate="2014-07-22T17:12:30.223" LastEditDate="2010-08-25T22:57:35.883" LastEditorUserId="88" OwnerUserId="795" PostTypeId="1" Score="8" Tags="&lt;modeling&gt;&lt;regression&gt;&lt;multivariable&gt;&lt;model-selection&gt;&lt;feature-selection&gt;" Title="Computing best subset of predictors for linear regression" ViewCount="1120" />
  
  <row AcceptedAnswerId="2123" AnswerCount="1" Body="&lt;p&gt;Some guys told me that it's appropriate to use Wald-Wolfowitz Runs Test as a normality test (like Shapiro-Wilk's or Kolmogorov-Smirnov...). Do you think this is good way to test normality assumptions?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-08-25T21:56:01.473" FavoriteCount="1" Id="2119" LastActivityDate="2010-08-25T22:54:03.987" OwnerUserId="1356" PostTypeId="1" Score="1" Tags="&lt;normality&gt;&lt;hypothesis-testing&gt;&lt;nonparametric&gt;" Title="Wald-Wolfowitz Runs Test for Normality Assumptions Testing" ViewCount="578" />
  <row AcceptedAnswerId="2128" AnswerCount="9" Body="&lt;p&gt;In particular, I am referring to the Pearson product-moment correlation coefficient.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2010-08-25T23:53:00.417" FavoriteCount="31" Id="2125" LastActivityDate="2014-10-22T16:57:45.710" LastEditDate="2013-08-01T17:07:24.153" LastEditorUserId="7290" OwnerUserId="74" PostTypeId="1" Score="31" Tags="&lt;correlation&gt;&lt;regression&gt;" Title="What's the difference between correlation and simple linear regression?" ViewCount="46645" />
  <row Body="&lt;p&gt;What's the difference between the correlation between $X$ and $Y$ and a linear regression predicting $Y$ from $X$?&lt;/p&gt;&#10;&#10;&lt;p&gt;First, some &lt;strong&gt;similarities&lt;/strong&gt;:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;the standardised regression coefficient is the same as Pearson's correlation coefficient&lt;/li&gt;&#10;&lt;li&gt;The square of Pearson's correlation coefficient is the same as the $R^2$ in  simple linear regression&lt;/li&gt;&#10;&lt;li&gt;Neither simple linear regression nor correlation answer questions of causality directly. This point is important, because I've met people that think that simple regression can magically allow an inference that $X$ causes $Y$.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Second, some &lt;strong&gt;differences&lt;/strong&gt;:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;The regression equation (i.e., $a + bX$) can be used to make predictions on $Y$ based on values of $X$&lt;/li&gt;&#10;&lt;li&gt;While correlation typically refers to the linear relationship, it can refer to other forms of dependence, such as polynomial or truly nonlinear relationships&lt;/li&gt;&#10;&lt;li&gt;While correlation typically refers to Pearson's correlation coefficient, there are other types of correlation, such as Spearman's.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="2" CreationDate="2010-08-26T02:48:15.233" Id="2128" LastActivityDate="2013-09-20T17:41:09.067" LastEditDate="2013-09-20T17:41:09.067" LastEditorUserId="21054" OwnerUserId="183" ParentId="2125" PostTypeId="2" Score="46" />
  
  
  
  
  
  <row Body="&lt;p&gt;I use Python for statistical analysis and forecasting.  As mentioned by others above, Numpy and Matplotlib are good workhorses.  I also use ReportLab for producing PDF output.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm currently looking at both Resolver and Pyspread which are Excel-like spreadsheet applications which are based on Python.  Resolver is a commercial product but &lt;a href=&quot;http://pyspread.sourceforge.net/&quot;&gt;Pyspread&lt;/a&gt; is still open-source. (Apologies, I'm limited to only one link)&lt;/p&gt;&#10;" CommentCount="2" CommunityOwnedDate="2010-08-27T15:07:18.913" CreationDate="2010-08-27T10:10:35.860" Id="2159" LastActivityDate="2010-08-27T10:10:35.860" OwnerUserId="1105" ParentId="1595" PostTypeId="2" Score="11" />
  <row AnswerCount="10" Body="&lt;p&gt;I'm interested in getting some books about multivariate analysis, and need your recommendations. Free books are always welcome, but if you know about some great non-free MVA book, please, state it.&lt;/p&gt;&#10;" CommentCount="2" CommunityOwnedDate="2010-08-28T17:07:59.760" CreationDate="2010-08-28T17:07:59.760" FavoriteCount="9" Id="2181" LastActivityDate="2011-05-07T04:00:18.503" OwnerUserId="1356" PostTypeId="1" Score="13" Tags="&lt;books&gt;&lt;multivariate-analysis&gt;" Title="Book recommendations for multivariate analysis" ViewCount="5443" />
  
  
  <row Body="&lt;p&gt;An alternative approach would be to impute the missing raw data using a missing data replacement procedure. You could then run the PCA on the correlation matrix that resulted from the imputed dataset (see also &lt;a href=&quot;http://www.stat.psu.edu/~jls/mifaq.html&quot; rel=&quot;nofollow&quot;&gt;multiple imputation&lt;/a&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;Here are a few links on missing data imputation in R:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://www.stat.columbia.edu/~gelman/arm/missing.pdf&quot; rel=&quot;nofollow&quot;&gt;Gelman on missing data imputation&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://www.statmethods.net/input/missingdata.html&quot; rel=&quot;nofollow&quot;&gt;Quick-R&lt;/a&gt; has links to R packages such as &lt;code&gt;Amelia II&lt;/code&gt;, &lt;code&gt;Mice&lt;/code&gt; and &lt;code&gt;mitools&lt;/code&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="2" CreationDate="2010-08-30T03:31:36.610" Id="2201" LastActivityDate="2010-08-30T03:31:36.610" OwnerUserId="183" ParentId="2197" PostTypeId="2" Score="5" />
  
  
  
  <row AcceptedAnswerId="2218" AnswerCount="2" Body="&lt;p&gt;What is the difference between a &lt;a href=&quot;http://en.wikipedia.org/wiki/Feedforward_neural_network&quot;&gt;feed-forward&lt;/a&gt; and &lt;a href=&quot;http://en.wikipedia.org/wiki/Recurrent_neural_networks&quot;&gt;recurrent&lt;/a&gt; neural network?  Why would you use one over the other?  Do other network topologies exist?  &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-08-30T15:33:28.180" FavoriteCount="5" Id="2213" LastActivityDate="2014-11-15T12:25:10.600" OwnerUserId="5" PostTypeId="1" Score="17" Tags="&lt;machine-learning&gt;&lt;neural-networks&gt;&lt;topologies&gt;" Title="Feed-forward and recurrent neural networks?" ViewCount="4324" />
  
  <row Body="&lt;p&gt;I asked this question last week and obtained two excellent answers.  The question is readily accessible through links on your &quot;meta-analysis&quot; tag.  Here's the URL:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/1963/looking-for-good-introductory-treatment-of-meta-analysis&quot;&gt;http://stats.stackexchange.com/questions/1963/looking-for-good-introductory-treatment-of-meta-analysis&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="5" CommunityOwnedDate="2010-08-31T13:03:46.473" CreationDate="2010-08-30T20:33:07.587" Id="2224" LastActivityDate="2010-08-30T20:33:07.587" OwnerUserId="919" ParentId="2223" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;There is &lt;a href=&quot;http://www.isds.duke.edu/~mw/ABS04/Lecture_Slides/4.Stats_Regression.pdf&quot;&gt;this one&lt;/a&gt; on Bayesian learning:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/64Pe4.jpg&quot; alt=&quot;alt text&quot;&gt;&lt;/p&gt;&#10;" CommentCount="2" CommunityOwnedDate="2010-08-30T23:04:11.447" CreationDate="2010-08-30T23:04:11.447" Id="2227" LastActivityDate="2012-05-04T22:21:31.813" LastEditDate="2012-05-04T22:21:31.813" LastEditorUserId="919" OwnerUserId="881" ParentId="423" PostTypeId="2" Score="61" />
  <row Body="&lt;p&gt;I understand it as follows,&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Convergence in probability&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The probability that the sequence of random variables equals the target value is asymptotically decreasing and approaches 0 but never actually attains 0.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Almost Sure Convergence&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The sequence of random variables will equal the target value asymptotically but you cannot predict at what point it will happen.&lt;/p&gt;&#10;&#10;&lt;p&gt;Almost sure convergence is a stronger condition on the behavior of a sequence of random variables because it states that &quot;something will definitely happen&quot; (we just don't know when). In contrast, convergence in probability states that &quot;while something is likely to happen&quot; the likelihood of &quot;something &lt;em&gt;not&lt;/em&gt; happening&quot; decreases asymptotically but never actually reaches 0. (something $\equiv$ a sequence of random variables converging to a particular value). &lt;/p&gt;&#10;&#10;&lt;p&gt;The &lt;a href=&quot;http://en.wikipedia.org/wiki/Convergence_of_random_variables&quot; rel=&quot;nofollow&quot;&gt;wiki&lt;/a&gt; has some examples of both which should help clarify the above (in particular see the example of the archer in the context of convergence in prob and the example of the charity in the context of almost sure convergence).&lt;/p&gt;&#10;&#10;&lt;p&gt;From a practical standpoint, convergence in probability is enough as we do not particularly care about very unlikely events. As an example, consistency of an estimator is essentially convergence in probability. Thus, when using a consistent estimate, we implicitly acknowledge the fact that in large samples there is a very small probability that our estimate is far from the true value. We live with this 'defect' of convergence in probability as we know that asymptotically the probability of the estimator being far from the truth is vanishingly small.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-08-31T04:39:45.463" Id="2231" LastActivityDate="2010-08-31T04:39:45.463" OwnerDisplayName="user28" ParentId="2230" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;Check out the R &lt;a href=&quot;http://cran.r-project.org/web/packages/Epi/index.html&quot;&gt;Epi&lt;/a&gt; and &lt;a href=&quot;http://cran.r-project.org/web/packages/epitools/index.html&quot;&gt;epitools&lt;/a&gt; packages, which include many functions for computing exact and approximate CIs/p-values for various measures of association found in epidemiological studies, including relative risk (RR). I know there is also &lt;a href=&quot;http://cran.r-project.org/web/packages/PropCIs&quot;&gt;PropCIs&lt;/a&gt;, but I never tried it. Bootstraping is also an option, but generally these are exact or approximated CIs that are provided in epidemiological papers, although most of the explanatory studies rely on GLM, and thus make use of odds-ratio (OR) instead of RR (although, wrongly it is often the RR that is interpreted because it is easier to understand, but this is another story).&lt;/p&gt;&#10;&#10;&lt;p&gt;You can also check your results with online calculator, like on &lt;a href=&quot;http://statpages.org/ctab2x2.html&quot;&gt;statpages.org&lt;/a&gt;, or &lt;a href=&quot;http://www.phsim.man.ac.uk/risk/&quot;&gt;Relative Risk and Risk Difference Confidence Intervals&lt;/a&gt;. The latter explains how computations are done.&lt;/p&gt;&#10;&#10;&lt;p&gt;By &quot;exact&quot; tests, we generally mean tests/CIs not relying on an asymptotic distribution, like the chi-square or standard normal; e.g. in the case of an RR, an 95% CI may be approximated as&#10;$\exp\left[ \log(\text{rr}) - 1.96\sqrt{\text{Var}\big(\log(\text{rr})\big)} \right], \exp\left[ \log(\text{rr}) + 1.96\sqrt{\text{Var}\big(\log(\text{rr})\big)} \right]$, &#10;where $\text{Var}\big(\log(\text{rr})\big)=1/a - 1/(a+b) + 1/c - 1/(c+d)$ (assuming a 2-way cross-classification table, with $a$, $b$, $c$, and $d$ denoting cell frequencies). The explanations given by @Keith are, however, very insightful. &lt;/p&gt;&#10;&#10;&lt;p&gt;For more details on the calculation of CIs in epidemiology, I would suggest to look at Rothman and Greenland's textbook, &lt;a href=&quot;http://www.lww.com/product/?978-0-7817-5564-1&quot;&gt;Modern Epidemiology&lt;/a&gt; (now in it's 3rd edition), &lt;a href=&quot;http://www.wiley.com/remtitle.cgi?0471526290&quot;&gt;Statistical Methods for Rates and Proportions&lt;/a&gt;, from Fleiss et al., or &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1637917/&quot;&gt;Statistical analyses of the relative risk&lt;/a&gt;, from J.J. Gart (1979). &lt;/p&gt;&#10;&#10;&lt;p&gt;You will generally get similar results with &lt;code&gt;fisher.test()&lt;/code&gt;, as pointed by @gd047, although in this case this function will provide you with a 95% CI for the odds-ratio (which in the case of a disease with low prevalence will be very close to the RR).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Notes:&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;I didn't check your Excel file, for the reason advocated by @csgillespie.&lt;/li&gt;&#10;&lt;li&gt;Michael E Dewey provides an interesting summary of &lt;a href=&quot;http://www.zen103156.zen.co.uk/rr.pdf&quot;&gt;confidence intervals for risk ratios&lt;/a&gt;, from a digest of posts on the R mailing-list.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2010-08-31T07:47:14.557" Id="2233" LastActivityDate="2010-08-31T11:14:57.893" LastEditDate="2010-08-31T11:14:57.893" LastEditorUserId="930" OwnerUserId="930" ParentId="1531" PostTypeId="2" Score="9" />
  <row Body="&lt;p&gt;I found this &lt;a href=&quot;http://www.erlang-factory.com/upload/presentations/282/neo4j-is-not-erlang-but-i-still-heart-you-2010-06-10.pdf&quot;&gt;from a NoSQL presentation&lt;/a&gt;, but the cartoon can be found directly at&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://browsertoolkit.com/fault-tolerance.png&quot;&gt;http://browsertoolkit.com/fault-tolerance.png&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/MV8HX.png&quot; alt=&quot;alt text&quot;&gt;&lt;/p&gt;&#10;" CommentCount="1" CommunityOwnedDate="2010-08-31T15:58:42.227" CreationDate="2010-08-31T15:58:42.227" Id="2241" LastActivityDate="2010-08-31T15:58:42.227" OwnerUserId="1080" ParentId="423" PostTypeId="2" Score="31" />
  <row Body="&lt;p&gt;This answer is incorrect, as the true value A is not a random variable and hence P(A &amp;lt; some value) is meaningless. I tried to formalize the idea that overlapping confidence intervals doesn't mean that both samples are likely to be  drawn from the same population. My bad.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;WRONG ANSWER&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;More mathematically : Assuming the&#10;  scores values are independent (which&#10;  they aren't, but with the right&#10;  setting it should be close), the&#10;  chance that A has as low as 34.5 and B&#10;  as high as 35.5 equals&#10;  p(A&amp;lt;=34.5)*p(B&gt;=35.5) = 0.000625.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="3" CreationDate="2010-08-31T16:32:21.467" Id="2242" LastActivityDate="2010-09-03T23:51:38.993" LastEditDate="2010-09-03T23:51:38.993" LastEditorUserId="1124" OwnerUserId="1124" ParentId="2182" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;This question is a little unusual because the nature of &quot;different&quot; is unspecified.  This response is formulated in the spirit of trying to detect as many kinds of differences as possible, not just changes of location (&quot;trend&quot;).&lt;/p&gt;&#10;&#10;&lt;p&gt;One approach that might have more power than most, while remaining agnostic about the relative magnitudes represented by the five groups (e.g., adopting a multinomial model) yet retaining the ordering of the groups, is a Kolmogorov-Smirnov test: as a test statistic use the size of the largest deviation between the two empirical cdfs.  This is easy and quick to compute and it would also be easy to bootstrap a p-value by pooling the two sets of results.&lt;/p&gt;&#10;&#10;&lt;p&gt;Specifically, let the count in bin $j$ for group $i$ be $k_{ij}$.  Then the empirical cdf for group $i$ is essentially the vector $\left( 0 = m_{i0}, m_{i1}, \ldots, m_{i5}=n_i \right) / n_i$ where $m_{i,j} = m_{i-1,j} + k_{ij}, 1 \le i \le 5$.  The test statistic is the sup norm of the difference of these two vectors.&lt;/p&gt;&#10;&#10;&lt;p&gt;Critical values ($\alpha = 0.05$) with two groups of ten individuals are going to be around 0.2 - 0.4, with the higher values occurring when the 20 values are spread evenly between the two extremes.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-09-01T00:27:21.270" Id="2253" LastActivityDate="2010-09-01T00:27:21.270" OwnerUserId="919" ParentId="2248" PostTypeId="2" Score="4" />
  <row AcceptedAnswerId="2257" AnswerCount="1" Body="&lt;p&gt;I have a dataset forwhich i have performed an mds and visualized the results using scatterplot3d library. However i would like to see the names of the points on the 3d plot. How do i accomplish that? Each column belongs to a certain group i would like to see which points belong to which groups on the 3dplot.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#generate a distance matrix of the data&#10;d &amp;lt;- dist(data)&#10;&#10;#perform the MDS on  3 dimensions and include a Goodness-of-fit (GOF)&#10;&#10;fit.mds &amp;lt;- cmdscale(d,eig=TRUE, k=3) # k is the number of dimensions; 3 in this case&#10;&#10;#Assign names x,y,z to the result vectors (dimension numbers)&#10;x &amp;lt;- fit.mds$points[,1]&#10;y &amp;lt;- fit.mds$points[,2]&#10;z &amp;lt;- fit.mds$points[,3]&#10;&#10;plot3d &amp;lt;- scatterplot3d(x,y,z,highlight.3d=TRUE,xlab=&quot;&quot;,ylab=&quot;&quot;,pch=16,main=&quot;Multidimensional Scaling 3-D Plot&quot;,col.axis=&quot;blue&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2010-09-01T05:55:30.923" Id="2256" LastActivityDate="2010-09-18T21:56:35.090" LastEditDate="2010-09-18T21:56:35.090" LastEditorUserId="930" OwnerUserId="18462" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;multidimensional-scaling&gt;" Title="Adding labels to points using mds and scatter3d package with R" ViewCount="2506" />
  <row Body="&lt;p&gt;That really depends on the form and the height of the curve. If you assume the curves are all gaussian and you know the heights, then you can calculate the area under the curve by using the normal density function. In R this would become:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;heights &amp;lt;- 1&#10;avg &amp;lt;- 3&#10;sdev &amp;lt;- 2&#10;&#10;AUC &amp;lt;- heights/dnorm(avg,avg,sd) # the density function at the mean&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;As the value of the density function at the mean is only determined by the sd, this information suffices for calculation of the AUC, given the assumptions are correct. If all heights are the same, the AUC is proportional to the sd only. &lt;/p&gt;&#10;&#10;&lt;p&gt;Without information about the shape of the curve and the heights, you simply cannot calculate the AUC as far as I know.&lt;/p&gt;&#10;" CommentCount="10" CreationDate="2010-09-01T10:10:54.380" Id="2263" LastActivityDate="2010-09-01T10:10:54.380" OwnerUserId="1124" ParentId="2262" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;In their classic book on the &lt;em&gt;Federalist&lt;/em&gt; papers, Mosteller and Wallace argue for a log penalty function: you penalize yourself $-\log(p)$ when you predict an event with probability $p$ and it occurs; the penalty for it not occurring equals $-\log(1-p)$.  Thus, the penalty is high when whatever happens is unexpected according to your prediction.&lt;/p&gt;&#10;&#10;&lt;p&gt;Their argument in favor of this function rests on a simple natural criterion: &quot;the penalty function should encourage the prediction of the correct probabilities if they are known.&quot;  Assuming the total penalty is summed over all predictions and there will be three or more of them, M&amp;amp;W claim that the log penalty function is the &lt;em&gt;only&lt;/em&gt; one (up to affine transformation) for which the &quot;expected penalty is minimized over all predictions&quot; by the correct probabilities.&lt;/p&gt;&#10;&#10;&lt;p&gt;Following this, then, a good test for you to use is to track your accumulated log penalties.  If, after a long time (or by means of some independent oracle), you obtain accurate estimates of what the probabilities actually were, you can compare your penalty with the minimum possible one.  The average of that difference measures your long-run predictive performance (the lower the better).  This is an excellent way to compare two or more competing predictors, too.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-09-01T16:49:30.997" Id="2284" LastActivityDate="2010-09-01T16:49:30.997" OwnerUserId="919" ParentId="2275" PostTypeId="2" Score="7" />
  
  
  <row Body="&lt;p&gt;Some references to help you out. &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Tipping, M. E. &amp;amp; Bishop, C. M.&#10;Probabilistic principal component&#10;analysis Journal of the Royal&#10;Statistical Society (Series B),&#10;1999, 21, 611-622&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Tom Minka. Automatic choice of&#10;dimensionality for PCA. NIPS 2000&#10;url:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://research.microsoft.com/en-us/um/people/minka/papers/pca/&quot;&gt;http://research.microsoft.com/en-us/um/people/minka/papers/pca/&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;Šmídl, V. &amp;amp; Quinn, A. On Bayesian&#10;principal component analysis&#10;Computational Statistics &amp;amp; Data&#10;Analysis, 2007, 51, 4101-4123&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;If you are familiar with information theoretic model selection (MML, MDL, etc.), I highly recommend checking out:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Wallace, C. S. &amp;amp; Freeman, P. R.&#10;Single-Factor Analysis by Minimum&#10;Message Length Estimation Journal of&#10;the Royal Statistical Society&#10;(Series B), 1992, 54, 195-209&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;C. S. Wallace. Multiple Factor&#10;Analysis by MML Estimation.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.allisons.org/ll/Images/People/Wallace/Multi-Factor/&quot;&gt;http://www.allisons.org/ll/Images/People/Wallace/Multi-Factor/&lt;/a&gt;&lt;br&gt;&#10;Tech report:&#10;&lt;a href=&quot;http://www.allisons.org/ll/Images/People/Wallace/Multi-Factor/TR95.218.pdf&quot;&gt;http://www.allisons.org/ll/Images/People/Wallace/Multi-Factor/TR95.218.pdf&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="1" CreationDate="2010-09-02T02:19:37.283" Id="2297" LastActivityDate="2010-09-02T02:19:37.283" OwnerUserId="530" ParentId="2296" PostTypeId="2" Score="5" />
  <row AcceptedAnswerId="2303" AnswerCount="2" Body="&lt;p&gt;What broad methods are there to detect fraud, anomalies, fudging, etc. in scientific works produced by a third party? (I was motivated to ask this by the recent &lt;a href=&quot;http://en.wikipedia.org/wiki/Marc_Hauser#Scientific_misconduct&quot;&gt;Marc Hauser affair&lt;/a&gt;.) Usually for election and accounting fraud, some variant of &lt;a href=&quot;http://en.wikipedia.org/wiki/Benfords_law&quot;&gt;Benford's Law&lt;/a&gt; is cited. I am not sure how this could be applied to &lt;em&gt;e.g.&lt;/em&gt; the Marc Hauser case, because Benford's Law requires numbers to be approximately log uniform. &lt;/p&gt;&#10;&#10;&lt;p&gt;As a concrete example, suppose a paper cited the p-values for a large number of statistical tests. Could one transform these to log uniformity, then apply Benford's Law? It seems like there would be all kinds of problems with this approach (&lt;em&gt;e.g.&lt;/em&gt; some of the null hypotheses might legitimately be false, the statistical code might give p-values which are only approximately correct, the tests might only give p-values which are uniform under the null asymptotically, etc.)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-09-02T04:01:56.450" FavoriteCount="8" Id="2299" LastActivityDate="2010-09-26T22:53:23.677" LastEditDate="2010-09-18T21:53:38.603" LastEditorUserId="930" OwnerUserId="795" PostTypeId="1" Score="20" Tags="&lt;meta-analysis&gt;&lt;fraud&gt;" Title="Statistical forensics: Benford and beyond" ViewCount="435" />
  
  <row Body="&lt;p&gt;Cluster Analysis by Brian S. Everitt is a nice book length applied treatment of Cluster Analysis.&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2010-09-02T09:35:37.073" CreationDate="2010-09-02T04:23:58.130" Id="2301" LastActivityDate="2010-09-02T04:23:58.130" OwnerUserId="485" ParentId="2291" PostTypeId="2" Score="2" />
  
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I am trying to develop some algorithm to compute probabilities in multi-type branching trees, and I doubt I am doing this right...&lt;/p&gt;&#10;&#10;&lt;p&gt;Let us consider a multi-type branching process with two types, denoted by 0 and 1. The process starts in state 0 with probability 1, so that the root vertex of any tree generated by this process has state 0. A vertex in state 0 generates two vertices in state 1 with probability 1. A vertex in state 1 generates either two vertices in state 0 (with probability 0.5) or one vertex in state 0 and one vertex in state 1 (with probability 0.5).&#10;We denote those three possible transitions by:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;A: 0 -&gt; 1 1 (1.)&lt;/li&gt;&#10;&lt;li&gt;B: 1 -&gt; 0 0 (0.5)&lt;/li&gt;&#10;&lt;li&gt;C: 1 -&gt; 0 1 (0.5)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;(their probabilities are between brackets).&lt;/p&gt;&#10;&#10;&lt;p&gt;In the sequel, a &quot;tree&quot; will refer to an unordered tree, &lt;em&gt;i.e.&lt;/em&gt; two isomorphic ordered trees such that the tree isomorphism preserves the labels will be considered as a same mathematical object (following the principle in Chi (2004), p. 1993, paragraph 3 - see link at the end of the post). &lt;/p&gt;&#10;&#10;&lt;p&gt;Now let us consider the particular labeled tree of height three, composed by one root vertex in state 0 with &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;one first child in state 1 that itself has one child in state 0 and one child in state 1&lt;/li&gt;&#10;&lt;li&gt;one second child in state 1 that itself has two children in state 0&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;This tree should be depicted in some file I am not allowed to post, so I should draw it in ascii mode with limited guarantee on the result:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;           0  &#10;           |  &#10;     -------------  &#10;     |           |  &#10;     1           1&#10;     |           |  &#10; ---------   ---------  &#10; |       |   |       |  &#10; 0       1   0       0  &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I would like to compute the probability that this tree was generated by the above multi-type branching process after 2 generation steps (the generation of the root vertex does not count as a step).&lt;/p&gt;&#10;&#10;&lt;p&gt;Using equation 2 p. 1994 in Chi (2004), this probability should be 0.25, since each transition A, B and C is applied once. However, each possible tree of height 3 among the three trees generated by this process has probability 0.25, and the sum of the probabilities is 0.75 instead of 1.&lt;/p&gt;&#10;&#10;&lt;p&gt;Another possibility is to consider that in this given tree, the set of children (0, 0) of vertex 1 may have been generated by any vertex in state 1 (and same principle for the set of children (0, 1)), so that the tree probability is in fact 0.25+0.25 = 0.5. &lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, how to compute the probability that a given tree &lt;em&gt;t&lt;/em&gt; of height &lt;em&gt;n&lt;/em&gt; was generated by a multi-type branching process after &lt;em&gt;n-1&lt;/em&gt; generation steps ? Can equation 2 p. 1994 in Chi (2004) be used ? Or do we have to compute the number of trees that are isomorphic to &lt;em&gt;t&lt;/em&gt; in some sense ? Or do we have to give up the idea that isomorphic trees are equivalent representations of same object ?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for your help ! &#10;JB&lt;/p&gt;&#10;&#10;&lt;p&gt;Ref.&lt;br&gt;&#10;Z. Chi. &lt;a href=&quot;http://projecteuclid.org/DPubS?service=UI&amp;amp;version=1.0&amp;amp;verb=Display&amp;amp;handle=euclid.aoap/1099674086&quot; rel=&quot;nofollow&quot;&gt;Limit laws of estimators for critical multi-type Galton–Watson processes.&lt;/a&gt; Ann. Appl. Probab. Volume 14, Number 4 (2004), 1992-2015. &lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;As a beginning of answer to the suggestion of Aniko, I think the following equivalence relation could lead to equivalence classes of trees such that formula (2) could be applied. It consists in adding constraints to the usual notion of isomorphism (by &quot;isomorphism&quot;, I mean &quot;an isomorphism preserving labels&quot;). I denote by $I(v)$ the label of vertex $v$, and &#10;$t_v$ the complete subtree rooted in $v$. A complete subtree rooted in $v$ is the subgraph induced by the descendants of $v$. &lt;/p&gt;&#10;&#10;&lt;p&gt;The property $P(t)$ is defined inductively by: &lt;/p&gt;&#10;&#10;&lt;p&gt;$P(t)$ is satisfied iif $t$ has a single vertex, or if for every pair ${v, v'}$ of children of the root of $t$ such that $I(v)$ = $I(v')$, $t_v$ and $t_{v'}$ are isomorphic and they satisfy $P(t_{v'})$ and $P(t_{v})$. &lt;/p&gt;&#10;&#10;&lt;p&gt;The two trees $t$ and $t'$ are in the same equivalence class iif [they are isomorphic, and property $P$ is satisfied for $t$ and $t'$] or [$t=t'$].&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-09-03T13:21:55.897" FavoriteCount="2" Id="2343" LastActivityDate="2010-11-22T16:36:15.373" LastEditDate="2010-09-10T13:33:36.397" LastEditorUserId="1185" OwnerUserId="1185" PostTypeId="1" Score="3" Tags="&lt;probability&gt;&lt;algorithms&gt;&lt;stochastic-processes&gt;" Title="Trees generated by multi-type branching processes in n steps" ViewCount="333" />
  <row Body="&lt;p&gt;Regarding making it reproducible, the best way is to &lt;em&gt;provide reproducible research&lt;/em&gt; (i.e. code and data) along with the paper.  Make it available on your website, or on a hosting site (like github).&lt;/p&gt;&#10;&#10;&lt;p&gt;Regarding visualization, Leo Breiman has done some interesting work on this (see &lt;a href=&quot;http://www.stat.berkeley.edu/~breiman/RandomForests/&quot;&gt;his homepage&lt;/a&gt;, in particular the &lt;a href=&quot;http://www.stat.berkeley.edu/~breiman/RandomForests/cc_graphics.htm&quot;&gt;section on graphics&lt;/a&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;But if you're using R, then the &lt;code&gt;randomForest&lt;/code&gt; package has some useful functions:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;data(mtcars)&#10;mtcars.rf &amp;lt;- randomForest(mpg ~ ., data=mtcars, ntree=1000, keep.forest=FALSE,&#10;                           importance=TRUE)&#10;plot(mtcars.rf, log=&quot;y&quot;)&#10;varImpPlot(mtcars.rf)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(1)&#10;data(iris)&#10;iris.rf &amp;lt;- randomForest(Species ~ ., iris, proximity=TRUE,&#10;                        keep.forest=FALSE)&#10;MDSplot(iris.rf, iris$Species)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I'm not aware of a simple way to actually plot a tree, but you can use the &lt;code&gt;getTree&lt;/code&gt; function to retrieve the tree and plot that separately.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;getTree(randomForest(iris[,-5], iris[,5], ntree=10), 3, labelVar=TRUE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The &lt;a href=&quot;http://www.statistik.uni-dortmund.de/useR-2008/slides/Strobl+Zeileis.pdf&quot;&gt;Strobl/Zeileis presentation on &quot;Why and how to use random forest variable importance measures (and how you shouldn’t)&quot;&lt;/a&gt; has examples of trees which must have been produced in this way.  This &lt;a href=&quot;http://www.statmethods.net/advstats/cart.html&quot;&gt;blog post on tree models&lt;/a&gt; has some nice examples of CART tree plots which you can use for example.&lt;/p&gt;&#10;&#10;&lt;p&gt;As @chl commented, a single tree isn't especially meaningful in this context, so short of using it to explain what a random forest is, I wouldn't include this in a paper.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-09-03T14:32:35.293" Id="2346" LastActivityDate="2010-09-03T15:30:56.143" LastEditDate="2010-09-03T15:30:56.143" LastEditorUserId="5" OwnerUserId="5" ParentId="2344" PostTypeId="2" Score="27" />
  
  
  <row Body="&lt;p&gt;You need to specify the purpose of the model before you can say whether Shao's results are applicable. For example, if the purpose is prediction, then LOOCV makes good sense and the inconsistency of variable selection is not a problem. On the other hand, if the purpose is to identify the important variables and explain how they affect the response variable, then Shao's results are obviously important and LOOCV is not appropriate.&lt;/p&gt;&#10;&#10;&lt;p&gt;The AIC is asymptotically LOOCV and BIC is asymptotically equivalent to a leave-$v$-out CV where $v=n[1-1/(\log(n)-1)]$ --- the BIC result for linear models only. So the BIC gives consistent model selection. Therefore a short-hand summary of Shao's result is that AIC is useful for prediction but BIC is useful for explanation.&lt;/p&gt;&#10;" CommentCount="12" CreationDate="2010-09-04T03:12:13.830" Id="2367" LastActivityDate="2010-09-14T02:33:16.480" LastEditDate="2010-09-14T02:33:16.480" LastEditorUserId="159" OwnerUserId="159" ParentId="2352" PostTypeId="2" Score="9" />
  
  
  <row Body="&lt;p&gt;From what I've seen so far, FA is used for attitude items as it is for other kind of rating scales. The problem arising from the metric used (that is, &quot;are Likert scales really to be treated as numeric scales?&quot; is a long-standing debate, but providing you check for the bell-shaped response distribution you may handle them as continuous measurements, otherwise check for non-linear FA models or &lt;em&gt;optimal scaling&lt;/em&gt;) may be handled by polytmomous IRT models, like the Graded Response, Rating Scale, or Partial Credit Model. The latter two may be used as a rough check of whether the threshold distances, as used in Likert-type items, are a characteristic of the response format (RSM) or of the particular item (PCM).&lt;/p&gt;&#10;&#10;&lt;p&gt;Regarding your second point, it is known, for example, that response distributions in attitude or health surveys differ from one country to the other (e.g. chinese people tend to highlight 'extreme' response patterns compared to those coming from western countries, see e.g. Song, X.-Y. (2007) Analysis of multisample structural equation models with applications to Quality of Life data, in &lt;em&gt;Handbook of Latent Variable and Related Models&lt;/em&gt;, Lee, S.-Y. (Ed.), pp 279-302, North-Holland). Some methods to handle such situation off the top of my head:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;use of log-linear models (marginal approach) to highlight strong between-groups imbalance at the item level (coefficients are then interpreted as relative risks instead of odds);&lt;/li&gt;&#10;&lt;li&gt;the multi-sample SEM method from Song cited above (Don't know if they do further work on that approach, though).&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Now, the point is that most of these approaches focus at the item level (ceiling/floor effect, decreased reliability, bad item fit statistics, etc.), but when one is interested in how people deviate from what would be expected from an ideal set of observers/respondents, I think we must focus on person fit indices instead.&lt;/p&gt;&#10;&#10;&lt;p&gt;Such $\chi^2$ statistics are readily available for IRT models, like INFIT or OUTFIT mean square, but generally they apply on the whole questionnaire. Moreover, since estimation of items parameters rely in part on persons parameters (e.g., in the marginal likelihood framework, we assume a gaussian distribution), the presence of outlying individuals may lead to potentially biased estimates and poor model fit. &lt;/p&gt;&#10;&#10;&lt;p&gt;As proposed by Eid and Zickar (2007), combining a latent class model (to isolate group of respondents, e.g. those always answering on the extreme categories vs. the others) and an IRT model (to estimate item parameters and persons locations on the latent trait in both groups) appears a nice solution. Other modeling strategies are described in their paper (e.g. HYBRID model, see also Holden and Book, 2009).&lt;/p&gt;&#10;&#10;&lt;p&gt;Likewise, &lt;a href=&quot;http://www.psychology.gatech.edu/unfolding/publications.html&quot;&gt;unfolding models&lt;/a&gt; may be used to cope with &lt;em&gt;response style&lt;/em&gt;, which is defined as a consistent and content-independent pattern of response category (e.g. tendency to agree with all statements). In the social sciences or psychological literature, this is know as Extreme Response Style (ERS). References (1–3) may be useful to get an idea on how it manifests and how it may be measured.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is a short list of papers that may help to progress on this subject:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Hamilton, D.L. (1968). &lt;a href=&quot;http://www.psych.umn.edu/faculty/waller/classes/meas08/Readings/Hamilton1968.pdf&quot;&gt;Personality attributes associated with extreme response style&lt;/a&gt;. &lt;em&gt;Psychological Bulletin&lt;/em&gt;, &lt;strong&gt;69(3)&lt;/strong&gt;: 192–203.&lt;/li&gt;&#10;&lt;li&gt;Greanleaf, E.A. (1992). Measuring extreme response style. &lt;em&gt;Public Opinion Quaterly&lt;/em&gt;, &lt;strong&gt;56(3)&lt;/strong&gt;: 328-351.&lt;/li&gt;&#10;&lt;li&gt;de Jong, M.G., Steenkamp, J.-B.E.M., Fox, J.-P., and Baumgartner, H. (2008). Using Item Response Theory to Measure Extreme Response Style in Marketing Research: A Global Investigation. &lt;em&gt;Journal of marketing research&lt;/em&gt;, &lt;strong&gt;45(1)&lt;/strong&gt;: 104-115.&lt;/li&gt;&#10;&lt;li&gt;Morren, M., Gelissen, J., and Vermunt, J.K. (2009). &lt;a href=&quot;http://spitswww.uvt.nl/~vermunt/morren2009.pdf&quot;&gt;Dealing with extreme response style in cross-cultural research: A restricted latent class factor analysis approach&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;Moors, G. (2003). Diagnosing Response Style Behavior by Means of a Latent-Class Factor Approach. Socio-Demographic Correlates of Gender Role Attitudes and Perceptions of Ethnic Discrimination Reexamined. &lt;em&gt;Quality &amp;amp; Quantity&lt;/em&gt;, 37(3), 277-302.&lt;/li&gt;&#10;&lt;li&gt;de Jong, M.G. Steenkamp J.B., Fox, J.-P., and Baumgartner, H. (2008). Item Response Theory to Measure Extreme Response Style in Marketing Research: A Global Investigation. &lt;em&gt;Journal of Marketing Research&lt;/em&gt;, 45(1), 104-115.&lt;/li&gt;&#10;&lt;li&gt;Javaras, K.N. and Ripley, B.D. (2007). An “Unfolding” Latent Variable Model for Likert Attitude Data. &lt;em&gt;JASA&lt;/em&gt;, 102(478): 454-463.&lt;/li&gt;&#10;&lt;li&gt;slides from Moustaki, Knott and Mavridis, &lt;a href=&quot;http://www.rcec.nl/Publicaties/Downloads%2025ste%20IRT%20workshop/Irini%20Moustaki.pdf&quot;&gt;Methods for detecting outliers in latent variable models&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;Eid, M. and Zickar, M.J. (2007). Detecting response styles and faking in personality and organizational assessments by Mixed Rasch Models. In von Davier, M. and Carstensen, C.H. (Eds.), &lt;em&gt;Multivariate and Mixture Distribution Rasch Models&lt;/em&gt;, pp. 255–270, Springer.&lt;/li&gt;&#10;&lt;li&gt;Holden, R.R. and Book, A.S. (2009). Using hybrid Rasch-latent class modeling to improve the detection of fakers on a personality inventory. &lt;em&gt;Personality and Individual Differences&lt;/em&gt;, &lt;strong&gt;47(3)&lt;/strong&gt;: 185-190.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="2" CreationDate="2010-09-04T12:21:17.757" Id="2375" LastActivityDate="2010-09-07T10:18:19.340" LastEditDate="2010-09-07T10:18:19.340" LastEditorUserId="930" OwnerUserId="930" ParentId="2374" PostTypeId="2" Score="18" />
  <row Body="&lt;p&gt;In physics, statistical physics in particular, Metropolis-type algorithm(s) are used extensively. There are really countless variants of these, and the new ones are being actively developed. It's much too broad topic to give any sort of expanation here, so if you're interested you can start e.g. from &lt;a href=&quot;http://mcwa.csi.cuny.edu/umass/lectures.html&quot; rel=&quot;nofollow&quot;&gt;these lecture notes&lt;/a&gt; or from the ALPS library webpage (http://alps.comp-phys.org/mediawiki).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-09-05T03:23:37.750" Id="2378" LastActivityDate="2010-09-05T03:23:37.750" OwnerUserId="1197" ParentId="2348" PostTypeId="2" Score="0" />
  
  <row AcceptedAnswerId="2409" AnswerCount="5" Body="&lt;p&gt;I've never liked how people typically analyze data from Likert scales as if error were continuous &amp;amp; Gaussian when there are reasonable expectations that these assumptions are violated at least at the extremes of the scales. What do you think of the following alternative:&lt;/p&gt;&#10;&#10;&lt;p&gt;If the response takes value $k$ on an $n$-point scale, expand that data to $n$ trials, $k$ of which have the value 1 and $n-k$ of which have the value 0. Thus, we're treating response on a Likert scale as if it is the overt aggregate of a covert series of binomial trials (in fact, from a cognitive science perspective, this is actually an appealing model for the mechanisms involved in such decision making scenarios). With the expanded data, you can now use a mixed effects model specifying respondent as a random effect (also question as a random effect if you have multiple questions) and using the binomial link function to specify the error distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can anyone see any assumption violations or other detrimental aspects of this approach?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-09-06T00:58:21.607" FavoriteCount="5" Id="2401" LastActivityDate="2013-11-22T23:09:49.413" LastEditDate="2013-01-17T14:36:43.033" LastEditorUserId="17230" OwnerUserId="364" PostTypeId="1" Score="10" Tags="&lt;binomial&gt;&lt;psychometrics&gt;&lt;likert&gt;&lt;psychology&gt;" Title="Is it appropriate to treat n-point Likert scale data as n trials from a binomial process?" ViewCount="1378" />
  <row Body="&lt;p&gt;To give you a few more things to look at:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Principal components - look at some previous answers about &lt;a href=&quot;http://stats.stackexchange.com/questions/tagged/pca&quot;&gt;PC&lt;/a&gt;. In particular, this &lt;a href=&quot;http://stats.stackexchange.com/questions/1289/visualizing-multiple-histograms&quot;&gt;answer&lt;/a&gt; may be helpful.&lt;/li&gt;&#10;&lt;li&gt;Cluster analysis. This &lt;a href=&quot;http://www.statmethods.net/advstats/cluster.html&quot; rel=&quot;nofollow&quot;&gt;page&lt;/a&gt; gives quite a nice overall in R.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I would recommend trying as many things as possible and see what comes out. Once you have your data in R in a reasonable format, it shouldn't take too long to try these things.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-09-06T07:48:01.307" Id="2407" LastActivityDate="2010-09-06T07:48:01.307" OwnerUserId="8" ParentId="2385" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;You might consider using &lt;a href=&quot;http://en.wikipedia.org/wiki/Leo_Breiman&quot; rel=&quot;nofollow&quot;&gt;Breiman's&lt;/a&gt; bagging or &lt;a href=&quot;http://en.wikipedia.org/wiki/Random_forest&quot; rel=&quot;nofollow&quot;&gt;random forests&lt;/a&gt;.  One good reference is &lt;a href=&quot;http://www.springerlink.com/content/l4780124w2874025/&quot; rel=&quot;nofollow&quot;&gt;Breiman &quot;Bagging Predictors&quot;&lt;/a&gt; (1996).  Also summarized in Clifton Sutton's &lt;a href=&quot;http://mason.gmu.edu/~csutton/vt6.pdf&quot; rel=&quot;nofollow&quot;&gt;&quot;Classification and Regression Trees, Bagging, and Boosting&quot;&lt;/a&gt; in the Handbook of Statistics.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can also see &lt;a href=&quot;http://cran.r-project.org/doc/Rnews/Rnews_2002-3.pdf&quot; rel=&quot;nofollow&quot;&gt;Andy Liaw and Matthew Wiener R News discussion&lt;/a&gt; of the randomForest package.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2010-09-06T15:20:57.673" Id="2412" LastActivityDate="2010-09-06T15:30:27.687" LastEditDate="2010-09-06T15:30:27.687" LastEditorUserId="5" OwnerUserId="5" ParentId="2410" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0387402721&quot; rel=&quot;nofollow&quot;&gt;&lt;em&gt;All of Statistics&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="5" CommunityOwnedDate="2010-09-07T03:19:29.143" CreationDate="2010-09-07T03:19:29.143" Id="2431" LastActivityDate="2010-09-07T03:19:29.143" OwnerUserId="183" ParentId="2423" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="2440" AnswerCount="1" Body="&lt;p&gt;A cursory search reveals that &lt;a href=&quot;http://en.wikipedia.org/wiki/Latin_square&quot; rel=&quot;nofollow&quot;&gt;Latin squares&lt;/a&gt; are fairly extensively used in the design of experiments.  During my PhD, I have studied various theoretical properties of Latin squares (from a combinatorics point-of-view), but do not have a deep understanding of what is it about Latin squares that make them particularly well-suited to experimental design.&lt;/p&gt;&#10;&#10;&lt;p&gt;I understand that Latin squares are good at allowing statisticians to efficiently study situations where there are two factors which vary in different &quot;directions&quot;.  But, I'm also fairly confident there would be many other techniques that could be used.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;What is it, in particular, about Latin squares that make them so well suited for the design of experiments, that other designs do not have?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Moreover, there are zillions of Latin squares to choose from, so which Latin square do you choose?  I understand that choosing one at random is important, but there would still be some Latin squares that would be less suited to running experiments than others (e.g. the Cayley table of a cyclic group).  This raises the following question.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Which properties of Latin squares are desirable and which properties of Latin squares are undesirable for experimental design?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="0" CreationDate="2010-09-07T11:48:49.003" FavoriteCount="3" Id="2439" LastActivityDate="2010-09-07T17:32:38.397" LastEditDate="2010-09-07T17:32:38.397" LastEditorUserId="8" OwnerUserId="386" PostTypeId="1" Score="7" Tags="&lt;experiment-design&gt;" Title="Desirable and undesirable properties of Latin squares in experiments?" ViewCount="536" />
  <row Body="&lt;p&gt;A comprehensive and authoratative reference is Kendall's &lt;em&gt;Advanced Theory of Statistics&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;Volume 1 &lt;em&gt;Distribution Theory&lt;/em&gt;  &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Volume 2A &lt;em&gt;Classical Inference and Linear Models&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;There is also a Volume 2B but it is &lt;em&gt;Bayesian Inference&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Other than those, I agree the Casella and Berger is an excellent reference at the graduate level, and suggest Bain and Engelhardt's &lt;em&gt;Introduction to Probability and Mathematical Statistics&lt;/em&gt; for upper-level undergraduates.&lt;/p&gt;&#10;" CommentCount="3" CommunityOwnedDate="2010-09-07T17:02:12.730" CreationDate="2010-09-07T17:02:12.730" Id="2450" LastActivityDate="2010-09-07T17:02:12.730" OwnerUserId="1107" ParentId="2423" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;I'm going to ask the consultant's dumb question.  Why do you want to know if these distributions are different in a statistically significant way?  &lt;/p&gt;&#10;&#10;&lt;p&gt;Is it that the data that you are using are representative samples from populations or processes, and you want to assess the evidence that those populations or processes differ?  If so, then a statistical test is right for you.  But this seems like a strange question to me.&lt;/p&gt;&#10;&#10;&lt;p&gt;Or, are you interested in whether you really need to behave as though those populations or processes are different, regardless of the truth?  Then you will be better off determining a loss function, ideally one that returns units that are meaningful to you, and predicting the expected loss when you (a) treat the populations as different, and (b) treat them as the same.  Or you can choose some quantile of the loss distribution if you want to adopt a more or less conservative position.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-09-07T23:43:17.103" Id="2453" LastActivityDate="2010-09-07T23:43:17.103" OwnerUserId="187" ParentId="4" PostTypeId="2" Score="7" />
  <row Body="&lt;p&gt;if you are going to go the numerical route, a simple observation: the problem appears to be subject to red-black parity (a flea on a red square always moves to a black square, and vice-versa). This can help reduce your problem size by a half (just consider two moves at a time, and only look at fleas on the red squares, say.) &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2010-09-08T02:11:18.783" Id="2459" LastActivityDate="2010-09-08T02:11:18.783" OwnerUserId="795" ParentId="2427" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;One solution to your 1. question is to use cross-validation. You compute classification accuracy for models with different number of components and then pick one with the highest classification accuracy. You can check the references below:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.bepress.com/sagmb/vol3/iss1/art33/&quot; rel=&quot;nofollow&quot;&gt;PLS Dimension Reduction for Classification with Microarray Data&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.sciencedirect.com/science?_ob=ArticleURL&amp;amp;_udi=B6V03-4Y34W95-2&amp;amp;_user=10&amp;amp;_coverDate=07%2F31%2F2010&amp;amp;_rdoc=1&amp;amp;_fmt=high&amp;amp;_orig=search&amp;amp;_origin=search&amp;amp;_sort=d&amp;amp;_docanchor=&amp;amp;view=c&amp;amp;_searchStrId=1454239636&amp;amp;_rerunOrigin=google&amp;amp;_acct=C000050221&amp;amp;_version=1&amp;amp;_urlVersion=0&amp;amp;_userid=10&amp;amp;md5=e2279e3a8f1e8a43932e99beadcdd90b&amp;amp;searchtype=a&quot; rel=&quot;nofollow&quot;&gt;Rasch-based high-dimensionality data reduction and class prediction with applications to microarray gene expression data&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;To my experience factor rotation does not improve classification accuracy. Please report your results.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2010-09-08T07:36:36.987" Id="2473" LastActivityDate="2010-09-08T07:36:36.987" OwnerUserId="609" ParentId="2467" PostTypeId="2" Score="4" />
  
  
  
  
  
  <row Body="&lt;p&gt;When thinking about whether normality testing is 'essentially useless', one first has to think about what it is supposed to be useful for. Many people (well... at least, many scientists) misunderstand the question the normality test answers. &lt;/p&gt;&#10;&#10;&lt;p&gt;The question normality tests answer: Is there convincing evidence of any deviation from the Gaussian ideal? With moderately large real data sets, the answer is almost always yes.  &lt;/p&gt;&#10;&#10;&lt;p&gt;The question scientists often expect the normality test to answer: Do the data deviate enough  from the Gaussian ideal to &quot;forbid&quot; use of a test that assumes a Gaussian distribution? Scientists often want the normality test to be the referee that decides when to abandon conventional (ANOVA, etc.) tests and instead analyze transformed data or use a rank-based nonparametric test or a resampling or bootstrap approach. For this purpose, normality tests are not very useful.&lt;/p&gt;&#10;" CommentCount="4" CommunityOwnedDate="2010-09-09T02:35:31.153" CreationDate="2010-09-09T02:35:31.153" Id="2501" LastActivityDate="2010-09-09T21:37:22.197" LastEditDate="2010-09-09T21:37:22.197" LastEditorUserId="25" OwnerUserId="25" ParentId="2492" PostTypeId="2" Score="74" />
  <row Body="&lt;p&gt;In both statistics (kernel density estimation or kernel smoothing) and machine learning (kernel methods) literature, kernel is used as a measure of similarity. In particular, the kernel function k(x,.) defines the distribution of similarities of points around a given point x. k(x,y) denotes the similarity of point x with another given point y.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2010-09-09T06:09:23.923" Id="2506" LastActivityDate="2010-09-09T06:53:49.670" LastEditDate="2010-09-09T06:53:49.670" LastEditorUserId="881" OwnerUserId="881" ParentId="2499" PostTypeId="2" Score="12" />
  <row Body="&lt;p&gt;No, this is not possible, because a finite sample of size $n$ cannot reliably distinguish between, say, a normal population and a normal population contaminated by a $1/N$ amount of a Cauchy distribution where $N$ &gt;&gt; $n$.  (Of course the former has finite variance and the latter has infinite variance.)  Thus any fully nonparametric test will have arbitrarily low power against such alternatives.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2010-09-09T12:57:35.380" Id="2511" LastActivityDate="2010-09-09T12:57:35.380" OwnerUserId="919" ParentId="2504" PostTypeId="2" Score="10" />
  <row AcceptedAnswerId="2530" AnswerCount="2" Body="&lt;p&gt;I have 2560 paired observations from an experiment in which participants provided two ratings for a set of objects, at two different points in time.  Half of the objects in the set had the value of an attribute A changed between the two time points, half did not.  Of the objects that were changed in each participant's set, half went from A' to A'' and half from A'' to A'. (i.e. all participants experienced both orders). My main hypothesis is that changing this attribute from A' to A'' leads on average to a higher rating, and this is indeed supported by the data.  I am also interested in determining whether the magnitude and perhaps direction of this effect depends on the A' rating.&lt;/p&gt;&#10;&#10;&lt;p&gt;For the purposes of this question, I am considering only those instances where A was changed (1280 pairs of obs).  The following GLMM&lt;/p&gt;&#10;&#10;&lt;p&gt;(A'' rating - A' rating) = participant + order + A' rating&lt;/p&gt;&#10;&#10;&lt;p&gt;where A' rating is a covariate and participant and order are categorical variables, leads to the conclusion that there is a significant positive correlation between A' rating and effect of changing to A'' and that this correlation is &amp;lt;1, such that objects with a low A' rating have their rating increased by changing to A'' but that objects with a high A' rating actually get rated lower when changed to A''.&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to test whether this is simply due to regression to the mean.  To this end, I have followed &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/16475086&quot; rel=&quot;nofollow&quot;&gt;Kelly and Price&lt;/a&gt; in using Pitman's test of equality of variances for paired samples and would appreciate some feedback on whether I've done the right thing. &lt;/p&gt;&#10;&#10;&lt;p&gt;This is what I did, following the suggestion of a colleague:&lt;/p&gt;&#10;&#10;&lt;p&gt;1) calculated the SD of A'' ratings $(SD_1)$ and the SD of A' ratings $(SD_2)$&lt;br&gt;&#10;2) regressed A'' rating on A' rating and recorded the correlation $r$.&lt;br&gt;&#10;3) calculated T as $T=\frac{\sqrt{(n-2)} [(SD_1/SD_2)-(SD_2/SD_1)]}{2 \sqrt{(1-r^2)}}$ &lt;/p&gt;&#10;&#10;&lt;p&gt;The 2-tailed p value of T (Student's t dist with 1280-2 DF) is 0.07, i.e. at alpha=0.05 there is no significant difference between the variances for the two sets of ratings and thus no effect of A' rating on rating difference beyond regression to the mean.  (We can argue about 2-t vs. 1-t p values later).&lt;/p&gt;&#10;&#10;&lt;p&gt;I now plan to adjust my difference scores to account for this and re-do the GLMM outlined above, as outlined by Kelly &amp;amp; Price.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you've got this far through the detail, then firstly well done, and secondly, can you tell me if 1) my colleague's suggestion was sensible and 2) if it was, have I implemented it correctly? I have a couple of concerns/apparent grey areas but I'd be interested to hear what others have to say first.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2010-09-09T15:04:28.853" FavoriteCount="4" Id="2513" LastActivityDate="2010-09-28T02:27:31.433" LastEditDate="2010-09-10T11:50:47.273" LastEditorUserId="266" OwnerUserId="266" PostTypeId="1" Score="6" Tags="&lt;regression&gt;&lt;standard-deviation&gt;&lt;variance&gt;" Title="Pitman's test of equality of variance and testing for regression to the mean: am I doing the right thing?" ViewCount="1592" />
  <row Body="&lt;p&gt;I am going to give a simple answer in response to your edit. If my answer does not meet your real needs please include it in the comments and I will change/complexify it a bit.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let:&lt;/p&gt;&#10;&#10;&lt;p&gt;$S$ be the severity of the event of interest with higher numbers representing more severity&lt;/p&gt;&#10;&#10;&lt;p&gt;$W$ be the warning generated by the system.&lt;/p&gt;&#10;&#10;&lt;p&gt;The range of values that these variables can take is assumed to be from 1 to $n$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Either via an experiment or via an observational study you have a set of observations about actual events and the warnings generated by the system. Thus, you can calculate the following empirical probabilities:&lt;/p&gt;&#10;&#10;&lt;p&gt;$P(W=w | S=s)$ which is the probability that the system generates warning $w$ when the event is $s$.&lt;/p&gt;&#10;&#10;&lt;p&gt;From a decision making perspective, you really want to know:&lt;/p&gt;&#10;&#10;&lt;p&gt;$P(S=s | W=w)$&lt;/p&gt;&#10;&#10;&lt;p&gt;In other words, you want to know the probability of different events occurring in the near future given that the system has given you a specific warning. &lt;/p&gt;&#10;&#10;&lt;p&gt;You can compute the required probabilities using &lt;a href=&quot;http://en.wikipedia.org/wiki/Bayes_theorem&quot; rel=&quot;nofollow&quot;&gt;Bayes theorem&lt;/a&gt;. Briefly, the calculation is:&lt;/p&gt;&#10;&#10;&lt;p&gt;$P(S=s | W=w) = \frac{P(W=w | S=s) P(S=s)}{\sum_s(P(W=w |S=s)P(S=s))}$&lt;/p&gt;&#10;&#10;&lt;p&gt;From your study you know:&lt;/p&gt;&#10;&#10;&lt;p&gt;$P(W=w | S=s)$ = Proportion of time the system generated warning $w$ when the event was $s$,&lt;/p&gt;&#10;&#10;&lt;p&gt;In order to compute the required probability, $P(S=s | W=w)$ you need to have some sense of $P(S=s)$ which you can estimate or guess based on previous experience.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can then assess the accuracy of the device by looking at the following probabilities: $P(S=s|W=s)$.&lt;/p&gt;&#10;" CommentCount="10" CreationDate="2010-09-10T16:30:12.310" Id="2539" LastActivityDate="2010-09-10T18:25:12.273" LastEditDate="2010-09-10T18:25:12.273" LastEditorDisplayName="user28" OwnerDisplayName="user28" ParentId="2483" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;IMO, it all depends on what you want to use your sample for.  Two &quot;silly&quot; examples to illustrate what I mean: If you need to estimate a mean, 30 observations is more than enough.  If you need to estimate a linear regression with 100 predictors, 30 observations will not be close to enough.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-09-10T18:41:42.803" Id="2544" LastActivityDate="2010-09-10T18:41:42.803" OwnerUserId="1204" ParentId="2541" PostTypeId="2" Score="5" />
  
  
  
  
  <row Body="&lt;p&gt;A histogram is pre-computer age estimate of a density.  A density estimate is an alternative. &lt;/p&gt;&#10;&#10;&lt;p&gt;These days we use both, and there is a rich literature about which defaults one should use.&lt;/p&gt;&#10;&#10;&lt;p&gt;A pdf, on the other hand, is a closed-form expression for a &lt;em&gt;given&lt;/em&gt; distribution. That is different from describing your dataset with an &lt;em&gt;estimated&lt;/em&gt; density or histogram.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2010-09-11T19:40:11.540" Id="2574" LastActivityDate="2010-09-11T19:40:11.540" OwnerUserId="334" ParentId="2573" PostTypeId="2" Score="9" />
  
  <row Body="&lt;p&gt;I am not sure if this is feasible in your context but one thing you can do to avoid these issues is to use bayesian estimation and to compute the expected values of different investing decisions based on the posterior distribution of the parameters. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-09-13T01:48:17.720" Id="2608" LastActivityDate="2010-09-13T01:48:17.720" OwnerDisplayName="user28" ParentId="2602" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Because the null hypothesis your McNemar test tests, is not the same as the one tested by the $\chi^2$ test. The McNemar test actually tests whether the probability of 1-2 equals that of 2-1 (say first number is the row and second the column). If you switch columns, then you get a completely different outcome. The $\chi^2$ test just tests whether the frequencies can be calculated from the marginal frequencies, which means both categorical variables are independent. &lt;/p&gt;&#10;&#10;&lt;p&gt;To illustrate in R :&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; x &amp;lt;- matrix(c(39,49,9,19),ncol=2)&#10;&#10;&amp;gt; y &amp;lt;-x[,2:1]&#10;&#10;&amp;gt; x&#10;     [,1] [,2]&#10;[1,]   39    9&#10;[2,]   49   19&#10;&#10;&amp;gt; y&#10;     [,1] [,2]&#10;[1,]    9   39&#10;[2,]   19   49&#10;&#10;&amp;gt; mcnemar.test(x)&#10;&#10;        McNemar's Chi-squared test with continuity correction&#10;&#10;data:  x &#10;McNemar's chi-squared = 26.2241, df = 1, p-value = 3.04e-07&#10;&#10;&#10;&amp;gt; mcnemar.test(y)&#10;&#10;        McNemar's Chi-squared test with continuity correction&#10;&#10;data:  y &#10;McNemar's chi-squared = 6.2241, df = 1, p-value = 0.01260&#10;&#10;&#10;&amp;gt; chisq.test(x)&#10;&#10;        Pearson's Chi-squared test with Yates' continuity correction&#10;&#10;data:  x &#10;X-squared = 0.8447, df = 1, p-value = 0.3581&#10;&#10;&#10;&amp;gt; chisq.test(y)&#10;&#10;        Pearson's Chi-squared test with Yates' continuity correction&#10;&#10;data:  y &#10;X-squared = 0.8447, df = 1, p-value = 0.3581&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;It is obvious the result of the McNemar test is completely different depending on which column comes first, whereas the $\chi^2$ test gives exactly the same outcome. Now why is this one non-significant? Well, take a look at the expected values :&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; m1 &amp;lt;- margin.table(x,1)/116&#10;&#10;&amp;gt; m2 &amp;lt;- margin.table(x,2)/116&#10;&#10;&amp;gt; outer(m1,m2)*116&#10;         [,1]     [,2]&#10;[1,] 36.41379 11.58621&#10;[2,] 51.58621 16.41379&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Pretty close to the table you have.&lt;/p&gt;&#10;&#10;&lt;p&gt;So both tests are not disagreeing at all. The $\chi^2$ rightfully concludes that both variables are independent, i.e. the counts in one variable are not influenced by the other and vice versa, and the McNemar test rightfully concludes that the probability of being first row-second column (0.07) is not the same as being second row-first column (0.42). &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-09-13T14:58:53.460" Id="2618" LastActivityDate="2010-09-13T14:58:53.460" OwnerUserId="1124" ParentId="2613" PostTypeId="2" Score="11" />
  
  
  <row Body="&lt;p&gt;@whuber gave a nice answer.  I would just add, that you can simulate this very easily in R:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;op &amp;lt;- par(mfrow = c(2,2), mar = .5 + c(0,0,0,0))&#10;&#10;N &amp;lt;- 500&#10;# Simulate a Gaussian noise process&#10;y1 &amp;lt;- rnorm(N)&#10;# Turn it into integrated noise (a random walk)&#10;y2 &amp;lt;- cumsum(y1)&#10;&#10;plot(ts(y1), xlab=&quot;&quot;, ylab=&quot;&quot;, main=&quot;&quot;, axes=F); box()&#10;plot(ts(y2), xlab=&quot;&quot;, ylab=&quot;&quot;, main=&quot;&quot;, axes=F); box()&#10;acf(y1, xlab=&quot;&quot;, ylab=&quot;&quot;, main=&quot;&quot;, axes=F); box()&#10;acf(y2, xlab=&quot;&quot;, ylab=&quot;&quot;, main=&quot;&quot;, axes=F); box()&#10;&#10;par(op)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Which ends up looking somewhat like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/5UZLJ.jpg&quot; alt=&quot;alt text&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;So you can easily see that the ACF function trails off slowly to zero in the case of a non-stationary series.  The rate of decline is some measure of the trend, as @whuber mentioned, although this isn't the best tool to use for that kind of analysis.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-09-13T17:55:10.243" Id="2627" LastActivityDate="2010-09-13T20:00:10.513" LastEditDate="2010-09-13T20:00:10.513" LastEditorUserId="5" OwnerUserId="5" ParentId="2623" PostTypeId="2" Score="10" />
  
  
  <row Body="&lt;p&gt;This may be one of the cases where there's more art than science to clustering.  I would suggest that you let your clustering algorithm run for a short time before letting the C-Index calculations kick in.  &quot;Short time&quot; may be after processing a few pairs, just when it starts to exceed 0, or some other heuristic.  (After all you don't expect to stop at 1 or 2 clusters, otherwise a different separation algorithm may have been deployed.)&lt;/p&gt;&#10;&#10;&lt;p&gt;For a book recommendation, I can suggest:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://books.google.com/books?id=htZzDGlCnQYC&quot; rel=&quot;nofollow&quot;&gt;Cluster Analysis&lt;/a&gt; by Brian Everitt, Sabine Landau, Morven Leese&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;You can scan/search the available contents on google books to see if it might meet your needs.  It's worked as a reference for me in the past.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-09-13T21:04:03.930" Id="2633" LastActivityDate="2010-09-13T21:04:03.930" OwnerUserId="251" ParentId="2631" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;Fun question.&lt;/p&gt;&#10;&#10;&lt;p&gt;Someone found out I work in biostatistics, and they asked me (basically) &quot;Isn't statistics just a way of lying?&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;(Which brings back the Mark Twain quote about Lies, Damn Lies, and Statistics.)&lt;/p&gt;&#10;&#10;&lt;p&gt;I tried to explain that statistics allows us to say with 100 percent precision that, given assumptions, and given data, that the probability of such-and-so was exactly such-and-such.&lt;/p&gt;&#10;&#10;&lt;p&gt;She wasn't impressed.&lt;/p&gt;&#10;" CommentCount="2" CommunityOwnedDate="2012-08-21T15:17:25.720" CreationDate="2010-09-14T01:02:43.727" Id="2638" LastActivityDate="2010-09-14T18:11:45.067" LastEditDate="2010-09-14T18:11:45.067" LastEditorUserId="1270" OwnerUserId="1270" ParentId="155" PostTypeId="2" Score="2" />
  
  
  
  
  
  <row Body="&lt;p&gt;Hmm, here goes for a completely non-mathematical take on PCA...&lt;/p&gt;&#10;&#10;&lt;p&gt;Imagine you have just opened a cider shop.  You have 50 varieties of cider and you want to work out how to allocate them onto shelves, so that similar-tasting ciders are put on the same shelf.  There are lots of different tastes and textures in cider - sweetness, tartness, bitterness, yeastiness, fruitiness, clarity, fizziness etc etc.  So what you need to do to put the bottles into categories is answer two questions:&lt;/p&gt;&#10;&#10;&lt;p&gt;1) What qualities are most important for identifying groups of ciders? e.g. does classifying based on sweetness make it easier to cluster your ciders into similar-tasting groups than classifying based on fruitiness?&lt;/p&gt;&#10;&#10;&lt;p&gt;2) Can we reduce our list of variables by combining some of them? e.g. is there actually a variable that is some combination of &quot;yeastiness and clarity and fizziness&quot; and which makes a really good scale for classifying varieties?&lt;/p&gt;&#10;&#10;&lt;p&gt;This is essentially what PCA does.  Principal components are variables that usefully explain variation in a data set - in this case, that usefully differentiate between groups.  Each principal component is one of your original explanatory variables, or a combination of some of your original explanatory variables. &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2010-09-15T21:14:37.067" Id="2697" LastActivityDate="2010-09-15T21:14:37.067" OwnerUserId="266" ParentId="2691" PostTypeId="2" Score="50" />
  <row Body="&lt;p&gt;Let's do (2) first.  PCA fits an ellipsoid to the data.  An ellipsoid is a multidimensional generalization of distorted spherical shapes like cigars, pancakes, and eggs.  These are all neatly described by the directions and lengths of their principal (semi-)axes, such as the axis of the cigar or egg or the plane of the pancake.  No matter how the ellipsoid is turned, the eigenvectors point in those principal directions and the eigenvalues give you the lengths.  The smallest eigenvalues correspond to the thinnest directions having the least variation, so ignoring them (which collapses them flat) loses relatively little information: that's PCA.&lt;/p&gt;&#10;&#10;&lt;p&gt;(1) Apart from simplification (above), we have needs for pithy description, visualization, and insight.  Being able to reduce dimensions is a &lt;em&gt;good&lt;/em&gt; thing: it makes it easier to describe the data and, if we're lucky to reduce them to three or less, lets us draw a picture.  Sometimes we can even find useful ways to interpret the combinations of data represented by the coordinates in the picture, which can afford insight into the joint behavior of the variables.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;The figure shows some clouds of $200$ points each, along with ellipsoids containing 50% of each cloud and axes aligned with the principal directions.  In the first row the clouds have essentially one principal component, comprising 95% of all the variance: these are the cigar shapes.  In the second row the clouds have essentially two principal components, one about twice the size of the other, together comprising 95% of all the variance: these are the pancake shapes.  In the third row all three principal components are sizable: these are the egg shapes.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/6X5NK.png&quot; alt=&quot;Figures&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Any 3D point cloud that is &quot;coherent&quot; in the sense of not exhibiting clusters or tendrils or outliers will look like one of these.  Any 3D point cloud &lt;em&gt;at all&lt;/em&gt;--provided not all the points are coincident--can be described by one of these figures &lt;em&gt;as an initial point of departure&lt;/em&gt; for identifying further clustering or patterning.&lt;/p&gt;&#10;&#10;&lt;p&gt;The intuition you develop from contemplating such configurations can be applied to higher dimensions, even though it is difficult or impossible to visualize those dimensions.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2010-09-15T21:33:12.360" Id="2698" LastActivityDate="2014-11-11T16:53:10.420" LastEditDate="2014-11-11T16:53:10.420" LastEditorUserId="919" OwnerUserId="919" ParentId="2691" PostTypeId="2" Score="59" />
  <row Body="&lt;p&gt;onestop is right, you're looking to do survival analysis. In general, you can use a nonparametric &lt;a href=&quot;http://en.wikipedia.org/wiki/Kaplan-Meier_estimator&quot; rel=&quot;nofollow&quot;&gt;Kaplan-Meier estimator&lt;/a&gt; to plot the survival curve and then derive the &quot;average time in service&quot;.  The area underneath the survival curve works if you don't have any censored data (i.e. subscribers who are still active).  Michael Berry had a nice and clear explanation of this in a blog post for a business scenario similar to yours:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://blog.data-miners.com/2010/06/why-is-area-under-survival-curve-equal.html&quot; rel=&quot;nofollow&quot;&gt;Why is the area under the survival curve equal to the average tenure?&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;In your case, where you have censored data, the median time (0.5 quantile) is available as the &quot;average time in service&quot; -- as noted by @onestop.&lt;/p&gt;&#10;&#10;&lt;p&gt;Harvey Motulsky's book has a nice discussion of this:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://www.graphpad.com/www/book/survive.htm&quot; rel=&quot;nofollow&quot;&gt;Survival Curves&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="4" CreationDate="2010-09-16T00:20:19.813" Id="2701" LastActivityDate="2010-09-16T16:23:10.463" LastEditDate="2010-09-16T16:23:10.463" LastEditorUserId="251" OwnerUserId="251" ParentId="2675" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;I view PCA as a geometric tool. If you are given a bunch of points in 3-space which are pretty much all on a straight line, and you want to figure out the equation of that line, you get it via PCA (take the first component). If you have a bunch of points in 3-space which are mostly planar, and want to discover the equation of that plane, do it via PCA (take the least significant component vector and that should be normal to the plane). &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-09-16T05:15:42.383" Id="2708" LastActivityDate="2010-09-16T05:15:42.383" OwnerUserId="795" ParentId="2691" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;To recap, you have a function $f(x,y)$ which is continuous, and pseudo-random uniform at integer $(x,y)$, but nearly normal over the plane. You wish to make $f$ pseudo-random uniform over the whole plane. Suppose you could somehow make $f$ parametrized by some parameter $\lambda$, say by using $\theta$ to twiddle the pseudo-random uniform number generated at the lattice points. Then if you took $g(x,y) = \sum_{i} f(x,y,\lambda_i)$ for a whole bunch of $\lambda_i$ values, $g(x,y)$ would look much more normal, via the Central Limit Theorem.  You could then invert the normal to get a uniform field. (I was inspired by Mike Dunlavey's comment on adding 12 uniforms to get a normal).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-09-16T05:33:58.557" Id="2709" LastActivityDate="2010-09-16T05:33:58.557" OwnerUserId="795" ParentId="2617" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;You could use generalized least squares, such as the gls() function from the nlme package, which allows you to specify a variance function using the weight argument.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-09-16T06:36:14.457" Id="2710" LastActivityDate="2010-09-16T06:36:14.457" OwnerUserId="187" ParentId="2688" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;One way to highlight clusters on your distance matrix is by way of &lt;a href=&quot;http://en.wikipedia.org/wiki/Multidimensional_scaling&quot;&gt;Multidimensional scaling&lt;/a&gt;. When projecting individuals (here what you call your nodes) in an 2D-space, it provides a comparable solution to PCA. This is unsupervised, so you won't be able to specify a priori the number of clusters, but I think it may help to quickly summarize a given distance or similarity matrix.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is what you would get with your data:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;tmp &amp;lt;- matrix(c(0,20,20,20,40,60,60,60,100,120,120,120,&#10;                20,0,20,20,60,80,80,80,120,140,140,140,&#10;                20,20,0,20,60,80,80,80,120,140,140,140,&#10;                20,20,20,0,60,80,80,80,120,140,140,140,&#10;                40,60,60,60,0,20,20,20,60,80,80,80,&#10;                60,80,80,80,20,0,20,20,40,60,60,60,&#10;                60,80,80,80,20,20,0,20,60,80,80,80,&#10;                60,80,80,80,20,20,20,0,60,80,80,80,&#10;                100,120,120,120,60,40,60,60,0,20,20,20,&#10;                120,140,140,140,80,60,80,80,20,0,20,20,&#10;                120,140,140,140,80,60,80,80,20,20,0,20,&#10;                120,140,140,140,80,60,80,80,20,20,20,0),&#10;              nr=12, dimnames=list(LETTERS[1:12], LETTERS[1:12]))&#10;d &amp;lt;- as.dist(tmp)&#10;mds.coor &amp;lt;- cmdscale(d)&#10;plot(mds.coor[,1], mds.coor[,2], type=&quot;n&quot;, xlab=&quot;&quot;, ylab=&quot;&quot;)&#10;text(jitter(mds.coor[,1]), jitter(mds.coor[,2]),&#10;     rownames(mds.coor), cex=0.8)&#10;abline(h=0,v=0,col=&quot;gray75&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/oA1vV.png&quot; alt=&quot;mds&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I added a small jittering on the x and y coordinates to allow distinguishing cases. Replace &lt;code&gt;tmp&lt;/code&gt; by &lt;code&gt;1-tmp&lt;/code&gt; if you'd prefer working with dissimilarities, but this yields essentially the same picture. However, here is the hierarchical clustering solution, with &lt;em&gt;single&lt;/em&gt; agglomeration criteria:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;plot(hclust(dist(1-tmp), method=&quot;single&quot;))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/mOKiI.png&quot; alt=&quot;hc&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;You might further refine the selection of clusters based on the dendrogram, or more robust methods, see e.g. this related question: &lt;a href=&quot;http://stats.stackexchange.com/questions/2597/what-stop-criteria-for-agglomerative-hierarchical-clustering-are-used-in-practice&quot;&gt;What stop-criteria for agglomerative hierarchical clustering are used in practice?&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-09-16T13:59:10.410" Id="2727" LastActivityDate="2010-09-16T13:59:10.410" OwnerUserId="930" ParentId="2717" PostTypeId="2" Score="13" />
  <row Body="&lt;p&gt;What about &lt;a href=&quot;http://bm2.genes.nig.ac.jp/RGM2/R_current/library/cluster/html/silhouette.html&quot; rel=&quot;nofollow&quot;&gt;silhouette&lt;/a&gt;? &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2010-09-16T15:43:10.977" Id="2729" LastActivityDate="2010-09-16T15:43:10.977" OwnerUserId="88" ParentId="2728" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;So, in addition to this paper, &lt;a href=&quot;http://mres.gmu.edu/pmwiki/uploads/Main/ancova.pdf&quot;&gt;Misunderstanding Analysis of Covariance&lt;/a&gt;, which enumerates common pitfalls when using ANCOVA, I would recommend starting with:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Frank Harrell's &lt;a href=&quot;http://biostat.mc.vanderbilt.edu/wiki/Main/FrankHarrell&quot;&gt;homepage&lt;/a&gt;, especially his handout on &lt;a href=&quot;http://biostat.mc.vanderbilt.edu/wiki/Main/RmS&quot;&gt;Regression Modeling Strategies&lt;/a&gt; and &lt;a href=&quot;http://biostat.mc.vanderbilt.edu/wiki/pub/Main/BioMod/notes.pdf&quot;&gt;Biostatistical Modeling&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;John Fox's &lt;a href=&quot;http://socserv.mcmaster.ca/jfox/&quot;&gt;homepage&lt;/a&gt; includes great material on &lt;a href=&quot;http://en.wikipedia.org/wiki/Linear_model&quot;&gt;Linear Model&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://cran.r-project.org/doc/contrib/Faraway-PRA.pdf&quot;&gt;Practical Regression and Anova using R&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;This is mostly R-oriented material, but I feel you might better catch the idea if you start playing a little bit with these models on toy examples or real datasets (and R is great for that).&lt;/p&gt;&#10;&#10;&lt;p&gt;As for a good book, I would recommend &lt;a href=&quot;http://eu.wiley.com/WileyCDA/WileyTitle/productCd-EHEP000137.html&quot;&gt;&lt;em&gt;Design and Analysis of Experiments&lt;/em&gt;&lt;/a&gt; by Montgomery (now in its 7th ed.); ANCOVA is described in chapter 15. &lt;a href=&quot;http://www.springer.com/statistics/statistical+theory+and+methods/book/978-0-387-95361-8&quot;&gt;&lt;em&gt;Plane Answers to Complex Questions&lt;/em&gt;&lt;/a&gt; by Christensen is an excellent book on the theory of linear model (ANCOVA in chapter 9); it assumes a good mathematical background. Any biostatistical textbook should cover both topics, but I like &lt;a href=&quot;http://www.pearsonhighered.com/educator/product/Biostatistical-Analysis/9780131008465.page&quot;&gt;&lt;em&gt;Biostatistical Analysis&lt;/em&gt;&lt;/a&gt; by Zar (ANCOVA in chapter 12), mainly because this was one of my first textbook.&lt;/p&gt;&#10;&#10;&lt;p&gt;And finally, H. Baayen's textbook is very complete, &lt;a href=&quot;http://magnuson.psy.uconn.edu/mirman/R/Baayen.pdf?bcsi_scan_D99544420D78AF92=0&amp;amp;bcsi_scan_filename=Baayen.pdf&quot;&gt;Practical Data Analysis for the Language Sciences with R&lt;/a&gt;. Although it focus on linguistic data, it includes a very comprehensive treatment of the Linear Model and mixed-effects models.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-09-16T16:06:13.050" Id="2732" LastActivityDate="2010-10-20T11:56:39.077" LastEditDate="2010-10-20T11:56:39.077" LastEditorUserId="930" OwnerUserId="930" ParentId="2730" PostTypeId="2" Score="15" />
  <row Body="&lt;p&gt;There are a bunch of helpful video tutorials on basic statistics &amp;amp; data mining with R and Weka at SentimentMining.net.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://sentimentmining.net/StatisticsVideos/&quot; rel=&quot;nofollow&quot;&gt;http://sentimentmining.net/StatisticsVideos/&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="1" CommunityOwnedDate="2010-09-16T16:27:02.927" CreationDate="2010-09-16T16:27:02.927" Id="2733" LastActivityDate="2010-09-16T16:27:02.927" OwnerUserId="653" ParentId="485" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="2747" AnswerCount="3" Body="&lt;p&gt;Several statistical packages, such as SAS, SPSS, and R, allow you to perform some kind of factor rotation following a PCA. &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Why is a rotation necessary after a PCA?&lt;/li&gt;&#10;&lt;li&gt;Why would you apply an oblique rotation after a PCA given that the aim of PCA is to produce orthogonal dimensions?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="1" CreationDate="2010-09-16T19:28:10.130" FavoriteCount="2" Id="2742" LastActivityDate="2010-09-17T06:40:34.453" LastEditDate="2010-09-17T06:40:34.453" LastEditorUserId="183" OwnerUserId="1154" PostTypeId="1" Score="7" Tags="&lt;pca&gt;&lt;factor-analysis&gt;" Title="On the use of oblique rotation after PCA" ViewCount="4918" />
  
  
  <row Body="&lt;p&gt;For data organization/management, ensure that when you generate new variables in the dataset (for example, calculating body mass index from height and weight), the original variables are never deleted. A non-destructive approach is best from a reproducibility perspective. You never know when you might mis-enter a command and subsequently need to redo your variable generation. Without the original variables, you will lose a lot of time!&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2010-09-16T22:36:18.300" CreationDate="2010-09-16T22:36:18.300" Id="2755" LastActivityDate="2010-09-16T22:36:18.300" OwnerUserId="561" ParentId="2715" PostTypeId="2" Score="8" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I am attempting to calculate Orwin's (1983) modification of Rosenthal's (1979) Fail-safe N for my meta-analysis of Odds Ratios. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, all the equations I am finding are using Cohen's d, which I cannot calculate (I don't have two groups). &lt;/p&gt;&#10;&#10;&lt;p&gt;I have STATA, SPSS, and MedCalc at my disposal.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-09-17T00:30:13.537" Id="2764" LastActivityDate="2010-09-17T20:18:34.320" LastEditDate="2010-09-17T20:18:34.320" LastEditorUserId="88" OwnerDisplayName="Jay" PostTypeId="1" Score="1" Tags="&lt;meta-analysis&gt;&lt;cohens-d&gt;&lt;odds-ratio&gt;" Title="Calculating Orwin's (1983) modified Fail-safe N in a meta-analysis with Odds Ratio as summary statistic?" ViewCount="1045" />
  
  
  
  
  <row Body="&lt;p&gt;I think a way in which you could analyze the two experiments together is by defining a &lt;a href=&quot;http://en.wikipedia.org/wiki/Hierarchical_linear_modeling&quot; rel=&quot;nofollow&quot;&gt;multilevel/hierarchical model&lt;/a&gt;. The individuals are nested within each experiment.&lt;/p&gt;&#10;&#10;&lt;p&gt;The standard for this approach is &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/052168689X&quot; rel=&quot;nofollow&quot;&gt;Gelman's book&lt;/a&gt; (I think).&lt;br&gt;&#10;The &lt;a href=&quot;http://www.sciencedirect.com/science?_ob=PublicationURL&amp;amp;_tockey=%23TOC%236896%232008%23999409995%23701947%23FLA%23&amp;amp;_cdi=6896&amp;amp;_pubType=J&amp;amp;view=c&amp;amp;_auth=y&amp;amp;_acct=C000054038&amp;amp;_version=1&amp;amp;_urlVersion=0&amp;amp;_userid=1634520&amp;amp;md5=f69a3f5826f12784ec9f11fa8c4210f0&quot; rel=&quot;nofollow&quot;&gt;Journal of Memory and Language had a special issue on analyzing data in 2008&lt;/a&gt; which covered hierarchical models and even some examples in &lt;code&gt;R&lt;/code&gt;.&lt;br&gt;&#10;Other's here can probably provide good web-resources as well.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-09-17T13:39:43.130" Id="2792" LastActivityDate="2010-09-17T13:50:02.507" LastEditDate="2010-09-17T13:50:02.507" LastEditorUserId="442" OwnerUserId="442" ParentId="2788" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;IMHO normality tests are absolutely useless for the following reasons:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;On small samples, there's a good chance that the true distribution of the population is substantially non-normal, but the normality test isn't powerful to pick it up.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;On large samples, things like the T-test and ANOVA are pretty robust to non-normality.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The whole idea of a normally distributed population is just a convenient mathematical approximation anyhow.  None of the quantities typically dealt with statistically could plausibly have distributions with a support of all real numbers.  For example, people can't have a negative height.  Something can't have negative mass or more mass than there is in the universe.  Therefore, it's safe to say that &lt;strong&gt;nothing&lt;/strong&gt; is &lt;strong&gt;exactly&lt;/strong&gt; normally distributed in the real world.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="4" CommunityOwnedDate="2010-09-18T02:32:42.093" CreationDate="2010-09-18T02:32:42.093" Id="2807" LastActivityDate="2010-09-18T02:32:42.093" OwnerUserId="1347" ParentId="2492" PostTypeId="2" Score="20" />
  <row Body="&lt;p&gt;It looks to me like &lt;a href=&quot;http://dx.doi.org/10.1007/s10940-007-9036-0&quot; rel=&quot;nofollow&quot;&gt;Growth Mixture Models&lt;/a&gt; might have potential to allow you to examine your error variance. (&lt;a href=&quot;http://statmodel2.com/download/kreutermuthen2006_34.pdf&quot; rel=&quot;nofollow&quot;&gt;PDF&lt;/a&gt; here). (I'm not sure what multiplicative heteroscedastic models are, but I will definitely have to check them out).&lt;/p&gt;&#10;&#10;&lt;p&gt;Latent group based trajectory models have become really popular lately in criminology. But many people simply take for granted that groups actually exist, and some &lt;a href=&quot;http://dx.doi.org/10.1111/j.1745-9125.2010.00185.x&quot; rel=&quot;nofollow&quot;&gt;astute research&lt;/a&gt; has pointed out that you will find groups even in random data. Also to note Nagin's group based modelling approach does not allow you to assess your error (and honestly I have never seen a model that would look anything like a discontinuity).&lt;/p&gt;&#10;&#10;&lt;p&gt;Although it would be difficult with 20 time points, for exploratory purposes creating simple heuristics to identify patterns could be helpful (e.g. always low or always high, coefficient of variation). I'm envisioning sparklines in a spreadsheet or parallel coordinates plots but I doubt they would be helpful (I honestly have not ever seen a parallel coordinate plot that is very enlightening).&lt;/p&gt;&#10;&#10;&lt;p&gt;Good luck&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2010-09-18T03:45:56.633" Id="2811" LastActivityDate="2010-09-18T03:45:56.633" OwnerUserId="1036" ParentId="2777" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;The proof should be rather simple. Let $C$ be your correlation matrix (it has all ones on the diagonal). You multiply each element on the off diagonal by $(1+k)$. This is equivalent to computing the matrix $\hat{C} = (1+k) C - k \operatorname{diag}(C) = (1+k)C - k I,$ where $\operatorname{diag}$ is the diagonal part of a matrix, which in the case of $C$ is $I$, the identity matrix.  It is well known that the eigenvalues of a matrix commute with a polynomial applied to the matrix, and we are computing a polynomial function of $C$, with polynomial function $f(x) = (1+k)x - k$. Thus if $\lambda$ is an eigenvalue of $C$, then $f(\lambda) = (1+k)\lambda - k$ is an eigenvalue of $\hat{C} = f(C)$. Setting $f(\lambda) \le 0$ for $\lambda$ an eigenvalue of $C$ gives the desired condition on $k$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Note that depending on whether $k$ is positive or negative, you will want to check either $f(\lambda_1)$ or $f(\lambda_n)$, where $\lambda_1, \lambda_n$ are the smallest and largest eigenvalue of $C$. Unless $C$ is the identity matrix, you will have $\lambda_1 \lt 1 \lt \lambda_n$. This is the case because the sum of the eigenvalues equals $n$, the size of the matrix $C$. &lt;/p&gt;&#10;&#10;&lt;p&gt;The commutivity of eigenvalues and polynomials is easy to check, actually. First if $x, \lambda$ are eigenvector, eigenvalue of matrix $A$, show that $x, c\lambda^j$ are eigenvector, eigenvalue of $c A^j$. This holds for integer $j$, even negative integers or zero.  Then show that if $x, \lambda_1$ are eigenvector, -value of $A$ and $x, \lambda_2$ are eigenvector, -value of $B$, then $x, \lambda_1 + \lambda_2$ are eigenvector, -value of $A + B$. From there, one can easily show that $x, c_0 + c_1 \lambda + c_2 \lambda^2 + \ldots c_n \lambda^n$ are eigenvector, -value of the matrix $c_0 I + c_1 A + c_2 A^2 + \ldots c_n A^n$. &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2010-09-18T05:03:55.393" Id="2814" LastActivityDate="2010-09-18T20:55:20.910" LastEditDate="2010-09-18T20:55:20.910" LastEditorUserId="795" OwnerUserId="795" ParentId="2615" PostTypeId="2" Score="0" />
  
  
  <row Body="&lt;p&gt;For histograms, a good rule of thumb for &lt;strong&gt;number of bins in a histogram&lt;/strong&gt;:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;&lt;strong&gt;square root of the number of data points&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2010-09-18T19:49:37.593" CreationDate="2010-09-18T19:49:37.593" Id="2827" LastActivityDate="2010-09-18T19:49:37.593" OwnerUserId="438" ParentId="2715" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;In regards to the leading questions, here are several options of how I would attempt to investigate if your suspicions are true;&lt;/p&gt;&#10;&#10;&lt;p&gt;1 - Conduct your own experiment. One of your conditions will be to mimic the leading questions in the prior surveys, the other condition will be a survey constructed with functionally similar questions and answers but without the leading question(s). Randomly allocate surveys, and differences in answer distributions can be attributable to the different questions (+ sampling error).&lt;/p&gt;&#10;&#10;&lt;p&gt;2 - Determine if any other surveys have functionally similar questions but are not leading, and look at the distribution of answers for those surveys. The only thing you have to worry about here is the differences in the sample characteristics between your unfair surveys and the fair surveys that could account for some of the observed differences in the answers.&lt;/p&gt;&#10;&#10;&lt;p&gt;3 - Identify constructs between questions in your unfair survey. If leading questions within that construct have low correlations with other fair items it could be taken as evidence that the question is not returning the information it should.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm sure their is some psychological/psychometrics literature on how to ask questions (or about other items such as &quot;environment&quot;). I'm sure it would do you some good to review some of that work.&lt;/p&gt;&#10;&#10;&lt;p&gt;Good luck&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2010-09-19T03:24:26.727" CreationDate="2010-09-19T03:24:26.727" Id="2841" LastActivityDate="2010-09-19T03:24:26.727" OwnerUserId="1036" ParentId="2823" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;I like &lt;a href=&quot;http://books.google.com/books?id=Qc8KWWtUokcC&amp;amp;lpg=PR1&amp;amp;dq=risk%20and%20asset%20allocation%20meucci&amp;amp;pg=PR1#v=onepage&amp;amp;q&amp;amp;f=false&quot; rel=&quot;nofollow&quot;&gt;Risk and Asset Allocation&lt;/a&gt; by A. Meucci. This book is a bit more advanced than Ruppert's book, but still very user-friendly.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-09-19T05:41:03.267" Id="2843" LastActivityDate="2010-09-19T05:41:03.267" OwnerUserId="795" ParentId="328" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="2847" AnswerCount="1" Body="&lt;p&gt;Example code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;(pc.cr &amp;lt;- princomp(USArrests))  # inappropriate&#10;summary(pc.cr)&#10;loadings(pc.cr)  ## note that blank entries are small but not zero&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I am getting different outputs from each, and I am not sure I understand what the difference is.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is the output:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; summary(pc.cr)&#10;Importance of components:&#10;                           Comp.1      Comp.2      Comp.3       Comp.4&#10;Standard deviation     82.8908472 14.06956001 6.424204055 2.4578367034&#10;Proportion of Variance  0.9655342  0.02781734 0.005799535 0.0008489079&#10;Cumulative Proportion   0.9655342  0.99335156 0.999151092 1.0000000000&#10;&#10;&#10;&amp;gt; loadings(pc.cr)  ## note that blank entries are small but not zero&#10;&#10;...&#10;&#10;               Comp.1 Comp.2 Comp.3 Comp.4&#10;SS loadings      1.00   1.00   1.00   1.00&#10;Proportion Var   0.25   0.25   0.25   0.25&#10;Cumulative Var   0.25   0.50   0.75   1.00&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;p.s: how can I get access to the table created by summary(pc.cr)?? (I can't seem to find it in str)&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-09-19T09:21:07.803" FavoriteCount="2" Id="2844" LastActivityDate="2012-11-29T10:02:24.947" LastEditDate="2010-09-19T10:52:02.287" LastEditorUserId="88" OwnerUserId="253" PostTypeId="1" Score="8" Tags="&lt;r&gt;&lt;pca&gt;" Title="What is the difference between summary and loadings for princomp?" ViewCount="2624" />
  
  <row Body="&lt;p&gt;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0824778812&quot; rel=&quot;nofollow&quot;&gt;Experimental Design in Biotechnology&lt;/a&gt; by Perry D. Haaland, ed Marcel Dekker. &lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2010-09-19T09:24:48.077" CreationDate="2010-09-19T09:24:48.077" Id="2845" LastActivityDate="2010-09-19T09:34:20.480" LastEditDate="2010-09-19T09:34:20.480" LastEditorDisplayName="Juan Riera" OwnerDisplayName="Juan Riera" ParentId="1815" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;It seems to me that the main function of PCP is to highlight homogeneous groups of individuals, or conversely (in the dual space, by analogy with PCA) specific patterns of association on different variables. It produces an effective graphical summary of a multivariate data set, when there are not too much variables. Variables are automatically scaled to a fixed range (typically, 0–1) which is equivalent to working with standardized variables (to prevent the influence of one variable onto the others due to scaling issue), but for very high-dimensional data set (# of variables &gt; 10), you definitely have to look at other displays, like &lt;a href=&quot;http://had.co.nz/ggplot/plot-templates.html&quot; rel=&quot;nofollow&quot;&gt;fluctuation plot&lt;/a&gt; or &lt;a href=&quot;http://en.wikipedia.org/wiki/Heat_map&quot; rel=&quot;nofollow&quot;&gt;heatmap&lt;/a&gt; as used in microarray studies.&lt;/p&gt;&#10;&#10;&lt;p&gt;It helps answering questions like: &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;are there any consistent pattern of individual scores that may be explained by specific class membership (e.g. gender difference)?&lt;/li&gt;&#10;&lt;li&gt;are there any systematic covariation between scores observed on two or more variables (e.g. low scores observed on variable $X_1$ is always associated to high scores on $X_2$)?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;In the following plot of the &lt;a href=&quot;http://en.wikipedia.org/wiki/Iris_flower_data_set&quot; rel=&quot;nofollow&quot;&gt;Iris data&lt;/a&gt;, it is clearly seen that species (here shown in different colors) show very discriminant profiles when considering petal length and width, or that &lt;em&gt;Iris setosa&lt;/em&gt; (blue) are more homogeneous with respect to their petal length (i.e. their variance is lower), for example.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/xKvQv.png&quot; alt=&quot;alt text&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;You can even use it as a backend to classification or dimension reduction techniques, like PCA. Most often, when performing a PCA, in addition to reducing the features space you also want to highlight clusters of individuals (e.g. are there individuals who systematically score higher on some combination of the variables); this is usually down by applying some kind of hierarchical clustering on the factor scores and highlighting the resulting cluster membership on the factorial space (see the &lt;a href=&quot;http://cran.r-project.org/web/packages/FactoClass/index.html&quot; rel=&quot;nofollow&quot;&gt;FactoClass&lt;/a&gt; R package). &lt;/p&gt;&#10;&#10;&lt;p&gt;It is also used in clustergrams (&lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.133.1405&amp;amp;rep=rep1&amp;amp;type=pdf&quot; rel=&quot;nofollow&quot;&gt;Visualizing non-hierarchical and hierarchical cluster analyses&lt;/a&gt;) which aims at examining how cluster allocation evolves when increasing the number of clusters (see also, &lt;a href=&quot;http://stats.stackexchange.com/questions/2597/what-stop-criteria-for-agglomerative-hierarchical-clustering-are-used-in-practice/2609#2609&quot;&gt;What stop-criteria for agglomerative hierarchical clustering are used in practice?&lt;/a&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;Such displays are also useful when linked to usual scatterplots (which by construction are restricted to 2D-relationships), this is called &lt;em&gt;brushing&lt;/em&gt; and it is available in the &lt;a href=&quot;http://www.ggobi.org/&quot; rel=&quot;nofollow&quot;&gt;GGobi&lt;/a&gt; data visualization system, or the &lt;a href=&quot;http://stats.math.uni-augsburg.de/Mondrian/&quot; rel=&quot;nofollow&quot;&gt;Mondrian&lt;/a&gt; software.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-09-19T12:57:26.100" Id="2850" LastActivityDate="2010-09-19T16:55:34.567" LastEditDate="2010-09-19T16:55:34.567" LastEditorUserId="930" OwnerUserId="930" ParentId="2846" PostTypeId="2" Score="5" />
  
  <row Body="&lt;p&gt;Almost all statistics implicitly condition on N. We treat N as a constant that can come out from the expression $\mathbb{E}\left[\frac{1}{N}\sum_{i=1}^{N}{x_i}\right]$, for example. For that to be appropriate, N has to be a fixed value, which we get by conditioning. Without conditioning on N, as you said, we'd need to know the distribution of N, which is kind of strange to think about (&quot;what are my chances of selecting each value of N for my study?&quot;). &lt;/p&gt;&#10;&#10;&lt;p&gt;It is important to assume that the $x_i$ are independent of N, however. This might be violated if, for example, a study has $100,000 to spend on its subjects and will spend it all no matter how many people are involved. If you are looking at the impact of this spending, then the spending-per-person declines as the number of people increases and N is correlated with your treatment (i.e., spending-per-person).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-09-19T16:41:11.350" Id="2855" LastActivityDate="2010-09-19T16:41:11.350" OwnerUserId="401" ParentId="2010" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;In regards to questions 3, 4, and 5 I would suggest you check out this work&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://dx.doi.org/10.1057/palgrave.ivs.9500166&quot; rel=&quot;nofollow&quot;&gt;Perceiving patterns in parallel coordinates: determining thresholds for identification of relationships&#10;by: Jimmy Johansson, Camilla Forsell, Mats Lind, Matthew Cooper in &#10;&lt;em&gt;Information Visualization, Vol. 7, No. 2.&lt;/em&gt; (2008), pp. 152-162.&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;To sum up their findings people are ok at identifying the direction of the slope of the relationship between each node, but aren't that good at identifying the strength of the relationship or the degree of the slope. They give suggested levels of noise in which people can still decipher the relationship in the article. Unfortunately the article does not discuss identifying subgroups via color like chl demonstrates.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-09-19T17:10:15.873" Id="2856" LastActivityDate="2010-09-19T17:28:17.197" LastEditDate="2010-09-19T17:28:17.197" LastEditorUserId="1036" OwnerUserId="1036" ParentId="2846" PostTypeId="2" Score="3" />
  
  
  <row AcceptedAnswerId="2887" AnswerCount="2" Body="&lt;p&gt;As title, I am thinking of merging both into &quot;missing data&quot;, which is to name it as NA in R. Since I don't see it will make much sense (or even any sense), to separate the &quot;don't know&quot; row out and to compare the information with other rows.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is it OK for me to do so? Thanks!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-09-20T06:44:20.220" FavoriteCount="3" Id="2886" LastActivityDate="2010-09-20T07:39:17.980" OwnerUserId="588" PostTypeId="1" Score="7" Tags="&lt;survey&gt;&lt;missing-data&gt;" Title="How will you deal with &quot;don't know&quot; and &quot;missing data&quot; in survey data?" ViewCount="3241" />
  <row AnswerCount="2" Body="&lt;p&gt;I am trying to estimate the mean of a more-or-less Gaussian distribution via sampling. I have no prior knowledge about its mean or its variance. Each sample is expensive to obtain. How do I dynamically decide how many samples I need to get a certain level of confidence/accuracy? Alternatively, how do I know when I can stop taking samples?&lt;/p&gt;&#10;&#10;&lt;p&gt;All the answers to questions like this that I can find seem to presume some knowledge of the variance, but I need to discover that along the way as well. Other are geared towards taking polls, and it's not clear to me (beginner that I am) how that generalizes -- my mean isn't w/in [0,1], etc.&lt;/p&gt;&#10;&#10;&lt;p&gt;I think this is probably a simple question with a well known answer, but my Google-fu is failing me. Even just telling me what to search for would be helpful.&lt;/p&gt;&#10;" CommentCount="8" CommunityOwnedDate="2010-09-20T13:25:02.783" CreationDate="2010-09-20T13:24:09.147" FavoriteCount="2" Id="2894" LastActivityDate="2010-09-20T15:51:22.327" LastEditDate="2010-09-20T13:29:12.717" LastEditorUserId="1376" OwnerUserId="1376" PostTypeId="1" Score="9" Tags="&lt;estimation&gt;&lt;sample-size&gt;" Title="Dynamic calculation of number of samples required to estimate the mean" ViewCount="278" />
  
  
  <row Body="&lt;p&gt;&lt;em&gt;I tend to hear that usually 3 largest eigenvalues are the most important, while those close to zero are noise&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;You can test for that. See the paper linked in &lt;a href=&quot;http://stats.stackexchange.com/questions/2860/pca-on-out-of-sample-data/2877#2877&quot;&gt;this&lt;/a&gt; post for more detail. Again if your dealing with financial times series you might wanna correct for leptokurticity first (i.e. consider the series of garch-adjusted returns, not the raw returns).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;I've seen a few research papers investigating how naturally occuring eigenvalue distributions differ from those calculated from random correlation matrices (again, distinguising noise from signal).&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Edward:&gt; Usually, one would do it the other way arround: look at the multivariate distribution of eigenvalues (of correlation matrices) coming from the application you want. Once you have identified a credible candidate for the distribution of eigenvalues, it should be fairly easy to generate from them. &lt;/p&gt;&#10;&#10;&lt;p&gt;The best procedure on how to identify the multivariate distribution of your eigenvalues depends on how many assets you want to consider simultaneously (i.e. what are the dimensions of your correlation matrix). There is a neat trick if $p\leq 10$ ($p$ being the number of assets). &lt;/p&gt;&#10;&#10;&lt;p&gt;Edit (comments by Shabbychef)&lt;/p&gt;&#10;&#10;&lt;p&gt;four step procedure:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Suppose you have $j=1,...,J$ sub samples of multivariate data. You need an estimator of the variance-covariance matrix $\tilde{C}_j$ for each sub-sample $j$ ( you could use the classical estimator or a robust alternative such as the &lt;a href=&quot;http://agoras.ua.ac.be/abstract/Fasalg99.htm&quot; rel=&quot;nofollow&quot;&gt;fast MCD&lt;/a&gt;, which is well implemented in matlab, SAS, S,R,...). As usual, if your dealing with financial times series you would want to consider the series of garch-adjusted returns, not raw returns.&lt;/li&gt;&#10;&lt;li&gt;For each sub sample $j$, compute $\tilde{\Lambda}_j=$  $\log(\tilde{\lambda}_1^j)$ ,..., $\log(\tilde{\lambda}_p^j)$ ,  the eigen values of $\tilde{C}_j$.&lt;/li&gt;&#10;&lt;li&gt;Compute $CV(\tilde{\Lambda})$, the convex hull of the $J \times p$ matrix whose j-th entry is $\tilde{\Lambda}_j$ (again, this is well implemented in Matlab, R,...).&lt;/li&gt;&#10;&lt;li&gt;Draw points at random from inside $CV(\tilde{\Lambda})$  (this done by giving weight $w_i$ to each of the edges of $CV(\tilde{\Lambda})$ where $w_i=\frac{\gamma_i}{\sum_{i=1}^{p}\gamma_i}$, where $\gamma_i$ is a draw from an unit exponential distribution (more details &lt;a href=&quot;http://geomblog.blogspot.com/2005/10/sampling-from-simplex.html3.&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;).&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;A limitation is that fast computation of the convex hull of a series of points becomes extremely slow when the number of dimensions is larger than 10. $J\geq2$&lt;/p&gt;&#10;" CommentCount="5" CommunityOwnedDate="2010-09-21T00:39:28.817" CreationDate="2010-09-20T18:49:08.317" Id="2905" LastActivityDate="2010-09-21T11:59:13.977" LastEditDate="2010-09-21T11:59:13.977" LastEditorUserId="603" OwnerUserId="603" ParentId="2892" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;This doesn't specifically provide an answer, but you may want to look at these related stackoverflow questions: &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://stackoverflow.com/questions/1429907/workflow-for-statistical-analysis-and-report-writing&quot;&gt;&quot;Workflow for statistical analysis and report writing&quot;&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://stackoverflow.com/questions/2284446/organizing-r-source-code&quot;&gt;&quot;Organizing R Source Code&quot;&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://stackoverflow.com/questions/1266279/how-to-organize-large-r-programs&quot;&gt;&quot;How to organize large R programs?&quot;&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://stackoverflow.com/questions/2712421/r-and-version-control-for-the-solo-data-analyst&quot;&gt;&quot;R and version control for the solo data analyst&quot;&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://stackoverflow.com/questions/2295389/how-does-software-development-compare-with-statistical-programming-analysis&quot;&gt;&quot;How does software development compare with statistical programming/analysis ?&quot;&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://stackoverflow.com/questions/2286831/how-do-you-combine-revision-control-with-workflow-for-r&quot;&gt;&quot;How do you combine “Revision Control” with “WorkFlow” for R?&quot;&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;You may also be interested in &lt;a href=&quot;http://www.johnmyleswhite.com/notebook/2010/09/19/why-use-projecttemplate-or-any-other-framework/&quot;&gt;John Myles White's recent project&lt;/a&gt; to create a statistical project template. &lt;/p&gt;&#10;" CommentCount="3" CommunityOwnedDate="2010-09-20T20:42:21.877" CreationDate="2010-09-20T20:42:21.877" Id="2911" LastActivityDate="2010-09-25T10:59:17.233" LastEditDate="2010-09-25T10:59:17.233" LastEditorUserId="930" OwnerUserId="5" ParentId="2910" PostTypeId="2" Score="17" />
  <row Body="&lt;p&gt;What computational savings?  The PCA computation is based on the covariance (or correlation) matrix, whose size depends on the number of variables, not the number of data points.  The calculation of a covariance matrix is fast.  Even if you were doing PCA repeatedly (as part of a simulation, for instance), reducing from 1000 data points to 500 wouldn't even reduce the time by 50%.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2010-09-20T21:29:31.903" Id="2913" LastActivityDate="2010-09-20T21:29:31.903" OwnerUserId="919" ParentId="2860" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Eigenvalues give magnitudes of principle components of data spread.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://yaroslavvb.com/upload/eigenvalues.png&quot;&gt;&#10; First dataset was generated from Gaussian with covariance matrix $\left(\matrix{3&amp;amp;0\\0&amp;amp;1}\right)$ second dataset is the first dataset rotated by $\pi/4$&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2010-09-21T00:56:34.300" CreationDate="2010-09-20T23:05:48.593" Id="2916" LastActivityDate="2010-09-20T23:05:48.593" OwnerUserId="511" ParentId="2892" PostTypeId="2" Score="4" />
  
  
  
  <row Body="&lt;p&gt;I can only agree with John. Furthermore, perhaps &lt;a href=&quot;http://eskes.psychiatry.dal.ca/Files/2003_-_Saville_-_Basic_statistics_and_the_inconsistency_of_m.pdf&quot;&gt;this&lt;/a&gt; paper by David Saville helps you with some formula to recalculate variability measures from LSDs et al.:&lt;br&gt;&#10;Saville D.J. (2003). &lt;a href=&quot;http://eskes.psychiatry.dal.ca/Files/2003_-_Saville_-_Basic_statistics_and_the_inconsistency_of_m.pdf&quot;&gt;Basic statistics and the inconsistency of multiple comparison procedures.&lt;/a&gt; Canadian Journal of Experimental Psychology, 57, 167–175&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;UPDATE:&lt;/strong&gt;&lt;br&gt;&#10;If you are looking for more formulas to convert between various effect sizes, books on meta-analysis should provide a lot of these. However, I am not an expert in this area and can't recommend one.&lt;br&gt;&#10;But, I remember that the book by Rosenthal and Rosnow once helped with some formula:&lt;br&gt;&#10;&lt;strong&gt;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0073531960&quot;&gt;Essentials of Behavioral Research: Methods and Data Analysis&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&#10;Furthermore, I have heard a lot of good things about the formulas in this book by Rosenthal, Rosnow &amp;amp; Rubin (although I have never used it):&lt;br&gt;&#10;&lt;strong&gt;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0521659809&quot;&gt;Contrasts and Effect Sizes in Behavioral Research: A Correlational Approach&lt;/a&gt;&lt;/strong&gt; (You should definitely give it a try if a nearby library has it).&lt;/p&gt;&#10;&#10;&lt;p&gt;If this is not enough, perhaps ask another question on literature for converting effect sizes for meta-analyses. Perhaps someone more into meta-analysis has more grounded recommendations.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-09-21T08:55:35.920" Id="2942" LastActivityDate="2010-09-23T16:56:50.490" LastEditDate="2010-09-23T16:56:50.490" LastEditorUserId="442" OwnerUserId="442" ParentId="2917" PostTypeId="2" Score="6" />
  
  
  <row Body="&lt;p&gt;I'd suggest that generalized eta square is considered (&lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/14664681&quot; rel=&quot;nofollow&quot;&gt;ref&lt;/a&gt;, &lt;a href=&quot;http://brm.psychonomic-journals.org/content/37/3/379.short&quot; rel=&quot;nofollow&quot;&gt;ref&lt;/a&gt;) a more appropriate measure of effect size. It is included in the ANOVA output in the &lt;a href=&quot;http://cran.r-project.org/package=ez&quot; rel=&quot;nofollow&quot;&gt;ez package&lt;/a&gt; for R.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-09-22T12:32:55.730" Id="2969" LastActivityDate="2010-09-22T12:32:55.730" OwnerUserId="364" ParentId="2962" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;Just a guess.&lt;/p&gt;&#10;&#10;&lt;p&gt;First I would explore transformations of the data, such as converting time to speed or acceleration. Then I would consider the log of that, since it obviously won't be negative.&lt;/p&gt;&#10;&#10;&lt;p&gt;Then, since you are interested in the asymptote, I would try fitting (by least squares) a simple exponential to the transformed data, with time t being the x axis, and log-transformed speed (or acceleration) being the y axis. See how that works in predicting new measurements as time increases.&lt;/p&gt;&#10;&#10;&lt;p&gt;A possible alternative to an an exponential function would be a Michaelis-Menten type of hyperbola.&lt;/p&gt;&#10;&#10;&lt;p&gt;Actually, I would strongly consider a mixed-effect population approach first (as with NONMEM), because each individual may not show enough information to evaluate different models.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you want to go Bayesian, you could use WinBugs, and provide any prior distribution you want to the parameters of the exponential function. The book I found useful is Gilks, Richardson, Spiegelhalter, &quot;Markov Chain Monte Carlo in Practice&quot;, Chapman &amp;amp; Hall, 1996.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-09-22T12:36:38.767" Id="2970" LastActivityDate="2010-09-22T13:28:08.330" LastEditDate="2010-09-22T13:28:08.330" LastEditorUserId="1270" OwnerUserId="1270" ParentId="2966" PostTypeId="2" Score="2" />
  
  
  
  <row Body="&lt;p&gt;Statistics is the pursuit of truth in the face of uncertainty. Probability is the tool that allows us to quantify uncertainty.&lt;/p&gt;&#10;&#10;&lt;p&gt;(I have provided another, longer, answer that assumed that what was being asked was something along the lines of &quot;how would you explain it to your grandmother?&quot;)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-09-22T14:31:43.833" Id="2973" LastActivityDate="2010-09-22T14:31:43.833" OwnerUserId="666" ParentId="665" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="2986" AnswerCount="1" Body="&lt;p&gt;With the help of several people in this community I have been wetting my feet in clustering some social network data using &lt;a href=&quot;http://igraph.sourceforge.net/doc/R/fastgreedy.community.html&quot;&gt;igraph's implementation of modularity-based clustering&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am having some trouble interpreting the output of this routine and how to use it to generate lists of members of each community detected. &lt;/p&gt;&#10;&#10;&lt;p&gt;This routine outputs a two-column matrix and a list of modularity values. From &lt;a href=&quot;http://igraph.sourceforge.net/doc/R/fastgreedy.community.html&quot;&gt;the docs&lt;/a&gt;:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;merges: A matrix with two column, this&#10;  represents a dendogram and contains&#10;  all the merges the algorithm&#10;  performed. Each line is one merge and&#10;  it is given by the ids of the two&#10;  communities merged. The community ids&#10;  are integer numbers starting from zero&#10;  and the communities between zero and&#10;  the number of vertices (N) minus one&#10;  belong to individual vertices. The&#10;  first line of the matrix gives the&#10;  first merge, this merge creates&#10;  community N, the number of vertices,&#10;  the second merge creates community&#10;  N+1, etc. &lt;/p&gt;&#10;  &#10;  &lt;p&gt;modularity:  A numeric vector&#10;  containing the modularity value of the&#10;  community structure after performing&#10;  every merge.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Working with this explanation and looking at the example at the bottom of the man page, I think the communities in this graph are&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;1st community: 0 1 2 3 4&#10;2nd community: 10 11 12 13 14&#10;3rd community: 5 6 7 8 9&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Can someone who has used this method before confirm whether the right approach produces this result? I have basically (i) ignored the last two merges and (ii) gone over each row in the 'merges' matrix, combining each pair of vertices into a set while watching out for vertex values that are larger than the number of vertices (and therefore refer to another row in the 'merges' matrix). &lt;/p&gt;&#10;&#10;&lt;p&gt;Many thanks!&#10;~l&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-09-22T18:49:28.433" Id="2981" LastActivityDate="2012-03-31T05:24:29.263" LastEditDate="2010-09-24T13:59:30.403" LastEditorUserId="930" OwnerUserId="1007" PostTypeId="1" Score="6" Tags="&lt;clustering&gt;&lt;networks&gt;&lt;data-visualization&gt;&lt;partitioning&gt;&lt;igraph&gt;" Title="Interpreting output of igraph's fastgreedy.community clustering method" ViewCount="1537" />
  <row AcceptedAnswerId="3002" AnswerCount="2" Body="&lt;p&gt;So let's say you have a distribution where X is the 16% quantile. Then you take the log of all the values of the distribution. Would log(X) still be the 16% quantile in the log distribution?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-09-23T03:57:03.107" FavoriteCount="1" Id="3001" LastActivityDate="2010-11-02T13:51:35.310" LastEditDate="2010-11-02T13:51:35.310" LastEditorUserId="8" OwnerUserId="1395" PostTypeId="1" Score="6" Tags="&lt;standard-deviation&gt;&lt;logarithm&gt;&lt;quantiles&gt;&lt;self-study&gt;" Title="Log graph question" ViewCount="294" />
  
  
  
  
  <row Body="&lt;p&gt;I usually find it easier and faster to run a simulation. Papers take a long time to read, to understand and finally come to the conclusion that they don't apply in the special case one is interested in.&lt;/p&gt;&#10;&#10;&lt;p&gt;Therefore, I would just pick a number of subjects, simulate the covariate you are interested in (distributed as you believe it will be), simulate good/bad outcomes based on the functional form you posit (threshold effects of the covariate? nonlinearity?) with the minimum (clinically) significant effect size you would like to detect, run the result through your analysis and see whether the effect is found at your alpha. Rerun this 10,000 times and look whether you found the effect in 80% of the simulations (or whatever other power you need). Adjust the number of subjects, repeat until you have a power you are happy with.&lt;/p&gt;&#10;&#10;&lt;p&gt;This has the advantage of being very general, so you are not confined to a specific functional form or a specific number or distribution of covariates. You can include dropouts, see chl's comment above, either at random or influenced by covariate or outcome. You basically code the analysis you are going to do on the final sample beforehand, which sometimes helps focus my thinking on the study design. And it is easily done in R (vectorize!).&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2010-09-23T08:16:37.353" Id="3008" LastActivityDate="2010-10-02T20:05:22.870" LastEditDate="2010-10-02T20:05:22.870" LastEditorUserId="1352" OwnerUserId="1352" ParentId="2988" PostTypeId="2" Score="6" />
  <row Body="Partitioning data into subsets of objects according to their mutual &quot;similarity,&quot; without using preexisting knowledge such as class labels. Clustered-standard-errors and/or cluster-samples should be tagged as such; do not use the &quot;clustering&quot; tag for them." CommentCount="0" CreationDate="2010-09-23T11:06:19.013" Id="3015" LastActivityDate="2013-03-07T18:29:50.807" LastEditDate="2013-03-07T18:29:50.807" LastEditorUserId="5739" OwnerUserId="88" PostTypeId="4" Score="0" />
  <row Body="Hypothesis testing assesses whether data support a given hypothesis rather than being an effect of random fluctuations or some other process described by an alternative hypothesis." CommentCount="0" CreationDate="2010-09-23T11:28:36.817" Id="3019" LastActivityDate="2012-04-23T01:21:39.673" LastEditDate="2012-04-23T01:21:39.673" LastEditorUserId="919" OwnerUserId="88" PostTypeId="4" Score="0" />
  
  
  <row AnswerCount="4" Body="&lt;p&gt;I just loaded a csv file in R. When I ran the &lt;code&gt;summary&lt;/code&gt; command for one of the columns, I got the following: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; Error: unexpected symbol in &quot;summary k_low&quot;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I'm pretty sure I know what the 'unexpected symbol' is: for observations in which we had no reliable data for the variable &lt;code&gt;k_low&lt;/code&gt;, we simply entered a period, or a &lt;code&gt;.&lt;/code&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a way to escape these observations in R? My boss uses STATA, which apparently escapes periods automatically, but I don't have STATA on my personal computer (and would prefer to use R in any case). &lt;/p&gt;&#10;&#10;&lt;p&gt;So basically: Is there a way to get R to bypass any 'symbols' and return a summary of only those observations for which numerical data was entered? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-09-24T02:56:56.010" Id="3031" LastActivityDate="2010-09-24T13:58:47.937" LastEditDate="2010-09-24T13:58:47.937" LastEditorUserId="930" OwnerUserId="1410" PostTypeId="1" Score="3" Tags="&lt;r&gt;" Title="How to escape symbolic value in R" ViewCount="1212" />
  
  
  <row AnswerCount="3" Body="&lt;p&gt;Does Lin's &lt;a href=&quot;http://en.wikipedia.org/wiki/Concordance_correlation_coefficient&quot; rel=&quot;nofollow&quot;&gt;concordance correlation coefficient&lt;/a&gt; assume that the 2 data sets have linear or monotonic tendencies? Or, can I measure the concordance between 2 data sets that have a sinusoidal tendency?&#10;Thanks&#10;NG&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2010-09-24T18:40:39.993" Id="3061" LastActivityDate="2013-07-11T14:08:54.833" LastEditDate="2010-11-24T17:41:40.837" LastEditorUserId="919" OwnerUserId="1417" PostTypeId="1" Score="4" Tags="&lt;correlation&gt;" Title="Does the concordance correlation coefficient make linearity or monotone assumptions?" ViewCount="619" />
  <row AnswerCount="1" Body="&lt;p&gt;Is there a statistical procedure that can measure the concordance between two sets of data that have, say, a sinusoidal tendency?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-09-24T19:09:28.577" Id="3062" LastActivityDate="2010-09-24T19:21:21.743" OwnerUserId="1417" PostTypeId="1" Score="1" Tags="&lt;correlation&gt;" Title="How do we find concordance between two sets of non-linear, non-monotonic data" ViewCount="270" />
  <row Body="&lt;p&gt;Lets assume mat_pages[] contains pages in the columns (which you want to cluster) and individuals in the rows. You can cluster pages based on individual data in Rby using the following command: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;  pc &amp;lt;- prcomp(x=mat_pages,center=TRUE,scale=TRUE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The loadings matrix is the matrix of eigenvectors of the SVD decomposition of&#10;the data. They give the relative weight of each PAGE in the calculation of scores.&#10;Loadings with larger absolute values have more influence in determining the score&#10;of the corresponding principle component.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, I should also point out the &lt;em&gt;short coming&lt;/em&gt; of using PCA to cluster pages. The reason for this is that loadings give larger weights to the PAGES with higher variation, regardless of whether this variation is actually because of the PAGE content or some other reason (may be technical or individual variation). The loadings do not necessarily reflect the true differences between groups, which (maybe) your main interest. BUT, this clustering truly reflects the differences in the group under the assumption that all the pages have same variance (I don't know if this is a valid assumption).&lt;/p&gt;&#10;&#10;&lt;p&gt;If you have a powerful computing facilities (which may be possible given your data size) - using hierarchical models may be a good idea. In R, it can be done using lme4 package.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;What do do after you have the scores?&lt;/p&gt;&#10;&#10;&lt;p&gt;This is a crude suggestion and the analysis depends greatly on how the data looks like. Also, I would guess this process would be highly infeasible to group the data of magnitude that you have. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;pc.col &amp;lt;- paste(&quot;page&quot;, 1:27000, sep=&quot;.&quot;)&#10;&#10;pdf(&quot;principle-components.pdf&quot;)&#10;plot(pc$x[,1:2]) ## Just look at the 1st two loadings (as you can see the groupings in a plane)&#10;dev.off()&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Hopefully, this can give you a picture of how the data is grouped into.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Warning:&lt;/strong&gt; this is not what I would recommend.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;My recommendation:&lt;/p&gt;&#10;&#10;&lt;p&gt;Problems like these arise frequently in genomics.In your case pages corresponds to genes and individuals corresponds to patients (basically individuals is has the same meaning as in genomics)&lt;/p&gt;&#10;&#10;&lt;p&gt;You want to cluster the pages based on data. &lt;/p&gt;&#10;&#10;&lt;p&gt;You can use a lot of clustering packages in R and have been pointed in other answers. A fundamental problem with packages is like hclust is how to determine the number of clusters. A few of my favorite ones are:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;pvclust (Gives you clusters and also gives a p-value for each cluster. Using the p-value you can determine the statistically significant clusters. &lt;em&gt;Problem&lt;/em&gt;: requires a lot of computational power and I am not sure if it will work with data of your size)&lt;/li&gt;&#10;&lt;li&gt;hopach (Gives you the estimated number of clusters, and the clusters)&lt;/li&gt;&#10;&lt;li&gt;there are other packages available in Bioconductor, please check them out in the task view.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;You can also use clustering algos like k-means etc. I am sure I saw a thread in this forum about clustering. The answers were very detailed. It was asked by Tal Galili if I remember correctly.&lt;/p&gt;&#10;" CommentCount="11" CreationDate="2010-09-25T00:23:28.127" Id="3066" LastActivityDate="2010-09-28T16:05:34.673" LastEditDate="2010-09-28T16:05:34.673" LastEditorUserId="1307" OwnerUserId="1307" ParentId="3048" PostTypeId="2" Score="7" />
  <row Body="&lt;p&gt;The &lt;a href=&quot;http://jeromyanglim.blogspot.com/2009/12/meta-analysis-tips-resources-and.html&quot; rel=&quot;nofollow&quot;&gt;meta analysis&lt;/a&gt; literature is relevant to your question. Using meta-analytic techniques you could generate an estimate of the effect of interest pooled across studies. Such techniques often weight studies in terms of their sample size.&lt;/p&gt;&#10;&#10;&lt;p&gt;Within the meta analysis context researchers talk about fixed effect and random effect models (see &lt;a href=&quot;http://onlinelibrary.wiley.com/doi/10.1111/1468-2389.00156/abstract&quot; rel=&quot;nofollow&quot;&gt;Hunter and Schmidt, 2002&lt;/a&gt;). A fixed effect model assumes that all studies are estimating the same population effect. A random-effects model assumes that studies differ in the population effect that is being estimated. A random-effects model is typically more appropriate. &lt;/p&gt;&#10;&#10;&lt;p&gt;As more studies accumulate looking at a particular relationship, more sophisticated approaches become possible. For example, you can code studies in terms of various properties, such as perceived quality, and then examine empirically whether the effect size varies with these study characteristics. Beyond quality there may be some theoretically relevant differences between the studies which would moderate the relationship (e.g., characteristic of the sample, dosage levels, etc.).&lt;/p&gt;&#10;&#10;&lt;p&gt;In general, I tend to trust studies with:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;bigger sample sizes&lt;/li&gt;&#10;&lt;li&gt;greater methodological rigour&lt;/li&gt;&#10;&lt;li&gt;a confirmatory orientation (e.g., not a study where they tested for correlations between 100 different nutrients and 50 health outcomes)&lt;/li&gt;&#10;&lt;li&gt;absence of conflict of interest (e.g., not by a company with a commercial interest in showing a relationship; not by a researcher who has an incentive to find a significant result)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;But that said you need to keep random sampling and theoretically meaningful differences between studies as a plausible explanation of conflicting study findings.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2010-09-26T06:18:11.867" Id="3081" LastActivityDate="2010-09-26T11:09:08.063" LastEditDate="2010-09-26T11:09:08.063" LastEditorUserId="183" OwnerUserId="183" ParentId="3080" PostTypeId="2" Score="8" />
  
  <row Body="&lt;p&gt;Some of the &lt;a href=&quot;http://www.bioconductor.org&quot; rel=&quot;nofollow&quot;&gt;Bioconductor&lt;/a&gt;'s packages (e.g., &lt;a href=&quot;http://www.bioconductor.org/packages/2.3/bioc/html/ShortRead.html&quot; rel=&quot;nofollow&quot;&gt;ShortRead&lt;/a&gt;, &lt;a href=&quot;http://www.bioconductor.org/packages/2.2/bioc/html/Biostrings.html&quot; rel=&quot;nofollow&quot;&gt;Biostrings&lt;/a&gt;, &lt;a href=&quot;http://www.bioconductor.org/packages/2.3/bioc/html/BSgenome.html&quot; rel=&quot;nofollow&quot;&gt;BSgenome&lt;/a&gt;, &lt;a href=&quot;http://www.bioconductor.org/packages/2.3/bioc/html/IRanges.html&quot; rel=&quot;nofollow&quot;&gt;IRanges&lt;/a&gt;, &lt;a href=&quot;http://www.bioconductor.org/packages/2.6/bioc/html/genomeIntervals.html&quot; rel=&quot;nofollow&quot;&gt;genomeIntervals&lt;/a&gt;) offer facilities for dealing with genome positions or coverage vectors, e.g. for &lt;a href=&quot;http://en.wikipedia.org/wiki/Chip-Sequencing&quot; rel=&quot;nofollow&quot;&gt;ChIP-seq&lt;/a&gt; and identifying enriched regions. As for the other answers, I agree that any method relying on ordered observations with some threshold-based filter would allow to isolate low signal within a specific bandwith.&lt;/p&gt;&#10;&#10;&lt;p&gt;Maybe you can also look at the methods used to identify so-called &quot;islands&quot;&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Zang, C, Schones, DE, Zeng, C, Cui, K,&#10;  Zhao, K, and Peng, W (2009). &lt;a href=&quot;http://bioinformatics.oxfordjournals.org/content/25/15/1952.full.pdf+html&quot; rel=&quot;nofollow&quot;&gt;A&#10;  clustering approach for identification&#10;  of enriched domains from histone&#10;  modification ChIP-Seq data&lt;/a&gt;.&#10;  Bioinformatics, &lt;em&gt;25(15)&lt;/em&gt;, 1952-1958.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="0" CreationDate="2010-09-26T20:15:09.507" Id="3094" LastActivityDate="2010-09-26T20:15:09.507" OwnerUserId="930" ParentId="3052" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;I like what chl has written.  Despite that, I am moved to discuss whether this situation necessarily requires a complicated model.  But first, let's begin with responses to some comments.&lt;/p&gt;&#10;&#10;&lt;p&gt;(1) You don't lose any degrees of freedom due to spatial correlation of forest cover.  This is an explanatory variable, not the one you're trying to model.  You might lose &quot;degrees of freedom&quot; when the &lt;em&gt;residuals&lt;/em&gt; of the &lt;em&gt;dependent&lt;/em&gt; variable exhibit spatial autocorrelation.  Such correlation is not necessarily a given even when a map of the dependent variable itself suggests strong spatial correlation.  The reason is that the correlation in the map likely derives from the correlation in the forest cover (and other spatially distributed covariates).  Remember, in such models you don't ask whether the &quot;samples are independent&quot;--it's usually obvious they are not--but whether any deviations in them from their modeled values are independent.&lt;/p&gt;&#10;&#10;&lt;p&gt;(2) Therefore, a conditional autoregressive model might not be necessary.  I think this modeling choice would be attractive only if you want to test a theory of &lt;em&gt;contagion&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now to answer the original question: yes, first run an ordinary logistic (or Poisson) model, because as a general principle it's good to try simple (but reasonable) models first.  &lt;em&gt;Provided its residuals do not exhibit strong spatial correlation,&lt;/em&gt; you are then ok with the results.  If there is evidence of correlation and evidence that it will appreciably affect your answers (coefficients, predictions, or whatever), consider using a generalized linear geostatistical model (GLM).  These are described in Diggle &amp;amp; Riberio, &lt;em&gt;Model-based Geostatistics&lt;/em&gt; (a relatively inexpensive and accessible text), which itself documents several R packages for making the estimates and some associated EDA tools (geoR and geoRglm).  The GLM approach lets you simultaneously fit your model and assess the degree of spatial autocorrelation.  The principal limitations I have found in these packages are (1) they don't handle anisotropy well--you can detect it but it's hard to incorporate it in a model--and (2) they don't have a provision for nested variograms, which somewhat limits your ability to model the spatial correlation.  Both of these are no problem for smallish datasets, because you need (typically) hundreds to thousands of observations, or more, to model correlation at this level of detail.&lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, a word about the &quot;population&quot; question.  I assume you are interested in more than a mere description of the data: you seek information about a possible association between disease and other observable factors.  Even when you have a comprehensive description of the data for a spatial region, it still does not act like a census, because the outcomes could have turned out otherwise.  Next year, with identical forest cover, there will be a slightly different pattern of disease.  In other regions of the country or the world and at other times, exactly the same combinations of explanatory variable values are likely to produce varying rates of disease.  Thus, you're modeling a &lt;em&gt;process&lt;/em&gt;, not a population.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-09-26T20:32:55.893" Id="3095" LastActivityDate="2010-09-26T20:32:55.893" OwnerUserId="919" ParentId="3091" PostTypeId="2" Score="3" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I previously asked how to estimate the latent potential of a runner who &lt;a href=&quot;http://stats.stackexchange.com/questions/2966/estimating-latent-performance-potential-based-on-a-sequence-of-observations&quot;&gt;ran the 100 metres each day for 200 days&lt;/a&gt;. Latent skill was defined as &quot;the latent time it would take the individual to run if they (a) applied maximal effort; and (b) had a reasonably good run for them (i.e., no major problems with the run; but still a typical run).&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;Now assume that I estimated latent skill for the 100 metres for each of the 200 days, but that I also had data on the same 200 days but this time on running the 400 metres. Obviously I could repeat whatever process I adopted for the 100 metres to form an estimate of latent skill for the 400 metres at each of the 200 time points. In both cases I would expect the time to complete the runs to generally get faster with practice, but that raw data would vary from day to day.&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to quantify the degree of consistency of the two curves. I don't really want to quantify the consistency of the observed data.&lt;/p&gt;&#10;&#10;&lt;p&gt;If it makes a difference, the two methods I were considering using for estimating the effect of time, were nonlinear regression and &lt;a href=&quot;http://en.wikipedia.org/wiki/Isotonic_regression&quot; rel=&quot;nofollow&quot;&gt;isotonic regression&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;My question:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Thus, what is a good way to quantify and calculate the consistency of the fitted curves for the 100 and 400 metres?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Initial thoughts:&lt;/strong&gt;&#10;I had a few initial thoughts:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;estimate the fitted values for both curves and correlate the fitted values&lt;/li&gt;&#10;&lt;li&gt;Use a parametric model like $\theta_1 \exp(-\theta_2t) + \theta_3 + \epsilon$ ($t$ is an index of day) and then quantify the degree to which constraining $\theta_2$ (the parameter that determines shape) to be equal across the 100 and 400 metres would lead to poorer fit.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="4" CreationDate="2010-09-27T06:24:37.780" Id="3100" LastActivityDate="2010-09-27T11:42:29.113" LastEditDate="2010-09-27T06:34:30.020" LastEditorUserId="183" OwnerUserId="183" PostTypeId="1" Score="3" Tags="&lt;regression&gt;&lt;curve-fitting&gt;&lt;nonlinear-regression&gt;" Title="Quantifying the degree of consistency of two fitted curves" ViewCount="277" />
  
  <row AnswerCount="3" Body="&lt;p&gt;I wanted to find out what kind of different usages of stochastic processes theory in EE &amp;amp; CS are out there. For example, I find these kinds of usages interesting:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;using stochastic signal as carrier that is modulated by the information signal for communication&lt;/li&gt;&#10;&lt;li&gt;using stochastic process analysis for improving photographed pictures after long exposition&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;It would be good if every example would be in separate answer, so it would be possible to up-vote those that are 'the best'. I would classify good example as one that:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;truly takes advantage of probability and statistics theory&lt;/li&gt;&#10;&lt;li&gt;has greater 'usefulness' then other examples&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CommunityOwnedDate="2010-09-27T12:01:56.200" CreationDate="2010-09-27T10:19:33.873" Id="3105" LastActivityDate="2010-09-28T01:25:32.580" OwnerDisplayName="user1429" PostTypeId="1" Score="7" Tags="&lt;stochastic-processes&gt;" Title="What are the examples for stochastic processes in Electrical Engineering and Computer Science?" ViewCount="1177" />
  
  
  <row AcceptedAnswerId="3124" AnswerCount="1" Body="&lt;p&gt;I have 2 classifiers with different scales. How can I adjust one classifier to the scale of the other without loss of quality? &lt;/p&gt;&#10;&#10;&lt;p&gt;On the scatter plot we have &lt;a href=&quot;http://i55.tinypic.com/2v0fm28.png&quot; rel=&quot;nofollow&quot;&gt;2 solutions plotted (x1, x2) against each other&lt;/a&gt;. I believe using linear combination as a link function is not appropriate in this particular case.&lt;/p&gt;&#10;&#10;&lt;p&gt;Help is much appreciated.&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2010-09-27T16:53:52.353" Id="3119" LastActivityDate="2010-09-27T19:46:54.843" LastEditDate="2010-09-27T19:42:06.567" LastEditorUserId="919" OwnerDisplayName="Bob" PostTypeId="1" Score="3" Tags="&lt;classification&gt;&lt;data-transformation&gt;&lt;scales&gt;&lt;eda&gt;" Title="How can I adjust classifier to the scale of the other" ViewCount="100" />
  <row Body="&lt;p&gt;More generally, mutual information is invariant under any smooth and uniquely invertible transformation of the variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;See &quot;Estimating mutual information&quot; by A Kraskov, H Stögbauer, P Grassberger - Physical Review E, 2004 [&lt;a href=&quot;http://arxiv.org/pdf/cond-mat/0305641&quot; rel=&quot;nofollow&quot;&gt;http://arxiv.org/pdf/cond-mat/0305641&lt;/a&gt;]&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-09-27T17:39:46.393" Id="3120" LastActivityDate="2010-09-27T17:39:46.393" OwnerUserId="439" ParentId="1357" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;I did a little simulation in R, which resulted in 98. If you go to 5% of the mean then you're down to 93 and if you go to 10% then it's 76.&#10;Similar results are obtained for the median.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Please, read whuber's comments to my answer&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The code I used:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;numbers &amp;lt;- 1:100&#10;numTrials &amp;lt;- 1000&#10;&#10;means &amp;lt;- matrix(nrow = numTrials, ncol = length(numbers))&#10;&#10;for (n in 1:length(numbers))&#10;    {&#10;    for (t in 1:numTrials)&#10;        {&#10;        vals &amp;lt;- sample(numbers, n)&#10;        means[t, n] &amp;lt;- mean(vals) # or median if you want the median &#10;        }&#10;    }&#10;&#10;realAvg &amp;lt;- mean(numbers)&#10;uplim &amp;lt;- realAvg + 0.05*realAvg&#10;downlim &amp;lt;- realAvg - 0.05*realAvg&#10;&#10;res &amp;lt;- unlist(apply(means, 2, function(x){&#10;       length(which(x&amp;gt;downlim &amp;amp; x&amp;lt;uplim))/length(x)}))&#10;plot(1:length(numbers), res*100, t=&quot;l&quot;, xlab=&quot;Numbers picked&quot;, &#10;       ylab=&quot;% trials with mean within limits&quot;)&#10;abline(h=95, col=&quot;red&quot;)&#10;&#10;print(which(res&amp;gt;=0.95))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/f6WSF.jpg&quot; alt=&quot;Numbers picked vs % good trials&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;PS: anyone would like to explain the &quot;sawtooth&quot; look of the graph?&lt;/p&gt;&#10;" CommentCount="10" CreationDate="2010-09-27T20:03:37.737" Id="3125" LastActivityDate="2010-09-28T21:11:22.577" LastEditDate="2010-09-28T21:11:22.577" LastEditorUserId="582" OwnerUserId="582" ParentId="3121" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;It sounds dodgy to me. Nonparametric methods almost always involve more degrees of freedom than parametric methods and so need more data. In your particular example, the Mann-Whitney test has lower power than the t-test and so more data are required for the same specified power and size.&lt;/p&gt;&#10;&#10;&lt;p&gt;A simple way to do sample size calculation for any method (non-parametric or otherwise) is to use a bootstrap approach.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-09-27T23:47:03.250" Id="3129" LastActivityDate="2010-09-27T23:47:03.250" OwnerUserId="159" ParentId="3113" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;A couple of points first:&lt;br&gt;&#10;1) The answer will be very large, so we are safe to assume the &lt;a href=&quot;http://en.wikipedia.org/wiki/Central_limit_theorem&quot; rel=&quot;nofollow&quot;&gt;Central Limit Theorem&lt;/a&gt; is in operation, even if the distribution is all 1s and 100s (i.e. Bernoulli).&lt;br&gt;&#10;2) We can use normal deviates (z-scores) rather than t-tests because the required N will be very large - this is a simplification.&lt;br&gt;&#10;3. If we are sampling without replacement the problem statement seems to have only one interpretation - the numbers 1,...,100 - a uniform discrete distribution. whuber and nico have already provided answers, so I will deal with sampling with replacement and an undefined probability mass function (PMF).  &lt;/p&gt;&#10;&#10;&lt;p&gt;Approach 1:&lt;br&gt;&#10;Assume a &lt;a href=&quot;http://en.wikipedia.org/wiki/Uniform_distribution_%28discrete%29&quot; rel=&quot;nofollow&quot;&gt;discrete uniform distribution&lt;/a&gt; and sampling with replacement. The mean is 50.5, the SD is sqrt((n^2 - 1)/12) ~ 29&lt;/p&gt;&#10;&#10;&lt;p&gt;We want a maximum error of 2% so delta = abs(est_mean - true_mean)/true_mean = 1.01&lt;/p&gt;&#10;&#10;&lt;p&gt;Assume a normal distribution, the standard error of the mean is:&#10;\begin{align}&#10;SEM &amp;amp;= SD/sqrt(n)\\&#10;sqrt(n) &amp;amp;= SD/SEM\\&#10;n &amp;amp;= (SD/SEM)^2\\&#10;n &amp;amp;= (SD/(delta/1.96))^2\\&#10;n &amp;amp;= (29/(1.01/1.96))^2\\&#10;n &amp;amp;\approx 3167\\&#10;\end{align}&lt;/p&gt;&#10;&#10;&lt;p&gt;Approach 2:&lt;br&gt;&#10;Assume the worst case.&#10;I'm pretty sure this is a 50:50 split in the &lt;a href=&quot;http://en.wikipedia.org/wiki/Probability_mass_function&quot; rel=&quot;nofollow&quot;&gt;pmf &lt;/a&gt; between 1 and 100. If so our mean is 50.5 and our &lt;a href=&quot;http://en.wikipedia.org/wiki/Standard_deviation#Discrete_random_variable&quot; rel=&quot;nofollow&quot;&gt;SD&lt;/a&gt; is sqrt(49.5^2)=49.5.&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{align}&#10;SEM &amp;amp;= SD/sqrt(n)\\&#10;sqrt(n) &amp;amp;= SD/SEM\\&#10;n &amp;amp;= (SD/SEM)^2\\&#10;n &amp;amp;= (SD/(delta/1.96))^2\\&#10;n &amp;amp;= (49.5/(1.01/1.96))^2\\&#10;n &amp;amp;\approx 9227\\&#10;\end{align}&lt;/p&gt;&#10;&#10;&lt;p&gt;I'll let someone else do the sample sizes for medians.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2010-09-28T07:19:03.973" Id="3134" LastActivityDate="2010-09-29T02:35:18.350" LastEditDate="2010-09-29T02:35:18.350" LastEditorUserId="521" OwnerUserId="521" ParentId="3121" PostTypeId="2" Score="0" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I am currently working on matlab and I am new to it. I would like to know how do i average the results from the folds (or otherwise combined) to produce a single estimation.&lt;/p&gt;&#10;&#10;&lt;p&gt;My dataset has 2 class labels +1 and -1. and k=5. I use the knnclassify() method in matlab in order to perform cross validation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Kindly help! &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-09-28T13:28:00.863" FavoriteCount="1" Id="3140" LastActivityDate="2011-01-04T19:11:00.380" LastEditDate="2010-09-28T13:30:42.950" LastEditorUserId="5" OwnerDisplayName="Aparna" PostTypeId="1" Score="2" Tags="&lt;cross-validation&gt;&lt;matlab&gt;" Title="Knnclassify() matlab" ViewCount="3524" />
  <row Body="&lt;p&gt;I am afraid that there is something you are missing. The documentation says, p is p-value (which is definitely a probability under the null, but the probability of observing extreme observations if the null holds) and h=1 means you are rejecting the null in favor of the alternative hypothesis.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is a good quote from Bert Gunter borrowed from R mailing list.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;If you are willing to trust p-values which are as low as yours, I have a bridge to sell.&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, going by the value of your mean, its better that you look at the data and see what distribution your data follows and check if the assumptions of 1 sample t-test hold. I have a feeling that probably you can't use ttest here. I say this because your p-values are unbelievably small. (I understand your means are large ... etc ..., but still ttest can't be applied without verifying assumptions etc.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks,&lt;/p&gt;&#10;&#10;&lt;p&gt;S.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-09-29T00:45:26.520" Id="3157" LastActivityDate="2010-09-29T00:45:26.520" OwnerUserId="1307" ParentId="3156" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Take a look at &lt;a href=&quot;http://mkweb.bcgsc.ca/circos/?&quot;&gt;Circos&lt;/a&gt;:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Circos is a software package for visualizing data and information. It visualizes data in a circular layout — this makes Circos ideal for exploring relationships between objects or positions.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;The flowing data blog also had a post on this that you might find interesting:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://flowingdata.com/2009/04/21/visual-representation-of-tabular-information-how-to-fix-the-uncommunicative-table/&quot;&gt;Visual Representation of Tabular Information – How to Fix the Uncommunicative Table&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="3" CreationDate="2010-09-29T05:45:42.893" Id="3159" LastActivityDate="2010-09-29T05:45:42.893" OwnerUserId="251" ParentId="3158" PostTypeId="2" Score="9" />
  
  <row AcceptedAnswerId="3174" AnswerCount="1" Body="&lt;p&gt;There are occasions where I would like to fit log-linear models where the independence assumption between observations is violated. It is the normal case that I have multiple observations from each subject. Is there a mixed-effects extension to log-linear models like there are for linear and generalized linear models? &lt;/p&gt;&#10;&#10;&lt;p&gt;I'm interested in the existence of such a thing in principle, but also its concrete existence in an R library.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-09-29T18:22:58.343" FavoriteCount="1" Id="3173" LastActivityDate="2010-12-22T16:13:40.510" OwnerUserId="287" PostTypeId="1" Score="4" Tags="&lt;categorical-data&gt;&lt;mixed-model&gt;&lt;log-linear&gt;" Title="Mixed effects log-linear models" ViewCount="770" />
  
  <row Body="&lt;p&gt;Since the odds ratio cannot be negative, it is restricted at the lower end, but not at the upper end, and so has a skew distribution. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-09-30T03:05:13.367" Id="3178" LastActivityDate="2010-09-30T03:05:13.367" OwnerDisplayName="Swetha" ParentId="1455" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I feel you are actually asking two questions:  1) what types of visualizations to use and 2) what R package can produce them.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the case of what type of graph to use, there are many, and it depends on your needs (e.g: types of variables - numeric, factor, geographic etc, and the type of connections you are interested to display):&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;If you have many numeric variables, you might want to use a scatter plot matrix (have a look &lt;a href=&quot;http://www.statmethods.net/graphs/scatterplot.html&quot;&gt;here&lt;/a&gt;)&lt;/li&gt;&#10;&lt;li&gt;If you have many factor variables, you might want to use a scatter plot matrix for factors (have a look &lt;a href=&quot;http://www.r-statistics.com/2010/04/correlation-scatter-plot-matrix-for-ordered-categorical-data/&quot;&gt;here&lt;/a&gt;)&lt;/li&gt;&#10;&lt;li&gt;You could also go with doing some &lt;a href=&quot;http://en.wikipedia.org/wiki/Parallel_coordinates&quot;&gt;Parallel coordinates&lt;/a&gt; there &lt;a href=&quot;http://stat.ethz.ch/R-manual/R-devel/library/MASS/html/parcoord.html&quot;&gt;are&lt;/a&gt; &lt;a href=&quot;http://rss.acs.unt.edu/Rdoc/library/rflowcyt/html/parallel.coordinates.html&quot;&gt;several&lt;/a&gt; &lt;a href=&quot;http://www.r-statistics.com/tag/parallel-coordinates/&quot;&gt;ways&lt;/a&gt; to do it in R.&lt;/li&gt;&#10;&lt;li&gt;For a wider range of graphical facilities in R, have a look at the &lt;a href=&quot;http://cran.r-project.org/web/views/Graphics.html&quot;&gt;graphics task view&lt;/a&gt;.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Now regarding how to do it.  One problem with many data points is time till the plot is created.  ggplot2, iplots, ggobi are not very good for too many data points (at least from my experience).  In which case you might want to focus on R base graphics facilities, or sample your data and on that to use all the other tools.  Or you can hope that the people developing iplots extreme (or &lt;a href=&quot;http://www.rforge.net/Acinonyx/&quot;&gt;Acinonyx&lt;/a&gt;) would get to an advance release stage.&lt;/p&gt;&#10;&#10;&lt;p&gt;Good luck,&lt;/p&gt;&#10;&#10;&lt;p&gt;Tal&lt;/p&gt;&#10;" CommentCount="4" CommunityOwnedDate="2010-09-30T14:04:56.293" CreationDate="2010-09-30T08:08:11.903" Id="3188" LastActivityDate="2010-09-30T08:08:11.903" OwnerUserId="253" ParentId="3179" PostTypeId="2" Score="5" />
  <row AcceptedAnswerId="3205" AnswerCount="2" Body="&lt;p&gt;Hello data analyst community. I have the following problem:&lt;/p&gt;&#10;&#10;&lt;p&gt;Given a set of n units and a timeline in days. A unit may be active at a certain day to a certain degree (in range from 0.0 to 1.0). A desirable outcome is that if a unit is active, it should be active for a series of consecutive days (or at maximum with one day break).&lt;/p&gt;&#10;&#10;&lt;p&gt;What I have, of course is the opposite :). Now I want to measure or even better visualize the activity frequencies to &quot;prove&quot; an image-affine person that not all units behave as desired. The brute-force-approach is to draw a line for each unit (along the timeline), colored according to the degree of activity, but since n &gt; 30, the graph is big, colorful and you see nothing at all. &lt;/p&gt;&#10;&#10;&lt;p&gt;I am afraid that I am searching in the wrong direction. Any ideas, suggestions ?&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT:&#10;I think I was not able to explain my goal: I do not want to visualize the activity of a singular unit, but getting an idea of the activity frequency of all units involved. In the far end, I will have two groups of units and want to see graphically whether one group performed better than the other (better according to property described above). My apologies for not stating this earlier (thanks to the contributions up to this point, I was able to see what I actually want to know). &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-09-30T11:16:25.453" FavoriteCount="2" Id="3193" LastActivityDate="2010-10-01T11:59:49.043" LastEditDate="2010-10-01T11:59:49.043" LastEditorUserId="264" OwnerUserId="264" PostTypeId="1" Score="6" Tags="&lt;time-series&gt;&lt;data-visualization&gt;" Title="Visualizing activity frequency" ViewCount="172" />
  <row Body="&lt;p&gt;How about creating small timelines for each unit, one on top of the other, sorted in order of most to least active? Think &lt;a href=&quot;http://en.wikipedia.org/wiki/Sparkline&quot; rel=&quot;nofollow&quot;&gt;sparklines&lt;/a&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;You could probably do something like highlight the inactive time as either a shaded portion of the chart, or a colored portion of the unit's timeline. &lt;/p&gt;&#10;&#10;&lt;p&gt;Since each unit would have a small plot, you'd be able to see an individual's activity at a given time. And sorting them by activity would show how poorly some units are performing, as the plots get flatter (and/or more full of your inactivity indicator) as you go down the graphic. &lt;/p&gt;&#10;&#10;&lt;p&gt;I don't have any great ideas on what software to create this with. You might be able to do it with Lattice in R. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-09-30T15:03:59.807" Id="3204" LastActivityDate="2010-09-30T15:03:59.807" OwnerUserId="298" ParentId="3193" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;You can do a seemingly unrelated regression and use an F test. Put your data in a form like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Out1 1 P11 P12 0  0   0&#10;Out2 0 0   0   1  P21 P22&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;so that the predictors for your first outcome have their values when that outcome is the y variable and 0 otherwise and vice-versa. So your y is a list of both outcomes. P11 and P12 are the two predictors for the first outcome and P21 and P22 are the two predictors for the second outcome. If sex, say, is a predictor for both outcomes, its use to predict outcome 1 should be in a separate variable/column when predicting outcome 2. This lets your regression have different slopes/impacts for sex for each outcome.&lt;/p&gt;&#10;&#10;&lt;p&gt;In this framework, you can use standard F testing procedures.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-09-30T16:04:05.910" Id="3208" LastActivityDate="2010-09-30T16:04:05.910" OwnerUserId="401" ParentId="3200" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;The &lt;code&gt;class()&lt;/code&gt; is used to define/identify what &quot;type&quot; an object is from the point of view of object-oriented programming in R. So for&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; x &amp;lt;- 1:3&#10;&amp;gt; class(x)&#10;[1] &quot;integer&quot;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;any generic function that has an &quot;integer&quot; method will be used.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;typeof()&lt;/code&gt; gives the &quot;type&quot; of object from R's point of view, whilst &lt;code&gt;mode()&lt;/code&gt; gives the &quot;type&quot; of object from the point of view of Becker, Chambers &amp;amp; Wilks (1988). The latter may be more compatible with other S implementations according to the &lt;a href=&quot;http://stat.ethz.ch/R-manual/R-devel/doc/manual/R-lang.html#Objects&quot;&gt;R Language Definition&lt;/a&gt; manual.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'd probably err on the side of using &lt;code&gt;typeof()&lt;/code&gt; in most cases unless it was for passing R objects to compiled code, where &lt;code&gt;storage.mode()&lt;/code&gt; will be useful.&lt;/p&gt;&#10;&#10;&lt;p&gt;This is usefully discussed in the R Language Definition as linked to above.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-09-30T16:32:06.770" Id="3213" LastActivityDate="2010-09-30T16:32:06.770" OwnerUserId="1390" ParentId="3212" PostTypeId="2" Score="16" />
  
  <row AcceptedAnswerId="3224" AnswerCount="2" Body="&lt;p&gt;I was wondering how to create a vector with equivalent spacing between its consecutive elements in R? In Matlab, I can do [start:step:end].&lt;/p&gt;&#10;&#10;&lt;p&gt;Also if I want to plot a function with analytical form, do I have to evaluate the function on some sample points in its domain and plot these pair of points? Is there a R function that can take the function form and a set in its domain as arguments, and plot the graph of the function on the set?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-10-01T00:16:07.093" FavoriteCount="1" Id="3221" LastActivityDate="2014-07-02T15:23:55.260" LastEditDate="2014-07-02T15:23:55.260" LastEditorUserId="196" OwnerUserId="1005" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;data-visualization&gt;" Title="How to create a vector with equivalent spacing between its consecutive elements in R?" ViewCount="123" />
  <row Body="&lt;p&gt;See &lt;a href=&quot;http://stuff.mit.edu/afs/sipb/project/r-project/arch/i386_rhel3/lib/R/library/base/html/seq.html&quot; rel=&quot;nofollow&quot;&gt;seq&lt;/a&gt; for sequence generation:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;seq(from, to, by)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;or &lt;code&gt;?seq&lt;/code&gt; for help.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-10-01T00:20:28.803" Id="3222" LastActivityDate="2010-10-01T00:20:28.803" OwnerUserId="251" ParentId="3221" PostTypeId="2" Score="3" />
  
  
  
  <row AcceptedAnswerId="3234" AnswerCount="1" Body="&lt;p&gt;My office is going to implement a bundle of infection control measures in hospital and see if it can effectively reduce the infection rate of some pathogen. The unit of measurement will be &quot;case per thousand patient bed days&quot;. We have chosen 4 wards for implementing the control measures for 12 months, and do the measurement monthly, but by looking at the current infection data, even thses wards are considered as having the highest count of infections, they are still considered as relatively clean, as they have several months with zero measurement in rates.&#10;I have created a regression model showing if they managed to reduce 50% of infection, and the betas in the model (with three variables inside), are not statistically significant. My colleagues are worried that with all these hard work, giving non-significant result will be very frustrating for the front line staff.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there any alernative outcome measure, or even alternative statistical methods, for time series of rare events? Thanks!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-10-01T03:25:35.153" FavoriteCount="2" Id="3232" LastActivityDate="2010-10-01T13:32:19.047" LastEditDate="2010-10-01T13:32:19.047" LastEditorUserId="88" OwnerUserId="588" PostTypeId="1" Score="5" Tags="&lt;time-series&gt;&lt;epidemiology&gt;&lt;monitoring&gt;" Title="Dependent variable selection for loglinear segmented regression in time-series analysis of rare events" ViewCount="173" />
  
  
  <row Body="&lt;p&gt;Clustering time series is done fairly commonly by population dynamacists, particularily those that study insects to understand trends in outbreak and collapse. Look for work on Gypsy moth, Spruce budoworm, mountain pine beetle and larch budmoth. &lt;/p&gt;&#10;&#10;&lt;p&gt;For the actual clustering you can choose whatever distance metric you like, each probably has it's own strengths and weeknesses relative to the kind of data being clustered, Kaufmann and Rousseeuw 1990. &lt;em&gt;Finding groups in data. An introduction to cluster analysis&lt;/em&gt; is a good place to start. Remember, the clustering method doesn't 'care' that you're using a time series, it only looks at the values measured at the same point of time.  If your two time series are not in enough synch over their lifespan they the won't (and perhaps shouldn't) cluster.&lt;/p&gt;&#10;&#10;&lt;p&gt;Where you will have problems is determining the number of clusters (families) to use after you've clusterd the time series. There are various ways of selecting a cut-off of informative clusters, but here the literature isn't that good. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-10-01T16:34:37.630" Id="3241" LastActivityDate="2010-10-01T16:34:37.630" OwnerUserId="1475" ParentId="3238" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;See Aaron Clauset's page:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://tuvalu.santafe.edu/~aaronc/powerlaws/&quot;&gt;Power-law Distributions in Empirical Data&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;which has links to code for fitting power laws (Matlab, R, Python, C++) as well as a paper by Clauset and Shalizi you should read first.  &lt;/p&gt;&#10;&#10;&lt;p&gt;You might want to read Clauset's and Shalizi's blogs posts on the paper first:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://cs.unm.edu/~aaron/blog/archives/2007/06/power_laws_and.htm&quot;&gt;Power laws and all that jazz&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://cscs.umich.edu/~crshalizi/weblog/491.html&quot;&gt;So You Think You Have a Power Law — Well Isn't That Special?&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="5" CreationDate="2010-10-01T18:22:17.717" Id="3246" LastActivityDate="2010-10-01T18:42:15.803" LastEditDate="2010-10-01T18:42:15.803" LastEditorUserId="251" OwnerUserId="251" ParentId="3242" PostTypeId="2" Score="17" />
  <row Body="&lt;p&gt;As an enthusiast user of R, bash, Python, asciidoc, (La)TeX, open source sofwtare or any un*x tools, I cannot provide an objective answer. Moreover, as I often argue against the use of MS Excel or spreadsheet of any kind (well, you see your data, or part of it, but what else?), I would not contribute positively to the debate. I'm not the only one, e.g.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://www.burns-stat.com/pages/Tutor/spreadsheet_addiction.html&quot;&gt;Spreadsheet Addiction&lt;/a&gt;, from P. Burns.    &lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://tolstoy.newcastle.edu.au/R/help/04/12/9427.html&quot;&gt;MS Excel’s precision and accuracy&lt;/a&gt;, a post on the 2004 R mailing-list     &lt;/li&gt;&#10;&lt;li&gt;L. Knusel, &lt;a href=&quot;http://www.sciencedirect.com/science?_ob=ArticleURL&amp;amp;_udi=B6V8V-3SX6MW1-7&amp;amp;_user=10&amp;amp;_coverDate=01%2F02%2F1998&amp;amp;_rdoc=1&amp;amp;_fmt=high&amp;amp;_orig=search&amp;amp;_origin=search&amp;amp;_sort=d&amp;amp;_docanchor=&amp;amp;view=c&amp;amp;_searchStrId=1482343457&amp;amp;_rerunOrigin=google&amp;amp;_acct=C000050221&amp;amp;_version=1&amp;amp;_urlVersion=0&amp;amp;_userid=10&amp;amp;md5=37c2574f11ec79e1450143a179e0f294&amp;amp;searchtype=a&quot;&gt;On the accuracy of statistical distributions in Microsoft Excel 97&lt;/a&gt;, Computational Statistics &amp;amp; Data Analysis, 26: 375–377, 1998. (&lt;a href=&quot;http://www.stat.uni-muenchen.de/~knuesel/elv/excelacc.pdf&quot;&gt;pdf&lt;/a&gt;)&lt;/li&gt;&#10;&lt;li&gt;B.D. McCullough &amp;amp; B. Wilson, &lt;a href=&quot;http://www.sciencedirect.com/science?_ob=ArticleURL&amp;amp;_udi=B6V8V-45M6FB7-1&amp;amp;_user=10&amp;amp;_coverDate=10%2F28%2F2002&amp;amp;_rdoc=1&amp;amp;_fmt=high&amp;amp;_orig=search&amp;amp;_origin=search&amp;amp;_sort=d&amp;amp;_docanchor=&amp;amp;view=c&amp;amp;_searchStrId=1482342476&amp;amp;_rerunOrigin=google&amp;amp;_acct=C000050221&amp;amp;_version=1&amp;amp;_urlVersion=0&amp;amp;_userid=10&amp;amp;md5=da5e1130403a5de62ec14ffa6a0f778e&amp;amp;searchtype=a&quot;&gt;On the accuracy of statistical procedures in Microsoft Excel&#10;2000 and Excel XP&lt;/a&gt;, &lt;em&gt;Computational Statistics &amp;amp; Data Analysis&lt;/em&gt;, 40: 713–721, 2002. &lt;/li&gt;&#10;&lt;li&gt;M. Altman, J. Gill &amp;amp; M.P. McDonald, &lt;em&gt;Numerical Issues in Statistical Computing for the Social Scientist&lt;/em&gt;, Wiley, 2004. [e.g., pp. 12–14]&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;A colleague of mine loose all his macros because of the lack of backward compatibility, etc. Another colleague tried to import genetics data (around 700 subjects genotyped on 800,000 markers, 120 Mo), just to &quot;look at them&quot;. Excel failed, Notepad gave up too... I am able to &quot;look at them&quot; with vi, and quickly reformat the data with some sed/awk or perl script. So I think there are different levels to consider when discussing about the usefulness of spreadsheets. Either you work on small data sets, and only want to apply elementary statistical stuff and maybe it's fine. Then, it's up to you to trust the results, or you can always ask for the source code, but maybe it would be simpler to do a quick test of all inline procedures with the &lt;a href=&quot;http://www.itl.nist.gov/div898/strd/&quot;&gt;NIST benchmark&lt;/a&gt;. I don't think it corresponds to a good way of doing statistics simply because this is not a &lt;em&gt;true&lt;/em&gt; statistical software (IMHO), although as an update of the aforementioned list, newer versions of MS Excel seems to have demonstrated improvements in its accuracy for statistical analyses, see Keeling and Pavur, &lt;a href=&quot;http://www.sciencedirect.com/science?_ob=ArticleURL&amp;amp;_udi=B6V8V-4JHMGWJ-1&amp;amp;_user=10&amp;amp;_coverDate=05%2F01%2F2007&amp;amp;_rdoc=1&amp;amp;_fmt=high&amp;amp;_orig=search&amp;amp;_origin=search&amp;amp;_sort=d&amp;amp;_docanchor=&amp;amp;view=c&amp;amp;_searchStrId=1488136142&amp;amp;_rerunOrigin=google&amp;amp;_acct=C000050221&amp;amp;_version=1&amp;amp;_urlVersion=0&amp;amp;_userid=10&amp;amp;md5=2babab0b51c03746d5d7a74d31f1498c&amp;amp;searchtype=a&quot;&gt;A comparative study of the reliability of nine statistical software packages&lt;/a&gt; (&lt;em&gt;CSDA&lt;/em&gt; 2007 51: 3811).&lt;/p&gt;&#10;&#10;&lt;p&gt;Still, about one paper out of 10 or 20 (in biomedicine, psychology, psychiatry) includes graphics made with Excel, sometimes without removing the gray background, the horizontal black line or the automatic legend (Andrew Gelman and Hadley Wickham are certainly as happy as me when seeing it). But more generally, it tend to be the most used &quot;software&quot; according to a &lt;a href=&quot;http://datafl.ws/120&quot;&gt;recent poll&lt;/a&gt; on FlowingData, which remind me of an old talk of Brian Ripley (who co-authored the MASS R package, and write an excellent book on pattern recognition, among others):&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Let's not kid ourselves: the most&#10;  widely used piece of software for&#10;  statistics is Excel (B. Ripley via Jan&#10;  De Leeuw), &lt;a href=&quot;http://bit.ly/dB5K6r&quot;&gt;http://bit.ly/dB5K6r&lt;/a&gt;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Now, if you feel it provides you with a quick and easier way to get your statistics done, why not? The problem is that there are still things that cannot be done (or at least, it's rather tricky) in such an environment. I think of bootstrap, permutation, multivariate exploratory data analysis, to name a few. Unless you are very proficient in VBA (which is neither a scripting nor a programming language), I am inclined to think that even minor operations on data are better handled under R (or Matlab, or Python, providing you get the right tool for dealing with e.g. so-called data.frame). Above all, I think Excel does not promote very good practices for the data analyst (but it also applies to any &quot;cliquodrome&quot;, see the discussion on Medstats about the need to maintain a record of data processing, &lt;a href=&quot;http://groups.google.com/group/medstats/browse_thread/thread/601793e6ce36e789&quot;&gt;Documenting analyses and data edits&lt;/a&gt;), and I found this post on &lt;a href=&quot;http://www.practicalstats.com/xlsstats/excelstats.html&quot;&gt;Practical Stats&lt;/a&gt; relatively illustrative of some of Excel pitfalls. Still, it applies to Excel, I don't know how it translates to GDocs.&lt;/p&gt;&#10;&#10;&lt;p&gt;About sharing your work, I tend to think that &lt;a href=&quot;http://github.com/&quot;&gt;Github&lt;/a&gt; (or &lt;a href=&quot;http://gist.github.com/&quot;&gt;Gist&lt;/a&gt; for source code) or &lt;a href=&quot;http://www.dropbox.com&quot;&gt;Dropbox&lt;/a&gt; (although EULA might discourage some people) are very good options (revision history, grant management if needed, etc.). I cannot encourage the use of a software which basically store your data in a binary format. I know it can be imported in R, Matlab, Stata, SPSS, but to my opinion:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;data should definitively be in a text format, that can be read by another statistical software;&lt;/li&gt;&#10;&lt;li&gt;analysis should be reproducible, meaning you should provide a complete script for your analysis and it should run (we approach the ideal case near here...) on another operating system at any time;&lt;/li&gt;&#10;&lt;li&gt;your own statistical software should implement acknowledged algorithms and there should be an easy way to update it to reflect current best practices in statistical modeling;&lt;/li&gt;&#10;&lt;li&gt;the sharing system you choose should include versioning and collaborative facilities.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;That's it.&lt;/p&gt;&#10;" CommentCount="6" CommunityOwnedDate="2010-10-01T18:31:56.090" CreationDate="2010-10-01T18:31:56.090" Id="3247" LastActivityDate="2010-10-07T07:32:26.627" LastEditDate="2010-10-07T07:32:26.627" LastEditorUserId="930" OwnerUserId="930" ParentId="3244" PostTypeId="2" Score="18" />
  <row AcceptedAnswerId="19865" AnswerCount="2" Body="&lt;p&gt;This question is about estimating cut-off scores on a multi-dimensional screening questionnaire to predict a binary endpoint, in the presence of correlated scales. &lt;/p&gt;&#10;&#10;&lt;p&gt;I was asked about the interest of controlling for associated subscores when devising cut-off scores on each dimension of a measurement scale (personality traits) which might be used for alcoholism screening. That is, in this particular case, the person was not interested in adjusting on external covariates (predictors) -- which leads to (partial) area under covariate-adjusted ROC curve, e.g. (1-2) -- but essentially on other scores from the same questionnaire because they correlate one to each other (e.g. &quot;impulsivity&quot; with &quot;sensation seeking&quot;). It amounts to build an GLM which includes on the left-side the score of interest (for which we seek a cut-off) and another score computed from the same questionnaire, while on the right-hand side the outcome may be drinking status. &lt;/p&gt;&#10;&#10;&lt;p&gt;To clarify (per @robin request), suppose we have $j=4$ scores, say $x_j$ (e.g., anxiety, impulsivity, neuroticism, sensation seeking), and we want to find a cut-off value $t_j$ (i.e. &quot;positive case&quot; if $x_j&gt;t_j$, &quot;negative case&quot; otherwise) for each of them. We usually adjust for other risk factors like gender or age when devising such cut-off (using ROC curve analysis). Now, what about adjusting impulsivity (IMP) on gender, age, and sensation seeking (SS) since SS is known to correlate with IMP? In other words, we would have a cut-off value for IMP where effect of age, gender and anxiety level are removed. &lt;/p&gt;&#10;&#10;&lt;p&gt;Apart from saying that a cut-off must remain as simple as possible, my response was&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;About covariates, I would recommend&#10;  estimating the AUCs with and without&#10;  adjustment, just to see if the&#10;  predictive performance increase. Here,&#10;  your covariates are merely other&#10;  subscores defined from the same&#10;  measurement instrument and I never&#10;  faced such a situation (usually, I&#10;  adjust on known risk factors, like Age&#10;  or Gender). [...] Also, since you are&#10;  interested in prognostic issues (i.e.&#10;  screening efficacy of the questionnaire), you&#10;  may also be interested in estimating&#10;  the positive predictive value (PPV,&#10;  probability of patients with positive&#10;  test results who are correctly&#10;  classified) provided you are able to&#10;  classify subjects as &quot;positive&quot; or&#10;  &quot;negative&quot; depending on their&#10;  subscores on your questionnaire. Note, however,&#10;  that it is necessary to know the&#10;  prevalence of this disorder to&#10;  correctly interpret the PPV in turn...&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Do you have a more thorough understanding of this particular situation, with link to relevant papers when possible?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Janes, H and Pepe, MS (2008). &lt;a href=&quot;http://aje.oxfordjournals.org/content/168/1/89.full&quot;&gt;Adjusting for Covariates in Studies of Diagnostic, Screening, or Prognostic Markers: An Old Concept in a New Setting&lt;/a&gt;. &lt;em&gt;American Journal of Epidemiology&lt;/em&gt;, 168(1): 89-97.&lt;/li&gt;&#10;&lt;li&gt;Janes, H and Pepe, MS (2008). &lt;a href=&quot;http://www.bepress.com/uwbiostat/paper322/&quot;&gt;Accommodating Covariates in ROC Analysis&lt;/a&gt;. &lt;em&gt;UW Biostatistics Working Paper Series&lt;/em&gt;, Paper 322.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="4" CreationDate="2010-10-02T09:29:19.373" FavoriteCount="4" Id="3259" LastActivityDate="2011-12-15T14:11:44.723" LastEditDate="2010-11-09T13:22:04.150" LastEditorUserId="930" OwnerUserId="930" PostTypeId="1" Score="16" Tags="&lt;epidemiology&gt;&lt;roc&gt;" Title="Adjusting for covariates in ROC curve analysis" ViewCount="1622" />
  
  
  
  <row Body="&lt;p&gt;First, there are different ways to construct so-called &lt;a href=&quot;http://en.wikipedia.org/wiki/Biplot&quot;&gt;biplots&lt;/a&gt; in the case of correspondence analysis. In all cases, the basic idea is to find a way to show the best 2D approximation of the &quot;distances&quot; between row cells and column cells. In other words, we seek a hierarchy (we also speak of &quot;ordination&quot;) of the relationships between rows and columns of a contingency table.&lt;/p&gt;&#10;&#10;&lt;p&gt;Very briefly, CA decomposes the chi-square statistic associated with the two-way table into orthogonal factors that maximize the separation between row and column scores (i.e. the frequencies computed from the table of profiles). Here, you see that there is some connection with PCA but the measure of variance (or the metric) retained in CA is the $\chi^2$, which only depends on column profiles (As it tends to give more importance to modalities that have large marginal values, we can also re-weight the initial data, but this is another story).&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is a more detailed answer.&#10;The implementation that is proposed in the &lt;code&gt;corresp()&lt;/code&gt; function (in &lt;code&gt;MASS&lt;/code&gt;) follows from a view of CA as an SVD decomposition of dummy coded matrices representing the rows and columns (such that $R^tC=N$, with $N$ the total sample). This is in light with canonical correlation analysis. &#10;In contrast, the French school of data analysis considers CA as a variant of the PCA, where you seek the directions that maximize the &quot;inertia&quot; in the data cloud. This is done by diagonalizing the inertia matrix computed from the centered and scaled (by marginals frequencies) two-way table, and expressing row and column profiles in this new coordinate system. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you consider a table with $i=1,\dots,I$ rows, and $j=1,\dots,J$ columns, each row is weighted by its corresponding marginal sum which yields a series of conditional frequencies associated to each row: $f_{j|i}=n_{ij}/n_{i\cdot}$. The marginal column is called the &lt;em&gt;mean profile&lt;/em&gt; (for rows). This gives us a vector of coordinates, also called a &lt;em&gt;profile&lt;/em&gt; (by row). For the column, we have $f_{i|j}=n_{ij}/n_{\cdot j}$. In both cases, we will consider the $I$ row profiles (associated to their weight $f_{i\cdot}$) as individuals in the column space, and the $J$ column profiles (associated to their weight $f_{\cdot j}$) as individuals in the row space. The metric used to compute the proximity between any two individuals is the $\chi^2$ distance. For instance, between two rows $i$ and $i'$, we have&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;d^2_{\chi^2}(i,i')=\sum_{j=1}^J\frac{n}{n_{\cdot j}}\left(\frac{n_{ij}}{n_{i\cdot}}-\frac{n_{i'j}}{n_{i'\cdot}} \right)^2&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;You may also see the link with the $\chi^2$ statistic by noting that it is simply the distance between observed and expected counts, where expected counts (under $H_0$, independence of the two variables) are computed as $n_{i\cdot}\times n_{\cdot j}/n$ for each cell $(i,j)$. If the two variables were to be independent, the row profiles would be all equal, and identical to the corresponding marginal profile. In other words, when there is independence, your contingency table is entirely determined by its margins.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you realize an PCA on the row profiles (viewed as individuals), replacing the euclidean distance by the $\chi^2$ distance, then you get your CA. The first principal axis is the line that is the closest to all points, and the corresponding eigenvalue is the inertia explained by this dimension. You can do the same with the column profiles. It can be shown that there is a symmetry between the two approaches, and more specifically that the principal components (PC) for the column profiles are associated to the same eigenvalues than the PCs for the row profiles. What is shown on a biplot is the coordinates of the individuals in this new coordinate system, although the individuals are represented in a separate factorial space. Provided each individual/modality is well represented in its factorial space (you can look at the $\cos^2$ of the modality with the 1st principal axis, which is a measure of the correlation/association), you can even interpret the proximity between elements $i$ and $j$ of your contingency table (as can be done by looking at the residuals of your $\chi^2$ test of independence, e.g. &lt;code&gt;chisq.test(tab)$expected-chisq.test(tab)$observed&lt;/code&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;The total inertia of your CA (= the sum of eigenvalues) is the $\chi^2$ statistic divided by $n$ (which is Pearson's $\phi^2$).&lt;/p&gt;&#10;&#10;&lt;p&gt;Actually, there are several packages that may provide you with enhanced CAs compared to the function available in the &lt;code&gt;MASS&lt;/code&gt; package: &lt;a href=&quot;http://cran.r-project.org/web/packages/ade4/index.html&quot;&gt;ade4&lt;/a&gt;, &lt;a href=&quot;http://cran.r-project.org/web/packages/FactoMineR/index.html&quot;&gt;FactoMineR&lt;/a&gt;, &lt;a href=&quot;http://cran.r-project.org/web/packages/anacor/index.html&quot;&gt;anacor&lt;/a&gt;, and &lt;a href=&quot;http://cran.r-project.org/web/packages/ca/index.html&quot;&gt;ca&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;The latest is the one that was used for your particular illustration, and a paper was published in the Journal of Statistical Software that explains most of its functionnalities: &lt;a href=&quot;http://www.jstatsoft.org/v20/i03&quot;&gt;Correspondence Analysis in R, with Two- and Three-dimensional Graphics: The ca Package&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, your example on eye/hair colors can be reproduced in many ways:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;data(HairEyeColor)&#10;tab &amp;lt;- apply(HairEyeColor, c(1, 2), sum) # aggregate on gender&#10;tab&#10;&#10;library(MASS)&#10;plot(corresp(tab, nf=2))&#10;corresp(tab, nf=2)&#10;&#10;library(ca)&#10;plot(ca(tab))&#10;summary(ca(tab, nd=2))&#10;&#10;library(FactoMineR)&#10;CA(tab)&#10;CA(tab, graph=FALSE)$eig  # == summary(ca(tab))$scree[,&quot;values&quot;]&#10;CA(tab, graph=FALSE)$row$contrib&#10;&#10;library(ade4)&#10;scatter(dudi.coa(tab, scannf=FALSE, nf=2))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In all cases, what we read in the resulting biplot is basically (I limit my interpretation to the 1st axis which explained most of the inertia):&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;the first axis highlights the clear opposition between light and dark hair color, and between blue and brown eyes;&lt;/li&gt;&#10;&lt;li&gt;people with blond hair tend to also have blue eyes, and people with black hair tend to have brown eyes.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;There is a lot of additional resources on data analysis on the &lt;a href=&quot;http://pbil.univ-lyon1.fr/R/enseignement.html&quot;&gt;bioinformatics lab&lt;/a&gt; from Lyon, in France. This is mostly in French, but I think it would not be too much a problem for you. The following two handouts should be interesting as a first start:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://pbil.univ-lyon1.fr/R/pdf/tdr620.pdf&quot;&gt;Initiation à l'analyse factorielle des correspondances&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://pbil.univ-lyon1.fr/R/pdf/tdr62.pdf&quot;&gt;Pratique de l'analyse des correspondances&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Finally, when you consider a full disjonctive (dummy) coding of $k$ variables, you get the &lt;em&gt;multiple correspondence analysis&lt;/em&gt;.&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2010-10-03T10:14:06.133" Id="3278" LastActivityDate="2010-10-04T20:40:05.720" LastEditDate="2010-10-04T20:40:05.720" LastEditorUserId="930" OwnerUserId="930" ParentId="3270" PostTypeId="2" Score="15" />
  <row Body="&lt;p&gt;Ridge regression (Hoerl and Kennard, 1988) was initially developed to overcome singularities when inverting $X^tX$ (by adding $\lambda$ to its diagonal elements). Thus, the &lt;em&gt;regularization&lt;/em&gt; in this case consists in working with a vc matrix $(X^tX-\lambda I)^{-1}$. This L2 penalization leads to &quot;better&quot; predictions than with usual OLS by optimizing the compromise between bias and variance (shrinkage), but it suffers from considering all coefficients in the model. The regression coefficients are found to be&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;\hat\beta=\underset{\beta}{\operatorname{argmin}}\|Y-X\beta\|^2 + \lambda\|\beta\|^2&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;with $\vert\vert\beta\vert\vert^2 = \sum_{j=1}^p\beta_j^2$ (L2-norm).&lt;/p&gt;&#10;&#10;&lt;p&gt;From a bayesian perspective, you can consider that the $\beta$'s must be small and plug them into a prior distribution. The likelihood $\ell (y,X,\hat\beta,\sigma^2)$ can thus be weighted by the prior probability for $\hat\beta$ (assumed i.i.d. with zero mean and variance $\tau^2$), and the posterior is found to be&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;f(\beta|y,X,\sigma^2,\tau^2)=(y-\hat\beta^tX)^t(y-\hat\beta^tX)+\frac{\sigma^2}{\tau^2}\hat\beta^t\hat\beta&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\sigma^2$ is the variance of your $y$'s. It follows that this density is the opposite of the residual sum of squares that is to be minimized in the Ridge framework, after setting $\lambda=\sigma^2/\tau^2$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The bayesian estimator for $\hat\beta$ is thus the same as the OLS one when considering the Ridge loss function with a prior variance $\tau^2$. More details can be found in &lt;em&gt;The Elements of Statistical Learning&lt;/em&gt; from Hastie, Tibshirani, and Friedman (§3.4.3, p.60 in the 1st ed.). The &lt;a href=&quot;http://www-stat.stanford.edu/~tibs/ElemStatLearn/&quot; rel=&quot;nofollow&quot;&gt;second edition&lt;/a&gt; is also available for free.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-10-03T11:12:14.313" Id="3280" LastActivityDate="2010-10-03T16:53:49.277" LastEditDate="2010-10-03T16:53:49.277" LastEditorUserId="930" OwnerUserId="930" ParentId="3276" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;Let $p_W$ be the probability of a win, $p_T$ that of a tie, and $p_L$ of a loss.  Because the probabilities sum to 1, so does their tenth power.  Expanding this out expresses the tenth power as a linear combination of the monomials $p_W^W p_T^T p_L^L$ where each term picks out the probability of $W$ wins, $T$ ties, and $L$ losses.&lt;/p&gt;&#10;&#10;&lt;p&gt;In more detail,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$(p_W + p_T + p_L)^{10} = \sum_{W+T+L=10} {W+T+L \choose {W \quad T \quad L}}p_W^W p_T^T p_L^L$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where the &lt;em&gt;multinomial coefficient&lt;/em&gt; is computed as&lt;/p&gt;&#10;&#10;&lt;p&gt;$${W+T+L \choose {W \quad T \quad L}} = \frac{(W+T+L)!}{W! T! L!}\text{.}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, the probability of 7 wins, 2 ties, and 1 loss is given by&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\Pr[W=7, T=2, L=1] = {7+2+1 \choose {7 \quad 2 \quad 1}}p_W^7 p_T^2 p_L^1$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$=\frac{10!}{7! 2! 1!}p_W^7 p_T^2 p_L^1 = 360p_W^7 p_T^2 p_L^1\text{.}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;This is called the &lt;a href=&quot;http://en.wikipedia.org/wiki/Multinomial_distribution&quot;&gt;Multinomial Distribution&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;When the probabilities vary from one game to the other, there is nothing simpler than enumerating all the possible sequences that produce a given number of wins, ties, and losses.  The multinomial coefficients count the numbers of such sequences.  The value of 360 in the example indicates there are 360 distinct ways you can arrive at 7 wins, 2 ties, and a loss.  Each one of those ways contributes its own distinct sequence of probabilities as given by each of the ten games.  To obtain a complete table of the 66 possible answers requires computing all $(1+1+1)^{10} = 59049$ products of ten probabilities.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2010-10-03T13:58:51.183" Id="3283" LastActivityDate="2010-10-03T14:08:45.343" LastEditDate="2010-10-03T14:08:45.343" LastEditorUserId="919" OwnerUserId="919" ParentId="3275" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;Here are some tutorials (available as PDFs):&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Dugad and Desai, &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.27.3772&quot; rel=&quot;nofollow&quot;&gt;A tutorial on hidden markov models&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;Valeria De Fonzo1, Filippo Aluffi-Pentini2 and Valerio Parisi (2007). &lt;a href=&quot;http://www.bentham.org/cbio/Samples/cbio2-1/0005CBIO.pdf&quot; rel=&quot;nofollow&quot;&gt;Hidden Markov Models in Bioinformatics&lt;/a&gt;. &lt;em&gt;Current Bioinformatics&lt;/em&gt;, &lt;em&gt;2&lt;/em&gt;, 49-61.&lt;/li&gt;&#10;&lt;li&gt;Smith, K. &lt;a href=&quot;http://www.google.fr/url?sa=t&amp;amp;source=web&amp;amp;cd=13&amp;amp;ved=0CCcQFjACOAo&amp;amp;url=http%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fdownload%3Fdoi%3D10.1.1.88.351%26rep%3Drep1%26type%3Dpdf&amp;amp;rct=j&amp;amp;q=bioinformatics%20introduction%20to%20markov%20chain%20HMM&amp;amp;ei=CKmpTJL0ENmJ4gaYraihDQ&amp;amp;usg=AFQjCNFy-AqKqpkKuQGOQpO42invnaLi4A&amp;amp;sig2=MPBuL2kvm0TVAN8JrtuvEA&quot; rel=&quot;nofollow&quot;&gt;Hidden Markov Models in Bioinformatics with Application to Gene Finding in Human DNA&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Also take a look at &lt;a href=&quot;http://www.bioconductor.org&quot; rel=&quot;nofollow&quot;&gt;Bioconductor&lt;/a&gt; tutorials. &lt;/p&gt;&#10;&#10;&lt;p&gt;I assume you want free resources; otherwise, &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/3540241663&quot; rel=&quot;nofollow&quot;&gt;Bioinformatics&lt;/a&gt; from Polanski and Kimmel (Springer, 2007) provides a nice overview (§2.8-2.9) and applications (Part II).&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2010-10-04T10:16:23.660" CreationDate="2010-10-04T10:16:23.660" Id="3298" LastActivityDate="2012-10-11T01:28:00.303" LastEditDate="2012-10-11T01:28:00.303" LastEditorUserId="12318" OwnerUserId="930" ParentId="3294" PostTypeId="2" Score="12" />
  <row Body="&lt;p&gt;I teach undergraduate biology students, and The Fear is rife among them.  I generally start by telling them three things:&lt;/p&gt;&#10;&#10;&lt;p&gt;1) Statistics is not maths, it's logic.  And if you're doing a science degree at a respected university, you eveidently don't have any problems with using logic to solve problems.&lt;/p&gt;&#10;&#10;&lt;p&gt;2) If you can add, subtract, multiply, divide and tell whether one number is bigger than another, you can do all the maths necessary for an undergrad stats course.&lt;/p&gt;&#10;&#10;&lt;p&gt;3) People learn differently, so if you don't understand one lecturer/textbook/explanation, ask or find another one.  (I try to give 2-3 types of explanation for a given idea where I can and tell them to remember the one that makes sense to them).&lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, I err on the side of visual explanations as opposed to purely verbal or mathematical ones, as this seems work for the majority of students.&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2010-10-04T13:02:30.140" CreationDate="2010-10-04T13:02:30.140" Id="3300" LastActivityDate="2010-10-04T13:02:30.140" OwnerUserId="266" ParentId="3262" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;First thing : define &quot;distance&quot;. Sounds like a stupid question, but what do you mean as distance? Is the data paired? Then -and only then- it makes sense to look at the sum of (squared) differences to decide about the distance between two datasets. If not, you have to resort to other means.&lt;/p&gt;&#10;&#10;&lt;p&gt;Next question is : is the data distributed in the same manner? If so, you can see the difference between the means as the &quot;location shift&quot; of your data (or the distance between both datasets).&lt;/p&gt;&#10;&#10;&lt;p&gt;But if neither of both is true, how do you define the distance between datasets then? Do you take shape of the distribution into account for example? You really have to think about those issues before trying to calculate a distance. &lt;/p&gt;&#10;&#10;&lt;p&gt;This said : One (naive) possibility is to use the mean of the differences between all possible x-y combinations. Formalized this is :&lt;/p&gt;&#10;&#10;&lt;p&gt;$$Dist=\sqrt{\frac{1}{n_1 n_2}\sum_{i=1}^{n_1} \sum_{j=1}^{n_2}(X_i - Y_j)^2}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;In R :&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x &amp;lt;- rnorm(10)&#10;y &amp;lt;- rnorm(10,2)&#10;sqrt(mean(outer(x,y,&quot;-&quot;)^2))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;If you allow for negative distances, you can drop the sqrt and the ^2 :&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;mean(outer(x,y,&quot;-&quot;))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;A simulation shows easily that this will give indeed the difference between the means in the example, as both distributions are equal in this case. But be warned that negative distances are not allowed in many applications. In the first scenario, the number will always a bit larger than the difference between the mean. In any case, if you're interested in the difference between the &lt;strong&gt;center&lt;/strong&gt; of your datasets, define the center and calculate the difference between those centers. That might very well be what you're after.&lt;/p&gt;&#10;&#10;&lt;p&gt;Contrary to the other suggestions, this approach does not make any assumptions about the distribution of your data. Which makes it applicable in all situations, but also difficult to interprete.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-10-04T16:33:09.597" Id="3306" LastActivityDate="2010-10-04T16:33:09.597" OwnerUserId="1124" ParentId="3286" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Properly normalized, it's closely approximated by a &lt;a href=&quot;http://en.wikipedia.org/wiki/Gumbel_distribution&quot; rel=&quot;nofollow&quot;&gt;Gumbel distribution&lt;/a&gt; as shown by &lt;a href=&quot;http://en.wikipedia.org/wiki/Extreme_value_theory&quot; rel=&quot;nofollow&quot;&gt;Extreme value theory&lt;/a&gt;.  Alternative names are provided in the links.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-10-04T17:47:51.937" Id="3310" LastActivityDate="2010-10-04T17:47:51.937" OwnerUserId="919" ParentId="3309" PostTypeId="2" Score="5" />
  <row AcceptedAnswerId="3319" AnswerCount="3" Body="&lt;p&gt;I have a S-Plus &lt;a href=&quot;https://home.zhaw.ch/~brw/index.html?tools&quot; rel=&quot;nofollow&quot;&gt;library&lt;/a&gt; which I'd like to convert to R. I am a programmer, but I don't know anything about S-Plus or R. From my research it seems that they are highly compatibile. Is that true? The code I want to convert only uses core S-Plus libraries.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have attached a picture of the library as seen in the S-Plus 8.0 Object Explorer. Besides function source files, there are a few entries which I'm not sure how to transpose to R. For example the last 5 (oneDay, ...), which seem to be some sort of global variables, and have specific values assigned to them. What would be the equivalent of them in R?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/102FK.png&quot; alt=&quot;alt text&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-10-04T19:47:23.197" Id="3313" LastActivityDate="2010-12-15T08:30:42.657" LastEditDate="2010-10-04T21:15:49.663" LastEditorUserId="8" OwnerUserId="749" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;splus&gt;" Title="How hard is to convert a library from S-PLUS 8.0 to R?" ViewCount="427" />
  <row Body="&lt;p&gt;I don't know Stata, so I can't comment on the specific model.  But the use of lagged variables is a fairly common approach when dealing with simultaneity bias in general and creating instrumental variables in particular.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Say you have a feedback between two variables in your model: the independent variable (such as price) and the dependent variable (such as quantity).  Then both are endogeneous (their causes arise from within the model) and perturbations to the error term will affect &lt;em&gt;both&lt;/em&gt; variables.  &lt;/p&gt;&#10;&#10;&lt;p&gt;To solve this, you want to make the independent variable (price) exogeneous so that perturbations in the error affect only the dependent variable (quantity).  This is accomplished by creating new exogeneous variables by regressing the other exogeneous variables in your model on price.  These new exogeneous variables are your instrumental variables (IVs).  The IVs are derived from exogeneous terms and thus not correlated with the error.&lt;/p&gt;&#10;&#10;&lt;p&gt;But to do this, you need to figure out which variables are exogeneous so they can be used to derive the IVs.  We can note that lagged variables &quot;occurred&quot; in the past and thus can't be correlated with the error in the present.  Lagged variables are thus exogeneous and become convenient candidates for deriving IVs.  (However, note that the preceding argument fails when the errors are autocorrelated.)&lt;/p&gt;&#10;&#10;&lt;p&gt;A good introduction and reference to this is &lt;a href=&quot;http://books.google.com/books?id=64vt5TDBNLwC&quot; rel=&quot;nofollow&quot;&gt;&lt;em&gt;Introductory econometrics: a modern approach&lt;/em&gt;&lt;/a&gt; by Wooldridge.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-10-05T05:28:39.143" Id="3330" LastActivityDate="2010-10-05T05:39:47.353" LastEditDate="2010-10-05T05:39:47.353" LastEditorUserId="159" OwnerUserId="251" ParentId="3324" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;Have you considered 'PRIM / bump hunting' ? (see e.g. Section 9.3. of 'The Elements of Statistical Learning' by Tibshirani et al. or ask your favourite search engine). Not sure whether that's implemented in R though.&lt;/p&gt;&#10;&#10;&lt;p&gt;[ As far as I understood are you trying to find the mode of the probability density from which your 100'000 rows are drawn. So your problem would be partially solved by finding an appropriate &lt;code&gt;density estimation&lt;/code&gt; method ].&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-10-05T07:54:19.357" Id="3332" LastActivityDate="2010-10-05T07:54:19.357" OwnerUserId="961" ParentId="3328" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;This is only a partial answer.&lt;/p&gt;&#10;&#10;&lt;p&gt;I recently used &lt;a href=&quot;http://www.umiacs.umd.edu/~morariu/figtree/&quot; rel=&quot;nofollow&quot;&gt;figtree&lt;/a&gt; for multidimensional kernel density estimates. It's a C package and I got it to work fairly easily. However, I only used it to estimate the density at particular points, not calculate summary statistics.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-10-05T09:39:19.830" Id="3335" LastActivityDate="2010-10-05T09:39:19.830" OwnerUserId="8" ParentId="3328" PostTypeId="2" Score="3" />
  
  <row AcceptedAnswerId="3360" AnswerCount="1" Body="&lt;p&gt;I am looking at trigger efficiencies, meaning that I have some device that fires on $k$ out of $n$ events. In the end I am interested in some estimate of the efficiency $\epsilon$ which is the probability to fire on a randomly given event. Using a Bayesian approach with a uniform prior over $[0,1]$ I can model the probability distribution for $\epsilon$ as a Beta distribution $\beta(\epsilon; k+1, n-k+1)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now comes the question: I calculate the efficiency using &quot;bootstrapping&quot; which means that the final trigger efficiency is the product of two trigger efficiencies, both of which can be modelled as Beta distributions.&lt;/p&gt;&#10;&#10;&lt;p&gt;How can I calculate this product of the two Beta PDFs for large values of $k_{1,2}$ and $n_{1,2}$ efficiently? Is there a closed form of the product (AFAIK not)? At the moment I am doing this numerically, but this is rather slow.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/2066/how-can-i-numerically-approximate-values-for-a-beta-distribution-with-large-alp&quot;&gt;This question&lt;/a&gt; has the answer how to evaluate integrals of the Beta distribution for large argument values, but this does not help here.&lt;/p&gt;&#10;&#10;&lt;p&gt;I hope my question is clear and not completely stupid...&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-10-06T11:52:36.327" FavoriteCount="1" Id="3359" LastActivityDate="2010-10-06T14:23:03.013" LastEditDate="2010-10-06T14:23:03.013" LastEditorUserId="8" OwnerUserId="1512" PostTypeId="1" Score="5" Tags="&lt;distributions&gt;&lt;beta&gt;" Title="Product of beta distributions" ViewCount="1377" />
  <row Body="&lt;p&gt;According to the abstract of &lt;a href=&quot;http://scitation.aip.org/getabs/servlet/GetabsServlet?prog=normal&amp;amp;id=SMJMAP000018000004000721000001&amp;amp;idtype=cvips&amp;amp;gifs=yes&amp;amp;ref=no&quot; rel=&quot;nofollow&quot;&gt;this paper&lt;/a&gt;, &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The density function of products of&#10;  random beta variables is a Meijer&#10;  $G$-function which is expressible in&#10;  closed form when the parameters are&#10;  integers.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;However, I imagine the closed form requires a great deal of combinatorial calculation and hence would not be practically useful.  The slow numerical algorithm you mentioned is probably faster.&lt;/p&gt;&#10;&#10;&lt;p&gt;This paper may be more useful since it does not require integer parameters.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.springerlink.com/content/j3w2789gn8374234/&quot; rel=&quot;nofollow&quot;&gt;The distribution of product of independent beta random variables with application to multivariate analysis&lt;/a&gt;  &lt;/p&gt;&#10;&#10;&lt;p&gt;I haven't read the paper, but the abstract sounds promising.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-10-06T12:19:12.380" Id="3360" LastActivityDate="2010-10-06T12:19:12.380" OwnerUserId="319" ParentId="3359" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="3375" AnswerCount="1" Body="&lt;p&gt;I'd like to assess the impact of an upcoming policy implementation, as measured by changes in questionnaire response to a Likert-scale question.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I understand I could use a difference-in-difference approach.  However, in my situation there is no single obvious comparison, non-treated population.  I think I'd like to use the &quot;Synthetic Control Method for Comparative Case Studies&quot; as described by Abadie et al and implemented as Synth in R.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Alberto Abadie, Alexis Diamond, Jens Hainmueller. Journal of the American Statistical Association. June 1, 2010, 105(490): 493-505. doi:10.1198/jasa.2009.ap08746.  &lt;a href=&quot;http://courses.gov.harvard.edu/gov3009/spring07/hainmueller.pdf&quot; rel=&quot;nofollow&quot;&gt;full text&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;As summarized in the R help for synth: &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;synth estimates the effect of an&#10;  intervention of interest by comparing&#10;  the evolution of an aggregate outcome&#10;  for a unit affected by the&#10;  intervention to the evolution of the&#10;  same aggregate outcome for a synthetic&#10;  control group.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;synth constructs this synthetic&#10;  control group by searching for a&#10;  weighted combination of control units&#10;  chosen to approximate the unit&#10;  affected by the intervention in terms&#10;  of the outcome predictors. The&#10;  evolution of the outcome for the&#10;  resulting synthetic control group is&#10;  an estimate of the counterfactual of&#10;  what would have been observed for the&#10;  affected unit in the absence of the&#10;  intervention. [..] the synth function&#10;  routinely searches for the set of&#10;  weights that generate the best fitting&#10;  convex combination of the control&#10;  units. In other words, the predictor&#10;  weight matrix V is chosen among all&#10;  positive definite diagonal matrices&#10;  such that MSPE is minimized for the&#10;  pre-intervention period.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;See also the useful summary by Srikant Vadali in answers below.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is this method appropriate for survey/sampled data?  Is there anything I need to do differently, or just use my Likert-response mean as the dependent variable?  Any suggestions about how I'd power such a beast?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-10-06T13:27:08.537" Id="3362" LastActivityDate="2010-10-07T12:59:02.717" LastEditDate="2010-10-07T12:59:02.717" LastEditorUserId="644" OwnerUserId="644" PostTypeId="1" Score="5" Tags="&lt;econometrics&gt;&lt;survey&gt;&lt;causal-inference&gt;&lt;panel-data&gt;" Title="Can I use Synthetic Control Method for Comparative Case Studies with survey data?" ViewCount="1395" />
  
  <row AnswerCount="5" Body="&lt;p&gt;I have a little problem that is making me freaking out.&#10;I have to write procedure for an online acquisition process of a multivariate time series.&#10;At every time interval (for example 1 second), I get a new sample, which is basically a floating point vector of size N.&#10;The operation I need to do is a little bit tricky:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;For each new sample, I compute the percentuals for that sample (by normalizing the vector so that the elements will sum to 1).&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;I calculate the average percentuals vector in the same way, but using the past values.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;For each past value, I compute the absolute deviation of the percentuals vector related to that sample with the global average percentuals vector computed at step 2. This way, the absolute deviation is always a number between 0 (when the vector is equal to the average vector) and 2 (when it is totaly different).&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Using the average of the deviations for all the previous samples, I compute the mean absolute deviation, which is again a number between 0 and 2.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;I use the mean absolute deviation to detect if a new sample is compatible with the other samples (by comparing its absolute deviation with the mean absolute deviation of the whole set computed at step 4).&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Since every time a new sample is collected the global average changes (and so the mean absolute deviation changes as well), is there a way to compute this value without scanning the whole data set multiple times? (one time for computing the global average percentuals, and one time for collecting the absolute deviations).&#10;Ok, I know it's absolutely easy to calculate the global averages without scanning the whole set, since I just have to use a temporary vector for storing the sum of each dimension, but what about the mean absolute deviation? Its calculation includes the &lt;code&gt;abs()&lt;/code&gt; operator, so I need to access to all the past data!&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for your help.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-10-07T03:26:23.003" FavoriteCount="3" Id="3377" LastActivityDate="2010-11-09T05:36:27.843" LastEditDate="2010-10-08T16:04:53.503" LastEditorUserId="8" OwnerUserId="667" PostTypeId="1" Score="11" Tags="&lt;algorithms&gt;&lt;quantiles&gt;&lt;online&gt;&lt;large-data&gt;" Title="Online algorithm for mean absolute deviation and large data set" ViewCount="1872" />
  <row Body="&lt;p&gt;If you can accept some inaccuracy, this problem can be solved easily by &lt;em&gt;binning&lt;/em&gt; counts. That is, pick some largeish number $M$ (say $M = 1000$), then initialize some integer bins $B_{i,j}$ for $i = 1\ldots M$ and $j = 1\ldots N$, where $N$ is the vector size, as zero. Then when you see the $k$th observation of a percentual vector, increment $B_{i,j}$ if the $j$th element of this vector is between $(i-1)/M$ and $i/M$, looping over the $N$ elements of the vector. (I am assuming your input vectors are non-negative, so that when you compute your 'percentuals', the vectors are in the range $[0,1]$. )&lt;/p&gt;&#10;&#10;&lt;p&gt;At any point in time, you can estimate the mean vector from the bins, and the mean absolute deviation. After observing $K$ such vectors, the $j$th element of the mean is estimated by &#10;$$\bar{X}_j = \frac{1}{K} \sum_i \frac{i - 1/2}{M} B_{i,j},$$ and the $j$th element of the mean absolute deviation is estimated by $$\frac{1}{K} \sum_i | \bar{X_j} - \frac{i - 1/2}{M} | B_{i,j}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;edit&lt;/strong&gt;: this is a specific case of a more general approach where you are building an empirical density estimate. This could be done with polynomials, splines, etc, but the binning approach is the easiest to describe and implement. &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2010-10-07T05:17:09.267" Id="3378" LastActivityDate="2010-10-07T16:44:47.327" LastEditDate="2010-10-07T16:44:47.327" LastEditorUserId="795" OwnerUserId="795" ParentId="3377" PostTypeId="2" Score="5" />
  
  
  
  
  <row Body="&lt;p&gt;I'm an ecologist, so I apologise in advance is this sounds a bit strange :-)&lt;/p&gt;&#10;&#10;&lt;p&gt;I like to think of these plots in terms of weighted averages. The region points are at the weighted averages of the smoking status classes and vice versa.&lt;/p&gt;&#10;&#10;&lt;p&gt;The problem with the above figure is the axis scaling and the fact that you can't display all the relationships (chi-square distance between regions and chi-square distance between smoking status) on the one figure. By the looks of it, the figure is using a what is known as symmetric scaling which has been shown to be a good compromise preserving as much of the information in the sets of scores as possible.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm not familiar with the &lt;code&gt;ca&lt;/code&gt; package but I am with the vegan package and it's &lt;code&gt;cca&lt;/code&gt; function:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;require(vegan)&#10;df &amp;lt;- data.frame(df)&#10;ord &amp;lt;- cca(df)&#10;plot(ord, scaling = 3)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The last plot is a bit easier to read than the one you show but AFAICT they are the same (or at least similarly scaled).&lt;/p&gt;&#10;&#10;&lt;p&gt;So I would say that occasional smokers are lower in number than expected in QC, BC and AB, and most associated with ON, but that in all regions, occasional smokers are low in number - they differ markedly from the expected number.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, there is a single dominant &quot;gradient&quot; or axis of variation in these data and as the second axis represents so little variation, I would likely not interpret this component at all.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-10-08T16:16:58.473" Id="3417" LastActivityDate="2010-10-08T16:22:10.900" LastEditDate="2010-10-08T16:22:10.900" LastEditorUserId="1390" OwnerUserId="1390" ParentId="3287" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;Addressing the issue mentioned under Update 2.  You are dealing with outliers.  Those outliers have a significant impact on your Logistic Regression coefficients.  By removing them, you found that your models performed better on the validation set.   &lt;/p&gt;&#10;&#10;&lt;p&gt;Does it mean that the outliers are &quot;bad&quot;?  No.  It means that they are influential.  There are several measures of statistical distances to confirm how far away and influential such outliers are.  Those include Cook's D and DFFITS. &lt;/p&gt;&#10;&#10;&lt;p&gt;Having identified the trouble makers, you are struggling with whether to keep them in or not.  Ultimately, this may be a qualitative judgment rather than a statistical question.  Here are a couple of investigative questions that may be helpful in making this qualitative decision: &#10;1) First, are the outliers truly bad due to poor measurements?&#10;2) Is it more important for your models to be correct in the tails where outliers reside or be more accurate in the vast majority of the cases?  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-10-08T19:38:39.063" Id="3420" LastActivityDate="2010-10-08T19:38:39.063" OwnerUserId="1329" ParentId="3296" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;To supplement Kwak's useful answer, allow me to add some simple principles and ideas.  A good way to determine the metric is by considering how the strings might vary from their target.  &quot;Edit distance&quot; is useful when the variation is a combination of typographic errors like transposing neighbors or mis-typing a single key.&lt;/p&gt;&#10;&#10;&lt;p&gt;Another useful approach (with a slightly different philosophy) is to map every string into one representative of a class of related strings.  The &quot;&lt;a href=&quot;http://en.wikipedia.org/wiki/Soundex&quot; rel=&quot;nofollow&quot;&gt;Soundex&lt;/a&gt;&quot; method does this: the Soundex code for a word is a sequence of four characters encoding the principal consonant and groups of similar-sounding internal consequence.  It is used when words are phonetic misspellings or variants of one another. In the example application you would fetch all target words whose Soundex code equals the Soundex code for each probe word.  (There could be zero or multiple targets fetched this way.)&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-10-09T00:12:17.553" Id="3432" LastActivityDate="2010-10-09T00:12:17.553" OwnerUserId="919" ParentId="3425" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;I would see each histogram as a different model (parametrized by the width). Fitting a smoothing spline or some other kind of smoother for each of the models is simple.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can then do model selection (such as cross-validation) to choose the histogram width that gives the best results, or do model stacking to fit least-squares weights on the models. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, why not directly smooth the data instead of clustering it into histogram bars first? There are finite-window width kernels that don't use the entire dataset for prediction at a given point. Practicality and speed depends on what you are really trying to obtain, but I am sure there exist simpler solutions.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-10-09T09:16:11.677" Id="3436" LastActivityDate="2010-10-09T09:16:11.677" OwnerUserId="1526" ParentId="3381" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="3440" AnswerCount="5" Body="&lt;p&gt;See this Wikipedia page:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Agresti-Coull_Interval&quot;&gt;http://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Agresti-Coull_Interval&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;To get the Agresti-Coull Interval, one needs to calculate a percentile of the normal distribution, called $z$. How do I calculate tha percentile? Is there a ready-made function that does this in Wolfram Mathematica and/or Python/NumPy/SciPy?&lt;/p&gt;&#10;" CommentCount="17" CreationDate="2010-10-09T13:34:40.713" FavoriteCount="0" Id="3438" LastActivityDate="2012-03-06T19:14:25.360" OwnerUserId="5793" PostTypeId="1" Score="8" Tags="&lt;python&gt;&lt;normal-distribution&gt;" Title="Calculating percentile of normal distribution" ViewCount="6681" />
  <row Body="&lt;p&gt;Well, you didn't ask about R, but in R you do it using ?qnorm&lt;/p&gt;&#10;&#10;&lt;p&gt;(It's actually the quantile, not the percentile, or so I believe)&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; qnorm(.5)&#10;[1] 0&#10;&amp;gt; qnorm(.95)&#10;[1] 1.644854&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="3" CreationDate="2010-10-09T13:40:55.500" Id="3439" LastActivityDate="2010-10-09T13:40:55.500" OwnerUserId="253" ParentId="3438" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;There is a subfield called Distance Metric Learning. One such method is Information Theoretic Metric Learning (ITML).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-10-10T06:12:37.237" Id="3456" LastActivityDate="2010-10-10T06:12:37.237" OwnerUserId="1540" ParentId="3419" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;I don't know for certain, but that won't stop me from wildly speculating:&lt;/p&gt;&#10;&#10;&lt;p&gt;The __nonfi file lists what's in the workspace.  You can open it with a text editor and look at the contents.  It might be possible to either manipulate the unix version (e.g. using dos2unix) or else copy the contents over into your new file.&lt;/p&gt;&#10;&#10;&lt;p&gt;That said, I doubt that this will work since some of the S-Plus files are in binary format, and I have run into trouble in the past when I have tried to manually change the __nonfi file.  This question might be better served by Tibco technical support.&lt;/p&gt;&#10;&#10;&lt;p&gt;Please update us here if you get an answer.&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2010-10-11T20:31:15.973" CreationDate="2010-10-11T13:11:51.507" Id="3480" LastActivityDate="2010-10-11T13:11:51.507" OwnerUserId="5" ParentId="3471" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Good question.  A trivial way to find &quot;cluster of high values in the upper left&quot;&#10;(as opposed to correlations)&#10;is to split the image into tiles and look at tile means. For example,&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;means of 100 x 100 tiles:&#10;[[ 82  78  80  94  99 100]&#10; [ 80  53  66  62  80 100]&#10; [ 82  61  65  64  72  98]&#10; [ 87  83  99  81  80 100]&#10; [100 100 100 100 100 100]]&#10;&#10;means of 50 x 50 tiles:&#10;[[100  85  84 100  70  96 100 100 100 100 100]&#10; [ 83  59  57  71  67  88  89  86  98 100 100]&#10; [ 87  58  54  49  71  74  71  61  61 100 100]&#10; [100  76  58  52  59  61  55  59  65  95 100]&#10; [100  62  59  60  57  63  60  60  59  97 100]&#10; [100  68  65  59  59  82  76  61  61  70  95]&#10; [ 83  64  76  66  96 100  96  61  80  67 100]&#10; [100 100  97  92 100 100  84  82  83  88 100]&#10; [100 100 100 100 100 100 100 100 100 100 100]]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;(a plot with average height / colour in each tile would be 10x better).&lt;/p&gt;&#10;&#10;&lt;p&gt;(If you're looking for features in images, what's a &quot;feature&quot; ?&#10;E.g. a red stop sign, as in&#10;&lt;a href=&quot;http://www.cse.yorku.ca/~sizints/histogram_search/histogram_search_main.html&quot; rel=&quot;nofollow&quot;&gt;Histograms for feature representation&lt;/a&gt;&#10;)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-10-11T15:46:43.540" Id="3485" LastActivityDate="2010-10-11T15:46:43.540" OwnerUserId="557" ParentId="3199" PostTypeId="2" Score="3" />
  
  
  <row Body="&lt;p&gt;It's important to bear in mind that there's no one algorithm that's always better than others. As stated by Wolpert and Macready, &quot;any two algorithms are equivalent when their performance is averaged across all possible problems.&quot; (See &lt;a href=&quot;http://en.wikipedia.org/wiki/No_free_lunch_in_search_and_optimization&quot;&gt;Wikipedia&lt;/a&gt; for details.)&lt;/p&gt;&#10;&#10;&lt;p&gt;For a given application, the &quot;best&quot; one is generally one that is most closely aligned to your application in terms of the assumptions it makes, the kinds of data it can handle, the hypotheses it can represent, and so on.&lt;/p&gt;&#10;&#10;&lt;p&gt;So it's a good idea to characterise your data according to criteria such as:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Do I have a very large data set or a modest one?&lt;/li&gt;&#10;&lt;li&gt;Is the dimensionality high?&lt;/li&gt;&#10;&lt;li&gt;Are variables numerical (continuous/discrete) or symbolic, or a mix, and/or can they be transformed if necessary?&lt;/li&gt;&#10;&lt;li&gt;Are variables likely to be largely independent or quite dependent?&lt;/li&gt;&#10;&lt;li&gt;Are there likely to be redundant, noisy, or irrelevant variables?&lt;/li&gt;&#10;&lt;li&gt;Do I want to be able to inspect the model generated and try to make sense of it?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;By answering these, you can eliminate some algorithms and identify others as potentially relevant, and then maybe end up with a small set of candidate methods that you have intelligently chosen as likely to be useful.&lt;/p&gt;&#10;&#10;&lt;p&gt;Sorry not to give you a simple answer, but I hope this helps nonetheless!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-10-12T08:56:16.850" Id="3510" LastActivityDate="2010-10-12T08:56:16.850" OwnerUserId="1436" ParentId="3458" PostTypeId="2" Score="10" />
  <row Body="&lt;p&gt;This  problem is typically solved by  fit by coordinate descent (&lt;a href=&quot;http://www.jstatsoft.org/v33/i01/paper&quot;&gt;see here&lt;/a&gt;). This method is both safer more efficient numerically, algorithmically easier to implement and applicable to a more general array of models (also including Cox regression). An R implementation is available in the &lt;strong&gt;R&lt;/strong&gt; package &lt;strong&gt;&lt;a href=&quot;http://cran.r-project.org/web/packages/glmnet/index.html&quot;&gt;glmnet&lt;/a&gt;&lt;/strong&gt;. The codes are open source (partly in and in C, partly in R), so you can use them as blueprints.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-10-12T10:47:21.650" Id="3513" LastActivityDate="2010-10-12T10:47:21.650" OwnerUserId="603" ParentId="3511" PostTypeId="2" Score="10" />
  <row AnswerCount="3" Body="&lt;p&gt;I have a binary variable (which takes values 0,1). I have about 100k records of it. How do I determine if it follows the binomial distribution?&lt;/p&gt;&#10;&#10;&lt;p&gt;(I'm bascially trying to test for normality. And, if the data is not normal, I might have to apply a transformation to get the variable into a binomial distribution.)&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Hey, thank you folks for clearing this up. &lt;/p&gt;&#10;&#10;&lt;p&gt;This was an effort as a prelude to cluster analysis. I also understand that normality of variables is more a nice to have feature for cluster analysis and that the distance measures would be valid even otherwise. Your views?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-10-12T13:17:55.790" FavoriteCount="1" Id="3516" LastActivityDate="2012-05-25T15:33:12.343" LastEditDate="2012-05-25T15:33:12.343" LastEditorUserId="919" OwnerDisplayName="Roger" PostTypeId="1" Score="6" Tags="&lt;binomial&gt;&lt;assumptions&gt;" Title="Binomial test for a binary variable" ViewCount="1519" />
  
  
  
  <row AcceptedAnswerId="3581" AnswerCount="2" Body="&lt;p&gt;I would like to apply KDE to inventory replenishment, but I am not sure how to use the analysis to predict future sales based on past sales. Given a set of data and having applied KDE to it (probably using a Gaussian distribution), how do I make a prediction about the future? &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for any help! Please let me know if I can clarify - I'm only starting to pick up the language for talking about KDE ... I'm glad to do reading on my own - pointers to any resources would be welcome.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2010-10-13T17:48:45.233" FavoriteCount="2" Id="3564" LastActivityDate="2010-10-13T22:49:26.513" LastEditDate="2010-10-13T22:24:24.733" LastEditorUserId="1574" OwnerUserId="1574" PostTypeId="1" Score="3" Tags="&lt;time-series&gt;&lt;forecasting&gt;&lt;smoothing&gt;&lt;kde&gt;&lt;kernel&gt;" Title="How to use Kernel Density Estimation for Prediction?" ViewCount="1432" />
  
  
  <row AcceptedAnswerId="3576" AnswerCount="4" Body="&lt;p&gt;I've got a data table like:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ID  Low Color    Med Color     High Color&#10; 1        234          123            324&#10; 2          4          432           3423&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The rows are widgets, the columns are color levels. Would you call this table &quot;widgets by color level&quot; or &quot;color levels by widget&quot;?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-10-13T20:45:25.290" Id="3575" LastActivityDate="2012-04-11T12:41:19.497" LastEditDate="2012-04-11T12:41:19.497" LastEditorUserId="7250" OwnerUserId="1531" PostTypeId="1" Score="4" Tags="&lt;terminology&gt;&lt;communication&gt;" Title="Is it &quot;rows by columns&quot; or &quot;columns by rows&quot;?" ViewCount="1065" />
  
  <row AcceptedAnswerId="3585" AnswerCount="1" Body="&lt;p&gt;I have a dataset asking people whether they have been to a certain places (e.g. A, B, C, D), and they can make more than one choice, then a specimen is taken from their nose to see if they are infected with some disease.&lt;/p&gt;&#10;&#10;&lt;p&gt;I need to find out the relative risk of getting infected for one going to a certain place, I can only think of logistic regression right now, is there any other suggestions?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-10-14T03:36:34.627" FavoriteCount="2" Id="3584" LastActivityDate="2010-10-14T20:37:16.073" LastEditDate="2010-10-14T20:37:16.073" LastEditorUserId="1352" OwnerUserId="588" PostTypeId="1" Score="8" Tags="&lt;logistic&gt;" Title="How to deal with survey question with multiple response?" ViewCount="640" />
  
  
  <row Body="&lt;p&gt;The article * D. G. Terrell; D. W. Scott (1992). &quot;Variable kernel density estimation&quot;. Annals of Statistics 20: 1236–1265.* cited at the end of the Wikipedia article you yourself cite clearly states that unless the observations space is very sparse the variable kernel method is &lt;strong&gt;not&lt;/strong&gt; recommended on the basis of global root mean squared error (both local and global) for Gaussian distributed random variables: (through theoretical arguments) they cite the figures of $n\leq 450$ ($n$ is the sample size) and (through bootstrapping results) $p\geq 4$ ($p$ is the number of dimension) as the settings in which variable kernel method become competitive with fixed width ones (judging from your question you are not in these settings). &lt;/p&gt;&#10;&#10;&lt;p&gt;The intuition behind these results is that if you are not in very sparse settings, then, the local density simply does not vary enough for the gain in bias to outdo the loss in efficiency (and hence the AMISE of variable width kernel increases relative to the AMISE of fixed width). Also, given the large sample size you have (and the small dimensions) the fixed width kernel will be very local already, diminishing any potential gains in terms of bias. &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2010-10-14T06:52:05.923" Id="3588" LastActivityDate="2010-10-14T06:52:05.923" OwnerUserId="603" ParentId="3556" PostTypeId="2" Score="5" />
  
  
  <row Body="&lt;p&gt;Not easy at all. This starts to sound like the sort of thing that &lt;a href=&quot;http://en.wikipedia.org/wiki/James_Robins&quot; rel=&quot;nofollow&quot;&gt;Jamie Robins&lt;/a&gt; and colleagues have done a lot of work on. To quote the start of the abstract of one of their papers:&lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;&lt;em&gt;In observational studies with exposures or treatments that vary over time, standard approaches for adjustment of confounding are biased when there exist time-dependent confounders that are also affected by previous treatment.&lt;/em&gt;&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;(Robins JM, Hernán MA, Brumback B.  Marginal Structural Models and Causal Inference in Epidemiology.  &lt;em&gt;Epidemiology&lt;/em&gt; 2000;11:550-560. &lt;a href=&quot;http://www.jstor.org/stable/3703997&quot; rel=&quot;nofollow&quot;&gt;http://www.jstor.org/stable/3703997&lt;/a&gt;)&lt;/p&gt;&#10;&#10;&lt;p&gt;In their example, low CD4 count means you're more likely to get anti-HIV drugs, which (hopefully) increase your CD4 count. Sounds very much like your setting. &lt;/p&gt;&#10;&#10;&lt;p&gt;There's been a lot of work in this area recently (i.e. more recently than that paper), and it's not easy to get to grips with from the journal papers. &lt;a href=&quot;http://www.hsph.harvard.edu/faculty/miguel-hernan/causal-inference-book/&quot; rel=&quot;nofollow&quot;&gt;Hernán &amp;amp; Robins are writing a book&lt;/a&gt;, which should help a lot -- it's not finished yet, but there's a draft of the first 10 chapters available.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-10-14T14:23:57.400" Id="3607" LastActivityDate="2010-10-14T14:23:57.400" OwnerUserId="449" ParentId="3599" PostTypeId="2" Score="1" />
  <row AnswerCount="2" Body="&lt;p&gt;It seems to me that rather than using non robust&#10;estimation methods with  robust standard errors it would be better to use&#10;robust estimation from the outset. I wonder what other people think.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-10-14T17:57:07.050" FavoriteCount="2" Id="3609" LastActivityDate="2010-10-15T04:27:41.783" LastEditDate="2010-10-14T17:59:07.143" LastEditorUserId="930" OwnerUserId="1585" PostTypeId="1" Score="3" Tags="&lt;robust&gt;&lt;panel-data&gt;" Title="Robust standard errors for panel data vs robust estimation for panel data" ViewCount="823" />
  
  <row AnswerCount="6" Body="&lt;p&gt;I want to calculate the probability distribution for the total of a combination of dice. &lt;/p&gt;&#10;&#10;&lt;p&gt;I remember that the probability of is the number of combinations that total that number over the total number of combinations (assuming the dice have a uniform distribution). &lt;/p&gt;&#10;&#10;&lt;p&gt;What are the formulas for&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;The number of combinations total&lt;/li&gt;&#10;&lt;li&gt;The number of combinations that total a certain number&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2010-10-14T19:23:39.887" FavoriteCount="4" Id="3614" LastActivityDate="2014-07-28T02:19:47.983" LastEditDate="2010-10-14T22:40:19.860" LastEditorUserId="88" OwnerUserId="1456" PostTypeId="1" Score="12" Tags="&lt;probability&gt;&lt;dice&gt;" Title="How to easily determine the results distribution for multiple dice?" ViewCount="12062" />
  
  <row AcceptedAnswerId="3626" AnswerCount="2" Body="&lt;p&gt;I have a logistic regression (in SAS, for reference) with continuous and categorical predictors (with reference coding), and an interaction term between one of each type (assume for now that the categorical variable in question has three response levels, reference coded to $c_1$ and $c_2$):&lt;/p&gt;&#10;&#10;&lt;p&gt;$logit(p) = a + (continuous terms) + (categorical terms) + b_1 (c_1 x) + b_2 (c_2 x)$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $b_1$ and $b_2$ are the estimated coefficients from my code. From this I can obviously get an expression for the probability $p$ of my outcome. I want to estimate the average p by plugging in the means of the continuous terms and the proportions of the categorical terms. But what do I do with the interaction term(s)? Do I set $(c_i x) = mean(x)$? Or do I set it to $proportion(c_i)mean(x)$?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-10-15T04:44:29.807" Id="3623" LastActivityDate="2010-10-15T08:27:19.977" LastEditDate="2010-10-15T08:27:19.977" LastEditorUserId="88" OwnerUserId="1144" PostTypeId="1" Score="2" Tags="&lt;logistic&gt;&lt;categorical-data&gt;&lt;fitting&gt;" Title="Plugging in mean values/proportions to a logistic regression with continuous-discrete interaction" ViewCount="293" />
  
  <row Body="&lt;p&gt;Yes, you do need to account for the irregularity of the time series because volatility scales with time.  Depending upon the distribution and independence assumptions, &lt;a href=&quot;http://ideas.repec.org/p/fmg/fmgdps/dp439.html&quot; rel=&quot;nofollow&quot;&gt;sometimes a &quot;square root of time&quot; rule can be appropriate&lt;/a&gt;.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Is this data sampled irregularly intraday or across a longer time period?  What kind of data is it?&lt;/p&gt;&#10;&#10;&lt;p&gt;For dealing with high-frequency financial data, you can apply a &lt;a href=&quot;http://www.ssc.upenn.edu/~fdiebold/papers/paper29/temp.pdf&quot; rel=&quot;nofollow&quot;&gt;realized volatility&lt;/a&gt; measure, which is available in R in the &lt;a href=&quot;http://cran.r-project.org/web/packages/realized/index.html&quot; rel=&quot;nofollow&quot;&gt;realized package&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-10-15T16:02:52.387" Id="3637" LastActivityDate="2010-10-15T16:02:52.387" OwnerUserId="5" ParentId="3636" PostTypeId="2" Score="1" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I would like to project the data in this graph for at least 4 or 5 periods. Unfortunately, that won't be possible with a moving average. A regression will result in negative values after the 3rd period. What are my forecasting options?&lt;/p&gt;&#10;&#10;&lt;p&gt;Basically, what i'm trying to do, is predict where the boomer hump is gonna be based soley on the population and age per year data&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt; AT work we have lots of image sites blocked, &lt;a href=&quot;http://97.107.136.148/age_dist.jpg&quot; rel=&quot;nofollow&quot;&gt;here's the image&lt;/a&gt; in case you can't see it.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT 2&lt;/strong&gt; Updated Image&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Ad80M.jpg&quot; alt=&quot;State Wide Age Distribution&quot;&gt;&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2010-10-15T16:50:38.817" Id="3641" LastActivityDate="2010-10-15T19:43:31.437" LastEditDate="2010-10-15T17:25:32.207" LastEditorUserId="59" OwnerUserId="59" PostTypeId="1" Score="3" Tags="&lt;time-series&gt;&lt;forecasting&gt;" Title="Forecasting Age distribution" ViewCount="188" />
  
  <row Body="&lt;p&gt;You might want to look at some of the references under the Wikipedia article on &lt;a href=&quot;http://en.wikipedia.org/wiki/Ratio_distribution&quot; rel=&quot;nofollow&quot;&gt;Ratio Distribution&lt;/a&gt;.  It's possible you'll find better approximations or distributions to use.  Otherwise, your approach seems sound.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Update&lt;/em&gt; I think a better reference might be:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://www.jstor.org/pss/2283145&quot; rel=&quot;nofollow&quot;&gt;Ratios of Normal Variables and Ratios of Sums of Uniform Variables&lt;/a&gt; (Marsaglia, 1965)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;See formulas 2-4 on page 195.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Update 2&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;On your updated question regarding variance from a Cauchy -- as John Cook pointed out in the comments, the variance doesn't exist.  So, taking a sample variance simply won't work as an &quot;estimator&quot;.  In fact, you'll find that your sample variance does not converge at all and fluctuates wildly as you keep taking samples.  &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2010-10-15T17:17:18.427" Id="3644" LastActivityDate="2010-10-15T19:45:31.940" LastEditDate="2010-10-15T19:45:31.940" LastEditorUserId="251" OwnerUserId="251" ParentId="3640" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;The &lt;a href=&quot;http://cc.oulu.fi/~jarioksa/softhelp/vegan.html&quot; rel=&quot;nofollow&quot;&gt;vegan&lt;/a&gt; package implements permutation testing for distance based ANOVA, which should work with multi-way, repeated measures data.  &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-10-15T17:43:27.083" Id="3645" LastActivityDate="2010-10-15T17:43:27.083" OwnerUserId="251" ParentId="3634" PostTypeId="2" Score="3" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I have 3 observers that each take 2 measurements (length and weight) on 100 individuals; these procedures are repeated once (i.e., the same measurements are taken on the same 100 individuals by the same 3 observers), so that the data set is duplicated (i.e., early reading and late reading). &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;What is the best way to figure out how each individual observer's measurements vary between the late and early trial measurements?&lt;/li&gt;&#10;&lt;li&gt;How can I best compare how close or different the measurements of length (or weight) differ among the 3 observers? &lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2010-10-16T01:44:55.913" FavoriteCount="3" Id="3661" LastActivityDate="2010-10-18T07:17:51.227" LastEditDate="2010-10-16T08:43:26.653" LastEditorUserId="88" OwnerUserId="1603" PostTypeId="1" Score="5" Tags="&lt;anova&gt;&lt;error&gt;&lt;measurement&gt;&lt;reliability&gt;&lt;inter-rater&gt;" Title="Repeatability and measurement error from and between observers" ViewCount="1093" />
  
  <row Body="&lt;p&gt;What you describe is a reliability study where each subject is going to be assessed by the same three raters on two occasions. Analysis can be done separately on the two outcomes (length and weight, though I assume they will be highly correlated and you're not interested in how this correlation is reflected in raters' assessments). Estimating measurement reliability can be done in two ways: &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;The original approach (as described in Fleiss, 1987) relies on the analysis of variance components through an ANOVA table, where we assume no subject by rater interaction (the corresponding SS is constrained to 0) -- of course, you won't look at $p$-values, but at the MSs corresponding to relevant effects;&lt;/li&gt;&#10;&lt;li&gt;A mixed-effects model allows to derive variance estimates, considering time as a fixed effect and subject and/or rater as random-effect(s) (the latter distinction depends on whether you consider that your three observers were taken or sampled from a pool of potential raters or not -- if the rater effect is small, the two analyses will yield quite the same estimate for outcome reliability).&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;In both cases, you will be able to derive a single intraclass correlation coefficient, which is a measure of reliability of the assessments (under the Generalizability Theory, we would call them generalizability coefficients), which would answer your second question. The first question deals with a potential effect of time (considered as a fixed effect), which I discussed here, &lt;a href=&quot;http://stats.stackexchange.com/questions/1015/reliability-in-elicitation-exercise&quot;&gt;Reliability in Elicitation Exercise&lt;/a&gt;. More details can be found in Dunn (1989) or Brennan (2001).&lt;/p&gt;&#10;&#10;&lt;p&gt;I have an &lt;a href=&quot;http://gist.github.com/631183&quot; rel=&quot;nofollow&quot;&gt;R example script&lt;/a&gt; on Github which illustrates both approaches. I think it would not be too difficult to incorporate rater effects in the model.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Fleiss, J.L. (1987). &lt;em&gt;The design and analysis of clinical experiments&lt;/em&gt;. New York: Wiley.&lt;/li&gt;&#10;&lt;li&gt;Dunn, G. (1989). &lt;em&gt;Design and analysis of reliability studies&lt;/em&gt;. Oxford&lt;/li&gt;&#10;&lt;li&gt;Brennan, R.L. (2001). &lt;em&gt;Generalizability Theory&lt;/em&gt;. Springer&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2010-10-16T08:43:54.767" Id="3663" LastActivityDate="2010-10-18T07:17:51.227" LastEditDate="2010-10-18T07:17:51.227" LastEditorUserId="930" OwnerUserId="930" ParentId="3661" PostTypeId="2" Score="8" />
  <row Body="&lt;p&gt;Maybe I misunderstood the question, but what you are describing sounds like a test-retest reliability study on your Q scores. You have a series of experts each going to assess a number of items or questions, at two occasions (presumably fixed in time). So, basically you can assess the temporal stability of the judgments by computing an &lt;em&gt;intraclass correlation coefficient&lt;/em&gt; (ICC), which will give you an idea of the variance attributable to subjects in the variability of observed scores (or, in other words of the closeness of the observations on the same subject relative to the closeness of observations on different subjects).&lt;/p&gt;&#10;&#10;&lt;p&gt;The ICC may easily be obtained from a mixed-effect model describing the measurement $y_{ij}$ of subject $i$ on occasion $j$ as&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;y_{ij}=\mu+u_i+\varepsilon_{ij},\quad \varepsilon\sim\mathcal{N}(0,\sigma^2)&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $u_i$ is the difference between the overall mean and subject $i$'s mean measurement, and $\varepsilon_{ij}$ is the measurement error for subject $i$ on occasion $j$. Here, this is a random-effect model. Unlike a standard ANOVA with subjects as factor, we consider the $u_i$ as random (i.i.d.) effects, $u_i\sim\mathcal{N}(0,\tau^2)$, independent of the error terms. Each measurement differ from the overall mean $\mu$ by the sum of the two error terms, among which the $u_i$ is shared between occasion on the same subjects. The total variance is then $\tau^2+\sigma^2$ and the proportion of the total variance that is accounted for by the subjects is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$&#10;\rho=\frac{\tau^2}{\tau^2+\sigma^2}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;which is the ICC, or the reliability index from a psychometrical point of view.&#10;Note that this reliability is sample-dependent (as it depends on the between-subject variance). Instead of the mixed-effects model, we could derive the same results from a two-way ANOVA (subjects + time, as factors) and the corresponding Mean Squares. You will find additional references in those related questions: &lt;a href=&quot;http://stats.stackexchange.com/questions/3661/repeatability-and-measurement-error-from-and-between-observers/3663#3663&quot;&gt;Repeatability and measurement error from and between observers&lt;/a&gt;, and &lt;a href=&quot;http://stats.stackexchange.com/questions/3539/inter-rater-reliability-for-ordinal-or-interval-data/3546&quot;&gt;Inter-rater reliability for ordinal or interval data&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;In R, you can use the &lt;code&gt;icc()&lt;/code&gt; function from the &lt;a href=&quot;http://cran.r-project.org/web/packages/psy/index.html&quot; rel=&quot;nofollow&quot;&gt;psy&lt;/a&gt; package; the random intercept model described above corresponds to the &quot;agreement&quot; ICC, while incorporating the time effect as a fixed factor would yield the &quot;consistency&quot; ICC. You can also use the &lt;code&gt;lmer()&lt;/code&gt; function from the &lt;a href=&quot;http://cran.r-project.org/web/packages/lme4/index.html&quot; rel=&quot;nofollow&quot;&gt;lme4&lt;/a&gt; package, or the &lt;code&gt;lme()&lt;/code&gt; function from the &lt;a href=&quot;http://cran.r-project.org/web/packages/nlme/index.html&quot; rel=&quot;nofollow&quot;&gt;nlme&lt;/a&gt; package. The latter has the advantage that you can easily obtain 95% CIs for the variance components (using the &lt;code&gt;intervals()&lt;/code&gt; function). Dave Garson provided a nice overview (with SPSS illustrations) in &lt;a href=&quot;http://faculty.chass.ncsu.edu/garson/PA765/reliab.htm&quot; rel=&quot;nofollow&quot;&gt;Reliability Analysis&lt;/a&gt;, and &lt;a href=&quot;http://www.indiana.edu/~statmath/stat/all/hlm/hlm.pdf&quot; rel=&quot;nofollow&quot;&gt;Estimating Multilevel Models using SPSS, Stata, SAS, and R&lt;/a&gt; constitutes a useful tutorial, with applications in educational assessment. But the definitive reference is Shrout and Fleiss (1979), &lt;a href=&quot;http://www3.uta.edu/faculty/ricard/COED/Shrout%20&amp;amp;%20Fleiss%20%281979%29%20Intraclass%20correlations.pdf&quot; rel=&quot;nofollow&quot;&gt;Intraclass Correlations: Uses in Assessing Rater Reliability&lt;/a&gt;, &lt;em&gt;Psychological Bulletin&lt;/em&gt;, 86(2), 420-428.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have also added an &lt;a href=&quot;http://gist.github.com/631183&quot; rel=&quot;nofollow&quot;&gt;example R script&lt;/a&gt; on Githhub, that includes the ANOVA and mixed-effect approaches.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, should you add a constant value to all of the values taken at the second occasion, the Pearson correlation would remain identical (because it is based on deviations of the 1st and 2nd measurements from their &lt;em&gt;respective means&lt;/em&gt;), whereas the reliability as computed through the random intercept model (or the agreement ICC) would decrease.&lt;/p&gt;&#10;&#10;&lt;p&gt;BTW, Cronbach's alpha is not very helpful in this case because it is merely a measure of the internal consistency (yet, another form of &quot;reliability&quot;) of an unidimensional scale; it would have no meaning should it be computed on items underlying different constructs. Even if your questions survey a single domain, it's hard to imagine mixing the two series of measurements, and Cronbach's alpha should be computed on each set separately. Its associated 95% confidence interval (computed by bootstrap) should give an indication about the stability of the internal structure between the two test occasions.&lt;/p&gt;&#10;&#10;&lt;p&gt;As an example of applied work with ICC, I would suggest&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Johnson, SR, Tomlinson, GA, Hawker,&#10;  GA, Granton, JT, Grosbein, HA, and&#10;  Feldman, BM (2010). &lt;a href=&quot;http://www.sciencedirect.com/science?_ob=ArticleURL&amp;amp;_udi=B6T84-4XR5W4K-1&amp;amp;_user=10&amp;amp;_coverDate=04%2F30%2F2010&amp;amp;_rdoc=1&amp;amp;_fmt=high&amp;amp;_orig=search&amp;amp;_origin=search&amp;amp;_sort=d&amp;amp;_docanchor=&amp;amp;view=c&amp;amp;_searchStrId=1505690480&amp;amp;_rerunOrigin=google&amp;amp;_acct=C000050221&amp;amp;_version=1&amp;amp;_urlVersion=0&amp;amp;_userid=10&amp;amp;md5=f532362cc8e30e572daf72564197eab1&amp;amp;searchtype=a&quot; rel=&quot;nofollow&quot;&gt;A valid and&#10;  reliable belief elicitation method for&#10;  Bayesian priors&lt;/a&gt;. &lt;em&gt;Journal of&#10;  Clinical Epidemiology&lt;/em&gt;, 63(4),&#10;  370-383.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="0" CreationDate="2010-10-16T18:41:49.013" Id="3667" LastActivityDate="2010-11-01T16:32:27.973" LastEditDate="2010-11-01T16:32:27.973" LastEditorUserId="930" OwnerUserId="930" ParentId="1015" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;See this SO question: &lt;a href=&quot;http://stackoverflow.com/questions/2218395/how-do-you-compare-the-similarity-between-two-dendrograms-in-r&quot;&gt;http://stackoverflow.com/questions/2218395/how-do-you-compare-the-similarity-between-two-dendrograms-in-r&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-10-17T14:37:56.007" Id="3674" LastActivityDate="2010-10-17T14:37:56.007" OwnerUserId="88" ParentId="3672" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;There was a read paper last week at the Royal Statistical Society on MCMC techniques over Riemann  manifolds, primarily using the Fisher information metric:&#10;&lt;a href=&quot;http://www.rss.org.uk/main.asp?page=1836#Oct_13_2010_Meeting&quot; rel=&quot;nofollow&quot;&gt;http://www.rss.org.uk/main.asp?page=1836#Oct_13_2010_Meeting&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The results seem promising, though as the authors point out, in many models of interest (such as mixture models) the Fisher information has no analytic form.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-10-17T20:07:00.620" Id="3680" LastActivityDate="2010-10-17T20:12:38.687" LastEditDate="2010-10-17T20:12:38.687" LastEditorUserId="495" OwnerUserId="495" ParentId="1621" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="3712" AnswerCount="2" Body="&lt;p&gt;I posted this question earlier and am rewriting it in hopes of getting some guidance. I am using a weighted regression (after propensity score matching) to obtain estimates of the effects of a treatment (treat) on an outcome for different income quintiles. I include interactions of the quintiles and the treatment in the equation, as shown below:  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;regress outcome treat quintile2 quin3 quin4 quin5 treatXquin2 treatXquin3 treatXquin4   treatXquin5&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;To obtain the effect of treatment on quintile2 I add the coefficient on treat with the coefficient on treatXquin2. However, what do I do with the standard errors? Can I simply add the standard errors on the two coefficients together? Can I add the t-stats together? &lt;/p&gt;&#10;&#10;&lt;p&gt;Any advice would be much appreciated. I'm not a stats expert but have a basic understanding of econometrics. &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2010-10-18T14:21:06.687" Id="3704" LastActivityDate="2010-10-18T22:08:54.650" LastEditDate="2010-10-18T17:48:59.000" LastEditorUserId="449" OwnerUserId="834" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;standard-error&gt;&lt;multivariate-analysis&gt;&lt;stata&gt;" Title="Adding coefficients to obtain interaction effects - can I add standard errors?" ViewCount="2056" />
  <row Body="&lt;p&gt;Given the nature of your data I would suggest you investigate the use of &lt;a href=&quot;http://en.wikipedia.org/wiki/Exponential_smoothing&quot; rel=&quot;nofollow&quot;&gt;exponential smoothing&lt;/a&gt; as well as fitting ARIMA type models, especially due to the temporal constraints within your data. Although I wouldn't doubt spatial dependencies exist, I would be abit skeptical about their usefulness in forecasting (in what I would imagine are fairly large areas), especially since any spatial dependency will likely be already captured (at least to a certain extent) in previous observations in the series.&lt;/p&gt;&#10;&#10;&lt;p&gt;Where the spatial dependencies may be helpful is if you have small area estimation problems, and you can use the spatial dependency in your data to help smooth out your estimations in those noisy geographic regions. This may not be a problem though since you have aggregated data for a full year.&lt;/p&gt;&#10;&#10;&lt;p&gt;You shouldn't take my word for it though, and should investigate economics literature on the subject and assess various forecasting methods yourself. Its quite possible other variables are useful predictors of future unemployment in similar panel settings.&lt;/p&gt;&#10;&#10;&lt;p&gt;Edit:&lt;/p&gt;&#10;&#10;&lt;p&gt;First I'd like to clarify that I did not mean that the OP should simply prefer some type of exponential smoothing over other techniques. I think the OP should assess performance of various forecasting methods using a hold out sample of 1 or 2 time periods. I do not know the literature for forecasting unemployment, but I have not seen any method so obviously superior that others should be dismissed outright in any context. &lt;/p&gt;&#10;&#10;&lt;p&gt;Kwak mentions a key point I did not consider initially (and Stephan's comment makes the same point very succinctly as well). The panel nature of the data allows one to estimate an auto-regressive compenent in the model much easier than in a single time series. So I would follow his suggestion and consider the A/B estimator a good bet to provide the best forecast accuracy. &lt;/p&gt;&#10;&#10;&lt;p&gt;I'm still sticking with my initial suggestion though that I am skeptical of the usefulness of the spatial dependence, and one should assess a models predictive accuracy with and without the spatial component. In terms of prediction it is not simply whether some sort of spatial auto-correlation exists, it is whether that spatial auto-correlation is useful in predicting future values independent of past observations in the series.&lt;/p&gt;&#10;&#10;&lt;p&gt;For simplifying my reasoning, lets denote&lt;/p&gt;&#10;&#10;&lt;p&gt;$R_{t}$ corresponds to a geographic region $R$ at time $t$&lt;/p&gt;&#10;&#10;&lt;p&gt;$R_{t-1}$ corresponds to a geographic region $R$ at the previous time period&lt;/p&gt;&#10;&#10;&lt;p&gt;$W_{t-1}$ corresponds to however one wants to define the spatial relationship for for the neighbors of $R_{t}$ at the previous time period&lt;/p&gt;&#10;&#10;&lt;p&gt;In this case $R$ is some attribute and $W$ is that same attribute in the neighbors of $R$ (i.e. an endogenous spatial lag.)&lt;/p&gt;&#10;&#10;&lt;p&gt;In pretty much all cases of lattice areal data, we have a relationship between $R$ and $W$. Two general explanations for this relationship are&lt;/p&gt;&#10;&#10;&lt;p&gt;1) The General Social Process Theory&lt;/p&gt;&#10;&#10;&lt;p&gt;This is when there are processes that affect $R$ and $W$ simultaneously that result in similar values with some sort of spatial organization. The support of the data does not distinguish between the forces that shape attributes in a broader scope than the areal units encompass. (I imagine there is a better name for this, so if someone could help me out.)&lt;/p&gt;&#10;&#10;&lt;p&gt;2) The Spatial Externalities Theory&lt;/p&gt;&#10;&#10;&lt;p&gt;This is when some attribute of $W$ directly affects an attribute of $R$. Srikant's example of job diffusion is an example of this.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the context of forecasting, the general social process model may not be all that helpful in forecasting. In this case, $R_{t-1}$ and $W_{t-1}$ are reflective of the same external shocks, and so $W_{t-1}$ is less likely to have exogenous power to predict $R_{t}$ independent of $R_{t-1}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;IMO the spatial externalities case I would expect $W_{t-1}$ to have a greater potential to forecast $R_{t}$ independent of $R_{t-1}$ in the short run because $R_{t-1}$ and $W_{t-1}$ can be reflective of different external shocks to the system. This is my opinion though and you typically can't distinguish between the general social process model and the spatial externalities model through empirical means in a cross sectional design (they are probably both occurring to a certain extent in many contexts). Hence I would attempt to validate its usefulness before simply incorporating it into the forecast. Better knowledge of the literature and social processes would definately be helpful here to guide your model building. In criminology only in a very limited set of circumstances does the externalities model make sense (but I imagine it is more likely in economics data). Models of spatial hedonic housing prices often show very strong spatial effects, and in that context I would expect the spatial component to have a strong ability to forecast housing prices. (I like Luc Anselin's explanation of these two different processes better than mine in &lt;a href=&quot;http://dx.doi.org/10.1007/978-3-7908-2070-6_2&quot; rel=&quot;nofollow&quot;&gt;this&lt;/a&gt; paper, PDF &lt;a href=&quot;https://www.ace.uiuc.edu/reap/SPARC/2007-01_AnselinLozano.pdf&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;)&lt;/p&gt;&#10;&#10;&lt;p&gt;Often how we define $W$ is a further problem in this setting. Most conceptions of $W$ are very simplistic and probably aren't entirely reflective of real geographic processes. Here kwaks suggestion of adding a random component to the $W$ effect for each $R$ makes alot of sense. An example would be we would expect New York City to influence its neighbors, but we wouldn't expect NYC's neighbors to have all that much influence on NYC. This still doesn't solve how to either decide what is a neighbor or how to best represent the effects of neighbors. What kwak suggests is essential a local version of Geary's C (spatial differences), local Moran's I (spatial averages) is a common approach as well.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm still alittle surprised at the negative responses to my suggestion to use simpler smoothing methods (even if they are meant for univariate time series). Am I naive to think exponential smoothing or some other type of moving window technique won't perform at least comparably well enough to more complicated procedures to assess it? I would be more worried if the series were such that we would expect seasonal components, but that is not the case here.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2010-10-18T18:36:53.263" Id="3717" LastActivityDate="2010-10-19T15:23:18.643" LastEditDate="2010-10-19T15:23:18.643" LastEditorUserId="1036" OwnerUserId="1036" ParentId="3708" PostTypeId="2" Score="3" />
  
  <row AcceptedAnswerId="3789" AnswerCount="3" Body="&lt;p&gt;What's the best way to approximate $Pr[n \leq X \leq m]$ for two given integers $m,n$ when you know the mean $\mu$, variance $\sigma^2$, skewness $\gamma_1$ and excess kurtosis $\gamma_2$ of a discrete distribution $X$, and it is clear from the (non-zero) measures of shape $\gamma_1$ and $\gamma_2$ that a normal approximation is not appropriate?&lt;/p&gt;&#10;&#10;&lt;p&gt;Ordinarily, I would use a normal approximation with integer correction...&lt;/p&gt;&#10;&#10;&lt;p&gt;$Pr[(n - \text{½})\leq X \leq (m + \text{½})]&#10; = Pr[\frac{(n - \text{½})-\mu}{\sigma}\leq Z \leq \frac{(m + \text{½})-\mu}{\sigma}]&#10; = \Phi(\frac{(m + \text{½})-\mu}{\sigma}) - \Phi(\frac{(n - \text{½})-\mu}{\sigma})$&lt;/p&gt;&#10;&#10;&lt;p&gt;...if the skewness and excess kurtosis were (closer to) 0, but that's not the case here. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have to perform multiple approximations for different discrete distributions with different values of $\gamma_1$ and $\gamma_2$. So I'm interested in finding out if there is an established a procedure that uses $\gamma_1$ and $\gamma_2$ to select a better approximation than the normal approximation.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-10-18T22:19:27.650" FavoriteCount="1" Id="3727" LastActivityDate="2010-10-20T08:32:13.313" LastEditDate="2010-10-19T17:31:18.077" LastEditorUserId="449" OwnerDisplayName="A. N. Other" PostTypeId="1" Score="9" Tags="&lt;distributions&gt;&lt;probability&gt;&lt;normality&gt;&lt;approximation&gt;&lt;moments&gt;" Title="Approximating $Pr[n \leq X \leq m]$ for a discrete distribution" ViewCount="160" />
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;This question is prompted by &lt;a href=&quot;http://stats.stackexchange.com/q/3556/159&quot;&gt;discussion elsewhere&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Variable kernels are often used in local regression. For example, loess is widely used and works well as a regression smoother, and is based on a kernel of variable width that adapts to data sparsity.&lt;/p&gt;&#10;&#10;&lt;p&gt;On the other hand, variable kernels are usually thought to lead to poor estimators in kernel density estimation (see &lt;a href=&quot;http://www.jstor.org/pss/2242011&quot;&gt;Terrell and Scott, 1992&lt;/a&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there an intuitive reason why they would work well for regression but not for density estimation?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2010-10-19T11:35:15.373" FavoriteCount="2" Id="3752" LastActivityDate="2013-02-15T04:25:57.767" OwnerUserId="159" PostTypeId="1" Score="12" Tags="&lt;nonparametric&gt;&lt;smoothing&gt;&lt;kde&gt;&lt;kernel&gt;&lt;loess&gt;" Title="If variable kernel widths are often good for kernel regression, why are they generally not good for kernel density estimation?" ViewCount="404" />
  <row Body="&lt;p&gt;Way back in 1965, Sir Austin Bradford Hill wrote &lt;a href=&quot;http://www.edwardtufte.com/tufte/hill&quot; rel=&quot;nofollow&quot;&gt;a great essay&lt;/a&gt; about something very akin to the Pyramid of Evidence, where he discussed how the piling up of evidence can increase our confidence in hypotheses of causality in Medicine.&lt;/p&gt;&#10;&#10;&lt;p&gt;Most of the factors he discusses can be applied to Economics and political sciences.&lt;/p&gt;&#10;" CommentCount="1" CommunityOwnedDate="2010-10-19T14:34:44.807" CreationDate="2010-10-19T14:34:44.807" Id="3760" LastActivityDate="2010-10-19T14:34:44.807" OwnerUserId="666" ParentId="3101" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;Since you're on 64-bit Windows, make sure that you have installed and are running the 64-bit version of R for Windows.  Then, follow the instructions on Gary King's page:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://gking.harvard.edu/zelig/docs/How_do_I2.html&quot; rel=&quot;nofollow&quot;&gt;How do I increase the memory for R?&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2010-10-19T16:35:03.540" Id="3768" LastActivityDate="2010-10-19T16:35:03.540" OwnerUserId="251" ParentId="3754" PostTypeId="2" Score="1" />
  
  
  
  
  <row AcceptedAnswerId="3877" AnswerCount="2" Body="&lt;p&gt;How does one calculate Cohen's d and confidence intervals after logit in Stata?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-10-20T20:51:59.437" Id="3826" LastActivityDate="2010-10-22T07:24:42.113" OwnerDisplayName="Fred Vars" PostTypeId="1" Score="4" Tags="&lt;confidence-interval&gt;&lt;stata&gt;&lt;effect-size&gt;&lt;cohens-d&gt;" Title="How does one calculate Cohen's d and confidence intervals after logit in Stata?" ViewCount="3766" />
  
  
  
  <row AcceptedAnswerId="3866" AnswerCount="1" Body="&lt;p&gt;I have two years of data which looks basically like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;Date   &lt;strong&gt;&lt;em&gt;_&lt;/em&gt;__&lt;em&gt;&lt;/strong&gt;    Violence Y/N? _&lt;/em&gt;  Number of patients&lt;/p&gt;&#10;&#10;&lt;p&gt;1/1/2008    &lt;strong&gt;&lt;em&gt;_&lt;/em&gt;___&lt;em&gt;&lt;/strong&gt;    0  &lt;strong&gt;&lt;/em&gt;__&lt;em&gt;_&lt;/em&gt;__&lt;em&gt;_&lt;/em&gt;____&lt;/strong&gt; 11&lt;/p&gt;&#10;&#10;&lt;p&gt;2/1/2008 &lt;strong&gt;&lt;em&gt;_&lt;/em&gt;__&lt;em&gt;_&lt;/em&gt;&lt;/strong&gt;       0  &lt;strong&gt;&lt;em&gt;_&lt;/em&gt;__&lt;em&gt;_&lt;/em&gt;__&lt;em&gt;_&lt;/em&gt;__&lt;/strong&gt; 11&lt;/p&gt;&#10;&#10;&lt;p&gt;3/1/2008 &lt;strong&gt;&lt;em&gt;_&lt;/em&gt;____&lt;/strong&gt;&lt;em&gt;1  &lt;strong&gt;&lt;/em&gt;__&lt;em&gt;_&lt;/em&gt;__&lt;em&gt;_&lt;/em&gt;____&lt;/strong&gt; 12&lt;/p&gt;&#10;&#10;&lt;p&gt;4/1/2008 &lt;strong&gt;&lt;em&gt;_&lt;/em&gt;____&lt;/strong&gt;&lt;em&gt;0  &lt;strong&gt;&lt;/em&gt;__&lt;em&gt;_&lt;/em&gt;__&lt;em&gt;_&lt;/em&gt;____&lt;/strong&gt; 12&lt;/p&gt;&#10;&#10;&lt;p&gt;...&lt;/p&gt;&#10;&#10;&lt;p&gt;31/12/2009_&lt;strong&gt;&lt;em&gt;_&lt;/em&gt;__&lt;/strong&gt;      0_&lt;strong&gt;&lt;em&gt;_&lt;/em&gt;__&lt;em&gt;_&lt;/em&gt;__&lt;em&gt;_&lt;/em&gt;__&lt;/strong&gt;                 14&lt;/p&gt;&#10;&#10;&lt;p&gt;i.e. two years of observations, one per day, of a psychiatric ward, which indicate whether there was a violence incident on that day (1 is yes, 0 no) as well as the number of patients on the ward. The hypothesis that we wish to test is that more patients on the ward is associated with an increased probability of violence on the ward.&lt;/p&gt;&#10;&#10;&lt;p&gt;We realise, of course, that we will have to adjust for the fact that when there are more patients on the ward, violence is more likely because there are just more of them- we are interested in whether each individual’s probability of violence goes up when there are more patients on the ward.&lt;/p&gt;&#10;&#10;&lt;p&gt;I've seen several papers which just use logistic regression, but I think that is wrong because there is an autoregressive structure (although, looking at the autocorrelation function, it doesn’t get above .1 at any lag, although this is above the “significant” blue dashed line that R draws for me).&lt;/p&gt;&#10;&#10;&lt;p&gt;Just to make things more complicated, I can if I wish to break down the results into individual patients, so the data would look just as it does above, except I would have the data for each patient, 1/1/2008, 2/1/2008 etc. and an ID code going down the side so the data would show the whole history of incidents for each patient separately (although not all patients are present for all days, not sure whether that matters).&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to use lme4 in R to model the autoregressive structure within each patient, but some Googling comes up with the quotation “lme4 is not set up to deal with autoregressive structures”. Even if it were, I’m not sure I grasp how to write the code anyway.&lt;/p&gt;&#10;&#10;&lt;p&gt;Just in case anyone notices, I asked a question like this a while ago, they are different datasets with different problems, although actually solving this problem will help with that one (someone suggested I use mixed methods previously, but this autoregression thing has made me unsure how to do this).&lt;/p&gt;&#10;&#10;&lt;p&gt;So I’m a bit stuck and lost to be honest. Any help gratefully received!&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-10-21T11:07:03.723" FavoriteCount="1" Id="3841" LastActivityDate="2010-10-21T19:22:27.857" OwnerUserId="199" PostTypeId="1" Score="10" Tags="&lt;r&gt;&lt;mixed-model&gt;&lt;autocorrelation&gt;&lt;longitudinal&gt;" Title="Two years of data describing occurence of violence- testing association with number of patients on ward" ViewCount="133" />
  
  <row Body="&lt;p&gt;David Collett. &lt;em&gt;Modelling Survival Data in Medical Research&lt;/em&gt;, Second Edition. Chapman &amp;amp; Hall/CRC. 2003. &lt;a href=&quot;http://en.wikipedia.org/wiki/Special%3aBookSources/9781584883258&quot; rel=&quot;nofollow&quot;&gt;ISBN 978-1584883258&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Software section focuses on SAS not R though.&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2010-10-21T14:42:28.390" CreationDate="2010-10-21T14:42:28.390" Id="3850" LastActivityDate="2010-10-21T14:42:28.390" OwnerUserId="449" ParentId="1053" PostTypeId="2" Score="3" />
  
  
  <row Body="&lt;p&gt;Here is an idea that connects your binary dependent variable to a continuous, unobserved variable; a connection that may let you leverage the power of time series models for continuous variables. &lt;/p&gt;&#10;&#10;&lt;p&gt;Define:&lt;/p&gt;&#10;&#10;&lt;p&gt;$V_{w,t} = 1$ if violent incident happened in ward $w$ during time period $t$ and 0 otherwise&lt;/p&gt;&#10;&#10;&lt;p&gt;$P_{w,t}$ : Propensity for violence in ward $w$ at time $t$.&lt;/p&gt;&#10;&#10;&lt;p&gt;$P_{w,t}$ is assumed to be a continuous variable that in some sense represents 'pent-up' feelings of the inmates which boil over at some time and results in violence. Following this reasoning, we have:&lt;/p&gt;&#10;&#10;&lt;p&gt;$V_{w,t} = \begin{cases} 1 &amp;amp; \mbox{if } P_{w,t} \ge \tau \\&#10;                         0 &amp;amp; \mbox{otherwise}&#10;           \end{cases}$&lt;/p&gt;&#10;&#10;&lt;p&gt;where,&lt;/p&gt;&#10;&#10;&lt;p&gt;$\tau$ is an unobserved threshold which triggers a violent act.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can then use a time series model for $P_{w,t}$ and estimate the relevant parameters. For example, you could model $P_{w,t}$ as:&lt;/p&gt;&#10;&#10;&lt;p&gt;$P_{w,t} = \alpha_0 + \alpha_1 P_{w,t-1} + ... + \alpha_p P_{w,t-p}+ \beta n_{w,t} + \epsilon_t$&lt;/p&gt;&#10;&#10;&lt;p&gt;where,&lt;/p&gt;&#10;&#10;&lt;p&gt;$n_{w,t}$ is the number of patients in ward $w$ at time $t$.&lt;/p&gt;&#10;&#10;&lt;p&gt;You could see if $\beta$ is significantly different from 0 to test your hypothesis that &quot;more patients lead to an increase in probability of violence&quot;. &lt;/p&gt;&#10;&#10;&lt;p&gt;The challenge of the above model specification is that you do not really observe $P_{w,t}$ and thus the above is not your usual time series model. I do not know anything about R so perhaps someone else will chip in if there is a package that would let you estimate models like the above.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-10-21T19:22:27.857" Id="3866" LastActivityDate="2010-10-21T19:22:27.857" OwnerDisplayName="user28" ParentId="3841" PostTypeId="2" Score="2" />
  
  
  
  
  
  <row Body="&lt;p&gt;Ok, I finally found them &lt;a href=&quot;http://finzi.psych.upenn.edu/R/library/IQCC/html/00Index.html&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-10-22T13:22:57.057" Id="3886" LastActivityDate="2010-10-22T13:22:57.057" OwnerUserId="339" ParentId="3885" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;That's because the package doesn't exist on CRAN (see &lt;a href=&quot;http://cran.r-project.org/web/packages/&quot;&gt;the package list&lt;/a&gt;).  It may have failed a build in a recent version of R.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can install it directly from the author's site like so:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;install.packages(c(&quot;R.basic&quot;), contriburl=&quot;http://www.braju.com/R/repos/&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;See &lt;a href=&quot;http://www.braju.com/R/&quot;&gt;Henrik Bengtsson's home page&lt;/a&gt; for more detail.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Edit&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Just to add a little further: it looks like this package fails to build on later versions of R.  You should probably get the source and build it yourself, or else contact the package author.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2010-10-22T14:12:55.267" Id="3891" LastActivityDate="2010-10-22T14:21:15.993" LastEditDate="2010-10-22T14:21:15.993" LastEditorUserId="5" OwnerUserId="5" ParentId="3890" PostTypeId="2" Score="6" />
  <row AcceptedAnswerId="3895" AnswerCount="3" Body="&lt;p&gt;Why does OLS estimation involve taking vertical deviations of the points to the line rather than horizontal distances?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-10-22T15:08:17.383" Id="3892" LastActivityDate="2011-01-19T04:56:27.243" OwnerUserId="333" PostTypeId="1" Score="9" Tags="&lt;least-squares&gt;" Title="Why vertical distances?" ViewCount="1032" />
  <row AcceptedAnswerId="4095" AnswerCount="7" Body="&lt;p&gt;In all contexts I am familiar with cross-validation it is solely used with the goal of increasing predictive accuracy. Can the logic of cross validation be extended in estimating the unbiased relationships between variables? &lt;/p&gt;&#10;&#10;&lt;p&gt;While &lt;a href=&quot;http://dx.doi.org/10.1007/s10940-009-9077-7&quot;&gt;this&lt;/a&gt; paper by Richard Berk demonstrates the use of a hold out sample for parameter selection in the &quot;final&quot; regression model (and demonstrates why step-wise parameter selection is not a good idea), I still don't see how that exactly ensures unbiased estimates of the effect X has on Y any more so than choosing a model based on logic and prior knowledge of the subject.&lt;/p&gt;&#10;&#10;&lt;p&gt;I ask that people cite examples in which one used a hold-out sample to aid in causal inference, or general essays that may help my understanding. I also don't doubt my conception of cross validation is naive, and so if it is say so. It seems offhand the use of a hold out sample would be amenable to causal inference, but I do not know of any work that does this or how they would do this. &lt;/p&gt;&#10;&#10;&lt;p&gt;Citation for the Berk Paper:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://dx.doi.org/10.1007/s10940-009-9077-7&quot;&gt;Statistical Inference After Model Selection&lt;/a&gt;&#10;by: Richard Berk, Lawrence Brown, Linda Zhao&#10;Journal of Quantitative Criminology, Vol. 26, No. 2. (1 June 2010), pp. 217-236. &lt;/p&gt;&#10;&#10;&lt;p&gt;PDF version &lt;a href=&quot;http://www-stat.wharton.upenn.edu/~lzhao/papers/MyPublication/StatInfAfterMS_JQC_2010.pdf&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://stats.stackexchange.com/q/3252/1036&quot;&gt;This&lt;/a&gt; question on exploratory data analysis in small sample studies by chl prompted this question. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-10-22T15:12:03.423" FavoriteCount="6" Id="3893" LastActivityDate="2012-05-02T18:57:33.890" LastEditDate="2010-10-22T19:40:14.977" LastEditorUserId="1036" OwnerUserId="1036" PostTypeId="1" Score="29" Tags="&lt;cross-validation&gt;&lt;causal-inference&gt;" Title="Can cross validation be used for causal inference?" ViewCount="1366" />
  <row AcceptedAnswerId="3916" AnswerCount="2" Body="&lt;p&gt;I am trying to assess a 20-item multliple choice test.  I want to perform an item analysis such as can be found in &lt;a href=&quot;http://www.utexas.edu/academic/ctl/assessment/iar/students/report/itemanalysis-example.php&quot; rel=&quot;nofollow&quot;&gt;this example&lt;/a&gt;.  So for each question I want the P-value and the correlation with the total, and the distribution of the options selected.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I don't know anything about the various statistical software packages out there, but I'd like to use R as I'm comfortable with programming and R is open source.  The pseudo-workflow I envision is:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;prepare data in excel and export to CSV &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;load data in R&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;load a package that does what I need&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;execute that package's commands &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;export and report.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I am confident with 1 and 2 but having trouble with 3, probably because I don't have the statistical vocabulary to compare the packages I browsed on CRAN.  &lt;code&gt;ltm&lt;/code&gt; looks like it could be the right package but I can't tell.  Whatever package is used, what would the commands be?&lt;/p&gt;&#10;&#10;&lt;p&gt;Side question: in the linked example, what do you suppose MC and MI stand for?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-10-23T11:19:34.040" Id="3915" LastActivityDate="2010-10-25T13:55:00.963" LastEditDate="2010-10-25T13:55:00.963" LastEditorUserId="930" OwnerUserId="1681" PostTypeId="1" Score="6" Tags="&lt;r&gt;&lt;correlation&gt;&lt;psychometrics&gt;&lt;scales&gt;" Title="Item Analysis for an R newbie" ViewCount="1234" />
  <row Body="&lt;p&gt;Just speaking on a practical level, in my discipline (psychology) I have never seen this done for pure factor analysis.&lt;/p&gt;&#10;&#10;&lt;p&gt;That being said, the significance (fit really) of a statistical model is normally tested by the use of Structural Equation Modelling, where you attempt to reproduce the observed matrix of data from the structure you have proposed through the use of factor analysis. &lt;/p&gt;&#10;&#10;&lt;p&gt;The SEM, lavaan or OpenMx Packages for R will all do this. &lt;/p&gt;&#10;&#10;&lt;p&gt;Technically, the Chi square test will tell you if a factor model fits perfectly, but this statistic is almost always significant with any appreciable (200+) sample size. &lt;/p&gt;&#10;&#10;&lt;p&gt;The psych package for R also gives you the Bayesian Information Criterion as a measure of fit after you specify a factor model, but I am unsure as to how useful this is. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-10-23T13:33:45.217" Id="3919" LastActivityDate="2010-10-23T13:33:45.217" OwnerUserId="656" ParentId="3898" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;A common one is that the definition of variance (of a distribution) is the second moment recentered around a &lt;em&gt;known, definite&lt;/em&gt; mean, whereas the estimator uses an &lt;em&gt;estimated&lt;/em&gt; mean.  This loss of a degree of freedom (given the mean, you can reconstitute the dataset with knowledge of just $n-1$ of the data values) requires the use of $n-1$ rather than $n$ to &quot;adjust&quot; the result.&lt;/p&gt;&#10;&#10;&lt;p&gt;Such an explanation is consistent with the estimated variances in ANOVA and variance components analysis.  It's really just a special case.&lt;/p&gt;&#10;&#10;&lt;p&gt;The need to make &lt;em&gt;some&lt;/em&gt; adjustment that inflates the variance can, I think, be made intuitively clear with a valid argument that isn't just &lt;em&gt;ex post facto&lt;/em&gt; hand-waving.  (I recollect that Student may have made such an argument in his 1908 paper on the t-test.)  Why the adjustment to the variance should be &lt;em&gt;exactly&lt;/em&gt; a factor of $n/(n-1)$ is harder to justify, especially when you consider that the adjusted SD is &lt;em&gt;not&lt;/em&gt; an unbiased estimator.  (It is merely the square root of an unbiased estimator of the variance.  Being unbiased usually does not survive a nonlinear transformation.)  So, in fact, the correct adjustment to the SD to remove its bias is &lt;em&gt;not&lt;/em&gt; a factor of $\sqrt{n/(n-1)}$ at all!&lt;/p&gt;&#10;&#10;&lt;p&gt;Some introductory textbooks don't even bother introducing the adjusted sd: they teach one formula (divide by $n$).  I first reacted negatively to that when teaching from such a book but grew to appreciate the wisdom: to focus on the concepts and applications, the authors strip out all inessential mathematical niceties.  It turns out that nothing is hurt and nobody is misled.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2010-10-23T22:21:58.407" Id="3932" LastActivityDate="2010-10-23T22:21:58.407" OwnerUserId="919" ParentId="3931" PostTypeId="2" Score="16" />
  
  <row Body="&lt;p&gt;This is a total intuition, but the simplest answer is that is a correction made to make standard deviation of one-element sample undefined rather than 0.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2010-10-24T10:28:15.323" Id="3941" LastActivityDate="2010-10-24T10:28:15.323" OwnerUserId="88" ParentId="3931" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;I found that Spearman correlation is mostly used in place of usual linear correlation when working with integer valued scores on a measurement scale, when it has a moderate number of possible scores or when we don't want to make rely on assumptions about the bivariate relationships. As compared to Pearson coefficient, the interpretation of Kendall's tau seems to me less direct than that of Spearman's rho, in the sense that it quantifies the difference between the % of concordant and discordant pairs among all possible pairwise events. In my understanding, Kendall's tau more closely resembles &lt;a href=&quot;http://en.wikipedia.org/wiki/Gamma_test_%28statistics%29&quot;&gt;Goodman-Kruskal Gamma&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;I just browsed an article from Larry Winner in the J. Statistics Educ. (2006) which discusses the use of both measures, &lt;a href=&quot;http://www.amstat.org/publications/jse/v14n3/datasets.winner.html&quot;&gt;NASCAR Winston Cup Race Results for 1975-2003&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;I also found &lt;a href=&quot;http://stats.stackexchange.com/questions/3730/pearsons-or-spearmans-correlation-with-non-normal-data/3744#3744&quot;&gt;@onestop&lt;/a&gt; answer about &lt;a href=&quot;http://stats.stackexchange.com/questions/3730/pearsons-or-spearmans-correlation-with-non-normal-data&quot;&gt;Pearson's or Spearman's correlation with non-normal data&lt;/a&gt; interesting in this respect.&lt;/p&gt;&#10;&#10;&lt;p&gt;Of note, Kendall's tau (the &lt;em&gt;a&lt;/em&gt; version) has connection to Somers' D (and Harrell's C) used for predictive modelling (see e.g., &lt;a href=&quot;http://www.imperial.ac.uk/nhli/r.newson/miscdocs/intsomd1.pdf&quot;&gt;Interpretation of Somers’ D under four simple models&lt;/a&gt; by RB Newson and reference &lt;a href=&quot;http://www.jstatsoft.org/v15/i01/paper&quot;&gt;6&lt;/a&gt; therein, and articles by Newson published in the Stata Journal 2006). An overview of rank-sum tests is provided in &lt;a href=&quot;http://www.jstatsoft.org/v15/i01/paper&quot;&gt;Efficient Calculation of Jackknife Confidence Intervals for Rank Statistics&lt;/a&gt;, that was published in the JSS (2006).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-10-24T14:26:35.747" Id="3946" LastActivityDate="2010-10-31T10:29:50.693" LastEditDate="2010-10-31T10:29:50.693" LastEditorUserId="930" OwnerUserId="930" ParentId="3943" PostTypeId="2" Score="22" />
  <row Body="&lt;p&gt;Onestop explained it pretty well, I'll just give a simple R example with made up data. Say x is weight and y is height, and we want to find out if there's a difference between males and females:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(69)&#10;x &amp;lt;- rep(1:10,2)&#10;y &amp;lt;- c(jitter(1:10, factor=4), (jitter(1:10, factor=4)+2))&#10;sex &amp;lt;- rep(c(&quot;f&quot;, &quot;m&quot;), each=10)&#10;df1 &amp;lt;- data.frame(x,y,sex)&#10;with(df1, plot(y~x, col=c(1,2)[sex]))&#10;lm1 &amp;lt;- lm(y~sex, data=df1)&#10;lm2 &amp;lt;- lm(y~sex+x, data=df1)&#10;anova(lm1); anova(lm2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;You can see that without controlling for weight (in anova(lm1)) there's very little difference between the sexes, but when weight is included as a covariate (controlled for in lm2) then the difference becomes more apparent.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#In case you want to add the fitted lines to the plot&#10;coefs2 &amp;lt;- coef(lm2)&#10;abline(coefs2[1], coefs2[3], col=1)&#10;abline(coefs2[1]+coefs2[2], coefs2[3], col=2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2010-10-24T17:43:38.477" Id="3953" LastActivityDate="2010-10-24T17:43:38.477" OwnerUserId="966" ParentId="3944" PostTypeId="2" Score="7" />
  <row Body="&lt;p&gt;The answers are no, not for all $r$ in general; yes, for a restricted range of $r$ that is readily computed; but there remain a wide set of choices to be made.&lt;/p&gt;&#10;&#10;&lt;p&gt;I will use a standard notation where the action of a permutation $\sigma$ is written $ X^\sigma_i = X_{\sigma (i)}$ and the set of all permutations of the $n$ coordinates is $S_n$.&lt;/p&gt;&#10;&#10;&lt;p&gt;As you note in the question, upon standardizing $X$ it suffices to investigate $\mathbb{E}[{X^\sigma}'X]$.  Because $X'X = 1$, a correlation of $r = 1$ is certainly attainable by means of the identity permutation $\epsilon$ (where $\epsilon(i) = i$ for all $i$).  However, for any given $X$ there is a minimum attainable correlation: it is realized by associating the $k^\text{th}$ smallest component of $X^\sigma$ with the $k^\text{th}$ largest component of $X$.  For example, with $X = (-2,1,1)/\sqrt{6}$ the smallest possible correlation of $-1/2$ is achieved by $X^\sigma = (1,1,-2)/\sqrt{6}$.  Let's call this minimum correlation $r_{min}(X)$ and let $\sigma_{min}(X)$ be any permutation achieving this minimum value.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Every&lt;/em&gt; possible expected correlation of value between $r_{min}(X)$ and $1$ is attainable by means of a distribution supported on just $\sigma_{min}$ and $\epsilon$.  Specifically, set&lt;/p&gt;&#10;&#10;&lt;p&gt;$$p = \frac{r - r_{min}}{1 - r_{min}}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and generate the permutation $\sigma_{min}$ with probability $1 - p$ and the permutation $\epsilon$ with probability $p$.  (If $r_{min} = 1$ this formula is undefined but there's nothing to do anyway.)&lt;/p&gt;&#10;&#10;&lt;p&gt;I suspect you would like a more &quot;interesting&quot; distribution of permutations than this.  To create this you will need to add more conditions.  Here's one way to frame your problem: to every permutation $\sigma$ corresponds the number $f(\sigma) = {X^\sigma}'X$. An arbitrary probability distribution over the permutations assigns a non-negative value $p(\sigma)$ to each permutation according to the axioms of probability.  The expectation of $f$, which is the expected correlation between $X$ and $X^\sigma$, of course equals&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\sum_{\sigma \in S_n}{p(\sigma)f(\sigma)}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Given a desired expected correlation $r$, you therefore have freedom to choose the $n!$ values $p(\sigma)$ subject to the conditions&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\sum_{\sigma \in S_n}{p(\sigma)} = 1,$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\sum_{\sigma \in S_n}{p(\sigma)f(\sigma)} = r,$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$p(\sigma) \ge 0 \text{ for all } \sigma \in S_n.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;I have merely demonstrated that this linear program is feasible if and only if $r_{min} \le r \le 1$.  You are free to choose among the solutions (a convex set of distributions) in any way you like.  For instance, you might prefer to use as uniform a choice of permutations as possible, in which case you might seek to minimize the variance of the $p(\sigma)$ (thought of just as a set of numbers) subject to the preceding conditions.  That's a &lt;em&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Quadratic_programming&quot;&gt;quadratic program&lt;/a&gt;&lt;/em&gt;, for which there are many good solution methods and much available software.  Solving this (exactly) will become problematic once $n$ exceeds about $8$ or so, because it involves $n!$ variables and you'll just overwhelm the software.  In such cases you might want to restrict the distributions further, such as requiring them to be only cyclic and anti-cyclic permutations of the sorted coordinates (just $2n$ variables).  Another possibility is to choose a bunch of permutations randomly--making sure to include the order-reversing permutation among them so the minimum correlation can be included--and then finding an approximately uniform distribution among them.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2010-10-25T14:46:32.913" Id="3971" LastActivityDate="2010-10-25T14:52:57.817" LastEditDate="2010-10-25T14:52:57.817" LastEditorUserId="919" OwnerUserId="919" ParentId="3961" PostTypeId="2" Score="6" />
  
  <row Body="&lt;p&gt;Would &lt;a href=&quot;http://en.wikipedia.org/wiki/Copula_%28statistics%29&quot; rel=&quot;nofollow&quot;&gt;copulas&lt;/a&gt; be any use here? I don't know enough about them, or your problem, to be sure.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2010-10-25T17:50:35.663" Id="3976" LastActivityDate="2010-10-25T17:50:35.663" OwnerUserId="449" ParentId="3974" PostTypeId="2" Score="4" />
  
  
  
  <row Body="&lt;p&gt;@Skirant you lost me on your last comment, but I agree with Whuber that by using birth at start start you are distorting your sample as it doesn't take into account people with that gene change that actually already had the event or died from it. On top of it once people enter the regstry there is a chance that they change their behaviour to compensate for the higher risk. I suggest you use entry into the registry as start time and age (at entry) as a covariate or age at event as a time dependent covariate&lt;/p&gt;&#10;&#10;&lt;p&gt;Let me know how you go&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2010-10-27T04:32:45.037" Id="4011" LastActivityDate="2010-10-27T04:32:45.037" OwnerUserId="10229" ParentId="3907" PostTypeId="2" Score="0" />
  
  
  
  
  <row Body="&lt;p&gt;&lt;strong&gt;No&lt;/strong&gt;, I'm afraid not. The kernel density estimand is the &lt;a href=&quot;http://en.wikipedia.org/wiki/Probability_density_function&quot;&gt;probability density function&lt;/a&gt;. The &lt;em&gt;y&lt;/em&gt;-value is an estimate of the probability density at that value of &lt;em&gt;x&lt;/em&gt;, so the area under the curve between &lt;em&gt;x&lt;/em&gt;&lt;sub&gt;1&lt;/sub&gt; and  &lt;em&gt;x&lt;/em&gt;&lt;sub&gt;2&lt;/sub&gt; estimates the probability of the random variable &lt;em&gt;X&lt;/em&gt;&amp;nbsp;  falling between &lt;em&gt;x&lt;/em&gt;&lt;sub&gt;1&lt;/sub&gt; and  &lt;em&gt;x&lt;/em&gt;&lt;sub&gt;2&lt;/sub&gt;, &lt;em&gt;assuming&lt;/em&gt; that &lt;em&gt;X&lt;/em&gt; was generated by the same process that generated the data which you fed into the kernel density estimate. The kernel density estimate doesn't say anything about the probability a new value was generated by the same process.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-10-28T07:55:48.317" Id="4038" LastActivityDate="2010-10-28T07:55:48.317" OwnerUserId="449" ParentId="4034" PostTypeId="2" Score="11" />
  <row Body="&lt;p&gt;So 'classical models' (whatever they are - I assume you mean something like simple models taught in textbooks and estimated by ML) fail on some, perhaps many, real world data sets.  &lt;/p&gt;&#10;&#10;&lt;p&gt;If a model fails then there are two basic approaches to fixing it:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Make fewer assumptions (less model)&lt;/li&gt;&#10;&lt;li&gt;Make more assumptions (more model)&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Robust statistics, quasi-likelihood, and GEE approaches take the first approach by changing the estimation strategy to one where the model does not hold for all data points (robust) or need not characterize all aspects of the data (QL and GEE).  &lt;/p&gt;&#10;&#10;&lt;p&gt;The alternative is to try to build a model that explicitly models the source of contaminating data points, or the aspects of the original model that seems to be false, while keeping the estimation method the same as before.&lt;/p&gt;&#10;&#10;&lt;p&gt;Some intuitively prefer the former (it's particularly popular in economics), and some intuitively prefer the latter (it's particular popular among Bayesians, who tend to be happier with more complex models, particularly once they realize they're going to have use simulation tools for inference anyway). &lt;/p&gt;&#10;&#10;&lt;p&gt;Fat tailed distributional assumptions, e.g. using the negative binomial rather than poisson or t rather than normal, belong to the second strategy.  Most things labelled 'robust statistics' belong to the first strategy.&lt;/p&gt;&#10;&#10;&lt;p&gt;As a practical matter, deriving estimators for the first strategy for realistically complex problems seems to be quite hard.  Not that that's a reason for not doing so, but it is perhaps an explanation for why it isn't done very often.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-10-28T18:14:53.187" Id="4051" LastActivityDate="2010-10-28T18:14:53.187" OwnerUserId="1739" ParentId="1164" PostTypeId="2" Score="25" />
  
  <row AnswerCount="0" Body="&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;strong&gt;Possible Duplicate:&lt;/strong&gt;&lt;br&gt;&#10;  &lt;a href=&quot;http://stats.stackexchange.com/questions/4025/mle-for-naive-bayes-in-r&quot;&gt;MLE for Naive Bayes in R&lt;/a&gt;  &lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&#10;&#10;&lt;p&gt;Hi,&lt;/p&gt;&#10;&#10;&lt;p&gt;I need the formula representation of maximum likelihood estimate for an example conditional probability distribution for Naive Bayes algorithm. I couldn't find anything from my Google searches. I hope somebody can answer my question or point out a good resource.&lt;/p&gt;&#10;" ClosedDate="2010-10-29T10:23:31.367" CommentCount="5" CreationDate="2010-10-29T04:44:40.503" Id="4059" LastActivityDate="2010-10-29T04:44:40.503" OwnerDisplayName="Enso" PostTypeId="1" Score="0" Tags="&lt;computational-statistics&gt;" Title="Maximum Likelihood formula for Naive Bayes" ViewCount="335" />
  <row Body="&lt;p&gt;I would suggest to use restricted cubic splines (&lt;code&gt;rcs&lt;/code&gt; in R, see the &lt;a href=&quot;http://cran.r-project.org/web/packages/Hmisc/index.html&quot;&gt;Hmisc&lt;/a&gt; and &lt;a href=&quot;http://cran.r-project.org/web/packages/Design/index.html&quot;&gt;Design&lt;/a&gt; packages for examples of use), instead of adding power of $X$ in your model. This approach is the one that is recommended by Frank Harrell, for instance, and you will find a nice illustration in his handouts (§2.5 and chap. 9) on &lt;em&gt;Regression Modeling Strategies&lt;/em&gt; (see the &lt;a href=&quot;http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/RmS&quot;&gt;companion website&lt;/a&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;You can compare the results with your Box-Tidwell test by using the &lt;code&gt;boxTidwell()&lt;/code&gt; in the &lt;a href=&quot;http://cran.r-project.org/web/packages/car/index.html&quot;&gt;car&lt;/a&gt; package.&lt;/p&gt;&#10;&#10;&lt;p&gt;Transforming continuous predictors into categorical ones is generally not a good idea, see e.g. &lt;a href=&quot;http://biostat.mc.vanderbilt.edu/wiki/Main/CatContinuous&quot;&gt;Problems Caused by Categorizing Continuous Variables&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-10-29T07:50:26.763" Id="4060" LastActivityDate="2010-10-29T08:09:39.157" LastEditDate="2010-10-29T08:09:39.157" LastEditorUserId="930" OwnerUserId="930" ParentId="4058" PostTypeId="2" Score="5" />
  
  <row Body="&lt;p&gt;Student's &lt;em&gt;t&lt;/em&gt;-distribution becomes closer and closer the the standard normal distribution as the degrees of freedom get larger. With  1313662 + 38704 – 2 = 1352364 degrees of freedom, the &lt;em&gt;t&lt;/em&gt;-distribution will be indistinguishable from the standard normal distribution, as can be seen in the picture below (unless perhaps you're in the very extreme tails and you're interested in distinguishing absolutely tiny &lt;em&gt;p&lt;/em&gt;-values from even tinier ones). So you can use the table for the standard normal distribution instead of the table for the &lt;em&gt;t&lt;/em&gt;-distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/r7sSi.png&quot; alt=&quot;alt text&quot;&gt;&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2010-10-30T08:41:36.613" Id="4077" LastActivityDate="2010-10-30T13:09:47.210" LastEditDate="2010-10-30T13:09:47.210" LastEditorUserId="930" OwnerUserId="449" ParentId="4075" PostTypeId="2" Score="9" />
  <row AcceptedAnswerId="4090" AnswerCount="1" Body="&lt;p&gt;I asked this question at Mathoverflow but they recommend me to ask here. I need to find factored joint distribution of &lt;a href=&quot;http://www.cs.huji.ac.il/~nir/Abstracts/FrGG1.html&quot; rel=&quot;nofollow&quot;&gt;Tree Augmented Naive Bayes&lt;/a&gt; algorithm. I read the paper but I couldn't figure out the answer. Any help or pointers appreciated.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-10-30T21:23:43.047" FavoriteCount="2" Id="4080" LastActivityDate="2010-11-01T08:50:52.740" LastEditDate="2010-11-01T08:50:52.740" LastEditorUserId="88" OwnerDisplayName="Nathan" PostTypeId="1" Score="4" Tags="&lt;machine-learning&gt;&lt;naive-bayes&gt;" Title="Factored Joint Distribution of Tree Augmented Naive Bayes Algorithm" ViewCount="243" />
  
  
  <row Body="&lt;p&gt;I think it's useful to review what we know about cross-validation.  Statistical results around CV fall into two classes: efficiency and consistency.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Efficiency is what we're usually concerned with when building predictive models.  The idea is that we use CV to determine a model with asymtptotic guarantees concerning the loss function.  The most famous result here is due to &lt;a href=&quot;http://www.jstor.org/pss/2984877&quot;&gt;Stone 1977&lt;/a&gt; and shows that LOO CV is asymptotically equivalent to AIC.  But, Brett provides a good example where you can find a predictive model which doesn't inform you on the causal mechanism.&lt;/p&gt;&#10;&#10;&lt;p&gt;Consistency is what we're concerned with if our goal is to find the &quot;true&quot; model.  The idea is that we use CV to determine a model with asymptotic guarantees that, given that our model space includes the true model, we'll discover it with a large enough sample.  The most famous result here is due to &lt;a href=&quot;http://www.jstor.org/pss/2290328&quot;&gt;Shao 1993&lt;/a&gt; concerning linear models, but as he states in his abstract, his &quot;shocking discovery&quot; is opposite of the result for LOO.  For linear models, you can achieve consistency using LKO CV as long as $k/n \rightarrow 1$ as $n \rightarrow \infty$.  Beyond linear mdoels, it's harder to derive statistical results.  &lt;/p&gt;&#10;&#10;&lt;p&gt;But suppose you can meet the consistency criteria and your CV procedure leads to the true model: $Y = \beta X + e$.  What have we learned about the causal mechanism?  We simply know that there's a well defined correlation between $Y$ and $X$, which doesn't say much about causal claims.  From a traditional perspective, you need to bring in experimental design with the mechanism of control/manipulation to make causal claims.  From the perspective of Judea Pearl's framework, you can bake causal assumptions into a structural model and use the probability based calculus of counterfactuals to derive some claims, but you'll need to satisfy &lt;a href=&quot;http://bayes.cs.ucla.edu/BOOK-2K/jw.html&quot;&gt;certain properties&lt;/a&gt;.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Perhaps you could say that CV can help with causal inference by identifying the true model (provided you can satisfy consistency criteria!).  But it only gets you so far; CV by itself isn't doing any of the work in either framework of causal inference.  &lt;/p&gt;&#10;&#10;&lt;p&gt;If you're interested further in what we can say with cross-validation, I would recommend Shao 1997 over the widely cited 1993 paper:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://www3.stat.sinica.edu.tw/statistica/j7n2/j7n21/j7n21.htm&quot;&gt;An Asymptotic Theory for Linear Model Selection&lt;/a&gt; (Shao, 1997)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;You can skim through the major results, but it's interesting to read the discussion that follows.  I thought the comments by Rao &amp;amp; Tibshirani, and by Stone, were particularly insightful.  But note that while they discuss consistency, no claims are ever made regarding causality.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-10-31T22:40:29.637" Id="4095" LastActivityDate="2010-10-31T22:40:29.637" OwnerUserId="251" ParentId="3893" PostTypeId="2" Score="12" />
  
  <row AcceptedAnswerId="4116" AnswerCount="8" Body="&lt;p&gt;A student asked me today, &quot;How do they know how many people attended a large group event, for example, the Stewart/Colbert 'Rally to Restore Sanity' in Washington D.C.?&quot;  News outlets report estimates in the tens of thousands, but what methods are used to get those estimates, and how reliable are they?&lt;/p&gt;&#10;&#10;&lt;p&gt;One article apparently based their estimate on parking permits...  but what other techniques do we have?  Please note I am not talking about capture/recapture experiments or anything of the like.&lt;/p&gt;&#10;&#10;&lt;p&gt;I don't have any idea.  I would guess in advance that there aren't specific methods for something like this, and whatever's there is very ad hoc (such as how many parking permits were sold). Is this true? For purposes of national security - of course - it would be possible to have an analyst sit down with satellite photographs and physically bean-count the number of people there. I doubt this method is used very often. &lt;/p&gt;&#10;" CommentCount="7" CreationDate="2010-11-01T20:00:57.660" FavoriteCount="2" Id="4111" LastActivityDate="2014-05-29T01:00:45.313" LastEditDate="2010-11-02T13:57:31.997" LastEditorUserId="8" OwnerUserId="1108" PostTypeId="1" Score="21" Tags="&lt;estimation&gt;&lt;sampling&gt;" Title="How to estimate how many people attended an event (say, a political rally)?" ViewCount="1987" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I've been reading some about Generalized Least Squares (GLS) and trying to tie it back to my basic econometric background. I recall in grad school using Seemingly Unrelated Regression (SUR) which seems somewhat similar to GLS. One paper I stumbled on even referred to SUR as &quot;special case&quot; of GLS. But I still can't wrap my brain around the similarities and differences. &lt;/p&gt;&#10;&#10;&lt;p&gt;so the question:&lt;/p&gt;&#10;&#10;&lt;p&gt;What are the similarities and differences between GLS and SUR? What are the hallmarks of a problem which should use one method over the other?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-11-01T20:31:17.030" Id="4114" LastActivityDate="2010-11-03T09:55:08.037" LastEditDate="2010-11-03T09:55:08.037" LastEditorUserId="930" OwnerUserId="29" PostTypeId="1" Score="8" Tags="&lt;regression&gt;&lt;gls&gt;" Title="Difference between GLS and SUR" ViewCount="440" />
  
  <row AcceptedAnswerId="4122" AnswerCount="1" Body="&lt;p&gt;If X and Y are standardized variables and are perfectly positively correlated with respect to each other, how can i prove that $E[(X-Y)^2] = 0$?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-11-01T23:13:00.463" Id="4121" LastActivityDate="2011-06-16T16:40:50.630" LastEditDate="2010-11-02T13:51:18.837" LastEditorUserId="8" OwnerUserId="1395" PostTypeId="1" Score="2" Tags="&lt;theory&gt;&lt;self-study&gt;" Title="Statistics Proof that $E[(X-Y)^2] = 0$" ViewCount="1181" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I feel I'm pretty new to this, since some time passed since my last statistics assignment, so please bear with me.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am analyzing results of a biological experiment.  Basically, I'm looking at a some graph over a genome, where each position in the genome has a value, and I'm looking for local minima (peaks).&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, I have to set some threshold since relatively high local minima also occur by chance. I can simulate the experiment computationally and get new data, but this is quite resource demanding (I can't run 1000 simulations, perhaps 100 or even only 20).&lt;/p&gt;&#10;&#10;&lt;p&gt;What I'm currently doing is run some simulations; for each simulation: find all local minima, build a CDF for the local minima values. I then average all the simulation CDFs over all simulations so I have one 'average' CDF (CDF_simulations) which is supposed to show how would local minima distribute if everything in my genome is random.&lt;/p&gt;&#10;&#10;&lt;p&gt;I do the same for the real data: find minima and build CDFs for their values, so I now have two CDFs - one for the real data and one for the average of the simulations.&lt;/p&gt;&#10;&#10;&lt;p&gt;I now search for the max x such that CDF_simulations(x) / CDF_realdata(x) is &amp;lt; 10%. I report all minima in the real data with value &amp;lt; x as &quot;true&quot;. &lt;/p&gt;&#10;&#10;&lt;p&gt;I think this method should get me to a FP rate of 10%.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;Does this make sense?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;What is this method called and where can I find more about it?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;How should I scan the CDFs to find the right x? Sometimes for low x's CDF_simulations(x) &gt; CDF_realdata(x).&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Where does the number of simulations come into play? Does it make sense to simply build an averaged CDF as I did?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I think this is quite common, and the name FDR also pops to mind, but when reading about FDR I couldn't exactly figure how to apply it to my situation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any comments and references (hopefully user-friendly ones) will be appreciated!&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&#10;Dave&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2010-11-02T15:27:13.997" FavoriteCount="0" Id="4140" LastActivityDate="2013-02-12T23:20:04.833" LastEditDate="2010-11-02T18:25:52.760" LastEditorUserId="449" OwnerUserId="634" PostTypeId="1" Score="5" Tags="&lt;multiple-comparisons&gt;&lt;statistical-significance&gt;&lt;cdf&gt;" Title="How can I control the false positives rate?" ViewCount="220" />
  
  <row Body="&lt;p&gt;The question is about marginal effects (of X on Y), I think, not so much about interpreting individual coefficients.  As folk have usefully noted, these are only sometimes identifiable with an effect size, e.g. when there are linear and additive relationships.&lt;/p&gt;&#10;&#10;&lt;p&gt;If that's the focus then the (conceptually, if not practically) simplest way to think about the problem would seem to be this: &lt;/p&gt;&#10;&#10;&lt;p&gt;To get the marginal effect of X on Y in a linear normal regression model with no interactions, you &lt;em&gt;can&lt;/em&gt; just look at the coefficient on X.  But that's not quite enough since it is estimated not known.  In any case, what one really wants for marginal effects is some kind of plot or summary that provides a prediction about Y for a range of values of X, and a measure of uncertainty.  Typically one might want the predicted mean Y and a confidence interval, but one might also want predictions for the complete conditional distribution of Y for an X.  That distribution is wider than the fitted model's sigma estimate because it takes into account uncertainty about the model coefficients.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are various closed form solutions for simple models like this one.  For current purposes we can ignore them and think instead more generally about how to get that marginal effects graph by simulation, in a way that deals with arbitrarily complex models.&lt;/p&gt;&#10;&#10;&lt;p&gt;Assume you want the effects of varying X on the mean of Y, and you're happy to fix all the other variables at some meaningful values.  For each new value of X, take a size B sample from the distribution of model coefficients.  An easy way to do so in R is to assume that it is Normal with mean &lt;code&gt;coef(model)&lt;/code&gt; and covariance matrix &lt;code&gt;vcov(model)&lt;/code&gt;.  Compute a new expected Y for each set of coefficients and summarize the lot with an interval.  Then move on to the next value of X.&lt;/p&gt;&#10;&#10;&lt;p&gt;It seems to me that this method should be unaffected by any fancy transformations applied to any of the variables, provided you also apply them (or their inverses) in each sampling step.  So, if the fitted model has log(X) as a predictor then log your new X before multiplying it by the sampled coefficient.  If the fitted model has sqrt(Y) as a dependent variable then square each predicted mean in the sample before summarizing them as an interval.  &lt;/p&gt;&#10;&#10;&lt;p&gt;In short, more programming but less probability calculation, and clinically comprehensible marginal effects as a result.  This 'method' is sometimes referred to CLARIFY in the political science literature, but is quite general.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-11-02T17:21:52.077" Id="4145" LastActivityDate="2010-11-02T17:21:52.077" OwnerUserId="1739" ParentId="2142" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;I don't know anything about unit roots, but your problem may be analogous to an &lt;em&gt;equivalence test&lt;/em&gt; where, say, a new drug is tested for inferiority, equivalence or superiority compared to another drug. That is a reasonably common issue in clinical testing and so there is quite a literature on it.&lt;/p&gt;&#10;&#10;&lt;p&gt;You might like to start here: &lt;a href=&quot;http://www.graphpad.com/library/BiostatsSpecial/article_182.htm&quot; rel=&quot;nofollow&quot;&gt;http://www.graphpad.com/library/BiostatsSpecial/article_182.htm&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-11-03T00:10:51.297" Id="4151" LastActivityDate="2010-11-03T00:10:51.297" OwnerUserId="1679" ParentId="4142" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;I think the premise of this question is flawed because it denies the distinction between the &lt;em&gt;uncertain&lt;/em&gt; and the &lt;em&gt;known&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Describing a coin flip provides a good analogy.  Before the coin is flipped, the outcome is uncertain; afterwards, it is no longer &quot;hypothetical.&quot;  Confusing this &lt;em&gt;fait accompli&lt;/em&gt; with the actual situation we wish to understand (the behavior of the coin, or decisions that are to be made as a result of its outcome) essentially denies a role for probability in understanding the world.&lt;/p&gt;&#10;&#10;&lt;p&gt;This contrast is thrown in sharp relief within an experimental or regulatory arena.  In such cases the scientist or the regulator know they will be faced with situations whose outcomes, at any time beforehand, are unknown, yet they must make important determinations such as how to design the experiment or establish the criteria to use in determining compliance with regulations (for drug testing, workplace safety, environmental standards, and so on).  These people and the institutions for which they work need &lt;em&gt;methods&lt;/em&gt; and knowledge of the &lt;em&gt;probabilistic characteristics of those methods&lt;/em&gt; in order to develop optimal and defensible strategies, such as good experimental designs and fair decision procedures that err as little as possible.&lt;/p&gt;&#10;&#10;&lt;p&gt;Confidence intervals, despite their classically poor justification, fit into this decision-theoretic framework.  When a method of constructing a random interval has a combination of good properties, such as assuring a minimal expected coverage of the interval and minimizing the expected length of the interval--both of them &lt;em&gt;a priori&lt;/em&gt; properties, not &lt;em&gt;a posteriori&lt;/em&gt; ones--then over a long career of using that method we can minimize the costs associated with the actions that are indicated by that method.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-11-03T00:38:16.730" Id="4153" LastActivityDate="2010-11-03T00:38:16.730" OwnerUserId="919" ParentId="3911" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;Stochastic processes underlie many ideas in statistics such as time series, markov chains, markov processes, bayesian estimation algorithms (e.g., Metropolis-Hastings) etc. Thus, a study of stochastic processes will be useful in two ways:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Enable you to develop models for situations of interest to you.&lt;/p&gt;&#10;&#10;&lt;p&gt;An exposure to such a course, may enable you to identify a standard stochastic process that works given your problem context. You can then modify the model as needed to accommodate the idiosyncrasies of your specific context.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Enable you to better understand the nuances of the statistical methodology that uses stochastic processes. &lt;/p&gt;&#10;&#10;&lt;p&gt;There are several key ideas in stochastic processes such as convergence, stationarity that play an important role when we want to analyze a stochastic process. It is my belief that a course in stochastic process will let you appreciate better the need for caring about these issues and why they are important.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Can you be a statistician without taking a course in stochastic processes? Sure. You can always  use the software that is available to perform whatever statistical analysis you want. However, a basic understanding of stochastic processes is very helpful in order to make a correct choice of methodology, in order to understand what is really happening in the black box etc. Obviously, you will not be able to contribute to the theory of stochastic processes with a basic course but in my opinion it will make you a better statistician. My general rule of thumb for coursework: The more advanced course you take the better off you will be in the long-run.&lt;/p&gt;&#10;&#10;&lt;p&gt;By way of analogy: You can perform a t-test without knowing any probability theory or  statistics testing methodology. But, a knowledge of probability theory and statistical testing methodology is extremely useful in understanding the output correctly and in choosing the correct statistical test.&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2010-11-03T15:51:11.590" CreationDate="2010-11-03T10:36:25.520" Id="4167" LastActivityDate="2010-11-03T10:42:13.277" LastEditDate="2010-11-03T10:42:13.277" LastEditorDisplayName="user28" OwnerDisplayName="user28" ParentId="4165" PostTypeId="2" Score="12" />
  <row Body="&lt;p&gt;I have a &lt;a href=&quot;http://www.thinkingaboutthinking.org/wp-content/uploads/2010/05/Lawrence_BRM_in_press.pdf&quot; rel=&quot;nofollow&quot;&gt;paper&lt;/a&gt; in press that explores application of EM to estimation of a Von Mises &amp;amp; uniform mixture in the circular domain. (The Von Mises is the circular analogue of a gaussian.)&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2010-11-03T11:38:25.917" Id="4169" LastActivityDate="2010-11-03T11:38:25.917" OwnerUserId="364" ParentId="4157" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;You need to be careful how you ask this question. Since you could substitute almost anything in place of &lt;em&gt;stochastic processes&lt;/em&gt; and it would still be potentially useful. For example, a course in biology could help with biological statistical consultancy since you know more biology!&lt;/p&gt;&#10;&#10;&lt;p&gt;I presume that you have a choice of modules that you can take, and you need to pick $n$ of them. The real question is what modules should I pick (that question probably isn't appropriate for this site!)&lt;/p&gt;&#10;&#10;&lt;p&gt;To answer your question, you are still very early in your career and at this moment in time you should try to get a wide selection of courses under your belt. Furthermore, if you are planning a career in academia then some more mathematical courses, like &lt;em&gt;stochastic processes&lt;/em&gt; would be useful.&lt;/p&gt;&#10;" CommentCount="2" CommunityOwnedDate="2010-11-03T15:51:11.590" CreationDate="2010-11-03T15:23:45.933" Id="4177" LastActivityDate="2010-11-03T15:23:45.933" OwnerUserId="8" ParentId="4165" PostTypeId="2" Score="4" />
  
  
  <row Body="&lt;p&gt;I believe M. Tibbit's answer refers to the general case of a gamma with unknown shape and scale. If the shape α is known and the sampling distribution for x is gamma(α, β) and the prior distribution on β is gamma(α0, β0), the posterior distribution for β is gamma(α0 + n, β0 + Σxi). See this &lt;a href=&quot;http://www.johndcook.com/conjugate_prior_diagram.html&quot; rel=&quot;nofollow&quot;&gt;diagram&lt;/a&gt; and the references at the bottom.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-11-04T10:12:31.840" Id="4201" LastActivityDate="2010-11-04T10:14:48.267" LastEditDate="2010-11-04T10:14:48.267" LastEditorUserId="159" OwnerUserId="319" ParentId="2635" PostTypeId="2" Score="4" />
  <row AcceptedAnswerId="4206" AnswerCount="1" Body="&lt;p&gt;Suppose I have a set of $N$ experimental points of the form &lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{equation}&#10;\{x_i, y_i, d_i\},&#10;\end{equation}&lt;/p&gt;&#10;&#10;&lt;p&gt;where $i=1,...,N,$ and $d_i$ are errorbars for $y_i$. To fit the data, I minimize the reduced chi-square&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{equation}&#10;\chi^2(p) = \sum_{i=1}^N \frac{[y_i - f(x_i,p)]^2}{d_i^2},&#10;\end{equation}&lt;/p&gt;&#10;&#10;&lt;p&gt;where $f(x,p)$ is a (generally non-linear) function parametrized by some parameter $p$ (there might be more than one parameter, but it doesn't really matter). &lt;/p&gt;&#10;&#10;&lt;p&gt;My question is: given the optimal parameter $p_0$, i.e. $\chi^2(p)$ is minimal at $p=p_0$, and assuming the $y_i$'s are independent and are Normally distributed, what can be said about the distribution of $f(x, p0)$?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-11-04T11:33:05.190" FavoriteCount="0" Id="4203" LastActivityDate="2010-11-04T13:28:31.173" LastEditDate="2010-11-04T13:28:31.173" LastEditorUserId="930" OwnerUserId="1197" PostTypeId="1" Score="1" Tags="&lt;distributions&gt;&lt;chi-squared&gt;&lt;fitting&gt;" Title="What is the distribution of a chi-square minimizing function?" ViewCount="357" />
  <row Body="&lt;p&gt;The &lt;a href=&quot;http://cran.r-project.org/package=ez&quot;&gt;ez&lt;/a&gt; package contains the ezPredict() function, which obtains predictions from lmer models where prediction is based on the fixed effects only. It's really just a wrapper around the approach detailed in the &lt;a href=&quot;http://glmm.wikidot.com/faq&quot;&gt;glmm wiki&lt;/a&gt;. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-11-04T12:36:04.560" Id="4204" LastActivityDate="2010-11-04T12:36:04.560" OwnerUserId="364" ParentId="4187" PostTypeId="2" Score="9" />
  <row Body="&lt;p&gt;It is not possible for a sequence to &quot;converge&quot; to one thing and then to another.  The higher-order terms in an asymptotic expansion will go to zero.  What they tell you is &lt;em&gt;how close to zero&lt;/em&gt; they are for any given value of $n$.&lt;/p&gt;&#10;&#10;&lt;p&gt;For the Central Limit Theorem (as an example) the appropriate expansion is that of the logarithm of the characteristic function: the cumulant generating function (cgf).  Standardization of the distributions fixes the zeroth, first, and second terms of the cgf.  The remaining terms, whose coefficients are the &lt;em&gt;cumulants&lt;/em&gt;, depend on $n$ in an orderly way.  The standardization that occurs in the CLT (dividing the sum of $n$ random variables by something proportional to $n^{1/2}$--without which convergence will not occur) causes the $m^\text{th}$ cumulant--which after all depends on $m^\text{th}$ moments--to be divided by $(n^{1/2})^m = n^{m/2}$, but at the same time because we are summing $n$ terms, the net result is that the $m^\text{th}$ order term is proportional to $n/n^{m/2} = n^{-(m-2)/2}$.  Thus the third cumulant of the standardized sum is proportional to $1/n^{1/2}$, the fourth cumulant is proportional to $1/n$, and so on.  These are the higher-order terms.  (For details, see &lt;a href=&quot;http://www.cs.toronto.edu/~yuvalf/CLT.pdf&quot; rel=&quot;nofollow&quot;&gt;this paper of Yuval Filmus&lt;/a&gt; for example.)&lt;/p&gt;&#10;&#10;&lt;p&gt;In general, a high negative power of $n$ is much smaller than a low negative power. &#10;We can always be assured of this by taking a sufficiently large value of $n$.  Thus, for really large $n$ we can neglect all negative powers of $n$: they converge to zero.  Along the way to convergence, &lt;em&gt;departures&lt;/em&gt; from the ultimate limit are measured with increasing accuracy by the additional terms: the $1/n^{1/2}$ term is an initial &quot;correction,&quot; or departure from the limiting value; the next $1/n$ term is a smaller, more quickly-vanishing correction added to that, and so on.  In brief, the additional terms give you a picture of how quickly the sequence converges to its limit.&lt;/p&gt;&#10;&#10;&lt;p&gt;These additional terms can help us make &lt;em&gt;corrections&lt;/em&gt; for finite (usually small) values of $n$.  They show up all the time in this regard, such as &lt;a href=&quot;http://projecteuclid.org/DPubS/Repository/1.0/Disseminate?view=body&amp;amp;id=pdf_1&amp;amp;handle=euclid.aos/1176324314&quot; rel=&quot;nofollow&quot;&gt;Chen's modification of the t-test&lt;/a&gt;, which exploits the third-order ($1/n^{1/2}$) term.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-11-04T14:46:20.700" Id="4209" LastActivityDate="2010-11-04T14:46:20.700" OwnerUserId="919" ParentId="4193" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;Did you see this post? &lt;a href=&quot;http://groups.google.com/group/ggplot2/browse_thread/thread/8e1efd0e7793c1bb&quot;&gt;http://groups.google.com/group/ggplot2/browse_thread/thread/8e1efd0e7793c1bb&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Take the example, add coord_polar() and reverse the axes and you get pretty close:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(cluster) &#10;data(mtcars)&#10;x &amp;lt;- as.phylo(hclust(dist(mtcars)))&#10;&#10;p &amp;lt;- ggplot(data=x)&#10;p &amp;lt;- p + geom_segment(aes(y=x,x=y,yend=xend,xend=yend), colour=&quot;blue&quot;,alpha=1) &#10;p &amp;lt;- p + geom_text(data=label.phylo(x), aes(x=y, y=x, label=label),family=3, size=3) + xlim(0, xlim) + coord_polar()&#10;&#10;theme &amp;lt;- theme_update(  axis.text.x = theme_blank(),&#10;                        axis.ticks = theme_blank(),&#10;                        axis.title.x = theme_blank(),&#10;                        axis.title.y = theme_blank(),&#10;                        legend.position = &quot;none&quot;&#10;                     )&#10;p &amp;lt;- p + theme_set(theme)&#10;print(p)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2010-11-05T03:17:06.993" Id="4224" LastActivityDate="2010-11-05T03:17:06.993" OwnerUserId="1809" ParentId="4062" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;In general, I think best practice for presenting measures of effect size depends on the question of interest and the usual practice in your field. There's little point reporting an effect measure that readers will be unfamiliar with. &lt;/p&gt;&#10;&#10;&lt;p&gt;Having said that, in this particular case if you want a &lt;em&gt;single&lt;/em&gt; effect measure I think the (conditional) odds ratio &lt;em&gt;b&lt;/em&gt; / &lt;em&gt;c&lt;/em&gt; is the obvious choice. Giving both proportions &lt;em&gt;b&lt;/em&gt; / (&lt;em&gt;b&lt;/em&gt; + &lt;em&gt;c&lt;/em&gt;) and &lt;em&gt;c&lt;/em&gt; / (&lt;em&gt;b&lt;/em&gt; + &lt;em&gt;c&lt;/em&gt;) gives more information and clearly the odds ratio can be derived from these two. I wouldn't call two proportions an 'effect size' though.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'd strongly encourage you to report &lt;em&gt;all four&lt;/em&gt; cells of the 2×2 table concordancy table, however. That way nothing is hidden and anyone can calculate whatever statistics they wish. It's only four numbers, after all.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-11-05T07:17:15.547" Id="4227" LastActivityDate="2010-11-05T10:40:01.677" LastEditDate="2010-11-05T10:40:01.677" LastEditorUserId="449" OwnerUserId="449" ParentId="4219" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;Maybe you are looking for the library ggplot2 that lets you plot things in a pretty way.&#10;Or you can check this website that seems to have lots of R graphic utilities&#10;&lt;a href=&quot;http://addictedtor.free.fr/graphiques/&quot; rel=&quot;nofollow&quot;&gt;http://addictedtor.free.fr/graphiques/&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-11-05T07:58:21.050" Id="4230" LastActivityDate="2010-11-05T07:58:21.050" OwnerUserId="1808" ParentId="4089" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;If you're interested in financial applications, I guess you can find some elements here, in the slides (starting from 50)&#10;&lt;a href=&quot;http://freakonometrics.blog.free.fr/index.php?post/2008/10/11/noise-traders&quot; rel=&quot;nofollow&quot;&gt;http://freakonometrics.blog.free.fr/index.php?post/2008/10/11/noise-traders&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-11-05T13:42:40.777" Id="4240" LastActivityDate="2010-11-05T13:42:40.777" OwnerDisplayName="Arthur" ParentId="4238" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;If you want to assume simple linear trend, you can take the difference of each data set at the various time points and test that the slope of the line is zero.&lt;/p&gt;&#10;&#10;&lt;p&gt;-Ralph Winters&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-11-05T14:54:01.720" Id="4244" LastActivityDate="2010-11-05T14:54:01.720" OwnerDisplayName="Ralph Winters" ParentId="3616" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;To average the ECDFs, I'd do something like:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;impute_resolution = 1e3&#10;values_to_impute = seq(&#10;    min(my_data$true_data)&#10;    , max(my_data$true_data)&#10;    , length.out = impute_resoluton&#10;)&#10;&#10;ecdfs = matrix(NA,nrow=length(values_to_impute))&#10;&#10;for(i in 1:(ncol(my_data)-1)){ #assumes column 1 is true_data&#10;    this_ecdf = ecdf(my_data[,i+1])&#10;    ecdfs[i,] = this_ecdf(values_to_impute)&#10;}&#10;&#10;mean_ecdf = colMeans(ecdfs)&#10;plot(&#10;    x = values_to_impute&#10;    , y = mean_ecdf&#10;    , type = 'l'&#10;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2010-11-05T23:20:38.963" Id="4263" LastActivityDate="2010-11-05T23:20:38.963" OwnerUserId="364" ParentId="4261" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="4268" AnswerCount="5" Body="&lt;p&gt;I recently read Skillicorn's book on matrix decompositions, and was a bit disappointed, as it was targeted to an undergraduate audience. I would like to compile (for myself and others) a short bibliography of essential papers (surveys, but also breakthrough papers) on matrix decompositions. What I have in mind primarily is something on SVD/PCA (and robust/sparse variants), and NNMF, since those are by far the most used. Do you all have any recommendation/suggestion? I am holding off mine not to bias the answers. I would ask to limit each answer to 2-3 papers.&lt;/p&gt;&#10;&#10;&lt;p&gt;P.S.: I refer to these two decompositions as the most used &lt;em&gt;in data analysis&lt;/em&gt;. Of course QR, Cholesky, LU and polar are very important in numerical analysis. That is not the focus of my question though.&lt;/p&gt;&#10;" CommentCount="1" CommunityOwnedDate="2010-11-06T13:01:36.173" CreationDate="2010-11-06T03:32:17.627" FavoriteCount="16" Id="4267" LastActivityDate="2010-12-16T08:38:52.443" LastEditDate="2010-11-06T18:59:40.517" LastEditorUserId="88" OwnerUserId="30" PostTypeId="1" Score="12" Tags="&lt;matrix-decomposition&gt;&lt;svd&gt;&lt;numerics&gt;" Title="Essential papers on matrix decompositions" ViewCount="1270" />
  
  
  
  <row Body="&lt;p&gt;I don't know if this is exactly what you are looking for (esp. I don't know how large is $n$ and what you intend to do with these results), however I have successfully used &lt;a href=&quot;http://pbil.univ-lyon1.fr/R/articles/arti113.pdf&quot; rel=&quot;nofollow&quot;&gt;coinertia analysis&lt;/a&gt; when I was working with two data sets (same observations in rows), and for more than two data sets there are K-table methods, as implemented in the &lt;a href=&quot;http://cran.r-project.org/web/packages/ade4/index.html&quot; rel=&quot;nofollow&quot;&gt;ade4&lt;/a&gt; R package. &lt;a href=&quot;http://pbil.univ-lyon1.fr/R/pdf/course7.pdf&quot; rel=&quot;nofollow&quot;&gt;An introduction to K-table analyses&lt;/a&gt; outlines the main principles. When the objective is to link two or more Tables, &lt;a href=&quot;http://en.wikipedia.org/wiki/Generalized_canonical_correlation&quot; rel=&quot;nofollow&quot;&gt;Generalized Canonical Correlation Analysis&lt;/a&gt; is also an option.&lt;/p&gt;&#10;&#10;&lt;p&gt;It seems to me that you can choose non-euclidean metric, provided it has some meaning for the data at hand and the interpretation of the factorial space. You can see an example with the use of &lt;code&gt;kdist()&lt;/code&gt; in &lt;a href=&quot;http://cran.r-project.org/web/packages/ade4/index.html&quot; rel=&quot;nofollow&quot;&gt;ade4&lt;/a&gt; for applying an PCA on different distance matrices. Jollife's book on &lt;em&gt;Principal component analysis&lt;/em&gt; should provide additional hints about this (but I didn't check). There's also all the work made in the spirit of Gifi on non-linear methods (in R, a lot of packages have been developed by Jan de Leeuw, see the &lt;a href=&quot;https://r-forge.r-project.org/projects/psychor/&quot; rel=&quot;nofollow&quot;&gt;PsychoR&lt;/a&gt; project).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-11-07T14:19:31.577" Id="4289" LastActivityDate="2010-11-07T14:19:31.577" OwnerUserId="930" ParentId="4286" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;&lt;a href=&quot;http://jboost.sourceforge.net/&quot; rel=&quot;nofollow&quot;&gt;JBoost&lt;/a&gt; is an awesome library. It is definitely not written in Python, however It is somewhat language agnostic, because it can be executed from the command line and such so it can be &quot;driven&quot; from Python. I've used it in the past and liked it a lot, particularly the visualization stuff.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-11-07T15:04:46.763" Id="4291" LastActivityDate="2010-11-07T15:04:46.763" OwnerUserId="1540" ParentId="2419" PostTypeId="2" Score="2" />
  
  
  
  <row Body="&lt;p&gt;I'll assume that you've already determined the number of categories you'll use. Let's say you want to use 20 categories. Then they will be:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Category 1: [0 - 100,000)&lt;/li&gt;&#10;&lt;li&gt;Category 2: [100,000 - 200,000)&lt;/li&gt;&#10;&lt;li&gt;Category 3: [200,000 - 300,000)&lt;/li&gt;&#10;&lt;li&gt;...&lt;/li&gt;&#10;&lt;li&gt;Category 19: [1,800,000 - 1,900,000)&lt;/li&gt;&#10;&lt;li&gt;Category 20: [1,900,000 - 2,000,000]&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Note that the label of each category can be defined as &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;FLOOR (x / category_size) + 1&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This is trivial to define as a computed column in SQL or as a formula in Excel.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that the last category is infinitesimally larger than the others, since it is closed on both sides. If you happen to get a value of exactly 2,000,000 you might erroneously classify it as falling into category 21, so you have to treat this exception with an ugly &quot;IF&quot; (in Excel) or &quot;CASE&quot; (in SQL).&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2010-11-09T12:25:21.007" Id="4343" LastActivityDate="2010-11-09T12:56:17.023" LastEditDate="2010-11-09T12:56:17.023" LastEditorUserId="666" OwnerUserId="666" ParentId="4341" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;Why group them? Instead, how about estimate the probability density function (PDF) of the distributions from which the data arise? Here's an R-based example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(123)&#10;dat &amp;lt;- c(sample(2000000, 500), rnorm(100, 1000000, 1000), &#10;         rnorm(150, 1500000, 100),rnorm(150, 500000, 10),&#10;         rnorm(180, 10000, 10), rnorm(10, 1000, 5), 1:10)&#10;dens &amp;lt;- density(dat)&#10;plot(dens)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;If the data are strictly bounded (0, 2,000,000) then the kernel density estimate is perhaps not best suited. You could fudge things by asking it to only evaluate the density between the bounds:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;dens2 &amp;lt;- density(dat, from = 0, to = 2000000)&#10;plot(dens2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Alternatively there is the histogram - a coarse version of the kernel density. What you specifically talk about is binning your data. There are lots of rules/approaches to selecting equal-width bins (i.e. the number of bins) from the data. In R the default is Sturges rule, but it also includes the Freedman-Diaconis rule and Scott's rule. There are others as well - see the Wikipedia page on &lt;a href=&quot;http://en.wikipedia.org/wiki/Histogram&quot;&gt;histograms&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;hist(dat)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;If you are not interested in the kernel density plot or the histogram per se, rather just the binned data, then you can compute the number of bins using the &lt;code&gt;nclass.X&lt;/code&gt; family of functions where &lt;code&gt;X&lt;/code&gt; is one of &lt;code&gt;Sturges&lt;/code&gt;, &lt;code&gt;scott&lt;/code&gt; or &lt;code&gt;FD&lt;/code&gt;. And then use &lt;code&gt;cut()&lt;/code&gt; to bin your data:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;cut.dat &amp;lt;- cut(dat, breaks = nclass.FD(dat), include.lowest = TRUE)&#10;table(cut.dat)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;which gives:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; cut.dat&#10;  [-2e+03,2.21e+05] (2.21e+05,4.43e+05] (4.43e+05,6.65e+05] (6.65e+05,8.88e+05] &#10;                247                  60                 215                  61 &#10;(8.88e+05,1.11e+06] (1.11e+06,1.33e+06] (1.33e+06,1.56e+06] (1.56e+06,1.78e+06] &#10;                153                  51                 205                  50 &#10;   (1.78e+06,2e+06] &#10;                 58&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;in R.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, binning is fraught with problems, most notably; How do you know that your choice of bins hasn't influenced the resulting impression you get of the way the data are distributed?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2010-11-09T13:00:47.157" Id="4345" LastActivityDate="2010-11-09T13:57:35.120" LastEditDate="2010-11-09T13:57:35.120" LastEditorUserId="1390" OwnerUserId="1390" ParentId="4341" PostTypeId="2" Score="7" />
  
&lt;/p&gt;&#10;&#10;&lt;p&gt;or its characteristic function. &lt;/p&gt;&#10;&#10;&lt;p&gt;As recalled in &lt;a href=&quot;http://stats.stackexchange.com/questions/4354/distributions-other-than-the-normal-where-mean-and-variance-are-independent&quot;&gt;this&lt;/a&gt; question it is also the only distribution for which the sample mean and variance are independent. &lt;/p&gt;&#10;&#10;&lt;p&gt;What are other surprising alternative characterization of Gaussian measures that you know ? I will accept the most surprising answer :)  &lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2010-11-09T22:31:31.643" CreationDate="2010-11-09T20:19:21.667" FavoriteCount="22" Id="4364" LastActivityDate="2012-06-08T16:07:56.723" LastEditDate="2011-02-15T15:06:31.437" LastEditorUserId="223" OwnerUserId="223" PostTypeId="1" Score="30" Tags="&lt;probability&gt;&lt;normal-distribution&gt;&lt;mathematical-statistics&gt;" Title="What is the most surprising characterization of the Gaussian (normal) distribution?" ViewCount="2856" />
  <row Body="&lt;p&gt;The correspondence can most easily be shown using the &lt;a href=&quot;http://books.google.com/books?id=NiQw5ZEw2IIC&amp;amp;lpg=PA177&amp;amp;dq=envelope%20theorem%20shadow%20price&amp;amp;pg=PA177#v=onepage&amp;amp;q=envelope%20theorem%20shadow%20price&amp;amp;f=false&quot; rel=&quot;nofollow&quot;&gt;Envelope Theorem&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;First, the standard Lagrangian will have an additional $\lambda \cdot t$ term. This will not affect the maximization problem if we are just treating $\lambda$ as given, so Hastie et al drop it.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, if you differentiate the full Lagrangian with respect to $t$, the Envelope Theorem says you can ignore the indirect effects of $t$ through $\beta$, because you're at a maximum. What you'll be left with is the Lagrange multipler from $\lambda \cdot t$.&lt;/p&gt;&#10;&#10;&lt;p&gt;But what does this mean intuitively?  Since the constraint binds at the maximum, the derivative of the Lagrangian, evaluated at the maximum, is the same as the deriviate the original objective.  Therefore the Lagrange multiplier gives the shadow price -- the value in terms of the objective -- of relaxing the constraint by increasing $t$.&lt;/p&gt;&#10;&#10;&lt;p&gt;I assume this is the correspondence Hastie et al. are referring to.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-11-10T03:54:06.940" Id="4379" LastActivityDate="2010-11-10T03:54:06.940" OwnerUserId="493" ParentId="4371" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="4389" AnswerCount="4" Body="&lt;p&gt;[edits made in response to feedback- thanks :-) ]&lt;/p&gt;&#10;&#10;&lt;p&gt;Doh! More edits! Sorry!&lt;/p&gt;&#10;&#10;&lt;p&gt;Hello-&lt;/p&gt;&#10;&#10;&lt;p&gt;I am doing some rather rough and ready data collection with a survey sent out to healthcare staff using a published scale about morale and other such issues.&lt;/p&gt;&#10;&#10;&lt;p&gt;The only thing is that the scale is rather long with all the other things in the survey and I would like to reduce its size by cutting each subscale in half and only using half the items. My intuition is that this is fine, since the subscales are inter-correlated, and while it's not ideal for publication-standard research, it's okay just for a bit of intra-organisational fact finding.&lt;/p&gt;&#10;&#10;&lt;p&gt;I wondered if anyone had any thoughts on the validity of doing this, pitfalls, or anything else. References particularly are gratefully received because my colleagues will need some convincing!&lt;/p&gt;&#10;&#10;&lt;p&gt;Many thanks,&#10;Chris B&lt;/p&gt;&#10;&#10;&lt;p&gt;edits-&lt;/p&gt;&#10;&#10;&lt;p&gt;Yes it is a validated scale with known psychometric properties.&lt;/p&gt;&#10;&#10;&lt;p&gt;It's unidimensional and it has subscales, if that's the right way to put it.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'll be working at the subscale and total, not the item, level.&lt;/p&gt;&#10;&#10;&lt;p&gt;30 items, probably about 40-60 individuals.&lt;/p&gt;&#10;&#10;&lt;p&gt;Cheers!&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2010-11-10T12:32:49.143" FavoriteCount="1" Id="4383" LastActivityDate="2011-03-30T18:43:46.680" LastEditDate="2011-03-30T18:43:46.680" LastEditorUserId="930" OwnerUserId="199" PostTypeId="1" Score="8" Tags="&lt;psychometrics&gt;&lt;scales&gt;&lt;reliability&gt;&lt;likert&gt;" Title="Can one validly reduce the numbers of items in a published Likert-scale?" ViewCount="1349" />
  
  <row Body="&lt;p&gt;I would recommend using Association Rule Learning for this. It allows you to find words that often co-occur. &lt;/p&gt;&#10;&#10;&lt;p&gt;If you have a lot of data, it will be much faster than calculating a correlation matrix. &lt;/p&gt;&#10;&#10;&lt;p&gt;See my video series on text mining &lt;a href=&quot;http://vancouverdata.blogspot.com/2010/11/text-analytics-with-rapidminer-part-3.html&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;. Includes a tutorial on Association Rules for text. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-11-10T20:53:29.547" Id="4412" LastActivityDate="2010-11-10T20:53:29.547" OwnerUserId="74" ParentId="1780" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;As far as books go, &quot;The Elements of Statistical Learning&quot; by Hastie, Tibshirani and Friedman is very good.&lt;/p&gt;&#10;&#10;&lt;p&gt;The full book is available on the &lt;a href=&quot;http://www-stat.stanford.edu/~tibs/ElemStatLearn/&quot; rel=&quot;nofollow&quot;&gt;authors' web site&lt;/a&gt;; you may want to take a look to see if it is at all suitable for your needs.&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2010-11-10T20:58:12.680" CreationDate="2010-11-10T20:58:12.680" Id="4413" LastActivityDate="2010-11-10T20:58:12.680" OwnerUserId="439" ParentId="4259" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;In Bayesian statistics, it is often mentioned that the posterior distribution is intractable and thus approximate inference must be applied. What are the factors that cause this intractability? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-11-11T00:33:28.430" Id="4417" LastActivityDate="2010-11-11T06:53:21.377" OwnerUserId="1913" PostTypeId="1" Score="2" Tags="&lt;bayesian&gt;&lt;approximation&gt;&lt;inference&gt;" Title="Intractable posterior distributions" ViewCount="478" />
  
  <row Body="&lt;p&gt;I'm not sure what sort of data you need to process, but statistical algorithms are used quite frequently in robotics, media art, digital signal processing etc. It may be worthwhile to borrow from these domains.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-11-11T02:16:48.133" Id="4419" LastActivityDate="2010-11-11T02:16:48.133" OwnerUserId="162" ParentId="898" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;here is the best book I would recomend for the subject:&#10;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0131011715&quot; rel=&quot;nofollow&quot;&gt;http://www.amazon.com/Fuzzy-Sets-Logic-Theory-Applications/dp/0131011715&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is an easy to read book:&#10;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0671875353&quot; rel=&quot;nofollow&quot;&gt;http://www.amazon.com/Fuzzy-Logic-Revolutionary-Computer-Technology/dp/0671875353&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Besides here are a list of links that might help:&#10;&lt;a href=&quot;http://www.seattlerobotics.org/encoder/mar98/fuz/flindex.html&quot; rel=&quot;nofollow&quot;&gt;http://www.seattlerobotics.org/encoder/mar98/fuz/flindex.html&lt;/a&gt;&#10;http://www.fuzzy-logic.com/&#10;&lt;a href=&quot;http://videolectures.net/acai05_berthold_fl/&quot; rel=&quot;nofollow&quot;&gt;http://videolectures.net/acai05_berthold_fl/&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-11-11T04:45:20.507" Id="4423" LastActivityDate="2010-11-11T04:45:20.507" OwnerUserId="1808" ParentId="4347" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;I also follow &lt;a href=&quot;http://stats.stackexchange.com/users/303/john-myles-white&quot;&gt;John Myles White&lt;/a&gt;'s GitHub &lt;a href=&quot;https://github.com/johnmyleswhite&quot; rel=&quot;nofollow&quot;&gt;repository&lt;/a&gt;. There are several data-oriented projects, but also interesting stuff for R developers:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;https://github.com/johnmyleswhite/ProjectTemplate&quot; rel=&quot;nofollow&quot;&gt;ProjectTemplate&lt;/a&gt;, a template system for building R project;&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;https://github.com/johnmyleswhite/log4r&quot; rel=&quot;nofollow&quot;&gt;log4r&lt;/a&gt;, a logging system.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CommunityOwnedDate="2010-11-11T15:07:15.367" CreationDate="2010-11-11T08:57:25.920" Id="4435" LastActivityDate="2010-11-12T10:35:00.423" LastEditDate="2010-11-12T10:35:00.423" LastEditorUserId="930" OwnerUserId="930" ParentId="4429" PostTypeId="2" Score="8" />
  <row AcceptedAnswerId="4438" AnswerCount="2" Body="&lt;p&gt;so we have a0:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt;a0[1,1:2]&#10;l11      l12&#10;-0.921 0.389593&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Then; &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; is.numeric(a0[1,1:2])&#10;[1] FALSE&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Ok, the text file containing them is a bit of a mess. Then:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; as.numeric(a0[1,1:2])&#10;[1] 131   3&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I &lt;em&gt;know&lt;/em&gt; there was a trick to solve that. I just can't remember what it was...&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT: sample file: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; -0.921      0.389593    0.99998742210431    -0.00501553917135373    0.999984216926007   -0.00561835375178836   1   2.36    10  2   0.05    1   1&#10; 0.923842    0.382812    0.999998286086073   -0.00185143860707771    0.999999995689246   9.28520798418186e-05   0   2.03    10  2   0.05    2   1&#10;     -0.82904    0.559261    0.905909593351804   -0.423471142668741      0.899659482046882   -0.436592277031025     1   1.51    10  2   0.05    3   1&#10; -0.796621   0.604487    0.852012998984401   -0.523520629547306      0.882445298428051   -0.470415024507325     0   2.53    10  2   0.05    4   1&#10; 0.836046    0.54875     0.9211906384523     0.389111561930306   0.926385125966013   0.376577479928014  1   2.66    10  2   0.05    5   1&#10; -0.873104   -0.487579   0.942147732871282   0.335197925777448   0.957896161639222   0.287114861191206  0   2.5 10  2   0.05    6   1&#10; -0.728606   0.684948    0.838364634070403   -0.545109842453794      0.770224459340949   -0.637772908042465     1   3.97    10  2   0.05    7   1&#10; 0.759379    -0.650765   0.842087096982091   -0.539341562552223      0.765143566165036   -0.643859707666391     0   1.13    10  2   0.05    8   1&#10; -0.497911   -0.867269   0.480229829449393   0.877142697003747   0.225403470821452   0.974265505569012  1   0.62    10  2   0.05    9   1&#10; 0.465581    0.885042    0.173457702239973   0.98484131997679    0.159038890626454   0.987272318698497  0   0.79    10  2   0.05    10  1&#10; -0.772559   -0.634978   0.866527018491628   0.499130169619119   0.859460127819      0.511202786269155  1   19.53   10  2   0.05    11  1&#10; 0.81446     0.580278    0.943553667636478   0.331219679804432   0.895967382559323   0.444119859260758  0   1.73    10  2   0.05    12  1&#10; -0.792377   0.610095    0.865518659616982   -0.500876681284748      0.877697131009408   -0.479215761654241     1   1.24    10  2   0.05    13  1&#10; 0.844213    -0.536081   0.899692596172177   -0.436524034152723      0.906010964105467   -0.423254217841573     0   2.02    10  2   0.05    14  1&#10; 0.421542    0.906835    0.103422987236111   0.99463746446188    0.102255903720432   0.994758126458044  1   0.47    10  2   0.05    15  1&#10; 0.409305    0.912408    0.0729480114866381      0.997335744681873   0.0838704194992194      0.996476669437386  0   0.45    10  2   0.05    16  1&#10; -0.664573   -0.747275   0.58764731149828    0.809117196263214   0.603464234608522   0.797390066120936  1   0.85    10  2   0.05    17  1&#10; 0.599777    0.800191    0.524086235602366   0.851665202795172   0.567148160033905   0.823615786984536  0   1.32    10  2   0.05    18  1&#10; -0.397025   -0.917846   0.030139624142486   0.999545698333273   0.0311215453692834      0.999515607388813  1   0.46    10  2   0.05    19  1&#10; -0.393222   -0.919468   0.0278258557873373      0.999612785907474   0.0265821201726647      0.999646633009448  0   0.42    10  2   0.05    20  1&#10; 0.772559    0.634978    0.924006045897534   0.38237785911949    0.908075241643739   0.418807062397072  1   8.12    20  2   0.05    1   2&#10; -0.807518   -0.589907   0.947419893700908   0.319993039017663   0.96499482496042    0.262268922672146  0   5.8 20  2   0.05    2   2&#10; V1      V2      V3      V4      V5      V6     NA  NA  NA  NA  NA  NA  NA&#10; -0.526762   0.850092    -0.311928648717808      0.95010552998553    -0.316412744506619      0.94862161851488   0   3.25    20  2   0.05    4   2&#10; -0.5161     -0.856543   0.309033921455991   0.951051016186583   0.248027721702134   0.96875293510123   1   1.82    20  2   0.05    5   2&#10; 0.491867    0.870721    0.268019541588267   0.96341347578639    0.167808915882198   0.98581954116889   0   2.69    20  2   0.05    6   2&#10; 0.58991     0.807579    0.51533762936385    0.856987238972464   0.421665311535527   0.906751545379244  1   1.91    20  2   0.05    7   2&#10; -0.549902   -0.835295   0.442022781946357   0.897003823983155   0.390584116545383   0.920567242466547  0   2.06    20  2   0.05    8   2&#10; 0.800218    -0.599709   0.852476180705526   -0.522766067500292      0.833708092721497   -0.552205411174757     1   47.26   20  2   0.05    9   2&#10; 0.837387    -0.546652   0.910736035285173   -0.41298895146607   0.863018073213623   -0.505173044912974     0   5.36    20  2   0.05    10  2&#10; 0.46345     0.886149    0.179828914086857   0.98369790162343    0.176294786076906   0.984337415931193  1   1.84    20  2   0.05    11  2&#10; V1      V2      V3      V4      V5      V6     NA  NA  NA  NA  NA  NA  NA&#10;     0.759379    0.650765    0.817799312011448   0.575503505874294   0.824911507994246   0.565261889727814  1   7.7 20  2   0.05    13  2&#10; 0.793956    0.608013    0.822124323319108   0.56930799836916    0.812590005961236   0.582835724893317  0   3.8 20  2   0.05    14  2&#10; 0.706746    0.707516    0.70298536741427    0.711204311855196   0.695840302062382   0.718196542755348  1   5.1 20  2   0.05    15  2&#10; 0.663932    0.747827    0.717210127099939   0.696856967809958   0.711000892677057   0.70319110532801   0   4.17    20  2   0.05    16  2&#10; 0.813568    -0.581494   0.904931790793546   -0.425556640191628      0.912655929947496   -0.408728704071137     1   4.24    20  2   0.05    17  2&#10; -0.836046   0.54875     0.924852673074549   -0.380325561995065      0.92915112499605    -0.369700131077304     0   3.78    20  2   0.05    18  2&#10; V1      V2      V3      V4      V5      V6     NA  NA  NA  NA  NA  NA  NA&#10; 0.586824    -0.809738   -0.610122851736008      0.792306825535108   -0.588615757355873      0.80841294533943   0   3.07    20  2   0.05    20  2&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2010-11-11T09:26:39.780" Id="4437" LastActivityDate="2010-11-11T10:19:23.073" LastEditDate="2010-11-11T09:45:06.437" LastEditorUserId="603" OwnerUserId="603" PostTypeId="1" Score="-1" Tags="&lt;r&gt;" Title="R and as.numeric()" ViewCount="1153" />
  
  
  <row Body="&lt;p&gt;Expanding the square we get:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\sum_i(X_i-m)^2 = \sum_i(X_i^2 + m^2 - 2 X_i m)$&lt;/p&gt;&#10;&#10;&lt;p&gt;Thus,&lt;/p&gt;&#10;&#10;&lt;p&gt;$\sum_i(X_i-m)^2 = \sum_i{X_i^2} + \sum_i{m^2} - 2 \sum_i{X_i m}$&lt;/p&gt;&#10;&#10;&lt;p&gt;Since $m$ is a constant, we have:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\sum_i(X_i-m)^2 = \sum_i{X_i^2} + n m^2 - 2 m \sum_i{X_i}$&lt;/p&gt;&#10;&#10;&lt;p&gt;But,&lt;/p&gt;&#10;&#10;&lt;p&gt;$\sum_i{X_i} = n m$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thus,&lt;/p&gt;&#10;&#10;&lt;p&gt;$\sum_i(X_i-m)^2 = \sum_i{X_i^2} + n m^2 - 2 n m^2$&lt;/p&gt;&#10;&#10;&lt;p&gt;Which on simplifying gets us:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\sum_i(X_i-m)^2 = \sum_i{X_i^2} - n m^2$ &lt;/p&gt;&#10;&#10;&lt;p&gt;Thus, we get can rewrite the rhs of the above in two ways:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\sum_i{X_i^2} - m (n m)  = \sum_i{X_i^2} - m \sum_i{X_i}$ &lt;/p&gt;&#10;&#10;&lt;p&gt;(as $n m = \sum_i{X_i}$)&lt;/p&gt;&#10;&#10;&lt;p&gt;and&lt;/p&gt;&#10;&#10;&lt;p&gt;$\sum_i{X_i^2} - n (m)^2  = \sum_i{X_i^2} - \frac{(\sum_i{X_i})^2}{n}$ &lt;/p&gt;&#10;&#10;&lt;p&gt;(as $m = \frac{\sum_i{X_i}}{n}$)&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-11-11T15:15:36.757" Id="4448" LastActivityDate="2010-11-11T15:15:36.757" OwnerDisplayName="user28" ParentId="4446" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;You can build this model with AD Model Builder's random effects package.&#10;This is free software available at &lt;a href=&quot;http://admb-project.org&quot; rel=&quot;nofollow&quot;&gt;http://admb-project.org&lt;/a&gt;.  What you will&#10;get is full information maximum likelihood solutions with the ability to try&#10;MCMC methods afterwards if you wish. The idea is to regard this as a random&#10;effects problem and integrate over the random effects via the Laplace approximation.  The trick is parameterize if properly so that the Hessian&#10;with respect to the random effects is sparse.  I built the model and a simulator. It seems to work well.  To give you an idea I have included the&#10;ADMB source for the model.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;DATA_SECTION&#10;  init_int nobs&#10;  init_vector Y(1,nobs)&#10;  vector resids(2,nobs)&#10;PARAMETER_SECTION&#10;  init_number Phi1&#10;  init_number Phi2&#10;  init_bounded_number log_sigma(-5.0,5.0,2);&#10;  init_bounded_number log_p1(-5.0,5.0,2);&#10;  init_bounded_number log_p2(-5.0,5.0,2);&#10;  objective_function_value f&#10;  init_number A2&#10;  init_number B2&#10;  random_effects_vector A(3,nobs)&#10;  random_effects_vector B(3,nobs)&#10;  vector nu_1(3,nobs);&#10;  vector nu_2(3,nobs);&#10;  vector pred_Y(2,nobs);&#10;  sdreport_number sigma&#10;  sdreport_number p1&#10;  sdreport_number p2&#10;PROCEDURE_SECTION&#10;&#10;  f0(log_sigma,log_p1,log_p2,A2,B2);&#10;&#10;  f2(3,log_sigma,log_p1,log_p2,A(3),B(3),A2,B2,Phi1,Phi2);&#10;&#10;  for (int i=4;i&amp;lt;=nobs;i++)&#10;  {&#10;    f2(i,log_sigma,log_p1,log_p2,A(i),B(i),A(i-1),B(i-1),Phi1,Phi2);&#10;  }&#10;&#10;  if (sd_phase())&#10;  {&#10;    sigma=exp(log_sigma);&#10;    p1=exp(log_p1);&#10;    p2=exp(log_p2);&#10;  }&#10;&#10;&#10;SEPARABLE_FUNCTION void f0( const prevariable&amp;amp; log_sigma, const prevariable&amp;amp; log_p1, const prevariable&amp;amp; log_p2, const prevariable&amp;amp; A2, const prevariable&amp;amp; B2)&#10;&#10;  dvariable sigma=exp(log_sigma);&#10;  dvariable p1=exp(log_p1);&#10;  dvariable p2=exp(log_p2);&#10;  f+=square(A2)+square(B2);&#10;  resids(2)=value(r);&#10;  f+=log_sigma+0.5*square(r/sigma);&#10;&#10;&#10;SEPARABLE_FUNCTION void f2(int i, const prevariable&amp;amp; log_sigma, const prevariable&amp;amp; log_p1, const prevariable&amp;amp; log_p2, const prevariable&amp;amp; Ai, const prevariable&amp;amp; Bi, const prevariable&amp;amp; Ai1, const prevariable&amp;amp; Bi1, const prevariable&amp;amp; Phi1,const prevariable&amp;amp; Phi2)&#10;&#10;  dvariable sigma=exp(log_sigma);&#10;  dvariable p1=exp(log_p1);&#10;  dvariable p2=exp(log_p2);&#10;&#10;  dvariable r=Y(i)-Ai-Bi*Y(i-1);&#10;  resids(i)=value(r);&#10;  f+=log_sigma+0.5*square(r/sigma);&#10;  dvariable nu_1i=(Ai-Phi1*Ai1);&#10;  dvariable nu_2i=(Bi-Phi2*Bi1);&#10;  f+=log_p1+0.5*square(nu_1i/p1);&#10;  f+=log_p2+0.5*square(nu_2i/p2);&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;There is a &quot;gotcha&quot; with this approach.&#10;Note that one can set A(i) such that&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Y(i)-A(i)-B(i)*Y(i-1)=0&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Then if you let sigma-&gt;0 the log-likelihood -&gt; infinity.&#10;So the &quot;real&quot; answer is a local maximum. You can stabilize the&#10;estimation by putting a lower bound on sigma or keeping sigma fixed&#10;at a reasonable value for a while. &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2010-11-11T20:45:50.343" Id="4456" LastActivityDate="2010-11-12T18:10:48.460" LastEditDate="2010-11-12T18:10:48.460" LastEditorUserId="1585" OwnerUserId="1585" ParentId="4334" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;For mocking up what you want the survey to look like for a programmer, I would definitely just code it up in HTML.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;HTML is easy in general; for things like this, it will be dead simple.  I don't think you'd need to fuss with any CSS to make something useful for your programmer.  Conversely...&lt;/li&gt;&#10;&lt;li&gt;You or your programmer can use CSS to drastically change the appearance of the survey without needing to edit the individual elements.&lt;/li&gt;&#10;&lt;li&gt;Version control becomes very easy - just use something like &lt;a href=&quot;http://git-scm.com/&quot; rel=&quot;nofollow&quot;&gt;git&lt;/a&gt;, &lt;a href=&quot;http://subversion.tigris.org/&quot; rel=&quot;nofollow&quot;&gt;SVN&lt;/a&gt;, or &lt;a href=&quot;http://www.fossil-scm.org/&quot; rel=&quot;nofollow&quot;&gt;Fossil&lt;/a&gt; that will let you easily distribute the most recent version of the survey and will let you track exactly which changes were made, when, and by whom.&lt;/li&gt;&#10;&lt;li&gt;It's free!&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="1" CreationDate="2010-11-11T22:02:31.417" Id="4461" LastActivityDate="2010-11-11T22:02:31.417" OwnerUserId="71" ParentId="4454" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Here's some thoughts on what I'd do using relatively simple methods (i.e. avoiding frailty models, which I admit I've never used and don't really understand, so someone else may like to provide an answer involving them). I'm assuming you don't have other forms of censoring apart from the end of the experiment and that there are no time-dependent exposures (i.e. the treatment is either constant or applied only at the start before any deaths have taken place)&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Do some descriptive statistics and Kaplan-Meier plots ignoring the issue of dependence and therefore without reporting or displaying standard errors, confidence intervals or p-values.&lt;/li&gt;&#10;&lt;li&gt;Ignore the time component and just count the number of deaths out of the total starting number in each aquarium. Fit a generalized linear model with a binomial distribution to these counts, using either a logistic link function (to give odds ratios) or a log link function (giving easier-to-interpret risk ratios at the price of potential problems with fitting the model). I think this is along the same lines as the analysis you said you're considering in the first comment to the question. As your mortalities are reasonably low, the loss in power over a full  survival analysis with frailty modelling will probably be modest. This overcomes the dependence issue as you are using each aquarium as the unit of analysis instead of each individual. &lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="1" CreationDate="2010-11-12T13:22:26.720" Id="4479" LastActivityDate="2010-11-12T13:22:26.720" OwnerUserId="449" ParentId="4465" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;If you reorder a data frame the row.names with be reordered to match. Note that if you haven't supplied rownames, R makes them up for you as characters &lt;code&gt;as.character(1:nrow(obj))&lt;/code&gt;, where &lt;code&gt;obj&lt;/code&gt; is your data frame. Hence these are row &lt;em&gt;names&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; df &amp;lt;- data.frame(A = 5:1, B = LETTERS[1:5])&#10;&amp;gt; df&#10;  A B&#10;1 5 A&#10;2 4 B&#10;3 3 C&#10;4 2 D&#10;5 1 E&#10;&amp;gt; df[order(df$A),]&#10;  A B&#10;5 1 E&#10;4 2 D&#10;3 3 C&#10;2 4 B&#10;1 5 A&#10;&amp;gt; df[sample(5),]&#10;  A B&#10;3 3 C&#10;2 4 B&#10;4 2 D&#10;1 5 A&#10;5 1 E&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;But I could just as well have applied some rownames:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; rownames(df) &amp;lt;- letters[1:5]&#10;&amp;gt; df&#10;  A B&#10;a 5 A&#10;b 4 B&#10;c 3 C&#10;d 2 D&#10;e 1 E&#10;&amp;gt; df[sample(5),]&#10;  A B&#10;d 2 D&#10;c 3 C&#10;a 5 A&#10;b 4 B&#10;e 1 E&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;You would need to show us how your data were read in, example data, and what code you are using if the above doesn't answer your Q.&lt;/p&gt;&#10;&#10;&lt;p&gt;If the rownames are not important, ignore them or do:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;rownames(df) &amp;lt;- NULL&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2010-11-12T16:35:15.587" Id="4482" LastActivityDate="2010-11-12T16:35:15.587" OwnerUserId="1390" ParentId="4480" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;If you are interested in the virtual machine route, I think it would be doable via a small linux distribution with the specific version of R and packages installed.  Data is included, along with scripts, and package the whole thing in a &lt;a href=&quot;http://www.virtualbox.org/&quot; rel=&quot;nofollow&quot;&gt;virtual box&lt;/a&gt; file.&lt;/p&gt;&#10;&#10;&lt;p&gt;This does not get around hardware problems mentioned earlier such as the Intel CPU bug.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-11-12T17:35:23.330" Id="4484" LastActivityDate="2010-11-12T17:35:23.330" OwnerUserId="1965" ParentId="4466" PostTypeId="2" Score="7" />
&lt;/p&gt;&#10;&#10;&lt;p&gt;Given&lt;/p&gt;&#10;&#10;&lt;p&gt;$$MS_x = SS_x/df_x$$
  
  
  
  <row AnswerCount="2" Body="&lt;p&gt;Lets say we have $T$ tags and $N$ articles and lets say that for each tag $t_{i}$ we know that it has tagged $n_{i}$ articles. Meaning that, frequency($t_{i}$)=$n_{i}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Given the above information how could I compute the probability P($t_{i}$ to tag a new article) or more generally how could I go about scoring the &quot;importance&quot; of each tag in Tag set ?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-11-13T22:59:12.890" FavoriteCount="3" Id="4515" LastActivityDate="2011-04-28T13:06:39.350" LastEditDate="2011-04-28T13:06:39.350" LastEditorUserId="1643" OwnerDisplayName="user1975" PostTypeId="1" Score="3" Tags="&lt;probability&gt;&lt;data-mining&gt;&lt;predictive-models&gt;&lt;scores&gt;" Title="How to measure/weight the importance of tags?" ViewCount="343" />
  <row Body="&lt;p&gt;Actually, if you want ranks, try the RANK command.  Or use the menus: Transform&gt;Rank Cases.  It can generate ranks within groups.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you prefer the AGGREGATE approach, note that it does NOT require sorting first.&lt;/p&gt;&#10;&#10;&lt;p&gt;HTH,&#10;Jon Peck&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-11-13T23:21:12.727" Id="4516" LastActivityDate="2010-11-13T23:21:12.727" OwnerUserId="1971" ParentId="4396" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;I keep a separate repository for each project, with a project being centered around a particular data set or question being addressed. The repo contains the data, code, and Sweave documents/plots that explain and express the results.&lt;/p&gt;&#10;&#10;&lt;p&gt;I maintain a separate repo for each discrete publication or presentation because&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;A single project may result in multiple publications or presentations. Once a publication is out or you've given the presentation, you are essentially &quot;done&quot; with the contents of that repo so they don't need to be dragged around with the project.&lt;/li&gt;&#10;&lt;li&gt;A &lt;em&gt;output&lt;/em&gt; (publication/presentation/chapter) may contain data from more than one project.&lt;/li&gt;&#10;&lt;li&gt;Not all of the results from a project will end up in a particular piece of &lt;em&gt;output&lt;/em&gt;. &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Code that is reusable across projects gets its own repo, as well. If I come up with a new &amp;amp; discrete question using data that is already in one repo, I'll copy that data to a new repo.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you want to be really strict about it, many version control systems offer the idea of &quot;subprojects&quot;, but I've found that to be overkill.&lt;/p&gt;&#10;" CommentCount="1" CommunityOwnedDate="2010-11-14T17:50:08.343" CreationDate="2010-11-14T17:50:08.343" Id="4525" LastActivityDate="2010-11-14T17:50:08.343" OwnerUserId="1916" ParentId="4500" PostTypeId="2" Score="2" />
&lt;/p&gt;&#10;&#10;&lt;p&gt;shows that the chance of forming a word with two $x_1$'s and two $x_2$'s (which can occur in four ways out of the 36 possible ways: $x_1x_2x_1x_2$, $x_1x_2x_2x_1$, $x_2x_1x_1x_2$, and $x_2x_1x_2x_1$) is 1/9.&lt;/p&gt;&#10;&#10;&lt;p&gt;When we set some of the $x_i$ equal to zero, they drop out of $f_r^k$ altogether, and when we set the remaining $x_i$ equal to one, what remains is the sum of their coefficients.  Thus we could hope to find the probability of having $m$ distinct letters by setting exactly $m$ of the $x_i$ to one (and the remainder to zero) and summing the value of $f_r^k$ over all such possibilities.  (This is easily done because the symmetry of $f_r$ implies all these values are equal, so we need only compute $f_r^k(1,\ldots,1,0,\ldots,0)$ and multiply that by ${n}\choose{m}$.)  Unfortunately, this also over-counts words having fewer than $m$ distinct letters, so we have to apply the &lt;a href=&quot;http://en.wikipedia.org/wiki/Inclusion%E2%80%93exclusion_principle&quot; rel=&quot;nofollow&quot;&gt;Principle of Inclusion-Exclusion&lt;/a&gt; to extract the desired value.  The punchline is that $f_r$ is easy to evaluate; when we set $n-m$ of its arguments to $0$, there remain ${n}\choose{m}$ terms, all of which will evaluate to $1$, whence&lt;/p&gt;&#10;&#10;&lt;p&gt;$$f_r^k(1,\ldots,1,0,\ldots,0) = {{m}\choose{k}} / {{n}\choose{k}}.$$
&#10;&amp;amp;+ (m+1)(n-m+1)p_{n,k,2}(m+1) \cr
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;After running a study based on interactive genetic algorithms, I have a univariate data file containing multiple participants each doing multiple generations (blocks) of multiple trials.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there an established approach used to analyse this?&lt;/p&gt;&#10;&#10;&lt;p&gt;I've looked at using a grand mean on each of the component variables. I've also looked at the modal values of these variables after filtering out all but the final generation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Neither approach seems to properly capture the richness of the data, so I'd like to know how other people would handle this.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-11-15T16:23:13.670" Id="4542" LastActivityDate="2010-11-16T17:23:45.307" OwnerUserId="1950" PostTypeId="1" Score="2" Tags="&lt;genetic-algorithms&gt;" Title="Are there any recommended approaches for analysing data from genetic algorithms?" ViewCount="147" />
  <row AnswerCount="41" Body="&lt;p&gt;I'm a grad student in psychology, and as I pursue more and more independent studies in statistics, I am increasingly amazed by the inadequacy of my formal training. Both personal and second hand experience suggests that the paucity of statistical rigor in undergraduate and graduate training is rather ubiquitous within psychology. As such, I thought it would be useful for independent learners like myself to create a list of &quot;Statistical Sins&quot;, tabulating statistical practices taught to grad students as standard practice that are in fact either superseded by superior (more powerful, or flexible, or robust, etc) modern methods or shown to be frankly invalid. Anticipating that other fields might also experience a similar state of affairs, I propose a community wiki where we can collect a list of Statistical Sins across disciplines. Please submit one &quot;sin&quot; per answer.&lt;/p&gt;&#10;" CommentCount="6" CommunityOwnedDate="2010-11-15T20:28:53.713" CreationDate="2010-11-15T18:46:37.113" FavoriteCount="129" Id="4551" LastActivityDate="2014-12-09T13:19:40.583" LastEditDate="2012-03-30T13:30:25.143" LastEditorUserId="9007" OwnerUserId="364" PostTypeId="1" Score="132" Tags="&lt;best-practices&gt;&lt;big-list&gt;" Title="What are common statistical sins?" ViewCount="11570" />
  <row Body="&lt;p&gt;Analysis of rate data (accuracy, etc) using ANOVA, thereby assuming that rate data has Gaussian distributed error when it's actually binomially distributed.&#10;&lt;a href=&quot;http://dx.doi.org/10.1016/j.jml.2007.11.004&quot;&gt;Dixon (2008)&lt;/a&gt; provides a discussion of the consequences of this sin and exploration of more appropriate analysis approaches.&lt;/p&gt;&#10;" CommentCount="5" CommunityOwnedDate="2010-11-15T20:28:53.713" CreationDate="2010-11-15T19:12:03.287" Id="4555" LastActivityDate="2010-11-16T15:47:31.527" LastEditDate="2010-11-16T15:47:31.527" LastEditorUserId="364" OwnerUserId="364" ParentId="4551" PostTypeId="2" Score="19" />
  
  
  <row Body="&lt;p&gt;If you have counts low enough that the Yates Correction is a worry (as in your example), you probably should be using Fisher's exact test.  Otherwise, I recommend that after you use the chi-square test on a 2x2 table, you confirm your test with a log odds-ratio z-test.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-11-16T02:48:09.623" Id="4571" LastActivityDate="2010-11-16T02:48:09.623" OwnerUserId="5792" ParentId="4569" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="4576" AnswerCount="2" Body="&lt;p&gt;Are there good tutorials on object-oriented programming in R?&lt;/p&gt;&#10;&#10;&lt;p&gt;It would be great if it included the following:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;how to define a class;&lt;/li&gt;&#10;&lt;li&gt;differences between S3 and S4 classes;&lt;/li&gt;&#10;&lt;li&gt;operator overloading (I'd like to be able to write &lt;code&gt;a+b&lt;/code&gt; where &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; are instances of the class I have in mind).&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="2" CreationDate="2010-11-16T08:10:48.690" FavoriteCount="6" Id="4575" LastActivityDate="2010-11-16T08:30:14.563" OwnerUserId="439" PostTypeId="1" Score="11" Tags="&lt;r&gt;" Title="Tutorials on object-oriented programming in R" ViewCount="2198" />
  <row Body="&lt;p&gt;Hadley Wicham's wiki on &lt;a href=&quot;https://github.com/hadley/devtools/wiki/&quot;&gt;devtools&lt;/a&gt; is great resource for the necessary information in a concise form. However, if you want an exhaustive resource, the R language manual's &lt;a href=&quot;http://cran.r-project.org/doc/manuals/R-lang.html#Object_002doriented-programming&quot;&gt;OOP&lt;/a&gt; section may be helpful. I am sure more experienced members will have better suggestions.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-11-16T08:20:20.087" Id="4576" LastActivityDate="2010-11-16T08:20:20.087" OwnerUserId="1307" ParentId="4575" PostTypeId="2" Score="9" />
  
  <row Body="&lt;p&gt;Especially in epidemiology and public health - using arithmetic instead of logarithmic scale when reporting graphs of relative measures of association (hazard ratio, odds ratio or risk ratio).&lt;/p&gt;&#10;&#10;&lt;p&gt;More information &lt;a href=&quot;http://www.thelancet.com/journals/lancet/article/PIIS0140-6736%2810%2960541-7/fulltext&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="1" CommunityOwnedDate="2010-11-16T12:19:21.080" CreationDate="2010-11-16T12:19:21.080" Id="4589" LastActivityDate="2010-11-16T12:19:21.080" OwnerUserId="22" ParentId="4551" PostTypeId="2" Score="19" />
  <row Body="&lt;p&gt;Another option using Excel that gives you a fair amount of flexibility over the number &amp;amp; size of bins is the &lt;code&gt;=frequency(data_array, bins_array)&lt;/code&gt; function. It is an array function that expects two arguments. The first argument is your data, the second is your bins that you define.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's assume your data is in cells A1 - A1000 and create bins in cells B1 - B20. You would want to highlight cells C1 - C21 and then type something like &lt;code&gt;=FREQUENCY(A1:A100, B1:B21)&lt;/code&gt;. Unlike normal functions, array functions must be entered with the key combination &lt;code&gt;SHIFT + CTRL + ENTER&lt;/code&gt;. You should see the counts fill down for all bins, if not - you most likely only hit enter and the first cell is calculated.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are plenty of good tutorials online explaining this in more detail, here's a decent &lt;a href=&quot;http://www.exceluser.com/solutions/q0001/frequency.htm&quot; rel=&quot;nofollow&quot;&gt;one&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-11-16T14:27:29.113" Id="4591" LastActivityDate="2010-11-16T14:27:29.113" OwnerUserId="696" ParentId="4341" PostTypeId="2" Score="2" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I have run a factorial type test in a processing plant and have run a forward and backward step regression in R. &lt;/p&gt;&#10;&#10;&lt;p&gt;How can I use the regression results and the anova created from the regression to know what percent of the measured variation of the dependent variable was caused by the purposeful manipulation of the independent variables?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-11-16T22:11:41.093" Id="4600" LastActivityDate="2011-06-06T04:50:30.013" LastEditDate="2011-06-06T04:50:30.013" LastEditorUserId="183" OwnerDisplayName="Lynn Hales" PostTypeId="1" Score="3" Tags="&lt;regression&gt;&lt;anova&gt;&lt;interpretation&gt;" Title="Explaining variation in a dependent variable based on a factorial experiment" ViewCount="1187" />
  <row Body="&lt;p&gt;The &lt;strong&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Travelling_salesman_problem&quot; rel=&quot;nofollow&quot;&gt;travelling salesman problem&lt;/a&gt;&lt;/strong&gt; is surely an archetypal hard optimization problem. To quote Wikipedia, &quot;it is used as a benchmark for many optimization methods&quot;.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2010-11-16T22:29:48.383" Id="4602" LastActivityDate="2010-11-16T22:29:48.383" OwnerUserId="449" ParentId="4595" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;Depends on what level you're after. At a postgraduate level, the one i've most often seen referenced and recommended, and have therefore looked at most myself, is:&lt;/p&gt;&#10;&#10;&lt;p&gt;Wooldridge, Jeffrey M. &lt;em&gt;Econometric Analysis of Cross Section and Panel Data&lt;/em&gt;. MIT Press, 2001. ISBN &lt;a href=&quot;http://en.wikipedia.org/w/index.php?title=Special%3ABookSources&amp;amp;isbn=9780262232197&quot;&gt;9780262232197&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Most of what little I know about econometrics I learnt from this book. 776 pages without a single graph.&lt;/p&gt;&#10;" CommentCount="1" CommunityOwnedDate="2010-11-17T14:15:50.470" CreationDate="2010-11-17T07:56:14.970" Id="4617" LastActivityDate="2010-11-17T07:56:14.970" OwnerUserId="449" ParentId="4612" PostTypeId="2" Score="7" />
  <row AnswerCount="2" Body="&lt;p&gt;I have two surveys of two separate populations (I don't know that they are necessarily distinct, but they are from two different databases) that ask a similar set of questions.  Some questions are basic demographics (e.g. age, income), while other questions are a bit more detailed or about their opinions (e.g. brand preferences, spending habits).&lt;/p&gt;&#10;&#10;&lt;p&gt;How do I prove statistically that the two populations are &quot;the same,&quot; or at least comparable?  I know that I can do a t-test for individual questions, but is there a way to establish similarity on more than one dimension?&lt;/p&gt;&#10;&#10;&lt;p&gt;The goal is to combine the surveys from these two populations into one series of survey data.  For example, we may run survey A every six months, but we run survey B every month, except when we run survey A.  I would then like to combine the results from survey A and survey B to have a monthly series of survey data.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-11-17T08:08:19.927" Id="4619" LastActivityDate="2010-11-18T13:06:58.253" LastEditDate="2010-11-17T14:37:06.837" LastEditorUserId="930" OwnerUserId="1195" PostTypeId="1" Score="1" Tags="&lt;psychometrics&gt;&lt;survey&gt;" Title="Establishing that the population sampled of two separate surveys is the same" ViewCount="190" />
  <row AcceptedAnswerId="4626" AnswerCount="1" Body="&lt;p&gt;I am fitting an L1-regularized linear regression to a very large dataset (with n&gt;&gt;p.) The variables are known in advance, but the observations arrive in small chunks. I would like to maintain the lasso fit after each chunk.&lt;/p&gt;&#10;&#10;&lt;p&gt;I can obviously re-fit the entire model after seeing each new set of observations. This, however, would be pretty inefficient given that there is a lot of data. The amount of &lt;i&gt;new&lt;/i&gt; data that arrives at each step is very small, and the fit is unlikely to change much between steps.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there anything I can do to reduce the overall computational burden?&lt;/p&gt;&#10;&#10;&lt;p&gt;I was looking at the LARS algorithm of Efron et al., but would be happy to consider any other fitting method if it can be made to &quot;warm-start&quot; in the way described above.&lt;/p&gt;&#10;&#10;&lt;p&gt;Notes:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;I am mainly looking for an algorithm, but pointers to existing software packages that can do this may also prove insightful.&lt;/li&gt;&#10;&lt;li&gt;In addition to the current lasso trajectories, the algorithm is of course welcome to keep other state.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Bradley Efron, Trevor Hastie, Iain&#10;  Johnstone and Robert Tibshirani,&#10;  &lt;a href=&quot;http://projecteuclid.org/euclid.aos/1083178935&quot;&gt;Least Angle Regression&lt;/a&gt;, &lt;em&gt;Annals&#10;  of Statistics&lt;/em&gt; (with discussion)&#10;  (2004) 32(2), 407--499.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="0" CreationDate="2010-11-17T10:33:27.813" FavoriteCount="4" Id="4624" LastActivityDate="2014-06-12T15:07:53.047" LastEditDate="2010-11-17T15:53:54.630" LastEditorUserId="439" OwnerUserId="439" PostTypeId="1" Score="11" Tags="&lt;regression&gt;&lt;lasso&gt;" Title="Updating the lasso fit with new observations" ViewCount="337" />
  
  
  <row Body="&lt;p&gt;As @onestop writes, the GM and GSD are natural parameters for a lognormal distribution.  However, they can be estimated from the arithmetic mean ($\mu$) and (usual) SD ($\sigma$) just by solving the &lt;a href=&quot;http://en.wikipedia.org/wiki/Log-normal_distribution&quot; rel=&quot;nofollow&quot;&gt;formulas&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\mu = \exp(\nu+ \tau^2/2) \text{ and}$$
&lt;/p&gt;&#10;&#10;&lt;p&gt;for $\nu$ (the logarithm of the GM) and $\tau$ (the logarithm of the GSD).  Evidently&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\tau^2 = \log{\frac{\sigma^2 + \mu^2}{\mu^2}} \text{ and}$$
&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\nu = \frac{1}{2} \log{\frac{\mu^4}{\sigma^2 + \mu^2}}.$$
  
&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, this is just notation. No actual information has been put in. If you're asking how can we model the Dow Jones Industrial Average, I'm afraid this is not something that can be given a quick answer to. I'd suggest you start reading up on the subject because it involves a lot of math. Maybe an easy start:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Stock_market&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Stock_market&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Expect the learning curve to be steep though.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;The important thing to understand in your case though is that just because you can express something with % it doesn't mean it is a probability. It could correspond to a relative change, for instance, a relative change in the value of a stock. That is not a probability. But there is a probability associated to the occurance of that change.&lt;/strong&gt;&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2010-11-17T21:36:42.957" Id="4650" LastActivityDate="2010-11-17T22:22:02.307" LastEditDate="2010-11-17T22:22:02.307" LastEditorUserId="2036" OwnerUserId="2036" ParentId="4649" PostTypeId="2" Score="1" />
  <row AnswerCount="3" Body="&lt;p&gt;I'm trying to run a basic gradient descent algorithm with a absolute loss function. I can get it to converge to a good solution by it requires a much lower step size and more iterations than had I used square loss. Is this normal? Should I expect absolute loss to take a longer time to come to a good solution or potentially oscillate around a solution more than say squared loss?  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-11-17T23:10:31.310" FavoriteCount="1" Id="4652" LastActivityDate="2010-11-19T05:14:16.130" LastEditDate="2010-11-19T05:03:38.713" LastEditorUserId="2023" OwnerUserId="2023" PostTypeId="1" Score="5" Tags="&lt;optimization&gt;&lt;loss-functions&gt;" Title="Gradient descent oscillating a lot. Have I chosen my step direction incorrectly?" ViewCount="813" />
&lt;/p&gt;&#10;&#10;&lt;p&gt;An analogous property of the median has been presented (&lt;a href=&quot;http://dx.doi.org/10.1016/j.spl.2004.11.010&quot; rel=&quot;nofollow&quot;&gt;Merkle et al 2005&lt;/a&gt;, &lt;a href=&quot;http://milanmerkle.com/documents/radovi/SPL-71.pdf&quot; rel=&quot;nofollow&quot;&gt;pdf&lt;/a&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;motivation&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I have a nonlinear &lt;a href=&quot;http://www.esajournals.org/doi/full/10.1890/0012-9615%282001%29071%5B0557%3AAMFSVD%5D2.0.CO%3B2&quot; rel=&quot;nofollow&quot;&gt;function&lt;/a&gt; of positive random variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;In practice, I find that the function of the medians provides a much better estimate of the median of the function than does the estimate of the mean of the function from the function of the means. I am interested in learning the conditions for which this is true.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;question&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;Under what conditions will the function of a median be closer to the median of a function than the mean of a function is to a function of the mean?&lt;/p&gt;&#10;&#10;&lt;p&gt;Specifically for what types of $f(x)$ and $x$ is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\mu (f(x)) - f(\mu (x)) &amp;gt; m (f(x)) - f(m (x))$$
&lt;/p&gt;&#10;&#10;&lt;p&gt;In other words, the cdf of $Y$ is given by:&lt;/p&gt;&#10;&#10;&lt;p&gt;$F_Y(y) = F_X(\sqrt{y}) - F_X(-\sqrt{y})$
  <row Body="&lt;p&gt;The Chi Square test (Goodness-of-Fit test) will be very good at comparing the tails of two distributions since it is structured to compare two distributions by buckets of values (graphically represented by a histogram).  And, the tails will consist in the far most buckets.&lt;/p&gt;&#10;&#10;&lt;p&gt;Even though this test focuses on the whole distribution, not just the tail you can readily observe how much of the Chi Square value or divergence is derived by the difference in the tails's fatness.&lt;/p&gt;&#10;&#10;&lt;p&gt;Watch that the derived histogram may actually give you visually much more information regarding the respective fatness of the tails than any test related statistical significance.  It is one thing to state that tails fatness are statistically different.  It is another to visually observe it.  They say a picture is worth a thousand words.  Sometimes it is also worth a thousand numbers (it makes sense given that graphs encapsulate all the numbers).      &lt;/p&gt;&#10;" CommentCount="7" CreationDate="2010-11-18T03:06:39.537" Id="4660" LastActivityDate="2010-11-18T19:16:24.867" LastEditDate="2010-11-18T19:16:24.867" LastEditorUserId="1329" OwnerUserId="1329" ParentId="4658" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;Following on from the post by Stephan Kolassa (I can't add this as a comment), I have some alternative code for a simulation.  This uses the same basic structure, but is exploded a bit more, so perhaps it is a little easier to read.  It also is based on the code by &lt;a href=&quot;http://sas-and-r.blogspot.com/2009/06/example-72-simulate-data-from-logistic.html&quot; rel=&quot;nofollow&quot;&gt;Kleinman and Horton&lt;/a&gt; to simulate the logistic regression.  &lt;/p&gt;&#10;&#10;&lt;p&gt;nn is the number in the sample.  The covariate should be continuously normally distributed, and standardized to mean 0 and sd 1.  We use rnorm(nn) to generate this.  We select an odds ratio and store it in odds.ratio.  We also pick a number for the intercept.  Choice of this number governs what proportion of the sample experience the &quot;event&quot; (e.g. 0.1, 0.4, 0.5).  You have to play around with this number until you get the right proportion.  The following code gives you a proportion of 0.1 with a sample size of 950 and an OR of 1.5:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;nn &amp;lt;- 950&#10;runs &amp;lt;- 10000&#10;intercept &amp;lt;- log(9)&#10;odds.ratio &amp;lt;- 1.5&#10;beta &amp;lt;- log(odds.ratio)&#10;proportion  &amp;lt;-  replicate(&#10;              n = runs,&#10;              expr = {&#10;                  xtest &amp;lt;- rnorm(nn)&#10;                  linpred &amp;lt;- intercept + (xtest * beta)&#10;                  prob &amp;lt;- exp(linpred)/(1 + exp(linpred))&#10;                  runis &amp;lt;- runif(length(xtest),0,1)&#10;                  ytest &amp;lt;- ifelse(runis &amp;lt; prob,1,0)&#10;                  prop &amp;lt;- length(which(ytest &amp;lt;= 0.5))/length(ytest)&#10;                  }&#10;            )&#10;summary(proportion)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;summary(proportion) confirms that the proportion is ~ 0.1&lt;/p&gt;&#10;&#10;&lt;p&gt;Then using the same variables, the power is calculated over 10000 runs:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;result &amp;lt;-  replicate(&#10;              n = runs,&#10;              expr = {&#10;                  xtest &amp;lt;- rnorm(nn)&#10;                  linpred &amp;lt;- intercept + (xtest * beta)&#10;                  prob &amp;lt;- exp(linpred)/(1 + exp(linpred))&#10;                  runis &amp;lt;- runif(length(xtest),0,1)&#10;                  ytest &amp;lt;- ifelse(runis &amp;lt; prob,1,0)&#10;                  summary(model &amp;lt;- glm(ytest ~ xtest,  family = &quot;binomial&quot;))$coefficients[2,4] &amp;lt; .05&#10;                  }&#10;            )&#10;print(sum(result)/runs)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I think that this code is correct - I checked it against the examples given in Hsieh, 1998 (table 2), and it seems to agree with the three examples given there.  I also tested it against the example on p 342 - 343 of Hosmer and Lemeshow, where it found a power of 0.75 (compared to 0.8 in Hosmer and Lemeshow).  So it may be that in some circumstances this approach underestimates power.  However, when I've run the same example in this &lt;a href=&quot;http://biostat.hitchcock.org/MeasurementError/Analytics/PowerCalculationsforLogisticRegression.asp&quot; rel=&quot;nofollow&quot;&gt;on-line calculator&lt;/a&gt;, I've found that it agrees with me and not the result in Hosmer and Lemeshow.&lt;/p&gt;&#10;&#10;&lt;p&gt;If anyone can tell us why this is the case, I'd be interested to know.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2010-11-18T14:47:29.653" Id="4681" LastActivityDate="2010-12-03T11:40:53.517" LastEditDate="2010-12-03T11:40:53.517" LastEditorUserId="1991" OwnerUserId="1991" ParentId="2988" PostTypeId="2" Score="3" />
&lt;/p&gt;&#10;&#10;&lt;p&gt;(Why is this?  Here is a non-rigorous but memorable demonstration.  The chance that $x_{[k]}$ lies between $p$ and $p + dp$ is the chance that out of $n+1$ uniform values, $k$ of them lie between $0$ and $p$, at least one of them lies between $p$ and $p + dp$, and the remainder lie between $p + dp$ and $1$.  To first order in the infinitesimal $dp$ we only need to consider the case where exactly one value (namely, $x_{[k]}$ itself) lies between $p$ and $p + dp$ and therefore $n - k$ values exceed $p + dp$.  Because all values are independent and uniform, this probability is proportional to $p^k (dp) (1 - p - dp)^{n-k}$.  To first order in $dp$ this equals $p^k(1-p)^{n-k}dp$, precisely the integrand of the Beta distribution.  The term $\frac{1}{B(k+1, n-k+1)}$ can be computed directly from this argument as the multinomial coefficient ${n+1}\choose{k,1, n-k}$ or derived indirectly as the normalizing constant of the integral.)&lt;/p&gt;&#10;&#10;&lt;p&gt;By definition, the event $x_{[k]} \le p$ is that the $k+1^\text{st}$ value does not exceed $p$.  Equivalently, &lt;em&gt;at least&lt;/em&gt; $k+1$ of the values do not exceed $p$: this simple (and I hope obvious) assertion provides the intuition you seek. The probability of the equivalent statement is given by the Binomial distribution,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\Pr[\text{at least }k+1\text{ of the }x_i \le p] = \sum_{j=k+1}^{n+1}{{n+1}\choose{j}} p^j (1-p)^{n+1-j}.$$
&#10;\text{Prob.} &amp;amp;b_1 &amp;amp;b_2 &amp;amp;B &amp;amp;\text{Observation} &amp;amp;\hat{B} &amp;amp;\text{Error} \cr
&lt;/p&gt;&#10;&#10;&lt;p&gt;In computing the expected absolute error I have assumed $\pi \le 1/2$: we will favor sampling the cheaper bit whenever possible.&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose, for example, $k = 3/2$.  That is, we aim to find a sampling scheme that keeps the absolute error to $3/2$ or less with &quot;high probability&quot; while minimizing the &lt;em&gt;expected&lt;/em&gt; cost.  (I realize this choice of $k$ is artificial because we might attempt to improve the estimator--at risk of biasing it slightly--by constraining its estimates to 0, 1, or 2; but the purpose here is to look ahead to a situation with large $n$, where such improvements will be unlikely.  The &lt;em&gt;mathematical patterns&lt;/em&gt; are important in this example, not its (lack of) realism.)  Evidently we would like to minimize the chance of paying for the expensive bit; that is, to make $\pi$ as small as possible.  The final column in the previous table constrains $\pi$; it implies that &lt;/p&gt;&#10;&#10;&lt;p&gt;$$2(1-\pi) \le k,\quad 2\pi \le k,\quad 2 - 4\pi \le k.$$
&lt;/p&gt;&#10;&#10;&lt;p&gt;and so what is being compared in the Bayes factor $K$ is the ratio of the average probability over all the samples for each model? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-11-18T22:54:30.957" Id="4698" LastActivityDate="2010-11-30T09:33:50.900" OwnerUserId="2052" PostTypeId="1" Score="2" Tags="&lt;probability&gt;&lt;bayesian&gt;&lt;modeling&gt;" Title="Bayesian model comparison for randomly sampled sets of models" ViewCount="169" />
  <row Body="&lt;p&gt;Fixed effect: Something the experimenter directly manipulates and is often repeatable, e.g., drug administration - one group gets drug, one group gets placebo.&lt;/p&gt;&#10;&#10;&lt;p&gt;Random effect: Source of random variation / experimental units e.g., individuals drawn (at random) from a population for a clinical trial.&#10;Random effects estimates the variability&lt;/p&gt;&#10;&#10;&lt;p&gt;Mixed effect: Includes both, the fixed effect in these cases are estimating the population level coefficients, while the random effects can account for individual differences in response to an effect, e.g., each person receives both the drug and placebo on different occasions, the fixed effect estimates the effect of drug, the random effects terms would allow for each person to respond to the drug differently.&lt;/p&gt;&#10;&#10;&lt;p&gt;General categories of mixed effects - repeated measures, longitudinal, hierarchical, split-plot.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-11-19T04:11:03.943" Id="4703" LastActivityDate="2010-11-19T04:11:03.943" OwnerUserId="966" ParentId="4700" PostTypeId="2" Score="14" />
  
  
  <row Body="&lt;p&gt;Based on the paper you linked to, I would argue that the term EM usually refers to the &quot;soft&quot; version. The key distinction seems to be that instead of taking an expectation in the E-step, the &quot;hard&quot; version finds a mode.&lt;/p&gt;&#10;&#10;&lt;p&gt;A good explanation of the distinction is available in chapters 20-22 of David Mackay's book (which is &lt;a href=&quot;http://www.inference.phy.cam.ac.uk/mackay/itila/&quot; rel=&quot;nofollow&quot;&gt;available online&lt;/a&gt;).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-11-19T14:31:20.210" Id="4728" LastActivityDate="2010-11-19T14:31:20.210" OwnerUserId="495" ParentId="4685" PostTypeId="2" Score="2" />
&lt;/p&gt;&#10;&#10;&lt;p&gt;(The coefficient when $|A| = n$ appears to be undefined but actually is zero; the sum really needs to extend only over the small subsets where randomization is actually needed.)&lt;/p&gt;&#10;&#10;&lt;p&gt;We have obtained a &lt;em&gt;linear program&lt;/em&gt; for the sample probabilities $\pi_A$; to wit,&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Minimize&lt;/strong&gt; the expected cost $\sum_{A}{\pi_A c(A)}$
  
  <row Body="&lt;p&gt;It isn't entirely clear from your question what sort of 'weight' you are talking about.  But I imagine it is a simple matter of wanting to count certain observations more than than others... &lt;/p&gt;&#10;&#10;&lt;p&gt;If you wanted to, and your weights were integer values (or you can find the lowest common denominator to multiply by that will give you integer values), you could simply expand your data out to match the weights.  That is, a data point with a weight of 2 could be represented twice in your data array.  This is fine for descriptives such as mean, median, kurtosis, and skew.  However, it may be be problematic for calculations such as a sample estimate of the standard deviation where N matters and the difference between a raw N-1 and a N-1 where N is representative of the restructured array might be meaningful.&lt;/p&gt;&#10;&#10;&lt;p&gt;The only shortcut I can think of that might apply is to multiply your array by the weights and then analyze those results.  For the mean, to renormalize the value, you will need to divide the result by (sum)Weights/N_original.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, for the median, kurtosis, and skew you won't be able to (readily) use this technique and I think you will have to revert to altering your function to produce new data arrays.&lt;/p&gt;&#10;&#10;&lt;p&gt;Good luck.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-11-19T20:34:51.880" Id="4740" LastActivityDate="2010-11-19T20:34:51.880" OwnerUserId="196" ParentId="4737" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;There &lt;em&gt;are&lt;/em&gt; symmetric confidence intervals for the Binomial distribution: asymmetry is not forced on us, despite all the reasons already mentioned.  The symmetric intervals are usually considered inferior in that&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Although they are &lt;em&gt;numerically&lt;/em&gt; symmetric, they are not symmetric &lt;em&gt;in probability&lt;/em&gt;: that is, their one-tailed coverages differ from each other.  This--a necessary consequence of the possible asymmetry of the Binomial distribution--is the crux of the matter.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Often one endpoint has to be unrealistic (less than 0 or greater than 1), as @Rob Hyndman points out.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Having said that, I suspect that numerically symmetric CIs might have some good properties, such as tending to be shorter than the probabilistically symmetric ones in some circumstances.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2010-11-19T22:03:50.017" Id="4744" LastActivityDate="2010-11-19T22:03:50.017" OwnerUserId="919" ParentId="4713" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;A moving standard deviation sounds like a reasonable thing to use... here is a toy example in poorly written untested poorly optimized pseudo-C, things may go out of bounds or not work as I expect, but you should get the general idea:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;const int NPixelColumns; //The number of pixels columns&#10;const int WindowSize; //The size of the moving window for the standard deviation&#10;double BrightnessVals[NPixelColumns]; //Someplace to store your data initially&#10;int startIndex; //Where the moving window starts&#10;int lcv; //Generic loop control variable&#10;&#10;for (startIndex = 0; startIndex++; startIndex &amp;lt; (NPixelColumns-WindowSize))&#10;{&#10;   int endIndex = startIndex + (WindowSize-1);&#10;   double sum; //the sum of values in the windows&#10;   double xbar; //the mean in the window&#10;   double deltasq[WindowSize]; //the squared differences between the mean and the value&#10;   double SS=0; //the sum of deltasq&#10;   for (lcv = startIndex; lcv++; lcv &amp;lt;= endIndex)&#10;   {&#10;       sum += BrightnessVals[lcv];&#10;   }&#10;   xbar = sum/WindowSize;&#10;   for (lcv = 0; lcv++; lcv &amp;lt; WindowSize)&#10;   {&#10;       deltasq[lcv] = pow(BrightnessVals[startIndex+lcv]-xbar,2);&#10;       SS += deltasq[lcv];&#10;   }&#10;   printf(&quot;At step %i the moving SD is: %f&quot;, startIndex, SS/sqrt(WindowSize-1));&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In R this kind of thing is a snap:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;sdwindow &amp;lt;- function(start,end,data)&#10;{&#10;    return(sd(data[start:end]))&#10;}&#10;nsamp &amp;lt;- 1000 #The number of samples to look over&#10;windowsize &amp;lt;- 10 #The size of the window to get the SD of&#10;x &amp;lt;- rnorm(nsamp) #Sample data&#10;start &amp;lt;- 1:(nsamp-windowsize) #starting points for the window&#10;end &amp;lt;- (windowsize+1):nsamp #ending points for the window&#10;doit &amp;lt;- Vectorize(sdwindow, vectorize.args = c(&quot;start&quot;,&quot;end&quot;)) #save me the trouble of figuring out mapply for the nth time.&#10;doit(start,end,x) #generate the result&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2010-11-20T07:53:17.650" Id="4754" LastActivityDate="2010-11-20T08:07:46.623" LastEditDate="2010-11-20T08:07:46.623" LastEditorUserId="196" OwnerUserId="196" ParentId="4597" PostTypeId="2" Score="1" />
  
&lt;/p&gt;&#10;&#10;&lt;p&gt;assuming that, in the cases of 2 or 3 &quot;offspring&quot; their extinction probabilities are IID.  This equation has two feasible roots, $1$ and $\sqrt{2}-1$.  Someone smarter than me might be able to explain why the $1$ isn't plausible.&lt;/p&gt;&#10;&#10;&lt;p&gt;Jobs must be getting tight -- what kind of interviewer expects you to solve cubic equations in your head?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2010-11-21T11:47:10.303" Id="4772" LastActivityDate="2010-11-21T18:41:24.833" LastEditDate="2010-11-21T18:41:24.833" LastEditorUserId="88" OwnerUserId="5792" ParentId="4768" PostTypeId="2" Score="20" />
  
  <row AnswerCount="4" Body="&lt;p&gt;I have some cumulative frequency data. A line $y=ax+b$ looks like it fits the data extremely well, but there is cyclic/periodic wiggle in the line. I would like to estimate when the cumulative frequency will reach a certain value $c$. When I plot the residuals vs. fitted values, I get a beautiful sinusoidal behavior. &lt;/p&gt;&#10;&#10;&lt;p&gt;Now, to add another complication, note that in the residuals plots&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/3GIg5.png&quot; alt=&quot;alt text&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;there are two cycles that have lower values than the others, which represents a weekend effect that also must be taken into account.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, where do I go from here? How can I combine some cosine, sine, or cyclic term into a regression model to approx. estimate when the cumulative frequency will equal $c$?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-11-21T21:21:46.837" FavoriteCount="2" Id="4783" LastActivityDate="2010-11-22T17:43:59.917" LastEditDate="2010-11-21T23:08:42.967" LastEditorUserId="88" OwnerUserId="2083" PostTypeId="1" Score="9" Tags="&lt;time-series&gt;&lt;regression&gt;" Title="How to add periodic component to linear regression model?" ViewCount="3377" />
  <row Body="&lt;p&gt;R and SAS have each there pros and cons. I think more statisticians need to embrace the fact that lots of great statistical software is available, rather than endlessly bicker about which is superior.&lt;/p&gt;&#10;&#10;&lt;p&gt;R is free. SAS is very expensive. R gives you the ability to do just about anything. SAS may or may not. R and has amazing graphical abilities. Seeing SAS graphics makes it feel like 1985 all over again. SAS has great customer support. R support = hours of searching mailing list archives. Also with a name like &quot;R&quot;, search engine results are often poor. R is extremely slow and does not deal well with large data sets. SAS does fine with large data sets. SAS tends to be more robust. In my experience, when it comes to mixed effects modeling or anything involving design of experiments (such as analyzing crossover designs), SAS is superior.&lt;/p&gt;&#10;&#10;&lt;p&gt;For large scale, brute force simulations, I use Fortran. I used to use C, but have found  Fortran is much easier to use. I've never used MATLAB. If I need statistical power of R but the speed of Fortran, I will write the time-intensive operations (i.e. loops) in Fortran and call the subroutine from R.&lt;/p&gt;&#10;" CommentCount="3" CommunityOwnedDate="2010-11-22T05:44:40.397" CreationDate="2010-11-22T05:44:40.397" Id="4789" LastActivityDate="2010-11-22T05:44:40.397" OwnerDisplayName="MichaelSnot" ParentId="4759" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;You might try to find the correlation of the series of differences with the moving average of the series of differences using some scale for the moving average (for example, use a 10 point moving average as the scale). This way you can get an idea about how &quot;wiggly&quot; the series is at different scales. This tells you whether the series has a tendency to move back and forth or keep moving in the same direction.&lt;/p&gt;&#10;&#10;&lt;p&gt;If the correlation is reasonably negative that implies that it is wiggly, it goes one way then comes back near where it was before. If the correlation is zero then it's not wiggly, if it moves in one direction, it has no tendency to move back where it came from. If it's positive then you might call it &quot;anti-wiggly&quot;, it has a tendency to keep moving in the same direction it has been moving in (it has lots of trends). &lt;/p&gt;&#10;&#10;&lt;p&gt;Repeat this at several different scales, say 2, 4, 8, 16, 24 ... data points, and then you can look at the graph.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-11-22T22:26:51.780" Id="4806" LastActivityDate="2010-11-22T22:26:51.780" OwnerUserId="1146" ParentId="4597" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Maybe the Box–Cox transformation or other sort of &lt;a href=&quot;http://en.wikipedia.org/wiki/Power_transform&quot; rel=&quot;nofollow&quot;&gt;power transform&lt;/a&gt;?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-11-22T23:51:15.713" Id="4808" LastActivityDate="2010-11-22T23:51:15.713" OwnerUserId="449" ParentId="4805" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="15516" AnswerCount="1" Body="&lt;p&gt;I'm working on a meta-analysis and have generated a quirky question for which I'm at a bit of a loss.  The MA is for a large set of factorial experiments.  Calculating the Log Response Ratio (LRR) and variance in said ratio for the experimental data is a cinch, and we're comparing the effects of one type of treatment to the other (and any interactions).&lt;/p&gt;&#10;&#10;&lt;p&gt;However, my group is curious at examining the effect of HALF the level of one of the treatments (it's a continuous treatment, and we've been looking at the highest versus the lowest level of the treatment).  We have fit nonlinear curves for each experiment that describe how the treatment effects the response over a wide range of treatment levels.  We've got the coefficient error and residual error for each of these curve fits.  And how we want to calculate log(full treatment) - log(half treatment) from the fitted curve for the this half-treatment log response ratio.  Easy.&lt;/p&gt;&#10;&#10;&lt;p&gt;But...how would we then calculate the variance in the half-treatment log response ratio?  Would we use the SE estimates for the curve coefficients?  The residual error?  What would the sample size be for the variance calculation?  Or is this unimportant?  Thoughts? &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-11-22T23:57:08.500" Id="4809" LastActivityDate="2011-09-13T20:28:24.587" OwnerUserId="101" PostTypeId="1" Score="0" Tags="&lt;variance&gt;&lt;meta-analysis&gt;" Title="Determining variance of meta-analysis log-response ratio generated from fitted curve" ViewCount="581" />
  
  <row AcceptedAnswerId="4815" AnswerCount="1" Body="&lt;p&gt;I have two LME models with the same interaction, one containing both main effects and one containing only one main effect, say :&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ H\_CE = Season + Crownlevel + Season:Crownlevel , random = 1|CollectorID $$&#10;and&#10;$$ H\_CE = Season + Season:Crownlevel , random = 1|CollectorID $$&lt;/p&gt;&#10;&#10;&lt;p&gt;There are 4 levels of each, and every combination of Season, Crownlevel and CollectorID&#10;The AIC, BIC and log likelihood of both models are completely equal. Given the formula for AIC being&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \mathit{AIC} = 2k - 2\ln(L)\ $$&lt;/p&gt;&#10;&#10;&lt;p&gt;one would expect this to be different, even if the likelihoods are exactly the same. In the end, they have a different number of parameters. Or so I thought...&lt;/p&gt;&#10;&#10;&lt;p&gt;Trying this toy example in R :&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(nlme)&#10;&#10;Season &amp;lt;- rep(as.factor(rep(letters[1:4],each=4)),4)&#10;Crownlevel &amp;lt;-rep(as.factor(rep(letters[11:14],4)),4)&#10;CollectorID &amp;lt;-rep(letters[20:23],each=16)&#10;X &amp;lt;-  model.matrix(~Season+Crownlevel+Season:Crownlevel)&#10;B &amp;lt;- c(1,1,-2,2,0.3,0.4,0.4,2,3,1,-2,-3,-4,2,1,2)&#10;H_CE &amp;lt;- X %*% B + rnorm(16*4)&#10;KBM &amp;lt;- data.frame(Season,Crownlevel,H_CE,CollectorID)&#10;&#10;model1 &amp;lt;- lme(H_CE~Season+Crownlevel+Season:Crownlevel,data=KBM,&#10;       method=&quot;ML&quot;,random=~1|CollectorID)&#10;model1e &amp;lt;- lme(H_CE~Season+Season:Crownlevel,data=KBM,&#10;       method=&quot;ML&quot;,random=~1|CollectorID)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I get : &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;anova(model1,model1e)&#10;        Model df      AIC      BIC    logLik&#10;model1      1 18 174.1834 213.0433 -69.09168&#10;model1e     2 18 174.1834 213.0433 -69.09168&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;What am I missing here? Why are the numbers completely equal? It has to do something with the model specification, but I can't really see what. &lt;/p&gt;&#10;&#10;&lt;p&gt;The model specification in itself is faulty, I know that. But I can't explain what makes it return a different set of parameters, but exactly the same residuals, likelihood and degrees of freedom :&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; all.equal(residuals(model1),residuals(model1e))&#10;[1] TRUE&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;As &lt;a href=&quot;http://stats.stackexchange.com/a/4815/615&quot;&gt;fabians&lt;/a&gt; rightfully pointed out, both models are perfectly equivalent. Yet, I fail to see why in the AIC calculation the same value for the number of parameters &lt;code&gt;k&lt;/code&gt; is used.&lt;/p&gt;&#10;&#10;&lt;p&gt;The &lt;code&gt;k&lt;/code&gt; in AIC uses the df, which explains everything.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-11-23T09:38:30.733" FavoriteCount="1" Id="4814" LastActivityDate="2012-10-10T14:12:14.540" LastEditDate="2012-10-10T14:12:14.540" LastEditorUserId="615" OwnerUserId="1124" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;mixed-model&gt;&lt;maximum-likelihood&gt;&lt;aic&gt;&lt;bic&gt;" Title="Why do I get equal AIC, BIC and log likelihood for different models in LME framework?" ViewCount="1065" />
  
&lt;/p&gt;&#10;&#10;&lt;p&gt;and&lt;/p&gt;&#10;&#10;&lt;p&gt;$$
  
  <row Body="&lt;p&gt;I might suggest non-negative matrix factorization. The iterative algorithm of Lee and Seung is easy to implement and should be amenable to sparse matrices (although it involves Hadamard products, which some sparse matrix packages may not support.).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-11-23T17:00:09.463" Id="4829" LastActivityDate="2010-11-23T17:00:09.463" OwnerUserId="795" ParentId="4753" PostTypeId="2" Score="0" />
&lt;/p&gt;&#10;&#10;&lt;p&gt;to the estimate.  When $x \gt 0$, $z_x = 0$ so the second term drops out leaving just $\beta \log(x)$.  When $x = 0$, &quot;$\log(x)$&quot; has been set to zero while $z_x = 1$, leaving just the value $\beta_0$.  Thus, $\beta_0$ estimates the effect when $x = 0$ and otherwise $\beta$ is the coefficient of $\log(x)$.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-11-23T17:55:52.673" Id="4833" LastActivityDate="2010-11-23T18:03:33.650" LastEditDate="2010-11-23T18:03:33.650" LastEditorUserId="919" OwnerUserId="919" ParentId="4831" PostTypeId="2" Score="25" />
  
  
&#10;\phi_2 &amp;amp; 0 &amp;amp; 1 &amp;amp; ... &amp;amp; 0\\ 
  
  
  
  <row Body="&lt;p&gt;The paper &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.ricam.oeaw.ac.at/Groebner-Bases-Bibliography/gbbib_files/publication_582.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.ricam.oeaw.ac.at/Groebner-Bases-Bibliography/gbbib_files/publication_582.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;by Pistone et al seems to be the source paper for this.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-11-24T08:52:57.620" Id="4869" LastActivityDate="2010-11-24T08:52:57.620" OwnerDisplayName="Ravi" ParentId="4184" PostTypeId="2" Score="1" />
  <row AnswerCount="0" Body="&lt;p&gt;what is the minimum entry degree to be employed as a statistician&lt;/p&gt;&#10;" ClosedDate="2010-11-24T11:42:41.677" CommentCount="4" CreationDate="2010-11-24T10:56:16.390" Id="4872" LastActivityDate="2010-11-24T10:56:16.390" OwnerDisplayName="aja" PostTypeId="1" Score="3" Tags="&lt;careers&gt;" Title="degree to become a statistician" ViewCount="258" />
  
  <row Body="&lt;p&gt;If I am reading your question correctly, something like the following should do it:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;aggregate(x$Objects,by=list(x$SessionNo.),sum)&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;where &lt;code&gt;x&lt;/code&gt; is the data frame containing your data. This will give you, for each unique session number, the sum of the object counts.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can of course substitute other functions (including your own) on place of the &lt;code&gt;sum&lt;/code&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-11-24T17:45:13.697" Id="4887" LastActivityDate="2010-11-25T07:42:10.487" LastEditDate="2010-11-25T07:42:10.487" LastEditorUserId="439" OwnerUserId="439" ParentId="4886" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;(I would post this as a comment to the previous answer by 'chi', but don't see that that is possible here.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Be very careful with &lt;code&gt;intervals()&lt;/code&gt; and &lt;code&gt;anova()&lt;/code&gt; from the &lt;code&gt;nlme&lt;/code&gt; package, these have exactly the flaws that @fabians points out above -- they rely on the standard chi-squared distribution applied to a quadratic approximation of the shape of the likelihood profile for the variances. The newer &lt;code&gt;lme4a&lt;/code&gt; package (on r-forge) allows the creation of likelihood profiles for the variance parameters, which takes care of the quadratic approximation part (although not the distributional part).&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, it is fairly hotly debated whether dropping non-significant variance components is a good idea or not (don't have an immediate reference, but this has been discussed on r-sig-mixed-models).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-11-24T20:48:13.937" Id="4894" LastActivityDate="2010-11-24T20:48:13.937" OwnerUserId="2126" ParentId="4858" PostTypeId="2" Score="7" />
  <row Body="&lt;p&gt;The answer that best satisfied me is that it falls out naturally from the generalization of a sample to n-dimensional euclidean space. It's certainly debatable whether that's something that should be done, but in any case:&lt;/p&gt;&#10;&#10;&lt;p&gt;Assume your $n$ measurements $X_i$ are each an axis in $\mathbb R^n$. Then your data $x_i$ define a point $\bf x$ in that space. Now you might notice that the data are all very similar to each other, so you can represent them with a single location parameter $\mu$ that is constrained to lie on the line defined by $X_i=\mu$. Projecting your datapoint onto this line gets you $\hat\mu=\bar x$, and the distance from the projected point $\hat\mu\bf 1$ to the actual datapoint is $\sqrt{\frac{n-1} n}\hat\sigma=\|\bf x-\hat\mu\bf 1\|$.&lt;/p&gt;&#10;&#10;&lt;p&gt;This approach also gets you a geometric interpretation for correlation, $\hat\rho=\cos \angle(\vec{\bf\tilde x},\vec{\bf\tilde y})$.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2010-11-24T20:49:39.210" Id="4895" LastActivityDate="2010-11-24T20:49:39.210" OwnerUserId="2456" ParentId="118" PostTypeId="2" Score="12" />
  <row Body="&lt;p&gt;LASSO is not an algorithm per se, but an operator. &lt;/p&gt;&#10;&#10;&lt;p&gt;There are many different ways to derive efficient algorithms for $\ell_1$ regularized problems. For instance, one can use quadratic programming to them tackle directly. I guess this is what you refer to as LASSO. &lt;/p&gt;&#10;&#10;&lt;p&gt;Another one is LARS, very popular because of its simplicity, connection with forward procedures (yet not too greedy), very constructive proof and easy generalization. &lt;/p&gt;&#10;&#10;&lt;p&gt;Even compared with state of the art quadratic programming solvers, LARS can be much more efficient.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-11-25T00:29:06.963" Id="4897" LastActivityDate="2010-11-25T00:29:06.963" OwnerUserId="358" ParentId="4663" PostTypeId="2" Score="6" />
&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;where $c$ refers to control, $t$ to treatment, and 1 and 2 to pre and post respectively.&#10;$\sigma$ could be the pooled standard deviation at time 1.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Questions:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Is it appropriate to label this effect size measure &lt;code&gt;d&lt;/code&gt;?&lt;/li&gt;&#10;&lt;li&gt;Does this approach seem reasonable?&lt;/li&gt;&#10;&lt;li&gt;What is standard practice for effect size measures for such designs?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2010-11-25T01:33:26.563" FavoriteCount="1" Id="4898" LastActivityDate="2011-04-19T02:15:25.713" OwnerUserId="183" PostTypeId="1" Score="8" Tags="&lt;anova&gt;&lt;mixed-model&gt;&lt;effect-size&gt;&lt;cohens-d&gt;" Title="Effect size for interaction effect in pre-post treatment-control design" ViewCount="2056" />
  <row AcceptedAnswerId="4905" AnswerCount="2" Body="&lt;p&gt;I'm searching for a statistical method to determine if a player is cheating in an online game. The game is a Quake3 like game (ego-shooter).&lt;/p&gt;&#10;&#10;&lt;p&gt;Given a number of positive points and a number of negative points per player (score) and given n players (n&amp;lt;=64). &lt;/p&gt;&#10;&#10;&lt;p&gt;The score comes together like this (positive/negative seen from anti-cheat-perspective):&lt;br /&gt;&#10;positive = number of time the player died himself&lt;br /&gt;&#10;negative = number of opposite team's players killed&lt;br /&gt;&#10;&lt;br /&gt;&lt;br /&gt;&#10;Additional available values: &lt;br /&gt;&#10;t = Time in which this score has been accomplished&lt;br /&gt;&#10;c = number of current number of players&lt;br /&gt;&#10;m = number of times a player killed someone from the same team (teamkiller, very negative if higher than 1 or 2 over a 15 minutes period)&lt;br /&gt;&#10;&lt;br /&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Each player can cheat independent of each other. &lt;/p&gt;&#10;&#10;&lt;p&gt;I used the standard deviation (on the value v = (negative+1)/(postive+1) per player, assigning the exceeding of 3 times standard deviation with absolute certainty (100%) of cheating - although my experience has been that exceeding stddev alone would be sufficient in most cases), which works great on a single player but fails miserably as soon as several players are cheating at the same time.  Also the practise of defining 3 times stddev as 100% allows for &gt; 100% probabilities. Unfortunately, the percentage of players cheating in an unfiltered game is roughly 70 to 80 percent or even higher, if accounting for more subtle cheats.&lt;/p&gt;&#10;&#10;&lt;p&gt;One more thing to consider is, if there is a group of people cheating, and one that isn't, the distribution is no longer a normal distribution, but a camel-bump like distribution.&#10;Is there any useful algorithm or formula for this problem ?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2010-11-25T11:16:41.540" FavoriteCount="2" Id="4904" LastActivityDate="2010-12-03T04:58:22.370" LastEditDate="2010-11-25T11:21:15.593" LastEditorUserId="2132" OwnerUserId="2132" PostTypeId="1" Score="5" Tags="&lt;outliers&gt;" Title="Method to reliably determine abnormal statistical values" ViewCount="440" />
  
  <row AnswerCount="2" Body="&lt;p&gt;In a data set with thousands of data points, I am testing different short-term and longer term data outputs based on 5 rolling data points all the way to 100 rolling data points (which each value being a separate column in excel: 5, 6,..., 100). &lt;/p&gt;&#10;&#10;&lt;p&gt;The test I developed (with rudimentary knowledge of the whole thing) is to check which are the better &quot;fits&quot; of these rolling data outputs (kinda like moving averages, but not quite), in other words, if column 5, 17 or 98. In theory, the better fits should produce lower standard deviations, in the conventional calculation. &lt;/p&gt;&#10;&#10;&lt;p&gt;An analogy for better understanding is something like this: if I do a 5 point moving average on a five period sin wave, the output should be zero or flat, the same for a 6 point moving average on a six period sin wave, and so on all the way to 100. This is the same as saying that the standard deviation in each case should be zero. &lt;/p&gt;&#10;&#10;&lt;p&gt;So the test really is choosing the lowest standard deviation in each column from 5 all the way to 100, which should then provide the best &quot;fit&quot; (to go back to the analogy, fit with the &quot;sin wave&quot;). &lt;/p&gt;&#10;&#10;&lt;p&gt;However, as the statistical inclined will quickly note (am afraid am not one of them for sure), this method presents two major difficulties:  &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;The longer data sets, ie closer to 100 rolling data points, have larger absolute deviations, thus producing higher standard deviations. So a lower standard deviation in the shorter data series may not be indicative of better &quot;fitting&quot;, just lower absolute deviations;   &lt;/li&gt;&#10;&lt;li&gt;Each column has a different number of rolling data points, so once again the numbers and statistical fit may not be comparable between the columns. &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;So my question is therefore: is there a statistical measure better than the standard deviation I used above, such that it is not sensitive to absolute deviations and number of data points? &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-11-25T17:26:03.753" FavoriteCount="2" Id="4914" LastActivityDate="2010-11-27T18:30:58.310" LastEditDate="2010-11-25T22:44:35.630" LastEditorUserId="88" OwnerUserId="2137" PostTypeId="1" Score="5" Tags="&lt;time-series&gt;&lt;standard-deviation&gt;&lt;goodness-of-fit&gt;" Title="Developing a statistical test to ascertain better &quot;fit&quot;" ViewCount="326" />
  
  
  <row Body="&lt;p&gt;Sounds like you'll need a HMM to do that.&#10;Have you read&lt;br&gt;&#10;Lawrence R. Rabiner (February 1989). &lt;a href=&quot;http://www.ece.ucsb.edu/Faculty/Rabiner/ece259/Reprints/tutorial%20on%20hmm%20and%20applications.pdf&quot; rel=&quot;nofollow&quot;&gt;&quot;A tutorial on Hidden Markov Models and selected applications in speech recognition&quot;&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;There are a few examples of models parameters discovery on page 259.&lt;/p&gt;&#10;&#10;&lt;p&gt;The question is how do you plan to update the probabilites? Maybe you can use a kind of success ratio? (if the player has not managed to win much recently using one of the 3 strategies, he may use it less and try more the two others...)&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-11-26T06:24:56.867" Id="4928" LastActivityDate="2010-11-26T06:47:18.580" LastEditDate="2010-11-26T06:47:18.580" LastEditorUserId="1709" OwnerUserId="1709" ParentId="4908" PostTypeId="2" Score="1" />
  
  
  <row AcceptedAnswerId="4951" AnswerCount="1" Body="&lt;p&gt;The fisher linear classifier for two classes is a classifier with this discriminant function:&lt;/p&gt;&#10;&#10;&lt;p&gt;$h(x) = V^{T}X + v_0$
  <row AnswerCount="1" Body="&lt;p&gt;I am trying to estimate parameters of a two dimensional Normal distribution using Gibbs sampling. While it was very easy transform the posterior equation for mean vector to a single dimension normal function for sampling, I am not able to same for sigma(covariance).&lt;/p&gt;&#10;&#10;&lt;p&gt;Do I need to use the Wishart distribution as prior and then convert the posterior into a single dimensional gamma function ? &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-11-27T06:11:33.187" Id="4954" LastActivityDate="2010-11-27T12:27:28.083" LastEditDate="2010-11-27T11:42:17.817" LastEditorUserId="449" OwnerUserId="2157" PostTypeId="1" Score="1" Tags="&lt;mcmc&gt;&lt;gibbs&gt;" Title="Posterior expression for Gibbs sampling" ViewCount="425" />
  <row Body="&lt;p&gt;O.k,&#10;I found that there is an unpaired solution to a sign test (A test of medians).  It is called &quot;Median test&quot; And you can read about it &lt;a href=&quot;http://en.wikipedia.org/wiki/Median_test&quot; rel=&quot;nofollow&quot;&gt;in Wikipedia&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-11-27T07:53:59.150" Id="4956" LastActivityDate="2010-11-27T07:53:59.150" OwnerUserId="253" ParentId="4817" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/u5HhK.png&quot; alt=&quot;http://imgs.xkcd.com/comics/extrapolating.png&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;By the third trimester, there will be hundreds of babies inside you.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Also from &lt;a href=&quot;http://xkcd.com/605/&quot;&gt;XKCD&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2010-11-27T22:27:41.770" CreationDate="2010-11-27T22:27:41.770" Id="4968" LastActivityDate="2010-11-29T14:53:40.757" LastEditDate="2010-11-29T14:53:40.757" LastEditorUserId="442" OwnerUserId="2166" ParentId="423" PostTypeId="2" Score="63" />
  <row Body="&lt;p&gt;I don't know much about eigenvalues or what they are applicable to, but R seems to have a built in function for this purpose named &lt;code&gt;eigen()&lt;/code&gt;. Calculating the eigenvalues &amp;amp; eigenvectors for a 2500 * 2500 matrix took ~ 1 minute on my machine.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; sampData &amp;lt;- runif(6250000, 0, 2)&#10;&amp;gt; x &amp;lt;- matrix(sampData, ncol = 2500, byrow = TRUE)&#10;&amp;gt; system.time(eigen(x))&#10;   user  system elapsed &#10;  79.74    2.90   65.69 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This question has also come up on &lt;a href=&quot;http://stackoverflow.com/questions/713878/how-expensive-is-it-to-compute-the-eigenvalues-of-a-matrix&quot;&gt;Stack Overflow&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-11-28T05:09:44.260" Id="4971" LastActivityDate="2010-11-28T05:09:44.260" OwnerUserId="696" ParentId="4970" PostTypeId="2" Score="5" />
  
  
  
  <row Body="&lt;p&gt;I have a MATLAB and C/C++ implementation &lt;a href=&quot;http://www.emakalic.org/blog/?page_id=7&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;Let me know if you find it useful.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-11-29T00:23:57.847" Id="4986" LastActivityDate="2010-11-29T00:23:57.847" OwnerUserId="530" ParentId="4980" PostTypeId="2" Score="4" />
  <row AcceptedAnswerId="5003" AnswerCount="2" Body="&lt;p&gt;I'm using AIC (Akaike's Information Criterion) to compare non-linear models in R. Is it valid to compare the AICs of different types of model? Specifically, I'm comparing a model fitted by glm versus a model with a random effect term fitted by glmer (lme4).&lt;/p&gt;&#10;&#10;&lt;p&gt;If not, is there a way such a comparison can be done? Or is the idea completely invalid?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-11-29T16:08:13.153" FavoriteCount="5" Id="4997" LastActivityDate="2010-11-29T21:48:11.017" LastEditDate="2010-11-29T17:49:53.220" LastEditorUserId="88" OwnerUserId="2182" PostTypeId="1" Score="13" Tags="&lt;model-selection&gt;&lt;aic&gt;" Title="Can AIC compare across different types of model?" ViewCount="3466" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Is there any comprehensive reference on (or introduction to) how people have tried to model non-independent random variables? I already know about mixing processes, which express in various ways according to various coefficients how &quot;future&quot; events depend on &quot;past&quot; events, but that's about it...&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2010-11-30T08:37:51.447" FavoriteCount="2" Id="5023" LastActivityDate="2010-11-30T16:00:56.360" OwnerUserId="2197" PostTypeId="1" Score="4" Tags="&lt;random-variable&gt;&lt;non-independent&gt;" Title="Modelling dependence between random variables" ViewCount="203" />
  
  <row AcceptedAnswerId="5039" AnswerCount="1" Body="&lt;p&gt;&lt;br&gt;&#10;I'm experimenting with R and found that an anova() needs an object of type lm. But why should I continue with an anova after this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; x &amp;lt;- data.frame(rand=rnorm(100), factor=sample(c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;),100,replace=TRUE))&#10;&amp;gt; head(x)&#10;        rand factor&#10;1  0.9640502      B&#10;2 -0.5038238      C&#10;3 -1.5699734      A&#10;4 -0.8422324      B&#10;5  0.2489113      B&#10;6 -1.4685439      A&#10;&#10;&amp;gt; model &amp;lt;- lm(x$rand ~ x$factor))&#10;&amp;gt; summary(model)&#10;&#10;Call:&#10;lm(formula = x$rand ~ x$factor)&#10;&#10;Residuals:&#10;     Min       1Q   Median       3Q      Max &#10;-2.74118 -0.89259  0.02904  0.59726  3.19762 &#10;&#10;Coefficients:&#10;            Estimate Std. Error t value Pr(&amp;gt;|t|)&#10;(Intercept)  -0.1878     0.1845  -1.018    0.311&#10;x$factorB    -0.1284     0.2689  -0.477    0.634&#10;x$factorC     0.4246     0.2689   1.579    0.118&#10;&#10;Residual standard error: 1.107 on 97 degrees of freedom&#10;Multiple R-squared: 0.04345, Adjusted R-squared: 0.02372 &#10;F-statistic: 2.203 on 2 and 97 DF,  p-value: 0.1160&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This tells me everything I need, or does it not? I'm curious why you want to continue with an anova(model)&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-11-30T21:01:07.213" FavoriteCount="5" Id="5038" LastActivityDate="2010-12-01T00:12:14.820" LastEditDate="2010-12-01T00:12:14.820" LastEditorUserId="88" OwnerUserId="2091" PostTypeId="1" Score="7" Tags="&lt;r&gt;&lt;anova&gt;" Title="What are the ANOVA's benefits over a normal linear model?" ViewCount="1009" />
  
  <row AcceptedAnswerId="5066" AnswerCount="1" Body="&lt;p&gt;For classification, what theoretical results are between cross-validation estimate of accuracy and generalisation accuracy?&lt;/p&gt;&#10;&#10;&lt;p&gt;I particularly asking about results in a PAC-like framework where no assumptions are made that your function class contains the &quot;true&quot; function.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would love to know if there are theorems of the form: If your leave-one-out cross validation error-rate is $\theta$ on $N$ examples, then your generalisation error rate is lower than $\theta+\varepsilon$ with probability $f(\theta, \varepsilon, N)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;If so, what are the general proof techniques to obtain them? What is the theoretical framework?&lt;/p&gt;&#10;&#10;&lt;p&gt;If a fully general theorem is impossible, what extra conditions, if any, allow you to arrive at this type of conclusion?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-12-01T05:22:59.070" FavoriteCount="3" Id="5041" LastActivityDate="2010-12-02T02:45:59.310" OwnerUserId="2067" PostTypeId="1" Score="6" Tags="&lt;classification&gt;&lt;cross-validation&gt;" Title="Theoretical results for cross-validation estimation of classification accuracy?" ViewCount="256" />
  <row AcceptedAnswerId="5044" AnswerCount="2" Body="&lt;p&gt;So, I have 16 trials in which I am trying to authenticate a person from a biometric trait using Hamming Distance. My threshold is set to 3.5. My data is below and only trial 1 is a True Positive:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Trial   Hamming Distance&#10;1   0.34&#10;2   0.37&#10;3   0.34&#10;4   0.29&#10;5   0.55&#10;6   0.47&#10;7   0.47&#10;8   0.32&#10;9   0.39&#10;10  0.45&#10;11  0.42&#10;12  0.37&#10;13  0.66&#10;14  0.39&#10;15  0.44&#10;16  0.39&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;My point of confusion is that I am really unsure about how to make an ROC curve (FPR vs. TPR OR FAR vs. FRR) from this data. It doesn't really matter which one, but I'm just really confused about how to go about calculating it. Any help would be appreciated.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-12-01T06:49:03.427" FavoriteCount="1" Id="5042" LastActivityDate="2010-12-01T09:21:31.927" OwnerUserId="1224" PostTypeId="1" Score="5" Tags="&lt;mathematical-statistics&gt;&lt;roc&gt;" Title="Calculate ROC curve for data" ViewCount="3509" />
  <row AcceptedAnswerId="5091" AnswerCount="1" Body="&lt;p&gt;I have a time serie that I want to analyse through a wavelet decomposition.&lt;br&gt;&#10;I am using the R package &lt;strong&gt;WaveThres&lt;/strong&gt;.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I am interested in the wavelet autocorrelation, but I struggle to understand what does it mean precisely.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have from the book &lt;a href=&quot;http://www.springer.com/statistics/statistical+theory+and+methods/book/978-0-387-75960-9&quot; rel=&quot;nofollow&quot;&gt;Wavelet Methods in Statistics with R&lt;/a&gt; the following formula  &lt;/p&gt;&#10;&#10;&lt;p&gt;$\Psi_j(\tau)=\sum_{k}\phi_{j,k}(0)\phi_{j,k}(\tau)$  &lt;/p&gt;&#10;&#10;&lt;p&gt;$\tau\in\mathbb{Z}$ being the lag of the autocorrelation&lt;br&gt;&#10;and&lt;br&gt;&#10;$\left \{\phi_{j,k}(t)=\phi_{j,k-t}  \right \}_{j,k}$ a set of non decimated wavelets&lt;/p&gt;&#10;&#10;&lt;p&gt;I would really appreciate to understand the meaning of this formula, and (why/if) it is different from performing a multi resolution analysis (MRA) and computing the Pearson autocorrelation coefficient on a detail.&lt;/p&gt;&#10;&#10;&lt;p&gt;fRed&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-12-01T10:05:09.643" FavoriteCount="3" Id="5048" LastActivityDate="2010-12-02T22:25:41.843" LastEditDate="2010-12-01T10:18:28.720" LastEditorUserId="930" OwnerUserId="1709" PostTypeId="1" Score="1" Tags="&lt;correlation&gt;&lt;autocorrelation&gt;&lt;wavelet&gt;" Title="Wavelet auto correlation" ViewCount="471" />
  <row Body="&lt;p&gt;It's a linear combination of the support vectors where the coefficients are given by the Lagrange multipliers corresponding to these support vectors.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-12-01T20:04:56.970" Id="5057" LastActivityDate="2010-12-01T20:04:56.970" OwnerUserId="881" ParentId="5056" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;I'd add some observations to what's been said...&lt;/p&gt;&#10;&#10;&lt;p&gt;AI is a very broad term for anything that has to do with machines doing reasoning-like or sentient-appearing activities, ranging from planning a task or cooperating with other entities, to learning to operate limbs to walk. A pithy definition is that AI is anything computer-related that we don't know how to do well yet. (Once we know how to do it well, it generally gets its own name and is no longer &quot;AI&quot;.)&lt;/p&gt;&#10;&#10;&lt;p&gt;It's my impression, contrary to Wikipedia, that Pattern Recognition and Machine Learning are the same field, but the former is practiced by computer-science folks while the latter is practiced by statisticians and engineers. (Many technical fields are discovered over and over by different subgroups, who often bring their own lingo and mindset to the table.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Data Mining, in my mind anyhow, takes Machine Learning/Pattern Recognition (the techniques that work with the data) and wrap them in database, infrastructure, and data validation/cleaning techniques. &lt;/p&gt;&#10;" CommentCount="4" CommunityOwnedDate="2010-12-01T21:17:22.760" CreationDate="2010-12-01T21:17:22.760" Id="5061" LastActivityDate="2010-12-01T21:17:22.760" OwnerUserId="1764" ParentId="5026" PostTypeId="2" Score="7" />
  
  <row Body="&lt;p&gt;$\theta = argmin_{\theta}  (Y - \sum_{i=1}^k \theta_i X_i)$ would be an affine function in $\theta$ and hence an unconstrained linear program.&lt;/p&gt;&#10;&#10;&lt;p&gt;But $\theta = argmin_{\theta} || Y - \sum_{i=1}^k \theta_i X_i ||$ has an arbitrary norm.  Fortunately, norms are convex and convexity is preserved under compositions with affine functions, so the problem is simply an unconstrained convex optimization program.  They can be solved easily.  Here is an example from MATLAB using &lt;a href=&quot;http://cvxr.com/&quot; rel=&quot;nofollow&quot;&gt;cvx&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Initialize 5 random normal 2x2 matrices and try to regress $Y$ on the 4 $X_i$'s in the Euclidean norm in this example (to change this, just use a different norm() function, of which cvx has many).  If Y and the $X_i$'s are linearly independent, since there are only 4 degrees of freedom, we should be able to achieve 0 error, which we do.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;N=2;&#10;Y=randn(N,N);&#10;X1=randn(N,N);&#10;X2=randn(N,N);&#10;X3=randn(N,N);&#10;X4=randn(N,N);&#10;&#10;cvx_begin&#10;    variable theta(4);&#10;    minimize(norm(Y-theta(1)*X1-theta(2)*X2-theta(3)*X3-theta(4)*X4))&#10;cvx_end&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Status: Solved&lt;/p&gt;&#10;&#10;&lt;p&gt;Optimal value (cvx_optval): +1.50457e-11&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;blockquote&gt;&#10;    &lt;p&gt;norm(Y)&lt;/p&gt;&#10;  &lt;/blockquote&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;ans = 1.3731&lt;/p&gt;&#10;&#10;&lt;p&gt;The norm of Y is about 1.4, and the norm of the difference given our thetas is effectively zero.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2010-12-02T03:39:19.017" Id="5067" LastActivityDate="2010-12-02T03:47:30.290" LastEditDate="2010-12-02T03:47:30.290" LastEditorUserId="1815" OwnerUserId="1815" ParentId="5065" PostTypeId="2" Score="3" />
  
  
  <row Body="&lt;p&gt;The &lt;a href=&quot;http://www-stat.stanford.edu/~tibs/ElemStatLearn/&quot;&gt;Elements of Statistical Learning&lt;/a&gt;, from Hastie et al., has a complete chapter on support vector classifiers and SVMs (in your case, start page 418 on the 2nd edition). Another good tutorial is &lt;a href=&quot;http://www.ci.tuwien.ac.at/~meyer/svm/final.pdf&quot;&gt;Support Vector Machines in R&lt;/a&gt;, by David Meyer.&lt;/p&gt;&#10;&#10;&lt;p&gt;Unless I misunderstood your question, the decision boundary (or hyperplane) is defined by $x^T\beta + \beta_0=0$ (with $\|\beta\|=1$, and $\beta_0$ the intercept term), or as @ebony said a linear combination of the support vectors. The margin is then $2/\|\beta\|$, following Hastie et al. notations.&lt;/p&gt;&#10;&#10;&lt;p&gt;From the on-line help of &lt;code&gt;ksvm()&lt;/code&gt; in the &lt;a href=&quot;http://cran.r-project.org/web/packages/kernlab/index.html&quot;&gt;kernlab&lt;/a&gt; R package, but see also &lt;a href=&quot;http://www.jstatsoft.org/v11/i09/paper&quot;&gt;kernlab – An S4 Package for Kernel Methods in R&lt;/a&gt;, here is a toy example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(101)&#10;x &amp;lt;- rbind(matrix(rnorm(120),,2),matrix(rnorm(120,mean=3),,2))&#10;y &amp;lt;- matrix(c(rep(1,60),rep(-1,60)))&#10;svp &amp;lt;- ksvm(x,y,type=&quot;C-svc&quot;)&#10;plot(svp,data=x)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Note that for the sake of clarity, we don't consider train and test samples.&#10;Results are shown below, where color shading helps visualizing the fitted decision values; values around 0 are on the decision boundary. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/oNWf7.png&quot; alt=&quot;alt text&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Calling &lt;code&gt;attributes(svp)&lt;/code&gt; gives you attributes that you can access, e.g.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;alpha(svp)  # support vectors whose indices may be &#10;            # found with alphaindex(svp)&#10;b(svp)      # (negative) intercept &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So, to display the decision boundary, with its corresponding margin, let's try the following (in the rescaled space), which is largely inspired from a tutorial on SVM made some time ago by &lt;a href=&quot;http://cbio.ensmp.fr/~jvert/&quot;&gt;Jean-Philippe Vert&lt;/a&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;plot(scale(x), col=y+2, pch=y+2, xlab=&quot;&quot;, ylab=&quot;&quot;)&#10;w &amp;lt;- colSums(coef(svp)[[1]] * x[unlist(alphaindex(svp)),])&#10;b &amp;lt;- b(svp)&#10;abline(b/w[1],-w[2]/w[1])&#10;abline((b+1)/w[1],-w[2]/w[1],lty=2)&#10;abline((b-1)/w[1],-w[2]/w[1],lty=2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And here it is:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ZvQpJ.png&quot; alt=&quot;alt text&quot;&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-12-02T10:37:22.867" Id="5080" LastActivityDate="2010-12-02T10:37:22.867" OwnerUserId="930" ParentId="5056" PostTypeId="2" Score="20" />
  
  
  <row AcceptedAnswerId="5095" AnswerCount="2" Body="&lt;p&gt;I am currently working on a RandomForest based prediction method using protein sequence data. I have generated two models first model (NF) using standard set of features and the second model (HF) using hybrid features. I have done Mathews Correlation Coefficient (MCC) and Accuracy calculation and the following are my results:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Model 1 (NF): Training Accuracy - 62.85% Testing Accuracy - 56.38 MCC - 0.1673&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Model 2 (HF): Training Accuracy - 60.34 Testing Accuracy - 61.78 MCC - 0.1856&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The testing data is an independent dataset (means not included in the training data).&lt;/p&gt;&#10;&#10;&lt;p&gt;Since there is a trade-off in accuracy and MCC between the models am confused about the prediction power of the models. Could you please share your thoughts on which model I should consider for further analysis? Apart from Accuracy and MCC is there any other measure that I should consider for validation? &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-12-03T00:12:49.363" FavoriteCount="2" Id="5093" LastActivityDate="2010-12-03T00:53:44.013" LastEditDate="2010-12-03T00:35:31.087" LastEditorUserId="88" OwnerUserId="529" PostTypeId="1" Score="5" Tags="&lt;machine-learning&gt;&lt;cross-validation&gt;&lt;random-forest&gt;" Title="Statistical validation of RandomForest models" ViewCount="898" />
  <row Body="&lt;p&gt;I think the answer to your question is a couple other questions: how rare does a given test outcome need to be before you don't care about it?  How certain do you want to be that you'll actually find at least test that comes out that way if it occurs right at the threshold where you've stopped caring about it.  Given those values you can do a power analysis.  I'm not 100% confident whether you need to do a multinomial (involving more than one outcome) power analysis or not, I'm guessing a binomial one (either the rare test or not) will work just fine, e.g. &lt;a href=&quot;http://statpages.org/proppowr.html&quot; rel=&quot;nofollow&quot;&gt;http://statpages.org/proppowr.html&lt;/a&gt;.  Alpha = .05, Power = 80%, Group on proportion 0, Group 1 proportion .0015.  Relative sample size, 1; total - just south of 13,000 tests.  At which the expected number of test 4s is ~20.&lt;/p&gt;&#10;&#10;&lt;p&gt;That will help you find the number of tests you need to have to detect one of those rare occurring results.  However if what you really care about is relative frequency, the problem is harder.  I'd conjecture that if you simply multiplied the resulting N from the power analysis by 20 or 30 you'd find a reasonable guess.&lt;/p&gt;&#10;&#10;&lt;p&gt;In practice, if you don't really need to decide the number of tests ahead of time, you might consider running tests until you get 20 or 30 result 4s.  By the time you've gotten that many 4s you should start to have a reasonable though not absolute estimate of their relative frequency IMO.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Ultimately - there are trade-offs between number of tests run and accuracy.  You need to know how precise you want your estimates to be before you can really determine how many is &quot;enough&quot;.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-12-03T05:41:03.173" Id="5098" LastActivityDate="2010-12-03T05:41:03.173" OwnerUserId="196" ParentId="5092" PostTypeId="2" Score="5" />
  
  
  <row Body="&lt;p&gt;The best introductory Bayesian book I have found is &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0198518897&quot; rel=&quot;nofollow&quot;&gt;Data Analysis - A Bayesian Tutorial&lt;/a&gt;. It is quite practical.&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2010-12-03T16:23:28.423" CreationDate="2010-12-03T16:23:28.423" Id="5105" LastActivityDate="2010-12-03T16:23:28.423" OwnerUserId="1146" ParentId="4259" PostTypeId="2" Score="0" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have a question regarding the sign test when the individual measurements may be correlated. Let me start off with some background. Suppose we have 4 Organisms (a,b,c,d),and we make measurements in two separate ways, say A and B. Our data may look as follows&lt;/p&gt;&#10;&#10;&lt;p&gt;a = 3 for measurement A and 1 for measurement B&lt;br&gt;&#10;b = 4 for measurement A and 3 for measurement B&lt;br&gt;&#10;c = 0 for A and 4 for B&lt;br&gt;&#10;d = 2 for A and 0 for B&lt;/p&gt;&#10;&#10;&lt;p&gt;We now take the difference between A and B: $2,1,-4,2$. Looking at the signs we get the pattern $++-+$. We want to test if there is any difference between method A and method B. Take:&lt;/p&gt;&#10;&#10;&lt;p&gt;$H_0$(null hypothesis) = distribution for A is equal to the distribution for B&lt;/p&gt;&#10;&#10;&lt;p&gt;Under $H_0$ we would expect $\textrm{Pr}(A&amp;gt;B)=\textrm{Pr}(B&amp;gt;A)=.5$, therefore any pattern of $+$'s and $-$'s would be equally likely. i.e. $--+-$ is as likely to occur as&#10;$+-++$ etc. Let $U =$ number of $+$'s (in our case $U=3$). Assuming $H_0$ one can show that $\textrm{Pr}(U\ge3) = (1+4)/2^4 = 5/16=0.3125$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, suppose the a and b are strongly positively correlated. Therefore not&#10;all combinations of $+$'s and $-$'s would be equally likely.For example one would not expect to have a &gt; b for method A and a &amp;lt; b for method B. Therefore we would not expect sequences like $+-..$ or $-+..$ to occur. Taking this into account assuming $H_0$ it turns out that $\textrm{Pr}(U\ge3) = 3/8=0.375$, i.e. our p&#10;value increases.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now I come to my question: &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;If instead of 4 organisms, I have say 100&#10;  organisms, and also suppose I have an upper bound on the number of&#10;  correlations and the size of each correlation. Is there any way to&#10;  construct an upper bound on the p value?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="0" CreationDate="2010-12-03T22:19:21.333" FavoriteCount="2" Id="5111" LastActivityDate="2010-12-21T18:22:10.200" LastEditDate="2010-12-20T19:28:26.343" LastEditorUserId="919" OwnerDisplayName="Tom" PostTypeId="1" Score="6" Tags="&lt;hypothesis-testing&gt;&lt;nonparametric&gt;" Title="Nonparametric sign test for correlated variables" ViewCount="374" />
  
  
  
  <row Body="&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Ronald_Fisher&quot;&gt;Ronald Fisher&lt;/a&gt; for his fundamental contributions to the way we analyze data, whether it be the analysis of variance framework, maximum likelihood, permutation tests, or any number of other ground-breaking discoveries. &lt;/p&gt;&#10;" CommentCount="5" CommunityOwnedDate="2010-12-04T00:36:08.460" CreationDate="2010-12-04T00:36:08.460" Id="5117" LastActivityDate="2010-12-04T00:36:08.460" OwnerUserId="1118" ParentId="5115" PostTypeId="2" Score="64" />
  
  <row Body="&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Bradley_Efron&quot;&gt;Bradley Efron&lt;/a&gt; for the &lt;a href=&quot;http://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29&quot;&gt;Bootstrap&lt;/a&gt; - one of the most useful techniques in computational statistics.&lt;/p&gt;&#10;" CommentCount="1" CommunityOwnedDate="2010-12-04T11:07:26.660" CreationDate="2010-12-04T11:07:26.660" Id="5133" LastActivityDate="2010-12-04T11:07:26.660" OwnerUserId="887" ParentId="5115" PostTypeId="2" Score="29" />
  
  <row Body="&lt;p&gt;A quick google brings up this, which indicates that when working with circular data you'll need a different definition of 'bias' for a start:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;However, when using data on the circle, we cannot use distance in Euclidean space, so all differences &lt;em&gt;θ − θ&lt;sub&gt;i&lt;/sub&gt;&lt;/em&gt; should be replaced by considering the angle between two vectors:&lt;/p&gt;&#10;  &#10;  &lt;p&gt;$d_i\theta)= \| \theta -\theta_i \| = \min(|\theta-\theta_i|, 2π -|\theta-\theta_i|).$&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;-- Charles C Taylor. Automatic bandwidth selection for circular density estimation. &lt;em&gt;Computational Statistics &amp;amp; Data Analysis&lt;/em&gt;&#10;Volume 52, Issue 7, 15 March 2008, Pages 3493-3500. doi: &lt;a href=&quot;http://dx.doi.org/10.1016/j.csda.2007.11.003&quot; rel=&quot;nofollow&quot;&gt;10.1016/j.csda.2007.11.003&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;He references these books:&lt;/p&gt;&#10;&#10;&lt;p&gt;S. Rao Jammalamadaka and A. SenGupta, &lt;em&gt;Topics in Circular Statistics&lt;/em&gt;, World Scientific, Singapore (2001).&lt;/p&gt;&#10;&#10;&lt;p&gt;K.V. Mardia and P.E. Jupp, &lt;em&gt;Directional Statistics&lt;/em&gt;, John Wiley, Chichester (1999).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-12-04T12:13:36.183" Id="5137" LastActivityDate="2010-12-04T12:13:36.183" OwnerUserId="449" ParentId="5011" PostTypeId="2" Score="3" />
  
&lt;/p&gt;&#10;&#10;&lt;p&gt;where $y_{ijkl}$ is the temperature for unit $l$ when considering levels $i=1\dots a$, $j=1\dots b$, and $k=1\dots c$, of factors $\alpha$ (decay), $\beta$ (particles), and $\gamma$ (velocity); the $\varepsilon_{ijk}$ are the residuals assumed to follow a gaussian distribution of unknown variance, $\sigma^2$. They can be viewed as random fluctuations around $\mu$, the overall mean, and reflect the between-unit variations that are not accounted for by the other factors. The $\alpha_i$, $\beta_j$, and $\gamma_k$ can be viewed as factor-specific deviations from the overall mean $\mu$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The so-called &lt;em&gt;main effect&lt;/em&gt; of decay, particles, and velocity will be estimated by forming a ratio between the variance that they account for (known as &lt;em&gt;mean squares&lt;/em&gt;) and the residual variance (what is left after considering all variance explained by those factors), which is known to follow a Fisher-Snedecor (F) distribution, with $d-1$ and $N-abc$ degrees of freedom, where $d=a$, $b$, or $c$ stands for the number of levels of $\alpha$ (decay), $\beta$ (particles), and $\gamma$ (velocity). A significant effect (following an &lt;a href=&quot;http://en.wikipedia.org/wiki/Statistical_hypothesis_testing&quot; rel=&quot;nofollow&quot;&gt;hypothesis test&lt;/a&gt; of a null effect, i.e. $H_0:\, \mu_i=\mu_j\,\, \forall i\neq j$ &lt;em&gt;vs.&lt;/em&gt; $H_1:$ at least two of the $\mu_i$'s differ) would indicate that the factor under consideration has a significant effect on the outcome. This is readily obtained by any statistical software. For instance, in R you would use something like&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;summary(aov(temperature ~ decay + particles + velocity, data=df))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;provided temperature and factor levels are organized in four columns, in a data.frame named &lt;code&gt;df&lt;/code&gt;, as suggested below:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;t1 0.1 10 30&#10;t2 0.1 10 30&#10;t3 0.1 10 30&#10;t4 0.1 10 30&#10;t5 0.1 10 30&#10;t6 0.2 10 30&#10;t7 0.2 10 30&#10;...&#10;t60 0.3 100 70&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The effect of any of the three factors can also be summarized under an equation like the one you referred to sy simply calling (again under R):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;summary.lm(aov(temperature ~ decay + particles + velocity))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This follows from the fact that an ANOVA is nothing more than a &lt;a href=&quot;http://en.wikipedia.org/wiki/Linear_model&quot; rel=&quot;nofollow&quot;&gt;Linear Model&lt;/a&gt; that you may have heard about (think of a regression model where the explanatory variables are all categorical).&lt;/p&gt;&#10;&#10;&lt;p&gt;Should you want to account for possible interactions between all three factors, you need to add three second-order and one three-order interaction terms. If any of these effects prove to be significant, this would mean that the effect of the corresponding factors cannot be considered in isolation one from the other (e.g., the effect of decay on temperature is not the same depending on the number of particles).&lt;/p&gt;&#10;&#10;&lt;p&gt;As for references, I would suggest starting with on-line tutorial or textbook like &lt;a href=&quot;http://www.psych.nyu.edu/cohen/three_way_ANOVA.pdf&quot; rel=&quot;nofollow&quot;&gt;Three-Way ANOVA&lt;/a&gt;, by Barry Cohen, or &lt;a href=&quot;http://cran.r-project.org/doc/contrib/Faraway-PRA.pdf&quot; rel=&quot;nofollow&quot;&gt;Practical Regression and Anova using R&lt;/a&gt;, by John MainDonald (but see also other textbooks available on &lt;a href=&quot;http://cran.r-project.org/other-docs.html&quot; rel=&quot;nofollow&quot;&gt;CRAN documentation&lt;/a&gt;). The definitive reference is Montgomery, &lt;a href=&quot;http://bcs.wiley.com/he-bcs/Books?action=index&amp;amp;itemId=047148735X&amp;amp;bcsId=2172&quot; rel=&quot;nofollow&quot;&gt;Design and Analysis of Experiments&lt;/a&gt; (Wiley, 2005).&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-12-04T21:13:59.517" Id="5146" LastActivityDate="2010-12-04T21:23:14.870" LastEditDate="2010-12-04T21:23:14.870" LastEditorUserId="930" OwnerUserId="930" ParentId="5144" PostTypeId="2" Score="6" />
  
  
  
  <row Body="&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/John_Tukey&quot; rel=&quot;nofollow&quot;&gt;John Tukey&lt;/a&gt; for Fast Fourier Transforms, exploratory data analysis (EDA), box plots, projection pursuit, jackknife (along with Quenouille). Coined the words &quot;software&quot; and &quot;bit&quot;.&lt;/p&gt;&#10;" CommentCount="2" CommunityOwnedDate="2010-12-05T05:18:37.237" CreationDate="2010-12-05T05:18:37.237" Id="5155" LastActivityDate="2012-08-02T03:11:29.257" LastEditDate="2012-08-02T03:11:29.257" LastEditorUserId="74" OwnerUserId="74" ParentId="5115" PostTypeId="2" Score="35" />
&lt;/p&gt;&#10;&#10;&lt;p&gt;$\hspace{1cm} \sigma_{\overline{x}}=6.9/5=1.38$
  
  <row Body="&lt;p&gt;Here are some online ressources I found interesting without going into detail (and I'm not a specialist of this topic):&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://www.cse.buffalo.edu/faculty/mbeal/papers/hdp.pdf&quot;&gt;Hierarchical Dirichlet Processes&lt;/a&gt;, by Teh et al. (2005)&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://www.cs.cmu.edu/~kbe/dp_tutorial.pdf&quot;&gt;Dirichlet Processes A gentle tutorial&lt;/a&gt;, by El-Arini (2008)&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://www.mit.edu/~9.520/spring10/Classes/class21_dirichlet_2010.pdf&quot;&gt;Bayesian Nonparametrics&lt;/a&gt;, by Rosasco (2010)&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://learning.eng.cam.ac.uk/zoubin/talks/uai05tutorial-b.pdf&quot;&gt;Non-parametric Bayesian Methods&lt;/a&gt;, by Ghahramani (2005)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;The definitive reference seems to be&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;N. Hjort, C. Holmes, P. Müller, and S.&#10;  Walker, editors. &lt;em&gt;&lt;a href=&quot;http://www.cambridge.org/gb/knowledge/isbn/item2707877/?site_locale=en_GB&quot;&gt;Bayesian&#10;  Nonparametrics&lt;/a&gt;&lt;/em&gt;. Number 28 in&#10;  Cambridge Series in Statistical and&#10;  Probabilistic Mathematics. Cambridge&#10;  University Press, 2010.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;About R, there seems to be some other packages worth to explore if the &lt;a href=&quot;http://cran.r-project.org/web/packages/DPpackage/index.html&quot;&gt;DPpackage&lt;/a&gt; does not suit your needs, e.g. &lt;a href=&quot;http://cran.r-project.org/web/packages/dpmixsim/&quot;&gt;dpmixsim&lt;/a&gt;, &lt;a href=&quot;http://www.bioconductor.org/help/bioc-views/release/bioc/html/BHC.html&quot;&gt;BHC&lt;/a&gt;, or &lt;a href=&quot;http://www.stat.washington.edu/hoff/Code/MBSC/&quot;&gt;mbsc&lt;/a&gt; found on &lt;a href=&quot;http://www.rseek.org&quot;&gt;Rseek.org&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="3" CommunityOwnedDate="2010-12-05T21:06:08.147" CreationDate="2010-12-05T20:54:42.357" Id="5168" LastActivityDate="2012-01-18T15:58:29.560" LastEditDate="2012-01-18T15:58:29.560" LastEditorUserId="2728" OwnerUserId="930" ParentId="5160" PostTypeId="2" Score="10" />
&lt;/p&gt;&#10;&#10;&lt;p&gt;where, as usual, $\Phi$ is the cumulative distribution function for the standard Normal distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;Because normal distributions are symmetric, bias in rounding of values less than the mean ought to balance any bias in rounding of values greater than the mean.  The balancing will be perfect when the distribution's mode is a half integer, which is the case here.  Thus the mean of this &quot;discrete Normal&quot; distribution is &lt;em&gt;exactly&lt;/em&gt; 174.5.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;The rounding will increase the variance&lt;/strong&gt;.  &lt;em&gt;As an approximation,&lt;/em&gt; people usually think of rounding as acting &lt;em&gt;at random&lt;/em&gt; to vary a number by some amount uniformly distributed between $-1/2$ and $+1/2$.  The variance of this uniform distribution is $1/12$, whence we can &lt;em&gt;estimate&lt;/em&gt; the variance of the discrete normal distribution as&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\sqrt{sd^2 + 1/2} = \sqrt{(6.9/5)^2 + 1/12} = 1.40986\ 99703\ 63697\ 52354,$$
  <row AnswerCount="1" Body="&lt;p&gt;I have several sets of data, unfortunately the data comes to me in a &quot;summary&quot; form. My job is to consolidate the several data sources into one general summary. I'm currently using the median to summarise the data, but I don't know if this is statistically sound. Here's a description of my problem:&lt;/p&gt;&#10;&#10;&lt;p&gt;There are $N_P$ samples, each with varying sample sizes, but all from a single population. Neither the sample size or the standard variation are known. Each sample can be divided into $N_Q$ disjoint groups (or qualities). From each sample, the only data that is known is what percent of the sample falls within a group (or category). For example, population $A$ contains, $x\%$ of $a$, $y\%$ of $b$ and $z\%$ of $c$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The different samples are not disjoint, so a single item might be in several of the samples; but I don't know how much overlapping there is. There are 5-8 different samples with 5-7 categories. An example (smaller) table is the following.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;            cat. a    cat. b    cat. c    &#10;sample A    47.34%    30.05%    11.92%&#10;sample B    41.60%    29.90%    11.90%&#10;sample c    47.74%    29.67%    12.69%&#10;--------    ------    ------    ------&#10;median      47.34%    29.90%    11.92%&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now is it statistically sound to create this &quot;median&quot; summary, which takes each group from the different samples and finds the median? Maybe I should be using the mean? The problem I'm seeing is the &quot;median sample&quot; usually sums to less than 100%, even though the percentages from each sample sum to 100%. Should this matter?&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Sample sizes: 100k - 100m&#10;Population size: ~1 billion&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2010-12-06T00:17:18.907" FavoriteCount="1" Id="5172" LastActivityDate="2010-12-06T05:02:30.237" LastEditDate="2010-12-06T05:02:30.237" LastEditorUserId="2271" OwnerUserId="2271" PostTypeId="1" Score="4" Tags="&lt;sampling&gt;&lt;median&gt;&lt;population&gt;" Title="Is taking the median of a set of percentages statistically sound?" ViewCount="2097" />
  <row AcceptedAnswerId="5175" AnswerCount="2" Body="&lt;p&gt;&quot;Spurious regression&quot; (in the context of time series) and associated terms like unit root tests are something I've heard a lot about, but never understood.&lt;/p&gt;&#10;&#10;&lt;p&gt;Why/when, intuitively, does it occur? (I believe it's when your two time series are cointegrated, i.e., some linear combination of the two is stationary, but I don't see why cointegration should lead to spuriousness.) What do you do to avoid it?&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm looking for a high-level understanding of what cointegration/unit root tests/Granger causality have to do with Spurious regression (those three are terms I remember being associated with spurious regression somehow, but I don't remember what exactly), so either a custom response or a link to references where I can learn more would be great.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-12-06T00:33:15.987" FavoriteCount="7" Id="5173" LastActivityDate="2010-12-06T15:36:20.017" LastEditDate="2010-12-06T09:08:35.157" LastEditorUserId="159" OwnerUserId="1106" PostTypeId="1" Score="6" Tags="&lt;time-series&gt;&lt;regression&gt;&lt;cointegration&gt;" Title="Resources for learning about spurious time series regression" ViewCount="480" />
  
  <row Body="&lt;p&gt;&lt;code&gt;ets()&lt;/code&gt; and &lt;code&gt;auto.arima()&lt;/code&gt; are not really set up to handle &lt;code&gt;zoo&lt;/code&gt; objects. Although &lt;code&gt;ets()&lt;/code&gt; is not returning an error, it will be ignoring any seasonality. &lt;code&gt;auto.arima()&lt;/code&gt; is failing because it is confused by the &lt;code&gt;zoo&lt;/code&gt; object with apparent seasonality. I will try to include better checking in a future version.&lt;/p&gt;&#10;&#10;&lt;p&gt;When using the forecast package, use &lt;code&gt;ts&lt;/code&gt; objects instead. In this example, &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x &amp;lt;- ts(x)&#10;auto.arima(x)&#10;ets(x)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;That will ignore the frequency component of &lt;code&gt;x&lt;/code&gt;. It looks like weekly data, so&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x &amp;lt;- ts(x,start=2009+(31+28+31+4)/365,f=52)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;will capture the frequency (and start period). However, note that &lt;code&gt;ets()&lt;/code&gt; will not handle weekly data and will return an error with this latter formulation.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2010-12-06T03:12:22.033" Id="5176" LastActivityDate="2010-12-06T03:12:22.033" OwnerUserId="159" ParentId="5170" PostTypeId="2" Score="6" />
  
  
  <row Body="&lt;p&gt;The object returned by &lt;code&gt;read.bugs&lt;/code&gt; is an object of S3 class &lt;code&gt;mcmc.list&lt;/code&gt;. &#10;You can use the double brackets &lt;code&gt;[[&lt;/code&gt; to access the separate chains, i.e. the different &lt;code&gt;mcmc&lt;/code&gt;-objects that make up the larger &lt;code&gt;mcmc.list&lt;/code&gt; object, which really is simply a list of &lt;code&gt;mcmc&lt;/code&gt;-objects that inherits some information about thinning and chain length from its components.&lt;/p&gt;&#10;&#10;&lt;p&gt;More to the point, s.th. like &lt;code&gt;lapply(codaobject, function(x){ colMeans(x) })&lt;/code&gt; should return the posterior means for each parameter in each chain and &lt;code&gt;lapply(codaobject, function(x){ apply(x, 2, sd) })&lt;/code&gt; should give chain- and parameter-specific posterior sd's, since each chain is essentially just a numeric matrix with rows corresponding to the (saved) iterations and columns corresponding to the different params.&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT:&#10;I think Gelman/Hill's &quot;Bayesian Data Analysis&quot; contains some worked examples using R2WinBUGS.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-12-06T18:03:04.027" Id="5188" LastActivityDate="2010-12-06T18:10:12.523" LastEditDate="2010-12-06T18:10:12.523" LastEditorUserId="1979" OwnerUserId="1979" ParentId="5187" PostTypeId="2" Score="4" />
&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2010-12-06T21:08:37.437" Id="5191" LastActivityDate="2010-12-07T00:03:46.107" OwnerUserId="2189" PostTypeId="1" Score="2" Tags="&lt;kernel&gt;" Title="Order of the kernel for periodic case" ViewCount="74" />
  <row AcceptedAnswerId="5201" AnswerCount="2" Body="&lt;p&gt;In order to calibrate a confidence level to a probability in supervised learning (say to map the confidence from an SVM or a decision tree using oversampled data) one method is to use Platt's Scaling (e.g., &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.60.5153&amp;amp;rep=rep1&amp;amp;type=pdf&quot; rel=&quot;nofollow&quot;&gt;Obtaining Calibrated Probabilities from Boosting&lt;/a&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;Basically one uses logistic regression to map $[-\infty;\infty]$ to $[0;1]$. The dependent variable is the true label and the predictor is the confidence from the uncalibrated model. What I don't understand is the use of a target variable other than 1 or 0. The method calls for creation of a new &quot;label&quot;: &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;To avoid overfitting to the sigmoid train set, an out-of-sample model is used.  If there are $N_+$ positive examples and $N_-$ negative examples in the train set, for each training example Platt Calibration uses target values $y_+$ and $y_-$ (instead of 1 and 0, respectively), where&#10;  $$&#10;y_+=\frac{N_++1}{N_++2};\quad\quad y_-=\frac{1}{N_-+2}&#10;$$&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;What I don't understand is how this new target is useful. Isn't logistic regression simply going to treat the dependent variable as a binary label (regardless of what label is given)?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;UPDATE:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I found that in SAS changing the dependent from $1/0$ to something else reverted back to the same model (using &lt;code&gt;PROC GENMOD&lt;/code&gt;). Perhaps my error or perhaps SAS's lack of versatility. I was able to change the model in R. As an example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;data(ToothGrowth) &#10;attach(ToothGrowth) &#10;&#10;  # 1/0 coding &#10;dep          &amp;lt;- ifelse(supp == &quot;VC&quot;, 1, 0) &#10;OneZeroModel &amp;lt;- glm(dep~len, family=binomial) &#10;OneZeroModel &#10;predict(OneZeroModel) &#10;&#10;  # Platt coding &#10;dep2           &amp;lt;- ifelse(supp == &quot;VC&quot;, 31/32, 1/32) &#10;plattCodeModel &amp;lt;- glm(dep2~len, family=binomial) &#10;plattCodeModel &#10;predict(plattCodeModel) &#10;&#10;compare        &amp;lt;- cbind(predict(OneZeroModel), predict(plattCodeModel)) &#10;&#10;plot(predict(OneZeroModel), predict(plattCodeModel))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2010-12-07T01:31:14.380" FavoriteCount="9" Id="5196" LastActivityDate="2013-09-27T21:59:39.513" LastEditDate="2013-09-27T21:59:39.513" LastEditorUserId="7290" OwnerUserId="2040" PostTypeId="1" Score="7" Tags="&lt;logistic&gt;&lt;cross-validation&gt;&lt;calibration&gt;" Title="Why use Platt's scaling?" ViewCount="3971" />
  <row AnswerCount="1" Body="&lt;p&gt;I have database of 78706 resident incidents in aged care facilities (5 years of data).&#10;I want to to learn and implement a tool allowing analyzing these data using following attributes:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Resident&lt;/li&gt;&#10;&lt;li&gt;Date/Time&lt;/li&gt;&#10;&lt;li&gt;Location&lt;/li&gt;&#10;&lt;li&gt;Result&lt;/li&gt;&#10;&lt;li&gt;Injury&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I want to be able to get following assumptions from my system which will be passed to specialists for further research, decision making and action:&lt;/p&gt;&#10;&#10;&lt;p&gt;Examples of outputs:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Most of incidents in facility A with residents X, Y and Z&lt;/li&gt;&#10;&lt;li&gt;Falls occur in North Wing between 2am and 5am&lt;/li&gt;&#10;&lt;li&gt;Skin tears happen during showering in facility B&lt;/li&gt;&#10;&lt;li&gt;Most incidents in a facility C related to repositioning&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;My question is &lt;strong&gt;not&lt;/strong&gt; what software package can help me but what &lt;strong&gt;type&lt;/strong&gt; of statistical analysis solves this problem - regression, cluster etc.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can you recommend some &lt;strong&gt;practical books&lt;/strong&gt; for a starter too?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-12-07T02:08:09.843" Id="5197" LastActivityDate="2010-12-07T03:07:27.183" OwnerDisplayName="Igor" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;clustering&gt;" Title="What type of statistical analysis solves this problem?" ViewCount="201" />
  <row Body="&lt;p&gt;I suggest to check out the &lt;a href=&quot;http://en.wikipedia.org/wiki/Logistic_regression&quot;&gt;wikipedia page of logistic regression&lt;/a&gt;. It states that in case of a binary dependent variable logistic regression maps the predictors to the probability of occurrence of the dependent variable. Without any transformation, the probability used for training the model is either 1 (if y is positive in the training set) or 0 (if y is negative). &lt;/p&gt;&#10;&#10;&lt;p&gt;So: Instead of using the absolute values 1 for positive class and 0 for negative class when fitting $p_i=\frac{1}{(1+exp(A*f_i+B))}$ (where $f_i$ is the uncalibrated output of the SVM), Platt suggests to use the mentioned transformation to allow the opposite label to appear with some probability. In this way some regularization is introduced. When the size of the dataset reaches infinity, $y_+$ will become 1 and $y_{-}$ will become zero. For details, see the &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1639&quot;&gt;original paper of Platt&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2010-12-07T07:43:17.147" Id="5201" LastActivityDate="2010-12-07T07:43:17.147" OwnerUserId="264" ParentId="5196" PostTypeId="2" Score="8" />
  
  
  <row Body="&lt;p&gt;You could start with David B Wilson's website on &quot;&lt;a href=&quot;http://mason.gmu.edu/~dwilsonb/ma.html&quot; rel=&quot;nofollow&quot;&gt;meta-analysis stuff&lt;/a&gt;&quot;. He offers spss, stata, and sas macros for performing meta-analytic analyses (including meta-regression; metareg.sps) + PPT slides (analysis.ppt, interpretation.ppt).&lt;/p&gt;&#10;&#10;&lt;p&gt;Another presentation I really like(d) was given by Marsh et al. „&lt;a href=&quot;http://www.self.ox.ac.uk/rdimaterials.htm&quot; rel=&quot;nofollow&quot;&gt;Meta-Analysis: Session 3.3 &amp;amp; 3.4: Teacher Expectancy Example&lt;/a&gt;”  (see &quot;Practical example - fixed, random, &amp;amp; multilevel Meta20 data&quot;). Unfortunately, the presentation seems to be no longer available... But you might want to check the &lt;a href=&quot;http://www.self.ox.ac.uk/addit.materials.htm&quot; rel=&quot;nofollow&quot;&gt;other resources&lt;/a&gt; (see esp. &quot;Advanced Meta-Anaylsis Seminar Presentations&quot;).&lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, you can find a presentation on &quot;&lt;a href=&quot;http://www.campbellcollaboration.org/artman2/uploads/1/Pigott_random_mixed_effects.pdf&quot; rel=&quot;nofollow&quot;&gt;Random and Mixed-effects Modeling&lt;/a&gt;&quot; on the website of the Campbell Collaboration.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-12-07T17:57:35.953" Id="5222" LastActivityDate="2011-10-17T20:28:11.763" LastEditDate="2011-10-17T20:28:11.763" LastEditorUserId="919" OwnerUserId="307" ParentId="5220" PostTypeId="2" Score="7" />
  
  
  
  
  <row Body="&lt;p&gt;There's a &lt;a href=&quot;http://www.mathworks.com/help/toolbox/stats/kurtosis.html&quot; rel=&quot;nofollow&quot;&gt;bias correction&lt;/a&gt;.  It's not huge.  I believe the sampling variance of the kurtosis is proportional to the &lt;em&gt;eighth&lt;/em&gt; (!) central moment, which can be enormous for a lognormal distribution.  You would need millions of trials (or far more) in a simulation to detect bias unless the CV is tiny.  (Plot a histogram of kvals to see how extraordinarily skewed they are.)&lt;/p&gt;&#10;&#10;&lt;p&gt;The correct kurtosis is indeed about 113.9364.&lt;/p&gt;&#10;&#10;&lt;p&gt;As far as R style goes, it can be convenient to encapsulate the simulation in a function so you can easily modify the sample size or number of trials. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-12-08T04:51:09.113" Id="5232" LastActivityDate="2010-12-08T04:51:09.113" OwnerUserId="919" ParentId="5228" PostTypeId="2" Score="5" />
  
  <row Body="&lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt; [Having been going through a large backlog of email from R-Help] I am reminded of the thread on &quot;&lt;a href=&quot;http://thread.gmane.org/gmane.comp.lang.r.general/213174/focus=213179&quot; rel=&quot;nofollow&quot;&gt;The behaviour of &lt;code&gt;read.csv()&lt;/code&gt;&lt;/a&gt;&quot;. In this, Duncan Murdoch mentions that he prefers to use &lt;a href=&quot;http://en.wikipedia.org/wiki/Data_Interchange_Format&quot; rel=&quot;nofollow&quot;&gt;Data Interchange Format (DIF)&lt;/a&gt; files instead of csv for some of the reason Jeromy mentions. I just tried this and Gnumeric gets it wrong (loading 12/3 as a date), but OpenOffice.org reads this correctly and preserves the 12/3 information intact. (Anyone care to check this in MS Excel?)&lt;/p&gt;&#10;&#10;&lt;p&gt;DIF files are plain text and can be read by spreadsheets and R (as long as you use a recent R revision (SVN revision &gt;= r53778)) will read the data in in the correct format.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Original&lt;/strong&gt;: I would try to avoid using a spreadsheet full stop for data editing / manipulation whenever possible. It is incredibly difficult, if not impossible, to document any changes you make to an existing data set so that pretty much rules it out from a reproducible research point of view. At most, I use a spreadsheet to quickly view existing data.&lt;/p&gt;&#10;&#10;&lt;p&gt;For data processing, I tend to write an R script that will take the raw csv file and apply all the necessary processing steps required. I heavily comment that script to explain exactly what I am doing at each stage and why. My data analysis script would then call the data processing script which loads and processes the data.&lt;/p&gt;&#10;&#10;&lt;p&gt;For data entry, is it more hassle to enter the data in a text editor or in a spreadsheet? I suspect the problems you mention for the latter do not outweigh those of trying to enter CSV data into a text editor.&lt;/p&gt;&#10;&#10;&lt;p&gt;You could try a better spreadsheet; OpenOffice.org refuses to stop formatting 12/3 as a date (or it converts it to the numeric representation) even if one formats the column as &quot;numeric&quot; first. Gnumeric on the other hand will leave 12/3 as it is &lt;strong&gt;if&lt;/strong&gt; you format the column as &quot;numeric&quot; first.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can force OpenOffice.org to not reformat 12/3 as a date by prepending a &lt;code&gt;'&lt;/code&gt; to the entries, i.e. &lt;code&gt;'12/3&lt;/code&gt; will get displayed as 12/3 in the spreadsheet and saved out as text. This is probably quite safe to use.&lt;/p&gt;&#10;&#10;&lt;p&gt;Not sure why you would want 12/3 stored numerically as 12/3 in the text file - how should something like R read this?&lt;/p&gt;&#10;&#10;&lt;p&gt;Your comment on warnings about losing features or only saving the active sheet aren't really problems are they? (If they are, then I want your problems in my life ;-)&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2010-12-08T07:31:04.013" Id="5239" LastActivityDate="2010-12-08T10:35:34.760" LastEditDate="2010-12-08T10:35:34.760" LastEditorUserId="1390" OwnerUserId="1390" ParentId="5238" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;I like Gnumeric because it does not try to be so much idiot-resistant as others (it doesn't shout about lost functionality) and works with large data... yet I think it is Linux-only.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-12-08T09:09:34.283" Id="5243" LastActivityDate="2010-12-08T09:09:34.283" OwnerUserId="88" ParentId="5238" PostTypeId="2" Score="0" />
&#10;$Y_{2i}=X_{2i}\beta_1+U_{2i}$
  
  
  
  
  <row Body="&lt;p&gt;I suggest you look at google refine (http://code.google.com/p/google-refine/). I think is a very good tool for editing CSV files&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-12-08T17:19:52.813" Id="5265" LastActivityDate="2010-12-08T17:19:52.813" OwnerDisplayName="cesc" ParentId="5238" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;In all seriousness, I would consider RData files created by R itself as it fits&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;reliable (check)&lt;/li&gt;&#10;&lt;li&gt;simple (call it a draw--the format is binary)&lt;/li&gt;&#10;&lt;li&gt;open (check: doesn't get more open than R source code)&lt;/li&gt;&#10;&lt;li&gt;interoperable (check: works everywhere R works)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Close enough for me.  If by systems you mean &lt;em&gt;applications&lt;/em&gt; rather than operating system then the last point is a fail.&lt;/p&gt;&#10;&#10;&lt;p&gt;Oh, and RData is &lt;em&gt;efficient&lt;/em&gt; as the files are now by default compressed (which used to be an option which was turned off by default).&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2010-12-08T22:11:26.423" Id="5279" LastActivityDate="2010-12-08T22:26:10.397" LastEditDate="2010-12-08T22:26:10.397" LastEditorUserId="930" OwnerUserId="334" ParentId="5249" PostTypeId="2" Score="5" />
  
  <row AnswerCount="2" Body="&lt;p&gt;My name is Tuhin.&#10;I came up with a couple of questions when I was doing an&#10;analysis in R.&lt;/p&gt;&#10;&#10;&lt;p&gt;I did a logistic regression analysis in R and tried to check&#10;how good the model fits the data.&lt;/p&gt;&#10;&#10;&lt;p&gt;But, I got stuck as I could not get the pseudo R square value&#10;for the model which could give me some idea about the variation&#10;explained by the model.&lt;/p&gt;&#10;&#10;&lt;p&gt;Could you please guide me on how to achieve this value (pseudo&#10;R square for Logistic regression analysis).&#10;It would also be helpful if you could show me a way to get the&#10;Hosmer Lemeshow statistic for the model as well. I found out a&#10;user defined function to do it, but if there is a quicker way&#10;possible, it would be really helpful.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would be very grateful if you can provide me the answers to&#10;my queries.&lt;/p&gt;&#10;&#10;&lt;p&gt;Eagerly waiting for your response.&lt;/p&gt;&#10;&#10;&lt;p&gt;Regards&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-12-09T13:56:21.027" FavoriteCount="2" Id="5293" LastActivityDate="2010-12-09T17:18:23.010" LastEditDate="2010-12-09T15:33:07.360" LastEditorUserId="930" OwnerDisplayName="Tuhin" PostTypeId="1" Score="5" Tags="&lt;r&gt;&lt;logistic&gt;&lt;goodness-of-fit&gt;" Title="Find out pseudo R square value for a Logistic Regression analysis" ViewCount="2365" />
  
  <row AcceptedAnswerId="51167" AnswerCount="1" Body="&lt;p&gt;I am looking for applied references to data augmentation (preferably with some written code).  Either online references are books would be great.&lt;/p&gt;&#10;&#10;&lt;p&gt;I found this book online:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/142007749X&quot; rel=&quot;nofollow&quot;&gt;http://www.amazon.com/Bayesian-Missing-Data-Problems-Biostatistics/dp/142007749X/ref=sr_1_1?ie=UTF8&amp;amp;s=books&amp;amp;qid=1291905761&amp;amp;sr=1-1&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;But with no reviews I am hesitant on purchasing it.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;&#10;&lt;p&gt;Edit:  I have two variables X and Y.  Let's say X follows a mixture of normals and there is a logistic relationship between X and Y.  There is measurement error when observing X.  We observe 100 X Y pairs and need to estimate the function between the two.&lt;/p&gt;&#10;&#10;&lt;p&gt;In a book on measurement error (John P. Buonaccorsi) the author recommends data augmentation (I believe the introduced variables are the true X means) for estimation.  However no details are given.  I am looking for simple examples (R code but doesn't really matter) to get started.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2010-12-09T14:49:52.150" FavoriteCount="0" Id="5299" LastActivityDate="2013-03-01T23:50:43.640" LastEditDate="2011-03-27T16:06:35.893" LastEditorUserId="919" OwnerUserId="2310" PostTypeId="1" Score="1" Tags="&lt;mcmc&gt;&lt;error&gt;&lt;measurement&gt;&lt;mixture&gt;" Title="Data Augmentation Examples" ViewCount="852" />
  
  <row Body="&lt;p&gt;I would second @Shane's recommendation for &lt;a href=&quot;http://www.deducer.org/pmwiki/pmwiki.php?n=Main.DeducerManual&quot;&gt;Deducer&lt;/a&gt;, and would also recommend &lt;a href=&quot;http://socserv.mcmaster.ca/jfox/Misc/Rcmdr/&quot;&gt;the R Commander&lt;/a&gt; by John Fox.  The CRAN package is &lt;a href=&quot;http://cran.r-project.org/web/packages/Rcmdr/index.html&quot;&gt;here&lt;/a&gt;.  It's called the R &quot;Commander&quot; because it returns the R commands associated with the point-and-click menu selections, which can be saved and run later from the command prompt.  &lt;/p&gt;&#10;&#10;&lt;p&gt;In this way, if you don't know how to do &lt;em&gt;something&lt;/em&gt; then you can find it in the menus and get an immediate response for the proper way to do &lt;em&gt;something&lt;/em&gt; with R code. It looks like Deducer operates similarly, though I haven't played with Deducer for a while.&lt;/p&gt;&#10;&#10;&lt;p&gt;The base R Commander is designed for beginner-minded tasks, but there are plugins available for some more sophisticated analyses (Deducer has plugins, too). Bear in mind, however, that no GUI can do &lt;em&gt;everything&lt;/em&gt;, and at some point the user will need to wean him/herself from pointing-and-clicking.  Some people (myself included) think that is a good thing.&lt;/p&gt;&#10;" CommentCount="5" CommunityOwnedDate="2010-12-09T15:05:19.457" CreationDate="2010-12-09T15:05:19.457" Id="5301" LastActivityDate="2011-10-15T07:24:31.043" LastEditDate="2011-10-15T07:24:31.043" LastEditorUserId="-1" OwnerUserId="1108" ParentId="5292" PostTypeId="2" Score="34" />
  <row AcceptedAnswerId="5309" AnswerCount="3" Body="&lt;p&gt;I'm trying to transition to &lt;code&gt;R&lt;/code&gt; from using &lt;code&gt;SPSS&lt;/code&gt;. In the past, I've setup my data in the wide format for my repeated measures ANOVAs in &lt;code&gt;SPSS&lt;/code&gt;. How do I need to setup my data to run &lt;code&gt;nlme()&lt;/code&gt;? The data is balanced. It has within-subjects variables of trial type(3 levels) each measured on 8 times, and a between-subjects factor with 2 levels. 2 separate analyses will be run; one with response time as the DV and another with accuracy as the DV. I know the data has to be in a long format but I'm not sure how the columns should be arranged. Should 1 column be the subject id, another the trial type, another the time, and then 1 for the DV? Does this matter? Any points in the right direction would be greatly appreciated. Thanks.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-12-09T15:55:42.223" FavoriteCount="2" Id="5306" LastActivityDate="2013-09-04T15:59:02.337" LastEditDate="2013-09-04T15:59:02.337" LastEditorUserId="21599" OwnerUserId="2322" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;mixed-model&gt;&lt;repeated-measures&gt;&lt;dataset&gt;&lt;nlme&gt;" Title="How do I setup up repeated measures data for analysis with nlme()?" ViewCount="2119" />
  <row AcceptedAnswerId="5326" AnswerCount="1" Body="&lt;p&gt;For this dataset:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;data my;&#10;    input x y;&#10;    datalines;&#10;    -122.413582861209 37.7828877716232&#10;    -122.417876547159 37.7848288325307&#10;;&#10;proc print;&#10;run;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The output and related tables are:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/lGS8r.png&quot; alt=&quot;alt text&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;How can I import, save and use these values to their maximum precision?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-12-09T16:34:50.800" Id="5308" LastActivityDate="2010-12-10T06:47:56.540" OwnerUserId="1077" PostTypeId="1" Score="0" Tags="&lt;sas&gt;&lt;pc-sas&gt;" Title="Read decimal values in SAS" ViewCount="809" />
  
  
  <row AcceptedAnswerId="5337" AnswerCount="1" Body="&lt;p&gt;I want to cluster a graph using the FCM algorithm, I used the adjacency matrix of the graph as the data, and the &quot;Euclidean&quot; distance  as metric.&#10;The problem is that the adjacency matrix is full of zero's (depends actually on the degree of the node) and my network is a kinda huge more than 2000 node.&#10;For the results I get many small size clusters and some big size clusters, that some time contain about 50% of the population and it is not right.&lt;/p&gt;&#10;&#10;&lt;p&gt;I think my problem is from the data representation. is there any conditions that must be satisfied apart from the FCM algorithm conditions.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you in advance.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-12-10T01:40:31.767" Id="5329" LastActivityDate="2010-12-10T04:39:47.820" OwnerUserId="2325" PostTypeId="1" Score="1" Tags="&lt;clustering&gt;" Title="Are there any conditions on the data in Fuzzy c-mean clustering?" ViewCount="142" />
  
  
  
  <row Body="&lt;p&gt;On Microsoft Office for Mac 2008 &lt;code&gt;LOGNORMDIST(5;1;1)&lt;/code&gt; gives &lt;code&gt;0,728882893&lt;/code&gt; in Excel. On R &lt;code&gt;plnorm(5,1,1)&lt;/code&gt; gives &lt;code&gt;0.7288829&lt;/code&gt;. In R you need to supply mean and standard deviation on log scale, so it seems that in Excel you need to do the same.&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2010-12-10T08:19:06.807" Id="5342" LastActivityDate="2010-12-10T08:19:06.807" OwnerUserId="2116" ParentId="5340" PostTypeId="2" Score="4" />
  
  
  
  <row Body="&lt;p&gt;PCA actually ends up with orthogonal variables, therefore the covariance between the components should be 0. &lt;/p&gt;&#10;&#10;&lt;p&gt;It does not make sense to do factor analysis which selects the factors from a dataset after a procedure that makes factors from the dataset.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2010-12-11T06:27:19.523" Id="5369" LastActivityDate="2010-12-11T06:27:19.523" OwnerUserId="1808" ParentId="5368" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;It seems the ship has already sailed in terms of an answer, but I think that if this is an introductory course that most of the displays offered here are going to be too difficult to grasp for introductory students... or at the very least too difficult to grasp without an introductory display which provides a very simplified explanation of partitioning variance.  Show them how SST total increases with the number of subjects.  Then after showing it inflate for several subjects (maybe adding one in each group several times), explain that SST = SSB + SSW (though I prefer to call it SSE from the outset because it avoids confusion when you go to the within subjects test IMO).  Then show them a visual representation of the variance partitioning, e.g. a big square color coded such that you can see how SST is made of SSB and SSW.  Then, graphs similar to Tals or EDi may become useful, but I agree with EDi that the scale should be SS rather than MS for pedagogical purposes when first explaining things.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-12-11T14:43:17.020" Id="5376" LastActivityDate="2010-12-12T04:24:44.690" LastEditDate="2010-12-12T04:24:44.690" LastEditorUserId="196" OwnerUserId="196" ParentId="5278" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Do you know this article: &lt;a href=&quot;http://www.iasbs.ac.ir/chemistry/chemometrics/history/4th/5.pdf&quot;&gt;PLS-regression: a basic tool of chemometrics&lt;/a&gt;? Deriving SE and CI for the PLS parameters is described in §3.11. &lt;/p&gt;&#10;&#10;&lt;p&gt;I generally rely on Bootstrap for computing CIs, as suggested in e.g., Abdi, H. &lt;a href=&quot;http://www.utdallas.edu/~herve/abdi-wireCS-PLS2010.pdf&quot;&gt;Partial least squares regression and projection on latent structure regression (PLS Regression)&lt;/a&gt;. I seem to remember there are theoretical solutions discussed in Tenenhaus M. (1998) &lt;em&gt;La régression PLS: Théorie et pratique&lt;/em&gt; (Technip), but I cannot check for now as I don't have the book. For now, there are some useful R packages, like &lt;a href=&quot;http://cran.r-project.org/web/packages/plsRglm/index.html&quot;&gt;plsRglm&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;P.S.&lt;/em&gt; I just discovered &lt;a href=&quot;http://128.84.158.119/pdf/1002.4112v1&quot;&gt;Nicole Krämer&lt;/a&gt;'s article, in reference to the &lt;a href=&quot;http://cran.r-project.org/web/packages/plsdof/index.html&quot;&gt;plsdof&lt;/a&gt; R package.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-12-11T21:07:43.147" Id="5385" LastActivityDate="2010-12-11T21:07:43.147" OwnerUserId="930" ParentId="5364" PostTypeId="2" Score="8" />
  <row AcceptedAnswerId="5398" AnswerCount="2" Body="&lt;p&gt;In a logit model, is there a smarter way to determine the effect of an independent ordinal variable than to use dummy variables for each level?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2010-12-11T22:50:26.880" FavoriteCount="9" Id="5387" LastActivityDate="2010-12-12T11:26:47.150" LastEditDate="2010-12-11T23:15:04.053" LastEditorUserId="82" OwnerUserId="82" PostTypeId="1" Score="14" Tags="&lt;logistic&gt;&lt;logit&gt;&lt;ordinal&gt;" Title="Logit with ordinal independent variables" ViewCount="4864" />
  <row Body="&lt;p&gt;it's perfectly fine to use a categorical predictor in a logit (or OLS) regression model if the levels are ordinal. But if you have a reason to treat each level as discrete (or if in fact your categorical variable is nominal rather than ordinal), then, as alternative to dummy coding, you can also use orthogonal contrast coding.  For very complete &amp;amp; accessible discussion, see Judd, C.M., McClelland, G.H. &amp;amp; Ryan, C.S. Data analysis : a model comparison approach, Edn. 2nd. (Routledge/Taylor and Francis, New York, NY; 2008), or just google &quot;contrast coding&quot;&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2010-12-11T23:29:31.963" Id="5388" LastActivityDate="2010-12-11T23:29:31.963" OwnerUserId="11954" ParentId="5387" PostTypeId="2" Score="4" />
&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ p_i \sim Beta(\alpha, \beta) $$
&lt;/p&gt;&#10;&#10;&lt;p&gt;Using &lt;a href=&quot;http://en.wikipedia.org/wiki/Beta_distribution&quot; rel=&quot;nofollow&quot;&gt;wikipedia&lt;/a&gt; we can get estimates of $\hat{\alpha}$ and $\hat{\beta}$ (see parameter estimation section).&lt;/p&gt;&#10;&#10;&lt;p&gt;Now you can generate draws for the $i^{th}$ step, generate $p_i$ from $Beta(\hat{\alpha},\hat{\beta})$ and then generate $ X_i$ from $Ber(p_i)$. After you have done this $N$ times you can get $Y = \sum X_i$. This is a single cycle for generation of Y, do this $M$(large) number of times and the histogram for $M$ Ys will be the estimate of density of Y. &lt;/p&gt;&#10;&#10;&lt;p&gt;$$Prob[Y \leq y] = \frac {\#Y \leq y} {M}$$
  <row Body="&lt;p&gt;You may check the paper: Efficient L1-regularized logistic regression, which is an IRLS-based algorithm for LASSO. &#10;Regarding the implementation, the link may be useful for you&#10;(http://ai.stanford.edu/~silee/softwares/irlslars.htm).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-12-13T03:03:02.913" Id="5422" LastActivityDate="2010-12-13T03:03:02.913" OwnerDisplayName="eaudex" ParentId="3511" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;Here's another perspective on using multilevel vs. regression models: In an interesting paper by Afshartous and de Leeuw, they show that if the purpose of the modeling is predictive (that is, to predict new observations), the choice of model is different from when the goal is inference (where you try to match the model with the data structure). The paper that I am referring to is &lt;/p&gt;&#10;&#10;&lt;p&gt;Afshartous, D., de Leeuw, J. (2005). Prediction in multilevel models. J. Educat. Behav. Statist. 30(2):109–139.&lt;/p&gt;&#10;&#10;&lt;p&gt;I just found another related paper by these authors here: &lt;a href=&quot;http://moya.bus.miami.edu/~dafshartous/Afshartous_CIS.pdf&quot;&gt;http://moya.bus.miami.edu/~dafshartous/Afshartous_CIS.pdf&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-12-13T05:08:35.643" Id="5425" LastActivityDate="2010-12-13T05:08:35.643" OwnerUserId="1945" ParentId="1995" PostTypeId="2" Score="6" />
  
  
  
  <row AcceptedAnswerId="5547" AnswerCount="1" Body="&lt;p&gt;When building a CART model (specifically classification tree) using rpart (in R), it is sometimes obvious that there are variables (X's) that are meaningful for predicting some of the outcome (y) variables - while other predictors are relevant for other y's only.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;How can it be estimated, which explanatory variable is &quot;used&quot; for which of the predicted value in the outcome variable?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is an example code in which x2 is the only important variable for predicting &quot;b&quot; (one of the y outcomes).  There is no predicting variable for &quot;c&quot;, and x1 is a predictor for &quot;a&quot;, assuming that x2 permits it.&lt;/p&gt;&#10;&#10;&lt;p&gt;How can this situation be extracted from the fitted model? &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;N &amp;lt;- 200&#10;set.seed(5123)&#10;x1 &amp;lt;- runif(N)&#10;x2 &amp;lt;- runif(N)&#10;x3 &amp;lt;- runif(N)&#10;y &amp;lt;- sample(letters[1:3], N, T)&#10;y[x1 &amp;lt;.5] &amp;lt;- &quot;a&quot;&#10;y[x2 &amp;lt;.1] &amp;lt;- &quot;b&quot;&#10;&#10;fit &amp;lt;- rpart(y ~ x1+x2)&#10;fit2 &amp;lt;- prune(fit, cp= 0.07)&#10;plot(fit2)&#10;text(fit2, use.n=TRUE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Thanks.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-12-13T19:24:57.693" FavoriteCount="1" Id="5443" LastActivityDate="2010-12-16T02:34:59.040" OwnerUserId="253" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;cart&gt;&lt;rpart&gt;" Title="Estimate the &quot;meaningful&quot; predictors for a value in a CART model (rpart)" ViewCount="633" />
  <row AcceptedAnswerId="5471" AnswerCount="2" Body="&lt;p&gt;All,&lt;/p&gt;&#10;&#10;&lt;p&gt;I am &lt;em&gt;trying&lt;/em&gt; to study the GBM package in R. &lt;/p&gt;&#10;&#10;&lt;p&gt;I. I wanted to try and figure out where the deviance, initial value, gradient  and terminal node estimates came from. Please see this snippet:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/xeCwF.jpg&quot; alt=&quot;alt text&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;To start out, I was not sure where the Bernoulli deviance came from. I expected that the deviance, would be -2 multiplied by the log-likelihood&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/GNSRh.jpg&quot; alt=&quot;alt text&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;, ignoring the weight variable w. Can anyone suggest what I am doing (or missing) wrong to dervive what is shown in GBM?&lt;/p&gt;&#10;&#10;&lt;p&gt;II. I was also confused on where the terminal node estimates came from?&lt;/p&gt;&#10;&#10;&lt;p&gt;Any help is very appreciated!&lt;/p&gt;&#10;&#10;&lt;p&gt;Brian&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-12-14T00:09:21.473" FavoriteCount="2" Id="5452" LastActivityDate="2010-12-14T13:20:06.913" OwnerUserId="2040" PostTypeId="1" Score="5" Tags="&lt;machine-learning&gt;" Title="R Package GBM - Bernoulli Deviance" ViewCount="1761" />
  <row Body="&lt;p&gt;I admire your ambition! To answer your question directly, I'm not familiar with any site. Perhaps a message board you're familiar with? Has your instructor addressed the issue of bias? It may be enough that you recognize the issue of bias in your sample and speak to how this will affect your survey. Most properly randomized nationally representative surveys have an N of somewhere between 1800 and 2300. &lt;a href=&quot;http://www.norc.uchicago.edu/GSS+Website/&quot; rel=&quot;nofollow&quot;&gt;The GSS probably has a few questions regarding global warming&lt;/a&gt;.&#10;However, this will be inaccessible if you're not familiar with SPSS or Stata. I know others on here can help you better than I in regards to weighting responses to correct for bias. &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2010-12-14T05:45:34.817" Id="5463" LastActivityDate="2014-04-28T11:30:47.080" LastEditDate="2014-04-28T11:30:47.080" LastEditorUserId="22047" OwnerUserId="2262" ParentId="5462" PostTypeId="2" Score="4" />
  
  
  
  <row AcceptedAnswerId="5505" AnswerCount="2" Body="&lt;p&gt;I am teaching myself DLM's using R's &lt;code&gt;dlm&lt;/code&gt; package and have two strange results. I am modeling a time series using three combined elements: a trend (&lt;code&gt;dlmModPoly&lt;/code&gt;), seasonality (&lt;code&gt;dlmModTrig&lt;/code&gt;), and moving seasonality (&lt;code&gt;dlmModReg&lt;/code&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;The first strange result is with the &lt;code&gt;$f&lt;/code&gt; (one-step-ahead foreacast) result. Most of this forecast appears to be one month behind the actual data, which I believe I've seen in many examples of one-step-ahead forecasting online and in books. The strange thing is that the moving seasonality is NOT similarly lagged, but hits exactly where it should. Is this normal?&lt;/p&gt;&#10;&#10;&lt;p&gt;If I use the result's &lt;code&gt;$m&lt;/code&gt; to manually assemble the componenet, everything lines up perfectly, so it's weird, though it makes sense in a way: the moving seasonality has exogenous data to help it while the rest of the forecast does not. (Still, it'd be nice to simply &lt;code&gt;lag&lt;/code&gt; the resulting &lt;code&gt;$f&lt;/code&gt; and see a nice match.)&lt;/p&gt;&#10;&#10;&lt;p&gt;More troubling is the difference I see if I change the degree of &lt;code&gt;dlmModPoly&lt;/code&gt;'s polynomial (from 1 to 2) in an attempt to get a smoother level. This introduces a huge spike in all three components at month 9. The spikes all basically cancel out in the composite, but obviously make each piece, say the level or the seasonality, look rather ridiculous there.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is this just one of those things that happens and I should be prepared to throw away the result's first year of data as &quot;break-in&quot;? Or is it an indication that something is wrong? (Even in the degree 1 polynomial case, the first year's moving seasonality's level is a bit unsettled, but no huge spike as when I use a degree 2 polynomial.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is my R code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lvl0 &amp;lt;- log (my.data[1])&#10;slp0 &amp;lt;- mean (diff (log (my.data)))&#10;&#10;buildPTR2 &amp;lt;- function (x)&#10;   {&#10;   pm &amp;lt;- dlmModPoly (order=1, dV=exp (x[1]), dW=exp (x[2]), m0=lvl0)&#10;   tm &amp;lt;- dlmModTrig (s=12, dV=exp (x[1]), q=2, dW=exp (x[3:4]))&#10;   rm &amp;lt;- dlmModReg (moving.season, dV=exp (x[1]))&#10;&#10;   ptrm &amp;lt;- pm + tm + rm&#10;   return (ptrm)&#10;   }&#10;&#10;mlptr2 &amp;lt;- dlmMLE (log (my.data), rep (1, 6), buildPTR2)&#10;dptr2 &amp;lt;- buildPTR2 (mlptr2$par)&#10;dptrf2 &amp;lt;- dlmFilter (log (my.data), dptr2)&#10;&#10;tsdiag (dptrf2)&#10;&#10;buildPTR3 &amp;lt;- function (x)&#10;   {&#10;   pm &amp;lt;- dlmModPoly (order=2, dV=exp (x[1]), dW=c(0, exp (x[2])), m0=c(lvl0, slp0))&#10;   tm &amp;lt;- dlmModTrig (s=12, dV=exp (x[1]), q=2, dW=exp (x[3:4]))&#10;   rm &amp;lt;- dlmModReg (moving.season, dV=exp (x[1]))&#10;&#10;   ptrm &amp;lt;- pm + tm + rm&#10;   return (ptrm)&#10;   }&#10;&#10;mlptr3 &amp;lt;- dlmMLE (log (my.data), rep (1, 8), buildPTR3)&#10;dptr3 &amp;lt;- buildPTR3 (mlptr3$par)&#10;dptrf3 &amp;lt;- dlmFilter (log (my.data), dptr3) &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Per the follow-on question: the data itself is monthly data for 10 years, with each month being the weekly average attendance at a theatrical production. The data definitely has seasonal and moving seasonal effects. I want to model the trend and the seasonal effects to give the management some insight, and to prepare for forecasting. (Which is not directly possible with &lt;code&gt;dlm&lt;/code&gt; when you include a &lt;code&gt;dlmModReg&lt;/code&gt; component, though that's the next step.)&lt;/p&gt;&#10;&#10;&lt;p&gt;(I am trying to use an order=2 polynomial component that I believe creates an IRW trend, which is supposed to be nicely smooth.)&lt;/p&gt;&#10;&#10;&lt;p&gt;If it matters, my moving seasonality is a yearly Big Bash Gala event that can fall in two different months, and I indicate it with 0 for most months and 1 for months in which the Big Bash falls.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2010-12-14T14:05:31.890" FavoriteCount="2" Id="5479" LastActivityDate="2011-09-16T18:35:35.160" LastEditDate="2010-12-14T20:13:14.577" LastEditorUserId="1764" OwnerUserId="1764" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;time-series&gt;&lt;dlm&gt;" Title="DLM results looking wonky" ViewCount="1010" />
  <row Body="&lt;p&gt;My attempt at a simple answer that is both applicable across sub-domains and understandable (in gist) to the lay-person: When science develops theories about the world, these theories are compared to real-world data. The role of the statistician is to assess how well one or more competing theories account for the data. This is achieved with mathematics that let the statistician quantify their uncertainty about the conclusions they draw from the comparison of theory to data.&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2010-12-14T20:00:01.800" CreationDate="2010-12-14T14:12:47.083" Id="5480" LastActivityDate="2010-12-14T14:12:47.083" OwnerUserId="364" ParentId="5457" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;(Because this is approach is independent of the other solutions posted, including one that I have posted, I'm offering it as a separate response).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;You can compute the exact distribution in seconds (or less) provided the sum of the p's is small.&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;We have already seen suggestions that the distribution might approximately be Gaussian (under some scenarios) or Poisson (under other scenarios).  Either way, we know its mean $\mu$ is the sum of the $p_i$ and its variance $\sigma^2$ is the sum of $p_i(1-p_i)$.  Therefore the distribution will be concentrated within a few standard deviations of its mean, say $z$ SDs with $z$ between 4 and 6 or thereabouts.  Therefore we need only compute the probability that the sum $X$ equals (an integer) $k$ for $k = \mu - z \sigma$ through $k = \mu + z \sigma$.  When most of the $p_i$ are small, $\sigma^2$ is approximately equal to (but slightly less than) $\mu$, so to be conservative we can do the computation for $k$ in the interval $[\mu - z \sqrt{\mu}, \mu + z \sqrt{\mu}]$.  For example, when the sum of the $p_i$ equals $9$ and choosing $z = 6$ in order to cover the tails well, we would need the computation to cover $k$ in $[9 - 6 \sqrt{9}, 9 + 6 \sqrt{9}]$ = $[0, 27]$, which is just 28 values.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;The distribution is computed recursively&lt;/strong&gt;.  Let $f_i$ be the distribution of the sum of the first $i$ of these Bernoulli variables.  For any $j$ from $0$ through $i+1$, the sum of the first $i+1$ variables can equal $j$ in two mutually exclusive ways: the sum of the first $i$ variables equals $j$ and the $i+1^\text{st}$ is $0$ or else the sum of the first $i$ variables equals $j-1$ and the $i+1^\text{st}$ is $1$.  Therefore&lt;/p&gt;&#10;&#10;&lt;p&gt;$$f_{i+1}(j) = f_i(j)(1 - p_{i+1}) + f_i(j-1) p_{i+1}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;We only need to carry out this computation for integral&lt;/strong&gt; $j$ &lt;strong&gt;in the interval from&lt;/strong&gt; $\max(0, \mu - z \sqrt{\mu})$ &lt;strong&gt;to&lt;/strong&gt; $\mu + z \sqrt{\mu}.$&lt;/p&gt;&#10;&#10;&lt;p&gt;When most of the $p_i$ are tiny (but the $1 - p_i$ are still distinguishable from $1$ with reasonable precision), this approach is not plagued with the huge accumulation of floating point roundoff errors used in the solution I previously posted.  Therefore, extended-precision computation is not required.  For example, a double-precision calculation for an array of $2^{16}$ probabilities $p_i = 1/(i+1)$ ($\mu = 10.6676$, requiring calculations for probabilities of sums between $0$ and $31$) took 0.1 seconds with Mathematica 8 and 1-2 seconds with Excel 2002 (both obtained the same answers).  Repeating it with quadruple precision (in Mathematica) took about 2 seconds but did not change any answer by more than $3 \times 10^{-15}$.  Terminating the distribution at $z = 6$ SDs into the upper tail lost only $3.6 \times 10^{-8}$ of the total probability.&lt;/p&gt;&#10;&#10;&lt;p&gt;Another calculation for an array of 40,000 double precision random values between 0 and 0.001 ($\mu = 19.9093$) took 0.08 seconds with Mathematica.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;This algorithm is parallelizable.&lt;/strong&gt; Just break the set of $p_i$ into disjoint subsets of approximately equal size, one per processor.  Compute the distribution for each subset, then convolve the results (using FFT if you like, although this speedup is probably unnecessary) to obtain the full answer.  This makes it practical to use even when $\mu$ gets large, when you need to look far out into the tails ($z$ large), and/or $n$ is large.&lt;/p&gt;&#10;&#10;&lt;p&gt;The timing for an array of $n$ variables with $m$ processors scales as $O(n(\mu + z \sqrt{\mu})/m)$.  Mathematica's speed is on the order of a million per second.  For example, with $m = 1$ processor, $n = 20000$ variates, a total probability of $\mu = 100$, and going out to $z = 6$ standard deviations into the upper tail, $n(\mu + z \sqrt{\mu})/m = 3.2$ million: figure a couple seconds of computing time.  If you compile this you might speed up the performance two orders of magnitude.&lt;/p&gt;&#10;&#10;&lt;p&gt;Incidentally, in these test cases, graphs of the distribution clearly showed some positive skewness: they aren't normal.&lt;/p&gt;&#10;&#10;&lt;p&gt;For the record, here is a Mathematica solution:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;pb[p_, z_] := Module[&#10;  {\[Mu] = Total[p]},&#10;  Fold[#1 - #2 Differences[Prepend[#1, 0]] &amp;amp;, &#10;   Prepend[ConstantArray[0, Ceiling[\[Mu] + Sqrt[\[Mu]] z]], 1], p]&#10;  ]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;(&lt;strong&gt;NB&lt;/strong&gt; The color coding applied by this site is meaningless for Mathematica code.  In particular, the gray stuff is &lt;em&gt;not&lt;/em&gt; comments: it's where all the work is done!)&lt;/p&gt;&#10;&#10;&lt;p&gt;An example of its use is&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;pb[RandomReal[{0, 0.001}, 40000], 8]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;h3&gt;Edit&lt;/h3&gt;&#10;&#10;&lt;p&gt;An &lt;code&gt;R&lt;/code&gt; solution is ten times slower than &lt;em&gt;Mathematica&lt;/em&gt; in this test case--perhaps I have not coded it optimally--but it still executes quickly (about one second):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;pb &amp;lt;- function(p, z) {&#10;  mu &amp;lt;- sum(p)&#10;  x &amp;lt;- c(1, rep(0, ceiling(mu + sqrt(mu) * z)))&#10;  f &amp;lt;- function(v) {x &amp;lt;&amp;lt;- x - v * diff(c(0, x));}&#10;  sapply(p, f); x  &#10;}&#10;y &amp;lt;- pb(runif(40000, 0, 0.001), 8)&#10;plot(y)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/TbfOz.png&quot; alt=&quot;Plot of PDF&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-12-14T16:46:07.687" Id="5482" LastActivityDate="2012-11-12T23:15:33.837" LastEditDate="2012-11-12T23:15:33.837" LastEditorUserId="919" OwnerUserId="919" ParentId="5347" PostTypeId="2" Score="5" />
&#10;Now partition the sum into parts which are more extreme than my score and those which are less extreme:&#10;$$(n-1) s^2 = \sum_{i \in I} (X_i - \bar{X})^2 + \sum_{i \not\in I}(X_i - \bar{X})^2$$,&#10;where $I$ is the set of indices $i$ such that $|X_i - \bar{X}| \ge |X_{me} - \bar{X}|$. Now bound the summands in the right sum by zero from below, and bound those in the left sum by the condition defining $I$:&#10;$$(n-1) s^2 \ge \sum_{i \in I} (X_{me} - \bar{X})^2 + \sum_{i \not\in I}0 = |I| (X_{me} - \bar{X})^2$$
  
  
  <row Body="&lt;p&gt;The TV show Numb3rs is useful as many people have seen it. I tell them that I'm like the guys on Numb3rs except I deal with solving business problems rather than crimes. (Substitute &quot;business problems&quot; for whatever field you work in.) That usually gets the response &quot;Wow, cool!&quot; which is better than what I used to get.&lt;/p&gt;&#10;" CommentCount="2" CommunityOwnedDate="2010-12-14T23:17:09.983" CreationDate="2010-12-14T23:17:09.983" Id="5497" LastActivityDate="2010-12-14T23:17:09.983" OwnerUserId="159" ParentId="5457" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;From what I can tell, your problems seem to be:&lt;/p&gt;&#10;&#10;&lt;p&gt;1) smooth the time series data to remove correlated fluctuations. &lt;/p&gt;&#10;&#10;&lt;p&gt;2) Identify which of the inputs differs, using the smoothed data. &lt;/p&gt;&#10;&#10;&lt;p&gt;You're seem to be worried about not being able to solve (2) once you solve (1). But let's solve 1 first and then worry about 2, right? &lt;/p&gt;&#10;&#10;&lt;p&gt;Here's one idea. You say you're sampling in a round robin fashion and you have 16 inputs. So maybe treat each 16 draws as one &quot;round&quot;, sum up all of the values in each round, and divide each value in the 16-draw round by that sum to normalize. &lt;/p&gt;&#10;&#10;&lt;p&gt;It seems that this would work if your time series data is correlated on a longer time scale than 16 data points. If the data is correlated on a much longer time scale you could even normalize in larger chunks, like 160 or 1,600 data points, to maximize noise reduction (and comp. efficiency). &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-12-15T03:04:59.567" Id="5500" LastActivityDate="2010-12-15T03:04:59.567" OwnerUserId="2073" ParentId="5461" PostTypeId="2" Score="3" />
  
  
  
  
  <row AnswerCount="2" Body="&lt;p&gt;Could anyone kindly provide an explanation (mathematically or non-mathematically) about the non-existence of the intercept term in &lt;a href=&quot;http://www.ats.ucla.edu/stat/sas/library/logistic.pdf&quot; rel=&quot;nofollow&quot;&gt;conditional logistic regression&lt;/a&gt;? Is the interpretation of the coefficients similar to that of (unconditional) &lt;a href=&quot;http://en.wikipedia.org/wiki/Logistic_regression&quot; rel=&quot;nofollow&quot;&gt;logistic regression&lt;/a&gt;? &lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you for your help. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-12-15T14:18:11.087" Id="5514" LastActivityDate="2013-09-03T09:53:10.170" LastEditDate="2013-09-03T09:53:10.170" LastEditorUserId="21599" OwnerDisplayName="user2264" PostTypeId="1" Score="3" Tags="&lt;logistic&gt;&lt;survival&gt;&lt;epidemiology&gt;&lt;clogit&gt;" Title="Queries on conditional logistic regression" ViewCount="248" />
  
  <row Body="&lt;p&gt;SIAM's &lt;a href=&quot;http://www.siam.org/meetings/sdm11/&quot; rel=&quot;nofollow&quot;&gt;Data Mining Conference&lt;/a&gt;, SDM11.&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2010-12-15T17:28:47.470" CreationDate="2010-12-15T17:28:47.470" Id="5518" LastActivityDate="2010-12-15T17:28:47.470" OwnerUserId="795" ParentId="1906" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="5568" AnswerCount="3" Body="&lt;p&gt;I am wondering if there is any reasonably simple way of calculating the following problem:&lt;/p&gt;&#10;&#10;&lt;p&gt;Drawing, with replacement, $n$ balls from a bin of $N$ different colored balls, with a known probability of drawing each color of ball, what is the expected number of &quot;unique&quot; balls, &lt;em&gt;i.e.,&lt;/em&gt; balls with no other ball of the same color?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;e.g.&lt;/em&gt;&lt;br&gt;&#10;$P(red) = 0.25$&lt;br&gt;&#10;$P(blue) = 0.3$&lt;br&gt;&#10;$P(green) = 0.2$&lt;br&gt;&#10;$P(yellow) = 0.25$  &lt;/p&gt;&#10;&#10;&lt;p&gt;Some example outcomes with 5 balls:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\{red, red, green, blue, yellow\}$ - 3 unique balls&lt;br&gt;&#10;$\{red, red, green, green, blue\}$ - 1 unique ball&lt;br&gt;&#10;$\{blue, blue, blue,  yellow, yellow\}$ - 0 unique balls  &lt;/p&gt;&#10;&#10;&lt;p&gt;Or, with 3 balls:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\{red, green, blue\}$ - 3 unique&lt;br&gt;&#10;$\{red, red, blue\}$ - 1 unique&lt;br&gt;&#10;$\{red, red, red\}$ - 0 unique  &lt;/p&gt;&#10;&#10;&lt;p&gt;For 1 ball, it's trivially 1; for 2 balls, it's 1 - the probability of the outcomes where the two balls are the same color * 2 balls, after that it starts getting more complicated.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-12-15T18:08:15.290" Id="5520" LastActivityDate="2010-12-21T15:01:39.427" LastEditDate="2010-12-16T22:12:01.823" LastEditorUserId="919" OwnerUserId="2395" PostTypeId="1" Score="4" Tags="&lt;expected-value&gt;&lt;multinomial&gt;" Title="Expected number of uniques in a non-uniformly distributed population" ViewCount="418" />
  
  <row AcceptedAnswerId="5526" AnswerCount="5" Body="&lt;p&gt;Is there a way to get the number of parameters of a linear model like that?&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;model &amp;lt;- lm(Y~X1+X2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I would like to get the number 3 somehow (intercept + X1 + X2). I looked for something like this in the structures that &lt;code&gt;lm&lt;/code&gt;, &lt;code&gt;summary(model)&lt;/code&gt; and &lt;code&gt;anova(model)&lt;/code&gt; return, but I didn't figure it out. In case I don't get an answer, I'll stick on&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;dim(model.matrix(model))[2]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Thank you&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-12-15T19:41:44.733" Id="5525" LastActivityDate="2014-04-25T18:40:21.540" OwnerUserId="632" PostTypeId="1" Score="5" Tags="&lt;r&gt;&lt;regression&gt;" Title="Get the number of parameters of a linear model" ViewCount="2764" />
  <row Body="&lt;p&gt;This is merely saying that $F(x) = \Pr[X \le x] = \Pr[F(X) \le F(x)]$ which is exactly what it means for $F(X)$ to have a uniform distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;OK, let's go a little slower.&lt;/p&gt;&#10;&#10;&lt;p&gt;For continuous distributions, forget for a moment that the CDF $F$ is a CDF and think of it as just a nonlinear way to re-express the values of $X$.  In fact, to make the distinction clear, suppose that $G$ is &lt;em&gt;any&lt;/em&gt; monotonically increasing way of re-expressing $X$.  Let $Y$ be the name of its re-expressed value.  $G^{-1}$, by definition, is the &quot;back transform&quot;: it expresses $Y$ back in terms of the original $X$.&lt;/p&gt;&#10;&#10;&lt;p&gt;What is the distribution of $Y$?  As always, we discover this by picking an arbitrary value that $Y$ might take on, say $y$, and ask for the chance that $Y$ is less than or equal to $y$.  Back-transform this question in terms of the original way of expressing $X$: we are inquiring about the chance that $X$ is less than or equal to $x = G^{-1}(y)$.  Now take $G$ to be $F$ and remember that $F$ is the CDF of $X$: by definition, the chance that $X$ is less than or equal to any $x$ is $F(x)$.  In this case,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$F(x) = F(G^{-1}(y)) = F(F^{-1}(y)) = y.$$
  
  <row AnswerCount="1" Body="&lt;p&gt;I have to run repeated correlations. If one of them reach the significant value (as given in the Pearson table), how can I be sure that it isn't actually a false positive?&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm pretty new in the statistical analysis, so I apologize if my question seems naive.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for any answers.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-12-16T02:04:39.983" Id="5545" LastActivityDate="2010-12-16T09:59:10.713" LastEditDate="2010-12-16T09:59:10.713" LastEditorUserId="930" OwnerUserId="2402" PostTypeId="1" Score="5" Tags="&lt;correlation&gt;&lt;multiple-comparisons&gt;" Title="How to prevent false positive findings with repeated correlations?" ViewCount="461" />
  
  
  
&lt;/p&gt;&#10;&#10;&lt;p&gt;where $p_r$ is probability of picking red ball, $p_g$ - green, $p_b$ - blue, $p_y$ - yellow.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Denote the number of unique balls in a draw by $U$. Then $U=f(R,G,B,Y)$. Since you have the distribution of vector $(R,G,B,Y)$ you can calculate distribution of $U$. Since we have four colours $U$ can get values $0,1,2,3,4$. So to get probability that $U=0$ you need to find all the possible combinations of $(R,G,B,Y)$ for which $U=0$ and add the probabilities of these combinations. So when $U=0$? When&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;All the balls appear more than once: $R&amp;gt;1$, $G&amp;gt;1$, $B&amp;gt;1$, $Y&amp;gt;1$
  
  <row Body="&lt;p&gt;The two key elements that you need to be familiar with before working through this exercise are:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;The &lt;a href=&quot;http://en.wikipedia.org/wiki/Sampling_distribution&quot; rel=&quot;nofollow&quot;&gt;sampling distribution&lt;/a&gt; of a mean: in your case, the average of your four measurements will follow a normal distribution, with a mean that equals that of the parent distribution but with a standard deviation of $3/\sqrt{4}$.&lt;/li&gt;&#10;&lt;li&gt;How to translate probabilistic assertions in terms of the underlying &lt;a href=&quot;http://en.wikipedia.org/wiki/Probability_density_function&quot; rel=&quot;nofollow&quot;&gt;Probability Density Function&lt;/a&gt;: given that there is an infinite number of possible PDFs for a distribution described by two parameters (its location and shape), it is often more convenient to work with the standardized normal distribution which is simply $\mathcal{N}(0;1)$, because if $X\sim \mathcal{N}(\mu; \sigma)$, then we know that $Z=\frac{X-\mu}{\sigma}\sim\mathcal{N}(0;1)$.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Then, you just have to figure out how to find $z_1$ and $z_2$ such that $\Pr(z_1\leq Z\leq z_2)=1-0.01$. It often helps to draw the graph of $\Pr(Z\leq z)$ which is bell-shaped, centered on its mean, and whose total area equals 1. For instance, for a given quantile $z_1$, $\Pr(Z&amp;lt;z_1)=p_1$ where $p_1$ is the shaded area shown below (here, $z_1=-1$, that is 1 standard deviation below the mean):&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Bua04.png&quot; alt=&quot;alt text&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;As the total area equals 1, the remaining (unshaded) area equals $1-p_1$. Likewise, you may readily express any bounded area as a sum or difference of such inequalities.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2010-12-16T11:07:02.010" Id="5557" LastActivityDate="2010-12-16T11:12:31.337" LastEditDate="2010-12-16T11:12:31.337" LastEditorUserId="930" OwnerUserId="930" ParentId="5555" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;Ok, here is another way (which is certainly not as elegant or concise than the other ones, but it has the merit of not requiring Emacs to check parenthesis matches :-)&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's say we have a vector of predictors of interest like this&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; Xs &amp;lt;- paste(&quot;X&quot;, 1:4, sep=&quot;&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Then, we can just use&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; allXs &amp;lt;- lapply(seq(along=Xs), &#10;                  function(x) combn(Xs, x, simplify=FALSE, &#10;                                    FUN=paste, collapse=&quot;+&quot;))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;where &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; unlist(allXs)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;gives you all 15 combinations of the X's.&lt;/p&gt;&#10;&#10;&lt;p&gt;Another option is to just change the right-hand side of a formula, say &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; fm &amp;lt;- as.formula(paste(&quot;Y ~ &quot;, paste(Xs, collapse= &quot;+&quot;)))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;so as to reflect the different combinations that you enumerate in your &lt;code&gt;comb_list&lt;/code&gt; object. This can be done using the &lt;a href=&quot;http://cran.r-project.org/web/packages/Formula/index.html&quot; rel=&quot;nofollow&quot;&gt;Formula&lt;/a&gt; package:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; fm &amp;lt;- as.formula(paste(&quot;Y ~ &quot;, paste(Xs, collapse= &quot;|&quot;)))&#10;&amp;gt; fm2 &amp;lt;- Formula(fm)&#10;&amp;gt; foo &amp;lt;- function(x) formula(fm2, rhs=x, collapse=TRUE)&#10;&amp;gt; foo(1:2)&#10;Y ~ X1 + X2&#10;&amp;lt;environment: 0x102593040&amp;gt;&#10;&amp;gt; foo(c(1,3,4))&#10;Y ~ X1 + X3 + X4&#10;&amp;lt;environment: 0x1021d02a0&amp;gt;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2010-12-16T15:04:03.347" Id="5561" LastActivityDate="2010-12-16T15:04:03.347" OwnerUserId="930" ParentId="5550" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;Let $X_i$ be the random variable equal to $1$ when there is exactly one ball of color $i$ ($i = 1, 2, \ldots, m$; to avoid confusion I write $m$ instead of $N$).  The count of color $i$ follows a Binomial($p_i$, $n$) distribution, implying the expectation of $X_i$ is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\eqalign{
  <row Body="&lt;p&gt;You are looking for &lt;a href=&quot;http://en.wikipedia.org/wiki/Quantile&quot; rel=&quot;nofollow&quot;&gt;quantiles&lt;/a&gt;; in R there is a function &lt;code&gt;quantile&lt;/code&gt; that will calculate them for you; &lt;code&gt;Hmisc&lt;/code&gt; R package provides &lt;code&gt;cut2&lt;/code&gt; function which explicitly calculates such &quot;equilibrated bins&quot;. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-12-16T20:14:35.177" Id="5575" LastActivityDate="2010-12-16T20:14:35.177" OwnerUserId="88" ParentId="5573" PostTypeId="2" Score="6" />
  
  
&#10;\end{matrix}
  
  
  
  
  <row AcceptedAnswerId="5640" AnswerCount="3" Body="&lt;p&gt;How does one approach the problem of modeling a &quot;birth-death process&quot; where the arrivals are dependent on the current state in the following way: if the population is above a certain point, the probability of an arrival decreases.&lt;/p&gt;&#10;&#10;&lt;p&gt;Basically, I'm interested in complicating (slightly) an existing model of &quot;births&quot; that just has Poisson distributed arrivals and thinking of adding in the idea that there's a &quot;saturation point&quot; above which arrivals are less likely (waiting until the population drops back below the point). &lt;/p&gt;&#10;&#10;&lt;p&gt;Should I be reading about nth-order Markov processes? Or should I be looking at queueing theory?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-12-17T17:24:10.373" FavoriteCount="1" Id="5603" LastActivityDate="2010-12-24T02:13:25.680" LastEditDate="2010-12-23T15:53:46.737" LastEditorUserId="446" OwnerUserId="446" PostTypeId="1" Score="3" Tags="&lt;stochastic-processes&gt;" Title="Modeling a birth-death process that is not memoryless" ViewCount="520" />
&#10;$$
&#10;with probability 1.&lt;/p&gt;&#10;&#10;&lt;p&gt;It should be easy to convince yourself that $Y_i$ has Uniform$[0,1]$ distribution by looking at pictures.  Doing so rigorously is tedious, but can be done.  We have to verify that $P(Y_i \leq u) = u$ for all $u \in (0,1)$.  Fix such $u$ and let $x^* = \inf\{x : F(x) \geq u \}$ — this is just the value of quantile function at $u$.  It’s defined this way to deal with flat sections.  We’ll consider two separate cases.&lt;/p&gt;&#10;&#10;&lt;p&gt;First suppose that $F(x^*) = u$.  Then &#10;$$
&#10;Y_i \leq u
&#10;\iff Y_i \leq F(x^*)
&#10;$$
&#10;= P[X_i \leq x^*]
&lt;/p&gt;&#10;&#10;&lt;p&gt;Now suppose that $F(x^*) \neq u$.  Then necessarily $F(x^*) &amp;gt; u$, and $u$ falls inside one of the gaps.  Moreover, $x^* \in D$, because otherwise $F(x^*) = u$ and we have a contradiction.&#10;Let $u^* = F(x^*)$ be the upper part of the gap.  Then by the previous case, &#10;$$
&#10;\end{align}
&#10;&amp;amp;= P[u &amp;lt; Y_i &amp;lt; u^* , X_i = x^*] \\
&#10;&amp;amp;= u^* - u .
&#10;\Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21}
  <row Body="&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Stein%27s_lemma&quot;&gt;Stein’s Lemma&lt;/a&gt; provides a very useful characterization.  $Z$ is standard Gaussian iff &#10;$$E f’(Z) = E Z f(Z)$$
&#10;for all absolutely continuous functions $f$ with $E|f’(Z)| &amp;lt; \infty$.&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2010-12-17T22:00:34.923" CreationDate="2010-12-17T22:00:34.923" Id="5610" LastActivityDate="2010-12-17T23:24:38.617" LastEditDate="2010-12-17T23:24:38.617" LastEditorUserId="1670" OwnerUserId="1670" ParentId="4364" PostTypeId="2" Score="10" />
  
&#10;~      &amp;amp; \mu_{.1} &amp;amp; \ldots   &amp;amp; \mu_{.k} &amp;amp; \ldots   &amp;amp; \mu_{.q} &amp;amp; \mu
&lt;/p&gt;&#10;&#10;&lt;p&gt;With these definitions, the model can also be written as:&#10;$Y_{ijk} = \mu + \alpha_{j} + \beta_{k} + (\alpha \beta)_{jk} + \epsilon_{i(jk)}$
  <row Body="&lt;p&gt;With increasing sample size $n$, $r_{z} = \sqrt{n-1} r_{S}$ is asymptotically $N(0, 1)$ distributed (standard normal distribution). In R&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;rSz   &amp;lt;- sqrt(n-1) * rS&#10;(pVal &amp;lt;- 1-pnorm(rSz))   # one-sided p-value, test for positive rank correlation&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2010-12-18T17:55:17.180" Id="5623" LastActivityDate="2010-12-18T17:55:17.180" OwnerUserId="1909" ParentId="5619" PostTypeId="2" Score="2" />
&#10;(x, \epsilon) - (y, \delta) = &amp;amp;(x-y, (\epsilon^2 + \delta^2)^{1/2}) \cr
  <row Body="&lt;p&gt;Select the two rows and do a scatterplot which I think is called an XY plot in Excel (sorry, I run a Linux machine, so I do not have Excel installed).&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2010-12-19T15:56:51.113" Id="5635" LastActivityDate="2010-12-19T15:56:51.113" OwnerUserId="582" ParentId="5634" PostTypeId="2" Score="5" />
  
  
  <row Body="&lt;p&gt;The only problem with Christopher answer is that it will mix up the original ordering of the factor. Here is my fix:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; Merge.factors &amp;lt;- function(x, p) {&#10;     t &amp;lt;- table(x)&#10;     levt &amp;lt;- cbind(names(t), names(t)) &#10;     levt[t/sum(t)&amp;lt;p, 2] &amp;lt;- &quot;Other&quot;&#10;     change.levels(x, levt)&#10; }&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;where &lt;code&gt;change.levels&lt;/code&gt; is the following function. I wrote it some time ago, so I suspect there might be better ways of achieving what it does.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; change.levels &amp;lt;- function(f, levt) {&#10;     ##Change the the names of the factor f levels from&#10;     ##substitution table levt.&#10;     ## In the first column there are the original levels, in&#10;     ## the second column -- the substitutes&#10;     lv &amp;lt;- levels(f)&#10;     if(sum(sort(lv) != sort(levt[, 1]))&amp;gt;0)&#10;     stop (&quot;The names from substitution table does not match given level names&quot;)&#10;     res &amp;lt;- rep(NA, length(f))&#10;&#10;     for(i in lv) {&#10;          res[f==i] &amp;lt;- as.character(levt[levt[, 1]==i, 2])&#10;     }&#10;     factor(res)&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2010-12-21T04:51:40.117" Id="5658" LastActivityDate="2010-12-21T04:51:40.117" OwnerUserId="2116" ParentId="5656" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;suppose $(X,Y)$ is bivariate normal with zero means and correlation $\rho$. then &lt;/p&gt;&#10;&#10;&lt;p&gt;${\mathrm E} XY= cov(X,Y)= \rho\sigma_X\sigma_Y$.&lt;/p&gt;&#10;&#10;&lt;p&gt;all of the entries in the matrix $x_1x_2^T$ are of the form $XY$.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-12-21T05:30:02.967" Id="5659" LastActivityDate="2010-12-21T05:30:02.967" OwnerUserId="1112" ParentId="5399" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;The &lt;a href=&quot;http://docs.lib.noaa.gov/rescue/mwr/078/mwr-078-01-0001.pdf&quot; rel=&quot;nofollow&quot;&gt;Brier Score&lt;/a&gt; approach is very simple and the most directly applicable way verify accuracy of a predicted outcome versus binary event.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Don't rely on just formulas ...plot the scores for different periods of time, data, errors, [weighted] rolling average of data, errors ... it's tough to say what visual analysis might reveal ... after you think you see something, you will better know what kind of hypothesis test to perform until AFTER you &lt;strong&gt;look&lt;/strong&gt; at the data.   &lt;/p&gt;&#10;&#10;&lt;p&gt;The Brier Score inherently assumes stability of the variation/underlying distributions weather and technology driving the forecasting models, lack of linearity, no bias, lack of change in bias ... it assumes that same general level of accuracy/inaccuracy is consistent.  As climate changes in ways that are not yet understood, the accuracy of weather predictions would decrease; conversely, the scientists feeding information to the weatherman have more resources, more complete models, more computing power so perhaps the accuracy of the predictions would increase.  Looking at the errors would tell something about stability, linearity and bias of the forecasts ... you may not have enough data to see trends; you may learn that stability, linearity and bias are not an issue.  You may learn that weather forecasts are getting more accurate ... or not.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-12-21T08:54:45.653" Id="5661" LastActivityDate="2010-12-21T22:21:01.123" LastEditDate="2010-12-21T22:21:01.123" LastEditorUserId="2342" OwnerUserId="2342" ParentId="1875" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;In fact, you should not do MCMC, since your problem is so much simpler. Try this algorithm:&lt;/p&gt;&#10;&#10;&lt;p&gt;Step 1: Generate a X from Log Normal &lt;/p&gt;&#10;&#10;&lt;p&gt;Step 2: Keeping this X fixed, generate a Y from the Singh Maddala.&lt;/p&gt;&#10;&#10;&lt;p&gt;Voila! Sample Ready!!!&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2010-12-21T17:16:52.633" Id="5671" LastActivityDate="2010-12-21T17:16:52.633" OwnerUserId="2472" ParentId="5207" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Depends on assumptions on your disturbances. If they are normal and homoscedastic, then yes use t-statistic. In economic applications though these assumptions rarely hold, so in that case I would suggest using z-statistic with &lt;a href=&quot;http://en.wikipedia.org/wiki/White_standard_errors&quot; rel=&quot;nofollow&quot;&gt;robust standard errors&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-12-21T21:10:43.467" Id="5679" LastActivityDate="2010-12-22T07:04:10.727" LastEditDate="2010-12-22T07:04:10.727" LastEditorUserId="2116" OwnerUserId="2116" ParentId="5675" PostTypeId="2" Score="3" />
  
  
  <row Body="&lt;p&gt;First of all, your usage of the term &quot;prior probability&quot; seems to be wrong. For any node N with discrete values $n_i$ the probability that a certain value of N occurs &lt;em&gt;a priori&lt;/em&gt; is $p(N=n_i)$. If a node has no parents, one is interested in calculate this prob. But if a node has parents P, one is interested in calculating the &lt;em&gt;conditional probability&lt;/em&gt;, i.e. the probability that a certain value of N occurs &lt;em&gt;given&lt;/em&gt; its parents. This is $p(N=n_i|P)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Regarding your actual question: How to calculate the conditional probabilities of a child node if the parents are continuous and the child node is a) discrete or b) continuous. I will to explain one approach using a concrete example, but I hope the general idea will be clear though.&lt;/p&gt;&#10;&#10;&lt;p&gt;Consider this network:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/HkcnB.png&quot; alt=&quot;alt text&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;where subsidy and buys are discrete nodes meanwhile harvest and cost are continuous. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;a)&lt;/strong&gt; p(Cost|Subsidy, Harvest)&#10;Two options:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Discretize Harvest and treat it as discrete =&gt; maybe information loss&lt;/li&gt;&#10;&lt;li&gt;Model a mapping from the current value of harvest to a parameter describing the distribution of Cost.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Details to 2.:&#10;Let's assume cost can be modeled using a normal distribution. In this case it is common to fix the variance and map the value of Harvest linearly to the mean of gaussian. The parent subsidy (binary) only add the constraint to create a separate distribution for subsidy=true and subsidy=false.&lt;/p&gt;&#10;&#10;&lt;p&gt;Result:&#10;$p(Cost=c|Harvest=h,Subsidy=true)=N(\alpha_1 * h + \beta_1,\sigma_1^2)(c)$ &lt;/p&gt;&#10;&#10;&lt;p&gt;$p(Cost=c|Harvest=h,Subsidy=false)=N(\alpha_2 * h + \beta_2,\sigma_2^2)(c)$ &lt;/p&gt;&#10;&#10;&lt;p&gt;for some $\alpha$s and $\beta$s. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;b)&lt;/strong&gt; p(Buys|Cost)&#10;In this case on need to map the probability of occurrence of certain costs to the probability of Buy=True (p(Buy=False) = 1-p(Buy=True)). (Note that this is the same task as in logistic regression). One approach is if the parent has a normal distribution, to calculate the integral from 0 to x of a standard normal distribution, where x is the z-transformed value of the parent. In our example:&#10;$p(Buys=true|Cost=c) = integral(0,\frac{-c+\mu}{\sigma})$ with $\mu$=mean and $\sigma$=standard deviation of the Cost distribution. In this case $-c+\mu$ instead of $\mu-c$, because an observation (knowledge extracted from data !) is, that the lower the cost the more probable is a buy.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the case of a non-binary discrete child-node, one approach is to transform the one multi-value-problem into multiple binary problems. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Example-Source / Further reading:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;&quot;Artificial Intelligence: A modern approach&quot;&lt;/em&gt; by Russell and Norvig&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2010-12-22T12:52:08.763" Id="5693" LastActivityDate="2010-12-22T13:04:42.997" LastEditDate="2010-12-22T13:04:42.997" LastEditorUserId="264" OwnerUserId="264" ParentId="4687" PostTypeId="2" Score="4" />
  <row AnswerCount="2" Body="&lt;p&gt;I have multiple independent coders who are trying to identify events in a time series -- in this case, watching video of face-to-face conversation and looking for particular nonverbal behaviors (e.g., head nods) and coding the time and category of each event.  This data could reasonable be treated as a discrete-time series with a high sampling rate (30 frames/second) or as a continuous-time series, whichever is easier to work with.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'd like to compute some measure of inter-rater reliability, but I expect there to be some uncertainty in &lt;em&gt;when&lt;/em&gt; events occurred; that is, I expect that one coder might, for example, code that a particular movement began a quarter second later than other coders thought it started.  These are rare events, if that helps; typically at least several seconds (hundreds of video frames) between events.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a good way of assessing inter-rater reliability that looks at both of these kinds of agreement and disagreement: (1) do raters agree on &lt;em&gt;what&lt;/em&gt; event occurred (if any), and (2) do they agree on &lt;em&gt;when&lt;/em&gt; it occurred?  The second is important to me because I'm interested in looking at the timing of these events relative to other things happening in the conversation, like what people are saying.&lt;/p&gt;&#10;&#10;&lt;p&gt;Standard practice in my field seems to be to divide things up into time slices, say 1/4 of a second or so, aggregate the events each coder reported per time slice, then compute Cohen's kappa or some similar measure.  But the choice of slice duration is ad-hoc, and I don't get a good idea of uncertainty in time of events.&lt;/p&gt;&#10;&#10;&lt;p&gt;Best thought I have so far is that I could compute some kind of reliability curve; something like kappa as a function of the size of the window within which I consider two events as being coded at the same time.  I'm not really sure where to go from there, though...&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2010-12-22T15:41:17.387" FavoriteCount="1" Id="5696" LastActivityDate="2013-07-26T14:41:13.317" LastEditDate="2010-12-22T15:59:33.153" LastEditorUserId="930" OwnerDisplayName="dschulman" PostTypeId="1" Score="10" Tags="&lt;time-series&gt;&lt;reliability&gt;&lt;inter-rater&gt;" Title="Interrater reliability for events in a time series with uncertainty about event time" ViewCount="418" />
  <row Body="&lt;p&gt;If the number of points is not too big, you may try all possibilities. Let's assume that the points are $X_i=(x_i,y_i)$ where $i=1,..,N$. Than, you may loop with $j$ from $2$ to $N-2$ and fit two lines to both $\{X_1,...,X_j\}$ and $\{X_{(j+1)},...,X_N\}$. Finally, you pick $j$ for which the sum of sum of squared residuals for both lines is minimal.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-12-22T17:56:04.617" Id="5701" LastActivityDate="2010-12-22T17:56:04.617" OwnerUserId="88" ParentId="5700" PostTypeId="2" Score="5" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Given $n$-vectors $x, y_1, y_2$ such that the Spearman correlation coefficient of $x$ and $y_i$ is $\rho_i = \rho(x,y_i)$, are there known bounds on the Spearman coefficient of $x$ with $y_1 + y_2$, in terms of the $\rho_i$ (and $n$, presumably)? That is, can one find (non-trivial) functions $l(\rho_1,\rho_2,n), u(\rho_1,\rho_2,n)$ such that &#10;$$l(\rho_1,\rho_2,n) \le \rho(x,y_1+y_2) \le u(\rho_1,\rho_2,n)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;edit&lt;/strong&gt;: per @whuber's example in the comment, it appears that in the general case, only the trivial bounds $l = -1, u = 1$ can be made. Thus, I would like to further impose the constraint:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;$y_1, y_2$ are permutations of the integers $1 \ldots n$.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="9" CreationDate="2010-12-22T17:57:02.523" Id="5703" LastActivityDate="2010-12-23T20:53:49.907" LastEditDate="2010-12-22T21:15:59.063" LastEditorUserId="795" OwnerUserId="795" PostTypeId="1" Score="7" Tags="&lt;correlation&gt;&lt;spearman&gt;&lt;bounds&gt;" Title="Are there bounds on the Spearman correlation of a sum of two variables?" ViewCount="327" />
  <row Body="&lt;p&gt;Spearman's rank correlation is just the Pearson product-moment correlation between the ranks of the variables. Shabbychef's extra constraint means that $y_1$ and $y_2$ are the same as their ranks and that there are no ties, so they have equal standard deviation $\sigma_y$ (say). If we also replace x by its ranks, the problem becomes the equivalent problem for the Pearson product-moment correlation.&lt;br&gt;&#10;By definition of the Pearson product-moment correlation,&#10;$$\begin{align}
&#10;          {\sqrt{2}\left(1+\rho(y_1,y_2)\right)^{1/2}}. \\
&#10;\end{align}$$&#10;For any set of three variables, if we know two of their three correlations we can put bounds on the third correlation (see e.g. &lt;a href=&quot;http://www.informaworld.com/smpp/content~content=a906010843&quot; rel=&quot;nofollow&quot;&gt;Vos 2009&lt;/a&gt;, or from &lt;a href=&quot;http://en.wikipedia.org/wiki/Partial_correlation#Using_recursive_formula&quot; rel=&quot;nofollow&quot;&gt;the formula for partial correlation&lt;/a&gt;):&#10;$$\rho_1\rho_2 - \sqrt{1-\rho_1^2}\sqrt{1-\rho_2^2} \leq \rho(y_1,y_2) \leq 
&#10; \frac{\rho_1 + \rho_2}
  <row Body="&lt;p&gt;You might want to pick up (or look at) a copy of Rick Wicklin's book: Statistical Programming with SAS IML software&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.etymonline.com/index.php?search=hysterical&amp;amp;searchmode=none&quot; rel=&quot;nofollow&quot;&gt;http://www.etymonline.com/index.php?search=hysterical&amp;amp;searchmode=none&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;He also has a blog about IML.&lt;/p&gt;&#10;&#10;&lt;p&gt;And, on SAS' site, there is a section about IML:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://support.sas.com/forums/forum.jspa?forumID=47&quot; rel=&quot;nofollow&quot;&gt;http://support.sas.com/forums/forum.jspa?forumID=47&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;And you will want IMLStudio, which offers a multiple window view that is much easier to integrate with Base SAS than the old IML was.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have used Base SAS and SAS Stat a lot.  I've only barely looked at IML.  But, from what I've seen, your knowledge of R should help.  &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-12-23T20:42:30.510" Id="5723" LastActivityDate="2010-12-23T20:42:30.510" OwnerUserId="686" ParentId="5682" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;It's true that the choice of coding method influences how you interpret the model coefficients. In my experience though (and I realise this can depend on your field), dummy coding is so prevalent that people don't have a huge problem dealing with it.&lt;/p&gt;&#10;&#10;&lt;p&gt;In this example, if male = 0 and female = 1, then the intercept is basically the mean response for males, and the Sex coefficient is the impact on the response due to being female (the &quot;female effect&quot;). Things get more complicated once you are dealing with categorical variables with more than two levels, but the interpretation scheme extends in a natural way.&lt;/p&gt;&#10;&#10;&lt;p&gt;What this ultimately means is that you should be careful that any substantive conclusions you draw from the analysis don't depend on the coding method used.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-12-24T00:48:56.240" Id="5725" LastActivityDate="2010-12-24T00:48:56.240" OwnerUserId="1569" ParentId="5713" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="5764" AnswerCount="2" Body="&lt;p&gt;I am running a model for a problem in insurance domain. The final results show some false positive x and some false negative y. I am using SAS Enterprise Miner for this. Can somebody suggest me how to reduce false positive? I know for this i have to increase the false negative. I want to know two things:  &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Is there any option in e-miner where I can give more weight to false negative and less to false positive?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Is there any general approach in modeling which tells us any ways to reduce false negatives or is it just a hit and trial approach?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="2" CreationDate="2010-12-24T06:49:14.123" FavoriteCount="1" Id="5728" LastActivityDate="2010-12-26T20:58:12.193" LastEditDate="2010-12-24T09:52:56.380" LastEditorUserId="88" OwnerUserId="1763" PostTypeId="1" Score="1" Tags="&lt;data-mining&gt;&lt;sas&gt;" Title="Reducing false positive rate" ViewCount="1088" />
  <row Body="&lt;p&gt;I think this is related to &lt;a href=&quot;http://stats.stackexchange.com/questions/4334/state-space-form-of-time-varying-ar1&quot;&gt;a previously asked question&lt;/a&gt;. One of the answers suggested to use the &lt;a href=&quot;http://admb-project.org&quot; rel=&quot;nofollow&quot;&gt;AD Model Builder software&lt;/a&gt;. Although I haven't used it myself, looking at the manual it looks like an alternative.&lt;/p&gt;&#10;&#10;&lt;p&gt;I wonder though if your problem is sufficiently specified. How does the coefficient At change? You need to put some structure on it, perhaps a smoothness constraint, it it is to be estimated at all.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2010-12-24T09:22:15.193" Id="5731" LastActivityDate="2010-12-24T09:22:15.193" OwnerUserId="892" ParentId="5724" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;R is an open source project, so you can look at the file &lt;code&gt;src/library/stats/src/loessc.c&lt;/code&gt; which implements the C-level computation behind &lt;code&gt;loess()&lt;/code&gt;.  You should be able to use that for an extension module to the other languages you listed. Or, and this may be easier, you some of the existing ways to access R from Java, Python or C#.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2010-12-24T14:10:09.720" Id="5740" LastActivityDate="2010-12-24T14:10:09.720" OwnerUserId="334" ParentId="5734" PostTypeId="2" Score="6" />
&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Obviously in this case the variances are not equal, yet if they had been equal to a common value then we could estimate it with the pooled estimator&#10;$$
  <row Body="&lt;p&gt;This is an example of multiple comparisons.  There's a large literature on this.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you have, say, 100 variables, then  you will have 100*99/2 =4950 correlations.&lt;/p&gt;&#10;&#10;&lt;p&gt;If the data are just noise, then you would expect 1 in 20 of these to be significant at p = .05.  That's 247.5&lt;/p&gt;&#10;&#10;&lt;p&gt;Before going farther, though, it would be good if you could say WHY you are doing this.  What are these variables, why are you correlating them, what is your substantive idea?&lt;/p&gt;&#10;&#10;&lt;p&gt;Or, are you just fishing for high correlations?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-12-25T22:22:20.943" Id="5752" LastActivityDate="2010-12-25T22:22:20.943" OwnerUserId="686" ParentId="5750" PostTypeId="2" Score="6" />
  
  
  <row Body="&lt;p&gt;&lt;strong&gt;Regarding first (and second) question:&lt;/strong&gt; A general approach to reduce misclassifications error by iteratively training models and reweighting rows (based on classification error) is &lt;a href=&quot;http://en.wikipedia.org/wiki/Boosting&quot; rel=&quot;nofollow&quot;&gt;Boosting&lt;/a&gt;. I think you might find that technique interesting.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Regarding second question:&lt;/strong&gt; The question sounds a little bit naive to me (but I maybe did not understand your true intention), since reducing misclassification error = improving model performance is one of &lt;em&gt;the&lt;/em&gt; challenges in Data Mining / Machine Learning. So if there were a general all-time working strategy, we all would have been replaced by machines (earlier than we will anyways). So I think that &lt;em&gt;yes&lt;/em&gt;, the general approach here is &lt;strong&gt;educated&lt;/strong&gt; trial and error. I suggest this question, &lt;a href=&quot;http://stats.stackexchange.com/questions/4830/better-classification-of-default-in-logistic-regression&quot;&gt;Better Classification of default in logistic regression&lt;/a&gt;, which may give you some ideas for both questioning and model improvement.&lt;/p&gt;&#10;&#10;&lt;p&gt;I suggest to play around a little bit and then come back to ask more specific questions. General questions regarding model improvement are hard to answer without data and/or additional information of the circumstances. Good luck !&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2010-12-26T20:51:31.283" Id="5764" LastActivityDate="2010-12-26T20:58:12.193" LastEditDate="2010-12-26T20:58:12.193" LastEditorUserId="930" OwnerUserId="264" ParentId="5728" PostTypeId="2" Score="3" />
  
  <row AcceptedAnswerId="5790" AnswerCount="2" Body="&lt;p&gt;Lets say we have random variable $X$ with known variance and mean. The question is: what is the variance of $f(X)$ for some given function f. The only general method that I'm aware of is the delta method, but it gives only aproximation. Now I'm interested in $f(x)=\sqrt{x}$, but it'd be also nice to know some general methods.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit 29.12.2010&lt;/strong&gt;&lt;br&gt;&#10;I've done some calculations using Taylor series, but I'm not sure whether they are correct, so I'd be glad if someone could &lt;strong&gt;confirm&lt;/strong&gt; them.  &lt;/p&gt;&#10;&#10;&lt;p&gt;First we need to approximate $E[f(X)]$&lt;br&gt;&#10;$E[f(X)] \approx E[f(\mu)+f'(\mu)(X-\mu)+\frac{1}{2}\cdot f''(\mu)(X-\mu)^2]=f(\mu)+\frac{1}{2}\cdot f''(\mu)\cdot Var[X]$&lt;/p&gt;&#10;&#10;&lt;p&gt;Now we can approximate $D^2 [f(X)]$&lt;br&gt;&#10;$E[(f(X)-E[f(X)])^2] \approx E[(f(\mu)+f'(\mu)(X-\mu)+\frac{1}{2}\cdot f''(\mu)(X-\mu)^2 -E[f(X)])^2]$  &lt;/p&gt;&#10;&#10;&lt;p&gt;Using the approximation of $E[f(X)]$ we know that $f(\mu)-Ef(x) \approx -\frac{1}{2}\cdot f''(\mu)\cdot Var[X]$  &lt;/p&gt;&#10;&#10;&lt;p&gt;Using this we get:&lt;br&gt;&#10;$D^2[f(X)] \approx \frac{1}{4}\cdot f''(\mu)^2\cdot Var[X]^2-\frac{1}{2}\cdot f''(\mu)^2\cdot Var[X]^2 + f'(\mu)^2\cdot Var[X]+\frac{1}{4}f''(\mu)^2\cdot E[(X-\mu)^4] +\frac{1}{2}f'(\mu)f''(\mu)E[(X-\mu)^3]$&lt;br&gt;&#10;$D^2 [f(X)] \approx \frac{1}{4}\cdot f''(\mu)^2 \cdot [D^4 X-(D^2 X)^2]+f'(\mu)\cdot D^2 X +\frac{1}{2}f'(\mu)f''(\mu)D^3 X$&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2010-12-28T14:13:16.227" FavoriteCount="8" Id="5782" LastActivityDate="2014-04-23T17:50:05.987" LastEditDate="2013-05-11T22:13:54.820" LastEditorUserId="-1" OwnerUserId="1643" PostTypeId="1" Score="16" Tags="&lt;variance&gt;&lt;random-variable&gt;&lt;delta-method&gt;" Title="Variance of a function of one random variable" ViewCount="8951" />
  <row Body="&lt;p&gt;&lt;strong&gt;No, this is not right, because no observational context has been provided.&lt;/strong&gt;  All the following scenarios are consistent with the information given:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Two games will be played.  &lt;em&gt;Beforehand&lt;/em&gt;, you hypothesize that Bill will win both.  Assuming the results are independent and Bill has 1/8 chance of winning, the chance of this occurring is 1/64 = &lt;strong&gt;about 1.5%&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Again two games will be played.  &lt;em&gt;Beforehand&lt;/em&gt;, you hypothesize that &lt;em&gt;somebody&lt;/em&gt; will win both games.  Under the same assumptions the chance of this occurring is 1/64 &lt;em&gt;for each player&lt;/em&gt;.  Because all eight possibilities (for the eight players) are mutually exclusive, the chances add, giving a probability of 1/8 = &lt;strong&gt;12.5%&lt;/strong&gt; for this outcome.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;This time an indefinite number of games will be played, with the series stopping when Bill wins two in a row.  The chance that the series does end is &lt;strong&gt;100%&lt;/strong&gt; and we observe that Bill won the last two games.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Again an indefinite number of games will be played, with the series stopping when &lt;em&gt;anybody&lt;/em&gt; wins two in a row.  The chance the series ends is 100% unless there is perfect anti-correlation among the winners.  Assuming all players have equal chances of winning, the chance that Bill is the one to end it is 1/8 = &lt;strong&gt;12.5%&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Because neither a clear null hypothesis nor an alternative have been specified, this is not a well-defined hypothesis testing situation.  Therefore the idea of &quot;p-value&quot; is &lt;em&gt;meaningless&lt;/em&gt; (and is unnecessary anyway).  In scenario #1 the low probability provides some evidence--not much--that Bill is not just winning independently at random.  Some alternative explanations include (a) some other players have essentially no chances of winning and Bill just got lucky; (b) the cards are not shuffled well between games, causing the winner of one to be the likely winner of the next, and Bill just got lucky in the first game; (c) in some games certain players have higher chances of winning not through skill but due to their position in the deal, and Bill happened to be in a good position in one or both games.&lt;/p&gt;&#10;&#10;&lt;p&gt;We cannot generally conclude that the winner of two successive games has &quot;skill&quot;.  For instance, he might have a partner who is setting him up to win.  Or he might be the mark in a group of card sharks who are letting him win to get his confidence, &lt;em&gt;etc.&lt;/em&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-12-28T16:24:26.547" Id="5785" LastActivityDate="2010-12-28T16:24:26.547" OwnerUserId="919" ParentId="5776" PostTypeId="2" Score="6" />
  
  
  
  <row Body="&lt;p&gt;It is hard to tell without data, but the set may be &quot;too homogeneous&quot; to make LOO work -- imagine you have a set $X$ and you duplicate all objects to make a set $X_d$ -- while BRT usually have very good accuracy on its train, it is pretty obvious that LOO on $X_d$ will probably give identical results to test-on-train.&lt;/p&gt;&#10;&#10;&lt;p&gt;So if the accuracy if good I would even try resampling CV (on each of let's say 10 folds you make train of an equal size to the full set by sampling objects with replacement and test from objects that were not placed in train -- this should spit them in about 1:2 proportion) on this data to verify this result. &lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT: More precise algorithm of resampling CV  &lt;/p&gt;&#10;&#10;&lt;p&gt;Given a dataset with $N$ objects and $M$ attributes:  &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Training set is made by randomly selecting $N$ objects from the original set &lt;strong&gt;with replacement&lt;/strong&gt;&lt;/li&gt;&#10;&lt;li&gt;The objects that were not selected in the step 1 form the test set (this is roughly $\frac{1}{3}N$ objects) &lt;/li&gt;&#10;&lt;li&gt;Classifier is trained on a train set and tested on test set, and the measured error is gathered&lt;/li&gt;&#10;&lt;li&gt;Steps 1-3 are repeated $T$ times, where $T$ is more less arbitrary, say 10, 15 or 30&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="8" CreationDate="2010-12-29T17:27:19.787" Id="5812" LastActivityDate="2010-12-29T21:20:27.750" LastEditDate="2010-12-29T21:20:27.750" LastEditorUserId="88" OwnerUserId="88" ParentId="5807" PostTypeId="2" Score="3" />
  
  
  <row AcceptedAnswerId="5883" AnswerCount="2" Body="&lt;p&gt;I think it is fair to say statistics is an applied science so when averages and standard deviations are calculated it is because someone is looking to make some decisions based on those numbers. Part of being a good statistician then I would hope is being able to &quot;sense&quot; when the sample data can be trusted and when some statistical test is completely misrepresenting the true data we're interested in. Being a programmer that is interested in analysis of big data sets I'm relearning some statistics and probability theory but I can't shake this nagging feeling that all the books I've looked at are kind of like politicians that get up on stage and say a whole bunch of things and then append the following disclaimer at the end of their speech &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Now, I'm not saying that this is good or bad but the numbers say it's good so you should vote for me anyway.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Maybe you get that but maybe you don't so here's a question. Where do I go to find war stories by statisticians where some decisions were based on some statistical information that later turned out to be completely wrong?&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2010-12-31T10:46:17.657" CreationDate="2010-12-30T10:11:11.710" FavoriteCount="5" Id="5831" LastActivityDate="2011-01-25T06:09:58.707" OwnerUserId="2293" PostTypeId="1" Score="14" Tags="&lt;probability&gt;&lt;summary-statistics&gt;&lt;intuition&gt;&lt;learning&gt;&lt;history&gt;" Title="Statistics, war stories, data intuition" ViewCount="474" />
  <row Body="&lt;p&gt;As for a good reference, I would recommend Philip Good, &lt;em&gt;&lt;a href=&quot;http://www.springer.com/birkhauser/applied+probability+and+statistics/book/978-0-8176-4386-7&quot;&gt;Resampling Methods: A Practical Guide to Data Analysis&lt;/a&gt;&lt;/em&gt; (Birkhäuser Boston, 2005, 3rd ed.) for an applied companion textbook. And here is &lt;a href=&quot;http://www-stat.wharton.upenn.edu/~stine/mich/bibliography.pdf&quot;&gt;An Annotated Bibliography for Bootstrap Resampling&lt;/a&gt;. &lt;a href=&quot;http://www.creative-wisdom.com/pub/mirror/resampling_methods.pdf&quot;&gt;Resampling methods: Concepts, Applications, and Justification&lt;/a&gt; also provides a good start.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are many R packages that facilitate the use of resampling techniques:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://cran.r-project.org/web/packages/boot/index.html&quot;&gt;boot&lt;/a&gt;, for bootstraping -- but see also P. Burns, &lt;a href=&quot;http://lib.stat.cmu.edu/S/Spoetry/Tutor/bootstrap_resampling.html&quot;&gt;The Statistical Bootstrap and Other Resampling Methods&lt;/a&gt;, for illustrations&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://cran.r-project.org/web/packages/coin/index.html&quot;&gt;coin&lt;/a&gt;, for permutation tests (bit see the accompagnying &lt;a href=&quot;http://cran.r-project.org/web/packages/coin/vignettes/coin.pdf&quot;&gt;vignette&lt;/a&gt; which includes extensive help) &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;(There are many other packages...)&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2010-12-30T20:18:40.273" Id="5849" LastActivityDate="2010-12-30T20:24:55.710" LastEditDate="2010-12-30T20:24:55.710" LastEditorUserId="930" OwnerUserId="930" ParentId="5845" PostTypeId="2" Score="6" />
  
  
  <row Body="&lt;p&gt;A boxplot isn't that complicated. After all, you just need to compute the three &lt;a href=&quot;http://en.wikipedia.org/wiki/Quartile&quot; rel=&quot;nofollow&quot;&gt;quartiles&lt;/a&gt;, and the min and max which define the range; a subtlety arises when we want to draw the whiskers and various methods have been proposed. For instance, in a &lt;a href=&quot;http://mathworld.wolfram.com/Box-and-WhiskerPlot.html&quot; rel=&quot;nofollow&quot;&gt;Tukey boxplot&lt;/a&gt; values outside 1.5 times the inter-quartile from the first or third quartile would be considered as outliers and displayed as simple points. See also &lt;a href=&quot;http://j.mp/hS5RJW&quot; rel=&quot;nofollow&quot;&gt;Methods for Presenting Statistical Information: The Box Plot for a good overview&lt;/a&gt;, by Kristin Potter. The &lt;a href=&quot;http://cran.r-project.org/&quot; rel=&quot;nofollow&quot;&gt;R&lt;/a&gt; software implements a slightly different rule but the source code is available if you want to study it (see the &lt;code&gt;boxplot()&lt;/code&gt; and &lt;code&gt;boxplot.stats()&lt;/code&gt; functions). However, it is not very useful when the interest is in identifying outliers from a very skewed distribution (but see, &lt;a href=&quot;ftp://ftp.win.ua.ac.be/pub/preprints/04/AdjBox04.pdf&quot; rel=&quot;nofollow&quot;&gt;An adjusted boxplot for skewed distributions&lt;/a&gt;, by Hubert and Vandervieren, CSDA 2008 52(12)).&lt;/p&gt;&#10;&#10;&lt;p&gt;As far as online visualization is concerned, I would suggest taking a look at &lt;a href=&quot;http://vis.stanford.edu/protovis/&quot; rel=&quot;nofollow&quot;&gt;Protovis&lt;/a&gt; which is a plugin-free js toolbox for interactive web displays. The &lt;a href=&quot;http://vis.stanford.edu/protovis/ex/&quot; rel=&quot;nofollow&quot;&gt;examples&lt;/a&gt; page has very illustrations of what can be achieved with it, in very few lines.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2010-12-31T10:17:27.987" Id="5861" LastActivityDate="2010-12-31T10:29:34.243" LastEditDate="2010-12-31T10:29:34.243" LastEditorUserId="930" OwnerUserId="930" ParentId="5854" PostTypeId="2" Score="11" />
  <row Body="&lt;p&gt;If you are targeting the general population (i.e. a non statistical-savvy audience) you should focus on eye-candy rather than statistical accuracy.&lt;/p&gt;&#10;&#10;&lt;p&gt;Forget about boxplots, let alone violin plots (I personally find them very difficult to read)! If you'd ask the average street man what a quantile is, you would mostly get some wide eyed silence... &lt;/p&gt;&#10;&#10;&lt;p&gt;You should use barplots, bubble charts, maybe some pie charts (brrrr). Forget about error bars (although I would put SD in text somewhere where applicable).&lt;/p&gt;&#10;&#10;&lt;p&gt;Use colors, shapes, thick lines, 3D. You should make each chart unique and immediately easy to understand, even without having to read all the legends/axes etc.&#10;Make a smart use of maps by coloring them.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.informationisbeautiful.net/&quot; rel=&quot;nofollow&quot;&gt;Information is beautiful&lt;/a&gt; is a very good resource to get ideas. Look at this chart for instance: &lt;a href=&quot;http://www.informationisbeautiful.net/visualizations/caffeine-and-calories/&quot; rel=&quot;nofollow&quot;&gt;Caffeine and Calories&lt;/a&gt;: anyone can understand it, and it's pleasing to the eye.&lt;/p&gt;&#10;&#10;&lt;p&gt;And, of course, have a look at Edward Tufte's work.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2010-12-31T11:14:35.397" Id="5863" LastActivityDate="2010-12-31T11:14:35.397" OwnerUserId="582" ParentId="5854" PostTypeId="2" Score="2" />
  
  <row AcceptedAnswerId="5869" AnswerCount="1" Body="&lt;p&gt;With real-valued $X_1, X_2, \ldots$, define&lt;br&gt;&#10;$Max_n := \max(X_1,\ldots,X_n)$ record value or high-water mark&lt;br&gt;&#10;$NextMax_n :=$ the next greater high water, $Max_{n+m} &amp;gt; Max_n$&lt;br&gt;&#10;$Up_n := NextMax_n - Max_n$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can anyone suggest an introduction to estimating $NextMax$ and $Up$, for&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;$X$ i.i.d. from a known distribution,  &lt;/li&gt;&#10;&lt;li&gt;nonparametric, estimate-as-you-go?  &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Either a book chapter or an online stats course would do nicely.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2010-12-31T15:07:55.817" Id="5868" LastActivityDate="2011-01-01T15:44:40.887" LastEditDate="2011-01-01T15:44:40.887" LastEditorUserId="557" OwnerUserId="557" PostTypeId="1" Score="2" Tags="&lt;books&gt;&lt;extreme-value&gt;" Title="Basics of extreme values / high-water marks?" ViewCount="140" />
  
  
  
&#10; k=0: &amp;amp;-\frac{7}{64} \left(-1+4 \beta ^2\right)^3 \cr
  <row Body="&lt;p&gt;For online tutorials, there are&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://www.soton.ac.uk/~sks/utrecht/&quot;&gt;A tutorial in MCMC&lt;/a&gt;, by Sahut (2000)&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://kmh-lanl.hansonhub.com/talks/maxent00b.pdf&quot;&gt;Tutorial on Markov Chain Monte Carlo&lt;/a&gt;, by Hanson (2000)&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://civs.ucla.edu/old/MCMC/MCMC_tutorial.htm&quot;&gt;Markov Chain Monte Carlo for Computer Vision&lt;/a&gt;, by Zhu et al. (2005)&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://www.worldscibooks.com/etextbook/5904/5904_intro.pdf&quot;&gt;Introduction to Markov Chain Monte Carlo simulations and their statistical analysis&lt;/a&gt;, by Berg (2004).&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://projecteuclid.org/DPubS?service=UI&amp;amp;version=1.0&amp;amp;verb=Display&amp;amp;handle=euclid.ss/1177011137&quot;&gt;Practical Markov Chain Monte Carlo&lt;/a&gt;, by Geyer (&lt;em&gt;Stat. Science&lt;/em&gt;, 1992), is also a good starting point, and you can look at the &lt;a href=&quot;http://cran.r-project.org/web/packages/MCMCpack/index.html&quot;&gt;MCMCpack&lt;/a&gt; or &lt;a href=&quot;http://cran.r-project.org/web/packages/mcmc/&quot;&gt;mcmc&lt;/a&gt; R packages for illustrations.&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2011-01-02T11:22:43.313" CreationDate="2011-01-02T09:32:02.837" Id="5889" LastActivityDate="2011-01-02T12:08:40.193" LastEditDate="2011-01-02T12:08:40.193" LastEditorUserId="930" OwnerUserId="930" ParentId="5885" PostTypeId="2" Score="8" />
  <row Body="&lt;p&gt;I haven't read it (yet), but if you're into R, there is Christian P. Robert's and George Casella's book:&#10;&lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/1441915753&quot; rel=&quot;nofollow&quot;&gt;Introducing Monte Carlo Methods with R (Use R)&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I know of it from following his (very good) &lt;a href=&quot;http://xianblog.wordpress.com/&quot; rel=&quot;nofollow&quot;&gt;blog&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2011-01-02T11:22:43.313" CreationDate="2011-01-02T10:17:49.443" Id="5890" LastActivityDate="2015-01-16T07:23:13.243" LastEditDate="2015-01-16T07:23:13.243" LastEditorUserId="7224" OwnerUserId="253" ParentId="5885" PostTypeId="2" Score="5" />
  <row AcceptedAnswerId="5897" AnswerCount="4" Body="&lt;p&gt;I am new to statistics so I apologize for the simplicity of my question. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have a bag with 27 numbers inside; I will pull one number out until I get number 17. THe numbers will be returned to the bag after every try so the prob of success will always be 1/27.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am trying to determine on average how many pulls I will need.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-01-02T17:52:18.390" Id="5894" LastActivityDate="2011-01-03T00:10:55.017" OwnerUserId="2610" PostTypeId="1" Score="4" Tags="&lt;binomial&gt;&lt;negative-binomial&gt;" Title="Number of tried for a favorable result" ViewCount="122" />
  <row Body="&lt;p&gt;There's quite a wide range of information you can get depending on the sources you use. Census data is an obvious one. You can also get information from Facebook, MySpace and other social networking sites. You could also probably search public news archives for mentions of their name. Maybe even those ubclained property sites that some states have.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you want a real world world example of what can be done, take a look at pipl.com&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-01-02T19:41:43.203" Id="5908" LastActivityDate="2011-01-03T11:13:42.550" OwnerDisplayName="Keysmack" ParentId="5893" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="5921" AnswerCount="4" Body="&lt;p&gt;I’m writing some code &lt;em&gt;(JavaScript)&lt;/em&gt; to compare benchmark results. I’m using the &lt;a href=&quot;http://frank.mtsu.edu/~dkfuller/notes302/welcht.pdf&quot; rel=&quot;nofollow&quot;&gt;Welch T-test&lt;/a&gt; because the variance and/or sample size between benchmarks is most likely different. The critical value is pulled from a T-distribution table at 95% confidence &lt;em&gt;(two-sided)&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;The Welch formula is pretty straight-forward, but I am fuzzy on interpreting a significant result. I am not sure if the critical value should be divided by 2 or not. Help clearing that up is appreciated. Also should I be &lt;a href=&quot;https://developer.mozilla.org/en/JavaScript/Reference/Global_Objects/Math/round&quot; rel=&quot;nofollow&quot;&gt;rounding&lt;/a&gt; the degrees of freedom, &lt;code&gt;df&lt;/code&gt;, to lookup the critical value or would &lt;a href=&quot;https://developer.mozilla.org/en/JavaScript/Reference/Global_Objects/Math/ceil&quot; rel=&quot;nofollow&quot;&gt;&lt;code&gt;Math.ceil&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;https://developer.mozilla.org/en/JavaScript/Reference/Global_Objects/Math/floor&quot; rel=&quot;nofollow&quot;&gt;&lt;code&gt;Math.floor&lt;/code&gt;&lt;/a&gt; be more appropriate?&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;  /**&#10;   * Determines if the benchmark's hertz is higher than another.&#10;   * @member Benchmark&#10;   * @param {Object} other The benchmark to compare.&#10;   * @returns {Number} Returns `1` if higher, `-1` if lower, and `0` if indeterminate.&#10;   */&#10;  function compare(other) {&#10;    // use welch t-test&#10;    // http://frank.mtsu.edu/~dkfuller/notes302/welcht.pdf&#10;    // http://www.public.iastate.edu/~alicia/stat328/Regression%20inference-part2.pdf&#10;    var a = this.stats,&#10;        b = other.stats,&#10;        pow = Math.pow,&#10;        bitA = a.variance / a.size,&#10;        bitB = b.variance / b.size,&#10;        df = pow(bitA + bitB, 2) / ((pow(bitA, 2) / a.size - 1) + (pow(bitB, 2) / b.size - 1)),&#10;        t = (a.mean - b.mean) / Math.sqrt(bitA + bitB),&#10;        c = getCriticalValue(Math.round(df));&#10;&#10;    // check if t-statistic is significant&#10;    return Math.abs(t) &amp;gt; c / 2 ? (t &amp;gt; 0 ? 1 : -1) : 0;&#10;  }&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt; Thanks for all the replies so far! My colleague &lt;a href=&quot;http://stats.stackexchange.com/questions/5913/interpreting-two-sided-two-sample-welch-t-test/5961#5961&quot;&gt;posted some more info here&lt;/a&gt;, in case that affects the advice.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-01-03T13:17:49.960" FavoriteCount="3" Id="5913" LastActivityDate="2011-01-04T18:48:40.033" LastEditDate="2011-01-04T13:18:52.173" LastEditorUserId="2616" OwnerUserId="2616" PostTypeId="1" Score="7" Tags="&lt;distributions&gt;&lt;t-test&gt;&lt;javascript&gt;" Title="Interpreting two-sided, two-sample, Welch T-Test" ViewCount="1751" />
  
  <row Body="&lt;p&gt;Wooldridge &quot;Introductory Econometrics - A Modern Approach&quot; 2E p.261.&lt;/p&gt;&#10;&#10;&lt;p&gt;If Heteroskedasticity-robust standard errors are valid more often than the usual OLS standard errors, why do we bother we the usual standard errors at all?...One reason they are still used in cross sectional work is that, if the homoskedasticity assumption holds and the erros are normally distributed, then the usual t-statistics have exact t distributions, regardless of the sample size. The robust standard errors and robust t statistics are justified only as the sample size becomes large. With small sample sizes, the robust t statistics can have distributions that are not very close to the t distribution, and that could throw off our inference. In large sample sizes, we can make a case for always reporting only the Heteroskedasticity-robust standard errors in cross-sectional applications, and this practice is being followed more and more in applied work.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-01-03T23:00:01.527" Id="5939" LastActivityDate="2011-01-03T23:00:01.527" OwnerDisplayName="David Rebelo" ParentId="1164" PostTypeId="2" Score="8" />
  <row Body="&lt;p&gt;The &lt;a href=&quot;http://lib.stat.cmu.edu/DASL/&quot;&gt;data and story library&lt;/a&gt; is an &quot; online library of datafiles and stories that illustrate the use of basic statistics methods&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;This site seems to have what you need, and you can search it for particular data sets.&lt;/p&gt;&#10;" CommentCount="1" CommunityOwnedDate="2011-01-03T23:03:39.257" CreationDate="2011-01-03T23:03:39.257" Id="5940" LastActivityDate="2011-10-28T19:49:33.923" LastEditDate="2011-10-28T19:49:33.923" LastEditorUserId="1381" OwnerUserId="1381" ParentId="5937" PostTypeId="2" Score="16" />
  
  
  
  
  <row AcceptedAnswerId="5986" AnswerCount="1" Body="&lt;p&gt;Let's say we have two biased coins. The probability of tossing a head on the first coin is $\alpha$ and the probability of tossing a head on the second coin is $1-\alpha$. We toss both coins $n$ times and we say that there is a success when there is a head on both coins. If we denote this random variable by $X$ then&lt;/p&gt;&#10;&#10;&lt;p&gt;$$X\sim B(n,\alpha-\alpha^2).$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The question is how to properly estimate $\alpha$.  The problem with a 'standard' approach ($\alpha - \alpha^2=\frac{k}{n}$ where $k$ is number of successes) is that the number of successes might be greater than $0.25n$ and in such a case we obtain that $\alpha$ is a complex number.  We can treat such cases as $\alpha=0.5$, but then this estimator is not unbiased anymore.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-01-04T17:50:44.507" Id="5972" LastActivityDate="2011-01-13T23:11:34.813" LastEditDate="2011-01-13T23:11:34.813" LastEditorUserId="919" OwnerUserId="1643" PostTypeId="1" Score="6" Tags="&lt;estimation&gt;&lt;binomial&gt;&lt;unbiased-estimator&gt;" Title="Estimation of probability of a success in binomial distribution" ViewCount="640" />
  
  
  <row Body="&lt;p&gt;Consider the binary case.  If you don't know the proportions of the two classes, then the worst you can do is to &lt;strong&gt;flip a fair coin&lt;/strong&gt; in each case: the expected error rate is $1/2$.  If you do know the proportions, and the smaller of the two is $p$, say, then you should always classify objects in that category: the expected error rate is $1-p \gt 1/2$.  (However, it could be argued that this procedure is &lt;em&gt;not&lt;/em&gt; worse.  After observing it sufficiently long, you would notice its error rate is significantly greater than $1/2$ and then you would have the option of &lt;em&gt;negating&lt;/em&gt; it--that is, assigning the opposite classification--and that would have an error rate of only $p \lt 1/2$.)&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2011-01-04T21:17:03.833" Id="5990" LastActivityDate="2011-01-04T21:17:03.833" OwnerUserId="919" ParentId="5987" PostTypeId="2" Score="9" />
  <row Body="&lt;p&gt;The optimal values for the hyper-parameters will be different for different learning taks, you need to tune them separately for every problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;The reason you don't get a single optimum is becuase both the kernel parameter and the regularisation parameter control the complexity of the model.  If C is small you get a smooth model, likewise if the kernel with is broad, you will get a smooth model (as the basis functions are not very local).  This means that different combinations of C and the kernel width lead to similarly complex models, with similar performance (which is why you get the diagonal feature in many of the plots you have).&lt;/p&gt;&#10;&#10;&lt;p&gt;The optimum also depends on the particular sampling of the training set.  It is possible to over-fit the cross-validation error, so choosing the hyper-parameters by cross-validation can actually make performance worse if you are unlucky.  See &lt;a href=&quot;http://jmlr.csail.mit.edu/papers/v11/cawley10a.html&quot;&gt;Cawley and Talbot&lt;/a&gt; for some discussion of this.&lt;/p&gt;&#10;&#10;&lt;p&gt;The fact that there is a broad plateau of values for the hyper-parameters where you get similarly good values is actually a good feature of support vector machines as it suggests that they are not overly vulnerable to over-fitting in model selection.  If you had a sharp peak at the optimal values, that would be a bad thing as the peak would be difficult to find using a finite dataset which would provide an unreliable indication of where that peak actually resides.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-01-05T00:57:32.817" Id="5992" LastActivityDate="2011-01-05T00:57:32.817" OwnerUserId="887" ParentId="5962" PostTypeId="2" Score="8" />
  <row Body="&lt;p&gt;This answer may be way, way off base as I don't understand the medical context of your question and the nature of the medical test results you allude to, but it might be possible to estimate data mining bias by some sort of Monte Carlo permutation of your results. The type of approach I'm thinking of is taken from a book, Evidence Based Technical Analysis, which has a companion website &lt;a href=&quot;http://www.evidencebasedta.com/&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;Although this is written with the financial markets in mind it is, in my opinion, a book about Monte Carlo techniques as much as it is about markets. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-01-05T01:32:02.787" Id="5993" LastActivityDate="2011-01-05T01:32:02.787" OwnerUserId="226" ParentId="5797" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;You should first replace your for loop with something like &lt;code&gt;apply(d7_dataset, 1, foo)&lt;/code&gt;, where &lt;code&gt;foo()&lt;/code&gt; is either your function or something along those lines, e.g. &lt;code&gt;gregexpr()&lt;/code&gt;. The result of &lt;code&gt;gregexpr()&lt;/code&gt; is a list of numeric vectors with attributes similar to &lt;code&gt;regexpr()&lt;/code&gt; but giving all matches in each element.&lt;/p&gt;&#10;&#10;&lt;p&gt;On a related point, there was another function that was proposed as a more user-friendly alternative to &lt;code&gt;gregexpr()&lt;/code&gt;: &lt;a href=&quot;http://www.r-bloggers.com/developing-a-user-friendly-regular-expression-function-easygregexpr/&quot; rel=&quot;nofollow&quot;&gt;easyGregexpr&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;The following example gives you the number of matches for each row when considering a list of three motifs (in a 10x100 matrix):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;dd &amp;lt;- replicate(100, replicate(10, paste(sample(letters[1:4], 2),    &#10;                                         collapse=&quot;&quot;)))&#10;pat &amp;lt;- list(&quot;aa&quot;, &quot;ab&quot;, &quot;cd&quot;)&#10;foo &amp;lt;- function(d, p) apply(d, 1, function(x) length(grep(p, x)))&#10;lapply(pat, function(x) foo(dd, x))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2011-01-05T13:53:05.480" Id="6007" LastActivityDate="2011-01-05T16:59:03.550" LastEditDate="2011-01-05T16:59:03.550" LastEditorUserId="930" OwnerUserId="930" ParentId="6005" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;From someone who has used PCA a lot (and tried to explain it to a few people as well) here's an example from my own field of neuroscience.&lt;/p&gt;&#10;&#10;&lt;p&gt;When we're recording from a person's scalp we do it with 64 electrodes. So, in effect we have 64 numbers in a list that represent the voltage given off by the scalp. Now since we record with microsecond precision, if we have a 1-hour experiment (often they are 4 hours) then that gives us 10e6 * 60^3 == 216,000,000,000 time points at which a voltage was recorded at each electrode so that now we have a 216,000,000,000 x 64 matrix. Since a major assumption of PCA is that your variables are correlated, it is a great technique to reduce this ridiculous amount of data to an amount that is tractable. As has been said numerous times already, the eigenvalues represent the amount of variance explained by the variables (columns). In this case an eigenvalue represents the variance in the voltage at a particular point in time contributed by a particular electrode. So now we can say, &quot;Oh, well electrode &lt;code&gt;x&lt;/code&gt; at time point &lt;code&gt;y&lt;/code&gt; is what we should focus on for further analysis because that is where the most change is happening&quot;. Hope this helps. Loving those regression plots!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-01-05T21:11:38.220" Id="6017" LastActivityDate="2011-01-05T21:11:38.220" OwnerUserId="2660" ParentId="2691" PostTypeId="2" Score="5" />
  
  
  
  
  <row AcceptedAnswerId="6062" AnswerCount="2" Body="&lt;p&gt;As the title says, I'd like to calculate the percentage difference for two sets of points. For example, suppose I have $S_{1}=\{(1,x_{1}),(2,x_{2}),(3,x_{3})\}$ and $S_{2}=\{(1,y_{1}),(2,y_{2}),(3,y_{3})\}$. How can I know the difference in percentage between both sets of data. What is the correct way to do that? Is that kind of assessment meaningful to establish to which degree of precision a set of data is preferred over the other?&lt;/p&gt;&#10;&#10;&lt;p&gt;In my particular case, $S_{1}$ is simply a set of numerical results obtained by &lt;a href=&quot;http://en.wikipedia.org/wiki/Direct_simulation_Monte_Carlo&quot; rel=&quot;nofollow&quot;&gt;DSMC&lt;/a&gt; and $S_{2}$ was obtained by a theoretical result. I'd like to quantify how much difference exist between each other in order to establish when it is convenient to use one or the other.&lt;/p&gt;&#10;&#10;&lt;p&gt;By &quot;difference in percentage&quot; I mean &lt;a href=&quot;http://en.wikipedia.org/wiki/Percent_difference&quot; rel=&quot;nofollow&quot;&gt;percent difference&lt;/a&gt;. Hopefully that clarifies a bit the question.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;UPDATE:&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Another way to formulate my question would be: How can I arrive to conclusions such as &quot;The results from experiment A are inaccurate by 10% with respect to experiment B&quot;, when experiment A and B are a set of values.&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2011-01-06T19:35:49.930" Id="6044" LastActivityDate="2011-01-07T13:45:52.677" LastEditDate="2011-01-07T01:03:12.160" LastEditorUserId="2676" OwnerUserId="2676" PostTypeId="1" Score="0" Tags="&lt;quantiles&gt;" Title="Calculate percentage difference for two sets of points" ViewCount="1278" />
  <row Body="&lt;p&gt;See &lt;/p&gt;&#10;&#10;&lt;p&gt;Levina, E. and Bickel, P. (2004) “Maximum Likelihood Estimation of Intrinsic Dimension.” Advances in Neural Information Processing Systems 17&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://books.nips.cc/papers/files/nips17/NIPS2004_0094.pdf&quot; rel=&quot;nofollow&quot;&gt;http://books.nips.cc/papers/files/nips17/NIPS2004_0094.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Their idea is that if the data are sampled from a smooth density in $R^m$ embedded in $R^p$ with $m &amp;lt; p$, then locally the number of data points in a small ball of radius $t$ behaves roughly like a poisson process.  The rate of the process is related to the volume of the ball which in turn is related to the intrinsic dimension.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-01-06T19:40:32.547" Id="6045" LastActivityDate="2011-01-08T20:17:36.033" LastEditDate="2011-01-08T20:17:36.033" LastEditorUserId="1670" OwnerUserId="1670" ParentId="5922" PostTypeId="2" Score="7" />
  
&#10;}
&#10;\hat{\beta_{34}} &amp;amp;= 0.0055\ (t = 5.58,\ p \lt 0.001), \cr
&#10;  &amp;amp;= 0.103 X_1 + 0.206 X_2 + 0.0629 X_3 - 0.0839 X_4 \cr
&#10;}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;(The last line shows how this all relates to form of the original question.)  That's exactly the form used in the simulation: $Z_1$ and $Z_2$ enter with the same coefficient and $Z_3$ and $Z_4$ enter with opposite coefficients.  This method got the right answer.&lt;/p&gt;&#10;&#10;&lt;p&gt;I want to &lt;strong&gt;share a cool observation&lt;/strong&gt; in this regard.  First, here's the scatterplot matrix for the model.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/kR5Dd.png&quot; alt=&quot;alt text&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Notice how $X_{12}$ and $X_{34}$ look uncorrelated.  Furthermore, $Y$ is only weakly correlated with these variables.  Doesn't look like much of a relationship, does it?  Now consider an alternative set of pairs, $X_{13}$ and $X_{24}$.  The regression of $Y$ on these is still highly significant ($F(2, 57) = 16.61\ (p \lt 0.0001).$  Moreover, the coefficient of $X_{24}$ is significant ($t = 2.39,\ p = 0.020$) even though that of $X_{13}$ is not ($t = 0.24,\ p = 0.812$).  But look at the scatterplot matrix!&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/11E4s.png&quot; alt=&quot;alt text&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Clearly $X_{13}$ and $X_{24}$ are strongly correlated.  But, &lt;em&gt;even though this is the wrong model,&lt;/em&gt; $Y$ is also visibly correlated with these two variables, much more so than in the preceding scatterplot matrix!&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;The lesson here&lt;/strong&gt; is that mere bivariate plots can be deceiving in a multiple regression setting: to analyze the relationship between any candidate independent variable (such as $X_{12}$) and the independent variable ($Y$), we must make sure to &quot;factor out&quot; all other independent variables.  (This is done by regressing $Y$ on all other independent variables and, separately, regressing $X_{12}$ on all the others.  Then one looks at a scatterplot of the &lt;em&gt;residuals&lt;/em&gt; of the first regression against the &lt;em&gt;residuals&lt;/em&gt; of the second regression.  It's a theorem that the slope in this &lt;em&gt;bivariate&lt;/em&gt; regression equals the coefficient of $X_{12}$ in the full &lt;em&gt;multivariate&lt;/em&gt; regression of $Y$ against all the variables.)&lt;/p&gt;&#10;&#10;&lt;p&gt;This insight shows why we might want to systematically perform the &quot;identification regressions&quot; I have proposed, rather than using graphical methods or attempting to combine many of the pairs in one model.  Each identification regression assesses the strength of the contribution of a proposed linear combination of variables (a &quot;pair&quot;) &lt;em&gt;in the context of all the remaining independent variables.&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that although correlated variables were involved, &lt;em&gt;correlation is not an essential feature of the problem or of the solution.&lt;/em&gt;  Even where you don't expect the original variables $X_i$ to be strongly correlated, you could expect a model to have (unknown) linear constraints among the variables.  &lt;strong&gt;That&lt;/strong&gt; is the important issue to cope with.  The presence of correlation only means that it can be problematic to identify such pairs solely by inspecting the original regression results.&lt;/p&gt;&#10;&#10;&lt;p&gt;Following the procedure I have proposed does not guarantee you will find a unique solution.  It's conceivable, for instance, that you will find so many highly significant pairs that they are linearly dependent, forcing you to select among them by some other criterion. Nevertheless, the results you get ought to limit the sets of pairs you need to examine; they can be obtained with a straightforward procedure without intervention; and--if this simulation is any guide--they have a good chance of producing effective results.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-01-07T05:07:13.817" Id="6059" LastActivityDate="2011-01-07T05:26:54.567" LastEditDate="2011-01-07T05:26:54.567" LastEditorUserId="919" OwnerUserId="919" ParentId="6024" PostTypeId="2" Score="5" />
  
  
&#10;.
  
  
  
  <row Body="&lt;p&gt;This question appears to confuse two distinct things.  Any additional parameter in the model would (by definition) describe the distribution of $i$, &lt;strong&gt;not&lt;/strong&gt; the distributions of any of the $w_i$.  Unless you adopt a Bayesian prior for $\vec{w}$ (which does not seem to be part of this question), the parameters do not have any distribution at all: they are what they are.  When you use a particular &lt;em&gt;procedure&lt;/em&gt; to &lt;em&gt;estimate&lt;/em&gt; $\vec{w}$, however, then the &lt;em&gt;estimates&lt;/em&gt; $(\hat{w_i})$ do have a distribution.  It makes sense to talk about the variance of that distribution.  It can be estimated in standard ways, such as the &lt;a href=&quot;http://en.wikipedia.org/wiki/Maximum_likelihood&quot; rel=&quot;nofollow&quot;&gt;inverse of the expectation of the Hessian of the log likelihood&lt;/a&gt;.  It is unnecessary--and meaningless--to introduce yet another parameter to capture that information.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-01-10T02:14:16.637" Id="6117" LastActivityDate="2011-01-10T02:14:16.637" OwnerUserId="919" ParentId="6109" PostTypeId="2" Score="6" />
  <row AcceptedAnswerId="6125" AnswerCount="1" Body="&lt;p&gt;I'm analyzing people based on their twitter stream. We are using a 'word bag' model of users, which basically amounts to counting how often each word appears in a persons twitter stream (and then using that as a proxy for a more normalized 'probability they will use a given word' in a particular length of text). &lt;/p&gt;&#10;&#10;&lt;p&gt;Due to constraints further down the pipeline, we cannot retain full data on usage of all words for all users, so we are trying to find the most 'symbolically efficient' words to retain in our analysis. That is, we're trying to retain a subset of dimensions, which, knowing their values would allow a hypothetical seer to most accurately model the probabilities of all words (including any we left out of the analysis).&lt;/p&gt;&#10;&#10;&lt;p&gt;So a principal components analysis (PCA) type approach seems an appropriate first step. (happily ignoring for now the fact that PCA would also 'rotate' us into dimensions that don't correspond to any particular word).&lt;/p&gt;&#10;&#10;&lt;p&gt;But I am reading that &lt;a href=&quot;http://www.useit.com/alertbox/zipf.html&quot; rel=&quot;nofollow&quot;&gt;&quot;Zipf distributions .. characterize the use of words in a natural language (like English) &quot;&lt;/a&gt; and as far as I know, PCA analysis makes various assumptions about the data being normally distributed. So, I'm wondering whether the fundamental assumptions of the PCA analysis will be sufficiently far 'off' from reality to be a ral problem. That is, does PCA rely on the data being 'close to' Gaussian Normal for it to work at all well?&lt;/p&gt;&#10;&#10;&lt;p&gt;If this is a problem as I suspect, are there any other recommendations? That is, some other approach worth investigating that is 'equivalent' to PCA in some way but more appropriate for Zipf or power law distributed data?&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that I am a programmer, not a statistician, so apologies if I messed up my terminology in the above. (Corrections of course welcomed!)&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-01-10T05:50:59.077" FavoriteCount="1" Id="6119" LastActivityDate="2011-03-15T19:05:03.893" LastEditDate="2011-03-15T19:05:03.893" LastEditorUserId="8" OwnerUserId="2717" PostTypeId="1" Score="6" Tags="&lt;pca&gt;&lt;normal-distribution&gt;&lt;zipf&gt;" Title="Is principal components analysis valid if the distribution(s) are Zipf like? What would be similar to PCA but suited to non gaussian data?" ViewCount="1290" />
  <row Body="&lt;p&gt;We have $X_1\sim N(\mu_1,\sigma^2)$ and $X_2\sim N(\mu_2,\sigma^2)$, hence &lt;/p&gt;&#10;&#10;&lt;p&gt;$$EY_1=E(-X_1/\sqrt{2}+X_2/\sqrt{2})=-1/\sqrt{2}EX_1+1/\sqrt{2}EX_2=0$$&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{align*}&#10;EY_1^2&amp;amp;=E(-X_1/\sqrt{2}+X_2/\sqrt{2})^2\\\\&#10;&amp;amp;=E(X_1/\sqrt{2})^2-2E(X_1X_2/2)+E(X_2/\sqrt{2})^2\\\\&#10;&amp;amp;=1/2\sigma^2+1/2\sigma^2=\sigma^2&#10;\end{align*}&lt;/p&gt;&#10;&#10;&lt;p&gt;Hence $Y_1\sim N(0,\sigma^2)$ since it is the linear combination of normal variables. &lt;/p&gt;&#10;&#10;&lt;p&gt;Similarly we get $Y_2\sim N(0,\sigma^2)$  and $Y_3\sim N(0,\sigma^2)$&lt;/p&gt;&#10;&#10;&lt;p&gt;Now&lt;/p&gt;&#10;&#10;&lt;p&gt;$$EY_1Y_2=1/\sqrt{6}E(X_1)^2-1/\sqrt{6}EX_2^2=0$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and similarly $EY_2Y_3=EY_1Y_3=0$, hence $Y_1$, $Y_2$ and $Y_3$ are independent, since for normal variables independece coincided with zero correlation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Having established that we have &lt;/p&gt;&#10;&#10;&lt;p&gt;$$(Y_1^2+Y_2^2+Y_3^2)/\sigma^2=\left(\frac{Y_1}{\sigma}\right)^2+\left(\frac{Y_2}{\sigma}\right)^2+\left(\frac{Y_3}{\sigma}\right)^2=Z_1^2+Z_2^2+Z_3^2$$,&lt;/p&gt;&#10;&#10;&lt;p&gt;where $Z_i=Y_i/\sigma$. Since $Y_i\sim N(0,\sigma^2)$, we have $Z_i\sim N(0,1)$. &lt;/p&gt;&#10;&#10;&lt;p&gt;We have showed that our quantity of interest is a sum of squares of 3 independent standard normal variables, which by definition is $\chi^2$ with 3 degrees of freedom. &lt;/p&gt;&#10;&#10;&lt;p&gt;As I've said in the comments you do not need to calculate the densities. If you on the other hand want to do that, your formula is wrong. Here is why. Denote by $G(x)$ distribution of $Y_1^2$ and $F(x)$ the distribution of $Y_1$. Then we have&lt;/p&gt;&#10;&#10;&lt;p&gt;$$G(x)=P(Y_1^2&amp;lt;x)=P(-\sqrt{x}&amp;lt;Y_1&amp;lt;\sqrt{x})=F(\sqrt{x})-F(-\sqrt{x})$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Now the density of $Y_1^2$ is $G'(x)$, so&lt;/p&gt;&#10;&#10;&lt;p&gt;$$G'(x)=\frac{1}{2\sqrt{x}}(F'(\sqrt{x})+F'(-\sqrt{x})$$ &lt;/p&gt;&#10;&#10;&lt;p&gt;We have that&lt;/p&gt;&#10;&#10;&lt;p&gt;$$F'(x)=\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{x^2}{\sigma^2}},$$&lt;/p&gt;&#10;&#10;&lt;p&gt;so&lt;/p&gt;&#10;&#10;&lt;p&gt;$$G'(x)=\frac{1}{\sigma\sqrt{2\pi x}}e^{-\frac{x}{2}}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;If $\sigma^2=1$ we have a pdf of $\chi^2$ with one degree of freedom. (Note that for $Z_1$ instead of $Y_1$ the calculation is similar and $\sigma^2=1$ ) As @whuber pointed out, this is &lt;a href=&quot;http://en.wikipedia.org/wiki/Gamma_distribution&quot; rel=&quot;nofollow&quot;&gt;gamma&lt;/a&gt; distribution, and sums of independent gamma distributions is again gamma, the exact formula is provided in the wikipedia page.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-01-10T08:10:03.080" Id="6121" LastActivityDate="2014-04-17T19:27:03.137" LastEditDate="2014-04-17T19:27:03.137" LastEditorUserId="37240" OwnerUserId="2116" ParentId="6104" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="6124" AnswerCount="3" Body="&lt;p&gt;I have almost two questions. I need a single covariate logistic regression (LR) for each of my variables. Should I do it manually in SPSS, selecting each variable and do logistic regression? Is there a &quot;for each&quot; cycle to do it? I should switch to R language to have what I want. &lt;/p&gt;&#10;&#10;&lt;p&gt;In the multivariables (multi covariates) LR, could I have missing values?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-01-10T09:54:02.767" Id="6122" LastActivityDate="2011-01-10T15:04:25.203" LastEditDate="2011-01-10T10:37:10.423" LastEditorUserId="930" OwnerUserId="2719" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;logistic&gt;&lt;spss&gt;" Title="Simple and multiple logistic regression" ViewCount="951" />
  <row Body="&lt;p&gt;This is a (long!) comment on the nice work @vqv has posted in this thread.  It aims to obtain a definitive answer.  He has done the hard work of simplifying the dictionary.  All that remains is to exploit it to the fullest.  His results suggest that &lt;strong&gt;a brute-force solution is feasible&lt;/strong&gt;.  After all, including a wildcard, there are at most $27^7 = 10,460,353,203$ words one can make with 7 characters, and it looks like less than 1/10000 of them--say, around a million--will fail to include some valid word.  &lt;/p&gt;&#10;&#10;&lt;p&gt;The first step is to augment the minimal dictionary with a wildcard character, &quot;?&quot;.  22 of the letters appear in two-letter words (all but c, q, v, z).  Adjoin a wildcard to those 22 letters and add these to the dictionary: {a?, b?, d?, ..., y?} are now in.  Similarly we can inspect the minimal three-letter words, causing some additional words to appear in the dictionary.  Finally, we add &quot;??&quot; to the dictionary.  After removing repetitions that result, it contains 342 minimal words.&lt;/p&gt;&#10;&#10;&lt;p&gt;An elegant way to proceed--one that uses a very small amount of encoding indeed--is to &lt;strong&gt;view this problem as an algebraic one&lt;/strong&gt;.  A word, considered as an unordered set of letters, is just a monomial.  For example, &quot;spats&quot; is the monomial $a p s^2 t$.  The dictionary therefore is a collection of monomials.  It looks like&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\{a^2, a b, a d, ..., o z \psi, w x \psi, \psi^2\}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;(where, to avoid confusion, I have written $\psi$ for the wildcard character).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;A rack contains a valid word if and only if that word divides the rack.&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;A more abstract, but extremely powerful, way to say this is that the dictionary generates an ideal $I$ in the polynomial ring $R = \mathbb{Z}[a, b, \ldots, z, \psi]$ and that the racks with valid words become zero in the quotient ring $R/I$, whereas racks without valid words remain nonzero in the quotient.  If we form the sum of all racks in $R$ and compute it in this quotient ring, then &lt;em&gt;the number of racks without words equals the number of distinct monomials in the quotient.&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Furthermore, the sum of all racks in $R$ is straightforward to express.  Let $\alpha = a + b + \cdots + z + \psi$ be the sum of all letters in the alphabet.  $\alpha^7$ contains one monomial for each rack.  (As an added bonus, its coefficients count the number of ways each rack can be formed, allowing us to compute its probability if we like.)&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;As a simple example&lt;/strong&gt; (to see how this works), suppose (a) we don't use wildcards and (b) all letters from &quot;a&quot; through &quot;x&quot; are considered words.  Then the only possible racks from which words cannot be formed must consist entirely of y's and z's.  We compute $\alpha=(a+b+c+\cdots+x+y+z)^7$ modulo the ideal generated by $\{a,b,c, \ldots, x\}$ one step at a time, thus:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\eqalign{
&#10;\alpha^0 &amp;amp;= 1 \cr
  
  <row AnswerCount="1" Body="&lt;p&gt;I'm studying Biomedical Computer science and I have to research a paper about genotype-phenotype association.&lt;/p&gt;&#10;&#10;&lt;p&gt;In this paper the authors use a correlation analysis by first calculating the Pearson correlation and then calculating the hypergeometric distribution to filter out insignificant associations.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.biomedcentral.com/1471-2164/7/257&quot; rel=&quot;nofollow&quot;&gt;http://www.biomedcentral.com/1471-2164/7/257&lt;/a&gt;&lt;br&gt;&#10;Under Methods/Associating genes to phenotypes&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;While the correlation measures the strength of association between an organism's genomic  content and  its phenotype,  we  also  applied  another  method,  exploiting  the hypergeometric  distribution  function,  to  determine  the significance of these&lt;br&gt;&#10;  associations [...] where a result smaller or equal to 20% response is considered negative. So for a given gene found in M  species,  the  hypergeometric  function  provides  the probability by random chance that the gene is found in m species which contain  the COG and are also positive  in the laboratory test.&lt;br&gt;&#10;  The following criteria were applied to the correlated data set. The intersection between a specific COG and a phenotype had to contain at least 3 organisms, and for any intersection, 30% of the microbes had to share the COG. The scores were adjusted using the standard Bonferroni error correction for multiple testing.&lt;br&gt;&#10;  Since the Bonferroni correction  is  one  of  the most  conservative,  it  is  likely  that some biologically relevant associations were unnecessarily discarded. In this case $\alpha$ was set as less than equal to 0.01, therefore, any hypergeometric distribution score less than  or  equal  to  0.0001 was  deemed  significant. Using these criteria, we set a 0.8 and a 0.9 correlation threshold to assess the significance of the COG-phenotype associations.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;My question is: Is this a valid scientific correlation analysis or not? Are there any reservations?&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, can you give me an idea for a good statistics book for science?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-01-10T09:54:12.360" Id="6139" LastActivityDate="2012-06-01T20:17:24.320" OwnerDisplayName="kiepmad" PostTypeId="1" Score="7" Tags="&lt;correlation&gt;" Title="Is a correlation analysis with Pearson's correlation and Bonferroni Method a valid approach to find correlations between two sets of data" ViewCount="1046" />
  <row AnswerCount="2" Body="&lt;p&gt;I want to test if there is a rivalry among two siblings in a family.  I have 15 questions in my study and I let my 100 respondents ( distributed equally to two siblings)   ranked them 1 to 15. &lt;/p&gt;&#10;&#10;&lt;p&gt;How should I analyse this data?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-01-11T01:09:34.687" FavoriteCount="1" Id="6151" LastActivityDate="2011-01-13T22:33:59.487" LastEditDate="2011-01-13T20:58:11.047" LastEditorUserId="8" OwnerDisplayName="noli macnoli" PostTypeId="1" Score="0" Tags="&lt;hypothesis-testing&gt;&lt;correlation&gt;&lt;multiple-comparisons&gt;&lt;statistical-significance&gt;&lt;survey&gt;" Title="Analysing questionnaire data" ViewCount="1365" />
  <row AcceptedAnswerId="6175" AnswerCount="2" Body="&lt;p&gt;I have a (I suspect) simple question. I have time series cross section data on voting behaviour in the Council of the European Union (the monthly number of yes, no and abstentions for each member state from 1999 to 2007). So basically the variables are counts, thus a Poisson/negative binomial regression would be appropriate, possibly with lagged dependent variables on the right hand side to control for time dependencies. I have seen papers with people using such negative binomial models to forecast, for instance the number of monthly legislative acts adopted in the future, and I have three questions in this regard:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;How can i run a negative binomial regression on panel data without making any inferential mistakes?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;How can I use a negative binomial model with lags to forecast future values of the dependent variable.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Can this be done in R?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Thomas&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-01-11T01:45:42.400" FavoriteCount="5" Id="6152" LastActivityDate="2013-05-10T14:09:44.877" LastEditDate="2013-05-10T14:09:44.877" LastEditorUserId="88" OwnerUserId="2704" PostTypeId="1" Score="4" Tags="&lt;r&gt;&lt;time-series&gt;&lt;forecasting&gt;&lt;panel-data&gt;&lt;negative-binomial&gt;" Title="Time series cross section forecasting with R" ViewCount="1649" />
  <row Body="&lt;p&gt;&lt;a href=&quot;http://igraph.sourceforge.net/index.html&quot;&gt;iGraph&lt;/a&gt; is a very interesting cross-language (R, Python, Ruby, C) library.&#10;It allows you to work with unidirected and directed graphs and has quite a few analysis algorithms already implemented.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-01-11T07:47:02.697" Id="6156" LastActivityDate="2011-01-11T07:47:02.697" OwnerUserId="582" ParentId="6155" PostTypeId="2" Score="7" />
  
  
  
  
  <row AcceptedAnswerId="6166" AnswerCount="1" Body="&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Deming_regression&quot; rel=&quot;nofollow&quot;&gt;Deming Regression&lt;/a&gt; is a regression technique taking into account uncertainty in both the explanatory and dependent variable.&lt;/p&gt;&#10;&#10;&lt;p&gt;Although I have found some interesting references on the calculation of this property in &lt;a href=&quot;http://iopscience.iop.org/0957-0233/18/11/025&quot; rel=&quot;nofollow&quot;&gt;matlab&lt;/a&gt; and in &lt;a href=&quot;http://www.mail-archive.com/r-help@r-project.org/msg85070.html&quot; rel=&quot;nofollow&quot;&gt;R&lt;/a&gt; I'm stuck when I try to calculate the standard prediction error. The error on the model estimate is given in both methods, but I wonder if I can use that for prediction by using the variances of their prediction.&#10;Eg: &lt;code&gt;var(y_pred) = var(a*x+b) = E[a]^2*var(x) + E[x]^2*a+var(b)&lt;/code&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-01-11T13:53:36.093" FavoriteCount="1" Id="6163" LastActivityDate="2012-11-15T20:10:47.490" LastEditDate="2012-11-09T15:35:58.780" LastEditorUserId="8402" OwnerUserId="2732" PostTypeId="1" Score="3" Tags="&lt;regression&gt;&lt;variance&gt;&lt;deming-regression&gt;" Title="What is the prediction error while using deming regression (weighted total least squares)" ViewCount="1433" />
  <row AcceptedAnswerId="6173" AnswerCount="2" Body="&lt;p&gt;What are the appropriate uses for a grouped vs a stacked bar plot?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-01-11T19:10:46.507" FavoriteCount="4" Id="6169" LastActivityDate="2011-01-13T11:56:13.593" LastEditDate="2011-01-12T00:58:18.377" LastEditorUserId="559" OwnerUserId="559" PostTypeId="1" Score="7" Tags="&lt;data-visualization&gt;&lt;barplot&gt;" Title="Group vs Stacked Bar Plots" ViewCount="667" />
  
  <row Body="&lt;p&gt;I think grouped bars are preferable to stacked bars in most situations because they retain information about the sizes of the groups and stay readable even when you have multiple nominal categories.  For me, the segments of stacked bars get difficult to compare beyond two categories - and even with just two categories, they can be quite deceptive if your groups are of very different sizes.  I'd prefer a frequency table over a stacked bar plot any day.&lt;/p&gt;&#10;&#10;&lt;p&gt;You should also consider a stacked &lt;em&gt;series&lt;/em&gt; of bar plots, with each group in a separate plot and each plot stacked on top of the last (there's an example on &lt;a href=&quot;http://www.robertluttman.com/vms/Week5/page9.htm&quot; rel=&quot;nofollow&quot;&gt;this page&lt;/a&gt;, referred to as &quot;layered histograms&quot;, which I find confusing).  This is probably what I use most often.  The &lt;code&gt;lattice&lt;/code&gt; and &lt;code&gt;ggplot2&lt;/code&gt; packages in R both support this kind of plotting.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Historical note: &lt;a href=&quot;http://www.worsleyschool.net/science/files/bargraphs/page.html&quot; rel=&quot;nofollow&quot;&gt;histograms != bar plots&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-01-11T20:30:15.043" Id="6173" LastActivityDate="2011-01-12T18:50:28.300" LastEditDate="2011-01-12T18:50:28.300" LastEditorUserId="71" OwnerUserId="71" ParentId="6169" PostTypeId="2" Score="10" />
  <row Body="&lt;p&gt;You are looking for a class of models known as 'purchase incidence'. &lt;/p&gt;&#10;&#10;&lt;p&gt;A poisson distribution with the rate of sales $\lambda$ such that $Y_{t}=\lambda t$ is the number of units sold during time period $t$ is a good place to start.&lt;/p&gt;&#10;&#10;&lt;p&gt;$$P(Y_{t}=y_{t})=\frac{e^{-\lambda t}(\lambda t)^{y_{t}}}{y_{t}!}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;So, if you have 3 items left and 10 before restocking, the chance that you will have an item for each customer who wants to purchase one is $$1-P(Y_t=[0,1,2,3])$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Additional parameters can be added as needed to model known processes that cause the distribution of sales to vary with time. For example, heterogeneity in $\lambda$ can be modeled as $\lambda\sim\text{Gamma}(a,b)$. You can find a description of this and related models in chapter 12 of Leeflang 2000 &quot;Building models for marketing decisions&quot;&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-01-11T20:37:46.450" Id="6174" LastActivityDate="2011-01-11T20:37:46.450" OwnerUserId="1381" ParentId="6170" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;@whuber makes an important point about assumptions,&lt;/p&gt;&#10;&#10;&lt;p&gt;It is common to assume that failure occurs either because a part is defective or because it has worn out. This means that there are two processes causing failure, and defective units fail shortly after deployment whereas wearing out happens over time. It is difficult (or impossible) to justify parameterizing both of these processes if $n=4$.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Case 1&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;For a simple case, assume that defective units can be eliminated (e.g. by an initial screening or 'burn in'), and all units fail due to a constant wearing process. This assumption may be valid since the data sheet says that the minimum expected lifetime is $10,000$ events. One might model this process as an exponential distribution:&lt;/p&gt;&#10;&#10;&lt;p&gt;Example: assume that your four test units fail after $[5,10,20,40]x10^{5}$ reprogrammings. You can find the maximum likelihood estimate for the exponential distribution in R:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(MASS)&#10;fitdistr(c(5, 10, 20, 40)*1e+5, 'exponential')&#10;lambda &amp;lt;- fitdistr(c(5, 10, 20, 40)*1e+5, 'exponential')$estimate&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;To find the time during which only 1/1000 units will fail, estimate the 0.1th quantile of this distribution.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;qexp(0.001, lambda)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In this example, the result is an expected lifetime of 1875.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;edit&lt;/em&gt; As whuber points out, $n=4$ is still to small for this problem, because it is very difficult to estimate small probabilities, and distribution tails are very sensitive to assumptions about the probability distributions used to model the data, and to the data itself. &lt;/p&gt;&#10;&#10;&lt;p&gt;Since you have good prior knowledge from the product datasheet, you could incorporate this information into your analysis as a prior on lambda. The gamma distribution is a conjugate prior, $\lambda\sim \text{Gamma}(\alpha, \beta)$, but I can not find a parameterization appropriate to the prior information, with $\text{median}\simeq 100,000$, $P(Y&amp;lt;10,000)\simeq 0)$ and nonzero density above $10,000$. Once you determine the model to use, it would be worth asking a separate question.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Case 2&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;If you assume that failure is caused either by defect or by wear, you could consider the Weibull and Generalized Exponential distributions (Gupta and Kunda 1999, 2001, 2007). The field of survival analysis provides many options, but as whuber points out, $n=4$ is insufficient to justify a model other than, perhaps, the single parameter exponential.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://home.iitk.ac.in/~kundu/paper47.pdf&quot; rel=&quot;nofollow&quot;&gt;Gupta, R. D. and Kundu, D. (1999). Generalized exponential distributions&quot;, Australian and New Zealand Journal of Statistics, vol. 41, 173 - 188.&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://home.iitk.ac.in/~kundu/paper62.pdf&quot; rel=&quot;nofollow&quot;&gt;Gupta, R. D. and Kundu, D. (2001), Generalized exponential distributions: different methods of estimation&quot;, Journal of Statistical Computation and Simulation. vol. 69, 315-338&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://home.iitk.ac.in/~kundu/paper113.pdf&quot; rel=&quot;nofollow&quot;&gt;Gupta, R. D. and Kundu, D. 2007. Generalized Exponential Distribution: existing results and some recent developments. Journal of Statistical Planning and Inference&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="11" CreationDate="2011-01-12T05:18:54.760" Id="6185" LastActivityDate="2011-01-12T16:33:25.283" LastEditDate="2011-01-12T16:33:25.283" LastEditorUserId="1381" OwnerUserId="1381" ParentId="6182" PostTypeId="2" Score="5" />
  
  <row Body="&lt;p&gt;&quot;Practical&quot; and &quot;simple&quot; suggest &lt;strong&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Least_squares&quot;&gt;least squares regression&lt;/a&gt;.&lt;/strong&gt;  It's easy to set up, easy to do with lots of software (R, Excel, Mathematica, &lt;em&gt;any&lt;/em&gt; statistics package), easy to interpret, and can be extended in many ways depending on how accurate you want to be and how hard you're willing to work.&lt;/p&gt;&#10;&#10;&lt;p&gt;This approach is essentially your &quot;weighting scheme&quot; (2), but it finds the weights easily, guarantees as much accuracy as possible, and is easy and fast to update.  There are &lt;em&gt;loads&lt;/em&gt; of libraries to perform least squares calculations.&lt;/p&gt;&#10;&#10;&lt;p&gt;It will help to include not only the variables you listed--engine type, power, etc--but also &lt;em&gt;age&lt;/em&gt; of car.  Furthermore, make sure to adjust prices for inflation.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-01-12T15:58:50.810" Id="6198" LastActivityDate="2011-01-12T15:58:50.810" OwnerUserId="919" ParentId="6195" PostTypeId="2" Score="10" />
  
  
  <row AcceptedAnswerId="6216" AnswerCount="1" Body="&lt;p&gt;How should I define a model formula in R, when one (or more) exact linear restrictions binding the coefficients is available. As an example, say that you know that b1 = 2*b0 in a simple linear regression model. &lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-01-13T12:07:18.603" FavoriteCount="4" Id="6214" LastActivityDate="2013-05-23T22:28:19.683" LastEditDate="2013-05-23T22:28:19.683" LastEditorUserId="5829" OwnerUserId="339" PostTypeId="1" Score="11" Tags="&lt;r&gt;&lt;regression&gt;&lt;modeling&gt;" Title="Fitting models in R where coefficients are subject to linear restriction(s)" ViewCount="2796" />
  
  
  <row Body="&lt;p&gt;You could look after the par() function's mar(gin) paramater. A nice brief can be found in &lt;a href=&quot;http://research.stowers-institute.org/efg/R/Graphics/Basics/mar-oma/index.htm&quot; rel=&quot;nofollow&quot;&gt;efg's Research Notes&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-01-14T13:53:55.810" Id="6249" LastActivityDate="2011-01-14T13:53:55.810" OwnerUserId="2714" ParentId="6247" PostTypeId="2" Score="2" />
&#10;$$&#10;This is for a particularly $g$. We need to repeat this for all $g$. Using the R&#10;code at the end, we get the following plot:&#10;&lt;img src=&quot;http://i.stack.imgur.com/0fwic.png&quot; alt=&quot;alt text&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Since all marks are equally likely you get a plateau with edge effects. If you  really have &lt;em&gt;absolutely&lt;/em&gt; no idea of what mark you will get, then a sensible strategy would be to maximise the chance of passing the exam. If the pass mark is 40%, then set your guess mark at 35%. This now means that to pass the exam, you only need to get above 35% but more importantly your strategy for sitting the exam is to answer every question to the best of your ability. &lt;/p&gt;&#10;&#10;&lt;p&gt;If your guess mark was 30%, and towards the end of the exam you thought that you would score 42%, you are now in the strange position of deciding whether to intentionally make an error (as 42% results in a return mark of 37%).&lt;/p&gt;&#10;&#10;&lt;p&gt;Note: I think in most real life situations you would have some idea of how you would get on. For example, do you really think that you have equal probability of getting between 0-10%, 11-20%, ..., 90-100% in your exam.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;R code&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;f =function(s) {&#10;  mark = 0&#10;  for(i in 0:100){&#10;    if(i &amp;lt; (s-10) | i &amp;gt; (s + 10))&#10;      mark = mark +  max(0, i-5)&#10;    else&#10;      mark = mark + min(i+10, 100)&#10;  }&#10;  return(mark/101)&#10;}&#10;s = 0:100&#10;y = sapply(s, f)&#10;plot(s, y)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2011-01-14T15:17:31.510" Id="6255" LastActivityDate="2011-01-15T21:41:48.193" LastEditDate="2011-01-15T21:41:48.193" LastEditorUserId="8" OwnerUserId="8" ParentId="6253" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;The way you are thinking is one of the ways most people interpret blocks. But the bigger picture which sometimes people don't notice is: &lt;strong&gt;Blocks are a way to model a correlation structure. They let us &quot;eliminate&quot; or control for factors which we know influence the outcomes but are not really of interest.&lt;/strong&gt; However, your conclusion about fixed factors may not be true in general. If the sampling is random and the effects can be considered fixed (or the individual variances are not very large), fixed factor analysis can be generalized to the population. &lt;/p&gt;&#10;&#10;&lt;p&gt;On a different note, if you read Casella's &lt;em&gt;Statistical Design&lt;/em&gt;, he points out that blocks need not be treated as random. It does makes sense to treat them as random, but not all the time. In most of the cases, thinking about blocks as a tool to impose a correlation structure or control for the &quot;unknown&quot; factors helps.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-01-14T17:22:49.567" Id="6258" LastActivityDate="2011-01-14T17:30:57.563" LastEditDate="2011-01-14T17:30:57.563" LastEditorUserId="1307" OwnerUserId="1307" ParentId="6245" PostTypeId="2" Score="3" />
  
  
  
  <row Body="&lt;p&gt;&lt;em&gt;How&lt;/em&gt; are you comparing the calibrated means? If you're looking at differences between them, and testing if that's zero using a $t$-test, then surely the mean of the empty loop ($\bar{x}_0$, say) will simply cancel out:&#10;$$(\bar{x}_1 - \bar{x}_0) - (\bar{x}_2 - \bar{x}_0) = \bar{x}_1 - \bar{x}_2$$&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-01-15T13:58:40.187" Id="6274" LastActivityDate="2011-01-15T13:58:40.187" OwnerUserId="449" ParentId="6243" PostTypeId="2" Score="1" />
  <row AnswerCount="2" Body="&lt;p&gt;I am a very beginner of statistic. Recently a project require me to analyse data using logistic regression &amp;amp; SPSS within a specific time frame. Although I have read few books, but still very blur on how to start off. Can someone guide me through? What is the 1st ste and what next?&lt;/p&gt;&#10;&#10;&lt;p&gt;Anyway, I have started some. Once entered the data into SPSS, I have done crosstab (categorical IV), descriptive (continuous IV) and spearman correlation. &lt;/p&gt;&#10;&#10;&lt;p&gt;Then, I proceed to test for nonlinearity by transforming into Ln which give me some problems. I have re-coded all zero cells to a small value (0.0001) to enable the Ln transformation. Then, I re-test the nonlinearity.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Question:&lt;/p&gt;&#10;&#10;&lt;p&gt;1) The only solution for violation is to transform the variable from continuous to categorical? I got one violation. &lt;/p&gt;&#10;&#10;&lt;p&gt;2) One Exp(B) is extremely large (15203.835). What does this means? Why?&lt;/p&gt;&#10;&#10;&lt;p&gt;3) There is one interaction has Exp(B) = 0.00. Why?&lt;/p&gt;&#10;&#10;&lt;p&gt;Many thanks. &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-01-15T14:48:52.947" Id="6279" LastActivityDate="2011-01-16T04:48:53.307" OwnerUserId="2793" PostTypeId="1" Score="2" Tags="&lt;logistic&gt;" Title="Steps of data analysis using logistic regression" ViewCount="2818" />
  <row Body="&lt;p&gt;Thanks @mbq and @onestop. After running some tests the calibration was a wash. Subtracting the means raised the margin or error to the point that the calibrated and none calibrated test results where indeterminately different.&lt;/p&gt;&#10;&#10;&lt;p&gt;@mbq I will take your advice and reduce the critical value lookup to 30 &lt;em&gt;(as infinity)&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;When comparing against other benchmarks I think I can even avoid the walsh t-test because the variances are so low &lt;em&gt;(all below 1)&lt;/em&gt;. Some examples are the variances of benchmarks are:&#10;&lt;code&gt;1.1733873442408789e-16&lt;/code&gt;, &lt;code&gt;0.000012223589489868368&lt;/code&gt;, &lt;code&gt;3.772214786601029e-19&lt;/code&gt;, and &lt;code&gt;6.607725046958527e-16&lt;/code&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-01-15T20:28:57.713" Id="6285" LastActivityDate="2011-01-15T20:28:57.713" OwnerUserId="2634" ParentId="6243" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;You should consider using mixed-effect / multi-level models. The techniques used to fit these models work fine with unbalanced design, which is how it will interpret your missing data. As long as the data is missing at random, this is a reasonable way to proceed. SPSS is able to fit linear mixed-effect models.&lt;/p&gt;&#10;&#10;&lt;p&gt;Mixed-effect models also allow continuous covariates, so you could add, e.g., age as a person-level predictor.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-01-15T21:30:57.260" Id="6286" LastActivityDate="2011-01-15T21:30:57.260" OwnerUserId="2739" ParentId="6265" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;When used in stage-wise mode, the LARS algorithm is a greedy method that does not yield a provably consistent estimator (in other words, it does not converge to a stable result when you increase the number of samples). &lt;/p&gt;&#10;&#10;&lt;p&gt;Conversely, the LASSO (and thus the LARS algorithm when used in LASSO mode) solves a convex data fitting problem. In particular, this problem (the L1 penalized linear estimator) has plenty of nice proved properties (consistency, sparsistency).&lt;/p&gt;&#10;&#10;&lt;p&gt;I would thus try to always use the LARS in LASSO mode (or use another solver for LASSO), unless you have very good reasons to prefer stage-wise.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-01-16T17:42:01.520" Id="6301" LastActivityDate="2012-11-06T18:12:52.440" LastEditDate="2012-11-06T18:12:52.440" LastEditorUserId="16049" OwnerUserId="1265" ParentId="4663" PostTypeId="2" Score="19" />
  
  
  
  <row Body="&lt;p&gt;If you keep the log likelihoods, you can just select the one with the highest value. Also, if your interest is primarily the mode, just doing an optimization to find the point with the highest log likelihood would suffice.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-01-18T01:42:16.450" Id="6328" LastActivityDate="2011-01-18T01:42:16.450" OwnerUserId="1146" ParentId="3328" PostTypeId="2" Score="2" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;Most methods for &lt;a href=&quot;http://eu.wiley.com/WileyCDA/WileyTitle/productCd-0470090162.html&quot; rel=&quot;nofollow&quot;&gt;symbolic data analyis&lt;/a&gt; are currently implemented in the SODAS software.&lt;/p&gt;&#10;&#10;&lt;p&gt;Are there any R packages for symbolic data except clamix and clusterSim?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-01-18T20:46:44.443" FavoriteCount="1" Id="6354" LastActivityDate="2011-01-21T18:46:58.413" OwnerUserId="2831" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;clustering&gt;" Title="R package for symbolic data analysis" ViewCount="572" />
  <row Body="&lt;p&gt;You definitely can, by following the same method you'd use for the first categorical predictor.  Create dummy variables just as you would for the first such variable. But it's often easier to use SPSS's Unianova command.  You can look this up in any printed or pdf'd Syntax Guide, or you can access it through Analyze...General Linear Model...Univariate.  &lt;/p&gt;&#10;&#10;&lt;p&gt;Despite being a little more complicated, the Regression command has a number of advantages over Unianova, though.  The chief one is that you can choose 'missing pairwise' (you don't have to lose a case simply because it's missing a value for one or two predictors).  You can also get many valuable diagnostics such as partial plots and influence statistics.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-01-19T00:09:20.683" Id="6363" LastActivityDate="2011-01-19T00:09:20.683" OwnerUserId="2669" ParentId="6353" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;Here is code I've used in the past (using the SVD approach). I know you said you've tried it already, but it has always worked for me so I thought I'd post it to see if it was helpful. &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;function [sigma] = validateCovMatrix(sig)&#10;&#10;% [sigma] = validateCovMatrix(sig)&#10;%&#10;% -- INPUT --&#10;% sig:      sample covariance matrix&#10;%&#10;% -- OUTPUT --&#10;% sigma:    positive-definite covariance matrix&#10;%&#10;&#10;EPS = 10^-6;&#10;ZERO = 10^-10;&#10;&#10;sigma = sig;&#10;[r err] = cholcov(sigma, 0);&#10;&#10;if (err ~= 0)&#10;    % the covariance matrix is not positive definite!&#10;    [v d] = eig(sigma);&#10;&#10;    % set any of the eigenvalues that are &amp;lt;= 0 to some small positive value&#10;    for n = 1:size(d,1)&#10;        if (d(n, n) &amp;lt;= ZERO)&#10;            d(n, n) = EPS;&#10;        end&#10;    end&#10;    % recompose the covariance matrix, now it should be positive definite.&#10;    sigma = v*d*v';&#10;&#10;    [r err] = cholcov(sigma, 0);&#10;    if (err ~= 0)&#10;        disp('ERROR!');&#10;    end&#10;end&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2011-01-19T02:43:49.377" Id="6367" LastActivityDate="2011-01-19T02:43:49.377" OwnerUserId="1913" ParentId="6364" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;Here are a few links:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.stat.tamu.edu/dist/&quot; rel=&quot;nofollow&quot;&gt;http://www.stat.tamu.edu/dist/&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.stat.colostate.edu/distance_degree.html&quot; rel=&quot;nofollow&quot;&gt;http://www.stat.colostate.edu/distance_degree.html&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.worldcampus.psu.edu/AppliedStatisticsCertificate.shtml&quot; rel=&quot;nofollow&quot;&gt;http://www.worldcampus.psu.edu/AppliedStatisticsCertificate.shtml&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2011-01-19T19:03:46.913" CreationDate="2011-01-19T19:03:46.913" Id="6397" LastActivityDate="2011-01-19T19:03:46.913" OwnerUserId="2775" ParentId="6387" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;If your response variable is ordinal, you may want to consider and &quot;ordered logistic regression&quot;.  This is basically where you model the cumulative probabilities {in the simple example, you would model $Pr(Y\leq 1),Pr(Y\leq 2),Pr(Y\leq 3)$}.  This incorporates the ordering of the response into the model, &lt;em&gt;without&lt;/em&gt; the need for an arbitrary assumption which transforms the ordered response into a numerical one (although having said that, this can be a useful first step in exploratory analysis, or in selecting which $X$ and $Z$ variables are not necessary)&lt;/p&gt;&#10;&#10;&lt;p&gt;There is a way that you can get the glm() function in R to give you the MLE's for this model (other wise you would need to write your own algorithm to get the MLEs).  You define a new set of variables, say $W$, where these are defined as&lt;/p&gt;&#10;&#10;&lt;p&gt;$$W_{1jk} = \frac{Y_{1jk}}{\sum_{i=1}^{i=I} Y_{ijk}}$$&#10;$$W_{2jk} = \frac{Y_{2jk}}{\sum_{i=2}^{i=I} Y_{ijk}}$$&#10;$$...$$&#10;$$W_{I-1,jk} = \frac{Y_{I-1,jk}}{\sum_{i=I-1}^{i=R} Y_{ijk}}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Where $i=1,..,I$ indexes the $Y$ categories, $j=1,..,J$ indexes the $X$ categories, and $k=1,..,K$ indexes the $Z$ categories.  Then fit a glm() of W on X and Z using the complimentary log-log link function.  Denoting $\theta_{ijk}=Pr(Y_{ijk}\leq i)$ as the cumulative probability, the MLE's of the theta's (assuming a multi-nomial distribution for $Y_{ijk}$ values) is then&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\hat{\theta}_{ijk}=\hat{W}_{ijk}+\hat{\theta}_{(i-1)jk}(1-\hat{W}_{ijk}) \ \ \ i=1,\dots ,I-1$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Where $\hat{\theta}_{0jk}=0$ and $\hat{\theta}_{Ijk}=1$ and $\hat{W}_{ijk}$ are the fitted values from the glm.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can then use the deviance table (use the anova() function on the glm object) to assess the significance of the regressor variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;EDIT: one thing I forgot to mention in my original answer was that in the glm() function, you need to specify weights when fitting the model to&lt;/em&gt; $W$, &lt;em&gt;which are equal to the denominators in the respective fractions defining each&lt;/em&gt; $W$.&lt;/p&gt;&#10;&#10;&lt;p&gt;You could also try a Bayesian approach, but you would most likely need to use sampling techniques to get your posterior, and using the multinomial likelihood (but parameterised with respect to $\theta_{ijk}$, so the likelihood function will have &lt;em&gt;differences&lt;/em&gt; of the form $\theta_{ijk}-\theta_{i-1,jk}$), the MLE's are a good &quot;first crack&quot; at genuinely fitting the model, and give an approximate Bayesian solution (as you may have noticed, I prefer Bayesian inference)&lt;/p&gt;&#10;&#10;&lt;p&gt;This method is in my lecture notes, so I'm not really sure how to reference it (there are no references given in the notes) apart from what I've just said.&lt;/p&gt;&#10;&#10;&lt;p&gt;Just another note, I won't harp on it, but I p-values are not all they are cracked up to be.  A good post discussing this can be found &lt;a href=&quot;http://stats.stackexchange.com/questions/6308/article-about-misuse-of-statistical-method-in-nytimes&quot;&gt;here&lt;/a&gt;.  I like Harlod Jeffrey's quote above p-values (from his book &lt;em&gt;probability theory&lt;/em&gt;) &quot;A null hypothesis may be rejected because it did not predict something that was not observed&quot; (this is because p-values ask for the probability of events &lt;em&gt;more extreme&lt;/em&gt; than what was observed).&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2011-01-20T03:11:20.827" Id="6402" LastActivityDate="2011-01-21T17:19:22.187" LastEditDate="2011-01-21T17:19:22.187" LastEditorUserId="2392" OwnerUserId="2392" ParentId="6400" PostTypeId="2" Score="2" />
  
  
  
  <row Body="&lt;p&gt;You might want to check out &lt;a href=&quot;http://www.insidethebook.com/ee/&quot; rel=&quot;nofollow&quot;&gt;The Book Blog.&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Tom Tango and the other authors of &quot;The Book: Playing the Percentages in Baseball&quot; are probably the best sources of sabermetrics out there.  In particular, they love regression to the mean.  They came up with a forecasting system designed to be the most basic acceptable system (Marcel), and it relies almost exclusively on regression to the mean.&lt;/p&gt;&#10;&#10;&lt;p&gt;Off the top of my head, I suppose one method would be to use such a forecast to estimate true talent, and then find an appropriate distribution around that mean talent.  Once you have that, each plate appearance will be like a Bernoulli trial, so the binomial distribution could take you the rest of the way.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-01-21T15:32:30.207" Id="6434" LastActivityDate="2011-01-21T15:32:30.207" OwnerUserId="2485" ParentId="6404" PostTypeId="2" Score="1" />
  
  
  
  <row AcceptedAnswerId="6485" AnswerCount="3" Body="&lt;p&gt;When building a CART model (specifically classification tree) using rpart (in R), it is often interesting to know what is the importance of the various variables introduced to the model.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thus, my question is:  &lt;strong&gt;What common measures exists for ranking/measuring variable importance of participating variables in a CART model?  And how can this be computed using R (for example, when using the rpart package)&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, here is some dummy code, created so you might show your solutions on it.  This example is structured so that it is clear that variable x1 and x2 are &quot;important&quot; while (in some sense) x1 is more important then x2 (since x1 should apply to more cases, thus make more influence on the structure of the data, then x2).&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(31431)&#10;n &amp;lt;- 400&#10;x1 &amp;lt;- rnorm(n)&#10;x2 &amp;lt;- rnorm(n)&#10;x3 &amp;lt;- rnorm(n)&#10;x4 &amp;lt;- rnorm(n)&#10;x5 &amp;lt;- rnorm(n)&#10;&#10;X &amp;lt;- data.frame(x1,x2,x3,x4,x5)&#10;&#10;y &amp;lt;- sample(letters[1:4], n, T)&#10;y &amp;lt;- ifelse(X[,2] &amp;lt; -1 , &quot;b&quot;, y)&#10;y &amp;lt;- ifelse(X[,1] &amp;lt; 0 , &quot;a&quot;, y)&#10;&#10;require(rpart)&#10;fit &amp;lt;- rpart(y~., X)&#10;plot(fit); text(fit)&#10;&#10;info.gain.rpart(fit) # your function - telling us on each variable how important it is&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;(references are always welcomed)&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-01-23T22:06:03.373" FavoriteCount="7" Id="6478" LastActivityDate="2014-09-30T08:49:10.837" LastEditDate="2011-01-27T10:42:22.663" LastEditorUserId="8" OwnerUserId="253" PostTypeId="1" Score="10" Tags="&lt;r&gt;&lt;classification&gt;&lt;model-selection&gt;&lt;cart&gt;&lt;rpart&gt;" Title="How to measure/rank &quot;variable importance&quot; when using CART? (specifically using {rpart} from R)" ViewCount="5174" />
  
  
  
  <row Body="&lt;p&gt;Sounds like you want something like a &lt;a href=&quot;http://en.wikipedia.org/wiki/Kernel_density_estimation&quot; rel=&quot;nofollow&quot;&gt;Kernel Density Estimate&lt;/a&gt; of the distribution. In R I think you want &lt;a href=&quot;http://stat.ethz.ch/R-manual/R-patched/library/stats/html/density.html&quot; rel=&quot;nofollow&quot;&gt;density&lt;/a&gt; function. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-01-24T18:42:45.843" Id="6494" LastActivityDate="2011-01-24T18:42:45.843" OwnerUserId="1146" ParentId="6480" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;A simple way to turn categorical variables into a set of dummy variables for use in models in SPSS is using the do repeat syntax. This is the simplest to use if your categorical variables are in numeric order.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;*making vector of dummy variables.&#10;vector dummy(3,F1.0).&#10;*looping through dummy variables using do repeat, in this example category would be the categorical variable to recode. &#10;do repeat dummy = dummy1 to dummy3 /#i = 1 to 3.&#10;compute dummy = 0.&#10;if category = #i dummy = 1.&#10;end repeat.&#10;execute. &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Otherwise you can simply run a set of if statements to make your dummy variables. My current version (16) has no native ability to specify a set of dummy variables automatically in the regression command (like you can in Stata using the &lt;a href=&quot;http://www.stata.com/support/faqs/data/dummy.html&quot; rel=&quot;nofollow&quot;&gt;xi command&lt;/a&gt;) but I wouldn't be surprised if this is available in some newer version. Also take note of dmk38's point #2, this coding scheme is assuming nominal categories. If your variable is ordinal more discretion can be used.&lt;/p&gt;&#10;&#10;&lt;p&gt;I also agree with dmk38 and the talk about regression being better because of its ability to specify missing data in a particular manner is a completely separate issue.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-01-25T04:29:36.703" Id="6501" LastActivityDate="2011-01-25T04:29:36.703" OwnerUserId="1036" ParentId="6353" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="6512" AnswerCount="1" Body="&lt;p&gt;In a LASSO regression scenario where&lt;/p&gt;&#10;&#10;&lt;p&gt;$y= X \beta + \epsilon$,&lt;/p&gt;&#10;&#10;&lt;p&gt;and the LASSO estimates are given by the following optimization problem&lt;/p&gt;&#10;&#10;&lt;p&gt;$ \min_\beta ||y - X \beta|| + \tau||\beta||_1$&lt;/p&gt;&#10;&#10;&lt;p&gt;Are there any distributional assumptions regarding the $\epsilon$? &lt;/p&gt;&#10;&#10;&lt;p&gt;In an OLS scenario, one would expect that the $\epsilon$ are independent and normally distributed.&lt;/p&gt;&#10;&#10;&lt;p&gt;Does it make any sense to analyze the residuals in a LASSO regression?&lt;/p&gt;&#10;&#10;&lt;p&gt;I know that the LASSO estimate can be obtained as the posterior mode under independent double-exponential priors for the $\beta_j$. But I haven't found any standard &quot;assumption checking phase&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance (:&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-01-25T04:55:04.823" FavoriteCount="6" Id="6502" LastActivityDate="2012-10-18T07:53:30.350" OwnerUserId="2902" PostTypeId="1" Score="12" Tags="&lt;regression&gt;&lt;lasso&gt;&lt;assumptions&gt;&lt;residuals&gt;" Title="LASSO assumptions" ViewCount="992" />
  <row Body="&lt;p&gt;I would recommend using a &quot;Beta distribution of the second kind&quot; (Beta&lt;sub&gt;2&lt;/sub&gt; for short) for a &lt;em&gt;mildly informative&lt;/em&gt; distribution, and to use the conjugate inverse gamma distribution if you have &lt;em&gt;strong&lt;/em&gt; prior beliefs.  The reason I say this is that the conjugate prior is &lt;em&gt;non-robust&lt;/em&gt; in the sense that, if the prior and data conflict, the prior has an &lt;em&gt;unbounded&lt;/em&gt; influence on the posterior distribution.  Such behaviour is what I would call &quot;dogmatic&quot;, and not justified by &lt;em&gt;mild&lt;/em&gt; prior information.&lt;/p&gt;&#10;&#10;&lt;p&gt;The property which determines &lt;em&gt;robustness&lt;/em&gt; is the tail-behaviour of the prior and of the likelihood.  A very good article outlining the technical details is &lt;a href=&quot;http://ba.stat.cmu.edu/journal/2006/vol01/issue01/andrade.pdf&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.  For example, a likelihood can be chosen (say a t-distribution) such that as an observation $y_i \rightarrow \infty$ (i.e. becomes arbitrarily large) it is discarded from the analysis of a location parameter (much in the same way that you would intuitively do with such an observation).  The rate of &quot;discarding&quot; depends on how heavy the tails of the distribution are.&lt;/p&gt;&#10;&#10;&lt;p&gt;Some slides which show an application in the hierarchical modelling context can be found &lt;a href=&quot;http://www-stat.wharton.upenn.edu/statweb/Conference/OBayes09/slides/Luis%20Pericchi.pdf&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; (shows the mathematical form of the Beta&lt;sub&gt;2&lt;/sub&gt; distribution), with a paper &lt;a href=&quot;http://www-stat.wharton.upenn.edu/statweb/Conference/OBayes09/AbstractPapers/Pericchi_Paper.pdf&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you are not in the hierarchical modeling context, then I would suggest comparing the posterior (or whatever results you are creating) but use the &lt;em&gt;Jeffreys prior&lt;/em&gt; for a scale parameter, which is given by $p(\sigma)\propto\frac{1}{\sigma}$.  This can be created as a limit of the Beta&lt;sub&gt;2&lt;/sub&gt; density as both its parameters converge to zero. For an approximation you could use small values.  But I would try to work out the solution &lt;em&gt;analytically&lt;/em&gt; if at all possible (and if not a complete analytical solution, get the analytical solution as far progressed as you possibly can), because you will not only save yourself some computational time, but you are also likely to &lt;em&gt;understand&lt;/em&gt; what is happening in your model better.&lt;/p&gt;&#10;&#10;&lt;p&gt;A further alternative is to specify your prior information in the form of constraints (mean equal to $M$, variance equal to $V$, IQR equal to $IQR$, etc. with the values of $M,V,IQR$ specified by yourself), and then use the &lt;a href=&quot;http://en.wikipedia.org/wiki/Principle_of_maximum_entropy&quot; rel=&quot;nofollow&quot;&gt;maximum entropy distribution&lt;/a&gt; (search any work by Edwin Jaynes or Larry Bretthorst for a good explanation of what Maximum Entropy is and what it is not) with respect to Jeffreys' &quot;invariant measure&quot; $m(\sigma)=\frac{1}{\sigma}$.  &lt;/p&gt;&#10;&#10;&lt;p&gt;MaxEnt is the &quot;Rolls Royce&quot; version, while the Beta&lt;sub&gt;2&lt;/sub&gt; is more a &quot;sedan&quot; version.  The reason for this is that the MaxEnt distribution &quot;assumes the least&quot; subject to the constraints you have put into it (e.g., no constraints means you just get the Jeffreys prior), whereas the Beta&lt;sub&gt;2&lt;/sub&gt; distribution may contain some &quot;hidden&quot; features which may or may not be desirable in your specific case (e.g., if the prior information is &lt;em&gt;more reliable&lt;/em&gt; than the data, then Beta&lt;sub&gt;2&lt;/sub&gt; is bad).&lt;/p&gt;&#10;&#10;&lt;p&gt;The other nice property of MaxEnt distribution is that &lt;em&gt;if there are no unspecified constraints operating in the data generating mechanism&lt;/em&gt; then the MaxEnt distribution is &lt;em&gt;overwhelmingly&lt;/em&gt; the most likely distribution that you will see (we're talking odds way over billions and trillions to one).  Therefore, if the distribution you see is not the MaxEnt one, then there is likely &lt;em&gt;additional constraints which you have not specified&lt;/em&gt; operating on the true process, and the observed values can provide a clue as to what that constraint might be.&lt;/p&gt;&#10;" CommentCount="10" CreationDate="2011-01-25T06:05:53.627" Id="6506" LastActivityDate="2013-10-23T15:41:19.910" LastEditDate="2013-10-23T15:41:19.910" LastEditorUserId="17230" OwnerUserId="2392" ParentId="6493" PostTypeId="2" Score="14" />
  
  <row Body="&lt;p&gt;I am not an expert on LASSO, but here is my take. &lt;/p&gt;&#10;&#10;&lt;p&gt;First note that OLS is pretty robust to violations of indepence and normality. Then judging from the Theorem 7  and the discussion above it in the article &lt;a href=&quot;http://arxiv.org/pdf/0811.1790.pdf&quot; rel=&quot;nofollow&quot;&gt;Robust Regression and Lasso&lt;/a&gt; (by X. Huan, C. Caramanis and S. Mannor) I guess, that in LASSO regression we are more concerned not with the distribution of $\varepsilon_i$, but in the joint distribution of $(y_i,x_i)$. The theorem relies on the assumption that $(y_i,x_i)$ is a sample, so this is comparable to usual OLS assumptions. But LASSO is less restrictive, it does not constrain $y_i$ to be generated from the linear model. &lt;/p&gt;&#10;&#10;&lt;p&gt;To sum up, the answer to your first question is no. There are no distributional assumptions on $\varepsilon$, all distributional assumptions are on $(y,X)$. Furthermore they are weaker, since in LASSO nothing is postulate on conditional distribution $(y|X)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Having said that,  the answer to the second question is then also no. Since the $\varepsilon$ does not play any role it does not make any sense to analyse them the way you analyse them in OLS (normality tests, heteroscedasticity, Durbin-Watson, etc). You should however analyse them in context how good the model fit was.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-01-25T08:13:53.123" Id="6512" LastActivityDate="2012-10-18T07:53:30.350" LastEditDate="2012-10-18T07:53:30.350" LastEditorUserId="2116" OwnerUserId="2116" ParentId="6502" PostTypeId="2" Score="8" />
  <row AnswerCount="2" Body="&lt;p&gt;I'm currently working on audio data trying to perform a recognition for given classes (for example grinding coffee etc.). However, I have some trouble distinguishing the null class from interesting sound segments.&#10;Currently, I simply look at the audio intensity. As I have a limited, known number of classes I want to detect, I thought about saving some aggregate of the signals (mean fft) comparing the unclassified signal to it. If it is close enough to one of the saved aggregates do a classification, if not just drop it.&#10;My approach seems to me quite naive. Therefore, input/ideas appreciated ;)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-01-25T09:44:42.360" FavoriteCount="2" Id="6516" LastActivityDate="2011-01-25T15:11:37.393" OwnerUserId="2904" PostTypeId="1" Score="5" Tags="&lt;machine-learning&gt;" Title="Ideas for Segmenting Audio Data" ViewCount="151" />
  <row Body="&lt;p&gt;Here's one I found via a Google Image search, but maybe it's &lt;em&gt;too&lt;/em&gt; brief, and the diagram too simple: &lt;a href=&quot;http://labstats.net/articles/overview.html&quot;&gt;http://labstats.net/articles/overview.html&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-01-25T13:11:39.397" Id="6523" LastActivityDate="2011-01-25T13:11:39.397" OwnerUserId="449" ParentId="6518" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;I don’t know if I understood your question correctly, but I’ll give it a try.&#10;Any hierarchical agglomerative algorithm can do the job. Remember that agglomerative algorithms proceed by “pasting” observations to a cluster and treating the cluster as a single unit. &lt;/p&gt;&#10;&#10;&lt;p&gt;I would suggest this:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Substitute  your 1d or 2d observations with the clusters’ centroids.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Use a distance to assign the temporal observations into the 1d/2d cluster. (Euclidean distance might work).&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Hope this works =)&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-01-25T16:14:11.083" Id="6532" LastActivityDate="2011-01-25T16:19:54.587" LastEditDate="2011-01-25T16:19:54.587" LastEditorUserId="2902" OwnerUserId="2902" ParentId="6483" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="6536" AnswerCount="2" Body="&lt;p&gt;So, I have a data set of percentages like so:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;100   /   10000   = 1% (0.01)&#10;2     /     5     = 40% (0.4)&#10;4     /     3     = 133% (1.3) &#10;1000  /   2000    = 50% (0.5)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I want to find the standard deviation of the percentages, but weighted for their data volume. ie, the first and last data points should dominate the calculation.&lt;/p&gt;&#10;&#10;&lt;p&gt;How do I do that? And is there a simple way to do it in Excel?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-01-25T16:44:25.457" FavoriteCount="3" Id="6534" LastActivityDate="2014-09-24T20:56:45.733" LastEditDate="2011-01-25T23:42:29.083" LastEditorUserId="142" OwnerUserId="142" PostTypeId="1" Score="7" Tags="&lt;standard-deviation&gt;&lt;excel&gt;&lt;weighted-mean&gt;" Title="How do I calculate a weighted standard deviation? In Excel?" ViewCount="30116" />
  
  <row Body="&lt;p&gt;The &lt;a href=&quot;http://www.itl.nist.gov/div898/software/dataplot/refman2/ch2/weightsd.pdf&quot; rel=&quot;nofollow&quot;&gt;formula for weighted standard deviation&lt;/a&gt; is:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \sqrt{ \frac{ \sum_{i=1}^N w_i (x_i - \bar{x}^*)^2 }{ \frac{(M-1)}{M} \sum_{i=1}^N w_i } },$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where&lt;/p&gt;&#10;&#10;&lt;p&gt;$N$ is the number of observations.&lt;/p&gt;&#10;&#10;&lt;p&gt;$M$ is the number of nonzero weights.&lt;/p&gt;&#10;&#10;&lt;p&gt;$w_i$ are the weights&lt;/p&gt;&#10;&#10;&lt;p&gt;$x_i$ are the observations.&lt;/p&gt;&#10;&#10;&lt;p&gt;$\bar{x}^*$ is the weighted mean.&lt;/p&gt;&#10;&#10;&lt;p&gt;Remember that the formula for weighted mean is:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\bar{x}^* = \frac{\sum_{i=1}^N w_i x_i}{\sum_{i=1}^N w_i}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Use the appropriate weights to get the desired result. In your case I would suggest to use $\frac{\mbox{Number of cases in segment}}{\mbox{Total number of cases}}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;To do this in Excel, you need to calculate the weighted mean first. Then calculate the $(x_i - \bar{x}^*)^2$ in a separate column. The rest must be very easy.&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2011-01-25T18:22:20.437" Id="6536" LastActivityDate="2013-07-31T11:13:57.223" LastEditDate="2013-07-31T11:13:57.223" LastEditorUserId="28661" OwnerUserId="2902" ParentId="6534" PostTypeId="2" Score="12" />
  <row AcceptedAnswerId="6555" AnswerCount="2" Body="&lt;p&gt;Let say I have this kind of data:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;1 01/1/1980&#10;2 01/2/1999&#10;3 03/12/2000&#10;-1 03/6/2005&#10;-5 07/07/2007&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;how can I calculate the Present Value (PV) to them in respect to current date, let say with 5% interest rate, in spreadsheet? &lt;code&gt;Current date&lt;/code&gt; means today.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;[Update]&lt;/strong&gt; I am probably misunderstanding terms &lt;code&gt;PV&lt;/code&gt; and &lt;code&gt;FV&lt;/code&gt;. I am trying to find some ready function similar to &lt;a href=&quot;http://docs.google.com/support/bin/answer.py?hl=en&amp;amp;answer=155151&quot; rel=&quot;nofollow&quot;&gt;this one&lt;/a&gt;. Whatever method you use it must work with the above data. Please, stop spam.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-01-25T19:34:09.613" FavoriteCount="1" Id="6540" LastActivityDate="2011-10-17T12:45:09.317" LastEditDate="2011-10-17T12:45:09.317" LastEditorUserId="88" OwnerUserId="2914" PostTypeId="1" Score="1" Tags="&lt;econometrics&gt;&lt;google-spreadsheet&gt;" Title="How do calculate Present Value in Google Spreadsheet?" ViewCount="1293" />
  <row AcceptedAnswerId="6551" AnswerCount="1" Body="&lt;p&gt;Given a univariate sample $\vec X = X_1, ..., X_n$ with standard deviation 1 and a strictly monotone transformation $t: R \to R$ with the property that the standard deviation of $t(\vec X)$ is also 1 (where $t(\vec X)$ is $t$ applied to each $X_i$). If one know fits a normal distribution to $\vec X$ and $t(\vec X)$, one observes the following fact: the likelihood at the respective MLEs of $\vec X$ and $t(\vec X)$ is the same. The reason for that is, that the standard deviation is the MLE for the $\sigma$ parameter of the normal distribution, so just the mean changes, which does not change the likelihood.&lt;/p&gt;&#10;&#10;&lt;p&gt;In some computations with &lt;em&gt;multivariate&lt;/em&gt; data I observed the same fact, using transformations such that $\det(cov(t(\vec X)))=1)$. But I did not see a straightforward proof for that! Note that in the multivariate case, the MLEs for the variance parameters do change, even if the above determinant is always 1.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am sure I am not the first to notice, but my (somewhat limited) literature on the multivariate normal distribution does not give me a clue how to prove the above statement, i.e. that the multivariate normal likelihood does only depend on the determinant of the $cov(t(\vec X))$, regardless of choice of $t$.&lt;/p&gt;&#10;&#10;&lt;p&gt;A proof boils down to looking at the terms of the form $x&amp;#39;\Sigma x$ for $\det \Sigma  =  1$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Do you have more clue?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks, Philipp&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-01-25T22:33:24.837" Id="6549" LastActivityDate="2011-01-27T03:32:30.250" OwnerUserId="2916" PostTypeId="1" Score="1" Tags="&lt;normal-distribution&gt;&lt;multivariate-analysis&gt;&lt;maximum-likelihood&gt;" Title="Likelihood at MLE and transformations, the multivariate normal case" ViewCount="745" />
  
  
  
  
  
  <row Body="&lt;p&gt;I try to avoid questions with more than two answers, as it is impossible to compare them between users. (good vs. very good can be very subjective).&#10;I rephrase most questions into binary type (giving though the possibility to be indiffernt):&#10;&quot;Would you use the product everyday?&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;Yes No Indifferent&lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;Would you recommend the product to your friends?&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;etc.&lt;/p&gt;&#10;&#10;&lt;p&gt;I found results obtained with this method to be way more consistent with the feeling I got from later interviews and performance tests. However, my work so far focuses on Human Computer Interaction questionnaires.&#10;The best is anyways to conduct real person interviews, as you learn more from them. Of course, they are also very time consuming :(&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-01-26T19:16:51.430" Id="6586" LastActivityDate="2011-01-26T19:16:51.430" OwnerUserId="2904" ParentId="6570" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;@onestop points in the right direction.  Belsley, Kuh, and Welsch describe this approach on pp. 24-26 of their book.  To differentiate with respect to an &lt;em&gt;observation&lt;/em&gt; (and not just one of its attributes), they introduce a weight, perform weighted least squares, and differentiate with respect to the weight.&lt;/p&gt;&#10;&#10;&lt;p&gt;Specifically, let $\mathbb{X} = X_{ij}$ be the design matrix, let $\mathbf{x}_i$ be the $i$th observation, let $e_i$ be its residual, let $w_i$ be the weight, and define $h_i$ (the $i$th diagonal entry in the hat matrix) to be $\mathbf{x}_i (\mathbb{X}^T \mathbb{X})^{-1} \mathbf{x}_i^T$.  They compute&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac{\partial b(w_i)}{\partial w_i} = \frac{(\mathbb{X}^T\mathbb{X})^{-1} \mathbf{x}_i^T e_i}{\left[1 - (1 - w_i)h_i\right]^2},$$&lt;/p&gt;&#10;&#10;&lt;p&gt;whence&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac{\partial b(w_i)}{\partial w_i}\Bigg|_{w_i=1} = (\mathbb{X}^T\mathbb{X})^{-1} \mathbf{x}_i^T e_i.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;This is interpreted as a way to &quot;identify influential observations, ... provid[ing] a means for examining the sensitivity of the regression coefficients to a slight change in the weight given to the ith observation.  Large values of this derivative indicate observations that have large influence on the calculated coefficients.&quot;  They suggest it can be used as an alternative to the DFBETA diagnostic.  (DFBETA measures the change in $b$ when observation $i$ is completely deleted.)  The relationship between the influence and DFBETA is that DFBETA equals the influence divided by $1 - h_i$ [equation 2.1 p. 13].&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-01-27T04:21:12.800" Id="6602" LastActivityDate="2011-01-27T04:21:12.800" OwnerUserId="919" ParentId="6580" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;This first part of my response won't address your two questions directly since what I am suggesting departs from your correlational approach. If I understand you correctly, you have two blocks of variables, and they play an asymmetrical role in the sense that one of them is composed of response variables (performance on four cognitive tests) whereas the other includes explanatory variables (measures of blood flow at several locations). So, a nice way to answer your question of interest would be to look at &lt;a href=&quot;http://en.wikipedia.org/wiki/Partial_least_squares_regression&quot; rel=&quot;nofollow&quot;&gt;PLS regression&lt;/a&gt;. As detailed in an earlier response of mine, &lt;a href=&quot;http://stats.stackexchange.com/questions/4517/regression-with-multiple-dependent-variables&quot;&gt;Regression with multiple dependent variables?&lt;/a&gt;, the correlation between factor scores on the first dimension will reflect the overall link between these two blocks, and a closer look at the weighted combination of variables in each block (i.e., loadings) would help interpreting the contribution of each variable of the $X$ block in predicting the $Y$ block. The &lt;a href=&quot;http://faculty.chass.ncsu.edu/garson/PA765/pls.htm&quot; rel=&quot;nofollow&quot;&gt;SPSS implementation&lt;/a&gt; is detailed on Dave Garson's website. This prevents from using any correction for multiple comparisons.&lt;/p&gt;&#10;&#10;&lt;p&gt;Back to your specific questions, yes the Bonferroni correction is known to be conservative and step-down methods are to be preferred (instead of correcting the p-values or the test statistic in one shot for all the tests, we adapt the threshold depending on the previous HT outcomes, in a sequential manner).&#10;Look into SPSS documentation (or &lt;a href=&quot;http://www.uky.edu/ComputingCenter/SSTARS/www/documentation/MultipleComparisons_3.htm#b13&quot; rel=&quot;nofollow&quot;&gt;Pairwise Comparisons in SAS and SPSS&lt;/a&gt;) to find a suitable one, e.g. Bonferroni-Holm.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-01-27T10:46:55.900" Id="6612" LastActivityDate="2011-01-27T10:46:55.900" OwnerUserId="930" ParentId="6604" PostTypeId="2" Score="3" />
  
  
  
  <row Body="&lt;p&gt;&lt;strong&gt;Yes, they are the same.&lt;/strong&gt;  Another crucial assumption in the Poisson process is that what happens now is independent of what happened a moment ago or what will happen in the next moment (or at any other moment, for that matter).  Therefore the distribution of events during any (measurable) period of time depends only on the length of time, not on how it is broken up.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-01-27T23:42:22.013" Id="6649" LastActivityDate="2011-01-27T23:42:22.013" OwnerUserId="919" ParentId="6648" PostTypeId="2" Score="5" />
  
  <row AnswerCount="1" Body="&lt;p&gt;I have serial hematological measurements data and I have plotted their means and SE in Stata. On the y-axis I have for example hemoglobin and time (visit days) on the x-axis hence I can visualize hemoglobin levels with time (whether it is decreasing or in increasing). The level decreases up to sometime and increases again. What test can use to test whether this is significant or not? Stata and R related answers are welcomed.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you.&lt;/p&gt;&#10;&#10;&lt;p&gt;Julie&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-01-28T13:07:55.577" Id="6664" LastActivityDate="2011-01-28T13:49:56.673" LastEditDate="2011-01-28T13:09:32.870" LastEditorUserId="2116" OwnerUserId="2961" PostTypeId="1" Score="7" Tags="&lt;r&gt;&lt;stata&gt;" Title="Statistical test for trend (continuous variable) in Stata or R" ViewCount="1511" />
&#10;  125,000 &amp;amp; 0.128 &amp;amp; 0.122 \\
&#10;  62,500 &amp;amp; 0.074 &amp;amp; 0.072 \\
&#10;  \hline
&#10;  \hline
&#10;  \hline
  
  
&#10;     - attr(*, &amp;quot;out.attrs&amp;quot;)=List of 2
  <row AcceptedAnswerId="6691" AnswerCount="3" Body="&lt;p&gt;If I see &lt;code&gt;Rmath.h&lt;/code&gt; in &lt;code&gt;/usr/share/R/include&lt;/code&gt;, the signature of the function &lt;code&gt;dpois&lt;/code&gt; or &lt;code&gt;Rf_dpois&lt;/code&gt; is  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;double dpois(double, double, int); &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;However, if I do &lt;code&gt;?dpois&lt;/code&gt; in &lt;code&gt;R&lt;/code&gt;, I see:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;dpois(x, lambda, log = FALSE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Are these both the same thing? If yes, can someone please clarify?&lt;/p&gt;&#10;&#10;&lt;p&gt;Also I tried to find the code for &lt;code&gt;dpois&lt;/code&gt; at &lt;a href=&quot;http://svn.r-project.org/R/trunk/src/library/stats/&quot; rel=&quot;nofollow&quot;&gt;R-svn&lt;/a&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;Can someone please tell how to find the code for functions like these?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you for any help or pointers.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-01-29T06:45:35.563" FavoriteCount="2" Id="6690" LastActivityDate="2011-01-29T16:31:29.507" OwnerUserId="1307" PostTypeId="1" Score="4" Tags="&lt;r&gt;" Title="What is the difference between Rf_dpois in Rmath.h and the dpois that I use directly in R?" ViewCount="280" />
  
  
  <row Body="&lt;p&gt;What I usually do is to calculate the plugin bandwidth using Silverman's formula (h_p) and then crossvalidate in the range of [h_p/5, 5h_p] to find the optimal bandwidth. This crossvalidation can be done either by using leave-one-out least squares crossvalidation or by leave-one-out-likelihood crossvalidation.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-01-30T04:40:55.097" Id="6712" LastActivityDate="2011-01-30T04:40:55.097" OwnerDisplayName="user2964" ParentId="6670" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;When you use the &lt;code&gt;predict&lt;/code&gt; with &lt;code&gt;newdata&lt;/code&gt; argument you must supply the the data.frame with the same column names. In your code you have&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;newdata &amp;lt;- expand.grid(X1=c(-1,0,1),X2=c(-1,0,1))&#10;names(newdata) &amp;lt;- c(&quot;scale(xxA)&quot;,&quot;scale(xxB)&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;But the formula supplied to lm object is&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;out ~ scale(xxA)*scale(xxB)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So when you call the predict, it tries to find objects &lt;code&gt;xxA&lt;/code&gt; and &lt;code&gt;xxB&lt;/code&gt; in your data and apply function &lt;code&gt;scale&lt;/code&gt; as per your initial request. But all R finds are objects &lt;code&gt;scale(xxA)&lt;/code&gt; and &lt;code&gt;scale(xxB)&lt;/code&gt;. So naturally it produces the error.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now if you supply correctly named &lt;code&gt;newdata&lt;/code&gt; &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;newdata &amp;lt;- expand.grid(xxA=c(-1,0,1),xxB=c(-1,0,1)) &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and try to use it for prediction&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;newdata$Y &amp;lt;- predict(lm.res.scale,newdata)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;R will remember the how it scaled original data and apply the same scaling to your new data. In this case supplied value -1 for &lt;code&gt;xxA&lt;/code&gt; will be subtracted the original mean of &lt;code&gt;xxA&lt;/code&gt; and divided by the original standard value of &lt;code&gt;xxA&lt;/code&gt;. If you want to get prediction of 1 S.D below the mean, you will need to supply this value. In your case then newdata should look like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;newdata &amp;lt;- expand.grid(xxA=mean(dat$xxA)+sd(dat$xxA)*c(-1,0,1),xxB=mean(dat$xxB)+sd(dat$xxB)*c(-1,0,1))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I gathered all the solutions in one place to compare:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;##Prepare data&#10;set.seed(1)&#10;dat &amp;lt;- data.frame(xxA = rnorm(20,10), xxB = rnorm(20,20))&#10;dat$out &amp;lt;- with(dat,xxA+xxB+xxA*xxB+rnorm(20,20))&#10;dat &amp;lt;- within(dat,{&#10;    X1 &amp;lt;- as.numeric(scale(xxA))&#10;    X2 &amp;lt;- as.numeric(scale(xxB))&#10;    })&#10;&#10;##Estimate the models&#10;lm.res.scale &amp;lt;- lm(out ~ scale(xxA)*scale(xxB),data=dat)&#10;lm.res.correct &amp;lt;- lm(out~X1*X2,data=dat)&#10;lm.mod &amp;lt;- lm(out ~ I(scale(xxA))*I(scale(xxB)), data=dat)&#10;rms.res &amp;lt;- Glm(out ~ scale(xxA)*scale(xxB),data=dat)&#10;&#10;&#10;##Build data for prediction&#10;newdata &amp;lt;- expand.grid(xxA=c(-1,0,1),xxB=c(-1,0,1))&#10;newdata$X1&amp;lt;-newdata$xxA&#10;newdata$X2&amp;lt;-newdata$xxB&#10;&#10;##Gather the predictions&#10;newdata$Yscaled &amp;lt;- predict(lm.res.scale,newdata)&#10;newdata$Ycorrect &amp;lt;- predict(lm.res.correct,newdata)&#10;newdata$YwithI &amp;lt;- predict(lm.mod,newdata)&#10;newdata$Ywithrms &amp;lt;- Predict(rms.res,xxA=c(-1,0,1),xxB=c(-1,0,1),conf.int=FALSE)[,3]&#10;&#10;##Build alternative data for prediction&#10;newdata2 &amp;lt;- expand.grid(xxA=mean(dat$xxA)+sd(dat$xxA)*c(-1,0,1),xxB=mean(dat$xxB)+sd(dat$xxB)*c(-1,0,1))&#10;&#10;#Predict&#10;newdata$Yorigsc &amp;lt;- predict(lm.res.scale,newdata2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I used &lt;code&gt;set.seed&lt;/code&gt; so the results should be the same if you try to repeat it. The newdata looks like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; newdata&#10;  xxA xxB X1 X2  Yscaled Ycorrect   YwithI Ywithrms  Yorigsc&#10;1  -1  -1 -1 -1 25.79709 225.9562 221.7517 221.7517 225.9562&#10;2   0  -1  0 -1 25.63030 244.5181 243.0404 243.0404 244.5181&#10;3   1  -1  1 -1 25.46351 263.0800 264.3291 264.3291 263.0800&#10;4  -1   0 -1  0 25.36341 234.6981 231.7012 231.7012 234.6981&#10;5   0   0  0  0 26.21499 254.0704 254.0704 254.0704 254.0704&#10;6   1   0  1  0 27.06657 273.4427 276.4396 276.4396 273.4427&#10;7  -1   1 -1  1 24.92972 243.4400 241.6507 241.6507 243.4400&#10;8   0   1  0  1 26.79967 263.6227 265.1004 265.1004 263.6227&#10;9   1   1  1  1 28.66962 283.8054 288.5501 288.5501 283.8054&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;As expected &lt;code&gt;Yscaled&lt;/code&gt; produces not the result we need since the original scaling &#10;is applied. In the case when we scale data before &lt;code&gt;lm&lt;/code&gt; (&lt;code&gt;Ycorrect&lt;/code&gt;) and when we supply alternative unscaled values (&lt;code&gt;Yorigsc&lt;/code&gt;) results coincide and are the ones needed.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now the other prediction methods give different results. This happens since R is forced to forget the original scaling using formula&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;out ~ I(scale(xxA))*I(scale(xxB))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;or package &lt;code&gt;rms&lt;/code&gt;. But when we use predict, the values are still scaled, but now according to supplied values of &lt;code&gt;xxA&lt;/code&gt; and &lt;code&gt;xxB&lt;/code&gt;. This is best illustrated by following statement, which in some way mimics what predict does with the data:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; eval(expression(cbind(scale(xxA),scale(xxB))),env=as.list(newdata))&#10;           [,1]      [,2]&#10; [1,] -1.154701 -1.154701&#10; [2,]  0.000000 -1.154701&#10; [3,]  1.154701 -1.154701&#10; [4,] -1.154701  0.000000&#10; [5,]  0.000000  0.000000&#10; [6,]  1.154701  0.000000&#10; [7,] -1.154701  1.154701&#10; [8,]  0.000000  1.154701&#10; [9,]  1.154701  1.154701&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;We can see that in this case, scaling does not change original values too much, but this is even worse, since the values from predict look reasonable, when in fact they are wrong.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-01-30T07:50:33.400" Id="6718" LastActivityDate="2011-01-30T20:04:27.090" LastEditDate="2011-01-30T20:04:27.090" LastEditorUserId="2116" OwnerUserId="2116" ParentId="6684" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;Systematic and unsystematic are rather ambiguous terms. One of the possible explanations is given by @probabilityislogic. Another may be given &lt;a href=&quot;http://en.wikipedia.org/wiki/Decomposition_of_time_series&quot; rel=&quot;nofollow&quot;&gt;here.&lt;/a&gt; Since the context you gave is time series, I think this might be related to  &lt;a href=&quot;http://en.wikipedia.org/wiki/Wold%27s_theorem&quot; rel=&quot;nofollow&quot;&gt;Wold's theorem&lt;/a&gt;.  Unfortunately wikipedia text captures the essence, but does not go into the details of which part is systematic and non systematic. &lt;/p&gt;&#10;&#10;&lt;p&gt;I did not manage to find appropriate link to refer to, so I will try give some explanation based on the &lt;a href=&quot;http://www.setbook.com.ua/books/120541.html?PHPSESSID=i41mmv16i1t034pb3u0o0pkkf1&quot; rel=&quot;nofollow&quot;&gt;book&lt;/a&gt; I have. This subject is also discussed &lt;a href=&quot;http://books.google.com/books?id=BjaHPwAACAAJ&amp;amp;dq=cramer+leadbetter&amp;amp;hl=fr&amp;amp;ei=pGpFTdDQOZChOtzGuJEC&amp;amp;sa=X&amp;amp;oi=book_result&amp;amp;ct=result&amp;amp;resnum=1&amp;amp;ved=0CCkQ6AEwAA&quot; rel=&quot;nofollow&quot;&gt;in this book&lt;/a&gt;. I will not give precise and rigorous definitions, since they involve Hilbert spaces and other graduate mathematics stuff, which I think is not really necessary to get the point across. &lt;/p&gt;&#10;&#10;&lt;p&gt;Each &lt;a href=&quot;http://en.wikipedia.org/wiki/Stationary_process&quot; rel=&quot;nofollow&quot;&gt;covariance-stationary&lt;/a&gt; process $\{X_t,t\in \mathbb{Z}\}$ can be uniquely decomposed into two stationary proceses: $X_t=M_t+N_t$, singular $M_t$ and regular $N_t$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Singular and regular processes are defined via their prediction properties. In stationary process theory the prediction of process $X_t$ at time $t$ is formed from linear span of its history $(X_s,s&amp;lt;t)$. Singular processes are processes for which the prediction error:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$E(\hat{X}_t-X_t)^2$$&lt;/p&gt;&#10;&#10;&lt;p&gt;is zero. Such processes sometimes are called deterministic, and in your context can be also called systematic. The most simple example of such process is $X_t=\eta$ for all $t$ and $\eta$ some random variable. Then the linear prediction of $X_t$ based on its history will always be $\eta$. The error of such prediction as defined above would be zero. &lt;/p&gt;&#10;&#10;&lt;p&gt;Regular stationary processes on the other hand cannot be predicted without error from their history. It can be shown that the stationary process $N_t$ is regular if and only if it admits  $MA(\infty)$ decomposition. This means that there exists &lt;a href=&quot;http://en.wikipedia.org/wiki/White_noise&quot; rel=&quot;nofollow&quot;&gt;white-noise&lt;/a&gt; sequence $(\varepsilon_t)$ such that&lt;/p&gt;&#10;&#10;&lt;p&gt;$$N_t=\sum_{t=0}^{\infty}c_n\varepsilon_{t-n}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where coefficients $c_n$ are such, that the equality holds. These processes sometimes are called non-deterministic, or probably non-systematic in your case.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-01-30T09:21:36.527" Id="6719" LastActivityDate="2011-01-30T13:51:17.730" LastEditDate="2011-01-30T13:51:17.730" LastEditorUserId="2116" OwnerUserId="2116" ParentId="6707" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="6726" AnswerCount="3" Body="&lt;p&gt;An exponential model is a model described by following equation:&lt;br&gt;&#10;$$\hat{y_{i}}=\beta_{0}\cdot e^{\beta_{1}x_{1i}+\ldots+\beta_{k}x_{ki}}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The most common approach used to estimate such model is linearization, which can be done easily by calculating logarithms of both sides. What are the other approaches? I'm especially interested in those which can handle $y_{i}=0$ in some observations.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Update 31.01.2011&lt;/strong&gt;&lt;br&gt;&#10;I'm aware of the fact that this model can't produce zero. I'll elaborate a bit what I'm modeling and why I choose this model. Let's say we want to predict how much money does a client spend in a shop. Of course many clients are just looking and they don't buy anything, that why there are 0. I didn't want to use linear model because it produces a lot of negative values, which doesn't make any sense. The other reason is that this model works really good, much better than the linear. I've used genetic algorithm to estimate those parameters so it wasn't 'scientific' approach. Now I'd like to know how to deal with problem using more scientific methods. It can be also assumed that most, or even all, of the variables are binary variables.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-01-30T11:56:51.913" FavoriteCount="2" Id="6720" LastActivityDate="2011-01-31T14:48:08.047" LastEditDate="2011-01-31T09:50:09.067" LastEditorUserId="1643" OwnerUserId="1643" PostTypeId="1" Score="8" Tags="&lt;estimation&gt;&lt;nonlinear-regression&gt;" Title="Estimation of exponential model" ViewCount="1298" />
  <row Body="&lt;p&gt;There are several issues here.&lt;/p&gt;&#10;&#10;&lt;p&gt;(1) &lt;strong&gt;The model needs to be explicitly probabilistic&lt;/strong&gt;.  In almost all cases there will be &lt;em&gt;no&lt;/em&gt; set of parameters for which the lhs matches the rhs for all your data: there will be residuals.  You need to make assumptions about those residuals.  Do you expect them to be zero on the average?  To be symmetrically distributed?  To be approximately normally distributed?&lt;/p&gt;&#10;&#10;&lt;p&gt;Here are two models that agree with the one specified but allow drastically different residual behavior (and therefore will typically result in different parameter estimates).  You can vary these models by varying assumptions about the joint distribution of the $\epsilon_{i}$:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\text{A:}\ y_{i} =\beta_{0} \exp{\left(\beta_{1}x_{1i}+\ldots+\beta_{k}x_{ki} + \epsilon_{i}\right)}$$&#10;$$\text{B:}\ y_{i} =\beta_{0} \exp{\left(\beta_{1}x_{1i}+\ldots+\beta_{k}x_{ki}\right)} + \epsilon_{i}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;(Note that these are models for the &lt;em&gt;data&lt;/em&gt; $y_i$; there usually is no such thing as an &lt;em&gt;estimated&lt;/em&gt; data value $\hat{y_i}$.)&lt;/p&gt;&#10;&#10;&lt;p&gt;(2) The need to handle zero values for the y's implies &lt;strong&gt;the stated model (A) is both wrong and inadequate&lt;/strong&gt;, because it cannot produce a zero value no matter what the random error equals.  The second model above (B) allows for zero (or even negative) values of y's.  However, one should not choose a model solely on such a basis.  To reiterate #1: it is important to model the errors reasonably well.&lt;/p&gt;&#10;&#10;&lt;p&gt;(3) &lt;strong&gt;Linearization changes the model&lt;/strong&gt;.  Typically, it results in models like (A) but not like (B).  It is used by people who have analyzed their data enough to know this change will not appreciably affect the parameter estimates and by people who are ignorant of what is happening.  (It is hard, many times, to tell the difference.)&lt;/p&gt;&#10;&#10;&lt;p&gt;(4) &lt;strong&gt;A common way to handle the possibility of a zero value&lt;/strong&gt; is to propose that $y$ (or some re-expression thereof, such as the square root) has a strictly positive chance of equally zero.  Mathematically, we are mixing a point mass (a &quot;delta function&quot;) in with some other distribution.  These models look like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\eqalign{
  
&#10;(X^T X)^{-1} = V D^{-2} V^T .
&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Though I haven't checked, hopefully &lt;strong&gt;vcov&lt;/strong&gt; returns $\hat{\sigma}^2 V D^{-2} V^T$.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; You wrote $V D^2 V^T$, which is $X^T X$, but we need the inverse for the variance-covariance matrix. Also note that in $R$, to do this computation you need to do&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;vcov.matrix &amp;lt;- var.est * (v %*% d^(-2) %*% t(v))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;observing that for matrix multiplication we use &lt;code&gt;%*%&lt;/code&gt; instead of just &lt;code&gt;*&lt;/code&gt;. &lt;code&gt;var.est&lt;/code&gt; above is the estimate of the variance of the noise.&lt;/p&gt;&#10;&#10;&lt;p&gt;(Also, I've made the assumptions that $X$ is full-rank and $n \geq p$ throughout. If this is not the case, you'll have to make minor modifications to the above.)&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-01-31T03:04:22.890" Id="6732" LastActivityDate="2011-01-31T03:04:22.890" OwnerUserId="2970" ParentId="6731" PostTypeId="2" Score="13" />
  
  
  <row Body="&lt;p&gt;It's necessary to be a little bit careful about what you mean by &quot;random&quot; and &quot;completely random&quot; here.&lt;/p&gt;&#10;&#10;&lt;p&gt;What you are describing, at least in your first scheme is a set of random draws such that the resulting vector is multinomially distributed with n = 1000 and probabilities $p_1 = p_2 = \cdots = p_n = 1/n$. Even in your first scheme, the resulting variables are &lt;strong&gt;not&lt;/strong&gt; independent. Marginally, they do all have the same distribution and, intuitively (though I didn't check carefully), it seems that the vector of counts would have a property called &lt;em&gt;exchangeability&lt;/em&gt; which, in some sense, is &quot;&lt;em&gt;almost&lt;/em&gt; independent and identically distributed&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the second scheme, where you move each new selection to the end of the list after incrementing it, the reordered variables are not even exchangeable any more, nor do they have the same marginal distribution. Intuitively, this is because the ones being moved to the end of the list will tend to have larger values than ones at the beginning of the list. This is related to, but not quite, what are called &lt;em&gt;order statistics&lt;/em&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would have to think about the last scheme (i.e., reinserting in a random location) more carefully. On the surface, I believe you would essentially end up with the same distribution as in the first case. But, I've not checked that even remotely formally, so take that with a hefty dose of salt. &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-01-31T03:24:39.737" Id="6735" LastActivityDate="2011-01-31T04:16:40.627" LastEditDate="2011-01-31T04:16:40.627" LastEditorUserId="2970" OwnerUserId="2970" ParentId="6733" PostTypeId="2" Score="8" />
  
  <row Body="&lt;p&gt;Although not within the domain of geostatistics, for question #2, I would casually say the most frequent weighting function used in my field (Criminology) would be a a binary weighting scheme. Although I have rarely seen a good theoretical or empirical argument to use one weighting scheme over another (or how one defines a neighbor in a binary weighting scheme either). It may simply be because of historical preference and convienance that such a scheme is typically used.&lt;/p&gt;&#10;&#10;&lt;p&gt;There is a distinction that should be drawn between data driven approaches to constructing spatial weights and the theory based approach to deriving spatial weights. You are currently performing the former, and in this approach you are implicitly treating the estimation of spatial weights as a problem of measurement error, and hence should use techniques to validate your measurements (which is considerably complicated due to the endogeneity of the spatial weights). Using a weighting scheme based on some of the chance variation in the data and using it in subsequent causal models is synonymous with other fallacies related to inference and data snooping. Unfortunately I have no good references of spatial weight models validated in any meaningful way besides the extent of the auto-correlation, which to be frank isn't all that convincing of an empirical argument. Spatial dependence can be the result of either causal processes (i.e. the value at one point in space affects the value at another point in space), or it can be the result of other measurement errors (i.e. the measured support of the data do not match the support of the processes that generate those phenomena). &lt;/p&gt;&#10;&#10;&lt;p&gt;This is oppossed to theory based construction of spatial weights (or &quot;model-driven&quot; in Luc Anselin's terminology), in which one specifies the weight matrix &lt;em&gt;a priori&lt;/em&gt; to estimating a model. I did not read the Fauchald paper you cited, but it appears in the abstract they have plausible theoretical explanations for the observed patterns based on some optimal foraging strategy.&lt;/p&gt;&#10;&#10;&lt;p&gt;For readings I would suggest Luc Anselin's book, &lt;a href=&quot;http://books.google.com/books?id=3dPIXClv4YYC&amp;amp;dq=anselin&amp;amp;source=gbs_navlinks_s&quot; rel=&quot;nofollow&quot;&gt;Spatial Econometrics: Methods and Models&lt;/a&gt; (1988), particularly chapters 2 and 3 will be of most interest. Also as another work with a similar viewpoint to mine (although it will likely be of less interest) is an essay piece by Gary King, &quot;&lt;a href=&quot;http://dx.doi.org/10.1016/0962-6298%2895%2900079-8&quot; rel=&quot;nofollow&quot;&gt;Why context should not count&lt;/a&gt;&quot;. I would also suggest another paper as it appears they had similar goals to yours, and defined the weights for a lattice system based on variogram estimates (&lt;a href=&quot;http://dx.doi.org/10.1080/17538940903253898&quot; rel=&quot;nofollow&quot;&gt;Negreiros, 2010&lt;/a&gt;). &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-01-31T14:37:55.117" Id="6748" LastActivityDate="2011-01-31T14:43:16.333" LastEditDate="2011-01-31T14:43:16.333" LastEditorUserId="1036" OwnerUserId="1036" ParentId="6705" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;This is not a simple calculation in general, though the probability will be almost 1 if $n$ is any substantial size.   &lt;/p&gt;&#10;&#10;&lt;p&gt;To take your example of five people and 10 bars, there are $10^5$, i.e 100,000 possible equally-likely distributions.  All of these will have at least one person alone, except the 10 cases where all five are in a single bar or the 900 cases where three are in a particular bar and two in another.  That leaves 99090 distributions which satisfy your condition, more than 99%.   &lt;/p&gt;&#10;&#10;&lt;p&gt;Note that 99090 is not divisible by $\binom{5 + 10 - 1}{5} = 2002$.&lt;/p&gt;&#10;&#10;&lt;p&gt;For $n=1$ to $6$, the fractions seem to be $\frac{2}{2},$ $\frac{12}{16},$ $\frac{210}{216},$ $\frac{3920}{4096},$ $\frac{99090}{100000}$ and $\frac{2962872}{2985984}$, where the denominator in each case is $(2n)^n$. I would expect the probabilities then to keep rising as $n$ increases beyond 6.    &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-02-01T00:44:41.830" Id="6766" LastActivityDate="2011-02-01T14:56:57.243" LastEditDate="2011-02-01T14:56:57.243" LastEditorUserId="919" OwnerUserId="2958" ParentId="6762" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;You'll be hard-pressed to show an 8-D representation of sets of similar batches using anything but a dreary table.  But, along the lines of Bill's point I think, if you're willing to select the 3 most interesting or most discriminating dimensions, you could show where each batch falls within a cube defined by those 3.  Perhaps better is to first do a cluster analysis and then show where each cluster falls within such a cube.  Some software (such as SPSS) will allow you to assign each batch a color or a symbol according to its cluster, and you could draw spikes from each point to its cluster's centroid to create a nice, vivid effect.   &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-02-01T01:03:26.380" Id="6768" LastActivityDate="2011-02-01T01:03:26.380" OwnerUserId="2669" ParentId="6763" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;I'm not pretty sure, but i think that if you more variable than observation  the Partial Least Square (PLS) regression is the right tool for you, and it's commonly used by chemometrician.&lt;/p&gt;&#10;&#10;&lt;p&gt;As you know this site is R biased and i use R too, so i'll recommend to take a look at pls package package which can also do principal component regression, and it also have a biplot.mvr (S3 method for biplot) function to make a biplot.&lt;/p&gt;&#10;&#10;&lt;p&gt;A good starting point can be &lt;a href=&quot;http://mevik.net/work/software/pls.html&quot; rel=&quot;nofollow&quot;&gt;http://mevik.net/work/software/pls.html&lt;/a&gt; , is the website of package's author and have some useful link like the journal of statistical software article about the R package pls.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-02-01T07:30:18.610" Id="6774" LastActivityDate="2011-02-01T07:55:29.913" LastEditDate="2011-02-01T07:55:29.913" LastEditorUserId="2028" OwnerUserId="2028" ParentId="6763" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="6781" AnswerCount="2" Body="&lt;p&gt;I have several query frequencies, and I need to estimate the coefficient of Zipf's law. These are the top frequencies: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;26486&#10;12053&#10;5052&#10;3033&#10;2536&#10;2391&#10;1444&#10;1220&#10;1152&#10;1039&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2011-02-01T13:24:57.997" FavoriteCount="8" Id="6780" LastActivityDate="2015-01-19T17:58:47.520" LastEditDate="2011-03-15T19:04:43.443" LastEditorUserId="8" OwnerUserId="2998" PostTypeId="1" Score="15" Tags="&lt;distributions&gt;&lt;estimation&gt;&lt;pareto-distribution&gt;&lt;zipf&gt;" Title="How to calculate Zipf's law coefficient from a set of top frequencies?" ViewCount="5125" />
  
  
  <row Body="&lt;p&gt;I would recommend looking at the &lt;a href=&quot;http://www.bioconductor.org/packages/2.3/bioc/html/snpMatrix.html&quot; rel=&quot;nofollow&quot;&gt;snpMatrix&lt;/a&gt; R package. Within the &lt;code&gt;snp.lhs.tests()&lt;/code&gt; function, height will be the phenotype (or outcome), your SNP data will be stored in a vector (&lt;code&gt;snp.data&lt;/code&gt;), and your gene expression levels will enter the model as covariates. I never used this kind of covariates (in the GWAS I am familiar with we adjust for population stratification and subject-specific covariates), so I can't give an example of use. Just give it a try to see if it suits your needs.&lt;/p&gt;&#10;&#10;&lt;p&gt;Otherwise, a larger modeling framework is available in the &lt;a href=&quot;http://www.bioconductor.org/packages/2.2/bioc/html/GGtools.html&quot; rel=&quot;nofollow&quot;&gt;GGtools&lt;/a&gt; package, from the &lt;a href=&quot;http://www.bioconductor.org&quot; rel=&quot;nofollow&quot;&gt;Bioconductor&lt;/a&gt; project too.  It was developed by Vince J. Carey for dealing specifically with SNP and gene expression data, in GWAS or eQTL studies. There are numerous examples of use on the web (look up for tutorial by VJ Carey with &lt;code&gt;GGtools&lt;/code&gt;). However, it's mainly to model gene expression as a function of observed genotypes (chromosome- or genome-wide).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Caution.&lt;/strong&gt; Please note that &lt;code&gt;GGtools&lt;/code&gt; package has a lot of dependencies, in particular &lt;code&gt;GGBase&lt;/code&gt; (which provides specific environments for holding genetic data) and &lt;code&gt;snpMatrix&lt;/code&gt; (which an provides an efficient storage of SNP data). You will also need to install the base packages from the Bioconductor repository (this is not part of CRAN); detailed instructions are available &lt;a href=&quot;http://www.bioconductor.org/install/index.html&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-02-01T22:15:50.637" Id="6790" LastActivityDate="2011-02-01T22:15:50.637" OwnerUserId="930" ParentId="6787" PostTypeId="2" Score="4" />
  
&#10;\Pr(\text{at least one actor chooses a unique outcome}) \geq 1 - (e/4)^{n/2} .
&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, if no more than $n/2$ bins are occupied, then there &lt;em&gt;must&lt;/em&gt; be some subset of $3n/2$ bins that are &lt;em&gt;all&lt;/em&gt; empty. There are ${2n \choose 3n/2} = {2n \choose n/2}$ such subsets, and so by the union bound, we get&lt;/p&gt;&#10;&#10;&lt;p&gt;$$
  <row AcceptedAnswerId="6933" AnswerCount="1" Body="&lt;p&gt;I'm trying to solve a problem for least angle regression (LAR). This is a problem &lt;strong&gt;3.23&lt;/strong&gt; on page &lt;strong&gt;97&lt;/strong&gt; of &lt;a href=&quot;http://www.stanford.edu/~hastie/local.ftp/Springer/ESLII_print5.pdf#page=115&quot; rel=&quot;nofollow&quot;&gt;Hastie et al., Elements of Statistical Learning, 2nd. ed. (5th printing)&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Consider a regression problem with all variables and response having mean zero and standard deviation one.  Suppose also that each variable has identical absolute correlation with the response:&lt;/p&gt;&#10;&#10;&lt;p&gt;$
&#10;\frac{1}{N} | \left \langle \bf{x}_j, \bf{y}-u(\alpha) \right \rangle | = (1 - \alpha) \lambda, j = 1, ..., p
  
  
  
  <row AcceptedAnswerId="6825" AnswerCount="2" Body="&lt;p&gt;My intuition says that the third equation must be &quot;the length of the gradient squared less than epsilon&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;$x_{k+1} = x_k - f(x_k)$&lt;br&gt;&#10;$x_{k+1} = x_k + 1$&lt;br&gt;&#10;$|f(x_k)|^2 &amp;lt; \epsilon$  &lt;/p&gt;&#10;&#10;&lt;p&gt;However, I am not sure whether it is the standard form.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;How would you write the standard form of the gradient method, and particularly its ending rule?&lt;/strong&gt;&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2011-02-02T16:20:15.653" Id="6817" LastActivityDate="2011-02-03T05:07:19.070" LastEditDate="2011-02-03T05:07:19.070" LastEditorUserId="919" OwnerUserId="3017" PostTypeId="1" Score="2" Tags="&lt;optimization&gt;" Title="Optimizing the ending rule of gradient method" ViewCount="140" />
  
  <row AcceptedAnswerId="6826" AnswerCount="2" Body="&lt;p&gt;Can an effect size be calculated for an interaction effect in general and more specifically using the F-statistic and its associated degrees of freedom?  If yes, should this be done and what is the appropriate interpretation of the effect size given the nature of interaction effects?  Initially, I assumed the answer is &quot;no.&quot; However, a quick Google search turned up statements about computing effect sizes for interaction terms.  Any assistance in clarifying this issue will be greatly appreciated!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-02-02T20:21:45.820" FavoriteCount="1" Id="6824" LastActivityDate="2011-02-03T00:02:26.337" OwnerDisplayName="user3022" PostTypeId="1" Score="7" Tags="&lt;interaction&gt;" Title="Calculating and interpreting effect sizes for interaction terms" ViewCount="3157" />
  <row Body="&lt;p&gt;Interpreting your function $f(x)$ as a (scaled) version of the gradient, your termination rule is equivalent to $|f(x)| &amp;lt; \sqrt{\epsilon}$, &lt;em&gt;i.e.&lt;/em&gt; &quot;terminate when you've taken a step that is too small.&quot; This seems perfectly reasonable. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-02-02T20:59:15.827" Id="6825" LastActivityDate="2011-02-02T20:59:15.827" OwnerUserId="795" ParentId="6817" PostTypeId="2" Score="3" />
  
  
  <row AnswerCount="2" Body="&lt;p&gt;What are the best quantitative models for trend detection?&#10;I.e. market trend.&lt;/p&gt;&#10;" ClosedDate="2012-08-14T12:51:30.177" CommentCount="3" CreationDate="2011-02-03T04:02:57.213" Id="6835" LastActivityDate="2011-03-03T15:32:02.253" LastEditDate="2011-02-03T10:40:11.013" LastEditorUserId="88" OwnerDisplayName="user3027" PostTypeId="1" Score="0" Tags="&lt;time-series&gt;&lt;trend&gt;" Title="Trend detection quantitative models" ViewCount="781" />
  <row AnswerCount="0" Body="&lt;p&gt;I have fitted a dynamic panel data model with Arellano-Bond estimator in gretl, here is the output:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Model 5: 2-step dynamic panel, using 2332 observations&#10;Included 106 cross-sectional units&#10;H-matrix as per Ox/DPD&#10;Dependent variable: trvr&#10;&#10;              coefficient   std. error     z       p-value &#10;  ---------------------------------------------------------&#10;  Dtrvr(-1)    0.895381     0.0248490    36.03    2.55e-284 ***&#10;  const        0.0230952    0.00226823   10.18    2.39e-024 ***&#10;  x1          -0.0263556    0.00836633   -3.150   0.0016    ***&#10;  x2           0.127888     0.0171532     7.456   8.94e-014 ***&#10;&#10;Sum squared resid    605.9396   S.E. of regression   0.510180&#10;&#10;Number of instruments = 256&#10;Test for AR(1) errors: z = -4.29161 [0.0000]&#10;Test for AR(2) errors: z = 1.62503 [0.1042]&#10;Sargan over-identification test: Chi-square(252) = 105.139 [1.0000]&#10;Wald (joint) test: Chi-square(3) = 2333.35 [0.0000]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I have 2 questions about the results:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;How do I assess the fit?&lt;/li&gt;&#10;&lt;li&gt;How can I simulate from the model?&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Regards&lt;/p&gt;&#10;" CommentCount="10" CreationDate="2011-02-03T10:18:25.350" Id="6841" LastActivityDate="2011-02-03T10:18:25.350" OwnerUserId="1443" PostTypeId="1" Score="4" Tags="&lt;panel-data&gt;&lt;simulation&gt;" Title="Simulate Arellano-Bond" ViewCount="695" />
  
  <row Body="&lt;p&gt;The following doesn't answer exactly the question. It may give you some ideas, though. It's something I recently did in order to assess the fit of several regression models using one to four independent variables (the dependent variable was in the first column of the df1 dataframe). &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# create the combinations of the 4 independent variables&#10;library(foreach)&#10;xcomb &amp;lt;- foreach(i=1:4, .combine=c) %do% {combn(names(df1)[-1], i, simplify=FALSE) }&#10;&#10;# create formulas&#10;formlist &amp;lt;- lapply(xcomb, function(l) formula(paste(names(df1)[1], paste(l, collapse=&quot;+&quot;), sep=&quot;~&quot;)))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The contents of as.character(formlist) was&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; [1] &quot;price ~ sqft&quot;                     &quot;price ~ age&quot;                     &#10; [3] &quot;price ~ feats&quot;                    &quot;price ~ tax&quot;                     &#10; [5] &quot;price ~ sqft + age&quot;               &quot;price ~ sqft + feats&quot;            &#10; [7] &quot;price ~ sqft + tax&quot;               &quot;price ~ age + feats&quot;             &#10; [9] &quot;price ~ age + tax&quot;                &quot;price ~ feats + tax&quot;             &#10;[11] &quot;price ~ sqft + age + feats&quot;       &quot;price ~ sqft + age + tax&quot;        &#10;[13] &quot;price ~ sqft + feats + tax&quot;       &quot;price ~ age + feats + tax&quot;       &#10;[15] &quot;price ~ sqft + age + feats + tax&quot;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Then I collected some useful indices &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# R squared&#10;models.r.sq &amp;lt;- sapply(formlist, function(i) summary(lm(i))$r.squared)&#10;# adjusted R squared&#10;models.adj.r.sq &amp;lt;- sapply(formlist, function(i) summary(lm(i))$adj.r.squared)&#10;# MSEp&#10;models.MSEp &amp;lt;- sapply(formlist, function(i) anova(lm(i))['Mean Sq']['Residuals',])&#10;&#10;# Full model MSE&#10;MSE &amp;lt;- anova(lm(formlist[[length(formlist)]]))['Mean Sq']['Residuals',]&#10;&#10;# Mallow's Cp&#10;models.Cp &amp;lt;- sapply(formlist, function(i) {&#10;SSEp &amp;lt;- anova(lm(i))['Sum Sq']['Residuals',]&#10;mod.mat &amp;lt;- model.matrix(lm(i))&#10;n &amp;lt;- dim(mod.mat)[1]&#10;p &amp;lt;- dim(mod.mat)[2]&#10;c(p,SSEp / MSE - (n - 2*p))&#10;})&#10;&#10;df.model.eval &amp;lt;- data.frame(model=as.character(formlist), p=models.Cp[1,],&#10;r.sq=models.r.sq, adj.r.sq=models.adj.r.sq, MSEp=models.MSEp, Cp=models.Cp[2,])&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The final dataframe was&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;                      model p       r.sq   adj.r.sq      MSEp         Cp&#10;1                price~sqft 2 0.71390776 0.71139818  42044.46  49.260620&#10;2                 price~age 2 0.02847477 0.01352823 162541.84 292.462049&#10;3               price~feats 2 0.17858447 0.17137907 120716.21 351.004441&#10;4                 price~tax 2 0.76641940 0.76417343  35035.94  20.591913&#10;5            price~sqft+age 3 0.80348960 0.79734865  33391.05  10.899307&#10;6          price~sqft+feats 3 0.72245824 0.71754599  41148.82  46.441002&#10;7            price~sqft+tax 3 0.79837622 0.79446120  30536.19   5.819766&#10;8           price~age+feats 3 0.16146638 0.13526220 142483.62 245.803026&#10;9             price~age+tax 3 0.77886989 0.77173666  37884.71  20.026075&#10;10          price~feats+tax 3 0.76941242 0.76493500  34922.80  21.021060&#10;11     price~sqft+age+feats 4 0.80454221 0.79523470  33739.36  12.514175&#10;12       price~sqft+age+tax 4 0.82977846 0.82140691  29640.97   3.832692&#10;13     price~sqft+feats+tax 4 0.80068220 0.79481991  30482.90   6.609502&#10;14      price~age+feats+tax 4 0.79186713 0.78163109  36242.54  17.381201&#10;15 price~sqft+age+feats+tax 5 0.83210849 0.82091573  29722.50   5.000000&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Finally, a Cp plot (using library wle)&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/JbQyk.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-02-03T20:10:28.257" Id="6862" LastActivityDate="2011-02-03T20:23:39.190" LastEditDate="2011-02-03T20:23:39.190" LastEditorUserId="339" OwnerUserId="339" ParentId="6856" PostTypeId="2" Score="9" />
  <row Body="&lt;p&gt;Kolmogorov-Smirnov 3d test is when you have the sample of 3d vectors. The idea is to compare the sample distribution to a model distribution. So the main question is how does model distribution looks like. &lt;/p&gt;&#10;&#10;&lt;p&gt;Now KS-test compares the cumulative distributions of sample distribution and the model distribution. The 3d test does the same. If you worry about discrete-values look at the bottom of page 6 of your reference. The authors test the behaviour of their statistic when the model distribution (they call it parent distribution) is constant in certain cubes. This means that the the testable distribution is discrete. So the answer to your question seems to be yes, discreteness is not a problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;To make sure you can always do some Monte-Carlo simulations where you control the model distribution and choose it to be similar to the one you want to test, and see how the statistic performs.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-02-03T20:34:22.620" Id="6863" LastActivityDate="2011-02-03T20:34:22.620" OwnerUserId="2116" ParentId="6851" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;One obvious point that everyone's overlooked:  With ANOVA you're testing the null that the mean is identical regardless of the values of your explanatory variables.  With a T-Test you can also test the one-sided case, that the mean is specifically greater given one value of your explanatory variable than given the other.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-02-04T01:29:59.663" Id="6866" LastActivityDate="2011-02-04T01:29:59.663" OwnerUserId="1347" ParentId="1637" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="6877" AnswerCount="3" Body="&lt;p&gt;I've recently been looking for top-of-the-line statisticians in a recruiting process for our company. Myself, I'm a Physics Engineering major. I gather that great mathematical statisticians have studied a bit different courses, and much more in depth. &lt;/p&gt;&#10;&#10;&lt;p&gt;When evaluating a candidate, are courses a good indicators of this person being excellent?&lt;/p&gt;&#10;&#10;&lt;p&gt;Preferably we're talking graduate or post-graduate level.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;We're looking to fill roles of data miners, statistical modeling and data visualization. Thanks Chris, for the suggestion to clarify.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2011-02-04T13:09:58.190" FavoriteCount="4" Id="6876" LastActivityDate="2012-10-09T13:29:41.280" LastEditDate="2012-10-09T12:53:15.973" LastEditorUserId="615" OwnerUserId="3048" PostTypeId="1" Score="6" Tags="&lt;courses&gt;" Title="What university level statistics courses are considered advanced/hard?" ViewCount="925" />
  
  <row Body="&lt;p&gt;The formulas are straightforward but they are not as simple as intimated in the question.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let $Y$ be the previous EWMA and let $X = x_n$, which is presumed independent of $Y$.  By &lt;a href=&quot;http://en.wikipedia.org/wiki/Moving_average&quot; rel=&quot;nofollow&quot;&gt;definition&lt;/a&gt;, the new weighted average is $Z = \alpha X + (1 - \alpha)Y$ for a constant value $\alpha$.  For notational convenience, set $\beta = 1-\alpha$.  Let $F$ denote the CDF of a random variable and $\phi$ denote its &lt;a href=&quot;http://en.wikipedia.org/wiki/Moment-generating_function&quot; rel=&quot;nofollow&quot;&gt;moment generating function&lt;/a&gt;, so that&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\phi_X(t) = \mathbb{E}_F[\exp(t X)] = \int_\mathbb{R}{\exp(t x) dF_X(x)}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;With &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0340614307&quot; rel=&quot;nofollow&quot;&gt;Kendall and Stuart&lt;/a&gt;, let $\mu_k^{'}(Z)$ denote the non-central moment of order $k$ for the random variable $Z$; that is, $\mu_k^{'}(Z) = \mathbb{E}[Z^k]$.  The &lt;a href=&quot;http://en.wikipedia.org/wiki/Skewness&quot; rel=&quot;nofollow&quot;&gt;skewness&lt;/a&gt; and &lt;a href=&quot;http://en.wikipedia.org/wiki/Kurtosis&quot; rel=&quot;nofollow&quot;&gt;kurtosis&lt;/a&gt; are expressible in terms of the $\mu_k^{'}$ for $k = 1,2,3,4$; for example, the skewness is defined as $\mu_3 / \mu_2^{3/2}$ where &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\mu_3 = \mu_3^{'}  - 3 \mu_2^{'}\mu_1^{'} + 2{\mu_1^{'}}^3 \text{ and }\mu_2 = \mu_2^{'} - {\mu_1^{'}}^2$$ &lt;/p&gt;&#10;&#10;&lt;p&gt;are the third and second central moments, respectively.&lt;/p&gt;&#10;&#10;&lt;p&gt;By standard elementary results, &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\eqalign{&#10;&amp;amp;1 + \mu_1^{'}(Z) t + \frac{1}{2!} \mu_2^{'}(Z) t^2 + \frac{1}{3!} \mu_3^{'}(Z) t^3 + \frac{1}{4!} \mu_4^{'}(Z) t^4 +O(t^5) \cr&#10;&amp;amp;= \phi_Z(t) \cr&#10;&amp;amp;= \phi_{\alpha X}(t) \phi_{\beta Y}(t) \cr&#10;&amp;amp;= \phi_X(\alpha t) \phi_Y(\beta t) \cr&#10;&amp;amp;= (1 + \mu_1^{'}(X) \alpha t + \frac{1}{2!} \mu_2^{'}(X) \alpha^2 t^2 + \cdots)&#10;(1 + \mu_1^{'}(Y) \beta t + \frac{1}{2!} \mu_2^{'}(Y) \beta^2 t^2 + \cdots).&#10;}&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;To obtain the desired non-central moments, multiply the latter power series through fourth order in $t$ and equate the result term-by-term with the terms in $\phi_Z(t)$.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-02-04T15:38:21.347" Id="6884" LastActivityDate="2013-10-17T13:28:59.293" LastEditDate="2013-10-17T13:28:59.293" LastEditorUserId="919" OwnerUserId="919" ParentId="6874" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;The most dangerous trap I encountered when working on a predictive model is not to reserve a test dataset early on so as to dedicate it to the &quot;final&quot; performance evaluation.&lt;/p&gt;&#10;&#10;&lt;p&gt;It's really easy to overestimate the predictive accuracy of your model if you have a chance to somehow use the testing data when tweaking the parameters, selecting the prior, selecting the learning algorithm stopping criterion...&lt;/p&gt;&#10;&#10;&lt;p&gt;To avoid this issue, before starting your work on a new dataset you should split your data as:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;development set&lt;/li&gt;&#10;&lt;li&gt;evaluation set&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Then split your development set as a &quot;training development set&quot; and &quot;testing development set&quot; where you use the training development set to train various models with different parameters and select the bests according to there performance on the testing development set. You can also do grid search with cross validation but only on the development set. Never use the evaluation set while model selection is not 100% done.&lt;/p&gt;&#10;&#10;&lt;p&gt;Once your are confident with the model selection and parameters, perform a 10 folds cross-validation on the evaluation set to have an idea of the &quot;real&quot; predictive accuracy of the selected model.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also if your data is temporal, it is best to choose the development / evaluation split on a time code: &quot;It's hard to make predictions - especially about the future.&quot;&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-02-04T15:38:59.070" Id="6885" LastActivityDate="2012-04-08T18:20:51.200" LastEditDate="2012-04-08T18:20:51.200" LastEditorUserId="2150" OwnerUserId="2150" ParentId="4551" PostTypeId="2" Score="45" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I have a series of measurements in which something is sampled and placed into either category 1 or category 2.  I have 3x2x3 factors for each set of measurements, and each set of measurements is done independently 3 times.  &lt;/p&gt;&#10;&#10;&lt;p&gt;What is a good statistical test to look examine the combined effects of each factor, both independently and interactively with each other?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-02-04T18:50:12.683" Id="6891" LastActivityDate="2011-03-21T04:22:44.343" LastEditDate="2011-02-04T19:32:26.510" LastEditorUserId="930" OwnerUserId="1327" PostTypeId="1" Score="2" Tags="&lt;anova&gt;&lt;repeated-measures&gt;&lt;statistical-significance&gt;" Title="What is a good statistical test for independent replicates?" ViewCount="435" />
  <row Body="&lt;p&gt;I don't know a specific function for that. The ones I used generally take raw data or a distance matrix. However, it would not be very difficult to hack already existing code, without knowing more than basic R. Look at the source code for the &lt;code&gt;cim()&lt;/code&gt; function in the &lt;a href=&quot;http://cran.r-project.org/web/packages/mixOmics/index.html&quot;&gt;mixOmics&lt;/a&gt; package for example (I choose this one because source code is very easy to read; you will find other functions on the &lt;a href=&quot;http://www.bioconductor.org&quot;&gt;Bioconductor&lt;/a&gt; project). The interesting parts of the code are l. 92-113, where they assign the result of HC to &lt;code&gt;ddc&lt;/code&gt;, and around l. 193-246 where they devised the plotting regions (you should input the values of your distance matrix in place of mat when they call &lt;code&gt;image()&lt;/code&gt;). HTH&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;A recent Google search on a related subject lead me to &lt;code&gt;dendrogramGrob()&lt;/code&gt; from the &lt;a href=&quot;http://latticeextra.r-forge.r-project.org/&quot;&gt;latticeExtra&lt;/a&gt; package. Assuming you already have your sorted dendrogram object, you can skip the first lines of the example code from the on-line help and get something like this (here, with the &lt;code&gt;mtcars&lt;/code&gt; dataset):&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/tFFAS.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-02-04T19:29:58.010" Id="6893" LastActivityDate="2011-04-30T08:25:17.213" LastEditDate="2011-04-30T08:25:17.213" LastEditorUserId="930" OwnerUserId="930" ParentId="6890" PostTypeId="2" Score="9" />
  
  <row Body="&lt;p&gt;It seems that you are fitting &lt;a href=&quot;http://en.wikipedia.org/wiki/Simultaneous_equations_model&quot; rel=&quot;nofollow&quot;&gt;simultaneous equation model&lt;/a&gt; in &lt;a href=&quot;http://en.wikipedia.org/wiki/Panel_data&quot; rel=&quot;nofollow&quot;&gt;panel data&lt;/a&gt; setting. In this case using different dummies for different equations is entirely appropriate from statistical point of view. The justification should come from economic model. &lt;/p&gt;&#10;&#10;&lt;p&gt;The thing you should worry most is finding appropriate instrumental variables for your endogenous variables. Simultaneous equation estimates for panel data are covered &lt;a href=&quot;http://books.google.com/books?id=yTVSqmufge8C&amp;amp;lpg=PP1&amp;amp;dq=baltagi%20panel%20data&amp;amp;hl=fr&amp;amp;pg=PA113#v=onepage&amp;amp;q&amp;amp;f=false&quot; rel=&quot;nofollow&quot;&gt;in Baltagi book &quot;Econometric analysis of panel data&quot;&lt;/a&gt;. You can also use Arrelano-Bond type estimates which are covered in the same book.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-02-04T20:01:34.703" Id="6895" LastActivityDate="2011-02-04T20:01:34.703" OwnerUserId="2116" ParentId="3176" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;I haven't seen the &lt;a href=&quot;http://scikit-learn.org/&quot; rel=&quot;nofollow&quot;&gt;scikit-learn&lt;/a&gt; explicitly mentioned in the answers above. It's a Python package for machine learning in Python. It's fairly young but growing extremely rapidly (disclaimer: I am a scikit-learn developer). It's goals are to provide standard machine learning algorithmic tools in a unified interface with a focus on speed, and usability. As far as I know, you cannot find anything similar in Matlab. It's strong points are:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;A &lt;a href=&quot;http://scikit-learn.org/user_guide.html&quot; rel=&quot;nofollow&quot;&gt;detailed documentation&lt;/a&gt;, with &lt;a href=&quot;http://scikit-learn.org/stable/auto_examples/index.html&quot; rel=&quot;nofollow&quot;&gt;many examples&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;High quality standard &lt;a href=&quot;http://scikit-learn.org/stable/supervised_learning.html&quot; rel=&quot;nofollow&quot;&gt;supervised learning&lt;/a&gt; (regression/classification) tools. Specifically: &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;very versatile &lt;a href=&quot;http://scikit-learn.org/stable/modules/svm.html&quot; rel=&quot;nofollow&quot;&gt;SVM&lt;/a&gt; (based on libsvm, but with integration of external patches, and a lot of work on the Python binding)&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://scikit-learn.org/stable/modules/linear_model.html&quot; rel=&quot;nofollow&quot;&gt;penalized linear models&lt;/a&gt; (&lt;a href=&quot;http://scikit-learn.org/stable/modules/linear_model.html#lasso&quot; rel=&quot;nofollow&quot;&gt;Lasso&lt;/a&gt;, &lt;a href=&quot;http://scikit-learn.org/stable/auto_examples/logistic_l1_l2_coef.html&quot; rel=&quot;nofollow&quot;&gt;sparse logistic regression&lt;/a&gt;...) with efficient implementations.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The ability to perform &lt;a href=&quot;http://scikit-learn.org/stable/modules/grid_search.html&quot; rel=&quot;nofollow&quot;&gt;model selection&lt;/a&gt; by cross-validation using multiple CPUs&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://scikit-learn.org/stable/unsupervised_learning.html&quot; rel=&quot;nofollow&quot;&gt;Unsupervised learning&lt;/a&gt; to explore the data or do a first dimensionality reduction, that can easily be chained to supervised learning.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Open source, BSD licensed. If you are not in a purely academic environment (I am in what would be a national lab in the state) this matters a lot as Matlab costs are then very high, and you might be thinking of deriving products from your work.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Matlab is a great tool, but in my own work, scipy+scikit-learn is starting to give me an edge on Matlab because Python does a better job with memory due to its view mechanism (and I have big data), and because the scikit-learn enables me to very easily compare different approaches.&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2011-02-05T10:49:45.910" CreationDate="2011-02-05T10:49:45.910" Id="6902" LastActivityDate="2014-08-19T20:55:48.897" LastEditDate="2014-08-19T20:55:48.897" LastEditorUserId="17230" OwnerUserId="1265" ParentId="1595" PostTypeId="2" Score="25" />
  
  
  <row Body="&lt;p&gt;Maybe stepwise regression and other forms of testing after model selection.&lt;/p&gt;&#10;&#10;&lt;p&gt;Selecting independent variables for modelling without having any &lt;em&gt;a priori&lt;/em&gt; hypothesis behind the existing relationships can lead to logical fallacies or spurious correlations, among other mistakes.&lt;/p&gt;&#10;&#10;&lt;p&gt;Useful references (from a biological/biostatistical perspective):&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Kozak, M., &amp;amp; Azevedo, R. (2011). Does using stepwise variable selection to build sequential path analysis models make sense? Physiologia plantarum, 141(3), 197–200. doi:10.1111/j.1399-3054.2010.01431.x&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Whittingham, M. J., Stephens, P., Bradbury, R. B., &amp;amp; Freckleton, R. P. (2006). Why do we still use stepwise modelling in ecology and behaviour? The Journal of animal ecology, 75(5), 1182–9. doi:10.1111/j.1365-2656.2006.01141.x&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Frank Harrell, &lt;em&gt;Regression Modeling Strategies&lt;/em&gt;, Springer 2001.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="0" CreationDate="2011-02-05T15:22:55.597" Id="6911" LastActivityDate="2014-12-09T12:51:56.987" LastEditDate="2014-12-09T12:51:56.987" LastEditorUserId="2126" OwnerUserId="2126" ParentId="4551" PostTypeId="2" Score="13" />
  <row Body="&lt;p&gt;Because your pairwise $t$-test above is not adjusted for age, and age explains a lot of the variance in StressReduction.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-02-05T16:31:07.540" Id="6914" LastActivityDate="2011-02-05T16:31:07.540" OwnerUserId="449" ParentId="6912" PostTypeId="2" Score="9" />
  
  <row Body="&lt;p&gt;Maindonald describes a sequential method based on &lt;a href=&quot;http://en.wikipedia.org/wiki/Givens_rotation&quot;&gt;Givens rotations&lt;/a&gt;.  (A Givens rotation is an orthogonal transformation of two vectors that zeros out a given entry in one of the vectors.)  At the previous step you have decomposed the &lt;a href=&quot;http://en.wikipedia.org/wiki/Design_matrix&quot;&gt;design matrix&lt;/a&gt; $\mathbf{X}$ into a triangular matrix $\mathbf{T}$ via an orthogonal transformation $\mathbf{Q}$ so that $\mathbf{Q}\mathbf{X} = (\mathbf{T}, \mathbf{0})'$.  (It's fast and easy to get the regression results from a triangular matrix.)  Upon adjoining a new row $v$ below $\mathbf{X}$, you effectively extend $(\mathbf{T}, \mathbf{0})'$ by a nonzero row, too, say $t$.  The task is to zero out this row while keeping the entries in the position of $\mathbf{T}$ diagonal.  A sequence of Givens rotations does this: the rotation with the first row of $\mathbf{T}$ zeros the first element of $t$; then the rotation with the second row of $\mathbf{T}$ zeros the second element, and so on.  The effect is to premultiply $\mathbf{Q}$ by a series of rotations, which does not change its orthogonality.&lt;/p&gt;&#10;&#10;&lt;p&gt;When the design matrix has $p+1$ columns (which is the case when regressing on $p$ variables plus a constant), the number of rotations needed does not exceed $p+1$ and each rotation changes two $p+1$-vectors.    The storage needed for $\mathbf{T}$ is $O((p+1)^2)$.  Thus this algorithm has a computational cost of $O((p+1)^2)$ in both time and space.&lt;/p&gt;&#10;&#10;&lt;p&gt;A similar approach lets you determine the effect on regression of deleting a row.  Maindonald gives formulas; so do &lt;a href=&quot;http://books.google.com/books?id=GECBEUJVNe0C&amp;amp;printsec=frontcover&amp;amp;dq=Belsley,+Kuh,+%26+Welsch&amp;amp;source=bl&amp;amp;ots=b6k5lVb7E3&amp;amp;sig=vOuq7Ehg3OrO01CepQ8p-DZ1Bww&amp;amp;hl=en&amp;amp;ei=m7hNTeyfNsSAlAeY3eXcDw&amp;amp;sa=X&amp;amp;oi=book_result&amp;amp;ct=result&amp;amp;resnum=5&amp;amp;ved=0CDMQ6AEwBA#v=onepage&amp;amp;q&amp;amp;f=false&quot;&gt;Belsley, Kuh, &amp;amp; Welsh&lt;/a&gt;.  Thus, if you are looking for a moving window for regression, you can retain data for the window within a circular buffer, adjoining the new datum and dropping the old one with each update.  This doubles the update time and requires additional $O(k (p+1))$ storage for a window of width $k$.  It appears that $1/k$ would be the analog of the influence parameter.&lt;/p&gt;&#10;&#10;&lt;p&gt;For exponential decay, I think (speculatively) that you could adapt this approach to weighted least squares, giving each new value a weight greater than 1.  There shouldn't be any need to maintain a buffer of previous values or delete any old data.&lt;/p&gt;&#10;&#10;&lt;h3&gt;References&lt;/h3&gt;&#10;&#10;&lt;p&gt;J. H. Maindonald, &lt;em&gt;Statistical Computation.&lt;/em&gt;  J. Wiley &amp;amp; Sons, 1984.  Chapter 4.&lt;/p&gt;&#10;&#10;&lt;p&gt;D. A. Belsley, E. Kuh, R. E. Welsch, &lt;em&gt;Regression Diagnostics: Identifying Influential Data and Sources of Collinearity.&lt;/em&gt;  J. Wiley &amp;amp; Sons, 1980. &lt;/p&gt;&#10;" CommentCount="15" CreationDate="2011-02-05T19:09:20.667" Id="6923" LastActivityDate="2012-11-19T23:16:15.307" LastEditDate="2012-11-19T23:16:15.307" LastEditorUserId="919" OwnerUserId="919" ParentId="6920" PostTypeId="2" Score="16" />
  
  <row Body="&lt;p&gt;The problem is that you haven't really defined what it means to have a good or fair rating. You suggest in a comment on @Kevin's answer that you don't like it if one bad review takes down an item. But comparing two items where one has a &quot;perfect record&quot; and the other has one bad review, maybe that difference should be reflected.&lt;/p&gt;&#10;&#10;&lt;p&gt;There's a whole (high-dimensional) continuum between median and mean. You can order the votes by value, then take a weighted average with the weights depending on the position in that order. The mean corresponds to all weights being equal, the median corresponds to only one or two entries in the middle getting nonzero weight, a trimmed average corresponds to giving all except the first and last couple the same weight, but you could also decide to weight the $k$th out of $n$ samples with weight $\frac{1}{1 + (2 k - 1 - n)^2}$ or $\exp(-\frac{(2k - 1 - n)^2}{n^2})$, to throw something random in there. Maybe such a weighted average where the outliers get less weight, but still a nonzero amount, could combine good properties of median and mean?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-02-05T22:13:43.713" Id="6928" LastActivityDate="2011-02-05T22:13:43.713" OwnerUserId="2898" ParentId="6913" PostTypeId="2" Score="21" />
  
&#10;\frac{1}{N} | \langle x_j, y - u(\alpha) \rangle | = (1-\alpha) \lambda ,
  <row Body="&lt;p&gt;It should help to specify starting values, though it's hard to know how much. As you're doing simulation and bootstrapping, you should know the 'true' values or the un-bootstrapped estimates or both. Try using those in the &lt;code&gt;start =&lt;/code&gt; option of &lt;code&gt;glmer&lt;/code&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;You could also consider looking into the whether the tolerances for declaring convergence are stricter than you need. I'm not clear how to alter them from the &lt;code&gt;lme4&lt;/code&gt; documentation though.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-02-06T15:16:20.523" Id="6936" LastActivityDate="2011-02-06T15:16:20.523" OwnerUserId="449" ParentId="6927" PostTypeId="2" Score="3" />
&#10;\eta(P,Q)=\kappa(P|R)+\kappa(Q|R).
&#10;$$&#10;Then $\eta(P,Q)$ is nonnegative and finite for every $P$ and $Q$, $\eta$ is symmetric in the sense that $\eta(P,Q)=\eta(Q,P)$ for every $P$ and $Q$, and $\eta(P,Q)=0$ iff $P=Q$.&lt;/p&gt;&#10;&#10;&lt;p&gt;An equivalent formulation is&#10;$$
  
&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;or, equivalently,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$
&#10;$$&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-02-07T03:06:02.280" Id="6948" LastActivityDate="2011-02-07T03:42:29.363" LastEditDate="2011-02-07T03:42:29.363" LastEditorUserId="2970" OwnerUserId="2970" ParentId="6946" PostTypeId="2" Score="8" />
  <row AcceptedAnswerId="6973" AnswerCount="1" Body="&lt;p&gt;/edit: To clarify: The mtable function from the &lt;a href=&quot;http://cran.r-project.org/web/packages/memisc/index.html&quot; rel=&quot;nofollow&quot;&gt;memisc&lt;/a&gt; package does exactly what I need, but unfortunately does not work with arima models.&lt;/p&gt;&#10;&#10;&lt;p&gt;Similar to &lt;a href=&quot;http://stats.stackexchange.com/questions/6856/aggregating-results-from-linear-model-runs-r&quot;&gt;this question&lt;/a&gt;: I have multiple Arima models, some of which I've also fit with dependent variables. I'd like an easy way to make a table/graph of the coefficients in each model, as well as summary statistics about each model.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is some example code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;sim &amp;lt;- arima.sim(list(order = c(1,1,0), ar = 0.7), n = 200)&#10;&#10;ar1&amp;lt;-arima(sim,order=c(1,1,0))&#10;ar2&amp;lt;-arima(sim,order=c(2,1,0))&#10;ar3&amp;lt;-arima(sim,order=c(3,1,0))&#10;ar4&amp;lt;-arima(sim,order=c(2,2,1))&#10;&#10;#Try mtable&#10;library(memisc)&#10;mtable(&quot;Model 1&quot;=ar1,&quot;Model 2&quot;=ar2,&quot;Model 3&quot;=ar3,&quot;Model 4&quot;=ar4)&#10;#&amp;gt;Error in UseMethod(&quot;getSummary&quot;) : &#10;#  no applicable method for 'getSummary' applied to an object of class &quot;Arima&quot;&#10;&#10;#Try  apsrtable&#10;library(apsrtable)&#10;apsrtable(&quot;Model 1&quot;=ar1,&quot;Model 2&quot;=ar2,&quot;Model 3&quot;=ar3,&quot;Model 4&quot;=ar4)&#10;#&amp;gt;Error in est/x$se : non-numeric argument to binary operator&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="5" CreationDate="2011-02-07T18:20:52.983" FavoriteCount="3" Id="6967" LastActivityDate="2011-02-08T00:26:12.043" LastEditDate="2011-02-07T23:22:29.097" LastEditorUserId="2817" OwnerUserId="2817" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;time-series&gt;&lt;regression&gt;&lt;arima&gt;" Title="Aggregating results from Arima runs R" ViewCount="481" />
  
  <row AnswerCount="1" Body="&lt;p&gt;This question might sound very broad, but here is what I am looking for. I know there are many excellent books about econometric methods, and many excellent expository articles about econometric techniques. There are even excellent &lt;em&gt;reproducible&lt;/em&gt; examples of econometrics, as described in this CrossValidated &lt;a href=&quot;http://stats.stackexchange.com/questions/1980/complete-substantive-examples-of-reproducible-research-using-r&quot;&gt;question&lt;/a&gt;. In fact the examples in this question come very close to what I am looking for; the only thing missing in those examples is that they are only &lt;em&gt;research&lt;/em&gt; reports, without any mention of how the results of the study fared in a &lt;em&gt;real-world application&lt;/em&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;What I am looking for are documented/reproducible examples of real-world applications of econometric theory that ideally have the following characteristics:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;They should be &lt;strong&gt;reproducible&lt;/strong&gt;, i.e. contain detailed description of (and pointers to) data, econometric techniques, and code. Ideally the code would be in the R language.&lt;/li&gt;&#10;&lt;li&gt;There should be detailed documentation showing that the technique &lt;strong&gt;succeeded in the real world&lt;/strong&gt;, according to a well-quantified measure of success (e.g. &quot;the technique helped increase revenues because it enabled improved forecasting of demand, and here are the numbers involved&quot;)&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I am using the term &lt;em&gt;econometric&lt;/em&gt; quite broadly here -- I mean any sort of &lt;em&gt;data-mining&lt;/em&gt;, &lt;em&gt;statistical data-analysis&lt;/em&gt;, &lt;em&gt;predictiion&lt;/em&gt;, &lt;em&gt;forecasting&lt;/em&gt; or &lt;em&gt;machine-learning&lt;/em&gt; technique.&#10;One immediate problem in finding such examples:  many successful applications of econometrics are done in a for-profit setting and are therefore proprietary, so if a technique worked well, it probably will &lt;em&gt;not&lt;/em&gt; be published (this is especially true in the case of proprietary trading strategies), but I am nevertheless hoping there are published examples that have at least property (2) above if not both (1) and (2).&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2011-02-08T02:24:03.660" FavoriteCount="3" Id="6976" LastActivityDate="2011-02-08T12:46:40.163" OwnerUserId="2544" PostTypeId="1" Score="10" Tags="&lt;r&gt;&lt;machine-learning&gt;&lt;forecasting&gt;&lt;data-mining&gt;&lt;econometrics&gt;" Title="Documented/reproducible examples of successful real-world applications of econometric methods?" ViewCount="594" />
  <row Body="&lt;p&gt;Hypothesis testing is a useful way to frame a lot of questions: is the effect of a treatment zero or nonzero? The ability between statements such as these and a statistical model or procedure (including the construction of an interval estimator) is important for practitioners I think.&lt;/p&gt;&#10;&#10;&lt;p&gt;It also bears mentioning that a confidence interval (in the traditional sense) isn't inherently any less &quot;sin-prone&quot; than hypothesis testing - how many intro stats students know the real definition of a confidence interval?&lt;/p&gt;&#10;&#10;&lt;p&gt;Perhaps the problem isn't hypothesis testing or interval estimation as it is the classical versions of the same; the Bayesian formulation avoids these quite nicely.&lt;/p&gt;&#10;" CommentCount="12" CreationDate="2011-02-08T05:07:01.333" Id="6979" LastActivityDate="2011-02-08T05:07:01.333" OwnerUserId="26" ParentId="6966" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;I would start with building the sums:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Stage 1                   Stage 2&#10;            A   B   C   D   E | Sum&#10;-----------------------------------&#10;Group A    18   0   0   0   2 |  20&#10;Group B     0   6   0   0   3 |   9&#10;Group C     4   0   2   0   2 |   8&#10;Group D     0   0   0   0   0 |   0&#10;Group E     4   2   0   1   8 |  15&#10;-----------------------------------&#10;Sum        26   8   2   1  15 |  52 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;As long as you don't tell us, that this is just a sample from a bigger population, I don't see any statistic involved, just frequency distributions. &lt;/p&gt;&#10;&#10;&lt;p&gt;From A to B went 0 person, not much to calculate here. 2 Persons went from A to E. A had initially 20 members (am I right?) and 2/20 is 10%. 10 Percent of the A group went to E. How many people have been later in group A, or where in total is not of interest here. &#10;If you ask 'How many from group E came from A, then you would use the total of E in stage 2; there are 15 people, two of them came from A: 2/15.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-02-08T06:55:16.950" Id="6982" LastActivityDate="2011-02-08T06:55:16.950" OwnerUserId="3100" ParentId="6974" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;It is good to remember what is the purpose of the weighting. Suppose we have linear regression&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{align}&#10;\mathbf{y}=\mathbf{X}\mathbf{\beta}+\mathbf{\varepsilon}&#10;\end{align}&lt;/p&gt;&#10;&#10;&lt;p&gt;In this case $\sigma_{y_i}=\sigma_{\varepsilon_i}=\sigma_i$ and we can write that &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\Omega=E(\varepsilon\varepsilon&amp;#39;)=diag(\sigma_i)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Now weighted regression with weights inverse to $\sigma_i$ is &lt;a href=&quot;http://en.wikipedia.org/wiki/Generalized_least_squares&quot; rel=&quot;nofollow&quot;&gt;generalised least squares estimate&lt;/a&gt; with $\Omega$. Generalised least squares estimates are best linear unbiased estimates if variance of errors is $\Omega$. &lt;/p&gt;&#10;&#10;&lt;p&gt;So you need to know the variance of errors, not the $y$. If you fit logarithmic regression and you know that $y$ is log normal, you need to weight with the variance of $\log y$, which in your case will correspond to variance of $\epsilon$. Then your weighted least squares estimates will be best linear unbiased estimates, and you will retain the same property as in linear case.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-02-08T15:26:07.017" Id="6991" LastActivityDate="2011-02-08T15:26:07.017" OwnerUserId="2116" ParentId="6986" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;You can fit multilevel GLMM with a Poisson distribution (with over-dispersion) using R in multiple ways. Few &lt;code&gt;R&lt;/code&gt; packages are: &lt;code&gt;lme4&lt;/code&gt;, &lt;code&gt;MCMCglmm&lt;/code&gt;, &lt;code&gt;arm&lt;/code&gt;, etc. A good reference to see is &lt;a href=&quot;http://www.stat.columbia.edu/~gelman/arm/&quot;&gt;Gelman and Hill (2007)&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I will give an example of doing this using &lt;code&gt;rjags&lt;/code&gt; package in &lt;code&gt;R&lt;/code&gt;. It is an interface between &lt;code&gt;R&lt;/code&gt; and &lt;code&gt;JAGS&lt;/code&gt; (like &lt;code&gt;OpenBUGS&lt;/code&gt; or &lt;code&gt;WinBUGS&lt;/code&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;$$n_{ij} \sim \mathrm{Poisson}(\theta_{ij})$$&#10;$$\log \theta_{ij} = \beta_0 + \beta_1 \mbox{ } \mathtt{Treatment}_{i} + \delta_{ij}$$&#10;$$\delta_{ij} \sim N(0, \sigma^2_{\epsilon})$$ &#10;$$i=1 \ldots I, \quad j = 1\ldots J$$&#10;$\mathtt{Treatment}_i = 0 \mbox{ or } 1, \ldots, J-1 \mbox{ if the } i^{th} \mbox{ observation belongs to treatment group } 1 \mbox{, or, } 2, \ldots, J$&lt;/p&gt;&#10;&#10;&lt;p&gt;The $\delta_{ij}$ part in the code above models overdispersion. But there is no one stopping you from modeling correlation between individuals (you don't believe that individuals are really independent) and within individuals (repeated measures). Also, the rate parameter may be scaled by some other constant as in &lt;code&gt;rate models&lt;/code&gt;. Please see Gelman and Hill (2007) for more reference. Here is the &lt;code&gt;JAGS&lt;/code&gt; code for the simple model:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;data{&#10;        for (i in 1:I){         &#10;            ncount[i,1] &amp;lt;- obsTrt1[i]&#10;            ncount[i,2] &amp;lt;- obsTrt2[i]&#10;                ## notice I have only 2 treatments and I individuals &#10;    }                               &#10;}&#10;&#10;model{&#10;    for (i in 1:I){ &#10;        nCount[i, 1] ~ dpois( means[i, 1] )&#10;        nCount[i, 2] ~ dpois( means[i, 2] )&#10;&#10;        log( means[i, 1] ) &amp;lt;- mu + b * trt1[i] + disp[i, 1]&#10;        log( means[i, 2] ) &amp;lt;- mu + b * trt2[i] + disp[i, 2]&#10;&#10;        disp[i, 1] ~ dnorm( 0, tau)&#10;        disp[i, 2] ~ dnorm( 0, tau)&#10;&#10;    }&#10;&#10;    mu  ~ dnorm( 0, 0.001)&#10;    b   ~ dnorm(0, 0.001)&#10;    tau ~ dgamma( 0.001, 0.001)&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Here is the &lt;code&gt;R&lt;/code&gt; code to implement use it (say it is named: &lt;code&gt;overdisp.bug&lt;/code&gt;)&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;dataFixedEffect &amp;lt;- list(&quot;I&quot;       = 10,&#10;                        &quot;obsTrt1&quot; = obsTrt1 , #vector of n_i1&#10;                        &quot;obsTrt2&quot; = obsTrt2,  #vector of n_i2&#10;                        &quot;trt1&quot;    = trt1,     #vector of 0&#10;                        &quot;trt2&quot;    = trt2,     #vector of 1&#10;                       )&#10;&#10;initFixedEffect &amp;lt;- list(mu = 0.0 , b = 0.0, tau = 0.01)&#10;&#10;simFixedEffect &amp;lt;- jags.model(file     = &quot;overdisp.bug&quot;,&#10;                             data     = dataFixedEffect,&#10;                             inits    = initFixedEffect,&#10;                             n.chains = 4,&#10;                             n.adapt  = 1000)&#10;&#10;sampleFixedEffect &amp;lt;- coda.samples(model          = simFixedEffect,&#10;                                  variable.names = c(&quot;mu&quot;, &quot;b&quot;, &quot;means&quot;),&#10;                                  n.iter         = 1000)&#10;&#10;meansTrt1 &amp;lt;- as.matrix(sampleFixedEffect[ , 2:11])&#10;meansTrt2 &amp;lt;- as.matrix(sampleFixedEffect[ , 12:21])&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;You can play around with your parameters' posteriors and you can introduce more parameters to make you modeling more precise (&lt;em&gt;we like to think this&lt;/em&gt;). Basically, you get the idea.&lt;/p&gt;&#10;&#10;&lt;p&gt;For more details on using &lt;code&gt;rjags&lt;/code&gt; and &lt;code&gt;JAGS&lt;/code&gt;, please see &lt;a href=&quot;http://www.johnmyleswhite.com/notebook/2010/08/20/using-jags-in-r-with-the-rjags-package/&quot;&gt;John Myles White's page&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-02-08T17:06:29.380" Id="6994" LastActivityDate="2011-02-08T17:15:16.543" LastEditDate="2011-02-08T17:15:16.543" LastEditorUserId="1307" OwnerUserId="1307" ParentId="6989" PostTypeId="2" Score="15" />
  
  
  <row AcceptedAnswerId="6999" AnswerCount="1" Body="&lt;p&gt;The geometric mean is the appropriate measure of central tendency for log-normally distributed variables. However, the arithmetic mean still has some use in relation to log-normal variables - in inferring totals from survey data, for instance. It is my understanding that the arithmetic mean of the sample of a log-normal distribtion is the same as the arithmetic mean of the population, with normal errors providing the sample is large enough for CLT to be invoked.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you operate a simple missing data imputation method of replacing with a mean, you would inflate the geometric mean if you used the arithmetic mean (and give high leverage to imputed data), and deflate the arithmetic mean (and therefore the total) if you used the geometric mean.&lt;/p&gt;&#10;&#10;&lt;p&gt;My question is what is most appropriate to use, the geometric mean or the arithmetic mean? Or is such a simple imputation method completely inappropriate for log-normal data?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-02-08T18:39:48.527" Id="6998" LastActivityDate="2011-02-08T18:48:31.850" OwnerUserId="229" PostTypeId="1" Score="3" Tags="&lt;mean&gt;&lt;data-imputation&gt;&lt;lognormal&gt;" Title="Means and imputation of log-normal variables" ViewCount="251" />
&#10;(n-1) \left(\frac{1}{\chi_{(n-1)}^{2\;(\alpha/2)}} - \frac{1}{\chi_{(n-1)}^{2\;(1-\alpha/2)}} \right) &amp;lt; \rho .
  <row AnswerCount="2" Body="&lt;p&gt;Given a set of data (~5000 values) I'd like to draw random samples from the same distribution as the original data. The problem is there is no way to know for sure what distribution the original data comes from. &lt;/p&gt;&#10;&#10;&lt;p&gt;It makes sense to use normal distribution in my case, although I'd like to be able to motivate that decision, and of course I also need to estimate the $(\mu,\sigma)$ pair. &lt;/p&gt;&#10;&#10;&lt;p&gt;Any idea on how to accomplish this, preferably within Java environment. I have been using &lt;a href=&quot;http://commons.apache.org/math/apidocs/org/apache/commons/math/distribution/NormalDistributionImpl.html&quot; rel=&quot;nofollow&quot;&gt;Apache Commons Math&lt;/a&gt; and recently stumbled upon &lt;a href=&quot;http://acs.lbl.gov/software/colt/&quot; rel=&quot;nofollow&quot;&gt;Colt&lt;/a&gt; library. I was hoping to get it done without bothering with MATLAB and R. &lt;/p&gt;&#10;" CommentCount="6" CreationDate="2011-02-09T16:50:00.400" FavoriteCount="0" Id="7022" LastActivityDate="2012-01-19T13:04:51.117" LastEditDate="2011-02-09T17:13:52.090" LastEditorUserId="88" OwnerUserId="3014" PostTypeId="1" Score="3" Tags="&lt;estimation&gt;&lt;normal-distribution&gt;&lt;java&gt;" Title="Parameter estimation for normal distribution in Java" ViewCount="1188" />
  <row AcceptedAnswerId="7039" AnswerCount="2" Body="&lt;p&gt;I ran across this density the other day.  Has someone given this a name?&lt;/p&gt;&#10;&#10;&lt;p&gt;$f(x) = \log(1 + x^{-2}) / 2\pi$&lt;/p&gt;&#10;&#10;&lt;p&gt;The density is infinite at the origin and it also has fat tails.  I saw it used as a prior distribution in a context where many observations were expected to be small, though large values were expected as well.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-02-09T19:34:28.307" FavoriteCount="1" Id="7029" LastActivityDate="2011-02-12T05:07:24.860" LastEditDate="2011-02-12T05:07:24.860" LastEditorUserId="183" OwnerUserId="319" PostTypeId="1" Score="24" Tags="&lt;distributions&gt;&lt;probability&gt;" Title="Does the distribution $\log(1 + x^{-2}) / 2\pi$ have a name?" ViewCount="1018" />
  <row AcceptedAnswerId="10475" AnswerCount="2" Body="&lt;p&gt;Is there a package or library that can help me suggest a formula given the independent variables which will work well in glm, for example this formula can be something like x^2+log(y)+Z, it does not necessarily need to be the standard linear model x+y+z in order to explain a variable.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2011-02-10T11:40:35.520" FavoriteCount="3" Id="7045" LastActivityDate="2012-07-31T19:27:54.183" LastEditDate="2011-02-10T13:59:01.247" LastEditorUserId="919" OwnerUserId="1808" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;logistic&gt;&lt;generalized-linear-model&gt;" Title="How to obtain in R a good formula for glm (general linear models) to predict a binomial variable?" ViewCount="1806" />
  
  
  <row Body="&lt;p&gt;Yes.  It is certainly possible that this is due to something like Simpson's paradox.  If the data looked like &#10;$$\begin{array}{rrrrrr}
  
  
  <row Body="&lt;p&gt;The key assumption of a generalized linear model that's relevant here is the relationship between the variance and mean of the response, given the values of the predictors. When you specify a Poisson distribution, what this implies is that you are assuming the conditional variance is equal to the conditional mean.* The actual &lt;em&gt;shape&lt;/em&gt; of the distribution doesn't matter as much: it could be Poisson, or gamma, or normal, or anything else as long as that mean-variance relationship holds.&lt;/p&gt;&#10;&#10;&lt;p&gt;* You can relax the assumption that the variance equals the mean to one of proportionality, and still usually get good results.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-02-11T01:29:36.383" Id="7076" LastActivityDate="2011-02-11T01:29:36.383" OwnerUserId="1569" ParentId="7049" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;This is very much a problem in Bayesian inference.  You appear to have taken the first step in realizing that the sample is not the same as the underlying probability distribution.  Even though Mike had a higher mean in his sample, John might have a higher mean for his true talent distribution.  When applying Bayesian inference, we start with a prior distribution, use the evidence to update our knowledge, and come out with a posterior distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;The point I alluded to in my comment is that the prior knowledge can matter a great deal.  If we happen to know that the &quot;John&quot; happens to be basketball Hall of Famer John Stockton and the &quot;Mike&quot; happens to be me, then 100 shots are not going to be nearly enough to convince you that I'm better.  You prior distribution for John Stockton's true talent would probably be somewhat tight with a mean near .8 or so (he was .8261 for his career), whereas who knows for me...it might be .3 for all I know.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you are estimating the true talent probabilities of making a free throw, you at least know that the values must lie in (0, 1), so a minimal prior distribution for each would be the uniform probability distribution on (0, 1).  This is equivalent to using the Beta distribution with parameters $\alpha = \beta = 1$.  Through some calculations I won't get into (along with Bayes theorem), it turns out that the posterior distribution is also a Beta distribution with parameters $\alpha + i$ and $\beta + j$ (assuming the player made $i$ shots in $i + j$ attempts).&lt;/p&gt;&#10;&#10;&lt;p&gt;So our posterior distribution for John is $Beta(39, 13)$ and for Mike it is $Beta(81, 21)$.  So with these priors our posterior estimate of John's mean talent is $\frac{39}{52} = .75$ and for Mike it is $\frac{81}{102} \approx .794.$  I believe finding the answer to how likely Mike is better in a nice closed form is messy, and I don't have the tools handy to calculate the exact answer, but you would integrate/sum the portions of those curves where Mike is better to find the answer.  Nick's code looks like it will probably get you a nice approximation.  I can tell you that with the prior I chose Mike will be higher (since they had the same prior and Mike had the higher sample mean, it does pass the smell test that Mike will have a higher posterior mean).  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-02-11T03:47:17.390" Id="7077" LastActivityDate="2011-02-11T03:47:17.390" OwnerUserId="2485" ParentId="7061" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;In time-series analysis usually &lt;a href=&quot;http://en.wikipedia.org/wiki/Ljung-Box_test&quot; rel=&quot;nofollow&quot;&gt;Ljung-Box test&lt;/a&gt; is used. Note though that it tests the correlations. If the correlations are zero, but variance varies, then the process is not white noise, but Ljung-Box test will fail to reject the null-hypothesis. Here is an example in R:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; Box.test(c(rnorm(100,0,1),rnorm(100,0,10)),type=&quot;Ljung-Box&quot;)&#10;&#10;    Box-Ljung test&#10;&#10;data:  c(rnorm(100, 0, 1), rnorm(100, 0, 10)) &#10;X-squared = 0.4771, df = 1, p-value = 0.4898&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Here is the plot of the process:&#10;&lt;img src=&quot;http://i.stack.imgur.com/DiFBx.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is more discussion &lt;a href=&quot;http://stats.stackexchange.com/questions/6455/how-many-lags-to-use-in-the-ljung-box-test-of-a-time-series&quot;&gt;about this test&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-02-11T07:47:54.650" Id="7083" LastActivityDate="2011-02-11T07:47:54.650" OwnerUserId="2116" ParentId="7074" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;deps_stats I am afraid that the size of my dataset is so large that LARS can not handle it, whereas on the other hand glmnet can handle my large dataset.&lt;/p&gt;&#10;&#10;&lt;p&gt;mpiktas I want to find the solution of (Y-Xb)^2+L\sum|b_j|&#10;but when I ask from the two algorithms(lars &amp;amp; glmnet) for their calculated coefficients for that particular L, I get different answers...and I wondering is that correct/ expected? or I am just using a wrong lambda for the two functions.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-02-11T09:52:26.037" Id="7087" LastActivityDate="2012-03-14T15:46:57.120" LastEditDate="2012-03-14T15:46:57.120" LastEditorUserId="919" OwnerDisplayName="user3139" ParentId="7057" PostTypeId="2" Score="0" />
  
  <row AnswerCount="3" Body="&lt;p&gt;I need a formula for the probability of an event in a n-variate Bernoulli distribution $X\in\{0,1\}^n$ with given $P(X_i=1)=p_i$ probabilities for a single element and for pairs of elements $P(X_i=1 \wedge X_j=1)=p_{ij}$. Equivalently I could give mean and covariance of $X$. &lt;/p&gt;&#10;&#10;&lt;p&gt;I already learned that there exist many $\{0,1\}^n$ distributions having the properties just as there are many distributions having a given mean and covariance. I am looking for a canonical one on $\{0,1\}^n$, just as the Gaussian is a canonical distribution for $R^n$ and a given mean and covariance.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-02-11T12:30:24.763" FavoriteCount="3" Id="7089" LastActivityDate="2012-01-13T00:32:52.583" LastEditDate="2011-02-11T12:51:48.473" LastEditorUserId="2116" OwnerDisplayName="user3152" PostTypeId="1" Score="5" Tags="&lt;multivariate-analysis&gt;&lt;discrete-data&gt;" Title="Probability formula for a multivariate-bernoulli distribution" ViewCount="2444" />
  
  <row Body="&lt;p&gt;We are running a customer service centre. We are getting 1 million calls per month. How do we reduce it to ten thousand ?&lt;/p&gt;&#10;" CommentCount="4" CommunityOwnedDate="2011-02-11T13:40:00.073" CreationDate="2011-02-11T13:40:00.073" Id="7095" LastActivityDate="2011-02-11T13:40:00.073" OwnerDisplayName="user3153" ParentId="5465" PostTypeId="2" Score="1" />
  
  
  <row AcceptedAnswerId="7114" AnswerCount="4" Body="&lt;p&gt;To perform principal component analysis (PCA), you have to subtract the means of each column from the data, compute the correlation coefficient matrix and then find the eigenvectors and eigenvalues. Well, rather, this is what I did to implement it in Python, except it only works with small matrices because the method to find the correlation coefficient matrix (corrcoef) doesn't let me use an array with high dimensionality. Since I have to use it for images, my current implementation doesn't really help me.&lt;/p&gt;&#10;&#10;&lt;p&gt;I've read that it's possible to just take your data matrix $D$ and compute $DD^\top/n$ instead of $D^\top D/n$, but that doesn't work for me. Well, I'm not exactly sure I understand what it means, besides the fact that it's supposed to be a $n \times n$ matrix instead of $p\times p$ (in my case $p\gg n$). I read up about those in the eigenfaces tutorials but none of them seemed to explain it in such a way I could really get it.&lt;/p&gt;&#10;&#10;&lt;p&gt;In short, is there a simple algorithmic description of this method so that I can follow it?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-02-11T22:56:39.283" FavoriteCount="6" Id="7111" LastActivityDate="2015-02-07T22:09:44.477" LastEditDate="2015-02-07T22:09:00.433" LastEditorDisplayName="user3164" LastEditorUserId="28666" OwnerDisplayName="user3164" PostTypeId="1" Score="9" Tags="&lt;pca&gt;&lt;python&gt;" Title="How to perform PCA for data of very high dimensionality?" ViewCount="2559" />
  
  
  
  <row Body="&lt;p&gt;Your distribution is parametric, and you should just store the parameters that are sufficient statistics, if you can identify them. That includes the distribution family. For a time series, you can take advantage of autocorrelation and store the parameters of the predictive distribution conditional on its previous values.&lt;/p&gt;&#10;&#10;&lt;p&gt;The  entropy of the prior (predictive) distribution of the parameters determines the upper bound for compression strength, but you may not need to compress them further. If you do, use arithmetic compression. Decreasing entropy, say by discretizing quantiles, will give greater compression.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2011-02-12T11:19:00.807" Id="7126" LastActivityDate="2011-02-12T11:26:39.763" LastEditDate="2011-02-12T11:26:39.763" LastEditorUserId="2456" OwnerUserId="2456" ParentId="7103" PostTypeId="2" Score="1" />
  <row AnswerCount="3" Body="&lt;h3&gt;Study:&lt;/h3&gt;&#10;&#10;&lt;p&gt;I simulated some surfaces materials at audio and haptic level, and I asked subjects to evaluate on a 9-point Likert scale the degree of coherence between the two stimuli.&#10;For example there are stimuli with metal at auditory level and snow at haptic level, or wood both at auditory and haptic level.&lt;/p&gt;&#10;&#10;&lt;p&gt;So it is like a dissimilarity rating experiment.&lt;/p&gt;&#10;&#10;&lt;p&gt;The experiment had only 12 participants, so for each stimulus I have 12 response (no repeated measures involved).&lt;/p&gt;&#10;&#10;&lt;h3&gt;Question:&lt;/h3&gt;&#10;&#10;&lt;p&gt;How should I analyse the following experiment?&lt;/p&gt;&#10;&#10;&lt;h3&gt;Initial Thoughts:&lt;/h3&gt;&#10;&#10;&lt;p&gt;So far the only way I am thinking to use is ANOVA.&lt;/p&gt;&#10;&#10;&lt;h3&gt;Update&lt;/h3&gt;&#10;&#10;&lt;p&gt;Hello again,&#10;I used the cmdscale command in R, but I did not get thecperceptual map I would love to see.&#10;I as you an help! Maybe the problem is that I misunderstood if my table is well built for the purpose of the analsys.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let´s summarize my experiment, my goals and the problem:&lt;/p&gt;&#10;&#10;&lt;p&gt;I simulated some surfaces materials at audio and haptic level, and I asked &#10;subjects to evaluate on a 9-point Likert scale the degree of coherence between &#10;the two stimuli. For example there are trials where at auditory level a metal&#10;surface was simulated and at haptic level the snow surface, or wood both at &#10;auditory and haptic level.&lt;/p&gt;&#10;&#10;&lt;p&gt;The experiment had only 12 participants, so for each trial I have 12 response &#10;(no repeated measures involved).&#10;In total there were 36 trials, and each trial was evaluated only once by each&#10;of the 12 participants. So each subject provided 36 ratings on the 9 point Likert &#10;scale, one for each trial.&lt;/p&gt;&#10;&#10;&lt;p&gt;Basically, these are my data:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; WD   MT   SW   GR   SN   DL&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;WD 7.00 6.50 4.91 4.83 5.50 5.00&lt;/p&gt;&#10;&#10;&lt;p&gt;MT 7.33 6.91 2.08 3.16 4.25 3.25&lt;/p&gt;&#10;&#10;&lt;p&gt;SW 2.91 1.75 7.91 6.25 6.83 5.41&lt;/p&gt;&#10;&#10;&lt;p&gt;GR 2.91 2.66 6.25 6.41 7.25 6.75&lt;/p&gt;&#10;&#10;&lt;p&gt;SN 4.00 4.00 5.58 6.00 7.00 6.58&lt;/p&gt;&#10;&#10;&lt;p&gt;DL 3.91 3.08 5.16 6.25 6.50 6.83&lt;/p&gt;&#10;&#10;&lt;p&gt;On the rows the haptic stimuli and on the columns the auditory stimuli. In each&#10;cell there is the average score for each trial (e.g. the trial GR-MT is 2.66, that &#10;is the average score given by participants to the trial where the material &quot;gravel&quot;&#10;was provided at haptic level and the material &quot;metal&quot; was provided ar auditory level).&lt;/p&gt;&#10;&#10;&lt;p&gt;Now I want to analyze the data in the correct ways, and as said MDS is the best &#10;analysis instead of ANOVA as I was thinking.&lt;/p&gt;&#10;&#10;&lt;p&gt;My first goal is to print a perceptual map where to place the pairs of &#10;audio-haptic stimuli (e.g. WD-WD, MT-DL, etc.) and see how far are the trials &#10;from each other.&#10;I used cmdscale in R but I did not get the wanted result. Any suggestion?&lt;/p&gt;&#10;&#10;&lt;p&gt;My second goal would be to find some p-values like I normally get with ANOVA.&lt;/p&gt;&#10;&#10;&lt;p&gt;For example I would like to understand if having the coherent couple of stimuli&#10;SW-SW (which means &quot;snow&quot; both at audio and haptic level) produces significant&#10;differences n the evaluations rather than the couple SW-MT (which means &quot;snow&quot; &#10;at audio and &quot;metal&quot; at haptic level) &lt;/p&gt;&#10;&#10;&lt;p&gt;Again I would like to undestand if there is any statistical difference between&#10;all the couples of stimuli corresponding to solid surfaces (like the couples MT-MT, &#10;MT-WD, WD-WD, MT-MT) and all the couples where a solid surface and a aggregate surface&#10;are presented (like the couples MT-SN, or WD-GR, etc.).&lt;/p&gt;&#10;&#10;&lt;p&gt;...I want to get as many information as possible from that table.&#10;I really thanks anyone who can provide any suggestion or useful information.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-02-12T15:22:27.697" Id="7128" LastActivityDate="2011-02-16T01:26:19.933" LastEditDate="2011-02-15T01:10:26.023" LastEditorUserId="183" OwnerUserId="4701" PostTypeId="1" Score="4" Tags="&lt;rating&gt;" Title="Dissimilarity rating experiment: how to analyze the results" ViewCount="374" />
  
  
  <row AcceptedAnswerId="23742" AnswerCount="2" Body="&lt;p&gt;I have a dataset with yearly levels of corruption in a number of countries, as well as whether they changed their government that year.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;year, corruption, change of president&#10;2001, 5, 0&#10;2002, 7, 1&#10;2003, 8, 0&#10;etc.&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I want to test whether corruption is affected by a change in power (defined as the election of a new president who isn't part of the same political party as the previous one).&lt;/p&gt;&#10;&#10;&lt;p&gt;The idea is to either look at the slope of corruption/year for the two years leading into the election, and the two years after, e.g. $t-2$, $t-1$ and $t$ for the before slope.&lt;/p&gt;&#10;&#10;&lt;p&gt;The other idea is to look at the average level of corruption three years before and after.&lt;/p&gt;&#10;&#10;&lt;p&gt;The rate might make more sense since there are more things that affect corruption and different countries may be on different trajectories. However, there could also be some benefit to just looking at average levels three years before and after.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any thoughts on which one I should look at, and also how to go about measuring this?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2011-02-13T16:00:25.593" FavoriteCount="2" Id="7156" LastActivityDate="2012-02-27T13:59:25.527" LastEditDate="2011-02-13T22:06:14.257" LastEditorUserId="2970" OwnerUserId="3182" PostTypeId="1" Score="4" Tags="&lt;hypothesis-testing&gt;" Title="Difference between two slopes" ViewCount="780" />
  
  
  <row Body="&lt;p&gt;&lt;strong&gt;Definition 1:&lt;/strong&gt; As already mentioned, an outlier in a group of data reflecting the same process (say process A) is an observation (or a set of observations) that is &lt;strong&gt;unlikely&lt;/strong&gt; to be a result of  process A. &lt;/p&gt;&#10;&#10;&lt;p&gt;This definition certainly involves an estimation of the likelihood function of the process A (hence a model) and setting what unlikely means (i.e. deciding where to stop...). This definition is at the root of the answer I gave &lt;a href=&quot;http://stats.stackexchange.com/questions/213/what-is-the-best-way-to-identify-outliers-in-multivariate-data/386#386&quot;&gt;here&lt;/a&gt;. It is more related to ideas of &lt;strong&gt;hypothesis testing&lt;/strong&gt; of significance or &lt;strong&gt;goodness of fit&lt;/strong&gt;.  &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Definition 2&lt;/strong&gt; An outlier is an observation $x$ in a group of observations $G$ such that when modeling the group of observation with a given model the accuracy is higher if $x$ is removed and treated separately (with a mixture, in the spirit of what I mention &lt;a href=&quot;http://stats.stackexchange.com/questions/1444/how-should-i-transform-non-negative-data-including-zeros/1445#1445&quot;&gt;here&lt;/a&gt;). &lt;/p&gt;&#10;&#10;&lt;p&gt;This definition involves a &quot;given model&quot; and a measure of accuracy. I think this definition is more from the practical side and is more at the origin of outliers. At the Origin, outlier detection was a tool for &lt;strong&gt;robust statistics&lt;/strong&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;Obviously these definitions can be made very similar if you understand that calculating likelihood in the first definition involves modeling and calculation of a score :) &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-02-13T19:30:44.873" Id="7162" LastActivityDate="2013-08-27T20:00:52.887" LastEditDate="2013-08-27T20:00:52.887" LastEditorUserId="17230" OwnerUserId="223" ParentId="7155" PostTypeId="2" Score="6" />
  
  <row AnswerCount="0" Body="&lt;p&gt;I came by Bernard Silverman concept of &quot;rabbit ears hunting&quot; interactive procedure for density estimation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Does anybody know:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;A recommended reference for this procedure?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;An explanation/justification for when it should &quot;work&quot;?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Any other suggested ad-hoc interactive criteria for density estimation?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" ClosedDate="2013-02-17T22:11:26.363" CommentCount="5" CreationDate="2011-02-13T22:41:18.760" FavoriteCount="0" Id="7172" LastActivityDate="2013-02-12T19:01:04.487" LastEditDate="2013-02-12T19:01:04.487" LastEditorUserId="19879" OwnerUserId="253" PostTypeId="1" Score="2" Tags="&lt;kernel&gt;&lt;pdf&gt;&lt;kde&gt;" Title="When/why should &quot;rabbit ears hunting&quot; density estimation &quot;work&quot;?" ViewCount="147" />
  
  <row Body="&lt;p&gt;Your problem can be described as &quot;binary classification problem&quot; with the response variable &quot;become_customer&quot; $\in$ {yes,no}.&lt;/p&gt;&#10;&#10;&lt;p&gt;As far as see, your next steps should be:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;create a tabular dataset with one row = one visitor and columns = predictors + response variable. Predictor variables reflect the input into the forms (as described by you), meanwhile response is &quot;yes&quot;, if the visitor became a customer, else &quot;no&quot;.&lt;/li&gt;&#10;&lt;li&gt;learn and validate (keyword: Crossvalidation; metrics like AUC, Precision, Recall etc.) an expressive (non black-box) classification model. On suggestion is &quot;Decision Trees&quot;, which are mainly used in the area of Direct Marketing / Performance Marketing since they can give you a description of potential customers.&lt;/li&gt;&#10;&lt;li&gt;By calibrating the output of the scores of the classification model you can additionally get an estimate of the conversion rate for each customer.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="3" CreationDate="2011-02-14T09:37:18.190" Id="7193" LastActivityDate="2011-02-14T09:37:18.190" OwnerUserId="264" ParentId="7179" PostTypeId="2" Score="2" />
  <row AnswerCount="3" Body="&lt;p&gt;I'm looking for sample R code, or pointers to sample R code for the following.  (Gentle) critique of the approach would also be appreciated.  I'm not a statistician and I'm pretty new to R.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have duration of hospitalization (&quot;length of stay&quot;=LOS) data for 2,000 patients from my hospital and 50,000 patients from a comparison data set.  Each patient has a discharge year (YEAR) and is assigned to a diagnosis group (DG).  My goal is to compare (with confidence intervals) the overall average LOS at my hospital to the expected average LOS if those patients had been part of the comparison data set.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that for a specific year and specific DG there may be anywhere from a few to 1,000s of patients, and, of course, the distribution of LOS is not normal.  There could possibly even be a DG with 1 or more patients for a given year in my hospital's data but none in the comparison data set.&lt;/p&gt;&#10;&#10;&lt;p&gt;The approach I was considering was to create a comparison group where a random patient from the comparison data set is chosen for each patient from my hospital.  The random patient would be matched by YEAR and DG.  I would calculate the expected average LOS for this group and then repeat the process 10,000 times to determine the 2.5th to 97.5th percentile.  I would repeat for each year and plot my hospital's average LOS versus the 95% CI for the expected average LOS.&lt;/p&gt;&#10;&#10;&lt;p&gt;To deal with the issue of there not being a match for a patient for a given DG for a given YEAR I was thinking of loosening the match criteria to pick patients from the previous or next year.  I could keep broadening the match year until there were at least N patients from which to randomly pick.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thoughts?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-02-14T14:36:39.887" FavoriteCount="5" Id="7197" LastActivityDate="2011-06-26T21:39:02.970" LastEditDate="2011-06-26T13:44:06.477" LastEditorUserId="88" OwnerUserId="690" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;confidence-interval&gt;&lt;bootstrap&gt;" Title="Calculating confidence interval for average hospital length of stay, case-mix adjusted, in R" ViewCount="885" />
  <row Body="&lt;p&gt;It appears to me you have multiple measures for each id at each time.  You need to aggregate these for the aov because it unfairly increases power in that analysis.  I'm not saying doing the aggregate will make the outcomes the same but it should make them more similar.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;dat.agg &amp;lt;- aggregate(UOP.kg ~ time + treat + id, raw3.42, mean)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Then run your aov model as before replacing the data with dat.agg.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, I believe anova(lme) is more what you want to do to compare the outcomes. The direction and magnitude of an effect is not the same as the ratio of model variance to error.&lt;/p&gt;&#10;&#10;&lt;p&gt;(BTW, if you do the lme analysis on the aggregate data, which you shouldn't, and check anova(lme) you'll get almost the same results as aov)&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-02-14T15:10:27.500" Id="7198" LastActivityDate="2011-02-14T15:10:27.500" OwnerUserId="601" ParentId="7185" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="7338" AnswerCount="2" Body="&lt;p&gt;I am using &lt;a href=&quot;http://en.wikipedia.org/wiki/Cohen%27s_kappa&quot; rel=&quot;nofollow&quot;&gt;Cohen's Kappa&lt;/a&gt; to calculate the inter-agreement between two judges.&lt;/p&gt;&#10;&#10;&lt;p&gt;It is calculated as:&lt;/p&gt;&#10;&#10;&lt;p&gt;$ \frac{P(A) - P(E)}{1 - P(E)}  $&lt;/p&gt;&#10;&#10;&lt;p&gt;where $P(A)$ is the proportion of agreement and $P(E)$ the probability of agreement by chance.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now for the following dataset, I get the expected results:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;User A judgements: &#10;  - 1, true&#10;  - 2, false&#10;User B judgements: &#10;  - 1, false&#10;  - 2, false&#10;Proportion agreed: 0.5&#10;Agreement by chance: 0.625&#10;Kappa for User A and B: -0.3333333333333333&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;We can see that both judges have not agreed very well. However in the following case where both judges evaluate one criteria, kappa evaluates to zero:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;User A judgements: &#10;  - 1, false&#10;User B judgements: &#10;  - 1, false&#10;Proportion agreed: 1.0&#10;Agreement by chance: 1.0&#10;Kappa for User A and B: 0&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now I can see that the agreement by chance is obviously 1, which leads to kappa being zero, but does this count as a reliable result? The problem is that I normally don't have more than two judgements per criteria, so these will all never evaluate to any kappa greater than 0, which I think is not very representative.&lt;/p&gt;&#10;&#10;&lt;p&gt;Am I right with my calculations? Can I use a different method to calculate inter-agreement?&lt;/p&gt;&#10;&#10;&lt;p&gt;Here we can see that kappa works fine for multiple judgements:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;User A judgements: &#10;  - 1, false&#10;  - 2, true&#10;  - 3, false&#10;  - 4, false&#10;  - 5, true&#10;User A judgements: &#10;  - 1, true&#10;  - 2, true&#10;  - 3, false&#10;  - 4, true&#10;  - 5, false&#10;Proportion agreed: 0.4&#10;Agreement by chance: 0.5&#10;Kappa for User A and B: -0.19999999999999996&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="6" CreationDate="2011-02-14T17:16:52.580" FavoriteCount="5" Id="7208" LastActivityDate="2011-02-18T01:26:44.263" LastEditDate="2011-02-17T22:01:30.213" LastEditorUserId="930" OwnerUserId="1205" PostTypeId="1" Score="7" Tags="&lt;reliability&gt;&lt;information-retrieval&gt;" Title="Can one use Cohen's Kappa for two judgements only?" ViewCount="1259" />
  
  
  <row Body="&lt;p&gt;Please see the &lt;a href=&quot;http://cran.r-project.org/web/views/NaturalLanguageProcessing.html&quot;&gt;CRAN Task View on Natural Language Processing&lt;/a&gt; which includes, among others, the &lt;a href=&quot;http://cran.r-project.org/package=tm&quot;&gt;tm&lt;/a&gt; package already mentioned by Josh.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-02-14T20:50:04.587" Id="7214" LastActivityDate="2011-02-14T20:50:04.587" OwnerUserId="334" ParentId="7211" PostTypeId="2" Score="13" />
  <row Body="&lt;p&gt;Sure, RapidMiner with the text mining extension.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are many videos that show how it is done.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-02-14T21:24:14.247" Id="7216" LastActivityDate="2011-02-14T21:24:14.247" OwnerDisplayName="user3196" ParentId="7211" PostTypeId="2" Score="3" />
  
  
  <row Body="&lt;p&gt;In a first approximation, 1:1 is a good proportion, but:  &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Some methods are more vulnerable to unequal classes, some are less -- plain decision tree will almost always vote for a much larger class, 1-NN will be not affected at all. It is a good idea to check this out (in literature or by asking here) in context of your problem. &lt;/li&gt;&#10;&lt;li&gt;You should beware of possible inhomogeneities (let's say hidden &quot;subclasses&quot;) -- subsampling may change the proportions between subclasses and thus lead to strange effects. To this end, it is good to at least try few subsampling realizations to have a change to spot possible problems. (Well, I called it a problem, but this may be also a quite enlightening insight in data)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="1" CreationDate="2011-02-14T23:36:54.893" Id="7222" LastActivityDate="2011-02-14T23:36:54.893" OwnerUserId="88" ParentId="7209" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;&lt;a href=&quot;http://face.com/&quot;&gt;http://face.com/&lt;/a&gt; has free a facial recognition API&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-02-15T03:30:32.663" Id="7234" LastActivityDate="2011-02-15T03:30:32.663" OwnerUserId="2973" ParentId="7224" PostTypeId="2" Score="5" />
  
  <row Body="&lt;p&gt;The &lt;a href=&quot;http://www.google.com/search?hl=en&amp;amp;tbs=bks%3A1&amp;amp;q=inauthor%3Ajohnson+inauthor%3Akotz+intitle%3Adistributions&quot; rel=&quot;nofollow&quot;&gt;canonical textbooks describing properties of the various probability distributions by Johnson &amp;amp; Kotz&lt;/a&gt; and later co-authors are entitled &lt;em&gt;Univariate Discrete Distributions&lt;/em&gt;, &lt;em&gt;Continuous Univariate Distributions&lt;/em&gt;, &lt;em&gt;Continuous Multivariate Distributions&lt;/em&gt; and &lt;em&gt;Discrete Multivariate Distributions&lt;/em&gt;. So I think you're on safe ground describing a distribution as 'multivariate' rather than 'joint'.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;sub&gt;Conflict of interest statement: The author is a member of &lt;a href=&quot;http://en.wikipedia.org/wiki/Wikipedia%3aWikiProject_Statistics&quot; rel=&quot;nofollow&quot;&gt;Wikipedia:WikiProject Statistics&lt;/a&gt;.&lt;/sub&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-02-15T20:56:41.460" Id="7267" LastActivityDate="2011-02-15T20:56:41.460" OwnerUserId="449" ParentId="7263" PostTypeId="2" Score="1" />
&#10;$ Host     : Factor w/ 14 levels &quot;app1&quot;,&quot;app2&quot;,..: 9 7 5 4 3 10 6 8 2 1 ...  &#10;$ CPUIOWait: int  0 0 0 0 0 0 0 0 0 0 ...
  
&#10;    &amp;quot;6493&amp;quot;,&amp;quot;139&amp;quot;
  
  <row Body="&lt;p&gt;Devil's advocate:  I could imagine the principal components differing depending on who's sampled.  I'd think this validity issue would take precedence over the precision issue Richard points out.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-02-15T23:43:27.807" Id="7272" LastActivityDate="2011-02-15T23:43:27.807" OwnerUserId="2669" ParentId="7256" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;What is paired t-test, and under which circumstances should I use paired t-test?&#10;Is there any difference between paired t-test and pairwise t-test?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-02-16T04:34:46.397" FavoriteCount="1" Id="7279" LastActivityDate="2011-09-23T05:43:12.140" LastEditDate="2011-09-23T05:43:12.140" LastEditorUserId="183" OwnerUserId="3269" PostTypeId="1" Score="11" Tags="&lt;hypothesis-testing&gt;&lt;anova&gt;&lt;t-test&gt;" Title="Is there any difference between the terms &quot;paired t-test&quot; and &quot;pairwise t-test&quot;?" ViewCount="2736" />
  <row Body="&lt;p&gt;Roughly, &lt;a href=&quot;http://en.wikipedia.org/wiki/Paired_difference_test&quot;&gt;paired t-test&lt;/a&gt; is a t-test in which each subject is compared with itself or, in &lt;a href=&quot;http://mathworld.wolfram.com/Pairedt-Test.html&quot;&gt;other words&lt;/a&gt;, determines whether they differ from each other in a significant way under the assumptions that the paired differences are independent and identically normally distributed. &lt;/p&gt;&#10;&#10;&lt;p&gt;Pairwise t-test, on the other hand is a function in R which performs all possible pairwise comparisons. See &lt;a href=&quot;https://stat.ethz.ch/pipermail/r-help/2004-August/056136.html&quot;&gt;this&lt;/a&gt; discussion for more information&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-02-16T05:57:30.550" Id="7281" LastActivityDate="2011-02-16T05:57:30.550" OwnerUserId="1496" ParentId="7279" PostTypeId="2" Score="10" />
  <row Body="&lt;p&gt;Why not just have a look-up table of numbers for each possible character in an email. Then concatenate the numbers to form a seed. For example, &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;A 1&#10;B 2&#10;C 3&#10;....&#10;@ 27&#10;....&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So abc@ccc, would be converted to 12327333. This would give you a unique seed for each person. You would then use this to generate the 1, 2, 3, 4.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;From your question, it looks like you don't mind a &quot;quick and dirty solution&quot;. One problem with my solution is that email addresses aren't random - for example you will probably get very few email addresses that contain the letter &quot;z&quot;, but all email addresses contain &quot;@&quot;.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-02-16T10:59:12.323" Id="7287" LastActivityDate="2011-02-16T10:59:12.323" OwnerUserId="8" ParentId="7285" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;Regarding 1) Yes, you do lose this.  See e.g. Harrell Regression Modeling Strategies, a book published by Wiley or a paper I presented with David Cassell called &quot;Stopping Stepwise&quot; available e.g. www.nesug.org/proceedings/nesug07/sa/sa07.pdf&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-02-16T13:18:20.643" Id="7289" LastActivityDate="2011-02-16T13:18:20.643" OwnerUserId="686" ParentId="7223" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="7299" AnswerCount="3" Body="&lt;p&gt;Currently i am using RF toolbox on MATLAB for a binary classification Problem&lt;/p&gt;&#10;&#10;&lt;p&gt;Data Set: 50000 samples  and more than 250 features&lt;/p&gt;&#10;&#10;&lt;p&gt;So what should be the number of trees and randomly selected feature on each split to grow the trees?&#10;can any other parameter greatly affect the results?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-02-16T15:20:21.510" FavoriteCount="4" Id="7295" LastActivityDate="2012-03-28T12:49:04.333" LastEditDate="2011-02-16T17:39:26.287" LastEditorUserId="88" OwnerUserId="2534" PostTypeId="1" Score="8" Tags="&lt;machine-learning&gt;&lt;classification&gt;&lt;random-forest&gt;" Title="What should be the optimal parameters for Random Forest classifier?" ViewCount="3753" />
&#10;B = U D U^T
  
  <row AnswerCount="1" Body="&lt;p&gt;suppose you might buy some software to be used in a ML/NLP/DM laboratory. What software package would you ask for? Let's say: MATLAB (with some toolboxes), SPSS, and what else?&lt;/p&gt;&#10;&#10;&lt;p&gt;I know that there is a lot of free software one can use like R, Weka, Rapidminer, python packages, and so on. However, the above question targets commercial software.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks.&lt;/p&gt;&#10;" CommentCount="1" CommunityOwnedDate="2011-02-16T18:00:20.950" CreationDate="2011-02-16T17:29:10.427" Id="7302" LastActivityDate="2012-06-05T17:17:26.633" LastEditDate="2011-02-16T18:00:10.717" LastEditorUserId="88" OwnerUserId="976" PostTypeId="1" Score="4" Tags="&lt;machine-learning&gt;&lt;data-mining&gt;&lt;software&gt;" Title="Software for ML/NLP/DM laboratory" ViewCount="172" />
  
  <row Body="&lt;ul&gt;&#10;&lt;li&gt;Roughly speaking, the median is the &quot;middle value&quot;. Now, if you change the highest  value (which is supposed to be positive here) from $x_{(n)}$ to $2 * x_{(n)}$, say, it does not change the median. But it does change the arithmetic mean. This shows, in simple terms, that the median does not depend on every value while the mean does. Actually, the median only depends on the ranks. The mathematical logic behind this simply arises from the mathematical definitions of the median and the mean.&lt;/li&gt;&#10;&lt;li&gt;Now, it can be shown that, for any $ a \in \mathbb{R}$ &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;$\sum_{i=1}^{n} |x_{i} - median| \leq \sum_{i=1}^{n} |x_{i} - a|$&lt;/p&gt;&#10;&#10;&lt;p&gt;and&lt;/p&gt;&#10;&#10;&lt;p&gt;$\sum_{i=1}^{n} (x_{i} - mean)^{2} \leq \sum_{i=1}^{n} (x_{i} - a)^{2}$&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-02-16T20:19:38.270" Id="7312" LastActivityDate="2011-02-16T20:26:15.590" LastEditDate="2011-02-16T20:26:15.590" LastEditorUserId="3019" OwnerUserId="3019" ParentId="7307" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;&lt;strong&gt;How to specify the knots in R&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The &lt;code&gt;ns&lt;/code&gt; function generates a &lt;em&gt;natural regression spline&lt;/em&gt; basis given an input vector. The knots can be specified either via a degrees-of-freedom argument &lt;code&gt;df&lt;/code&gt; which takes an integer or via a knots argument &lt;code&gt;knots&lt;/code&gt; which takes a &lt;strong&gt;&lt;em&gt;vector&lt;/em&gt;&lt;/strong&gt; giving the desired placement of the knots. Note that in the code you've written&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(splines)&#10;lda.pred &amp;lt;- lda(y ~ ns(x, knots=5))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;you have not requested five knots, but rather have requested a &lt;em&gt;single&lt;/em&gt; (interior) knot at &lt;em&gt;location&lt;/em&gt; 5.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you use the &lt;code&gt;df&lt;/code&gt; argument, then the interior knots will be selected based on quantiles of the vector &lt;code&gt;x&lt;/code&gt;. For example, if you make the call&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ns(x, df=5)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Then the basis will include two boundary knots and &lt;strong&gt;4&lt;/strong&gt; internal knots, placed at the 20th, 40th, 60th, and 80th quantiles of &lt;code&gt;x&lt;/code&gt;, respectively. The boundary knots, by default, are placed at the min and max of &lt;code&gt;x&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is an example to specify the locations of the knots&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x &amp;lt;- 0:100&#10;ns(x, knots=c(20,35,50))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;If you were to instead call &lt;code&gt;ns(x, df=4)&lt;/code&gt;, you would end up with 3 internal knots at locations 25, 50, and 75, respectively.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can also specify whether you want an intercept term. Normally this &lt;strong&gt;&lt;em&gt;isn't&lt;/em&gt;&lt;/strong&gt; specified since &lt;code&gt;ns&lt;/code&gt; is most often used in conjunction with &lt;code&gt;lm&lt;/code&gt;, which includes an intercept implicitly (unless forced not to). If you use &lt;code&gt;intercept=TRUE&lt;/code&gt; in your call to &lt;code&gt;ns&lt;/code&gt;, make sure you know &lt;strong&gt;why&lt;/strong&gt; you're doing so, since if you do this and then call &lt;code&gt;lm&lt;/code&gt; naively, the design matrix will end up being rank deficient.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Strategies for placing knots&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Knots are most commonly placed at quantiles, like the default behavior of &lt;code&gt;ns&lt;/code&gt;. The intuition is that if you have lots of data clustered close together, then you might want more knots there to model any potential nonlinearities in that region. But, that doesn't mean this is either (a) the only choice or (b) the best choice.&lt;/p&gt;&#10;&#10;&lt;p&gt;Other choices can obviously be made and are domain-specific. Looking at histograms and density estimates of your predictors may provide clues as to where knots are needed, unless there is some &quot;canonical&quot; choice given your data.&lt;/p&gt;&#10;&#10;&lt;p&gt;In terms of interpreting regressions, I would note that, while you can certainly &quot;play around&quot; with knot placement, you should realize that you incur a model-selection penalty for this that you should be careful to evaluate and should adjust any inferences as a result.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-02-17T04:00:30.850" Id="7317" LastActivityDate="2011-09-27T00:25:53.663" LastEditDate="2011-09-27T00:25:53.663" LastEditorUserId="2970" OwnerUserId="2970" ParentId="7316" PostTypeId="2" Score="14" />
  <row Body="&lt;p&gt;&lt;a href=&quot;http://arxiv.org/abs/1102.2166&quot; rel=&quot;nofollow&quot;&gt;This paper&lt;/a&gt; uses a &lt;a href=&quot;http://people.maths.ox.ac.uk/~porterm/data/facebook100.zip&quot; rel=&quot;nofollow&quot;&gt;facebook dataset&lt;/a&gt; that is available here. Here is the description from the authors:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;The data includes the complete set of nodes and links (and some&#10;  demographic information) from &lt;strong&gt;100&lt;/strong&gt; US colleges and universities from a&#10;  single-time snapshot in September 2005.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="0" CommunityOwnedDate="2011-02-17T10:55:37.620" CreationDate="2011-02-17T10:55:37.620" Id="7325" LastActivityDate="2011-02-17T10:55:37.620" OwnerUserId="3291" ParentId="4451" PostTypeId="2" Score="0" />
  
  
  <row Body="&lt;p&gt;From past experience with a network of 7 million nodes, I think visualizing your complete network will give you an uninterpretable image. I might suggest different visualizations using subsets of your data such as just using the top 10 nodes with the most inbound or outbound links. I second celenius's suggestion on using gephi.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-02-17T16:06:24.620" Id="7335" LastActivityDate="2011-02-17T16:06:24.620" OwnerUserId="3298" ParentId="7270" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;You need to use a RNG specifically designed for parallel computing.  See the &quot;Parallel computing: Random numbers&quot; section of the &lt;a href=&quot;http://cran.r-project.org/web/views/HighPerformanceComputing.html&quot; rel=&quot;nofollow&quot;&gt;High Performance Computing Task View&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-02-17T16:39:34.187" Id="7337" LastActivityDate="2011-02-17T16:39:34.187" OwnerUserId="1657" ParentId="7336" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;I did not participate to the beta, but I have been very happy to contribute (Q&amp;amp;A, edits, votes, etc.) during the last six months. Despite being also very happy with our actual moderators, I would like to propose myself for fulfilling this task if one of them feel the need to get some time off.&lt;/p&gt;&#10;&#10;&lt;p&gt;Needless to say, I will vote for them if they nominate themselves for a second round.&lt;/p&gt;&#10;" CommentCount="4" CommunityOwnedDate="2011-02-17T21:56:24.370" CreationDate="2011-02-17T21:56:24.370" Id="7346" LastActivityDate="2011-02-17T21:56:24.370" LastEditDate="2011-02-17T21:56:24.370" LastEditorUserId="930" OwnerUserId="930" PostTypeId="6" Score="0" />
  
  
  <row Body="&lt;p&gt;The criterion for a 'trivial' effect size (odds ratio in your example) should be decided based on the size of effect that would be considered 'trivial' in the particular scenario, rather than on statistical grounds. If you're looking at an intervention that may be given to a considerable segment of the population with few side-effects and may prevent early death in a few (statins are one example that come to my mind, but you're the medic), then a small reduction in death rates might still be important, so a trivial reduction could perhaps be 1% or less, i.e. an odds ratio of 0.99 or closer to 1. If you're looking at an invasive or costly intervention or one with severe side-effects, or a condition that is an irritation or of short duration, the trivial reduction would be very much larger.&lt;/p&gt;&#10;&#10;&lt;p&gt;Rosenthal's original fail-safe N based on statistical significance assumed the mean effect size in missing studies was the null effect size. Orwin's method allows you to choose this, but the null effect size remains the simplest choice.&lt;/p&gt;&#10;&#10;&lt;p&gt;Having said all that, I don't like either Rosenthal's or Orwin's 'fail-safe N' myself (though I prefer Orwin's to Rosenthal's). As Rosenberg points out in the abstract of the paper below, they &quot;are unweighted and are not based on the framework in which most meta-analyses are performed&quot;. He suggests a general, weighted fail-safe N using either the fixed- or random-effects frameworks that are far more commonly used for meta-analysis.&lt;/p&gt;&#10;&#10;&lt;p&gt;Michael S. Rosenberg. &lt;a href=&quot;http://dx.doi.org/10.1111/j.0014-3820.2005.tb01004.x&quot; rel=&quot;nofollow&quot;&gt;The file-drawer problem revisited: a general weighted method for calculating fail-safe numbers in meta-analysis.&lt;/a&gt; &lt;em&gt;Evolution&lt;/em&gt; &lt;strong&gt;59&lt;/strong&gt; (2):464-468, 2005.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-02-18T10:01:57.817" Id="7368" LastActivityDate="2011-02-18T10:01:57.817" OwnerUserId="449" ParentId="7362" PostTypeId="2" Score="6" />
  <row AcceptedAnswerId="14670" AnswerCount="1" Body="&lt;p&gt;I'm reading through someone else's code for plotting the results of a psychology experiment, and (according to the code comments) they calculate the accuracy error of their behavioral paradigm as follows:&lt;/p&gt;&#10;&#10;&lt;p&gt;$\textit{accuracy error} = \sqrt{\frac{(\textit{accuracy}) (1-\textit{accuracy})}{\textit{total trials}}}$&lt;/p&gt;&#10;&#10;&lt;p&gt;It's output seems to be very similar to the original accuracy. What is this? Is this some sort of multiple comparison correction? Why would they do this?&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2011-02-18T16:40:42.403" FavoriteCount="1" Id="7379" LastActivityDate="2011-08-23T00:38:20.873" LastEditDate="2011-02-18T16:54:52.120" LastEditorUserId="919" OwnerUserId="2019" PostTypeId="1" Score="4" Tags="&lt;binomial&gt;&lt;error&gt;" Title="What is this measure of error?" ViewCount="164" />
  <row Body="&lt;p&gt;Hmm, good first question! I don't know of a package that meets your precise requirements. I think Stata's &lt;a href=&quot;http://www.stata.com/help.cgi?xtgee&quot;&gt;xtgee&lt;/a&gt; is a good choice if you also specify the &lt;code&gt;vce(robust)&lt;/code&gt; option to give Huber-White standard errors, or &lt;code&gt;vce(bootstrap)&lt;/code&gt; if that's practical. Either of these options will ensure the standard errors are consistently estimated despite the model misspecification that you'll have by ignoring the zero truncation.&lt;/p&gt;&#10;&#10;&lt;p&gt;That leaves the question of what effect ignoring the zero truncation will have on the point estimate(s) of interest to you. It's worth a quick search to see if there is relevant literature on this in general, i.e. not necessarily in a GEE context -- i would have thought you can pretty safely assume any such results will be relevant in the GEE case too. If you can't find anything, you could always simulate data with zero-truncation and known effect estimates and assess the bias by simulation.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-02-19T10:01:05.533" Id="7392" LastActivityDate="2011-02-19T10:01:05.533" OwnerUserId="449" ParentId="7385" PostTypeId="2" Score="9" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I recently stumbled upon the concept of &lt;a href=&quot;http://www.google.com/search?q=%22sample%20complexity%22&quot; rel=&quot;nofollow&quot;&gt;&lt;strong&gt;sample complexity&lt;/strong&gt;&lt;/a&gt;, and was wondering if there are any texts, papers or tutorials that provide:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;An introduction to the concept (rigorous or informal)&lt;/li&gt;&#10;&lt;li&gt;An analysis of the sample complexity of established and popular classification methods or kernel methods.&lt;/li&gt;&#10;&lt;li&gt;Advice or information on how to measure it in practice.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Any help with the topic would be greatly appreciated.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-02-19T22:41:23.000" FavoriteCount="1" Id="7407" LastActivityDate="2014-10-28T12:27:00.393" LastEditDate="2012-05-02T14:19:06.373" LastEditorUserId="2798" OwnerUserId="2798" PostTypeId="1" Score="4" Tags="&lt;machine-learning&gt;" Title="Measuring and analyzing sample complexity" ViewCount="163" />
&#10;~     &amp;amp; y_{1} &amp;amp; y_{2} &amp;amp; \Sigma \\\hline
  
  
  <row Body="&lt;p&gt;Here are two further integrated projects:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Python &lt;a href=&quot;http://www.nltk.org/&quot; rel=&quot;nofollow&quot;&gt;Natural Language Toolkit&lt;/a&gt; (easy installation, good documentation)&lt;/li&gt;&#10;&lt;li&gt;Java &lt;a href=&quot;http://mallet.cs.umass.edu/&quot; rel=&quot;nofollow&quot;&gt;MALLET&lt;/a&gt; (no experience with it, but looks promising; included in the link given by @Nick)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Both are open-source software.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-02-20T09:20:16.843" Id="7413" LastActivityDate="2011-02-20T09:20:16.843" OwnerUserId="930" ParentId="7211" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;For R two options spring to mind, both of which I am only vaguely familiar with at best.&lt;/p&gt;&#10;&#10;&lt;p&gt;The first is the &lt;code&gt;pscl&lt;/code&gt; package, which can fit zero &lt;strike&gt;truncated&lt;/strike&gt; inflated and hurdle models in a very nice, flexible manner. The &lt;code&gt;pscl&lt;/code&gt; package suggests the use of the &lt;code&gt;sandwich&lt;/code&gt; package which provides &quot;Model-robust standard error estimators for cross-sectional, time series and longitudinal data&quot;. So you could fit your count model and then use the &lt;code&gt;sandwich&lt;/code&gt; package to estimate an appropriate covariance matrix for the residuals taking into account the longitudinal nature of the data.&lt;/p&gt;&#10;&#10;&lt;p&gt;The second option might be to look the &lt;code&gt;geepack&lt;/code&gt; package which looks like it can do what you want but only for a negative binomial model with known theta, as it will fit any type of GLM that R's &lt;code&gt;glm()&lt;/code&gt; function can (so use the family function from MASS).&lt;/p&gt;&#10;&#10;&lt;p&gt;A third option has raised it's head: &lt;code&gt;gamlss&lt;/code&gt; and it's add-on package &lt;code&gt;gamlss.tr&lt;/code&gt;. The latter includes a function &lt;code&gt;gen.trun()&lt;/code&gt; that can turn any of the distributions supported by &lt;code&gt;gamlss()&lt;/code&gt; into a truncated distribution in a flexible way - you can specify left truncated at 0 negative binomial distribution for example. &lt;code&gt;gamlss()&lt;/code&gt; itself includes support for random effects which should take care of the longitudinal nature of the data. It isn't immediately clear however if you have to use at least one smooth function of a covariate in the model or can just model everything as linear functions like in a GLM.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-02-20T11:51:29.197" Id="7418" LastActivityDate="2012-01-21T18:35:40.003" LastEditDate="2012-01-21T18:35:40.003" LastEditorUserId="1390" OwnerUserId="1390" ParentId="7385" PostTypeId="2" Score="11" />
  
  
  <row Body="&lt;p&gt;I don't use PASW anymore, but implementation of the Egger's test for asymmetry is quite simple. First please look at the Egger's &lt;a href=&quot;http://goo.gl/6gnEj&quot; rel=&quot;nofollow&quot;&gt;paper&lt;/a&gt; where he propose &quot;theory&quot; behind the test.&lt;/p&gt;&#10;&#10;&lt;p&gt;Basically you have two variables: (i) normalized effect estimate (your estimate divided by its standard error), and (ii) precision (reciprocal of the standard error of the estimate). Then you should conduct simple linear regression and test for intercept $\beta_0 = 0$.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-02-20T21:53:59.553" Id="7427" LastActivityDate="2011-02-23T04:50:37.073" LastEditDate="2011-02-23T04:50:37.073" LastEditorUserId="609" OwnerUserId="609" ParentId="7426" PostTypeId="2" Score="2" />
  
  <row AcceptedAnswerId="7434" AnswerCount="2" Body="&lt;p&gt;A uniform prior for a scale parameter (like the variance) is uniform on the logarithmic scale. &lt;/p&gt;&#10;&#10;&lt;p&gt;What functional form does this prior have on the linear scale? And why so?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-02-21T02:44:28.760" Id="7430" LastActivityDate="2011-03-27T09:09:00.470" LastEditDate="2011-02-21T05:59:30.047" LastEditorUserId="183" OwnerUserId="1098" PostTypeId="1" Score="3" Tags="&lt;bayesian&gt;&lt;prior&gt;" Title="Creating a uniform prior on the logarithmic scale" ViewCount="948" />
  
  <row Body="&lt;p&gt;To supplement caracal's answer, if you are looking for a user-friendly GUI option for calculating Type II error rates or power for many common designs including the ones implied by your question, you may wish to check out the free software, &lt;a href=&quot;http://www.psycho.uni-duesseldorf.de/abteilungen/aap/gpower3/&quot; rel=&quot;nofollow&quot;&gt;G Power 3&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-02-21T06:37:28.657" Id="7435" LastActivityDate="2011-02-21T06:37:28.657" OwnerUserId="183" ParentId="7402" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;If you are using &lt;a href=&quot;http://en.wikipedia.org/wiki/Pearson%27s_chi-square_test#Test_of_independence&quot; rel=&quot;nofollow&quot;&gt;Pearson's chi-square test as a test independence&lt;/a&gt; of two variables in a two-way &lt;a href=&quot;http://en.wikipedia.org/wiki/Contingency_table&quot; rel=&quot;nofollow&quot;&gt;contingency table&lt;/a&gt;, you'll get a zero &lt;em&gt;expected&lt;/em&gt; value only if you have a whole row of zeroes or a whole column of zeroes. You can simply remove that row or column.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-02-21T08:14:45.830" Id="7436" LastActivityDate="2011-02-21T08:14:45.830" OwnerUserId="449" ParentId="7429" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;The paper&lt;/p&gt;&#10;&#10;&lt;p&gt;Clauset, A &lt;em&gt;et al&lt;/em&gt;, &lt;a href=&quot;http://arxiv.org/abs/0706.1062&quot; rel=&quot;nofollow&quot;&gt;Power-law Distributions in Empirical Data&lt;/a&gt;. 2009&lt;/p&gt;&#10;&#10;&lt;p&gt;contains a very good description of how to go about fitting power law models. The associated &lt;a href=&quot;http://tuvalu.santafe.edu/~aaronc/powerlaws/&quot; rel=&quot;nofollow&quot;&gt;web-page&lt;/a&gt; has code samples. Unfortunately, it doesn't give code for truncated distributions, but it may give you a pointer.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;As an aside, the paper discusses the fact that many &quot;power-law datasets&quot; can be modelled equally well (and in some cases better) with the Log normal or exponential distributions!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-02-21T14:36:44.550" Id="7452" LastActivityDate="2011-02-21T14:36:44.550" OwnerUserId="8" ParentId="7450" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;I would go to the curriculum websites of the top stats schools, write down the books they use in their undergrad courses, see which ones are highly rated on Amazon, and order them at your public/university library.&lt;/p&gt;&#10;&#10;&lt;p&gt;Some schools to consider:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://statistics.mit.edu/classes.php&quot; rel=&quot;nofollow&quot;&gt;MIT&lt;/a&gt; - technically, cross-taught with Harvard. &lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://catalog.caltech.edu/pdf/catalog_10_11.pdf&quot; rel=&quot;nofollow&quot;&gt;Caltech&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://www.stat.cmu.edu/programs/undergraduate/the-major-in-statistics&quot; rel=&quot;nofollow&quot;&gt;Carnegie Mellon&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://www-stat.stanford.edu/academics/undergrad.html&quot; rel=&quot;nofollow&quot;&gt;Stanford&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Supplement the texts with the various lecture video sites such as MIT OCW and videolectures.net.&lt;/p&gt;&#10;&#10;&lt;p&gt;Caltech doesn't have an undergrad degree in statistics, but you won't go wrong by following the curriculum of their undergrad stats courses. &lt;/p&gt;&#10;" CommentCount="4" CommunityOwnedDate="2011-02-21T19:51:03.823" CreationDate="2011-02-21T19:51:03.823" Id="7465" LastActivityDate="2011-02-22T00:26:09.117" LastEditDate="2011-02-22T00:26:09.117" LastEditorUserId="74" OwnerUserId="74" ParentId="6538" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;&lt;strong&gt;(Very) short story&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Long story short, in some sense, statistics is like any other technical field: &lt;strong&gt;&lt;a href=&quot;http://norvig.com/21-days.html&quot;&gt;There is no fast track&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Long story&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Bachelor's degree programs in statistics are relatively rare in the U.S. One reason I believe this is true is that it is quite hard to pack all that is necessary to learn statistics well into an undergraduate curriculum. This holds particularly true at universities that have significant general-education requirements.&lt;/p&gt;&#10;&#10;&lt;p&gt;Developing the necessary skills: mathematical, computational, and intuitive takes a lot of effort and time. Statistics can begin to be understood at a fairly decent &quot;operational&quot; level once the student has mastered calculus and a decent amount of linear and matrix algebra. However, any applied statistician knows that it is quite easy to find oneself in territory that doesn't conform to a cookie-cutter or recipe-based approach to statistics. To really understand what is going on beneath the surface requires as a &lt;em&gt;prerequisite&lt;/em&gt; mathematical and, in today's world, computational maturity that are only really attainable in the later years of undergraduate training. This is one reason that true statistical training mostly starts at the M.S. level in the U.S. (India, with their dedicated ISI is a little different story. A similar argument might be made for some Canadian-based education. I'm not familiar enough with European-based or Russian-based undergraduate statistics education to have an informed opinion.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Nearly any (interesting) job would require an M.S. level education and the &lt;em&gt;really&lt;/em&gt; interesting (in my opinion) jobs essentially require a doctorate-level education. &lt;/p&gt;&#10;&#10;&lt;p&gt;Seeing as you have a doctorate in mathematics, though we don't know in what area, here are my suggestions for something closer to an M.S.-level education. I include some parenthetical remarks to explain the choices.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;D. Huff, &lt;em&gt;How to Lie with Statistics&lt;/em&gt;. (Very quick, easy read. Shows many of the conceptual ideas and pitfalls, in particular, in presenting statistics to the layman.)&lt;/li&gt;&#10;&lt;li&gt;Mood, Graybill, and Boes, &lt;em&gt;Introduction to the Theory of Statistics&lt;/em&gt;, 3rd ed., 1974. (M.S.-level intro to theoretical statistics. You'll learn about sampling distributions, point estimation and hypothesis testing in a classical, frequentist framework. My opinion is that this is generally better, and a bit more advanced, than modern counterparts such as Casella &amp;amp; Berger or Rice.)&lt;/li&gt;&#10;&lt;li&gt;Seber &amp;amp; Lee, &lt;em&gt;Linear Regression Analysis&lt;/em&gt;, 2nd ed. (Lays out the theory behind point estimation and hypothesis testing for linear models, which is probably the most important topic to understand in applied statistics. Since you probably have a good linear algebra background, you should immediately be able to understand what is going on geometrically, which provides a lot of intuition. Also has good information related to assessment issues in model selection, departures from assumptions, prediction, and robust versions of linear models.)&lt;/li&gt;&#10;&lt;li&gt;Hastie, Tibshirani, and Friedman, &lt;em&gt;Elements of Statistical Learning&lt;/em&gt;, 2nd ed., 2009. (This book has a much more applied feeling than the last and broadly covers lots of modern machine-learning topics. The major contribution here is in providing statistical interpretations of many machine-learning ideas, which pays off particularly in quantifying uncertainty in such models. This is something that tends to go un(der)addressed in typical machine-learning books. Legally available for free &lt;strong&gt;&lt;a href=&quot;http://www-stat.stanford.edu/~tibs/ElemStatLearn/&quot;&gt;here&lt;/a&gt;&lt;/strong&gt;.)&lt;/li&gt;&#10;&lt;li&gt;A. Agresti, &lt;em&gt;Categorical Data Analysis&lt;/em&gt;, 2nd ed. (Good presentation of how to deal with discrete data in a statistical framework. Good theory and good practical examples. Perhaps on the traditional side in some respects.)&lt;/li&gt;&#10;&lt;li&gt;Boyd &amp;amp; Vandenberghe, &lt;em&gt;Convex Optimization&lt;/em&gt;. (Many of the most popular modern statistical estimation and hypothesis-testing problems can be formulated as convex optimization problems. This also goes for numerous machine-learning techniques, e.g., SVMs. Having a broader understanding and the ability to recognize such problems as convex programs is quite valuable, I think. Legally available for free &lt;strong&gt;&lt;a href=&quot;http://www.stanford.edu/~boyd/cvxbook/&quot;&gt;here&lt;/a&gt;&lt;/strong&gt;.)&lt;/li&gt;&#10;&lt;li&gt;Efron &amp;amp; Tibshirani, &lt;em&gt;An Introduction to the Bootstrap&lt;/em&gt;. (You ought to at least be familiar with the bootstrap and related techniques. For a textbook, it's a quick and easy read.)&lt;/li&gt;&#10;&lt;li&gt;J. Liu, &lt;em&gt;Monte Carlo Strategies in Scientific Computing&lt;/em&gt; or P. Glasserman, &lt;em&gt;Monte Carlo Methods in Financial Engineering&lt;/em&gt;. (The latter sounds very directed to a particular application area, but I think it'll give a good overview and practical examples of all the most important techniques. Financial engineering applications have driven a fair amount of Monte Carlo research over the last decade or so.)&lt;/li&gt;&#10;&lt;li&gt;E. Tufte, &lt;em&gt;The Visual Display of Quantitative Information&lt;/em&gt;. (Good visualization and presentation of data is [highly] underrated, even by statisticians.)&lt;/li&gt;&#10;&lt;li&gt;J. Tukey, &lt;em&gt;Exploratory Data Analysis&lt;/em&gt;. (Standard. Oldie, but goodie. Some might say outdated, but still worth having a look at.)&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Complements&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Here are some other books, mostly of a little more advanced, theoretical and/or auxiliary nature, that are helpful.&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;F. A. Graybill, &lt;em&gt;Theory and Application of the Linear Model&lt;/em&gt;. (Old fashioned, terrible typesetting, but covers all the same ground of Seber &amp;amp; Lee, and more. I say old-fashioned because more modern treatments would probably tend to use the SVD to unify and simplify a lot of the techniques and proofs.)&lt;/li&gt;&#10;&lt;li&gt;F. A. Graybill, &lt;em&gt;Matrices with Applications in Statistics&lt;/em&gt;. (Companion text to the above. A wealth of good matrix algebra results useful to statistics here. Great desk reference.)&lt;/li&gt;&#10;&lt;li&gt;Devroye, Gyorfi, and Lugosi, &lt;em&gt;A Probabilistic Theory of Pattern Recognition&lt;/em&gt;. (Rigorous and theoretical text on quantifying performance in classification problems.)&lt;/li&gt;&#10;&lt;li&gt;Brockwell &amp;amp; Davis, &lt;em&gt;Time Series: Theory and Methods&lt;/em&gt;. (Classical time-series analysis. Theoretical treatment. For more applied ones, Box, Jenkins &amp;amp; Reinsel or Ruey Tsay's texts are decent.)&lt;/li&gt;&#10;&lt;li&gt;Motwani and Raghavan, &lt;em&gt;Randomized Algorithms&lt;/em&gt;. (Probabilistic methods and analysis for computational algorithms.)&lt;/li&gt;&#10;&lt;li&gt;D. Williams, &lt;em&gt;Probability and Martingales&lt;/em&gt; and/or R. Durrett, &lt;em&gt;Probability: Theory and Examples&lt;/em&gt;. (In case you've seen measure theory, say, at the level of D. L. Cohn, but maybe not probability theory. Both are good for getting quickly up to speed if you already know measure theory.)&lt;/li&gt;&#10;&lt;li&gt;F. Harrell, &lt;em&gt;Regression Modeling Strategies&lt;/em&gt;. (Not as good as &lt;em&gt;Elements of Statistical Learning&lt;/em&gt; [ESL], but has a different, and interesting, take on things. Covers more &quot;traditional&quot; applied statistics topics than does ESL and so worth knowing about, for sure.)&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;More Advanced (Doctorate-Level) Texts&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Lehman and Casella, &lt;em&gt;Theory of Point Estimation&lt;/em&gt;. (PhD-level treatment of point estimation. Part of the challenge of this book is reading it and figuring out what is a typo and what is not. When you see yourself recognizing them quickly, you'll know you understand. There's plenty of practice of this type in there, especially if you dive into the problems.)&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Lehmann and Romano, &lt;em&gt;Testing Statistical Hypotheses&lt;/em&gt;. (PhD-level treatment of hypothesis testing. Not as many typos as TPE above.)&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;A. van der Vaart, &lt;em&gt;Asymptotic Statistics&lt;/em&gt;. (A beautiful book on the asymptotic theory of statistics with good hints on application areas. Not an applied book though. My only quibble is that some rather bizarre notation is used and details are at times brushed under the rug.)&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="20" CommunityOwnedDate="2011-02-22T02:08:30.283" CreationDate="2011-02-22T02:08:30.283" Id="7477" LastActivityDate="2011-02-26T17:19:06.130" LastEditDate="2011-02-26T17:19:06.130" LastEditorUserId="2970" OwnerUserId="2970" ParentId="6538" PostTypeId="2" Score="45" />
  <row AnswerCount="3" Body="&lt;p&gt;I've performed a study which yielded (?) the following results:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;-                   no bike box      bike box     % change&#10;correct procedure      173              55           -27%&#10;incorrect procedure    68               50           69%&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Since a result could only be one of the two - correct and incorrect procedure, do I need both quantities in my data, or should I just interpret one of them? If so, which one should I use, or does it depend what i'm trying to demonstrate?&lt;/p&gt;&#10;&#10;&lt;p&gt;Sorry for being so vague, but I hope this is enough to answer my question.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-02-22T02:32:12.893" Id="7478" LastActivityDate="2011-02-22T23:27:11.683" LastEditDate="2011-02-22T07:47:37.857" LastEditorUserId="2116" OwnerUserId="3357" PostTypeId="1" Score="3" Tags="&lt;basic-concepts&gt;" Title="How to compare outcomes from single variable experiment?" ViewCount="229" />
  
  <row AcceptedAnswerId="7507" AnswerCount="2" Body="&lt;p&gt;There is a catalog of noninformative priors over here:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.stats.org.uk/priors/noninformative/YangBerger1998.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.stats.org.uk/priors/noninformative/YangBerger1998.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;in page 11, they give the noninformative Jeffreys prior for the Dirichlet distribution. They give the Fisher information matrix for the Dirichlet. Can someone tell me exactly what is cell (i,j) there for the matrix?&lt;/p&gt;&#10;&#10;&lt;p&gt;Is it all 0s, except for the diagonals and the upper right element and the bottom left element?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-02-22T14:24:42.300" FavoriteCount="1" Id="7494" LastActivityDate="2011-02-22T16:26:30.153" LastEditDate="2011-02-22T14:48:49.360" LastEditorUserId="2116" OwnerUserId="3347" PostTypeId="1" Score="3" Tags="&lt;distributions&gt;" Title="Fisher information matrix for the Dirichlet distribution" ViewCount="481" />
  
  <row Body="&lt;p&gt;The standard approach is to use the &lt;strong&gt;partial F test&lt;/strong&gt;, explained on fine websites all over town.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-02-22T14:37:56.237" Id="7496" LastActivityDate="2011-02-22T14:37:56.237" OwnerUserId="5792" ParentId="7487" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;&lt;strong&gt;Short answer.&lt;/strong&gt;&#10;The problem you mention is well studied by Granger C.W.J. with co-authors, and known as the forecasts combination (or pooling) problem. The general idea is to choose the loss function criterion and the parameters (may be time dependent) that minimize the latter. Below I put some references that may be useful (only publicly available, look for the original works in references after the text).&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://www2.warwick.ac.uk/fac/soc/economics/staff/academic/wallis/publications/wallis_obes_05.pdf&quot; rel=&quot;nofollow&quot;&gt;K.F.Wallis&lt;/a&gt; Combining Density and Interval Forecasts: A Modest Proposal // &lt;em&gt;Oxford bulletin of economics and statistics, 67, supplement (2005) 0305-9049&lt;/em&gt; (provides a general idea of how to combine interval forecasts, though there is no details on how to choose the weights)&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://management.ucsd.edu/faculty/directory/timmermann/docs/forecast-combinations.pdf&quot; rel=&quot;nofollow&quot;&gt;Allan Timmermann&lt;/a&gt; Forecast combinations. (a survey on different aspects of the forecast combinations by one of the co-editors of &lt;a href=&quot;http://books.google.lt/books?id=FSejuLtBaHUC&amp;amp;lpg=PA522&amp;amp;ots=ljqi12N_Vz&amp;amp;dq=Granger%20forecasts%20combining&amp;amp;pg=PP1#v=onepage&amp;amp;q=Granger%20forecasts%20combining&amp;amp;f=false&quot; rel=&quot;nofollow&quot;&gt;Handbook of economic forecasting&lt;/a&gt; that I would like to study myself)&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Hoping for the &lt;strong&gt;longer answer&lt;/strong&gt; from the community.  &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-02-22T15:26:12.237" Id="7501" LastActivityDate="2011-02-22T15:26:12.237" OwnerUserId="2645" ParentId="7251" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;What you suggest should work.  See if this makes sense:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lm(c(8000, 50000, 116000) ~ c(6, 7, 8))&#10;lm(c(8000, 50000, 116000) ~ c(6, 7, 8), weight = c(123, 123, 246))&#10;lm(c(8000, 50000, 116000, 116000) ~ c(6, 7, 8, 8))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The second line produces the same intercept and slope as the third line (distinct from the first line's result), by giving one observation relatively twice the weight of each of the other two observations, similar to the impact of duplicating the third observation.  &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-02-22T23:05:22.627" Id="7520" LastActivityDate="2011-02-22T23:05:22.627" OwnerUserId="2958" ParentId="7513" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="7529" AnswerCount="1" Body="&lt;p&gt;I'm trying to implement a &quot;change point&quot; analysis, or a multiphase regression using &lt;code&gt;nls()&lt;/code&gt; in R. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/27f1S.png&quot;&gt;Here's some fake data I've made&lt;/a&gt;. The formula I want to use to fit the data is:&lt;/p&gt;&#10;&#10;&lt;p&gt;$y = \beta_0 + \beta_1x + \beta_2\max(0,x-\delta)$&lt;/p&gt;&#10;&#10;&lt;p&gt;What this is supposed to do is fit the data up to a certain point with a certain intercept and slope ($\beta_0$ and $\beta_1$), then, after a certain x value ($\delta$), augment the slope by $\beta_2$. That's what the whole max thing is about. Before the $\delta$ point, it'll equal 0, and $\beta_2$ will be zeroed out.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, here's my function to do this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;changePoint &amp;lt;- function(x, b0, slope1, slope2, delta){ &#10;   fit &amp;lt;- b0 + (x*slope1) + (max(0, x-delta) * slope2)&#10;}&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;And I try to fit the model this way&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;nls(y ~ changePoint(x, b0, slope1, slope2, delta), &#10;    data = data, &#10;    start = c(b0 = 50, slope1 = 0, slope2 = 2, delta = 48))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I chose those starting parameters, because I &lt;em&gt;know&lt;/em&gt; those are the starting parameters, because I made the data up.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, I get this error:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Error in nlsModel(formula, mf, start, wts) : &#10;  singular gradient matrix at initial parameter estimates&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Have I just made unfortunate data? I tried fitting this on real data first, and was getting the same error, and I just figured that my initial starting parameters weren't good enough. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-02-23T03:27:05.930" FavoriteCount="5" Id="7527" LastActivityDate="2011-04-14T19:28:00.103" LastEditDate="2011-04-14T19:28:00.103" LastEditorUserId="88" OwnerUserId="287" PostTypeId="1" Score="12" Tags="&lt;r&gt;&lt;regression&gt;&lt;change-point&gt;&lt;nls&gt;" Title="Change point analysis using R's nls()" ViewCount="1410" />
  
  
  <row Body="&lt;h3&gt;The question:&lt;/h3&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;How can normality be validated without using visual cues such as QQ plots? (the validation will be a part of larger software)&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Can a &quot;goodness of fit&quot; score be calculated?&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Although enumerated separately, these parts are (appropriately) one question: you compute an appropriate goodness of fit and use that as a test statistic in a hypothesis test.&lt;/p&gt;&#10;&#10;&lt;h3&gt;Some answers&lt;/h3&gt;&#10;&#10;&lt;p&gt;There are plenty of such tests; the best among them are the Kolmogorov-Smirnov, Shapiro-Wilks, and Anderson-Darling tests.  Their properties have been extensively studied.  An excellent resource is the work of M. A. Stephens, especially the 1974 article, &lt;a href=&quot;http://www.jstor.org/pss/2286009&quot; rel=&quot;nofollow&quot;&gt;EDF Statistics for Goodness of Fit and Some Comparisons&lt;/a&gt;.  Rather than supply a long list of references, I will leave it to you to Google this title: the trail quickly leads to useful information.&lt;/p&gt;&#10;&#10;&lt;p&gt;One thing I like about Stephens' work, in addition to the comparisons of the properties of various GoF tests, is that it provides clear descriptions of how to compute the statistics and how to compute, or at least approximate, their null distributions.  This gives you the option to implement your favorite test yourself.  The EDF statistics (empirical distribution function) are easy to compute: they tend to be linear combinations of the order statistics, so all you have to do is sort the data and go.  The complications concern (a) computing the coefficients--this used to be a barrier in applying the S-W test, but good approximations now exist--and (b) computing the null distributions.  Most of those can be computed or have been adequately tabulated.&lt;/p&gt;&#10;&#10;&lt;p&gt;What is characteristic about any GoF tests for distributions is that (a) they need a certain amount of data to become powerful (for detecting true deviations) and (b) very quickly thereafter, as you acquire more data, they become &lt;em&gt;so&lt;/em&gt; powerful that deviations that are &lt;em&gt;practically&lt;/em&gt; inconsequential become &lt;em&gt;statistically significant.&lt;/em&gt;  (This is very well known and is easily confirmed with simulation or mathematical analysis.)  In this is the origin of the reluctance to answer the original question without obtaining substantial clarification.  If you have a few hundred values or more, you will find that any of these tests demonstrate your data are not &quot;normal.&quot;  But does this matter for your intended analysis?  We simply cannot say.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-02-23T16:21:47.927" Id="7543" LastActivityDate="2011-02-23T16:21:47.927" OwnerUserId="919" ParentId="7084" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;The standard error of the mean tells you how precise your estimate of the mean is; that doesn't seem to capture what you're trying to do.  I would use either a) a histogram, if you care mostly about showing variation, or b) a line chart or area chart, if you want to say something about variation while also showing progression over time.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-02-24T01:02:48.370" Id="7558" LastActivityDate="2011-02-24T01:02:48.370" OwnerUserId="2669" ParentId="7554" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Actually the @mpiktas comment is the answer to your particular question. Sales models are usually multiplicative by the nature (some intuition could be found in Market response  models &lt;a href=&quot;http://books.google.ru/books?id=xZyJamKdpIsC&amp;amp;printsec=frontcover&amp;amp;dq=market+response+models&amp;amp;source=bl&amp;amp;ots=0-G9pzeYtd&amp;amp;sig=Qgt51utD-Hcg1E0kgvgMdC8_2ZI&amp;amp;hl=ru&amp;amp;ei=3llmTeKjJtHJswa0yMHaDA&amp;amp;sa=X&amp;amp;oi=book_result&amp;amp;ct=result&amp;amp;resnum=4&amp;amp;ved=0CDsQ6AEwAw#v=onepage&amp;amp;q&amp;amp;f=false&quot; rel=&quot;nofollow&quot;&gt;book&lt;/a&gt;). There is also a number of reasons for logs discussed for ARIMA models in my earlier &lt;a href=&quot;http://stats.stackexchange.com/questions/6330/when-to-log-transform-a-time-series-before-fitting-an-arima-model/6348#6348&quot;&gt;post&lt;/a&gt;. In your case it is the &lt;strong&gt;scale effect&lt;/strong&gt; that troubles you, therefore log transformation works well here. Another useful trick is to divide by some size variable (plot of the store, number of workers, etc.), so moving to fractions could help also.&lt;/p&gt;&#10;&#10;&lt;p&gt;In &lt;strong&gt;addition&lt;/strong&gt; to your question. What you have to pay attention to are other important explanatory variables: location variables or density of the population, size, variety of products (categories) and there average prices, number of workers, distances to the rival shops etc. that will matter (omitting them will cause you some estimates with poor properties: probably biased and inconsistent). Regulation can't be put as the solo explanatory variable in this context.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-02-24T13:20:24.633" Id="7566" LastActivityDate="2011-02-24T13:20:24.633" OwnerUserId="2645" ParentId="7521" PostTypeId="2" Score="3" />
  <row AnswerCount="1" Body="&lt;p&gt;I'm trying to estimate the state of a Gaussian random walk with central tendency based on time series measurements with varying uncertainties.  My random variable has the following form:&lt;/p&gt;&#10;&#10;&lt;p&gt;$ \frac{d x}{d t} \equiv F(t) - \alpha x $&lt;/p&gt;&#10;&#10;&lt;p&gt;Where F is a Gaussian random variable.  I've noticed that this problem is analogous to the velocity of a bubble experiencing Brownian motion.  (See for example, F. Reif, &lt;em&gt;Fundamentals of Statistical and Thermal Physics&lt;/em&gt;, p. 565).  As a result of the $ -\alpha x $ term, the position has a central tendency (i.e. the variance does not become infinity as time approaches infinity).&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, like any good physicist, I know that I cannot exactly measure the value $x$.  The best I can do is to measure it at time $ t_i $ within some uncertainty, $ \sigma_i $.  Using a Kalman filter, I can estimate the value of $x$ from several measurements.  Let's call that $ \hat x $.  The approach is as follows.  For each measurement, we compute:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;$ \delta t = t_i - t_{i-1} $&lt;/p&gt;&#10;  &#10;  &lt;p&gt;$ P(t) = P(t_{i-1}) * e^{-\alpha\,  \delta t} + \langle x^2 | \delta t \rangle $&lt;/p&gt;&#10;  &#10;  &lt;p&gt;$ K = {{P}\over{P + \sigma_i}} $&lt;/p&gt;&#10;  &#10;  &lt;p&gt;Our incoming est estimate of $x_i$:&lt;/p&gt;&#10;  &#10;  &lt;p&gt;$ \hat x_{i-} = \hat x_{i-1} \, e^{-\alpha\,\delta t} $&lt;/p&gt;&#10;  &#10;  &lt;p&gt;$ \hat x_i = \hat x_{i-} + K [x_{obs} - \hat x_{i-}] $&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;This works great for propagating our estimates forward in time.  My question is: If I have measurements at times that span the time at which I want the best estimate, how do I compute an $\hat x(t) $ where $ t_i &amp;lt; t &amp;lt; t_{i+1} $?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-02-24T14:48:35.657" FavoriteCount="1" Id="7571" LastActivityDate="2011-02-27T02:18:19.110" LastEditDate="2011-02-27T02:18:19.110" LastEditorUserId="3405" OwnerUserId="3405" PostTypeId="1" Score="4" Tags="&lt;regression&gt;&lt;estimation&gt;&lt;kalman-filter&gt;" Title="How to apply a Kalman filter to use both previous and future measurements of a random variable?" ViewCount="465" />
  
  <row AnswerCount="2" Body="&lt;p&gt;What is the relation between estimator and estimate?&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2011-02-24T18:57:13.633" FavoriteCount="4" Id="7581" LastActivityDate="2014-10-11T17:18:27.600" LastEditDate="2011-02-25T02:56:18.663" LastEditorUserId="183" OwnerDisplayName="user3412" PostTypeId="1" Score="8" Tags="&lt;estimation&gt;" Title="What is the relation between estimator and estimate?" ViewCount="9822" />
  <row Body="&lt;p&gt;You may want to check out &lt;code&gt;hurdle()&lt;/code&gt; from the &lt;code&gt;pscl&lt;/code&gt; package in R. It specifies two-component models, one that handles the zero counts and one that handles the positive counts. Check out the &lt;code&gt;hurdle&lt;/code&gt; help page &lt;a href=&quot;http://rss.acs.unt.edu/Rdoc/library/pscl/html/hurdle.html&quot; rel=&quot;nofollow&quot;&gt;here.&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT: I just found &lt;a href=&quot;http://r.789695.n4.nabble.com/Problems-using-gamlss-to-model-zero-inflated-and-overdispersed-count-data-quot-the-global-deviance-i-td2239925.html&quot; rel=&quot;nofollow&quot;&gt;this&lt;/a&gt; post in R help that describes the &lt;code&gt;zeroinf()&lt;/code&gt; function in R (also from the pscl package), as well as &lt;code&gt;gamlss&lt;/code&gt; and &lt;code&gt;VGAM&lt;/code&gt; options. However, I don't believe that the &lt;code&gt;VGAM&lt;/code&gt; options will allow you to take into account non-independent correlation structures.&lt;/p&gt;&#10;&#10;&lt;p&gt;Another option is the &lt;code&gt;zinb&lt;/code&gt; command in Stata. Fitting a model using the negative binomial family will account for the overdispersion.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am not sure if they allow for seasonality adjustments, however.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-02-24T19:21:15.930" Id="7582" LastActivityDate="2011-02-24T19:27:49.170" LastEditDate="2011-02-24T19:27:49.170" LastEditorUserId="3309" OwnerUserId="3309" ParentId="7579" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;It might be helpful to illustrate whuber's answer in the context of a linear regression model. Let's say you have some bivariate data and you use Ordinary Least Squares to come up with the following model:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&lt;code&gt;Y = 6X + 1&lt;/code&gt;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;At this point, you can take any value of X, plug it into the model and predict the outcome, Y. In this sense, you might think of the individual components of the generic form of the model (&lt;em&gt;mX + B&lt;/em&gt;) as &lt;em&gt;estimators&lt;/em&gt;. The sample data (which you presumably plugged into the generic model to calculate the specific values for &lt;em&gt;m&lt;/em&gt; and &lt;em&gt;B&lt;/em&gt; above) provided a basis on which you could come up with &lt;em&gt;estimates&lt;/em&gt; for &lt;em&gt;m&lt;/em&gt; and &lt;em&gt;B&lt;/em&gt; respectively.&lt;/p&gt;&#10;&#10;&lt;p&gt;Consistent with @whuber's points in our thread below, whatever values of &lt;em&gt;Y&lt;/em&gt; a particular set of estimators generate you for are, in the context of linear regression, thought of as predicted values.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;(edited -- a few times -- to reflect the comments below)&lt;/em&gt;&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-02-24T21:18:57.810" Id="7586" LastActivityDate="2011-02-24T22:37:39.223" LastEditDate="2011-02-24T22:37:39.223" LastEditorUserId="3396" OwnerUserId="3396" ParentId="7581" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;Another option for negative binomial regression in R is the excellent MASS package's &lt;code&gt;glm.nb()&lt;/code&gt; function. UCLA's statistical consulting group has a &lt;a href=&quot;http://www.ats.ucla.edu/stat/R/dae/nbreg.htm&quot; rel=&quot;nofollow&quot;&gt;pretty clear vignette&lt;/a&gt;, which unfortunately does not seem to provide any obvious insights into your autocorrelation issues, but maybe searching these various nb-regression options on R-seek or elsewhere would help?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-02-24T22:21:30.493" Id="7589" LastActivityDate="2011-02-24T22:21:30.493" OwnerUserId="3396" ParentId="7579" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;If you were considering conducting Poisson or related regressions on this data (with your outcome variable as a rate), remember to include an offset term for the patient bed days as it technically becomes the &quot;exposure&quot; to your counts.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, in that case, you may also want to consider using just the infection count (not the rate) as your dependent variable, and include the patient bed days as a covariate. I am working on a data set with a similar count vs. rate decision and it seems like converting your dependent variable to a rate leads to a decrease in variability, an increase in skewness and a proportionally larger standard deviation. This makes it more difficult to detect any significant effects.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also watch out if your data is zero-truncated or zero-inflated, and make the appropriate adjustments.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-02-24T22:29:52.487" Id="7590" LastActivityDate="2011-02-24T22:29:52.487" OwnerUserId="3309" ParentId="1099" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Sometimes I just go to crantastic and search for keywords&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://crantastic.org/search?q=Box+Cox&quot;&gt;Search for Box Cox on Crantastic&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-02-24T23:36:35.363" Id="7596" LastActivityDate="2011-02-24T23:36:35.363" OwnerUserId="569" ParentId="7591" PostTypeId="2" Score="5" />
  
  
  <row AcceptedAnswerId="7954" AnswerCount="1" Body="&lt;p&gt;&lt;strong&gt;What are the methods to minimize the effect of boundaries in a wavelet decomposition?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I use R and the package &lt;a href=&quot;http://cran.r-project.org/web/packages/waveslim/index.html&quot; rel=&quot;nofollow&quot;&gt;waveslim&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have found for instance the function &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;?brick.wall&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;but&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;I am not too use how to use it.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;I am not sure the best solution is to remove some coefficient. I have read somewhere that it exists some wavelets that are not the same everywhere and their shape change at the boudaries.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Any ideas?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-02-25T06:40:57.203" FavoriteCount="4" Id="7607" LastActivityDate="2013-04-23T05:16:07.760" LastEditDate="2011-03-12T10:03:39.110" LastEditorUserId="223" OwnerUserId="1709" PostTypeId="1" Score="8" Tags="&lt;r&gt;&lt;signal-processing&gt;&lt;wavelet&gt;" Title="Boundary effect in a wavelet multi resolution analysis" ViewCount="462" />
  
  
  
  <row Body="&lt;p&gt;OK, in my experience cases like this mean that you should have allowed the factors to correlate in the first place. You should probably rerun a factor analysis using either oblimin or promax rotation and test the fit of your uncorrelated model against your correlated model.&lt;/p&gt;&#10;&#10;&lt;p&gt;Please do note that SEM loses its utility as a method for testing theories once you start changing the model based on fit indices. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-02-25T11:42:09.717" Id="7614" LastActivityDate="2011-02-25T11:42:09.717" OwnerUserId="656" ParentId="7605" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;Gaussian process classifier (not using the Laplace approximation), preferably with marginalisation rather than optimisation of the hyper-parameters.  Why?&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;because they give a probabilistic classification&lt;/li&gt;&#10;&lt;li&gt;you can use a kernel function that allows you to operate directly on non-vectorial data and/or incorporate expert knowledge&lt;/li&gt;&#10;&lt;li&gt;they deal with the uncertainty in fitting the model properly, and you can propagate that uncertainty through to the decision making process&lt;/li&gt;&#10;&lt;li&gt;generally very good predictive performance.  &lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Downsides &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;slow &lt;/li&gt;&#10;&lt;li&gt;requires a lot of memory&lt;/li&gt;&#10;&lt;li&gt;impractical for large scale problems.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;First choice though would be regularised logistic regression or ridge regression [without feature selection] - for most problems, very simple algorithms work rather well and are more difficult to get wrong (in practice the differences in performance between algorithms is smaller than the differences in performance between the operator driving them).&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2011-02-25T13:25:01.677" CreationDate="2011-02-25T12:01:31.843" Id="7615" LastActivityDate="2011-02-25T16:30:28.397" LastEditDate="2011-02-25T16:30:28.397" LastEditorUserId="930" OwnerUserId="887" ParentId="7610" PostTypeId="2" Score="4" />
&#10;\frac{E[X_{j}]}{E[\sum_{i}X_{i}]} 
  
  <row Body="&lt;p&gt;If you have access to SAS 9.2 you could use PROC COUNTREG. It's a fairly new procedure and if you poke around the SAS site you can find out about it in the SAS/ETS(R) 9.2 User's Guide. COUNTREG does count modeling with or without zero inflation, has a &quot;by&quot; clause to split analyses, and allows both categorical and continuous variables. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-02-25T16:31:35.000" Id="7624" LastActivityDate="2011-02-25T16:31:35.000" OwnerUserId="3434" ParentId="7579" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Save your table as a csv file, with one header row that contains the names of your 4 variables.  Save the following code as an R script file in the same directory as your data, and then run it.  Lets say your variables are called X1, X2, X3, and Y, where X1, X2, X3 are your independent variables and Y is your dependent variable.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;MyData &amp;lt;- read.csv('path/to/MyData.csv')&#10;model &amp;lt;- lm(Y~X1+X2+X3,data=MyData)&#10;predictions &amp;lt;- predict(model)&#10;&#10;model&#10;plot(model)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Building a linear regression model and diagnosing it is relatively simple in R, even with 4000+ lines of data.  What exactly are you trying to do with this dataset?&lt;/p&gt;&#10;&#10;&lt;p&gt;You can build a non-linear model using the &quot;loess&quot; or &quot;glm&quot; commands, but specifying and diagnosing such models is more difficult than simple linear models.  Again, if you can be a bit clearer about what exactly you need to do, we can help you more.&lt;/p&gt;&#10;&#10;&lt;p&gt;Fitting a linear model with 3 dependent variables and 4000 observations isn't exactly unorthodox, and you can do it quickly and easily in R.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-02-26T02:06:46.497" Id="7642" LastActivityDate="2011-02-28T15:30:51.777" LastEditDate="2011-02-28T15:30:51.777" LastEditorUserId="2817" OwnerUserId="2817" ParentId="7639" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="7666" AnswerCount="1" Body="&lt;p&gt;I am looking for methods which can be used to estimate the &quot;OLS&quot; measurement error model.&lt;/p&gt;&#10;&#10;&lt;p&gt;$$y_{i}=Y_{i}+e_{y,i}$$&#10;$$x_{i}=X_{i}+e_{x,i}$$&#10;$$Y_{i}=\alpha + \beta X_{i}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Where the errors are independent normal with unknown variances $\sigma_{y}^{2}$ and $\sigma_{x}^{2}$.  &quot;Standard&quot; OLS won't work in this case.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Measurement_error_model&quot; rel=&quot;nofollow&quot;&gt;Wikipedia&lt;/a&gt; has some unappealing solutions - the two given force you to assume that either the &quot;variance ratio&quot; $\delta=\frac{\sigma_{y}^{2}}{\sigma_{x}^{2}}$ or the &quot;reliability ratio&quot; $\lambda=\frac{\sigma_{X}^{2}}{\sigma_{x}^{2}+\sigma_{X}^{2}}$ is known, where $\sigma_{X}^2$ is the variance of the true regressor $X_i$.  I am not satisfied by this, because how can someone who doesn't know the variances know their ratio?&lt;/p&gt;&#10;&#10;&lt;p&gt;Anyways, are there any other solutions besides these two which don't require me to &quot;know&quot; anything about the parameters?       &lt;/p&gt;&#10;&#10;&lt;p&gt;Solutions for just the intercept and slope are fine.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-02-26T03:19:55.543" FavoriteCount="2" Id="7644" LastActivityDate="2011-03-29T17:38:35.623" LastEditDate="2011-02-26T17:50:46.793" LastEditorUserId="2970" OwnerUserId="2392" PostTypeId="1" Score="9" Tags="&lt;regression&gt;&lt;estimation&gt;&lt;errors-in-variables&gt;" Title="Methods for fitting a &quot;simple&quot; measurement error model" ViewCount="390" />
  <row Body="&lt;p&gt;My initial thought would have been to display the probability of of acceptance as a function of relative GPA for each of your four schools, using some kind of &lt;a href=&quot;http://cm.bell-labs.com/cm/ms/departments/sia/project/trellis/&quot; rel=&quot;nofollow&quot;&gt;trellis displays&lt;/a&gt;. In this case, facetting should do the job well as the number of schools is not so large. This is very easy to do with &lt;a href=&quot;http://cran.r-project.org/web/packages/lattice/index.html&quot; rel=&quot;nofollow&quot;&gt;lattice&lt;/a&gt; (&lt;code&gt;y ~ gpa | school&lt;/code&gt;) or &lt;a href=&quot;http://had.co.nz/ggplot2/&quot; rel=&quot;nofollow&quot;&gt;ggplot2&lt;/a&gt; (&lt;code&gt;facet_grid(. ~ school)&lt;/code&gt;). In fact, you can choose the conditioning variable you want: this can be school, but also situation at undergrad institution. In the latter case, you'll have 4 curves for each plot, and three three plot of &lt;code&gt;Prob(admitting) ~ GPA&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, if you are looking for effective displays of effects in GLM, I would recommend the &lt;a href=&quot;http://cran.r-project.org/web/packages/effects/index.html&quot; rel=&quot;nofollow&quot;&gt;effects&lt;/a&gt; package, from John Fox. Currently, it works with binomial and multinomial link, and ordinal logistic model. Marginalizing over other covariates is handled internally, so you don't have to bother with that. There are a lot of illustrations in the on-line help, see &lt;code&gt;help(effect)&lt;/code&gt;. But, for a more thorough overview of effects displays in GLM, please refer to&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Fox (2003). &lt;a href=&quot;http://www.jstatsoft.org/v08/i15/paper&quot; rel=&quot;nofollow&quot;&gt;Effect Displays in R for Generalised Linear Models&lt;/a&gt;. JSS 8(15).&lt;/li&gt;&#10;&lt;li&gt;Fox and Andersen (2004). &lt;a href=&quot;http://socserv.mcmaster.ca/jfox/Misc/polytomous-effect-displays/polytomous-effect-displays.pdf&quot; rel=&quot;nofollow&quot;&gt;Effect displays for multinomial and proportional-odds logit models&lt;/a&gt;. ASA Methodology Conference -- Here is the corresponding &lt;a href=&quot;http://www.jstatsoft.org/v32/i01/paper&quot; rel=&quot;nofollow&quot;&gt;JSS paper&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="3" CreationDate="2011-02-26T09:08:28.737" Id="7650" LastActivityDate="2011-02-26T09:08:28.737" OwnerUserId="930" ParentId="7638" PostTypeId="2" Score="5" />
  <row AcceptedAnswerId="7658" AnswerCount="2" Body="&lt;p&gt;I read &lt;em&gt;&lt;a href=&quot;http://eu.wiley.com/WileyCDA/WileyTitle/productCd-0471727814.html&quot; rel=&quot;nofollow&quot;&gt;Clinical Trials, a Methodologic Perspective&lt;/a&gt;&lt;/em&gt; (S. Piantadosi) as I was suggested by one of you.&lt;/p&gt;&#10;&#10;&lt;p&gt;According to the author:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;A trialist must understand two&#10;  different modes of thinking that&#10;  support the science-clinical and&#10;  statistical. They both underlie the&#10;  re-emergence of therapeutics as a&#10;  modern science. Each method of&#10;  reasoning arose independently and must&#10;  be combined skillfully if they are to&#10;  serve therapeutic questions&#10;  effectively.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I cannot figure out what the author means by &quot;clinical reasoning&quot;. Can you help me to understand that notion?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you in advance&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-02-26T13:52:56.903" Id="7653" LastActivityDate="2011-02-27T10:47:08.083" LastEditDate="2011-02-27T10:47:08.083" LastEditorUserId="183" OwnerUserId="3019" PostTypeId="1" Score="4" Tags="&lt;clinical-trials&gt;" Title="In relation to clinical trials, what is clinical reasoning in contrast to statistical reasoning?" ViewCount="197" />
  <row Body="&lt;p&gt;I have not read the book, but my best guess would be that the author wants to points out that sometimes a critical reasoning has to be made when applying statistics to biological and medical issues.&lt;/p&gt;&#10;&#10;&lt;p&gt;The sole fact that, for instance, a treatment does not have a &quot;statistically significant&quot; effect does not imply that the treatment does not have a biological effect and viceversa. Statistics can tell you if a certain event is likely or unlikely to be happening, but does not give you any hint as to whether something is biologically plausible.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-02-26T14:46:08.097" Id="7655" LastActivityDate="2011-02-26T14:46:08.097" OwnerUserId="582" ParentId="7653" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;There are definitely more and less complex ways to address this kind of problem. From the sound of things, you started out with a fairly simple solution (the formula you found on SO). With that kind of simplicity in mind, I thought I would revisit a few key points you make in (the current version of) your post.&lt;/p&gt;&#10;&#10;&lt;p&gt;So far, you've said you want your measurement of &quot;site activity&quot; to capture:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Slope changes in visits/day over &quot;the past few&#10;days&quot;&lt;/li&gt;&#10;&lt;li&gt;Magnitude changes in visits/day over &quot;the past few days&quot;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;As @jan-galkowski points out, you also seem to be (at least tacitly) interested in the rank of the sites relative to each other along these dimensions.&lt;/p&gt;&#10;&#10;&lt;p&gt;If that description is accurate, I would propose exploring the simplest possible solution that incorporates those three measures (change, magnitude, rank) as separate components. For example, you could grab:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;The results of your SO solution to capture slope variation (although I would incorporate 3 or 4 days of data)&lt;/li&gt;&#10;&lt;li&gt;Magnitude of each site's most recent visits/day value &lt;code&gt;(y2)&lt;/code&gt; divided by the mean visits/day for that site (&lt;code&gt;Y&lt;/code&gt;):&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;y2 / mean(Y)&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;For W0, W1, and W2 respectively, that yields 0.16, 1.45, and 2.35. (For the sake of interpretation, consider that a site whose most recent visits-per-day value was equal to it's mean visits-per-day would generate a result of 1). Note that you could also adjust this measure to capture the most recent 2 (or more) days:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;y2 + y1 / 2 * mean(Y)&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;That yields: 0.12, 1.33, 1.91 for your three sample sites.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you do, in fact, use the mean of each site's visit/day distribution for this kind of measure, I would also look at the distribution's standard deviation to get a sense of its relative volatility. The standard deviation for each site's visit/day distribution is: 12.69, 12.12, and 17.62. Thinking about the &lt;code&gt;y2/mean(Y)&lt;/code&gt; measure relative to the standard deviation is helpful because it allows you to keep the recent magnitude of activity on site W2 in perspective (bigger standard deviation = less stable/consistent overall).&lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, if you're interested in ranks, you can extend these approaches in that direction too. For example, I would think that knowing a site's rank in terms of the most recent visits per day values as well as the rank of each site's mean visits per day (the rank of &lt;code&gt;mean (Y)&lt;/code&gt; for each &lt;code&gt;W&lt;/code&gt; in &lt;code&gt;Wn&lt;/code&gt;) could be useful. Again, you can tailor to suit your needs.&lt;/p&gt;&#10;&#10;&lt;p&gt;You could present the results of all these calculations as a table, or create a regularly-updated visualization to track them on a daily basis.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-02-26T14:46:45.880" Id="7656" LastActivityDate="2011-02-26T14:46:45.880" OwnerUserId="3396" ParentId="3955" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;p&gt;There are a range of possibilities described by J.W. Gillard in &lt;a href=&quot;http://www.cardiff.ac.uk/maths/resources/Gillard_Tech_Report.pdf&quot;&gt;An Historical Overview&#10;of Linear Regression with Errors in both Variables&lt;/a&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;If you are not interested in details or reasons for choosing one method over another, just go with the simplest, which is to draw the line through the centroid $(\bar{x},\bar{y})$ with slope $\hat{\beta}=s_y/s_x$, i.e. the ratio of the observed standard deviations (making the sign of the slope the same as the sign of the covariance of $x$ and $y$); as you can probably work out, this gives an intercept on the $y$-axis of $\hat{\alpha}=\bar{y}-\hat{\beta}\bar{x}.$  &lt;/p&gt;&#10;&#10;&lt;p&gt;The merits of this particular approach are &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;it gives the same line comparing $x$ against $y$ as $y$ against $x$, &lt;/li&gt;&#10;&lt;li&gt;it is scale-invariant so you do not need to worry about units,&lt;/li&gt;&#10;&lt;li&gt;it lies between the two ordinary linear regression lines &lt;/li&gt;&#10;&lt;li&gt;it crosses them where they cross each other at the centroid of the observations, and &lt;/li&gt;&#10;&lt;li&gt;it is very easy to calculate.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;The slope is the geometric mean of the slopes of the two ordinary linear regression slopes. It is also what you would get if you standardised the $x$ and $y$ observations, drew a line at 45&amp;deg; (or 135&amp;deg; if there is negative correlation) and then de-standardised the line.  It could also be seen as equivalent to making an implicit assumption that the variances of the two sets of errors are proportional to the variances of the two sets of observations; as far as I can tell, you claim not to know which way this is wrong. &lt;/p&gt;&#10;&#10;&lt;p&gt;Here is some R code to illustrate: the red line in the chart is OLS regression of $Y$ on $X$, the blue line is OLS regression of $X$ on $Y$, and the green line is this simple method. Note that the slope should be about 5.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;X0 &amp;lt;- 1600:3600&#10;Y0 &amp;lt;- 5*X0 + 700&#10;X1 &amp;lt;- X0 + 400*rnorm(2001)&#10;Y1 &amp;lt;- Y0 + 2000*rnorm(2001)&#10;slopeOLSXY  &amp;lt;- lm(Y1 ~ X1)$coefficients[2]     #OLS slope of Y on X&#10;slopeOLSYX  &amp;lt;- 1/lm(X1 ~ Y1)$coefficients[2]   #Inverse of OLS slope of X on Y&#10;slopesimple &amp;lt;- sd(Y1)/sd(X1) *sign(cov(X1,Y1)) #Simple slope&#10;c(slopeOLSXY, slopeOLSYX, slopesimple)         #Show the three slopes&#10;plot(Y1~X1)&#10;abline(mean(Y1) - slopeOLSXY  * mean(X1), slopeOLSXY,  col=&quot;red&quot;)&#10;abline(mean(Y1) - slopeOLSYX  * mean(X1), slopeOLSYX,  col=&quot;blue&quot;)&#10;abline(mean(Y1) - slopesimple * mean(X1), slopesimple, col=&quot;green&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="24" CreationDate="2011-02-26T21:48:43.443" Id="7666" LastActivityDate="2011-02-27T17:23:09.987" LastEditDate="2011-02-27T17:23:09.987" LastEditorUserId="2958" OwnerUserId="2958" ParentId="7644" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;I was taught the stock phrase &quot;recognising the density of a &lt;em&gt;whatever&lt;/em&gt; distribution,&quot; to use as part of a mathematical proof. I agree this 'trick' is useful, but AFAIK it doesn't have a name.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-02-26T22:07:32.897" Id="7667" LastActivityDate="2011-02-26T22:07:32.897" OwnerUserId="449" ParentId="7664" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I never used it directly, so I can only share some papers I had and general thoughts about that technique (which mainly address your questions 1 and 3).&lt;/p&gt;&#10;&#10;&lt;p&gt;My general understanding of biclustering mainly comes from genetic studies (2-6) where we seek to account for clusters of genes and grouping of individuals: in short, we are looking to groups samples sharing similar profile of gene expression together (this might be related to disease state, for instance) &lt;em&gt;and&lt;/em&gt; genes that contribute to this pattern of gene profiling. A survey of the state of the art for biological &quot;massive&quot; datasets is available in Pardalos's slides, &lt;a href=&quot;http://www.ise.ufl.edu/cao/DMinAgriculture/Lecture6.biclustering.pdf&quot;&gt;Biclustering&lt;/a&gt;. Note that there is an R package, &lt;a href=&quot;http://cran.r-project.org/web/packages/biclust/index.html&quot;&gt;biclust&lt;/a&gt;, with applications to microarray data.&lt;/p&gt;&#10;&#10;&lt;p&gt;In fact, my initial idea was to apply this methodology to clinical diagnosis, because it allows to put features or variables in more than one cluster, which is interesting from a semeiological perpective because symptoms that cluster together allow to define &lt;em&gt;syndrome&lt;/em&gt;, but some symptoms can overlap in different diseases. A good discussion may be found in Cramer et al., &lt;a href=&quot;http://sites.google.com/site/borsboomdenny/CramerEtAl2010.pdf?attredirects=0&quot;&gt;Comorbidity: A network perspective&lt;/a&gt; (Behavioral and Brain Sciences 2010, 33, 137-193). &lt;/p&gt;&#10;&#10;&lt;p&gt;A somewhat related technique is &lt;em&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Collaborative_filtering&quot;&gt;collaborative filtering&lt;/a&gt;&lt;/em&gt;. A good review was made available by Su and Khoshgoftaar (&lt;em&gt;Advances in Artificial Intelligence&lt;/em&gt;, 2009): &lt;a href=&quot;http://www.hindawi.com/journals/aai/2009/421425.html&quot;&gt;A Survey of Collaborative Filtering Techniques&lt;/a&gt;. Other references are listed at the end. Maybe analysis of &lt;a href=&quot;http://www.albionresearch.com/data_mining/market_basket.php&quot;&gt;frequent itemset&lt;/a&gt;, as exemplified in the &lt;a href=&quot;http://www.albionresearch.com/data_mining/market_basket.php&quot;&gt;market-basket problem&lt;/a&gt;, is also linked to it, but I never investigated this. Another example of co-clustering is when we want to simultaneously cluster words and documents, as in text mining, e.g. Dhillon (2001). &lt;a href=&quot;http://www.cs.utexas.edu/users/inderjit/public_papers/kdd_bipartite.pdf&quot;&gt;Co-clustering documents and words using bipartite spectral graph partitioning&lt;/a&gt;. &lt;em&gt;Proc. KDD&lt;/em&gt;, pp. 269–274.&lt;/p&gt;&#10;&#10;&lt;p&gt;About some general references, here is a not very exhaustive list that I hope you may find useful:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Jain, A.K. (2010). &lt;a href=&quot;http://biometrics.cse.msu.edu/Publications/Clustering/JainClustering_PRL10.pdf&quot;&gt;Data clustering: 50 years beyond K-means&lt;/a&gt;. &lt;em&gt;Pattern Recognition Letters&lt;/em&gt;, &lt;em&gt;31&lt;/em&gt;, 651–666&lt;/li&gt;&#10;&lt;li&gt;Carmona-Saez et al. (2006). &lt;a href=&quot;http://www.biomedcentral.com/1471-2105/7/78&quot;&gt;Biclustering of gene expression data by non-smooth non-negative matrix factorization&lt;/a&gt;. &lt;em&gt;BMC Bioinformatics&lt;/em&gt;, &lt;em&gt;7&lt;/em&gt;, 78.&lt;/li&gt;&#10;&lt;li&gt;Prelic et al. (2006). &lt;a href=&quot;http://bioinformatics.oxfordjournals.org/content/22/9/1122.abstract&quot;&gt;A systematic comparison and evaluation of biclustering methods for gene expression data&lt;/a&gt;. &lt;em&gt;Bioinformatics&lt;/em&gt;, &lt;em&gt;22(9)&lt;/em&gt;, 1122-1129. &lt;a href=&quot;http://www.tik.ee.ethz.ch/sop/bimax&quot;&gt;www.tik.ee.ethz.ch/sop/bimax&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;DiMaggio et al. (2008). &lt;a href=&quot;http://www.biomedcentral.com/1471-2105/9/458&quot;&gt;Biclustering via optimal re-ordering of data matrices in systems biology: rigorous methods and comparative studies&lt;/a&gt;. &lt;em&gt;BMC Bioinformatics&lt;/em&gt;, &lt;em&gt;9&lt;/em&gt;, 458.&lt;/li&gt;&#10;&lt;li&gt;Santamaria et al. (2008). &lt;a href=&quot;http://bioinformatics.oxfordjournals.org/content/24/9/1212.abstract&quot;&gt;BicOverlapper: A tool for bicluster visualization&lt;/a&gt;. &lt;em&gt;Bioinformatics&lt;/em&gt;, &lt;em&gt;24(9)&lt;/em&gt;, 1212-1213.&lt;/li&gt;&#10;&lt;li&gt;Madeira, S.C. and Oliveira, A.L. (2004) &lt;a href=&quot;http://www.inesc-id.pt/ficheiros/publicacoes/1281.pdf&quot;&gt;Bicluster algorithms for biological data analysis: a survey&lt;/a&gt;. &lt;em&gt;IEEE Trans. Comput. Biol. Bioinform.&lt;/em&gt;, &lt;em&gt;1&lt;/em&gt;, 24–45.&lt;/li&gt;&#10;&lt;li&gt;Badea, L. (2009). &lt;a href=&quot;http://ijcai.org/papers09/Papers/IJCAI09-232.pdf&quot;&gt;Generalized Clustergrams for Overlapping Biclusters&lt;/a&gt;. IJCAI&lt;/li&gt;&#10;&lt;li&gt;Symeonidis, P. (2006). &lt;a href=&quot;http://webmining.spd.louisville.edu/webkdd06/papers/paper-3-Nearest%20Bi-Cluster%20CF_SymeonidiswebKDD06.pdf&quot;&gt;Nearest-Biclusters Collaborative Filtering&lt;/a&gt;. WEBKDD&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="3" CreationDate="2011-02-27T11:51:49.200" Id="7674" LastActivityDate="2011-02-27T11:51:49.200" OwnerUserId="930" ParentId="7419" PostTypeId="2" Score="14" />
  <row AcceptedAnswerId="7677" AnswerCount="2" Body="&lt;p&gt;An experiment I conducted recently used a 2 (between participants) x 3 (within participants) design. That is, participants were randomly allocated to one of two conditions, and then completed three similar tasks each (in a counterbalanced order). &lt;/p&gt;&#10;&#10;&lt;p&gt;In each of these tasks, participants made binary choices (2AFC) in a number of trials, each of which had a normatively correct answer. &lt;br /&gt;&#10;In every trial, participants were presented a distractor, which was assumed to bias responses towards one of the alternatives. The tasks differed only in presence and magnitude of this distractor (i.e. no distractor vs. distractor of small and large magnitude).&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to examine the error rates (deviations from the normatively correct answer) across these conditions. I hypothesize that the error rate will increase when a distractor is present, but will not increase further when the magnitude of the distractor is increased. Also, I expect this increase to differ between the between-subjects conditions. The latter interaction is the central interest.&lt;/p&gt;&#10;&#10;&lt;p&gt;From &lt;a href=&quot;http://stats.stackexchange.com/questions/3874/unbalanced-mixed-effect-anova-for-repeated-measures&quot;&gt;discussions here&lt;/a&gt; and from the literature (Dixon, 2008; Jaeger, 2008), I gather that logit mixed-models are the appropriate analysis method, and that, in R, the &lt;a href=&quot;http://lme4.r-forge.r-project.org/&quot; rel=&quot;nofollow&quot;&gt;lme4 package&lt;/a&gt; is the tool of choice. &lt;br /&gt;&#10;While I could compute some basic analyses (e.g. random intercept model, random effects ANCOVA) with lme4, I am stuck as to how to apply the models to the design in question -- I have the feeling that I am very much thinking in terms of HLMs, and have not yet quite understood the entirety of mixed effects models. Therefore, I would be very grateful for your help.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have two basic questions:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;In a first analysis, I would like to consider the error rates in only those trials in which participants were biased towards the &lt;em&gt;wrong&lt;/em&gt; answer. The first model would therefore look only at trials in which the bias would point away from the correct answer.&lt;/p&gt;&#10;&#10;&lt;p&gt;If my observations were independent, i would probably use a model like this:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;correct ~ condition + distractor + condition:distractor&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;... but obviously, they aren't: Observations are grouped within a task (with a constant presence of a distractor) and within participants. My question, then, is this: How do I add the random effects to reflect this?&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;(If I haven't lost you already :-) ) Would it be possible to include all trials (those where the bias would be into the direction of the wrong and of the correct answer), and include this difference (i.e. direction of the bias) as a trial-level predictor?&lt;/p&gt;&#10;&#10;&lt;p&gt;In my imagination of HLMs, such a predictor (at the level of the trial) would depend on on the magnitude of the distractor present (at the level of the block), which again would depend on the condition of the participant (plus, possibly, a unique factor for each participant).&lt;br /&gt;The interactions would then emerge ›automatically‹ as cross-level interactions. How would such a model be specified in the ›flat‹ lme4 syntax? (Would such a model make sense at all?)&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Ok, I hope all of this makes sense -- I will gladly elaborate otherwise. Again, I would be most grateful for any ideas and comments regarding this analysis, and would like to thank you for taking the time and trouble to respond.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Dixon, P. (2008). Models of accuracy in repeated-measures designs. &lt;em&gt;Journal of Memory and Language, 59&lt;/em&gt;(4), 447-456. doi: 10.1016/j.jml.2007.11.004&lt;/p&gt;&#10;&#10;&lt;p&gt;Jaeger T. F. (2008). Categorical data analysis: Away from ANOVAs (transformation or not) and towards logit mixed models. &lt;em&gt;Journal of Memory and Language, 59&lt;/em&gt;(4), 434-446. doi: 10.1016/j.jml.2007.11.007&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-02-27T14:09:22.073" FavoriteCount="1" Id="7675" LastActivityDate="2011-04-18T10:22:26.577" OwnerUserId="3451" PostTypeId="1" Score="5" Tags="&lt;r&gt;&lt;logistic&gt;&lt;mixed-model&gt;&lt;repeated-measures&gt;" Title="Analyzing a 2x3 repeated measures design using a logit mixed model" ViewCount="998" />
  
  <row AnswerCount="5" Body="&lt;p&gt;Say we have the following 5 cities, each with the same population &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;CityA with 20% each of 5 ethnicities&lt;/li&gt;&#10;&lt;li&gt;CityB with 99% of one ethnicity, but 100 different ethnicities in the remaining 1%&lt;/li&gt;&#10;&lt;li&gt;CityC with 40% of one ethnicity and the remaining 60% distributed evenly over 10 different ethnicities&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;How can one measure their relative diversities?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-02-27T19:05:14.440" FavoriteCount="1" Id="7681" LastActivityDate="2013-06-27T15:21:09.553" LastEditDate="2011-02-28T05:49:46.323" LastEditorUserId="276" OwnerUserId="276" PostTypeId="1" Score="6" Tags="&lt;distributions&gt;&lt;population&gt;" Title="Is there a way to compute diversity in a population?" ViewCount="226" />
  
  <row AnswerCount="0" Body="&lt;p&gt;The Pareto distribution can be used to give a pdf for the wealth of a person chosen randomly from a population.  (In fact, this was its origin.  See, for instance, &lt;a href=&quot;http://en.wikipedia.org/wiki/Pareto_principle&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Pareto_principle&lt;/a&gt; ).&lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to explore the reciprocal question: Given the total amount of wealth in a population, what is the pdf of the portion that a randomly chosen person has. I conjecture that this is simply a constant times the Pareto distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;More interestingly: What is the shape of the distribution curve, if the richest person would be at the 0 point on the x axis, the next richest person to the right, and so on - we would see a monotonically decreasing curve. But what is its shape? What is its derivative?&lt;/p&gt;&#10;&#10;&lt;p&gt;It's quite likely that I'm not phrasing that question properly. Let me ask a more basic question: What is the appropriate terminology to explore the question? Give a probability distribution applied many times over, what is the shape of the resultant allocation curve?&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2011-02-28T08:55:17.277" Id="7695" LastActivityDate="2011-02-28T09:26:52.597" LastEditDate="2011-02-28T09:26:52.597" LastEditorDisplayName="user3463" OwnerDisplayName="user3463" PostTypeId="1" Score="2" Tags="&lt;distributions&gt;&lt;modeling&gt;&lt;predictive-models&gt;&lt;application&gt;&lt;pareto-distribution&gt;" Title="Understanding the Pareto distribution as applied to wealth" ViewCount="272" />
  
  <row AcceptedAnswerId="7700" AnswerCount="1" Body="&lt;p&gt;I have just done a Chow test on a regression in order to see whether there is a structural break. I am a bit stumped however as my Chow test returns a negative number. Now what do I do?&lt;/p&gt;&#10;&#10;&lt;p&gt;More specifically, this expression (from the Wikipedia &lt;a href=&quot;http://en.wikipedia.org/wiki/Chow_test&quot; rel=&quot;nofollow&quot;&gt;entry&lt;/a&gt; for the Chow test):&lt;/p&gt;&#10;&#10;&lt;p&gt;$\frac{(S_c-(S_1+S_2))/(k)}{(S_1+S_2)/(N_1+N_2-2k)}$&lt;/p&gt;&#10;&#10;&lt;p&gt;turns out negative for me.&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT: It turns out it was a simple programming error.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-02-28T11:36:50.210" Id="7699" LastActivityDate="2012-04-07T20:27:30.643" LastEditDate="2012-04-07T20:27:30.643" LastEditorUserId="2116" OwnerUserId="3086" PostTypeId="1" Score="1" Tags="&lt;change-point&gt;&lt;discontinuity&gt;&lt;chow-test&gt;" Title="What to do with negative Chow test?" ViewCount="943" />
  
  <row Body="&lt;p&gt;If you use a logistic activation function in the output layer it will restrict the output to the range 0-1 as you require.  However if you have a regression problem with a restricted output range the sum-of-squares error metric may not be ideal and maybe a beta noise model might be more appropriate (c.f. beta regression, which IIRC is implemented in an R package but I have never used it myself)&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-02-28T13:01:10.830" Id="7703" LastActivityDate="2011-02-28T13:01:10.830" OwnerUserId="887" ParentId="7698" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;I asked &lt;a href=&quot;http://stackoverflow.com/questions/5130808/how-to-correlate-two-time-series-with-gaps-and-different-time-bases&quot;&gt;&lt;strong&gt;this question&lt;/strong&gt;&lt;/a&gt; over on StackOverflow, and was recommended to ask it here. &lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;I have two time series of 3D accelerometer data that have different time bases (clocks started at different times, with some very slight creep during the sampling time), as well as containing many gaps of different size (due to delays associated with writing to separate flash devices).&lt;/p&gt;&#10;&#10;&lt;p&gt;The accelerometers I'm using are the inexpensive &lt;a href=&quot;http://www.gcdataconcepts.com/x250-2.html&quot; rel=&quot;nofollow&quot;&gt;GCDC X250-2&lt;/a&gt;.  I'm running the accelerometers at their highest gain, so the data has a significant noise floor.&lt;/p&gt;&#10;&#10;&lt;p&gt;The time series each have about 2 million data points (over an hour at 512 samples/sec), and contain about 500 events of interest, where a typical event spans 100-150 samples (200-300 ms each).  Many of these events are affected by data outages during flash writes.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, the data isn't pristine, and isn't even very pretty.  But my eyeball inspection shows it clearly contains the information I'm interested in.  (I can post plots, if needed.)&lt;/p&gt;&#10;&#10;&lt;p&gt;The accelerometers are in similar environments but are only moderately coupled, meaning that I can tell by eye which events match from each accelerometer, but I have been unsuccessful so far doing so in software.  Due to physical limitations, the devices are also mounted in different orientations, where the axes don't match, but they are as close to orthogonal as I could make them.  So, for example, for 3-axis accelerometers A &amp;amp; B, +Ax maps to -By (up-down), +Az maps to -Bx (left-right), and +Ay maps to -Bz (front-back).&lt;/p&gt;&#10;&#10;&lt;p&gt;My initial goal is to correlate shock events on the vertical axis, though I would eventually like to a) automatically discover the axis mapping, b) correlate activity on the mapped aces, and c) extract behavior differences between the two accelerometers (such as twisting or flexing).&lt;/p&gt;&#10;&#10;&lt;p&gt;The nature of the time series data makes Python's numpy.correlate() unusable.  I've also looked at R's Zoo package, but have made no headway with it.  I've looked to different fields of signal analysis for help, but I've made no progress.&lt;/p&gt;&#10;&#10;&lt;p&gt;Anyone have any clues for what I can do, or approaches I should research?&lt;/p&gt;&#10;&#10;&lt;p&gt;Update 28 Feb 2011: Added some plots &lt;a href=&quot;https://picasaweb.google.com/FlyMyPG/VibData?authkey=Gv1sRgCLPo0u-7jafQjwE#&quot; rel=&quot;nofollow&quot;&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt; showing examples of the data.&lt;/p&gt;&#10;" CommentCount="11" CreationDate="2011-03-01T01:13:29.410" FavoriteCount="3" Id="7727" LastActivityDate="2015-01-14T12:26:34.597" LastEditDate="2013-07-31T21:45:31.687" LastEditorUserId="22047" OwnerUserId="3479" PostTypeId="1" Score="6" Tags="&lt;time-series&gt;&lt;correlation&gt;" Title="How to correlate two time series with gaps and different time bases?" ViewCount="3629" />
  
  
  
  
  <row Body="&lt;p&gt;Despite all of the good recommendations, I've not found anything radically better than the default Mac GUI. R-Studio shows promise, but it's not currently that much more customizable or featureful than R and, say, BBEdit to edit.&lt;/p&gt;&#10;" CommentCount="1" CommunityOwnedDate="2011-03-01T16:16:25.957" CreationDate="2011-03-01T16:16:25.957" Id="7753" LastActivityDate="2011-03-01T16:16:25.957" OwnerUserId="1764" ParentId="5292" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;I've had good experiences with Madigan's and Lewis's &lt;a href=&quot;http://www.bayesianregression.org&quot; rel=&quot;nofollow&quot;&gt;BMR and BBR&lt;/a&gt; packages for multiple category dependent variables, lasso or ridge priors on parameters, and high dimensional input data.  Not quite as high as yours, but it might still be worth a look.  Instructions are here: &lt;a href=&quot;http://bayesianregression.com/bmr.html&quot; rel=&quot;nofollow&quot;&gt;http://bayesianregression.com/bmr.html&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-03-01T23:49:23.417" Id="7770" LastActivityDate="2011-03-02T20:44:34.913" LastEditDate="2011-03-02T20:44:34.913" LastEditorUserId="1739" OwnerUserId="1739" ParentId="7721" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="7819" AnswerCount="1" Body="&lt;p&gt;I'm trying to fit a multivariate multiple regression model where the independent variable X is latent but I don't know where to start (I have prior information about the coefficient matrix so I can use some iterative method).&lt;/p&gt;&#10;&#10;&lt;p&gt;The dependent variable Y is a NxM matrix denoting N observations each from M variables. The latent variable X is a NxP matrix about which we know nothing except the dimensions. In addition to these, we have an initial estimate of the coefficient matrix beta based on prior knowledge. My goal is to find the estimates of latent X and coefficient matrix beta by both using the data matrix Y and the initial coefficient matrix. I thought of constructing an EM algorithm but because of the complexity of multivariate data and latent variable concept, I am totally confused.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you.&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2011-03-02T03:05:21.403" Id="7772" LastActivityDate="2011-03-02T20:42:51.933" LastEditDate="2011-03-02T20:20:02.473" LastEditorUserId="3499" OwnerUserId="3499" PostTypeId="1" Score="5" Tags="&lt;regression&gt;&lt;multivariate-analysis&gt;&lt;latent-variable&gt;" Title="Is it possible to fit a multivariate regression model where the independent variable is latent?" ViewCount="372" />
  
  
  <row AnswerCount="4" Body="&lt;p&gt;I would like to find out the values &lt;code&gt;(x, y)&lt;/code&gt; used in plotting &lt;code&gt;plot(b, seWithMean=TRUE)&lt;/code&gt; in &lt;strong&gt;mgcv&lt;/strong&gt; package.  Does anyone know how I can extract or compute these values?&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is an example: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(mgcv) &#10;set.seed(0)&#10;dat &amp;lt;- gamSim(1, n=400, dist=&quot;normal&quot;, scale=2) &#10;b   &amp;lt;- gam(y~s(x0), data=dat) &#10;plot(b, seWithMean=TRUE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2011-03-02T13:16:19.200" FavoriteCount="1" Id="7795" LastActivityDate="2015-01-19T10:01:32.153" LastEditDate="2014-04-29T14:26:27.517" LastEditorUserId="7290" OwnerDisplayName="user2264" PostTypeId="1" Score="6" Tags="&lt;r&gt;&lt;time-series&gt;&lt;smoothing&gt;&lt;mgcv&gt;" Title="How to obtain the values used in plot.gam in mgcv?" ViewCount="2721" />
&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The constant $c \in \mathbb{R}$ and the limits are taken as $n \to \infty$. Note that, though they're separated into two results, they're pretty much the same result since $c$ is not constrained to be nonnegative in either case.&lt;/p&gt;&#10;&#10;&lt;p&gt;See, e.g., Motwani and Raghavan, &lt;em&gt;Randomized Algorithms&lt;/em&gt;, pp. 60--63 for a proof.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Also&lt;/strong&gt;: David kindly provides a proof for his stated upper bound in the comments to this answer.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-03-02T13:50:29.307" Id="7796" LastActivityDate="2011-03-07T13:01:34.917" LastEditDate="2011-03-07T13:01:34.917" LastEditorUserId="2970" OwnerUserId="2970" ParentId="7774" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;This will not be a complete answer. All the plotting for &lt;code&gt;gam&lt;/code&gt; objects is being done with function &lt;code&gt;plot.gam&lt;/code&gt;. You can look at its code by simply typing&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; plot.gam&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;in R console. As you will see the code is huge. What I've gleaned from it, that all the plotting is done by gathering relevant information in &lt;code&gt;pd&lt;/code&gt; object which is a list. So one of the possible solution would be to edit &lt;code&gt;plot.gam&lt;/code&gt;, using &lt;code&gt;edit&lt;/code&gt; for example, so that it returns that object. Adding &lt;code&gt;pd&lt;/code&gt; before last &lt;code&gt;}&lt;/code&gt; will be enough. I would advise adding &lt;code&gt;invisible(pd)&lt;/code&gt;, so that this object is returned only if you ask for it:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; pd &amp;lt;- plot(b,seWithMean = TRUE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Then inspect this object and search in the code of &lt;code&gt;plot.gam&lt;/code&gt; for the lines with &lt;code&gt;plot&lt;/code&gt; and &lt;code&gt;lines&lt;/code&gt;. Then you will see which of the relevant &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; values appear in the plot.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-03-02T14:45:37.597" Id="7801" LastActivityDate="2011-03-02T14:45:37.597" OwnerUserId="2116" ParentId="7795" PostTypeId="2" Score="3" />
  
&#10;||\mathbf{x} - \mathbf{y}||_2 = \sqrt{\sum_i (x_i - y_i)^2}
  <row Body="&lt;p&gt;The question really concerns pairs of normal variates.  Let's call them $x_1$ and $x_2$ with means $\mu_i$, standard deviations $\sigma_i$, and correlation $\rho$.  Whence their joint pdf is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac{1}{2 \pi \sqrt{1 - \rho^2} \sigma_1 \sigma_2}
  
  
  
  
  
  
  
  <row AcceptedAnswerId="7847" AnswerCount="3" Body="&lt;p&gt;Let's say I have two distributions I want to compare in detail, i.e. in a way that makes shape, scale and shift easily visible.  One good way to do this is to plot a histogram for each distribution, put them on the same X scale, and stack one underneath the other.  &lt;/p&gt;&#10;&#10;&lt;p&gt;When doing this, how should binning be done?  Should both histograms use the same bin boundaries even if one distribution is much more dispersed than the other, as in Image 1 below?  Should binning be done independently for each histogram before zooming, as in Image 2 below?  Is there even a good rule of thumb on this?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Kqzdr.png&quot; alt=&quot;Image 1&quot;&gt;&#10;&lt;img src=&quot;http://i.stack.imgur.com/BpPwP.png&quot; alt=&quot;Image 2&quot;&gt;&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2011-03-03T16:28:11.247" FavoriteCount="2" Id="7845" LastActivityDate="2011-03-08T14:43:57.757" OwnerUserId="1347" PostTypeId="1" Score="12" Tags="&lt;data-visualization&gt;&lt;histogram&gt;&lt;density-function&gt;&lt;binning&gt;" Title="Best way to put two histograms on same scale?" ViewCount="1889" />
  <row Body="&lt;p&gt;Probably what you need is &lt;code&gt;table&lt;/code&gt; or &lt;code&gt;aggregate&lt;/code&gt;. If you add more details I can give you a more in-depth explanation.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-03-03T19:29:42.647" Id="7852" LastActivityDate="2011-03-03T19:29:42.647" OwnerUserId="582" ParentId="7850" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;I did something similar recently.  There are quite a few ways to aggregate data like this in R, but the &lt;code&gt;ddply&lt;/code&gt; function from the package &lt;code&gt;plyr&lt;/code&gt; is my security blanket, and I turn to it for things like this.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm assuming that you have individual records for each person in your dataset, with age, sex, and marital status.  There's no need to split up the data into multiple tables for this approach - if you have women in the original table, just leave them in and add sex as a grouping variable.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;require(plyr)&#10;results.by.age &amp;lt;- ddply(.data = yourdata, .var = c(&quot;sex&quot;, &quot;age&quot;), .fun = function(x) {&#10;    data.frame(n = nrow(x),&#10;               ever.married.n = nrow(subset(x, marital.status %in%&#10;                                            c(&quot;Married&quot;, &quot;Divorced&quot;))),&#10;               ever.married.prop = nrow(subset(x, marital.status %in%&#10;                                            c(&quot;Married&quot;, &quot;Divorced&quot;))) / nrow(x)&#10;               )&#10;    }&#10;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This splits the data.frame &lt;code&gt;yourdata&lt;/code&gt; by unique combinations of the variables &lt;code&gt;sex&lt;/code&gt; and &lt;code&gt;age&lt;/code&gt;.  Then, for each of those chunks (referred to as &lt;code&gt;x&lt;/code&gt;), it calculates the number of people who belong to that group (&lt;code&gt;n&lt;/code&gt;), how many of them are married (&lt;code&gt;ever.married.n&lt;/code&gt;), and what proportion of them are married (&lt;code&gt;ever.married.prop&lt;/code&gt;).  It will then return a data.frame called &lt;code&gt;results.by.age&lt;/code&gt; with rows like&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;sex       age    n       ever.married.n     ever.married.prop&#10;&quot;Male&quot;    25     264     167                0.633&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This is perhaps not the most elegant or efficient way to do this, but this general pattern has been very helpful for me.  One advantage of this is that you can easily and transparently collect whatever statistics you want from the subset, which can be helpful if you want to, say, add a regression line to the plot (weight by &lt;code&gt;n&lt;/code&gt;) or have both male and female proportions on the same plot and color the points by sex.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Here's a revised version using the &lt;code&gt;summarise()&lt;/code&gt; function from plyr - the effect is the same, but &lt;code&gt;summarise()&lt;/code&gt; has a couple of key advantages:&#10; - It works within the environment of the current subset - so rather than typing &lt;code&gt;x$marital.status&lt;/code&gt;, I can just type &lt;code&gt;marital.status&lt;/code&gt;.&#10; - It lets me refer to other variables I've already created, which makes percentages, transformations and the like much easier - if I've already made &lt;code&gt;num&lt;/code&gt; and &lt;code&gt;denom&lt;/code&gt;, the proportion of &lt;code&gt;num&lt;/code&gt; is just &lt;code&gt;num / denom&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;results.by.age &amp;lt;- ddply(.data = yourdata, .var = c(&quot;sex&quot;, &quot;age&quot;), .fun = summarise,&#10;    n = length(marital.status),&#10;    ever.married = sum(marital.status %in% c(&quot;Married&quot;, &quot;Divorced&quot;)),&#10;    ever.married.prop = ever.married / n # Referring to vars I just created&#10;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2011-03-03T19:57:08.010" Id="7855" LastActivityDate="2014-07-20T20:40:20.357" LastEditDate="2014-07-20T20:40:20.357" LastEditorUserId="71" OwnerUserId="71" ParentId="7850" PostTypeId="2" Score="5" />
  
  
  <row Body="&lt;p&gt;mpq has already explained &lt;a href=&quot;http://stats.stackexchange.com/questions/256/how-boosting-works&quot;&gt;boosting in plain english&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;A picture may substitute a thousand words ... (stolen from &lt;a href=&quot;http://www.boosting.org/papers/MeiRae03.ps.gz&quot; rel=&quot;nofollow&quot;&gt;R. Meir and G. Rätsch. An introduction to boosting and leveraging&lt;/a&gt;)&#10;&lt;img src=&quot;http://i.stack.imgur.com/0QPUi.png&quot; alt=&quot;example adaboost&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Image remark:&lt;/strong&gt;  In the 1st Iteration the classifier based on all datapoints classifies all points correctly except those in x&amp;lt;0.2/y&gt;0.8 and the point around 0.4/0.55 (see the circles in the second picture). In the second iteration exactly those points gain a higher weight so that the classifier based on that weighted sample classifies them correctly (2nd Iteration, added dashed line). The combined classifiers (i.e. &quot;the combination of the dashed lines&quot;) result in the classifier represent by the green line. Now the second classifier produces another missclassifications  (x in [0.5,0.6] / y in [0.3,0.4]), which gain more focus in the third iteration and so on and so on. In every step, the combined classifier gets closer and closer to the best shape (although not continuously). The final classifier (i.e. the combination of all single classifiers) in the 100th Iteration classifies all points correctly. &lt;/p&gt;&#10;&#10;&lt;p&gt;Now it should be more clear how boosting works. Two questions remain about the algorithmic details.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;1. How to estimate missclassifications ?&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;In every iteration, only a sample of the training data available in this iteration is used for training the classifier, the rest is used for estimating the error / the missclassifications. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;2. How to apply the weights ?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;You can do this in the following ways:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;You sample the data using an appropriate sample algorithm which can handle weights (e.g. weighted random sampling or rejection sampling) and build classification model on that sample. The resulting sample contains missclassified examples with a higher probability than correctly classified ones, hence the model learned on that sample is forced to concentrate on the missclassified part of the data space. &lt;/li&gt;&#10;&lt;li&gt;You use a classification model which is capable of handling such weights implicitly like e.g. Decision Trees. DT simply count. So instead of using 1 as counter/increment if an example with a certain predictor and class values is presented, it uses the specified weight w. If w = 0, the example is practically ignored. As a result, the missclassified examples have &lt;em&gt;more influence&lt;/em&gt; on the class probability estimated by the model.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Regarding your document example: &lt;/p&gt;&#10;&#10;&lt;p&gt;Imagine a certain word separates the classes perfectly, but it only appears in a certain part of the data space (i.e. near the decision boundary). Then the word has no power to separate all documents (hence it's expressiveness for the whole dataset is low), but only those near the boundary (where the expressiveness is high). Hence the documents containing this word will be misclassified in the first iteration(s), and hence gain more focus in later applications. Restricting the dataspace to the boundary (by document weighting), your classifier will/should detect the expressiveness of the word and classify the examples in that subspace correctly.&lt;/p&gt;&#10;&#10;&lt;p&gt;(God help me, I can't think of a more precise example. If the muse later decides to spend some time with me, I will edit my answer).&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that boosting assumes weak classifiers. E.g. Boosting applied together with NaiveBayes will have no significant effect (at least regarding my experience).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;edit:&lt;/strong&gt; Added some details on the algorithm and explanation for the picture.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-03-04T11:46:01.650" Id="7877" LastActivityDate="2011-03-08T07:41:14.860" LastEditDate="2011-03-08T07:41:14.860" LastEditorUserId="264" OwnerUserId="264" ParentId="7813" PostTypeId="2" Score="7" />
  <row Body="&lt;p&gt;You can try to use the &lt;a href=&quot;http://cran.r-project.org/web/packages/data.table/index.html&quot; rel=&quot;nofollow&quot;&gt;data.table&lt;/a&gt; package.&lt;/p&gt;&#10;&#10;&lt;p&gt;For your particular case, the upside is that it's (insanely) fast. The first time I was introduced to it, I was working on data.frame objects with hundreds of thousands of rows. &quot;Normal&quot; &lt;code&gt;aggregate&lt;/code&gt; or &lt;code&gt;ddply&lt;/code&gt; methods were taken ~ 1-2 mins to complete (this was before Hadley introduced the &lt;code&gt;idata.frame&lt;/code&gt; mojo into &lt;code&gt;ddply&lt;/code&gt;). Using &lt;code&gt;data.table&lt;/code&gt;, the operation was literally done in a matter of seconds.&lt;/p&gt;&#10;&#10;&lt;p&gt;The downside is that its so fast because it will resort your data.table (it's just like a data.frame) by &quot;key columns&quot; and use a smart searching strategy to find subsets of your data. This will result in a reordering of your data before you collect stats over it.&lt;/p&gt;&#10;&#10;&lt;p&gt;Given that you will just want the first row of each group -- maybe the reordering will mess up which row is first, which is why it might not be appropriate in your situation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Anyway, you'll have to judge whether or not &lt;code&gt;data.table&lt;/code&gt; is appropriate here, but this is how you would use it with the data you've presented:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;install.packages('data.table') ## if yo udon't have it already&#10;library(data.table)&#10;dxt &amp;lt;- data.table(dx, key='ID')&#10;dxt[, .SD[1,], by=ID]&#10;     ID AGE FEM&#10;[1,]  1  30   1&#10;[2,]  2  40   0&#10;[3,]  3  35   1&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt; Matthew Dowle (the main developer of the data.table package) has provided a better/smarter/(extremely) more efficient way to use data.table to solve this problem as one of the answers here ... definitely check that out.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-03-04T17:33:51.057" Id="7886" LastActivityDate="2011-03-10T04:01:35.727" LastEditDate="2011-03-10T04:01:35.727" LastEditorUserId="227" OwnerUserId="227" ParentId="7884" PostTypeId="2" Score="9" />
  
&#10;  a &amp;amp;= \frac{y_{(n)}-\bar{y}}{x_{(n)}-\bar{x}} \quad \text{and} \\
  <row Body="&lt;p&gt;A bit vague answer, but I'll give it a chance. I always felt that the size constraint is the central idea behind this method -- without is seems just to converge to other approaches, effectively to 2-means and ideologically to unsupervised SVM. The previous rather invalidates this idea, the latter way is more intriguing while you may hope to save some pain using SVM optimization framework and kernel tricks.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-03-05T00:02:10.823" Id="7905" LastActivityDate="2011-03-05T00:02:10.823" OwnerUserId="88" ParentId="7891" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;This can be done using relational database. R has a nice implementation of this (see this post on &lt;a href=&quot;http://stackoverflow.com/questions/1169551/sql-like-functionality-in-r&quot;&gt;sqldf&lt;/a&gt;). MS Access (or even Excel) will work just as well.&lt;/p&gt;&#10;&#10;&lt;p&gt;The idea here is you want to create a table that maps a number (as you say, of 5/6 digits) to a geographical region (75 or however many you have). Then, you join your table of 10000 records onto your reference table.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let table &lt;code&gt;mydata&lt;/code&gt; contain your 10000 records and holds at least 1 column:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;ID - contains your 'cell number'&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;Let table &lt;code&gt;myreftable&lt;/code&gt; contain your reference table, which should have exactly 1 row for each geographic region, and holds 2 columns:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;ID - contains the relevent 5/6 digits of your 'cell number'&lt;/li&gt;&#10;&lt;li&gt;Geo - contains the description of the geographic region&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;The table you'd want would be generate by the following SQL:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;select&#10;   m.ID as cell_number&#10;   ,r.Geo as geo_region&#10;from mydata m&#10;inner join myreftable r on left(m.ID,6)=r.ID&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;... where 'left()' is any function that takes the first 'n' characters of a string. Each database has different text/string functions you can use for this purpose. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-03-05T05:12:14.717" Id="7909" LastActivityDate="2011-03-05T05:12:14.717" OwnerUserId="3551" ParentId="7868" PostTypeId="2" Score="1" />
  
&#10;\mathbb{E} e^{-s T} = \prod_{i=1}^n \mathbb{E} e^{- s T_i}
&#10;e^{s t} = n e^{-c}
&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Putting this together, we get that&#10;$$
  
  <row Body="&lt;p&gt;Good programming skills are a must.  You need to be able to write efficient code that can deal with huge amounts of data without choking, and maybe be able to parallelize said code to get it to run in a reasonable amount of time.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-03-06T21:38:24.733" Id="7932" LastActivityDate="2011-03-06T21:38:24.733" OwnerUserId="1347" ParentId="7815" PostTypeId="2" Score="9" />
  
  
  <row Body="&lt;p&gt;&lt;strong&gt;Intro&lt;/strong&gt; As @vqv mentionned Total variation and Kullback Leibler are two interesting distance. The first one is meaningfull because it can be directly related to first and second type errors in hypothesis testing. The problem with the Total variation distance is that it can be difficult to compute. The Kullback Leibler distance is easier to compute and I will come to that later. It is not symetric but can be made symetric (somehow a little bit artificially).&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Answer&lt;/strong&gt;  Something I mention &lt;a href=&quot;http://mathoverflow.net/questions/29054/l1-distance-between-gaussian-measures&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; is that if $\mathcal{L}$ is the log likelihood ratio between your two gaussian measures $P_0,P_1$ (say that for $i=0,1$ $P_i$ has mean $\mu_i$ and covariance $C_i$) error measure that is also interseting (in the gaussian case I found it quite central actually) is &lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \|\mathcal{L}\|^2_{L_2(P_{1/2})} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;for a well chosen $P_{1/2}$. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;In simple words&lt;/strong&gt;: &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;there might be different interesting &quot;directions&quot; rotations, that are obtained using your formula with one of the &quot;interpolated&quot; covariance matrices $\Sigma=C_{i,1/2}$ ($i=1,2,3,4$ or $5$) defined at the end of this post (the number $5$ is the one you propose in your comment to your question).  &lt;/li&gt;&#10;&lt;li&gt;since your two distributions have different covariances, it is &lt;strong&gt;not sufficiant to compare the means&lt;/strong&gt;, you also need to compare the covariances.  &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Let me explain you why this is my feeling, how you can compute this in the case of $C_1\neq C_0$ and how to choose $P_{1/2}$. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Linear case&lt;/strong&gt; If $C_1=C_0=\Sigma$. &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\sigma= \Delta \Sigma^{-1} \Delta=\|2\mathcal{L}\|^2_{L_2(P_{1/2})}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $P_{1/2}$ is the &quot;interpolate&quot; between $P_1$ and $P_0$ (gaussian with covariance $\Sigma$ and mean $(\mu_1+\mu_0)/2$). Note that in this case, the Hellinger distance, the total variation distance can all be written using $\sigma$. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;How to compute $\mathcal{L}$ in the general case&lt;/strong&gt; A natural question that arises from your question (and &lt;a href=&quot;http://mathoverflow.net/questions/29054/l1-distance-between-gaussian-measures&quot; rel=&quot;nofollow&quot;&gt;mine&lt;/a&gt;) is what is a natural &quot;interpolate&quot; between $P_1$ and $P_0$ when $C_1\neq C_0$. Here the word natural may be user specific but for example it may be related to the best interpolation to have a tight upper bound with another distance (e.g. $L_1$ distance &lt;a href=&quot;http://mathoverflow.net/questions/29054/l1-distance-between-gaussian-measures&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;) &lt;/p&gt;&#10;&#10;&lt;p&gt;Writting &#10;$$ \mathcal{L}= \phi (C^{-1/2}_i(x-\mu_i))-\phi (C^{-1/2}_j(x-\mu_j))-\frac{1}{2}\log \left ( C_iC_j^{-}\right )
&#10;m_{ij},m_{ij}\rangle_{\mathbb{R}^p}+\frac{1}{2}\log|\det(C_j^{-}C_i)| $$&lt;/p&gt;&#10;&#10;&lt;p&gt;and&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ m_{ij}=\mu_i-\mu_j \;\; and\;\; s_{ij}=\frac{\mu_i+\mu_j}{2}$$ &lt;/p&gt;&#10;&#10;&lt;p&gt;is more relevant for computational purpose. &#10;For any gaussian $P_{1/2}$ with mean $s_{01}$ and covariance $C$ the calculation of $\|\mathcal{L}\|^2_{L_2(P_{1/2})}$ from Equation $1$ is a bit technical but faisible. You might also use it to compute the Kulback leibler distance. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;What interpolation should we choose (i.e. how to choose $P_{1/2}$)&lt;/strong&gt;&#10;It is clearly understood from Equation $1$ that there are many different candidates for $P_{1/2}$ (interpolate) in the &quot;quadratic&quot; case. The two candidates I found &quot;most natural&quot; (subjective:) ) arise from defining for $t\in [0,1]$ a gaussian distribution $P_t$ with mean $t\mu_1+(1-t)\mu_0$:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;$P^1_t$ as the distribution of $$ \xi_t=t\xi_1+(1-t)\xi_0$$  (where $\xi_i$ is drawn from $P_i$ $i=0,1$) which has covariance $C_{t,1}=(tC_1^{1/2}+(1-t)C_0^{1/2})^2$). &lt;/li&gt;&#10;&lt;li&gt;$P^2_t$ with inverse covariance $C_{t,2}^{-1}=tC_{1}^{-1}+(1-t)C_0^{-1}$ &lt;/li&gt;&#10;&lt;li&gt;$P^3_t$ with covariance $C_{t,3}=tC_1+(1-t)C_0$&lt;/li&gt;&#10;&lt;li&gt;$P^4_t$ with inverse covariance $C_{t,4}^{-1}=(tC^{-1/2}_1+(1-t)C^{-1/2}_0)^{2}$&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;EDIT: The one you propose in a comment to your question could be $C_{t,5}=C_1^{t}C_0^{1-t}$, why not ... &lt;/p&gt;&#10;&#10;&lt;p&gt;I have my favorite choice which is not the first one :) don't have much time to discuss that here. Maybe I'll edit this answer later... &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-03-07T09:19:14.240" Id="7949" LastActivityDate="2011-03-11T12:33:23.773" LastEditDate="2011-03-11T12:33:23.773" LastEditorUserId="223" OwnerUserId="223" ParentId="7912" PostTypeId="2" Score="3" />
  
  <row AcceptedAnswerId="7953" AnswerCount="1" Body="&lt;p&gt;I am using Sweave and &lt;strong&gt;xtable&lt;/strong&gt; to generate a report. &lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to add some coloring on a table. But I have not managed to find any way to generate colored tables with xtable. &lt;/p&gt;&#10;&#10;&lt;p&gt;Is there any other option?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-03-07T11:15:38.937" FavoriteCount="8" Id="7952" LastActivityDate="2011-03-08T10:30:23.020" LastEditDate="2011-03-07T11:35:29.127" LastEditorUserId="2116" OwnerUserId="1709" PostTypeId="1" Score="14" Tags="&lt;r&gt;&lt;reproducible-research&gt;&lt;sweave&gt;" Title="How to create coloured tables with Sweave and xtable?" ViewCount="5339" />
  <row AnswerCount="0" Body="&lt;p&gt;Would you please give an intuitive illustration of Newton's Method, when we deal with nonlinear regression?&lt;br&gt;&#10; Basically I understand that if we can use Taylor's theorem to expand the RSS function of parameter beta, we can change it into quadratic form, and minimize RSS w.r.t parameter. Please give me a multivariate example by using gradient and Hessian matrix.&lt;br&gt;&#10;Sorry I intend to input equation and function here, but I have no idea how to use LaTeX code here. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-03-07T14:10:05.547" Id="7955" LastActivityDate="2011-03-07T14:55:08.810" LastEditDate="2011-03-07T14:55:08.810" LastEditorUserId="88" OwnerUserId="3525" PostTypeId="1" Score="1" Tags="&lt;econometrics&gt;&lt;nonlinear-regression&gt;" Title="The usage of Newton's method in nonlinear regression" ViewCount="204" />
  
  
  <row Body="&lt;p&gt;Although @cardinal has already given an answer that gives precisely the bound I was looking for, I have found a similar Chernoff-style argument that can give a stronger bound:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Proposition&lt;/strong&gt;:&#10;$$
&#10;$$&#10;(this is stronger for $c &amp;gt; \frac{\pi^2}{3}$ )&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;em&gt;Proof&lt;/em&gt;:&lt;/p&gt;&#10;&#10;&lt;p&gt;As in @cardinal's answer, we can use the fact that $T$ is a sum of independent geometric random variables $T_i$ with success probabilities $p_i = 1 - i/n$. It follows that $E[T_i] = 1/p_i$ and $E[T] = \sum_{i=1}^{n} E[T_i] = n \sum_{i=1}^n \frac{1}{i}\geq n \log n$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Define now new variables $S_i  : = T_i  - E[T_i]$, and $S : = \sum_i S_i$. We can then write&#10;$$
  <row Body="&lt;p&gt;&quot;Your teaching score depends on how well your students did compared to a prediction made based on&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;What they knew beforehand, as measured by a pretest,&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;How well we think the students can learn based on what we know about them individually (their &quot;characteristics&quot;), &lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;And how well students do on average in your district, school, and classroom (if there are other teachers in your classroom).&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;&quot;In other words, we are evaluating you based on the &lt;em&gt;amount of learning&lt;/em&gt; that was measured, after factoring in the preparation and characteristics of your students and the typical performances of all students in settings like yours with the resources that were available to you.&lt;/p&gt;&#10;&#10;&lt;p&gt;&quot;In this way your score reflects what &lt;em&gt;you&lt;/em&gt; contributed to the student performances, insofar as we can determine that.  Of course we cannot know everything: we know you had unique and special students and that the situation you faced could never be duplicated.  Therefore we know this score is only an estimate that imperfectly reflects how well you taught, but it is a fairer and more accurate estimate than one based solely on post testing or on raw test gains made by your class.&quot;&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-03-07T19:31:19.560" Id="7968" LastActivityDate="2011-03-07T19:48:59.913" LastEditDate="2011-03-07T19:48:59.913" LastEditorUserId="919" OwnerUserId="919" ParentId="7960" PostTypeId="2" Score="10" />
  
  <row Body="&lt;p&gt;Your idea about the &quot;rasters&quot; is not very clearly stated, but you might have a look at the paper by Borcard and Legendre (1994) and &lt;a href=&quot;http://www.bio.umontreal.ca/legendre/reprints/index.html&quot; rel=&quot;nofollow&quot;&gt;their later works&lt;/a&gt; on spatial eigenvector-based analyses to see if one of the approaches will fit to your problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.bio.umontreal.ca/legendre/reprints/B&amp;amp;L_EES1994.pdf&quot; rel=&quot;nofollow&quot;&gt;Borcard, D., Legendre, P., (1994) Environmental control and spatial structure in ecological communities: an example using oribatid mites (Acari, Oribatei).  Environmental and Ecological Statistics 1, 37–61.&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-03-07T23:13:09.110" Id="7978" LastActivityDate="2011-03-07T23:28:39.240" LastEditDate="2011-03-07T23:28:39.240" LastEditorUserId="3467" OwnerUserId="3467" ParentId="7972" PostTypeId="2" Score="4" />
  <row AnswerCount="2" Body="&lt;p&gt;I have a general question. What kind of noise is additive, and what about multiplicative noise? How to determine the nature of noise?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks a lot for your help.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-03-08T01:48:03.257" Id="7989" LastActivityDate="2011-04-29T01:04:54.580" LastEditDate="2011-04-29T01:04:54.580" LastEditorUserId="3911" OwnerUserId="3552" PostTypeId="1" Score="2" Tags="&lt;regression&gt;" Title="Determining the nature of noise" ViewCount="139" />
  
  
  
  
  <row Body="&lt;p&gt;They don't randomly multiply by 1.  What they do is split the joint density into the product of two functions: $g(\text{sufficient statistic},\text{parameter})$ and $h(\text{data})$.  &lt;/p&gt;&#10;&#10;&lt;p&gt;The advantage of $h()$ is that it removes sometimes complicated parts of the density function which provide no useful information about estimating the parameter. At other times this is unnecessary, in which case $h()$ can be set to 1 and ignored. In either case you can concentrate on the &lt;a href=&quot;http://en.wikipedia.org/wiki/Sufficient_statistic&quot; rel=&quot;nofollow&quot;&gt;sufficient statistic&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-03-08T19:20:35.300" Id="8032" LastActivityDate="2011-03-08T19:20:35.300" OwnerUserId="2958" ParentId="8028" PostTypeId="2" Score="2" />
  
  
  
  
  <row Body="&lt;p&gt;You would make k-1 dummy variables for each of your categorical variables. The textbook argument holds; if you were to make k dummies for any of your variables, you would have a collinearity. You can think of the k-1 dummies as being contrasts between the effects of their corresponding levels, and the level whose dummy is left out.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-03-09T06:43:14.660" Id="8060" LastActivityDate="2011-03-09T06:43:14.660" OwnerUserId="1569" ParentId="8058" PostTypeId="2" Score="7" />
  
  
  <row Body="&lt;p&gt;A couple things to add to what @matt said:&lt;/p&gt;&#10;&#10;&lt;p&gt;In addition to SUGI (which is now renamed SAS Global Forum, and will be held this year in Las Vegas) there are numerous local and regional SAS user groups.  These are smaller, more intimate, and (usually) a lot cheaper.  Some local groups are even free. See &lt;a href=&quot;http://support.sas.com/usergroups/&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;SAS-L.  This is a mailing list for SAS questions.  It is quite friendly, and some of the participants are among the best SAS programmers there are.  &lt;/p&gt;&#10;&#10;&lt;p&gt;The book &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/1420070576&quot; rel=&quot;nofollow&quot;&gt;SAS and R: Data Management, Statistical Analysis and Graphics&lt;/a&gt; by Kleinman and Horton.  Look up what you want to do in the R index, and you'll find how to do it in SAS as well.  Sort of like a inter-language dictionary.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-03-09T10:18:53.657" Id="8067" LastActivityDate="2011-03-09T10:18:53.657" OwnerUserId="686" ParentId="8044" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;There is no simple answer for how to &quot;establish a relationship between two variables;&quot; indeed, your question is one of the central issues in statistics and research is still going on on how to do this.  But some basics:  first you will want to plot your data, and then you will want to carry out a linear regression to test some specific type of relationship between variables in your data.  You will need to obtain the &quot;p-score&quot; of the regression to get an idea of how well your purported relationship is supported by the data.  Generally if you can get a very low p-score (e.g. p &amp;lt; 0.01), then it will be safe to say that there is a relationship between variables.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-03-09T10:51:36.343" Id="8069" LastActivityDate="2011-03-09T10:56:46.197" LastEditDate="2011-03-09T10:56:46.197" LastEditorUserId="3567" OwnerUserId="3567" ParentId="8065" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;If you weight your measurements (proportion of subpopulation/proportion of subpopulation in sample), your estimates will be &lt;em&gt;unbiased&lt;/em&gt;.  I assume this is what you meant by &quot;poll results being skewed&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;If I interpret your question correctly, your goal is the simultaneous estimation of multiple population proportions, where your proportions are &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;P_1 = proportion of population voting yes on poll question 1&lt;/p&gt;&#10;  &#10;  &lt;p&gt;P_2 = proportion of population voting yes on poll question 2&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;etc.  (Let's work with one region at a time for now.) These can be represented in a proportion vector $P= (P_1, P_2, ...)$.  We will denote a &lt;em&gt;point estimate&lt;/em&gt; of $P$ by $\hat{P}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;By what you want is probably not a point estimate, but a &lt;em&gt;95% confidence interval&lt;/em&gt;.  This is an interval $(P_1 \pm t, P_2 \pm t, ...)$ where $t$ is your tolerance.  (What 95% confidence means is a tricky issue which is hard to explain and easy to misunderstand, so I'll skip it for now.)&lt;/p&gt;&#10;&#10;&lt;p&gt;The thing is, it is &lt;em&gt;always&lt;/em&gt; possible to construct a 95% confidence set no matter how small your sample size is.  For your problem to be properly defined you need to specify the $t$, which is how accurate you require your estimates to be.  The more accuracy you require the more samples you will need.  In the problem as I have set it up it is possible to find the minimum number of samples given $t$, but there you can get better results if you can estimate the variability of your respective subpopulations ahead of time (which does not seem to be the case in your problem.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Please give further clarification to your problem, though.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-03-09T12:13:57.863" Id="8075" LastActivityDate="2011-03-09T12:28:18.687" LastEditDate="2011-03-09T12:28:18.687" LastEditorUserId="3567" OwnerUserId="3567" ParentId="8072" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;Using the inverse gamma distribution, we get:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$p(\sigma^2|\alpha,\beta) \propto (\sigma^2)^{-\alpha-1} \exp(-\frac{\beta}{\sigma^2})$$&lt;/p&gt;&#10;&#10;&lt;p&gt;You can see easily that if $\beta \rightarrow 0$ and $\alpha \rightarrow 0$ then the inverse gamma will approach the Jeffreys prior.  This distribution is called &quot;uninformative&quot; because it is a proper approximation to the Jeffreys prior&lt;/p&gt;&#10;&#10;&lt;p&gt;$$p(\sigma^2) \propto \frac{1}{\sigma^2}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Which is uninformative for scale parameters &lt;a href=&quot;http://bayes.wustl.edu/etj/articles/prior.pdf&quot;&gt;see page 18 here for example&lt;/a&gt;, because this prior is the only one which remains invariant under a change of scale  (note that the approximation is not invariant).  This has a indefinite integral of $\log(\sigma^2)$ which shows that it is improper if the range of $\sigma^2$ includes either $0$ or $\infty$.  But these cases are only problems in the maths - not in the real world.  Never actually observe infinite value for variance, and if the observed variance is zero, you have perfect data!.  For you can set a lower limit equal to $L&amp;gt;0$ and upper limit equal $U&amp;lt;\infty$, and your distribution is proper.&lt;/p&gt;&#10;&#10;&lt;p&gt;While it may seem strange that this is &quot;uninformative&quot; in that it prefers small variance to large ones, but this is only on one scale.  You can show that $\log(\sigma^2)$ has an improper uniform distribution.  So this prior does not favor any one scale over any other&lt;/p&gt;&#10;&#10;&lt;p&gt;Although not directly related to your question, I would suggest a &quot;better&quot; non-informative distribution by choosing the upper and lower limits $L$ and $U$ in the Jeffreys prior rather than $\alpha$ and $\beta$.  Usually the limits can be set fairly easily with a bit of thought to what $\sigma^2$ actually means in the real world.  If it was the error in some kind of physical quantity - $L$ cannot be smaller than the size of an atom, or the smallest size you can observe in your experiment.  Further $U$ could not be bigger than the earth (or the sun if you wanted to be really conservative).  This way you keep your invariance properties, and its an easier prior to sample from: take $q_{(b)} \sim \mathrm{Uniform}(\log(L),\log(U))$, and then the simulated value as $\sigma^{2}_{(b)}=\exp(q_{(b)})$.&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2011-03-10T04:27:49.310" Id="8108" LastActivityDate="2013-10-23T16:17:08.657" LastEditDate="2013-10-23T16:17:08.657" LastEditorUserId="17230" OwnerUserId="2392" ParentId="8104" PostTypeId="2" Score="24" />
&#10;  0 &amp;amp; Y_{i} \leq y_{0}
  
  <row AnswerCount="1" Body="&lt;p&gt;I have two variables and 1000 cases. How can I statistically find representative cases from total of 1000, based on statistical properties of both variables and correlation between them. Perhaps something based on T-test and 95% (or 99%) interval but for both variables? I would like to know which statistical method can find cases that have both values (simultaneously) statistically the most significant. I know that this deals with sample distribution and estimating the proportions.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-03-10T14:12:47.033" Id="8127" LastActivityDate="2013-12-22T08:58:57.147" LastEditDate="2013-12-22T08:58:57.147" LastEditorDisplayName="user3656" LastEditorUserId="930" OwnerDisplayName="user3656" PostTypeId="1" Score="0" Tags="&lt;distributions&gt;&lt;sample&gt;" Title="How to make representative sample in dataset with two variables?" ViewCount="368" />
  <row AnswerCount="0" Body="&lt;p&gt;I read from a textbook that Gauss-Newton regression is also called 'artificial regression'.  Please give me an example, how does it work? And what's the relation with Newton's method? Thank you.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-03-10T15:41:13.017" FavoriteCount="1" Id="8133" LastActivityDate="2011-03-11T04:05:28.043" LastEditDate="2011-03-11T04:05:28.043" LastEditorUserId="919" OwnerUserId="3525" PostTypeId="1" Score="3" Tags="&lt;econometrics&gt;&lt;nonlinear-regression&gt;" Title="Intuitive explanation of Gauss-Newton regression" ViewCount="582" />
  
  <row Body="&lt;p&gt;Take a look at the &lt;a href=&quot;http://gking.harvard.edu/amelia/&quot; rel=&quot;nofollow&quot;&gt;Amelia II&lt;/a&gt; package, by Honaker, King and Blackwell.&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Amelia II &quot;multiply imputes&quot; missing data in a single cross-section (such as a survey), from a time series (like variables collected for each year in a country), or from a time-series-cross-sectional data set (such as collected by years for each of several countries).&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;q1. Yes, it is possible with the above package.&lt;/p&gt;&#10;&#10;&lt;p&gt;q3. I guess it would be possible to impute workplace using a more general imputation method. (&lt;a href=&quot;http://web.inter.nl.net/users/S.van.Buuren/mi/hmtl/mice.htm&quot; rel=&quot;nofollow&quot;&gt;MICE&lt;/a&gt; could help).&lt;/p&gt;&#10;&#10;&lt;p&gt;q4. As a general rule, do not throw information out. The imputation model, at a minimun, should include all covariates in all your models. But if extra information help predicting the missing data, include it! &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-03-11T13:24:39.780" Id="8153" LastActivityDate="2011-03-11T13:50:53.047" LastEditDate="2011-03-11T13:50:53.047" LastEditorUserId="375" OwnerUserId="375" ParentId="8152" PostTypeId="2" Score="3" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;Does any body know how to run a post hoc comparison in a 2X2 ANOVA with covariate in R. multcomp package seems very nice but I could not find clear answer or example to my question with this package. Thanks so much in advance&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-03-11T21:24:19.273" FavoriteCount="3" Id="8180" LastActivityDate="2011-03-12T19:35:02.983" OwnerUserId="3682" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;multiple-comparisons&gt;&lt;contrasts&gt;" Title="Post hoc comparison in two way ANOVA with covariate using R" ViewCount="1804" />
  
  <row Body="&lt;h3&gt;Data or Theory Driven?&lt;/h3&gt;&#10;&#10;&lt;p&gt;The first issue is whether you want the composite to be data driven or theory driven?&#10;If you are wishing to form a composite variable, it is likely that you think that each component variable is important in measuring some overall domain. &lt;/p&gt;&#10;&#10;&lt;p&gt;In this case, you are likely going to prefer a theoretical set of weights. If, alternatively, you are interested in whatever is shared or common amongst the component variables, at the risk of not including one of the variables because it measures something that is  orthogonal or less related to the remaining set, then you might want to explore data driven approaches.&lt;/p&gt;&#10;&#10;&lt;p&gt;This question maps on to the discussion in the structural equation modelling literature between reflective and formative measures&#10;( e.g., see &lt;a href=&quot;http://smib.vuw.ac.nz:8081/WWW/ANZMAC2004/CDsite/papers/Bucic1.PDF&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;Whatever you do it is important to align your measurement with your actual research question.&lt;/p&gt;&#10;&#10;&lt;h3&gt;Theory Driven&lt;/h3&gt;&#10;&#10;&lt;p&gt;If the composite is theoretically driven then you will want to form a weighted composite of the component variables where the weight assigned aligns with your theoretical weighting of the component.&#10;If the variables are ordinal, then you'll have to think about how to scale the variable.&#10;After scaling each component variable, you'll have to think about theoretical relative weighting and issues related to differential standard deviations of the variable.&#10;One simple strategy is to convert all component variables into z-scores, and sum the z-scores.&#10;If you have component variables, where some are positive and others are negative, then you'll need to reverse either just the negative or just the positive component variables.&lt;/p&gt;&#10;&#10;&lt;p&gt;I wrote a &lt;a href=&quot;http://jeromyanglim.blogspot.com/2009/03/calculating-composite-scores-of-ability.html&quot; rel=&quot;nofollow&quot;&gt;post on forming composites&lt;/a&gt; which addresses several scenarios for forming composites.&lt;/p&gt;&#10;&#10;&lt;p&gt;Theoretical driven approaches can be implemented easily in any statistical packages.&#10;&lt;code&gt;score.items&lt;/code&gt; in the &lt;code&gt;psych&lt;/code&gt; package is one function that makes it a little easier, but it is limited.&#10;You might just write your own equation using simple arithmetic, and perhaps the &lt;code&gt;scale&lt;/code&gt; function.&lt;/p&gt;&#10;&#10;&lt;h3&gt;Data Driven&lt;/h3&gt;&#10;&#10;&lt;p&gt;If you are more interested in being data driven, then there are many possible approaches.&lt;/p&gt;&#10;&#10;&lt;p&gt;Taking the first principal component sounds like a reasonable idea.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you have ordinal variables you might think about categorical PCA which would allow the component variables to be reweighted. This could automatically handle the quantification given the constraints you provide.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-03-12T06:02:41.313" Id="8191" LastActivityDate="2011-03-13T01:06:48.167" LastEditDate="2011-03-13T01:06:48.167" LastEditorUserId="183" OwnerUserId="183" ParentId="8160" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;There is a good definition of spurious relationship in &lt;a href=&quot;http://en.wikipedia.org/wiki/Spurious_relationship&quot; rel=&quot;nofollow&quot;&gt;wikipedia&lt;/a&gt;. Spurious means that there is some hidden variable or feature which causes both of the variables. In both time-series and in usual regression then terminology means the same, the relationship between two variables is spurious when something else causes both variables. In time-series context this something else is inherent property of random walks, in usual regression analysis some other variable. &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-03-12T08:51:13.900" Id="8195" LastActivityDate="2011-03-12T11:44:43.593" LastEditDate="2011-03-12T11:44:43.593" LastEditorUserId="2116" OwnerUserId="2116" ParentId="8185" PostTypeId="2" Score="3" />
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I'd like to perform semi-supervised LDA (Latent Dirichlet Allocation) in the following sense:&#10;I have several topics that I'd like to use, and have seed documents that relate to these topics. I'd like to run LDA to classify other documents, and potentially discover other topics.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would guess there is work done on that, as the problem is natural, and the LDA framework seems to suggest it, nevertheless, I'm not an expert and do not know about such work. &#10;Can you guide me to papers or tools ?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-03-13T07:09:44.603" FavoriteCount="1" Id="8220" LastActivityDate="2012-04-19T11:50:14.677" LastEditDate="2012-04-19T11:50:14.677" LastEditorUserId="930" OwnerUserId="3696" PostTypeId="1" Score="3" Tags="&lt;text-mining&gt;&lt;references&gt;&lt;unsupervised-learning&gt;" Title="References on semi-supervised LDA" ViewCount="191" />
  <row Body="&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;John Myles White has a &lt;a href=&quot;http://www.johnmyleswhite.com/notebook/2009/11/15/the-top-scores-for-canabalt-take-2/&quot; rel=&quot;nofollow&quot;&gt;dataset and analysis of Canabalt scores as posted on Twitter&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Stats at Berkeley has a dataset for a &lt;a href=&quot;http://www.stat.berkeley.edu/users/statlabs/labs.html#video&quot; rel=&quot;nofollow&quot;&gt;Video Games Survey&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CommunityOwnedDate="2011-03-13T12:45:03.647" CreationDate="2011-03-13T12:45:03.647" Id="8231" LastActivityDate="2011-03-13T13:13:15.433" LastEditDate="2011-03-13T13:13:15.433" LastEditorUserId="183" OwnerUserId="183" ParentId="8222" PostTypeId="2" Score="1" />
  
  
  <row AnswerCount="0" Body="&lt;p&gt;I am evaluating a Quadratic Discriminant Analysis (QDA) classifier on a high-dimensionality feature set.  The features come from highly non-Gaussian distributions.  However, when I transform the features to each have a Gaussian distribution in two different ways, the resulting classifiers performs worse than QDA applied on the raw features with the following three metrics:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Accuracy (classes are balanced)&lt;/li&gt;&#10;&lt;li&gt;Area under the ROC&lt;/li&gt;&#10;&lt;li&gt;A probabilistic metric&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;The first way of transforming each feature to a Gaussian distribution disregarded the class.  For each feature, it found the parameters of the the corresponding distribution, used the distribution's CDF function or approximation for each data point, and then used the Gaussian's inverse CDF.&lt;/p&gt;&#10;&#10;&lt;p&gt;The second way did the same thing, but included class labels and treated data from each class independently.&lt;/p&gt;&#10;&#10;&lt;p&gt;Any idea why this occurs?&lt;/p&gt;&#10;&#10;&lt;p&gt;I have confirmed that it is not due to ...&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;A bug in the code&lt;/li&gt;&#10;&lt;li&gt;Distributions changing from train set to test set&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="6" CreationDate="2011-03-13T20:45:44.760" FavoriteCount="0" Id="8246" LastActivityDate="2014-10-21T08:35:13.920" OwnerUserId="3595" PostTypeId="1" Score="4" Tags="&lt;distributions&gt;&lt;classification&gt;&lt;discriminant-analysis&gt;" Title="Using QDA for Non-Gaussian distributions" ViewCount="228" />
  
  <row Body="&lt;p&gt;The following example (taken from kernlab reference manual) shows you how to access the various components of the kernel PCA:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;data(iris)&#10;test &amp;lt;- sample(1:50,20)&#10;kpc &amp;lt;- kpca(~.,data=iris[-test,-5],kernel=&quot;rbfdot&quot;,kpar=list(sigma=0.2),features=2)&#10;&#10;pcv(kpc)        # returns the principal component vectors&#10;eig(kpc)        # returns the eigenvalues&#10;rotated(kpc)    # returns the data projected in the (kernel) pca space&#10;kernelf(kpc)    # returns the kernel used when kpca was performed&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Does this answer your question?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-03-13T23:02:22.433" Id="8249" LastActivityDate="2014-09-19T12:33:41.923" LastEditDate="2014-09-19T12:33:41.923" LastEditorUserId="28666" OwnerUserId="21360" ParentId="8182" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Just a remark: is there an inequality constraint $\sum_{i=1}^{n} w_{i} \leq 1$ in your system? I merely ask because $\mu$ is usually the KKT multiplier for inequality constraints. If indeed there is an inequality constraint, you will need to satisfy more conditions than just $\nabla L = 0$ to attain optimality (i.e. dual feasibility, complementarity slackness conditions etc.) In which case, you'd be better off using a proper optimization solver. I don't know if $\bar r_{i}, \bar r$ are constants and if the Hessian is positive semidefinite, but if so, this looks like a quadratic program (QP), and you can get solvers for this type of problem (e.g. &lt;code&gt;quadprog&lt;/code&gt; in MATLAB). &lt;/p&gt;&#10;&#10;&lt;p&gt;But, if you know what you're doing.... here are some ways to get derivatives.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Finite differencing&lt;/strong&gt;. You did not mention if you wanted numerical or exact derivatives. If accuracy isn't too big a deal, a finite difference perturbation is the easiest option. Choose a small enough $h$ for your application and you're all set. &lt;a href=&quot;http://en.wikipedia.org/wiki/Numerical_differentiation#Finite_difference_formulae&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Numerical_differentiation#Finite_difference_formulae&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Complex methods&lt;/strong&gt;. If you want a more accurate derivative, you can also calculate a complex derivative. &lt;a href=&quot;http://en.wikipedia.org/wiki/Numerical_differentiation#Complex_variable_methods&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Numerical_differentiation#Complex_variable_methods&lt;/a&gt;. Essentially, the idea is this:&#10;$$F&amp;#39;(x) \approx \frac{\mathrm{Im}(F(x + ih))}{h}
  
  <row AcceptedAnswerId="8269" AnswerCount="3" Body="&lt;p&gt;In R, I am trying to write a function to subset and exclude observations in a data frame based on three variables. My data looks something like this:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;data.frame':   43 obs. of  8 variables:&#10; $ V1: chr  &amp;quot;ENSG00000008438&amp;quot; &amp;quot;ENSG00000048462&amp;quot; &amp;quot;ENSG00000006075&amp;quot; &amp;quot;ENSG00000049130&amp;quot; ...
&#10;     $ V4: chr  &quot;PGLYRP1&quot; &quot;TNFRSF17&quot; &quot;CCL3&quot; &quot;KITLG&quot; ...&#10; $ V5: chr  &amp;quot;19&amp;quot; &amp;quot;16&amp;quot; &amp;quot;17&amp;quot; &amp;quot;12&amp;quot; ...
&#10;     $ V6: chr  &quot;q13.32&quot; &quot;p13.13&quot; &quot;q12&quot; &quot;q21.32&quot; ...&#10; $ V7: int  46522415 12058964 34415602 88886566 8276226 150285143 29138654 76424442 136871919 6664568 ...
  <row AnswerCount="2" Body="&lt;p&gt;I wanted to carry out a two-way ANCOVA for my data. However, SPSS isn't liking that I have only one IV. &lt;/p&gt;&#10;&#10;&lt;p&gt;I have &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;One IV: Group (3 levels, i.e. 2 experimental groups and a control group),&lt;/li&gt;&#10;&lt;li&gt;One DV: outcome measure (2 levels, i.e. time 1 and time 2),&lt;/li&gt;&#10;&lt;li&gt;One covariate (I have checked and this demographic continuous variable moderately correlates with the DV).&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Please could someone help as to whether I am missing something in terms of putting this into SPSS?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-03-14T15:36:51.410" FavoriteCount="1" Id="8271" LastActivityDate="2011-04-15T04:13:26.403" LastEditDate="2011-04-15T04:13:26.403" LastEditorUserId="183" OwnerDisplayName="user3706" PostTypeId="1" Score="1" Tags="&lt;spss&gt;&lt;repeated-measures&gt;&lt;ancova&gt;" Title="How to set up a two-way mixed ANOVA with a covariate  in SPSS?" ViewCount="4727" />
  
  <row AnswerCount="3" Body="&lt;p&gt;I got asked something similar to this in interview today.&lt;/p&gt;&#10;&#10;&lt;p&gt;The interviewer wanted to know what is the probability that an at-the-money option will end up in-the-money when volatility tends to infinity.&lt;/p&gt;&#10;&#10;&lt;p&gt;I said 0% because the normal distributions that underly the Black-Scholes model and the random walk hypothesis will have infinite variance. And so I figured the probability of all values will be zero.&lt;/p&gt;&#10;&#10;&lt;p&gt;My interviewer said the right answer is 50% because the normal distribution will still be symmetric and almost uniform. So when you integrate from mean to +infinity you get 50%.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am still not convinced with his reasoning.&lt;/p&gt;&#10;&#10;&lt;p&gt;Who is right?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-03-14T16:19:27.127" FavoriteCount="2" Id="8273" LastActivityDate="2012-04-27T06:31:46.490" OwnerUserId="2283" PostTypeId="1" Score="8" Tags="&lt;normal-distribution&gt;&lt;variance&gt;" Title="What is the probability that a normal distribution with infinite variance has a value greater than its mean?" ViewCount="2132" />
  <row Body="&lt;p&gt;You should be doing your analysis based on a log normal distribution, not a normal one. You interviewer is wrong when he states that the distribution is symmetrical. It would never be, regardless of the variance.  You also need to distinguish between volatility and what you are calling infinite variance. A stocks price, for example,  has no upper limit, thus it has &quot;infinite variance&quot;.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-03-14T18:17:05.443" Id="8280" LastActivityDate="2011-03-14T18:17:05.443" OwnerUserId="3489" ParentId="8273" PostTypeId="2" Score="-1" />
  <row AnswerCount="1" Body="&lt;p&gt;I know that Mclust does the fit on its own but I am trying to implement an optimization with the aim to generate a mixture of 2 gaussians with the combine moments as closed as possible to the moment of my returns' distribution. &lt;/p&gt;&#10;&#10;&lt;p&gt;The objective is to &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;code&gt;Min Abs((Mean Ret - MeanFit)/Mean Fit) + Abs((Std Ret -Stdev Fit)/Stdev) + Abs((Sk Ret-Sk fit)/Sk Fit) + Abs((Kurt Ret- Kurt Fit))&lt;/code&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;Taking into account that I fix the weight between the two gaussians at (0.2;0.8) I implement the below code in R: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;distance &amp;lt;-function(parameter,x) { &#10; u=mean(x) &#10; s=sd(x) &#10; sk=skewness(x) &#10; kurt=kurtosis(x) &#10; d1=dnorm(x,parameter[1],parameter[2]) &#10; d2=dnorm(x,parameter[3],parameter[4]) &#10; dfit=0.2d1+0.8d2 &#10; ufit=mean(dfit) &#10; sdfit=sd(dfit) &#10; skfit=skewness(fit) &#10; kurtfit=kurtosis(fit) &#10;&#10;&#10; abs((u-ufit)/ufit)+abs(s-sdfit)/sdfit)+abs((sk-skfit)/skfit)+abs((kurt-kurtfit)/kurtfit)) &#10;} &#10;Parameter&amp;lt;-c(0,0.01,0,0.01)  # starting point of the optimization &#10;opp&amp;lt;-optim(parameter,distance,x=conv) &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;could anybody tell me whether it is the right approach ? &lt;/li&gt;&#10;&lt;li&gt;should I add some constraint like &#10;&lt;code&gt;ufit=0.2*mean(d1)+0.8*mean(d2)...&lt;/code&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;thank you very much in advance for your time and help. &lt;/p&gt;&#10;&#10;&lt;p&gt;Sam&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-03-14T17:42:26.557" FavoriteCount="1" Id="8283" LastActivityDate="2011-03-27T16:03:13.370" LastEditDate="2011-03-27T16:03:13.370" LastEditorUserId="919" OwnerDisplayName="Sam R" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;distributions&gt;&lt;optimization&gt;&lt;mixture&gt;" Title="Fitting 4-moment distribution with mixture gaussian" ViewCount="494" />
  <row Body="&lt;p&gt;post your sessionInfo(). &lt;/p&gt;&#10;&#10;&lt;p&gt;lmtest:::lrtest(x) should call a generic function which then determines the appropriate code to run on 'x'.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you have not updated your packages, you may perhaps need to do so, or simply re-run install.packages('lmtest') for your updated R install.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-03-14T23:12:21.900" Id="8293" LastActivityDate="2011-03-14T23:12:21.900" OwnerUserId="317" ParentId="8292" PostTypeId="2" Score="3" />
  <row AnswerCount="2" Body="&lt;p&gt;I am investigating the effect of a drug on EEG and cognition in epilepsy patients.&#10;We have done EEG and neuropsychological (NP) tests twice, before and after medical treatment.&#10;Some of parameters in EEG and NP tests were significantly altered after drug treatment (done by Wilcoxon rank sum test, as the distribution was not normal).&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, I would like to calculate correlation between change in EEG parameters and change in NP tests.&#10;There are 7 parameters in EEG parameters and 27 in NP tests. I would like to run correlation analysis on every pair and see if any pair is significantly correlated.&lt;/p&gt;&#10;&#10;&lt;p&gt;My questions are:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;&lt;p&gt;Should I use nonparametric correlation, say Spearman, to calculate $\rho$? (as values of EEG and NP paramaters are not normal)&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;Since there are 7 parameters in EEG and 27 parameters in NP tests, there should be 7*27 number of testing, which requires correction of p value for multiple comparisons. Which method should I use? &lt;/p&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I am looking for not-so-conservative method. I have seen from another posting that I can use permutation test for multiple comparisons. Could someone explain how I can do this?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-03-15T06:59:28.983" FavoriteCount="1" Id="8300" LastActivityDate="2012-07-04T15:47:08.500" LastEditDate="2012-07-04T15:47:08.500" LastEditorUserId="4856" OwnerDisplayName="user3720" PostTypeId="1" Score="5" Tags="&lt;correlation&gt;&lt;multiple-comparisons&gt;&lt;permutation&gt;" Title="Correlation analysis and correcting $p$-values for multiple testing" ViewCount="1655" />
  <row Body="&lt;p&gt;The worst case of code length of a code induced by a probability density is just the negative log of the most unlikely event, since $L_p(x) = -\log p(x)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am currently working through Peter Grünwald's book 'The MDL principle'. He defines the complexity of a model class as the log of the sum of probabilities it can assign to data.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thus, the more different datasets a model can fit, the more complex it is. &lt;/p&gt;&#10;&#10;&lt;p&gt;$x^n$ here is a complete data set. $X^n$ is the set of all possible data sets of length $n$. This might seem confusing, but it is done this way wo allow non-iid data. &lt;/p&gt;&#10;&#10;&lt;p&gt;The formal definition goes as follows: $$COMP(\mathcal{M}) := \log \sum_{x^n \in X^n} p_{x^n}(x^n)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $p_{x^n}$ is the optimal $p \in \mathcal{M}$ with regard to $x^n$. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-03-15T07:36:50.213" Id="8302" LastActivityDate="2011-03-15T07:42:07.103" LastEditDate="2011-03-15T07:42:07.103" LastEditorUserId="2860" OwnerUserId="2860" ParentId="8299" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;First of all $R^2$ is not an appropriate goodness-of-fit measure for logistic regression, take an information criterion $AIC$ or $BIC$, for example, as a good alternative.&lt;/p&gt;&#10;&#10;&lt;p&gt;Logistic regression is estimated by maximum likelihood method, so &lt;code&gt;leaps&lt;/code&gt; is not used directly here. An extension of &lt;code&gt;leaps&lt;/code&gt; to &lt;code&gt;glm()&lt;/code&gt; functions is the &lt;a href=&quot;http://cran.r-project.org/web/packages/bestglm/index.html&quot;&gt;&lt;strong&gt;bestglm&lt;/strong&gt;&lt;/a&gt; package (as usually recommendation follows, consult vignettes there).&lt;/p&gt;&#10;&#10;&lt;p&gt;You may be also interested in the article by David W. Hosmer, Borko Jovanovic and Stanley Lemeshow &lt;strong&gt;&lt;a href=&quot;http://www.jstor.org/pss/2531779&quot;&gt;Best Subsets Logistic Regression&lt;/a&gt;&lt;/strong&gt; // Biometrics Vol. 45, No. 4 (Dec., 1989), pp. 1265-1270 (usually accessible through the university networks).&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-03-15T08:54:26.503" Id="8305" LastActivityDate="2011-03-15T08:54:26.503" OwnerUserId="2645" ParentId="8303" PostTypeId="2" Score="8" />
  
  <row Body="&lt;p&gt;Stepwise and &quot;all subsets&quot; methods are generally bad.  See Stopping Stepwise: &lt;a href=&quot;http://www.nesug.org/proceedings/nesug07/sa/sa07.pdf&quot;&gt;Why Stepwise Methods are Bad and what you Should Use&lt;/a&gt; by David Cassell and myself (we used SAS, but the lesson applies) or Frank Harrell Regression Modeling Strategies.  If you need an automatic method, I recommend LASSO or LAR.  A LASSO package for logistic regression is available &lt;a href=&quot;http://www4.stat.ncsu.edu/~boos/var.select/lasso.adaptive.html&quot;&gt;here&lt;/a&gt;, another interesting article is on the &lt;a href=&quot;http://www.stat.uiowa.edu/techrep/tr392.pdf&quot;&gt;iterated LASSO for logistic&lt;/a&gt; &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-03-15T11:20:17.760" Id="8312" LastActivityDate="2011-03-15T11:20:17.760" OwnerUserId="686" ParentId="8303" PostTypeId="2" Score="19" />
  
  
  <row Body="&lt;p&gt;Since your data are categorical, a t test is likely inappropriate for any parameter I can think you might want to do inference on. Here is a brief intro to a very similar problem as yours:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://walkerbioscience.com/word-files/Categorical%20data.doc&quot; rel=&quot;nofollow&quot;&gt;http://walkerbioscience.com/word-files/Categorical%20data.doc&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-03-15T17:10:20.340" Id="8325" LastActivityDate="2011-03-15T17:10:20.340" OwnerUserId="1893" ParentId="8278" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;Fully customizable &lt;a href=&quot;http://had.co.nz/ggplot2/&quot; rel=&quot;nofollow&quot;&gt;ggplot2&lt;/a&gt; boxplot...&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#bootstrap&#10;data &amp;lt;- data.frame(value=rnorm(100,mean = 0.5, sd = 0.2),group=0)&#10;#processing&#10;metaData &amp;lt;- ddply(data,~group,summarise,&#10;            mean=mean(data$value),&#10;            sd=sd(data$value),&#10;            min=min(data$value),&#10;            max=max(data$value),&#10;            median=median(data$value),&#10;            Q1=0,Q3=0&#10;            )&#10;bps &amp;lt;- boxplot.stats(data$value,coef=1.5) &#10;metaData$min &amp;lt;- bps$stats[1] #lower wisker&#10;metaData$max &amp;lt;- bps$stats[5] #upper wisker&#10;metaData$Q1 &amp;lt;- bps$stats[2] # 1st Quartile&#10;metaData$Q3 &amp;lt;- bps$stats[4] # 3rd Quartile&#10;&#10;#adding outliers&#10;out &amp;lt;- data.frame() #initialising storage for outliers&#10;if(length(bps$out) &amp;gt; 0){&#10;    for(n in 1:length(bps$out)){&#10;        pt &amp;lt;-data.frame(value=bps$out[n],group=0) &#10;        out&amp;lt;-rbind(out,pt) &#10;    }&#10;}&#10;#adding labels&#10;labels &amp;lt;-data.frame(value=metaData$max, label=&quot;Upper bound&quot;)&#10;labels &amp;lt;-rbind(labels,data.frame(value=metaData$min, label=&quot;Lower bound&quot;))&#10;labels &amp;lt;-rbind(labels,data.frame(value=metaData$median, label=&quot;Median&quot;))&#10;labels &amp;lt;-rbind(labels,data.frame(value=metaData$Q1, label=&quot;First quartile&quot;))&#10;labels &amp;lt;-rbind(labels,data.frame(value=metaData$Q3, label=&quot;Third quartile&quot;))&#10;&#10;#drawing&#10;library(ggplot2)&#10;p &amp;lt;- ggplot(metaData,aes(x=group,y=mean))&#10;p &amp;lt;- p + geom_segment(aes(x=c(0.1,0,0.1),y=c(0,0,1),xend=c(0,0,-0.1),yend=c(0,1,1)))&#10;p &amp;lt;- p + geom_text(aes(y=c(0,1),label=c(0,1),x=0.2))&#10;p &amp;lt;- p + geom_errorbar(aes(ymin=min,ymax=max),linetype = 1,width = 0.5) #main range&#10;p &amp;lt;- p + geom_linerange(aes(ymin=min,ymax=max),linetype = 1,width = 0, color=&quot;white&quot;)# white line range&#10;p &amp;lt;- p + geom_linerange(aes(ymin=min,ymax=max),linetype = 2)    #main range dotted&#10;p &amp;lt;- p + geom_crossbar(aes(y=median,,ymin=Q1,ymax=Q3),linetype = 1,fill='white') #box&#10;if(length(out) &amp;gt;0) p &amp;lt;- p + geom_point(data=out,aes(x=group,y=value),shape=4) # drawning outliers if any&#10;p &amp;lt;- p + scale_x_discrete(breaks=c(0))&#10;p &amp;lt;- p + scale_y_continuous(name= &quot;Value&quot;)&#10;p &amp;lt;- p + geom_text(data=labels,aes(x=0.5,y=value,label=round(value,2)),colour=&quot;black&quot;,angle=0,hjust=0.5, vjust=0.5,size=3)&#10;&#10;p &amp;lt;- p + opts(panel.background = theme_rect(fill = &quot;white&quot;,colour = NA)) &#10;p &amp;lt;- p + opts(panel.grid.minor = theme_blank(), panel.grid.major = theme_blank())&#10;p &amp;lt;- p + opts(axis.title.x=theme_blank())&#10;p &amp;lt;- p + opts(axis.text.x = theme_blank())&#10;p &amp;lt;- p + opts(axis.title.y=theme_blank())&#10;p &amp;lt;- p + opts(axis.text.y = theme_blank())&#10;&#10;p + coord_flip()&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Result:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/HMkMM.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;...code maybe a bit ugly but works the right way.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-03-15T23:45:40.963" Id="8336" LastActivityDate="2011-03-16T00:07:39.330" LastEditDate="2011-03-16T00:07:39.330" LastEditorUserId="3376" OwnerUserId="3376" ParentId="8206" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;This result looks great. One thing that I would recommend doing is looking at the residuals for each of the two models to see if there are any trends in the first model that are not present in the second. If the model has residuals with an approximately normal distribution then you won't need to add more terms like A^2. The command for this is:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;my_model &amp;lt;- (formula = T ~ A + S + V)&#10;resid(my_model&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2011-03-16T05:06:26.793" Id="8343" LastActivityDate="2011-03-16T05:06:26.793" OwnerUserId="3727" ParentId="8340" PostTypeId="2" Score="1" />
  
  <row AnswerCount="3" Body="&lt;p&gt;While attending conferences, there has been a bit of a push by advocates of Bayesian statistics for assessing the results of experiments.  It is vaunted as both more sensitive, appropriate, and selective towards genuine findings (fewer false positives) than frequentist statistics.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have explored the topic somewhat, and I am left unconvinced so far of the benefits to using Bayesian statistics.  Bayesian analyses were used to refute &lt;a href=&quot;http://dbem.ws/&quot; rel=&quot;nofollow&quot;&gt;Daryl Bem&lt;/a&gt;'s research supporting precognition, however, so I remain cautiously curious about how Bayesian analyses might benefit even my own research.&lt;/p&gt;&#10;&#10;&lt;p&gt;So I am curious about the following:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Power in a Bayesian analysis vs. a frequentist analysis&lt;/li&gt;&#10;&lt;li&gt;Susceptibility to Type 1 error in each type of analysis&lt;/li&gt;&#10;&lt;li&gt;The trade-off in complexity of the analysis (Bayesian seems more complicated) vs. the benefits gained.  Traditional statistical analyses are straightforward, with well-established guidelines for drawing conclusions.  The simplicity could be viewed as a benefit.  Is that worth giving up?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Thanks for any insight!&lt;/p&gt;&#10;" CommentCount="16" CreationDate="2011-03-11T19:47:33.830" FavoriteCount="5" Id="8347" LastActivityDate="2011-03-18T11:36:47.960" LastEditDate="2011-03-16T07:50:42.073" LastEditorUserId="449" OwnerDisplayName="MindDetective" PostTypeId="1" Score="17" Tags="&lt;bayesian&gt;&lt;power&gt;&lt;frequentist&gt;" Title="Is Bayesian statistics genuinely an improvement over traditional (frequentist) statistics for behavioral research?" ViewCount="2382" />
  
  <row AcceptedAnswerId="8359" AnswerCount="3" Body="&lt;p&gt;I have a quarterly time series and test for stationarity with an augmented Dickey-Fuller test using R.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;adf.test(myseries)&#10;# returns&#10;# Dickey-Fuller = -3.9828, Lag order = 4, p-value = 0.01272&#10;# alternative hypothesis: stationary &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;so the H0 is rejected. I tried to validate this intuitively and regressed the same series on a linear trend.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x&amp;lt;- 104:1&#10;fit.1&amp;lt;-lm(myseries~x)&#10;summary(fit.1)&#10;#returns&#10;# x      0.024  1.31e-05 ***&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Even though a simple linear model is not so appropriate here and the intercept is large (around 80), there seems to be a slight downwards trend over time, which is in line with my thoughts after looking at the initial data. So do I get the adf.test wrong or is the trend just to small to be discovered? &lt;/p&gt;&#10;&#10;&lt;p&gt;Besides I used&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;plot(stl(myseries,&quot;per&quot;))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and ended up with a graph which sidebars suggested that trend and remainder were the main components driving the data, while seasonal influence was negligible. I saw that &lt;code&gt;stl()&lt;/code&gt; uses Local Polynomial Regression Filtering and got a rough idea how that works (still I wonder why smoothed trends of Hadley's ggplot2 package looked that different even though it uses the same method by default).&lt;/p&gt;&#10;&#10;&lt;p&gt;So summing up I got:&#10;- adf finding no evidence for a trend&#10;- a slight downwards trends &quot;detected&quot; by eyeballing and the naive approach&#10;- loess decomposition stating that the trend has strong influence (by the relation of its bars in the plot)&lt;/p&gt;&#10;&#10;&lt;p&gt;So what can I learned from this? Probably I do have a terminology problem here, because the former two seem to address time trends while the latter address some other trend I cannot fully grasp yet. Maybe my question is just: Can you help me to understand the trend extracted by loess? And how is it related to smoothed / filtered stuff like HP-Filter or Kalman Smoothing (if there is a relationship and similarity does not only occur in my case)?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-03-16T09:29:27.813" FavoriteCount="2" Id="8351" LastActivityDate="2011-03-16T22:55:24.480" LastEditDate="2011-03-16T15:06:42.183" LastEditorUserId="704" OwnerUserId="704" PostTypeId="1" Score="6" Tags="&lt;r&gt;&lt;time-series&gt;&lt;loess&gt;" Title="Trend or no trend?" ViewCount="3837" />
&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;And you can see how the effect of removing a single observation can be approximated without having to re-fit the model.  You can also see how an x equal to the average has &lt;em&gt;no influence on the slope of the line&lt;/em&gt;.  Think about this and you will see how it makes sense.  You can also write this more succinctly in terms of the standardised values $\tilde{x}=\frac{x-\overline{x}}{s_{x}}$ (similarly for y):&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\beta_{(i)}\approx \beta-\frac{\tilde{x_{i}}}{n-1}\left[\tilde{y_{i}}\frac{s_y}{s_x}-\tilde{x_{i}}\beta\right]
  <row AnswerCount="0" Body="&lt;p&gt;I have a log-log scale plot by gnuplot. With following ranges,&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set xrange [1:1000]&#10;set yrange [1:1000]&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;How do I draw slope 5/3 slope, ie, a slope that intersects y-axis at 5 and x-axis at 3?&lt;/p&gt;&#10;&#10;&lt;p&gt;Many thanks!&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-03-16T12:17:32.300" Id="8356" LastActivityDate="2011-04-15T13:40:26.583" OwnerUserId="3172" PostTypeId="1" Score="2" Tags="&lt;gnuplot&gt;" Title="How to draw a slope on a log-log scale in gnuplot" ViewCount="520" />
  
  <row Body="&lt;p&gt;A quick response to the bulleted content:&lt;/p&gt;&#10;&#10;&lt;p&gt;1) Power / Type 1 error in a Bayesian analysis vs. a frequentist analysis&lt;/p&gt;&#10;&#10;&lt;p&gt;Asking about Type 1 and power (i.e. one minus the probability of Type 2 error) implies that you can put your inference problem into a repeated sampling framework.  Can you?  If you can't then there isn't much choice but to move away from frequentist inference tools.  If you can, and if the behavior of your estimator over many such samples is of relevance, and if you are not particularly interested in making probability statements about particular events, then I there's no strong reason to move.&lt;/p&gt;&#10;&#10;&lt;p&gt;The argument here is not that such situations never arise - certainly they do - but that they typically don't arise in the fields where the methods are applied.&lt;/p&gt;&#10;&#10;&lt;p&gt;2) The trade-off in complexity of the analysis (Bayesian seems more complicated) vs. the benefits gained. &lt;/p&gt;&#10;&#10;&lt;p&gt;It is important to ask where the complexity goes.  In frequentist procedures the &lt;em&gt;implementation&lt;/em&gt; may be very simple, e.g. minimize the sum of squares, but the &lt;em&gt;principles&lt;/em&gt; may be arbitrarily complex, typically revolving around which estimator(s) to choose, how to find the right test(s), what to think when they disagree.  For an example. see the still lively discussion, picked up in this forum, of different confidence intervals for a proportion!&lt;/p&gt;&#10;&#10;&lt;p&gt;In Bayesian procedures the &lt;em&gt;implementation&lt;/em&gt; can be arbitrarily complex even in models that look like they 'ought' to be simple, usually because of difficult integrals but the &lt;em&gt;principles&lt;/em&gt; are extremely simple.  It rather depends where you'd like the messiness to be.&lt;/p&gt;&#10;&#10;&lt;p&gt;3) Traditional statistical analyses are straightforward, with well-established guidelines for drawing conclusions. &lt;/p&gt;&#10;&#10;&lt;p&gt;Personally I can no longer remember, but certainly my students &lt;em&gt;never&lt;/em&gt; found these straightforward, mostly due to the principle proliferation described above.  But the question is not really whether a procedure is straightforward, but whether is closer to being right given the structure of the problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, I strongly disagree that there are &quot;well-established guidelines for drawing conclusions&quot; in either paradigm.  And I think that's a &lt;em&gt;good&lt;/em&gt; thing.  Sure, &quot;find p&amp;lt;.05&quot; is a clear guideline, but for what model, with what corrections, etc.? And what do I do when my tests disagree?  Scientific or engineering judgement is needed here, as it is elsewhere.&lt;/p&gt;&#10;" CommentCount="12" CreationDate="2011-03-16T14:54:27.780" Id="8364" LastActivityDate="2011-03-16T14:54:27.780" OwnerUserId="1739" ParentId="8347" PostTypeId="2" Score="13" />
  <row Body="&lt;p&gt;One idea would be to use a random forest and then use the variable importance measures it outputs to choose your best 8 variables.  Another idea would be to use the &quot;boruta&quot; package to repeat this process a few hundred times to find the 8 variables that are consistently most important to the model.&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2011-03-16T16:04:09.750" Id="8367" LastActivityDate="2011-03-16T16:04:09.750" OwnerUserId="2817" ParentId="8303" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="9657" AnswerCount="2" Body="&lt;p&gt;French polling institutes are currently facing a major crisis after they recently published what can only be called &lt;a href=&quot;http://www.bbc.co.uk/news/world-europe-12660329&quot; rel=&quot;nofollow&quot;&gt;the most ridiculous poll so far&lt;/a&gt; on the 2012 presidential election horse race. The French Senate is now considering to legislate on the issue by forcing polling institutes to publish, among other things, the confidence intervals for their results.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, some pollsters are &lt;a href=&quot;http://www.lemonde.fr/idees/article/2011/03/08/il-est-inutile-de-legiferer-sur-la-question-des-sondages_1490046_3232.html&quot; rel=&quot;nofollow&quot;&gt;opposing&lt;/a&gt; the measure, claiming that confidence intervals do not apply to &lt;a href=&quot;http://en.wikipedia.org/wiki/Quota_sampling&quot; rel=&quot;nofollow&quot;&gt;quota sampling&lt;/a&gt;, which is the method used by polling institutes in France. Since quota sampling is formally non-probabilistic sampling, there is some truth to the claim. But since quota sampling is fundamentally &lt;a href=&quot;http://en.wikipedia.org/wiki/Stratified_sampling&quot; rel=&quot;nofollow&quot;&gt;stratified sampling&lt;/a&gt;, confidence intervals should apply, right?&lt;/p&gt;&#10;&#10;&lt;p&gt;May I ask for experiences about this issue outside of France, in countries where pollsters also use quota sampling?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2011-03-16T17:38:34.997" FavoriteCount="2" Id="8371" LastActivityDate="2011-04-18T16:21:07.000" OwnerUserId="3582" PostTypeId="1" Score="5" Tags="&lt;sampling&gt;&lt;polling&gt;" Title="Do confidence intervals apply to quota sampling?" ViewCount="800" />
  <row AnswerCount="1" Body="&lt;p&gt;I am trying to correlate age (6-90 yrs) with loudness of voice (in dB). However, my data do not contain any data points in the range of 20-50 yrs. &lt;/p&gt;&#10;&#10;&lt;p&gt;What correlation measure is most appropriate with such a considerable gap, and why? I have been using Kendall Tau so far.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that we are not dealing with bimodally distributed data here, but with a substantial missing data gap in the age range.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-03-16T18:52:37.840" Id="8374" LastActivityDate="2011-03-16T22:59:14.990" LastEditDate="2011-03-16T22:44:11.883" LastEditorUserId="919" OwnerDisplayName="user3742" PostTypeId="1" Score="8" Tags="&lt;distributions&gt;&lt;correlation&gt;&lt;missing-data&gt;" Title="Which correlation measure should be used with a large gap (missing data)?" ViewCount="123" />
  
  
  <row Body="&lt;p&gt;It does. Use the &lt;code&gt;accuracy()&lt;/code&gt; command.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt; here is an example.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(forecast)&#10;x &amp;lt;- EuStockMarkets[1:200,1]&#10;f &amp;lt;- EuStockMarkets[201:300,1]&#10;fit1 &amp;lt;- ses(x,h=100)&#10;accuracy(fit1,f)&#10;        ME       RMSE        MAE        MPE       MAPE       MASE       ACF1  Theil's U &#10; 0.8065983 78.1801986 63.2728352 -0.1725009  3.7876802  7.0619776  0.9586859  6.6120277 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;If you want the &lt;em&gt;in-sample&lt;/em&gt; value of U (which is of limited value), the following will work:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;fpe &amp;lt;- fit1$fitted[2:200]/x[1:199] - 1&#10;ape &amp;lt;- x[2:200]/x[1:199] - 1&#10;U &amp;lt;- sqrt(sum((fpe - ape)^2)/sum(ape^2))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2011-03-17T05:01:07.543" Id="8398" LastActivityDate="2011-03-17T09:28:55.800" LastEditDate="2011-03-17T09:28:55.800" LastEditorUserId="159" OwnerUserId="159" ParentId="8396" PostTypeId="2" Score="9" />
  
  
  <row AcceptedAnswerId="8434" AnswerCount="1" Body="&lt;p&gt;Supposing a Bayesian classifier with multivariate normal densities, how do I find the error rate of the classifier when we have two classes?&lt;/p&gt;&#10;&#10;&lt;p&gt;I am using this:&lt;/p&gt;&#10;&#10;&lt;p&gt;When dimension $d = 1$:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$P(x | \mu , \sigma^2) = N(x, \mu, \sigma^2) = \frac{1}{\sqrt{2\pi \sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}} $$&lt;/p&gt;&#10;&#10;&lt;p&gt;In $d$ dimensions:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$P(x | \mu, \Sigma) = \frac{1}{(2\pi)^{d/2} |\Sigma|^{1/2}} \cdot e^{-\frac{1}{2}{(x-μ)^T\Sigma^{-1}(x-μ)}}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\mu$ is in $\mathbb{R}^d$ and $\Sigma$ is a $d \times d$ variance-covariance matrix.&lt;/p&gt;&#10;&#10;&lt;p&gt;What would be the algebra to get the error rate, or could you give an example? I would like to do this in &lt;code&gt;Matlab&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Lets say I have 2 classes and 2 atributes with 20 examples:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;features:&#10;    1.8756    1.4236&#10;    2.0677    1.3759&#10;    0.8540    0.7782&#10;    0.5651   -0.3511&#10;    0.6103   -0.6901&#10;   -0.1945    1.6438&#10;   -0.2620    0.8022&#10;    1.5326    0.0188&#10;    0.3334    1.0578&#10;    0.8535   -1.8545&#10;    0.3066    1.9716&#10;   -1.4424    0.4216&#10;    0.3275   -0.2844&#10;   -0.0079    2.8506&#10;    0.0114    1.4001&#10;   -0.4049   -0.3981&#10;   -0.0913    2.2094&#10;    0.3376   -1.0467&#10;    0.3455    2.4960&#10;    0.3232   -0.5614&#10;&#10;&#10;and targets&#10;&#10;     2&#10;     2&#10;     1&#10;     1&#10;     1&#10;     2&#10;     1&#10;     1&#10;     2&#10;     1&#10;     2&#10;     1&#10;     1&#10;     2&#10;     2&#10;     1&#10;     2&#10;     1&#10;     2&#10;     1&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;What is the process to classify them and get the error rate of this examples, so we divide the set in 15 examples for training and the other 5 for testing?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-03-17T17:18:25.883" Id="8419" LastActivityDate="2011-04-29T00:57:16.927" LastEditDate="2011-04-29T00:57:16.927" LastEditorUserId="3911" OwnerUserId="3681" PostTypeId="1" Score="2" Tags="&lt;machine-learning&gt;&lt;naive-bayes&gt;" Title="Bayesian classifier with multivariate normal densities" ViewCount="1303" />
  <row AcceptedAnswerId="8450" AnswerCount="3" Body="&lt;p&gt;Other then gapminder.org, could you please direct me to any good examples (interactive or static) of visualizations that compare between countries?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-03-17T17:33:20.797" Id="8420" LastActivityDate="2011-03-18T15:59:39.973" LastEditDate="2011-03-18T15:59:39.973" LastEditorUserId="88" OwnerUserId="253" PostTypeId="1" Score="3" Tags="&lt;data-visualization&gt;" Title="Examples of visualization featuring comparisons between countries?" ViewCount="128" />
  <row Body="&lt;p&gt;I am not totally sure to figure out what model is fitted when you use the lme function. (I guess the random effect is supposed to follow a normal distribution with zero mean?). However, the linear model is a special case of the mixed model when the variance of the random effect is zero. Although some technical difficulties exist (because $0$ is in the boundary of the parameter space for the variance) it should be possible to test $H_0:variance = 0$ vs $H_1: variance &amp;gt; 0$... &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;In order to avoid confusion: The test mentioned above is sometimes used to decide whether or not the random effect is significant... but not to decide whether or not it should be transformed into a fixed effect.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-03-17T20:08:02.357" Id="8429" LastActivityDate="2011-03-17T20:34:47.713" LastEditDate="2011-03-17T20:34:47.713" LastEditorUserId="3019" OwnerUserId="3019" ParentId="8428" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;I would have thought the &quot;best&quot; (most intuitive) estimate of error would be the probability that the classification is incorrect.  Or alternatively/equivalently the odds against the class.  Using the &lt;a href=&quot;http://en.wikipedia.org/wiki/Naive_Bayes_classifier&quot; rel=&quot;nofollow&quot;&gt;wikipedia page&lt;/a&gt;, you classify as Male or female.  I would have thought the estimate of accuracy should be:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$O(Male|evidence)=\frac{P(Male|evidence)}{P(Female|evidence)}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;So you would report the data in terms of &quot;the evidence gives odds O:1 in favour of this classification&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;In a problem with more than 2 classes, you should report the &quot;worst&quot; odds ratio.  That is given classes $C_{i}\;\;(i=1,\dots,R+1)$, one (and only one) of which is assumed to be true.  Suppose you classify an observation as $C_{R+1}$, then you would report odds of:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$O(C_{R+1}|evidence)=\frac{P(C_{R+1}|evidence)}{Max_{i\in[1,\dots,R]}P(C_{i}|evidence)}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;So you would report the data in terms of &quot;the evidence gives odds O:1 in favour of this classification against the next best alternative&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT/UPDATE&lt;/strong&gt;: in response to the new part of the question, to put some numbers into the calculation.  Using the first 15 observations to &quot;train&quot; the classifier.  Because you are dealing with normal distribution, you only need the sufficient statistics, which in this case are:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$ \begin{pmatrix}
&#10;=
&#10;\times
&#10;\times\frac{\hat{\sigma}_{2}\hat{\tau}_{2}}{\hat{\sigma}_{1}\hat{\tau}_{1}}
&#10;\right]^{\frac{7}{2}}
&#10;0.3232 &amp;amp; -0.5614 &amp;amp; 54.6 &amp;amp; 49.7 &amp;amp; 1
&#10; \end{array}
  
  
  
  <row Body="&lt;p&gt;This depends on what you mean by &quot;multivariate&quot;.  Doing a quick google search, the term seems to refer to &quot;more than 1 co variate&quot;.  So the model equation is for &quot;ordinary logistic regression&quot; you have:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$log(\frac{\pi}{1-\pi})=\beta_0 + \beta_1 X_1$$&lt;/p&gt;&#10;&#10;&lt;p&gt;For &quot;multivariate logistic regression&quot; you have:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$logit(\frac{\pi}{1-\pi})=\beta_0 + \beta_1 X_1 + \beta_2 X_2 +\dots+ \beta_p X_p=X\beta$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Where $X\beta$ is the &quot;matrix version&quot; - just for more compact notation.  But as @onestop points out, the term &quot;multivariate&quot; could also refer to more than 1 probability.  The model equations are then:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$logit(\frac{\pi_1}{\pi_m})=X^{(1)}\beta^{(1)}$$&#10;$$logit(\frac{\pi_2}{\pi_m})=X^{(2)}\beta^{(2)}$$&#10;$$\dots$$&#10;$$logit(\frac{\pi_{m-1}}{\pi_m})=X^{(m-1)}\beta^{(m-1)}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Where the brackets $(1),(2),..,(m-1)$ just indicate that the co variates and betas could be different in each model.  The &quot;multinomial&quot; simplifies back to the &quot;multivariate&quot; when $m=2$ (only 2 categories), and the &quot;multivariate&quot; simplifies back to the &quot;ordinary logistic regression&quot; when you have only 1 co variate.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now these are all &quot;independence&quot; based models, in that given the covariates and betas, any one individual's probability tells you nothing about any others.  When you move from this framework, which is called &quot;generalised linear modelling&quot; or GLM in the literature, to the &quot;extra M&quot; of GLMM, the observations are now assumed to somehow depend on each other.  The easiest way I can see a reason for this happening is that you know a particular attribute will increase or decrease the probability of a certain group of observations, but you have not measured that covariate in your data (e.g. you are analyzing the incidence of cancer among people, but you do not know which people in your data set are smokers.  However have variables which could predict smoking status.).  The usual GLM model goes:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$DATA=MODEL+NOISE^{(1)}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;(NOTE: although the likelihood isn't linear for GLMs, the usual fisher-scoring IRLS or newton-rhapson algorithm means this is the effective model one is actually estimating - because it is based on a 1 term taylor series expansion &quot;the delta method&quot;).&lt;/p&gt;&#10;&#10;&lt;p&gt;Whereas the GLMM goes:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$DATA=MODEL+MODEL(UNOBSERVED)+NOISE^{(2)}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The unobserved model is sometimes written as $Z\gamma$.  So your model equation becomes:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$logit(\frac{\pi}{1-\pi})=X\beta+Z\gamma$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The vector $\gamma$ is often called a &quot;random effect&quot; (and is usually assumed to have a normal distribution) and the vector $\beta$ is called a &quot;fixed effect&quot; (although I don't particularly like this terminology).&lt;/p&gt;&#10;&#10;&lt;p&gt;And the idea is to &quot;decompose&quot; the noise into the &quot;correlated part&quot; and the &quot;independent part&quot;.  You fit the &quot;fixed effect&quot; part of the model - this is like the &quot;global model&quot;.  The GLMM basically allows you to recognise that there can be &quot;local effects&quot; in your data - systematic deviations from the &quot;global model&quot; for small groups, in a very similar way to ANOVA and ANCOVA.  In fact, all of the GLMMs I have fit, my $Z$ matrix has been a classical design matrix for ANOVA.&lt;/p&gt;&#10;&#10;&lt;p&gt;So basically what the GLMM does is make a compromise between the global model and the local model.  The compromise depends on the estimated variance for the &quot;random effect&quot; and on the estimate variance of the noise.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-03-18T13:08:03.853" Id="8452" LastActivityDate="2011-03-18T13:08:03.853" OwnerUserId="2392" ParentId="8435" PostTypeId="2" Score="2" />
  <row AnswerCount="2" Body="&lt;p&gt;I am a senior doing a science fair project on the efficiency of fuels. For each fuel I tested, I have a time series of temperature.&#10;I want to plot these into an excel xy chart, how do I plot them if the constants (time) are not the same for each set of data?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-03-18T14:03:39.683" Id="8456" LastActivityDate="2013-07-26T09:01:36.963" LastEditDate="2011-03-18T16:07:45.020" LastEditorUserId="88" OwnerDisplayName="user3778" PostTypeId="1" Score="2" Tags="&lt;data-visualization&gt;&lt;excel&gt;" Title="Excel xy chart with unequal x values in series" ViewCount="5367" />
  <row Body="&lt;p&gt;What you've illustrated is a time series column (or bar) graph. The two graphs are of differing time resolution or differing time aggregation. &lt;/p&gt;&#10;&#10;&lt;p&gt;There may be industry specific terms for these types of charts. In finance, for example, the &lt;a href=&quot;http://en.wikipedia.org/wiki/Open-high-low-close_chart&quot; rel=&quot;nofollow&quot;&gt;open-high-low-close chart&lt;/a&gt; is a very common time series plot:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/2eiG2.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;When the x axis is time, as in your example, it's often common to illustrate the points as a line graph, instead of bars/columns. The reason for this is to put the visual emphasis on the change from one period to the next. &lt;/p&gt;&#10;&#10;&lt;p&gt;You might also consider graphing period-over-period. For example a &lt;a href=&quot;http://spotfire.tibco.com/community/blogs/stn/archive/2009/01/28/create-a-year-over-year-comparison-chart.aspx&quot; rel=&quot;nofollow&quot;&gt;year-over-year&lt;/a&gt; would show how the numbers for a given month (typically, although could be month or day) compare to the numbers of the prior year for the same month.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/nrlo1.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;But I realize your question was about naming, not all the other cool graphs you can do ;) &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-03-18T19:05:58.343" Id="8465" LastActivityDate="2011-03-18T19:05:58.343" OwnerUserId="29" ParentId="8462" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;You will find help on allocating categorical variables with &lt;a href=&quot;http://www.ats.ucla.edu/stat/spss/dae/logit.htm&quot; rel=&quot;nofollow&quot;&gt;UCLA's tutorials&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;logistic regression HIV with age sex&#10;   /categorical = sex.&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;You might also find help &lt;a href=&quot;http://www.childrens-mercy.org/stats/weblog2004/categorical.asp&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-03-19T12:43:19.813" Id="8492" LastActivityDate="2011-03-19T12:43:19.813" OwnerUserId="1351" ParentId="8483" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;An ellipse can be parametrized as the affine image of any given circle. If we consider the unit circle:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$x=a \cos (t)$$&#10;$$y=b \sin (t)$$&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ellipse(center, shape, radius, log=&quot;&quot;, center.pch=19, center.cex=1.5, &#10;  segments=51, add=TRUE, xlab=&quot;&quot;, ylab=&quot;&quot;, &#10;   col=palette()[2], lwd=2, fill=FALSE, fill.alpha=0.3, grid=TRUE, ...)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;You can notice the &lt;code&gt;ellipse&lt;/code&gt; function asks for the center and the radius of the circle, as well as the covariance matrix, which is equivalent to giving the parameters of the affine transformation.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;center  2-element vector with coordinates of center of ellipse.&#10;shape   2 * 2 shape (or covariance) matrix.&#10;radius  radius of circle generating the ellipse.&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Let us have a look at the &lt;code&gt;car&lt;/code&gt; package function:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ellipse &amp;lt;- t(center + radius * t(unit.circle %*% chol(shape)))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The &lt;code&gt;radius&lt;/code&gt; parameter can be set to 1 if you want to use the covariance matrix directly for the &lt;code&gt;shape&lt;/code&gt; parameter. I believe it was introduced to help people use normalized matrices instead if they prefer so.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;Edit: As mentioned in whuber's comment, the two ellipses below are the same.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; library(car)&#10;&amp;gt; s=matrix(c(1,0,0,1), nrow=2, ncol=2)&#10;&amp;gt; plot(0, 0, xlim=c(-5,5), ylim=c(-5,5))&#10;&amp;gt; ellipse(c(0,0), 4*s, 1)&#10;&amp;gt; ellipse(c(0,0), s, 2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2011-03-19T18:49:26.673" Id="8507" LastActivityDate="2011-03-19T19:40:46.057" LastEditDate="2011-03-19T19:40:46.057" LastEditorUserId="1351" OwnerUserId="1351" ParentId="8504" PostTypeId="2" Score="3" />
  
  
  
  
  
  <row Body="&lt;p&gt;Why stop at $t$-tests?&lt;/p&gt;&#10;&#10;&lt;p&gt;You can think of two variables being uncorrelated as two orthogonal vectors, exactly like the $x$ and $y$ axes in a two dimensional Cartesian coordinate system.&lt;/p&gt;&#10;&#10;&lt;p&gt;When either of two vectors, let's say $\mathbf{x}$ and $\mathbf{y}$ is correlated with the other, there will be a certain part of x that can be projected onto y and vice versa. With that in mind, it's fairly easy to see that since,&lt;/p&gt;&#10;&#10;&lt;p&gt;$$
&#10;\frac{\left&amp;lt;\mathbf{x},\mathbf{y}\right&amp;gt;}{\|x\|\|y\|}&amp;amp;=\cos\left(\theta\right)=r
  <row AcceptedAnswerId="8613" AnswerCount="3" Body="&lt;h3&gt;Context:&lt;/h3&gt;&#10;&#10;&lt;p&gt;I'm a Psychology PhD student. As with many psychology PhD students, I know how to perform various statistical analyses using statistical software, up to techniques such as PCA, classification trees, and cluster analysis.&#10;But it's not really satisfying because though I can explain why I did an analysis and what the indicators mean, I can't explain how the technique works.&lt;/p&gt;&#10;&#10;&lt;p&gt;The real problem is that mastering statiscal software is easy, but it is limited.&#10;To learn new techniques in articles requires that I understand how to read mathematical equations. At present I couldn't calculate eigenvalues or K-means. Equations are like a foreign language to me.&lt;/p&gt;&#10;&#10;&lt;h3&gt;Question:&lt;/h3&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Is there a comprehensive guide that helps with understanding equations in journal articles?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;h3&gt;EDIT&lt;/h3&gt;&#10;&#10;&lt;p&gt;Thanks for your comments.&#10; I thought the question would be more self explanatory: above a certain complexity, statistical notation becomes gibberish for me; let's say I would like to code my own functions in R or C++ to understand a technique but there's a barrier. I can't transform an equation into a program.&#10;And really: I don't know the situation in US doctoral schools, but in mine (France), the only courses I can follow is about some 16th century litterary movement...&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2011-03-21T18:33:17.083" FavoriteCount="2" Id="8581" LastActivityDate="2011-03-22T13:58:45.933" LastEditDate="2011-03-22T09:45:41.400" LastEditorUserId="3827" OwnerUserId="3827" PostTypeId="1" Score="9" Tags="&lt;books&gt;&lt;software&gt;&lt;learning&gt;" Title="How to transition from using statistical software to understanding mathematical equations in journal articles?" ViewCount="475" />
  <row Body="&lt;p&gt;If you report the mean, then it is more appropriate to report the standard deviation as it is expressed in the same unity. Think about dimensional homogeneity in physics.&lt;/p&gt;&#10;&#10;&lt;p&gt;Moreover, it is easier for the reader to consider confidence intervals (for large n, in order to use the Central Limit Theorem and consider a normal distribution) if the standard deviation is provided rather than the variance.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, you may consider reporting the variance if you are interested in comparing variance and bias, or giving &quot;different variance components&quot;, since the total variance is the sum of the intra and inter variances, while the standard deviations do not sum up.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-03-21T19:57:14.420" Id="8584" LastActivityDate="2011-03-21T19:57:14.420" OwnerUserId="1351" ParentId="8583" PostTypeId="2" Score="12" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I want to understand how to make calculations on the prevalence of a disease in a country population and the impact that the element of average life expectancy (of those suffering with the disease at time of diagnosis) has on this calculation. &lt;/p&gt;&#10;&#10;&lt;p&gt;Are there any 'best practice' papers on making epidemiology calculations? &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-03-22T14:12:22.203" FavoriteCount="2" Id="8631" LastActivityDate="2012-09-02T15:56:25.523" LastEditDate="2012-09-02T15:56:25.523" LastEditorUserId="919" OwnerUserId="3844" PostTypeId="1" Score="7" Tags="&lt;epidemiology&gt;" Title="How would life expectancy impact the calculation of disease prevalence?" ViewCount="203" />
  <row AcceptedAnswerId="8641" AnswerCount="1" Body="&lt;p&gt;Greetings, &#10;Is it possible to use evidence in a Winbug model? For example, a random variable in a model has been observed, and I'd like to update the other variables in the model, pretty much the same update perfomed in tools like Smile, or other inference software. &lt;/p&gt;&#10;&#10;&lt;p&gt;Gibbs sampling is supposed to use observed values in full conditional distribution when there observations/evidence are entered into the model, but I am not sure if Winbugs allows this. &lt;/p&gt;&#10;&#10;&lt;p&gt;Regards&lt;/p&gt;&#10;&#10;&lt;p&gt;edit to clarify:&lt;/p&gt;&#10;&#10;&lt;p&gt;The winbug documentation says a stochastic node is a data node if it has been observed, but a stochastic node is described via a distribution as in some_var ~ dbin(theta,n) &#10;if it has been observed, then I'd like to tell this to winbugs without losing the semantics of the stochastic node, expressing something like &quot;some_var  has this distribution, and it has been observed to have this particular value&quot;. &#10;so how do I do that? by declaring some_var as I've done above and than setting a value to it as in some_var = 5 ? Would that express what I want to express?&lt;/p&gt;&#10;&#10;&lt;p&gt;In this case, in each observation of a node in a bayesian network, I'd need to redefine the winbugs model, (quite likely) replacing initial values of the unobserved nodes with the outcomes of the previous simulation. &lt;/p&gt;&#10;&#10;&lt;p&gt;In short, I'm trying to understand how to perform updates on a Bayesian network similar to message passing in exact inference, but using Gibbs sampling instead, via Winbugs.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-03-22T17:23:12.153" Id="8639" LastActivityDate="2011-03-23T10:13:02.737" LastEditDate="2011-03-23T10:13:02.737" LastEditorUserId="3280" OwnerUserId="3280" PostTypeId="1" Score="2" Tags="&lt;bugs&gt;&lt;inference&gt;&lt;updating&gt;" Title="Can I insert an observation (evidence) to a Winbugs model?" ViewCount="105" />
  
&#10;\varphi_m(x) = \alpha \cdot \frac{1}{\sqrt{2\pi}} e^{-\frac{(x+\mu)^2}{2}} + (1-\alpha) \cdot
  
  <row Body="&lt;p&gt;If you are familiar with programming, you could perhaps think of it as a method (not to be confused with a logical part of some code) being a brief description of an algorithm in pseudo-code, whereas a procedure is a specific implementation with exact syntax.&lt;/p&gt;&#10;&#10;&lt;p&gt;Admitted that this is not a perfect metaphor but I think it portrays the difference. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-03-23T08:41:36.297" Id="8658" LastActivityDate="2011-03-23T08:41:36.297" OwnerUserId="3014" ParentId="8632" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="8674" AnswerCount="3" Body="&lt;p&gt;I have the sample population of a certain signal's registered amplitude maxima. Population is about 15 million samples. I produced a histogram of the population, but cannot guess the distribution with such a histogram.&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT1: File with raw sample values is here: &lt;a href=&quot;http://hotfile.com/dl/111583549/5c73384/TDETQ_Z2M_ALL_TIM50_N.txt.zip.html&quot;&gt;raw data&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Can anyone help estimate the distribution with the following histogram:&#10;&lt;img src=&quot;http://i.stack.imgur.com/h1n2y.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2011-03-23T10:20:57.830" FavoriteCount="8" Id="8662" LastActivityDate="2013-07-03T03:00:04.457" LastEditDate="2011-03-24T04:54:13.743" LastEditorUserId="2820" OwnerUserId="2820" PostTypeId="1" Score="10" Tags="&lt;distributions&gt;&lt;histogram&gt;" Title="Need help identifying a distribution by its histogram" ViewCount="1386" />
  
  <row Body="&lt;p&gt;The UCLA stats page has &lt;a href=&quot;http://www.ats.ucla.edu/stat/r/dae/logit.htm&quot;&gt;a nice walk-through&lt;/a&gt; of performing logistic regression in R. It includes a brief section on calculating odds ratios.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-03-23T14:39:55.110" Id="8672" LastActivityDate="2011-03-23T14:39:55.110" OwnerUserId="124" ParentId="8661" PostTypeId="2" Score="12" />
  <row Body="&lt;p&gt;You can find minimum voting ages in &lt;a href=&quot;http://en.wikipedia.org/wiki/Voting_age&quot; rel=&quot;nofollow&quot;&gt;Wikipedia&lt;/a&gt;.  Most large countries use 18, except for Brazil and Indonesia.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can find country population by age in the &lt;a href=&quot;http://www.census.gov/ipc/www/idb/&quot; rel=&quot;nofollow&quot;&gt;U.S Census Bureau International Data Base&lt;/a&gt;. It does not seem to use 18 as a break point, but you should be able to divide the 15-19 age group safely. This suggests slightly less than 69% of the world population is 18+ in 2011.&lt;/p&gt;&#10;&#10;&lt;p&gt;Voter turnout by age is harder, though you may not be interested. I would expect there to be national figures in scattered sources but not a central collection.  The &lt;a href=&quot;http://www.idea.int/vt/by_age.cfm&quot; rel=&quot;nofollow&quot;&gt;International Institute for Democracy and Electoral Assistance&lt;/a&gt; tried but only managed to list three countries.   &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-03-23T16:42:38.700" Id="8679" LastActivityDate="2011-03-23T18:45:20.533" LastEditDate="2011-03-23T18:45:20.533" LastEditorUserId="930" OwnerUserId="2958" ParentId="8657" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;Do you know the &lt;a href=&quot;http://sites.google.com/site/qgraphproject/&quot;&gt;qgraph&lt;/a&gt; project (and the related &lt;a href=&quot;http://cran.r-project.org/web/packages/qgraph/index.html&quot;&gt;R package&lt;/a&gt;)? It aims at providing various displays for psychometric models, especially those relying on correlations. I discovered this approach for displaying correlation measures when I was reading a very nice and revolutionary article on diagnostic medicine by Denny Borsboom and coll.: &lt;a href=&quot;http://sites.google.com/site/borsboomdenny/CramerEtAl2010.pdf&quot;&gt;Comorbidity: A network perspective&lt;/a&gt;, BBS (2010) 33: 137-193. &lt;/p&gt;&#10;&#10;&lt;p&gt;An oversimplified summary of their &lt;em&gt;network approach&lt;/em&gt; of comorbidity is that it is “hypothesized to arise from direct relations between symptoms of multiple disorders”, contrary to the more classical view where these are comorbid disorders themselves that causes their associated symptoms to correlate (as reflected in a latent variable model, like factor or item response models, where a given symptom would allow to measure a particular disorder). In fact, symptoms are part of disorder, but they don’t measure it (and this is a mereological relationship). Their figure 5 describes such a &quot;comorbidity network&quot; and is particularly interesting as it embeds the frequency of symptoms and magnitude of their bivariate association in the same picture. They were using &lt;a href=&quot;http://www.cytoscape.org/&quot;&gt;Cytoscape&lt;/a&gt; at that time, but the qgraph project has now reached a mature state. &lt;/p&gt;&#10;&#10;&lt;p&gt;Here are some examples from the on-line R help; basically, these are (1) an association graph with circular or (2) spring layout, (3) a concentration graph with spring layout, and (4) a factorial graph with spring layout (but see &lt;code&gt;help(qgraph.panel)&lt;/code&gt;):&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/hYT3r.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;(See also &lt;code&gt;help(qgraph.pca)&lt;/code&gt; for nice circular displays of an observed correlation matrix for the NEO-FFI, which is a 60-item personality inventory.)&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2011-03-23T18:32:52.837" Id="8687" LastActivityDate="2011-03-23T18:32:52.837" OwnerUserId="930" ParentId="8677" PostTypeId="2" Score="7" />
  
  
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;Is it possible to perform logarithmic regression on multiple variables with Excel? If I just have a single independent variable than it's very easy to do this using the best-fit line option (it lets me switch from linear to logarithmic). But this feature does not work for multiple variable regression and the regression feature under the Data Analysis plugin only seems to support linear multiple regression.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, I have a table that has 3 columns containing 3 independent variables and 1 column with the corresponding dependent variable (outcome). I'm pretty sure there's a logarithmic relationship, but I'm not sure how to use Excel to get the coefficients. Thanks!&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-03-24T05:30:59.240" Id="8714" LastActivityDate="2011-03-24T17:34:04.710" LastEditDate="2011-03-24T16:10:25.313" LastEditorUserId="88" OwnerDisplayName="user3881" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;excel&gt;" Title="Performing logarithmic multiple regression with Excel?" ViewCount="2244" />
  
  
  
  <row AcceptedAnswerId="8741" AnswerCount="3" Body="&lt;p&gt;It's pretty tough to search the Web for info on something when you don't know what words are commonly used to describe it. In this case, I'm wondering what it's called when you include another predictor in a time series.&lt;/p&gt;&#10;&#10;&lt;p&gt;As an example, say I'm modeling a variable $X$ using AR(3):&lt;/p&gt;&#10;&#10;&lt;p&gt;$ X_t = \varphi_1 X_{t-1} + \varphi_2 X_{t-2} + \varphi_3 X_{t-3} + \varepsilon_t $&lt;/p&gt;&#10;&#10;&lt;p&gt;I want my model to include the effects of another variable—say, $Y$—so my model is now described as:&lt;/p&gt;&#10;&#10;&lt;p&gt;$ X_t = \varphi_1 X_{t-1} + \varphi_2 X_{t-2} + \varphi_3 X_{t-3} + \beta_1 Y_{t-1} + \beta_2 Y_{t-2} + \beta_3 Y_{t-3} + \varepsilon_t $&lt;/p&gt;&#10;&#10;&lt;p&gt;What would the term (or terms) be that distinguishes the former model from the latter?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-03-24T20:25:16.957" FavoriteCount="2" Id="8738" LastActivityDate="2011-04-07T21:01:14.247" OwnerUserId="1583" PostTypeId="1" Score="7" Tags="&lt;time-series&gt;&lt;terminology&gt;" Title="What is the term for a time series regression having more than one predictor?" ViewCount="462" />
  <row Body="&lt;p&gt;Have a look in a Multivariate text for MANOVA--multivariate ANOVA.  Here is a website...&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://faculty.chass.ncsu.edu/garson/PA765/manova.htm&quot; rel=&quot;nofollow&quot;&gt;http://faculty.chass.ncsu.edu/garson/PA765/manova.htm&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Though, that's a lot of dependent variables and it could be hard to interpret.  It might be simpler to do some sort of data reduction first, like PCA among your set of fatty acids.  I suppose it depends on how much overlap there is and how many PCs you'd need to extract to account for the 30 vars.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-03-24T20:29:46.690" Id="8739" LastActivityDate="2011-03-24T21:05:57.457" LastEditDate="2011-03-24T21:05:57.457" LastEditorUserId="485" OwnerUserId="485" ParentId="8733" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;This is called a Transfer Function Model. It has also been referred to as a Dynamic Regression Model.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-03-24T23:59:50.460" Id="8746" LastActivityDate="2011-03-24T23:59:50.460" OwnerUserId="3382" ParentId="8738" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;In this particular context, PCA is mainly used to account for population-specific variations in alleles distribution on the SNPs (or other DNA markers, although I'm only familiar with the SNP case) under investigation. Such &quot;population substructure&quot; mainly arises as a consequence of varying frequencies of minor alleles in genetically distant ancestries (e.g. japanese and black-african or european-american). The general idea is well explained in &lt;a href=&quot;http://www.genome.duke.edu/education/seminars/journal-club/documents/PopStructure.pdf&quot;&gt;Population Structure and Eigenanalysis&lt;/a&gt;, by Patterson et al. (&lt;em&gt;PLoS Genetics&lt;/em&gt; 2006, 2(12)), or the &lt;em&gt;Lancet&lt;/em&gt;'s special issue on genetic epidemiology (2005, 366; most articles can be found on the web, start with Cordell &amp;amp; Clayton, &lt;a href=&quot;http://www.molmed.nl/uploads/abstracts/127/Lancet%20genetic%20epi%203.pdf&quot;&gt;Genetic Association Studies&lt;/a&gt;).&lt;/p&gt;&#10;&#10;&lt;p&gt;The construction of principal axes follows from the classical approach to PCA, which is applied to the scaled matrix (individuals by SNPs) of observed genotypes (AA, AB, BB; say B is the minor allele in all cases), to the exception that an additional normalization to account for population drift might be applied. It all assumes that the frequency of the minor allele (taking value in {0,1,2}) can be considered as numeric, that is we work under an &lt;em&gt;additive model&lt;/em&gt; (also called allelic dosage) or any equivalent one that would make sense. As the successive orthogonal PCs will account for the maximum variance, this provides a way to highlight groups of individuals differing at the level of minor allele frequency. The software used for this is known as &lt;a href=&quot;http://genepath.med.harvard.edu/~reich/Software.htm&quot;&gt;Eigenstrat&lt;/a&gt;. It is also available in the &lt;code&gt;egscore()&lt;/code&gt; function from the &lt;a href=&quot;http://cran.r-project.org/web/packages/GenABEL/&quot;&gt;GenABEL&lt;/a&gt; R package (see also &lt;a href=&quot;http://www.genabel.org/&quot;&gt;GenABEL.org&lt;/a&gt;). It is worth to note that other methods to detect population substructure were proposed, in particular model-based cluster reconstruction (see references at the end). More information can be found by browsing the &lt;a href=&quot;http://hapmap.ncbi.nlm.nih.gov/&quot;&gt;Hapmap&lt;/a&gt; project, and available tutorial coming from the &lt;a href=&quot;http://www.bioconductor.org&quot;&gt;Bioconductor&lt;/a&gt; project. (Search for Vince J Carey or David Clayton's nice tutorials on Google). &lt;/p&gt;&#10;&#10;&lt;p&gt;Apart from clustering subpopulations, this approach can also be used for detecting outliers which might arise in two cases (AFAIK): (a) genotyping errors, and (b) when working with an homogeneous population (or assumed so, given self-reported ethnicity), individuals exhibiting unexpected genotype. What is usually done in this case is to apply PCA in an iterative manner, and remove individuals whose scores are below $\pm 6$ SD on at least one of the first 20 principal axes; this amounts to &quot;whiten&quot; the sample, in some sense. Note that any such measure of genotype distance (this also holds when using Multidimensional Scaling in place of PCA) will allow to spot relatives or siblings. The &lt;a href=&quot;http://pngu.mgh.harvard.edu/~purcell/plink/&quot;&gt;plink&lt;/a&gt; software provides additional methods, see the section on &lt;a href=&quot;http://pngu.mgh.harvard.edu/~purcell/plink/strat.shtml&quot;&gt;Population stratification&lt;/a&gt; in the on-line help.&lt;/p&gt;&#10;&#10;&lt;p&gt;Considering that eigenanalysis allows to uncover some structure at the level of the individuals, we can use this information when trying to explain observed variations in a given phenotype (or any distribution that might be defined according to a binary criterion, e.g. disease or case-control situation). Specifically, we can adjust our analysis with those PCs (i.e., the factor scores of individuals), as illustrated in &lt;a href=&quot;http://www.biostat.jhsph.edu/~iruczins/teaching/misc/gwas/papers/price2006.pdf&quot;&gt;Principal components analysis corrects for stratification in genome-wide association studies&lt;/a&gt;, by Price et al. (&lt;em&gt;Nature Genetics&lt;/em&gt; 2006, 38(8)), and later work (there was a nice picture showing axes of genetic variation in Europe, but I can't find it actually). Note also that another solution is to carry out a stratified analysis (by including ethnicity in an GLM)--this is readily available in the &lt;a href=&quot;http://www.bioconductor.org/packages/2.3/bioc/html/snpMatrix.html&quot;&gt;snpMatrix&lt;/a&gt; package, for example.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Daniel Falush, Matthew Stephens, and Jonathan K Pritchard (2003). &lt;a href=&quot;http://pritch.bsd.uchicago.edu/software/FalushEtAl03.pdf&quot;&gt;Inference of population structure using multilocus genotype data: linked loci and correlated allele frequencies&lt;/a&gt;. &lt;em&gt;Genetics&lt;/em&gt;, 164(4): 1567–1587.&lt;/li&gt;&#10;&lt;li&gt;B Devlin and K Roeder (1999). &lt;a href=&quot;http://www.biostat.jhsph.edu/~iruczins/teaching/misc/gwas/papers/devlin1999.pdf&quot;&gt;Genomic control for association studies&lt;/a&gt;. &lt;em&gt;Biometrics&lt;/em&gt;, 55(4): 997–1004.&lt;/li&gt;&#10;&lt;li&gt;JK Pritchard, M Stephens, and P Donnelly (2000). &lt;a href=&quot;http://pritch.bsd.uchicago.edu/publications/structure.pdf&quot;&gt;Inference of population structure using multilocus genotype data&lt;/a&gt;. &lt;em&gt;Genetics&lt;/em&gt;, 155(2): 945–959.&lt;/li&gt;&#10;&lt;li&gt;Gang Zheng, Boris Freidlin, Zhaohai Li, and Joseph L Gastwirth (2005). &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.167.8448&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;Genomic control for association studies under various genetic models&lt;/a&gt;.   &lt;em&gt;Biometrics&lt;/em&gt;, 61(1): 186–92.    &lt;/li&gt;&#10;&lt;li&gt;Chao Tian, Peter K. Gregersen, and Michael F. Seldin1 (2008). &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/18852203&quot;&gt;Accounting for ancestry: population substructure and genome-wide association studies&lt;/a&gt;. &lt;em&gt;Human Molecular Genetics&lt;/em&gt;, 17(R2): R143-R150.&lt;/li&gt;&#10;&lt;li&gt;Kai Yu, &lt;a href=&quot;http://biowulf.nih.gov/symposium/slides/Kai_Yu.pdf&quot;&gt;Population Substructure and Control Selection in Genome-wide Association Studies&lt;/a&gt;.&lt;/li&gt;&#10;&lt;li&gt;Alkes L. Price, Noah A. Zaitlen, David Reich and Nick Patterson (2010). &lt;a href=&quot;http://genetics.med.harvard.edu/reich/Reich_Lab/Publications_files/2010_Price_stratificationreview.pdf&quot;&gt;New approaches to population stratification in genome-wide association studies&lt;/a&gt;, &lt;em&gt;Nature Reviews Genetics&lt;/em&gt;&lt;/li&gt;&#10;&lt;li&gt;Chao Tian, et al. (2009). &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2730349/&quot;&gt;European Population Genetic Substructure: Further Definition of Ancestry Informative Markers for Distinguishing among Diverse European Ethnic Groups&lt;/a&gt;, Molecular Medicine, 15(11-12): 371–383.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;" CommentCount="2" CreationDate="2011-03-25T19:32:21.563" Id="8780" LastActivityDate="2011-04-04T09:12:52.413" LastEditDate="2011-04-04T09:12:52.413" LastEditorUserId="930" OwnerUserId="930" ParentId="8777" PostTypeId="2" Score="11" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I am wondering that how one can calculate &lt;a href=&quot;http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence&quot; rel=&quot;nofollow&quot;&gt;KL-divergence&lt;/a&gt; on two probability distributions. For example, if we have&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;t1 = 0.4, 0.2, 0.3, 0.05, 0.05&#10;t2 = 0.23, 0, 0.14, 0.17&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The formula is bit complicated for me :( &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-03-26T00:33:57.720" FavoriteCount="1" Id="8783" LastActivityDate="2011-04-08T20:44:20.170" LastEditDate="2011-04-08T20:44:20.170" LastEditorUserId="919" OwnerUserId="3900" PostTypeId="1" Score="2" Tags="&lt;distributions&gt;&lt;machine-learning&gt;&lt;distance-functions&gt;&lt;information-retrieval&gt;" Title="KL divergence calculation" ViewCount="617" />
  <row Body="&lt;p&gt;Using brute force and the first formula &lt;a href=&quot;http://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; based on the first formula for the &lt;a href=&quot;http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence&quot; rel=&quot;nofollow&quot;&gt;Kullback-Leibler divergence&lt;/a&gt;, you are starting from two multisets each with 5 values, 3 of which are shared between them.  So the combination of them is the multiset &#10;$$M={0, 0.05, 0.05, 0.1, 0.2, 0.2, 0.3, 0.3, 0.4, 0.4}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;so using $D_{\mathrm{KL}}(P\|Q) = \sum_i P(i) \log \frac{P(i)}{Q(i)}$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$JSD(t_1 \parallel t_2)= \frac{1}{2}D_{\mathrm{KL}}(t_1 \parallel M)+\frac{1}{2}D_{\mathrm{KL}}(t_2 \parallel M)$$&#10;$$=\frac{1}{2}\left(1\cdot\frac{2}{5} \log\left(\frac{2/5}{2/10}\right) +3\cdot\frac{1}{5} \log\left(\frac{1/5}{2/10}\right)\right)  $$&#10;$$+\frac{1}{2}\left(2\cdot\frac{1}{5} \log\left(\frac{1/5}{1/10}\right) +3\cdot\frac{1}{5} \log\left(\frac{1/5}{2/10}\right)\right)  $$&#10;$$= \dfrac{2}{5}\log(2) \approx 0.277$$&lt;/p&gt;&#10;&#10;&lt;p&gt;though you may want to check this. Other calculations, such as using Shannon entropy should produce the same result. &lt;/p&gt;&#10;" CommentCount="7" CreationDate="2011-03-26T01:09:49.170" Id="8785" LastActivityDate="2011-03-26T01:09:49.170" OwnerUserId="2958" ParentId="8783" PostTypeId="2" Score="2" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I have a problem when I run the Komogorov-Smirnov test.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have to samples of daily prices distributions estimated with density(). Now I would like to compare these two distributions with each other.&lt;/p&gt;&#10;&#10;&lt;p&gt;data.1:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Date           price&#10;01.01.2010     1.2&#10;02.01.2010     1.5&#10;etc.&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;data.2:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Date           price&#10;01.01.2009     0.1&#10;02.01.2009     0.05&#10;etc.&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;For the probability density, I calculated&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;density.1 &amp;lt;- density(data.1$price)&#10;density.2 &amp;lt;- density(data.2$price)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now I wanted to run the KS-test:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;ks &amp;lt;- ks.test(density.1$x, density.2$x)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and got the results that p=1, hence the two distributions are the same. However, it is already observable from eye that they differ quite heavily from each other.&lt;/p&gt;&#10;&#10;&lt;p&gt;Where is my mistake?&#10;Thank you, Dani&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-03-25T11:57:14.800" FavoriteCount="1" Id="8788" LastActivityDate="2011-04-08T20:44:41.933" LastEditDate="2011-04-08T20:44:41.933" LastEditorUserId="919" OwnerDisplayName="Dani" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;distributions&gt;" Title="Goodness-of-fit test using Kolmogorov-Smirnov" ViewCount="1083" />
  <row Body="&lt;p&gt;Sounds like you might want to look at &lt;a href=&quot;http://en.wikipedia.org/wiki/Moving_average&quot; rel=&quot;nofollow&quot;&gt;(Weighted) Moving Average&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-03-26T01:59:01.030" Id="8792" LastActivityDate="2011-03-26T01:59:01.030" OwnerDisplayName="Suroot" ParentId="8791" PostTypeId="2" Score="2" />
  
  <row Body="&lt;p&gt;I assume that by &quot;my independent variable is the economy&quot; you're using shorthand for some specific predictor.&lt;/p&gt;&#10;&#10;&lt;p&gt;At one level, I see nothing wrong with making a statement such as &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;X predicts Y1 with an odds ratio of _ and a 95% confidence interval of [ _ , _ ]&#10;   while &#10;  X predicts Y2 with an odds ratio of _ and a 95% confidence interval of [ _ , _ ].&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;@dmk38's recent suggestions look very helpful in this regard. &lt;/p&gt;&#10;&#10;&lt;p&gt;You might also want to standardize the coefficients to facilitate comparison.&lt;/p&gt;&#10;&#10;&lt;p&gt;At another level, beware of taking inferential statistics (standard errors, &lt;em&gt;p&lt;/em&gt;-values, CIs) literally when your sample constitutes a nonrandom sample of the population of years to which you might want to generalize.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-03-26T12:21:03.737" Id="8794" LastActivityDate="2011-04-23T15:38:48.887" LastEditDate="2011-04-23T15:38:48.887" LastEditorUserId="2669" OwnerUserId="2669" ParentId="8784" PostTypeId="2" Score="2" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I've made a little questionnaire where participants can rate an answer between 1 and 5. I calculated the mean value, the average value and the standard deviation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now I was asking myself if it is possible to calculated a confidence interval for these results and if yes, if this would tell me anything. So I just tested it and used excel to calculate a 95% confidence interval.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here are the values:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Arithmetic average: 4.60&#10;Median: 5.00&#10;Max: 5.00&#10;Min: 3.00&#10;Standard deviation: 0.63&#10;95% Confidence interval: 0.32&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;But what is this value telling me? I can be sure by 32% that the values aren't random values? Or is a confidence interval for those kinds of questions useless?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-03-26T15:37:23.837" FavoriteCount="4" Id="8797" LastActivityDate="2011-03-28T13:54:58.437" LastEditDate="2011-03-26T17:16:41.450" LastEditorUserId="88" OwnerUserId="3908" PostTypeId="1" Score="2" Tags="&lt;confidence-interval&gt;" Title="Understanding confidence interval" ViewCount="851" />
  
  <row Body="&lt;p&gt;So you have a density of:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$p(X_i|\theta)=\frac{1}{2\theta}\;\;\;\; X_i\in[-\theta,\theta]$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Now this is what is called a scale density, and $\theta$ is a scale parameter, just like the standard deviation in a normal distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now to do a Bayesian CI you require a prior distribution for $\theta$.  Because $\theta$ is a scale parameter, the prior describing complete initial ignorance is given by:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$p(\theta|\theta_L,\theta_U) = \frac{1}{\theta [log(\frac{\theta_U}{\theta_L})]} \;\;\; \theta\in[\theta_L,\theta_U]$$&lt;/p&gt;&#10;&#10;&lt;p&gt;In your comment, you state that &quot;the improper prior is to be used&quot; so this means that you take the upper limit $\theta_U\rightarrow \infty$ and the lower limit $\theta_L\rightarrow 0$.  But this is in principle, to be done at the end of the calculation, and not at the start.  This is to ensure that you don't have an &quot;infinity&quot; floating around in your results, making them arbitrary.  If the limit does not exist, then &quot;probability theory&quot; is &quot;telling you&quot; that the actual bounds are important to your conclusion.&lt;/p&gt;&#10;&#10;&lt;p&gt;I assume you want to work this out for yourself, so the remainder just goes:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;calculate the posterior (Note: I have used $s$ as a dummy variable to indicate that the denominator is independent of $\theta$). $$p(\theta|D,\theta_L,\theta_U)=\frac{p(\theta|\theta_L,\theta_U)\prod_{i=1}^{n}p(X_i|\theta)}{\int_{\theta_L}^{\theta_U}p(s|\theta_L,\theta_U)\prod_{i=1}^{n}p(X_i|s)ds}$$&lt;/li&gt;&#10;&lt;li&gt;Calculate calculate the lower bound $C_L$ and upper bound $C_U$ such that there is a $100(1-\alpha)$% probability that $C_L&amp;lt;\theta&amp;lt;C_U$.  You can do this by solving for general limits&#10;$$\int_{C_L}^{C_U}p(\theta|D,L,U)d\theta=1-\alpha$$&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;This should give you enough of the &quot;machinery&quot; to go and solve the problem.  However, if you need more details, I can post them.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-03-27T09:43:27.663" Id="8829" LastActivityDate="2011-03-27T09:43:27.663" OwnerUserId="2392" ParentId="8808" PostTypeId="2" Score="3" />
  
  
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Given the following hierarchical model,&#10;$$&#10;X \sim {\mathcal N}(\mu,1),&#10;$$&#10;and,&#10;$$&#10;\mu \sim {\rm Laplace}(0, c)&#10;$$&#10;where $\mathcal{N}(\cdot,\cdot)$ is a normal distribution. Is there a way to get an exact expression for the Fisher information of the marginal distribution of $X$ given $c$. That is, what is the Fisher information of:&#10;$$&#10;p(x | c) = \int p(x|\mu) p(\mu|c) d\mu&#10;$$&#10;I can get an expression for the marginal distribution of $X$ given $c$, but differentiating w.r.t. $c$ and then taking expectations seems very difficult. Am I missing something obvious? Any help would be appreciated.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-03-28T06:33:55.567" FavoriteCount="1" Id="8867" LastActivityDate="2013-07-20T23:49:38.383" LastEditDate="2013-07-20T23:49:38.383" LastEditorUserId="7290" OwnerUserId="530" PostTypeId="1" Score="7" Tags="&lt;multilevel-analysis&gt;&lt;information&gt;&lt;fisher-information&gt;" Title="Fisher information in a hierarchical model" ViewCount="201" />
  
  
  <row AcceptedAnswerId="17070" AnswerCount="3" Body="&lt;p&gt;How to order a set of vectors $W$ if we are given a training set $V$ consisting of $k$ $n$-dimensional vectors and partial order of them? It is not the total order, so some vectors might not be comparable with some other. The answer will depend on assumptions, so feel free to make any reasonable assumptions.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;br&gt;&#10;Let: $k=4$ and $n=2$&lt;br&gt;&#10;$v_{1}=(1,2)$&lt;br&gt;&#10;$v_{2}=(5,8)$&lt;br&gt;&#10;$v_{3}=(4,3)$&lt;br&gt;&#10;$v_{4}=(9,6)$&lt;br&gt;&#10;We know that $v_{1}&amp;lt;v_{3}$, $v_{2}&amp;lt;v_{4}$ and $v_{3}&amp;lt;v_{4}$&lt;/p&gt;&#10;&#10;&lt;p&gt;Vectors that we want to order are following:&lt;br&gt;&#10;$w_{1} = (2,6)$&lt;br&gt;&#10;$w_{2} = (7,4)$&lt;br&gt;&#10;$w_{3} = (5,5)$  &lt;/p&gt;&#10;&#10;&lt;p&gt;The most intuitive order is $w_{1}&amp;lt;w_{3}&amp;lt;w_{2}$, because it seems that the first attribute is the most important.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-03-28T20:01:44.653" Id="8885" LastActivityDate="2011-10-16T12:32:23.090" OwnerUserId="1643" PostTypeId="1" Score="4" Tags="&lt;ordered-variables&gt;" Title="Prediction of an order of vectors using partially ordered set" ViewCount="137" />
  
  
  <row Body="&lt;p&gt;The real estate prices that you are tying to predict , are they consecutive/chronological values i.e. time series data or are they prices for different classes e.g. this years prices for  different classes for the same time frame. You might want to read something I wrote on these two kinds of problems as it warns that if you are dealing with longitudinal data ( time series) then the tools of ordinary cross-sectional regression will not ordinarily apply. It is entitled &quot;Regression vs Box-Jenkins&quot; &lt;a href=&quot;http://www.autobox.com/pdfs/regvsbox.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.autobox.com/pdfs/regvsbox.pdf&lt;/a&gt; .&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2011-03-28T23:57:31.773" Id="8895" LastActivityDate="2011-03-28T23:57:31.773" OwnerUserId="3382" ParentId="8891" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;When you set $Pr(B)=1$ other things will change, though some can remain the same.  So you have to decide what is remaining the same.  &lt;/p&gt;&#10;&#10;&lt;p&gt;For example, in the first part, you could have worked out $Pr(A|B)$, $Pr( \lnot A|B)$, $Pr(A| \lnot B) $ and  $Pr( \lnot A| \lnot B)$.  So $Pr(A|B) = \frac{Pr(B|A)Pr(A)}{Pr(B)} = \frac{0.0005}{0.0104} \approx 0.0480769\ldots$ &lt;/p&gt;&#10;&#10;&lt;p&gt;If you assume $Pr(A|B)$ stays the same into the second part of the question then $Pr(B)=1$ would give $Pr(A)=\approx 0.0480769\ldots$&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-03-29T09:39:14.657" Id="8906" LastActivityDate="2011-03-29T09:39:14.657" OwnerUserId="2958" ParentId="8899" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="8913" AnswerCount="2" Body="&lt;p&gt;I'm working up a taxonomy showing different methods used in pattern recognition and I'd be curious to hear about how it could be improved. The Mind Map groups different methods based on the discipline which influenced their development.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://bentham.k2.t.u-tokyo.ac.jp/media/zoo.png&quot; alt=&quot;taxonomy&quot;&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-03-29T10:09:49.400" Id="8908" LastActivityDate="2011-10-26T09:55:35.580" LastEditDate="2011-10-26T09:55:35.580" LastEditorUserId="183" OwnerUserId="2624" PostTypeId="1" Score="2" Tags="&lt;algorithms&gt;" Title="What is missing from this taxonomy  of methods used in pattern recognition?" ViewCount="273" />
  
  
  <row Body="&lt;p&gt;Adaboost can use multiple instances of the same classifier with different parameters. Thus, a previously linear classifier can be combined into nonlinear classifiers. Or, as the AdaBoost people like to put it, multiple weak learners can make one strong learner. A nice picture can be found &lt;a href=&quot;http://doc.prsdstudio.com/2.1/kb/16.html&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;, on the bottom.&lt;/p&gt;&#10;&#10;&lt;p&gt;Basically, it goes as with any other learning algorithm: on some datasets it works, on some it doesn't. There sure are datasets out there, where it excels. And maybe you haven't chosen the right weak learner yet. Did you try logistic regression? Did you visualize how the decision boundaries evolve during adding of learners? Maybe you can tell what is going wrong.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-03-29T19:06:49.200" Id="8938" LastActivityDate="2011-03-29T19:06:49.200" OwnerUserId="2860" ParentId="8930" PostTypeId="2" Score="4" />
  
  <row Body="&lt;p&gt;There's many different kinds of matrices that can be obtained from data. Off the top of my head:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;a covariance matrix: you have several variables measured on each data point, and you want to see how they are correlated&lt;/li&gt;&#10;&lt;li&gt;a misclassification matrix: you have a categorical output or response variable, and another variable that attempts to estimate or predict the response&lt;/li&gt;&#10;&lt;li&gt;a grid of spatial measurements: adjacent rows/columns are in some sense closer to each other than to non-adjacent rows/columns&lt;/li&gt;&#10;&lt;li&gt;a contingency table of counts or cell averages&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;The methods you'd use to analyse these matrices would differ. I'd guess that your matrix is the last one judging from the ANOVA mention, but it would be good to have more information.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-03-30T01:32:16.820" Id="8949" LastActivityDate="2011-03-30T01:32:16.820" OwnerUserId="1569" ParentId="8947" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/K-means_clustering&quot; rel=&quot;nofollow&quot;&gt;K-means&lt;/a&gt; clustering for unsupervised learning.&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2011-03-30T04:20:40.470" CreationDate="2011-03-30T04:20:40.470" Id="8954" LastActivityDate="2011-03-30T04:20:40.470" OwnerUserId="3270" ParentId="258" PostTypeId="2" Score="2" />
  
  <row AcceptedAnswerId="27646" AnswerCount="1" Body="&lt;p&gt;Consider an experiment with multiple human participants, each measured multiple times in two conditions. A mixed effects model can be formulated (using &lt;a href=&quot;http://cran.r-project.org/web/packages/lme4/index.html&quot;&gt;lme4&lt;/a&gt; syntax) as:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;fit = lmer(&#10;    formula = measure ~ (1|participant) + condition&#10;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Now, say I want to generate bootstrapped confidence intervals for the predictions of this model. I think I've come up with a simple and computationally efficient method, and I'm sure I'm not the first to think of it, but I'm having trouble finding any prior publications describing this approach. Here it is:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Fit the model (as above), call this the &quot;original model&quot;&lt;/li&gt;&#10;&lt;li&gt;Obtain predictions from the original model, call these the &quot;original predictions&quot;&lt;/li&gt;&#10;&lt;li&gt;Obtain residuals from the original model associated with each response from each participant&lt;/li&gt;&#10;&lt;li&gt;Resample the residuals, sampling &lt;em&gt;participants&lt;/em&gt; with replacement&lt;/li&gt;&#10;&lt;li&gt;Fit a linear mixed effects model with gaussian error to the &lt;em&gt;residuals&lt;/em&gt;, call this the &quot;interim model&quot;&lt;/li&gt;&#10;&lt;li&gt;Compute predictions from the interim model for each condition (these predictions will be very close to zero), call these the &quot;interim predictions&quot;&lt;/li&gt;&#10;&lt;li&gt;Add the interim predictions to the original predictions, call the result the &quot;resample predictions&quot;&lt;/li&gt;&#10;&lt;li&gt;Repeat steps 4 through 7 many times, generating a distribution of resample predictions for each condition from which once can compute CIs.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I've seen &lt;a href=&quot;http://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29#Resampling_residuals&quot;&gt;&quot;residual bootstrapping&quot;&lt;/a&gt; procedures in the context of simple regression (i.e. not a mixed model) where residuals are sampled as the unit of resampling and then added to the predictions of the original model before fitting a new model on each iteration of the bootstrap, but this seems rather different from the approach I describe where residuals are never resampled, people are, and only &lt;em&gt;after&lt;/em&gt; the interim model is obtained do the original model predictions come into play. This last feature has a really nice side-benefit in that no matter the complexity of the original model, the interim model can always be fit as a gaussian linear mixed model, which can be substantially faster in some cases. For example, I recently had binomial data and 3 predictor variables, one of which I suspected would cause strongly non-linear effects, so I had to employ &lt;a href=&quot;http://cran.r-project.org/web/packages/gamm4/index.html&quot;&gt;Generalized Additive Mixed Modelling&lt;/a&gt; using a binomial link function. Fitting the original model in this case took over an hour, whereas fitting the gaussian LMM on each iteration took mere seconds.&lt;/p&gt;&#10;&#10;&lt;p&gt;I really don't want to claim priority on this if it's already a known procedure, so I'd be very grateful if anyone can provide information on where this might have been described before. (Also, if there are any glaring problems with this approach, do let me know!)&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2011-03-30T15:27:52.997" FavoriteCount="3" Id="8972" LastActivityDate="2012-05-03T15:44:56.057" OwnerUserId="364" PostTypeId="1" Score="8" Tags="&lt;mixed-model&gt;&lt;bootstrap&gt;" Title="Is there a name for this type of bootstrapping?" ViewCount="592" />
  <row Body="&lt;p&gt;I wouldn't say there's an increasing interest or debate about the use of pie charts. They are just found everywhere on the web and in so-called &quot;predictive analytic&quot; solutions. &lt;/p&gt;&#10;&#10;&lt;p&gt;I guess you know Tufte's work (he also discussed the use of &lt;a href=&quot;http://www.edwardtufte.com/bboard/q-and-a-fetch-msg?msg_id=00018S&quot;&gt;multiple pie charts&lt;/a&gt;), but more funny is the fact that the second chapter of Wilkinson's &lt;em&gt;Grammar of Graphics&lt;/em&gt; starts with &quot;How to make a pie chart?&quot;.&#10;You're probably also aware that Cleveland's &lt;a href=&quot;http://www.b-eye-network.com/view/2468&quot;&gt;dotplot&lt;/a&gt;, or even a barchart, will convey much more precise information. The problem seems to really stem from the way our visual system is able to deal with spatial information. It is even quoted in the R software; from the on-line help for &lt;code&gt;pie&lt;/code&gt;, &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Cleveland (1985), page 264: “Data that&#10;  can be shown by pie charts always can&#10;  be shown by a dot chart.  This means&#10;  that judgements of position along a&#10;  common scale can be made instead of&#10;  the less accurate angle judgements.”&#10;  This statement is based on the&#10;  empirical investigations of Cleveland&#10;  and McGill as well as investigations&#10;  by perceptual psychologists.&lt;/p&gt;&#10;  &#10;  &lt;p&gt;Cleveland, W. S. (1985) &lt;em&gt;The elements&#10;  of graphing data&lt;/em&gt;. Wadsworth:&#10;  Monterey, CA, USA.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;There are variations of pie charts (e.g., donut-like charts) that all raise the same problems: We are not good at evaluating angle and area. Even the ones used in &quot;corrgram&quot;, as described in Friendly, &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.19.1268&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;Corrgrams: Exploratory displays for correlation matrices&lt;/a&gt;, &lt;em&gt;American Statistician&lt;/em&gt; (2002) 56:316, are hard to read, IMHO.&lt;/p&gt;&#10;&#10;&lt;p&gt;At some point, however, I wondered whether they might still be useful, for example (1) displaying two classes is fine but increasing the number of categories generally worsen the reading (especially with strong imbalance between %), (2) relative judgments are better than absolute ones, that is displaying two pie charts side by side should favor a better appreciation of the results than a simple estimate from, say a pie chart mixing all results (e.g. a two-way cross-classification table). Incidentally, I asked a similar question to Hadley Wickham who kindly pointed me to the following articles:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Spence, I. (2005). &lt;a href=&quot;http://www.psych.utoronto.ca/users/spence/Spence%202005.pdf&quot;&gt;No Humble Pie: The Origins and Usage of a Statistical Chart&lt;/a&gt;. &lt;em&gt;Journal of Educational and Behavioral Statistics&lt;/em&gt;, 30(4), 353–368.&lt;/li&gt;&#10;&lt;li&gt;Heer, J. and Bostock, M. (2010). &lt;a href=&quot;http://hci.stanford.edu/publications/2010/crowd-perception/heer-chi2010.pdf&quot;&gt;Crowdsourcing Graphical Perception: Using Mechanical Turk to Assess Visualization Design&lt;/a&gt;. &lt;em&gt;CHI 2010&lt;/em&gt;, April 10–15, 2010, Atlanta, Georgia, USA.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;In sum, I think they are just good for grossly depicting the distribution of 2 to 3 classes (I use them, from time to time, to show the distribution of males and females in a sample on top of an histogram of ages), but they must be accompanied by relative frequencies or counts for being really informative. A table would still do a better job since you can add margins, and go beyond 2-way classifications.&lt;/p&gt;&#10;&#10;&lt;p&gt;Finally, there are alternative displays that are built upon the idea of pie chart. I can think of square pie or &lt;a href=&quot;http://eagereyes.org/communication/Engaging-readers-with-square-pie-waffle-charts.html&quot;&gt;waffle chart&lt;/a&gt;, described by Robert Kosara in &lt;a href=&quot;http://eagereyes.org/techniques/pie-charts&quot;&gt;Understanding Pie Charts&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-03-30T18:17:53.490" Id="8976" LastActivityDate="2011-03-30T18:47:15.337" LastEditDate="2011-03-30T18:47:15.337" LastEditorUserId="930" OwnerUserId="930" ParentId="8974" PostTypeId="2" Score="18" />
  
  <row Body="&lt;p&gt;My personal problem with pie charts is while they may be useful to show differences like this: &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/r5mw1.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;way too many people use it to show that:&#10;&lt;img src=&quot;http://i.stack.imgur.com/dD9sk.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2011-03-30T19:05:42.137" CreationDate="2011-03-30T19:05:42.137" Id="8978" LastActivityDate="2013-07-01T00:55:02.403" LastEditDate="2013-07-01T00:55:02.403" LastEditorUserId="22047" OwnerUserId="88" ParentId="8974" PostTypeId="2" Score="21" />
  <row Body="&lt;p&gt;As a follow-up to my comment, if &lt;code&gt;independence.test&lt;/code&gt; refers to &lt;code&gt;coin::independence_test&lt;/code&gt;, then you can reproduce a Cochrane and Armitage trend test, as it is used in GWAS analysis, as follows:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; library(SNPassoc)&#10;&amp;gt; library(coin)&#10;&amp;gt; data(SNPs)&#10;&amp;gt; datSNP &amp;lt;- setupSNP(SNPs,6:40,sep=&quot;&quot;)&#10;&amp;gt; ( tab &amp;lt;- xtabs(~ casco + snp10001, data=datSNP) )&#10;     snp10001&#10;casco T/T C/T C/C&#10;    0  24  21   2&#10;    1  68  32  10&#10;&amp;gt; independence_test(casco~snp10001, data=datSNP, teststat=&quot;quad&quot;,&#10;                    scores=list(snp10001=c(0,1,2)))&#10;&#10;Asymptotic General Independence Test&#10;&#10;data:  casco by snp10001 (T/T &amp;lt; C/T &amp;lt; C/C) &#10;chi-squared = 0.2846, df = 1, p-value = 0.5937&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This is a conditional version of the CATT. About scoring of the ordinal variable (here, the frequency of the minor allele denoted by the letter &lt;code&gt;C&lt;/code&gt;), you can play with the &lt;code&gt;scores=&lt;/code&gt; arguments of &lt;code&gt;independence_test()&lt;/code&gt; in order to reflect the model you want to test (the above result is for a log-additive model).&lt;/p&gt;&#10;&#10;&lt;p&gt;There are five different genetic models that are generally considered&#10;in GWAS, and they reflect how genotypes might be collapsed: codominant&#10;(T/T (92) C/T (53) C/C (12), yielding the usual $\chi^2(2)$&#10;association test), dominant (T/T (92) vs. C/T-C/C (65)), recessive&#10;(T/T-C/T (145) vs. C/C (12)), overdominant (T/T-C/C (104) vs. C/T&#10;(53)) and log-additive (0 (92) &amp;lt; 1 (53) &amp;lt; 2 (12)). Note that genotype&#10;recoding is readily available in inheritance functions from the&#10;&lt;a href=&quot;http://cran.r-project.org/web/packages/SNPassoc/index.html&quot; rel=&quot;nofollow&quot;&gt;SNPassoc&lt;/a&gt;&#10;package. The &quot;scores&quot; should reflect these collapsing schemes.&lt;/p&gt;&#10;&#10;&lt;p&gt;Following Agresti (&lt;a href=&quot;http://www.stat.ufl.edu/~aa/cda/cda.html&quot; rel=&quot;nofollow&quot;&gt;CDA&lt;/a&gt;, 2002, p. 182), CATT is computed as $n\cdot r^2$,&#10;where $r$ stands for the linear correlation between the numerical&#10;scores and the binary outcome (case/control), that is&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;z.catt &amp;lt;- sum(tab)*cor(datSNP$casco, as.numeric(datSNP$snp10001))^2&#10;1 - pchisq(z.catt, df = 1)  # p=0.5925&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;There also exist various built-in CATT functions in R/Bioconductor&#10;ecosystem for GWAS, e.g.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;&lt;code&gt;CATT()&lt;/code&gt; from&#10;&lt;a href=&quot;http://cran.r-project.org/web/packages/Rassoc/index.html&quot; rel=&quot;nofollow&quot;&gt;Rassoc&lt;/a&gt;, e.g.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;with(datSNP, CATT(table(casco, snp10001), 0.5)) # p=0.5925 &#10;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;(additive/multiplicative)&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;in &lt;a href=&quot;http://www.bioconductor.org/packages/2.3/bioc/html/snpMatrix.html&quot; rel=&quot;nofollow&quot;&gt;snpMatrix&lt;/a&gt;, there are headed as 1-df $\chi^2$-test when you call&#10;&lt;code&gt;single.snp.tests()&lt;/code&gt; (see the&#10;&lt;a href=&quot;http://www.bioconductor.org/packages/2.6/bioc//vignettes/snpMatrix/inst/doc/snpMatrix-vignette.pdf&quot; rel=&quot;nofollow&quot;&gt;vignette&lt;/a&gt;);&#10;please note that the default mode of inheritance is the codominant/additive effect.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Finally, here are two references that discuss the choice of scoring scheme&#10;depending on the genetic model under consideration, and some issues&#10;with power/robustness&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Zheng, G, Freidlin, B, Li, Z and Gastwirth, JL (2003). Choice of&#10;scores in trend tests for case-control studies of candidate-gene&#10;associations. &lt;em&gt;Biometrical Journal&lt;/em&gt;, &lt;strong&gt;45&lt;/strong&gt;: 335-348. &lt;/li&gt;&#10;&lt;li&gt;Freidlin, B, Zheng, G, Li, Z, and Gastwirth, JL (2002). &lt;a href=&quot;http://wiki.bmi.utah.edu/student/images/f/f5/Trend_Tests_for_Case-Control_Studies_of_Genetic_Markers_%28Power_Sample_Size_and_Robustness%29.pdf&quot; rel=&quot;nofollow&quot;&gt;Trend Tests&#10;for Case-Control Studies of Genetic Markers: Power, Sample Size and&#10;Robustness&lt;/a&gt;. &lt;em&gt;Human Heredity&lt;/em&gt;, &lt;strong&gt;53&lt;/strong&gt;: 146-152.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;See also the &lt;a href=&quot;http://www.bioconductor.org/packages/2.3/bioc/html/GeneticsDesign.html&quot; rel=&quot;nofollow&quot;&gt;GeneticsDesign&lt;/a&gt; (bioc) package for power calculation with&#10;linear trend tests.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-03-30T19:06:57.707" Id="8979" LastActivityDate="2011-05-04T16:55:49.133" LastEditDate="2011-05-04T16:55:49.133" LastEditorUserId="930" OwnerUserId="930" ParentId="8774" PostTypeId="2" Score="4" />
  
  
  <row AcceptedAnswerId="8991" AnswerCount="1" Body="&lt;p&gt;Following a previous &lt;a href=&quot;http://stats.stackexchange.com/questions/8899/several-questions-about-conditional-probability&quot;&gt;question&lt;/a&gt;, lets say we now have 3 variables:&lt;/p&gt;&#10;&#10;&lt;p&gt;$L$, $B$, $S$:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;     S&#10;   /   \&#10;  L     B&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;So &#10;$L$ depends on $S$&#10;$B$ depends on $S$&lt;/p&gt;&#10;&#10;&lt;p&gt;$P(S) =$ 0.5&#10;$P( \lnot S) =$ 0.5&lt;/p&gt;&#10;&#10;&lt;p&gt;$L$ that depends on $S$:&lt;/p&gt;&#10;&#10;&lt;p&gt;$P(L|S) =$ 0.10&lt;br&gt;&#10;$P( \lnot L|S) =$ 0.90&lt;br&gt;&#10;$P(L| \lnot S) =$ 0.01&lt;br&gt;&#10;$P( \lnot L| \lnot S) =$ 0.99  &lt;/p&gt;&#10;&#10;&lt;p&gt;$B$ that depends on $S$:&lt;/p&gt;&#10;&#10;&lt;p&gt;$P(B|S) =$ 0.60&lt;br&gt;&#10;$P( \lnot B|S) =$ 0.40&lt;br&gt;&#10;$P(B| \lnot S) =$ 0.30&lt;br&gt;&#10;$P( \lnot B| \lnot S) =$ 0.70  &lt;/p&gt;&#10;&#10;&lt;p&gt;I have gotten&lt;/p&gt;&#10;&#10;&lt;p&gt;$P(L) =$ 0.055    $P(\lnot L) =$ 0.945    &lt;/p&gt;&#10;&#10;&lt;p&gt;$P(B) =$ 0.45     $P(\lnot B) =$ 0.55  &lt;/p&gt;&#10;&#10;&lt;p&gt;As the previos question to get &#10;$P(S)$ after I observe that $P(L)=1$&lt;br&gt;&#10;$P(S∣L)=  P(L∣S)P(S)  / P(L) =$ (0.10)(0.5)  / (0.055)  = 0.9091&lt;br&gt;&#10;so&#10;$P(S)=$ 0.9091  &lt;/p&gt;&#10;&#10;&lt;p&gt;Similarly when we have $P(B)=$1&lt;br&gt;&#10;$P(S∣B)=  P(B∣S)P(S)  / P(B) =$ (0.40)(0.5)  / (0.45)  = 0.3636&lt;br&gt;&#10;so&#10;$P(S)=$ 0.3636    &lt;/p&gt;&#10;&#10;&lt;p&gt;But What do you do when you observe both events:&lt;/p&gt;&#10;&#10;&lt;p&gt;$P(L)=$1  and&lt;br&gt;&#10;$P(B)=$1 &lt;/p&gt;&#10;&#10;&lt;p&gt;How do you modify the above formula to get $P(S)$?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-03-30T23:49:42.127" FavoriteCount="1" Id="8987" LastActivityDate="2011-03-31T09:51:45.473" LastEditDate="2011-03-31T02:11:30.083" LastEditorUserId="3681" OwnerUserId="3681" PostTypeId="1" Score="0" Tags="&lt;conditional-probability&gt;" Title="3 variables and conditional probability" ViewCount="179" />
  
  <row AcceptedAnswerId="8998" AnswerCount="2" Body="&lt;p&gt;This is in reference to the &lt;a href=&quot;http://en.wikipedia.org/wiki/Girsanov_theorem&quot; rel=&quot;nofollow&quot;&gt;Girsanov theorem&lt;/a&gt; however question is general. If $X$ is a standard normal variable $N(0,1)$, why is expectation of $e^{-\mu X - \mu^2/2}$ equal to 1?&lt;/p&gt;&#10;&#10;&lt;p&gt;Shouldn't it be $e^{-\mu^2/2}$?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-03-31T05:23:01.543" FavoriteCount="1" Id="8997" LastActivityDate="2012-10-20T21:16:29.413" LastEditDate="2011-03-31T08:31:28.927" LastEditorUserId="449" OwnerUserId="862" PostTypeId="1" Score="3" Tags="&lt;probability&gt;&lt;expected-value&gt;" Title="Expected value of certain exponential transformation of standard normal variable " ViewCount="2798" />
  <row Body="&lt;p&gt;As a first step you could take a sample from the documents. This can be random sampling, but if you know that certain characteristics of the documents are particularly relevant you could use stratified sampling.&lt;/p&gt;&#10;&#10;&lt;p&gt;The second step can be feature extraction. Define characteristics of the documents that may help predicting the &lt;code&gt;accept&lt;/code&gt; or &lt;code&gt;reject&lt;/code&gt; labels. Give clear definitions; you can use numerical scale, ordinal and nominal (including binary) variables. Determine features and labels in the sample.&lt;/p&gt;&#10;&#10;&lt;p&gt;Third step, developing a prediction model. To predict the binary response there are many kinds of methods available. If you have some experience how some features affect acceptance you may want to build it into a model and use regression techniques e.g. logistic regression, probit regression, and model selection (e.g. stepwise variable selection). If you can't or don't want to build in pre-existing knowledge into the automatic assessment you can use machine learning techniques like random forests, logistic regression tree, support vector machines.&lt;/p&gt;&#10;&#10;&lt;p&gt;Fourth step is validating your model on a second sample. The way to do this can be randomly partitioning your original sample into a modelling and a testing subset before modelling.&lt;/p&gt;&#10;&#10;&lt;p&gt;When you know how well your automatic classification system performs you will be able to judge which documents need human review.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-03-31T09:48:42.233" Id="9000" LastActivityDate="2011-03-31T12:47:51.767" LastEditDate="2011-03-31T12:47:51.767" LastEditorUserId="3911" OwnerUserId="3911" ParentId="8036" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;The classic reference is Royston (1982)[1] which has algorithms going beyond explicit formulas. It also quotes a well-known formula by Blom (1958):&#10;$E(r:n) \approx \mu + \Phi^{-1}(\frac{r-\alpha}{n-2\alpha+1})\sigma$ with $\alpha=0.375$. This formula gives a multiplier of -2.73 for $n=200, r=1$.&lt;/p&gt;&#10;&#10;&lt;p&gt;[1]: &lt;a href=&quot;http://www.jstor.org/stable/2347982&quot;&gt;Algorithm AS 177: Expected Normal Order Statistics (Exact and Approximate)&lt;/a&gt; J. P. Royston. Journal of the Royal Statistical Society. Series C (Applied Statistics) Vol. 31, No. 2 (1982), pp. 161-165&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-03-31T12:52:27.970" Id="9007" LastActivityDate="2011-03-31T17:54:50.003" LastEditDate="2011-03-31T17:54:50.003" LastEditorUserId="449" OwnerUserId="279" ParentId="9001" PostTypeId="2" Score="18" />
&#10;{{\rm erf}\left(1/2\,{\it \_t0}\,\sqrt {2}\right)} \right) ^{-1+n}}{
  
  <row AnswerCount="1" Body="&lt;p&gt;In &lt;a href=&quot;http://en.wikipedia.org/wiki/Girsanov_theorem&quot; rel=&quot;nofollow&quot;&gt;Girsanov&lt;/a&gt; theorem, the change of probability measure variable $Z_t = \frac{dQ}{dP}|_{\mathcal{F}_t}$, why does it need to be a martingale with respect to measure $P$ for the change of measure $\frac{dQ}{dP}$ to exist?&lt;/p&gt;&#10;&#10;&lt;p&gt;I am having trouble understanding this. Anyone familiar with this?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-03-31T18:00:00.810" FavoriteCount="1" Id="9014" LastActivityDate="2011-04-12T11:45:39.793" LastEditDate="2011-04-01T08:19:03.897" LastEditorUserId="2116" OwnerUserId="862" PostTypeId="1" Score="4" Tags="&lt;probability&gt;&lt;stochastic-processes&gt;" Title="Why local martingale property is important in Girsanov theorem?" ViewCount="340" />
  
  <row Body="&lt;p&gt;After reading up about Girsanov theorem and martingale theory, I can come up with the following observations. First if we have a filtration $\mathcal{F}_t$ and two probability measures $P$ and $Q$ for which Radon-Nikodym derivative $\frac{dQ}{dP}$ exist, then  for each $\mathcal{F}_t$ there exists a Radon-Nikodym derivative $D_t$ with respect to $\mathcal{F}_t$ and $D_t$ is a uniformly integrable martingale with respect to $\mathcal{F}_t$ and $P$. &lt;/p&gt;&#10;&#10;&lt;p&gt;Now if we have measure $P$ and a martingale $Z_t$ with filtration $\mathcal{F}_t$ we can define set function $Q=Z_t\cdot P$ defined on $\cup \mathcal{F}_t$. It will define a unique probability measure $Q$ on $\sigma(\cup \mathcal{F}_t)$ if $Z_t$ has additional properties, $EZ_t\equiv 1$ being one of them.&lt;/p&gt;&#10;&#10;&lt;p&gt;Going into more details requires reposting some book on this topic, which is not feasible. I read &lt;a href=&quot;http://books.google.com/books?id=1ml95FLM5koC&amp;amp;printsec=frontcover&amp;amp;dq=revuz%20yor&amp;amp;hl=fr&amp;amp;ei=bJOVTY-GCcmbOqW8-acH&amp;amp;sa=X&amp;amp;oi=book_result&amp;amp;ct=result&amp;amp;resnum=1&amp;amp;ved=0CC0Q6AEwAA#v=onepage&amp;amp;q&amp;amp;f=false&quot; rel=&quot;nofollow&quot;&gt;this one&lt;/a&gt;. Chapter VIII is a good read for clarifying things up. Naturally other books can be found. &lt;/p&gt;&#10;&#10;&lt;p&gt;Note that this is really a comment not an answer, I suggest trying to ask at math.SE, with details what exactly you did not understand, the question in the current format can be answered in many different ways, since there is a lot of things going on.&lt;/p&gt;&#10;" CommentCount="9" CreationDate="2011-04-01T09:02:56.170" Id="9032" LastActivityDate="2011-04-12T11:45:39.793" LastEditDate="2011-04-12T11:45:39.793" LastEditorUserId="2116" OwnerUserId="2116" ParentId="9014" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;The first test should read&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; fisher.test(expData[,1], expData[,2])&#10;&#10;    Fisher's Exact Test for Count Data&#10;&#10;data:  expData[, 1] and expData[, 2] &#10;p-value = 0.4857&#10;alternative hypothesis: true odds ratio is not equal to 1 &#10;95 percent confidence interval:&#10; 0.001607888 4.722931239 &#10;sample estimates:&#10;odds ratio &#10;  0.156047 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;as per the doc: &lt;code&gt;x&lt;/code&gt; is the outcome and &lt;code&gt;y&lt;/code&gt; is the factor (or vice-versa).&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-04-01T11:39:40.613" Id="9037" LastActivityDate="2011-04-01T11:39:40.613" OwnerUserId="930" ParentId="9036" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;Graphviz can optimise the layout, see something similar &lt;a href=&quot;http://www.graphviz.org/content/siblings&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-04-01T12:10:43.893" Id="9041" LastActivityDate="2011-04-01T12:10:43.893" OwnerUserId="3911" ParentId="9040" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;My answer is more intuitive than rigorous, but maybe it will help...&lt;/p&gt;&#10;&#10;&lt;p&gt;As I understand it, overfitting is the result of model selection based on training and testing using the same data, where you have a flexible fitting mechanism: you fit your sample of data so closely that you're fitting the noise, outliers, and all the other variance.&lt;/p&gt;&#10;&#10;&lt;p&gt;Splitting the data into a training and testing set keeps you from doing this. But a static split is not using your data efficiently and your split itself could be an issue. Cross-validation keeps the don't-reward-an-exact-fit-to-training-data advantage of the training-testing split, while also using the data that you have as efficiently as possible (i.e. all of your data is used as training and testing data, just not in the same run).&lt;/p&gt;&#10;&#10;&lt;p&gt;If you have a flexible fitting mechanism, you need to constrain your model selection so that it doesn't favor &quot;perfect&quot; but complex fits somehow. You can do it with AIC, BIC, or some other penalization method that penalizes fit complexity directly, or you can do it with CV. (Or you can do it by using a fitting method that is not very flexible, which is one reason linear models are nice.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Another way of looking at it is that learning is about generalizing, and a fit that's too tight is in some sense not generalizing. By varying what you learn on and what you're tested on, you generalize better than if you only learned the answers to a specific set of questions.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-04-01T18:23:29.910" Id="9059" LastActivityDate="2011-04-01T18:23:29.910" OwnerUserId="1764" ParentId="9053" PostTypeId="2" Score="4" />
  
  <row AcceptedAnswerId="9096" AnswerCount="4" Body="&lt;p&gt;I have a 2x3 contingency table - the row variable is a factor, the column variable is an ordered factor (ordinal level). I'd like to apply either symmetrical or asymmetrical association technique. What do you recommend me to do? Which technique do you find the most appropriate?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-04-01T21:45:15.740" FavoriteCount="2" Id="9062" LastActivityDate="2011-04-02T16:46:40.420" OwnerUserId="1356" PostTypeId="1" Score="4" Tags="&lt;correlation&gt;&lt;categorical-data&gt;&lt;contingency-tables&gt;&lt;association-measure&gt;" Title="Measure of association for 2x3 contingency table" ViewCount="2033" />
  
  <row AcceptedAnswerId="9073" AnswerCount="1" Body="&lt;p&gt;If I have two normally distributed independent random variables $X$ and $Y$ with means $\mu_X$ and $\mu_Y$ and standard deviations $\sigma_X$ and $\sigma_Y$ and I discover that $X+Y=c$, then (assuming I have not made any errors) the conditional distribution of $X$ and $Y$ given $c$ are also normally distributed with means &#10;$$\mu_{X|c} = \mu_X + (c - \mu_X - \mu_Y)\frac{ \sigma_X^2}{\sigma_X^2+\sigma_Y^2}$$ $$\mu_{Y|c} = \mu_Y + (c - \mu_X - \mu_Y)\frac{ \sigma_Y^2}{\sigma_X^2+\sigma_Y^2}$$&#10;and standard deviation &#10;$$\sigma_{X|c} = \sigma_{Y|c} = \sqrt{ \frac{\sigma_X^2 \sigma_Y^2}{\sigma_X^2 + \sigma_Y^2}}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;It is no surprise that the conditional standard deviations are the same as, given $c$, if one goes up the other must come down by the same amount.  It is interesting that the conditional standard deviation does not depend on $c$.&lt;/p&gt;&#10;&#10;&lt;p&gt;What I cannot get my head round are the conditional means, where they take a share of the excess $(c - \mu_X - \mu_Y)$ proportional to the original variances, not to the original  standard deviations.   &lt;/p&gt;&#10;&#10;&lt;p&gt;For example, if they have zero means, $\mu_X=\mu_Y=0$, and standard deviations $\sigma_X =3$ and $\sigma_Y=1$ then conditioned on $c=4$ we would have $E[X|c=4]=3.6$ and $E[Y|c=4]=0.4$, i.e. in the ratio $9:1$ even though I would have intuitively thought that the ratio $3:1$ would be more natural.  &lt;strong&gt;Can anyone give an intuitive explanation for this?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;This was provoked by &lt;a href=&quot;http://math.stackexchange.com/questions/30365/posterior-distribution-after-having-partial-information-on-some-linear-combinatio&quot;&gt;a Math.SE question&lt;/a&gt; &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-04-02T01:28:12.757" FavoriteCount="5" Id="9071" LastActivityDate="2011-04-02T03:05:20.290" OwnerUserId="2958" PostTypeId="1" Score="12" Tags="&lt;normal-distribution&gt;&lt;conditional-probability&gt;" Title="Intuitive explanation of contribution to sum of two normally distributed random variables" ViewCount="873" />
  <row Body="&lt;p&gt;The question readily reduces to the case $\mu_X = \mu_Y = 0$ by looking at $X-\mu_X$ and $Y-\mu_Y$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Clearly the conditional distributions are Normal.  Thus, the mean, median, and mode of each are coincident.  The modes will occur at the coordinates of a local maximum of the bivariate PDF of $X$ and $Y$ constrained to the curve $g(x,y) = x+y = c$.  This implies the contour of the bivariate PDF at this location and the constraint curve have parallel tangents.  (This is the theory of Lagrange multipliers.)  Because the equation of any contour is of the form $f(x,y) = x^2/(2\sigma_X^2) + y^2/(2\sigma_Y^2) = \rho$ for some constant $\rho$ (that is, all contours are ellipses), their gradients must be parallel, whence there exists $\lambda$ such that&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\left(\frac{x}{\sigma_X^2}, \frac{y}{\sigma_Y^2}\right) = \nabla f(x,y) = \lambda \nabla g(x,y) = \lambda(1,1).$$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/zezHu.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;It follows immediately that the &lt;em&gt;modes&lt;/em&gt; of the conditional distributions (and therefore also the means) are determined by the ratio of the variances, not of the SDs.&lt;/p&gt;&#10;&#10;&lt;p&gt;This analysis works for correlated $X$ and $Y$ as well and it applies to any linear constraints, not just the sum.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-04-02T03:05:20.290" Id="9073" LastActivityDate="2011-04-02T03:05:20.290" OwnerUserId="919" ParentId="9071" PostTypeId="2" Score="13" />
  
  
  
  
  <row AnswerCount="2" Body="&lt;p&gt;The objective of this research was to investigate the long-term effects of irrigation with treated waste water on some chemical soil properties. &lt;/p&gt;&#10;&#10;&lt;p&gt;The investigation was carried out by comparison of soil properties in two different fields: one irrigated with the effluent from Parkan Waste water Treatment Plant over a period of six years, and the other one irrigated with water over the same period of time. Soil samples were taken from different depths of 0-15, 15-30, 30-60, 60-100 and 100-150 cm in both fields, and analyzed for various chemical properties. &lt;/p&gt;&#10;&#10;&lt;p&gt;For visual summaries, I am going to plot depths of soil and element (with line and bar plots). However, I also need to fit a linear model with one categorical and one continuous predictor. How can this be done in R?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for your answer!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-04-02T09:09:33.990" Id="9080" LastActivityDate="2011-04-02T12:45:38.977" LastEditDate="2011-04-02T10:36:18.717" LastEditorUserId="930" OwnerUserId="3996" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;regression&gt;" Title="How to do a linear model in R?" ViewCount="288" />
  
  
  
&#10;p(T_{1}=\overline{t},T_{2}=\overline{t},\dots,T_{N}=\overline{t}|t_{1},t_{2},\dots,t_{N})}$$&#10;$$=\frac{
&#10;}{
&#10;\left[\prod_{i=1}^{N}\frac{1}{t_{i}}\right] exp\left(-\sum_{i=1}^{N}\frac{\overline{t}}{t_{i}}\right)
  
  
  <row AcceptedAnswerId="9108" AnswerCount="2" Body="&lt;p&gt;I would like to plot the attached &lt;a href=&quot;http://i.stack.imgur.com/acNhK.jpg&quot; rel=&quot;nofollow&quot;&gt;income distribution dataset&lt;/a&gt; (rendered as an image) as an area chart.&#10;As you can see, personal income is divided into 26 intervals of varying width. I also have the average and mean income in the intervals.&lt;/p&gt;&#10;&#10;&lt;p&gt;To convey a truthful area graphic of this data, I wonder what my options really are?&lt;br&gt;&#10;Plotting the ordinal categorical data at hand would yield a big hump in the area chart for the 400-499 interval. But this is only because that interval is wider and the user can hence be misguided by the shape. Another issue with the categorical data is that the average of the &quot;1000+ interval&quot; is very far from 1000 (= 1644). An area graphic not taking this into account would do a bad job in showing the actual distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;How would you go about and is there any way in which I can use the average/mean to &quot;convert the categorical scale to a continuous scale&quot;?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-04-02T22:08:57.993" Id="9107" LastActivityDate="2011-04-03T12:15:46.313" LastEditDate="2011-04-03T09:10:18.443" LastEditorUserId="930" OwnerUserId="4003" PostTypeId="1" Score="4" Tags="&lt;distributions&gt;&lt;data-visualization&gt;&lt;data-transformation&gt;&lt;categorical-data&gt;" Title="Categorical or continuous scale for area chart?" ViewCount="190" />
  
  <row Body="&lt;p&gt;A histogram with a continuous scale as described by GaBorgulya is clearly the way to go. When the blocks are wider, you need to adjust the density appropriately: the block 380-399 with 42246 people should be about 1.6 times the density of the block 400-499 with 132485 people.     &lt;/p&gt;&#10;&#10;&lt;p&gt;Except for the extremes of 0 and 1000+, you can just use the blocks you have, with the densities (number of people divided by width of block) as height.  You can get even closer to the distribution by dividing each block at the medians: so for example you have 58700 in the interval &quot;600 to 799 tkr&quot; (i.e to just under 800), for a density of 293.5.  Or you could divide this at the median of 681.3 into two blocks representing 29350 each, to have an interval from 600 to 672.6 of density 404.3 and an interval from 672.6 to 800 of density 230.4.  You could go further and also take into account the means in each interval, but I don't think it is a priority.&lt;/p&gt;&#10;&#10;&lt;p&gt;The extreme of 1000+ (23143 people, median 1281.0, mean 1644.2) is slightly harder but you can use the median to give you an interval from 1000 to 1281 with density 41.2.  Now it is worth using the mean.  You could for example have the top interval from 1281 to 3014.8&#10;with density 6.7.  This is not realistic as the maximum income is likely to be higher than 3014.8 and the curve is likely to be decreasing rather than flat, but it does illustrate the issue. &lt;/p&gt;&#10;&#10;&lt;p&gt;Illustrating the extreme at 0 is even harder.  Depending on how wide you make the block, you can have a spike there as high as you like.  Here is an example from &lt;em&gt;&lt;a href=&quot;http://research.dwp.gov.uk/asd/hbai/hbai_2009/pdf_files/full_hbai10.pdf&quot; rel=&quot;nofollow&quot;&gt;Households Below Average Income&lt;/a&gt;&lt;/em&gt; where they used £10 blocks.  It has other design features, some of which you may find interesting, such as a cutoff at the top end and words describing how many were cut off.&#10;&lt;img src=&quot;http://i.stack.imgur.com/4zgVm.png&quot; alt=&quot;HBAI 2008/09 figure 2.1&quot;&gt;  &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-04-03T01:51:49.713" Id="9115" LastActivityDate="2011-04-03T01:51:49.713" OwnerUserId="2958" ParentId="9107" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;I can recommend Tableau as a good tool for data exploration and visualization, simply because of the different ways that you can explore and view the data, simply by dragging and dropping.  The graphs are fairly sharp and you can easily output to PDF for presentation purposes. If you want you can extend it with some &quot;programming&quot;. I regularly use this tool along with &quot;R&quot; and SAS and they all work together well. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-04-03T03:07:13.240" Id="9117" LastActivityDate="2011-04-03T03:07:13.240" OwnerUserId="3489" ParentId="9085" PostTypeId="2" Score="6" />
  
  <row AnswerCount="1" Body="&lt;p&gt;Where can I obtain all hourly weather data available? &lt;/p&gt;&#10;&#10;&lt;p&gt;Notes: &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;Ideally, this would include the history of all data currently published at: &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://weather.noaa.gov/pub/SL.us008001/DF.an/DC.sflnd/DS.synop/&quot; rel=&quot;nofollow&quot;&gt;http://weather.noaa.gov/pub/SL.us008001/DF.an/DC.sflnd/DS.synop/&lt;/a&gt; &lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://weather.noaa.gov/pub/SL.us008001/DF.an/DC.sflnd/DS.metar/&quot; rel=&quot;nofollow&quot;&gt;http://weather.noaa.gov/pub/SL.us008001/DF.an/DC.sflnd/DS.metar/&lt;/a&gt; &lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://weather.noaa.gov/pub/SL.us008001/DF.an/DC.sfmar/DS.dbuoy/&quot; rel=&quot;nofollow&quot;&gt;http://weather.noaa.gov/pub/SL.us008001/DF.an/DC.sfmar/DS.dbuoy/&lt;/a&gt; &lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://weather.noaa.gov/pub/SL.us008001/DF.an/DC.sfmar/DS.ships/&quot; rel=&quot;nofollow&quot;&gt;http://weather.noaa.gov/pub/SL.us008001/DF.an/DC.sfmar/DS.ships/&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;I've obtained 10 years of METAR data from wunderground.com like this: &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;curl -H 'Cookie: Prefs=|SHOWMETAR:1|;' -o data.txt 'http://www.wunderground.com/history/airport/KABQ/2005/05/02/DailyHistory.html?req_city=NA&amp;amp;req_state=NA&amp;amp;req_statename=NA&amp;amp;theprefset=SHOWMETAR&amp;amp;theprefvalue=1&amp;amp;format=1' &#10;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;but am sure there's a better way. &lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;I'd love per-minute/etc data too, though I don't think anyone kept &#10;measurements better than hourly back then. &lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2011-04-03T14:27:58.373" FavoriteCount="2" Id="9127" LastActivityDate="2011-04-03T16:19:31.297" LastEditDate="2011-04-03T16:19:31.297" LastEditorUserId="930" OwnerUserId="1566" PostTypeId="1" Score="2" Tags="&lt;time-series&gt;&lt;dataset&gt;" Title="How to fetch all historically available hourly weather data?" ViewCount="1363" />
  <row Body="&lt;p&gt;Are you by chance after the different types of prediction intervals? The &lt;code&gt;predict.lm&lt;/code&gt; manual page has&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt; ## S3 method for class 'lm'&#10; predict(object, newdata, se.fit = FALSE, scale = NULL, df = Inf, &#10;         interval = c(&quot;none&quot;, &quot;confidence&quot;, &quot;prediction&quot;),&#10;         level = 0.95, type = c(&quot;response&quot;, &quot;terms&quot;),&#10;         terms = NULL, na.action = na.pass,&#10;         pred.var = res.var/weights, weights = 1, ...)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Setting&#10;  ‘intervals’ specifies computation of confidence or prediction&#10;  (tolerance) intervals at the specified ‘level’, sometimes referred&#10;  to as narrow vs. wide intervals.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Is that what you had in mind?&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-04-03T21:04:33.583" Id="9134" LastActivityDate="2011-04-03T21:04:33.583" OwnerUserId="334" ParentId="9131" PostTypeId="2" Score="7" />
  
  
  <row AcceptedAnswerId="9158" AnswerCount="4" Body="&lt;p&gt;I have heard of survival analysis and life data analysis, but don't quite get the big picture.&lt;/p&gt;&#10;&#10;&lt;p&gt;I was wondering what topics they are covering? &lt;/p&gt;&#10;&#10;&lt;p&gt;Is it pure statistics, or just application of statistics on some specific area?&lt;/p&gt;&#10;&#10;&lt;p&gt;Is life date analysis part of survival analysis? &lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks and regards!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-04-04T01:09:10.603" FavoriteCount="3" Id="9140" LastActivityDate="2015-01-15T18:37:59.160" LastEditDate="2011-04-04T05:55:45.793" LastEditorUserId="1005" OwnerUserId="1005" PostTypeId="1" Score="6" Tags="&lt;survival&gt;&lt;mathematical-statistics&gt;" Title="Big picture on survival analysis and life data analysis" ViewCount="353" />
  <row Body="&lt;p&gt;@Tal: Might I suggest &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/007310874X&quot; rel=&quot;nofollow&quot;&gt;Kutner et al&lt;/a&gt; as a fabulous source for linear models. &lt;/p&gt;&#10;&#10;&lt;p&gt;There is the distinction between 1) a prediction of Y from an individual new observation X_vec, 2) the expected value of a Y conditioned on X_vec, $E(Y|X_{vec})$ and 3) Y from several instances of x_vec  - all covered in detail in the text. &lt;/p&gt;&#10;&#10;&lt;p&gt;I think you are looking for the formula for the confidence interval around $E(Y|X_{vec})$ and that is $\hat{Y}$ $\pm$ t(1-$\alpha$ /2)s{$\hat{Y}$} where t has n-2 d.f. and s{$\hat{Y}$} is the standard error of $\hat{Y}$, $\frac{\sigma^{2}}{n}$+($X_{vec}-\bar{X})^{2}\frac{\sigma^{2}}{\sum(X_{i}-\bar{X})^{2}}$&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-04-04T01:15:10.187" Id="9142" LastActivityDate="2011-04-04T01:15:10.187" OwnerUserId="2040" ParentId="9131" PostTypeId="2" Score="4" />
&#10;\sigma^2 \left[1 + \mathbf{X}^* (\mathbf{X}&amp;#39;\mathbf{X})^{-1} (\mathbf{X}^*)&amp;#39;\right].
  <row Body="&lt;p&gt;The log probability of a GMM is non-convex, which makes it converge only locally. Also, EM scales with the number of points in your dataset - you might want to try online EM if you have a big dataset.&lt;/p&gt;&#10;&#10;&lt;p&gt;Compared to fitting univariate models (like a single Gaussian) performance is of course horrible, since it's an iterative procedure. To speed it up, a good heuristic is to start of with a couple of iterations of K-Means, use the centers as means and go on from there with the GMM modeling.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have had the feeling that GMMs converge globally for very simple and low dimensional datasets. I have never checked this for big datasets though.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-04-04T06:35:27.023" Id="9152" LastActivityDate="2011-04-04T06:35:27.023" OwnerUserId="2860" ParentId="9109" PostTypeId="2" Score="0" />
  <row AcceptedAnswerId="9196" AnswerCount="1" Body="&lt;p&gt;I would like to clarify how &lt;a href=&quot;http://en.wikipedia.org/wiki/Granger_causality&quot;&gt;the Granger causality&lt;/a&gt; can/should be used in practice, and how to interpret the statistical significance given by the test.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, I would like to fill this table with things like &quot;we don't know&quot; or if we know something, what do we know (It will for sure not be causality, but maybe something else?).&lt;/p&gt;&#10;&#10;&lt;p&gt;$\begin{matrix}
  <row Body="&lt;p&gt;If you have tuned the model parameters using cross-validation, then you won't get an unbiased estimnate of performance without using some completely new data.  Even if you re-cross-validate using a different partition of the data, or make a random test/training split using the data you have used already, this will still bias the performance evaluation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note the &quot;cross-validated measures&quot; you already have are a (possibly heavily) biased performance estimate if you have directly optimised it to choose the (hyper-) prarameters.&lt;/p&gt;&#10;&#10;&lt;p&gt;The thing to do would be to used a nested cross-validation, where the outer cross-validation is used for performance estimation, where the model parameters are tuned indpendently in each fold via an &quot;inner&quot; cross-validation.  &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-04-04T11:28:31.587" Id="9157" LastActivityDate="2011-04-04T11:28:31.587" OwnerUserId="887" ParentId="9156" PostTypeId="2" Score="4" />
  
  
  <row Body="&lt;p&gt;I'm not sure if somebody has already made this point, but pelwei can actually be forced to work as a 2 parameter weibull function by adding in a fixed bound.&lt;/p&gt;&#10;&#10;&lt;p&gt;Insead of calling &lt;code&gt;moments&amp;lt;-pelwei(wind.moments)&lt;/code&gt; you should simply call &lt;code&gt;moments&amp;lt;-pelwei(wind.moments,bound=0)&lt;/code&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;you can always check what the zeta value is. If it's not 0 and you're using dweibull, you need to do something about it.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-04-04T15:12:40.560" Id="9162" LastActivityDate="2011-04-04T15:12:40.560" OwnerUserId="4025" ParentId="7146" PostTypeId="2" Score="1" />
  <row AnswerCount="2" Body="&lt;p&gt;I have a survival cancer clinical trials dataset from which I have generated Cox models using forward likelihood ratio testing within R. These models are based on 'traditional' cancer variables (eg. age, histology, metastasis etc). &lt;/p&gt;&#10;&#10;&lt;p&gt;I would like to extend the model using high dimensional data (where we have measured many thousands of genes - FWIW, this is DNA methylation data, which can range from zero to one, rather than gene expression). Several approaches have been suggested for investigating survival using high-dimensional data, but I am not aware of any approaches that fit my requirements, i.e. adding high-dimensional data to a base multi-variate model constructed using previously identified survival correlates. &lt;/p&gt;&#10;&#10;&lt;p&gt;As a first step, I am testing for bi-modality and reducing dimensionality by selecting the most bi-modal probes for further analysis. These probes would be the most amenable to testing and verification in the lab. &lt;/p&gt;&#10;&#10;&lt;p&gt;One approach would simply be to carry on with the forward LR testing, although this would leave me very prone to overfitting. &lt;/p&gt;&#10;&#10;&lt;p&gt;Another (more sensible, in my opinion) approach would be to aggregate collections of genes into (survival-related) metagenes and then trim the metagenes into a handful of testable genes, so that this could be a usable test clinically, although this may also be prone to overfitting.&lt;/p&gt;&#10;&#10;&lt;p&gt;The cancer I work on is rare and test/training cohorts are tricky. To put things into perspective, the clinical trials dataset is 135 cases, with a further 55 age-matched non-clinical-trials cases, which show no difference in survival to the clinical trials dataset.&lt;/p&gt;&#10;&#10;&lt;p&gt;So my question is, what sort of approaches should I be considering and is what I have done so far sensible?&lt;/p&gt;&#10;&#10;&lt;p&gt;Any advice from this rather rambling question is most appreciated.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for reading!&lt;/p&gt;&#10;&#10;&lt;p&gt;Ed &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-04-04T15:29:56.450" FavoriteCount="2" Id="9165" LastActivityDate="2011-04-05T01:18:24.983" OwnerUserId="3429" PostTypeId="1" Score="5" Tags="&lt;survival&gt;&lt;microarray&gt;&lt;large-data&gt;" Title="Adding high-dimensional data to mutivariate Cox model" ViewCount="175" />
  <row Body="&lt;p&gt;Say that you want to measure a certain polluant in drinking water, the golden standard will be the method which detects it with the highest sensitivity and accuracy. Any other method can then be compared to it, knowing that -under certain conditions- the golden standard is the best (e.g.: if you need to measure the polluant on site, the golden standard will not be any method that require huge machinery to be used, as you will not be able to bring it with you).&lt;/p&gt;&#10;&#10;&lt;p&gt;I think the &lt;a href=&quot;http://en.wikipedia.org/wiki/Gold_standard_%28test%29&quot;&gt;Wikipedia article&lt;/a&gt; explains it quite well:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;In medicine and statistics, gold standard test refers to a diagnostic, test or benchmark that is &lt;strong&gt;the best available under reasonable conditions&lt;/strong&gt;. It does not have to be necessarily the best possible test for the condition in absolute terms. For example, in medicine, dealing with conditions that require an autopsy to have a perfect diagnosis, the gold standard test is normally less accurate than the autopsy. Anyway, it is obviously preferred by the patients.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="3" CreationDate="2011-04-04T15:34:19.880" Id="9166" LastActivityDate="2011-04-04T15:34:19.880" OwnerUserId="582" ParentId="9159" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;I have observed the term &quot;gold standard&quot; in quotes more times than not, so I take it to mean something that is highly subjective. Even in the Wikipedia article some paragraphs refer to it in quotes. The OP is also referring to a &quot;gold standard dataset&quot; which I take it to mean a &quot;Gold Standard&quot; for descriminant Analysis, as in Fishers' Iris dataset being the &quot;Gold Standard&quot;.  But I am not 100% sure usage is consistant.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-04-04T15:47:16.553" Id="9167" LastActivityDate="2011-04-04T15:57:54.190" LastEditDate="2011-04-04T15:57:54.190" LastEditorUserId="3489" OwnerUserId="3489" ParentId="9159" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Looking at individual p-values can be misleading. If you have variables that are collinear (have high correlation), you will get big p-values. This does not mean the variables are useless.&lt;/p&gt;&#10;&#10;&lt;p&gt;As a quick rule of thumb, selecting your model with the AIC criteria is better than looking at p-values.&lt;/p&gt;&#10;&#10;&lt;p&gt;One reason one might not select the model with the lowest AIC is when your variable to datapoint ratio is large.&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that model selection and prediction accuracy are somewhat distinct problems. If your goal is to get accurate predictions, I'd suggest cross-validating your model by separating your data in a training and testing set.&lt;/p&gt;&#10;&#10;&lt;p&gt;A paper on variable selection: &lt;a href=&quot;http://arxiv.org/PS_cache/arxiv/pdf/1003/1003.5930v2.pdf&quot; rel=&quot;nofollow&quot;&gt;Stochastic Stepwise Ensembles for Variable Selection&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-04-04T18:59:19.727" Id="9174" LastActivityDate="2013-09-21T22:15:49.933" LastEditDate="2013-09-21T22:15:49.933" LastEditorUserId="17230" OwnerUserId="3834" ParentId="9171" PostTypeId="2" Score="14" />
  <row Body="&lt;p&gt;As the comments suggest, it is only by fully understanding and specifying your research design that you will establish what regression method best corresponds to your data.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the case where your DV is a categorical variable, which seems likely if you are dealing with social data, I would recommend reading extensively from &lt;a href=&quot;http://www.stata.com/bookstore/regmodcdvs.html&quot; rel=&quot;nofollow&quot;&gt;Long and Freese&lt;/a&gt; to make an informed choice. Long and Freese use Stata, but equivalent commands exist in both R and SPSS.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-04-05T00:20:39.873" Id="9183" LastActivityDate="2011-04-05T00:20:39.873" OwnerUserId="3582" ParentId="8881" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;AIC is a goodness of fit measure that favours smaller residual error in the model, but penalises for including further predictors and helps avoiding overfitting. In your second set of models model 1 (the one with the lowest AIC) may perform best when used for prediction outside your dataset. A possible explanation why adding Var4 to model 2 results in a lower AIC, but higher p values is that Var4 is somewhat correlated with Var1, 2 and 3. The interpretation of model 2 is thus easier.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-04-05T01:20:55.760" Id="9185" LastActivityDate="2011-04-05T01:20:55.760" OwnerUserId="3911" ParentId="9171" PostTypeId="2" Score="8" />
  
  <row Body="&lt;p&gt;I would say that the validity of the test has less to do with the number of data points and more to do with the validity of the assumption that you have the correct model.  For example, the regression analysis that is used to generate a standard curve may be based on only 3 standards (low, med, and high) but the result is highly valid since there is strong evidence that the response is linear between the points.  On the other hand, even a regression with 1000s of data points will be flawed if the wrong model is applied to the data.  In the first case any variation between the model predictions and the actual data is due to random error.  In the second case some of the variation between the model predictions and the actual data is due to bias from choosing the wrong model. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-04-05T23:14:11.423" Id="9234" LastActivityDate="2011-04-05T23:14:11.423" OwnerUserId="4048" ParentId="9233" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;The small number of data points limits what kinds of models you may fit on your data. However it does not necessarily mean that it would make no sense to start modelling. With few data you will only be able to detect associations if the effects are strong and the scatter is weak.&lt;/p&gt;&#10;&#10;&lt;p&gt;It's an other question what kind of model suits your data. You used the word 'regression' in the title. The model should to some extent reflect what you know about the phenomenon. This seems to be an ecological setting, so the previous year may be influential as well. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-04-05T23:25:14.203" Id="9236" LastActivityDate="2011-04-05T23:25:14.203" OwnerUserId="3911" ParentId="9233" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;In medical research trials may be unethical if they recruit too many patients. For example if the goal is to decide which treatment is better it's not ethical any more to treat patients with the worse treatment after it was established to be inferior. Increasing the sample size would, of course, give you a more accurate estimate of the effect size, but you may have to stop well before the effects of factors like &quot;slight biases in the sampling process&quot; appear.&lt;/p&gt;&#10;&#10;&lt;p&gt;It may also be unethical to spend public money of sufficiently confirmed research.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-04-06T00:22:28.443" Id="9239" LastActivityDate="2011-04-06T03:30:28.593" LastEditDate="2011-04-06T03:30:28.593" LastEditorUserId="183" OwnerUserId="3911" ParentId="9225" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;Doesn't it just mean that your treatment was almost completely ineffective?&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-04-06T03:04:46.387" Id="9245" LastActivityDate="2011-04-06T03:04:46.387" OwnerUserId="1679" ParentId="9242" PostTypeId="2" Score="1" />
  
  <row AcceptedAnswerId="9256" AnswerCount="2" Body="&lt;p&gt;For the $\nu$-SVM (for both classification and regression cases) the $\nu \in (0;1)$ should be selected.&#10;The LIBSVM guide suggests to use grid search for identifying the optimal value of the $C$ parameter for $C$-SVM, it also recommends to try following values $C = 2^{-5}, 2^{-3}, \dots, 2^{15}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;So the question is, are there any recommendations for values of the $\nu$ parameter in case of $\nu$-SVMs?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-04-06T07:34:56.343" FavoriteCount="2" Id="9253" LastActivityDate="2012-12-21T19:45:38.843" OwnerUserId="4051" PostTypeId="1" Score="3" Tags="&lt;machine-learning&gt;&lt;svm&gt;" Title="$\nu$-svm parameter selection" ViewCount="976" />
  <row Body="&lt;p&gt;The lm() procedure handles the entire range of linear models, not just multiple regression. All you have to do is make sure your predictors are set up to&#10;be of the right type. &lt;/p&gt;&#10;&#10;&lt;p&gt;Binary is the special case of nominal where the number of levels is two. &lt;/p&gt;&#10;&#10;&lt;p&gt;Nomimal variables must be set to mode factor. They can be coerced to factors from character variables by using factor(). Note that linear models use one of the levels as a baseline, so it effectively disappears. By default this will be the first in your list of levels. If you don't specify the order of the levels they will be put in alphabetic order. You can change the order using relevel().&lt;/p&gt;&#10;&#10;&lt;p&gt;For ordinal data you need them to be ordered factors. Use ordered() to coerce characters or factors to ordered factors. &lt;/p&gt;&#10;&#10;&lt;p&gt;For continuous predictors you want the predictor to be a double. Use double() to enforce this. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-04-06T09:30:46.060" Id="9255" LastActivityDate="2011-04-06T09:30:46.060" OwnerDisplayName="user4052" ParentId="8881" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Rather than use a grid search, you can just optimise the hyper-parameters using standard numeric optimisation techniques (e.g. gradient descent).  If you don't have estimates of the gradients, you can use the &lt;a href=&quot;http://en.wikipedia.org/wiki/Nelder-Mead_simplex_method&quot; rel=&quot;nofollow&quot;&gt;Nelder-Mead simplex method&lt;/a&gt;, which doesn't require gradient information and is vastly more efficient than grid-search methods.  I would use the &lt;a href=&quot;http://en.wikipedia.org/wiki/Logit&quot; rel=&quot;nofollow&quot;&gt;logit&lt;/a&gt; function to map the (0;1) range of $\nu$ onto $(-\infty;+\infty)$ to get an unconstrained optimisation problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;If you really want to use grid search, then just spacing the evaluation points linearly in the range 0 - 1 should be fine.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-04-06T10:06:32.467" Id="9256" LastActivityDate="2011-04-06T10:06:32.467" OwnerUserId="887" ParentId="9253" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;Your question in the title is an interesting question that crossed my mind today too. I just want to add a correction. The F-ratio is :&#10;$$\frac{MS_{treatment}}{MS_{residual}}=\frac{\frac{SS_{treatment}}{t-1}}{\frac{SS_{residual}}{t(r-1)}}$$&#10;What you wrote is the &#10;$$\frac{E(MS_{treatment})}{E(MS_{residual})}$$&#10;While the first fraction &lt;strong&gt;can&lt;/strong&gt; be less than 1, the second fraction cannot be less than 1. But that's not a problem since it's a quotient of expectations. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-04-06T10:54:17.303" Id="9257" LastActivityDate="2011-04-06T10:54:17.303" OwnerUserId="3454" ParentId="9242" PostTypeId="2" Score="6" />
  
  
  
  
  <row Body="&lt;p&gt;In &lt;a href=&quot;http://en.wikipedia.org/wiki/Statistical_hypothesis_testing&quot; rel=&quot;nofollow&quot;&gt;statistical hypothesis testing&lt;/a&gt;, the &lt;em&gt;size&lt;/em&gt; is the largest chance of rejecting the null when the null is true (a &quot;false positive&quot; error).  The &lt;em&gt;power&lt;/em&gt; is the chance of not rejecting the null; it depends on the &quot;effect size&quot; (a measure of how far reality actually departs from the null).  &lt;em&gt;Caeteris paribus,&lt;/em&gt; power and size are inversely related (one must increase if the other is decreased), so considerations often focus on size, which is simpler to analyze.&lt;/p&gt;&#10;&#10;&lt;p&gt;When more than one hypothesis test is performed to make a binary decision, the chance of a false positive is usually greater than the size of any of the tests used for that decision.  For example, suppose groups of &quot;control&quot; and &quot;treatment&quot; subjects are randomly selected from the same population and each subject is given a questionnaire comprising 20 yes-no questions.  Let the groups be compared separately for each question using a test of size .05.  If the comparisons are independent, then the chance of at least one of them rejecting the null equals $1 - (1 - 0.05)^{20}$ = 0.64.  Thus a nominal false positive rate of 0.05 in each test is inflated to a decision false positive rate of 0.64.&lt;/p&gt;&#10;&#10;&lt;p&gt;To avoid unacceptably large chances of reaching mistaken conclusions in such &quot;multiple comparisons&quot; cases, either an overall test of significance is initially conducted or the sizes of the &lt;em&gt;individual&lt;/em&gt; tests leading up to the decision are decreased (that is, the tests are made more stringent).  Examples of the former are the &lt;a href=&quot;http://en.wikipedia.org/wiki/Analysis_of_variance#Follow_up_tests&quot; rel=&quot;nofollow&quot;&gt;F-test in an ANOVA&lt;/a&gt; setting and &lt;a href=&quot;http://en.wikipedia.org/wiki/Tukey%27s_range_test&quot; rel=&quot;nofollow&quot;&gt;Tukey's HSD test&lt;/a&gt;.  Exemplary of the latter approach is the &lt;a href=&quot;http://en.wikipedia.org/wiki/Bonferroni_correction&quot; rel=&quot;nofollow&quot;&gt;Bonferroni correction&lt;/a&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-04-06T21:41:34.313" Id="9285" LastActivityDate="2011-04-06T21:41:34.313" LastEditDate="2011-04-06T21:41:34.313" LastEditorUserId="919" OwnerUserId="919" PostTypeId="5" Score="0" />
  <row Body="&lt;p&gt;There are at least two possibilities for this data. One possibility is that your microarrays contain no disease markers whatsoever. But, they do contain information about age, and since in your case the sick and control populations are of different age, you get the illusion of good classification performance. Another possibility is that the microarrays do contain disease markers, and, moreover, these markers is exactly what SVM focuses on. &lt;/p&gt;&#10;&#10;&lt;p&gt;It seems like the principal components of the data may be correlated with age in both of these possibilities. In the first case it will be because age is what the data expresses. In the second case it will be because disease is what the data expresses, and this disease is itself correlated with age (for your dataset). I don't think there is an easy way to look at the correlation value and conclude which case it is.&lt;/p&gt;&#10;&#10;&lt;p&gt;I could think of several ways to assess the effect differently. One option is to split your training set into groups of equal age. In this case, for 'young' ages the normal class will have more training examples than the disease class, and vice versa for the older ages. But as long as there are enough examples, this should not be a problem. Another option is to do the same with the test sets, i.e. see whether the classifier tends to say 'sick' more often for older patients. Both of these options could be difficult since you don't have that many examples.&lt;/p&gt;&#10;&#10;&lt;p&gt;One more option is to train two classifiers. In the first, the only feature will be the age. It seems this has AUC of 0.82. In the second, there will be age and the microarray data. (It seems that currently you train a different classifier which only uses the microarray data, and it gives you AUC 0.95. Adding the age feature explicitly is likely to improve performance, so AUC will be even higher.) If the second classifier performs better than the first, this indicates that age is not the only thing of interest in this data. Based on your comment, the improvement in AUC is 0.13 or more, which seems fair. &lt;/p&gt;&#10;" CommentCount="10" CreationDate="2011-04-06T22:15:08.573" Id="9287" LastActivityDate="2011-04-06T22:23:28.577" LastEditDate="2011-04-06T22:23:28.577" LastEditorUserId="3369" OwnerUserId="3369" ParentId="9228" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;I will concur with what has been mentioned that Bray-Curtis can handle abundance as well as presence/absence, also to add another good book to the mix: Analysis of Ecological Communities by McCune and Grace.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are a lot of factors to consider as you compare ecological communities and I don't think there is a single test that will do the job.  The appropriateness of the test will depend a lot on the type of question you are asking about the communities and the nature of your dataset.  Common approaches include ordination techniques, which array sites within a multidimensional taxon-space.  However if you truly only have 2 sites then this is not likely to work.  Mantel tests correlate a the distance matrix based on composition (e.g., the pairwise Bray-Curtis distance across all sites) with a distance matrix based on other potential factors of influence.  The simplest case can be just the euclidean distance between the sites in space.  Cluster analysis groups sites with respect to their community composition.  &lt;/p&gt;&#10;&#10;&lt;p&gt;In general I would take the approach of using a subset of the many statistical tools described in any of the above books to provide a statistical description of the differences between the communities in question.  There is no single measure of the difference in community composition so the stats are used to synthesize multidimensional data into a more easily interpretable form. &lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT: I also just thought of this paper which lays out many of the different options pretty clearly and thoroughly.&lt;/p&gt;&#10;&#10;&lt;p&gt;Anderson, M. J. et al. 2011. Navigating the multiple meanings of Beta diversity: a roadmap for the practicing ecologist.  Ecology Letters 14:19-28&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-04-07T00:17:04.250" Id="9293" LastActivityDate="2011-04-07T11:42:05.100" LastEditDate="2011-04-07T11:42:05.100" LastEditorUserId="4048" OwnerUserId="4048" ParentId="9281" PostTypeId="2" Score="3" />
  <row AcceptedAnswerId="9320" AnswerCount="1" Body="&lt;p&gt;This is a more &quot;statistical&quot; version of my &lt;a href=&quot;http://math.stackexchange.com/questions/31267/mean-chord-length-in-a-convex-polygon&quot;&gt; Math.SE problem &lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;b&gt; Question &lt;/b&gt;  Let $\mathbf{P}$ be a convex polygon of $m$ sides. Pick two of its edges at random, and further pick a random point on each of these edges, and compute the length of the chord we get. If we repeat this process $n$ times,and average the chord lengths we thus obtained, we get an estimate $\hat{\ell}_n$ of the expected chord length $\ell$ of $\mathbf{P}$.  Then,how large should $n$ be to ensure that $\hat{\ell}_n$ differs from $\ell$ by (say) at most 5%,with a confidence level of (say) 95%, and precisely how  should we collect our samples?&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that determining the expected chord length analytically leads to intractable integrals as shown in my SE post. Also,note that there are multiple inequivalent ways to define the expected chord length of $\mathbf{P}$; the one we need here is $\ell = \lim_{n\rightarrow \infty} \hat{\ell}_n$.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-04-07T01:50:31.040" Id="9295" LastActivityDate="2011-04-07T18:41:45.193" LastEditDate="2011-04-07T02:31:00.333" LastEditorUserId="4011" OwnerUserId="4011" PostTypeId="1" Score="4" Tags="&lt;monte-carlo&gt;" Title="Monte-Carlo estimation of the mean chord length in a polygon" ViewCount="199" />
  <row AcceptedAnswerId="9303" AnswerCount="1" Body="&lt;p&gt;a journal article has a method for designing experiments to be fit to a 4-parameter logistic model.  The model used is $y= D + \frac{A - D}{1 + (\frac{x}{C}) ^ B}$&lt;br&gt;&#10;A = upper asymptote&lt;br&gt;&#10;B = maximum slope&lt;br&gt;&#10;C = x value when y = 50% of maximum (i.e. 1/2 of upper asymptote)&lt;br&gt;&#10;D = lower asymptote&lt;br&gt;&#10;Using pilot data, further experiments are optimally designed by plugging the preliminary parameter estimates into equations presented in the article.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, the nonlinear modeling software that I have access to parameterizes the 4-parameter logistic model differently.  The model used is &#10;$y = D + \frac{A - D}{1 + e^{B(x-C)}}$&lt;/p&gt;&#10;&#10;&lt;p&gt;Once I estimate the parameters from the software, how do I translate these to the exponential parameterization used by the software?  Thank you.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-04-07T02:42:00.870" Id="9298" LastActivityDate="2011-04-07T07:02:17.783" LastEditDate="2011-04-07T07:02:17.783" LastEditorUserId="88" OwnerUserId="2473" PostTypeId="1" Score="4" Tags="&lt;nonlinear-regression&gt;" Title="Reconciling different parameterizations of the same nonlinear model" ViewCount="84" />
  
  <row AnswerCount="3" Body="&lt;p&gt;I'm given an $n\times n$ grid of positive integer values. These numbers represent an intensity that should correspond to the strength of belief of a person occupying that grid location (a higher value indicating a higher belief). A person will in general have an influence over multiple grid cells. &lt;/p&gt;&#10;&#10;&lt;p&gt;I believe that the pattern of intensities should &quot;look Gaussian&quot; in that there will be a central location of high intensity, and then the intensities taper off radially in all directions. Specifically, I'd like to model the values as coming from a &quot;scaled Gaussian&quot; with a parameter for the variance and another for the scale factor.&lt;/p&gt;&#10;&#10;&lt;p&gt;There are two complicating factors:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;the absence of a person will not correspond to a zero value, because of background noise and other effects, but the values should be smaller. They can be erratic though, and at a first approximation might be difficult to model as simple Gaussian noise. &lt;/li&gt;&#10;&lt;li&gt;The intensity range can vary. For one instance, the values might range between 1 and 10, and in another, between 1 and 100. &lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I'm looking for an appropriate parameter estimation strategy, or pointers to relevant literature. Pointers to why I'm approaching this problem the wrong way altogether would also be appreciated :). I've been reading about kriging, and Gaussian processes, but that seems like very heavy machinery for my problem. &lt;/p&gt;&#10;" CommentCount="8" CreationDate="2011-04-07T03:02:36.983" FavoriteCount="2" Id="9299" LastActivityDate="2011-05-11T03:57:17.720" OwnerUserId="139" PostTypeId="1" Score="11" Tags="&lt;estimation&gt;&lt;normal-distribution&gt;&lt;spatial&gt;" Title="Estimating parameters for a spatial process" ViewCount="405" />
  <row AcceptedAnswerId="9316" AnswerCount="1" Body="&lt;p&gt;A colleague just asked me this question:&lt;/p&gt;&#10;&#10;&lt;h3&gt;Context:&lt;/h3&gt;&#10;&#10;&lt;p&gt;A psychological study had&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;2 &lt;code&gt;group&lt;/code&gt;s  of participants (between subjects)&lt;/li&gt;&#10;&lt;li&gt;4 &lt;code&gt;context&lt;/code&gt;s (within subjects))&lt;/li&gt;&#10;&lt;li&gt;each participant provided a &lt;code&gt;response&lt;/code&gt; in each of the four contexts&lt;/li&gt;&#10;&lt;li&gt;there were three categorically distinct response options (lets call them $A$, $B$, and $C$)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;h3&gt;Question&lt;/h3&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;What is an appropriate statistical model for analysing the effect of &lt;code&gt;group&lt;/code&gt; and &lt;code&gt;context&lt;/code&gt; on &lt;code&gt;response&lt;/code&gt;?&lt;/li&gt;&#10;&lt;li&gt;What would you tell someone who analysed this data as a 2 by 4 by 3 log-linear model? (e.g., problems with assuming independence of observations)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2011-04-07T06:20:27.157" FavoriteCount="2" Id="9304" LastActivityDate="2011-04-07T15:20:29.337" OwnerUserId="183" PostTypeId="1" Score="5" Tags="&lt;modeling&gt;" Title="Modelling the effect of a 2 by 4 mixed design on a three-level nominal dependent variable" ViewCount="53" />
  <row AcceptedAnswerId="9479" AnswerCount="1" Body="&lt;p&gt;I have used LDA on a corpus of documents and found some topics. The output of my code is two matrices containing probabilities; one doc-topic probabilities and the other word-topic probabilities. But I actually don't know how to use these results to predict the topic of a new document. I am using Gibbs sampling. Does anyone know how? thanks&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-04-07T14:42:24.517" FavoriteCount="8" Id="9315" LastActivityDate="2012-07-15T20:57:39.640" LastEditDate="2012-07-15T20:57:39.640" LastEditorUserId="8413" OwnerUserId="2986" PostTypeId="1" Score="6" Tags="&lt;text-mining&gt;&lt;topic-models&gt;" Title="Topic prediction using latent Dirichlet allocation" ViewCount="2009" />
  <row AcceptedAnswerId="15472" AnswerCount="1" Body="&lt;p&gt;From &lt;em&gt;Econometrics&lt;/em&gt;, by Fumio Hyashi (Chpt 1):&lt;/p&gt;&#10;&#10;&lt;p&gt;Unconditional Homoskedasticity:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;The second moment of the error terms E(εᵢ²) is constant across the observations&lt;/li&gt;&#10;&lt;li&gt;The functional form E(εᵢ²|xi) is constant across the observations&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Conditional Homoskedasticity:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;The restriction that the second moment of the error terms E(εᵢ²) is constant across the observations is lifted&#10;&lt;ul&gt;&#10;&lt;li&gt;Thus the conditional second moment E(εᵢ²|xi) can differ across the observations through possible dependence on xᵢ.&lt;/li&gt;&#10;&lt;/ul&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;So then, my question:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;How does Conditional Homoskedasticity differ from Heteroskedasticity?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;My understanding is that there is heteroskedasticity when the second moment differs across observations (xᵢ). &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-04-07T18:43:13.910" FavoriteCount="1" Id="9322" LastActivityDate="2011-09-12T15:40:23.600" LastEditDate="2011-04-07T18:47:01.283" LastEditorUserId="88" OwnerUserId="4072" PostTypeId="1" Score="6" Tags="&lt;regression&gt;&lt;econometrics&gt;&lt;assumptions&gt;" Title="Conditional homoskedasticity vs heteroskedasticity" ViewCount="1780" />
  <row Body="&lt;p&gt;How many sites did you have?  The models only differ by one df, so either you only have two sites or you treated site as a continuous variable when it should have been categorical.  If it should have been a factor, use &lt;code&gt;factor(SITE)&lt;/code&gt; instead of &lt;code&gt;SITE&lt;/code&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, try plotting the data (always a good idea!) -- do you see any visual differences?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-04-08T02:21:51.427" Id="9332" LastActivityDate="2011-04-08T02:21:51.427" OwnerUserId="3601" ParentId="9324" PostTypeId="2" Score="1" />
  <row Body="&lt;ul&gt;&#10;&lt;li&gt;You might want to check out the free software called &lt;a href=&quot;http://ccsl.mae.cornell.edu/eureqa&quot;&gt;Eureqa&lt;/a&gt;. It has the specific aim of automating the process of finding both the functional form and the parameters of a given functional relationship.&lt;/li&gt;&#10;&lt;li&gt;If you are comparing models, with different numbers of parameters, you will generally want to use a measure of fit that penalises models with more parameters. There is a rich literature on which fit measure is most appropriate for model comparison, and issues get more complicated when the models are not nested. I'd be interested to hear what others think is the most suitable model comparison index given your scenario (as a side point, there was recently a &lt;a href=&quot;http://jeromyanglim.blogspot.com/2011/02/r-optimisation-tips-using-optim-and.html&quot;&gt;discussion on my blog&lt;/a&gt; about model comparison indices in the context of comparing models for curve fitting).&lt;/li&gt;&#10;&lt;li&gt;From my experience, non-linear regression models are used for reasons beyond pure statistical fit to the given data: &#10;&lt;ol&gt;&#10;&lt;li&gt;Non-linear models make more plausible predictions outside the range of the data&lt;/li&gt;&#10;&lt;li&gt;Non-linear models require fewer parameters for equivalent fit&lt;/li&gt;&#10;&lt;li&gt;Non-linear regression models are often applied in domains where there is substantial prior research and theory guiding model selection.&lt;/li&gt;&#10;&lt;/ol&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2011-04-08T03:41:09.710" Id="9337" LastActivityDate="2011-04-08T03:41:09.710" OwnerUserId="183" ParentId="9334" PostTypeId="2" Score="8" />
  
  
  
  <row Body="&lt;p&gt;Firstly, you need to set &lt;code&gt;REML=F&lt;/code&gt;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;lmerfit1&amp;lt;-lmer(size ~ (1|FEMALE), data=Data, REML=F)&#10;lmerfit2&amp;lt;-lmer(size ~ SITE+(1|FEMALE), data=Data, REML=F)&#10;anova(lmerfit1, lmerfit2)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;This will use MLE instead of REML, which is necessary because likelihoods from mixed models with different fixed effects are not comparable when REML is used.&lt;/p&gt;&#10;&#10;&lt;p&gt;Secondly, you could do the following quick checks:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;summary(lmerfit2) # To see the size of the SITE coefficient&#10;summary(lm(size ~ SITE, data=Data)) # To check the fixed effects estimates&#10;plot(size ~ SITE, data=Data) # Box plot&#10;dotplot(size ~ SITE, data=Data) # Another visual check&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;But given the non-significance of &lt;code&gt;SITE&lt;/code&gt; in your reported test, and the lack of visual difference you reported in your comment, I'm guessing there is no significant main effect of &lt;code&gt;SITE&lt;/code&gt;.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-04-08T14:43:53.077" Id="9356" LastActivityDate="2011-04-08T14:43:53.077" OwnerUserId="3432" ParentId="9324" PostTypeId="2" Score="3" />
  
  
  <row Body="&lt;p&gt;First, I think you're mistaken about what the three partitions do. You don't make any choices based on the test data. Your algorithms adjust their parameters based on the training data. You then run them on the validation data to compare your algorithms (and their trained parameters) and decide on a winner. You then run the winner on your test data to give you a forecast of how well it will do in the real world.&lt;/p&gt;&#10;&#10;&lt;p&gt;You don't validate on the training data because that would overfit your models. You don't stop at the validation step's winner's score because you've iteratively been adjusting things to get a winner in the validation step, and so you need an independent test (that you haven't specifically been adjusting towards) to give you an idea of how well you'll do outside of the current arena.&lt;/p&gt;&#10;&#10;&lt;p&gt;Second, I would think that one limiting factor here is how much data you have. Most of the time, we don't even want to split the data into fixed partitions at all, hence CV.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-04-08T17:58:07.700" Id="9364" LastActivityDate="2011-04-08T17:58:07.700" OwnerUserId="1764" ParentId="9357" PostTypeId="2" Score="24" />
  <row Body="&lt;p&gt;It's a bit difficult for me to see what paper might be of interest to you, so let me try and suggest the following ones, from the psychometric literature:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Borsboom, D. (2006). &lt;a href=&quot;http://sites.google.com/site/borsboomdenny/BorsboomPM2006.pdf?attredirects=0&quot; rel=&quot;nofollow&quot;&gt;The attack of&#10;  the psychometricians&lt;/a&gt;.&#10;  &lt;em&gt;Psychometrika&lt;/em&gt;, &lt;em&gt;71&lt;/em&gt;, 425-440.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;for dressing the scene (Why do we need to use statistical models that better reflect the underlying hypotheses commonly found in psychological research?), and &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Borsboom, D. (2008). &lt;a href=&quot;http://sites.google.com/site/borsboomdenny/PsychometricPerspectivesandDiagnosti.pdf?attredirects=0&quot; rel=&quot;nofollow&quot;&gt;Psychometric&#10;  perspectives on diagnostic&#10;  systems&lt;/a&gt;. &lt;em&gt;Journal of Clinical&#10;  Psychology&lt;/em&gt;, &lt;em&gt;64&lt;/em&gt;, 1089-1108.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;for an applied perspective on diagnostic medicine (transition from yes/no assessment as used in the DSM-IV to the &quot;dimensional&quot; approach intended for the DSM-V). A larger review of latent variable models in biomedical research that I like is:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Rabe-Hesketh, S. and Skrondal, A.&#10;  (2008). &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/17855748&quot; rel=&quot;nofollow&quot;&gt;Classical latent variable&#10;  models for medical research&lt;/a&gt;.&#10;  &lt;em&gt;Statistical Methods in Medical Research&lt;/em&gt;, &lt;em&gt;17(1)&lt;/em&gt;, 5-32.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="2" CommunityOwnedDate="2011-04-08T21:02:38.377" CreationDate="2011-04-08T21:02:38.377" Id="9369" LastActivityDate="2011-04-08T21:02:38.377" OwnerUserId="930" ParentId="9365" PostTypeId="2" Score="8" />
  <row Body="&lt;p&gt;First idea is just to mimic the knock-out strategy from variable importance and just test how mixing each attribute will degenerate the forest confidence in object classification (on OOB and with some repetitions obviously). This requires some coding, but is certainly achievable. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, I feel it is just a bad idea -- the result will be probably variable like hell (without stabilizing impact of averaging over objects), noisy (for not-so-confident objects the nonsense attributes could have big impacts) and hard to interpret (two or more attribute cooperative rules will probably result in random impacts of each contributing attributes). &lt;/p&gt;&#10;&#10;&lt;p&gt;Not to leave you with negative answer, I would rather try to look at the proximity matrix and the possible archetypes it may reveal -- this seems much more stable and straightforward.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-04-08T23:21:36.193" Id="9371" LastActivityDate="2011-04-08T23:21:36.193" OwnerUserId="88" ParentId="9329" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Basically you would need to make a &lt;a href=&quot;http://www.google.com/search?q=probability+tree&quot; rel=&quot;nofollow&quot;&gt;probability tree&lt;/a&gt; with resulting penalty sums in leafs and sum the whole thing up for a criterion of your choice, possibly using some software for more milestones than few (there will be $2^N$ leafs for $N$ milestones). You will be able to build the distribution too.&lt;/p&gt;&#10;&#10;&lt;p&gt;For a really huge number of milestones, you'd need a Monte Carlo simulation, so basically just simulate the process huge number of times (using random number generator to decide whether to fire or not certain milestone) and then obtain a histogram of outputs as a distribution approximation.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-04-09T00:26:56.037" Id="9374" LastActivityDate="2011-04-09T00:26:56.037" OwnerUserId="88" ParentId="9372" PostTypeId="2" Score="2" />
&#10;\rd X(t) = \kappa (m - X(t)) \rd t + \sigma \rd W(t)
&#10;\end{aligned}
&#10;$$&#10;was obtained, it is via the (remarkable and beautiful!) &lt;a href=&quot;http://en.wikipedia.org/wiki/Ito_isometry&quot; rel=&quot;nofollow&quot; rel=&quot;nofollow&quot;&gt;Ito isometry&lt;/a&gt; and the fact that an Ito integral is a zero-mean martingale; namely, in this instance,&#10;$$
  
  <row Body="&lt;p&gt;Concerning (1). You need to keep positive and negative observations if you want meaningful results.&lt;br&gt;&#10;(2) There is no wiser method of subsampling than uniform distribution if you don't have any a priori on your data. &lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-04-09T23:47:33.450" Id="9399" LastActivityDate="2011-04-10T00:28:15.327" OwnerDisplayName="Ugo" OwnerUserId="3896" ParentId="9398" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;Think &lt;em&gt;hard&lt;/em&gt; about the underlying data generating process. If the model you want to use doesn't reflect the DGP, you need to find a new model.&lt;/p&gt;&#10;" CommentCount="0" CommunityOwnedDate="2011-04-10T14:26:46.080" CreationDate="2011-04-10T14:26:46.080" Id="9413" LastActivityDate="2011-04-10T14:26:46.080" OwnerUserId="3265" ParentId="2715" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Mutual_information&quot; rel=&quot;nofollow&quot;&gt;Mutual information&lt;/a&gt; has properties somewhat analogous to covariance. Covariance is a number which is 0 for independent variables and nonzero for variables which are linearly dependent. In particular, if two variables are the same, then the covariance is equal to variance (which is usually a positive number). One issue with covariance is that it may be zero even if two variables are not independent, provided the dependence is nonlinear. &lt;/p&gt;&#10;&#10;&lt;p&gt;Mutual information (MI) is a non-negative number. It is zero if and only if the two variables are statistically independent. This property is more general than that of covariance and covers any dependencies, including nonlinear ones. &lt;/p&gt;&#10;&#10;&lt;p&gt;If the two variable are the same, MI is equal to the variable's entropy (again, usually a positive number). If the variables are different and not deterministically related, then MI is smaller than the entropy. In this sense, MI of two variables goes between 0 and H (the entropy), with 0 only if independent and H only if deterministically dependent.&lt;/p&gt;&#10;&#10;&lt;p&gt;One difference from covariance is that the &quot;sign&quot; of dependency is ignored. E.g. $Cov(X, -X) = -Cov(X, X) = -Var(X)$, but $MI(X, -X) = MI(X, X) = H(X)$.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-04-10T16:30:36.543" Id="9417" LastActivityDate="2011-04-11T18:37:15.167" LastEditDate="2011-04-11T18:37:15.167" LastEditorUserId="3369" OwnerUserId="3369" ParentId="9415" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;By &quot;circular&quot; I understand that the distribution is concentrated on a circular region, as in this contour plot of a pdf.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/qGDE6.png&quot; alt=&quot;Contour plot of a circular distribution&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;If such a structure exists, even partially, a natural way to identify and measure it is to &lt;strong&gt;average the distribution circularly around its center&lt;/strong&gt;.  (Intuitively, this means that for each possible radius $r$ we should spread the probability of being at distance $r$ from the center equally around in all directions.)  Denoting the variables as $(X,Y)$, the center must be located at the point of first moments $(\mu_X, \mu_Y)$.  To do the averaging it is convenient to define the radial distribution function&lt;/p&gt;&#10;&#10;&lt;p&gt;$$F(\rho) = \Pr[(X-\mu_X)^2 + (Y-\mu_Y)^2 \le \rho^2], \rho \ge 0;$$&#10;$$F(\rho) = 0, \rho \lt 0.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;This captures the total probability of lying between distance $0$ and $\rho$ of the center.  To spread it out in all directions, let $R$ be a random variable with cdf $F$ and $\Theta$ be a uniform random variable on $[0, 2\pi]$ independent of $R$.  The bivariate random variable $(\Xi, H) = (R\cos(\Theta) + \mu_X, R\sin(\Theta)+\mu_Y)$ is the &lt;em&gt;circular average&lt;/em&gt; of $(X,Y)$.  (This does the job our intuition demands of a &quot;circular average&quot; because (a) it has the correct radial distribution, namely $F$, by construction, and (b) all directions from the center ($\Theta$) are equally probable.)&lt;/p&gt;&#10;&#10;&lt;p&gt;At this point you have many choices: all that remains is to compare the distribution of $(X,Y)$ to that of $(\Xi, H)$.  Possibilities include an &lt;a href=&quot;http://en.wikipedia.org/wiki/Convergence_of_random_variables&quot; rel=&quot;nofollow&quot;&gt;$L^p$ distance&lt;/a&gt; and the &lt;a href=&quot;http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence&quot; rel=&quot;nofollow&quot;&gt;Kullback-Leibler divergence&lt;/a&gt; (along with myriad related distance measures: symmetrized divergence, Hellinger distance, mutual information, &lt;em&gt;etc.&lt;/em&gt;).  The comparison suggests $(X,Y)$ may have a circular structure when it is &quot;close&quot; to $(\Xi, H)$.  In this case the structure can be &quot;extracted&quot; from properties of $F$.  For instance, a measure of central location of $F$, such as its mean or median, identifies the &quot;radius&quot; of the distribution of $(X,Y)$, and the standard deviation (or other measure of scale) of $F$ expresses how &quot;spread out&quot; $(X,Y)$ are in the radial directions about their central location $(\mu_X, \mu_Y)$.&lt;/p&gt;&#10;&#10;&lt;p&gt;When &lt;strong&gt;sampling&lt;/strong&gt; from a distribution, with data $(x_i,y_i), 1 \le i \le n$, a reasonable test of circularity is to estimate the central location as usual (with means or medians) and thence convert each value $(x_i,y_i)$ into polar coordinates $(r_i, \theta_i)$ relative to that estimated center.  Compare the standard deviation (or IQR) of the radii to their mean (or median).  For non-circular distributions the ratio will be large; for circular distributions it should be relatively small.  (If you have a specific model in mind for the underlying distribution, you can work out the sampling distribution of the radial statistic and construct a significance test with it.)  Separately, test the angular coordinate for uniformity in the interval $[0, 2\pi)$.  It will be approximately uniform for circular distributions (and for some other distributions, too); non-uniformity indicates a departure from circularity.  &lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-04-10T16:51:07.647" Id="9418" LastActivityDate="2011-04-11T14:20:58.970" LastEditDate="2011-04-11T14:20:58.970" LastEditorUserId="919" OwnerUserId="919" ParentId="9415" PostTypeId="2" Score="6" />
  <row AnswerCount="2" Body="&lt;p&gt;Contingency tables tables are typically formatted as tables similar to matrices in mathematics, see &lt;a href=&quot;http://en.wikipedia.org/wiki/Contingency_table#Example&quot; rel=&quot;nofollow&quot;&gt;this example&lt;/a&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;Is the equation below an accepted notation of expressing the probabilities of the outcomes as a matrix? If not, &lt;strong&gt;what would be the accepted way?&lt;/strong&gt; Are there any published materials using the same notation?&#10;$$
&#10;\text{RH} &amp;amp; \text{LH}
&#10;\end{matrix}
&#10;\end{bmatrix}
  
  <row AcceptedAnswerId="9528" AnswerCount="2" Body="&lt;p&gt;I'm running a binary logit regression where I know the dependent variable is miscoded in a small percentage of cases.  So I'm trying to estimate $\beta$ in this model:&lt;/p&gt;&#10;&#10;&lt;p&gt;$prob(y_i) = 1/(1 + e^{-z_i})$&lt;/p&gt;&#10;&#10;&lt;p&gt;$z_i = \alpha + X_i\beta$&lt;/p&gt;&#10;&#10;&lt;p&gt;But instead of the vector $Y$, I have $\tilde{Y}$, which includes some random errors (i.e. $y_i = 1$, but $\tilde{y_i} = 0$, or vice versa, for some $i$).&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there a (reasonably) simple correction for this problem?&lt;/p&gt;&#10;&#10;&lt;p&gt;I know that logit has some nice properties in case-control studies. It seems likely that something similar applies here, but I haven't been able to find a good solution.&lt;/p&gt;&#10;&#10;&lt;p&gt;A few other constraints: this is a text-mining application, so the dimensions of $X$ are large (in the thousands or tens of thousands). This may rule out some computationally intensive procedures.&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, I don't care about correctly estimating $\alpha$, only $\beta$.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-04-11T14:03:13.367" FavoriteCount="1" Id="9431" LastActivityDate="2011-04-13T18:13:24.987" LastEditDate="2011-04-11T14:38:25.397" LastEditorUserId="3911" OwnerUserId="4110" PostTypeId="1" Score="7" Tags="&lt;logistic&gt;&lt;measurement-error&gt;" Title="How can I correct for measurement error in the dependent variable in a logit regression?" ViewCount="582" />
  
  
  
  <row Body="&lt;p&gt;Yes, there is a difference. &lt;/p&gt;&#10;&#10;&lt;p&gt;A classic time series decomposition model is&#10;Y = T * S * C * I&lt;/p&gt;&#10;&#10;&lt;p&gt;Y = data&#10;T = trend&#10;S = seasonal = REGULAR patterns that occur with time, e.g. oatmeal sales higher in winter, or Starbucks coffee sales being highest at 7 a.m. These are usually very predictable.&#10;C = cyclical = longer term patterns like business cycles. These aren't as regular as seasonality, and may involve some subjectivity in estimation.&#10;I = irregular (i.e. error left over)&lt;/p&gt;&#10;&#10;&lt;p&gt;Periodicity refers to seasonal component. Periodicity could be monthly, biweekly, hourly, etc.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-04-12T04:41:37.783" Id="9459" LastActivityDate="2011-04-12T04:41:37.783" OwnerUserId="3919" ParentId="9457" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="9466" AnswerCount="2" Body="&lt;p&gt;I'm trying to fully understand the confidence interval formal given on &lt;a href=&quot;http://www.iro.umontreal.ca/~lisa/twiki/bin/view.cgi/Public/DeepVsShallowComparisonICML2007#Results&quot; rel=&quot;nofollow&quot;&gt;this site&lt;/a&gt;:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\hat{\mu}\pm z_{1-\alpha/2}\sqrt{\frac{\hat{\mu}(1-\hat{\mu})}{n}}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;so I can reproduce the same type of intervals for my own data.  But I don't quite understand what the parameters such as $\alpha$ and $Z$ mean.  I'm guessing they're related to defining a 95% confidence interval if your data were distributed normally.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can explain to me how that formula works or a reference I can find a description of the formula used?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-04-12T06:54:42.497" FavoriteCount="1" Id="9464" LastActivityDate="2012-01-10T15:00:45.907" LastEditDate="2012-01-10T15:00:45.907" LastEditorUserId="919" OwnerUserId="4127" PostTypeId="1" Score="3" Tags="&lt;confidence-interval&gt;&lt;binomial&gt;" Title="Can someone give me details about a particular confidence interval formula?" ViewCount="293" />
  
  
&#10;$$&#10;is the probability density function of a lognormal distribution with parameters $\mu$ and $\sigma$. Below, I'll abbreviate the notation to $g(x)$ and use $G(x)$ for the cumulative distribution function.&lt;/p&gt;&#10;&#10;&lt;p&gt;I need to calculate the integral&#10;$$
  
  <row AcceptedAnswerId="9513" AnswerCount="5" Body="&lt;p&gt;If I wanted to get the probability of 9 successes in 16 trials with each trial having a probability of 0.6 I could use a binomial distribution. What could I use if each of the 16 trials has a different probability of success?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-04-13T13:34:06.930" FavoriteCount="6" Id="9510" LastActivityDate="2014-10-08T14:34:02.737" LastEditDate="2011-04-14T07:16:48.973" LastEditorUserId="88" OwnerUserId="4150" PostTypeId="1" Score="22" Tags="&lt;distributions&gt;&lt;probability&gt;&lt;binomial&gt;" Title="Probability distribution for different probabilities" ViewCount="4156" />
  <row Body="&lt;p&gt;I think you can set up your base model, that is the one with your 12 IVs and then use &lt;code&gt;add1()&lt;/code&gt; with the remaining predictors. So, say you have a model &lt;code&gt;mod1&lt;/code&gt; defined like &lt;code&gt;mod1 &amp;lt;- lm(y ~ 0+x1+x2+x3)&lt;/code&gt; (&lt;code&gt;0+&lt;/code&gt; means &lt;em&gt;no intercept&lt;/em&gt;), then&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;add1(mod1, ~ .+x4+x5+x6, test=&quot;F&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;will add and test one predictor after the other on top of the base model.&lt;/p&gt;&#10;&#10;&lt;p&gt;More generally, if you know in advance that a set of variables should be included in the model (this might result from prior knowledge, or whatsoever), you can use &lt;code&gt;step()&lt;/code&gt; or &lt;code&gt;stepAIC()&lt;/code&gt; (in the &lt;code&gt;MASS&lt;/code&gt; package) and look at the &lt;code&gt;scope=&lt;/code&gt; argument.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is an illustration, where we specify a priori the functional relationship between the outcome, $y$, and the predictors, $x_1, x_2, \dots, x_{10}$. We want the model to include the first three predictors, but let the selection of other predictors be done by stepwise regression:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;set.seed(101)&#10;X &amp;lt;- replicate(10, rnorm(100))&#10;colnames(X) &amp;lt;- paste(&quot;x&quot;, 1:10, sep=&quot;&quot;)&#10;y &amp;lt;- 1.1*X[,1] + 0.8*X[,2] - 0.7*X[,5] + 1.4*X[,6] + rnorm(100)&#10;df &amp;lt;- data.frame(y=y, X)&#10;&#10;# say this is one of the base model we think of&#10;fm0 &amp;lt;- lm(y ~ 0+x1+x2+x3+x4, data=df)&#10;&#10;# build a semi-constrained stepwise regression&#10;fm.step &amp;lt;- step(fm0, scope=list(upper = ~ 0+x1+x2+x3+x4+x5+x6+x7+x8+x9+x10, &#10;                                lower = ~ 0+x1+x2+x3), trace=FALSE)&#10;summary(fm.step)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The results are shown below:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Coefficients:&#10;   Estimate Std. Error t value Pr(&amp;gt;|t|)    &#10;x1   1.0831     0.1095   9.888 2.87e-16 ***&#10;x2   0.6704     0.1026   6.533 3.17e-09 ***&#10;x3  -0.1844     0.1183  -1.558    0.123    &#10;x6   1.6024     0.1142  14.035  &amp;lt; 2e-16 ***&#10;x5  -0.6528     0.1029  -6.342 7.63e-09 ***&#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 &#10;&#10;Residual standard error: 1.004 on 95 degrees of freedom&#10;Multiple R-squared: 0.814,  Adjusted R-squared: 0.8042 &#10;F-statistic: 83.17 on 5 and 95 DF,  p-value: &amp;lt; 2.2e-16 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;You can see that $x_3$ has been retained in the model, even if it proves to be non-significant (well, the usual caveats with univariate tests in multiple regression setting and model selection apply here -- at least, its relationship with $y$ was not specified).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-04-13T13:43:37.057" Id="9511" LastActivityDate="2011-04-13T13:43:37.057" OwnerUserId="930" ParentId="9503" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;You can use &lt;a href=&quot;http://en.wikipedia.org/wiki/MinHash&quot; rel=&quot;nofollow&quot;&gt;MinHashing&lt;/a&gt; to get a fast approximate jacard similiarity match for your current item set against a database of existing of item sets. &lt;/p&gt;&#10;&#10;&lt;p&gt;You might use a few min hashes to find quickly find candidate recommendations, and only do the full jacard computation against only the candidates found via min hashing.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-04-14T05:53:49.233" Id="9543" LastActivityDate="2011-04-14T05:53:49.233" OwnerUserId="4164" ParentId="9505" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;One alternative to @whuber's normal approximation is to use &quot;mixing&quot; probabilities, or a hierarchical model.  This would apply when the $p_i$ are similar in some way, and you can model this by a probability distribution $p_i\sim Dist(\theta)$ with a density function of $g(p|\theta)$ indexed by some parameter $\theta$.  you get a integral equation:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$Pr(s=9|n=16,\theta)={16 \choose 9}\int_{0}^{1} p^{9}(1-p)^{7}g(p|\theta)dp $$&lt;/p&gt;&#10;&#10;&lt;p&gt;The binomial probability comes from setting $g(p|\theta)=\delta(p-\theta)$, the normal approximation comes from (I think) setting $g(p|\theta)=g(p|\mu,\sigma)=\frac{1}{\sigma}\phi\left(\frac{p-\mu}{\sigma}\right)$ (with $\mu$ and $\sigma$ as defined in @whuber's answer) and then noting the &quot;tails&quot; of this PDF fall off sharply around the peak.&lt;/p&gt;&#10;&#10;&lt;p&gt;You could also use a beta distribution, which would lead to a simple analytic form, and which need not suffer from the &quot;small p&quot; problem that the normal approximation does - as beta is quite flexible.  Using a $beta(\alpha,\beta)$ distribution with $\alpha,\beta$ set by the solutions to the following equations (this is the &quot;mimimum KL divergence&quot; estimates):&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\psi(\alpha)-\psi(\alpha+\beta)=\frac{1}{n}\sum_{i=1}^{n}log[p_{i}]$$&#10;$$\psi(\beta)-\psi(\alpha+\beta)=\frac{1}{n}\sum_{i=1}^{n}log[1-p_{i}]$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Where $\psi(.)$ is the digamma function - closely related to harmonic series.&lt;/p&gt;&#10;&#10;&lt;p&gt;We get the &quot;beta-binomial&quot; compound distribution:&lt;/p&gt;&#10;&#10;&lt;p&gt;$${16 \choose 9}\frac{1}{B(\alpha,\beta)}\int_{0}^{1} p^{9+\alpha-1}(1-p)^{7+\beta-1}dp ={16 \choose 9}\frac{B(\alpha+9,\beta+7)}{B(\alpha,\beta)}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;This distribution converges towards a normal distribution in the case that @whuber points out - but should give reasonable answers for small $n$ and skewed $p_i$ - but not for multimodal $p_i$, as beta distribution only has one peak.  But you can easily fix this, by simply using $M$ beta distributions for the $M$ modes.  You break up the integral from $0&amp;lt;p&amp;lt;1$ into $M$ pieces so that each piece has a unique mode (and enough data to estimate parameters), and fit a beta distribution within each piece. then add up the results, noting that making the change of variables $p=\frac{x-L}{U-L}$ for $L&amp;lt;x&amp;lt;U$ the beta integral transforms to:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$B(\alpha,\beta)=\int_{L}^{U}\frac{(x-L)^{\alpha-1}(U-x)^{\beta-1}}{(U-L)^{\alpha+\beta-1}}dx$$&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-04-14T06:48:46.820" Id="9545" LastActivityDate="2013-12-01T03:18:23.493" LastEditDate="2013-12-01T03:18:23.493" LastEditorUserId="2392" OwnerUserId="2392" ParentId="9510" PostTypeId="2" Score="8" />
  <row Body="&lt;p&gt;I'd probably call this a &lt;a href=&quot;http://en.wikipedia.org/wiki/Crossover_design&quot; rel=&quot;nofollow&quot;&gt;cross-over design&lt;/a&gt; with three treatments and two periods, although you could also think of it as a &lt;a href=&quot;http://en.wikipedia.org/wiki/Incomplete_block_design&quot; rel=&quot;nofollow&quot;&gt;balanced incomplete block design&lt;/a&gt; with three treatments and 'participant' as the blocking factor with a block size of 2.&lt;/p&gt;&#10;&#10;&lt;p&gt;Not my area of expertise so I can't accept or reject all your points but I'm sure you can analyse it with ANOVA. One point that comes to mind is, as for any designed experiment, don't forget to randomize, and &lt;a href=&quot;http://en.wikipedia.org/wiki/Allocation_concealment#Allocation_concealment&quot; rel=&quot;nofollow&quot;&gt;conceal allocation&lt;/a&gt;! &lt;/p&gt;&#10;&#10;&lt;p&gt;Two standard textbooks on crossover trials are:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Jones, Byron; Kenward, Michael G. (2003). &lt;a href=&quot;http://books.google.com/books?id=Cc3oTtld0F4C&quot; rel=&quot;nofollow&quot;&gt;&lt;em&gt;Design and Analysis of Cross-Over Trials (2nd ed.).&lt;/em&gt;&lt;/a&gt; Chapman and Hall. &lt;/li&gt;&#10;&lt;li&gt;Senn, Stephen (2002). &lt;a href=&quot;http://books.google.com/books?id=Mx9FCKnRnVQC&quot; rel=&quot;nofollow&quot;&gt;&lt;em&gt;Cross-over trials in clinical research (2nd ed.).&lt;/em&gt;&lt;/a&gt; Wiley.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="1" CreationDate="2011-04-14T06:50:16.403" Id="9546" LastActivityDate="2011-04-14T06:50:16.403" OwnerUserId="449" ParentId="9519" PostTypeId="2" Score="2" />
  <row AnswerCount="2" Body="&lt;p&gt;Assume you have a class of approximately 800 students and following a set of assessments each student has a raw grade.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;How should these raw grades be converted into a final grade?&lt;/li&gt;&#10;&lt;li&gt;Is it a good idea to scale the raw grades to a normal distribution?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="9" CreationDate="2011-04-14T09:43:45.847" FavoriteCount="3" Id="9549" LastActivityDate="2014-08-12T09:16:02.363" LastEditDate="2011-04-14T10:41:27.343" LastEditorUserId="183" OwnerUserId="4167" PostTypeId="1" Score="8" Tags="&lt;normal-distribution&gt;" Title="Should grades be assigned to students based on a normal distribution?" ViewCount="1141" />
&#10;$$&#10;which is an $O(n^2)$ update of $A^{-1}$. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-04-14T14:34:05.890" Id="9563" LastActivityDate="2011-04-14T14:34:05.890" OwnerUserId="4062" ParentId="9544" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;You're looking for a quick and easy check.&lt;/p&gt;&#10;&#10;&lt;p&gt;Under the null hypothesis that the rates (lambda values) are equal, say to $\lambda$, then you could view the two measurements as observing a single process for time $t = t_1+t_2$ and counting the events during the interval $[0, t_1]$ ($n_1$ in number) and the events during the interval $[t_1, t_1+t_2]$ ($n_2$ in number).  You would estimate the rate as&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\hat{\lambda} = \frac{n_1+n_2}{t_1+t_2}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;and from that you can estimate the distribution of the $n_i$: they are Poisson of intensity near $t_i\hat{\lambda}$.  If one or both $n_i$ are situated on tails of this distribution, most likely the claim is valid; if not, the claim may be relying on chance variation.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-04-14T14:56:48.807" Id="9567" LastActivityDate="2011-04-14T14:56:48.807" OwnerUserId="919" ParentId="9561" PostTypeId="2" Score="7" />
  
  <row Body="&lt;p&gt;&lt;strong&gt;Normality assumption of a t-test&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Consider a large population from which you could take many different samples of a particular size. (In a particular study, you generally collect just one of these samples.)&lt;/p&gt;&#10;&#10;&lt;p&gt;The t-test assumes that the means of the different samples are normally distributed; it does not assume that the population is normally distributed.&lt;/p&gt;&#10;&#10;&lt;p&gt;By the central limit theorem, means of samples from a population with finite variance approach a normal distribution regardless of the distribution of the population. Rules of thumb say that the sample means are basically normally distributed as long as the sample size is at least 20 or 30. For a t-test to be valid on a sample of smaller size, the population distribution would have to be approximately normal.&lt;/p&gt;&#10;&#10;&lt;p&gt;The t-test is invalid for small samples from non-normal distributions, but it is valid for large samples from non-normal distributions.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Small samples from non-normal distributions&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;As Michael notes below, sample size needed for the distribution of means to approximate normality depends on the degree of non-normality of the population. For approximately normal distributions, you won't need as large sample as a very non-normal distribution.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here are some simulations you can run in R to get a feel for this. First, here are a couple of population distributions.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;curve(dnorm,xlim=c(-4,4)) #Normal&#10;curve(dchisq(x,df=1),xlim=c(0,30)) #Chi-square with 1 degree of freedom&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Next are some simulations of samples from the population distributions. In each of these lines, &quot;10&quot; is the sample size, &quot;100&quot; is the number of samples and the function after that specifies the population distribution. They produce histograms of the sample means.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;hist(colMeans(sapply(rep(10,100),rnorm)),xlab='Sample mean',main='')&#10;hist(colMeans(sapply(rep(10,100),rchisq,df=1)),xlab='Sample mean',main='')&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;For a t-test to be valid, these histograms should be normal.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;require(car)&#10;qqp(colMeans(sapply(rep(10,100),rnorm)),xlab='Sample mean',main='')&#10;qqp(colMeans(sapply(rep(10,100),rchisq,df=1)),xlab='Sample mean',main='')&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Utility of a t-test&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I have to note that all of the knowledge I just imparted is somewhat obselete; now that we have computers, we can do better than t-tests. As Frank notes, you probably want to use &lt;a href=&quot;http://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test&quot;&gt;Wilcoxon tests&lt;/a&gt; anywhere you were taught to run a t-test.&lt;/p&gt;&#10;" CommentCount="7" CreationDate="2011-04-15T01:07:12.330" Id="9575" LastActivityDate="2014-03-07T10:33:43.760" LastEditDate="2014-03-07T10:33:43.760" LastEditorUserId="36515" OwnerUserId="3874" ParentId="9573" PostTypeId="2" Score="30" />
  <row Body="&lt;p&gt;See my previous answer to a question on the &lt;a href=&quot;http://stats.stackexchange.com/questions/1386/robust-t-test-for-mean/1391#1391&quot;&gt;robustnest off the t-test&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;In particular, I recommend playing around with the &lt;a href=&quot;http://onlinestatbook.com/stat_sim/robustness/index.html&quot;&gt;onlinestatsbook applet&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;The image below is based on the following scenario:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Null hypothesis is true&lt;/li&gt;&#10;&lt;li&gt;fairly severe skewness&lt;/li&gt;&#10;&lt;li&gt;Same distribution in both groups&lt;/li&gt;&#10;&lt;li&gt;same variance in both groups&lt;/li&gt;&#10;&lt;li&gt;sample size per group 5 (i.e., much less than 50 as per your question)&lt;/li&gt;&#10;&lt;li&gt;I pressed the 10,000 simulations button about 100 times to get up to over one million simulations.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;The simulation obtained suggests that instead of getting a 5% Type I errors, I was only getting 4.5% Type I errors.&lt;/p&gt;&#10;&#10;&lt;p&gt;Whether you consider this robust depends on your perspective.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/gOXMq.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-04-15T07:23:36.947" Id="9580" LastActivityDate="2011-04-15T07:23:36.947" OwnerUserId="183" ParentId="9573" PostTypeId="2" Score="11" />
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I am interested in using the MacKinnon product of coefficients test to assess mediation. Is there a quick way to carry out these calculations and determine significance based on the obtained statistic?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-04-16T06:50:43.337" FavoriteCount="1" Id="9605" LastActivityDate="2011-04-16T08:00:23.233" LastEditDate="2011-04-16T08:00:23.233" LastEditorUserId="930" OwnerDisplayName="user3958" PostTypeId="1" Score="2" Tags="&lt;mediation&gt;" Title="Calculating the MacKinnon empirical distribution test to test mediation" ViewCount="250" />
  
  
  <row Body="&lt;p&gt;This is a problem which can be relatively easily solved using a Bayesian approach.  This is one of the &quot;re-using&quot; the original data answers, rather than one which uses the CIs.&lt;/p&gt;&#10;&#10;&lt;p&gt;Each coin has some long run frequency of heads $\theta_i$ $(i=1,\dots,50)$.  Now you have some partial information about these frequencies, that you suspect them to be below 5%.  Not exactly clear what is meant by this: do you want to test this? or do you have information which says that it should be less than 5%?  And do you think its 5% because of the data, or did you think this before seeing the data?  Given that you have a reasonably large amount of data, this kind of information probably won't make much difference - so I will simply ignore it.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now there has been no logical connection stated between each coin - so I will not pre-suppose that one exists.  This means in words that knowing the results of one coin doesn't tell you anything about any other coin.&lt;/p&gt;&#10;&#10;&lt;p&gt;So we start by supposing that the only thing that was known about each coin is that it is possible for each to give heads, and possible for it to give tails (i.e. we assume that if we flipped the coin until we saw both a head and a tail we would not sample forever).  This results in a uniform prior for each:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$p(\theta_{i}|I)=1$$&lt;/p&gt;&#10;&#10;&lt;p&gt;(where $I$ simply denotes the &quot;prior information&quot; or assumptions contained in the problem).  Now you will throw each coin $n_{i}=30$ times, and conditional on $\theta_{i}$ the number of heads observed $x_{i}$ will have a binomial sampling distribution&lt;/p&gt;&#10;&#10;&lt;p&gt;$$(x_{i}|n_{i},\theta_{i},I)\sim Binomial(n_{i},\theta_{i})$$&lt;/p&gt;&#10;&#10;&lt;p&gt;The posterior distribution for each $\theta_{i}$ will have a beta distribution:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$(\theta_{i}|x_{i},n_{i},I)\sim Beta(x_{i}+1,n_{i}-x_{i}+1)\implies p(\theta_{i}|x_{i},n_{i},I)= \frac{\theta_{i}^{x_{i}}(1-\theta_{i})^{n_{i}-x_{i}}}{B(x_{i}+1,n_{i}-x_{i}+1)}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Where $B(a,b)$ is the &lt;a href=&quot;http://en.wikipedia.org/wiki/Beta_function&quot; rel=&quot;nofollow&quot;&gt;beta function&lt;/a&gt;.  Note that each posterior has mode (most likely value) equal to the observed proportion $\frac{x_{i}}{n_{i}}$.  Now this is the posterior for each of the $50$ coins is always proper, well behaved, and &lt;em&gt;exact even when the observed fraction is zero&lt;/em&gt; (no approximations have been made).  Denote the number of future trials as $m_{i}$ and the unknown number of heads in these trials by $y_{i}$&lt;/p&gt;&#10;&#10;&lt;p&gt;So if you knew which coin you picked, then you have a posterior predictive for the next result:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$p(y_{i}|m_{i},x_{i},n_{i},\text{ith coin},I)=\int_{0}^{1}p(y_{i}|m_{i},\theta_{i},I)p(\theta_{i}|x_{i},n_{i},I)d\theta_{i}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$={m_{i} \choose y_{i}}\int_{0}^{1}\theta_{i}^{y_{i}}(1-\theta_{i})^{m_{i}-y_{i}}\frac{\theta_{i}^{x_{i}}(1-\theta_{i})^{n_{i}-x_{i}}}{B(x_{i}+1,n_{i}-x_{i}+1)}d\theta_{i}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$={m_{i} \choose y_{i}}\frac{B(x_{i}+y_{i}+1,m_{i}+n_{i}-x_{i}-y_{i}+1)}{B(x_{i}+1,n_{i}-x_{i}+1)}=\frac{{x_{i}+y_{i} \choose y_{i}}{m_{i}+n_{i}-x_{i}-y_{i} \choose m_{i}-y_{i}}}{{m_{i}+n_{i}+1 \choose m_{i}}}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Which looks like a hypergeometric like distribution, but not quite, as the &quot;random variable&quot; $y_{i}$ appears in different places to what you would see in a hypergeometric.  This is called a &lt;a href=&quot;http://en.wikipedia.org/wiki/Beta-binomial_distribution&quot; rel=&quot;nofollow&quot;&gt;beta-binomial compound distribution&lt;/a&gt;.  Note that it is &quot;parameter free&quot; - it only depends on what is unknown but of interest $y_{i}$ and what is known $m_{i},x_{i},n_{i}$ - no &quot;pluggin in&quot; of any estimate is required - hence this is an exact inference.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;What if we chose one coin randomly and flipped it $m$ times?&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;But if you now don't know which coin is going to be picked, then the answer cannot depend on $i$.  To do this we simply average out, or marginalise out the $i$.  Assuming that you have no reason to suspect any one coin will be preferred in the choice (what is really meant by &quot;randomly&quot; I think), then each is equally likely, and we get:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$p(y|m,D,I)=\frac{1}{50}\sum_{i=1}^{50}\frac{{x_{i}+y \choose y}{m+n_{i}-x_{i}-y \choose m-y}}{{m+n_{i}+1 \choose m}}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Where $D\equiv x_{1},\dots,x_{50},n_{1},\dots,n_{50}$ (&quot;the data&quot;) and the $i$ subscript has been dropped from $m,y$ to indicate that the result doesn't depend on $i$ (i.e. doesn't depend on which coin is flipped).  Now this is an &lt;em&gt;exact&lt;/em&gt; answer to your problem.  You can calculate a 95% interval for $y$ using this probability distribution, setting $m=30$ (since this is the sample size you used).  This should give you what you are after - in that the frequentist CI is usually phrased in a predictive fashion &quot;if we sampled again what would we likely the true value be covered by the CI 95% of the time&quot; - a CI talks about the future.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-04-16T09:58:53.657" Id="9608" LastActivityDate="2011-04-16T09:58:53.657" OwnerUserId="2392" ParentId="9599" PostTypeId="2" Score="1" />
  <row AnswerCount="1" Body="&lt;p&gt;I wonder if two identical time series are cointegrated. Can anyone shed some light on this? Thanks&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-04-16T17:29:20.930" Id="9615" LastActivityDate="2011-04-16T17:38:37.700" OwnerUserId="4192" PostTypeId="1" Score="3" Tags="&lt;time-series&gt;&lt;cointegration&gt;" Title="Are two identical time series cointegrated?" ViewCount="296" />
  
  <row Body="&lt;p&gt;Imagine that you want to know the average height of men on earth. This average height exists but obviously you will never be able to know it (unless you're able to measure several millions men...).&#10;What you can do is measure hundreds or thousands of people and calculate the average height of these people. The average height among these people is probably not exactly equal to the average height of men on earth (because they are particular men in the whole population) but, if you did a good job (use a representative sample of the population), it should be close enough.&#10;The difference between the quantity that you want to know (average height of men on earth) and its estimation through your sample (average height of men in the sample) is the sampling error.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-04-16T22:15:11.883" Id="9634" LastActivityDate="2011-04-16T22:15:11.883" OwnerUserId="3377" ParentId="9623" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;The geometric mean can be motivated by noting that it is the exponent of the arithmetic mean of the logarithm:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\overline{x}_{geo}=\left(\prod_{i}x_{i}\right)^{\frac{1}{n}}=exp\left[\frac{1}{n}\sum_{i}log(x_{i})\right]=exp\left[\overline{log(x)}\right]$$&lt;/p&gt;&#10;&#10;&lt;p&gt;So it makes sense to take the exponent of the standard deviations of the logs as the equivalent measure:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$s_{x,geo}=exp\left[s_{log(x)}\right]=exp\left[\sqrt{\overline{\{log(x)\}^{2}}-\{\overline{log(x)}\}^{2}}\right]$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Where:&#10;$$\overline{log(x)}=\frac{1}{n}\sum_{i}log(x_{i})$$&#10;$$\overline{\{log(x)\}^{2}}=\frac{1}{n}\sum_{i}[log(x_{i})]^2$$&lt;/p&gt;&#10;&#10;&lt;p&gt;However, in a particular context, better estimates of error may be more appropriate in a specific modeling context, such as the (observed) fisher information.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-04-17T05:38:56.727" Id="9639" LastActivityDate="2011-04-17T05:38:56.727" OwnerUserId="2392" ParentId="9637" PostTypeId="2" Score="2" />
  <row AnswerCount="1" Body="&lt;p&gt;I am using R software (R commander) to cluster my data. I have a smaller subset of my data containing 200 rows and about 800 columns. I am getting the following error when trying kmeans cluster and plot on a graph:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;'princomp' can only be used with more units than variables&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I then created a test doc of 10 row and 10 columns whch plots fine but when I add an extra column I get te error again. Why is this? I need to be able to plot my cluster. When I view my data set after performing kmeans on it I can see the extra results column which shows which clusters they belong to.&lt;/p&gt;&#10;&#10;&lt;p&gt;Is there anything I am doing wrong, can I ger rid of this error and plot my larger sample? &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-04-17T12:42:40.607" Id="9643" LastActivityDate="2011-04-17T13:10:59.397" LastEditDate="2011-04-17T12:56:15.330" LastEditorUserId="88" OwnerUserId="4020" PostTypeId="1" Score="1" Tags="&lt;r&gt;&lt;clustering&gt;" Title="Clustering can be plotted only with more units than variables?" ViewCount="377" />
  
  
  
  <row Body="&lt;p&gt;Note that @whuber adresses all the points raised in the question, I will provide only the mathematical details. The regression with spatial error assumes that model is the following:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$y=X\beta+u$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $Euu&amp;#39;=\Omega$. The efficient estimate of $\beta$  then is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\hat\beta=(X&amp;#39;\Omega X)^{-1}X&amp;#39;\Omega y$$&lt;/p&gt;&#10;&#10;&lt;p&gt;When you have new observation it also follows the model&lt;/p&gt;&#10;&#10;&lt;p&gt;$$y^*=X^*\beta+u^*$$&lt;/p&gt;&#10;&#10;&lt;p&gt;We predict $y^*$ by $X^*\beta$ so the prediction error is&lt;/p&gt;&#10;&#10;&lt;p&gt;$$y^*-\hat{y^*}=X^*\hat\beta+u^*$$&lt;/p&gt;&#10;&#10;&lt;p&gt;For linear model $u^*$ will be independent of $u$ so &lt;/p&gt;&#10;&#10;&lt;p&gt;$$Var(y^*-\hat{y^*})=Var(X^*\hat\beta+u^*)=Var(X^*\hat\beta)+Var(u^*)=\sigma^2+\sigma^2X^{* } (X&amp;#39;X)^{-1})(X^{* })&amp;#39;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;For the spatial error model we have&#10;\begin{align*}&#10;Var(X^{* }\hat\beta)=&#10;Var(X^{* } (X'\Omega X)^{-1}X'\Omega u)&#10;=X^{* } (X'\Omega X)^{-1})(X^{* })'&#10;\end{align*}&lt;/p&gt;&#10;&#10;&lt;p&gt;But now $u^*$ correlates with $u$ and we do not have such convenient decomposition. Thus the formula in your question cannot be used.&lt;/p&gt;&#10;&#10;&lt;p&gt;The general approach how to calculate the error is describe in &lt;a href=&quot;http://www.jstor.org/pss/2281645?searchUrl=%2Faction%2FdoBasicSearch%3FQuery%3Dgoldberger%2Bjournal%2Bof%2Bamerican%2Bstatistical%2Bassociation%26acc%3Doff%26wc%3Don&amp;amp;Search=yes&quot; rel=&quot;nofollow&quot;&gt;this article&lt;/a&gt;. It shows how to get the best unbiased linear prediction in linear model when the model errors are correlated. Note that this heavily relies on the fact that you have reliable estimate of $\Omega$, so @whuber answer applies very strongly. Even if you have correct formulas they might give bad results if it is known that $\Omega$ cannot be reliably estimated.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-04-18T09:08:31.047" Id="9669" LastActivityDate="2011-04-18T13:52:57.690" LastEditDate="2011-04-18T13:52:57.690" LastEditorUserId="2116" OwnerUserId="2116" ParentId="9280" PostTypeId="2" Score="3" />
  <row AnswerCount="1" Body="&lt;p&gt;I have a bit of a problem with the &lt;a href=&quot;http://cran.r-project.org/web/packages/tm/index.html&quot; rel=&quot;nofollow&quot;&gt;tm&lt;/a&gt; R package for cleaning text documents.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is how my code looks like:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(&quot;tm&quot;)&#10;&#10;# import text files in corpus text – ok&#10;c_txt &amp;lt;- Corpus((DirSource(directory = &quot;.&quot;, pattern =&quot;txt&quot;, &#10;                encoding = &quot;UTF-8&quot;)), readerControl = list(language = &quot;rus&quot;))&#10;&#10;# convert to Lower Case – ok&#10;txt_cl &amp;lt;- tm_map(c_txt, tolower)&#10;&#10;# remove Stopwords - not working&#10;txt_cl &amp;lt;- tm_map(txt_t, removeWords, stopwords(&quot;russian&quot;))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I am inspecting the file &lt;code&gt;russian.dat&lt;/code&gt; which is located in the subdirectory &lt;code&gt;tm\stopwords&lt;/code&gt;. There I can notice strange characters -- not Russian stopwords, although it is UTF-8 encoding.&lt;/p&gt;&#10;&#10;&lt;p&gt;Where is my mistake?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks in advance.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-04-18T09:37:37.580" FavoriteCount="2" Id="9674" LastActivityDate="2011-04-18T12:04:01.667" LastEditDate="2011-04-18T09:46:25.323" LastEditorUserId="930" OwnerUserId="4212" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;text-mining&gt;" Title="How to remove stopwords with Russian documents?" ViewCount="859" />
  
  
  
  <row Body="&lt;p&gt;I've just downloaded the Windows binary version of &lt;code&gt;tm&lt;/code&gt; from CRAN: &lt;a href=&quot;http://cran.r-project.org/web/packages/tm/index.html&quot; rel=&quot;nofollow&quot;&gt;http://cran.r-project.org/web/packages/tm/index.html&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The &lt;code&gt;russian.dat&lt;/code&gt; file that's included there isn't UTF-8, but is encoded using &lt;a href=&quot;http://en.wikipedia.org/wiki/KOI8-R&quot; rel=&quot;nofollow&quot;&gt;KOI-8&lt;/a&gt;. I am unfamiliar with the inner workings of &lt;code&gt;tm&lt;/code&gt;, but you state that you think the file is UTF-8. I am wondering if the problem has to do with the different character encodings?&lt;/p&gt;&#10;&#10;&lt;p&gt;You could you try to figure out the encoding of your file using &lt;a href=&quot;http://2cyr.com/decode/&quot; rel=&quot;nofollow&quot;&gt;http://2cyr.com/decode/&lt;/a&gt; and specifying Autodetect as the source encoding.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-04-18T11:46:17.800" Id="9682" LastActivityDate="2011-04-18T12:04:01.667" LastEditDate="2011-04-18T12:04:01.667" LastEditorUserId="439" OwnerUserId="439" ParentId="9674" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;Yeah, the sampling package handles this, you can do cluster sampling or stratified or a few others:  &lt;a href=&quot;http://cran.r-project.org/web/packages/sampling/sampling.pdf&quot; rel=&quot;nofollow&quot;&gt;http://cran.r-project.org/web/packages/sampling/sampling.pdf&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;It can then also handle a lot of the special variance estimation techniques you'll have to do for any metric you calculate from the complex design. However, I prefer Lumley's survey package for that.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-04-18T12:02:13.153" Id="9684" LastActivityDate="2011-04-18T12:02:13.153" OwnerUserId="1893" ParentId="7236" PostTypeId="2" Score="2" />
  
  <row AcceptedAnswerId="9698" AnswerCount="1" Body="&lt;p&gt;In a (one or multi) way anova model, once a new individual is assigned to a treatment, the predicted value for him is calculated using the coefficients of the ANOVA model (simply assigning the treatment mean value to the individual). &lt;/p&gt;&#10;&#10;&lt;p&gt;How should I construct a confidence (or prediction) interval for that predicted value? Can the &lt;code&gt;predict&lt;/code&gt; function be used in R for an &lt;code&gt;aov&lt;/code&gt; model? &lt;/p&gt;&#10;&#10;&lt;p&gt;There must be something that I'm missing here, as I can't find anything like this in my books. Thank you.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-04-18T17:01:07.550" FavoriteCount="1" Id="9692" LastActivityDate="2011-04-19T03:30:54.897" LastEditDate="2011-04-19T03:30:54.897" LastEditorUserId="3582" OwnerUserId="632" PostTypeId="1" Score="3" Tags="&lt;r&gt;&lt;anova&gt;&lt;confidence-interval&gt;" Title="Prediction in simple and multiple ANOVA" ViewCount="1107" />
  <row AcceptedAnswerId="9700" AnswerCount="5" Body="&lt;p&gt;Is there a possibility to use R in a webinterface without the need to install it?&lt;/p&gt;&#10;&#10;&lt;p&gt;I have only one small script which I like to run but I just want to give it a shot without a long installation procedure.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you.&lt;/p&gt;&#10;" ClosedDate="2014-04-20T22:19:48.033" CommentCount="7" CreationDate="2011-04-18T19:26:12.323" FavoriteCount="9" Id="9699" LastActivityDate="2014-04-20T05:25:50.537" OwnerUserId="230" PostTypeId="1" Score="22" Tags="&lt;r&gt;" Title="Using R online - without installing it" ViewCount="13521" />
&#10;(H_{B1}|D) &amp;amp; 0.004790669 \\
  <row AcceptedAnswerId="13369" AnswerCount="3" Body="&lt;p&gt;I'm hoping to hear from someone who has worked on mouse models or similar biological analyses where there is a tendency to run 'replicates' of an experiment. I know multiple testing is a sizeable kettle of fish which is definitely relevant to this discussion. I have some applications for projects where they talk about running 3 replicates of an experiment, where each experiment has n = 3 to 7. However there seems to be no mention of what they will do with the multiple sets of results, as in how they will handle success, failure, success vs failure, failure, success etc. It seems like this 'replicates' approach is quite common practise in this field.&lt;/p&gt;&#10;&#10;&lt;p&gt;What are your thoughts / experiences with this situation?&lt;/p&gt;&#10;&#10;&lt;p&gt;I know there are different types of replication, technical vs biological, however I've found little useful reading on this issue.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-04-19T05:19:00.737" FavoriteCount="1" Id="9724" LastActivityDate="2011-07-22T13:09:00.340" LastEditDate="2011-06-20T19:19:31.947" LastEditorUserId="82" OwnerUserId="4226" PostTypeId="1" Score="2" Tags="&lt;repeated-measures&gt;&lt;multiple-comparisons&gt;&lt;experiment-design&gt;&lt;biostatistics&gt;" Title="Mouse models - 'replicates' and analysis" ViewCount="280" />
  
  
  
  <row Body="&lt;p&gt;For the linear-nonlinear part, see: &lt;a href=&quot;http://stats.stackexchange.com/questions/8689/what-does-linear-stand-for-in-linear-regression&quot;&gt;CrossValidated article on the topic&lt;/a&gt;, particularly the second-ranked answer by Charlie. I don't think there are any changes when dealing with mixed effects.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-04-19T20:00:02.607" Id="9760" LastActivityDate="2011-04-19T20:00:02.607" OwnerUserId="1764" ParentId="9759" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;If you have the probability density function $f$ of the random variable, then it is a matter of checking for which $n$ the integral &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\int_{\mathbb{R}}x^nf(x)dx&amp;lt;\infty$$&lt;/p&gt;&#10;&#10;&lt;p&gt;This is then the standard exercise in real analysis. Alternatively if you know the &lt;a href=&quot;http://en.wikipedia.org/wiki/Characteristic_function_%28probability_theory%29&quot; rel=&quot;nofollow&quot;&gt;characteristic function&lt;/a&gt; of random variable $\phi$ then it is a matter of checking how many derivatives the function $\phi$ has.&lt;/p&gt;&#10;&#10;&lt;p&gt;Yet another alternative if you have the distribution function is to look for highest $n$ for which the the following limit is zero:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\lim_{t\to\infty}t^nP(|X|\ge t)=\lim_{t\to\infty}t^n(1-P(|X|&amp;lt;t))=\lim_{t\to\infty}(1-F(t)+F(-t))=0$$&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-04-20T08:13:14.423" Id="9771" LastActivityDate="2011-05-20T20:03:52.177" LastEditDate="2011-05-20T20:03:52.177" LastEditorUserId="2116" OwnerUserId="2116" ParentId="9766" PostTypeId="2" Score="6" />
  
  
  
  
  <row Body="&lt;p&gt;I don't doubt a particular test statistic aiming to accomplish what your asking for exists, but I will offer some alternatives that you may be interested in that offer different answers (but probably still interesting) given the nature of the question.&lt;/p&gt;&#10;&#10;&lt;p&gt;Like Rolando already stated, the extent to which the partial correlation is moderated depends on the correlation between the three variables. Below I have inserted a picture of the situation in the form of a path diagram (and you can just pretend the box with X is demographic dissimilarity, Z is career development, and Y is satisfaction). The test for whether the direct correlation between X and Y is different than the partial correlation of X and Y controlling for Z amounts to stating whether the blue line &lt;em&gt;and&lt;/em&gt; the red line in the below picture are non-zero. So simply examining both of those correlations will be informative. If the product of those correlations is near zero, then the direct correlation between X and Y will be largely unchanged when partialling out the variance of Z. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/tQmHE.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Another test you may be interested in would be a &lt;a href=&quot;http://en.wikipedia.org/wiki/Likelihood-ratio_test&quot; rel=&quot;nofollow&quot;&gt;likelihood ratio test&lt;/a&gt;. In this example you have a case of nested models, and hence you can test the model with only the direct correlation between X and Y and the model that includes both X and Z. This can be more easily extended to multiple Z's as well. But this is not the same as testing whether the effect of X is moderated by Z(s), but whether the model including X and Z is a better fit to the data than the model only including X.&lt;/p&gt;&#10;&#10;&lt;p&gt;Some might interpret this question as asking about moderating and mediating effects. By the names of the variables though I wouldn't immediately suggest these are what you are after. &lt;a href=&quot;http://goo.gl/oBTBg&quot; rel=&quot;nofollow&quot;&gt;Kline (2010)&lt;/a&gt; has a few references regarding these, so those might be a good forray to search. Although to conduct those tests it appears you need to make much stronger assumptions about causality (&lt;a href=&quot;http://dx.doi.org/10.1037/0022-0167.51.1.115&quot; rel=&quot;nofollow&quot;&gt;Frazier, Barron, &amp;amp; Tix 2004&lt;/a&gt; &lt;a href=&quot;http://dionysus.psych.wisc.edu/Lit/Topics/Statistics/Mediation/frazier_barron_mediation_jcc.pdf&quot; rel=&quot;nofollow&quot;&gt;PDF&lt;/a&gt;). &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-04-20T14:01:56.807" Id="9788" LastActivityDate="2011-04-20T14:01:56.807" OwnerUserId="1036" ParentId="9718" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;The reason why you are conducting this test is to determine which policy is more valuable, and if value is measured in profitability, then it makes no sense to do statistical testing on any other variable.  A properly conducted test on profitability gives you all the information needed for your companies' decision: once you have the results of this test, results about other variables (e.g. response rate) provide no decision-relevant information.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-04-20T14:05:00.977" Id="9789" LastActivityDate="2011-04-21T00:40:09.410" LastEditDate="2011-04-21T00:40:09.410" LastEditorUserId="3567" OwnerUserId="3567" ParentId="9774" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;First of all it could be useful to read a bit about the &lt;a href=&quot;http://en.wikipedia.org/wiki/Unit_root&quot; rel=&quot;nofollow&quot;&gt;unit root&lt;/a&gt; problem (you may start from the hypothesis section). So the nature of the explosiveness (exponential growth) is what matters. Roughly the growth could be explained either by deterministic part (for example linear trend) or by random walk with drift. Dickey Fuller (not the best unit root tests choice though, if you are familiar with $R$ I would suggest to go for &lt;code&gt;library(urca)&lt;/code&gt; Zivot and Andrews test &lt;code&gt;?ur.za&lt;/code&gt;, since this one also counts for possible structural breaks) is designed to distinguish between the too, thus $t$ statistic will help to decide if the nature of explosive behavior is deterministic or not. But bear in mind that DF test is very sensitive to deviations from the assumptions it was build on, even ADF would be more robust to apply in practice.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-04-20T15:31:31.433" Id="9793" LastActivityDate="2011-04-20T15:31:31.433" OwnerUserId="2645" ParentId="9744" PostTypeId="2" Score="5" />
&#10; \vdots \\
&#10; y_n
&#10;\end{smallmatrix}\right)
&#10;\left(\begin{smallmatrix}
&#10; \beta_0 \\
&#10; y_1 \\
&#10; \vdots \\
  
&#10;\begin{array}{c}
  
  <row Body="&lt;p&gt;if deviance were proportional to log likelihood, and one uses the definition (see for example McFadden's &lt;a href=&quot;http://www.ats.ucla.edu/stat/mult_pkg/faq/general/psuedo_rsquareds.htm&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;)&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;pseudo R^2 = 1 - L(model) / L(intercept)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;then the psuedo-R^2 above would be 1 - 198.63 / 958.66 = 0.7928&lt;/p&gt;&#10;&#10;&lt;p&gt;The question is: is reported deviance proportional to log likelihood?&lt;/p&gt;&#10;" CommentCount="1" CommunityOwnedDate="2011-04-20T20:08:26.750" CreationDate="2011-04-20T20:08:26.750" Id="9805" LastActivityDate="2011-07-09T02:01:42.977" LastEditDate="2011-07-09T02:01:42.977" LastEditorUserId="2849" OwnerUserId="2849" ParentId="8511" PostTypeId="2" Score="2" />
&#10; &amp;amp;  &amp;amp; +\underbrace{(\boldsymbol{\Lambda}_{E_{e\times k}}\otimes\boldsymbol{I}_{g})\:\boldsymbol{f}_{G_{kg\times1}}+(\boldsymbol{I}_{e}\otimes\boldsymbol{I}_{g})\delta_{eg\times1}}\end{eqnarray*}
  
  <row AcceptedAnswerId="9830" AnswerCount="1" Body="&lt;p&gt;I am trying to use Maximum Likelihood Estimation to learn the structure of a DAG, G.&lt;/p&gt;&#10;&#10;&lt;p&gt;How is the number of free parameters of G calculated to compare the complexity of different graphical models?&lt;/p&gt;&#10;&#10;&lt;p&gt;Is it based on one of the following, or something else?&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Number of edges in the graph&lt;/li&gt;&#10;&lt;li&gt;Maximum number of possible edges in the graph&lt;/li&gt;&#10;&lt;li&gt;Maximum number of parents (children)&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="8" CreationDate="2011-04-21T08:56:58.870" Id="9817" LastActivityDate="2011-04-21T15:07:33.127" OwnerUserId="3595" PostTypeId="1" Score="1" Tags="&lt;bayesian&gt;&lt;graphical-model&gt;" Title="What is the number of free parameters for a directed acyclic graph?" ViewCount="465" />
  
  <row Body="&lt;p&gt;You could apply linear regression with regularization (&lt;a href=&quot;http://www-stat.stanford.edu/~tibs/lasso.html&quot; rel=&quot;nofollow&quot;&gt;Lasso&lt;/a&gt;) to solve this problem. The idea would be to fit the data with piece-wise constant functions adding a penalty for every jump that occurs. The objective you have to minimize is&lt;/p&gt;&#10;&#10;&lt;p&gt;$x^* = \arg\min_{x\in\mathbb{R}^m} \|z - x\|_2^2 + \lambda \|\nabla x\|_1$,&lt;/p&gt;&#10;&#10;&lt;p&gt;where $(\nabla x)_i = x_{i-1} - x_{i}$ is the (backward) difference operator on the grid. The parameter $\lambda$ controls the tradeoff between small- vs. large intervals and low vs. high variance within the intervals.&lt;/p&gt;&#10;&#10;&lt;p&gt;From the solution $x^*$ you can then reconstruct the intervals $I$.&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-04-21T12:43:00.117" Id="9821" LastActivityDate="2011-04-21T12:43:00.117" OwnerUserId="4272" ParentId="9813" PostTypeId="2" Score="2" />
  
  
  <row Body="&lt;pre&gt;&lt;code&gt;set.seed(20); y = rnorm(20); x = y + rnorm(20, 0, 0.2) # generate correlated data&#10;summary(lm(y ~ x))                  # original model&#10;summary(lm(y ~ x, offset= 1.00*x))  # testing against slope=1&#10;summary(lm(y-x ~ x))                # testing against slope=1&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Outputs:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;            Estimate Std. Error t value Pr(&amp;gt;|t|)    &#10;(Intercept)  0.01532    0.04728   0.324     0.75    &#10;x            0.91424    0.04128  22.148 1.64e-14 ***&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;            Estimate Std. Error t value Pr(&amp;gt;|t|)  &#10;(Intercept)  0.01532    0.04728   0.324   0.7497  &#10;x           -0.08576    0.04128  -2.078   0.0523 .&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;            Estimate Std. Error t value Pr(&amp;gt;|t|)  &#10;(Intercept)  0.01532    0.04728   0.324   0.7497  &#10;x           -0.08576    0.04128  -2.078   0.0523 .&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="1" CreationDate="2011-04-21T14:25:14.817" Id="9827" LastActivityDate="2011-04-21T14:38:38.310" LastEditDate="2011-04-21T14:38:38.310" LastEditorUserId="3911" OwnerUserId="3911" ParentId="9825" PostTypeId="2" Score="10" />
  <row AcceptedAnswerId="9847" AnswerCount="2" Body="&lt;p&gt;Do there exist any systems for symbolically solving expectations?&lt;/p&gt;&#10;&#10;&lt;p&gt;This is sort of a follow-up to my question &lt;a href=&quot;http://stats.stackexchange.com/q/9809/3577&quot;&gt;List of Tricks for Solving Messy Expectations?&lt;/a&gt;  Basically, I'm looking for ways to solve a messy expectation after I've exhausted all obvious routes. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT: BACKGROUND&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm trying to solve the following for $\alpha$ (constrained to be greater than 0) as a function of $\sigma_X^2$, $\sigma_Y^2$, and $p$ &lt;/p&gt;&#10;&#10;&lt;p&gt;$E\left[\ln(F) F^\alpha X^2 (X + Y)^2 + \ln(F) F^{2\alpha}(X + Y)^4\right] = 0$&lt;/p&gt;&#10;&#10;&lt;p&gt;where:&lt;/p&gt;&#10;&#10;&lt;p&gt;$Y \sim N(0,\sigma_Y^2)$&lt;/p&gt;&#10;&#10;&lt;p&gt;$ X = \left\{ \begin{array}{cc} 
  
  
  <row Body="&lt;p&gt;Relationships between physical quantities (like pressure and temperature) often may be described using functions or equations, and sometimes the measurement error is small – this might be the case here as well. If so you could derive the type of relationship and the specific function from physics knowledge you could use statistical (including Bayesian) techniques to determine the values and uncertainties of the parameters.&lt;/p&gt;&#10;&#10;&lt;p&gt;If the error is large the relationship between the variables will be less apparent and (compared to the physical equation behind the phenomenon) a more simple function could be fitted on the data. Linear regression is a widely used, simple method.&lt;/p&gt;&#10;&#10;&lt;p&gt;You tagged your question “machine-learning”, so the relationship between your variables may be more complex, but you may not be able to derive it from physics. In this case you can choose from a wide spectrum of non-linear machine learning and statistical techniques.&lt;/p&gt;&#10;&#10;&lt;p&gt;In any case I suggest you to plot your data using a scatterplot, think about the possible mechanisms between the variables. Although machine learning can be Bayesian, too, the best Bayesian approach may be fitting plausible models on the data.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-04-21T21:54:04.843" Id="9846" LastActivityDate="2011-04-21T21:54:04.843" OwnerUserId="3911" ParentId="9845" PostTypeId="2" Score="1" />
  
&#10;P[p_1 &amp;gt; p_2;f_1,f_2] = \frac{\int_0^1 \int_0^1 I(p_1 &amp;gt; p_2) P[D_1|p_1] P[D_2|p_1] dF_1(p_1) dF_2(p_2)}{\int_0 ^1 \int_0^1 P[D_1|p_1] P[D_2|p_1] dF_1(p_1) dF_2(p_2) }
  
  <row AnswerCount="2" Body="&lt;p&gt;Hey, im a beginner to R and trying to run pvclust so as to test a cluster solution.&lt;/p&gt;&#10;&#10;&lt;p&gt;I've managed to load data and run the heirachical cluster, however the code i find online for running pvclust is constantly producing errors - just wondering if someone can point out where I'm going wrong...&lt;/p&gt;&#10;&#10;&lt;p&gt;here is my code (data already transposed. my dataset is called &quot;transpose&quot; below..)&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;##loaddata&#10;&#10;transpose &amp;lt;- &#10;  read.table(&quot;C:/Users/Tim/University/Advanced Design and Data Analysis/Assignment/transposed.csv&quot;,&#10;   header=TRUE, sep=&quot;,&quot;, na.strings=&quot;NA&quot;, dec=&quot;.&quot;, strip.white=TRUE)&#10;&#10;View(transpose)&#10;hello &amp;lt;- hclust(dist(model.matrix(~-1 + &#10;  var001+var002+var003+var004+var005+var006+var007+var008+var009+var010+var011+var012+var013+var014+var015+var016+var017+var018+var019+var020+var021+var022+var023+var024+var025+var026+var027+var028+var029+var030+var031+var032+var033+var034+var035+var036+var037+var038+var039+var040+var041+var042+var043+var044+var045+var046+var047+var048+var049+var050+var051+var052+var053+var054+var055+var056+var057+var058+var059+var060+var061+var062+var063+var064+var065+var066+var067+var068+var069+var070+var071+var072+var073+var074+var075+var076+var077+var078+var079+var080+var081+var082+var083+var084+var085+var086+var087+var088+var089+var090+var091+var092+var093+var094+var095+var096+var097+var098+var099+var100+var101+var102+var103+var104+var105+var106+var107+var108+var109+var110+var111+var112+var113+var114+var115+var116+var117+var118+var119+var120+var121+var122+var123+var124+var125+var126+var127+var128+var129+var130+var131+var132+var133+var134+var135+var136+var137+var138+var139+var140+var141+var142+var143+var144+var145+var146+var147+var148+var149+var150+var151+var152+var153+var154+var155+var156+var157+var158+var159+var160+var161+var162+var163+var164+var165+var166+var167+var168+var169+var170+var171+var172+var173+var174+var175+var176,&#10;   transpose)) , method= &quot;ward&quot;)&#10;&#10;plot(hello, main= &quot;Cluster Dendrogram for Solution hello&quot;, xlab= &#10;  &quot;Observation Number in Data Set transpose&quot;, sub=&quot;Method=ward; &#10;  Distance=euclidian&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;all the above works fine, except the below where i try the pvclust&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(pvclust)&#10;&#10;fit &amp;lt;- pvclust(transpose, method.hclust=&quot;ward&quot;,&#10;   method.dist=&quot;euclidean&quot;)&#10;&#10;plot(fit) &#10;pvrect(fit, alpha=.95)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;the error comes back=&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(pvclust)&#10;&amp;gt; fit &amp;lt;- pvclust(transpose, method.hclust=&quot;ward&quot;,&#10;+    method.dist=&quot;euclidean&quot;)&#10;Warning in dist(t(x), method) : NAs introduced by coercion&#10;Error in hclust(distance, method = method.hclust) : &#10;  NA/NaN/Inf in foreign function call (arg 11)&#10;&amp;gt; plot(fit) &#10;Error in plot(fit) : object 'fit' not found&#10;&amp;gt; pvrect(fit, alpha=.95)&#10;Error in nrow(x$edges) : object 'fit' not found&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2011-04-22T12:00:12.613" FavoriteCount="1" Id="9868" LastActivityDate="2014-09-29T11:21:29.533" LastEditDate="2011-04-22T16:05:37.807" LastEditorUserId="88" OwnerDisplayName="user4287" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;clustering&gt;" Title="Problem with pvclust in R" ViewCount="1680" />
  <row AcceptedAnswerId="9875" AnswerCount="3" Body="&lt;p&gt;I have a &quot;basic statistics&quot; concept question. As a student I would like to know if I'm thinking about this totally wrong and why, if so:     &lt;/p&gt;&#10;&#10;&lt;p&gt;Let's say I am hypothetically trying to look at the relationship between &quot;anger management issues&quot; and say divorce (yes/no) in a logistic regression and I have the option of using two different anger management scores -- both out of 100.&lt;br&gt;&#10;Score 1 comes from questionnaire rating instrument 1 and my other choice; score 2 comes from a different questionnaire. Hypothetically, we have reason to believe from previous work that anger management issues give rise to divorce.&lt;br&gt;&#10;If, in my sample of 500 people, the variance of score 1 is much higher than that of score 2, is there any reason to believe that score 1 would be a  better score to use as a predictor of divorce based on its variance? &lt;/p&gt;&#10;&#10;&lt;p&gt;To me, this instinctively seems right, but is it so?&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-04-22T13:28:43.467" FavoriteCount="2" Id="9871" LastActivityDate="2011-04-23T15:48:17.837" LastEditDate="2011-04-22T16:11:54.387" LastEditorUserId="88" OwnerUserId="4054" PostTypeId="1" Score="10" Tags="&lt;regression&gt;&lt;logistic&gt;" Title="Is a predictor with greater variance &quot;better&quot;? " ViewCount="862" />
  <row AcceptedAnswerId="9914" AnswerCount="2" Body="&lt;p&gt;Good day all,&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose that I am conducting a questionnaire study that is trying to measure level of awareness of subjects about a programming language and find the relation of those level of awareness to working conditions and methods etc.&lt;/p&gt;&#10;&#10;&lt;p&gt;To improve my precision I decided to go with stratified sampling. If I have 1 criteria for stratification such as geo-distribution (to make sure I don't over represent subjects from areas that have less programmers), then I end up with 6 distinct strata (country provinces).&lt;/p&gt;&#10;&#10;&lt;p&gt;I know how to go about analysing these to find margin error, standard error etc but I realised that is not good enough and I need to introduce more criteria's for stratification, such as level of education (so i don't over represent a group who are not very present between programmers based on their education), level of seniority etc.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have the proportion (%) for all these criteria's but I don't know how I go about sampling when I have more than one criteria?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for your help :)  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-04-22T14:31:04.797" FavoriteCount="1" Id="9876" LastActivityDate="2011-04-25T18:05:38.407" OwnerUserId="4043" PostTypeId="1" Score="3" Tags="&lt;stratification&gt;" Title="Stratified sampling question" ViewCount="390" />
  
  
  <row Body="&lt;p&gt;A simple example helps us identify what is essential.&lt;/p&gt;&#10;&#10;&lt;p&gt;Let &#10;$$Y = C + \gamma X_1 + \varepsilon$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $C$ and $\gamma$ are parameters, $X_1$ is the score on the first instrument (or independent variable), and $\varepsilon$ represents unbiased iid error.  Let the score on the second instrument be related to the first one via &lt;/p&gt;&#10;&#10;&lt;p&gt;$$X_1 = \alpha X_2 + \beta.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;For example, scores on the second instrument might range from 25 to 75 and scores on the first from 0 to 100, with $X_1 = 2 X_2 - 50$.  The variance of $X_1$ is $\alpha^2$ times the variance of $X_2$.  Nevertheless, we can rewrite&lt;/p&gt;&#10;&#10;&lt;p&gt;$$Y = C + \gamma(\alpha X_2 + \beta) = (C + \beta \gamma)  + (\gamma \alpha) X_2 + \varepsilon = C&amp;#39; + \gamma&amp;#39; X_2 + \varepsilon.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;The parameters change, &lt;em&gt;and the variance of the independent variable changes&lt;/em&gt;, yet the predictive capability of the model remains unchanged&lt;/strong&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;In general the relationship between $X_1$ and $X_2$ may be nonlinear.  Which is a better predictor of $Y$ will depend on which has a closer linear relationship to $Y$.  Thus &lt;strong&gt;the issue is not one of scale&lt;/strong&gt; (as reflected by the variance of the $X_i$) but has to be decided by the relationships between the instruments and what they are being used to predict.  This idea is closely related to one explored in a recent question about &lt;a href=&quot;http://stats.stackexchange.com/q/9590/919&quot;&gt;selecting independent variables in regression&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;There can be mitigating factors.  For instance, if $X_1$ and $X_2$ are discrete variables and both are equally well related to $Y$, then the one with larger variance &lt;em&gt;might&lt;/em&gt; (if it is sufficiently uniformly spread out) allow for finer distinctions among its values and thereby afford more precision.  &lt;em&gt;E.g.&lt;/em&gt;, if both instruments are questionnaires on a 1-5 Likert scale, both are equally well correlated with $Y$, and the answers to $X_1$ are all 2 and 3 and the answers to $X_2$ are spread among 1 through 5, $X_2$ might be favored on this basis.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-04-22T20:15:55.110" Id="9883" LastActivityDate="2011-04-22T20:15:55.110" OwnerUserId="919" ParentId="9871" PostTypeId="2" Score="8" />
  <row AcceptedAnswerId="55221" AnswerCount="3" Body="&lt;p&gt;I've tough luck with the use of  nls() in &lt;em&gt;R&lt;/em&gt; for the following model&lt;/p&gt;&#10;&#10;&lt;p&gt;$$N_e = N_o\{1-exp[\frac{(d+bN_o)(T_h N_e - T)}{(1+c N_o)}]\}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $b&amp;gt;0$, $c\geq 0$, $T_h&amp;gt;0$, and $T=72$. &lt;/p&gt;&#10;&#10;&lt;p&gt;This code&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;T &amp;lt;- 72&#10;NLS.Fit3 &amp;lt;- nls(Ne~No*(1-exp((d+b*No)*(Th*Ne-T)/(1+c*No))), data = Data,&#10;            start = list(d = 0.01, b = 0.01, Th = 0.01, c = 0.01),&#10;            control = nls.control(maxiter=50, tol=1e-05, minFactor=1/1024))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;gives the following error message: &lt;/p&gt;&#10;&#10;&lt;p&gt;Error in nls(Ne ~ No * (1 - exp((d + b * No) * (Th * Ne - T)/(1 + c *  : &#10;  singular gradient&lt;/p&gt;&#10;&#10;&lt;p&gt;And the following &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;NLS.Fit31 &amp;lt;- nls(Ne~No*(1-exp((d+b*No)*(Th*Ne-T)/(1+c*No))), data = Data,&#10;            start = list(d = 0.01, b = 0.01, Th = 0.01, c = 0.01),&#10;            control = nls.control(maxiter=50, tol=1e-05, minFactor=1/1024),&#10;            algorithm = &quot;port&quot;, lower=c(0, 0, 0, 0))&#10;summary(NLS.Fit31)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;code converges but provides the wrong results (drastically different from PROC NLIN)&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;Formula: Ne ~ No * (1 - exp((d + b * No) * (Th * Ne - T)/(1 + c * No)))&#10;&#10;Parameters:&#10;Estimate Std. Error t value Pr(&amp;gt;|t|)  &#10;d  0.008325   0.003488   2.387   0.0192 *&#10;b  0.000000   0.000064   0.000   1.0000  &#10;Th 0.000000   0.614220   0.000   1.0000  &#10;c  0.020670   0.034439   0.600   0.5500  &#10;---&#10;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 &#10;&#10;Residual standard error: 4.631 on 85 degrees of freedom&#10;&#10;Algorithm &quot;port&quot;, convergence message: relative convergence (4)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I'd prefer to do this in &lt;em&gt;R&lt;/em&gt; rather than &lt;em&gt;SAS&lt;/em&gt; and how the constrains can be placed on only few paramters. Any help in this regard will be highly appreciated. Thanks&lt;/p&gt;&#10;&#10;&lt;p&gt;Data is here:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;No  Ne&#10;5   0&#10;5   1&#10;5   1&#10;5   2&#10;5   2&#10;5   2&#10;5   2&#10;5   3&#10;7   0&#10;7   0&#10;7   1&#10;7   1&#10;7   2&#10;7   2&#10;7   2&#10;7   3&#10;10  1&#10;10  1&#10;10  2&#10;10  2&#10;10  3&#10;10  3&#10;10  3&#10;10  4&#10;10  7&#10;15  1&#10;15  1&#10;15  3&#10;15  3&#10;15  4&#10;15  5&#10;15  5&#10;15  5&#10;20  3&#10;20  4&#10;20  7&#10;20  7&#10;20  8&#10;20  8&#10;20  9&#10;20  11&#10;25  4&#10;25  5&#10;25  6&#10;25  7&#10;25  9&#10;25  9&#10;25  13&#10;25  14&#10;30  5&#10;30  8&#10;30  10&#10;30  11&#10;30  11&#10;30  12&#10;30  14&#10;30  20&#10;45  4&#10;45  7&#10;45  8&#10;45  10&#10;45  11&#10;45  14&#10;45  15&#10;45  19&#10;60  9&#10;60  14&#10;60  14&#10;60  16&#10;60  18&#10;60  21&#10;60  24&#10;60  26&#10;80  7&#10;80  11&#10;80  12&#10;80  15&#10;80  17&#10;80  12&#10;80  21&#10;80  23&#10;100 7&#10;100 8&#10;100 10&#10;100 11&#10;100 15&#10;100 24&#10;100 26&#10;100 33&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" ClosedDate="2013-04-05T13:24:46.983" CommentCount="10" CreationDate="2011-04-23T03:51:41.710" Id="9895" LastActivityDate="2013-04-05T08:06:00.117" LastEditDate="2011-04-23T09:16:25.693" LastEditorUserId="3903" OwnerUserId="3903" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;nonlinear-regression&gt;" Title="Having trouble with nls function in R" ViewCount="2520" />
  <row Body="&lt;p&gt;I think this is the R code that you want. I borrowed the R-code from this &lt;a href=&quot;https://stat.ethz.ch/pipermail/r-help/2006-October/114652.html&quot;&gt;thread&lt;/a&gt; on the r-mailing list. The idea basically is: the major and minor half-diameters are the two eigen values and you rotate the ellipse by the amount of angle between the first eigen vector and the x-axis  &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;mat &amp;lt;- matrix(c(2.2, 0.4, 0.4, 2.8), 2, 2)&#10;eigens &amp;lt;- eigen(mat)&#10;evs &amp;lt;- sqrt(eigens$values)&#10;evecs &amp;lt;- eigens$vectors&#10;&#10;a &amp;lt;- evs[1]&#10;b &amp;lt;- evs[2]&#10;x0 &amp;lt;- 0&#10;y0 &amp;lt;- 0&#10;alpha &amp;lt;- atan(evecs[ , 1][2] / evecs[ , 1][1])&#10;theta &amp;lt;- seq(0, 2 * pi, length=(1000))&#10;&#10;x &amp;lt;- x0 + a * cos(theta) * cos(alpha) - b * sin(theta) * sin(alpha)&#10;y &amp;lt;- y0 + a * cos(theta) * sin(alpha) + b * sin(theta) * cos(alpha)&#10;&#10;&#10;png(&quot;graph.png&quot;)&#10;plot(x, y, type = &quot;l&quot;, main = expression(&quot;x = a cos &quot; * theta * &quot; + &quot; * x[0] * &quot; and y = b sin &quot; * theta * &quot; + &quot; * y[0]), asp = 1)&#10;arrows(0, 0, a * evecs[ , 1][2], a * evecs[ , 1][2])&#10;arrows(0, 0, b * evecs[ , 2][3], b * evecs[ , 2][2])&#10;dev.off()&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/F5tPk.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-04-23T09:18:26.517" Id="9899" LastActivityDate="2011-04-23T18:20:42.837" LastEditDate="2011-04-23T18:20:42.837" LastEditorUserId="1307" OwnerUserId="1307" ParentId="9898" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;I now think you have a problem with data. Why?&lt;/p&gt;&#10;&#10;&lt;p&gt;First of all, let's get rid of $e^x$, solving it out and taking $\log$ of both sides&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\log\left(\frac{N_0-N_e}{N_0}\right)=\frac{(d+bN_0)(T_h N_e -T)}{1+cN_0}.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, this log should be a linear function of $N_e$ for fixed $N_0$. As I understand, $T-T_h N_e $ is a time available to the predator to predate and this $-\frac{d+bN_0}{1+cN_0}$ (let's call it $\lambda$) is the frequency of attacks, which is told to be somehow related to the number of prey. Putting this $\lambda$ into the model, we have&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\log\left(\frac{N_0-N_e}{N_0}\right)=\lambda T- \lambda T_h N_e,$$&lt;/p&gt;&#10;&#10;&lt;p&gt;linear function of $N_e$. Is this in data? It seems so:&#10;&lt;img src=&quot;http://i.stack.imgur.com/njTeO.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Thus, we fit lines, get line parameters&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;       lamT       -lamTh   No&#10;1  0.05753546 -0.29739723   5&#10;2  0.01408135 -0.18090769   7&#10;3  0.13772614 -0.18005256  10&#10;4  0.02065162 -0.08429763  15&#10;5  0.09416886 -0.07751886  20&#10;6  0.11148562 -0.06477383  25&#10;7  0.19165200 -0.06134858  30&#10;8  0.04328295 -0.03025566  45&#10;9  0.06706140 -0.02399182  60&#10;10 0.02236404 -0.01553757  80&#10;11 0.01947130 -0.01247877 100&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;and try to calculate the $\lambda$ and $T_h$; but something is wrong at this point:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/pFFZX.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;$-\lambda T_h$ is a clearly $(\alpha N_0+\beta)^{-1}$ (what corresponds to $b=0$), but $T/T_h$ is a total mess; thus my guess is that $T$ was not equal to 72 for all samples and this is the main origin of your problems. &lt;/p&gt;&#10;" CommentCount="10" CreationDate="2011-04-23T10:40:49.003" Id="9901" LastActivityDate="2011-04-24T18:04:48.237" LastEditDate="2011-04-24T18:04:48.237" LastEditorUserId="88" OwnerUserId="88" ParentId="9895" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;Your initial logistic regression approach seems reasonable, assuming good diagnostics.  However, if I understand the scientific question right, you'll need an interaction between year and day to see &lt;em&gt;differences&lt;/em&gt; between years in the tendency to be in diapause as day increases.  The main effect of year only says that one year has a higher per day probability of being in diapause, which is not, I think, what you care about.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Alternatively&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The other way to think about the problem is to have days as a dependent variable.  This means you would reverse the conditional probability formulation that you implicitly started with by not asking 'are later pupae more likely to be in diapause?' i.e. P(diapause | day, year), but rather 'do pupae in dipause tend to occur later in the year' i.e. P(day | diapause, year).&lt;/p&gt;&#10;&#10;&lt;p&gt;In such a model the observations are Pupation_Day which is classified by Diapause status which is nested within Year.  The effect of interest is a difference of differences, specifically: the difference between the difference in mean Pupation_Day for Diapause==1 and Diapause==0 in Year==2005 versus Year=2006.  (I'm not still sure what role the 'pairs of years' play in your design though.)&lt;/p&gt;&#10;&#10;&lt;p&gt;The only potentially tricky bits I see are remembering that Diapause is a random effect, and making a sensible assumption about the conditional distribution of Pupation_Day.  Frankly, I'd try Normal with shared variance first and then see if the model diagnostics object.  If they do, I'd crack open JAGS/BUGS, and just write out the full model, which would make inference about difference in differences easier too.&lt;/p&gt;&#10;&#10;&lt;p&gt;This alternative approach may be too much machinery for the scientific question though.  If we could tell what the ideal inferential endpoint was it would be easier to recommend an approach.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-04-23T11:22:22.667" Id="9902" LastActivityDate="2011-04-23T11:22:22.667" OwnerUserId="1739" ParentId="9841" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;If all the samples come from the same distribution, then yes the median of the sample medians is a fairly robust estimate of the median of the underlying distribution (though this need not be the same as the mean), since the median of a sample from a continuous distribution has probability 0.5 of being below (or above) the population median.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Added&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is some illustrative R code.  It takes a sample from a normal distribution and a case with outliers where 1% of data is 10,000 time bigger than it should be.  It looks at the various statistics for the overall sample data (50,000 points) and then by the centre (mean or median) of the statistics of the 10,000 samples with 5 points in each sample.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(matrixStats)&#10;&#10;wholestats &amp;lt;- function(x,n) {&#10;     mea &amp;lt;- sum(x)/n  &#10;     var &amp;lt;- sum((x-mea)^2)/(n-1)&#10;     sdv &amp;lt;- sqrt(var) &#10;     qun &amp;lt;- quantile(x, c(0.25,0.5,0.75))&#10;     mad &amp;lt;- median(abs(x-qun[2]))&#10;     c(mean=mea, variance=var, st.dev=sdv, &#10;       median=qun[2], IQR=qun[3]-qun[1], &#10;       MAD=mad)&#10;    }&#10;&#10;rowstats &amp;lt;- function(x,b) {&#10;     rmea &amp;lt;- rowSums(x)/b     &#10;     rvar &amp;lt;- rowSums((x-rmea)^2)/(b-1)&#10;     rsdv &amp;lt;- sqrt(rvar)&#10;     rqun &amp;lt;- rowQuantiles(x, c(0.25,0.5,0.75))  &#10;     rmad &amp;lt;- rowMedians(abs(x-rqun[,2]))&#10;     c(mean=mean(rmea), variance=mean(rvar), st.dev=mean(rsdv), &#10;       median=median(rqun[,2]), IQR=median(rqun[,3]-rqun[,1]), &#10;       MAD=median(rmad))&#10;    }&#10;&#10;a &amp;lt;- 10000 # number of samples&#10;b &amp;lt;- 5     # samplesize&#10;&#10;set.seed(1)&#10;d &amp;lt;- array(rnorm(a*b), dim=c(a,b))&#10;doutlier &amp;lt;- array(d * ifelse(runif(a*b)&amp;gt;0.99, 10000, 1) , dim=c(a,b))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;The median based statistics as expected are more robust, though they fail to show that the heavy tailed outlier variant is heavy tailed.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; wholestats(d,a*b)&#10;        mean     variance       st.dev   median.50%      IQR.75%          MAD &#10;-0.002440456  1.011306552  1.005637386 -0.001610677  1.357029247  0.678706371 &#10;&amp;gt; wholestats(doutlier,a*b) &#10;         mean      variance        st.dev    median.50%       IQR.75%           MAD &#10;-3.425664e+00  9.591583e+05  9.793663e+02 -1.610677e-03  1.373658e+00  6.871415e-01 &#10;&amp;gt; rowstats(d,b)&#10;        mean     variance       st.dev       median          IQR          MAD &#10;-0.002440456  1.014611308  0.947630870  0.003460172  0.917642167  0.510115277 &#10;&amp;gt; rowstats(doutlier,b) &#10;         mean      variance        st.dev        median           IQR           MAD &#10;-3.425664e+00  9.607212e+05  1.685929e+02  3.460172e-03  9.301795e-01  5.175084e-01 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2011-04-23T13:42:53.630" Id="9906" LastActivityDate="2011-04-24T02:09:31.230" LastEditDate="2011-04-24T02:09:31.230" LastEditorUserId="2958" OwnerUserId="2958" ParentId="9904" PostTypeId="2" Score="2" />
  
  <row AcceptedAnswerId="9944" AnswerCount="4" Body="&lt;p&gt;I have a matrix of 1000 observations and 50 variables each measured on a 5-point scale.  These variables are organized into groups, but there aren't an equal number of variables in each group.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'd like to calculate two types of correlations:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Correlation within groups of variables (among characteristics): some measure of whether the variables within the group of variables are measuring the same thing.&lt;/li&gt;&#10;&lt;li&gt;Correlation between groups of variables: some measure, assuming that each group reflects one overall trait, of how each trait (group) is related to every other trait.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;These characteristics have been previously classified into groups.  I'm interested in finding the correlation between the groups - i.e. assuming that the characteristics within in group are measuring the same underlying trait (having completed #1 above - Cronbach's alpha), are the traits themselves related?&lt;/p&gt;&#10;&#10;&lt;p&gt;Does anybody have suggestions for where to start?&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-04-24T03:41:44.080" FavoriteCount="4" Id="9918" LastActivityDate="2013-01-22T18:52:18.873" LastEditDate="2011-04-25T08:26:32.587" LastEditorUserId="930" OwnerUserId="4301" PostTypeId="1" Score="8" Tags="&lt;correlation&gt;&lt;psychometrics&gt;&lt;scales&gt;" Title="How to compute correlation between/within groups of variables?" ViewCount="10546" />
  
  <row AnswerCount="4" Body="&lt;p&gt;This is probably demonstrating a fundamental lack of understanding of how partial correlations work.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have 3 variables, x,y,z. When I control for z, the correlation between x and y increases over the correlation between x and y when z was not controlled for.&lt;/p&gt;&#10;&#10;&lt;p&gt;Does this make sense? I tend to think that when one controls for the effect of a 3rd variable, the correlation should decreases.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thank you for your help!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-04-24T20:08:52.457" FavoriteCount="6" Id="9930" LastActivityDate="2011-04-29T07:20:04.600" LastEditDate="2011-04-29T00:49:44.813" LastEditorUserId="3911" OwnerUserId="4307" PostTypeId="1" Score="13" Tags="&lt;correlation&gt;" Title="Does it make sense for a partial correlation to be larger than a zero-order correlation?" ViewCount="1832" />
  <row Body="&lt;p&gt;I think you need to know about moderator and mediator variables. The classic paper is Baron and Kenny [cited 21,659 times]&lt;/p&gt;&#10;&#10;&lt;p&gt;A moderator variable &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&quot;In general terms, a moderator is a&#10;  qualitative (e.g., sex, race, class)&#10;  or quantitative (e.g., level of&#10;  reward) variable that affects the&#10;  direction and/or strength of the&#10;  relation between an independent or&#10;  predictor variable and a dependent or&#10;  criterion variable. Specifically&#10;  within a correlational analysis&#10;  framework, a moderator is a third&#10;  variable that affects the zero-order&#10;  correlation between two other&#10;  variables. ... In the more familiar&#10;  analysis of variance (ANOVA) terms, a&#10;  basic moderator effect can be&#10;  represented as an interaction between&#10;  a focal independent variable and a&#10;  factor that specifies the appropriate&#10;  conditions for its operation.&quot; &lt;a href=&quot;http://psych.wisc.edu/henriques/mediator.html&quot; rel=&quot;nofollow&quot;&gt;p.&#10;  1174&lt;/a&gt;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;A mediator variable &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&quot;In general, a given variable may be&#10;  said to function as a mediator to the&#10;  extent that it accounts for the&#10;  relation between the predictor and the&#10;  criterion. Mediators explain how&#10;  external physical events take on&#10;  internal psychological significance.&#10;  Whereas moderator variables specify&#10;  when certain effects will hold,&#10;  mediators speak to how or why such&#10;  effects occur.&quot; p. 1176&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="1" CreationDate="2011-04-24T20:53:18.513" Id="9932" LastActivityDate="2011-04-25T05:19:38.197" LastEditDate="2011-04-25T05:19:38.197" LastEditorUserId="183" OwnerUserId="3597" ParentId="9930" PostTypeId="2" Score="1" />
  <row Body="&lt;p&gt;Sure, just add a &lt;code&gt;weights=&lt;/code&gt; argument to &lt;code&gt;lm()&lt;/code&gt; (in case of &lt;a href=&quot;http://www.r-project.org&quot;&gt;R&lt;/a&gt;):&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;R&amp;gt; x &amp;lt;- 1:10    ## mean of this is 5.5&#10;R&amp;gt; lm(x ~ 1)    ## regression on constant computes mean&#10;&#10;Call:&#10;lm(formula = x ~ 1)&#10;&#10;Coefficients:&#10;(Intercept)  &#10;        5.5  &#10;&#10;R&amp;gt; lm(x ~ 1, weights=0.9^(seq(10,1,by=-1)))&#10;&#10;Call:&#10;lm(formula = x ~ 1, weights = 0.9^(seq(10, 1, by = -1)))&#10;&#10;Coefficients:&#10;(Intercept)  &#10;       6.35  &#10;&#10;R&amp;gt; &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Here is give 'more recent' (&lt;em&gt;i.e.&lt;/em&gt;, higher) values more weight and the mean shifts from 5.5 to 6.35.  The key, if any, is the $\lambda ^ \tau$ exponential weight I compute on the fly; you can change the weight factor to any value you choose and depending on how you order your data you can also have the exponent run the other way.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can do the same with regression models involving whichever regressors you have.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-04-24T21:27:08.933" Id="9933" LastActivityDate="2011-04-24T21:27:08.933" OwnerUserId="334" ParentId="9931" PostTypeId="2" Score="6" />
  <row AcceptedAnswerId="9938" AnswerCount="2" Body="&lt;p&gt;In a data frame, I would like to get the column's index by name.  For example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;x &amp;lt;- data.frame(foo=c('a','b','c'),bar=c(4,5,6),quux=c(4,5,6))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I want to know the column index for &quot;bar&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;I came up with the following but it seems inelegant.  Is there a more straightforward builtin that I am missing?&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;seq(1,length(names(x)))[names(x) == &quot;bar&quot;]&#10;[1] 2&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" ClosedDate="2013-10-02T14:22:50.323" CommentCount="4" CreationDate="2011-04-25T00:44:09.760" FavoriteCount="1" Id="9937" LastActivityDate="2011-04-25T14:45:57.530" LastEditDate="2011-04-25T14:45:57.530" LastEditorUserId="88" OwnerUserId="1138" PostTypeId="1" Score="6" Tags="&lt;r&gt;" Title="Finding the column index by its name in R" ViewCount="4131" />
  
  <row Body="&lt;p&gt;This smells like archetypal analysis -- extracting some underlying prototypical objects. However, the vanilla AA will give you linear combination as PCA; thus I would suggest making something similar by first making some k-means-like clustering of the events and then selecting those which are closest to the centroids. &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-04-25T08:54:17.140" Id="9946" LastActivityDate="2011-04-25T08:54:17.140" OwnerUserId="88" ParentId="9911" PostTypeId="2" Score="1" />
&#10;\gamma_{jk}(h)=\frac{1}{2n(h)}\sum_{i=1}^{n(h)}\Big\{\big[dn_j(x_i)-dn_j(x_i+h)\big]\cdot\big[dn_k(x_i)-dn_k(x_i+h)\big]\Big\}
  
  
  <row Body="&lt;p&gt;Sounds like what you want to do is a two-stage model.  First transform your data into exponentially smoothed form using a specified smoothing factor, and then input the transformed data into your linear regression formula.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.jstor.org/pss/2627674&quot; rel=&quot;nofollow&quot;&gt;http://www.jstor.org/pss/2627674&lt;/a&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Exponential_smoothing&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Exponential_smoothing&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-04-25T15:18:03.097" Id="9955" LastActivityDate="2011-04-25T15:18:03.097" OwnerUserId="3489" ParentId="9931" PostTypeId="2" Score="4" />
  
  
  <row Body="&lt;p&gt;I think you may want to &lt;em&gt;reshape&lt;/em&gt; your data, not reduce it. This will let you change the structure of your data set so that you can use all of your observations.  You don't mention which statistical package you're using, but R, stata, and MATLAB all have a nice out-of-the-box reshape command you can use.&lt;/p&gt;&#10;&#10;&lt;p&gt;Side thought: you may need to adjust for clustered errors in the reshaped data, since it doesn't sound like your observations are completely independent.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-04-25T18:15:06.373" Id="9964" LastActivityDate="2011-04-25T18:15:06.373" OwnerUserId="4110" ParentId="9911" PostTypeId="2" Score="0" />
  
  
&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Here $f$ denotes the density with respect to counting measure on the set of values $\Omega = \{x_i: i \in \mathbb{N}\}$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The (general) Renyi entropy is also apparently related to free energy of a system in thermal equilibrium, though I'm not personally up on that. A (very) recent paper on the subject is&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;J. C. Baez, &lt;a href=&quot;http://arxiv.org/abs/1102.2098&quot;&gt;Renyi entropy and free energy&lt;/a&gt;, arXiv [quant-ph] 1101.2098 (Feb. 2011).&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="3" CreationDate="2011-04-26T01:36:18.780" Id="9975" LastActivityDate="2011-04-26T01:36:18.780" OwnerUserId="2970" ParentId="9926" PostTypeId="2" Score="21" />
  <row Body="&lt;p&gt;The problem you're trying to solve is a &lt;a href=&quot;http://en.wikipedia.org/wiki/Hungarian_algorithm&quot; rel=&quot;nofollow&quot;&gt;min-cost matching problem&lt;/a&gt;, specifically the problem of minimizing the functional &lt;/p&gt;&#10;&#10;&lt;p&gt;$F(\pi) = \sum_i \|c_i - k_{\pi(i)}\|^2 $&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\pi$ is over all permutations in $S_n$. &lt;/p&gt;&#10;&#10;&lt;p&gt;This can be solved by the Hungarian algorithm (which is a primal-dual method in disguise) and takes $n^3$ time. &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-04-26T04:29:33.870" Id="9978" LastActivityDate="2011-04-26T04:29:33.870" OwnerUserId="139" ParentId="9947" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;&lt;strong&gt;Non-spatial model&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;My House Value is a function of my home Gardening Investment.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;SAR model&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;My House Value is a function of the House Values of my neighbours. &lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;CAR model&lt;/strong&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;My House Value is a function of the Gardening Investment of my neighbours.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-04-26T07:24:16.177" Id="9983" LastActivityDate="2011-04-26T07:24:16.177" OwnerUserId="4329" ParentId="277" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;The definition of fat-tail in &lt;a href=&quot;http://en.wikipedia.org/wiki/Fat_tail&quot; rel=&quot;nofollow&quot;&gt;wikipedia&lt;/a&gt; is that &lt;/p&gt;&#10;&#10;&lt;p&gt;$$p(x)\sim x^{-(\alpha+1)}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;as $x\to\infty$ for some $\alpha&amp;gt;0$. Now &lt;/p&gt;&#10;&#10;&lt;p&gt;$$\frac{e^x}{x^{\alpha+1}}\to\infty,$$&lt;/p&gt;&#10;&#10;&lt;p&gt;as $x\to\infty$, so the $Ee^X$ cannot exist for such type of distributions. So you need to precise what do you have in mind by saying fat-tailed.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-04-26T07:53:57.130" Id="9984" LastActivityDate="2011-04-26T07:53:57.130" OwnerUserId="2116" ParentId="9928" PostTypeId="2" Score="3" />
  
  <row Body="&lt;p&gt;There are no hard and fast rules.  They convey different information and have different properties.  You select the statistic that best conveys what you want to convey.  Or better yet, select statistics that best describe the data.  Keep this same thing in mind when you're selecting a measure of central tendency to analyze.&lt;/p&gt;&#10;&#10;&lt;p&gt;(snipped a bunch of stuff repeating Mike Lawrence's answer)&lt;/p&gt;&#10;&#10;&lt;p&gt;Note that Mike Lawrence is referring to something that's surprising for a lot of people.  In the behavioural sciences there's a lot of folk wisdom that you use medians with small sample sizes.  But in actual fact that's exactly the wrong thing to do because the median quickly becomes more biased than the mean with small samples.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-04-26T13:17:12.987" Id="9993" LastActivityDate="2011-04-26T13:17:12.987" OwnerUserId="601" ParentId="9987" PostTypeId="2" Score="1" />
  <row Body="&lt;h3&gt;Framing the question&lt;/h3&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;You are asking an applied and subjective question, and thus, any answer needs to be infused with applied and subjective considerations.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;From a purely statistical perspective, the mean and median both provide different information about the central tendency of a sample of data. Thus, neither is correct or incorrect by definition.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;From an applied perspective, we often want to say something meaningful about the central tendency of a sample, where central tendency maps onto some subjective notion of &quot;typical&quot;.&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;h3&gt;General thoughts&lt;/h3&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;&lt;p&gt;When summarising what is typical in a sample, observations that are many standard deviations away from the mean (perhaps 3 or 4 SD) will have a large influence on the mean, but not the median. Such observations may lead the mean to deviate from what we think of as the &quot;typical&quot;  value of the sample. This helps to explain the popularity of the median when it comes to reporting house prices and income, where a single island in the pacific or billionaire could dramatically influence the mean, but not the median. Such distributions can often include extreme outliers, and the distribution is positively skewed. In contrast, the median is robust.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;The median can be problematic when the data takes on a limited number of values. &#10;For example, the median of a 5-point Likert item lacks the nuance possessed by the mean. For example, means of 2.8, 3.0, and 3.3 might all have a median of 3.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;In general, the mean has the benefit of using more of the information from the data.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;When skewed distributions exist, it is also possible to transform the distribution and report the mean of the transformed distribution.&lt;/p&gt;&lt;/li&gt;&#10;&lt;li&gt;&lt;p&gt;When a distribution includes outliers, it is possible to use a trimmed mean, or remove the outliers, or adjust the value of the outlier to a less extreme value (e.g., 2 SD from the mean).&lt;/p&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="5" CreationDate="2011-04-26T14:02:40.367" Id="9994" LastActivityDate="2011-04-26T14:02:40.367" OwnerUserId="183" ParentId="9987" PostTypeId="2" Score="6" />
  <row Body="&lt;p&gt;&quot;deviations in the data are the devil&quot; is just not true I think - well I don't agree with it at least.  I'd say its more like &quot;chilli&quot; than the &quot;devil&quot; - as much as you can reasonably handle is good, but it can get nasty if there is too much.&lt;/p&gt;&#10;&#10;&lt;p&gt;The most general procedure I know of to &quot;choose a statistic&quot; to report your data is a combination of two things&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Bayesian inference (describing what is known)&lt;/li&gt;&#10;&lt;li&gt;Decision theory (taking actions under uncertainty)&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;However, both of these methods are only partially &quot;algorithmic&quot; so to speak.  You have to supply the inputs though.  Perhaps the most important part of this stage is that &lt;em&gt;you have to ask a question&lt;/em&gt; that your procedure is going to answer.  Naturally, different questions get different answers.  As the saying goes &quot;I have just derived a very elegant and beautiful answer.  All I have to do now is figure out the question.&quot;  This is a common problem that I have seen with many statistical procedures, is that there is not always a clear statement of the class of problems that it is the best procedure to use.&lt;/p&gt;&#10;&#10;&lt;p&gt;Bayesian inference requires you to specify your prior information in a mathematical framework.  This involves&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Specifying the hypothesis space - what possibilities am I going to consider?&lt;/li&gt;&#10;&lt;li&gt;Assigning probabilities to each part of the space&lt;/li&gt;&#10;&lt;li&gt;Using the rules of probability theory to manipulate the assigned probabilities&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;this is basically an open-ended problem (you can always analyse a given English statement more deeply, to extract more or different information from it).  Decision theory also requires you to specify a loss function - and there are basically no rules or principles by which to do this, at least as far as I know (computational simplicity is a key driver).&lt;/p&gt;&#10;&#10;&lt;p&gt;One useful question to ask yourself though is &quot;what information about the sample do I convey by presenting this statistic?&quot;  or &quot;how much of the complete data set can I recover from using just this set of statistics?&quot;&lt;/p&gt;&#10;&#10;&lt;p&gt;One way you could use Bayesian statistics to help you here is to propose a hypothesis:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\begin{array}{l l}
&#10;\\H_{IQR}:\text{The IQR is the best statistic}
  
  
  
  
  
  <row Body="&lt;p&gt;The $k$-means objective function strictly decreases with each change of assignment, which automatically implies convergence without cycling. Moreover, the partitions produced in each step of $k$-means satisfy a &quot;Voronoi property&quot; in that each point is always assigned to its nearest center. This implies an upper bound on the total number of possible partitions, which yields a finite upper bound on the termination time for $k$-means. &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-04-27T16:44:37.730" Id="10046" LastActivityDate="2011-04-27T16:44:37.730" OwnerUserId="139" ParentId="10024" PostTypeId="2" Score="3" />
  
  
  
  <row Body="&lt;p&gt;Here's a few options using the ggplot2 package. If you haven't started learning much about plotting with R, taking the time now to learn &lt;a href=&quot;http://had.co.nz/ggplot2/&quot;&gt;ggplot2&lt;/a&gt; or lattice may be worth the effort. &lt;/p&gt;&#10;&#10;&lt;p&gt;ggplot requires data to be in &quot;long&quot; format, so the first thing we'll do is reformat to long:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;require(ggplot2)&#10;&#10;dat &amp;lt;- data.frame(V1 = c(100, 120, 140, 150, 210), V2 = c(16, -17, 18, -19, 20)&#10;, V3 = c(11, -12, 13, -14, 15), V4 = c(-6, 7, -8, 9, -10), V5 = c(1, -2, 3, -4, 5))&#10;&#10;dat.m &amp;lt;- melt(dat, &quot;V1&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;It sounds like V1 represents discrete levels, and should be treated as categorical data. That is why I treat V1 as a factor. If that's not the case, simply use V1 in the x-axis.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#Bar chart&#10;ggplot(dat.m, aes(x = factor(V1), y = value, fill = variable)) + &#10;geom_bar(position = &quot;dodge&quot;)&#10;&#10;#Line chart&#10;ggplot(dat.m, aes(x = factor(V1), y = value, group = variable, colour = variable)) + &#10;geom_line()&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/zLCVu.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-04-28T00:23:08.120" Id="10069" LastActivityDate="2011-04-28T00:23:08.120" OwnerUserId="696" ParentId="10005" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;Try  &lt;a href=&quot;http://cran.r-project.org/web/packages/mixdist/index.html&quot; rel=&quot;nofollow&quot;&gt;mixdist&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Here's an example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(mixdist)  &#10;&#10;#Build data vector &quot;x&quot; as a mixture of data from 3 Normal Distributions  &#10;x1 &amp;lt;- rnorm(1000, mean=0, sd=2.0)  &#10;x2 &amp;lt;- rnorm(500, mean=9, sd=1.5)  &#10;x3 &amp;lt;- rnorm(300, mean=13, sd=1.0)  &#10;x &amp;lt;- c(x1, x2, x3)  &#10;&#10;#Plot a histogram (you'll play around with the value for &quot;breaks&quot; as    &#10;#you zero-in on the fit).   Then build a data frame that has the  &#10;#bucket midpoints and counts.  &#10;breaks &amp;lt;- 30  &#10;his &amp;lt;- hist(x, breaks=breaks)  &#10;df &amp;lt;- data.frame(mid=his$mids, cou=his$counts)  &#10;head(df)  &#10;&#10;#The above Histogram shows 3 peaks that might be represented by 3 Normal  &#10;#Distributions.  Guess at the 3 Means in Ascending Order, with a guess for  &#10;#the associated 3 Sigmas and fit the distribution.  &#10;guemea &amp;lt;- c(3, 11, 14)  &#10;guesig &amp;lt;- c(1, 1, 1)  &#10;guedis &amp;lt;- &quot;norm&quot;  &#10;(fitpro &amp;lt;- mix(as.mixdata(df), mixparam(mu=guemea, sigma=guesig), dist=guedis))  &#10;&#10;#Plot the results  &#10;plot(fitpro, main=&quot;Fit a Probability Distribution&quot;)  &#10;grid()  &#10;legend(&quot;topright&quot;, lty=1, lwd=c(1, 1, 2), c(&quot;Original Distribution to be Fit&quot;, &quot;Individual Fitted Distributions&quot;, &quot;Fitted Distributions Combined&quot;), col=c(&quot;blue&quot;, &quot;red&quot;, rgb(0.2, 0.7, 0.2)), bg=&quot;white&quot;)  &#10;&#10;&#10;===========================  &#10;&#10;&#10;Parameters:  &#10;      pi     mu  sigma  &#10;1 0.5533 -0.565 1.9671  &#10;2 0.2907  8.570 1.6169  &#10;3 0.1561 12.725 0.9987  &#10;&#10;Distribution:  &#10;[1] &quot;norm&quot;  &#10;&#10;Constraints:  &#10;   conpi    conmu consigma   &#10;  &quot;NONE&quot;   &quot;NONE&quot;   &quot;NONE&quot;   &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/BzSbw.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-04-28T00:38:55.720" Id="10070" LastActivityDate="2011-04-28T00:38:55.720" OwnerUserId="2775" ParentId="10062" PostTypeId="2" Score="7" />
  
  
  
  
  <row AnswerCount="2" Body="&lt;p&gt;I am trying to understand Logistic Regression in relation to credit scoring model. I wish to understand the significance of &quot;20/ln(2)&quot; in logistic regression. Why and how is it used? &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-04-28T11:54:46.047" FavoriteCount="2" Id="10092" LastActivityDate="2015-02-27T22:34:37.973" LastEditDate="2011-07-29T06:22:27.997" LastEditorUserId="183" OwnerUserId="4368" PostTypeId="1" Score="3" Tags="&lt;logistic&gt;" Title="What does &quot;20/ln(2)&quot; mean in logistic regression?" ViewCount="2081" />
  
  <row Body="&lt;p&gt;A few quick points:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;You could draw on the general literature on missing data.&lt;/li&gt;&#10;&lt;li&gt;If the 200 sessions with missing data are known to be completely random, then you could choose to ignore these observations and just analyse the data where you have complete data.&lt;/li&gt;&#10;&lt;li&gt;In general it is important to think through what mechanisms might explain the missing data.&lt;/li&gt;&#10;&lt;li&gt;You may find it useful to read up on strategies for dealing with &lt;a href=&quot;http://www.stat.psu.edu/~jls/missing_data/&quot; rel=&quot;nofollow&quot;&gt;missing data&lt;/a&gt;.&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2011-04-28T15:33:56.817" Id="10102" LastActivityDate="2011-04-29T05:13:18.330" LastEditDate="2011-04-29T05:13:18.330" LastEditorUserId="183" OwnerUserId="183" ParentId="10100" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;If a potential outcome has never been observed, then you have no information about the effect of covariates on it. So any outcome-specific covariate effects are unidentifiable. If you assume constant covariate effects, then this outcome has no effect on their estimation, so you might as well omit it from the data.&lt;/p&gt;&#10;&#10;&lt;p&gt;More generally, you do have some information about the frequency of that outcome, but logistic regression (multinomial or otherwise) cannot handle this at all, because if you insist on having this outcome, the corresponding intercept has to be $-\infty$.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-04-28T15:52:05.907" Id="10103" LastActivityDate="2011-04-28T15:52:05.907" OwnerUserId="279" ParentId="10031" PostTypeId="2" Score="0" />
  <row AnswerCount="1" Body="&lt;p&gt;I am trying to use R to develop a corporate financial model. &lt;/p&gt;&#10;&#10;&lt;p&gt;The model includes various line items, X, of the following form with actual values for time period 1, 2.. n and projected values for periods n+1, n+2,.. n+k. g is the average growth rate for the forecast period.&lt;/p&gt;&#10;&#10;&lt;p&gt;I need to construct a vector of the following form:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;  X=c(X1,X2,...,Xn,Xn+1=(1+g)Xn,Xn+2=(1+g)Xn+1,...,Xn+k=(1+g)Xn+k-1))&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;How would I do this in R?&lt;/p&gt;&#10;&#10;&lt;p&gt;I have tried looking up the R literature on lagged variables but could not find a simple example which does what is required. I look forward to any guidance that can be given.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-04-28T16:51:00.163" FavoriteCount="1" Id="10109" LastActivityDate="2011-04-28T18:47:02.267" LastEditDate="2011-04-28T18:12:11.967" LastEditorUserId="3911" OwnerUserId="4375" PostTypeId="1" Score="0" Tags="&lt;r&gt;&lt;time-series&gt;&lt;forecasting&gt;" Title="Lagged Variables in R" ViewCount="294" />
  <row Body="&lt;p&gt;I'm not a python expert, but it is extremely helpful to plot the 1st 2 principal components against each other on the x,y axes.&lt;/p&gt;&#10;&#10;&lt;p&gt;Not sure which packages you are using, but here is a sample link:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://pyrorobotics.org/?page=PyroModuleAnalysis&quot; rel=&quot;nofollow&quot;&gt;http://pyrorobotics.org/?page=PyroModuleAnalysis&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-04-28T18:06:58.480" Id="10113" LastActivityDate="2011-04-28T18:06:58.480" OwnerUserId="3489" ParentId="9850" PostTypeId="2" Score="0" />
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;Related to an &lt;a href=&quot;http://stats.stackexchange.com/questions/10079/rules-of-thumb-for-minimum-sample-size-for-multiple-regression&quot;&gt;earlier question on power analysis for multiple regression&lt;/a&gt;, a social science researcher asked me about power analysis for &lt;a href=&quot;http://www.davidakenny.net/cm/moderation.htm&quot; rel=&quot;nofollow&quot;&gt;moderator regression&lt;/a&gt; (i.e., an interaction effect).&#10;The researcher asked me:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;I seem to recall that power of tests&#10;  for moderation with two continuous&#10;  predictor variables is low - do you know the&#10;  minimum sample size requirement in&#10;  this context?&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;From the context, it can  further be assumed that this is an observational study (not an experimental study) and that the dependent variable is continuous.&lt;/p&gt;&#10;&#10;&lt;h3&gt;Question&lt;/h3&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;What advice would you give regarding calculating the minimum sample size required?&lt;/li&gt;&#10;&lt;li&gt;Are there any caveats that you would present?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="0" CreationDate="2011-04-29T06:24:13.510" Id="10137" LastActivityDate="2011-04-29T08:34:31.240" LastEditDate="2011-04-29T07:55:17.863" LastEditorUserId="183" OwnerUserId="183" PostTypeId="1" Score="2" Tags="&lt;regression&gt;&lt;sample-size&gt;&lt;power&gt;&lt;moderator&gt;" Title="Power analysis for moderator effect in regression with two continuous predictors" ViewCount="1232" />
  <row AnswerCount="1" Body="&lt;p&gt;I have been doing multinomial logistic regression analysis using SPSS 19.&#10;I have encountered the following problem when I run the analysis procedure:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;&quot;Unexpected singularities in the&#10;  Hessian matrix are encountered. This&#10;  indicates that either some predictor&#10;  variables should be excluded or some&#10;  categories should be merged.&quot;&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;A little background about my data used. I have four categorical predictors with two levels each, 1 or 2. The response variable in my model is a three-level categorical variable. I used the last level as the reference category. I tried to compare the coefficients of the intercept with that of the four predictors in the two logits so as to find which level of the response variable may cause this problem. The big differences in coefficients between the intercept and three of the predictors suggest that it might be the reference category that has the problem. However, I could not combine the levels of the response variable (which I'm not allowed for my research). &lt;/p&gt;&#10;&#10;&lt;p&gt;I have also tried to exclude the predictors one by one, but still got the same problem.&lt;/p&gt;&#10;&#10;&lt;p&gt;Could anyone please tell me what I should do to solve this problem?&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-04-29T14:34:35.197" Id="10146" LastActivityDate="2011-06-29T08:57:38.737" LastEditDate="2011-04-29T15:23:42.813" LastEditorUserId="183" OwnerUserId="4380" PostTypeId="1" Score="8" Tags="&lt;logistic&gt;&lt;spss&gt;&lt;multinomial&gt;" Title="Unexpected singularities in the Hessian matrix error in multinomial logistic regression" ViewCount="2638" />
  
  
  <row Body="&lt;p&gt;You could try running the same model in WinBugs or OpenBugs. They generally give slightly more detailed error messages, so you might get something more useful in this specific case, too.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-05-01T00:32:48.660" Id="10189" LastActivityDate="2011-05-01T00:32:48.660" OwnerUserId="3911" ParentId="10185" PostTypeId="2" Score="1" />
  <row AcceptedAnswerId="10203" AnswerCount="2" Body="&lt;p&gt;I have a set of sessions and urls that have been accessed in each of these sessions and frequencies with which they have been accessed. I've put them in a matrix-like representation.&lt;/p&gt;&#10;&#10;&lt;p&gt;Imagine I have the following &quot;Pageview matrix&quot;:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;COLUMN HEADINGS&#10;&#10;books placement resources br aca&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Each row represents a session.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is an example of the records:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;4 5 0 2 2&#10;1 2 1 7 3&#10;1 3 6 1 6&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;saved in a &lt;code&gt;.txt&lt;/code&gt; file&lt;/p&gt;&#10;&#10;&lt;p&gt;Can I give this as an input to a k-means program and obtain clusters based on the highest frequency of occurrence? How do I use it?&lt;/p&gt;&#10;&#10;&lt;p&gt;If not k-means, what other cluster method can I use?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-05-01T05:24:17.993" FavoriteCount="2" Id="10193" LastActivityDate="2011-05-01T11:16:55.293" LastEditDate="2011-05-01T11:16:55.293" LastEditorUserId="88" OwnerUserId="4402" PostTypeId="1" Score="4" Tags="&lt;clustering&gt;" Title="Clustering elements by access counts in sessions" ViewCount="87" />
&#10;+\frac{\sum_{i}x_{i1}n_{i1}}{\sum_{i}x_{i1}^{2}}\begin{pmatrix} 1\\-1\end{pmatrix}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Now if the true slopes are parallel, so that $\beta_{2,true}=0$, then the OLS estimates will be:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\hat{\beta}_{1}=\beta_{1,true}+\frac{\sum_{i}x_{i2}n_{i2}+\sum_{i}x_{i1}n_{i1}}{\sum_{i}x_{i2}^{2}}+\frac{\sum_{i}x_{i1}n_{i1}}{\sum_{i}x_{i1}^{2}}$$&#10;$$\hat{\beta}_{2}=-\frac{\sum_{i}x_{i1}n_{i1}}{\sum_{i}x_{i1}^{2}}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Now this shows that the OLS estimate can indeed lead to erroneous interactions, just choose the &quot;true&quot; noise such that it is highly correlated with $x_{i1}$ - essentially you need to violate one of the assumptions of OLS, non-heteroscedasticity of the noise.  So if you generate data according to:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$y_{i1}=x_{i1}(\beta_{1,true}+n_{i1})$$&#10;$$y_{i2}=x_{i2}\beta_{1,true}+n_{i2}$$&lt;/p&gt;&#10;&#10;&lt;p&gt;And then try to fit an interaction model using OLS of $y$ on $x$ with an interaction, you will find a significant result, even though the true betas are the same.  The plots will cross because of the fanning in the first group.&lt;/p&gt;&#10;&#10;&lt;p&gt;One example data set (true beta is 2 and noise was generated from standard normal).  You get a t-statistic above 10 for the interaction effect:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\begin{array}{c|c}
&#10;1 &amp;amp; 2.026032115 &amp;amp; 2 \\
  
  <row AnswerCount="3" Body="&lt;p&gt;Let's say you want to cluster some objects, say documents, or sentences, or images.&lt;/p&gt;&#10;&#10;&lt;p&gt;On the technical side, you first represent these object somehow so that you could calculate distance between them, and then you feed those representations to some clustering algorithm.&lt;/p&gt;&#10;&#10;&lt;p&gt;Externally, however, you just want to group similar (in &lt;em&gt;some sense&lt;/em&gt; -- and that's where things become pretty vague for me) objects together. For example, in case of sentences we want for clusters to contain sentences about similar topic/concept; we feel that sentences &quot;oh look at this pic of a cute lolcat&quot; and &quot;facebook revealed new shiny feature tonight&quot; should be in different clusters.&lt;/p&gt;&#10;&#10;&lt;p&gt;What are the usual approaches for measuring this &quot;external&quot; quality of clustering? I.e. we want to measure how well our clustering procedure groups initial objects (sentences, images); we're not interested in internal measures (like averaged cluster radius, clusters sparseness), since those measures deal with objects' representations, not with real objects. Meaning, the chosen representation may be awful, and even if internal measures is great, externally we'll end up with clusters that are complete junk from our vague, subjective, &quot;some sense&quot;-ish point of view.&lt;/p&gt;&#10;&#10;&lt;p&gt;P.S. Having limited knowledge in clustering domain, I suspect I may be asking about really obvious thing, or my terminology may sound strange to clustering experts. If so, please advice what should I read on the subject.&lt;/p&gt;&#10;&#10;&lt;p&gt;P.P.S. Just in case, I asked the very same question on Quora: &lt;a href=&quot;http://www.quora.com/How-to-evaluate-external-quality-of-clustering&quot;&gt;http://www.quora.com/How-to-evaluate-external-quality-of-clustering&lt;/a&gt;&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-05-01T16:08:12.030" FavoriteCount="1" Id="10210" LastActivityDate="2014-03-05T15:25:35.493" LastEditDate="2011-05-01T17:16:20.670" LastEditorUserId="930" OwnerUserId="4425" PostTypeId="1" Score="5" Tags="&lt;clustering&gt;&lt;data-mining&gt;" Title="How to evaluate &quot;external&quot; quality of clustering?" ViewCount="425" />
&#10;V_n = 1 + 2 \sum_{i = 1}^n \e X_0 X_i = \left\{
&#10;\end{array}
  
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I was reading OkTrends and &lt;a href=&quot;http://blog.okcupid.com/index.php/page/3/&quot; rel=&quot;nofollow&quot;&gt;came across this&lt;/a&gt;: &lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;In fact, 32% of successful couples&#10;  agreed on all of them—which is 3.7×&#10;  the rate of simple coincidence.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;So, my question is: what is simple coincidence? How is it calculated?&lt;/p&gt;&#10;&#10;&lt;p&gt;I can't find a Wikipedia article about &lt;em&gt;simple&lt;/em&gt; coincidence, and nowhere can I find a numerical definition. So, I'm confused about how the author got &quot;3.7x&quot; the rate of &quot;simple&quot; coincidence.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-05-02T18:04:51.363" Id="10249" LastActivityDate="2013-09-04T21:17:02.627" OwnerUserId="4441" PostTypeId="1" Score="0" Tags="&lt;probability&gt;" Title="What is simple coincidence?" ViewCount="210" />
  <row Body="&lt;p&gt;If you know the domain of the random variable and maybe have knowledge of some other properties like the mean, variance, etc. but want to be ignorant in a fair way about all other aspects of the distribution, you can find a distribution by applying the &lt;a href=&quot;http://en.wikipedia.org/wiki/Principle_of_maximum_entropy&quot; rel=&quot;nofollow&quot;&gt;principle of maximum entropy&lt;/a&gt;. Or put simply&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Given a collection of facts,&#10;     choose a model which is consistent with all the facts,&#10;     but otherwise as uniform as possible.&#10;         (&lt;a href=&quot;http://www.cs.cmu.edu/afs/cs/user/aberger/www/html/tutorial/node2.html#SECTION00011000000000000000&quot; rel=&quot;nofollow&quot;&gt;Adam Berger&lt;/a&gt;)&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;Even if you don't want to derive these distributions yourself i think it's still good to know that there is such a general principle that may be used.&#10;For many common cases people have already done this and you can just look up the solution, depending on your domain and given statistics: &lt;a href=&quot;http://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution&quot; rel=&quot;nofollow&quot;&gt;Wikipedia&lt;/a&gt; lists some of them.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-05-03T00:58:11.377" Id="10254" LastActivityDate="2011-05-03T01:11:31.460" LastEditDate="2011-05-03T01:11:31.460" LastEditorUserId="4360" OwnerUserId="4360" ParentId="10220" PostTypeId="2" Score="1" />
&#10;\S = \frac{1}{n} \sum_{i=1}^n \x_i \x_i^T = \m{X}^T \m{X} / n
&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Since $\u^T \u = \|\u\|_2^2 = \|\u\| \|\u\|$, the above unconstrained problem is equivalent to the constrained problem&#10;$$
  
  
  <row AnswerCount="1" Body="&lt;p&gt;I am analyzing a large of dataset (n&gt;100) of incident rates, with the aim of forming a normal distribution. Then I will know if a future incident rate (x%) is either close to a historical mean or not, and can score/rate it accordingly with an already created formula. &lt;/p&gt;&#10;&#10;&lt;p&gt;The data is positively-skewed, as most data points cluster around or near zero percent. I HAVE to transform this data into a normal distribution, correct? What is the preferred method when dealing with percentages (these will always be between 0 and 100%)?  Are there alternative non-normalizing methods I can use to reach my desired output? &lt;/p&gt;&#10;&#10;&lt;p&gt;Anyway, let's say I've transformed the data and it follows a normal distribution. Now I can find the mean and std dev, then plot these in Excel using z-scores. Then I should be able to determine if incident rate x% is in the top 10%, top 20% of values, and score it with my formula accordingly. &lt;/p&gt;&#10;&#10;&lt;p&gt;Any problems with this method?  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-05-03T15:23:07.630" Id="10279" LastActivityDate="2011-05-03T17:17:19.583" LastEditDate="2011-05-03T17:17:19.583" LastEditorUserId="88" OwnerUserId="4450" PostTypeId="1" Score="0" Tags="&lt;normal-distribution&gt;&lt;data-transformation&gt;&lt;skewness&gt;" Title="Analyzing historical incident rates and rating future performance" ViewCount="78" />
  
  
  <row Body="&lt;p&gt;If you are at all familiar with &lt;a href=&quot;http://www.r-project.org/&quot; rel=&quot;nofollow&quot;&gt;R&lt;/a&gt; (if you're building time series models, you should be), check out the &lt;a href=&quot;http://cran.r-project.org/web/packages/forecast/index.html&quot; rel=&quot;nofollow&quot;&gt;forecast&lt;/a&gt; package. It's designed to choose parameters for Arima as well as exponential smoothing models, and uses a solid methodology to do so.  It will probably get you a lot farther than what you are building in excel, especially because it will also allow you to explore exponential smoothing models.  The two functions you are interested in are 'auto.arima' and 'ets.'&lt;/p&gt;&#10;&#10;&lt;p&gt;/Edit: auto.arima can also be used to fit ARMAX models, which (if properly specified) can solve many of the problems identified by IrishStat.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-05-03T18:23:46.203" Id="10284" LastActivityDate="2011-05-04T15:17:58.293" LastEditDate="2011-05-04T15:17:58.293" LastEditorUserId="2817" OwnerUserId="2817" ParentId="10267" PostTypeId="2" Score="3" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I am trying to put together a data-mining package for StackExchange sites and in particular, I am stuck in trying to determine the &quot;most interesting&quot; questions. I would like to use the question score, but remove the bias due to the number of views, but I don't know how to approach this rigorously.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the ideal world, I could sort the questions by calculating $\frac{v}{n}$, where $v$ is the votes total and $n$ is the number of views. After all it would measure the percentage of people that upvote the question, minus the percentage of people that downvote the question.&lt;/p&gt;&#10;&#10;&lt;p&gt;Unfortunately, the voting pattern is much more complicated. Votes tend to &quot;plateau&quot; to a certain level and this has the effect of drastically underestimating wildly popular questions. In practice, a question with 1 view and 1 upvote would certainly score and be sorted higher than any other question with 10,000 views, but less than 10,000 votes.&lt;/p&gt;&#10;&#10;&lt;p&gt;I am currently using $\frac{v}{\log{n}+1}$ as an empirical formula, but I would like to be precise. How can I approach this problem with mathematical rigorousness?&lt;/p&gt;&#10;&#10;&lt;p&gt;In order to address some of the comments, I'll try to restate the problem in a better way:&lt;/p&gt;&#10;&#10;&lt;p&gt;Let's say I have a question with $v_0$ votes total and $n_0$ views. I would like to be able to estimate what votes total $v_1$ is most likely when the views reach $n_1$.&lt;/p&gt;&#10;&#10;&lt;p&gt;In this way I could simply choose a nominal value for $n_1$ and order all the question according to the expected $v_1$ total.&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;I've created two queries on the SO datadump to show better the effect I am talking about:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://data.stackexchange.com/stackoverflow/q/99605/&quot;&gt;Average Views by Score&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Result:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/AjwgO.png&quot; alt=&quot;Views by Score&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://data.stackexchange.com/stackoverflow/q/99606/&quot;&gt;Average Score by Views (100-views buckets)&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Result:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/2dVzr.png&quot; alt=&quot;Score by Views&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;hr&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://data.stackexchange.com/stackoverflow/q/99608/&quot;&gt;The two formulas compared&lt;/a&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;Results, not sure if straighter is better: ($\frac{v}{n}$ in blue, $\frac{v}{log{n}+1}$ in red)&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/v8RVc.png&quot; alt=&quot;Formulas&quot;&gt;&lt;/p&gt;&#10;" CommentCount="14" CreationDate="2011-05-03T21:53:26.910" FavoriteCount="4" Id="10295" LastActivityDate="2011-05-05T01:16:53.857" LastEditDate="2011-05-05T00:13:04.400" LastEditorUserId="4456" OwnerDisplayName="Sklivvz" OwnerUserId="4456" PostTypeId="1" Score="20" Tags="&lt;data-mining&gt;&lt;predictive-models&gt;" Title="&quot;Interestingness&quot; function for StackExchange questions" ViewCount="315" />
  
  
  <row Body="&lt;p&gt;Judging from your equations there is no reason for the OLS estimate of $\alpha_1$ not to be consistent and asymptotically normal. So you can use plug-in estimate for $\lambda$:&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\hat{\lambda}=-\frac{1}{60}\log(1-\hat{\alpha}_1)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Using &lt;a href=&quot;http://en.wikipedia.org/wiki/Delta_method&quot; rel=&quot;nofollow&quot;&gt;delta method&lt;/a&gt; it would be possible to show that this estimate is also consistent and  asymptotically normal. The only caveat is that the estimate of $\alpha_1$ can lie outside the interval $[0,1]$, but this would indicate that your postulated model is incorrect.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-05-04T07:20:52.663" Id="10304" LastActivityDate="2011-05-04T07:20:52.663" OwnerUserId="2116" ParentId="10303" PostTypeId="2" Score="5" />
  <row Body="&lt;p&gt;checkout Gephi, this software has some very good layout algorithms to handle the spaghetti problem: &lt;a href=&quot;http://gephi.org/features/&quot;&gt;http://gephi.org/features/&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Especially, try the ForceAtlas layout: &lt;a href=&quot;http://forum.gephi.org/viewtopic.php?f=26&amp;amp;t=926&quot;&gt;http://forum.gephi.org/viewtopic.php?f=26&amp;amp;t=926&lt;/a&gt;&#10;The software let you control the parameters in real time, and you can move the nodes manually.&lt;/p&gt;&#10;&#10;&lt;p&gt;(disclamer: I'm part of this community)&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-05-04T13:50:39.353" Id="10314" LastActivityDate="2011-05-04T13:50:39.353" OwnerUserId="4443" ParentId="9040" PostTypeId="2" Score="5" />
  <row AcceptedAnswerId="10327" AnswerCount="5" Body="&lt;p&gt;My girlfriend is an Actuarial Analyst at a large insurance company in the Netherlands and because we'll soon have our two year anniversary, I thought of gifts for her.&lt;/p&gt;&#10;&#10;&lt;p&gt;On &lt;a href=&quot;http://proofmathisbeautiful.tumblr.com/post/5104877044/baseln-statistical-distribution-plushies-am&quot; rel=&quot;nofollow&quot;&gt;Proof: Math is beautiful&lt;/a&gt; I discovered these &lt;a href=&quot;http://www.etsy.com/listing/71739287/collection-of-10-distribution-plushies&quot; rel=&quot;nofollow&quot;&gt;Distribution pluffies&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;So here's my question: What distribution is of the most relevance in the field of an econometrician?&lt;/p&gt;&#10;&#10;&lt;p&gt;The available pluffies are:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Standard Normal Distribution&lt;/li&gt;&#10;&lt;li&gt;t Distribution&lt;/li&gt;&#10;&lt;li&gt;Chi-Square Distribution&lt;/li&gt;&#10;&lt;li&gt;Log Normal Distribution&lt;/li&gt;&#10;&lt;li&gt;Continuous Uniform Distribution&lt;/li&gt;&#10;&lt;li&gt;Weibull Distribution&lt;/li&gt;&#10;&lt;li&gt;Cauchy Distribution&lt;/li&gt;&#10;&lt;li&gt;Poisson Distribution&lt;/li&gt;&#10;&lt;li&gt;Gumbel Distribution&lt;/li&gt;&#10;&lt;li&gt;Erlang Distribution&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;Any help much appreciated.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; Thanks a lot for all the suggestions despite this being just off-topic! I'll get her the t Distribution pluffy.&lt;/p&gt;&#10;" ClosedDate="2013-05-06T21:42:10.240" CommentCount="2" CommunityOwnedDate="2011-05-04T20:12:48.133" CreationDate="2011-05-04T18:25:56.640" FavoriteCount="1" Id="10322" LastActivityDate="2013-05-06T21:09:32.367" LastEditDate="2013-05-06T21:09:32.367" LastEditorUserId="25315" OwnerUserId="4468" PostTypeId="1" Score="4" Tags="&lt;distributions&gt;" Title="What distribution pluffy to buy for an aspiring econometrician?" ViewCount="287" />
  <row AcceptedAnswerId="10333" AnswerCount="1" Body="&lt;p&gt;What functionality should exist in a &lt;a href=&quot;http://en.wikipedia.org/wiki/Computer_algebra_system&quot; rel=&quot;nofollow&quot;&gt;CAS&lt;/a&gt; that was specifically geared toward Statistics?&lt;/p&gt;&#10;&#10;&lt;p&gt;Symbolic algebra systems like Mathematica and Maple are often used for calculus, logic, and physics problems but are rarely used for statistics. Why is this?&lt;/p&gt;&#10;&#10;&lt;p&gt;What statistical constructs could be added to a symbolic algebra system to improve its use in this field? What are some specific code samples that many people would like to be able to do. &lt;/p&gt;&#10;&#10;&lt;p&gt;Please think about the following three users: research statistician, non-statistics researcher using statistics in another field (such as biology), statistics student. &lt;/p&gt;&#10;&#10;&lt;p&gt;I'll be working on &lt;a href=&quot;http://sympy.org/&quot; rel=&quot;nofollow&quot;&gt;SymPy's&lt;/a&gt; statistics code over the next few months and would like to solicit input for desired functionality. The things I use are not necessarily what the broader community uses. &lt;/p&gt;&#10;" CommentCount="5" CommunityOwnedDate="2011-05-04T20:45:04.117" CreationDate="2011-05-04T20:32:27.960" FavoriteCount="1" Id="10329" LastActivityDate="2011-05-06T04:12:27.167" LastEditDate="2011-05-06T04:12:27.167" LastEditorUserId="3830" OwnerUserId="3830" PostTypeId="1" Score="7" Tags="&lt;python&gt;&lt;computing&gt;&lt;computational-statistics&gt;" Title="Symbolic computer algebra for statistics" ViewCount="271" />
  
  
  <row Body="&lt;p&gt;From your independence statement,&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{equation} &#10;P( A_{+} \bigcap B_{+} ) = P(A_{+} ) P(B_{+})&#10;\nonumber&#10;\end{equation}&#10;and the definitions&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{equation} &#10;P(A_{+} \bigcap B_{+} | c) \equiv \frac{P(A_{+} \bigcap B_{+} \bigcap c)}{P(c)}&#10;\nonumber&#10;\end{equation}&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{equation}&#10;P(A_{+} | c) \equiv  \frac{P(A_{+} \bigcap c)}{P(c)}&#10;\nonumber&#10;\end{equation}&#10;and&lt;/p&gt;&#10;&#10;&lt;p&gt;\begin{equation}&#10;P(B_{+} | c) \equiv  \frac{P(B_{+} \bigcap c)}{P(c)}&#10;\nonumber&#10;\end{equation}&#10;can you algebraically show that&#10;\begin{equation} &#10;P(A_{+} \bigcap B_{+} | c) \equiv  \frac{P(A_{+} \bigcap B_{+} \bigcap c)}{P(c)}&#10; = \frac{P(A_{+} \bigcap c)}{P(c)} \frac{P(B_{+} \bigcap c)}{P(c)} \equiv   P(A_{+} | c) P(B_{+} | c)&#10;\nonumber&#10;\end{equation}&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-05-05T18:09:20.020" Id="10364" LastActivityDate="2011-05-05T18:09:20.020" OwnerUserId="3805" ParentId="10361" PostTypeId="2" Score="0" />
  
  <row Body="&lt;p&gt;Another one from &lt;a href=&quot;http://xkcd.org/892/&quot;&gt;xkcd&lt;/a&gt;:&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/hN4lW.png&quot; alt=&quot;Hell, my eighth grade science class managed to conclusively reject it just based on a classroom experiment. It's pretty sad to hear about million-dollar research teams who can't even manage that.&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;Alt-text:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Hell, my eighth grade science class managed to conclusively reject it just based on a classroom experiment. It's pretty sad to hear about million-dollar research teams who can't even manage that.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;" CommentCount="1" CommunityOwnedDate="2011-05-06T12:13:40.790" CreationDate="2011-05-06T12:13:40.790" Id="10403" LastActivityDate="2013-08-21T01:39:08.423" LastEditDate="2013-08-21T01:39:08.423" LastEditorUserId="9007" OwnerUserId="565" ParentId="423" PostTypeId="2" Score="43" />
  
  <row Body="&lt;p&gt;In my opinion , you might need to add/detect day-of-the-week ; week-of-the-year ; Holiday effects ( lead ,contemporaneous and lag effects ); possible level shifts and or Local Time Trends ; Possible fixed-days-of-the-month; Pulses/Outlier correction via Intervention Detection schemes ; AND then INTRODUCE a set of possible predictors (K-1) reflecting EACH of the K algorithmic change effects. Additionally you might need to incorporate an ARMA component to your predictors to render your error process Gaussian. Care should also be taken to ensure that the parameters of your final model did not significantly change over time and that the residuals from your final model have constant variance. Heterogenous error variance can be caused by structural changes in variance at particular time points , coupling of the error dispersion and the level of the output series and/or a pure stochastic variance change over time.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-05-06T13:02:33.687" Id="10406" LastActivityDate="2011-05-06T13:02:33.687" OwnerUserId="3382" ParentId="10096" PostTypeId="2" Score="2" />
  <row AcceptedAnswerId="10830" AnswerCount="3" Body="&lt;p&gt;Repeating an experiment with $n$ possible outcomes $t$ times independently, where all but one outcomes have probability $\frac{1}{n+1}$ and the other outcome has the double probability $\frac{2}{n+1}$, is there a good approximate formula for the probability that the outcome with the higher probability happens more often than any other one?&lt;/p&gt;&#10;&#10;&lt;p&gt;For me, $n$ is typically some hundreds, and $t$ is chosen depending on $n$ such that the probability that the most likely outcome occurs most often is between 10% and 99.999%.&lt;/p&gt;&#10;&#10;&lt;p&gt;In the moment I use a small program that calculates a crude approximation by assuming that the counts for how often each outcome shows up in $t$ trials are independent and approximate the counts using the Poisson distribution. How can I improve on this?&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; I'd strongly appreciate comments/votes on the two (maybe soon more) answers given.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT 2:&lt;/strong&gt; As none of the two answers is convincing me, but as I don't want to let the 100 points bounty to vanish (and as nobody voted for/against one of the two answers), I'll just pick one of the answers. I'd still appreciate other answers.&lt;/p&gt;&#10;" CommentCount="22" CreationDate="2011-05-06T13:16:53.577" FavoriteCount="1" Id="10407" LastActivityDate="2011-05-15T19:28:58.837" LastEditDate="2011-05-15T19:28:58.837" LastEditorUserId="919" OwnerUserId="565" PostTypeId="1" Score="6" Tags="&lt;probability&gt;&lt;approximation&gt;" Title="Probability for finding a double-as-likely event" ViewCount="295" />
  
  
  <row Body="&lt;p&gt;absolutely agreed to what Matt said, first you have to think about the background of the data... It doesn't make any sense to fit ZI models, when there are no Zero generating triggers in the population! The advantage of NB models are that they can display unobserved heterogenity in a gamma distributed random variable. Techniqually: The main reasons for overdispersion are unobs Heterogenity and Zero inflation. I do not believe that your fit is bad. Btw to get the goodness of fit you should always compare the Deviance with the degrees of freedom of your model. If the Deviance D is higher than n-(p+1) (this is df) than you should search a better model. Although there are mostly no better models than ZINB to get rid of overdispersion.   &lt;/p&gt;&#10;&#10;&lt;p&gt;if you want to fit a ZINB with R, get the package &lt;code&gt;pscl&lt;/code&gt; and try to use the command &lt;code&gt;zeroinfl(&amp;lt;model&amp;gt;, dist=negative)&lt;/code&gt;. For further information see &lt;code&gt;?zeroinfl&lt;/code&gt; after loading the required package!&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-05-06T19:48:18.143" Id="10435" LastActivityDate="2011-05-06T19:48:18.143" OwnerUserId="4496" ParentId="7535" PostTypeId="2" Score="2" />
  <row Body="&lt;p&gt;The 0-4 years group refers to the following age interval: $0 \leq x &amp;lt; 5$, i.e. a child which is 4 years and 364 days old still belongs to this group. So, let's compute the midpoint for that range:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; ((365+365+365+365+364)/2)/365&#10;[1] 2.49863  &#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="0" CreationDate="2011-05-06T19:53:46.040" Id="10436" LastActivityDate="2011-05-06T19:53:46.040" OwnerUserId="307" ParentId="10433" PostTypeId="2" Score="4" />
  <row AcceptedAnswerId="10453" AnswerCount="1" Body="&lt;p&gt;Suppose I have a quadratic regression model &#10;$$
  
  
  <row Body="&lt;p&gt;The formula is fairly straightforward if all the sub-sample have the same sample size. If you had $g$ sub-samples of size $k$ (for a total of $gk$ samples), then the variance of the combined sample depends on the mean $E_j$ and variance $V_j$ of each sub-sample: &#10;$$ Var(X_1,\ldots,X_{gk}) = \frac{k-1}{gk-1}(\sum_{j=1}^g V_j + \frac{k(g-1)}{k-1} Var(E_j)),$$ where by $Var(E_j)$ means the variance of the sample means.&lt;/p&gt;&#10;&#10;&lt;p&gt;A demonstration in R:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; x &amp;lt;- rnorm(100)&#10;&amp;gt; g &amp;lt;- gl(10,10)&#10;&amp;gt; mns &amp;lt;- tapply(x, g, mean)&#10;&amp;gt; vs &amp;lt;- tapply(x, g, var)&#10;&amp;gt; 9/99*(sum(vs) + 10*var(mns))&#10;[1] 1.033749&#10;&amp;gt; var(x)&#10;[1] 1.033749&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;If the sample sizes are not equal, the formula is not so nice.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;EDIT: formula for unequal sample sizes&lt;/strong&gt; &lt;/p&gt;&#10;&#10;&lt;p&gt;If there are $g$ sub-samples, each with $k_j, j=1,\ldots,g$ elements for a total of $n=\sum{k_j}$ values, then&#10;$$ Var(X_1,\ldots,X_{n}) = \frac{1}{n-1}\left(\sum_{j=1}^g (k_j-1) V_j + \sum_{j=1}^g k_j (\bar{X}_j - \bar{X})^2\right), $$&#10;where $\bar{X} = (\sum_{j=1}^gk_j\bar{X}_j)/n$ is the weighted average of all the means (and equals to the mean of all values).&lt;/p&gt;&#10;&#10;&lt;p&gt;Again, a demonstration:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; k &amp;lt;- rpois(10, lambda=10)&#10;&amp;gt; n &amp;lt;- sum(k)&#10;&amp;gt; g &amp;lt;- factor(rep(1:10, k))&#10;&amp;gt; x &amp;lt;- rnorm(n)&#10;&amp;gt; mns &amp;lt;- tapply(x, g, mean)&#10;&amp;gt; vs &amp;lt;- tapply(x, g, var)&#10;&amp;gt; 1/(n-1)*(sum((k-1)*vs) + sum(k*(mns-weighted.mean(mns,k))^2))&#10;[1] 1.108966&#10;&amp;gt; var(x)&#10;[1] 1.108966&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;By the way, these formulas are easy to derive by writing the desired variance as the scaled sum of $(X_{ji}-\bar{X})^2$, then introducing $\bar{X}_j$:  $[(X_{ji}-\bar{X}_j)-(\bar{X}_j-\bar{X})]^2$, using the square of difference formula, and simplifying.&lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-05-06T20:50:07.443" Id="10445" LastActivityDate="2011-05-10T16:34:37.680" LastEditDate="2011-05-10T16:34:37.680" LastEditorUserId="4499" OwnerUserId="279" ParentId="10441" PostTypeId="2" Score="7" />
  
  
  
  <row Body="&lt;p&gt;I can't see that standardization is a good idea in ordinary regression or with a longitudinal model.  It makes predictions harder to obtain and doesn't solve a problem that needs solving, usually.  And what if you have $x$ and $x^2$ in the model.  How do you standardize $x^2$?  What if you have a continuous variable and a binary variable in the model?  How do you standardize the binary variable?  Certainly not by its standard deviation, which would cause low prevalence variables to have greater importance.&lt;/p&gt;&#10;&#10;&lt;p&gt;In general it's best to interpret model effects on the original scale of $x$.&lt;/p&gt;&#10;" CommentCount="10" CreationDate="2011-05-07T13:23:49.177" Id="10462" LastActivityDate="2011-05-07T13:23:49.177" OwnerUserId="4253" ParentId="10444" PostTypeId="2" Score="7" />
  
  
  
  
  
  
  
  
  
  
  <row Body="&lt;p&gt;One of my absolutely most valuable books over the years has been &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0126913609&quot; rel=&quot;nofollow&quot;&gt;Tinsley and Brown's Handbook&lt;/a&gt;.  There are many places in the book where this topic is discussed, by different contributing authors.&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-05-08T18:51:15.643" Id="10516" LastActivityDate="2011-05-08T18:51:15.643" OwnerUserId="2669" ParentId="10423" PostTypeId="2" Score="1" />
  
  
  <row Body="&lt;p&gt;That's an answer for question 2.&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;STL: &lt;a href=&quot;http://cs.wellesley.edu/~cs315/Papers/stl%20statistical%20model.pdf&quot; rel=&quot;nofollow&quot;&gt;http://cs.wellesley.edu/~cs315/Papers/stl%20statistical%20model.pdf&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;X-12-ARIMA (and much more): &lt;a href=&quot;http://www.census.gov/srd/www/sapaper/sapaper.html&quot; rel=&quot;nofollow&quot;&gt;http://www.census.gov/srd/www/sapaper/sapaper.html&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;" CommentCount="1" CreationDate="2011-05-09T08:05:36.423" Id="10542" LastActivityDate="2011-05-09T08:05:36.423" OwnerUserId="1709" ParentId="9198" PostTypeId="2" Score="3" />
  <row Body="&lt;p&gt;Do your analysis with several different kernels.  Make sure you cross-validate. Choose the kernel that performs the best during cross-validation and fit it to your whole dataset.&lt;/p&gt;&#10;&#10;&lt;p&gt;/edit: Here is some example code in R, for a classification SVM:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;#Use a support vector machine to predict iris species&#10;library(caret)&#10;library(caTools)&#10;&#10;#Choose x and y&#10;x &amp;lt;- iris[,c(&quot;Sepal.Length&quot;,&quot;Sepal.Width&quot;,&quot;Petal.Length&quot;,&quot;Petal.Width&quot;)]&#10;y &amp;lt;- iris$Species&#10;&#10;#Pre-Compute CV folds so we can use the same ones for all models&#10;CV_Folds &amp;lt;- createMultiFolds(y, k = 10, times = 5)&#10;&#10;#Fit a Linear SVM&#10;L_model &amp;lt;- train(x,y,method=&quot;svmLinear&quot;,tuneLength=5,&#10;    trControl=trainControl(method='repeatedCV',index=CV_Folds))&#10;&#10;#Fit a Poly SVM&#10;P_model &amp;lt;- train(x,y,method=&quot;svmPoly&quot;,tuneLength=5,&#10;    trControl=trainControl(method='repeatedCV',index=CV_Folds))&#10;&#10;#Fit a Radial SVM&#10;R_model &amp;lt;- train(x,y,method=&quot;svmRadial&quot;,tuneLength=5,&#10;    trControl=trainControl(method='repeatedCV',index=CV_Folds))&#10;&#10;#Compare 3 models:&#10;resamps &amp;lt;- resamples(list(Linear = L_model, Poly = P_model, Radial = R_model))&#10;summary(resamps)&#10;bwplot(resamps, metric = &quot;Accuracy&quot;)&#10;densityplot(resamps, metric = &quot;Accuracy&quot;)&#10;&#10;#Test a model's predictive accuracy Using Area under the ROC curve&#10;#Ideally, this should be done with a SEPERATE test set&#10;pSpecies &amp;lt;- predict(L_model,x,type='prob')&#10;colAUC(pSpecies,y,plot=TRUE)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="4" CreationDate="2011-05-09T14:21:32.587" Id="10552" LastActivityDate="2011-05-09T18:16:21.190" LastEditDate="2011-05-09T18:16:21.190" LastEditorUserId="2817" OwnerUserId="2817" ParentId="10551" PostTypeId="2" Score="8" />
  
  <row AcceptedAnswerId="14073" AnswerCount="3" Body="&lt;p&gt;I have a large distance matrix $3400\times 3400$.&lt;/p&gt;&#10;&#10;&lt;p&gt;I need to cluster them hierarchically and then cut the tree into clusters (like a partitional approach). &lt;/p&gt;&#10;&#10;&lt;p&gt;Which algorithm is most sensitive to finding natural clusters in the data based on the distance matrix?&lt;/p&gt;&#10;&#10;&lt;p&gt;How can I evaluate the result? I am planning on using average silhouette coefficient of the tree at various levels to identify the 'natural' clusters from the tree.&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-05-09T17:16:08.777" FavoriteCount="1" Id="10562" LastActivityDate="2011-08-10T05:20:23.550" LastEditDate="2011-05-11T09:54:17.467" LastEditorUserId="930" OwnerUserId="4534" PostTypeId="1" Score="6" Tags="&lt;clustering&gt;" Title="Which hierarchical clustering algorithm?" ViewCount="787" />
  <row Body="&lt;p&gt;I'm sure that Durrett's proof is nice. A straight forward solution to the question asked is as follows.&lt;/p&gt;&#10;&#10;&lt;p&gt;For $n \geq 1$ &lt;/p&gt;&#10;&#10;&lt;p&gt;$$
&#10;&amp;amp; = &amp;amp; \int_0^t P(T_{n+1} &amp;gt; t-s) P(S_n \in ds) \\
  <row AnswerCount="2" Body="&lt;p&gt;I am not comfortable with Fisher information, what it measures and how and how is it helpful. Also it's relationship with the Cramer-Rao bound is not apparent to me.&lt;/p&gt;&#10;&#10;&lt;p&gt;Can someone please give an intuitive explanation of these concepts?&lt;/p&gt;&#10;" CommentCount="6" CreationDate="2011-05-09T20:43:10.830" FavoriteCount="18" Id="10578" LastActivityDate="2011-05-10T15:09:16.277" LastEditDate="2011-05-10T05:53:19.687" LastEditorUserId="88" OwnerUserId="4101" PostTypeId="1" Score="25" Tags="&lt;estimation&gt;" Title="Intuitive explanation of Fisher Information and Cramer-Rao bound" ViewCount="4756" />
  
  
  <row Body="&lt;p&gt;Here I explain why the asymptotic variance of the &lt;em&gt;maximum likelihood estimator&lt;/em&gt; is the Cramer-Rao lower bound.  Hopefully this will provide some insight as to the relevance of the Fisher information.&lt;/p&gt;&#10;&#10;&lt;p&gt;Statistical inference proceeds with the use of a likelihood function $\mathcal{L}(\theta)$ which you construct from the data.  The point estimate $\hat{\theta}$ is the value which maximizes $\mathcal{L}(\theta)$. The estimator $\hat{\theta}$ is a random variable, but it helps to realize that the &lt;em&gt;likelihood function&lt;/em&gt; $\mathcal{L}(\theta)$ is a &quot;random curve&quot;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here we assume iid data drawn from a distribution $f(x|\theta)$, and we define the likelihood&#10;$$
  
  <row Body="&lt;p&gt;This seems like a classic problem for &lt;a href=&quot;http://en.wikipedia.org/wiki/Logistic_regression&quot; rel=&quot;nofollow&quot;&gt;logistic regression&lt;/a&gt;.  Rather than specifying these groups, turn math and business coursework into predictors for employment status.  This is easy to code up in pretty much whatever statistical software you have around, although you'll want to look into different contrasts (it sounds like you'd want to use treatment contrasts with &quot;no class&quot; set as the reference level).&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-05-09T23:28:35.240" Id="10582" LastActivityDate="2011-05-09T23:28:35.240" OwnerUserId="71" ParentId="10579" PostTypeId="2" Score="2" />
  
  
  
  <row AnswerCount="0" Body="&lt;p&gt;Consider $\boldsymbol{x}= [x_1,x_2,...x_n]$ and $\boldsymbol{y}= [y_1,y_2,...y_n]$ to be two multivariate Gaussians with an isotropic diagonal variance structure and uninformative priors so that:&lt;/p&gt;&#10;&#10;&lt;p&gt;$p(x_i,\mu_{x_i},\sigma_x) = \frac{e^{\frac{-(x_i-\mu_{x_i})^2}{2\sigma_x^2}}}{\sqrt{2\pi \sigma_x}}$&lt;/p&gt;&#10;&#10;&lt;p&gt;and,&lt;/p&gt;&#10;&#10;&lt;p&gt;$p(y_i,\mu_{y_i},\sigma_y) = \frac{e^{\frac{-(y_i-\mu_{y_i})^2}{2\sigma_y^2}}}{\sqrt{2\pi \sigma_y}}$&lt;/p&gt;&#10;&#10;&lt;p&gt;where $\sigma_x$ is a scalar, constant for all $i$, $\sigma_y$ is a scalar, constant for all $i$; and $\mu_{x_i}$ and $\mu_{y_i}$ are scalars for each $i$. We can define vectors $\boldsymbol{\mu_x} = [\mu_{x_1},...\mu_{x_n}]$, and $\boldsymbol{\mu_y} = [\mu_{y_1},...\mu_{y_n}]$. For simplicity, let us assume that we have unnormalized uninformative uniform priors of all $\mu$ and $\sigma$ so that,&lt;/p&gt;&#10;&#10;&lt;p&gt;$p(\boldsymbol{x},\boldsymbol{\mu_x},\sigma_x) =\Pi_i p(x_i,\mu_{x_i},\sigma_x) = \Pi_i \frac{e^{\frac{-(x_i-\mu_{x_i})^2}{2\sigma_x^2}}}{\sqrt{2\pi \sigma_x}}$&#10;and&#10;$p(\boldsymbol{y},\boldsymbol{\mu_y},\sigma_y) =\Pi_i p(x_i,\mu_{x_i},\sigma_x) = \Pi_i \frac{e^{\frac{-(y_i-\mu_{y_i})^2}{2\sigma_y^2}}}{\sqrt{2\pi \sigma_y}}$&lt;/p&gt;&#10;&#10;&lt;p&gt;Given this, can I evaluate&lt;/p&gt;&#10;&#10;&lt;p&gt;$p(\boldsymbol{\mu_y} \propto \boldsymbol{\mu_x}|\boldsymbol{x},\boldsymbol{y})$&lt;/p&gt;&#10;&#10;&lt;p&gt;i.e.,&#10;$p(\boldsymbol{\mu_y}=m\boldsymbol{\mu_x}+c|\boldsymbol{x},\boldsymbol{y})$&#10;for any scalars $m$ and $c$?&lt;/p&gt;&#10;&#10;&lt;p&gt;Just to explain what I mean, since we already have expressions for $p(\boldsymbol{x},\boldsymbol{\mu_x},\sigma_x)$ and $p(\boldsymbol{y},\boldsymbol{\mu_y},\sigma_y)$, I   am interested in&lt;/p&gt;&#10;&#10;&lt;p&gt;$\frac{\int p(\boldsymbol{x},\boldsymbol{\mu_x},\sigma_x)p(\boldsymbol{y},\boldsymbol{\mu_y},\sigma_y)\mathbf{1}_{[\frac{\mu_{y_1}-\bar{\mu_y}}{\mu_{x_1}-\bar{\mu_x}}=\frac{\mu_{y_2}-\bar{\mu_y}}{\mu_{x_2}-\bar{\mu_x}}=...=\frac{\mu_{y_n}-\bar{\mu_y}}{\mu_{x_n}-\bar{\mu_x}}]}d\sigma_x d\sigma_y d\boldsymbol{\mu_x} d\boldsymbol{\mu_y}}
  <row AcceptedAnswerId="10686" AnswerCount="2" Body="&lt;p&gt;I'm measuring distances of various samples from a reference point. The distance is defined as a non-negative number, where $d=0$ means that the test case is identical to the reference. &lt;/p&gt;&#10;&#10;&lt;p&gt;My general question is: Given a set  of &quot;typical&quot; distances, what is the proper way to tell whether a given $d_1$ &quot;too large&quot;, compared to the &quot;typical&quot;?&lt;/p&gt;&#10;&#10;&lt;p&gt;In my particular case the distance distribution is shown on the following graph&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/BcFYJ.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;I failed to transform these data to anything symmetrical, so I can't use normal approximation. Any suggestions?&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-05-11T09:01:45.637" Id="10643" LastActivityDate="2011-05-11T23:04:01.823" LastEditDate="2011-05-11T09:50:04.673" LastEditorUserId="930" OwnerUserId="1496" PostTypeId="1" Score="2" Tags="&lt;distributions&gt;&lt;hypothesis-testing&gt;&lt;distance-functions&gt;" Title="How to properly analyze distance from a reference?" ViewCount="162" />
  
  
  <row Body="&lt;p&gt;'The' percentile for a given age implies some sort of regression (i.e. you can find 'the' mean predicted height from a given age).&lt;/p&gt;&#10;&#10;&lt;p&gt;Once you have found this, the result depends on your assumptions: if you want no assumptions (besides the regression's), find how many of the heights in your original data are smaller than the predicted one for your age (=use empirical distribution).&lt;/p&gt;&#10;&#10;&lt;p&gt;Otherwise, you can fit whatever model you like to the marginal distribution of height and then find the percentile of the predicted height in that distribution.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-05-11T14:18:39.307" Id="10659" LastActivityDate="2011-05-11T14:18:39.307" OwnerUserId="4257" ParentId="10655" PostTypeId="2" Score="1" />
  
  <row Body="&lt;p&gt;Here is a sketch of a proof which combines three ideas: &lt;strong&gt;(a)&lt;/strong&gt; the delta method, &lt;strong&gt;(b)&lt;/strong&gt; variance-stabilization transformations and &lt;strong&gt;(c)&lt;/strong&gt; the closure of the Poisson distribution under independent sums.&lt;/p&gt;&#10;&#10;&lt;p&gt;First, let's consider a &lt;em&gt;sequence&lt;/em&gt; of iid Poisson random variables $X_1, X_2, \ldots$ with mean $\lambda &amp;gt; 0$. Then, the Central Limit Theorem asserts that&#10;$$\newcommand{\barX}{\bar{X}_n}\newcommand{\convd}{\,\xrightarrow{\,d\,}\,}\newcommand{\Nml}{\mathcal{N}}
  
  <row Body="&lt;p&gt;Cluster analysis does not involve hypothesis testing per se, but is really just a collection of different similarity algorithms for exploratory analysis. You can force hypothesis testing somewhat but the results are often inconsistent, since cluster changes are very sensitive to changes in parameters. So the answer is yes, you can do it, but be careful about making specific statistical inference.&lt;/p&gt;&#10;&#10;&lt;p&gt;As to your other point. As in any data analysis, your data should be as clean and as representative as possible, so I would avoid jumping steps.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-05-11T20:07:20.180" Id="10673" LastActivityDate="2011-05-11T20:07:20.180" OwnerUserId="3489" ParentId="10657" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;Unless the people in the room are a random sample of the world's population, any conclusions based on statistics about the world's population are going to be very suspect.  One out of every 5 people in the world is Chinese, but none of my five children are...&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-05-11T17:56:10.473" Id="10681" LastActivityDate="2011-05-11T17:56:10.473" OwnerDisplayName="Robert Israel" ParentId="10680" PostTypeId="2" Score="7" />
  
  
  
&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;for $0 &amp;lt; p &amp;lt; 1$.&lt;/p&gt;&#10;&#10;&lt;p&gt;Taking $p = 1/2$, we calculate&lt;/p&gt;&#10;&#10;&lt;p&gt;$$f(x_A) = 2\sqrt{23} \approx 9.6$$&#10;$$f(x_B) = 2\sqrt{15} \approx 7.7$$&#10;$$f(x_C) = 3\sqrt{10} \approx 9.5$$&lt;/p&gt;&#10;&#10;&lt;p&gt;According to the $1/2$-norm, User A would be considered the most well-rounded of the three, by a narrow margin over User C.&lt;/p&gt;&#10;&#10;&lt;p&gt;Another choice for $f$ is the (scaled) &lt;em&gt;Shannon entropy&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;$$
  
  
  <row Body="&lt;p&gt;In theory (with $0 \lt p \lt 1$) it means the point a fraction $p$ up the cumulative distribution.  In practice there are various definitions used, particularly in statistical computing.  For example in R there are &lt;a href=&quot;http://stat.ethz.ch/R-manual/R-devel/library/stats/html/quantile.html&quot;&gt;nine different definitions&lt;/a&gt;, the first three for a discrete interpretation and the rest for a variety of continuous interpolations.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is an example: if your sample is {400, 1, 1000, 40}, and you are looking for the 0.6 quantile (60th centile) then the different calculation methods give &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;&amp;gt; for (t in 1:9) { x[t] &amp;lt;- quantile(c(400, 1, 1000, 40), probs=0.6, type = t ) }&#10;&amp;gt; x&#10;60%                                 &#10;400 400  40 184 364 400 328 376 373 &#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;My personal view is that the correct figure is that 400 is correct since &#10;$$Pr(X&amp;lt;400) = 0.5 &amp;lt; 0.6 \text{ and } Pr(X&amp;gt;400) = 0.25 &amp;lt; 1-0.6.$$  This comes from treating the sample as the population, and if the empirical CDF is drawn it will be a sequence of steps. There are opposing arguments for interpolating so the empirical CDF is continuous, as being likely to be a better or more useful approximation to the population, and the method of interpolation will affect the result.    &lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-05-12T18:51:03.867" Id="10732" LastActivityDate="2011-05-12T18:51:03.867" OwnerUserId="2958" ParentId="10728" PostTypeId="2" Score="9" />
  
  
  
  <row Body="&lt;p&gt;One option is to fit a series of ARMA models with combinations of $p$ and $q$ and work with the model that has the best &quot;fit&quot;. Here I evaluate &quot;fit&quot; using BIC to attempt to penalise overly complex fits. An example is shown below for the in-built Mauna Loa $\mathrm{CO}_2$ concentration data set&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;## load the data&#10;data(co2)&#10;## take only data up to end of 1990 - predict for remaining data later&#10;CO2 &amp;lt;- window(co2, end = c(1990, 12))&#10;&#10;## Set up the parameter sets over which we want to operate&#10;CO2.pars &amp;lt;- expand.grid(ar = 0:2, diff = 1, ma = 0:2, sar = 0:1,&#10;                        sdiff = 1, sma = 0:1)&#10;## As you are only wanting ARMA, then you would need something like&#10;## pars &amp;lt;- expand.grid(ar = 0:4, diff = 0, ma = 0:4)&#10;## and where you choose the upper and lower limits - here 0 and 4&#10;&#10;## A vector to hold the BIC values for each combination of model&#10;CO2.bic &amp;lt;- rep(0, nrow(CO2.pars))&#10;&#10;## loop over the combinations, fitting an ARIMA model and recording the BIC&#10;## for that model. Note we use AIC() with extra penalty given by `k`&#10;for (i in seq(along = CO2.bic)) {&#10;    CO2.bic[i] &amp;lt;- AIC(arima(CO2, unlist(CO2.pars[i, 1:3]), &#10;                            unlist(CO2.pars[i, 4:6])),&#10;                      k = log(length(CO2)))&#10;}&#10;&#10;## identify the model with lowest BIC&#10;CO2.pars[which.min(CO2.bic), ]&#10;&#10;## Refit the model with lowest BIC&#10;CO2.mod &amp;lt;- arima(CO2, order = c(0, 1, 1), seasonal = c(0, 1, 1))&#10;CO2.mod&#10;## Diagnostics plots&#10;tsdiag(CO2.mod, gof.lag = 36)&#10;&#10;## predict for the most recent data&#10;pred &amp;lt;- predict(CO2.mod, n.ahead = 7 * 12)&#10;upr &amp;lt;- pred$pred + (2 * pred$se) ## upper and lower confidence intervals&#10;lwr &amp;lt;- pred$pred - (2 * pred$se) ## approximate 95% pointwise&#10;&#10;## plot what we have done&#10;ylim &amp;lt;- range(co2, upr, lwr)&#10;plot(co2, ylab = ylab, main = expression(bold(Mauna ~ Loa ~ CO[2])),&#10;     xlab = &quot;Year&quot;, ylim = ylim)&#10;lines(pred$pred, col = &quot;red&quot;)&#10;lines(upr, col = &quot;red&quot;, lty = 2)&#10;lines(lwr, col = &quot;red&quot;, lty = 2)&#10;legend(&quot;topleft&quot;, legend = c(&quot;Observed&quot;, &quot;Predicted&quot;, &quot;95% CI&quot;),&#10;       col = c(&quot;black&quot;, &quot;red&quot;, &quot;red&quot;), lty = c(1, 1, 2), bty = &quot;n&quot;)&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="7" CreationDate="2011-05-13T07:17:44.377" Id="10752" LastActivityDate="2011-05-13T07:17:44.377" OwnerUserId="1390" ParentId="10750" PostTypeId="2" Score="6" />
  
&#10;A:\text{out of the set of K models being considered, one of them is the best}
&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;And then continue to assign the same probability models (same parameters, same data, same approximations, etc.), I will get the same set of BIC values.  It is only by attaching some sort of unique meaning to the logical letter &quot;M&quot; that one gets drawn into irrelevant questions about &quot;the true model&quot; (echoes of &quot;the true religion&quot;).  The only thing that &quot;defines&quot; M is the mathematical equations which use it in their calculations - and this is hardly ever singles out one and only one definition.  I could equally put in a prediction proposition about M (&quot;the ith model will give the best predictions&quot;).  I personally can't see how this would change any of the likelihoods, and hence how good or bad BIC will be (AIC for that matter as well - although AIC is based on a different derivation)&lt;/p&gt;&#10;&#10;&lt;p&gt;And besides, what is wrong with the statement &lt;em&gt;If the true model is in the set I am considering, then there is a 57% probability that it is model B&lt;/em&gt;.  Seems reasonable enough to me, or you could go the more &quot;soft&quot; version &lt;em&gt;there is a 57% probability that model B is the best out of the set being considered&lt;/em&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;One last comment: I think you will find about as many opinions about AIC/BIC as there are people who know about them.&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-05-13T14:06:44.390" Id="10763" LastActivityDate="2011-05-13T14:06:44.390" OwnerUserId="2392" ParentId="577" PostTypeId="2" Score="4" />
  
  <row AnswerCount="2" Body="&lt;p&gt;I have two cases:&lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;Two random poisson variables $X_1 \sim \text{Pois}(\lambda_1)$, $X_2 \sim \text{Pois}(\lambda_2)$, and testing:&#10;&lt;ul&gt;&#10;&lt;li&gt;Null Hypothesis: $\lambda_1 = \lambda_2$&lt;/li&gt;&#10;&lt;li&gt;Alternate hyp: $\lambda_1 \neq \lambda_2$&lt;/li&gt;&#10;&lt;/ul&gt;&lt;/li&gt;&#10;&lt;li&gt;Two random binomial variables $X_1 \sim \text{Binom}(n_1, p_1)$, $X_2 \sim \text{Binom}(n_2, p_2)$, where $n_1=n_2$:&#10;&lt;ul&gt;&#10;&lt;li&gt;Null Hypothesis: $p_1 = p_2$&lt;/li&gt;&#10;&lt;li&gt;Alternate hyp  : $p_1 \neq p_2$&lt;/li&gt;&#10;&lt;/ul&gt;&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;I started with likelihood ratio test for both cases, where I calculated maximum likelihood estimates for both cases, and then estimated likelihood ratio test statistics. But while reading some literature and discussion, I learned likelihood ratio test may not be a valid test in all cases, such as at low lambda values or one lambda low and other high, so for first case (poisson case), I could also explore the option of poisson random variable condition on sum of $X_1+X_2$ which will be $\text{Binom}(X_1+X_2, 0.5)$ and can be tested using Fisher exact test. Similarly for second case with two binom random variables a good choice will be McNamara test. And there are also some Bayesian tests which can be used (I do not know of any).&lt;/p&gt;&#10;&#10;&lt;p&gt;So I am wondering how could I do Fisher exact test for two poisson r.v condition on sum of them, and McNamara test for two binom, and if there is any bayesian test which i could use.&lt;/p&gt;&#10;&#10;&lt;p&gt;Preferably, i will try these test in R as I am trying to learn R. In past I received very good help and would like to thank in advance. thanks&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-05-13T14:45:17.463" FavoriteCount="1" Id="10766" LastActivityDate="2012-12-07T05:31:46.147" LastEditDate="2011-05-13T18:49:21.900" LastEditorUserId="4360" OwnerUserId="4098" PostTypeId="1" Score="2" Tags="&lt;r&gt;&lt;maximum-likelihood&gt;" Title="Two poisson random variables and likelihood ratio test" ViewCount="1035" />
  <row Body="&lt;p&gt;&lt;strong&gt;With respect to the question in the header&lt;/strong&gt;&#10;With logistic regression predicting posterior probabilities, the &lt;em&gt;dependent&lt;/em&gt; variable (outcome) is both bounded and continuous.&lt;/p&gt;&#10;&#10;&lt;p&gt;One train of thoughts to arrive at logistic regression in fact is thinking how to construct a regression with limits for the continuous outcome. &lt;/p&gt;&#10;&#10;&lt;ol&gt;&#10;&lt;li&gt;You want e.g. to do a regression directly on the probability&lt;/li&gt;&#10;&lt;li&gt;&quot;Common&quot; regression methods (e.g. linear regression) give you continuous output in the set of real numbers, $\mathbb R$.&lt;/li&gt;&#10;&lt;li&gt;But probabilities are in [0, 1]&lt;/li&gt;&#10;&lt;li&gt;So put a sigmoid transformation into your model to transform $\mathbb R \mapsto [0, 1]$&lt;/li&gt;&#10;&lt;li&gt;If you choose the &lt;a href=&quot;http://en.wikipedia.org/wiki/Logistic_function&quot; rel=&quot;nofollow&quot;&gt;logistic function $\frac{1}{1 + e^{-x}}$&lt;/a&gt; (a standard choice of a sigmoid), you end up  with logistic regression.&lt;/li&gt;&#10;&lt;/ol&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;With respect to modeling angles in general&lt;/strong&gt; I'd like to follow up with another question: how to model cyclic behaviour, how would I tell a model that 359° is almost the same as 0° (regardless of whether the variable is dependent or independent)?&lt;/p&gt;&#10;" CommentCount="8" CreationDate="2011-05-13T18:15:07.350" Id="10772" LastActivityDate="2011-05-14T07:40:35.677" LastEditDate="2011-05-14T07:40:35.677" LastEditorUserId="2669" OwnerUserId="4598" ParentId="10567" PostTypeId="2" Score="0" />
  <row Body="&lt;p&gt;If you permute the individual values then you are testing the combined null hypothesis that there is no difference between groups and that there is no structure within values from the same individual.  So if you reject the null you don't know if it is because the groups differ or because there is structure within an individual.&lt;/p&gt;&#10;&#10;&lt;p&gt;I would suggest permuting individuals and keeping all values with the individual.  &lt;/p&gt;&#10;&#10;&lt;p&gt;I don't know of a program that does this out of the box, but writing a permutation test in R is often easier/quicker than figuring out a precanned program.&lt;/p&gt;&#10;&#10;&lt;p&gt;Here is a simple example:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;library(nlme)&#10;&#10;Odist &amp;lt;- split(Orthodont$distance, Orthodont$Subject)&#10;&#10;out &amp;lt;- replicate( 1999, { tmp &amp;lt;- sample(Odist);&#10;    mean( unlist( tmp[1:16] ) ) - mean( unlist( tmp[17:27] ) ) } )&#10;&#10;out &amp;lt;- c(out, mean(unlist(Odist[1:16])) - mean(unlist(Odist[17:27])) )&#10;&#10;hist(out)&#10;abline( v=out[2000] )&#10;&#10;mean(out &amp;gt;= out[2000])&#10;&lt;/code&gt;&lt;/pre&gt;&#10;" CommentCount="2" CreationDate="2011-05-13T19:04:56.143" Id="10775" LastActivityDate="2011-05-13T19:29:16.703" LastEditDate="2011-05-13T19:29:16.703" LastEditorUserId="4505" OwnerUserId="4505" ParentId="10773" PostTypeId="2" Score="2" />
&#10;7 &amp;amp; 5 &amp;amp; 4\\
&#10;5 &amp;amp; 2 &amp;amp; 4
&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$
&#10;(\bar{x}_{*j}) = \left(\begin{matrix}5.25 &amp;amp; 2.75 &amp;amp; 3.5\end{matrix}\right)
&#10;$$&lt;/p&gt;&#10;&#10;&lt;p&gt;$$
  
&#10;5 &amp;amp; 2 &amp;amp; 3\\
&#10;7 &amp;amp; 2 &amp;amp; 3\\
  <row Body="&lt;p&gt;You can always choose one or several real valued functions of the categorical variables and look at the auto-correlation for the resulting sequence(s). You can, for instance, consider indicators of some subsets of the variables. &lt;/p&gt;&#10;&#10;&lt;p&gt;However, if I understood your question correctly, your sequence is obtained by an MCMC algorithm on the discrete space. In that case, it may be more interesting to look directly at the convergence rate for the Markov chain. Chapter 6 in &lt;a href=&quot;http://books.google.com/books?id=KF0LgxRCgQsC&amp;amp;printsec=frontcover&amp;amp;dq=bremaud+markov+chains&amp;amp;hl=en&amp;amp;ei=nDjOTcj5KZS-sAPavPizCw&amp;amp;sa=X&amp;amp;oi=book_result&amp;amp;ct=result&amp;amp;resnum=1&amp;amp;ved=0CEUQ6AEwAA#v=onepage&amp;amp;q=bremaud%20markov%20chains&amp;amp;f=false&quot; rel=&quot;nofollow&quot;&gt;this book&lt;/a&gt; by Brémaud treats this in details. The size of the second largest absolute value of the eigenvalues determines the convergence rate of the matrix of transition probabilities and thus the mixing of the process.  &lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-05-14T08:26:49.843" Id="10799" LastActivityDate="2011-05-16T05:34:30.250" LastEditDate="2011-05-16T05:34:30.250" LastEditorUserId="4376" OwnerUserId="4376" ParentId="10798" PostTypeId="2" Score="6" />
  <row AnswerCount="1" Body="&lt;p&gt;Is there someone who knows an econmetric model to describe count datas with an upper bound? Thank you! &lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-05-14T14:05:14.403" FavoriteCount="1" Id="10805" LastActivityDate="2013-01-17T04:31:53.147" OwnerUserId="4496" PostTypeId="1" Score="3" Tags="&lt;poisson&gt;&lt;count-data&gt;" Title="Count data regression for data with upper bound" ViewCount="210" />
&#10;F(x) = (1 - \mathrm{e}^{-x}),\ x &amp;gt; 0,
&#10;F_{Y}(y) = \left( 1 - \mathrm{e}^{-y} \right)^{\alpha}.
  <row AnswerCount="0" Body="&lt;p&gt;I am trying to evaluate the effect of an intervention. I have a group-randomised rct design and have established that I have a design effect and therefore need to control for school. So far I have managed to restructure my data so that it is now vertical and I have a variable called 'time' which represents my pre and post time points. Although I have randomised effectively in terms of SES, my recruitment method (inviting parents to take part in a parenting program versus a questionnaire study) has resulted in a significantly more distressed Intervention group when reported by parents (but not when reported by the child).&lt;/p&gt;&#10;&#10;&lt;p&gt;I have used the book &lt;em&gt;Multilevel and longitudinal Modeling with IBM SPSS&lt;/em&gt; by Heck, Thomas &amp;amp; Tabata, 2010, which is great, but I can not figure out how to run a simple two level repeated measures model. Does anyone know of a reference or is able to help me with syntax? I seem to get an error message with what I have used and don't know where to put school, my group variable, so I can control for it.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;MIXED SDQPT WITH Time Int_Cont&#10;  /CRITERIA=CIN(95) MXITER(100) MXSTEP(5) SCORING(1) SINGULAR(0.000000000001) HCONVERGE(0,ABSOLUTE) LCONVERGE(0, ABSOLUTE) PCONVERGE(0.000001, ABSOLUTE)&#10;/FIXED=Time Int_Cont Time*Int_Cont | SSTYPE(3)&#10;/METHOD=REML&#10;/PRINT=G  SOLUTION TESTCOV&#10;/RANDOM=INTERCEPT Time | SUBJECT(CodeNumb) COVTYPE(UN)&#10;/REPEATED=Time | SUBJECT(CodeNumb) COVTYPE(DIAG).&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Thanks for your help!!!&lt;/p&gt;&#10;&#10;&lt;p&gt;Is this it? I have established that autoregressive fits the model best (SCASPT is my measure of anxiety; Int_Cont is my grouping variable, and Time is 0=time 1 and 1= time 2): &lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;MIXED SCASPT BY Int_Cont Time&#10;  /CRITERIA=CIN(95) MXITER(100) MXSTEP(5) SCORING(1) SINGULAR(0.000000000001) HCONVERGE(0, &#10;    ABSOLUTE) LCONVERGE(0, ABSOLUTE) PCONVERGE(0.000001, ABSOLUTE)&#10;  /FIXED=Int_Cont Time Int_Cont*Time | SSTYPE(3)&#10;  /METHOD=REML&#10;  /PRINT=G  R SOLUTION TESTCOV&#10;  /REPEATED=Time | SUBJECT(School*CodeNumb) COVTYPE(AR1).&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;I would really appreciate if you know of a reference that would explain this in a really simple way for someone with very limited stats knowledge. I am finding the Heck reference a little too hard to follow. Thanks!&lt;/p&gt;&#10;" CommentCount="1" CreationDate="2011-05-15T13:17:55.733" Id="10821" LastActivityDate="2013-03-01T08:18:57.427" LastEditDate="2013-03-01T08:18:57.427" LastEditorUserId="7448" OwnerUserId="4269" PostTypeId="1" Score="1" Tags="&lt;mixed-model&gt;&lt;spss&gt;&lt;repeated-measures&gt;" Title="How do I run a Mixed Model repeated measures analysis in SPSS to evaluate intervention effect while controlling for school?" ViewCount="1395" />
  
  <row Body="&lt;p&gt;Partition the outcomes by the frequency of occurrences $x$ of the &quot;double outcome&quot;, $0 \le x \le t$.  Conditional on this number, the distribution of the remaining $t-x$ outcomes is multinomial across $n-1$ &lt;em&gt;equiprobable&lt;/em&gt; bins.  Let $p(t-x, n-1, x)$ be the chance that no bin out of $n-1$ equally likely ones receives more than $x$ outcomes.  The sought-for probability therefore equals&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\sum_{x=0}^{t} \binom{t}{x}\left(\frac{2}{n+1}\right)^x \left(\frac{n-1}{n+1}\right)^{t-x} p(t-x,n-1,x).$$&lt;/p&gt;&#10;&#10;&lt;p&gt;In &lt;a href=&quot;http://www.stat.purdue.edu/~dasgupta/mult.pdf&quot; rel=&quot;nofollow&quot;&gt;Exact Tail Probabilities and Percentiles of the Multinomial Maximum&lt;/a&gt;, Anirban DasGupta points out (after correcting typographical errors) that $p(n,K,x)K^n/n!$ equals the coefficient of $\lambda^n$ in the expansion of $\left(\sum_{j=0}^{x}\lambda^j/j!\right)^K$ (using his notation).  For the values of $t$ and $n$ involved here, this coefficient can be computed in at most a few seconds (making sure to discard all $O(\lambda^{n+1})$ terms while performing the successive convolutions needed to obtain the $K^{\text{th}}$ power).  (I checked the timing and corrected the typos by reproducing DasGupta's Table 4, which displays the complementary probabilities $1 - p(n,K,x)$, and extending it to values where $n$ and $K$ are both in the hundreds.)&lt;/p&gt;&#10;&#10;&lt;p&gt;Quoting a theorem of Kolchin &lt;em&gt;et al.&lt;/em&gt;, DasGupta provides an approximation for the computationally intensive case where $t$ is substantially larger than $n$.  Between the exact computation and the approximation, it looks like all possibilities are covered.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-05-15T19:07:36.967" Id="10830" LastActivityDate="2011-05-15T19:07:36.967" OwnerUserId="919" ParentId="10407" PostTypeId="2" Score="4" />
  <row Body="&lt;p&gt;&lt;a href=&quot;http://taxonomy.zoology.gla.ac.uk/rod/treeview.html&quot;&gt;TreeView&lt;/a&gt; -- it is not a statistical tool, but it is very light and I have a great sentiment to it; and it is easy to make output to &lt;a href=&quot;http://en.wikipedia.org/wiki/Newick_format&quot;&gt;Newick format&lt;/a&gt;, which TV eats without problems.&lt;/p&gt;&#10;&#10;&lt;p&gt;More powerful solution is to use R, but here you would have to invest some time in making conversion to the &lt;code&gt;dendrogram&lt;/code&gt; object (basically list-of-lists). &lt;/p&gt;&#10;" CommentCount="4" CreationDate="2011-05-15T20:17:01.317" Id="10834" LastActivityDate="2011-05-15T20:17:01.317" OwnerUserId="88" ParentId="10832" PostTypeId="2" Score="5" />
  
  <row AcceptedAnswerId="10843" AnswerCount="1" Body="&lt;p&gt;If you're modelling a proportion response against numerous predictors that are also proportions, is it necessary to transform the response if the standard OLS model is seemingly well behaved?&lt;/p&gt;&#10;&#10;&lt;p&gt;By well behaved I mean:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;None of the fitted values are outside the range [0,1] (In fact they are fairly accurate) &lt;/li&gt;&#10;&lt;li&gt;Residuals look good&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;I believe arcsine transform is typically used in this scenario to make the data look normal, but what if this is not needed?&lt;/p&gt;&#10;&#10;&lt;p&gt;Also, say the data wasn't normal, would a transform still be necessary if one were modelling the proportions with the Random Forest technique?&lt;/p&gt;&#10;&#10;&lt;p&gt;Cheers&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-05-16T02:23:47.057" FavoriteCount="1" Id="10841" LastActivityDate="2011-05-16T04:33:27.030" OwnerUserId="845" PostTypeId="1" Score="6" Tags="&lt;modeling&gt;&lt;data-transformation&gt;&lt;proportion&gt;" Title="Is it necessary to perform a transformation on proportion data if it's reasonably well behaved?" ViewCount="334" />
  
  <row AcceptedAnswerId="10853" AnswerCount="1" Body="&lt;p&gt;My current dataset has three conditions, and we've measured the activity levels of 10,000 genes in each condition. Replicated 8 times.&lt;/p&gt;&#10;&#10;&lt;p&gt;Using 10,000 linear models, we determine for each pair of conditions (ie for each of three contrasts) the number of genes with significantly different activity levels. This is &lt;a href=&quot;http://bioinf.wehi.edu.au/limma/&quot; rel=&quot;nofollow&quot;&gt;standard procedure&lt;/a&gt; for this kind of microarray data. &lt;/p&gt;&#10;&#10;&lt;p&gt;We find:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;2000 genes have significantly different activity levels between A and B&lt;/li&gt;&#10;&lt;li&gt;1500 genes have significantly different activity levels between A and C&lt;/li&gt;&#10;&lt;li&gt;100 genes have significantly different activity levels between B and C&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;This suggests that conditions B and C are more similar to each other than to C. PCA suggests the same result.&#10;Is there any way for us to quantify the extent to which &quot;conditions B and C are more similar to each other than to C (ie to put a p-value on it?)&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks for your help, and apologies if this question is trivial. &lt;/p&gt;&#10;&#10;&lt;p&gt;Kind regards,&lt;/p&gt;&#10;" CommentCount="0" CreationDate="2011-05-16T11:28:46.337" FavoriteCount="1" Id="10850" LastActivityDate="2011-05-16T12:12:57.250" LastEditDate="2011-05-16T11:51:00.613" LastEditorUserId="930" OwnerUserId="3773" PostTypeId="1" Score="5" Tags="&lt;genetics&gt;&lt;microarray&gt;" Title="Comparing numbers of p-values from many linear models" ViewCount="96" />
  <row AnswerCount="1" Body="&lt;p&gt;I have this long running experiment. Each time I run it I get a new goodness value, since the algorithm has random variables in it. So I need to report the mean and the std of some n runs. What should n be?&lt;/p&gt;&#10;&#10;&lt;p&gt;I need to be able to defend n based on some statistical ideas. Some kind of scientific reference (a book, a paper) would be wonderful, too.&lt;/p&gt;&#10;&#10;&lt;p&gt;I provide more details as you say, thanks for the answers:&lt;/p&gt;&#10;&#10;&lt;p&gt;In computer vision, an important challenge is to recognize objects from images. Different computer algorithms are developed for this purpose. To see how good a new algorithm is, one sometimes constructs a test and a training set of images, say 1000 images for each, train the algorithm with the training images, and produce a success rate using the test set. If out of the 1000 objects in the test images, 800 is recognized by the algorithm, the success rate is said to be 80 percent.&lt;/p&gt;&#10;&#10;&lt;p&gt;Now, my algorithm analyses, say, 1000 RANDOM points in the image, and using that analysis, tries to recognize the objects in the image. Each time I run the algorithm, I get a different success rate, since 1000 points are produced RANDOMLY. So I think its best to report some kind of summary statistics (e.g. the main and std deviation) of the success rate. &lt;/p&gt;&#10;&#10;&lt;p&gt;Also, one sometimes needs to say, &quot;well in addition to my algorithm, I tried these, say, 10 algorithms on the same dataset, and this table shows that mine is the best in this and this way...&quot; Some of these algorithms may need to run more than once, too. So one can really have a long experiment.&lt;/p&gt;&#10;&#10;&lt;p&gt;So, as I said before, at least how many times should I run the long running experiment?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thx.&lt;/p&gt;&#10;" CommentCount="3" CreationDate="2011-05-16T12:59:12.657" Id="10855" LastActivityDate="2011-05-18T03:09:31.003" LastEditDate="2011-05-17T12:27:13.450" LastEditorUserId="4629" OwnerUserId="4629" PostTypeId="1" Score="5" Tags="&lt;experiment-design&gt;" Title="Number of times to run a lengthy experiment" ViewCount="254" />
  <row Body="&lt;p&gt;In boxplots, values that are more than 1.5 times the IQR (interquartile range, difference between quartile 1 and 3) away from (as in: in the direction away from the median) the quartiles are typically considered outliers.&lt;/p&gt;&#10;&#10;&lt;p&gt;I cannot say whether this is an appropriate measure for your data, though...&lt;/p&gt;&#10;" CommentCount="2" CreationDate="2011-05-16T14:22:43.363" Id="10860" LastActivityDate="2011-05-16T14:22:43.363" OwnerUserId="4257" ParentId="10858" PostTypeId="2" Score="6" />
  
  <row AnswerCount="2" Body="&lt;p&gt;What are important/notable publishing houses for books in statistics?&lt;/p&gt;&#10;&#10;&lt;p&gt;When I come across a book published at O'Reilly or Springer I imagine its quality will be high.  What other notable publishing houses are out there (for statistics books)?  Any recommendation on a way to find out? (I'd imagine we could check it if we had the database dump of amazon, but I don't imagine something like that is available anywhere...)&lt;/p&gt;&#10;" CommentCount="7" CommunityOwnedDate="2011-05-16T21:48:29.193" CreationDate="2011-05-16T20:41:15.513" FavoriteCount="1" Id="10874" LastActivityDate="2011-10-30T03:14:35.967" LastEditDate="2011-10-30T03:14:35.967" LastEditorUserId="5256" OwnerUserId="253" PostTypeId="1" Score="7" Tags="&lt;books&gt;" Title="High quality publishing house for books in the field of statistics" ViewCount="295" />
  <row Body="&lt;p&gt;In explaining the meaning of regression coefficient I found that the following explanation very useful. Suppose we have the regression&lt;/p&gt;&#10;&#10;&lt;p&gt;$$Y=a+bX$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Say  $X$ changes by $\Delta X$  and $Y$ changes by $\Delta Y$.  Since we have the linear relationship  we have&lt;/p&gt;&#10;&#10;&lt;p&gt;$$Y+\Delta Y= a+ b(X+\Delta X)$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Since $Y=a+bX$ we get that&lt;/p&gt;&#10;&#10;&lt;p&gt;$$\Delta Y = b \Delta X.$$&lt;/p&gt;&#10;&#10;&lt;p&gt;Having this is easy to see that if $b$ positive, then positive change in $X$ will result in positive change in $Y$. If $b$ is negative then positive change in $X$ will result in negative change in $Y$.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; I treated this question as a pedagogical one, i.e. provide simple explanation.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;strong&gt;Note 2:&lt;/strong&gt; As pointed out by @whuber this explanation has an important assumption that &#10;the relationship holds for all possible values of $X$ and $Y$. In reality this is a very restricting assumption, on the other hand the the explanation is valid for small values of $\Delta X$, since Taylor theorem says that relationships which can be expressed as differentiable functions (and this is a reasonable assumption to make) are linear locally.&lt;/p&gt;&#10;" CommentCount="5" CreationDate="2011-05-17T05:36:19.520" Id="10885" LastActivityDate="2011-05-17T06:03:25.247" LastEditDate="2011-05-17T06:03:25.247" LastEditorUserId="2116" OwnerUserId="2116" ParentId="10884" PostTypeId="2" Score="6" />
  
  
